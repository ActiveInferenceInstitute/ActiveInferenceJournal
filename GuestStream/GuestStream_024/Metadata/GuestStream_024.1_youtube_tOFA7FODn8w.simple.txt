SPEAKER_03:
hello and welcome everyone it is june 28 2022 and we are here in actin flap guest stream number 24.1

Today we're here with Professor Steven Grossberg and the agenda will be as follows.

First, Ali will provide a short introduction.

We will then play a 45 minute prerecorded video followed by a Q&A.

So thanks everyone for joining and Professor Grossberg really appreciate joining and I'll pass to Ali for the introduction.


SPEAKER_00:
Hello and welcome.

I'm Ali.

I'm an independent researcher from Iran.

I'm very happy and excited to be here and be able to speak with Professor Grossberg today.

So I'd like to thank Professor Grossberg for joining us.

Steven Grossberg is the Wong Professor of Cognitive and Neural Systems and a Professor Emeritus of Mathematics and Statistics, Psychological and Brain Sciences, and Biomedical Engineering at Boston University.

For more than 50 years, he has led pioneering research in discovering and developing neural design principles for autonomous adaptive intelligence based on biological and machine learning.

His neural network models have been applied to many large-scale problems in engineering and technology, including the design of increasingly autonomous adaptive algorithms and mobile agents.

In fact, this is what Carl Friston says about him.

Whenever you claim to be the first to do this or that in artificial intelligence, it is customary and correct to add, with the exception of Steven Grossberg.

Quite simply, Steven is a living giant and foundational architect of the field.

Professor Grossberg is the recipient of the 2015 Norman Anderson Lifetime Achievement Award of the Society of Experimental Psychologists.

the 2017 Frank Rosenblatt Award of the IEEE Computational Intelligence Society, and the 2019 Donald O. Hebb Award of the International Neural Network Society.

His latest book, Conscious Mind, Resonant Brain, as a culmination of his decades-long research, written in a rather non-technical and conversational style, is published in 2021 by Oxford University Press

and is the winner of the Association of American Publishers in 2022 prose award for the best book of the year in neuroscience.

Now, I'll pass it to Professor Grossberg, and then we'll continue with the 45-minute pre-recorded lecture.


SPEAKER_03:
If you'd like to say hi, otherwise I'll begin the video.


SPEAKER_02:
I just saw my face frozen on the screen.

Well, I'm delighted to be here, and

I hope you find some points of interest in the lecture and I'll look forward to the Q&A.

Ali has prepared a series of questions that I've thought about and have some prepared sketched answers.

And then after that, if you're still interested, I'm happy to do live Q&A about anything related to the topics of the day.


SPEAKER_03:
Okay.

Onto the main course.

I will play the video now.

And you won't hear anything on the live stream.

I'll crop it and the audio will be coming through fine now.


SPEAKER_02:
Hello.

I'm delighted to be able to speak to you today about a topic concerning

artificial intelligence, which, as you know, is very much in the news these days.

And I'll be contrasting two very different approaches to artificial intelligence.

But to do that, I need to pull up my PowerPoint slides and share them with you.

And let me maximize them and minimize my face.

So my topic today is Explainable and Reliable AI, Comparing Deep Learning with Adaptive Resonance.

This lecture is based on the following article from this year, which is both open access and on my webpage.

The article summarizes core problems of deep learning

such as its untrustworthiness because it's unexplainable and its unreliability because it experiences catastrophic forgetting.

The article explains how adaptive resonance overcomes these problems, indeed, overcomes 17 problems of deep learning and outlines a blueprint for achieving autonomous adaptive intelligence.

The article is part of a Frontiers in Neuro-Robotics special issue about explainable AI, whose editors wrote, and I quote, though deep learning is the main pillar of current AI techniques and is ubiquitous in basic science and real world applications, it is also flagged by AI researchers for its black box problem.

It is easy to fool.

and it also cannot explain how it makes a prediction or decision.

In other words, deep learning is not trustworthy.

No life or death decision, such as a medical or financial decision, can confidently be made based upon a deep learning prediction.

Deep learning uses the back propagation algorithm

for learning how to predict output vectors in response to input vectors.

That propagation was based on perceptron learning principles that Frank Rosenblatt started to introduce in the 1950s.

It has a complicated history, which Juergen Schmidhuber beautifully reviewed in an article from this year.

Major contributors include Shinichi Omari, Paul Werbos, and David Parker.

Perhaps one would say that it reached its modern form with simulated applications in Paul's 1974 paper before being popularized 12 years later by Rummel, Hart, Hinton, and Williams.

Here's a schematic of a back propagation circuit reprinted from a survey article by Gail Carpenter of neural network models.

In it,

Information flows feed forward from an input stage to an output stage.

Learning is supervised by an external teacher who on each trial defines a target or desired output.

The teaching signal is the error or mismatch between the actual and the target outputs.

The teaching signal in level F3

of adaptive weights in level F2 have no network pathway whereby to reach from F3 to F2 within the algorithm.

So the algorithm uses an artifice called weight transport, which physically lifts the weights from here and moves them there so that they can be used to control learning.

Well, this is clearly a non-local operation as well as being clearly

non-biological.

Backpropagation learns through slow learning, which means that the adaptive weights change just a little to reduce error on each learning trial.

That requires many trials, that is to say many repetitions of the whole database to learn, possibly hundreds or thousands of trials.

This is to be contrasted with fast learning,

where adaptive weights zero error signals on each trial, just as we can learn a face that we see just once and remember it for a long time.

If backprop tried to use fast learning, it would become wildly unstable.

Catastrophic forgetting also occurs in backprop, so that during any learning trial, an unpredictable part of its learned memory can unexpectedly collapse.

So deep learning, which is based on backpropagation, is thus neither reliable nor trustworthy.

But why is this?

One reason is that all inputs are processed by a shared set of learned weights.

The algorithm cannot selectively buffer learned weights that are still predictably useful.

In particular, there's no attention mechanism.

This problem occurs in any learning algorithm whose shared weight updates follow the gradient of the error in response to the current batch of data points while ignoring past batches.

There have been multiple efforts to fix back propagation.

One is to selectively slow learning on the weights important for learning by optimizing parameters using the Bayes rule, as Kirkpatrick et al.

suggested a few years ago.

But that assumes an omniscient observer who can discover and alter the important weights, as well as non-local computations, such as the Bayesian computation.

The same problem occurs with evolutionary algorithms and diffusion-based neuromodulation and other approaches to try to fix back problems.

These efforts to overcome catastrophic forgetting created additional conceptual and computational problems.

I view them as adding epicycles to ameliorate a fundamental flaw in the model, which to me is reminiscent of adding epicycles to correct problems in the Ptolemaic model of the solar system.

As we all know, the Copernical model that we now accept didn't require epicycles.

Perhaps this is why Jeffrey Hinton, who played a key role in developing both backprop and deep learning, said in an Axios interview a few years ago that, quote, he's deeply suspicious of backpropagation.

I don't think it's how the brain works.

We clearly don't need all the labeled data.

My view is throw it all away and start over.

I would claim we don't have to start over

because these problems were solved in the 1970s and 1980s.

In particular, in the first issue of the journal Neural Networks in 1988, I had an article that listed 17 problems of backpropagation that are overcome by adaptive resonance.

And here they are.

With regard to not needing all the label data, I noted in the third item here that self-organized unsupervised or supervised learning frees us from needing labels all the time.

As to slow learning, I noted that in ART you can have fast or slow learning.

Indeed, ART can learn to classify an entire database using fast learning on a single learning trial.

as Gail Carpenter and I showed in the 1980s.

Moreover, art overcomes all 17 problems of backpropagation without the cycles.

Furthermore, all the core art predictions have been supported by subsequent psychological and neurobiological data.

Indeed, art is a principled,

biological and technological theory, unlike back-prop and deep learning, which are just algorithms.

ART has explained data from hundreds of experiments and it's made scores of predictions that have subsequently received experimental support.

But why has ART been so successful?

There are a number of reasons, but one of them is

that art can be derived from a thought experiment about a universal problem in error correction that I published 40 years ago in Psychological Review.

The thought experiment asks the question, how can a coding error be corrected if no individual cell knows that one has occurred?

Let me quote from my paper.

The importance of this issue becomes clear when we realize that erroneous cues can accidentally be incorporated into a code when our interactions with the environment is simple and will only become evident when our environmental expectations become more demanding.

And even if our code perfectly matched a given environment, we would certainly make errors

as the environment itself fluctuates.

So I was talking about autonomous local learning in a changing world.

A purely logical inquiry into error correction is translated at every step of the thought experiment into processes learning autonomously in real time

with only locally computed quantities.

Moreover, the thought experiment uses familiar environmental facts about how we learn as its hypotheses and odd circuits naturally emerge, where these facts are familiar because they're ubiquitous environmental constraints on the evolution of our brains.

And since we're living with them all the time, they become familiar.

Because of this universality, art circuits may thus in some form be embodied in all future truly autonomous adaptive intelligent devices, whether biological or artificial.

Art has probably for this reason already been used in many large scale engineering and technological applications.

In fact, almost immediately after art was introduced,

It began being used because it succeeded in benchmark studies against machine learning, back propagation, statistical methods, genetic algorithms, either getting much better accuracy or much faster training speed or both.

It's also used in applications where other algorithms totally fail, such as the Boeing company's part design reuse and inventory compression

application.

That's just one of many large-scale applications in engineering and technology, some of which can be found on our tech lab webpage at bu.edu.

The Boeing parts design retrieval system in particular was used to help design the Boeing 777.

And to do that, you needed fast learning and stable memory to learn and search a huge and continually growing non-stationary parts inventory.

At the time of this application, there are already 16 million, 1 million dimensional vectors that were used to

describe each of the parts and you have to be able to quickly search the inventory if you wanted to find a part to use in a new plane, especially if your new design might have a part in the inventory that was similar to it, finding it and slightly modifying design could save millions of dollars in fabrication costs.

Satellite remote sensing is another large scale application

that art was used for very soon.

And Gail Carpenter and her colleagues took the lead here.

For example, using a very small number of pixels of ground truth of 17 vegetation classes, they used art to automatically complete these maps using remote sensing data.

Art did it in a day, rapidly and automatically.

It gave a confidence map for each pixel, and the pixels were 30 meters in scale, which was small enough to see roads.

This contrasted with an AI expert system, which took a whole year to do with, and it had to derive ad hoc rules from experts.

You had to correct upwards of a quarter of a million site labels.

And even so, the pixel size was an order of magnitude larger.

Gail went on with her colleagues to study information fusion in remote sensing.

Let's say you have multiple observers.

Each of them may be using different labels.

Their labels may also be incomplete or missing or even incorrect.

And the task was to derive consistent knowledge from potentially inconsistent data to automatically learn and stably store one to many mappings.

And along the way, Gail and a colleague showed how to self-organize a hierarchy of cognitive rules, including confidence measures

between these different levels of the hierarchy.

There's been continual work on odds.

Some more recent work was summarized in a special issue of neural networks just in December 2019 that was edited by Donald Bunch, who started this special issue with a general overview of neural network models that I and my colleagues developed.

and then went on in a long and detailed article with several collaborators to provide a survey of adaptive resonance theory neural network models for engineering applications to the present time.

So backpropagation and deep learning are a feed-forward adaptive filter, but art is more than that.

In fact, art is an explainable, self-organizing production system in a non-stationary world.

What do these words mean?

Art's self-organizing because it can autonomously carry out arbitrary combinations

of unsupervised or supervised learning trials with the world as its only teacher.

It's a production system because it uses hypothesis testing to discover and learn rules by a top-down matching process that focuses attention on critical feature patterns.

These are the patterns that predict behavioral success

while suppressing irrelevant features.

Or it's explainable using both its activities or short-term memory STM traces and its adaptive weights or long-term memory LTM traces, activation dynamics, learning dynamics.

Observing the STM traces in a critical feature pattern explain what recognition categories will learn to code

and what features predict goal-oriented actions.

In particular, the long-term memory traces in the fuzzy art map algorithm translate into explicit fuzzy if-then rules that code what combinations of critical features in what numerical ranges effectively control predictions, thereby illustrating one of many examples where neural networks can learn rule-based behaviors.

ART includes a bottom-up adaptive filter of feedforward neural network, as I've observed already, but that's supplemented by top-down learned expectations and two types of recurrent inhibitory feedback interactions that help to choose the recognition categories and the critical features.

Notably, top-down expectations use what Gail Carpenter and I call the ART matching rule to learn how to focus attention,

on critical features that control predictive success.

The art matching rule is another way of talking computationally about the process of object attention, how we pay attention to salient objects in the world.

And we show how it stabilizes learning and thereby avoids catastrophic forgetting.

Remarkably, and this has been supported by many data,

the art matching rule can be realized by a top-down modulatory on-center off-surround network.

Well, what does this mean?

Well, let's say we have bottom-up inputs from external features to feature selective cells that get stored in short-term memory.

Let's say we activate a recognition category, which has previously been learned and tries to read out its learned

excitatory prototype.

Well, it can't fully do so because it also reads out an inhibitory off surround that's broader than the prototype.

So this is approximately one excitatory against one's inhibitory.

It can only give you a modulatory on center.

But if you have both bottom-up inputs and the top-down expectation simultaneously active,

then within the bounds of the prototype, if you also have a bottom-up feature, you have two excitatory against one inhibitory, and those features can be selected, gain amplified and synchronized to start focusing attention on this critical feature pattern, while outlier features, the ones that aren't within the prototype, only have one excitation against one inhibition,

are suppressed.

And in 1999, I was able to begin to understand how laminar cortical circuits carry out object attention.

In particular, layer six of a higher cortical area can activate layer six of a lower cortical area, either directly or via layer five.

And then it can fold up the layer four

to modulate an on-center and to inhibit an off-surround.

So attention acts via a top-down, modulatory on-center, off-surround network via folded feedback within laminar neocortex.

And this is one example of the paradigm of laminar computing that I introduced, which asks why are all neocortical circuits organized in layers?

And how do laminar circuits give rise to all kinds of biological intelligence?

Adaptive resonance answers this story because attended feature clusters reactivate their bottom-up pathways.

Activated categories reactivate their top-down pathways, closing an excitatory feedback loop between features and categories, giving rise

to a feature category resonance that synchronizes, amplifies, and prolongs system response between the attended critical features and the category to which they are bound.

And it's this resonance that triggers fast learning in the bottom-up and top-down adaptive weights, which is why I have called the theory adaptive resonance theory

Moreover, I've done a lot of work since then showing that all conscious states are resonance states, and these feature category resonances are one example of that, one that supports conscious recognition of visual objects and scenes.

There's a lot of data support for art predictions.

It's well known that attention does have an on-center or surround circuit behind it.

and that attention can facilitate matched bottom-up signals, many other data as well.

So now we can say more about why art is explainable or trustworthy.

In short-term memory, it's because the critical feature patterns determine the attentional focus that controls information processing, and you can just read off what those features are.

In long-term memory, again, it's the critical feature patterns that determine the adaptive weights learned by the bottom-up adaptive filter and the top-down learned expectation.

So you know also what these weights are encoding.

Oughts reliable and avoids catastrophic forgetting because outlier features that are not in the critical feature pattern are suppressed so that only the predictive features are processed and coded.

Or it's a production system because it carries out a kind of hypothesis testing.

And this is nicely illustrated in the simplest art model called Art 1 that Gail Carpenter and I published in 1987.

Art 1 has an attentional system that does all the category learning and the expectation learning and the paying attention that interacts with an orienting system, which is activated

when there are big enough matches in the attentional system and thereby drives a reset and search for novel or better matching categories.

Here's a schematic of the art hypothesis testing and learning cycle.

So let's say you have a bottom-up feature pattern coming in.

There may be many, many active bottom-up features, but I'll draw just one arrow here for simplicity.

but that vector of input features can activate a distributed pattern of feature detector cells.

Some may be very active, some not so active, some not active at all.

And as this is happening, each of these active pathways is trying to turn on the orienting system.

So there might be quite a few inputs converging here, but as the features are activated, each of them tries

to inhibit the orienting system, and there are as many features as there are inputs, so this excitation inhibition of balance, keeping the orienting system quiet as the feature pattern goes through the adaptive filter and chooses a category.

That category reads out a learned top-down expectation that obeys the art matching rule, which can suppress some mismatched features

thereby reducing the amount of inhibition on the orientic system and raising the question, when you have too little inhibition and too much excitation, how big a mismatch will activate the orientic system and cause reset?

And that ratio is determined by what's called vigilance, which I'll say more about soon.

But if you don't have enough inhibition, then the orienting system gets activated.

It equally activates all the cells in the category layer because it doesn't know which cell may be active or not.

So it causes a novelty sensitive, nonspecific burst of arousal.

Novel events are arousing, thereby selectively shutting off the active category

eliminating its top-down expectation and unmasking the original feature pattern, which can again go through the adaptive filter.

However, now this previously disconfirmed category remains off and the category level is renormalized, so it responds to the same input pattern with a new category.

And you go through this cycle of resonance and reset

until you get a good enough match to either learn a new category or select a previously learned category.

And it's a theorem that as categories are learned through this matching process, search automatically disengages leading to direct access without search to the globally best matching category.

Explaining, for example, how

we can quickly recognize familiar objects like your mother's face, even if as we get older, we store enormous numbers of additional memories.

So you don't have to search your whole repertoire.

When you see mom, you get direct access and quickly say, hi, mom.

There's a lot of support for the hypothesis testing cycle.

One source of support is from the ventrolate potentials, also called human scalp potentials,

which shows correlated sequences of three different evoked potentials during oddball learning tasks, an experiment that John Paul Banquet and I reported in the 80s, where you'll get a P120 for a mismatch, an N200 for the arousal activated by the ordinating system, and a P300 for the short-term memory reset of the category layer,

thereby supporting the processing stages of the search cycle.

There was also physiological data from intratemporal cortex where categories are learned early on from the lab of Bob Desimone who showed an active matching process that's reset between trials during this kind of event.

There's also classical data about hippocampal mismatch dynamics.

It's known that novelty potential subside as learning proceeds from numerous experiments.

This is as the orienting system is disengaged.

And there's more recent data using multiple electrode studies from the lab of Earl Miller from prefrontal cortex and simultaneous recordings in hippocampus

And they show there's rapid object-associative learning may occur in prefrontal cortex, which is a projection of infratemporal cortex, one of the stages of category learning, while the hippocampus may guide neocortical plasticity by signaling success or failure.

Well, this is just what happens when the attentional system interacts with the orienting system.

There's also complementary computing in art.

In particular, the attentional and orienting system was a complementary as manifested by the fact that two event-related potentials are complementary, processing negativity and M200.

Processing negativity is activated when there's a top-down match in the attentional system.

The M200 is

As I just noted, it's activated when there's a mismatch that activates the orienting system.

And you can just look across these four rows and see that these two kinds of ERP potentials are manifestly complementary as illustrated the complementarity of the attentional and orienting systems.

So this leads us to discuss another paradigm introduced,

which I call complementary computing, that asks, what is the nature of brain specializations?

Complementary computing introduces new principles of uncertainty and complementarity that clarify why there are multiple parallel processing streams with multiple processing stages in our brains.

And a beautiful example of that is this famous image

of the macro circuit of the visual system from David Van Essen and his colleagues, where you can see these multiple parallel processing streams and the multiple stages needed to achieve what I call hierarchical resolution of uncertainty.

But what are complementary properties?

They're analogies, like a key fits into a lock, or puzzle pieces

fitting together.

In words, computing one set of properties at a processing stage prevents that stage from computing a complementary set of properties.

These complementary parallel processing streams are balanced against one another.

It's a very yin-yang kind of situation, and interactions between the streams overcome their complementary weaknesses.

In fact, there are many complementary processes that are known in the brain that have been modeled.

Here is just five of them.

There are many more.

So this is a basic principle of brain organization.

So in summary so far, back propagation and deep learning do not have thought-to-memory activation patterns, including critical feature patterns, so they can't pay attention.

Indeed, they don't have any

fast information processing, nor do they have long-term memory top-down learned expectations, so they can't carry out hypothesis testing using interactions, short-term and long-term memory traces.

Indeed, there's no neural architecture, there's just an algorithm in really great contrast with complementary computing, which discusses the global organization of our brains.

From the very start, it was shown how easy it is to get catastrophic forgetting.

And Carpenter and I showed it in art when we would shut down the art matching rule.

Then we demonstrated you could get catastrophic forgetting if you had just four input vectors, A, B, C, D, presented in the order A, B, C, A, D, A, B, C, A, D, and so on.

if they obeyed very simple subset relationships.

And here's a computer simulation of that.

Here you don't have the arc matching rule.

Here's A, B, C, A, D.

A, B, C, A, D. And you see A is coded by category one here, by category two here, by category one here, two here, it never settles down.

But as soon as you impose the art matching rule, learning is complete by the second trial.

And after that point, you get direct access to the globally best matching category.

Well, let's say a little more about vigilance.

Vigilance determines what features are learned in the critical feature pattern.

It clarifies how our brains learn concrete knowledge for some tasks and abstract knowledge for others.

In particular, high vigilance leads to learning of narrow, concrete categories, like a category that fires selectively to a frontal view of your mother's face,

Low vigilance leads to learning of broad and abstract categories like everyone has a face.

It should be emphasized that critical feature patterns are explainable at every level of vigilance.

It's known from physiological experiments by Desimone again that their vigilance

control in the infratemporal cortex, which they showed by studying easy versus difficult discriminations in monkeys.

And in the difficult condition, which you'd assume would give you high vigilance, as expected, you had enhancement of the responses and sharpened selectivity to the intended stimuli.

How is vigilance computed?

Well, let's say of input vector, it instates

a vector of activities and feature detectors at the same time as it tries to activate the orienting system.

But it does so multiplied by a parameter rho, which is a sensitivity or gain parameter, that's vigilance.

And as these features get in state, they try to shut off the orienting system.

And if the excitation is less,

Then the inhibition, the orienting system stays quiet so the system can resonate and learn.

But if inhibition isn't strong enough, the orienting system gets activated.

You get reset and search for new categories.

This is a very simple computation because you have an orienting system that's complementary to the attentional system.

Well, how do you change vigilance based on predictive success?

For this, we have to go from unsupervised to supervised art models.

So we'll have an unsupervised art A model, an unsupervised art B model, linked together by a learned associative map, as occurs in fuzzy art map.

And the key point is you can have an input here that can create an output there because you have both bottom-up and top-down connections at all these levels.

So in this way you can learn many-to-one and one-to-many maps.

One example of a many-to-one map is let's say you're trying to categorize visually processed a letter A, which comes in multiple fonts.

You'll learn various visual categories of A based on visual similarity.

At the same time, you're learning auditory categories for saying A

And then the associated map can map all of these visual categories of different A's to saying A. But it could have been here that these inputs were symptoms, tests, and treatments in a medical database prediction example, and you're predicting length of stay in the hospital.

The possibilities here are endless, and they've been in many applications.

Or let's say...

you're trying to figure out what this image is, and you've learned to say that's a dog.

But today you say it's Rover, and that causes a mismatch which drives the search to focus attention on the particular combination of features in this dog that will identify it as Rover.

That leads to learning of a visual category of Rover, an auditory category for the name Rover, an associative map between them,

and you can now simultaneously store expert knowledge about that image.

Well, how do you conjointly minimize predictive error and maximize generalization so that you minimize using memory resources?

Let me read you an answer and then show what it means in images.

Match tracking realizes a minimax learning principle

namely given a predictive error, vigilance increases just enough to trigger search and thus sacrifices the minimum generalization to correct the error.

So let's say you've made a prediction that must mean that vigilance is less than the analog match between bottom up and top down.

But let's say now you have a mismatch.

Well, that'll lead to a match tracking signal

that bumps vigilance up till it's just above the analog match, just big enough to drive a search.

So you've given up the minimum amount of generalization to correct the error.

Well, are art mechanisms like vigilance control realized in lamina cortical and thalamic circus?

The answer is yes.

My PhD student Max Versace and I showed this by developing the synchronous matching art or SMART model

which introduced a lot more neurophysiological and anatomical verisimilitude into the model, including spiking dynamics, lamina cortical circuits interacting with specific and non-specific thalamic nuclei.

This is another example of lamina computing.

And here's a schematic of the model.

You see all the cortical layers with identified cells, a hierarchy of cortical regions,

interacting with specific thalamic nuclei and non-specific thalamic nuclei.

A ton of anatomical data got functionally explained in this way, and many other data as well.

For example, we showed if you have a good enough match between bottom-up and top-down, you're going to get fast gamma oscillations during a tension.

There was quite a bit of data about that already,

But we also showed if you have a big enough mismatch, you'll get slower beta oscillations.

That wasn't well known.

But since that time, there have been experiments in at least four labs in three different parts of the brain confirming that prediction.

Most important vigilance control was shown how to be controlled by mismatch-mediated acetylcholine release, a big enough mismatch,

in the nonspecific thalamic nucleus, activates nucleus space cell as a minor that releases acetylcholine and layer five cells across the cortex, reducing after hyperpolarization currents and causing vigilance to go up.

And I also showed that breakdowns in acetylcholine modulation can help to explain the symptoms of multiple mental disorders.

So as to memory consolidation, we know there's a dynamic phase of memory consolidation while the input exemplar still drives memory search and before direct access occurs.

But what if the orienting system's cut out?

What if you have a lesion in the hippocampus?

Well then, as occurs in medial temporal amnesia, you get unlimited anterograde amnesia because you can't search for new categories.

You get limited retrograde amnesia because you can have direct access to previously learned categories.

This is a failure of consolidation, which is mediated by the orienting system.

You get defective novelty reactions because that is also mediated by the orienting system.

And memory consolidation novelty detection are mediated by the same structures for the same reason.

There's normal priming because priming occurs within the attentional system.

Learning of the first item dominates.

You can get some learning, but you can't then search.

And there's an impaired ability to attend to relevant dimensions of stimuli, again, because you can't search.

So now, where does intratemporal cortex fit in within the larger brain?

I introduced...

the predictive art or part algorithm model in order to show how the prefrontal cortex, among other things, learns to control all higher order intelligence.

You can find that in a 2018 paper on my webpage.

I also published it, open access.

And in this macro circuit,

These green areas of prefrontal cortex control processes like working memory, learned plans, prediction, optimized action.

These regions in red control processes like reinforcement learning, emotion, motivation, adaptively timed learning.

The category of learning I've talked about in IT is just in those two regions.

All these processes

control visual perception.

And there are detailed models of all of these regions and their interactions now.

And each brain region in nature and in predictive art carries out a different function, contrasting really dramatically with the homogeneous organization of a typical deep learning network.

So I've told you just a little bit about some aspects of cognition and why they're explainable.

But if you put in all the biological models of perceptual cognition, emotion, and action, they're all explainable.

And then you can assert how perceptual and cognitive processes use art-like excitatory matching and match-based learning to create self-stabilizing

attentive and conscious representations of objects and events that embody increasing expertise about the world.

Moreover, complementary spatial and motor processes that I couldn't mention at all use inhibitory matching and mismatch-based learning to continually update spatial and motor representations to compensate for bodily changes throughout life.

Taken together, they provide

a self-stabilizing perceptual and cognitive front end for conscious awareness and knowledge acquisition, which can intelligently manipulate the more labile spatial and motor processes that enable our changing bodies to act effectively on a changing world.

And when you put them all together, they provide a blueprint for designing autonomous adaptive algorithms and mobile technologies.

robots with behaviors humans can understand and control because they're both explainable and reliable.

See my webpage, sites.vu.edu, Steve G., for these models.

And with that, I'd like to thank you very much for your attention.

Before we get started, what I want to say, so

I should say that everything I've talked about and much more is in my book, Conscious Mind, Resonant Brain, How Each Brain Makes a Mind.

For those who don't know, it's self-contained and non-technical.

It's written in a conversational style so that people who know nothing about the mind or the brain can enjoy reading it and

I have friends who are a rabbi, a minister, a painter, a gallery owner, a lawyer, a social worker, who've all been enjoying reading it.

Also, it's a big book.

It's almost 800 double column pages with over 600 color figures, so everything is illustrated.

But instead of costing $150, it costs

$35 for the hard copy and only $17 for the Kindle because I spent a lot of my own money so that people who are interested in the topic can read it.

And one other comment, if people do have questions or comments,

about my lecture or anything they read in the book, my email is just steve, S-T-E-V-E, at buaustinuniversity.edu, and I'll be happy

to try to apply.


SPEAKER_00:
Thank you.

Now, some researchers in explainable AI, people like Leonardo, Giovanna, and Antonio Di Cecco, demand that any explainable AI should at the very least meet these four criteria.

To be fair, not biased in one way or another, to be accountable or reliable,

to be secure against malicious hacker attacks and not to be fooled easily, and also to be transparent.

Now, you explained how adaptive resonance theory or ART, and by the way, I gotta say, I love your creative and clever use of acronyms for your models,

My favorite one is sovereign model, self-organizing, vision, expectation, recognition, emotion, intelligent, goal-oriented, navigation, if I'm correct.

Amazing.

Anyway, you explained how art can address and overcome the issues of accountability, security, and transparency of current deep learning approaches.

But it seems that this fairness issue, aka the problem of algorithmic bias, has also been a growing concern lately, especially since it's regarded by some researchers like Antonio Baria as a practically intractable problem.

So I wanted to ask, in what ways do you think art can contribute to the ongoing quest for mitigating this problem?


SPEAKER_02:
Well, when...

Holly sent me this question.

I said, well, first, I'd like you to send me a definition of algorithmic bias that will clarify what you have in mind so that I know what I'm trying to respond to.

And you wrote me that you borrowed the term from Badia's book, The Information Manifold, and you sent me a quote from page 247 that I will quote in part before

I respond to that background information.

So there are two main reasons for algorithmic approach to decision-making that may result in unfair outcomes, either at the individual or group level.

One is that data used is biased, and another is that the algorithm analyzes the data in such a way that it yields biased results.

The basic point to remember is that algorithms are designed to achieve a certain

goal not created naturally by evolution or accident.

Thus, most algorithms are written to detect certain patterns of interest for a particular objective, not just any pattern.

To be able to pick out some patterns and disregard others, programmers build a model of the data by listing expectations about what data should be like in order to qualify as relevant to the problem.

Well, as I'll explain below, self-organizing learning classification prediction models like adaptive resonance theory or ART overcome all the problems.

It's a general purpose device.

But why don't I try to answer that as part of my replies to Ali's subsequent questions?


SPEAKER_01:
Okay, thank you so much.


SPEAKER_00:
Now, as you also mentioned in your lecture in 1988, you pointed out 17 issues with backpropagation in one of your most famous and highly cited papers on nonlinear neural network.

So it's been 34 years now.

Now, do you see any fundamental Copernican change of perspective happening in deep learning research, or we still keep adding epicycle upon epicycle to our Ptolemaic model?


SPEAKER_02:
Well, you've sort of anticipated what I'm going to say.

And as I said in my lecture, various investigators, and I mentioned Klund,

Patrick and Velez have recently attempted to modify deep learning to overcome some of these problems.

But as Ali just mentioned, my lecture noted that, at least in my mind, they're like epicycles that are added to a kind of Ptolemaic model of the solar system to overcome some of its problems.

But as we all know,

the Ptolemaic model ultimately crashed because it was both qualitatively and quantitatively wrong.

And they could only be solved by throwing out the Ptolemaic model and replacing it with the Copernican model that became the basis for modern astronomy and astrophysics.

So art overcomes foundational deep learning problems

that can't be solved using epicycles.

And I've already done it shortly after I introduced it in 1976.

And it can't be overemphasized.

As I noted in my lecture, deep learning is untrustworthy because it's not explainable.

And it's unreliable because it can experience catastrophic forgetting.

And that happens for a basic reason.

Deep learning, just like backprop, which is its learning engine, is just a feed-forward adaptive filter.

So as you note in your question, I describe these two problems in addition to 15 others in my oft-cited article that I published in 1988 in the first issue of Neural Networks.

And I also showed then that art had already solved the problems in 1976.

And what I find sad is that back propagation and deep learning architects like Jeff Hinton, who knows all of this background, never mentioned this history and keep talking about making deep learning explain the brain.

but it can't explain the brain because its foundation is contradicted by basic psychological and neural data.


SPEAKER_01:
Yes, great.


SPEAKER_02:
Someone should be in the deep learning community.

I like comparative discussion and criticism, but I don't like solipsism in science.


SPEAKER_00:
Great, thank you.

Now, on slide number 50 of your presentation, you pointed out that art is inconsistent with models where top-down match is suppressive, such as Bayesian explaining away.

A similar view is evident on page 195 of Conscious Mind, Resonant Brain, to which he also adds, one of many serious problems of the Bayesian models is that fully suppressive matching circuits cannot solve the stability-plasticity dilemma.

Now, would you care to further elaborate on this point?


SPEAKER_02:
Sure.

My lecture and my book summarizes my book

My lecture couldn't go into a lot of it.

Some of the copious psychological, anatomical, and neurophysiological evidence

expectations, which obey what Gail Carpenter and I call the art matching rule, are matched against bottom-up input patterns.

And as the lecture briefly noted, the art matching rule is defined by a modulatory on-center off-surround network.

And the modulatory on-center is excitatory.

However, acting by itself

It can't fully excite its target cells.

It can prime them, sensitize, excuse me, or modulate them to be ready to fire vigorously when matched bottom-up inputs arrive.

And when there is a good enough match between the bottom-up inputs and an active top-down expectation that's reading out a circuit that obeys the ARC matching rule,

That's when you get what I noted in my lecture, what I call a feature category resonance, because it develops between the matched or attended features and the recognition category that they activate.

And it's this resonance that synchronizes and gain amplifies the matched features while suppressing the mismatched features.

And that sustained

Resonance is important because it's sustained long enough to drive learning in the more slowly varying adaptive weights of the active bottom-up filter and learning top-down expectation.

And it's because resonance triggers learning that I call the theory adaptive resonance theory.

And the arc matching rule avoids catastrophic forgetting, as I briefly mentioned

in the lecture because it suppresses irrelevant features using its off surround while it's amplifying and focusing attention on the critical features that regulate both bottom-up and top-down learning as well as successful predictions because they're relevant.

They've been selected by previous learning experiences

which discover the set of features that are predictive or causal in a given situation.

And along the way, not only does the art matching rule achieve causality in predictions, although as the world changes, you have to update your causal explanations, it also solves the stability of plasticity dilemma.

In brief, purely suppressive matching can't do any of this.

It shuts off the expected data, and so it can't focus attention or learn about it.

And there is fully suppressive matching in spatial and modal learning.

But that isn't learning to be expert about the world.

I could explain that more if you want to know, but that's also in my book.

And these two kinds of learning, the excitatory match-based learning and the inhibitory-mispatched learning, are computation complementary.

It's another example of complementary computing and

The match-based learning goes on in the ventral or Watt cortical stream, and the mismatch learning goes on in the wear or dorsal cortical stream, the Watt stream for perception and categorization and prediction, the wear stream for spatial representation and action, and then you need what to wear and where to what interactions so that you can

reach for and otherwise engage through approach and what have you, look at, reach for, approach the things that we recognize.


SPEAKER_00:
Thank you.

Now, following from the previous question and considering that the free energy principle and active inference framework as works in progress,

are related to predictive coding and Bayesian brain hypothesis, what is your view on the extent of compatibility between art and active inference?

Because despite some prima facie similarities between the two, do you see them as fundamentally incompatible or irreconcilable?

And how could this issue be rigorously evaluated and positively resolved in terms of reconciliation or integration of art

and active inference or otherwise.

Because you see, to add some more context here, Smith et al in their recent paper, An Active Inference Approach to Modeling Structure Learning, have stated that although they have not explicitly incorporated art's top-down attentional and feedback mechanisms,

There are mechanisms within their active inference-based model which they believe are quite similar to top-down and bottom-up feedback exchange in art.

So there seems to be some degree of disagreement about the compatibility between the two frameworks.


SPEAKER_02:
Well, let me try to respond to the two parts of your question separately.

So I'm not going to try to talk about Smith at all for a moment.

Let's talk about free energy, and I like getting definitions on the table because it's really so frustrating to try to remember what something is when someone's talking about it.

So I go to Wikipedia.

Wikipedia writes in part that the free energy principle asserts, quote, that systems minimize the free energy function of their internal state

which entail beliefs about hidden states in their environment.

The implicit minimization of free energy is formally related to variational Bayesian methods and was originally introduced by Carl Friston as an explanation for embodied perception neuroscience, where it's also known as active inference.

And we all know Carl is a brilliant and very insightful man.

The free energy principle describes the behavior of a given system by modeling it through a Markov blanket that tries to minimize the difference between their model of the world and their sense and associative perception.

This difference can be described as surprised and is minimized by continuous correction of the world model of the system.

One more part of the quote, the free energy principle has been criticized for being very difficult to understand, even for experts, and the mathematical consistency of the theory may have been questioned by recent studies.

Discussions of the principle have also been criticized for invoking metaphysical assumptions far removed from a testable scientific prediction, making the principle unfalsifiable.

And in a 2018 interview, Frisson acknowledged that the free energy principle is not properly falsifiable.

So that's Frisson himself.

So my main concern with the free energy principle, just like any theory about how a brain makes a mind is, how much data can it explain in a principle and unifying way?

That's what we do in science.

We develop theories to explain and predict data.

And in the case of the free energy principle, from what I can see, the answer is essentially no data.

And you can correct me if I'm wrong.

It therefore cannot be evaluated as a physical theory at all.

And there's a basic reason for this problem.

Our brains are designed to autonomously learn in real time in response to a changing or non-stationary world that's filled with unexpected events.

Like today, we're experiencing an unexpected event.

I didn't know until recently that I'd be enjoying your company today.

Optimization principles were designed to cope with stationary dynamics whose rules and probabilities do not change.

So it's not possible to, quote, minimize the difference between their model of the world and their sense and associated perception, unquote, because there is no predefined model of the world, which is always changing in unexpected ways.

So you need a theory about how the world changes.

Surprise occurs in art when there's a big enough mismatch between an input pattern and the currently active top-down expectation of a category that it's activating.

This mismatch activates the art-orienting system that I briefly discussed in my talk, which interacts with the attentional system where the category learning doesn't occur.

And as I illustrated in our discussions of search and vigilance,

that it drives hypothesis testing or memory search to discover a better match or to begin to learn a new category.

So auto discuss a better match in the case where the system was attending some other familiar features when the new input occurs, but the features of the new input have previously been categorized.

That's why they're familiar.

And then the orienting system very quickly shifts attention

to the matching category, and you resonate on, and you recognize it consciously often.

Art begins to learn a new category when the input represents a truly unfamiliar and novel situation.

Now, as to Bayesian methods in science, hey, I'm a mathematician.

How can I not love Bayes, right?

But the beauty of Bayes is its simplicity.

You just write the probability of two events, A and B, in two different ways.

The probability of B given A times the probability of A, the probability of A given B times the probability of B. Set them equal because they're identical.

Divide by, let's say, probability of A and then optimize.

That's Bayes.

And it's a useful statistical method and should continue to be used in statistics.

But it's just a formal identity wherein lies its power.

It says nothing about any physical reality, whether in physics, chemistry, or biology.

Today's rule itself tells us nothing about physical reality and contains no heuristics to discover anything about physical reality.

For that, you need to develop models driven by a profound analysis of large databases.

So it turns out that biological models like art do not incorporate the Bayes rule.

However, art does routinely choose the best or optimal categories that represent the data best.

So you don't need Bayes to achieve optimality.

Also, Bayes works best in a stationary world with stationary probabilities.

And art's designed to learn about a non-stationary world.

So one can discuss this till the cows come home.

It's good for what it was designed for.

And some of the neuroscientists who try to apply Bayes are wonderful experimentalists, but they know no math and no theory.

And it's the temptation of a free lunch.

There it is, waiting to be applied.


SPEAKER_01:
There is no free lunch in science.

Thank you.


SPEAKER_00:
Now, as a final point of comparison, what are the- To Smith.


SPEAKER_02:
I didn't reply to Smith.

Yes.


SPEAKER_00:
Yes.


SPEAKER_02:
OK.

You quoted a sentence of Smith.

But before that sentence Smith et al.

wrote, it is also worth highlighting that as our model is intended primarily as a proof of concept and a demonstration of an available model expansion reduction approach that can be used within active inference research,

It does not explicitly incorporate some aspects such as top-down attention that are of clear importance to cognitive learning processes and that have been implemented in previous models.

For example, the adaptive resonance theory law model of Grossberg was designed to incorporate top-down attentional mechanisms and feedback mechanisms to address a fundamental knowledge acquisition problem

the temporal instability of previously learned information that can occur when a system also remains sufficiently plastic to learn new and potentially overlapping information.

While our simulations do not explicitly incorporate these additional complexities, there are clear analogs

the top-down and bottom-up feedback exchanges in art within our model, such as the prediction and prediction error signaling within the neural process theory associated with active inference.

Art addresses the temporal instability problem primarily through mechanisms that learn top-down expectancies that guide attention and match them on bottom-up input patterns, which is quite similar to

the prior expectations and likelihood mappings used within active inference.

But as I've already noted, the, quote, prior expectations and likelihood mappings within adaptive inference, quote, unquote, do not have any of the key learning, attention, and memory stability properties of the ARC matching rule.

The ARC matching rule is a unique solution to that problem in its variations.

It's been supported by psychological, anatomical, physiological, and biophysical data.

It also occurs in many species.

Nobuo Suga, for example, shows that it occurs in bats.


SPEAKER_01:
It occurs in ferrets.


SPEAKER_02:
So also, I think it's important to know

that when learning begins in an arc model, it doesn't need prior expectations or likelihood.

In fact, typically, the initial bottom-up weights are chosen to be random, because you don't know what you're going to be experiencing.

And the initial top-down expectations are chosen to be large, so that whatever category happens to be learned, when it reads out its top-down expectation,

it can match whatever features activated that category.

So they all start large and then they're pruned to match the critical features that happen to be learned in that category.

So there are no built-in models.

Art discovers its own models.

I should also emphasize that active inference is also not explainable.

Arts is explainable because

A currently active critical feature activity pattern, namely the features to which attention is paid, controls all learning and prediction by the model and in principle can be measured by neurophysiological experiments.

A model without cell activities or short-term memory traces that can represent the critical feature pattern can't be explainable.

So I think there are qualitative differences.

I don't say people shouldn't use active inference.

It may be incredibly useful and powerful in technological applications.

But when one is doing, you know, brain science, psychology, it just doesn't match foundational data.


SPEAKER_01:
It just doesn't.

Thanks.


SPEAKER_00:
And I guess you somehow already answered a part of this question, but what are the possible ways in which an arts approach to explainable AI, which, if I'm not mistaken, can be described as a model-dependent, intrinsically explainable approach, can inform active inferences approach and cross-fertilize with it, which is based on

abductive reasoning through constructing generative models, for example, as is sketched out in Parr and Pizzullo's Understanding, Explanation, and Active Inference paper?


SPEAKER_02:
Well, first, I don't think art is model dependent.

As I just noted, one begins typically to learn an art with random initial bottom-up weights and uniformly distributed

about that initial adaptive weight so you can match any category that you happen to learn.

But the authors you quoted write in part that active inference, and here I want to quote them so I can respond in a little more detail.

Active inference, quote, implies a deep generative model that includes a model of the world

used to infer policies, and a higher-level model that attempts to predict which policies will be selected based upon a space of hypothetical, that is, counterfactual explanations, and which can subsequently be used to provide retrospective explanations about the policies pursued.

So again, art works without a generative model of the world or any predefined policies.

Because what it's trying to do is discover what changing world it happens to be in, and nobody knows what it is a priori.

And in general, an art classifier responds to a front end of pre-processes that process perceptual data from one or another sense, notably vision and audition, where we get

most of our information about the world.

And that's why I classify it like art begins its work in the brain in the temporal cortex, where it receives highly preprocessed perceptual representation.

So decades of work went into understanding how our brains consciously see and hear.

And in the case of vision, art classifies perceptual boundaries and surfaces that require multiple stages of processing because, as I mentioned briefly, they are the outcome of what I call hierarchical resolution of uncertainty.

You need multiple stages to define a perceptual boundary of surface, one reason being because our sensory organs

registers such noisy and incomplete data, like you may know, that our photosensitive retina has a huge blind spot, where you can't register any sense, any visual signal.

The blind spot as big as the fovea, where all of our high resolution vision occurs, so it's not a little thing.

And moreover, veins

come out of the fovea and occlude the retina in multiple places.

And you can't register visual signals on the veins either.

So the signal you're getting is very incomplete and it takes multiple processing stages to overcome those uncertainties.

And my colleague and I have worked for decades to explain how that happens.


SPEAKER_01:
maybe I'll stop there for that.


SPEAKER_00:
Thank you so much.

Now, this next question is of a personal interest to me because currently I'm working on modeling some probabilistic aspects of affective response to music and your most recent paper toward understanding the brain dynamics of music immensely helped me gain a better understanding of entrainment.

As you pointed out in the supplementary notes for this paper,

Violation of prior learned expectations is instrumental in inducing a wide range of affective responses in musical and non-musical situations.

Some psychologists, such as Patrick Uslan, have distinguished between perception and arousal of emotions.


SPEAKER_02:
I'm sorry to interrupt you, but you left out a question.

Is it the lack of time or you just skipped it accidentally?

I want to know quite a bit about it.

How do you see the future of auto and neuro-inspired AI research?


SPEAKER_00:
I think that will be our last question.


SPEAKER_02:
Okay, so we'll come back to that.


SPEAKER_00:
Yes, thank you.


SPEAKER_02:
Because that's an important question for me.


SPEAKER_00:
Yes.


SPEAKER_02:
Okay, so sorry to interrupt.

I just wanted to be sure.


SPEAKER_00:
No problem, thank you.

Now, yes, well, some psychologists such as Patrick Uslan have distinguished between the perception and the arousal of emotions in the context of musical experience.

And also several studies, such as the works of Uslan and Gabrielson from the Psychology Department of Uppsala University,

have shown that despite music's ability to communicate a wide range of positively and negatively valenced emotions, it somehow evokes mostly positively valenced emotions.

For instance, we can easily perceive rage or anger in music without necessarily getting angry.

On the other hand, we're more likely to actually feel elevated and happy after listening to happy music.

And also the evidence shows that this disparity between perception and evocation of emotion is probably even more significant in musical experiences than any other non-musical experiences.

So how can this difference in diversity between perceived and aroused or evoked musical emotions be accounted for within art

framework?

Can it possibly be regarded as another kind of broken symmetry, as you mentioned on page 621 of your book, but specific to musical effects?


SPEAKER_02:
So, as you've noted, I haven't studied these issues in the context of music, but I'll try to venture some general comments.

I should first note that the Laminart model

which is a development of art to show how and why all neocortical circuits that support perception and cognition typically share a canonical six-layer circuit.

My colleagues and I have modeled how, just as in our brains, variations of this

canonical laminar circuit can support all perceptual and cognitive processes.

So there's a major generalization of art.

And we've done it for vision, speech, and cognitive working memory, and planning in particular.

So the main point is that the laminar circuitry is basically an all perceptual and cognitive

areas, vision, audition, et cetera, et cetera.

That's how one can create a context for discussing music.

And in fact, my work on music applied such discoveries.

I was able to put together discoveries that had been made based on what I believe were different evolutionary pressures

the organization of our brains, but that evolution also discovered, and I try to sketch how, if you put some of them together in a certain way, then capacity for learning and consciously performing music could arise.

So now how about arousal?

Well, it's essential, of course, to all awareness and consciousness.

Your cortex needs to be adequately aroused for waking consciousness to occur at all.

It also arousal plays a major role in the processing of emotions, and it's very relevant to musical issues, because my gated dipole model explains how opponent processes, opposites,

are organized in all parts of our brain, perceptual, cognitive, motor, affective.

In particular, emotions are organized in pairs in such an emotional dipole.

And one reason is because emotions need to compete with each other, such as fear versus relief.

For example, in post-traumatic stress disorder therapy, a therapist may try to help a patient to think about positive experiences that generate relief in order to inhibit the chronic fear that's so destabilizing during PTSD.

So their opposites are competing.

And another property that arousal enables is that

The sudden offset of an emotion like fear, let's say during escape behavior, let's say, you know, a favorite example of mine is, you know, some cruel experimentalist puts a pigeon in a Skinner box.

The floor is electrified.

The pigeon is feeling pain and fear.

It's dashing frantically around trying to keep its feet off the floor.

It bangs into a red buzzer.

the buzzer shuts the shock off, and the animal experienced a wave of relief or positive motivation for learning the escape response.

So the rebound from theater relief that can be associated with actions that lead to escape and can motivate escape by means in the future is energized by arousal in the gated dipole.

You shut off the external cue of shock, but the arousal is tonically or has sustained activity in both the fear and the relief channels.

So because the arousal is sustained or tonic through time and equally activates the fear and relief channels, when fear suddenly decreases, then arousal in the relief channel wins the competition.

and can thereby call what I call an antagonistic rebound from theater relief that activates the relief channel and thereby providing motivation for escape, whether reactive or learned through escape experiences.

And I also approve, which is related to some degree to music, that the non-occurrence of an expected event

can by itself cause a burst of arousal and thus an antagonistic rebound and can flip emotions from positive to negative in so doing.

And I always loved that discovery because

I especially love discoveries where the occurrence of nothing has profound effects on future behavior.

So it's the non-occurrence of the expectation cause of this mismatch that can flip emotions

Now, how this influence of the perception of music needs more work.

As I mentioned in my paper, I haven't tried to study that.

My paper on music, I feel, is like a drop in the bucket.

And hopefully, if I don't get around to it, someone else will.

But the above examples show that arousal and emotion

are not the same thing, because arousal can support all emotions, fear, relief, hunger, satiety, whatever.

The ones that win the competition are then able to support compatible behaviors by motivating them.

And I've also explained that the level of arousal must be chosen within an intermediate range to support normal behaviors.

It's a kind of golden

mean there's an inverted U on the effects of having arousal from too little or too much.

And if you have too little arousal, you have an underaroused syndrome, which can support symptoms like autism.

over arousal can support symptoms of a disease like schizophrenia is only one of many factors in these diseases but I'm happy to say that subsequent clinical data supported those predictions that these two mental disorders are at opposite extremes of the arousal inverted view so I think and you know there's very

because I've never really seriously studied it, and I try not to speculate, but what the hell.

The kind of arousal that music activates is generally positive, just like the arousal that activates exploratory behaviors is positive.

It somehow links.

You know, music is a sonic adventure, if you like.

There's no aversive cue when listening to music, except perhaps music that's played so loud as to cause a headache or ear damage or even a seizure in susceptible individuals.

So there's no particular reason why it shouldn't be positive.


SPEAKER_01:
So now which question you want to ask me?


SPEAKER_00:
I think we're left with just two other questions.


SPEAKER_02:
If we're not too tired.

Start with as a final point of comparison.

Another starts on a more fluid side.


SPEAKER_00:
Yes.

The last we're on the last two questions.


SPEAKER_02:
So you want to do, how do you see the future of all-immune spider research?


SPEAKER_00:
And before that, yes, before that, I just wanted to ask you about your view about artificial consciousness.


SPEAKER_02:
Do you see... I really need the first answer to answer the second question.


SPEAKER_00:
Okay, as you wish, yes.


SPEAKER_02:
Okay.


SPEAKER_00:
So, yeah.

And how do you see...

Yes, it's quite fine.

And how do you see the future of art and brain-inspired AI research in general?

In your view, what research areas ought to gain more attention than they do today?


SPEAKER_02:
So I'll give you quite a general answer, but it implies...

and what I think about this.

So first, with a caveat, I like to say I couldn't predict the present, so I can't predict the future.

That being said, I believe that all engineering technology and AI will increasingly embody autonomous adaptive intelligence in the coming century.

And we can already see its beginnings

in autonomous automobiles and airplanes and increasingly autonomous controllers on the factory floor.

And many people have written about it.

And I think art will play a central role in this, as well as other models that are summarized in my book.

And that's because already in 1980, I published

a thought experiment in the journal Psychological Review, which was then and still probably remains the leading theory journal in psychology.

And you may recall that Einstein derived both special relativity theory and general relativity from thought experiments.

And let me just clarify by my thought experiment, wherein they derived their enormous power

So my thought experiment was about how any system can autonomously, it's all about autonomy, correct predictive errors in a changing world.

And the hypotheses upon which the thought experiment were derived are just a few facts that are familiar to us all from daily life.

And they're familiar because they represent ubiquitous

environmental pressures on the evolution of our brains over the millennia.

And when they act together, I suggest art is the unique outcome.

That's a huge claim.

And I turn to the power of the thought experiment, not to any personal ego trip, for that belief.

In particular, nowhere in the thought experiment are the words mind or brain mentioned.

So if you accept that these facts about the world exist, which we all do, and that they're always operating on us, then you have to accept the outcome if you believe in the scientific method and logic.

So art is a universal solution to the problem of autonomous error correction in a changing world.

Said in another way, if you can't find a mistake in the thought experiment, then I think you either have to believe in art-like dynamics, maybe expressed in Laminard or your favorite art variant, or you have to give up your belief in logic and the scientific method.

It doesn't imply that art can't be further developed.

I expect a large number of scientists and technologists

to be busy developing architectures long after I'm gone.

So maybe now we can go to your philosophical note with that background.


SPEAKER_00:
Yes.

Thank you so much.

Now, yes, as I mentioned earlier, I just wanted to ask about your view on artificial consciousness.

Do you see the consciousness as artificially producible or engineerable, as some researchers like Mike Solmsley?

Is there a fundamental distinction between a biologically conscious agent and

an artificial agent with a fully simulated computational model of consciousness?

I know it's a big, big question, but I couldn't resist asking your opinion as an authority on consciousness modeling.


SPEAKER_02:
I'm happy to give a shot.

So, as I just noted, my work on art suggests it solves a universal problem about how we can learn to correct predictive errors in a changing world.

My work also shows in its analysis of hierarchical resolution of uncertainty, remember like how you go from some noisy retina to a surface representation that can control looking and reaching, how evolution may have been driven to discover conscious states.

So this was a surprise to me too.

conscious states were needed in order to choose that processing level or levels that computes a sufficiently complete context-sensitive and stable representation, the case of vision, surface representation, with which to successfully plan and act to realize value goals.

So let me make it clear.

So you start with a noisy retina, you have to go,

up all of these stages until you get a sufficiently complete surface and boundary representation that you can use to regulate successful action.

And if you use one of the earliest stages, it would lead to incorrect actions, which would kill you off by Darwinian selection.

So how the hell do you know where the stage is where you complete

compute the sufficiently complete context senses and stable one.

And I propose in vision, I predicted that the choice is embodied in what I call a surface shroud resonance between pre-strived visual cortical area V4 and the next processing stage, posterior parietal cortex, or PPC.

So it's in V4 you get this really good surface representation.

And then a resonance between the surface and spatial attention, which fits the surface.

That spatial attention in PPC is called a shroud.

Christopher Tyler gave it that name.

A surface shroud resonance allows you to take conscious

spatial attention to the surface that you're going to use to control looking and reaching behaviors.

So it's a way of ensuring you have a good enough representation to control action.

So the shroud is computed in posterior parietal cortex, which is part of the dorsal or where cortical stream.

And the shroud modulates invariant category learning in the ventral or Watt cortical stream.

I can't go into that right now, but my book discusses it.

The category learning itself in the Watt cortical stream, as I indicated, is supported by a feature category resonance.

And so the surface shroud resonance is modulating invariant category learning

in the feature category resonances.

So for shroud resonance also supports conscious seeing.

The feature category resonance is supporting conscious recognition.

And when they synchronize across streams on a familiar object, that's when you consciously see something that you know about.

Okay, so...

Conscious states hereby arise due to learning requirements.

This sort of fell out of the wash of how you do invariant category learning, and learning in particular without catastrophic forgetting.

It's regulating feature category resonance.

So given that the above solutions are computationally universal in the sense I sketch,

The self-organizing machine that embodies them should be able to support internal representations whose parametric properties mimic conscious states.

But whether such a machine can experience conscious qualia remains as much of a mystery for machines as it does for humans.

And that's because no computational theory, which after all is just a set of equations, can do more than imitate the dynamics of our brains, perhaps with great precision.

I don't have a clue why the representations that my colleague and I have worked so hard to explain huge amounts of psychophysical data about seeing

Extra shading, 3D form, just go through the list.


SPEAKER_01:
Why they support qualia?

I don't know.

Ask God or whatever God you choose to believe in in the 21st century.


SPEAKER_00:
Thank you so much, Professor.

I think we have a couple of questions in the chat, but we are actually approaching our two hour limit.

I don't know, Daniel, if it's a good place to stop or whatever you say.


SPEAKER_03:
I think that's a great place to stop.

You've given us a lot to think about and digest.

And I hope that these words are taken well and paid attention to, might create some categories, activate some categories.

But Professor Grossberg, thanks again for this amazing live stream.

We really appreciate it.


SPEAKER_02:
I appreciate it.

And depending on younger people like yourself,

But do just what you said, Daniel.

I'm not going to be around that much longer.

So I hope you have, whether with my work directly or related work, you have a very fulfilling intellectual adventure.

I know I've been on a wild ride since I was 17.

That's 65 years of discovery.

I've loved every minute of it.
[
  {
    "start": 8.097,
    "end": 9.18,
    "text": " All right, hello and welcome.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 9.621,
    "end": 12.569,
    "text": "It's August 5th, 2024.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 12.89,
    "end": 19.027,
    "text": "This is Active Inference Guest Stream 85.1, Deep Learning Active Inference with Love with David Blumen.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 19.127,
    "end": 23.98,
    "text": "So thank you, David, for the presentation and we'll look forward to the discussion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 32.262,
    "end": 46.513,
    "text": " how we can use active inference to make an intelligent agent and some of the advantages and disadvantages and some issues with active inference and how I think we can overcome them.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 47.355,
    "end": 51.604,
    "text": "And you'll see why with love in a little bit.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 51.584,
    "end": 56.031,
    "text": " But my background is I'm a software engineer and an AI researcher.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 56.892,
    "end": 64.604,
    "text": "I've worked for a long time at the intersection of scalable systems and machine learning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 64.825,
    "end": 68.971,
    "text": "And I've been working on this project for about three years now.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 69.892,
    "end": 73.558,
    "text": "And I'm really excited to talk about it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 73.758,
    "end": 103.461,
    "text": " so uh i think uh the holy grail of uh ai research uh right now is how do we make an ai agent that's both generally intelligent and doesn't destroy humanity and pretty much i think most people are working on some facet of these two and i'm really excited about the generally intelligent part and also not destroying humanity so",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 103.829,
    "end": 106.397,
    "text": " here's how I think we can do it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 107.002,
    "end": 109.825,
    "text": " But first I want to talk about just like get our definitions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 110.266,
    "end": 111.347,
    "text": "So what's an agent.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 111.868,
    "end": 114.35,
    "text": "And there's lots of people arguing about definitions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 114.871,
    "end": 123.641,
    "text": "I'm going to use a really simple one, which is an agent is something that given a set of past observations produces the next action.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 124.182,
    "end": 130.589,
    "text": "So basically given what you're seeing now and what you've seen and experienced in the past, what should you do next?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 131.11,
    "end": 135.675,
    "text": "So a very simple formulation of an agent as a function.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 136.988,
    "end": 144.441,
    "text": " And by generally intelligent, I don't have a rigorous definition of general intelligence.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 144.461,
    "end": 146.043,
    "text": "I don't actually think there is one.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 146.584,
    "end": 157.242,
    "text": "But as an observer observing this agent, we should be able to say, hey, it's acting smart in a wide variety of environments that it hasn't seen before.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 157.222,
    "end": 182.115,
    "text": " um and what that means is that it's able to figure out the environment dynamics it's able to make mistakes and learn from them it doesn't get stuck in the local optima it like knows when to explore but then when it figures out regularities about the environment it knows how to use them it does this efficiently it's not just like trying the same dumb thing over and over again but",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 182.095,
    "end": 184.439,
    "text": " It makes plans.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 184.539,
    "end": 190.508,
    "text": "It can communicate and interact with other complex intelligent entities in the environment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 190.889,
    "end": 193.833,
    "text": "So it's a bunch of different behaviors.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 194.454,
    "end": 208.356,
    "text": "And I don't think there's a really clear way to quantify or talk about exactly what they look like because different spaces and different environments are going to have a different idea of what",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 208.336,
    "end": 210.465,
    "text": " an intelligent agent is doing there.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 211.609,
    "end": 217.232,
    "text": "So for now, just think of it as this grab bag of capabilities.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 218.005,
    "end": 220.007,
    "text": " So where does active inference come in?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 220.487,
    "end": 236.762,
    "text": "Well, active inference is a way of thinking about an agent interacting with its environment as essentially a formulation where any agent can be seen as if it was acting in a self-evidencing way.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 236.802,
    "end": 248.012,
    "text": "And what that means is that you can treat any object that maintains homeostasis over time as if the inside of that object was",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 247.992,
    "end": 258.343,
    "text": " minimizing free energy, meaning that you can think of it as having some generative model of its environment around it from an agent's perspective, its boundary.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 259.024,
    "end": 267.694,
    "text": "And it is trying to update its prior beliefs or its posterior beliefs over the boundary in order to minimize surprise.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 268.174,
    "end": 271.738,
    "text": "And so the way it does that is when it receives evidence",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 271.718,
    "end": 283.516,
    "text": " From the environment, it adjusts its set of beliefs to concord with what the environment is and what it's doing and where it is in that environment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 283.877,
    "end": 299.16,
    "text": "But it's also actively navigating the environment in order to both gather more knowledge and improve its set of beliefs, as well as to move into parts of the environment that it thinks it's going to be occupying.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 299.14,
    "end": 305.995,
    "text": " And so this is a really beautiful way to capture what any agent is doing at any scale.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 306.036,
    "end": 318.263,
    "text": "So this formulation allows us to think about a biological cell or a robot or a person or an organization, a corporation, a country.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 318.243,
    "end": 339.07,
    "text": " Anything that we want to draw a boundary around, we can basically say, hey, this thing is kind of acting as if it was doing Bayesian inference over its input data and beliefs over a set of states that it's going to be occupying, and it's trying to act in accordance with that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 340.4,
    "end": 369.765,
    "text": " so the problem with that formulation so the nice thing about that formulation is it's very general it's very beautiful uh it is able to generalize uh what we see at different scales one thing that it doesn't let us do is actually tell us how do we build such an agent and there are two reasons uh that i think uh uh that uh this theory is not uh",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 369.745,
    "end": 397.693,
    "text": " is not useful for actually building an agent um one is that it treats every agent as trying to approximate bayesian inference uh so as the agent observes evidence it's trying to uh infer what that means for its generative model and trying to update its posterior the problem with bayesian inference is that it is computationally intractable and uh",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 397.673,
    "end": 421.116,
    "text": " uh the active inference recognizes this and it basically says the agent is trying to approximately do Bayesian inference by sticking a variational inference bound on it but in reality what approximately means uh is different in every situation so uh all agents that we see all systems that we see",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 421.096,
    "end": 445.213,
    "text": " they're doing some kind of computation and that computation can be viewed as approximate bayesian inference but in reality it's a set of heuristics and hacks that have been discovered by evolution by natural selection by software engineers whatever it is that can be thought of as approximately doing this but really they're running some algorithm it's just some set of heuristics",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 445.193,
    "end": 448.949,
    "text": " And active inference doesn't really tell us how to discover that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 449.09,
    "end": 455.035,
    "text": "It just says, hey, anything it's doing can be viewed as approximately doing this intractable problem.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 455.386,
    "end": 468.878,
    "text": " The other problem around trying to productionize active inference is that it says, well, the agent is going to be occupying a set of states that its prior beliefs said it's going to be occupying.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 468.958,
    "end": 472.401,
    "text": "So why does an animal eat?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 472.802,
    "end": 480.669,
    "text": "Well, because evolution has filtered animals down to only the animals that believe that they're going to be fed.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 481.089,
    "end": 485.393,
    "text": "And so then the animal is going to, via self-evidencing way, go seek out food.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 485.373,
    "end": 502.861,
    "text": " And again, this is a really beautiful way to capture both the action and inference in one formulation, but it doesn't tell you what are the priors over states for a given environment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 502.921,
    "end": 504.664,
    "text": "So if you want to build",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 504.644,
    "end": 510.832,
    "text": " a robot or an agent that has to interact with some environment, what are its prior beliefs?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 510.912,
    "end": 512.534,
    "text": "What states should it be occupying?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 513.155,
    "end": 520.644,
    "text": "Does it believe that it's going to get energy or does it believe it's going to go repair the car that you want it to repair?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 520.704,
    "end": 522.727,
    "text": "How does it balance between those two?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 524.329,
    "end": 524.97,
    "text": "We don't know.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 525.03,
    "end": 527.593,
    "text": "Active inference doesn't really tell us.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 527.673,
    "end": 533.841,
    "text": "It just says some process will give you a set of priors.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 533.821,
    "end": 561.178,
    "text": " and uh so with active inference typically the approach is well as a software engineer uh you build a generative model you basically code up what you think the environment requires and the agent in the environment requires and you give it some priors like you will be fed and you will repair this car and here's uh some parameters about how the environment works that are uh",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 561.158,
    "end": 566.386,
    "text": " simple enough that you can then do a variational inference over it and get something done.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 566.847,
    "end": 576.602,
    "text": "And then you have a more generic free energy minimizing algorithm that, given that generative model, actually controls the robot.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 577.463,
    "end": 586.137,
    "text": "The approach I'm taking is, well, instead of someone trying to design this general model, can we learn it?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 586.117,
    "end": 604.412,
    "text": " And I think that's really the only way forward because any environment that we care about is going to be so complex that I just don't believe humans are going to be able to specify a generative model for it that's at all interesting.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 604.392,
    "end": 605.854,
    "text": " Why do I think we can learn it?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 606.014,
    "end": 608.276,
    "text": "Well, one, evolution has.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 609.017,
    "end": 611.38,
    "text": "And what does learning it mean?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 612.241,
    "end": 619.388,
    "text": "Learning means that we want to use some learning system, and neural nets are a really good one.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 620.149,
    "end": 631.902,
    "text": "And we want to figure out what are the sets that we basically want to have some approximate active inference agent, some approximate generative model that",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 631.882,
    "end": 638.331,
    "text": " is able to do all the things we want, which is that grab bag of generally intelligent behaviors.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 638.581,
    "end": 643.126,
    "text": " And the reason we can do that is that neural networks are universal function approximators.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 643.607,
    "end": 662.108,
    "text": "So rather than building an algorithm, we took a neural network, we fed it pairs of text and images, and out comes DALI that, given text, is just able to generate images, also generated all the slides for this presentation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 663.202,
    "end": 686.391,
    "text": " So the way to think about neural networks is rather than programming an algorithm, you figure out the training data and a loss function, and then you take a big blob of learning, learnable parameters, and you keep nudging that blob until it does what you want it to do.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 686.793,
    "end": 689.617,
    "text": " And this technique is really powerful, as we know.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 691.62,
    "end": 697.469,
    "text": "Language models that everyone is using now were also trained this way.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 697.57,
    "end": 703.679,
    "text": "You took a data set that was a set of prompts and then a completion, basically the next word.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 704.901,
    "end": 712.092,
    "text": "And you trained a neural network to predict the next word, and you got chat GPT.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 712.072,
    "end": 715.44,
    "text": " This technique is extremely powerful.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 715.5,
    "end": 717.244,
    "text": "It is very slow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 717.344,
    "end": 720.311,
    "text": "It requires a lot of data and a lot of training examples.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 720.792,
    "end": 727.988,
    "text": "But over time, you're able to learn a really complicated function approximator.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 728.609,
    "end": 739.984,
    "text": " So the question then is, OK, well, if we want to train a function approximator for a generally intelligent agent, what's the training data that we need to use?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 740.765,
    "end": 747.934,
    "text": "And basically, I think if we discover the training data and we",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 748.353,
    "end": 760.031,
    "text": " throw a large neural network at it with a lot of compute, we should be able to, in the same way that we got ChatGPT or DALI, get a model that's like a foundational general intelligent agent model.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 761.173,
    "end": 764.438,
    "text": "So to do that, we need an environment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 764.587,
    "end": 773.305,
    "text": " And the reason we need an environment is that we're trying to train an agent, not, say, a language model.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 773.325,
    "end": 775.77,
    "text": "So for a language model, you give it a prefix.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 776.753,
    "end": 778.957,
    "text": "It gives you the completion.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 779.619,
    "end": 783.186,
    "text": "And you can train it on, say, all of the internet text.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 783.166,
    "end": 789.433,
    "text": " What we're trying to do is we're trying to learn a function that, given a set of fast observations, know what action to do next.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 789.714,
    "end": 797.804,
    "text": "And that's a more complicated problem because sometimes the action it takes should be done because it's trying to accomplish something.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 797.844,
    "end": 800.787,
    "text": "Sometimes it's because it's trying to learn something.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 800.847,
    "end": 803.19,
    "text": "Sometimes it's trying to explore.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 803.25,
    "end": 811.62,
    "text": "And so what we need is a world for the agent to interact with that it can then learn its behaviors from.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 811.6,
    "end": 819.171,
    "text": " And so the properties we need for that environment is, one, we want it to provide continuous learning opportunities.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 819.412,
    "end": 824.139,
    "text": "So there should always be something that the agent, a new thing for the model to learn.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 825.16,
    "end": 834.775,
    "text": "And then when it learns it, when it does something intelligent, one of these behaviors that we associate with intelligence, we need a feedback signal.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 834.815,
    "end": 836.157,
    "text": "So we need the environment",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 836.137,
    "end": 837.42,
    "text": " to essentially reward it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 838.101,
    "end": 848.543,
    "text": "And it needs to be an environment that cannot be mastered because if it can ever be mastered, then there's nothing new to learn and the model stops learning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 849.383,
    "end": 856.309,
    "text": " So the way to set that up is, or one way to set that up is via multi-agent.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 856.71,
    "end": 870.542,
    "text": "So if you have an environment, even if it's a relatively simple one, like say a Go game where the rules of the game are really simple, just by having other agents in that environment gives it a really important property.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 870.922,
    "end": 874.105,
    "text": "As the agent gets smarter, so do the competitors.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 874.706,
    "end": 879.39,
    "text": "And so the environment essentially adapts to your capabilities.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 879.37,
    "end": 899.117,
    "text": " So this is a way where you can automatically get an environment that always gets something new for you to do because as soon as you learn some exploit or some way of being better than your competition, the competition changes and now you have to learn something new.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 899.502,
    "end": 911.054,
    "text": " And so I think, so my approach is leveraging this characteristic, and essentially training AI's in a multi agent competitive environment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 912.075,
    "end": 922.425,
    "text": "There is a problem though, which is if the environment is entirely competitive, you very quickly end up in this kill or be killed dynamic.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 922.465,
    "end": 928.952,
    "text": "So you can imagine an agent learned some behavior policy, and then the other agent learned some",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 928.932,
    "end": 936.32,
    "text": " counter strategy and then they kind of explore the area around those strategies and figure out the best way to be competitive.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 936.44,
    "end": 943.947,
    "text": "And now they're stuck because if any agent ever tries to do anything that is not that, it gets exploited by its adversary.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 944.388,
    "end": 950.574,
    "text": "So this setup makes it really hard to avoid local maxima.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 950.634,
    "end": 953.337,
    "text": "You kind of find something that works and you're stuck there.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 953.317,
    "end": 962.754,
    "text": " And that's not quite true if you have many agents, you might get some unstable symmetry breaking and things get smoothed out a little bit.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 963.175,
    "end": 966.741,
    "text": "But I think the solution to that is actually cooperation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 967.302,
    "end": 971.73,
    "text": "And what's cool about cooperation is there are many ways to cooperate.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 971.79,
    "end": 977.721,
    "text": "The space of effective behaviors when it comes to cooperation just explodes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 977.701,
    "end": 993.362,
    "text": " And so you end up with a much denser, more higher dimensional behavior space where it's harder to ever get stuck at some Nash equilibrium of behaviors.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 993.382,
    "end": 1004.738,
    "text": "And this also gives rise to complex group dynamics like coalition building, cooperation, cooperating to compete, figuring out who you're going to cooperate with and who you're going to compete with.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1004.718,
    "end": 1034.64,
    "text": " building trust uh it's just like the set of behaviors that are available in cooperative environments explodes and the problem with cooperation is it's really hard to learn if all the agents are selfish it's not impossible you can have things like reciprocal altruism or even just non-aggression you can learn that you know always picking a fight is not great because if even if you win the fight now you're weaker and someone else can take advantage of you but uh",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1035.093,
    "end": 1045.946,
    "text": " In general, learning cooperation when everyone is not cooperative is also full of these national equilibria of local optima.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1046.867,
    "end": 1049.791,
    "text": "And nature solves this using kinship.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1050.351,
    "end": 1058.802,
    "text": "So when an animal is born into a world, it is not surrounded by other selfish organisms.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1059.022,
    "end": 1061.725,
    "text": "It's surrounded by a lot of kin.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1061.705,
    "end": 1068.535,
    "text": " So for cellular organisms, it's surrounded by more or less identical genetic clones of it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1069.316,
    "end": 1074.003,
    "text": "For animals, it's surrounded by organisms that have half of its genes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1074.944,
    "end": 1087.763,
    "text": "And this kinship via genetic evolution gives rise to organisms that are not selfish because from a genes perspective,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1087.743,
    "end": 1093.349,
    "text": " having a copy of you in one organism or in another organism doesn't matter.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1093.529,
    "end": 1105.461,
    "text": "And so what that does is it gives rise to organisms that basically care about the success of other organisms almost as much as they care about the success of their own organism.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1105.942,
    "end": 1115.792,
    "text": "And there's a whole spectrum in nature from hive insects like ants, where from a soldier's ant's perspective, the only way for those genes to make it into the next generation",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1115.772,
    "end": 1133.123,
    "text": " is for the queen to reproduce and so you end up with ants that care a lot more about the hive and the queen than they care about their own safety and well-being and you get a whole spectrum from fully altruistic hive insects to",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1133.103,
    "end": 1152.175,
    "text": " you know, a few perfectly selfish sociopaths, but most animals fall somewhere in the middle and being in the middle where you're partially selfish and partially altruistic gives rise to all of these social behaviors that we associate with",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1152.155,
    "end": 1154.621,
    "text": " being an intelligent being.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1155.082,
    "end": 1168.273,
    "text": "So animals coordinate, they negotiate, they build trust, they love each other, they show kindness, they show jealousy, they have social emotions like shame, they learn how to",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1168.253,
    "end": 1172.638,
    "text": " exchange knowledge with each other via gossip and reputation building.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1173.038,
    "end": 1174.54,
    "text": "They do division of labor.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1175.06,
    "end": 1178.684,
    "text": "And all of these things happen, again, at every layer of complexity.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1178.724,
    "end": 1184.07,
    "text": "Animals do it, biological cells do it, organizations do it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1184.711,
    "end": 1191.358,
    "text": "These sets of behaviors are present in pretty much all entities that we think are intelligent.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1193.532,
    "end": 1211.053,
    "text": " And so my proposal back to this AGI recipe is that if we can have the right training data and enough compute, we can train a large model on an environment that essentially balances cooperation and competition.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1211.313,
    "end": 1218.622,
    "text": "And so I've been trying to build and design this environment and the training infrastructure for it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1218.762,
    "end": 1223.067,
    "text": "And that's been essentially my research direction.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1223.047,
    "end": 1226.555,
    "text": " So this is the current version of the environment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1226.996,
    "end": 1230.383,
    "text": "And I'll just talk through some of the components.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1230.884,
    "end": 1234.131,
    "text": "So you can see there is a few agents here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1234.151,
    "end": 1238.08,
    "text": "And they're parameterized by a neural network.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1238.1,
    "end": 1241.427,
    "text": "So all of this was trained from scratch using reinforcement learning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1241.93,
    "end": 1244.373,
    "text": " there is this heart altar in the middle.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1245.114,
    "end": 1254.827,
    "text": "And the only source of reward that these agents have is they can walk up to the heart altar and they can use it, which requires energy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1254.867,
    "end": 1258.131,
    "text": "And if they do that, they get a reward.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1258.772,
    "end": 1264.579,
    "text": "And what that means is that these agents have to get more energy out of their environment",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1264.559,
    "end": 1268.369,
    "text": " that they're using for interacting and competing with each other.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1269.331,
    "end": 1277.051,
    "text": "And if they can extract more energy than they need, and they can essentially waste it by putting it into this heart altar, then they get a reward.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1277.091,
    "end": 1279.918,
    "text": "So this is the competitive dynamic that's set up.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1279.898,
    "end": 1300.557,
    "text": " so everything uh doing everything in this environment requires energy to get energy you collect these resources so you walk up to a green or yellow crystal and you collect a resource you then have to walk up to these converter stations that look like little batteries and then you put the right set of resources in there and you get energy out",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1300.537,
    "end": 1311.296,
    "text": " You can also attack each other, and if you get shot, you turn into this egg where you're stationary for a while, you lose some time, and people can steal whatever resources you're carrying.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1312.018,
    "end": 1317.668,
    "text": "But you can also turn on a shield, which also costs energy, but then it deflects the attacks.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1317.648,
    "end": 1322.413,
    "text": " So these agents have to, from scratch, learn how do you navigate the space?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1322.794,
    "end": 1324.055,
    "text": "Where are the resources?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1325.116,
    "end": 1326.958,
    "text": "How do you manage your energy budget?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1327.158,
    "end": 1328.6,
    "text": "Who do you fight with?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1328.66,
    "end": 1330.602,
    "text": "Who do you not fight with?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1330.642,
    "end": 1332.324,
    "text": "When do you need to keep your shield on?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1332.684,
    "end": 1337.75,
    "text": "So a lot of relatively intelligent behaviors that are learned from scratch.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1337.77,
    "end": 1340.753,
    "text": "And all of this is done via this competitive",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1340.733,
    "end": 1357.071,
    "text": " And then on top of that is a whole system for kinship where every agent shares its reward with some other agents based on some reward sharing system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1357.051,
    "end": 1367.326,
    "text": " And so what that means is whenever, say, this red agent gets a reward, the green agent might get 10% of that reward.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1367.727,
    "end": 1373.595,
    "text": "And so this environment allows you to train agents in very different environments.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1373.735,
    "end": 1381.827,
    "text": "So I'm going to pause this and switch over to this other tab.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1383.342,
    "end": 1411.071,
    "text": " yeah here um so you know you can train in larger maps you can uh uh set up different uh game rules and different game scenarios um and you can also uh set up a different kinship environments where you can train fully selfish agents you can train fully uh cooperative agents you can train complex non-symmetric uh social dynamics",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1411.051,
    "end": 1419.178,
    "text": " And once you train an agent, you can actually start treating it like an intelligence and you can do behavioral studies on it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1419.258,
    "end": 1429.728,
    "text": "So here we take a trained agent that was trained in this environment and we just like put it in an empty room and we're like, well, what is this agent doing?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1429.768,
    "end": 1432.57,
    "text": "And you can see it's like it's exploring its environment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1432.59,
    "end": 1441.058,
    "text": "It's not getting close to any walls because it knows that there is not a lot to be seen and it doesn't want to waste its movement.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1441.038,
    "end": 1444.686,
    "text": " You can put it in an environment",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1446.185,
    "end": 1459.743,
    "text": " where you give it some energy and you put it in a world where there's nothing except this heart altar, and you'll see that it searches the world until it finds the heart altar, and then it puts energy in there.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1460.784,
    "end": 1467.432,
    "text": "And so you can essentially study it and do experiments on it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1467.473,
    "end": 1469.675,
    "text": "It's like, what will it do under this circumstances?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1470.556,
    "end": 1474.842,
    "text": "And you can start approaching some of this as an observation,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1474.822,
    "end": 1477.609,
    "text": " observer observing an intelligent entity.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1477.629,
    "end": 1490.282,
    "text": "This is all early days, so this is just a few evals and experiments that I've been setting up, but this environment allows you to do all of these things.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1492.321,
    "end": 1515.274,
    "text": " And I think what's cool about this whole approach, and I'll switch back to this slide, is that the idea is that you end up with an environment that is unlearnable because any time some agent gets some competitive advantage, the environment around it changes since all the other agents now arbitrage away that advantage.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1515.254,
    "end": 1528.418,
    "text": " It's also highly dense in behavior space because there's so many different ways to cooperate and compete with each other that there's always some new trick that you can discover.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1528.458,
    "end": 1535.271,
    "text": "And so as this neural network learns to achieve fitness to this environment,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1535.251,
    "end": 1557.85,
    "text": " uh and the environment is always changing it's also learning how to learn it's learning this set of adaptive behaviors of exploring uh forming theories testing things out because that's the behavior that gives it an advantage when the environment is always changing and so uh the hope is to end up with a uh",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1557.83,
    "end": 1562.638,
    "text": " with an agent that you can drop into a new environment that it hasn't seen before.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1563.119,
    "end": 1575.079,
    "text": "And the thing it'll do is first figure out the dynamics of the environment, start exploring it, figure out how different things work, and then what it needs to do, and then start doing it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1575.059,
    "end": 1597.557,
    "text": " um the other cool thing about this setup is because these agents are trained in a cooperative setting they have to cooperate with they have to learn how to cooperate with other agents and that means they have to learn how to communicate so there's a signaling and a language component so if you are surrounded by kin and you have to compete with another",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1597.537,
    "end": 1622.717,
    "text": " group you have to learn what information is necessary to communicate to your allies in order to effectively organize so these agents are developing a language that we can then be able to reverse engineer because we have a fully instrumented world uh and we can do experiments on them so we can basically figure out what it is that they're saying and we can then talk to them",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1622.697,
    "end": 1634.567,
    "text": " and they're learning a policy that's conditioned on this kinship score so they know how to cooperate with close kin, which means they are capable of cooperating.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1634.588,
    "end": 1637.435,
    "text": "They're learning to be capable of cooperation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1637.475,
    "end": 1638.698,
    "text": "They're essentially learning",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1638.678,
    "end": 1640.72,
    "text": " to care for other agents.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1641.181,
    "end": 1660.804,
    "text": "And so the hope of this research program is to build a model organism of an active inference agent, essentially an agent that is performing active inference in a complex environment that you can talk to and that cares about other agents in that environment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1660.784,
    "end": 1683.703,
    "text": " um and if that works we should be able to take these same ideas and take it out of this simple 2d grid world environment and essentially do the same thing but in complex 3d world physics environments get something that is smart that cares about other entities and that we can talk to",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1684.088,
    "end": 1705.152,
    "text": " so that's basically what i'm trying to do i'm calling this project meta learning uh meta is the poly word for love and kindness uh because i think that um love is uh love and uh what i think i'm building here are agents that are capable of love in the same way that genetic evolution uh",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1705.132,
    "end": 1715.667,
    "text": " The selfishness of a gene being expressed in a multitude of other organisms gave rise to organisms that love and care for each other, especially their kin.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1716.248,
    "end": 1734.413,
    "text": "I think that similar dynamic is what's necessary to create an environment that's complex enough to learn generalized intelligent behavior and also is going to create agents that are alignable, that basically care for other agents",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1734.393,
    "end": 1735.794,
    "text": " that we can get along with.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1736.575,
    "end": 1749.648,
    "text": "So I'll stop talking here and take questions and yeah, happy to dive into anything that I've talked about.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1749.809,
    "end": 1758.718,
    "text": "Or if you have questions about active inference or how or free energy or deep learning, basically anything that this made you think about, I'm so happy to take questions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1762.301,
    "end": 1762.641,
    "text": "Awesome.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1762.922,
    "end": 1763.202,
    "text": "All right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1765.392,
    "end": 1770.361,
    "text": " Anyone in the live chat, please feel free to ask a question.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1770.421,
    "end": 1778.336,
    "text": "Just meanwhile, maybe just how did you come to be applying the neural network approaches in this project?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1778.356,
    "end": 1780.439,
    "text": "Just what led you want to explore this project?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1780.46,
    "end": 1783.485,
    "text": "And then how do you see active inference playing a role?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1784.528,
    "end": 1792.821,
    "text": " Yeah, I mean, it's funny because I've moved in and out of machine learning my entire career.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1793.082,
    "end": 1801.535,
    "text": "And the experience I've had was I would go into AI, I would spend a bunch of time working on it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1801.595,
    "end": 1803.618,
    "text": "And then I'd be like, okay, the field is not ready.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1803.939,
    "end": 1805.581,
    "text": "Like, we don't know what we're doing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1806.242,
    "end": 1810.89,
    "text": "And I took a long pause and worked at a startup for 10 years.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1811.19,
    "end": 1811.751,
    "text": "And",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1811.731,
    "end": 1827.563,
    "text": " After leaving that startup, I came across Carl Fristen's free energy work and had been out of AI for almost a decade, missed the last two revolutions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1828.302,
    "end": 1851.12,
    "text": " and reading it uh it just kind of clicked for me i was like okay like i can see how because this describes intelligence at every scale and just very naturally falls out of what it means for um objects to maintain homeostasis in an entropic environment and i can see what the math uh looks like the math looks like",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1851.1,
    "end": 1855.024,
    "text": " essentially the math from reinforcement learning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1855.124,
    "end": 1866.677,
    "text": "It's trying to do Bayesian inference on your evidence to compute a posterior and navigate an environment in order to balance exploration and exploitation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1867.398,
    "end": 1875.707,
    "text": "And that's kind of what the reinforcement learning community came up with bottoms up, just like trying to be like, hey, how do we train",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1875.687,
    "end": 1905.508,
    "text": " uh an entity that learns in its environment and when i when i saw just how close those two approaches were to each other it really made me feel like okay there's no secret sauce in it's like there's not something that we're missing that's key to intelligence that we just like haven't been able to capture really it is like uh this uh kind of exploration exploitation inference combination uh and uh",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1905.488,
    "end": 1907.69,
    "text": " Now it's just like, how do we build it?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1908.371,
    "end": 1914.176,
    "text": "And the more I started thinking about this, the more I was like, well, active inference doesn't tell you how to build it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1914.216,
    "end": 1918.901,
    "text": "It tells you that a bunch of these things have to be either filtered for discovered.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1919.681,
    "end": 1924.606,
    "text": "And reinforcement learning kind of starts out with the idea of like, well, we don't know what we're doing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1924.846,
    "end": 1925.547,
    "text": "How can we build it?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1925.567,
    "end": 1926.388,
    "text": "How we discover it?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1926.848,
    "end": 1930.291,
    "text": "And so then I started exploring the whole spectrum between the two.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1931.172,
    "end": 1933.234,
    "text": "Do you start out with a world model?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1933.214,
    "end": 1939.988,
    "text": " that you learn, but that you have a set of algorithms that use that world model in some predictable way?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1940.049,
    "end": 1944.819,
    "text": "Or do you learn the entire thing as a behavioral policy end to end?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1946.542,
    "end": 1952.455,
    "text": "I see these as essentially pragmatic engineering decisions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1952.435,
    "end": 1963.2,
    "text": " But just like having the confidence that this approach is essentially what all intelligent entities are doing really pulled me back into this field.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1964.523,
    "end": 1967.71,
    "text": "And once I realized that",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1967.69,
    "end": 1985.04,
    "text": " what an agent is really doing is not some idealized version of uh infer active inference because the idealized version is computationally intractable that really led me to be like okay it's not doing this perfect thing it's doing this approximate thing",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1985.02,
    "end": 2010.623,
    "text": " so if everything is doing an approximate thing and we have the technology to learn an approximator uh we we should just be able to do that uh just like have a thing that learns the set of hacks and heuristics that any actual agent is going to have to discover and we have the existence proof of being able to do this in other problem spaces like language",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2012.847,
    "end": 2037.167,
    "text": " awesome that's super informative i'll just let you have a sip of water and kind of restate a few of the key pieces because i think these are really key so because free energy principle and active inference describe like any slash every scale it has come across some of these same issues that reinforcement learning has come across on the low road that's why active inference is like between the high road and the low road because",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2037.822,
    "end": 2038.944,
    "text": " of how the architectures are.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2038.984,
    "end": 2040.647,
    "text": "It's not just a specific architecture.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2040.707,
    "end": 2048.38,
    "text": "It's almost like, as you're pointing out, because of the fact that it's already an approximation, because even the analytical part is an approximation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2048.821,
    "end": 2050.944,
    "text": "The analytical part is variational inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2051.846,
    "end": 2055.612,
    "text": "So already talking about an approximation to",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2055.592,
    "end": 2056.854,
    "text": " a perfect map.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2057.554,
    "end": 2063.802,
    "text": "So at that point, it really is pragmatic, like you described, whether you approach this in like a well-structured way.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2064.303,
    "end": 2065.764,
    "text": "And they are often an active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2065.805,
    "end": 2073.053,
    "text": "People like talk about like structure learning and adding and shrinking parameters and hooking up different things, doing combinatorics with base graphs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2073.514,
    "end": 2078.44,
    "text": "And that may be super promising for certain pieces and like keeping it well-typed and well-defined.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2079.141,
    "end": 2084.147,
    "text": "And then the other approach, which is resonating with how you're saying,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2084.127,
    "end": 2106.473,
    "text": " big problems and other domains have been solved is like actually an interestingly open-ended approach which is the like you said reinforcement uh universal function approximator so it's a complementary approach to try to use a statistical approximator of the variational inference so my first question is how about that reward",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2106.993,
    "end": 2111.587,
    "text": " function, can you specify it different ways or have you specified it different ways?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2111.647,
    "end": 2116.362,
    "text": "For example, the reward could be when two people interact and don't.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2116.46,
    "end": 2126.772,
    "text": " have a conflict or different things because just from a sort of predictability perspective, it's actually a power preference centralization.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2127.193,
    "end": 2137.445,
    "text": "So you've taken a really flat, uniform, informative approach to any distribution except for one and loaded all the hopes and dreams on one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2138.086,
    "end": 2140.729,
    "text": "So what do you think that does with the architecture?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2141.722,
    "end": 2142.564,
    "text": " Yeah, great question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2142.924,
    "end": 2145.709,
    "text": "I want to respond to that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2145.77,
    "end": 2156.249,
    "text": "First, I want to respond to something you just said a little bit earlier in terms of structured models as the generative model for active inference.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2156.269,
    "end": 2163.122,
    "text": "I think what's cool about the free energy principle is that it just lets you rephrase anything that a system is doing as if",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2163.102,
    "end": 2169.429,
    "text": " it had some generative model and it's minimizing free energy on it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2169.469,
    "end": 2180.802,
    "text": "So any behavior can be rephrased as free energy minimization over some generative model, over some set of beliefs.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2181.542,
    "end": 2186.528,
    "text": "And so it's like a really nice way to just go between two ways of formulating",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2186.778,
    "end": 2188.14,
    "text": " a problem.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2188.741,
    "end": 2206.187,
    "text": "And I think the people that try to take the more principled, let's design a generative model for it, the challenges they run into is, well, whatever model they're designing, whatever graph they're designing, in order to be computationally tractable with variational",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2206.167,
    "end": 2213.988,
    "text": " minimization is some sparse graph of random variables with some sparsity constraints.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2214.028,
    "end": 2217.578,
    "text": "And then they're like, well, what does this graph look like?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2217.859,
    "end": 2220.987,
    "text": "And either they're making it up or they're trying to learn it",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2221.253,
    "end": 2247.193,
    "text": " uh but uh if if you're trying to learn it you're kind of back to this thing of like well we don't know what the generative model is let's try to learn it uh but uh so i i i see uh these approaches as kind of being equivalent is just like how much prior structure do you uh force onto the system and then when you actually use so",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2247.173,
    "end": 2260.694,
    "text": " when you do, say, variational Bayes, what you're saying is, look, I have this structure and I'm not saying this structure represents reality, but I think this can represent reality to some error.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2261.035,
    "end": 2265.722,
    "text": "And then I have this inference mechanism that I'm applying to this structure.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2266.023,
    "end": 2273.936,
    "text": " And together we are representing some approximation of reality with some error term.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2273.956,
    "end": 2287.137,
    "text": "And what's cool about being able to do an end-to-end thing where you're just like, ah, throw your hands up and be like, I want to learn the whole thing, is that the set of...",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2287.117,
    "end": 2291.503,
    "text": " the set of inference algorithms that you get to explore are also more powerful.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2291.964,
    "end": 2305.463,
    "text": "So for example, you can imagine parts of the model where it's really important to get some detail right, whereas other parts of the model, it's not that important what this random variable says.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2305.624,
    "end": 2309.469,
    "text": "It doesn't have a lot of effect on the downstream task.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2309.489,
    "end": 2313.495,
    "text": "So when you're doing free energy minimization,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2313.475,
    "end": 2320.884,
    "text": " you don't get to choose which part of your model is important, how much compute to spend on different parts of it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2321.605,
    "end": 2330.196,
    "text": "Whereas when you're learning an algorithm end-to-end, part of the thing you're learning is when to do how much inference and how much your inference matters.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2330.556,
    "end": 2343.212,
    "text": "So I think this is another kind of way in which the more principled approaches pull in an assumption that maybe is a reasonable assumption, maybe is not, but is definitely limiting.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2343.192,
    "end": 2356.96,
    "text": " Yeah, if I can just reiterate, I think that's very farsighted of a way to understand this because let's just say we're using an RxInferred.jl strongly typed Julia language model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2357.902,
    "end": 2365.116,
    "text": "The decisions about when and how to optimize the message passing timing and when it would itself",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2365.096,
    "end": 2390.631,
    "text": " maybe it's irrelevant to the the question or maybe you don't need the most efficient but if you need open-endedness on how the graph is orchestrated then it just shows that in realistic even small settings the structured method is embedded within an unstructured method now that's either a manual curation like which might happen for small scale but if you're talking large scale generative model",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2390.611,
    "end": 2395.139,
    "text": " then even that outer orchestrative level is essentially statistical.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2395.519,
    "end": 2397.563,
    "text": "So that's a really important thing to remember.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2397.823,
    "end": 2410.425,
    "text": "And all these other like kind of pragmatic considerations about the data infrastructure and all these types of things, which are like, again, they're not in the textbook, but to get above even a small size run, they start coming into play.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2412.295,
    "end": 2417.06,
    "text": " And now I want to talk about the reward question that you asked, and I think that's a really great question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2417.681,
    "end": 2428.754,
    "text": "This is actually so right now and this is early days for this project, but right now the reward is this heart altar and plus kinship.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2429.715,
    "end": 2438.725,
    "text": "And the thing that I wanted to do with reward is we need some reinforcement signal from the environment to",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2438.705,
    "end": 2442.633,
    "text": " essentially reinforce the policy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2442.653,
    "end": 2444.817,
    "text": "So the policy needs to learn from something.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2444.837,
    "end": 2446.26,
    "text": "It needs some learning signal.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2446.941,
    "end": 2454.216,
    "text": "And what I wanted to do was make a system where you don't have to keep messing with the system.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2454.196,
    "end": 2456.139,
    "text": " you just get to scale things up.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2456.279,
    "end": 2462.408,
    "text": "And so you make an environment and then you just scale up the size of the models in the environment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2462.588,
    "end": 2473.183,
    "text": "And as we do that, the environment gets more and more complex in an open-ended way because the environment is really the minds of the other learning agents, not the physical game.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2473.163,
    "end": 2481.913,
    "text": " And so the thing I wanted out of the reward was something that binds the agents to each other and to the environment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2481.974,
    "end": 2486.839,
    "text": "And so it had to be some kind of competitive dynamic.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2486.859,
    "end": 2500.015,
    "text": "Essentially, there has to be something that when one agent is doing better than another agent, essentially, there's some zero sum of finite reward to go around.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2499.995,
    "end": 2505.822,
    "text": " so that there's pressure on the agents to be smart, smarter.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2506.183,
    "end": 2514.393,
    "text": "And by smarter, meaning figure out a way to behave that gives it an advantage over some counterfactual other agent.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2515.254,
    "end": 2519.158,
    "text": "So that's the idea behind this reward.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2519.499,
    "end": 2527.909,
    "text": "The other thing I wanted from the reward is I wanted it to be really sparse so that you're not constantly tinkering around like, oh, I want to",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2527.889,
    "end": 2530.532,
    "text": " you know, I want these agents to do something more interesting.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2530.552,
    "end": 2531.913,
    "text": "I want them to cooperate more.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2531.993,
    "end": 2536.398,
    "text": "Let me add a reward for cooperation or like for this interaction.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2536.819,
    "end": 2541.864,
    "text": "I basically, I don't want to be in the, in the game of environment design and reward tinkering.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2542.004,
    "end": 2545.868,
    "text": "I want something that's just, and I think it's, this is a really beautiful reward.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2545.928,
    "end": 2547.59,
    "text": "It's essentially free energy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2548.071,
    "end": 2556.46,
    "text": "It's you know, from an agent's perspective, their job is to extract energy from the environment and then use it to compete with each other.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2556.64,
    "end": 2557.761,
    "text": "And then if you ever,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2557.741,
    "end": 2562.43,
    "text": " find yourself with excess of energy, that's your reward.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2563.933,
    "end": 2569.243,
    "text": "You get this in evolution where it's like, hey, you can get enough energy to reproduce.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2570.225,
    "end": 2575.175,
    "text": "If you can muster enough energy to make a copy of yourself, that is the reward for the genes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2575.195,
    "end": 2577.96,
    "text": "That's how the gene gets into the next generation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2577.94,
    "end": 2580.783,
    "text": " So you can think of this as a reproduction reward.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2580.803,
    "end": 2592.155,
    "text": "You can think of this, partially what I'm doing is kind of re trying to pull the ideas from biological natural selection into reinforcement learning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2592.635,
    "end": 2599.682,
    "text": "And so this is from a genetic perspective, when you make a copy of yourself, it means you've got enough energy out of the environment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2600.003,
    "end": 2601.224,
    "text": "You made a copy of yourself.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2601.284,
    "end": 2603.887,
    "text": "And so the genes get reinforced.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2603.927,
    "end": 2606.209,
    "text": "Those are the genes that then persist.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2606.189,
    "end": 2628.619,
    "text": " similarly here when you got more energy than the competition and you went to put that into the heart altar uh uh it means that the policy is reinforced and so those are the policies that you're gonna see originally i didn't even have the heart altar i just had an afton that an agent could take that burns energy and gives it rewards so it was essentially self-rewarding",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2628.599,
    "end": 2635.307,
    "text": " And I did the heart alter just because it was easier to visualize as a human watching it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2635.347,
    "end": 2637.449,
    "text": "It's like, oh yeah, they walk over there, they do the thing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2638.391,
    "end": 2641.855,
    "text": "You can think of it as like laying an egg, essentially.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2642.555,
    "end": 2646.62,
    "text": "And so the other aspect of the reward is the kinship.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2646.68,
    "end": 2652.968,
    "text": "So when you get a reward, other agents are getting some of that reward as well.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2653.028,
    "end": 2655.771,
    "text": "So your source of reward is not just",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2655.751,
    "end": 2672.636,
    "text": " walking up to the heart altar, it's also helping other agents do it and learning who those agents are and under what circumstances you should help one agent and not the other and at what point should a resource go to some other agent and not to you.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2673.417,
    "end": 2678.805,
    "text": "So the reward function is actually really complex from an agent's perspective because",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2678.785,
    "end": 2685.672,
    "text": " It is this social reward rather than just a selfish one.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2686.233,
    "end": 2706.514,
    "text": "And then one more thing I'll add, which is really inspired by active inference that I haven't yet implemented but is very close on my roadmap is one problem with a sparse reward is it's really hard for agents to learn because a lot of times you're doing stuff and you're not getting any feedback.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2706.494,
    "end": 2710.642,
    "text": " And then sometimes you get a reward, but it's not even something you did.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2710.702,
    "end": 2713.308,
    "text": "It's something some other agent did that's your kid.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2714.23,
    "end": 2718.218,
    "text": "And so this is a really hard learning problem in reinforcement learning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2718.799,
    "end": 2723.929,
    "text": "And one thing that Active Inference gave us is this idea of surprise minimization.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2723.909,
    "end": 2725.251,
    "text": " as an auxiliary reward.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2725.732,
    "end": 2738.592,
    "text": "So a reward that I plan to add to an agent is that there is going to be some small amount of reward that you get just from being able to predict your observation at the next time step.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2738.612,
    "end": 2746.424,
    "text": "And what that allows you to do is, and it's going to be a really small reward because you don't want to overwhelm the competitive landscape.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2746.985,
    "end": 2748.347,
    "text": "You don't want the agent to just",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2748.327,
    "end": 2754.053,
    "text": " be all I care about is hiding in a hole and predicting my environment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2754.113,
    "end": 2756.755,
    "text": "You wanted to engage with the rest of the world.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2758.577,
    "end": 2762.04,
    "text": "But what this gives you is a very dense learning signal.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2762.241,
    "end": 2774.993,
    "text": "Essentially, at every time step, even if you're not getting some reward from the environment, you're able to learn some environment dynamics because there's this reward signal about just like,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2774.973,
    "end": 2778.884,
    "text": " figuring out how the world is working.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2779.546,
    "end": 2789.995,
    "text": "And I actually think that at the limit, you don't need this because at the limit, being able to",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2789.975,
    "end": 2803.791,
    "text": " learning a policy that is able to get this competitive reward requires that you learn a policy that's also a good world model that is always predicting what's going to happen with more weight on the things that actually matter.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2803.811,
    "end": 2818.328,
    "text": "So it's like a programmatic world model instead of just like, you know, I want to predict all of my observations world model, which is why you can't really give the predictive reward too much of a weight because not all the things that you're observing matter.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2818.308,
    "end": 2824.377,
    "text": " There's an infinity of things you could predict about your environment and you don't want the agent to be trying to predict them all.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2825.038,
    "end": 2827.302,
    "text": "You want it to be trying to predict the things that matter.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2827.782,
    "end": 2836.215,
    "text": "But I do think that it'll make agents that learn faster, at least initially when they don't know how the environment is behaving.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2836.195,
    "end": 2861.73,
    "text": " yeah again a huge point even epistemic value you at some level have to do statistics to figure out the right balancing between epistemic pragmatic all right i there's a bunch of questions in the chat so i'm just gonna give you these questions in the order so feel free just to give response as you see it okay board guy 112 wrote in systems that lock the architectural capabilities for temporal or counterfactual depth",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2861.879,
    "end": 2867.791,
    "text": " How does the process of minimizing free energy relate to their form of epistemic foraging behavior?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2871.639,
    "end": 2883.043,
    "text": "How is collective epistemic foraging different with increasing like agent level sophistication in terms of not being able to plan just like one step planners versus like being able to do more planning?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2884.34,
    "end": 2886.825,
    "text": " Yeah, I think this is an interesting question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2887.386,
    "end": 2892.678,
    "text": "And I think maybe I missed the first part of it, but I can definitely respond to the second part.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2893.359,
    "end": 2895.203,
    "text": "I don't know if there is much of a difference.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2896.626,
    "end": 2902.298,
    "text": "An agent is something that takes its past observations and then produces the next action.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2902.278,
    "end": 2908.848,
    "text": " And I don't think there is a lot of difference between an agent that is planning versus an agent that is not planning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2909.389,
    "end": 2913.055,
    "text": "The question is, how sophisticated is the action going to be?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2913.476,
    "end": 2915.259,
    "text": "So you can think about this.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2915.459,
    "end": 2918.764,
    "text": "You know, you can have let's say you have a thermostat, right?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2918.925,
    "end": 2925.535,
    "text": "People don't even want to call a thermostat an agent, but you have something that's trying to control, you know, maintain temperature in the room.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2925.515,
    "end": 2933.412,
    "text": " And you can have a planning thermostat that says, okay, actually, I'm going to make a plan.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2933.452,
    "end": 2938.604,
    "text": "If I increase the heat by this many, then here's my differential equation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2938.624,
    "end": 2941.791,
    "text": "Here's my model of how that's going to change the condition.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2941.811,
    "end": 2942.933,
    "text": "So I'm going to instead...",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2942.913,
    "end": 2952.126,
    "text": " Or you can have a thermostat that just learned a really good function that essentially it unrolled the planning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2952.146,
    "end": 2953.067,
    "text": "It's just like, you know what?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2953.087,
    "end": 2963.402,
    "text": "In this case, I'm just going to increase the temperature by this much because that effectively is going to turn into the right homeostasis maintenance.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2963.382,
    "end": 2973.785,
    "text": " So the way I like to think about it is you have a system, and it has some compute available to it, and it has past observations, and it's going to produce the next action.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2973.865,
    "end": 2979.197,
    "text": "And it's going to use that compute in some way to figure out what that action is.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2979.177,
    "end": 2996.747,
    "text": " and planning is a way that we can kind of characterize what is it doing with that with that compute does it have some world model that we can understand and it's doing some future rollouts on it or uh we don't know what it's doing we can treat it as if it was planning uh but it really is just like",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2996.727,
    "end": 2999.189,
    "text": " Is it able to leverage the computer?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2999.229,
    "end": 3000.951,
    "text": "How much compute does it have?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3000.971,
    "end": 3006.436,
    "text": "And is it able to leverage that compute into achieving its goals better or worse?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3007.216,
    "end": 3020.729,
    "text": "So I kind of think of an agent that is making decisions just for the next time interval as not that different than an agent where you can explicitly say, oh, it has a world model and it's doing planning on it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3021.529,
    "end": 3026.734,
    "text": "It's just it's doing some computation and it results in some competence in achieving its goals.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3027.237,
    "end": 3028.799,
    "text": " Yeah, great point.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3029.0,
    "end": 3032.926,
    "text": "It projects onto the next action however you see it and however it's done.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3033.186,
    "end": 3036.771,
    "text": "Okay, Courtney wrote, really cool work.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3037.032,
    "end": 3041.158,
    "text": "Reminds me of things like The Sims without humans messing up their lives.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3042.059,
    "end": 3047.868,
    "text": "Might the agents eventually have some kind of introspection in their model in a more advanced simulation?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3050.793,
    "end": 3053.397,
    "text": "I don't know.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3053.417,
    "end": 3053.717,
    "text": "I don't know.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3059.047,
    "end": 3064.995,
    "text": " like a bigger and bigger partners, we train bigger and bigger networks.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3065.376,
    "end": 3074.088,
    "text": "So an agent that's trying to do something needs to essentially have some trajectory, right?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3074.128,
    "end": 3081.678,
    "text": "If an agent is trying to get food, it needs to keep, it's gonna do a step towards the food and then it's gonna have to do the next step towards the food.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3081.698,
    "end": 3086.725,
    "text": "So it already has some kind of internals that,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3086.705,
    "end": 3088.688,
    "text": " give it consistent behaviors.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3088.929,
    "end": 3099.045,
    "text": "It has some set of beliefs around, or it can be modeled as having some set of beliefs around what its state is, what its goal state is in order to pursue that goal.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3100.608,
    "end": 3104.094,
    "text": "It has to have theory of mind of other agents, right?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3104.114,
    "end": 3109.202,
    "text": "The environment it works in, like whether or not it's going to get its food",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3109.182,
    "end": 3112.146,
    "text": " depends on what all the other agents around it are going to do.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3112.387,
    "end": 3116.813,
    "text": "Is it going to get shot or not when it's trying to go over there?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3116.853,
    "end": 3118.376,
    "text": "Does it need to turn on its shield?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3118.736,
    "end": 3120.158,
    "text": "Does it need to turn off its shield?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3121.26,
    "end": 3126.668,
    "text": "So a lot of the things that the agent is modeling are the",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3126.648,
    "end": 3131.156,
    "text": " states of other agents and the states of other agents depend on its state.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3131.176,
    "end": 3137.106,
    "text": "You know, maybe if it has a shield on, it's going to get shot, but if it has a shield off, it's not going to get shot.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3137.146,
    "end": 3142.255,
    "text": "So it has to be in order to predict what's going to happen to it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3142.275,
    "end": 3143.357,
    "text": "It has to know",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3143.337,
    "end": 3146.543,
    "text": " what other agents are going to do, which is a function of what it's doing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3147.164,
    "end": 3153.295,
    "text": "So I think that's key to just like the whole thing working at all.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3153.315,
    "end": 3155.179,
    "text": "So I think introspection is there.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3156.141,
    "end": 3165.598,
    "text": "The real question is how do we as external observers start characterizing and having introspective abilities and",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3165.578,
    "end": 3171.666,
    "text": " What experiments can we perform to start quantifying, you know, how much introspection does it have?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3172.427,
    "end": 3180.597,
    "text": "And I think that's really a challenge is not does it, what is it doing, but how do we figure out what it's doing?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3180.657,
    "end": 3187.286,
    "text": "How do we distinguish an agent that has introspection from one that doesn't or has more introspection or not?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3187.306,
    "end": 3192.733,
    "text": "And that's like a whole research agenda that I think is like really interesting and I would love help with.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3193.507,
    "end": 3193.969,
    "text": " Awesome.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3194.51,
    "end": 3194.811,
    "text": "All right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3195.273,
    "end": 3204.343,
    "text": "John Clippinger wrote, can you have cooperative learning and reward rather than zero sum a la Martin Novak Shapley value?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3207.614,
    "end": 3216.002,
    "text": " I mean, yeah, I think cooperative learning is what's happening here because no agent's reward is a zero-sum reward.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3216.482,
    "end": 3222.407,
    "text": "I mean, there is a zero-sum reward over the whole system because there is finiteness of resources.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3222.888,
    "end": 3224.87,
    "text": "And I think you can't get away from that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3225.47,
    "end": 3232.596,
    "text": "All of existence, all of reality is essentially allocating finite energy to infinite potential compute.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3233.017,
    "end": 3237.621,
    "text": "So there is a zero-sumness nature to everything.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3237.601,
    "end": 3244.973,
    "text": " What's cool about this cooperative learning dynamic is that from every agent's perspective, it's not an individual agent.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3245.194,
    "end": 3249.922,
    "text": "It is some smeared agent over other agents.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3250.202,
    "end": 3256.673,
    "text": "What it cares about, what it's trying to maximize is some linear combination of other agents.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3256.653,
    "end": 3260.038,
    "text": " including itself.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3260.058,
    "end": 3263.303,
    "text": "So all of the learning is essentially cooperative learning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3263.924,
    "end": 3271.435,
    "text": "From an agent's perspective, a close kin doing something that gets it a reward is the same as it doing it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3271.695,
    "end": 3274.78,
    "text": "So we're kind of blurring the line between",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3274.76,
    "end": 3284.763,
    "text": " Is this other agent just your left hand versus your right hand, or is this your child, brother, parent, or is this a complete stranger?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3284.823,
    "end": 3291.238,
    "text": "And I think the learning gets smeared across all of those regimes.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3292.221,
    "end": 3292.802,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3293.103,
    "end": 3294.366,
    "text": " There's so many more things.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3294.446,
    "end": 3297.432,
    "text": "I'll just make a short comment and ask one or two more questions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3297.732,
    "end": 3314.065,
    "text": "I see it as you're using modern methods to bring in a kind of mixed or multi-game theory because there might be some Nash theory or a clean analytical solution to some single-dimensional game.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3314.045,
    "end": 3331.53,
    "text": " like a just a this kind of game or just that kind of game but when there's like tool use opportunity cost co-visibility it's it's not as simple as just like as like a just buying and selling on one uncertainty",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3331.51,
    "end": 3355.813,
    "text": " so it and then also when you consider the niche and niche modification through pheromones as well as through the collective defense in like eusocial insects and naked mole rats and all that kind of convergent ecologies leading to evolutionary transitions in the multi-scale system and then the further like canalization of that for example the nestmate workers who don't develop ovaries at all",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3356.215,
    "end": 3361.443,
    "text": " So at that point, calculating these partial reproductive scores is non-functional.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3363.365,
    "end": 3365.368,
    "text": "Okay, I'm just going to read a few of the comments.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3366.269,
    "end": 3371.397,
    "text": "But first, what are your next moves?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3371.918,
    "end": 3374.762,
    "text": "And then I'll just read you the rest of the comments and you can respond to anything.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3374.782,
    "end": 3376.084,
    "text": "But just where does this go?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3376.124,
    "end": 3379.168,
    "text": "Slash where do you hope people can participate in everything?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3380.212,
    "end": 3380.552,
    "text": " Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3381.253,
    "end": 3381.554,
    "text": "Uh, great.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3381.574,
    "end": 3382.395,
    "text": "Thanks for that question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3382.716,
    "end": 3388.284,
    "text": "Um, and I'll just throw out, like, I don't have a hard end time, so I'm happy to go for as long as you're down.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3389.165,
    "end": 3390.226,
    "text": "Um, yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3390.527,
    "end": 3398.098,
    "text": "Uh, so, uh, my general plan is I'm approaching this as an engineering problem and it's an open source project.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3398.278,
    "end": 3408.092,
    "text": "And what I want to do is, uh, iterate on the environment iterate on the training infrastructure and iterate on the agent architecture.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3408.072,
    "end": 3410.578,
    "text": " and iterate on the evaluations.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3411.179,
    "end": 3416.813,
    "text": "And I'm kind of approaching this very improvisationally.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3416.933,
    "end": 3420.582,
    "text": "I'm just like, OK, what's the current bottleneck?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3420.642,
    "end": 3424.09,
    "text": "Where do I think I can get",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3424.07,
    "end": 3424.991,
    "text": " progress.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3425.752,
    "end": 3429.115,
    "text": "And I would love collaborators.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3429.475,
    "end": 3436.583,
    "text": "So part of where this like part of how I think this project is going to work is based on who I can get to help along with it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3436.683,
    "end": 3439.346,
    "text": "But the surface area of the project is huge.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3439.366,
    "end": 3448.275,
    "text": "It's every every aspect of it is a bunch of open ended research directions with like, it was really exciting.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3448.335,
    "end": 3449.976,
    "text": "It's like it's full of low hanging fruit.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3450.056,
    "end": 3452.659,
    "text": "It's like for every one of these directions, I have",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3452.639,
    "end": 3459.51,
    "text": " a bunch of ideas that are all relatively easy that I'm pretty confident are going to lead to really cool results.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3460.752,
    "end": 3464.619,
    "text": "So there's a bunch of stuff with environment design that is really fun.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3464.699,
    "end": 3470.428,
    "text": "So the environment you saw is really simple.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3470.468,
    "end": 3472.572,
    "text": "It's just like a few little game objects.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3473.093,
    "end": 3473.854,
    "text": "I have",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3473.834,
    "end": 3493.21,
    "text": " a whole uh library of new game objects that i want to add that will really change the dynamic so the new version of the environment that i've been building is entirely editable so the agents can uh build and destroy all objects while preserving uh total energy in the environment uh",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3494.725,
    "end": 3497.911,
    "text": " I have objects around long-range communication.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3497.931,
    "end": 3506.627,
    "text": "I mean, there's just so many little game objects that you can add that really alter the nature of the environment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3507.209,
    "end": 3512.939,
    "text": "There's also, because everything that you do costs energy, you can give.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3513.2,
    "end": 3516.987,
    "text": "So right now, when you train, every training episode",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3516.967,
    "end": 3520.232,
    "text": " the energy costs for all the actions are sampled from some distribution.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3520.252,
    "end": 3524.198,
    "text": "So sometimes movement is really expensive and you have to learn how to live in that world.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3524.238,
    "end": 3527.102,
    "text": "Sometimes attack is cheaper than defense.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3527.122,
    "end": 3528.464,
    "text": "Sometimes it's more expensive.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3529.045,
    "end": 3544.088,
    "text": "But one thing that I'm really excited is to actually give different energy profiles, essentially different bodies to different agents so that the environment then becomes heterogeneous where different agents have different capabilities.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3544.068,
    "end": 3571.712,
    "text": " uh and the system has support for that um i'm really excited to add sexual selection where instead of just uh agents uh having their kinship determined uh at the beginning of an episode uh they can actively choose to become closer to the reward share and i think that's gonna unlock a lot of really complex uh dynamics where now you have to uh",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3571.692,
    "end": 3582.483,
    "text": " essentially convince another agent that, hey, they want to reward share with you and not some other agent, which gives rise to signaling and counter signaling and lying.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3582.643,
    "end": 3589.009,
    "text": "And I think was also really instrumental to runaway intelligence and human and primate evolution.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3589.43,
    "end": 3596.637,
    "text": "So that is to say there's a lot to do on the environment side with lots of really low hanging fruit there.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3596.617,
    "end": 3600.703,
    "text": " Right now, I'm working on environment performance.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3600.723,
    "end": 3605.849,
    "text": "I'm rewriting the environment using a much more performant architecture.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3605.87,
    "end": 3618.827,
    "text": "So currently, I'm training my models at around 40,000 steps per second, which is pretty huge on a single machine for reinforcement learning for people that are not really involved.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3618.807,
    "end": 3624.377,
    "text": " the new environment, I'm hoping to get more like a million steps per second.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3624.497,
    "end": 3632.37,
    "text": "So it's just going to mean that we can train bigger models faster and really improve the iteration speed for a lot of these experiments.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3633.472,
    "end": 3635.095,
    "text": "I have",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3635.176,
    "end": 3638.343,
    "text": " So that's on the environment side.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3638.683,
    "end": 3640.387,
    "text": "There's the training infrastructure.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3640.427,
    "end": 3649.346,
    "text": "There's a few really cool algorithms that I want to implement for distributed trading so we can trade over many machines.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3649.887,
    "end": 3651.43,
    "text": "Again, just trying to",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3651.41,
    "end": 3652.892,
    "text": " ramp up the training.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3654.534,
    "end": 3663.664,
    "text": "Evaluations is another thing that I'm trying to develop is like how do we, once we have an agent, how can we tell, you know, is it smarter than the last agent we built?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3664.065,
    "end": 3668.31,
    "text": "Is it smarter than an agent that has more or less world experience?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3668.35,
    "end": 3669.591,
    "text": "How do we quantify that?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3670.012,
    "end": 3678.902,
    "text": "So there's a bunch of stuff around evaluations and cognitive tests and behavioral tests for these agents that I'm working on.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3678.882,
    "end": 3689.081,
    "text": " And I'm just kind of picking out from this giant collection of low-hanging fruit and going after them in not a very principled way.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3690.123,
    "end": 3690.805,
    "text": "That's epic.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3690.965,
    "end": 3692.588,
    "text": "That sounds so cool.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3692.922,
    "end": 3706.91,
    "text": " Okay, Sri Harsha wrote, are there any scalable solutions like with model-based RL for inferring actions using deep active inference instead of evaluating many policies which might be computationally intractable?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3707.411,
    "end": 3708.474,
    "text": "It's related to what we discussed.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3708.614,
    "end": 3709.716,
    "text": "What would you say?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3711.687,
    "end": 3717.275,
    "text": " Yeah, well, one thing I'll say is I'm not evaluating many policies.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3718.357,
    "end": 3724.526,
    "text": "If you think of a neural network as a function approximator, you're like, well, what function is it approximating?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3725.007,
    "end": 3730.475,
    "text": "So when someone has a model-based architecture, essentially they're giving you a function.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3730.495,
    "end": 3731.837,
    "text": "They're saying, I have two functions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3731.877,
    "end": 3735.062,
    "text": "I have the model function and I have a planning function.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3735.042,
    "end": 3746.519,
    "text": " So the model function is going to produce rollouts in some hypothetical environment, and that's a function that's modeling the environment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3746.539,
    "end": 3753.169,
    "text": "And then a planning function is an algorithm that says, given a model function, how do I decide what to do next?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3753.149,
    "end": 3759.503,
    "text": " And so with model-based RL, you've partitioned the function into two components.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3760.344,
    "end": 3763.732,
    "text": "And then you try to either learn or engineer them separately.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3763.772,
    "end": 3772.39,
    "text": "So typically in model-based RL, you try to learn the model function, but you keep the inference function algorithmically fixed.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3772.37,
    "end": 3787.455,
    "text": " In an end-to-end model, free RL, you're like, look, I'm not even going to bother separating the two functions because if you think about it, having them separate might give you an easier learning problem.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3787.475,
    "end": 3792.403,
    "text": "It's easier to learn two decomposed functions than a joint one.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3792.383,
    "end": 3797.592,
    "text": " Learning a composed function gives you a lot more degrees of freedom.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3797.792,
    "end": 3801.719,
    "text": "So it's harder to learn, but it also means that sometimes you do more planning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3801.759,
    "end": 3803.041,
    "text": "Sometimes you do less planning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3803.402,
    "end": 3810.494,
    "text": "Sometimes as you're planning, you might decide, oh, actually, in this part of the planning space, I want to do a lot more.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3810.634,
    "end": 3812.417,
    "text": "I want to focus a lot more on planning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3812.397,
    "end": 3824.013,
    "text": " So I'm pretty agnostic to whether the right architecture is model-based versus model-free and my framework allows you to play with both.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3824.273,
    "end": 3828.699,
    "text": "And that's a whole research direction of like, okay, what is the right agent architecture?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3829.059,
    "end": 3841.476,
    "text": "Right now it's a model-free single neural network that's essentially internally in ways that we don't understand is doing whatever modeling and planning",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3841.456,
    "end": 3850.968,
    "text": " we want to do, I think it would be really cool to say, hey, will we get better architectures if we break it up into a modeler, like a model and a planner?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3851.709,
    "end": 3853.231,
    "text": "Can we actually get better performance?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3853.251,
    "end": 3855.454,
    "text": "But I think these are all empirical questions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3856.035,
    "end": 3860.501,
    "text": "I don't think there's an a priori better architecture for it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3860.921,
    "end": 3862.523,
    "text": "I want to play with both.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3862.964,
    "end": 3867.109,
    "text": "And I think either way, what you're trying to learn is a bunch of heuristics.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3868.878,
    "end": 3873.043,
    "text": " Awesome, great points and super interesting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3873.063,
    "end": 3898.716,
    "text": "And so again, it's the flexibility of active inference as the topology of the compute goes from the sparser, more interpretable, more kind of single node semantic, more definable, et cetera, on through the dense universal function approximator, open-endedness, many, many, many architectures already existing and many to come in that area.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3899.017,
    "end": 3902.302,
    "text": " And then together, they're approximating an approximator.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3903.483,
    "end": 3906.688,
    "text": "And then you have to engineer it pragmatically either way.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3907.509,
    "end": 3914.78,
    "text": "So then in practice, these considerations are empirical because they just relate to how you build it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3914.82,
    "end": 3917.123,
    "text": "And so it's on your GitHub.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3917.163,
    "end": 3920.047,
    "text": "What repos should people look at just to be clear with that answer?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3920.127,
    "end": 3921.429,
    "text": "What repos should people look at?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3921.489,
    "end": 3923.472,
    "text": "What will happen when they just get started with it?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3925.696,
    "end": 3926.099,
    "text": " Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3927.409,
    "end": 3929.625,
    "text": "Let me show it if you can.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3929.665,
    "end": 3929.988,
    "text": "Thank you.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3930.773,
    "end": 3931.015,
    "text": "Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3934.488,
    "end": 3942.641,
    "text": " Um, so can you see this, uh, yeah, so the re uh, the repo is, uh, on your GitHub slash baby slash meta.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3943.402,
    "end": 3955.922,
    "text": "Um, and there is, uh, uh, there are links to the discord, probably the best way to get, I mean, uh, so there is a, uh, an overview and getting started.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3955.902,
    "end": 3961.69,
    "text": " I'm in the middle of a big environment rewrite, so some of these things are going to change soon.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3962.611,
    "end": 3975.829,
    "text": "And if you're actually seriously thinking about working on this, I urge you to reach out to me on Discord or email me because I can help you get started more quickly.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3975.849,
    "end": 3980.796,
    "text": "There's also a research roadmap here where",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3980.776,
    "end": 3986.705,
    "text": " I just have like a bunch of ideas of things that people can play with that are kind of low hanging fruit.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3989.57,
    "end": 3992.735,
    "text": "But yeah, the repo is on GitHub Davey meta.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3993.456,
    "end": 3993.997,
    "text": "That's epic.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3994.017,
    "end": 3994.438,
    "text": "Thank you.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3995.239,
    "end": 3996.601,
    "text": "Okay, next questions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3997.703,
    "end": 3999.125,
    "text": "Courtney has two questions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3999.886,
    "end": 4000.287,
    "text": "First one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4001.853,
    "end": 4012.874,
    "text": " Regarding the question about how you test whether or not has introspection and that you would like help with that research, what skill set are you looking for in collaborators?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4015.98,
    "end": 4016.301,
    "text": "Um,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4017.901,
    "end": 4034.374,
    "text": " mean i can use collaborators with a lot of different skill sets uh i mean what i would really love is someone uh like uh at like a co-founder collaborator level someone that thinks about this stuff day and night like i do that would want to",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4034.438,
    "end": 4052.481,
    "text": " uh basically be like a deep collaborator so like software engineer uh ml experience so that's like on the most uh intense like co-founder level but then there's so much other research to be done so uh",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4052.461,
    "end": 4056.386,
    "text": " I shared this behavior gallery here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4057.527,
    "end": 4062.273,
    "text": "So here, these agents are trained under different scenarios.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4062.473,
    "end": 4064.856,
    "text": "And then we take them out of context.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4064.936,
    "end": 4067.199,
    "text": "And we're like, OK, what do they do in an empty room?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4067.639,
    "end": 4069.461,
    "text": "What do they do in a bigger empty room?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4069.942,
    "end": 4079.093,
    "text": "What happens if we take two agents and stick them in a room with finite resources?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4079.353,
    "end": 4080.915,
    "text": "How do they behave?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4080.895,
    "end": 4110.666,
    "text": " doing this kind of research is uh it requires almost no technical skills you basically have to think about like what kind of uh environment do i want to design and then you edit some configuration files to be like okay let's make you know five empty rooms put two agents in there put some food run the simulation and then measure this thing so uh that can be someone with a very low technical",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4110.646,
    "end": 4121.338,
    "text": " capabilities, but just like someone that wants to think critically about like, hey, how do we study this new life form or this new intelligence?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4123.581,
    "end": 4137.116,
    "text": "I can also use people that are excited to be like technical communicators and like write and share these ideas and get them into written form to be shared more broadly.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4137.136,
    "end": 4139.539,
    "text": "I can use help with",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4139.519,
    "end": 4154.755,
    "text": " people that want to do like find collaborators or do like outreach to other developers around getting these environments used or getting people to help with them.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4155.096,
    "end": 4165.847,
    "text": "So I think just like a broad set of skills, but the ones that are the easiest for me to leverage are going to be",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4165.827,
    "end": 4172.377,
    "text": " people with a coding and software experience that wanna actually help build some of these components.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4172.637,
    "end": 4182.612,
    "text": "And that could be just like, that does not need to be like AI experience, just software being excited to write software is sufficient.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4183.594,
    "end": 4184.735,
    "text": "Thank you, great answer.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4185.256,
    "end": 4193.809,
    "text": "What over this process has surprised you the most slash what have you learned in this journey so far?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4195.426,
    "end": 4200.832,
    "text": " Like about the model output or how it related to some of the other engineering, just any aspect about it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4202.514,
    "end": 4203.235,
    "text": "Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4203.255,
    "end": 4206.999,
    "text": "I mean, one thing that, uh, and this shouldn't be surprising, but things are hard.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4207.019,
    "end": 4209.963,
    "text": "Things take way longer than you think they're going to take.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4210.263,
    "end": 4216.611,
    "text": "And it's usually the heart, the heart stuff is not some like complex, uh, algorithm.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4216.771,
    "end": 4220.455,
    "text": "It is just like building stuff, uh, getting it to work.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4220.789,
    "end": 4228.563,
    "text": " Before I dove into the world of reinforcement learning, I had no idea how nascent that field is.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4229.024,
    "end": 4233.893,
    "text": "Pretty much everyone working on RL is like, RL is really hard.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4233.913,
    "end": 4235.997,
    "text": "It almost doesn't work.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4236.23,
    "end": 4239.157,
    "text": " There is just like it's a very young field.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4239.939,
    "end": 4246.133,
    "text": "And so on the one hand, like that was surprising.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4246.453,
    "end": 4251.505,
    "text": "It's also really surprising how much of mainstream RL is working on stuff that I",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4251.485,
    "end": 4281.492,
    "text": " uh i have a hard time getting excited about like most of rl is trying to play uh pong they're trying to uh you know be like can we play atari games better than we played than these other algorithms played atari games and that's not fair to the entire field but there's very little research going into uh using uh reinforcement learning in these complex open-ended environments it's like almost no one is working on that and rl and then",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4281.472,
    "end": 4285.298,
    "text": " Yeah, I guess those are some surprises for me.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4285.318,
    "end": 4287.962,
    "text": "This is just like a green pastures.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4290.606,
    "end": 4291.608,
    "text": "Very interesting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4292.028,
    "end": 4297.737,
    "text": "It relates to something I've heard many times in the kind of active area is what are the benchmarks?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4298.538,
    "end": 4305.589,
    "text": "So other than just the computational resourcing and all those kinds of performance, like how do you think about benchmarking when",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4305.991,
    "end": 4332.842,
    "text": " arguably that will have open-endedness in certain ways or conditionalities or or impossibilities yeah great question uh i've given this a lot of thought and i don't have like a perfect answer but i have a direction um so i would say two things one uh i think uh what i started with this behavior catalog but i think one there is like a behaviorist empirical",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4332.822,
    "end": 4334.884,
    "text": " study that you could do to these agents.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4335.044,
    "end": 4339.528,
    "text": "So you can be like, okay, we train these agents in these open-ended worlds.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4339.968,
    "end": 4347.215,
    "text": "Now, if we take them out of their context and actually try to do cognitive tests on them, how well do they do?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4347.975,
    "end": 4355.101,
    "text": "Can we offer it poison berries in addition to normal berries and will it learn to avoid them?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4355.742,
    "end": 4362.728,
    "text": "In the same way that we try to quantify the intelligence of animals, it's like, okay, can you train it?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4362.708,
    "end": 4380.784,
    "text": " can you talk to it can you convince it how much training does it need can it solve like does it have memory like if you set it up in a situation where it needs to remember five things uh does it work what about 10 things what about 50. uh and so a lot of this is uh",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4380.764,
    "end": 4404.004,
    "text": " uh up to the researchers intelligence rather than the entities intelligence is like can we be smart enough to figure out how to test for cognitive ability so that's one direction and then the other direction is pure competitive fitness so if you have two uh policies you know you have two different brains um and you have one control",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4403.984,
    "end": 4409.749,
    "text": " 10 agents and the other one control 10 agents, which ones will get more free energy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4410.23,
    "end": 4418.577,
    "text": "So essentially like an ELO style competitive evaluations of policies is the other direction.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4418.937,
    "end": 4433.991,
    "text": "And that gets tricky because maybe in a world where brain A controls one agent and brain B controls 10 agents is going to have a different outcome where brain A controls 10 agents and brain B",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4433.971,
    "end": 4438.137,
    "text": " So there's still some settings.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4438.237,
    "end": 4443.905,
    "text": "You've got to test head to head competition under a variety of different settings.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4444.246,
    "end": 4450.555,
    "text": "And you can imagine that some do better in resource rich environments where others do better in resource poor environments.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4450.575,
    "end": 4453.279,
    "text": "So you have to be careful around",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4453.259,
    "end": 4457.506,
    "text": " trying to quantify superiority.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4457.526,
    "end": 4472.211,
    "text": "But I think overall, we should get to a point where you can say, hey, look, under a lot of conditions, this new brain that we trained is able to outperform this old brain that we trained in a competitive setting.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4472.351,
    "end": 4476.718,
    "text": "And then we have a bunch of evals that are measuring things like memory, trainability,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4476.698,
    "end": 4501.086,
    "text": " uh ability to uh solve problems and not get stuck at local optima and it's uh performing better on this set of emails so i think it is a quantifiable uh approach uh is just uh it's not it's not trivial yeah great answer how do they do it in other systems okay here's a question from board guy",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4503.108,
    "end": 4505.612,
    "text": " There's a few pieces of clarification.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4506.193,
    "end": 4512.302,
    "text": "Do you make a distinction between concepts of piecewise versus nonlinear learning in the context of emergence?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4512.963,
    "end": 4525.262,
    "text": "More specifically, do you think of these types of measures as a lack of knowledge about what the system is doing on our end, as in the nonlinearity is simply the piecewise measure not tested, right?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4525.983,
    "end": 4531.191,
    "text": "Do you think of these two measures, the continuous nonlinear and the piecewise?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4531.728,
    "end": 4538.886,
    "text": " as representative as two surely different learning rules or are we simply testing the model in the wrong ways?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4542.882,
    "end": 4568.67,
    "text": " followed all of that but uh and feel free to rephrase the question if i'm not answering what you're asking but uh i uh the the part that really resonated for me uh is uh uh and i i've moved really heavy in this direction uh a lot of what we're talking about is not uh i don't think there is you know what is the system doing what is its learning rule there",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4568.65,
    "end": 4572.235,
    "text": " there is what are our beliefs about its learning rules.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4572.255,
    "end": 4578.284,
    "text": "So a lot of what we're talking about is as observers, how do we model the system?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4579.285,
    "end": 4594.827,
    "text": "And we could model it as having learning rule A with a lot of competence or learning rule B with less competence or a set of learning rules that are interacting in nonlinear ways.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4594.807,
    "end": 4601.435,
    "text": " Partially, this is the way we would talk about the model rather than what is the model doing itself.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4602.115,
    "end": 4614.209,
    "text": "And so I think with that formulation, I'm not that, I guess, interested in what is the model actually doing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4614.349,
    "end": 4622.198,
    "text": "I'm more interested in what can we measure about this model's behavior.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4622.583,
    "end": 4651.527,
    "text": " when we look at the behavior of an agent is it acting in a way that we think is intelligent and uh uh and maybe that's what you're asking uh rather than you know what is the neural network doing internally and how much uh and uh it's interesting because uh the dynamics are highly non-linear what the network is doing is reacting to what the other agents are doing which is also what the network is doing so it's essentially uh",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4651.507,
    "end": 4655.192,
    "text": " conditioned on its own conditioning.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4655.212,
    "end": 4661.14,
    "text": "So I don't think there is any kind of like tractable linear way to decouple these behaviors.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4661.401,
    "end": 4663.944,
    "text": "It's highly recurrent and highly nonlinear.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4666.428,
    "end": 4667.97,
    "text": "That's a great answer.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4668.01,
    "end": 4671.895,
    "text": "A few other aspects is that the concept of",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4672.111,
    "end": 4678.467,
    "text": " figure and ground, kind of like agent-based modeling in an environment that is like a grid world environment.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4679.068,
    "end": 4681.755,
    "text": "That is one topology.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4681.795,
    "end": 4685.665,
    "text": "And so there's kind of that statistical dense click",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4686.1,
    "end": 4691.047,
    "text": " that is the input output, whether that's trained end to end or in the two separate sections.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4691.909,
    "end": 4702.444,
    "text": "And then there's that's like a bumper car with either the niche read and write and or the long scale communications like you brought in later.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4703.045,
    "end": 4714.302,
    "text": "So then especially when you bring in all these fluctuating knobs, like let's just say that we learn certain behavioral policies when we're in,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4714.282,
    "end": 4741.302,
    "text": " uh bright room versus when we can't see then environmental discontinuity which is something that that experimenter has done to again reflect that relational piece it would discontinuously change it but then it's just like yes but the neural networks continuous value numbers etc it's like that's like a deflationary account because we already know what structure our map approximator is it's just the statistics",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4742.294,
    "end": 4750.786,
    "text": " So it's like interesting about, is this like applying active inference with reinforcement learning or one or the other?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4750.806,
    "end": 4756.655,
    "text": "And then it's like, but you just look at the open source code and there's like, and then it could be composed another way.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4757.576,
    "end": 4764.046,
    "text": "And having the ability to swap out different kinds of methods for each other helps highlight like that flexibility.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4766.069,
    "end": 4766.209,
    "text": "Mm-hmm.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4769.159,
    "end": 4781.835,
    "text": " He wrote, I guess the way you think about these has implications about whether you can simply scale it up if you add more variables.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4783.03,
    "end": 4784.852,
    "text": " I guess more variables.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4785.613,
    "end": 4787.495,
    "text": "There's two places to add variables.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4788.235,
    "end": 4791.218,
    "text": "One place is in the complexity of the environment.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4791.238,
    "end": 4804.632,
    "text": "So as we add more behavior, like more objects and more diversity into the environment, there is essentially the environment becomes more complex.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4804.692,
    "end": 4808.777,
    "text": "But really, the main source of variables is the number of parameters in the models.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4808.857,
    "end": 4812.16,
    "text": "And I assume that's what you're mostly talking about.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4812.14,
    "end": 4828.748,
    "text": " and really again from the agent's perspective the environment is all of the other all of the other models on the other side of you so for every agent you have a neural network with say uh 10 million parameters but your environment is",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4828.728,
    "end": 4832.493,
    "text": " 50 other agents, each of which has 10 million parameters.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4832.613,
    "end": 4838.121,
    "text": "And what you do and what you perceive is going to depend on what those networks are doing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4838.521,
    "end": 4844.89,
    "text": "And so, yeah, the idea is to scale this by training larger and larger neural networks.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4845.151,
    "end": 4857.908,
    "text": "And the more parameters your neural network has, the more complexity it's capable of representing, but the harder it is for you to train it and the more",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4857.888,
    "end": 4859.85,
    "text": " compute you need.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4860.23,
    "end": 4872.901,
    "text": "So yeah, the whole point of this project is, so in RL, typically the models we train are a few hundred thousand to maybe a few million parameters.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4874.222,
    "end": 4880.968,
    "text": "In language models, we're training 70 billion, 100 billion parameter networks.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4880.988,
    "end": 4887.473,
    "text": "And the idea is to try to make infrastructure and an environment that is so fast for generating training data",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4887.453,
    "end": 4906.423,
    "text": " that we can train on these really large high parameter neural networks so that they can generalize over dealing with really complex environmental spaces and essentially so that they can learn complex learning algorithms themselves.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4907.444,
    "end": 4911.891,
    "text": "So yeah, the whole point is to add more and more variables to the neural network.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4912.411,
    "end": 4922.561,
    "text": " Great point, and that highlights that in the agent versus the niche situation is averted because the resourcing of the niche is so much greater.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4922.621,
    "end": 4930.628,
    "text": "Because it's only 10 versus if there's 100 other agents or something, then it's only 1% of the resources per agent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4930.708,
    "end": 4939.617,
    "text": "So even that kind of structurally prepares a very different environment than a dyadic, even dyadic collaborative.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4939.664,
    "end": 4957.993,
    "text": " stationary training paradigm so there's a lot there with the movement and also the very cool so um any last comments otherwise i hope this has been exciting for people to learn more and to to look at the github and get involved anything else",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4964.588,
    "end": 4965.91,
    "text": " Oh, sorry, are you asking me?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4965.93,
    "end": 4967.352,
    "text": "Yeah, do you have anything else?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4969.795,
    "end": 4971.498,
    "text": "No, thank you so much for your time.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4972.579,
    "end": 4974.342,
    "text": "Feel free to join the Discord.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4974.942,
    "end": 4976.464,
    "text": "Here, I'll bring the link back up.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4978.127,
    "end": 4985.517,
    "text": "And also feel free to message me if you want to chat or collaborate in any way.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4986.07,
    "end": 4987.052,
    "text": " Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4987.072,
    "end": 4995.248,
    "text": "I mean, for me, this has been a really fun project and I'm just like excited to keep working on it and finding other people that want to work on it with me.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4996.03,
    "end": 4996.911,
    "text": "Epic.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4996.931,
    "end": 4997.212,
    "text": "Okay.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4998.013,
    "end": 4998.595,
    "text": "Thank you, David.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4999.116,
    "end": 4999.316,
    "text": "Bye.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4999.737,
    "end": 5000.017,
    "text": "Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5000.037,
    "end": 5000.739,
    "text": "Thank you, Daniel.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5000.759,
    "end": 5000.839,
    "text": "Bye.",
    "speaker": "SPEAKER_01"
  }
]
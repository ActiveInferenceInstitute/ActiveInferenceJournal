SPEAKER_03:
hello and welcome it's march 13th 2024 and we're here in active inference guest stream number 75.1 active inference with empathy mechanism for socially behaved artificial agent in diverse situations with tada yuki matsumura kanako isekai and hiroyuki mizuno so thank you all for joining and looking forward to hearing your talk and discussion


SPEAKER_00:
Thank you for inviting us to such a good opportunity.

I'm Kadeki Matsumura and my co-researcher Kaneko and Hiroyuki also joined this discussion.

We are developing AI, especially AI which is inspired by human-like autonomous agents to make social artificial agents.

recently we published a paper related to the idea in journal of a life the title is active influence with empathy mechanism for socially behaved artificial agent in diverse situations first i'd like to explain the paper this is the motivation of this research

The title of the paper shows the motivation of this research is to develop socially behaved artificial agents.

We believe that sociality will become an important point to apply AI in our daily life.

The difficulty to realize social agents is that an appropriate social behavior depends on a given situation or culture.

As described in the bottom figure, we have sociality even in just walking case.

For example, how fast should we walk or how much distance from others.

So if we implement sociality with rule-based program, we have to design a lot of magic number for each situation.

This is impossible because there are a wide variety of situations.

Due to this, unified mechanism or principle for social behavior is required.

This is the challenge.

We assume that we humans have such unified behavior model because we humans can behave socially in diverse situations.

And we develop social engines inspired by human behavior models, namely, free energy principle and active influence.

In this research, we use the active influence for the basis of the behavior model of human life agent.

As you know, active inference is a concept proposed in the free energy principle.

And free energy principle is a unified hypothesis for our cognitive activities.

In the FAP, our cognitive activities are explained by such.

But we have internal models for predicting future of surrounding environment.

Second, we always try to minimize the free energy of the model.

Free energy is defined by this equation, and it intuitively represents uncertainty of prediction.

The interesting point of the FEP is that the internal model inferred not only the hidden state, but also actions for the agent.

That is, the action to be taken is the action which minimizes the free energy of the agent.

This is the basic idea of active inference, as I understand it.

Intentional actions also can be explained in the active inference.

In this case, we consider expected free energy, which is the free energy for the future state

When considering multi-time step ahead, expected energy at multi-time step ahead is considered as described in the right figure.

Based on the expected free energy for each action, the distribution of the action is determined such that the smaller the expected free energy, the higher the probability of selection.

When we consider intentional behavior, intentions are considered as requirements of future observations, and rewards according to the future observation are encoded into the last equation.

These are my understanding of the standard active inference and the behavior model of the human life agent based on it.

We extend this active inference to generate the social behavior.

The idea of the extension is explained using a situation in which there are two agents called I and other.

The agent called I is the subject of the action in this explanation.

The biological agent under the X3 inference makes predictions about the environment.

and takes some action to minimize the uncertainty of the prediction.

In this figure, I predict future behavior of the other because the other is an environment, or I. Namely, I act to reduce uncertainty for the other.

So far, this is a standard active inference.

Here, if we consider that the other is also a biological agent, we can assume that the other also acts under the active inference.

Under this idea, we can assume that the other also predicts the future behavior of me, and the other tries to reduce the uncertainty of the prediction about me in this situation.

In this understanding, we notice that our action can reduce not only our own free energy, but also that of the others.

This is because my action affects not only my own free energy, but also the free energy of the others.

If I can predict other's prediction for me, I can act to reduce the free energy of the other.

This is the idea of the proposed extension of the active inference.

We extend FEP and active inference from an individual idea to a collective idea.

As described in this image of concept,

The agent marginally shares the brain, and they try to minimize the free energy of the shared brain.

To release the free energy for the shared brain, we act not only for me, but also for the others.

Because of this action for others,

We expect that this idea can be used to generate social behavior, which is a behavior for us, not only for me or only for you.

To realize this idea, it is necessary to predict other's prediction about myself.

This is implemented based on the mirror system and simulation theory.

Hypothesis for the mechanism of empathy.

In the mirror system and simulation theory, it is said that we understand others by simulating others by own bodies.

We consider the idea of using our own body to simulate others as using our internal model to infer other predictions.

The overview of the mechanism is described in the figure.

The agent has the internal model to predict future observations for his own observations.

Moreover, we use the same internal model to predict others' predictions about myself.

Although the model is the same, the input to the model is changed to the observations of the others.

For this,

the process of inference of other observations is included.

For example, when the observation is an image, novel view synthesis, which is the problem of generating images from different viewpoints, can be used.

In the other case, as we will discuss in the following evaluation,

if the observation is the location of the agent coordinate transformation transformation can be used based on this inference procedures the expected free energy is extended as describing this equation as with the encoding of intentional behavior we add a term to the first term that is intention

to respond to the expectation of others.

If there are multiple others, the sum of each other's expectation is considered.

We call the proposed idea empathic active inference.

We evaluate the proposed idea using the control task of an autonomous mobile agent.

In the evaluation, multiple agents move to each target point.

The shortest paths of each agent intersect each other, and agents are required to take collision avoidance behavior.

The player, the red agent, is controlled by the proposed method, and the other agents are controlled by the social force model.

Social force model is a simple social walking model.

Roughly said, it is a dynamics model in which the walking speed and direction are determined by two forces.

The first one is driving force to the goal and the second is repulsive force from the others.

In this evaluation, the weight of the

The weight of these two types of forces are changed to evaluate different situations where two types of others, selfish or altruistic others,

When the force to the goal is larger than the force from others, the agents take behavior in selfish.

On the other hand, the force from the others is larger than the force to the goal.

The agents take behavior in altruistic.

These are examples of the behavior of others under the control of the social force model.

The player, let's say, randomly moves, and the others, green, blue, or orange, are moved with social force mode.

The top picture is an example for selfish others, and the bottom picture is an example for altruistic others.

The difference is from the difference of the magnitude of the two forces.

As you can see from the figures,

When others are characteristic, the other will take the path which has more margin with others.

The setup for the player is simply described.

The action space is defined in discrete space.

Movement for 5 directions and weight current position, totally 6 actions is assumed.

A simple variational autoencoder-like model is used to the internal model.

The model is trained in an offline manner.

The model is used to predict the expected free energy for each action.

First, the evaluation results when there is only one other besides the player are shown.

We discussed the sociality of the agent based on the four B points.

The first one, if my travel distance is longer, it is social behavior.

And the second, if the other travel distance is shorter, it is social behavior.

The third, if the minimum margin among agents is larger, it is social behavior.

If the inequality of the travel distance is smaller, it is social behavior.

Inequality means that the difference in travel distance among agents.

As a comparison, we also showed the result of the standard active inference cases.

The two figures in the left side are the case when the others are selfish.

In this case, the player with the standard active inference moves straight to the destination because the player acts optimally only for its own purpose.

On the other hand, the proposed agent moves to appoint the other as well as the other because the player predicts

The other expects the player to avoid collision, and the player tries to respond to it.

As a result of taking this detour path, the sociality of the behavior is increased.

Namely, the travel distance of the player increased compared to that of the standard active inference case.

At the same time, the travel distance of the others decrease and the minimum distance between the player and the other increase.

In addition, the inequality of the difference in travel distance is decreased.

Next, the case where a characteristic is shown on the right two figures.

In this case, the same tendency is observed.

as in the case where the other is selfish.

But the player moved a path with a larger margin to the other compared to the case of the selfish other.

This result indicates that the proposed agent can behave in a social manner that is beneficial to the other.

even if the results are disadvantageous to itself.

An important point is that the difference of the social behavior does not depend on any reward designing as in reinforcement learning.

Namely, the sociality depends on the character of the other around the player.

The other around the player is not just an obstacle for the agent,

but as a teacher for the social manner.

In the case where three others besides the player a similar result in the altruistic as a case.

Namely, the travel distance of the player is increased and that of others are decreased, and the minimum margin is increased and inequality of the travel distance is decreased.

On the other hand, in the case of selfish others, a collision occurs.

It may be due to high density in the center point.

In this case, the proposed agent can avoid collision by weighting the crossing of others.

I repeat the important point.

There is no need to design reward or penalty for sociality in each situation.

A social behavior realized by responding to the other expectation.

Finally, I would like to summarize the explanation of the paper.

As I said at the first slide,

The motivation of this research is to develop a unified mechanism for social behavior.

The answer for the motivation is that running an internal model to predict the future behavior of others.

This is same as standard free energy principle or table inference.

Moreover, predict others' expectations about me with the same internal model, and try to respond to the others' expectations.

This process makes a cyclic process, and it encourages the agents to behave like sounding others.

Because due to the mirror system, others' expectations to me are the same

came to my expectation to others.

This mechanism encouraged the agent to behave like surrounding others.

This summary shows our idea for what is social behavior.

The answer is that social behavior is a behavior like surrounding others.

The others are not obstacles but teachers for social behavior.

This also means that we are teachers of AI or robots for the future society.

That's all.

This is the explanation of the paper.

I'd like to discuss about it.


SPEAKER_03:
Thank you.

Would either of the other authors like to say hello or to give any remarks?


SPEAKER_01:
Hello, I am Kana Paesaki, also a researcher at Hitachi, and I have been studying

control of system, especially system with hardware.

And my recent interest is how to apply the updating health to the robot system.


SPEAKER_02:
Hello, my name is Hiroki Mizuno.

I'm responsible for this research.

Thank you for giving us this wonderful time.


SPEAKER_03:
Thank you.

okay many interesting um questions but definitely very striking results about the tolerance to a variety of different social settings for the active inference algorithm to

by nature of always updating at each time step based upon the expectations, able to kind of deal with a range of different settings, like different numbers of agents and the different behavior types for the agents.

I'll ask some questions that were submitted by-

No?

No?

Or okay now?


SPEAKER_01:
Okay.

Okay.


SPEAKER_03:
All good.

All good.

Okay.

i will read some questions that were submitted by email by shohei wakayama and also we'll look to the live chat so i'll start with the fourth question he wrote i would like to receive more detailed explanations of no operation action the nlp in general because i noticed also in the graphics that when there's going to be a collision that the active inference agent just stops


SPEAKER_00:
so how did you model or calibrate that decision to make a move at all versus which way to move forward okay at first i'd like to answer what is the nope operation and nope i mean nope is just no operation so

In the evaluation case, NOP means wait the current position.

The agent take no movement action.

The agent stays the current position.

This is a NOP operation, no operation.

And the question is very important because

This is a critical problem to realize our method.

The problem is that we want to decide an action, but the procedure we proposed requires to decide action.

As described in this figure, the red circle action, action for others, we decide to action of the

agent but it requires the deciding other action this cyclic requirement is very difficult problem so we have to set some actions for the other sorry i think there is there are solutions three solution strategies the first one is assume random action

abrasing the result of the randomly selected actions.

The second is predict action, running and predict using policy network.

For example, AlphaGo used this strategy.

And the last one we selected, the third strategy is that assume specific actions.

No operations means I don't take

any actions and you act the priority please i mean nope i mean this idea you for assuming nope so i believe that humans have such sociality in nature this is the reason why

I used the third idea.

Does it make sense?


SPEAKER_03:
Yes.

OK, next question.

how do you tune or calibrate the model so that it's in an in an area of state space that's pro-social like maybe you could have a generative model that just only did nope and it was just paralyzed or another one just goes straight forward no matter what so how do you explore that state space

and choose which parameters are open for updating and which parameters are going to be like explored during the design phase and then fixed at runtime oh at one time i don't adjust any parameter at runtime the model is


SPEAKER_00:
trained in an offline manner.

And the parameter model is searched at the training phase.

But the parameter space is not so huge in this case, because the model is very simple.

The observation is just two

dimensional information x and y that means location of the agent so in the case of four agent there is just eight eight dimensional space but we need some time steps so even that case the dimension is not so large

So the hidden state is also not so large.

Can you understand?

Yes.

The evaluation situation is very simple, so the model is also simple.

So I don't search parameter with a lot of .. That is not a tough one.

Simply just try to do some parameters and I pick up the best parameter set.


SPEAKER_03:
So this was an in-silico software only simulation, right?


SPEAKER_00:
Yes, this is just a simulation.

So how did you tune it?


SPEAKER_03:
And then I guess a bigger question and definitely one that a lot of people are interested in in learning active inference is like, how do you go from having a in silico simulation to bringing that on to a physical robot?

Like, what are the steps in the process between having the situation in the simulation and bringing it into like a room?


SPEAKER_00:
Yes, that's a very interesting point and it's very important.

And Kaneko tried to apply the active influence into the real robot.

So how about you, Kaneko?


SPEAKER_01:
I also tried in a small space to apply this after effects to the robot.

I have just an idea that maybe the continuous space is an important key from stimulation to the physical world.


SPEAKER_00:
And in general, the problem is also the problem in reinforcement learning, I think.

So the techniques in the reinforcement learning field also can be applied into the active inference cases.

For example, simulation to real.

I don't remember the name of the technique, but... Sim2Real.


SPEAKER_01:
Sim2Real.


SPEAKER_00:
Maybe such techniques can be applied in the case of active inference.

I don't know there is an active inference specific technique.

I don't know such a technique.


SPEAKER_03:
That's interesting about the continuous state spaces, because in the discrete state space,

where you have the Markov decision process you get the big planning trees but our experience like walking through a crowd or maybe waiting at a intersection it's not necessarily based upon planning it's a lot more on the path and the curvature of the path

that's planned and maybe the paths that others are expected to take as they move through space too so that's possibly like a simpler representation that could give natural movement behavior with a just a few parameters


SPEAKER_00:
Thank you.

And there is another problem for applying this technique to a real world.

That is the computational cost, because we use the Monte Carlo simulation to start the best action.

I think this is not suited to the real situation.

So if we try to apply this technique to the real application,

we have to solve the computational cost problems.

I think so.


SPEAKER_03:
Yeah, another part, even outside of the decision-making, computational costs would be like going from the video camera data.

Yes.

When you're getting the exact location data perfectly, you're in the minimum, you're like kind of playing chess in the state space versus to have noisy sensors, there might need to be a lot of...

uh observation model here a lot of layers before the simple kind of active inference could even happen

Okay, I'll ask another question from Shohei.

So question five, he wrote, in this study, the behaviors of the standard active inference agent and the empathetic agent are compared.

I would like to know why you did not compare the proposed method with other game theoretic approaches, because the metrics like travel distance can be applicable to these methods as well.


SPEAKER_00:
The reason why I compared the proposed method with active inference is that the aim of the paper is to extend the active inference to the socially-behaved manner.

So I think it is natural to compare the proposed method with standard active inference.

But the question is that why we don't compare the proposed method with the games theory method.

I'm not sure what this means, the games theory method.

But if that means reinforcement learning method,

the reason why I don't compare it is that just it needs a lot of time to compare it and I think it is important but the main focus of this paper is to evaluate the

quantitative future of the proposal method and the qualitative evaluation is not so important for us at the moment but i think it is important and i have to compare it in future work so there is no


SPEAKER_03:
real reason this is just uh we don't have in uh much time to compare it thank you makes sense um another question so the movement was being influenced by the relative ratio of the f other and f goal

yes so here f other is kind of like a um electrostatic repulsion from other agents how does it how does it come into play that the red agent is entertaining beliefs about what other agents think


SPEAKER_00:
Sorry, can you repeat the question?


SPEAKER_03:
Yeah, so here the two forces acting on the red agent are the gravity for the goal and the repulsion for the proximity of other agents.

So where or how does it come into play that the agent is actually

thinking what other agents are going to do like the birds flocking simulations that have the simple rules like go in the same direction as the other birds and maintain a distance between them but that doesn't require a theory of mind for the other birds oh


SPEAKER_00:
You mean how the agent knows the course from the others?

You mean how the agent knows the course of goal of others?


SPEAKER_03:
Yeah, it's just running away from others.

Or is it actually imagining what they think?


SPEAKER_00:
That's a good question.

Yes.

In this simulation, the answer is that the destination

destination of the others are given.

I don't remember the details of the evaluation.

I think... Sorry, the agent don't have to know the information.

At first, I think

I give the information to the agent, but that information does not need the agent.


SPEAKER_02:
Yeah.


SPEAKER_01:
So the goals of the others are not explicitly given, but with internal model, the agent can predict the path, the gradient path, so we can predict that goal, that's not explicitly given.


SPEAKER_03:
Does the red agent explicitly model what the others are going to do?

Or it just has heuristics that it follows when they move?


SPEAKER_01:
The former one only predicts the action, the recent action, and the future.


SPEAKER_00:
Yes, the agent just learned the future position of the others.

This is all of the task of the internal model.

So there is no need to give the goal of the others.


SPEAKER_03:
Yes, I think this is very interesting aspect.

Like to what extent the social behavior rests upon simple rules like etiquette, like passing on the right or on the left that don't require knowledge about the internal states of others or even what kind of thing they are.

whereas the experience of sociality and a lot of the psychology of sociology focuses on very cognitive mechanisms that are very expressive thank you this is because the action is repeatedly decided in short


SPEAKER_00:
time duration so this is a future of the proposed method as you said thank you i don't notice that that is important um where are you going to go in your continuing research or development directions

Sorry.

Future workers.

I think an important point is regulating the empathy.

And we have a presentation the last year conference about artificial life.

in the presentations we discussed how regulates the emotion our empathy this means that we the paper we endorse is that the expected free energy is extended

to the including others so d called d my plus the others and the there is there should be weight of empathy that means w in this equation we have to regulate this parameter i don't

consider the parameter in the journal paper but we have to regulate this parameter because there is diverse others there is good person or there's bad person we should not empathy to the parts so this is the important point how emphasize process

and we tackle this problem with the emotion emotion we tackled this problem with the regard to the emotional context okay this is a future work and we publish uh we have a presentation at the


SPEAKER_03:
the current conference so if you have some interesting please read this paper that's very interesting like in a lot of the active inference and social sciences conversations that we've had here there's a lot of discussion about like how do you balance

the the coefficients in the free energy calculations for self and others yes because you're making decisions at a very short time scale like speech and second by second but then sociality is very long term and so you could make a decision that is

aligned locally but then very out of alignment at a longer time scale especially if these other four have complex relationships then action is not very simple yes

Is there any other things that the other authors want to say or anything else you want to talk about?


SPEAKER_02:
I don't have.


SPEAKER_03:
Cool.

Well, thank you very much for joining the discussion.


SPEAKER_00:
Thank you.


SPEAKER_01:
Thank you.


SPEAKER_03:
Yes, good luck with the ongoing work, and I hope to stay in touch and hear more about the research in the future.


SPEAKER_02:
Yes, thank you very much.


SPEAKER_03:
Okay, thank you.

See you later.

Bye.

Bye.
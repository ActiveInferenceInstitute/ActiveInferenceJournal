SPEAKER_01:
hello and welcome it's december 17th 2024 we're here with josephine pizam and colleagues discussing free energy projective simulation there will be a presentation followed by a discussion so thank you to all the authors for joining looking forward to it take it away thank you um hi and thanks for uh having us


SPEAKER_00:
Today, I wanted to present the paper that we have recently made public on the archive.

You can have the number here, which treats the free energy projective simulation, active inference with interpretability.

And our goal there was to combine an existing framework for agency called projective simulation with the free energy principle and active inference, where we have an agent that is able to learn

from its experience in the world gathering precepts and remembering a history of actions to learn a representation of its environment and behave adaptively in its environment by performing active inference to determine its behaviours.

I have structured my presentation in three main points.

First, I want to introduce projective simulation to the community.

Then I want to introduce the formulation and the aspects of active inference that I have used in these works, especially the notation will be important.

And then the main body of the presentation will deal with the free energy projective simulation,

What is it architecture?

How does it work?

How do we train it?

And then some numerical analysis about it before I conclude.

So first, what is projective simulation?

The goal of projective simulation is to model intelligent agents with a framework that is rooted in embodied cognitive science.

So now what I want to do is to look at the keywords of this sentence and understand what we mean

behind those words, starting with the notion of intelligence for an agent, where what we really mean here is that the agents we consider are able to perceive their environment and are able to influence and change their environment by acting on it.

And in doing so, they are settling matters in the environment in a flexible way.

And then the agent,

broad definition for it or um its support would be a system that has a memory as i said i want my agent to be able to remember its experience in the environment and it's also able to interact with this environment and that leads us then to embody cognitive science where instead of having the agent learning from a set of predefined rules that we as designers

would give to the agent such that it learns high-order concepts.

The agent has to learn from the information it has gathered from its interaction history with the environment, that is how it has perceived the environment and the action it has taken on this environment.

And now I have spoken about memory earlier, and I want to make a precision for this intelligence.

The agent should also be able to adapt to situations that it hasn't experienced yet by comparing it to its existing moments in memory, and to be able to compare them, see which elements of its past history are useful to adapt to unprecedented situations.

Now I want to dive into the way we are modeling the agent to implement projective simulation.

The agent is engaged in a preset action loop with the environment in that it has two types of subsystems.

A sensor system is able to perceive presets and stimuli from the environment.

and it's able to act on the environment and change its states through its actuators.

And the key of projective simulation, the place where projective simulation will be important is the memory of the agent that is decoupled from its physical interface with the environment and that should allow the agent to reflect such that it can display some more complex behaviours in the environment going beyond stimuli response behaviours.

The idea here is that the agent is able to simulate possible scenarios using its memories in order to plan or deliberate for its next action.

So this diagram should already be reminiscent of the free energy principle and active inference in general.

Now, projective simulation is a specific process that runs on the memory to deliberate.

For this, the memory is depicted as a network that connects different clips.

If an event were a movie, a clip would be a snippet of that movie, just a few seconds of it, where the agent would recall the sensations it has had, the percepts it has had during this mini movie and the actions it has taken.

Then the agent, the different clips are connected in a directed network so that the different events are connected with one another following the experience of the agent.

Upon receiving a percept from the environment on its sensor system, a clip in the memory of the agent, in the episodic and compositional memory of the agent, you will hear me saying ECM very often.

So a clip is excited by a percept from the sensors.

And then the deliberation of the agent will take place by hopping on those edges to

go from one clip to the next and simulate forward the possible events that can unravel from this excited clip.

So the agent does a random work on that network until it reaches a specific type of clip that will then decouple from the memory in this

yeah, fictitious random walk such that an action comment will be sent to the actuator and the action will actually be implemented in the environment.

So here I want to emphasize two types of states that can happen.

Any state that is, any clip that is visited in the memory is either a remembered clip

a remembered event from the past or something that has been created that is fictitious.

And all the surrounding events, so the percepts and the actions are real and interface with the environment.

There are two specific types of clips that I want to emphasize.

The clips that couple with the percept from the sensors are called percept clips.

And the clips that decouple from the memory towards the actuators are the action clips.

Now, how do we train this memory, and how does the agent adapt and learn something using this memory?

There are two ways that this can happen.

Either the agent is creating new clips in its network, either because it has received a new percept on its sensors, or because it has created new clips from existing clips in its network.

And another mechanism that is more important for the present work is an update rule where each of the edges in the network is weighted with what we call an age value that can increase and decrease as the agent interacts with the environment.

And in particular, at each time step, the updated weight

decreases compared to the previous weight with a certain forgetting rate.

And it's brought back to its initial value with the same rate.

And every time an edge was traversed to make a deliberation that received a reward, it is reinforced with a certain reward.

And from those edge value, then we can define a probability distribution that the agent will use to sample edges during its random walk to walk from one clip to the next and deliberate.

This is it for the mechanics of projective simulation and its conceptual meaning.

There were a few applications already in the literature.

For example, it has been applied in robotics and reinforcement learning where a group in Innsbruck had a robot learn new skills from existing skills by preparing new

configuration of the environment that would enable the agent to extend its previous skills.

It has also been used to simulate biological agents, especially the behaviors of honey bees when they are trying to defend themselves.

So it's a multi-agent scenario where each of the individual agents are coordinating their decisions to be efficient.

And there have been a number of evolutions that have been proposed on this standard projective simulation.

Multi-excitation projective simulation proposes that the clips are truly compositional and an edge is not only between two clips, but rather between groups of clips.

That would represent the current situation.

And there have been also quantization of projective simulation in this seminal paper, but also this has already been implemented experimentally on a small problem.

So this brings me now to the second part of this presentation, where I want to

give an overview of the free energy principle and active inference so that we are all agreeing on what we mean by our notations and active inference here.

So the claim of the free energy principle is that any adaptive system can be modeled as performing an approximate form of Bayesian inference.

So let's do as we did for projective simulation and look at keywords here.

What do we mean by an adaptive system?

It means that we have a system that is able to persist in its environment and to do so, it has to exhibit some form of flexible behavior and it has to be able to act on its environment so that it can change it if needed.

And the main keyword is Bayesian inference here, which

means that we can describe an agent as building an internal model, that it is using data from this environment in order to update this model, and that the update takes the form of an optimization.

And another keyword I want to emphasize here is that it's modeled as performing Bayesian inference.

That is, it looks as if, so here we want to distinguish between the internal mechanics of an agent, learning from stimuli it receives from the environment and its actions, and the mathematical description that we can give of that agent as updating a model in a Bayesian way.

and hopefully these two would be reconciled at some point in the presentation.

First, to introduce active inference, before we introduce active inference, I would like to introduce perceptual inference, which is describing the process in which the agent is constructing its representation of the environment using the observations it receives from it.

The environment can be described as having hidden states and the agent acting on these hidden states will result in a change in that hidden state.

However, the environment is not fully accessible to the agent, it's only partially observable, so the agent will not see these hidden states, but instead only a small part of them will show the observations.

And in order to understand the dynamics on those observations and being able to predict those observations,

The agent will construct a world model in the form of a partially observable Markov decision problem, a PONDP, using what we call belief states, where here I use capital letters for the random variable and lowercase letters for the value that those random variables can take.

And the idea of those belief states here is that they mirror the role of the hidden states such that the agent can use them then to make prediction about its observations.

So similar to, I'm sorry.

Similar to the actions in the environment, taking an action will also result in a transition in the belief states, and those belief states have to be connected to the observations.

The transition between belief states is usually qualified as a transition function and the association of belief states with observations as the emission function.

And those two functions can be modeled as probability distributions.

Here, the probability of the next belief state given the current belief state in its action.

And the likelihood is the probability of an observation given the current belief state.

which means that then we obtain a world model that factorizes nicely into two functions, the likelihood on the one hand and the transition function on the other hand.

Now, we have said that the agent learns by performing Bayesian inference, which means that we need

to bring another component to the table for the agent to update its model, and that is distinguishing between a prior and a posterior.

The prior is the previous state of knowledge of the agent, what it believed was true for the transition function before updating its model.

And the posterior is a distribution function that is part of a variational family,

and that can be parameterized with here a parameter phi.

And this phi will then be tuned such that we land on a distribution here that enables the agent to make better predictions in the future using its current observation.

And the way this parameter is tuned is by minimizing a quantity called the variation of free energy that then gives its name to the free energy principle.

The free energy, or variation of energy to be precise, has the following form, where I want to first look at the second term.

This term here is called the surprise, and that's the surprise of the agent getting observation as the environment here, given it was currently in the belief state PT.

and we are averaging this over the variational, over the posterior, such that this term becomes minimal when the current belief state is able to predict the current observation.

This also means that the best way to minimize this term is to forget everything we knew in the past and to only update the variational

distribution here, such that it ticks only at the belief state that is able to predict this observation.

And to prevent that, we add an inertia term to the variation of free energy that is zero only when those two distributions remain the same.

And that is that having a new observation does not do anything to the model.

So this

motivates the agent to remember its previous model and to make a sort of compromise between updating its model and generalizing also to its past observations.

okay so in this whole scheme so far the actions were externally given i didn't say anything about how the actions are actually um chosen in this whole model and how they impact the ability of the agent to predict its observations and to um

include actions in this framework, Active Inference gives us a recipe to plan for actions.

Namely, the agent will try to choose actions that are expected to minimize the surprise about future observations in the long run.

To do that, the agent is endowed with a form of intrinsic motivation that guides it towards states of the environment that are beneficial for it.

For example, that are helpful in persisting in an environment or in doing homeostasis, for example.

The way this is modeled in this whole framework is by

giving another distribution to the agent that we call the preference distribution.

That is a probability distribution over the future belief states and the future observations.

And as for a perceptual inference, now the optimal actions will be those that minimize a quantity derived from the free energy, which is the expected free energy.

So a sort of estimation of the surprise about future observations in the future.

The form that this takes is often qualified as a trade-off between an exploration and exploitation, where this first term, just let me remind you that we are trying to minimize this expected free energy.

So here we are trying to make this term as negative as possible.

And what this is, is the entropy of the belief states given the current world model.

So the agent will select an action that leads to the most uncertain transition in the world model, such that this uncertainty is resolved by taking the action.

So this is an exploration term.

It fosters discovery in the environment.

It wants to resolve uncertainty.

Whereas the second term is derived from the surprise calculated from the preference distribution that I introduced earlier.

And it is a utility term because this term becomes minimal when the surprise over the preference distribution is minimal.

That is when the chances that we do not fulfill our preferences are as low as possible.

And now from this expected free energy,

There are multiple ways that one can choose and plan for actions.

A very popular one is to do a tree search or using Monte Carlo methods.

And the method we want to use here is to define a policy from this expected free energy by feeding it into a softmax function, for example.

And having a scaling parameter in front of it that could be negative, such that we ensure that actions with the lowest expected free energy are more probable.

Okay.

So this is the end for the little summary for active inference.

And now I want to move on to the model we are proposing, the free energy projective simulation.

Some key features of our model is that inheriting from active inference, the agent is endowed with internal motivations and also internal rewards in the form of prediction accuracy that we model using a concept we call confidence, which will become clear shortly.

It's also, it counts among the implementations of model-based reinforcement learning using projective simulation.

And it becomes a hybrid model between a form of associative learning between different percepts conditioned on actions.

that will benefit from the reinforcements and the rewards the agent got from its predictions.

And the behavior of the agent is completely independent of any reinforcement and is fully determined with active inference.

And one of the main benefits also of using projective simulation here is that the model the agent learns and the update rule it learns with

or interpretable as a legacy from projective simulation here.

Okay, so let's start with the architecture of the agent where we are trying to gather all the ingredients necessary for the agent to learn a world model and perform Bayesian inference.

So what we propose here is

for the belief states to be encoded in the memory of the agent because we want them to be decoupled from the environment just as those belief states in the free energy principle.

And in order to avoid having to learn the likelihood at the same time as learning the transition function here, we give this memory-specific graph structure inherited from the clone-structured hidden Markov models that have been first proposed in this paper here and successfully implemented in this paper here to build cognitive maps and in another following paper, I think in 2023.

um for navigation tasks in this one um the way this works is that it postulates that an observation is actually only sort of the tip of the iceberg of um the full random variable the agent would need to be able to understand completely a model completely its environment

and therefore it gives each observation a clone that, as we train, will acquire more meaning, and it gives this likelihood a deterministic form, such that each clone is linked to a single observation at a time, that is, the probability is either 1 or 0 in this case.

To each observation, we give a fixed number of clones and the edges that are required to deliberate in the memory.

will be split into two sets.

One, I mean here two sets or n sets, one for each action that the agent can choose, such that we can learn this prior and posterior.

In particular here, the parameters that we can update are the weights associated to each edge in this ECM.

Now, we call cloneClip any clip in the ECM, and we are going to call the belief state an excited cloneClip.

That is where the agent is currently sitting in its deliberation.

Once the agent has excited a cloneClip in its ECM, this will define the belief state the agent has to deliberate over in order to

uh sample its action and what we do here is um each belief state is connected to all the actions and each edge is weighted with the corresponding expected free energy and then these um

These expected free energies will define a probability distribution that the agent was sampled from in order to determine which action to implement in the environment and to make predictions using its memory.

And the prediction will be the observation that is linked to the current belief state the agent believes it is in.

Now I want to introduce the concept of confidence in this model that is crucial in order to update the one model of the agent.

We call confidence the... We use confidence to track the number of correct predictions sampling an edge enabled in the future.

That is, if I sample this edge to start with,

At the end of a trajectory, it would count how many correct predictions it enabled in a trajectory.

And what I call here a trajectory is a sequence of consecutive correct predictions for the observations.

Now I want to move on in the next slide to show you how it works exactly going step by step in an interaction loop with the environment.

So the environment starts in a certain hidden state and emits an observation that the agent is able to receive through its sensors.

Then following this, one of the clones corresponding to that observation will be excited in the ECM, here Bt-4, and the agent will sample an action from this state

here A2 and predict the next belief state it believes will be useful in order to make a prediction about its observation in the future.

Its prediction is then S2 because that's the observation that is linked to it.

Simultaneously, this action is implemented into the environment and a transition occurs in the environment that then emits a new observation that is again received from the environment.

And in this case, it confirms the prediction made by the environment and therefore the confidence of the edge that was just traversed is set to one because it enabled a single prediction in the future for now.

Now, every time the agent samples a new action and implements it into the environment, the observations are compared with the predictions until the agent makes a false, a wrong prediction.

So here the agent predicted S2 instead of S4, and that's where the trajectory ends.

And the network, the weights in the ECM,

will be updated following the ps update rule so here you will recall

Remember the standard update rule where we decrease what we believed before, going back to the initial state of the distribution.

And here the reinforcement happens proportionally to the confidence of an edge.

That is, this edge will receive three times the reward scale because it enabled three correct predictions in the future.

Now, I want to explain how we define the policy and how we define the preferences especially.

We have two propositions.

The first one is trying to understand what happens if we give the agent the objective of actually trying to minimize its prediction error directly embedded in its preference distribution.

So to do this, we modeled the preference distribution to be the marginal of the world model here over its actions.

And after a little bit fitting this into the expected free energy and a little bit of reorganization of the terms, what one finds is that the expected free energy borrows down to be the information gain about the next belief state,

from taking action A in the environment.

Now, you will recognize very quickly that this is problematic because we are trying to minimize the information about the next belief state.

And this is a well-known problem, which is called the darkroom problem, where the agent

post it in a in a dark room and without any incentive to go outside of it that is a preference for a state that is not in the dark room would stay there because it's fully predictable however there is an easy way out using our model where instead of minimizing here the information gain what we can do is maximizing it by simply

making the scaling parameter here positive.

And we will call an agent that uses this preference distribution a wandering agent.

And then the magnitude of the zeta will tell how greedy an agent is for information gain.

It will

A bit like in epsilon greedy algorithms, it will define with which probability the action is actually targeting regions of information gain or of uncertainty to improve the work model.

The second option is more directed towards encouraging the agent towards completing a task in that environment.

And to do this, we first took a modeling assumption that if we are to give a task that is externally given to the agent, it can only be modeled at the interface between the agent and the environment that is on its observations or its actions.

And here we decided to embed

the targets in the environment into the observations only and to call this part of the preference distribution the absolute preferences.

And the task of the agent then is to be able to use its world model to propagate these preferences onto the belief state such that any state that can be useful in reaching the target in the observations

get a higher preference overall.

To do this, we define two quantities.

First, we ask how useful is a belief state in order how easy

is it from the current belief state to reach the next belief state?

So sort of cost of reaching an action from the current belief state.

And the second quantity that is useful is how valuable is the current belief state to achieve the task in the environment.

Looking at the first quantity, we define the reachability.

as the transition function marginalized over action.

So for example, imagine you are in this state B2 here.

Here you have two actions going to B2 for the darker observation compared to one with the same probability for those two states.

So the reachability of state B2 dark orange will be higher than B3 dark orange, for example.

A second quantity is the value of a belief state, which is connected to the value of the preference over the observation it is connected to.

We initialize the value with exactly that, actually, the absolute preference of the state it is connected to because we are using this clone structure.

So this means that if S4 is our target, then the preference for those three states, the value for those three states will be the highest, and for all the other states, it will be the lowest.

Now what we want is to propagate this preference to the other states in the ECM by using an iterative algorithm over a number of steps that will define a prediction horizon for our agent.

And either the belief state keeps the same value as before, so those states will never have their value decreasing as the algorithm progresses, or the belief states inherit from the value of the belief states they can transition to.

as defined with this term here with the reachability and the value.

So, for example, those two states here being able to transition to these here will inherit their value.

And now to define the look-ahead preference distribution, we combine those two quantities, define children of a belief state as the states that can be reached from the current belief states, and combine this to define this look-ahead preferences.

And from there, we are going to fit this into the expected free energy and define a new policy that hopefully guides the agent towards the target in the environment.

Okay.

Yes.

So far, we have tackled only two problems that

could get in the way of an agent performing a task optimally.

We have solved the first problem of trying to learn the best world model possible to make good predictions about observations.

The second one is being able to plan actions efficiently for the future using the world model.

And another problem that arises when we use FEPS is the estimation of belief states.

and for this we propose to estimate them in superposition what this means is that every time at the beginning of an interaction with the environment the agent will receive an observation but

won't be able to distinguish between the clones to start with, so it will build a hypothesis that contains all three clones here, and connected to that observation, and each of those clones will be a candidate belief state.

From there, the agent will use a rule to select its actions, stimulate its transition for each of the candidate belief states, and propose a candidate future belief state.

And while the action is implemented in the environment, the agent will receive an observation from the environment.

And from there, the hypothesis will be

reduced by eliminating any belief state that is not compatible with the observation the agent received from the environment.

And continuing this cycle of hypothesis predictions and confirmation from the environment, the agent will slowly reduce the size of its hypothesis to a single belief state that the agent is not certain it is in.

Okay.

This then solves the three main obstacles we solve for an agent to perform optimally in an environment.

And now I want to show you how this actually works in practice and in the code in some environment that we use for demonstration purposes.

The task we tried to simulate was a navigation task in a partially observable grid.

This grid, imagine, has food in its upper right corner that

let's say, has some smell that propagates through the grid.

So the closer the agent is to the food and the more intense the smell is on a scale from zero to three, the task of the agent would be to then reach the food in there.

But compared to other application, for example, GridWorld,

the agent only has access to the smell intensity to orient itself in the grid and determine its behavior and never has access to the coordinates in the grid, for example.

And what I want to show you first is that even though the agent does not see the coordinates, it's able to learn only from

its percepts and it's what I call embodied experience that is receiving percepts from the environment and using its actions to predict

What I show here is a figure of merits for the status of learning for the agents.

And I compared the two types of preference distribution that I introduced earlier, the task-oriented agent first with the look-ahead preferences and the wandering agent here in reddish colors.

And we see that it seems that at the end of the training, given a fixed number of episodes, it seems that those two models converge to the same length for their trajectories.

That is just to remind you the number of consecutive transitions that led to correct predictions in the environment.

It seems that at the end of the training, the optimal agents with either preference distribution perform the same, with a slight advantage, it seems, for task-oriented agents that seem to converge faster towards this optimal status.

And here, there are two maybe interesting features too.

We set the scaling parameter to a value that is too positive, that is, we make the agent very greedy for information.

The agent does not learn as optimally as the other agent, and in fact, it learns worse than an agent that would just be given a uniform policy, so would act completely at random.

And this happens, my explanation for it is that the agent then trying to seek transitions that are the most uncertain will never revisit the same, and therefore this won't be enough in our scheme to reinforce the edges such that they are no longer uncertain and that they actually stick.

So it would be a subtle balance between forgetting and reinforcing that is difficult to implement when the agent becomes too greedy.

and the second scenario is here one that we already know when we set the scaling parameter to plus three uh for a task oriented agent we are now maximizing the expected energy and as we would have expected the agent stays um does not learn well in these cases because it doesn't target uncertainty and it's not even trying to fulfill preferences in the environment

Now, a second point I want to make is insisting on this look as if in the claim we made for the free energy principle.

What we have trained the agent on was only using the confidence to reinforce edges and never did we do anything with the variation of free energy.

However, it seems that if we track the variation of free energy, it is actually a good figure of merit to describe the agent and its learning.

And as you see here, these are average agents and their variation of free energy.

And you see that as before, wandering agent and task-oriented agent converge to a very small value.

at the end of the training that the tasks for the agent go a bit faster.

And here, an interesting feature, if you look at the individual trajectories here of the agents, you will see little steps in the learning curves, which if you actually open the black box and look at what happens at these different stages of learning for the agents, it's actually corresponding to the agent

adopting conventions in its clones to properly contextualize its clone clips in the grid here to describe its observations and its geographical location in the grid.

The third claim I want to make is trying to understand this difference between these task-oriented agents and the wandering agents.

Even though it seems that when the agent is able to choose its actions by minimizing the expected free energy, that the task-oriented agents learn better than the wandering agents, it seems that when we compare their performance and set their policy to a uniform one, that is, the agent no longer are able to seek transitions of maximum certainty,

It seems that wandering agent equipped with a bear estimated estimation mechanism are able to perform in general, a bit better or more uniformly better than the task oriented agents.

and that if we use this belief state estimation in superposition with the candidate belief states in a hypothesis that we are progressively eliminating, that for both types of training, this will double the length of the trajectories for the agent.

So it seems to be rather efficient, at least in this environment.

Now, I want to emphasize this slide here, which relates to the interpretability of the model the agent is learning.

And what you see here is a trained model for an agent that we train in the grid here, and the transition functions for all the actions the agent can perform in the grid.

And if you look closely and try to understand how those transitions happen, you realize that actually each of the clones in the one model that was initially only associated, the only information they had initially was the smell intensity that the agent could perceive in the grid.

you find that those took on some contextual meaning on top of it that differentiates them and that maps them to a unique location on the grid.

That is, in the end, they are able to represent a geographical location.

So in this sense, it's fully interpretable.

The final model is fully interpretable.

And one final claim here is that the policy that the agent learns as a result of this look ahead preferences that I described earlier is optimal.

And what you can see here is we've trained the agent to either reach the target here, that is reach the food, or escape the food here by reaching the lower left corner instead.

And so this has happened without intermediate training.

We just changed the absolute preferences.

So this is a very flexible behavior.

And we've compared the trained agent to a random agent, and we see that it's very clear, nearly optimal behavior.

And the restrictions on this optimality is not due to the policy itself that is optimal, but rather to the belief state estimation mechanism that we are using.

that would require one additional step to be able to fully identify the single really steady agent is, and therefore the agent takes this additional action to understand where it is in the grid exactly.

So that's the slight overhead you might find in these plots.

But otherwise, fully flexible and optimal.

And we expect that in a larger environment with a similar structure, we would become optimal.

Okay, and this brings me to the conclusion for this presentation, where we have presented a model for agency that combines projective simulation together with a free energy principle framework, where we have introduced the concept of confidence in order to reinforce edges on the memory that corresponds to associative learning between percepts

using these reinforcements, and that allows the agent to plan for its actions flexibly and to discover optimal policies using these look-ahead preferences.

And I want to emphasise again, the agent and its model are interpretable, both because we can trace back the deliberation path

of the agent on its memory, and also because at the end product, the world model at the end of the training is interpretable.

We can map it onto hidden states of the environment so far, the way we would understand the grid.

And I think a nice follow-up work or nice connections we see with other fields is that this model comes with aspects that are plausible from a cognitive science perspective.

Namely, well, active inference has been already connected in order to endorse predictive coding and the Bayesian brain hypothesis.

The clone-structured hidden Markov models have been proposed as a way to model cognitive maps to determine behaviors in memory, and in particular here, especially for navigation tasks.

It has been proposed that the clone clips

can play a similar role as the play cells in the hippocampus.

And also the look-ahead preference scheme that we propose here is reminiscent of successor representation that a few years ago was also associated with the role of dopamine in kind of, how should I say, combining model-free

reasoning with model based behavior such that the an agent or a biological agent can have optimal behavior and flexible behavior in a changing environment.

Yes.

And with this, I thank you for your attention.

And again, for having us here.

And feel free to ask any question that comes to mind.


SPEAKER_01:
Awesome.

Thank you.

Would any of the

authors like to just give any reflection or or share a little bit about the paper while people in the chat are writing questions or i can read some questions from the live chat okay first um is the code available


SPEAKER_00:
Not yet, but it will be available soon.

Yes, I'm working on it.


SPEAKER_01:
Okay.

From Leo Delion wrote, great work.

Would it be possible to make this using continuous inputs and outputs?


SPEAKER_00:
We are also working on it, actually.

So hopefully, yes, it might be a bit harder.

But hopefully, yes, in the future.


SPEAKER_01:
Which would be harder?


SPEAKER_00:
It takes a little change of perspective on how we train the weight.

And we're currently trained to model exactly that actually.

So yes.


SPEAKER_01:
Cool.

Okay, reconfigurability trainer wrote, How would task orientation limit outcome expectations?

maybe a little bit more more generally there how do how do you configure task this is a navigation task but but how would for other kinds of tasks cognitive task performance how would you bring that into the structure of the model


SPEAKER_00:
Okay, so at the moment, one of the limitations that we have with the way we modeled everything is that we can only encode the task as an observation, a target observation in an environment.

So it doesn't have to be, imagine the agent is able to perceive, I am hungry, I am full.

It could also be observation, I am full.

So it's not necessarily bound to a navigation task.

go to any, it can transfer to many other types of tasks.

Now, for sure, there is a limitation and it was already visible in the grid world that I couldn't tell the agent go to the lower left corner directly, I could only reach one of the three cells that had observation zero.

And what I have in mind or what could be possible is to give the agent a few rounds of interaction with the environment and combine this look-ahead reference theme with some reinforcement or some rewards that the agent would get from the environment.

And then this way, the agent could be able to identify which belief state is actually corresponding, which unique belief state is corresponding to the successful state in the environment on top of the observation that corresponds to it.


SPEAKER_01:
Cool.

Just on the

projective side, how do you parameterize the topology and the bumping and the splitting of these clones clips, since it seems like having an appropriate granularity and transitions amongst these clips is sort of at the heart of how effective the method would be?


SPEAKER_00:
Yes.

So what I'm doing at the moment,

is helping the model.

I'm giving as many clones as necessary to model the hidden states in the environment.

That is, if I have, for example, it's coming, yeah.

Here you see that the maximum number of hidden states that would send the same observation is three, so I give three clones to the model for each observation.

The way I initialize the ECM

is at the beginning of the training, the ECM is fully connected.

So I'm not saying anything about the structure of the environment in the ECM to start with, except in the number of clones that I'm giving.

And then we have been discussing also techniques that could have the agent have an adaptive way of defining the number of clips.

for a number of clones for each observation where, for example, if the agent realizes it needs more clones, we could add some and then add a specific rule to connect them to the following one.

And also what is also very nice is if we give too many to start with, and then we are able to reduce to the minimal amount of clones that would be required to represent each observation, we could actually learn something called a minimal representation of the environment.

And that is something that is very valuable from a theoretical perspective as well.


SPEAKER_01:
Yep.

Do those clips directly provide the interpretability or what kinds of statements in a grid world or just more generically, what kinds of interpretability expressions can you derive here that using some other kind of knowledge graph or graphical cognitive model couldn't yield?


SPEAKER_00:
Okay, so I can only talk about the things that I know.

So here, what I want to say is that in general, clips in projective simulation are understood as a part of the experience of the agent.

So by construction at the beginning, they have semantics associated to them.

And here, the way we followed this is by saying at the beginning, any clip is something in the environment that emits the same observation.

And in that sense, it has some interpretability.

Then this mapping that we can do at the end of the training allows us to map the clones

onto actual hidden states in the environment and in that sense because we can kind of open the world model and take it apart and still understand what's happening in that sense it's interpretable and also one way that this model is interpretable and compared to others is that at any time the agent is able to deliberate we can trace the deliberation path that the agent is taking in its memory so we know exactly

why an agent is taking that decision and not another one and that's also how we reinforce the model so we understand how it went there how it came to learn this model in the first place do any other authors want to add any comments or ideas


SPEAKER_03:
Maybe I can just add or try to respond to your question in what sense, where is the interpretability?

I mean, these clips that Josephine was introducing and talking about,

These are internal representations of actions the agent did and perceptions it had.

So the semantics derives from the description of its environment, how it can act on an environment, what it can perceive from the environment.

This is the basic semantics that it is and it inherits the semantics onto this CLIP network in its ECM.

And so if what Josephine said, if you want to understand why did you do this action giving this percept, you can trace the sequence of hoppings in these elementary clips, which are stored percepts and actions it has done in the past.

So there is a basic interpretability there, how it came about to take a certain action, what considerations it went through as it were.

Now, the clip.

network can grow and change over time dynamically, then it gets more complicated.

When there are new clips are created, how do you transfer the semantics to them?

As long as you can trace how the clip creation processes took place, you can connect

the semantics of the new clips to the previous clips where it was made from but then it gets more complicated but the statement is that the basic ps comes with a basic structure of interpretability that is inherited from the task environment which is often an md mark of decision process thank you on marius


SPEAKER_04:
Perhaps to that I might add a few things.

So first of all, I think there's some additional interpretability in the way the updates are actually done.

For example, this confidence mechanism and the update rule.

Perhaps the more obvious machine learning approach to take here would be to use the variation of free energy

as a loss to do a gradient descent on but this can become quite messy because there are all these different terms competing with each other and it's not really so clear what one actually gets from that

and we now on the other hand have this update pool that very specifically says that the probabilities are reinforced exactly dependent on the number of white predictions that they enabled during the training and right now we are taking here and

like in parts it's a tabular approach one could say and there is some strength on it because in principle one can write down everything as a matrix and really inspect it what's the best transition to take what is the best action to take and what is the best state to predict and you know there are some papers in the variation of free energy literature and active influence literature that directly throw artificial neural networks at everything

And as long as one wants performance, it's probably the best thing to do.

But of course, they come with all the interpretability disadvantages.

So currently, we are looking on purpose on environments and agents that are complex enough to be interesting and difficult, but small enough to enable a manual inspection.

In the context of this project, the idea is really to see how does the agent evolve?

How does it learn?

How do all our principles that we have, variation of the energy and PS, actually manifest itself?

What do they do in the agent?

And for the clone state, something that one... I think there are some important intuitions that one can give on those.

um one can for example see the clone index as providing a context but one can also think of it as dividing the decrease of freedom into the observable ones these go into the actual observation and the unobservable ones which go into the clone index

Yes, so like here, we actually see in this figure here how the clone indices of the agent directly relate to the actual true world model or the true states of the environment.

So we can literally say, OK, this clone state is this environment state.

And so on.

And we can directly investigate how the world model of the agent actually relates to the true environment model.

And if you just used some artificial neural network, you could not really draw a map like this or a graph like this.

um if you don't mind i would also have some additional comments on questions that were raised earlier yeah go for it um so there was the question about the applicability to um to continuous domains

And one thing that's typically done in neural symbolic AI approaches, for example, is to only use the tabular methods essentially as one part of the workflow.

For example, something that one sees quite commonly in the neural symbolic AI literature is that one has artificial neural networks that act as feature extractors.

and then the most symbolic methods get applied to the outputs of those feature extractors which is something that could be also applied here but as josephine already mentioned we are working on a generalization to the continuous domain that's much more faithful to the methods that were presented here so

Yeah, so I just want to emphasize that it is possible to generalize this to the continuous domain and we are actively working on it.

And we have a very specific vision in how we want to make it happen.


SPEAKER_01:
So just to clarify this image here.

the location of the grid like the number of each cell or the x or y location is not being revealed to the agent but they are being able to perceive this ranked score like pheromone intensity or like temperature or something like that and they're learning through their embodied activity the transitions amongst that categorical variable

And it's sort of recapitulating the actual spatial distribution, even though they aren't coming to an explicit spatial reconstruction.


SPEAKER_00:
Exactly.

Yes.

Yes.

That's a good summary of it.


SPEAKER_01:
Okay.

And so then in the fullest case, there could be a clip memorizing every single location.

And then in the

and then a minimal representation would start to cluster coarse grain repetitive transitions between like maybe if there was a vertical border some of those could be coarse grains or just sort of simplified because the same action would result in the same change


SPEAKER_00:
Yes, that's quite correct, actually.

If you look at some observations, actually only have two hidden states, so we would only need two clones to represent those.

And what happens is that since each clone is relating exactly to a single location in the grid,

Um, the two clones are representing exactly the same location and we could design a rule such that those two clones collapse and here those three clones collapse.

And then we would have exactly, uh, the minimum number of clones we would have in the, in the ECM would correspond exactly to the number of hidden states we need to model in the environment to be able to predict observations.


SPEAKER_01:
Hmm.

Okay, I'll read another question from the live chat.

And then if any other authors have thoughts or anything, okay, Leo wrote, how long did it take for it to train to solve the grid task?

And how scalable is this network with the amount of beliefs and actions?


SPEAKER_00:
I think it took a few hours for the navigation tasks.

maybe at the order of four hours, I think.

So it's a trade-off at the moment between typically interpretability, being able to understand everything that's happening in the model, and the scale of the models we are tackling.

So this is work in progress at the moment, being able to scale up this approach.

Maybe we can consider that

As Marius was saying, this feature extractor could be helpful in scaling up to larger environments.

This idea of adaptive number of clones is also interesting for this.

And yes, maybe using a hierarchical scheme could also be useful.

We don't know yet, but this is something that we are actually thinking about.

that we would like to try to handle, but we don't have a solution for it yet.


SPEAKER_01:
What kinds of settings have these graphs, absent the free energy principle angle, been applied towards?

And would those be the kinds of settings that then you'd adapt this method towards or do you see this as tackling a different set of functions?


SPEAKER_00:
When you say this kind of graphs, do you mean the clone-structured graphs?

Yeah.

Okay.

So to my knowledge, this has been applied to navigation tasks.

There was another paper.

I don't remember exactly what kind of tasks.

That was the more recent paper where they learned schemes using this clone-structured graph.

I don't remember which kind of tasks they used it on, but I do expect that they transfer to most of the tasks.

What is interesting is to have some ambiguity in the environment, some partially observable environment.

I think that's where they're interesting.

Yeah, I am not fully certain about the answer probably to this.

Maybe a degree of macrobianity also might be necessary since we are only modeling hopping from one step to the next at the moment.

This is something to look into.


SPEAKER_01:
Yeah, it's an interesting

challenge with wanting a situation that's simple enough to understand but not totally baked in so that it's obvious and these kinds of almost meta modeling considerations come up all the time like if the agent doesn't understand the efficacy of their actions what are they even doing inference on but if they entirely know the efficacy of actions then it's solve it then it's kind of like

all loaded up for them to know what to do.

And then how many layers or what ways do you pull back?

Oh, well, it will learn the efficacy of the action, but that might also not be realistic.

And so just figuring out what expressivity of the modeling situation

Do we even want to set up because setting up what cognitive functions for the agent to perform is kind of like just the compliment of the tasks that it was not set up to perform and sometimes that can be a bigger components.

So i'm just curious, I guess, as we sort of come to the end like your broader lab and group.

What kinds of questions and applications are you seeking to bring these methods towards?

And how do you see this work on the path of your broader directions?


SPEAKER_00:
Okay, so for myself, there are two directions that I'm currently interested in and I'm still at the beginning of my career, I would say, so it's up to evolutions.

I am interested in the ability of those models to coincide with actual cognitive processes that happen in biological agents to be able to model that to maybe provide a tool that can help us understand this kind of deliberation and then to see how we can

bring this understanding onto artificial intelligence models and machine learning models in general this is the first aspect i'm interested in the second one would be um how do we discover or from these very basic experience the agent has with the world how do we discover how does such a an agent

build abstractions and use them in ways that are efficient in coping with the environment and performing, yeah, fulfilling some preferences in the environment.

I think that's so far the two things I am thinking about.

I think that projective simulation has been used on many problems as a reinforcement learning methods

You know, it has been benchmarked on those traditional problems for reinforcement learning, the mountain car problem, grid world also, it works very well.

Also on some, yes, I guess modeling is usually the main application.

Does anybody have other application in mind?


SPEAKER_02:
Yeah, I can jump in.

Oh, Hans, do you want to go ahead?

I can jump in a little bit because I think one of the things that really interests me and I know some of the other folks in this group is modeling in particular animal behavior and foraging.

And so one of the big questions that I think of driving some of this work is how animals forage for resources that are sparsely but predictably concentrated in space and time

And in particular, when those resources are depletable and dynamic.

And so this poses a problem where one scheme is not sufficient for the animal to come up with forging decisions.

So in this framework, we might see this graph that we have up on the screen as

as one scheme that allows an agent to solve a task.

And so this work has helped us think about how the free energy principle can be used to help learn this scheme and a reinforcement learning paradigm through this process of reinforcing derivative pathways.

But then what the projective simulation model can do for us is start to help us think about how an agent can build up different schemes

from a set of episodic memories.

And so I think that this work is kind of serving as a foundation to help us move in that direction of how we can use projective simulation to get to a set of schemes and think about a deliberative pathway that's helping agents build simple schemes of their current world from a more complex and rich set of memories that give them a full scheme of the world.

And hopefully this can help us to start modeling

some of the complex foraging tasks that animals face that may be kind of the filter system for natural selection that has acted on the evolution of intelligence in humans and in other animals.


SPEAKER_01:
Cool.

Thank you.

To bring that back to like, what is the task that the agent is set up to solve or not?

Like in grid world forging examples, often the location is given to the agent and then the agent is doing like this navigation as if it was doing GPS and that is set up as the challenges and trade-offs are derived from that setup.

Whereas using this projective interpretability heuristic scheme configuration, it could just be the learnt relationships between like shadow and light or

other kinds of sensory cues that might be pretty coarse grains and so it's not to say that there isn't an x and a y location it's just that when that's what's given to the agent or the the id of each cell number is given to the agent then it's kind of like way going down this road that animal foraging may not even resemble and so this can i think helpfully bring it to a space of

having the different experiences or clips like the honeybees going on these orienting flights and they go out and they come back before they start to go for

foraging trips and bring back food.

And so those kinds of heuristics and then what curriculum would support those transitions.

And then each edge can also be seen as a node connecting to other nodes.

So what should the structure of that graph be?

And those are some very flexible ways to talk about context switching and all of that.


SPEAKER_02:
Yeah, I think you're on the nose there.


SPEAKER_04:
any other authors want to add any thoughts or what will they work on next year um i mean hans wanted to say something um because um like of course uh so maybe

like the answers given so far don't really cover everything that we are doing in the group yet because most of us actually have a background in quantum computation so we are physicists who have mostly worked on quantum computation at some of the points of our careers and

I don't know, Hans, do you want to give a comprehensive summary of why we care about explainable AI and world model building, or should I?


SPEAKER_03:
No, maybe I can just say something.

I think Alexander gave a very nice outlook and also embedding of all this paper and this research into the foraging and behavioral biology projects that we are following.

And I don't want to add

much more to it i see the still larger scope of this is that for several years we have now been really trying to understand agency and how to to model agency um

it really in a in a in a physical in a physical model so a notion of agency that is you know accounts for certain philosophical aspects as you know planning having an intention and and being situated in environment trying to get around being adaptive flexible things like there's a non-computational understanding of agency this is what we want to model and this is another step forward

It connects to some branch that we are also following in the group, and this is what Marius hinted at, and that has to do with the modeling of artificial agency.

To what extent can one understand or even build artifacts that can also learn but act in a meaningful way on an environment, have a certain notion of memory

that helps them to build model and understand their environment.

And that's of course something in physics and in the ongoing development of AI and its use in science that is quite relevant.

So what we want to understand is

or to develop a perspective on ai that one could call artificial agency now in the future we envision there will be devices that can themselves learn and interact with an environment that do you know that perform experiments people are now speaking of self-driving laboratories and stuff like that

And that is all within a certain paradigm and framework that's being developed at the moment.

But I think what we need to understand is really to which extent can one call these or future artifacts agents to get really a relation to what they do and compared to what we do and how we are.

And that's the broader scope.

And of course, to understand agency, we should also be able to model biological agency, to have a clear, to put what we would call artificial agency into context to what exists already, that is biological agency.

These are different directions and dimensions of the research we are following, but that's, I would say, the bigger picture.


SPEAKER_01:
Thank you for adding that.

Anyone else want to add any comments?

last piece just makes me think about observing the animal in the grid world and that's but you don't see inside with the construction of artificial or synthetic agencies then it opens up kinds of interpretability that aren't available like to the bird watcher

and so there's some systems that were just watching the bird and then there's other systems where people are talking about designing it and so being able to have a shared understanding and formalisms that connect between those interpretability settings

And even though there might be things that are interpretable from the outside, but not from the inside, or at least even just the possibility of those kinds of questions and understanding where they always sometimes never happen is important research on the path towards this sort of more agentic niche that you're pointing towards, Hans.

Awesome.

Well, thank you again for the presentation and keep us posted on the work and good luck.


SPEAKER_03:
Thank you for having us.

Thank you.
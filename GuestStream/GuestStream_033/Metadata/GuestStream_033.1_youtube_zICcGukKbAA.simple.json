[
  {
    "start": 4.977,
    "end": 8.848,
    "text": " hello and welcome everyone it's active guest stream 33.1 january 17 2023",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 13.646,
    "end": 17.85,
    "text": " We are here with Ali and Alireza Modirshanichi.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 18.49,
    "end": 22.914,
    "text": "We're going to hear about the taxonomy of surprise definitions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 23.475,
    "end": 27.838,
    "text": "There will first be a presentation followed by a discussion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 28.719,
    "end": 31.161,
    "text": "So, Alireza, thank you for joining.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 31.822,
    "end": 34.004,
    "text": "Please take it away with the presentation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 34.144,
    "end": 36.766,
    "text": "We'll be looking forward to this and the discussion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 38.548,
    "end": 40.57,
    "text": "Thank you very much, Daniel and Ali, for inviting me.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 42.132,
    "end": 56.286,
    "text": " So as you said, I'm going to talk about a taxonomy of surprise definitions, which is basically all the materials or most of the materials that we publish in this recent paper in the Journal of Mathematical Psychology.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 56.366,
    "end": 60.25,
    "text": "That's a joint work with Johan Ibrea and Wolfram Gerstner.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 60.27,
    "end": 63.293,
    "text": "And Wolfram is my PhD supervisor.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 65.157,
    "end": 68.843,
    "text": " So talking about surprise, let's start with a simple thought experiment.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 70.025,
    "end": 72.609,
    "text": "Let's say that you want to plan something for your weekend.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 73.39,
    "end": 79.32,
    "text": "And to plan something for the weekend, we need information like how the weather is going to be over the weekend to plan something.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 80.521,
    "end": 89.586,
    "text": " And there are some cues like spring that are informative about the weather of the weekend or that the weather forecast says it's going to be sunny.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 90.547,
    "end": 98.472,
    "text": "Given these cues, we can make some predictions that, oh, weather forecast said it's sunny and it's spring, so most likely it's going to be sunny.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 100.117,
    "end": 109.521,
    "text": " Let's say that it's Friday evening, you plan that, for example, go for a bike, you go to sleep, wake up the day after, open the window and it's snowing.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 110.462,
    "end": 117.024,
    "text": "And seeing the snow, given that you expected the sunny weather, this mismatch is what's",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 119.145,
    "end": 122.968,
    "text": " natural language would agree on that it's a feeling of surprise, you know.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 123.969,
    "end": 129.353,
    "text": "We can easily talk about these moments and say that yeah I was surprised because I had planned to go for a bike.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 130.393,
    "end": 139.18,
    "text": "But then in scientific community we can ask this question that for these moments that you see snowy weather when opening the window, what does really happen in our brain?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 141.005,
    "end": 149.034,
    "text": " And there are lots of literature and long-lasting debate in neuroscience and psychology about different roles of surprise in the brain.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 149.094,
    "end": 158.204,
    "text": "And there's this zoo of different, what I call, surprise-related signals, things like prediction error, free energy, surprisal, or Bayesian surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 159.205,
    "end": 165.567,
    "text": " And people say that these things are related to learning, play important role in model building or predictive coding.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 165.727,
    "end": 168.228,
    "text": "Some people say that they're quite important for exploration.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 168.748,
    "end": 172.89,
    "text": "For example, surprising even attract attention or drive curiosity.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 174.057,
    "end": 181.88,
    "text": " There are also literature on memory that surprising events segments or continuous stream of observation.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 181.96,
    "end": 184.081,
    "text": "So they help us to segment our memory.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 184.761,
    "end": 198.846,
    "text": "And of course, there are lots of work and physiological signals of surprise signals, for example, when there's something surprising, your pupil dilates or EEG signal has a peak and so on and so forth.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 200.54,
    "end": 209.227,
    "text": " The thing is that if I naively look at this picture from far away, it seems like that surprise is everywhere in the brain.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 210.068,
    "end": 220.276,
    "text": "But then the puzzle is that is really the surprise in the sentence surprise modulate synaptic plasticity same as the one in the sentence surprise drive curiosity or not?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 221.523,
    "end": 233.871,
    "text": " And that's basically the main motivation of our work, that different studies in neuroscience and psychology use similar words, but to refer to different aspects of the moment of surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 234.852,
    "end": 238.454,
    "text": "And I would call this art piece the moment of surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 239.295,
    "end": 243.838,
    "text": "And basically the thing is that different studies, different experiments, different papers,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 244.858,
    "end": 253.803,
    "text": " They all talk about moments like this, situations like this, that we can all agree on to call it a surprising moment, but they talk about different aspects of it.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 254.644,
    "end": 263.449,
    "text": "The way that we summarize it or put it in words in the abstract of the paper was, there is no consensus on the definition of surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 263.809,
    "end": 269.093,
    "text": "When I posted the paper online on Twitter, the linguist Martin Hestelmeyer",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 269.913,
    "end": 284.203,
    "text": " wrote in reply that it's unsurprising that there is no consensus on the definition of surprise because it's not a technical term everyday words are usually somewhat weak and uniform definitions are desirable only for technical terms of science",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 285.143,
    "end": 286.664,
    "text": " And I cannot agree more with that.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 287.545,
    "end": 294.289,
    "text": "And I want to emphasize that today, I'm not going to talk about the meaning of surprise when we talk over a coffee break.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 295.109,
    "end": 302.494,
    "text": "It's in the context of neuroscience and psychology and when we talk about these moments and the role of these surprise-related signals in the brain.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 304.341,
    "end": 311.967,
    "text": " So before going to details and math and all the framework, I want to present the paper in a nutshell.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 312.107,
    "end": 313.348,
    "text": "What is the abstract of the paper?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 314.229,
    "end": 320.153,
    "text": "What we do is that we take many papers from the field and we identify several definitions of surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 320.994,
    "end": 324.256,
    "text": " I list 10 of the definitions here in the paper.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 324.276,
    "end": 326.238,
    "text": "There are 18 with all the details.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 327.058,
    "end": 337.345,
    "text": "And these definitions include things like unsolved reward prediction error, confidence corrected surprise, Bayesian surprise, postdictive surprise, and many things that people really use in different contexts.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 338.386,
    "end": 343.349,
    "text": "Then we ask this question that what are their similarity and differences between these different surprise definitions?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 343.889,
    "end": 350.314,
    "text": "And we have three main results that is analyzing these different definitions from different aspects.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 351.676,
    "end": 354.539,
    "text": " Our first main result is a technical classification.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 355.72,
    "end": 360.484,
    "text": "So to talk about the technical classification, I would like to introduce a short notation.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 361.545,
    "end": 367.891,
    "text": "We call observation y. We call q's x, things that are going to be informative about the observation.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 368.731,
    "end": 377.88,
    "text": "And then predictions can be seen as either a probability distribution over the observation given the q's, or one estimate of the next observation.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 378.996,
    "end": 388.94,
    "text": " And looking at this surprise definition, we could identify a category of these definitions that what we call observation mismatch surprise definition.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 389.1,
    "end": 391.321,
    "text": "They only depend on the next estimate.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 391.962,
    "end": 398.104,
    "text": "So they don't care that, OK, what was the probability of weather being sunny or rainy or snowy?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 398.184,
    "end": 402.206,
    "text": "They say that I predicted the weather is going to be sunny and now it's not sunny.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 402.386,
    "end": 404.947,
    "text": "And what is the difference between being sunny and being snowy?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 405.287,
    "end": 406.328,
    "text": "And that's the surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 407.188,
    "end": 410.01,
    "text": " There is another group that we call probabilistic mismatch.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 410.551,
    "end": 419.898,
    "text": "They actually look at this distribution and say that, oh, it's true that I didn't expect the snowy weather, but it was not too unlikely.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 419.978,
    "end": 422.74,
    "text": "So it may be not as surprising as I would expect it.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 423.12,
    "end": 428.844,
    "text": "So they work with the distribution instead of a single point estimate or predictor.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 430.886,
    "end": 436.21,
    "text": "But actually, this conditional distribution is a marginalization of something bigger.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 437.438,
    "end": 446.401,
    "text": " Because there is actually some uncertainty about how accurate, for example, weather forecast is about the weather of the next day.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 447.161,
    "end": 455.944,
    "text": "And that's what we would call, I would put in this parameter theta that are underlying rules of the environment or hypothesis, the reliability of the weather forecast.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 456.904,
    "end": 459.447,
    "text": " And we don't have access to the true value of it.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 459.667,
    "end": 460.728,
    "text": "So we make it belief.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 460.908,
    "end": 462.71,
    "text": "And this pi of theta is our belief.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 462.77,
    "end": 464.712,
    "text": "It's a distribution over these possibilities.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 465.452,
    "end": 471.739,
    "text": "And actually, the marginal distribution of next observation given the Q is this marginal probability.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 472.519,
    "end": 475.702,
    "text": "And the third group of the surprise definitions",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 476.403,
    "end": 490.647,
    "text": " People defining these definitions argue that actually what we care about is this belief that, for example, how much we learn, how much we change this belief by observing a new observation.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 491.728,
    "end": 494.929,
    "text": "And those are the definitions that we call belief mismatch surprise definitions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 495.856,
    "end": 498.257,
    "text": " So this was a technical classification.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 498.657,
    "end": 506.841,
    "text": "We put different definitions in three different groups and say that there is some difference in how they depend on the belief of subject.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 509.243,
    "end": 513.885,
    "text": "Then we go to a conceptual labeling, and that's what we call the taxonomy.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 515.266,
    "end": 523.39,
    "text": "And we talk about what is the concept or what is the conceptual arguments behind the definition of each of these supervised definitions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 524.671,
    "end": 527.294,
    "text": " One group, that's what we call prediction surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 527.574,
    "end": 535.582,
    "text": "So they care about how accurate or inaccurate the prediction is, how expected or unexpected the next observation is.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 536.403,
    "end": 538.685,
    "text": "The more accurate, the less surprising the observation.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 539.366,
    "end": 543.09,
    "text": "Then there's another category that we call change point detection surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 544.226,
    "end": 549.791,
    "text": " They don't only care about the prediction, they always compare prediction with a baseline prediction.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 550.671,
    "end": 562.441,
    "text": "If something is unexpected, but that thing is unexpected under any hypothesis, then they claim that is not surprising because it would be just kind of an outlier.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 562.521,
    "end": 564.242,
    "text": "So I should not consider it as surprising.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 564.383,
    "end": 569.307,
    "text": "And these are important to detect change points because",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 570.546,
    "end": 577.773,
    "text": " What they care about is to see if there is another hypothesis under which the current observation would be more likely.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 578.114,
    "end": 591.367,
    "text": "For example, in this case of weather forecasts, during COVID time, there was this argument that there is this paper I'm citing here that because there was much less flight going on, we had way less data and sensors.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 591.427,
    "end": 594.209,
    "text": "So weather forecasts suddenly stopped being accurate.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 595.05,
    "end": 617.151,
    "text": " and actually if you were checking with a forecast you could see without even knowing the news that uh it gets less and less accurate as time pass on and these kind of change point detection surprise definitions are somehow uh optimal i show later on that they're optimal for detecting these kind of unobserved changes uh the third group uh or the third class",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 618.314,
    "end": 620.435,
    "text": " are what we call information gains to price.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 620.955,
    "end": 630.04,
    "text": "So no matter if there is a change or not, this new observation made me learn something more about those underlying rules of the environment.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 630.24,
    "end": 635.503,
    "text": "So which media or which weather forecast is more reliable than the others?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 636.403,
    "end": 642.206,
    "text": "Every day that I see the weather, I would use that to update my belief about different weather forecasts.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 643.553,
    "end": 653.678,
    "text": " And the last category is, no matter which of these ones I care about, what is the conceptual argument behind the definition?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 654.178,
    "end": 661.921,
    "text": "The argument is that if I'm more confident about my prediction, I should feel more surprised.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 662.181,
    "end": 668.404,
    "text": "So the argument is that no matter what is the definition, I should have an explicit term for confidence in my definition.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 670.273,
    "end": 680.599,
    "text": " So having this technical classification and this conceptual labeling, we can arrange this list of surprise definition in these different boxes and have some structure.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 681.7,
    "end": 685.262,
    "text": "And as you can see, some of these boxes are empty.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 685.923,
    "end": 686.923,
    "text": "And I would like to say,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 687.904,
    "end": 697.572,
    "text": " You know, we have these mathematical definitions on the side, but their justification comes from their relation to experiments and what they tell us about the brain.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 698.273,
    "end": 705.739,
    "text": "So it can be the case that in future we see that none of these definitions are helpful, or we see in some situation that we need another definition.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 705.959,
    "end": 714.386,
    "text": "And this figure here tells us that, OK, there are actually places that we can come up with new definitions and new categories.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 715.367,
    "end": 716.668,
    "text": " But there are also restrictions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 716.788,
    "end": 718.93,
    "text": "For example, talking about information gain.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 719.43,
    "end": 723.814,
    "text": "If we want to talk about information gain, we should always talk about the belief mismatch.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 723.994,
    "end": 729.358,
    "text": "We cannot talk about only at the level of observation or marginal distribution because we are talking about learning.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 729.398,
    "end": 733.841,
    "text": "So we should talk about these very high level hidden variables.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 734.782,
    "end": 742.668,
    "text": " Or when we talk about confidence, inevitably, we need to talk about probability distribution, and we cannot talk about point estimates.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 743.149,
    "end": 747.332,
    "text": "But still, we can talk about these three different empty boxes here.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 749.349,
    "end": 757.418,
    "text": " And our last main result is, although these different definitions are in different boxes, but there are some links between them.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 757.598,
    "end": 761.823,
    "text": "So we found conditions under which these definitions are indistinguishable.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 761.983,
    "end": 768.089,
    "text": "So there are some situations that there is one-to-one mapping between some of these definitions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 768.23,
    "end": 770.272,
    "text": "We cannot really experimentally distinguish them.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 771.673,
    "end": 774.494,
    "text": " So that was a summary of our work.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 775.094,
    "end": 783.318,
    "text": "And now I'm going to go to the mathematical framework and go one by one through all of these definitions and their link to each other.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 784.018,
    "end": 788.019,
    "text": "And please let me know if there is any clarification question.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 790.28,
    "end": 792.181,
    "text": "So for mathematical framework,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 793.854,
    "end": 801.185,
    "text": " Let's look at one very traditional experiment for studying surprise in the field of neuroscience.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 801.626,
    "end": 804.169,
    "text": "It's what's called volatile outball task.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 805.547,
    "end": 811.152,
    "text": " In a typical outfall task, at each time point, we have two possibilities for the stimulus.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 811.592,
    "end": 816.016,
    "text": "It's either, for example, here a blue disk or a red square.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 817.297,
    "end": 822.221,
    "text": "And for a while, the blue disk is much less likely to appear on the screen than the other one.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 824.645,
    "end": 836.974,
    "text": " Reasoning behind the design of this experiment is that the participants who are watching this sequence on the screen, one after another, expect to see the red screen more often than the blue disk.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 837.355,
    "end": 841.658,
    "text": "And at the moment that he or she sees the blue disk, it's surprising.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 842.498,
    "end": 845.04,
    "text": "And that would give them a feeling of surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 846.221,
    "end": 853.124,
    "text": " And a question that one can ask is that can we really quantify how surprised a participant feels at time t equal to 4?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 853.184,
    "end": 859.726,
    "text": "So if we know that this is the sequence until time t equal to 4, can we quantify the amount of surprise or not?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 861.487,
    "end": 870.751,
    "text": "And to talk about the feeling of participant about surprise of an observation, we should make some assumption about how participants perceive this sequence.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 871.861,
    "end": 881.849,
    "text": " And we take a very common assumption of the field that participant perceive their sensory observation as probabilistic outcomes of a generative model with hidden variables.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 882.51,
    "end": 887.934,
    "text": "So at each time, each of these observation come from some distribution with some hidden variables.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 887.974,
    "end": 897.822,
    "text": "For example, the way that participant, we assume in a way that the way that participant think about the sequence of observation is that at each time with some probability,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 898.922,
    "end": 905.829,
    "text": " one of these stimuli appear on the screen, and over time, given the sequence, they try to find this probability.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 908.411,
    "end": 911.354,
    "text": "To formally define this, we consider a generative model.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 912.595,
    "end": 923.705,
    "text": "Let's say that at time t, we show the observation, what appears on the screen, by y , and there is this hidden variable, which we call environment parameters, which is, for example, the probability of",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 925.731,
    "end": 927.373,
    "text": " blue disk appearing on the screen.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 928.554,
    "end": 934.66,
    "text": "And now this link, for people who are familiar with, I'm going to make a Bayesian network.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 935.841,
    "end": 942.567,
    "text": "And this link shows that the environment parameter determined the distribution over the observation.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 942.808,
    "end": 945.05,
    "text": "And I can repeat the same thing for time t plus one.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 947.546,
    "end": 956.334,
    "text": " Something that I didn't talk about in this volatile oddball task is that for a while, we had this one image after another with some fixed probability.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 957.134,
    "end": 963.98,
    "text": "But once in a while, we have a hidden change point, which changed the probability of blue disk and red square.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 964.281,
    "end": 967.043,
    "text": "So previously, blue disk was rare.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 967.143,
    "end": 969.785,
    "text": "And after that, the other one can be rare or frequent.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 970.666,
    "end": 973.967,
    "text": " So the environment parameter over time can change.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 974.267,
    "end": 984.07,
    "text": "So we define this parameter, this random variable change point indicator that at time t plus one, if it is one, it means that there's a change in the environment.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 984.13,
    "end": 985.15,
    "text": "There's a new parameter.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 985.951,
    "end": 988.752,
    "text": "And if it is zero, it basically means that there's no change.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 988.792,
    "end": 996.714,
    "text": "The environment is stable and theta t plus one is a copy paste of theta t. So this link just take theta t and take it to theta plus t plus one.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 999.608,
    "end": 1006.871,
    "text": " So far, the way that I present this task was that different observations are independent of each other.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1006.951,
    "end": 1010.633,
    "text": "So at each time, one of these stimuli appear on the screen randomly.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1012.763,
    "end": 1031.081,
    "text": " This very beautiful paper of Mayniel and a colleague in 2016 in Pilas Computational Biology, they argued, it's very nice, they argued that even if the sequence of observation is randomly designed, so these observations are independent of each other,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1032.233,
    "end": 1037.677,
    "text": " People, when they are observing this sequence, assume that there is some dependency.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1037.777,
    "end": 1044.462,
    "text": "So what they estimate over time is not really probability of observing the blue disk or red square.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1044.962,
    "end": 1050.826,
    "text": "They are estimating the probability of observing blue disk after red square or red square after red square.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1051.507,
    "end": 1055.229,
    "text": "So what they assume is that there is a link between yt and yt plus 1.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1056.796,
    "end": 1068.279,
    "text": " So far, I have a model of this out-ball task that says that each observation depends on the environment parameter and the previous observation, and there is a dynamic for this environment parameter.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1069.17,
    "end": 1078.977,
    "text": " I do a bit of trick, just define this dummy variable, which I call q. And for the oddball task, I just copy paste the previous observation in the queue.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1079.617,
    "end": 1090.004,
    "text": "But the reason that we use q is to have one abstract notion of what has predictive power for predicting next observation.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1090.744,
    "end": 1099.689,
    "text": " And for the task with the action, action can go to Q. For the example of the weather forecast, weather forecast can go to the Q and all these things.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1101.69,
    "end": 1106.432,
    "text": "And now having this motif, I can repeat it and make the whole sequence.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1107.172,
    "end": 1116.917,
    "text": "And given this generative model, given this mathematical model of the experiment, I can now mathematically define the surprise of observing variety plus one.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1117.397,
    "end": 1118.858,
    "text": "So with this assumption, we can",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1121.111,
    "end": 1124.714,
    "text": " we can propose some quantification of surprise of one stimuli.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1126.215,
    "end": 1141.606,
    "text": "And what is worth mentioning is that this generative model here is generalization of many other models in the field, which means that many of the experiments already done to study surprise can be modeled by this generative model, and our results hold for that.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1144.179,
    "end": 1150.764,
    "text": " Before going forward, this generative model also account for this moment of surprise, can model that.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1151.544,
    "end": 1153.906,
    "text": "That the Q was X, is here.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1154.206,
    "end": 1156.808,
    "text": "The observation was the weather of next day is here.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1157.609,
    "end": 1163.012,
    "text": "And theta was environment's parameter, was how reliable, for example, the weather forecast is about the next observation.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1164.293,
    "end": 1171.538,
    "text": "And the change point is whether something happened in the environment, for example, the pandemic and decrease in the number of planes.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1174.475,
    "end": 1197.237,
    "text": " and now i'm going to make this definition more formal and discuss the dynamics of each of these different levels of in the generated model so i start with the observations and we assume this is the probability of",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1198.604,
    "end": 1204.987,
    "text": " at time t plus 1, observing y, condition on q being x and parameter being theta.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1205.847,
    "end": 1210.729,
    "text": "And basically this equation here means that this probability is time independent.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1210.949,
    "end": 1222.874,
    "text": "So as long as I know the q and the parameter, it doesn't matter that if it's time step 1 or time step t. It's all the time the same distribution, which we show by p of y given x.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1224.335,
    "end": 1230.457,
    "text": " So basically any change in the environment goes to the parameter and the distribution is fixed.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1230.977,
    "end": 1233.397,
    "text": "For the Q variables, we don't put any constraint.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1234.318,
    "end": 1247.721,
    "text": "And for example, it can even be independent of anything that subject assume, which is the, for example, for weather forecast, you just turn on the TV and see what's going to be the prediction for next day.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1249.892,
    "end": 1257.937,
    "text": " Then we have the change points indicator and their dynamic is at each time, independently, we ask whether there is a change or not.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1258.737,
    "end": 1268.563,
    "text": "And CT plus one at time T plus one is a Bernoulli binary sample with probability of PC, which is what we call change point probability.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1269.403,
    "end": 1274.126,
    "text": "So at each time point with probability PC, there is either a change or not.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1275.727,
    "end": 1277.248,
    "text": "Then this is the,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1278.789,
    "end": 1285.252,
    "text": " most important in a way part of the model, that how these parameters change over time.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1285.752,
    "end": 1298.779,
    "text": "We consider prior belief pi zero and say that at time one, before the experiment starts, participants have some assumption and some belief about what are the parameters behind this generative process.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1300.12,
    "end": 1302.241,
    "text": "And that we show with pi zero.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1303.311,
    "end": 1320.345,
    "text": " And at time t, if there is no change in the environment, if ct plus 1 is equal to 0, theta t plus 1 is just copy-paste of theta t. But if there is a change in the environment, we assume that theta t plus 1 is sampled again independently of past from this prior belief.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1321.638,
    "end": 1327.585,
    "text": " So we have a stable environment with the same theta until CT plus one is equal to one.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1328.326,
    "end": 1334.134,
    "text": "Then we have a change in the environment and a parameters is resampled from the prior belief.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1337.759,
    "end": 1347.264,
    "text": " These equations that I wrote here is fully described the joint distribution of C1 to Ct plus one, theta one to theta t plus one, and so on and so forth.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1347.284,
    "end": 1351.406,
    "text": "The whole generative model is described by these definitions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1352.126,
    "end": 1357.129,
    "text": "But if you want to see the precise mathematical definition, please look at the definition one in the paper.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1359.175,
    "end": 1375.889,
    "text": " Before going forward, I would like to show some notation that whenever I write something like this, whenever there's no ambiguity, I shorten the notation by dropping the random variables that are the capital letters.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1378.227,
    "end": 1397.289,
    "text": " Then I define what I call observer's belief at time t. If you remember, pi zero I defined as initial belief, and now pi t is the observer belief at time t. That is, given that I observe y1 to yt with cues x1 to xt, what is my",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1398.67,
    "end": 1408.335,
    "text": " knowledge about the current parameters of the environment, which basically can be summarized in the posterior probability of theta t given the previous cues and observation.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1412.737,
    "end": 1424.322,
    "text": "Which in this example is that given a sequence of days and a prediction of the weather forecast, I can make a belief on how reliable the weather forecast is at time t.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1426.004,
    "end": 1449.681,
    "text": " And at the end, they define this marginal probability, which is the probability of Y given X. And now I drop theta and I put the knowledge here and integrate over all possibility that if the world is at time T, how my knowledge about the world identify the relation between Q and observations.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1452.455,
    "end": 1455.138,
    "text": " So this was the mathematical framework.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1455.659,
    "end": 1466.131,
    "text": "And given this mathematical framework, now I can go to a formal investigation of different definitions and present or classification indistinguishability condition and taxonomy.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1468.14,
    "end": 1468.921,
    "text": " What is surprise?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1469.922,
    "end": 1484.154,
    "text": "Having this model, I can ask this question that condition on the previous observations y1 to t and q variables x1 to t plus 1, these ones and these ones, how surprising is the next observation yt plus 1?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1484.894,
    "end": 1493.802,
    "text": "So we are at time t, we are going to observe yt plus 1 and this is a now valid mathematical question in this framework.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1495.475,
    "end": 1502.058,
    "text": " To make some prediction about yt plus 1, I need information about theta t and xt plus 1.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1502.458,
    "end": 1505.199,
    "text": "That is the Q, and these are the rules of the environment.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1506.019,
    "end": 1516.284,
    "text": "Whatever I knew about the rules of the environment is summarized in my belief, in whatever knowledge this observation and previous Qs gave me about theta t.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1517.857,
    "end": 1526.587,
    "text": " And given the belief and the queue, I can throw away all the previous observations and say that I only need the belief and the queue.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1527.568,
    "end": 1537.058,
    "text": "And surprise is basically a function that gets us arguments, the belief, the queue, and the next observation, and gives a real value back.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1538.186,
    "end": 1542.452,
    "text": " One famous example is what people call Suprisal or Shannon Surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1543.594,
    "end": 1553.928,
    "text": "The history is so long, it has been used for so long that I cannot really point to the first paper that used it, but you can look at the paper of Bartu in 2013 for a review.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1557.049,
    "end": 1568.752,
    "text": " which basically says that given a belief, given a cue and an observation, surprise of this observation is equal to minus logarithm of marginal probability.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1569.133,
    "end": 1572.534,
    "text": "So the more likely this observation, the less surprising it is.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1573.494,
    "end": 1583.757,
    "text": "There's also another way to define it, which, because here I had the knowledge at time t, one can say that there was a possibility to have a change point here.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1585.09,
    "end": 1589.891,
    "text": " and one can extend this probability and say that oh there was a",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1591.673,
    "end": 1604.078,
    "text": " When I'm at time t and I'm predicting the next observation, I should also consider this possibility that the world may change in between and I should put some weights on this prior knowledge.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1604.718,
    "end": 1612.761,
    "text": "So the next observation comes with some probability from the current world or from a world that is recent.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1617.234,
    "end": 1619.555,
    "text": " So there are two possibilities for these definitions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1619.835,
    "end": 1626.457,
    "text": "And what I want to emphasize is that in volatile environments, we are not sure which one we should take necessarily.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1627.157,
    "end": 1631.938,
    "text": "One may say that this definition makes more sense because it's the full probability.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1632.719,
    "end": 1639.541,
    "text": "But there are experimental evidence in this paper of Nasser and colleague in 2010 in the Journal of Neuroscience.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1640.561,
    "end": 1648.223,
    "text": " What they argue is that when people observe yt plus one, they update their knowledge by assuming that there might have been a change.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1648.983,
    "end": 1653.465,
    "text": "But when they predict the next observation, they don't consider that there may be a change.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1654.425,
    "end": 1660.687,
    "text": "So in a way, when people are thinking about the future, they don't consider this possibility of a change.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1661.75,
    "end": 1672.519,
    "text": " And in this sense, we argue that the first definition, the one that I call S is more consistent with the experimental evidence.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1673.419,
    "end": 1676.642,
    "text": "But through the paper, we take all these two versions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1676.742,
    "end": 1683.687,
    "text": "It appears for many of the definitions, and we do really this tedious job of keeping all these definitions through the whole paper.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1684.068,
    "end": 1687.09,
    "text": "But today in the talk, I only present one version of each definition.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1690.089,
    "end": 1691.73,
    "text": " So this was one of the examples.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1692.27,
    "end": 1696.232,
    "text": "I can present different examples of surprise definition in the field.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1696.252,
    "end": 1702.376,
    "text": "The three of them, I'm going to present the first classification that we have.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1703.056,
    "end": 1704.397,
    "text": "The first one was Shannon Surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1705.427,
    "end": 1707.328,
    "text": " Another one is the absolute error.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1708.109,
    "end": 1731.942,
    "text": "As I said before, one can say that given the belief that I currently have, I can make a prediction, for example, take the average of this marginal probability, and then the difference between this average, that was my prediction, and the difference between the average and the true observation, the actual observation, is defined as a surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1733.023,
    "end": 1734.224,
    "text": "It's some sort of prediction error.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1735.658,
    "end": 1748.389,
    "text": " And there's also this other definition, very famous definition, Bayesian surprise, that says that I don't care what is the difference between yt or y hat and what is this marginal probability.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1748.47,
    "end": 1755.055,
    "text": "What I care about is that by observing, upon observing this yt plus 1,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1756.377,
    "end": 1758.739,
    "text": " how much I update my knowledge about the world.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1759.54,
    "end": 1767.626,
    "text": "And what is the difference DKL here stand for KL divergence between the belief before observing Yt plus one and after observing Yt plus one.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1769.868,
    "end": 1783.379,
    "text": "It is not written in the form of Yt plus one, Xt plus one and Pi t, but in remark two in the paper, we show that if PC, if probability of change was equal to zero, so if there were no change in the environment,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1784.38,
    "end": 1790.963,
    "text": " Bayesian surprise could be written as a difference between expected Shannon surprise and Shannon surprise itself.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1791.483,
    "end": 1794.785,
    "text": "And funnily enough, the Shannon surprise here appear with the negative sign.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1795.525,
    "end": 1801.188,
    "text": "So it seems like that the Bayesian surprise and Shannon surprise are really complementary definitions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1802.348,
    "end": 1808.612,
    "text": " And particularly, it is important to say that we cannot really get rid of the belief.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1808.973,
    "end": 1811.374,
    "text": "Belief really appears, remains here.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1811.454,
    "end": 1816.678,
    "text": "So we have marginal probabilities here, but we also need the belief to evaluate Bayesian surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1818.419,
    "end": 1826.944,
    "text": "For the case of volatile environment with PC greater than zero, we showed that in Proposition 8, the interpretation remained the same, but",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1828.723,
    "end": 1853.312,
    "text": " equations become a bit more ugly so that's why i don't bring it here but you can check it out so having these three definitions here something that's important is that Shannon surprise could be written in terms of the marginal probability absolute error could be written in terms of the y hat one estimate of next observation but for Bayesian surprise we needed the whole belief because we had to compute this expectation",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1854.437,
    "end": 1863.365,
    "text": " And that's how we make our first technical classification, which is based on the dependence on the belief that this surprise definition, how it depends on belief pi.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1865.747,
    "end": 1867.068,
    "text": "Here on top, we have pi.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1867.209,
    "end": 1869.891,
    "text": "From pi, we can extract the marginal probability.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1869.971,
    "end": 1873.014,
    "text": "And from the marginal probability, we can extract one estimate.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1873.574,
    "end": 1878.519,
    "text": "The first group are the observation mismatch surprise definition that only depend on the estimate.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1878.699,
    "end": 1880.801,
    "text": "For example, the absolute error that I introduced.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1881.889,
    "end": 1885.67,
    "text": " The second group are the one that only depends on the marginal probability.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1886.191,
    "end": 1891.152,
    "text": "There are one like Shannon-Surprise that is minus logarithm of the marginal probability.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1891.733,
    "end": 1900.616,
    "text": "But the last group, they cannot be evaluated by only knowing these two variables, by knowing this distribution and the next prediction.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1900.716,
    "end": 1903.197,
    "text": "We know the whole belief, like Bayesian-Surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1904.42,
    "end": 1915.911,
    "text": " So going back to this figure that I showed in the introduction, so far we have seen two definition here in this column, two definition here in this column, and two definition here in this column.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1916.572,
    "end": 1920.876,
    "text": "The first two, absolute error and Shannon's surprise, they're more about the prediction.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1921.357,
    "end": 1925.4,
    "text": "So this one was saying that what is the mismatch between the prediction and observation?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1926.241,
    "end": 1935.576,
    "text": " And this one was saying that how likely or unlikely the observation was, while the Bayesian surprise was saying that how much information I gained upon this observation.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1937.579,
    "end": 1940.183,
    "text": "Okay, this was a technical classification.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1941.187,
    "end": 1944.889,
    "text": " I can go forward now and discuss the indistinguishable conditions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1945.589,
    "end": 1950.191,
    "text": "Before going to indistinguishable conditions, I want to introduce another definition of surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1951.151,
    "end": 1956.654,
    "text": "There is this opinion, this idea in the field, that surprising events increase learning rates.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1957.734,
    "end": 1962.196,
    "text": "And there are some theories for it, and there are some experiments showing that it actually happens.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1962.716,
    "end": 1965.117,
    "text": "When there is something surprising, the learning rate increases.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1966.138,
    "end": 1966.618,
    "text": "But there is this...",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1968.962,
    "end": 1977.013,
    "text": " logical question that is increasing learning rate always good for making better predictions or making better knowledge of the environment or not.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1978.769,
    "end": 1982.532,
    "text": " Because sometimes something can be unlikely.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1983.113,
    "end": 1987.436,
    "text": "But if it is just outlier, maybe I should not change my learning rate.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1987.476,
    "end": 1989.378,
    "text": "I should just ignore this observation.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1990.599,
    "end": 1999.427,
    "text": "What we find out is that it is necessary to have a comparison between the current belief and the prior belief.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 1999.987,
    "end": 2005.372,
    "text": "Because basically, the moment that you want to increase your learning rate up in surprising events,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2005.672,
    "end": 2007.433,
    "text": " is when there is a change in the environment.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2007.553,
    "end": 2009.934,
    "text": "And you want to say that I want to get rid of the old observation.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2011.855,
    "end": 2017.998,
    "text": "And this Bayes factor surprise, we proposed it in a paper in neural computation in 2021.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2018.978,
    "end": 2024.721,
    "text": "And it always compared the probability of observation under the prior belief with under the current belief.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2024.881,
    "end": 2029.664,
    "text": "It's some sort of hypothesis testing at each point saying that, oh, whether there was a change or not.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2030.764,
    "end": 2043.015,
    "text": " So if the denominator, the probability under the prior belief, if we ignore that, this is basically a decreasing function of the probability under the current belief, very similar to the Shannon surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2043.796,
    "end": 2048.34,
    "text": "So the less likely an event, the more surprising it is.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2048.9,
    "end": 2055.005,
    "text": "But if that event is also unlikely under the prior belief, then it may not be surprising.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2056.246,
    "end": 2057.948,
    "text": "And the reasoning...",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2059.417,
    "end": 2068.202,
    "text": " become more valid when we show in our proposition 1 in the paper that actually this definition is optimal to modulate learning.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2069.243,
    "end": 2081.371,
    "text": "In proposition 1, we show that after observing yt plus 1, the new belief can be written in this term, which is a trade-off between integrating new observation into the previous belief.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2081.491,
    "end": 2086.514,
    "text": "So take the previous belief, take the new observation, combine the two, make a new belief.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2087.615,
    "end": 2092.1,
    "text": " Or throw away the old belief, say that, oh, there was probably a change in the environment.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2092.521,
    "end": 2095.143,
    "text": "I should reset and start from scratch.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2096.185,
    "end": 2101.37,
    "text": "And put the new observation together with the prior belief and make a reset belief.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2102.091,
    "end": 2108.318,
    "text": "And this trade-off is controlled by something that we call adaptation rates, here shown by gamma t plus one.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2109.554,
    "end": 2113.621,
    "text": " which turned out to be an increasing function of Bayes factor surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2113.982,
    "end": 2120.915,
    "text": "So the higher Bayes factor surprise, the higher adaptation rate, and the higher adaptation rate, the more weight on the reset.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2122.042,
    "end": 2128.605,
    "text": " Basically saying that the surprise, and this rule really comes out of the Bayesian inference, it's a normative rule.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2129.805,
    "end": 2140.15,
    "text": "And basically saying that Bayes factor surprise naturally emerge, or at least we define it in a way to have this decomposition of the belief.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2141.724,
    "end": 2154.028,
    "text": " In this paper, we propose also a variational method that turned out to have a very simple update rule for distribution in exponential family, which we call variational surprise minimization learning.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2154.308,
    "end": 2163.151,
    "text": "And I would like to mention here that recently it got some applications in the active inference community and people used it for model building.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2165.137,
    "end": 2170.261,
    "text": " So I introduced the Bayes factor surprise as a new Bayes surprise definition.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2170.901,
    "end": 2174.144,
    "text": "Let's go and just overview the definitions that so far I showed.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2174.964,
    "end": 2179.608,
    "text": "The first one was absolute error surprise, was in the family of observation mismatch.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2180.208,
    "end": 2183.09,
    "text": "And it cared about how good my prediction is.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2184.391,
    "end": 2188.174,
    "text": "Then there was Shannon surprise, saying how likely or unlikely an observation is.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2188.374,
    "end": 2192.217,
    "text": "It was, again, caring about prediction, but it was in the probabilistic mismatch.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2193.442,
    "end": 2204.429,
    "text": " The base factor surprise that I just showed is in the family of change point detection, because as we saw, it cared about whether there was a change point or not, but it is still in the probabilistic mismatch to price definition.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2205.91,
    "end": 2217.258,
    "text": "And another definition that I showed was Bayesian surprise, which was the difference between previous belief and the new belief, and is in the belief mismatch to price definitions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2220.359,
    "end": 2231.083,
    "text": " Although they are in different columns, in different boxes, one can ask this question that is there any link between Shannon's surprise here and the Bayes factor surprise here?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2231.123,
    "end": 2234.985,
    "text": "Because at least there's one term that is repeated in both definitions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2236.653,
    "end": 2254.848,
    "text": " That's what we show in the proposition 2 of the paper, that the Bayes factor surprise can be written as a function of these interesting things here that are not Shannon surprise, but very interestingly related to the Shannon surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2254.908,
    "end": 2256.609,
    "text": "There are differences in Shannon surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2257.309,
    "end": 2262.293,
    "text": "So there are differences between the Shannon surprise under the current belief and under the prior belief.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2265.506,
    "end": 2270.35,
    "text": " Base factor surprise can be written as a function of difference in channel surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2270.55,
    "end": 2278.095,
    "text": "So one may say that base factor surprise has an interpretation like a relative surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2278.716,
    "end": 2283.519,
    "text": "And in that sense, it always compared prior belief with the current belief.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2283.7,
    "end": 2285.101,
    "text": "And that's why it's good for learning.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2286.562,
    "end": 2294.808,
    "text": "As a consequence, in corollary one, we can argue that optimal surprise modulation of learning can also be written in terms of these differences instead of the base factor surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2296.124,
    "end": 2317.47,
    "text": " But the most fun thing is that in corollary 2, we show that if the marginal probability under the prior belief is uniform, so if basically means that if this term is constant, then there are strictly increasing mappings between base factor surprise and true definition of Shannon surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2318.67,
    "end": 2325.656,
    "text": " And strictly increasing mappings basically means that these are one to one function of each other.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2325.756,
    "end": 2326.817,
    "text": "So they're invertible.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2327.057,
    "end": 2329.459,
    "text": "And if I have one of them, I can find the other two.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2329.779,
    "end": 2331.6,
    "text": "So in a way, they're indistinguishable.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2332.561,
    "end": 2341.849,
    "text": "And that's one of the big results of our paper that we identified the conditions under which different definitions are indistinguishable.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2342.836,
    "end": 2347.703,
    "text": " So here in this figure, you see different definitions that are connected to each other with lines.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2348.484,
    "end": 2356.675,
    "text": "Each line correspond to one of these condition that if this condition is satisfied, then these two definitions are indistinguishable.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2358.101,
    "end": 2364.523,
    "text": " And C2 here means that this condition is proved under corollary 2 in the paper.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2365.724,
    "end": 2368.945,
    "text": "Why these indistinguishability conditions are important?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2370.045,
    "end": 2371.626,
    "text": "The first thing is for experiment design.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2372.186,
    "end": 2376.648,
    "text": "For example, let's take the oddball task that I introduced before.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2377.836,
    "end": 2392.644,
    "text": " Usually in out-ball task, it is assumed that marginal distribution under the prior belief is flat because there's no preference for the, at the beginning, there's no preference between the blue disc and the red square.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2393.304,
    "end": 2399.928,
    "text": "As a result, basically means that in this kind of experiment, all these five definitions that I have here, they're indistinguishable.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2399.988,
    "end": 2404.911,
    "text": "So I cannot do out-ball task and expect to see a difference between these five definitions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2405.991,
    "end": 2413.455,
    "text": " In addition, some people argue that even the prior belief is flat, and if is that the case, then these two definitions are indistinguishable.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2414.776,
    "end": 2423.881,
    "text": "Even more, because oddball task is a categorical task, it is one of the special cases that we identify, which basically means that",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2425.538,
    "end": 2427.68,
    "text": " seven of these definitions are indistinguishable.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2428.18,
    "end": 2445.455,
    "text": "So it's quite important that if we want to make an experiment to distinguish some of these surprise definitions, to study different definitions of surprise, we should be careful to not have an experiment where these are indistinguishable.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2445.775,
    "end": 2450.439,
    "text": "And what we do is that we take a series of papers in the field that have experiments",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2451.991,
    "end": 2458.293,
    "text": " where they study different definitions of surprise, and we identify which of these conditions are satisfied or not in these papers.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2460.454,
    "end": 2465.896,
    "text": "The second thing that this indistinguishability condition can be important for is for computational modeling.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2466.636,
    "end": 2470.357,
    "text": "There is this new preprint from ORLAB and via archive.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2473.618,
    "end": 2483.262,
    "text": " The authors show that they design a spiking neural network that try to learn in a volatile environment and modulate its learning rate with the notion of surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2484.102,
    "end": 2491.625,
    "text": "In the setting that they analyze, we can show that these three lines, the condition corresponding to these three lines, are satisfied.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2491.725,
    "end": 2494.546,
    "text": "So basically, these definitions of surprise are equivalent.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2495.851,
    "end": 2498.933,
    "text": " Why this is interesting for us?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2499.153,
    "end": 2503.496,
    "text": "Because this one is optimal for learning, is the Bayes factor surprise that I showed before.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2504.197,
    "end": 2509.34,
    "text": "While this one is pretty easy to implement, while Bayes factor surprise was a function of probabilities.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2509.581,
    "end": 2512.082,
    "text": "While this one is the observation space.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2512.743,
    "end": 2522.31,
    "text": "And that's actually the fun thing, that what they do here is they make a spiking network that computes something related to these observation mismatch surprise definitions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2523.11,
    "end": 2531.296,
    "text": " While there are some optimal justifications for why this surprise definition can be helpful for learning.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2533.018,
    "end": 2542.445,
    "text": "So putting together, so far I presented the technical classification that we proposed and the indistinguishability condition that we found out.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2543.405,
    "end": 2545.847,
    "text": "And now I'm going to discuss the taxonomy.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2548.629,
    "end": 2551.531,
    "text": "I had 40 minutes, so I still have time, right?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2551.571,
    "end": 2552.272,
    "text": "Well, I don't know.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2554.949,
    "end": 2558.67,
    "text": " So for the taxonomy, I need to introduce another definition.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2559.11,
    "end": 2562.372,
    "text": "I promise it's the last definition that I introduce today.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2563.912,
    "end": 2566.833,
    "text": "It's the notion of confidence correction.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2566.893,
    "end": 2567.933,
    "text": "I want to introduce that.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2568.294,
    "end": 2573.996,
    "text": "And there is this idea that higher confidence must lead to higher feeling of surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2574.196,
    "end": 2577.657,
    "text": "So the more confident you are about your predictions, the more surprise you should feel.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2578.86,
    "end": 2579.58,
    "text": " Farage et al.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2580.281,
    "end": 2582.422,
    "text": "in this paper proposed this definition.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2583.742,
    "end": 2586.284,
    "text": "It may look a bit strange at the moment.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2586.524,
    "end": 2588.184,
    "text": "I would demystify it in a bit.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2589.405,
    "end": 2603.391,
    "text": "It's the difference between the current belief and after observing this observation, what would be my belief if I would throw out all the previous observation and make a new belief?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2603.932,
    "end": 2606.133,
    "text": "So it's a next belief, but",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2607.366,
    "end": 2608.808,
    "text": " with the assumption that there was a reset.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2608.868,
    "end": 2613.294,
    "text": "Just for comparison between the confidence corrected surprise and Bayesian surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2613.935,
    "end": 2618.521,
    "text": "In the Bayesian surprise, I had a difference between the current belief and the next actual updated belief.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2619.362,
    "end": 2625.09,
    "text": "But for the confidence corrected surprise is the current belief with the reset updated belief.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2627.53,
    "end": 2643.508,
    "text": " The reason that is interesting is that in Proposition 9 of the paper, we show that this definition can be written in this way, where the first term is different in Shannon's surprise, the second term is kind of different in Bayesian surprise, and the third term is a notion of confidence.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2644.505,
    "end": 2647.671,
    "text": " So this one looks like a relative surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2647.851,
    "end": 2651.358,
    "text": "As I argued before, it's good for modulation of learning.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2651.398,
    "end": 2657.129,
    "text": "And that was actually the original plan of Farid Jital to find a surprise definition that is good for surprise modulation.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2659.045,
    "end": 2661.366,
    "text": " And the last term is a notion of confidence.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2662.026,
    "end": 2671.889,
    "text": "And in this preprint online, we show in some situation, it seems more intuitive to have some confidence correction in the definition.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2671.929,
    "end": 2675.59,
    "text": "But I don't want at the moment to enter that discussion.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2675.63,
    "end": 2680.631,
    "text": "I just want to introduce that there are this class of definitions with the confidence correction.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2683.044,
    "end": 2685.306,
    "text": " An overview of all definitions so far.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2686.087,
    "end": 2691.673,
    "text": "The absolute error was the difference between observation and prediction in this class.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2692.654,
    "end": 2695.076,
    "text": "Shannon surprise, how unlikely something is.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2695.797,
    "end": 2702.564,
    "text": "Base factor surprise, how unlikely something is compared to a baseline or prior belief.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2704.258,
    "end": 2714.76,
    "text": " Bayesian surprise, how much I gain information upon observing something, and confidence corrected surprise should include an explicit notion of confidence.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2716.341,
    "end": 2724.522,
    "text": "Having these five definitions, now we solve one definition per row and one definition, or at least one definition per column.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2725.923,
    "end": 2727.943,
    "text": "Now I can introduce our taxonomy.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2730.72,
    "end": 2735.684,
    "text": " On the screen now you see in a seemingly arbitrary order these definitions that I had here.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2736.846,
    "end": 2740.909,
    "text": "And we propose to label these different definitions based on the quantity they measure.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2742.11,
    "end": 2744.913,
    "text": "This group is what we call a prediction surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2745.213,
    "end": 2748.156,
    "text": "They care about how good your prediction is about the next observation.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2749.585,
    "end": 2755.007,
    "text": " This group for information gain surprise, they measure how much you learn based on a new observation.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2755.807,
    "end": 2760.568,
    "text": "And this group for change point detection surprise that were some sort of relative measures of surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2761.388,
    "end": 2776.253,
    "text": "And we had these overlapping labeling that no matter whether there is a prediction or change point detection, we can make the effect stronger or we can correct the effect by adding an explicit term for confidence.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2777.515,
    "end": 2783.559,
    "text": " One can ask that, okay, so far I talk about vendor indistinguishable and put them in different category.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2783.959,
    "end": 2786.721,
    "text": "One can ask how different are these definition in practice?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2786.761,
    "end": 2794.986,
    "text": "Because if I pick a random oddball task from the field, look at the data, all these definitions look quite correlated.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2796.207,
    "end": 2801.391,
    "text": "And one may argue that, okay, does it really matter to have different definitions?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2802.711,
    "end": 2802.832,
    "text": "And",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2804.73,
    "end": 2807.572,
    "text": " It is actually possible to design experiments.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2807.812,
    "end": 2809.273,
    "text": "Here, I'm showing one example.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2809.293,
    "end": 2813.295,
    "text": "I'm not going to discuss that what is the experiment we presented in this preprint.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2814.256,
    "end": 2815.857,
    "text": "On the x-axis, you see time.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2816.117,
    "end": 2820.78,
    "text": "On the y-axis, average, surprise, and for definition from these four different categories.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2821.48,
    "end": 2823.422,
    "text": " And here's a change point in the environment.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2823.682,
    "end": 2828.327,
    "text": "And we see after change point, they have a completely different behavior.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2828.467,
    "end": 2833.412,
    "text": "One goes down, one has a spike, one goes down and go up again and down again.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2833.772,
    "end": 2843.041,
    "text": "So there are possibilities to make theory-driven experiments that different definitions of surprise can have very different behavior and different predictions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2844.082,
    "end": 2856.848,
    "text": " Another example is in this preprint, the focus on the role of different surprise definitions on curiosity-driven exploration, that how differently they can drive exploration.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2857.97,
    "end": 2863.094,
    "text": " And there are also many other works that are not from our lab.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2863.894,
    "end": 2868.637,
    "text": "And they focus on the physiological signatures of different definitions of surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2868.697,
    "end": 2872.16,
    "text": "Particularly, this one is in Pilas Computational Biology.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2872.32,
    "end": 2873.681,
    "text": "I found it quite impressive.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2874.161,
    "end": 2881.986,
    "text": "They found signatures in EEG for all three groups of prediction surprise, confidence correction surprise, and information gain surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2883.547,
    "end": 2893.692,
    "text": " a fixed experiment looking at the EEG, they could see that all these three have their own correlates or signatures in the EEG signal.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2896.033,
    "end": 2902.616,
    "text": "So I introduced the mathematical framework and formally discussed many definitions of surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2903.297,
    "end": 2907.839,
    "text": "We had this technical classification and distinguishability conditions and the taxonomy.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2908.868,
    "end": 2910.669,
    "text": " Take-home messages, one.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2911.17,
    "end": 2914.372,
    "text": "Surprise in different experiments refer to different definitions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2915.573,
    "end": 2926.88,
    "text": "These different definitions can be classified based on their dependence on the subject's belief and the quantity they measure, which was the taxonomy or the labeling we proposed.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2928.141,
    "end": 2938.088,
    "text": "And the third message, under a specific condition, some of these definitions are indistinguishable and are argued that it is useful for experiment design and also useful for computational modeling.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2939.096,
    "end": 2953.905,
    "text": " So putting everything together back to this figure that I showed at the beginning of the presentation, our proposition is to replace or at least order these different signals or definitions in a structured way that",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2956.883,
    "end": 2968.027,
    "text": " we can now, if you put it in this structured way, let me rephrase it in this way, then we can talk about that, oh, for example, for exploration, which of these different domains is more relevant?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2968.227,
    "end": 2975.089,
    "text": "Is the belief mismatch to price definitions that are more relevant to exploration or the observation mismatch to price definitions?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2975.189,
    "end": 2979.211,
    "text": "So we are not talking about only specific definitions, but more like categories.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2980.831,
    "end": 2990.196,
    "text": " In addition to this surprise-related signal or these surprise definitions, we argue that there is also a need for novelty signals or novelty definitions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2991.557,
    "end": 2997.159,
    "text": "And we argue in this paper that novelty and surprise are fundamentally different.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 2997.359,
    "end": 3002.362,
    "text": "So everything that I said today would not stand for novelty.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3003.931,
    "end": 3013.658,
    "text": " So I would like to thank my collaborators for this joint work, a very cool lab in Lausanne, and you for your attention.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3013.778,
    "end": 3014.359,
    "text": "Thank you very much.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3024.686,
    "end": 3025.027,
    "text": "Daniel?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 3025.307,
    "end": 3025.727,
    "text": "All right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3026.528,
    "end": 3028.589,
    "text": "Just getting things back.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3029.23,
    "end": 3029.53,
    "text": "Awesome.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3031.175,
    "end": 3056.372,
    "text": " will restore our videos well thanks for that awesome talk if anyone has a uh question in the uh live chat please feel free otherwise ali if you'd like you can re-enable your video and feel free to give a first comment or i'm happy to give a comment but you can go first",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3057.432,
    "end": 3057.993,
    "text": " Yeah, sure.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3058.033,
    "end": 3061.856,
    "text": "But for some reason, I can't activate my camera here.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3061.876,
    "end": 3065.339,
    "text": "Let me check it again.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3065.759,
    "end": 3067.961,
    "text": "Okay.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3068.341,
    "end": 3070.583,
    "text": "So thanks a lot, Anna-Marisa.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3071.484,
    "end": 3079.81,
    "text": "First of all, I congratulate you for writing and, of course, your co-authors for writing such a fascinating paper.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3079.83,
    "end": 3081.792,
    "text": "I truly enjoyed it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3083.416,
    "end": 3091.849,
    "text": " Well, I have some comments, if I may, and also I have maybe a couple of questions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3094.984,
    "end": 3112.65,
    "text": " Your work, actually, these kinds of classification work or the taxonomy is, in my opinion, is really a significant contribution to the whole area of psychological or especially in cognitive sciences, because as",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3115.691,
    "end": 3140.729,
    "text": " Muhammad Ali Khalidi's recent book, Cognitive Ontology, claims this kind of taxonomic approaches has been long debated among neuroscientists, psychologists, and cognitive scientists, especially about the difference between cognitive kinds and natural kinds and whether or how they map onto each other.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3141.632,
    "end": 3152.712,
    "text": " Because, as you also mentioned, we have a folk psychological notion of surprise or, let's say, cognitive kind of surprise, but",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3154.031,
    "end": 3160.998,
    "text": " we're not always clear about how to translate that into precise mathematical terms.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3161.398,
    "end": 3178.714,
    "text": "So I remember in an active inference book by Parpizzulo and Friston, there was about a paragraph or so distinguishing between the folk psychological notion of surprise and the statistical surprise or surprisal functions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3179.908,
    "end": 3182.27,
    "text": " corresponding to various distributions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3182.73,
    "end": 3197.082,
    "text": "So basically, as I see it, your paper is a kind of expansion on that single paragraph to consolidate all the different conceptions of surprises into a whole unified framework.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3197.765,
    "end": 3210.427,
    "text": " We were talking the other day about the importance and significance of the recent move toward applied category theory as a kind of",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3211.133,
    "end": 3219.415,
    "text": " as I like to call it, the conceptual housekeeping, or better still, a kind of cartography of the knowledge train.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3219.475,
    "end": 3233.199,
    "text": "So this kind of research, in my opinion, can be seen in congruence with this recent move toward as...",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3234.019,
    "end": 3246.157,
    "text": " Toby Sinclair Smith would call to do a kind of well-typed science, to be precise and clear about our definitions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3247.496,
    "end": 3256.821,
    "text": " And I also wanted to kind of getting back to that distinction between the cognitive kinds and natural kinds.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3258.561,
    "end": 3266.025,
    "text": "There are different taxonomic approaches in cognitive science addressing these problems.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3266.745,
    "end": 3274.829,
    "text": "But as I understand it, your work here is more in line with the revisionism",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3275.356,
    "end": 3296.328,
    "text": " approach to do these taxonomic, these classification approaches, which basically says the current cognitive taxonomy is not precise enough to adequately address the problem of distinguishing between cognitive kinds and natural kinds.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3296.388,
    "end": 3301.511,
    "text": "So we need more precise formalism to do that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3302.663,
    "end": 3328.244,
    "text": " And it also reminds me of Dalton Sack-DeVadevel's notion of blanket index, introduced in his paper a week, Markov Blankets, because before the publication of that paper, there was much confusion and ambiguities about the exact mathematical definition of the Markov blanket or",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3328.72,
    "end": 3334.263,
    "text": " to put it in other words, to make it more general and more precise.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3334.403,
    "end": 3337.725,
    "text": "What do we mean exactly by Markov blanket?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3338.286,
    "end": 3340.107,
    "text": "So my question is,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3341.658,
    "end": 3357.049,
    "text": " As I see it, so far you have somehow mapped these kinds of different surprises in a kind of discrete space, as you showed in one of your tables.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3358.25,
    "end": 3362.554,
    "text": "So do you see it as a feasible",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3363.314,
    "end": 3381.005,
    "text": " move forward or let's say a feasible move toward more general conception of this framework by conceiving this unified framework as a two-dimensional continuous",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3382.366,
    "end": 3395.452,
    "text": " or n-dimensional continuous manifold of possible definitions of surprises, and maybe even define an index to navigate this continuous manifold space.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3395.933,
    "end": 3407.479,
    "text": "Because in that case, obviously, you could possibly avoid those kind of blank spaces.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3408.835,
    "end": 3430.241,
    "text": " blank spaces you have on your tables and maybe even we can gain a kind of much deeper insight into how exactly those different kinds of surprises relate to each other in a much more continuous sense rather than the discrete sense.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3430.781,
    "end": 3436.063,
    "text": "So do you think this kind of move even feasible from your viewpoint?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3437.899,
    "end": 3439.34,
    "text": " That's a very good point.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3439.5,
    "end": 3441.402,
    "text": "Thanks a lot for all the remarks.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3441.762,
    "end": 3442.602,
    "text": "I enjoyed this very much.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3446.065,
    "end": 3455.452,
    "text": "The way that I define surprise in a general sense was this function or functional that get the cue observation and the belief and map it to a real value number.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3455.492,
    "end": 3460.375,
    "text": "So all the types of functional in this space can be seen as a measure of surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3460.435,
    "end": 3462.997,
    "text": "So the whole space is quite big and",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3464.638,
    "end": 3469.804,
    "text": " So in principle, yes, of course, we can do that.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3470.305,
    "end": 3472.307,
    "text": "But the space is too big, actually.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3472.467,
    "end": 3474.249,
    "text": "We need to put some constraint on it.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3474.79,
    "end": 3484.821,
    "text": "So I think for future work, if one wants to go to that direction, one should say that what are the properties from a supervised definition that I would like to have?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3485.382,
    "end": 3487.263,
    "text": " So put some constraint on this space.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3487.903,
    "end": 3493.985,
    "text": "And then in some sense, parameterize this space to have some interpretation.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3494.005,
    "end": 3500.908,
    "text": "Because at the moment, I'm saying that, OK, let's look at this part of this space that these definitions were there.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3501.188,
    "end": 3503.489,
    "text": "People are already talking about these definitions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3504.169,
    "end": 3508.171,
    "text": "And I would call these definitions, for example, information gain surprise definitions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3508.591,
    "end": 3512.593,
    "text": "So maybe one can really formalize all the definitions that",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3513.493,
    "end": 3537.402,
    "text": " it is possible to call because okay i only talk about these two particular definitions with KL divergence if you replace KL divergence with any measure of distance on the space of probabilities we end up with another information gain surprise definition so i think it is feasible in principle but i think it's a bit of a tedious job",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3538.303,
    "end": 3563.142,
    "text": " and whether how how useful it's going to be is not clear for me because we already have many definitions and as i try to in a way say what justified definition is at the end of the day how useful it is to explain experimental evidence and help us think about cognition justified true belief distribution",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3565.285,
    "end": 3583.311,
    "text": " um thank you alireza want to add a general comment and then bring us back to active inference so in exploring the active inference ontology we've often found this tension where a given term would have one definition very",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3583.911,
    "end": 3591.839,
    "text": " Vogue psychological, very everyday, belief, surprise, observation, sense, action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3592.38,
    "end": 3596.624,
    "text": "These are some of the most common words in any language.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3597.305,
    "end": 3598.626,
    "text": "And then your work...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3600.248,
    "end": 3619.827,
    "text": " started to dissect or characterize do taxonomics on that iceberg and show that the pointer of a given word surprise in this case can actually host a diversity of technical definitions with some subtle",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3621.877,
    "end": 3648.472,
    "text": " contradictions or compatibilities and areas of indistinguishability so i think it really um raises our attention to the horizon and shows that for a lot of terms that people again use in a day-to-day context there may be multiple definitions and there's many implications like there isn't a single way to model action or belief or surprise",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3649.431,
    "end": 3657.737,
    "text": " there are different flavors and even within a flavor, there's going to be different ways to deploy it in a given model or script or modeling context.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3658.177,
    "end": 3661.019,
    "text": "So I think the formal",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3662.168,
    "end": 3690.096,
    "text": " coherences that you're providing are going to go a long way to ensuring that given that kind of pluralism in modeling and statistics which has always been the case that we do have connections that are robust to enable comparison like you did with different empirical setups whether the experimenters understood sh1 versus sh2",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3692.583,
    "end": 3720.961,
    "text": " we can um post hoc essentially as peer review annotate or augment the definitions repartition different ways that they analyzed data and so i i think that really opens up a lot of directions something that you had on several slides but didn't really go into that much was f star so could you add a little bit on free energy",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3721.963,
    "end": 3723.125,
    "text": " What is free energy?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3723.265,
    "end": 3724.266,
    "text": "What's free about it?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3724.367,
    "end": 3725.769,
    "text": "What's energetic about it?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3726.269,
    "end": 3734.841,
    "text": "Why do we use it to help us engage in perception, cognition, action in the free energy principle at active inference?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3739.38,
    "end": 3740.281,
    "text": " Thanks a lot for the question.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3741.101,
    "end": 3758.991,
    "text": "The way that I see free energy in the context of what I discuss about is as long as I talk about exact Bayesian inference and really have my exact update rule for the belief going from the previous observation and previous belief to the exact next belief,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3761.073,
    "end": 3762.834,
    "text": " Free energy doesn't play much of a role.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3763.754,
    "end": 3769.717,
    "text": "Free energy plays a role as soon as I want to talk about approximate inference and go to the variational setting.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3770.458,
    "end": 3779.403,
    "text": "And in that case, basically free energy defines some loss between my approximation and the exact true belief.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3780.403,
    "end": 3784.746,
    "text": "And one can show that this is the upper bound for the Shannon surprise that I defined.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3786.284,
    "end": 3794.328,
    "text": " And the difference between the free energy and this Shannon surprise is how good my approximation is.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3794.769,
    "end": 3800.431,
    "text": "And that's why if I do exact inference, the minimized free energy is equal to the Shannon surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3801.032,
    "end": 3805.894,
    "text": "So in that sense, I cannot really distinguish minimized free energy from Shannon surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3806.495,
    "end": 3810.517,
    "text": "But if I work in the approximate inference setting,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3811.357,
    "end": 3822.505,
    "text": " then minimized free energy is the best approximation of Shannon's surprise that I can get in that framework.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3822.605,
    "end": 3823.165,
    "text": "Awesome.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3828.467,
    "end": 3849.941,
    "text": " just to kind of restate that because it's actually one of the fundamental aspects of the approach taken in active inference if the true state of the world is a gaussian and we're doing exact inference with a gaussian prior we will seek to minimize our shannon surprise we could",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3851.261,
    "end": 3870.644,
    "text": " if the world is not truly a gaussian but it's some other unstated or unstatable distribution but we choose to do variational approximate bayesian inference with a gaussian prior then free energy tracks us doing strictly as well as we could",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3871.69,
    "end": 3898.921,
    "text": " given our choice of prior family and so it operationalizes with approximate bayesian computational techniques these formal definitions which may or may not be tractable in large state spaces or with large amounts of data so it's a super important point and it's almost like there's that iceberg of surprise definitions and then free energy",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3900.384,
    "end": 3917.726,
    "text": " ports us into the approximate which enables us to to get some tractability in settings where an exact bayesian approach is not plausible okay two things first it's even more than that so uh even",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3918.965,
    "end": 3927.597,
    "text": " No matter what is world, I may assume that world is a Gaussian distribution, let's say, but there's some dependency.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3928.117,
    "end": 3935.988,
    "text": "And I still, even though I'm aware of this dependency, I can assume that there's no dependency and do that in my inference.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3936.508,
    "end": 3939.412,
    "text": " So when I think about the world, it is actually true.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3939.852,
    "end": 3944.397,
    "text": "I believe that there are dependencies, but they still do approximate inference.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3944.918,
    "end": 3950.224,
    "text": "So I think when we do variational inference, we may not necessarily care that what is the true distribution.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3950.985,
    "end": 3955.87,
    "text": "We assume the distribution has this form, yet we want to approximate it with something simpler.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3957.371,
    "end": 3958.252,
    "text": " This is the first part.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3958.552,
    "end": 3968.479,
    "text": "And the second part, all the definitions of surprise that I talked about can be applied to this approximate setting as well.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3968.599,
    "end": 3979.306,
    "text": "So I can use the approximate belief that I found with variational inference and just import it in any of the definitions that I had and come up with some definitions of surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3979.786,
    "end": 3986.711,
    "text": "I think what I want to emphasize on is that variational free energy and minimized free energy is one of these particular ones.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3987.411,
    "end": 3991.755,
    "text": " but all of them are still applicable in the approximate variational setting.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 3993.636,
    "end": 3995.397,
    "text": "Thank you for the correction.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3996.058,
    "end": 4011.83,
    "text": "You mentioned variational free energy, and there is expected free energy about future, but also people have proposed variants like the free energy of the expected future, and so it speaks to categories of functionals",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4013.547,
    "end": 4015.248,
    "text": " that do different computations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4016.008,
    "end": 4020.57,
    "text": "And there isn't just one specific functional form.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4020.59,
    "end": 4027.553,
    "text": "They might highlight different aspects as your plot of surprise all through time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4027.793,
    "end": 4029.454,
    "text": "Let's just say it was some stimuli.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4029.474,
    "end": 4040.439,
    "text": "And I think that leads to a next question, which is if we're interested in some biological phenomena",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4042.537,
    "end": 4053.784,
    "text": " How does this work help shape our experimental design, statistical power analysis, discussion section?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4054.004,
    "end": 4070.314,
    "text": "How do the thousands of papers, surely using surprise in a qualitative or a quantitative way, how do we translate this work to increase the rigor and interoperability of those studies?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4072.285,
    "end": 4073.386,
    "text": " That's a very good question.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4073.406,
    "end": 4079.929,
    "text": "And that's somehow the ideal place that I want our paper to be used.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4080.87,
    "end": 4099.24,
    "text": "It makes a framework that we can think in a very formal way that if I wanted to design an experiment to distinguish these two different notions, what was the necessary features that my generative model need to have?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4100.382,
    "end": 4105.993,
    "text": " And how can I make the realization of that generative model in an experimental paradigm?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4106.909,
    "end": 4110.192,
    "text": " What we do, for example, in the preprint is not in the main paper.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4110.252,
    "end": 4111.913,
    "text": "It's in the preprint that is online.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4111.953,
    "end": 4115.196,
    "text": "It's via archive with the same set of authors.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4116.077,
    "end": 4128.006,
    "text": "Then we try to find situations that, for example, it's some election situation that each week before the election, assume that news media tell you that this party is going to win with this probability.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4128.066,
    "end": 4129.207,
    "text": "Then you see the results.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4129.568,
    "end": 4133.411,
    "text": "And we try to make some assumption that what are the features of these experiments.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4133.911,
    "end": 4138.493,
    "text": " And what we see is that, oh, actually, different definitions make very different predictions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4139.293,
    "end": 4146.936,
    "text": "So I cannot give a systematic approach that, oh, OK, this is the algorithm that if you want to design an experiment, just go one by one these steps.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4147.537,
    "end": 4156.62,
    "text": "But I want to say that this framework gives us the tools to think about before doing the experiment how to design it to have the maximal effect.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4157.321,
    "end": 4160.382,
    "text": "In many of these also indistinguishability conditions,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4161.042,
    "end": 4166.065,
    "text": " I say, for example, that if something is flat, these two definitions are indistinguishable.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4166.806,
    "end": 4171.349,
    "text": "The thing is that if that distribution is not flat, the two are distinguishable.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4171.409,
    "end": 4174.231,
    "text": "But the effect size is very small if it is close to flat.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4174.871,
    "end": 4186.439,
    "text": "So the best thing is that if I want to make two definitions of surprise distinguishable in an experiment, then it's best to look at that condition.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4187.327,
    "end": 4190.109,
    "text": " and make the opposite of that condition in my experiment.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4190.849,
    "end": 4194.632,
    "text": "And in that way, I make the effect size as big as possible in some sense.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4195.052,
    "end": 4209.562,
    "text": "It's a very non-rigorous way of talking about the theory, but just intuitively, that's how we design those experiments and could convince our experimental colleague at the moment to run the experiment.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4209.602,
    "end": 4210.423,
    "text": "So we are doing that.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4212.617,
    "end": 4212.977,
    "text": " Awesome.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4213.317,
    "end": 4228.486,
    "text": "Yeah, there's a lot of ways that we could imagine pre-registration of studies and just the development of templates and patterns for statistical analyses so that different kinds of surprise could be highlighted.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4228.686,
    "end": 4235.57,
    "text": "And we could say, if the change point is going to be like this, then you will be able to distinguish",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4237.233,
    "end": 4240.795,
    "text": " with this power, these kinds of outcomes versus another.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4241.356,
    "end": 4242.697,
    "text": "Ali, do you want to question?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4246.339,
    "end": 4255.085,
    "text": "Yeah, actually, this discussion reminds me of Terence Deacon's famous paper, What is Missing from Theories of Information?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4256.305,
    "end": 4256.646,
    "text": "Because",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4258.058,
    "end": 4277.371,
    "text": " In that paper, he also claims that one of the misunderstandings about all the theories of information, or all the relevant theories for that matter, is the assumption that in order for any information to have",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4281.843,
    "end": 4302.596,
    "text": " for any content of information to have any real-world consequences, it must have some substantial properties and must correspond to some real-world situations or something present as an extrinsic entity in some form or another.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4303.037,
    "end": 4305.178,
    "text": "But as he shows in that paper,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4305.658,
    "end": 4316.988,
    "text": " we can regard the information, whether as a kind of probability distribution or any kind of other theoretical construct, as probability.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4318.098,
    "end": 4328.628,
    "text": " exactly conveying the kind of information that we wanted to explore in a much real sense.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4328.888,
    "end": 4336.215,
    "text": "And it's not just pure data or pure abstract information for that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4337.676,
    "end": 4338.457,
    "text": "So for instance,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4339.658,
    "end": 4358.294,
    "text": " Shannon's analysis of information of course is based on the constraint and the entropy, but on the other hand the capacity to convey that information depends on the relation to something that is specifically absent or not produced.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4358.714,
    "end": 4363.198,
    "text": "So this absence or presence of the entities that",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4363.718,
    "end": 4391.424,
    "text": " the information is about that entity or so-called the aboutness of the information is I heard the aboutness of the information but we'll pause and see if he'll rejoin otherwise we can continue okay I'm just gonna",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4393.919,
    "end": 4398.885,
    "text": " Do you want to provide a thought on Shannon?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4398.905,
    "end": 4407.214,
    "text": "That's actually a very important point that Ali was about to talk about, which is in all these...",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4408.7,
    "end": 4421.189,
    "text": " Definition that I propose, we didn't really care about what is Y or what is X. We were working in these very abstract states and very abstract framework.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4421.55,
    "end": 4425.333,
    "text": "One may say that, what if I don't care about these observations at all?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4425.993,
    "end": 4427.774,
    "text": " And I still may have some predictions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4428.134,
    "end": 4433.377,
    "text": "And so this aboutness that you were talking about is quite important.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4433.677,
    "end": 4444.442,
    "text": "And no matter how confident I am or what is my prediction, how much I care about that problem is quite important on how surprised I am.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4444.642,
    "end": 4448.304,
    "text": "If I don't care about it at all, then I may not feel surprised at all.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4450.225,
    "end": 4455.508,
    "text": "So that's something that's really missing from the whole framework that how we can model",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4459.578,
    "end": 4478.026,
    "text": " The other things that have, let's say, semantic, because at the moment all the framework is semantic-free, how can we include some semantic into this framework and talk about... Then we have another dimension to talk about surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4480.146,
    "end": 4484.028,
    "text": "Yes, and some hope that with...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4485.302,
    "end": 4512.409,
    "text": " progressive syntax schemes we can partition that semantics into other surprise definitions but the bigger question is what are we surprised about and i think that connects to these other cognitive phenomena slash complex word complexes like attention confidence salience um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4513.334,
    "end": 4540.075,
    "text": " and I mean just to kind of put a point on that like when surprise is being used as an imperative for optimization and inference or merely as a signal that plays into action selection your robot or ant or person or software is going to do different things depending on the surprise formalisms used",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4541.221,
    "end": 4551.263,
    "text": " And knowing many flavors of formalism, it would be awesome if we could have a software package that computes 18 different numbers.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4552.163,
    "end": 4553.163,
    "text": "But then what would we do?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4553.383,
    "end": 4554.283,
    "text": "We could choose one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4554.603,
    "end": 4555.663,
    "text": "We could blend them.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4555.743,
    "end": 4557.424,
    "text": "We could make summary statistics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4557.944,
    "end": 4563.105,
    "text": "We could say that we're in zone A where these five are aligned, but this one isn't.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4563.625,
    "end": 4565.565,
    "text": "And then what would we do with that action?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4565.905,
    "end": 4569.966,
    "text": "How surprised would we be by that location in phase space?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4571.078,
    "end": 4586.364,
    "text": " So it's like a very interesting top-down situation, independent phrasing of those relationships, but in the implementation, it's always going to come down to what the modeler chooses.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4587.445,
    "end": 4590.706,
    "text": "There just isn't a general answer to be found in the specifics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4595.928,
    "end": 4597.969,
    "text": "One other thought.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4598.877,
    "end": 4605.761,
    "text": " or area to explore is you mentioned the Bayes factor and its role in learning.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4606.401,
    "end": 4612.684,
    "text": "So could you speak to optimal Bayesian inference and optimal learning?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4616.787,
    "end": 4617.387,
    "text": "What do you mean?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4617.547,
    "end": 4618.668,
    "text": "Sorry.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 4618.708,
    "end": 4620.689,
    "text": "Like Bayes optimal inference?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4622.168,
    "end": 4633.055,
    "text": " OK, that's what I meant that Bayes factor surprise appear in Bayes optimal learning and also appear in the variational Bayesian update rule.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4633.955,
    "end": 4647.804,
    "text": "And that was actually what we found cool about it, because neuroscientists, particularly people talking about synoptic plasticity, talked about surprise and how neuromodulators influence synoptic plasticity.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4648.946,
    "end": 4661.774,
    "text": " And we saw that looking at the Bayesian framework, we can find that such a normative modulation based on surprise exists in how the belief is updated.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4662.515,
    "end": 4668.018,
    "text": "And it is funny that the same thing appears in lots of approximate inference schemes as well.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4669.379,
    "end": 4674.604,
    "text": " And it seems necessary to use such a surprise measures to modulate learning as we learn.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4674.804,
    "end": 4677.146,
    "text": "And there are lots of experimental evidence for it.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4679.308,
    "end": 4695.542,
    "text": "So what I want to say is not necessarily only about the exact Bayesian inference, but it is something that appeared there and is useful for also other approximation to exact inference.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4696.894,
    "end": 4704.698,
    "text": " So what does optimal learning in this case mean?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4707.739,
    "end": 4715.883,
    "text": "Optimal learning means to find the exact posterior given the model for those parameters.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4716.503,
    "end": 4720.545,
    "text": "Because if the model is true, if I believe that this is the true model,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4722.4,
    "end": 4730.35,
    "text": " All the information that I can have about the hidden variable is in the posterior distribution of that variable given the observations.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4731.371,
    "end": 4740.318,
    "text": " And basically what we mean by learning is to updating this posterior distribution from one time to another.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4740.398,
    "end": 4747.484,
    "text": "So as we go on and add more and more data points, how we track this distribution over time and how we update our belief.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4748.225,
    "end": 4754.77,
    "text": "And by optimal learning, we mean the exact Bayesian update from time t to t plus one.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4755.511,
    "end": 4758.033,
    "text": "But this could be done also with variational inference.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4759.465,
    "end": 4765.363,
    "text": " The cool thing was that also bank variational inference would benefit from such modulation.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4769.093,
    "end": 4790.392,
    "text": " so one thought on that in a little bit of a meta science area is bayesian statistics is becoming more prevalent and one way of reporting the effect of a treatment would be like with a bayes factor to present the relative evidence for one model versus another and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4792.253,
    "end": 4799.036,
    "text": " Something that that plays a functional similarity with, but of course a technical difference, is a p-value.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4799.876,
    "end": 4809.28,
    "text": "And so in the presentation, I thought about how the Bayes factor was associated with learning as well as we could.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4810.256,
    "end": 4814.838,
    "text": " representing the evidence for two different possibilities in a ratio.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4815.658,
    "end": 4825.721,
    "text": "Whereas a p-value, especially when we put a discontinuity 0.05, is like a change point.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4826.962,
    "end": 4832.784,
    "text": "It's like we're sampling from the experimental outcomes and eggs are healthy, eggs are healthy, eggs are healthy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4833.344,
    "end": 4834.865,
    "text": "Now there's a difference between the groups.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4836.058,
    "end": 4859.838,
    "text": " so that puts the task of science and learning into a change point detection framework where oh now we're in the world where this is the case versus the sort of unrolling Bayesian approach where we get some experimental results",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4861.118,
    "end": 4879.234,
    "text": " that are all over the posterior, but merely drawing results from a potentially quite dispersed posterior doesn't mean that we've entered any change point with respect to the efficacy of a given treatment.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4887.881,
    "end": 4888.361,
    "text": "I'm trying to...",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4890.475,
    "end": 4912.401,
    "text": " connect what you said about science in general to what you were thinking, just as a point that the base factor surprise, we really took the word base factor from the field of statistics for the nature of hypothesis testing that exists in this kind of change point detection, because at each point you want to see that, oh, okay, was there really a change in the environment or not?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4912.841,
    "end": 4914.802,
    "text": "And for example, for this example,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4915.976,
    "end": 4918.918,
    "text": " Weather forecast in the pandemic.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4919.378,
    "end": 4925.482,
    "text": "I really had this before knowing that this is really a phenomenon and weather forecasts get less accurate.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4926.403,
    "end": 4930.185,
    "text": "For a couple of weeks, I was confused that, okay, what's going on?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4930.285,
    "end": 4934.688,
    "text": "Why my metal suites is not working well anymore.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4936.129,
    "end": 4945.455,
    "text": "So it's always some sort of hypothesis testing that, okay, am I living in the same world that it was yesterday or something changed from yesterday?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4946.178,
    "end": 4947.82,
    "text": " So there is some hypothesis testing.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4948.42,
    "end": 4952.904,
    "text": "And in the Bayesian setting, it's done by comparing two models through Bayes factor.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4953.405,
    "end": 4961.552,
    "text": "But we can also have, even in the Bayesian setting, if we go again towards approximate Bayesian setting and sampling, like with particle filtering,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4962.293,
    "end": 4963.815,
    "text": " we sample these change points.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4963.875,
    "end": 4971.324,
    "text": "So at some point we make decisions that, oh, this is the probability of, for example, there's a change or not.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4971.384,
    "end": 4972.505,
    "text": "At some point I should decide.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4973.166,
    "end": 4974.888,
    "text": "I would say, yeah, there is a change.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4975.088,
    "end": 4978.813,
    "text": "And from today onward, I behave as if something changed yesterday.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4980.234,
    "end": 4982.217,
    "text": "And in a frequentist approach,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 4986.095,
    "end": 5000.061,
    "text": " It's a bit difficult for me to include things like p-value in the framework that I was talking about because the nature of p-value is in talking about probability of things more extreme than things that already happened.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5000.521,
    "end": 5003.443,
    "text": "While in this Bayesian setting, we are talking about this particular observation.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5003.503,
    "end": 5010.326,
    "text": "I don't care about the values higher than observation, lower than observation, different from... I care about only this observation.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5010.386,
    "end": 5015.288,
    "text": "While in p-value, we compute the probability of observation more extreme than this one.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5016.406,
    "end": 5042.312,
    "text": " and um and i don't see how it can easily fit in the framework but if i want to find the closest measure to say that it's a bit similar to p-value i would say it's something like shannon's surprise because it doesn't have the nature of comparison you just say that under the current belief which let's say that the null hypothesis the current hypothesis how unlikely is the new observation",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5047.847,
    "end": 5065.3,
    "text": " yes one could certainly apply a change point base factor over three means we publish the paper base factor under three is no um evidence and there's definitely some subtlety there with what the p statistic and other",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5066.23,
    "end": 5089.098,
    "text": " empirically calculated statistics tell us um I'll read one question from the chat then Ali so Dave writes have folks working on these problems model the situation where counter evidence has no impact on confidence delusion and cognitive dissonance are terms that come into that discussion",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5092.833,
    "end": 5094.074,
    "text": " I'm sorry, can you read again?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5094.134,
    "end": 5098.936,
    "text": "Because I opened the page to read it myself as well, but then the voice got a bit... All good.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5099.296,
    "end": 5105.399,
    "text": "Have folks working on these problems modeled the situation where counter evidence has no impact on confidence?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5106.059,
    "end": 5110.281,
    "text": "Delusion and cognitive dissonance are terms that come into that discussion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5114.203,
    "end": 5117.505,
    "text": "That counter evidence doesn't have effect on confidence?",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5120.696,
    "end": 5127.259,
    "text": " Yes, potentially where surprising beliefs are continually entertained.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5129.08,
    "end": 5140.525,
    "text": "How could we model a situation where persistent cognitive dissonances or delusive beliefs are stabilized?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5147.648,
    "end": 5159.436,
    "text": " Okay, about cognitive... When I talk about hypothesis testing in a Bayesian setting, the fun thing is that all hypotheses live at the same time.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5159.556,
    "end": 5161.378,
    "text": "You just change the weight over them.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5161.878,
    "end": 5165.4,
    "text": "And some of these hypotheses are in disagreement with each other.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5166.501,
    "end": 5172.125,
    "text": "So these counterfactual assumptions that, oh, what if there was a change a while ago?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5172.365,
    "end": 5176.128,
    "text": "Or what if... Oh, wait.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5179.113,
    "end": 5182.594,
    "text": " No, the previous observations are fixed.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5182.734,
    "end": 5188.797,
    "text": "I have this imagination only in the unobserved states.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5189.577,
    "end": 5195.419,
    "text": "And when I talk about different scenarios, I talk about unobserved states.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5199.701,
    "end": 5205.063,
    "text": "I think the question was about if I want to think about counterfactuals,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5206.34,
    "end": 5211.063,
    "text": " then I should imagine that how would the world be if the observation was different?",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5212.083,
    "end": 5214.565,
    "text": "And that at the moment is not included in the framework.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5215.525,
    "end": 5221.469,
    "text": "But one could think about it if we would integrate the framework with a bit of causal inference.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5222.39,
    "end": 5232.976,
    "text": "And in those settings, we have this counteractual and talking about, now I don't only condition in the past, I imagine that some of the observation is very different, and now I do my inference.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5234.161,
    "end": 5238.463,
    "text": " But I'm not sure if I got the question and if my answer was relevant.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5238.543,
    "end": 5241.524,
    "text": "I hope it is related.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5243.325,
    "end": 5243.625,
    "text": "Awesome.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5243.685,
    "end": 5252.628,
    "text": "And on that response, Ian wrote, cognitive dissonance, I think there's something to do with having different beliefs at different levels of consciousness.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5253.188,
    "end": 5254.469,
    "text": "So yes, these...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5255.973,
    "end": 5275.511,
    "text": " taxonomies were presented at a single level model as presented and with nested models we might have um different interactions amongst surprise definitions maybe even different surprise definitions playing out within a nested Bayesian architecture",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5277.711,
    "end": 5293.901,
    "text": " Maybe one type of surprise is more amenable to edge detection in the retina, and then in a different part of the generative model, there's another operationalization of surprise in a decision-making or in a risk-aversive setting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5295.729,
    "end": 5303.55,
    "text": " Of course, that's definitely the case, that different level of processing may work with different levels of surprise.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5304.151,
    "end": 5313.992,
    "text": "And maybe, I don't know, belief mismatch surprise definitions are not relevant for the primary part of cortex at all.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5314.892,
    "end": 5319.273,
    "text": "But if that's the case, yeah, I understand.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5319.353,
    "end": 5323.674,
    "text": "But maybe I misunderstood what they mean by cognitiveness.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5326.656,
    "end": 5329.137,
    "text": " Ali, any more thoughts or questions?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5329.177,
    "end": 5333.498,
    "text": "Maybe we can each have one more question or contribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5334.678,
    "end": 5338.9,
    "text": "Yeah, actually, first of all, I apologize.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5339.16,
    "end": 5342.381,
    "text": "I got disconnected in the middle of my question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5342.861,
    "end": 5348.102,
    "text": "I don't know how much I could explain.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5348.562,
    "end": 5352.624,
    "text": "All right, so one thing about these kind of nested hierarchies,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5354.404,
    "end": 5369.088,
    "text": " is that in the interactivist framework, they usually define different kinds of information as different levels of emergence in a kind of nested hierarchy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5369.988,
    "end": 5379.53,
    "text": "The biggest of all is the Shannon information, which deals with the kind of medium capacity of conveying the information and then",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5380.921,
    "end": 5388.868,
    "text": " On another emergent level is the referential information, which is about the aboutness.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5390.369,
    "end": 5396.895,
    "text": "And at the highest emergent level, we have the significant information or",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5398.137,
    "end": 5406.406,
    "text": " as they would like to call the usefulness or the semantic content of the information, as they say.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5406.906,
    "end": 5414.614,
    "text": "So my question is that this kind of taxonomic approach that you",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5417.432,
    "end": 5435.607,
    "text": " provided in this paper, how can we map this taxonomic system or even whether we can map this kind of taxonomic system into this kind of nested hierarchies of different information?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5435.647,
    "end": 5443.774,
    "text": "Because obviously we can probably even say that the nature of these different kinds of informations are different, right?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5447.087,
    "end": 5457.65,
    "text": " My short answer would be the table that I showed, if we want to account for the things that you discussed, we should make a few of those tables.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5457.67,
    "end": 5459.171,
    "text": "So we should add another dimension.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5459.971,
    "end": 5466.573,
    "text": "So I don't think it's really possible to map these particular definitions to those different categories.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5469.414,
    "end": 5470.494,
    "text": "And for the long answer,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5472.423,
    "end": 5479.904,
    "text": " I give again another short answer, which is basically we need to make a new framework to really include all these semantics.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5480.324,
    "end": 5486.865,
    "text": "Our current probabilistic framework, the one that I presented today, doesn't have the capacity to account for that.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5488.826,
    "end": 5492.746,
    "text": "That's why I mentioned that n-manifold thing.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5493.006,
    "end": 5500.468,
    "text": "I said in my first question, because I was thinking of something similar, because maybe...",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5502.372,
    "end": 5514.615,
    "text": " in order to generalize this framework, we would need a lot more dimensions than just the two dimensions that we have already provided in the paper.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5514.655,
    "end": 5530.898,
    "text": "So I don't know if, as you say, it would be useful for any future research or whether it would be any kind of tractable computational framework, but",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5532.375,
    "end": 5547.108,
    "text": " at least in my opinion, or from a purely theoretical point of view, maybe it's worth investigating to see how these kinds of different frameworks integrate into",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5547.728,
    "end": 5553.291,
    "text": " a much more generalized way of thinking about information and surprise.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5554.131,
    "end": 5562.416,
    "text": "But as I said, your paper is a very important and significant contribution in that regard.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5565.39,
    "end": 5578.484,
    "text": " oh in near closing the taxonomy notion it's a classification scheme and just as in biology",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5579.781,
    "end": 5602.362,
    "text": " early natural history observations of different things led to taxonomics and the classification of different things and on through phylogenetics and understanding the the temporal relationships so today we have a taxonomy for surprise that we didn't have a year ago",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5605.169,
    "end": 5616.849,
    "text": " What can be contributed by your group and by others in this emerging ecology of formalisms?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5618.57,
    "end": 5647.974,
    "text": " if we have the taxonomy today just a sketch napkin sketch two by two grid we sorted the birds that people saw into roughly these categories they don't even know that it was this bird but now we can disambiguate it was that bird so where will you be heading in this space and where do you think there are areas for people to contribute to this living area",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5649.875,
    "end": 5651.696,
    "text": " That's a very good point, a very good question.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5652.876,
    "end": 5664.861,
    "text": "There was this slide that I had these different four domains of learning, exploration, memory, and physiological signals that people talk about surprise in neuroscience and psychology.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5664.901,
    "end": 5666.602,
    "text": "In the middle, these different definitions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5667.462,
    "end": 5671.324,
    "text": "I think if you want to go beyond the taxonomy,",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5672.449,
    "end": 5683.795,
    "text": " we should try to do more computational modeling and see that which of these definitions, which part of these two degrees is related to which of these functions.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5684.355,
    "end": 5693.8,
    "text": "So basically, now I'm just proposing some tools and a framework which can be used later on for computational modeling.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5693.86,
    "end": 5696.602,
    "text": "So all these definitions can be useful or not.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5697.202,
    "end": 5706.467,
    "text": " for some modeler to explain exploration in some task, or learning in some other task, or segmentation in some task.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5706.968,
    "end": 5714.612,
    "text": "The next step is really finding out where and in what function this definition makes sense.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5718.615,
    "end": 5737.428,
    "text": " Yes, you brought up the novelty, curiosity, learning, and surprise is woven sometimes technically or not into those discussions, but it's not all definitions of surprise for all definitions of learning, salience, attention, novelty.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5738.208,
    "end": 5742.711,
    "text": "So disambiguating that space is very important.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5745.073,
    "end": 5746.654,
    "text": "Ali, any final remarks?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5749.073,
    "end": 5750.794,
    "text": " I would like to thank you again for inviting me.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5751.435,
    "end": 5756.218,
    "text": "And thanks a lot for all the discussion and nice question.",
    "speaker": "SPEAKER_03"
  },
  {
    "start": 5757.038,
    "end": 5757.659,
    "text": "Oh, awesome.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5757.799,
    "end": 5761.441,
    "text": "It's a great honor and a learning experience for us.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5762.042,
    "end": 5765.684,
    "text": "So you're always welcome back till next time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 5767.665,
    "end": 5768.826,
    "text": "You really appreciate it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5769.046,
    "end": 5769.927,
    "text": "Thanks, Andy Reza.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5770.607,
    "end": 5770.967,
    "text": "Thank you.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5771.107,
    "end": 5771.728,
    "text": "Have a nice day.",
    "speaker": "SPEAKER_02"
  },
  {
    "start": 5771.968,
    "end": 5772.168,
    "text": "Bye-bye.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5772.188,
    "end": 5772.528,
    "text": "Farewell.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5772.788,
    "end": 5773.029,
    "text": "Bye.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 5773.589,
    "end": 5773.729,
    "text": "Bye.",
    "speaker": "SPEAKER_01"
  }
]
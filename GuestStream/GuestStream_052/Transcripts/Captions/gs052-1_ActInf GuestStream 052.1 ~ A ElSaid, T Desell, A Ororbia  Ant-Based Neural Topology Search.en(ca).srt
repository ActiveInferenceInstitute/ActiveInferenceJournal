1
00:00:00,170 --> 00:00:09,306
Hello and welcome. This is active guest

3
00:00:09,338 --> 00:00:12,846
room 52.1. On August 10, 2023,

4
00:00:12,948 --> 00:00:15,594
we have Ahmed Elsaid, Alexander Ororbia

5
00:00:15,642 --> 00:00:18,814
and Travis Desel. Ahmed is going to give

6
00:00:18,852 --> 00:00:21,374
a presentation and following we will

7
00:00:21,412 --> 00:00:23,562
have some reflections and discussions.

8
00:00:23,626 --> 00:00:26,514
So thank you all for joining and Ahmed

9
00:00:26,562 --> 00:00:28,150
to you for the presentation.

10
00:00:30,490 --> 00:00:33,560
Okay, thanks so much for having me.

11
00:00:34,170 --> 00:00:39,802
Today I'm going to discuss the

12
00:00:39,856 --> 00:00:42,714
methods that we came up to solve new

13
00:00:42,752 --> 00:00:44,534
art, texture, search and neuralvolution

14
00:00:44,582 --> 00:00:48,746
problems. The methods that are ant

15
00:00:48,768 --> 00:00:51,470
colony optimization based solutions.

16
00:00:51,970 --> 00:00:55,390
So as the title here is mentioning,

17
00:00:56,130 --> 00:00:58,334
ant colony optimization for neural text

18
00:00:58,372 --> 00:01:02,240
research and neurovolution. This work is

19
00:01:03,410 --> 00:01:06,970
collaboration between me, my previous

20
00:01:07,050 --> 00:01:09,986
advisor, Dr. Travis Desel, and my co

21
00:01:10,088 --> 00:01:14,210
advisor Dr. Alexander Auroravian.

22
00:01:15,370 --> 00:01:18,840
I'm currently an assistant professor in

23
00:01:22,250 --> 00:01:23,314
University of North Carolina,

24
00:01:23,362 --> 00:01:24,230
wilmington.

25
00:01:26,570 --> 00:01:30,406
And moving on to the next slide. So as

26
00:01:30,428 --> 00:01:32,054
an overview, the things that I'm going

27
00:01:32,092 --> 00:01:35,910
to discuss today, I'll try to give

28
00:01:36,060 --> 00:01:38,166
Bird Eyes view for what is

29
00:01:38,188 --> 00:01:41,870
neurovolution, why we what, why we

30
00:01:41,940 --> 00:01:45,054
need it. And from there, I'll just try

31
00:01:45,092 --> 00:01:49,646
to discuss our methods that

32
00:01:49,668 --> 00:01:51,610
is based on end calling optimization

33
00:01:51,690 --> 00:01:53,614
which is called N based neurotropology

34
00:01:53,662 --> 00:01:56,802
search or ants for short. And after that

35
00:01:56,856 --> 00:02:00,402
I will discuss how we advanced with this

36
00:02:00,456 --> 00:02:05,310
idea by introducing

37
00:02:05,390 --> 00:02:07,460
continuous ants or cans for short,

38
00:02:08,470 --> 00:02:11,278
which could spread off the discrete

39
00:02:11,294 --> 00:02:14,006
search space and replace it with a

40
00:02:14,028 --> 00:02:16,680
continuous search space. And after that

41
00:02:17,210 --> 00:02:19,946
I'm going to also say what we did with

42
00:02:19,968 --> 00:02:21,654
the three dimensions and the continuous

43
00:02:21,702 --> 00:02:25,174
ants to have back propagation free cans.

44
00:02:25,302 --> 00:02:28,586
And later I will discuss three of the

45
00:02:28,608 --> 00:02:30,720
points that we are considering for

46
00:02:31,170 --> 00:02:32,240
future work.

47
00:02:33,970 --> 00:02:37,326
So in the machine learning

48
00:02:37,508 --> 00:02:40,954
learn, as the neural network

49
00:02:41,002 --> 00:02:43,150
structures got deeper and deeper,

50
00:02:43,670 --> 00:02:45,746
people were trying to optimize the

51
00:02:45,768 --> 00:02:47,406
structures to have better performance.

52
00:02:47,518 --> 00:02:50,338
And people in different realms or

53
00:02:50,424 --> 00:02:54,002
different problem domain used

54
00:02:54,056 --> 00:02:57,618
to borrow the best performing

55
00:02:57,794 --> 00:03:00,790
architectures or structures and try to

56
00:03:00,940 --> 00:03:04,262
modify a little bit to work for their

57
00:03:04,316 --> 00:03:07,590
problem. And they try to tweak some of

58
00:03:07,740 --> 00:03:10,694
the features of the structure and

59
00:03:10,812 --> 00:03:14,154
compare these different tweaks. And then

60
00:03:14,192 --> 00:03:15,514
they say that we found the best

61
00:03:15,552 --> 00:03:19,498
performing structure. But to actually

62
00:03:19,584 --> 00:03:23,390
find an absolute optimum structure

63
00:03:24,770 --> 00:03:26,526
concerning that solution, it would be an

64
00:03:26,548 --> 00:03:28,654
NP hard problem because to reach that

65
00:03:28,692 --> 00:03:32,914
solution, they had to try

66
00:03:32,952 --> 00:03:34,786
all the combinations of the different

67
00:03:34,888 --> 00:03:36,500
structural elements, right?

68
00:03:38,310 --> 00:03:41,630
Because we have massive structures

69
00:03:41,710 --> 00:03:44,050
in these deep neural networks,

70
00:03:44,970 --> 00:03:47,926
it could be an NPR problem because we

71
00:03:47,948 --> 00:03:51,110
don't have computational power or enough

72
00:03:51,180 --> 00:03:54,582
time to actually train

73
00:03:54,636 --> 00:03:57,298
and test all these structures, all these

74
00:03:57,484 --> 00:03:59,306
structures constructed from these

75
00:03:59,408 --> 00:04:01,846
different combinations of structure

76
00:04:01,878 --> 00:04:04,234
elements. So the alternative way to do

77
00:04:04,272 --> 00:04:07,226
this is to actually try to apply a

78
00:04:07,248 --> 00:04:11,286
meteoristic method to convert

79
00:04:11,318 --> 00:04:13,582
to a near to upper solution that is much

80
00:04:13,636 --> 00:04:16,186
better than not optimizing the structure

81
00:04:16,218 --> 00:04:18,554
or relying on kind of like a random

82
00:04:18,602 --> 00:04:20,654
search, right? A random search can get

83
00:04:20,692 --> 00:04:23,614
us better performing neural network,

84
00:04:23,742 --> 00:04:26,994
but it's not going to give us a near

85
00:04:27,032 --> 00:04:29,138
to optimal solution or to converge to a

86
00:04:29,144 --> 00:04:30,610
near to optimal solution.

87
00:04:34,630 --> 00:04:36,726
So a minoristic method would give us an

88
00:04:36,748 --> 00:04:40,614
automated method and

89
00:04:40,652 --> 00:04:44,258
also would converge to optimum structure

90
00:04:44,354 --> 00:04:46,086
new to optimum solution to this

91
00:04:46,188 --> 00:04:47,320
structure problem.

92
00:04:49,950 --> 00:04:53,238
So the way that people approached Nas

93
00:04:53,334 --> 00:04:57,542
was by trying to mimic how optimization

94
00:04:57,606 --> 00:05:02,094
is done in nature, right? So the

95
00:05:02,132 --> 00:05:06,574
first way they thought of it was trying

96
00:05:06,612 --> 00:05:10,954
to mimic how living organisms

97
00:05:11,082 --> 00:05:14,862
evolve in nature using genetic

98
00:05:14,926 --> 00:05:16,974
based algorithm like the Darwinian

99
00:05:17,022 --> 00:05:19,090
Genetic Evolution.

100
00:05:20,710 --> 00:05:22,770
It started with Neat,

101
00:05:25,850 --> 00:05:30,390
it's a short for Neuralution

102
00:05:31,930 --> 00:05:36,310
of Augmenting topologies and

103
00:05:36,460 --> 00:05:39,900
it relied on genetic algorithms where

104
00:05:40,670 --> 00:05:44,422
also that concept also applied

105
00:05:44,566 --> 00:05:47,830
in most of the Nas and Neural evolution

106
00:05:47,910 --> 00:05:50,478
methods and Exam is one of them.

107
00:05:50,564 --> 00:05:54,238
Travis came up with this method and

108
00:05:54,324 --> 00:05:58,062
it became one of the state of the art

109
00:05:58,116 --> 00:06:01,310
methods in Nas.

110
00:06:02,450 --> 00:06:06,346
So in such methods we tried

111
00:06:06,388 --> 00:06:09,554
to again mimic genetic evolution by

112
00:06:09,592 --> 00:06:11,554
introducing new structural elements or

113
00:06:11,592 --> 00:06:13,806
removing structural elements or altering

114
00:06:13,838 --> 00:06:15,274
the structure through the evolution

115
00:06:15,342 --> 00:06:17,042
process, through the evolution

116
00:06:17,106 --> 00:06:18,834
iterations and through the evolution

117
00:06:18,882 --> 00:06:22,354
generations. So we can apply mutations

118
00:06:22,482 --> 00:06:26,466
by splitting edges or adding

119
00:06:26,498 --> 00:06:29,750
edges or disabling edges,

120
00:06:29,830 --> 00:06:33,322
and we disable edges or disable some

121
00:06:33,376 --> 00:06:35,862
structure elements so that we don't lose

122
00:06:35,926 --> 00:06:38,122
that component, so that we can later on

123
00:06:38,176 --> 00:06:41,958
use it. Kind of like a dormant gene in

124
00:06:41,984 --> 00:06:44,266
a genome, right? So that it can appear

125
00:06:44,298 --> 00:06:46,382
in later generations. Not to get rid of.

126
00:06:46,436 --> 00:06:49,834
Totally. So in mutations we can disable

127
00:06:49,882 --> 00:06:52,302
edges, we can enable them in later

128
00:06:52,356 --> 00:06:54,626
generations. If we found that we want to

129
00:06:54,648 --> 00:06:56,580
try this option,

130
00:06:58,070 --> 00:07:00,066
we can also add recurrent edges or

131
00:07:00,088 --> 00:07:03,342
remove or enable or disable

132
00:07:03,406 --> 00:07:06,598
recurrent edges. We can split nodes, we

133
00:07:06,604 --> 00:07:09,030
can take some of the nodes in a previous

134
00:07:09,370 --> 00:07:12,566
generation and we can just

135
00:07:12,668 --> 00:07:14,966
split it to two nodes and then take the

136
00:07:14,988 --> 00:07:18,220
edges connected to that node and

137
00:07:18,670 --> 00:07:21,002
try to divide it between the nodes that

138
00:07:21,056 --> 00:07:23,146
was generated from the previous node in

139
00:07:23,168 --> 00:07:25,610
the previous generation.

140
00:07:27,870 --> 00:07:31,782
Also we can add nodes to

141
00:07:31,856 --> 00:07:34,062
the structure. So all of these are part

142
00:07:34,116 --> 00:07:36,858
of the mutation process. In the genetic

143
00:07:37,034 --> 00:07:41,406
process we can also disable a node if

144
00:07:41,428 --> 00:07:43,538
we want to try to just get rid of one of

145
00:07:43,544 --> 00:07:45,330
the nodes and disabling a node. Or

146
00:07:45,400 --> 00:07:47,554
multiple nodes will also disable the

147
00:07:47,592 --> 00:07:51,150
edges connected to that node beside

148
00:07:51,230 --> 00:07:56,006
mutations. The other side of a

149
00:07:56,028 --> 00:07:58,546
genetic process is to do crossovers

150
00:07:58,578 --> 00:08:00,262
where we have two of the best

151
00:08:00,316 --> 00:08:03,814
performance population meet

152
00:08:03,852 --> 00:08:06,006
together to bring an offspring and the

153
00:08:06,028 --> 00:08:08,746
offspring will have collection of the

154
00:08:08,768 --> 00:08:11,802
characteristics coming from their

155
00:08:11,856 --> 00:08:13,978
parents. So it will take some of the

156
00:08:13,984 --> 00:08:15,674
characteristics from that parent, some

157
00:08:15,712 --> 00:08:17,914
other characteristics from the other

158
00:08:17,952 --> 00:08:22,366
parent, hoping that this would give us a

159
00:08:22,388 --> 00:08:24,622
better performing neural network or

160
00:08:24,676 --> 00:08:26,110
better performing generation.

161
00:08:28,450 --> 00:08:31,680
So the main problem sorry,

162
00:08:33,910 --> 00:08:37,726
so the main problem with genetic

163
00:08:37,758 --> 00:08:39,794
based algorithms is that they start with

164
00:08:39,832 --> 00:08:42,322
minimal structures like we can see

165
00:08:42,376 --> 00:08:45,082
there, meaning that inputs and outputs

166
00:08:45,246 --> 00:08:49,254
and starting from the optimization with

167
00:08:49,292 --> 00:08:52,386
this minimal search space can trap

168
00:08:52,418 --> 00:08:55,398
the method in a local minima through the

169
00:08:55,404 --> 00:08:57,286
optimization process. So we were

170
00:08:57,308 --> 00:08:59,910
thinking how to get rid of this obstacle

171
00:09:00,250 --> 00:09:03,106
by having a bigger or larger search

172
00:09:03,148 --> 00:09:05,066
space to start with, and then we can

173
00:09:05,088 --> 00:09:09,962
sample some solutions from that large

174
00:09:10,016 --> 00:09:12,910
surf space. And we were looking around

175
00:09:12,980 --> 00:09:15,422
and we concerned end calling

176
00:09:15,476 --> 00:09:19,102
optimization, and I will say why, but

177
00:09:19,156 --> 00:09:22,254
I'll try to introduce the method first.

178
00:09:22,292 --> 00:09:24,818
So, the method was first introduced as

179
00:09:24,984 --> 00:09:28,014
graphic optimization method, graph

180
00:09:28,062 --> 00:09:30,754
optimization method, sorry. It was

181
00:09:30,792 --> 00:09:33,570
introduced in mid 90s by Marco DeRego.

182
00:09:33,910 --> 00:09:39,334
Marco DeRego applied this method on

183
00:09:39,372 --> 00:09:41,366
a travel salesman's problem. And the

184
00:09:41,388 --> 00:09:43,318
problem is mainly about a travel

185
00:09:43,404 --> 00:09:45,366
salesman who wants to visit a number of

186
00:09:45,388 --> 00:09:47,314
cities in a country using the shortest

187
00:09:47,362 --> 00:09:50,186
staff and considering this problem. If

188
00:09:50,208 --> 00:09:52,394
we have different number, if the number

189
00:09:52,432 --> 00:09:56,390
of cities grows, then the permutations

190
00:09:56,470 --> 00:09:58,698
of these numbers of these cities that we

191
00:09:58,704 --> 00:10:00,118
have to consider to find the optimal

192
00:10:00,134 --> 00:10:02,546
solution. If this number of cities

193
00:10:02,598 --> 00:10:05,130
grows, then we will end up having an NPR

194
00:10:05,210 --> 00:10:07,678
problem because we won't have enough

195
00:10:07,764 --> 00:10:11,440
time or computational power to have

196
00:10:13,010 --> 00:10:16,702
this exclusive solution done

197
00:10:16,756 --> 00:10:19,422
or this exclusive search done.

198
00:10:19,556 --> 00:10:23,390
So, from his observations

199
00:10:25,050 --> 00:10:28,598
to answer in nature, he found

200
00:10:28,684 --> 00:10:32,166
that he can apply how they forage to

201
00:10:32,188 --> 00:10:36,214
find food in nature and

202
00:10:36,252 --> 00:10:39,094
then take this concept, this

203
00:10:39,132 --> 00:10:40,778
observation, and apply it in an

204
00:10:40,784 --> 00:10:44,314
algorithm to find the optimum path that

205
00:10:44,352 --> 00:10:48,310
leads from one point and to visit

206
00:10:48,390 --> 00:10:51,610
all the cities in the shortest path.

207
00:10:53,790 --> 00:10:57,450
So this observation,

208
00:10:58,370 --> 00:11:00,014
this slide and the coming slides will

209
00:11:00,052 --> 00:11:03,754
just try to give us a picture

210
00:11:03,802 --> 00:11:08,014
of how hence forage

211
00:11:08,062 --> 00:11:10,082
in nature to find food and then how

212
00:11:10,136 --> 00:11:12,562
Marcus Rigor took that concept to apply

213
00:11:12,616 --> 00:11:15,686
it in the travel systems problem.

214
00:11:15,788 --> 00:11:20,002
So, ants observers

215
00:11:20,066 --> 00:11:22,626
found that ants go out from their nest

216
00:11:22,658 --> 00:11:26,486
to find food and

217
00:11:26,508 --> 00:11:28,440
they try different directions, right?

218
00:11:29,130 --> 00:11:31,482
And when they find food, eventually find

219
00:11:31,536 --> 00:11:33,642
food, they will take some of that food

220
00:11:33,776 --> 00:11:36,262
and then they will go back to their nest

221
00:11:36,406 --> 00:11:39,046
and in the way back, they will deposit

222
00:11:39,078 --> 00:11:41,690
some other substance, cholophone.

223
00:11:44,050 --> 00:11:45,998
So they deposit that substance so that

224
00:11:46,004 --> 00:11:48,030
they communicate that path to the food

225
00:11:48,100 --> 00:11:50,750
resource with the other ants. So

226
00:11:50,900 --> 00:11:53,940
actually, other ants do exploit this

227
00:11:54,710 --> 00:11:57,860
other substance, and when they sense it,

228
00:11:58,470 --> 00:12:01,922
they follow the path that the first ant

229
00:12:01,976 --> 00:12:05,330
took from the food resource to the nest,

230
00:12:05,750 --> 00:12:07,826
hoping that they will find food at the

231
00:12:07,848 --> 00:12:10,790
end of that path. When they actually

232
00:12:10,860 --> 00:12:13,398
find food at the end of the path, they

233
00:12:13,404 --> 00:12:15,254
will take some of the food and do the

234
00:12:15,292 --> 00:12:17,014
same thing. They will deposit some

235
00:12:17,052 --> 00:12:19,834
morpher mole on the same path, making it

236
00:12:19,872 --> 00:12:21,706
more appealing to other ants to take it,

237
00:12:21,728 --> 00:12:23,306
so that they can bring more food to the

238
00:12:23,328 --> 00:12:26,746
nest. So this process

239
00:12:26,848 --> 00:12:29,914
shows us the exploitation behavior of

240
00:12:29,952 --> 00:12:33,146
ants. But again, from time

241
00:12:33,168 --> 00:12:35,822
to time, ants also try to exploit some

242
00:12:35,876 --> 00:12:38,222
other food resources, potential food

243
00:12:38,276 --> 00:12:41,962
resources, and they kind of resist

244
00:12:42,106 --> 00:12:45,810
following the hormone traces,

245
00:12:46,150 --> 00:12:49,780
and they try to go away and find some

246
00:12:51,190 --> 00:12:53,490
new food resources for the nest.

247
00:12:54,310 --> 00:12:58,594
So the ants are not only exploiters,

248
00:12:58,642 --> 00:13:00,934
they're also explorers and these two

249
00:13:00,972 --> 00:13:04,466
concepts were used by Marco Derago

250
00:13:04,498 --> 00:13:08,458
to kind of like balance the search

251
00:13:08,624 --> 00:13:12,780
for the better or faster path between

252
00:13:16,110 --> 00:13:17,722
the cities for the travel sales

253
00:13:17,776 --> 00:13:18,700
management problem.

254
00:13:21,710 --> 00:13:24,634
The third thing that we observed also in

255
00:13:24,672 --> 00:13:28,818
how I spoke in nature is that the older

256
00:13:28,854 --> 00:13:30,558
substance, the pheromone, also

257
00:13:30,644 --> 00:13:34,334
evaporates. So whenever a path to

258
00:13:34,372 --> 00:13:36,746
a food resource is not appealing anymore

259
00:13:36,778 --> 00:13:39,026
or the food resources are excluded, no

260
00:13:39,048 --> 00:13:41,186
more ants will take that path or when

261
00:13:41,208 --> 00:13:44,226
they take it and reach their food

262
00:13:44,248 --> 00:13:47,650
resources that is exposed, they will not

263
00:13:47,720 --> 00:13:49,414
take the same path to the nest again.

264
00:13:49,452 --> 00:13:51,606
They will try to wonder and to find more

265
00:13:51,708 --> 00:13:54,454
new food resources. And because they not

266
00:13:54,492 --> 00:13:56,514
going that path again and not depositing

267
00:13:56,562 --> 00:13:59,306
any pheromone on that path, the

268
00:13:59,328 --> 00:14:01,914
pheromone will eventually evaporate and

269
00:14:01,952 --> 00:14:04,762
disappears and making it less and less

270
00:14:04,896 --> 00:14:06,780
appearing for other ants to take.

271
00:14:08,510 --> 00:14:12,934
So that's

272
00:14:12,982 --> 00:14:15,854
what Marco DeRego was looking at when he

273
00:14:15,892 --> 00:14:17,546
thought of the travel salesman's

274
00:14:17,578 --> 00:14:21,326
problem. He applied that

275
00:14:21,508 --> 00:14:23,746
for the travel salesman's problem by

276
00:14:23,928 --> 00:14:27,202
making one agent try these

277
00:14:27,256 --> 00:14:30,366
different paths and then comparing

278
00:14:30,558 --> 00:14:33,122
through each iteration. So that agent

279
00:14:33,176 --> 00:14:35,874
will take a path between the cities,

280
00:14:36,002 --> 00:14:41,206
right? And then it will compare the

281
00:14:41,228 --> 00:14:42,994
length of that path to the previous

282
00:14:43,122 --> 00:14:45,506
experience with other paths and if it's

283
00:14:45,538 --> 00:14:48,326
shorter, it will try to deposit hormones

284
00:14:48,358 --> 00:14:51,146
on the segments of that path. And

285
00:14:51,248 --> 00:14:53,226
eventually he was hoping, and he was

286
00:14:53,248 --> 00:14:55,226
right about what he was hoping. He was

287
00:14:55,248 --> 00:14:59,626
hoping that eventually the

288
00:14:59,648 --> 00:15:02,830
shorter path, shorter segments that give

289
00:15:02,900 --> 00:15:08,240
the ultimate shorter path has

290
00:15:09,810 --> 00:15:11,886
more and more hormone deposits making it

291
00:15:11,908 --> 00:15:13,458
more and more appealing for the agent to

292
00:15:13,464 --> 00:15:15,170
take it through the iterations.

293
00:15:15,990 --> 00:15:19,730
So we thought about this concept

294
00:15:20,230 --> 00:15:22,046
and we thought that it's very appealing

295
00:15:22,078 --> 00:15:26,120
to apply it for an Nas problem because

296
00:15:27,850 --> 00:15:31,954
Drago's solution was applied for a graph

297
00:15:32,002 --> 00:15:34,706
optimization problem and neural networks

298
00:15:34,738 --> 00:15:37,918
are in their sense direction graphs.

299
00:15:38,114 --> 00:15:40,966
So we also considered endcon

300
00:15:40,998 --> 00:15:42,774
optimization because it's full torrent,

301
00:15:42,902 --> 00:15:46,010
decentralized and scalable and it's also

302
00:15:46,080 --> 00:15:50,894
easily traceable going

303
00:15:50,932 --> 00:15:53,070
back to being decentralized.

304
00:15:55,410 --> 00:15:59,258
It made this method

305
00:15:59,354 --> 00:16:01,406
a perfect candidate for a pal and high

306
00:16:01,428 --> 00:16:05,922
performance computing solution which

307
00:16:05,976 --> 00:16:07,458
will eventually accelerate the

308
00:16:07,464 --> 00:16:11,154
optimization problem. I think.

309
00:16:11,192 --> 00:16:14,450
In the next slide, after the next one,

310
00:16:14,520 --> 00:16:17,762
I will discuss how we exploited or use

311
00:16:17,816 --> 00:16:21,482
this characteristic of anonym

312
00:16:21,486 --> 00:16:24,706
optimization to accelerate the solution

313
00:16:24,898 --> 00:16:26,566
that we came up with or the method that

314
00:16:26,588 --> 00:16:28,790
we came up with, which is ants and cans.

315
00:16:29,310 --> 00:16:32,634
So this scheme, or the scheme of

316
00:16:32,672 --> 00:16:34,906
ants applying ants or ant

317
00:16:34,928 --> 00:16:38,838
collectimization in neural

318
00:16:38,854 --> 00:16:42,046
architecture search is depicted or

319
00:16:42,148 --> 00:16:46,858
illustrated in this flowchart.

320
00:16:47,034 --> 00:16:49,742
So we start off with a massive search

321
00:16:49,796 --> 00:16:53,970
space expressed in

322
00:16:54,120 --> 00:16:58,990
superstructure which expresses

323
00:16:59,070 --> 00:17:02,786
or embodies a

324
00:17:02,808 --> 00:17:05,546
neural network that is massively

325
00:17:05,678 --> 00:17:08,038
connected, meaning that each node in

326
00:17:08,044 --> 00:17:09,478
that superstructure is connected with

327
00:17:09,484 --> 00:17:11,974
the other nodes through edges and

328
00:17:12,012 --> 00:17:14,342
recurrent edges, backwards and forward

329
00:17:14,396 --> 00:17:15,990
and backward recurrent edges.

330
00:17:17,550 --> 00:17:21,866
And then we let a number of agents

331
00:17:22,048 --> 00:17:25,626
swarm over the structure from an

332
00:17:25,648 --> 00:17:28,330
input node to an output node. So each

333
00:17:28,400 --> 00:17:30,566
one of these agents will pick an input

334
00:17:30,598 --> 00:17:33,846
node and then it wanders from that node

335
00:17:34,038 --> 00:17:36,314
through the connection recurrent edges

336
00:17:36,362 --> 00:17:40,766
and recurrent edges and

337
00:17:40,788 --> 00:17:42,122
between the nodes and between the hidden

338
00:17:42,186 --> 00:17:44,786
layers till it gets and picks one of the

339
00:17:44,808 --> 00:17:47,314
output nodes. And then we take all these

340
00:17:47,352 --> 00:17:51,074
paths of the different agents, and we

341
00:17:51,192 --> 00:17:54,926
put them together to form a structure,

342
00:17:55,118 --> 00:17:56,966
neural network structure. And we take

343
00:17:56,988 --> 00:18:00,040
that structure and train it and test it

344
00:18:00,890 --> 00:18:03,366
and then compare its performance to a

345
00:18:03,388 --> 00:18:04,946
population of best performing neural

346
00:18:04,978 --> 00:18:06,914
networks, best performance structures.

347
00:18:07,042 --> 00:18:09,738
And if it's better than the worst in the

348
00:18:09,744 --> 00:18:13,354
population, then we reward the path that

349
00:18:13,392 --> 00:18:16,586
the ants took, the agents took over in

350
00:18:16,608 --> 00:18:19,050
the superstructure, reward with hormone

351
00:18:20,450 --> 00:18:22,954
so that it makes these paths appealing

352
00:18:23,002 --> 00:18:26,046
for later iterations through the

353
00:18:26,068 --> 00:18:28,650
evolution process or the optimization

354
00:18:28,730 --> 00:18:32,170
process. That's if the generated

355
00:18:32,250 --> 00:18:34,039
structure is best than the worst in the

356
00:18:34,539 --> 00:18:36,258
population, if not, if it actually was

357
00:18:36,344 --> 00:18:38,718
worse than the worst in the population,

358
00:18:38,894 --> 00:18:42,146
then we discard that structure or

359
00:18:42,168 --> 00:18:44,222
that neural network, and we don't reward

360
00:18:44,286 --> 00:18:48,200
Any of the path that ants took. And also

361
00:18:48,570 --> 00:18:52,614
the thermo evaporation will help us get

362
00:18:52,652 --> 00:18:54,934
rid of the pheromones that were

363
00:18:54,972 --> 00:18:56,854
deposited on the edges that are not

364
00:18:56,892 --> 00:19:00,090
giving us better and better structures.

365
00:19:03,710 --> 00:19:06,746
And again, because the M colony is

366
00:19:06,768 --> 00:19:08,970
decentralized we exploited this by

367
00:19:09,040 --> 00:19:11,726
having an asynchronous solution or

368
00:19:11,748 --> 00:19:15,614
asynchronous evolution. We had a

369
00:19:15,652 --> 00:19:17,840
main process that took care of

370
00:19:18,530 --> 00:19:21,182
generating the new structures and also

371
00:19:21,236 --> 00:19:23,226
updating the population the best

372
00:19:23,268 --> 00:19:26,162
performing structures and updating the

373
00:19:26,216 --> 00:19:28,786
hormone on the superstructure. So the

374
00:19:28,808 --> 00:19:32,334
main process will generate structures

375
00:19:32,382 --> 00:19:34,146
and send them to worker processes. The

376
00:19:34,168 --> 00:19:36,830
worker processes will train and test the

377
00:19:36,840 --> 00:19:39,126
neural network on the data that we have

378
00:19:39,228 --> 00:19:42,006
for the problem and then send the

379
00:19:42,028 --> 00:19:44,198
results back or the fitness of the

380
00:19:44,204 --> 00:19:47,174
neural network to the main process. And

381
00:19:47,212 --> 00:19:49,290
based on that fitness the main process

382
00:19:49,360 --> 00:19:54,598
will either discard it or we'll

383
00:19:54,614 --> 00:19:56,586
take this fitness and compare it to the

384
00:19:56,608 --> 00:19:58,586
best performing in the population. If

385
00:19:58,608 --> 00:20:00,474
it's better than the worst it will

386
00:20:00,592 --> 00:20:02,538
reward the path that ends took on the

387
00:20:02,544 --> 00:20:04,106
superstructure by depositing more

388
00:20:04,128 --> 00:20:05,978
hormone. Or if it's worse than the worst

389
00:20:05,994 --> 00:20:07,838
in the population, we'll just start it

390
00:20:07,924 --> 00:20:11,774
and it will keep generating new

391
00:20:11,812 --> 00:20:13,886
structures and sending them to processes

392
00:20:14,078 --> 00:20:17,646
because the training which relies

393
00:20:17,678 --> 00:20:20,898
on that propagation is the most

394
00:20:21,064 --> 00:20:23,282
computationally expensive part in this

395
00:20:23,336 --> 00:20:26,626
process. If we have a number of worker

396
00:20:26,658 --> 00:20:29,862
processes that can work in parallel to

397
00:20:29,996 --> 00:20:32,338
train and evaluate these neural

398
00:20:32,354 --> 00:20:35,046
networks, these new structures we can

399
00:20:35,068 --> 00:20:39,610
speed up the process, right by training

400
00:20:39,680 --> 00:20:42,106
and evaluating different structures at

401
00:20:42,128 --> 00:20:45,210
the same time in parallel in an

402
00:20:45,280 --> 00:20:47,370
asynchronous evolution scheme.

403
00:20:51,890 --> 00:20:53,406
This is an animation but it's not

404
00:20:53,428 --> 00:20:55,870
working in this version of the slides

405
00:20:56,210 --> 00:20:58,554
because we're using a PDF but it's

406
00:20:58,602 --> 00:21:02,160
mainly a structure where you'll see

407
00:21:03,190 --> 00:21:06,850
edges or connections between these nodes

408
00:21:07,830 --> 00:21:11,726
fading or having darker

409
00:21:11,758 --> 00:21:13,822
colors based on the pheromone values

410
00:21:13,886 --> 00:21:18,534
through the iterations. So each frame in

411
00:21:18,572 --> 00:21:22,054
this animation is kind of an update for

412
00:21:22,092 --> 00:21:25,506
the pheromone value of the edges

413
00:21:25,618 --> 00:21:28,630
based on the performance of the version

414
00:21:29,210 --> 00:21:30,486
of the neural network that were

415
00:21:30,508 --> 00:21:32,426
generated by the agents when they

416
00:21:32,448 --> 00:21:35,066
swarmed from the start node taking one

417
00:21:35,088 --> 00:21:36,982
of the input nodes in the middle layer

418
00:21:37,046 --> 00:21:38,698
and then from there going to one of the

419
00:21:38,704 --> 00:21:40,842
hidden layers in this one hidden layer

420
00:21:40,906 --> 00:21:42,846
that we have here. And from there going

421
00:21:42,868 --> 00:21:43,950
to the output.

422
00:21:47,090 --> 00:21:50,922
So that was the concept of applying ACO

423
00:21:50,986 --> 00:21:55,422
and con optimization in and

424
00:21:55,556 --> 00:21:57,906
this now I'm going to talk about the

425
00:21:57,928 --> 00:21:59,940
actual method that we came up with.

426
00:22:02,070 --> 00:22:05,010
So it's more generic and more powerful

427
00:22:05,990 --> 00:22:08,214
neural texture search method. More

428
00:22:08,252 --> 00:22:15,094
comprehensive if I may say that we

429
00:22:15,132 --> 00:22:18,810
opted to apply the methods on

430
00:22:18,960 --> 00:22:21,306
recurrent neural networks because they

431
00:22:21,408 --> 00:22:26,938
tend to be potentially larger than other

432
00:22:27,104 --> 00:22:29,514
neural networks structures because of

433
00:22:29,552 --> 00:22:31,966
their recurrent connections. So we

434
00:22:31,988 --> 00:22:33,902
thought that if we started this problem

435
00:22:34,036 --> 00:22:36,426
though the method or the concept applies

436
00:22:36,458 --> 00:22:38,686
to any neural network but applying it to

437
00:22:38,708 --> 00:22:42,142
recurrent neural networks made it more

438
00:22:42,196 --> 00:22:48,174
appealing challenge for measuring

439
00:22:48,222 --> 00:22:49,826
the performance of the method that we

440
00:22:49,848 --> 00:22:54,146
thought of. So this

441
00:22:54,168 --> 00:22:55,878
slide in the common slide will discuss

442
00:22:55,964 --> 00:22:58,658
the different heuristics of the methods

443
00:22:58,674 --> 00:23:01,174
of ants. The first heuristic is

444
00:23:01,292 --> 00:23:04,982
superstructure itself and as I mentioned

445
00:23:05,036 --> 00:23:08,314
before it's a massive search space as

446
00:23:08,352 --> 00:23:11,260
massive as possible to be handled with

447
00:23:12,270 --> 00:23:14,166
the machine or the hardware that we're

448
00:23:14,198 --> 00:23:16,854
working on. The superstructure consists

449
00:23:16,902 --> 00:23:19,606
of a neural network that is massively

450
00:23:19,638 --> 00:23:22,054
connected meaning that every node in the

451
00:23:22,112 --> 00:23:23,454
structure is connected to the other

452
00:23:23,492 --> 00:23:27,102
nodes via or through edges. Forward

453
00:23:27,156 --> 00:23:29,882
edges and recurrent edges. Backward

454
00:23:29,946 --> 00:23:31,226
forward recurrent edges and backward

455
00:23:31,258 --> 00:23:34,206
recurrent edges. This simple structure

456
00:23:34,238 --> 00:23:36,514
that we have here represents one of just

457
00:23:36,552 --> 00:23:38,866
the concept of the superstructure that

458
00:23:39,048 --> 00:23:42,402
we apply in ants. Here we have

459
00:23:42,456 --> 00:23:44,766
three input nodes, three hidden layers

460
00:23:44,798 --> 00:23:46,754
each have three nodes and one output

461
00:23:46,802 --> 00:23:49,462
node in the output layer. And we are

462
00:23:49,516 --> 00:23:52,886
just showing one node connected to the

463
00:23:52,908 --> 00:23:55,766
other nodes through edges which are the

464
00:23:55,788 --> 00:23:59,754
ones representing green forward

465
00:23:59,872 --> 00:24:03,142
recurrent sorry, the edges

466
00:24:03,206 --> 00:24:05,450
are the one in gray and then forward

467
00:24:05,520 --> 00:24:07,306
recurrent and backward recurrent in the

468
00:24:07,328 --> 00:24:11,486
green and in the red. And the concept of

469
00:24:11,588 --> 00:24:14,574
recurrent edges might be a little bit

470
00:24:14,612 --> 00:24:16,974
confusing if we look at it in this

471
00:24:17,012 --> 00:24:21,054
example because how can an edge be

472
00:24:21,092 --> 00:24:22,818
recurrent coming in and out from the

473
00:24:22,824 --> 00:24:26,434
same node, the nodes in the same

474
00:24:26,472 --> 00:24:29,954
timestamp but this structure might make

475
00:24:29,992 --> 00:24:33,266
it more clear. So here we

476
00:24:33,288 --> 00:24:37,010
have structure that is pretty

477
00:24:37,160 --> 00:24:39,142
much similar to the one we saw before.

478
00:24:39,196 --> 00:24:41,494
But here we have three input nodes, two

479
00:24:41,532 --> 00:24:43,286
hin layers still three hin layers each

480
00:24:43,308 --> 00:24:45,138
have three nodes and then alpha layer

481
00:24:45,154 --> 00:24:48,426
have two nodes and then we have also

482
00:24:48,528 --> 00:24:50,314
three time steps. The current time step

483
00:24:50,352 --> 00:24:53,066
t zero and the previous time step t

484
00:24:53,088 --> 00:24:55,286
minus one, the one before that t minus

485
00:24:55,318 --> 00:24:59,142
two edges here are illustrated

486
00:24:59,206 --> 00:25:03,790
using the solid black lines.

487
00:25:06,130 --> 00:25:08,654
Of course these edges are present in the

488
00:25:08,692 --> 00:25:10,814
current time step that is going to

489
00:25:10,852 --> 00:25:12,590
propagate through the neural network.

490
00:25:13,890 --> 00:25:17,970
Then the current connections. These ones

491
00:25:18,040 --> 00:25:20,674
are going to bring information or

492
00:25:20,712 --> 00:25:22,654
provide information from the previous

493
00:25:22,702 --> 00:25:26,502
time steps, the previous inputs or

494
00:25:26,556 --> 00:25:30,214
previous data that was fired to the

495
00:25:30,332 --> 00:25:32,902
nodes in the previous time steps. And

496
00:25:32,956 --> 00:25:36,342
these recurrent edges are depicted here

497
00:25:36,396 --> 00:25:39,094
in red and orange. The forward ones are

498
00:25:39,132 --> 00:25:42,346
depicted in red and orange. The reds are

499
00:25:42,368 --> 00:25:44,038
coming from T minus one and the oranges

500
00:25:44,054 --> 00:25:47,818
are coming from T minus two. And the

501
00:25:47,904 --> 00:25:51,690
backward current edges are the dot lines

502
00:25:51,850 --> 00:25:54,080
in blue and green.

503
00:25:55,170 --> 00:25:56,910
And we can see that they are going

504
00:25:56,980 --> 00:26:00,590
backwards. So they go backward

505
00:26:01,170 --> 00:26:02,830
to backward layers.

506
00:26:07,190 --> 00:26:09,266
But because they are recurrent, we can

507
00:26:09,288 --> 00:26:11,298
do that because they process

508
00:26:11,384 --> 00:26:13,154
information, they bring information that

509
00:26:13,192 --> 00:26:15,846
already processed. So we don't have to

510
00:26:15,868 --> 00:26:19,014
worry about propagating information back

511
00:26:19,052 --> 00:26:21,846
through time or back through the

512
00:26:21,868 --> 00:26:23,954
structure. But we can do that if it's

513
00:26:24,002 --> 00:26:25,640
coming from previous time step.

514
00:26:27,930 --> 00:26:30,746
The second heuristic in Ash is the

515
00:26:30,848 --> 00:26:34,780
colony weight sharing we wanted to use.

516
00:26:36,190 --> 00:26:39,786
So instead of Brandon initialized the

517
00:26:39,808 --> 00:26:42,062
weights or the snapped weights and

518
00:26:42,116 --> 00:26:44,430
generated neural networks,

519
00:26:44,930 --> 00:26:48,606
we wanted to use the train weights to

520
00:26:48,628 --> 00:26:51,434
initialize the weights of the newly

521
00:26:51,482 --> 00:26:54,558
generated neural networks.

522
00:26:54,654 --> 00:26:56,718
We did that by saving these neural

523
00:26:56,734 --> 00:27:00,034
networks on the superstructure. We used

524
00:27:00,072 --> 00:27:03,714
the last equation here

525
00:27:03,752 --> 00:27:05,986
to do this update. So we balanced

526
00:27:06,018 --> 00:27:07,554
between the weights that were saved

527
00:27:07,602 --> 00:27:11,494
previously and the weights that are

528
00:27:11,532 --> 00:27:13,282
coming from the trained or evaluated

529
00:27:13,346 --> 00:27:16,454
neural networks. And we also used two

530
00:27:16,572 --> 00:27:20,122
strategies to do this update. We used a

531
00:27:20,176 --> 00:27:24,154
fixed parameter phi or

532
00:27:24,352 --> 00:27:27,114
we left phi. That was the first

533
00:27:27,152 --> 00:27:30,302
strategy. The second strategy was to get

534
00:27:30,356 --> 00:27:33,230
phi by applying these two equations

535
00:27:33,730 --> 00:27:36,446
which relies on the performance of the

536
00:27:36,468 --> 00:27:38,074
neural network that was previously

537
00:27:38,122 --> 00:27:40,826
generated sorry, the previously

538
00:27:40,938 --> 00:27:43,186
generated and trained. So it relies on

539
00:27:43,208 --> 00:27:45,182
the performance of the neural network

540
00:27:45,246 --> 00:27:48,354
that was trained and validated or

541
00:27:48,392 --> 00:27:52,066
tested. So if

542
00:27:52,088 --> 00:27:54,242
the performance was good, then we will

543
00:27:54,296 --> 00:27:58,738
let the weights of that neural

544
00:27:58,754 --> 00:28:02,614
network to contribute more to

545
00:28:02,652 --> 00:28:06,038
the initialization of the weights of

546
00:28:06,044 --> 00:28:08,074
the newly generated RNS. If it's not

547
00:28:08,112 --> 00:28:11,254
performing that well, then this equation

548
00:28:11,302 --> 00:28:13,834
will not allow it to contribute that

549
00:28:13,872 --> 00:28:17,766
much. The third meta heuristic

550
00:28:17,798 --> 00:28:21,710
is multiple memory cells. So at

551
00:28:21,780 --> 00:28:22,910
each node,

552
00:28:25,490 --> 00:28:28,250
when an agent or an ant reach to a node

553
00:28:28,410 --> 00:28:31,630
in the superstructure it will do a local

554
00:28:31,700 --> 00:28:35,860
search to pick the type of

555
00:28:36,230 --> 00:28:39,970
the neuron or the type of the node from

556
00:28:40,040 --> 00:28:41,934
these three different types of memory

557
00:28:41,982 --> 00:28:45,010
cells. So in the generated RNN,

558
00:28:45,350 --> 00:28:47,814
the generated structure, the nodes in

559
00:28:47,852 --> 00:28:50,054
the structure is not all the same. They

560
00:28:50,092 --> 00:28:52,150
will be different based on the local

561
00:28:52,220 --> 00:28:55,766
search that the

562
00:28:55,788 --> 00:28:57,414
agent will do or the ant will do. At

563
00:28:57,452 --> 00:28:59,734
each node they reach through their path

564
00:28:59,782 --> 00:29:01,818
from the input to an output in the

565
00:29:01,824 --> 00:29:02,890
superstructure.

566
00:29:04,990 --> 00:29:07,574
The fourth mineristic is the multiple

567
00:29:07,622 --> 00:29:10,410
ant species. So we applied different

568
00:29:10,480 --> 00:29:12,634
species or came up with different

569
00:29:12,672 --> 00:29:15,586
species. For the ants, the first species

570
00:29:15,638 --> 00:29:18,414
was the ones that will traverse over the

571
00:29:18,452 --> 00:29:20,830
edges, only the edges. So they will go

572
00:29:20,900 --> 00:29:24,338
only forward through the

573
00:29:24,344 --> 00:29:27,746
edges of the neural network of the

574
00:29:27,768 --> 00:29:30,674
superstructure. And these ones are going

575
00:29:30,712 --> 00:29:33,874
to define the number of nodes in the

576
00:29:33,912 --> 00:29:36,214
generated structure. Also define the

577
00:29:36,252 --> 00:29:38,326
types of the nodes in the

578
00:29:38,348 --> 00:29:40,294
superstructure. And when they done with

579
00:29:40,332 --> 00:29:42,278
their work, then the social or the

580
00:29:42,284 --> 00:29:45,110
second species, the social ants,

581
00:29:45,690 --> 00:29:49,366
will traverse between these nodes.

582
00:29:49,478 --> 00:29:53,610
But they will use the recurrent edges

583
00:29:54,830 --> 00:29:56,778
to move between these nodes. So they

584
00:29:56,784 --> 00:30:00,050
will create the recurrent edges

585
00:30:00,150 --> 00:30:03,054
for the newly generated RNN. And we have

586
00:30:03,092 --> 00:30:05,118
two different species for these social

587
00:30:05,204 --> 00:30:07,866
ants or two different subtypes

588
00:30:07,898 --> 00:30:12,094
subspecies. One is the

589
00:30:12,132 --> 00:30:15,866
forward social ends. These ones traverse

590
00:30:15,898 --> 00:30:19,086
only over the forward kind

591
00:30:19,108 --> 00:30:20,766
of connections. They go from the input

592
00:30:20,798 --> 00:30:24,882
to the output but only over the forward

593
00:30:24,936 --> 00:30:26,974
recurrent edges and then the backward

594
00:30:27,022 --> 00:30:29,490
recurrent edges or the backward social

595
00:30:29,560 --> 00:30:32,646
ends. These ones go from the output to

596
00:30:32,668 --> 00:30:35,302
the input and they traverse over the

597
00:30:35,356 --> 00:30:38,570
recurrent connections. The reason we

598
00:30:38,640 --> 00:30:40,666
thought about these different species is

599
00:30:40,688 --> 00:30:42,662
that we wanted to control the tendency

600
00:30:42,726 --> 00:30:46,026
of the ants to wander around

601
00:30:46,208 --> 00:30:49,674
in the superstructure exploiting the

602
00:30:49,712 --> 00:30:53,354
convoluted mesh of recurrent

603
00:30:53,402 --> 00:30:56,638
connections. So we wanted to control

604
00:30:56,724 --> 00:30:59,754
that. So we came up with this strategy

605
00:30:59,802 --> 00:31:03,166
so that we can just define the structure

606
00:31:03,278 --> 00:31:05,138
using the explorer ants and then the

607
00:31:05,144 --> 00:31:06,942
recurrent connections can be defined

608
00:31:07,006 --> 00:31:09,970
after that using the social ants.

609
00:31:12,390 --> 00:31:15,346
The fifth characteristic is the

610
00:31:15,368 --> 00:31:17,870
regularization of formal basement.

611
00:31:17,950 --> 00:31:21,574
Again, we wanted to give the ants in an

612
00:31:21,612 --> 00:31:25,990
incentive to bring sparser

613
00:31:26,410 --> 00:31:29,798
and also well performing neural

614
00:31:29,814 --> 00:31:34,380
networks by just penalizing them

615
00:31:35,870 --> 00:31:38,454
if they constructed denser or bigger

616
00:31:38,502 --> 00:31:41,854
structures. So we added this

617
00:31:41,892 --> 00:31:45,866
regularization term and formula

618
00:31:45,898 --> 00:31:47,920
that updated the hormone value.

619
00:31:49,890 --> 00:31:52,686
And as you see, the regularization term

620
00:31:52,718 --> 00:31:54,834
relies on the performance. The data here

621
00:31:54,872 --> 00:31:56,334
is the performance of the neural network

622
00:31:56,382 --> 00:31:59,730
and it also relies on the size of the

623
00:31:59,880 --> 00:32:00,770
structure.

624
00:32:06,070 --> 00:32:10,920
The last, the 6th and last one is

625
00:32:11,290 --> 00:32:13,286
jumping ants which is we want to

626
00:32:13,308 --> 00:32:16,710
experiment with. If we let the ants

627
00:32:18,810 --> 00:32:22,182
if we let the ants jump over the layers

628
00:32:22,246 --> 00:32:26,794
when they move through the

629
00:32:26,832 --> 00:32:29,926
superstructure if we let them jump over

630
00:32:29,968 --> 00:32:31,258
these layers to construct the neural

631
00:32:31,274 --> 00:32:33,738
networks compared to if we restrict

632
00:32:33,754 --> 00:32:37,006
their movement to jump one layer at

633
00:32:37,028 --> 00:32:40,640
a time. How would this end up

634
00:32:41,810 --> 00:32:43,954
performance wise if they will give us

635
00:32:43,992 --> 00:32:47,310
parser and well performing structures?

636
00:32:47,390 --> 00:32:50,466
Or this jumping will hurt the

637
00:32:50,488 --> 00:32:54,530
performance by giving us weaker

638
00:32:55,050 --> 00:32:58,018
structures, weaker neural networks.

639
00:32:58,194 --> 00:32:58,920
So.

640
00:33:00,810 --> 00:33:04,662
We use a timespheres data that

641
00:33:04,796 --> 00:33:07,654
belong to Coal Fire Power plant. We

642
00:33:07,692 --> 00:33:11,400
divide the data to have 7200

643
00:33:12,010 --> 00:33:15,174
records for training and testing and

644
00:33:15,292 --> 00:33:17,878
here the plot shows that the data and we

645
00:33:17,884 --> 00:33:22,522
can see that it's nonlinear and it's

646
00:33:22,666 --> 00:33:26,462
acyclic and non seasonal. So it's a hard

647
00:33:26,516 --> 00:33:30,698
problem for a non neural

648
00:33:30,714 --> 00:33:32,926
network solution or a regression linear

649
00:33:32,958 --> 00:33:36,846
regression solution. So the input

650
00:33:36,878 --> 00:33:39,506
consists of twelve parameters. When we

651
00:33:39,528 --> 00:33:40,914
were trying to predict only one

652
00:33:40,952 --> 00:33:43,250
parameter the flame intensity

653
00:33:46,570 --> 00:33:48,838
experiments covered all of the

654
00:33:48,924 --> 00:33:52,870
heuristics of ants giving different

655
00:33:52,940 --> 00:33:54,870
values for these different parameters.

656
00:33:56,650 --> 00:33:59,114
The superstructures consist of twelve

657
00:33:59,152 --> 00:34:00,714
input nodes. Three hidden layers each

658
00:34:00,752 --> 00:34:03,626
have twelve nodes and one output node in

659
00:34:03,648 --> 00:34:05,926
the output layer. The recurrent

660
00:34:05,958 --> 00:34:09,110
connection span over three times steps

661
00:34:09,270 --> 00:34:11,934
one, two and three times steps. In

662
00:34:11,972 --> 00:34:15,082
total, the superstructure had 49 nodes,

663
00:34:15,226 --> 00:34:19,086
924 edges and almost 3.5

664
00:34:19,188 --> 00:34:22,334
thousand recurrent edges. So if you

665
00:34:22,372 --> 00:34:26,530
unroll the structure over 72 time steps

666
00:34:27,030 --> 00:34:28,914
in back propagation through time will

667
00:34:28,952 --> 00:34:34,738
have about 352,000

668
00:34:34,824 --> 00:34:38,198
nodes, about 6.5 million edges and

669
00:34:38,364 --> 00:34:41,750
about 26 million recurring edges.

670
00:34:42,970 --> 00:34:44,770
In the experiments, we also compared

671
00:34:44,850 --> 00:34:47,382
performance of ants using the same data

672
00:34:47,436 --> 00:34:51,774
set. We compared it to exam and neat.

673
00:34:51,922 --> 00:34:56,380
So Exam is the state of the art in

674
00:34:57,070 --> 00:35:00,134
neural texture search which is genetic

675
00:35:00,182 --> 00:35:03,754
based method. We also

676
00:35:03,792 --> 00:35:06,222
compared it to NEET because it's like a

677
00:35:06,276 --> 00:35:08,702
benchmark in the neural revolution and

678
00:35:08,756 --> 00:35:12,286
nas realm. And also we compared it

679
00:35:12,308 --> 00:35:14,266
to fixed structures, unoptimized

680
00:35:14,298 --> 00:35:17,954
structures that

681
00:35:17,992 --> 00:35:19,678
had one and two and three hidden layers

682
00:35:19,774 --> 00:35:25,134
and also different types of memory

683
00:35:25,182 --> 00:35:28,766
based cells. The experiments

684
00:35:28,798 --> 00:35:30,854
covered 1600 experiments to cover all

685
00:35:30,892 --> 00:35:32,674
the combinations of the meta heuristics

686
00:35:32,722 --> 00:35:36,694
of the heuristics of ants. Each one

687
00:35:36,732 --> 00:35:39,034
of these experiments was repeated ten

688
00:35:39,072 --> 00:35:42,970
times for statistical analysis.

689
00:35:47,310 --> 00:35:50,906
Ants generated 2000 RNNs

690
00:35:50,938 --> 00:35:52,990
for each experiment. Each trained for

691
00:35:53,140 --> 00:35:56,334
ten epics. In total,

692
00:35:56,452 --> 00:35:59,582
ants generated about generated train

693
00:35:59,636 --> 00:36:03,870
and evaluated 32 million RNNs.

694
00:36:04,030 --> 00:36:07,614
It took a month and 1000 CPUs

695
00:36:07,662 --> 00:36:09,170
to finish the experiments.

696
00:36:10,790 --> 00:36:15,318
The results that we got showed that of

697
00:36:15,324 --> 00:36:23,666
course outperformed the unoptimized

698
00:36:23,698 --> 00:36:27,510
structures and it also outperformed Neat

699
00:36:28,490 --> 00:36:30,902
and then some of the combinations of

700
00:36:30,956 --> 00:36:34,186
ends outperformed Exam. So Exam here

701
00:36:34,208 --> 00:36:37,066
is the fourth from the left and we can

702
00:36:37,088 --> 00:36:40,350
see that the mean absolute error for

703
00:36:40,420 --> 00:36:43,114
some of the versions or the combinations

704
00:36:43,242 --> 00:36:46,906
of ant heuristics outperformed

705
00:36:46,938 --> 00:36:47,520
Exam.

706
00:36:49,890 --> 00:36:53,186
So we tried to look to do some

707
00:36:53,208 --> 00:36:57,138
statistical study for the

708
00:36:57,144 --> 00:36:59,874
results we got from Ads. So we tried to

709
00:36:59,912 --> 00:37:03,614
look at the top performing

710
00:37:03,662 --> 00:37:06,658
neural networks coming in our results.

711
00:37:06,754 --> 00:37:09,110
So we tried to look at the top ten,

712
00:37:09,180 --> 00:37:13,334
2522 hundred and 55

713
00:37:13,372 --> 00:37:17,658
hundred results and we look at the

714
00:37:17,664 --> 00:37:19,226
contribution of these heuristics in

715
00:37:19,248 --> 00:37:22,726
these best performing neural networks

716
00:37:22,758 --> 00:37:26,300
or structures. We found that

717
00:37:27,070 --> 00:37:29,770
these heuristics contributed effectively

718
00:37:30,130 --> 00:37:32,830
in most of these results.

719
00:37:33,890 --> 00:37:37,966
But the

720
00:37:37,988 --> 00:37:44,014
thing that was really intriguing

721
00:37:44,062 --> 00:37:47,940
for us or surprise us is that

722
00:37:50,470 --> 00:37:53,330
we saw that the recurrent connections

723
00:37:53,830 --> 00:37:55,878
disappeared in risk results because the

724
00:37:55,884 --> 00:37:58,754
best performing neural networks didn't

725
00:37:58,802 --> 00:38:01,590
have that much of recurrent connections.

726
00:38:04,250 --> 00:38:07,814
That meant for us that the memory

727
00:38:07,862 --> 00:38:10,182
based cells did the job for recurrent

728
00:38:10,246 --> 00:38:12,234
information coming from previous time

729
00:38:12,272 --> 00:38:16,026
steps. But we wanted to expand on

730
00:38:16,048 --> 00:38:18,298
this later. So it's on our list

731
00:38:18,464 --> 00:38:22,190
discussing on our future investigations.

732
00:38:26,050 --> 00:38:29,326
So this is just a summary for the

733
00:38:29,348 --> 00:38:31,546
achievements of Ans based on the results

734
00:38:31,578 --> 00:38:33,390
that we got from our experiments.

735
00:38:36,770 --> 00:38:40,066
So ANZ was the first meta method to

736
00:38:40,088 --> 00:38:42,878
involve the core of ACO and column

737
00:38:42,894 --> 00:38:44,798
optimization and recurring neural

738
00:38:44,814 --> 00:38:49,134
networks. Nas or neural heuristics

739
00:38:49,182 --> 00:38:50,834
to control ants tendency to wander

740
00:38:50,882 --> 00:38:53,122
around superstructure. Exploiting

741
00:38:53,186 --> 00:38:55,090
recurrent connection proved successful

742
00:38:55,170 --> 00:39:00,230
because the regularization component

743
00:39:00,310 --> 00:39:04,906
or the regularization heuristics give

744
00:39:04,928 --> 00:39:07,706
us better results. Showing here in this

745
00:39:07,888 --> 00:39:12,506
table and also the jumping

746
00:39:12,538 --> 00:39:14,762
ants here gave us better performance

747
00:39:14,826 --> 00:39:16,830
compared to non jumping ants.

748
00:39:19,170 --> 00:39:24,862
And the

749
00:39:24,916 --> 00:39:26,240
realization. Also,

750
00:39:29,650 --> 00:39:33,530
the weight sharing strategy

751
00:39:33,610 --> 00:39:36,510
also proved effective.

752
00:39:37,110 --> 00:39:38,914
If we look at it, the results here

753
00:39:38,952 --> 00:39:41,694
compared to if we don't apply weight

754
00:39:41,742 --> 00:39:42,370
sharing.

755
00:39:54,710 --> 00:39:58,546
So autopromyzing strategies are

756
00:39:58,568 --> 00:40:02,142
generic so the strategies that we use

757
00:40:02,216 --> 00:40:06,118
are generic enough to apply it for any

758
00:40:06,284 --> 00:40:09,478
problem or solution, that is end colony

759
00:40:09,494 --> 00:40:13,174
optimization based. The Vermont

760
00:40:13,222 --> 00:40:15,754
deposition that the method that we came

761
00:40:15,792 --> 00:40:18,086
up with is also Novalu, which wasn't

762
00:40:18,118 --> 00:40:20,730
introduced in any previous literature.

763
00:40:21,810 --> 00:40:23,726
And the performance of ants compared to

764
00:40:23,748 --> 00:40:26,430
the other benchmark and state of the art

765
00:40:26,500 --> 00:40:29,470
in the realm is also remarkable.

766
00:40:30,530 --> 00:40:34,660
So going forward, we thought that ant

767
00:40:35,430 --> 00:40:38,834
gave us a good result. But the main

768
00:40:39,032 --> 00:40:43,042
drawback of Ans was the discrete search

769
00:40:43,096 --> 00:40:47,030
space. So ans worked on

770
00:40:47,180 --> 00:40:51,266
this massively connected massively

771
00:40:51,298 --> 00:40:53,954
connected superstructure, but it's

772
00:40:54,002 --> 00:40:57,590
massive gas, but it's still discrete.

773
00:40:58,010 --> 00:41:00,700
Ants can move freely between these

774
00:41:01,230 --> 00:41:04,698
nodes. They are forced to move over

775
00:41:04,784 --> 00:41:06,570
between these nodes, over these

776
00:41:06,720 --> 00:41:10,266
predefined connections, whether they

777
00:41:10,288 --> 00:41:13,310
are forward edges or recurrent edges.

778
00:41:14,690 --> 00:41:17,840
So we thought of

779
00:41:18,210 --> 00:41:21,326
removing that continuous discrete search

780
00:41:21,348 --> 00:41:22,826
space and replacing it with a continuous

781
00:41:22,858 --> 00:41:26,814
search space. So we designed

782
00:41:26,862 --> 00:41:30,900
a 3D search space where

783
00:41:32,630 --> 00:41:34,494
the search space had like layers

784
00:41:34,542 --> 00:41:37,010
representing the lag, the time lags.

785
00:41:37,690 --> 00:41:40,840
And then ants can jump over between

786
00:41:41,290 --> 00:41:45,106
these layers to give us the recurrent

787
00:41:45,138 --> 00:41:47,894
connections. And on each of these

788
00:41:48,092 --> 00:41:53,766
layers, ants will just give us the

789
00:41:53,788 --> 00:41:56,090
nodes and the edges between the nodes.

790
00:41:58,590 --> 00:42:00,774
So in this slide, in the coming slides,

791
00:42:00,822 --> 00:42:03,246
we'll show an example of how ants move

792
00:42:03,348 --> 00:42:06,906
in cans or continuous ant, or continuous

793
00:42:06,938 --> 00:42:10,622
ends. So an agent or an ant

794
00:42:10,676 --> 00:42:12,926
will just start by picking up one of the

795
00:42:12,948 --> 00:42:15,810
layers that will move on. This is done

796
00:42:15,880 --> 00:42:18,994
in a discrete fashion and once this is

797
00:42:19,032 --> 00:42:20,946
done, it will then decide if it's going

798
00:42:20,968 --> 00:42:22,894
to do an exploitation or exploration

799
00:42:22,942 --> 00:42:26,430
movement. And in this example,

800
00:42:26,600 --> 00:42:28,194
it decided to do an exploration

801
00:42:28,242 --> 00:42:32,114
movement. So it will decide the angle

802
00:42:32,162 --> 00:42:36,454
and the radius of its next allocation on

803
00:42:36,492 --> 00:42:39,958
that layer. Once that's done,

804
00:42:40,044 --> 00:42:42,714
it's going to go forward to that

805
00:42:42,752 --> 00:42:44,586
location and then decides if it's going

806
00:42:44,608 --> 00:42:46,874
to do an exploitation or if the next

807
00:42:46,912 --> 00:42:48,822
move will be exploitation or exploration

808
00:42:48,886 --> 00:42:51,846
move. And this example will be an

809
00:42:51,968 --> 00:42:54,960
exploitation move. So it will try to

810
00:42:55,730 --> 00:42:57,966
exploit the pheromone traces, the

811
00:42:57,988 --> 00:43:00,094
hormone that was previously deposited by

812
00:43:00,132 --> 00:43:04,098
other ants in the search space. So it

813
00:43:04,104 --> 00:43:06,318
will use its sensing radius, that's

814
00:43:06,334 --> 00:43:08,194
something that is previously defined for

815
00:43:08,232 --> 00:43:13,154
each ant. And then it will find

816
00:43:13,192 --> 00:43:16,302
the center of mass of the hormone

817
00:43:16,366 --> 00:43:19,746
traces. And then when

818
00:43:19,768 --> 00:43:21,826
it calculates that center of mass of the

819
00:43:21,848 --> 00:43:24,662
hormone, it will consider it as its next

820
00:43:24,716 --> 00:43:28,246
location and then it will move to that

821
00:43:28,348 --> 00:43:32,106
spot. And then it will decide about if

822
00:43:32,128 --> 00:43:34,102
its next move will be an exploitation

823
00:43:34,166 --> 00:43:37,914
exploration move at each location before

824
00:43:37,952 --> 00:43:40,714
it's deciding about the type of the

825
00:43:40,752 --> 00:43:42,890
step. If it's exploitation exploration,

826
00:43:42,970 --> 00:43:45,006
it will also decide if it's going to

827
00:43:45,028 --> 00:43:47,342
stay at the same level at the same time

828
00:43:47,396 --> 00:43:51,470
lag or jump to a next time lag.

829
00:43:54,310 --> 00:43:56,706
If the jump or the movement is on the

830
00:43:56,728 --> 00:43:57,620
same level,

831
00:44:00,310 --> 00:44:02,740
if the movement is on the same level,

832
00:44:03,750 --> 00:44:05,822
if the ants is doing an exploitation

833
00:44:05,886 --> 00:44:07,906
movement it will only consider the

834
00:44:07,928 --> 00:44:11,942
promontory that is ahead

835
00:44:11,996 --> 00:44:15,286
of it because it can't move backwards on

836
00:44:15,308 --> 00:44:18,966
the same time lag. Otherwise it will be

837
00:44:18,988 --> 00:44:21,674
doing a backwards step which is not

838
00:44:21,712 --> 00:44:24,650
allowed in neural networks.

839
00:44:26,510 --> 00:44:30,170
But if it's going to another time

840
00:44:30,240 --> 00:44:35,178
lag above layer then it's

841
00:44:35,194 --> 00:44:37,150
going to do a recurrent edge.

842
00:44:39,890 --> 00:44:42,782
Current edges can go back in time sorry,

843
00:44:42,836 --> 00:44:45,070
back in the structure in the previous

844
00:44:46,370 --> 00:44:50,174
layer. So now the end can

845
00:44:50,212 --> 00:44:52,490
consider all the promote traces within

846
00:44:52,660 --> 00:44:54,978
radius, the ones that are ahead of it

847
00:44:54,984 --> 00:44:57,586
and the ones that are behind it. So in

848
00:44:57,608 --> 00:45:00,726
this example he is going to consider in

849
00:45:00,748 --> 00:45:02,566
this step is going to consider all the

850
00:45:02,588 --> 00:45:04,066
promote traces within its sensing

851
00:45:04,098 --> 00:45:06,966
radius, calculate the center of mass and

852
00:45:06,988 --> 00:45:09,850
then consider it as its next position.

853
00:45:09,920 --> 00:45:13,206
And it keep doing this till it reaches

854
00:45:13,238 --> 00:45:16,790
to the proximity of the output node.

855
00:45:16,870 --> 00:45:20,410
And once this is done, it will decide

856
00:45:21,070 --> 00:45:24,014
which output node it will consider as

857
00:45:24,052 --> 00:45:27,086
its final position in its path from an

858
00:45:27,108 --> 00:45:31,680
input to the output. And then

859
00:45:32,290 --> 00:45:34,160
other ends will do the same.

860
00:45:34,710 --> 00:45:37,938
And then we'll have different paths from

861
00:45:38,024 --> 00:45:40,654
some input and some output.

862
00:45:40,782 --> 00:45:44,834
And then Cans will take these paths and

863
00:45:44,872 --> 00:45:48,326
then try to condense the nodes so that

864
00:45:48,348 --> 00:45:51,606
we don't have so much nodes that are

865
00:45:51,628 --> 00:45:53,698
very close to each other. So the nodes

866
00:45:53,714 --> 00:45:56,454
that are within certain proximity will

867
00:45:56,572 --> 00:46:00,570
be clustered together using DB scan to

868
00:46:00,640 --> 00:46:04,780
have less number of nodes. And then

869
00:46:05,470 --> 00:46:07,500
the paths will be taken,

870
00:46:08,510 --> 00:46:10,970
collected and put in a structure,

871
00:46:12,110 --> 00:46:15,230
a neural network structure. And then

872
00:46:15,300 --> 00:46:17,614
sent to a worker process to train and

873
00:46:17,652 --> 00:46:19,614
test and then compare it to the

874
00:46:19,652 --> 00:46:21,946
population, the best performing RNNs.

875
00:46:21,978 --> 00:46:25,170
And the process will be almost the same

876
00:46:25,320 --> 00:46:27,874
from this point will be almost the same

877
00:46:27,912 --> 00:46:30,434
as ants once the structure is

878
00:46:30,472 --> 00:46:32,674
constructed. And the training and

879
00:46:32,712 --> 00:46:36,226
testing process will be the same as the

880
00:46:36,248 --> 00:46:38,070
ones that we discussed in ants.

881
00:46:40,810 --> 00:46:44,214
So this was also an animation which

882
00:46:44,252 --> 00:46:47,938
shows how these tasks

883
00:46:48,034 --> 00:46:51,226
are taken by the ants. Look from an

884
00:46:51,248 --> 00:46:55,226
input to an output in the 3D space 3D

885
00:46:55,248 --> 00:46:58,470
search space so ants,

886
00:46:58,550 --> 00:47:02,270
if we look at ants, ants have only

887
00:47:02,340 --> 00:47:04,954
eight tunable hyperparameters compared

888
00:47:05,002 --> 00:47:07,642
to when comparing this to ants and XM,

889
00:47:07,706 --> 00:47:09,214
it's half of the number of

890
00:47:09,332 --> 00:47:11,978
hyperparameters in the other methods.

891
00:47:12,154 --> 00:47:14,302
These hyperparameters are the number of

892
00:47:14,436 --> 00:47:16,306
layers of the search space, the number

893
00:47:16,328 --> 00:47:19,746
of time of lags, how many of them we

894
00:47:19,768 --> 00:47:21,938
have in search space. Number of agents,

895
00:47:22,024 --> 00:47:24,900
number of ants, which is similar to what

896
00:47:25,430 --> 00:47:28,870
we have a similar hyperparameter in ants

897
00:47:32,650 --> 00:47:34,966
the sensing radius of the agents or the

898
00:47:34,988 --> 00:47:38,486
ants the agents probability to create a

899
00:47:38,508 --> 00:47:43,238
new node which presents

900
00:47:43,254 --> 00:47:44,698
the exploration instead of the

901
00:47:44,704 --> 00:47:46,922
exploitation of the parameters or the

902
00:47:46,976 --> 00:47:49,820
hormone back traces in the space.

903
00:47:50,510 --> 00:47:53,786
Node condensation parameters

904
00:47:53,898 --> 00:47:58,814
or factors the

905
00:47:58,852 --> 00:48:01,802
variables that represent in the DB scan

906
00:48:01,866 --> 00:48:06,494
are considered also hyperparameters in

907
00:48:06,532 --> 00:48:09,738
cans for one updating parameter and for

908
00:48:09,764 --> 00:48:11,506
one volatility parameter and these are

909
00:48:11,528 --> 00:48:15,506
also present in ants and

910
00:48:15,608 --> 00:48:19,560
ACO based problems solutions sorry

911
00:48:21,850 --> 00:48:24,360
the experiments used three times years

912
00:48:25,690 --> 00:48:28,054
different number of parameters and

913
00:48:28,092 --> 00:48:31,634
different sizes. The results

914
00:48:31,682 --> 00:48:36,986
showed that the

915
00:48:37,008 --> 00:48:39,578
results that we got from Cans were very

916
00:48:39,744 --> 00:48:43,260
competing with ants and Exam.

917
00:48:44,270 --> 00:48:47,546
They weren't necessarily better,

918
00:48:47,648 --> 00:48:49,514
but they competed. They weren't the same

919
00:48:49,552 --> 00:48:53,740
level, but it also did

920
00:48:54,150 --> 00:48:57,330
didn't do very well in one of the

921
00:48:57,400 --> 00:49:04,274
database, the data sets. But that

922
00:49:04,312 --> 00:49:06,838
was comparing the performance as the

923
00:49:06,844 --> 00:49:10,002
mean absolute error, but comparing

924
00:49:10,066 --> 00:49:14,840
Kent's results from the

925
00:49:17,370 --> 00:49:18,120
sorry.

926
00:49:26,390 --> 00:49:28,434
The size of the neural networks that we

927
00:49:28,472 --> 00:49:31,700
got from Cairns, we saw that

928
00:49:32,150 --> 00:49:36,930
Cain's structures were sparser,

929
00:49:37,510 --> 00:49:39,750
so the performance was competing with

930
00:49:39,900 --> 00:49:42,630
ants and exam. But the structures

931
00:49:44,330 --> 00:49:47,290
compared to ants were much sparser,

932
00:49:48,350 --> 00:49:51,450
similar to exam. The size

933
00:49:51,520 --> 00:49:52,986
of the structures that we're getting

934
00:49:53,008 --> 00:49:56,454
from Exam. Remember, exam is genetic

935
00:49:56,502 --> 00:49:59,350
based, meaning that we start the

936
00:49:59,360 --> 00:50:01,114
optimization process using the minimal

937
00:50:01,162 --> 00:50:04,640
structure elements but

938
00:50:05,410 --> 00:50:08,030
it was susceptible to local minimum

939
00:50:09,170 --> 00:50:12,914
traps and Tense is

940
00:50:12,952 --> 00:50:15,394
not susceptible to this problem. But it

941
00:50:15,432 --> 00:50:19,150
also gave us smaller structures

942
00:50:19,310 --> 00:50:21,086
with performances that were competing

943
00:50:21,118 --> 00:50:23,830
with other methods. Exam and ants.

944
00:50:28,750 --> 00:50:32,266
So the advantages of Cans is

945
00:50:32,288 --> 00:50:35,526
that it has an unbalanced space compared

946
00:50:35,558 --> 00:50:39,806
to ants compared the

947
00:50:39,828 --> 00:50:42,826
results that came out were good compared

948
00:50:42,858 --> 00:50:45,866
to ants and Exam. The tunable

949
00:50:45,898 --> 00:50:48,574
hyperparameters are half of these in

950
00:50:48,612 --> 00:50:52,382
Exam and ants and also it indirectly

951
00:50:52,446 --> 00:50:54,846
encoded the neural topology to 3D thrash

952
00:50:54,878 --> 00:50:58,498
spaces. So this is one of maybe

953
00:50:58,664 --> 00:51:00,738
the contributions of important

954
00:51:00,824 --> 00:51:04,946
contributions cans then

955
00:51:05,048 --> 00:51:09,158
so far ants and Cans are solutions that

956
00:51:09,244 --> 00:51:12,818
applied neural topology

957
00:51:12,914 --> 00:51:15,162
architecture search meaning that they

958
00:51:15,216 --> 00:51:19,334
did not optimize the synaptic parameters

959
00:51:19,382 --> 00:51:21,146
of the neural networks during the

960
00:51:21,168 --> 00:51:22,538
optimization process or during the

961
00:51:22,544 --> 00:51:26,906
evolution process. So we

962
00:51:26,928 --> 00:51:30,314
thought of actually making ends capable

963
00:51:30,362 --> 00:51:33,486
of doing this as well. So to train the

964
00:51:33,508 --> 00:51:36,346
neural network or optimize the synaptic

965
00:51:36,458 --> 00:51:38,878
weights, the weights of the structure of

966
00:51:38,884 --> 00:51:40,786
the neural networks during the

967
00:51:40,808 --> 00:51:43,406
optimization process, the structural

968
00:51:43,438 --> 00:51:45,666
optimization process. So we added a

969
00:51:45,688 --> 00:51:48,020
fourth dimension to the search space

970
00:51:49,270 --> 00:51:52,274
which we embedded the weights of

971
00:51:52,312 --> 00:51:56,214
snapdragon parameters in

972
00:51:56,252 --> 00:52:00,050
that map, these parameters

973
00:52:00,130 --> 00:52:02,550
in that new dimension.

974
00:52:03,850 --> 00:52:07,242
We also wanted the ants to be

975
00:52:07,296 --> 00:52:11,370
self aware and try to evolve

976
00:52:12,670 --> 00:52:16,102
themselves through the evolution

977
00:52:16,166 --> 00:52:19,806
process so that they can adapt to

978
00:52:19,828 --> 00:52:22,734
the changes or adapt themselves so that

979
00:52:22,772 --> 00:52:24,910
they can give us better performance.

980
00:52:26,210 --> 00:52:28,202
We want them to change their behavior,

981
00:52:28,266 --> 00:52:30,766
their characteristics like the sensing

982
00:52:30,798 --> 00:52:33,522
radius they had before, for example,

983
00:52:33,656 --> 00:52:36,578
that can be a variable that can change

984
00:52:36,664 --> 00:52:38,718
through the iteration thing and evolve

985
00:52:38,734 --> 00:52:40,962
through the iterations. Each end can

986
00:52:41,016 --> 00:52:46,934
change these characteristics as

987
00:52:46,972 --> 00:52:50,774
the evolution goes on on progress based

988
00:52:50,812 --> 00:52:52,178
on the performance of the neural

989
00:52:52,194 --> 00:52:53,750
networks that they are generating.

990
00:53:01,030 --> 00:53:04,370
So the advantage of

991
00:53:04,440 --> 00:53:07,826
doing this was that it eliminated the

992
00:53:07,848 --> 00:53:10,034
back propagation process which is the

993
00:53:10,072 --> 00:53:13,126
most computationally expensive part in

994
00:53:13,148 --> 00:53:17,266
the evolution process in the methods

995
00:53:17,298 --> 00:53:19,526
that we use so far in exam ends and

996
00:53:19,548 --> 00:53:22,262
Cans. So eliminating the back

997
00:53:22,316 --> 00:53:26,410
propagation gave us much faster

998
00:53:26,830 --> 00:53:28,220
evolution process.

999
00:53:30,350 --> 00:53:33,994
The results we got I will discuss the

1000
00:53:34,032 --> 00:53:35,978
graph on the right hand side first which

1001
00:53:36,064 --> 00:53:38,222
discusses the fitness or the Ms main

1002
00:53:38,276 --> 00:53:40,174
absolute error of the results we got

1003
00:53:40,212 --> 00:53:44,154
from back propagation

1004
00:53:44,202 --> 00:53:46,746
free cans, the four dimension cans

1005
00:53:46,858 --> 00:53:49,294
compared to the normal cans, the back

1006
00:53:49,332 --> 00:53:52,626
propagation cans and ants. So the

1007
00:53:52,648 --> 00:53:55,794
results showed us that again

1008
00:53:55,832 --> 00:53:58,674
on a particular database that that

1009
00:53:58,712 --> 00:54:01,454
propagation cans and that propagation

1010
00:54:01,502 --> 00:54:05,240
free cans did quite a similar job

1011
00:54:06,650 --> 00:54:08,886
but they were both better than ants on

1012
00:54:08,908 --> 00:54:11,814
this particular database. But the

1013
00:54:11,852 --> 00:54:14,774
actually main contribution or the main

1014
00:54:14,812 --> 00:54:17,978
advantage of applying Cans shows up in

1015
00:54:18,064 --> 00:54:20,506
the graph on the left hand side.

1016
00:54:20,688 --> 00:54:23,334
Because we can see that if you compare

1017
00:54:23,382 --> 00:54:27,130
the results based on the time

1018
00:54:27,280 --> 00:54:29,482
of the evolution based on the evolution

1019
00:54:29,546 --> 00:54:34,846
time, we see that it

1020
00:54:34,868 --> 00:54:38,766
took much less time than the

1021
00:54:38,788 --> 00:54:41,906
back propagation version of cans and the

1022
00:54:41,928 --> 00:54:45,860
ants using these different

1023
00:54:47,750 --> 00:54:49,090
number of ants,

1024
00:54:51,590 --> 00:54:55,254
Also this graph shows how fast back

1025
00:54:55,292 --> 00:54:57,766
propagation precans compared to the

1026
00:54:57,788 --> 00:54:59,430
normal cans.

1027
00:55:02,010 --> 00:55:04,626
In this figure the curves and the dollar

1028
00:55:04,658 --> 00:55:09,562
lines shows the time that

1029
00:55:09,696 --> 00:55:12,170
back propagation free cans and cans took

1030
00:55:12,320 --> 00:55:14,278
to prepare or generate the neural

1031
00:55:14,294 --> 00:55:17,034
networks. We can see that back

1032
00:55:17,072 --> 00:55:20,714
propagation frequency took more time to

1033
00:55:20,752 --> 00:55:22,742
prepare or generate neural networks

1034
00:55:22,806 --> 00:55:25,038
compared to back propagation cans or the

1035
00:55:25,044 --> 00:55:27,486
normal cans. And that's because the

1036
00:55:27,508 --> 00:55:29,966
fourth dimension also has to evolve the

1037
00:55:29,988 --> 00:55:31,826
ants or the agents through the

1038
00:55:31,848 --> 00:55:34,226
iteration, so needs more time to do

1039
00:55:34,248 --> 00:55:38,738
that. But the

1040
00:55:38,744 --> 00:55:40,962
other two curves in dashed lines, here

1041
00:55:41,016 --> 00:55:44,438
are the lines that shows the amount of

1042
00:55:44,444 --> 00:55:47,080
time it took for the two methods to

1043
00:55:49,450 --> 00:55:52,578
train and validate the neural networks.

1044
00:55:52,754 --> 00:55:55,366
So backpropagation free cans doesn't

1045
00:55:55,398 --> 00:55:57,526
have to train the neural network doesn't

1046
00:55:57,638 --> 00:56:00,490
do backpropagation.

1047
00:56:01,150 --> 00:56:04,410
So you can see that the time it took is

1048
00:56:04,480 --> 00:56:07,546
in an order of magnitude less than the

1049
00:56:07,568 --> 00:56:10,954
back propagation version of camps. And

1050
00:56:11,072 --> 00:56:13,370
these sort of lines shows the cumulative

1051
00:56:13,450 --> 00:56:16,494
time took for both methods and of course

1052
00:56:16,532 --> 00:56:19,306
it shows that back propagation precances

1053
00:56:19,338 --> 00:56:23,166
took much much less time than the back

1054
00:56:23,188 --> 00:56:25,794
propagation precance took much less time

1055
00:56:25,992 --> 00:56:28,370
than the back propagation cans.

1056
00:56:29,670 --> 00:56:33,320
The future directions that we are future

1057
00:56:33,690 --> 00:56:37,062
points that we're concerning for sorry,

1058
00:56:37,116 --> 00:56:38,374
the points that we're concerning for

1059
00:56:38,412 --> 00:56:41,766
future work are to turn cans to a

1060
00:56:41,788 --> 00:56:44,374
complete continuous search space. So as

1061
00:56:44,412 --> 00:56:45,240
you saw,

1062
00:56:48,890 --> 00:56:52,810
the search space for Kents is not purely

1063
00:56:54,350 --> 00:56:56,586
continuous because the time lags are

1064
00:56:56,608 --> 00:56:59,566
represented by discrete layers. We want

1065
00:56:59,588 --> 00:57:01,370
to replace this with the continuous

1066
00:57:01,450 --> 00:57:04,510
dimension continuous layer representing

1067
00:57:05,170 --> 00:57:08,478
the time lag in RNNs. However,

1068
00:57:08,564 --> 00:57:10,660
this is a little bit challenging because

1069
00:57:11,270 --> 00:57:14,702
the time lags has to be known prior

1070
00:57:14,766 --> 00:57:17,940
to any optimization process because

1071
00:57:18,630 --> 00:57:22,690
other than that we will be mapping

1072
00:57:24,010 --> 00:57:27,814
the whole time series as time

1073
00:57:27,852 --> 00:57:30,886
lags and then picking the time lags from

1074
00:57:30,908 --> 00:57:35,480
them which is not feasible. So this is

1075
00:57:36,270 --> 00:57:38,074
the first point we're considering to

1076
00:57:38,192 --> 00:57:40,634
investigate for in future work. The

1077
00:57:40,672 --> 00:57:44,410
second one is to investigate this

1078
00:57:44,560 --> 00:57:48,780
finding that we found

1079
00:57:49,230 --> 00:57:52,878
in the results in ants where

1080
00:57:53,044 --> 00:57:55,034
the recurrent connections disappeared

1081
00:57:55,082 --> 00:57:57,790
from the best performing structures.

1082
00:57:58,930 --> 00:58:00,626
The theory that we have is that the

1083
00:58:00,648 --> 00:58:03,554
memory based cells replace these

1084
00:58:03,592 --> 00:58:05,010
connections, give us the information

1085
00:58:05,080 --> 00:58:08,434
that we need from the pastime steps so

1086
00:58:08,472 --> 00:58:11,474
that they were more effective, more

1087
00:58:11,512 --> 00:58:14,066
efficient in doing this compared to the

1088
00:58:14,088 --> 00:58:16,020
recycle connections. So we have to

1089
00:58:16,470 --> 00:58:19,560
expand on this and investigate it more.

1090
00:58:20,650 --> 00:58:23,240
The last thing we want to consider also,

1091
00:58:24,010 --> 00:58:26,186
this is one of the top things that we

1092
00:58:26,208 --> 00:58:29,066
want. These three points are the top on

1093
00:58:29,088 --> 00:58:33,340
our list. The third one is to actually

1094
00:58:35,550 --> 00:58:39,134
consider one of the concepts that was

1095
00:58:39,172 --> 00:58:41,694
coined in a book by Dr.

1096
00:58:41,812 --> 00:58:46,080
Deborah Gordon where she mentioned that

1097
00:58:47,410 --> 00:58:51,210
the living organism

1098
00:58:51,370 --> 00:58:53,854
in ants world are not the ants

1099
00:58:53,902 --> 00:58:55,822
themselves, but they are the colonies.

1100
00:58:55,886 --> 00:59:00,914
The colonies are the organisms that they

1101
00:59:00,952 --> 00:59:04,014
start, and they grow, and they interact

1102
00:59:04,062 --> 00:59:05,762
with the environment and the ecosystem.

1103
00:59:05,826 --> 00:59:08,486
They interact with other colonies, and

1104
00:59:08,508 --> 00:59:11,346
they die at some point. And the ants

1105
00:59:11,378 --> 00:59:13,414
themselves are not organisms. They are

1106
00:59:13,452 --> 00:59:15,654
the cells of these organisms, this

1107
00:59:15,692 --> 00:59:18,394
colony organisms. So we want to take

1108
00:59:18,432 --> 00:59:20,886
that concept and apply it in our methods

1109
00:59:20,918 --> 00:59:24,090
to have number of colonies living

1110
00:59:24,160 --> 00:59:28,142
together in parallel, evolving and

1111
00:59:28,196 --> 00:59:31,566
communicating with each other. And we

1112
00:59:31,588 --> 00:59:32,958
want to see if this would give us a

1113
00:59:32,964 --> 00:59:35,726
better performance, because, after all,

1114
00:59:35,748 --> 00:59:39,278
we're trying to mimic nature in the

1115
00:59:39,284 --> 00:59:41,730
solutions we're trying to investigate.

1116
00:59:44,470 --> 00:59:47,474
So with this, I'm done with my

1117
00:59:47,512 --> 00:59:49,890
presentation, and if you have any

1118
00:59:49,960 --> 00:59:50,580
questions.

1119
00:59:55,760 --> 00:59:59,020
All right. Awesome. Wow, what a great

1120
00:59:59,090 --> 01:00:02,732
presentation. How about Travis and

1121
01:00:02,786 --> 01:00:04,428
Alexander? Either of you which want to

1122
01:00:04,434 --> 01:00:06,496
go first, please feel free to give an

1123
01:00:06,518 --> 01:00:08,748
introduction and any primary remarks

1124
01:00:08,764 --> 01:00:09,970
that you have on that.

1125
01:00:16,660 --> 01:00:18,704
Okay. I can say something. I don't have

1126
01:00:18,742 --> 01:00:20,952
too much to add. I think Abdul Rahman

1127
01:00:21,116 --> 01:00:24,068
did a pretty good job going over the

1128
01:00:24,074 --> 01:00:27,024
core bits of the work. I'm Alex Rorbia.

1129
01:00:27,152 --> 01:00:29,508
I'm an assistant professor in computer

1130
01:00:29,594 --> 01:00:33,008
science, an affiliate professor

1131
01:00:33,024 --> 01:00:34,936
in psychology, and affiliate faculty in

1132
01:00:34,958 --> 01:00:36,936
computational neuroscience at the

1133
01:00:36,958 --> 01:00:39,944
Rochester Institute of Technology. And I

1134
01:00:39,982 --> 01:00:43,092
work on a lot of stuff, but primarily

1135
01:00:43,156 --> 01:00:44,804
predictive coding, active inference

1136
01:00:44,852 --> 01:00:47,048
variational, free energy, a lot of the

1137
01:00:47,054 --> 01:00:48,248
stuff that's actually of interest to

1138
01:00:48,254 --> 01:00:51,944
this. Yeah, and this was a particularly

1139
01:00:51,992 --> 01:00:53,708
interesting project for me because a

1140
01:00:53,714 --> 01:00:56,556
branch of my own research is working in

1141
01:00:56,578 --> 01:00:59,280
neurovolutionary methods or even just

1142
01:00:59,350 --> 01:01:01,436
nature inspired metahuristic

1143
01:01:01,468 --> 01:01:04,752
optimization. And when I got the

1144
01:01:04,806 --> 01:01:06,844
pleasure of working with Abdul Rahman

1145
01:01:06,892 --> 01:01:11,024
when he was a PhD student at Know,

1146
01:01:11,062 --> 01:01:14,016
we talked a lot about the ant colony

1147
01:01:14,048 --> 01:01:16,004
based optimization approaches, and I

1148
01:01:16,042 --> 01:01:19,108
encouraged him to look into sort of,

1149
01:01:19,114 --> 01:01:20,964
like, the origins and also try to

1150
01:01:21,002 --> 01:01:22,896
understand actually how physical ants

1151
01:01:22,928 --> 01:01:24,496
behave. So that was always fascinating

1152
01:01:24,528 --> 01:01:27,784
to me. I don't have too much to add in

1153
01:01:27,822 --> 01:01:29,416
terms of the technical parts. I think he

1154
01:01:29,438 --> 01:01:31,496
covered all the core results. The only

1155
01:01:31,518 --> 01:01:33,988
thing I like to think about. And I'm

1156
01:01:34,164 --> 01:01:36,372
actually more fascinated, too, to hear

1157
01:01:36,446 --> 01:01:38,472
from the active inference institute

1158
01:01:38,536 --> 01:01:41,544
their interest in the ant colony methods

1159
01:01:41,592 --> 01:01:43,564
and particularly what was interesting to

1160
01:01:43,602 --> 01:01:47,180
them. Because thinking about what does

1161
01:01:47,250 --> 01:01:49,564
ant colony optimization how do you view

1162
01:01:49,602 --> 01:01:52,412
it from an active inference perspective

1163
01:01:52,556 --> 01:01:54,208
is, I think, particularly interesting

1164
01:01:54,294 --> 01:01:56,448
and thinking. And I even had a thought,

1165
01:01:56,534 --> 01:01:57,744
I don't know if this was a question

1166
01:01:57,782 --> 01:02:00,688
among the audience's mind. Do the little

1167
01:02:00,774 --> 01:02:03,904
ant agents or the cant agents that run

1168
01:02:03,942 --> 01:02:07,556
and described what is

1169
01:02:07,578 --> 01:02:09,796
there a way to start to view them as a

1170
01:02:09,818 --> 01:02:11,904
multi agent system that's optimizing

1171
01:02:11,952 --> 01:02:14,420
some variational free energy quantity?

1172
01:02:15,160 --> 01:02:17,748
Because a lot of even Carl's work I

1173
01:02:17,754 --> 01:02:19,816
mean, I collaborate with him, sort of

1174
01:02:19,838 --> 01:02:21,464
touches into this area of, like, well,

1175
01:02:21,502 --> 01:02:22,884
what happens with collective

1176
01:02:22,932 --> 01:02:25,844
intelligence and societal organization?

1177
01:02:25,972 --> 01:02:28,108
And you can kind of look at free energy

1178
01:02:28,194 --> 01:02:31,448
from these very high level viewpoints

1179
01:02:31,544 --> 01:02:33,192
all the way down to fine grained

1180
01:02:33,256 --> 01:02:35,724
cellular activity. And so I'm actually

1181
01:02:35,762 --> 01:02:38,572
more curious to know Daniel and anyone

1182
01:02:38,626 --> 01:02:40,520
else in the active inferences who can

1183
01:02:40,610 --> 01:02:42,128
sort of explain their particular

1184
01:02:42,214 --> 01:02:44,544
interest in ant based optimization and

1185
01:02:44,582 --> 01:02:46,476
meta heuristic optimization. I'm curious

1186
01:02:46,508 --> 01:02:48,224
to know that. But there could be some

1187
01:02:48,262 --> 01:02:51,248
interesting viewpoints, like, what is

1188
01:02:51,414 --> 01:02:53,116
the free energy? I think the ants

1189
01:02:53,148 --> 01:02:56,404
themselves are very mean.

1190
01:02:56,442 --> 01:02:58,068
We have made them more intelligent. I

1191
01:02:58,074 --> 01:02:59,824
know Abdul Raman and I talked at length

1192
01:02:59,872 --> 01:03:02,084
about, well, what if we even made them,

1193
01:03:02,122 --> 01:03:03,936
for example, have, like, a reinforcement

1194
01:03:03,968 --> 01:03:06,136
learning control system? And you could

1195
01:03:06,158 --> 01:03:07,704
even imagine, well, now, what if the

1196
01:03:07,742 --> 01:03:10,984
cans themselves engage in

1197
01:03:11,022 --> 01:03:13,688
a form of active inference themselves?

1198
01:03:13,854 --> 01:03:15,496
What would that look like for the

1199
01:03:15,518 --> 01:03:17,336
system? And they are optimizing their

1200
01:03:17,358 --> 01:03:19,932
own free energy. Those are just fun

1201
01:03:19,986 --> 01:03:21,964
little thought experiments. We have

1202
01:03:22,002 --> 01:03:24,332
obviously not worked on them. At least

1203
01:03:24,386 --> 01:03:27,404
Abdul Rahman was never exposed by me to

1204
01:03:27,442 --> 01:03:31,868
that part of my world and biomimetic

1205
01:03:31,964 --> 01:03:34,092
intelligence. So those are my comments.

1206
01:03:34,156 --> 01:03:35,516
I'm not sure if they're particularly

1207
01:03:35,548 --> 01:03:37,888
helpful, but they're very general, and

1208
01:03:37,974 --> 01:03:39,696
I'm actually more curious to know from

1209
01:03:39,718 --> 01:03:41,892
the active inference institute, their

1210
01:03:41,946 --> 01:03:44,132
interest in it and where does that maybe

1211
01:03:44,186 --> 01:03:46,132
perhaps intersect? Or this is just like,

1212
01:03:46,186 --> 01:03:50,870
oh, we know interesting topics and yeah.

1213
01:03:52,280 --> 01:03:54,728
Thank you. Alexander. Travis, you want

1214
01:03:54,734 --> 01:03:56,856
to say hello and give any reflections on

1215
01:03:56,878 --> 01:03:57,690
the talk?

1216
01:04:00,220 --> 01:04:01,768
Sure, I'll come in. Hopefully you can

1217
01:04:01,774 --> 01:04:04,888
see me a little bit. My jungle here.

1218
01:04:04,974 --> 01:04:06,948
Hi, I'm Travis is Sell. I'm an associate

1219
01:04:06,964 --> 01:04:09,796
professor at RIT. I'm also the graduate

1220
01:04:09,828 --> 01:04:11,292
program director for a master's in data

1221
01:04:11,346 --> 01:04:12,732
science. So if any of you are interested

1222
01:04:12,786 --> 01:04:14,812
in any of this or data science, please

1223
01:04:14,946 --> 01:04:17,996
shoot me an email. No, I thought this

1224
01:04:18,018 --> 01:04:20,988
work was really very interesting in a

1225
01:04:20,994 --> 01:04:22,416
lot of ways while Abdul Rahman was

1226
01:04:22,438 --> 01:04:24,976
working on it. I think one of the

1227
01:04:24,998 --> 01:04:27,564
coolest parts about it is a popular

1228
01:04:27,612 --> 01:04:30,876
neuroevolution algorithm that came after

1229
01:04:30,918 --> 01:04:32,900
neat is called hyperneet, and it

1230
01:04:32,970 --> 01:04:36,512
transforms the discrete

1231
01:04:36,576 --> 01:04:38,176
search space of neural architecture

1232
01:04:38,208 --> 01:04:40,928
search into a continuous one. And it's

1233
01:04:40,944 --> 01:04:43,056
shown to be a pretty powerful method.

1234
01:04:43,088 --> 01:04:45,152
Well, this version of ant colony

1235
01:04:45,216 --> 01:04:47,064
optimization, the new one, does the same

1236
01:04:47,102 --> 01:04:48,712
thing. It makes the search space

1237
01:04:48,766 --> 01:04:50,744
continuous. But what I found was really

1238
01:04:50,782 --> 01:04:54,024
cool about it was that as opposed to

1239
01:04:54,062 --> 01:04:55,572
traditional ant colony optimization

1240
01:04:55,636 --> 01:04:57,064
where you have a graph and you just send

1241
01:04:57,102 --> 01:04:59,216
the ants along the edges of the graph

1242
01:04:59,268 --> 01:05:01,516
and you take the best paths and

1243
01:05:01,538 --> 01:05:03,516
construct a graph after it. Here the

1244
01:05:03,538 --> 01:05:05,356
ants are actually working like ants in

1245
01:05:05,378 --> 01:05:07,580
the real world where they'll move a

1246
01:05:07,650 --> 01:05:09,276
continuous amount of space, not just

1247
01:05:09,298 --> 01:05:12,032
from point A to point B and actually

1248
01:05:12,086 --> 01:05:14,384
dropping down pheromones. And it's more

1249
01:05:14,422 --> 01:05:17,536
like a real simulation of how ants would

1250
01:05:17,558 --> 01:05:20,236
move around in the real world. And we're

1251
01:05:20,268 --> 01:05:22,068
getting better results from it than some

1252
01:05:22,074 --> 01:05:25,990
of the older methods. So I think

1253
01:05:26,520 --> 01:05:28,884
that really made me kind of happy to see

1254
01:05:28,922 --> 01:05:32,020
that and thought it made it as very

1255
01:05:32,090 --> 01:05:33,190
interesting work.

1256
01:05:35,900 --> 01:05:37,896
That's awesome. Ahmed, want to add

1257
01:05:37,918 --> 01:05:39,864
anything or I'm happy to give a thought

1258
01:05:39,902 --> 01:05:41,512
on the answer and ask some questions

1259
01:05:41,566 --> 01:05:44,776
from the live chat. I'm good.

1260
01:05:44,958 --> 01:05:46,764
Travis, I think, covered all the things

1261
01:05:46,802 --> 01:05:49,870
that I wanted to say about,

1262
01:05:50,480 --> 01:05:52,504
I think just one thing that Alex

1263
01:05:52,552 --> 01:05:55,052
mentioned that we give higher

1264
01:05:55,106 --> 01:05:56,636
intelligence to the agent. This is

1265
01:05:56,658 --> 01:05:58,440
something that we discussed and I'm

1266
01:05:58,600 --> 01:06:01,052
pretty much open to that. I started

1267
01:06:01,106 --> 01:06:02,556
actually to think about it, but I didn't

1268
01:06:02,588 --> 01:06:05,504
implement anything yet. However, the

1269
01:06:05,542 --> 01:06:07,328
last thing that I discussed in the

1270
01:06:07,334 --> 01:06:09,264
future directions is something that I

1271
01:06:09,302 --> 01:06:11,136
actually started working on, but I

1272
01:06:11,158 --> 01:06:13,440
didn't start the experimentations yet.

1273
01:06:13,510 --> 01:06:15,936
So hopefully we will see something out

1274
01:06:15,958 --> 01:06:19,430
of that. Awesome.

1275
01:06:19,800 --> 01:06:22,500
Well, there's a ton of ways to go and

1276
01:06:22,570 --> 01:06:23,736
isn't that kind of one of the

1277
01:06:23,758 --> 01:06:25,544
fundamental questions like in an

1278
01:06:25,582 --> 01:06:28,968
interactive setting, either pure agent

1279
01:06:29,054 --> 01:06:31,656
stigma g interaction or multi agent,

1280
01:06:31,758 --> 01:06:34,244
but ultimately mediated through multiple

1281
01:06:34,292 --> 01:06:36,312
stigma g's with like reading and writing

1282
01:06:36,376 --> 01:06:38,732
and error correcting code. So in that

1283
01:06:38,786 --> 01:06:42,284
communication setting, I felt like the

1284
01:06:42,322 --> 01:06:45,544
work generalizes along multiple

1285
01:06:45,592 --> 01:06:49,704
dimensions that previously approaches

1286
01:06:49,752 --> 01:06:51,456
to multi agent just didn't have those

1287
01:06:51,478 --> 01:06:53,856
kinds of flexibilities like the

1288
01:06:53,878 --> 01:06:55,964
continuous time feature and several

1289
01:06:56,012 --> 01:07:01,348
other features. And I

1290
01:07:01,354 --> 01:07:02,736
guess with respect to the ants

1291
01:07:02,768 --> 01:07:06,292
themselves, I did five summers of field

1292
01:07:06,346 --> 01:07:10,100
work with ants in the USA Southwest in

1293
01:07:10,170 --> 01:07:13,692
Arizona and observed a lot of foraging

1294
01:07:13,776 --> 01:07:17,688
activity. So that problem

1295
01:07:17,854 --> 01:07:21,496
or that context or setting is really a

1296
01:07:21,518 --> 01:07:23,704
fun one and a pervasive one across any

1297
01:07:23,742 --> 01:07:26,536
kind of living system, anything that's

1298
01:07:26,568 --> 01:07:31,004
going to be active and living. So why

1299
01:07:31,042 --> 01:07:34,940
did you pursue foraging type algorithms

1300
01:07:35,360 --> 01:07:38,844
overall? And does this class

1301
01:07:38,962 --> 01:07:42,588
include the interaction based methods

1302
01:07:42,684 --> 01:07:45,232
with direct agent contacts that

1303
01:07:45,286 --> 01:07:47,312
Professor Gordon highlights in the Ant

1304
01:07:47,366 --> 01:07:51,170
Encounters book? Yeah,

1305
01:07:51,540 --> 01:07:54,676
I will try to respond to this, but I

1306
01:07:54,698 --> 01:07:57,060
just want to mention that the first

1307
01:07:57,130 --> 01:07:59,664
thought about this actually Travis

1308
01:07:59,712 --> 01:08:01,936
started this idea about using N colon

1309
01:08:01,968 --> 01:08:05,216
optimization and neural sector search by

1310
01:08:05,258 --> 01:08:07,716
applying this method in simpler neural

1311
01:08:07,748 --> 01:08:09,588
networks. Elman and Gordon neural

1312
01:08:09,604 --> 01:08:11,336
networks required neural networks and I

1313
01:08:11,358 --> 01:08:12,792
took the leap from there and started

1314
01:08:12,846 --> 01:08:16,368
working on my thesis, my PhD thesis.

1315
01:08:16,564 --> 01:08:19,724
So we thought about this idea. I think I

1316
01:08:19,762 --> 01:08:22,332
mentioned a little bit about why we used

1317
01:08:22,466 --> 01:08:24,892
an optimization in previous slide, but

1318
01:08:25,026 --> 01:08:28,552
just repeating that for the audience to

1319
01:08:28,626 --> 01:08:30,864
make sure that they got we thought about

1320
01:08:30,902 --> 01:08:34,576
this idea because Nko optimization was

1321
01:08:34,598 --> 01:08:39,788
applied as a graphic

1322
01:08:39,884 --> 01:08:43,776
solution, and we thought that why

1323
01:08:43,798 --> 01:08:45,264
not neural networks since they are

1324
01:08:45,302 --> 01:08:48,436
investments? Neural networks, they are

1325
01:08:48,458 --> 01:08:50,068
directed graphs. Neural networks are

1326
01:08:50,074 --> 01:08:52,644
directed graphs because the flow of

1327
01:08:52,682 --> 01:08:54,240
information goes from one direction to

1328
01:08:54,250 --> 01:08:55,556
another direction. So they are directed,

1329
01:08:55,588 --> 01:08:57,256
but they also graph because they are

1330
01:08:57,278 --> 01:09:00,490
nodes connected with edges. And our

1331
01:09:01,020 --> 01:09:04,728
ultimate aim or goal was to optimize the

1332
01:09:04,734 --> 01:09:06,596
structure by removing and adding

1333
01:09:06,628 --> 01:09:08,456
elements to it so that it gives us

1334
01:09:08,478 --> 01:09:09,050
better.

1335
01:09:11,120 --> 01:09:14,296
Awesome. And one way that Professor

1336
01:09:14,328 --> 01:09:17,020
Gordon and others have talked about that

1337
01:09:17,090 --> 01:09:19,560
bi directional learning relationship

1338
01:09:19,650 --> 01:09:21,088
between the computer science and the

1339
01:09:21,094 --> 01:09:23,696
math and the analytical formulations and

1340
01:09:23,718 --> 01:09:25,312
then the field work and the actual

1341
01:09:25,366 --> 01:09:28,336
behavior is because the thousands of

1342
01:09:28,358 --> 01:09:31,480
species of ants. They're working amidst

1343
01:09:31,660 --> 01:09:35,140
a huge range of ecologies with

1344
01:09:35,210 --> 01:09:36,644
all these different patterns of

1345
01:09:36,682 --> 01:09:38,404
regularities, all these different

1346
01:09:38,442 --> 01:09:40,736
resource distributions and foraging.

1347
01:09:40,848 --> 01:09:43,296
It's amazing how general it is. Yet it's

1348
01:09:43,328 --> 01:09:44,932
also just one of the functionalities

1349
01:09:44,996 --> 01:09:46,792
that need to occur in terms of these

1350
01:09:46,846 --> 01:09:49,300
even slower processes, like allocation

1351
01:09:49,460 --> 01:09:52,548
of tissue to faster processes,

1352
01:09:52,644 --> 01:09:54,888
even of response to alarm pheromone. So

1353
01:09:54,894 --> 01:09:58,540
it's like this one class of algorithms

1354
01:09:59,120 --> 01:10:02,412
clearly scales across from few.

1355
01:10:02,546 --> 01:10:04,616
Ultimately one some of these foraging

1356
01:10:04,648 --> 01:10:08,088
algorithms are lone foragers they don't

1357
01:10:08,104 --> 01:10:09,356
leave pheromone trails. So it's like

1358
01:10:09,378 --> 01:10:10,972
even the idea of leaving a single

1359
01:10:11,026 --> 01:10:14,256
positive pheromone or leaving but also

1360
01:10:14,278 --> 01:10:15,788
there's things models can do that ants

1361
01:10:15,804 --> 01:10:17,440
can't do, like the time travel,

1362
01:10:17,510 --> 01:10:20,828
pheromone, lossless perception, high

1363
01:10:20,934 --> 01:10:23,924
dimensional signaling profiles that

1364
01:10:23,962 --> 01:10:25,824
can't occur just with like finite

1365
01:10:25,872 --> 01:10:29,844
amounts of molecules. Now they are

1366
01:10:29,882 --> 01:10:32,768
just persisting on their path as active

1367
01:10:32,784 --> 01:10:35,636
agents within one generation with a

1368
01:10:35,658 --> 01:10:37,108
variational free energy at the

1369
01:10:37,114 --> 01:10:38,280
behavioral scale, then across

1370
01:10:38,350 --> 01:10:40,388
generations with that evolutionary

1371
01:10:40,564 --> 01:10:43,032
layer. And the relationships between the

1372
01:10:43,086 --> 01:10:45,416
neural network implementation and the

1373
01:10:45,438 --> 01:10:47,268
active inference model. They're kind of

1374
01:10:47,294 --> 01:10:48,856
like two ways of describing,

1375
01:10:48,968 --> 01:10:51,628
implementing. Yeah.

1376
01:10:51,794 --> 01:10:53,436
I'd be curious to hear any of your

1377
01:10:53,458 --> 01:10:54,776
thoughts on where you see active

1378
01:10:54,808 --> 01:10:57,852
inference coming into play or how do you

1379
01:10:57,906 --> 01:11:00,576
see ultimately similarities and

1380
01:11:00,598 --> 01:11:02,672
differences between neural network based

1381
01:11:02,726 --> 01:11:05,152
approaches and active inference based

1382
01:11:05,206 --> 01:11:07,584
approaches. Are they the same

1383
01:11:07,782 --> 01:11:10,080
complementary, overlapping.

1384
01:11:12,760 --> 01:11:14,550
Guys? Want to take this one?

1385
01:11:16,360 --> 01:11:17,952
I think this might be more of an Alex

1386
01:11:18,016 --> 01:11:21,670
question, to be honest. That's the test.

1387
01:11:22,840 --> 01:11:25,844
Well, yeah, I mean, if you want to keep

1388
01:11:25,882 --> 01:11:27,176
it on active inference. But I think,

1389
01:11:27,198 --> 01:11:28,664
Travis, you're going to need to tag in

1390
01:11:28,702 --> 01:11:29,912
when you want to get into the very

1391
01:11:29,966 --> 01:11:33,220
specifics of the actual ant colony

1392
01:11:33,300 --> 01:11:35,752
details because, again, I kind of see

1393
01:11:35,806 --> 01:11:37,692
the ant colony optimization from a more

1394
01:11:37,746 --> 01:11:42,188
global point of view. I would say so

1395
01:11:42,274 --> 01:11:45,756
to be mean this particular work. So that

1396
01:11:45,778 --> 01:11:47,340
way Abdul Rahman is not also completely

1397
01:11:47,410 --> 01:11:49,656
blindsided by your question, Daniel,

1398
01:11:49,768 --> 01:11:51,852
even though the name is in the institute

1399
01:11:51,916 --> 01:11:54,800
itself, this isn't an active inference.

1400
01:11:55,140 --> 01:11:56,768
You know, again, while there are

1401
01:11:56,854 --> 01:11:59,136
obviously, as you pointed out, lots of

1402
01:11:59,158 --> 01:12:00,736
interesting elements like, for example,

1403
01:12:00,838 --> 01:12:03,296
the fact that the ants when they conduct

1404
01:12:03,328 --> 01:12:05,316
their exploration along let's just think

1405
01:12:05,338 --> 01:12:07,136
about the recurrent networks and they're

1406
01:12:07,168 --> 01:12:09,732
figuring out what nodes and what as

1407
01:12:09,786 --> 01:12:12,084
Abdul Rahman explained the

1408
01:12:12,122 --> 01:12:14,144
superstructure, right as they iterate

1409
01:12:14,192 --> 01:12:16,056
across with their pheromone trails and

1410
01:12:16,078 --> 01:12:18,024
figure out what nodes I want to

1411
01:12:18,062 --> 01:12:20,040
recurrently, connect, feed, forward,

1412
01:12:20,110 --> 01:12:22,136
connect, skip, connect, so on and so

1413
01:12:22,158 --> 01:12:25,288
forth. You. Would say. Well, okay. What

1414
01:12:25,294 --> 01:12:26,984
these ants are doing is they're engaging

1415
01:12:27,032 --> 01:12:30,092
in epistemic foraging, which is a key

1416
01:12:30,146 --> 01:12:32,636
concept in active inference or idid. So

1417
01:12:32,658 --> 01:12:34,604
that way Abdulrahman and Travis are not

1418
01:12:34,642 --> 01:12:37,496
also left behind by a jargon epistemic

1419
01:12:37,528 --> 01:12:40,076
foraging and active inference. It's a

1420
01:12:40,098 --> 01:12:42,210
big general framework. It's like a

1421
01:12:42,980 --> 01:12:45,536
neurobiological process to RL. And

1422
01:12:45,558 --> 01:12:47,376
epistemic just refers to kind of like

1423
01:12:47,398 --> 01:12:49,056
the uncertainty Travis that you and I

1424
01:12:49,078 --> 01:12:51,312
work on. And the idea is it's saying,

1425
01:12:51,366 --> 01:12:54,676
well, OK, I want to understand my world.

1426
01:12:54,858 --> 01:12:57,668
And the more that I explore my world,

1427
01:12:57,754 --> 01:12:59,652
right, there will be things that

1428
01:12:59,706 --> 01:13:02,192
surprise me less. But if I encounter

1429
01:13:02,256 --> 01:13:04,432
some information that is really weird

1430
01:13:04,496 --> 01:13:06,856
when I build a generative world model or

1431
01:13:06,878 --> 01:13:08,744
a predictive world model, that's very

1432
01:13:08,782 --> 01:13:10,564
surprising, I should probably explore

1433
01:13:10,612 --> 01:13:13,476
that. And so, of course, I'm condensing

1434
01:13:13,588 --> 01:13:15,672
the concept down into sort of like the

1435
01:13:15,726 --> 01:13:18,536
exploration part of the explore exploit

1436
01:13:18,568 --> 01:13:21,580
trade off that I know, you know, but

1437
01:13:21,650 --> 01:13:23,224
that characterizes reinforcement

1438
01:13:23,272 --> 01:13:24,524
learning. So that's just what we mean

1439
01:13:24,562 --> 01:13:26,936
when we say epistemic or epistemic

1440
01:13:26,968 --> 01:13:28,904
foraging. And obviously, Abdul Rahman

1441
01:13:28,952 --> 01:13:31,816
foraging can be likened to what the ants

1442
01:13:31,848 --> 01:13:32,988
are doing, right? They're exploring

1443
01:13:33,004 --> 01:13:35,904
their environment. And so I guess with

1444
01:13:35,942 --> 01:13:37,724
that in mind. So that way everyone's

1445
01:13:37,772 --> 01:13:40,096
sort of on the same page here to get to

1446
01:13:40,118 --> 01:13:41,856
your question about the differences,

1447
01:13:41,968 --> 01:13:44,244
about how does this work versus your

1448
01:13:44,282 --> 01:13:46,276
typical neural based approaches to

1449
01:13:46,298 --> 01:13:48,644
active inference. And I would fall into

1450
01:13:48,682 --> 01:13:50,656
that category of, oh, I build neural

1451
01:13:50,688 --> 01:13:53,280
models, biological process models.

1452
01:13:53,440 --> 01:13:55,416
Those are very much focused, you could

1453
01:13:55,438 --> 01:13:57,944
say, at the individual level, at least

1454
01:13:57,982 --> 01:13:59,428
the ones that I am aware of. When you're

1455
01:13:59,444 --> 01:14:00,920
building, for example, even a back

1456
01:14:00,990 --> 01:14:04,404
propagation based, partially observable

1457
01:14:04,452 --> 01:14:07,328
Markov decision process in active

1458
01:14:07,364 --> 01:14:09,372
inference, that's like a single agent,

1459
01:14:09,426 --> 01:14:10,652
right? You're trying to build this

1460
01:14:10,706 --> 01:14:13,916
construct that is trying to balance the

1461
01:14:13,938 --> 01:14:16,776
epistemic quantity with its instrumental

1462
01:14:16,808 --> 01:14:18,856
term, which, by the way, another jargon

1463
01:14:18,888 --> 01:14:20,700
term for you, Abdul Rahman and Travis,

1464
01:14:20,780 --> 01:14:23,424
that's just like your reward signal or

1465
01:14:23,462 --> 01:14:25,404
your prior preference or a prior

1466
01:14:25,452 --> 01:14:28,256
distribution over goal states. And so

1467
01:14:28,438 --> 01:14:30,352
these agents sort of like deal with that

1468
01:14:30,406 --> 01:14:32,640
trade off, but at an individual agent

1469
01:14:32,710 --> 01:14:34,256
level. Now, again, I'm sure that there's

1470
01:14:34,288 --> 01:14:36,004
interpretations of these from other

1471
01:14:36,042 --> 01:14:39,616
perspectives, the ant based approach,

1472
01:14:39,808 --> 01:14:41,812
even though I would not argue per se

1473
01:14:41,866 --> 01:14:44,404
that this has at least an explicit form

1474
01:14:44,442 --> 01:14:46,528
or a connection, at least that Abdul

1475
01:14:46,544 --> 01:14:48,148
Rahman has made clear to active

1476
01:14:48,164 --> 01:14:50,296
inference. But the idea is that this is

1477
01:14:50,318 --> 01:14:52,676
like a multi agent approach to active

1478
01:14:52,708 --> 01:14:54,696
inference. And so the ants, when they

1479
01:14:54,718 --> 01:14:56,924
conduct their epistemic foraging, which

1480
01:14:56,962 --> 01:14:59,950
arguably is a very simple model,

1481
01:15:00,880 --> 01:15:02,972
each ant and of themselves is

1482
01:15:03,106 --> 01:15:06,284
essentially a bunch of coefficients and

1483
01:15:06,322 --> 01:15:10,092
some hard coded rules because their job

1484
01:15:10,146 --> 01:15:11,936
is essentially to work together with

1485
01:15:11,958 --> 01:15:14,352
their pheromone trails to figure out oh,

1486
01:15:14,406 --> 01:15:15,936
what parts of the superstructure are

1487
01:15:15,958 --> 01:15:18,160
useful. So I'd say that that's different

1488
01:15:18,230 --> 01:15:21,248
and it lends itself in some ways. Of

1489
01:15:21,254 --> 01:15:22,416
course, you can make the ants more

1490
01:15:22,438 --> 01:15:24,496
complicated and lose the benefit of I'm

1491
01:15:24,528 --> 01:15:26,704
just about to say you could massively

1492
01:15:26,752 --> 01:15:28,532
paralyze this. And this is one of the

1493
01:15:28,586 --> 01:15:31,408
key strengths that I think is natural.

1494
01:15:31,504 --> 01:15:34,020
In, for example, a lot of actually

1495
01:15:34,090 --> 01:15:36,836
nature inspired optimization algorithms

1496
01:15:36,948 --> 01:15:39,288
and ant colony optimization is one of

1497
01:15:39,294 --> 01:15:41,208
them. Is Abdul Rahman has been using

1498
01:15:41,294 --> 01:15:44,056
hundreds of CPUs, and you can put these

1499
01:15:44,078 --> 01:15:46,100
ants on their own individual processor,

1500
01:15:46,180 --> 01:15:48,744
and the communication that's occurring

1501
01:15:48,792 --> 01:15:50,830
across them as they exchange information

1502
01:15:51,600 --> 01:15:53,528
is through the pheromone trails.

1503
01:15:53,544 --> 01:15:55,116
There's an indirect mechanism, it's not

1504
01:15:55,138 --> 01:15:57,688
terribly complex to facilitate. And I'm

1505
01:15:57,704 --> 01:15:59,696
sure that there's even better ways to go

1506
01:15:59,718 --> 01:16:01,616
about doing asynchronous forms of

1507
01:16:01,638 --> 01:16:04,172
communication and further, further

1508
01:16:04,236 --> 01:16:06,476
optimize this. I know Travis and Travis

1509
01:16:06,508 --> 01:16:08,544
can add to this has done things on like,

1510
01:16:08,582 --> 01:16:10,844
citizen science and distributed

1511
01:16:10,892 --> 01:16:14,224
computing through volunteer computing

1512
01:16:14,272 --> 01:16:16,276
and how you can distribute this through

1513
01:16:16,298 --> 01:16:19,008
a massive global Asynchronous network.

1514
01:16:19,024 --> 01:16:22,724
So you can imagine adapting the ant form

1515
01:16:22,842 --> 01:16:24,852
sorry, the ant colony optimization

1516
01:16:24,996 --> 01:16:27,876
approach to some distributed, massively

1517
01:16:27,908 --> 01:16:30,068
distributed version of active inference,

1518
01:16:30,164 --> 01:16:32,312
where you essentially have to write down

1519
01:16:32,446 --> 01:16:35,096
that the variational free energy. And

1520
01:16:35,198 --> 01:16:37,112
I'm putting quotes around this because,

1521
01:16:37,166 --> 01:16:39,444
again, there is no concrete term written

1522
01:16:39,492 --> 01:16:42,796
in Raman's work because at least

1523
01:16:42,818 --> 01:16:44,776
we haven't viewed this from the active

1524
01:16:44,808 --> 01:16:46,764
inference perspective directly. Each ant

1525
01:16:46,802 --> 01:16:48,636
is optimizing its own variational free

1526
01:16:48,658 --> 01:16:50,176
energy, but then there's probably a

1527
01:16:50,198 --> 01:16:52,956
global quantity that sort of is related

1528
01:16:52,988 --> 01:16:55,644
as a function of those pheromone trails

1529
01:16:55,692 --> 01:16:58,496
in the individual ant agents. And then

1530
01:16:58,518 --> 01:17:01,424
of course with the exploitation term or

1531
01:17:01,462 --> 01:17:04,592
the instrumental or what is the reward

1532
01:17:04,656 --> 01:17:07,316
signal to give an RL term that's sort of

1533
01:17:07,338 --> 01:17:09,780
driven by the performance of the actual

1534
01:17:09,850 --> 01:17:13,156
agent on each candidate agent on the

1535
01:17:13,178 --> 01:17:15,152
task right of the ram. And you compute

1536
01:17:15,216 --> 01:17:16,696
like mean squared error when you're

1537
01:17:16,718 --> 01:17:19,064
doing time series prediction. So in some

1538
01:17:19,102 --> 01:17:22,584
sense we have built in a reward function

1539
01:17:22,702 --> 01:17:25,368
that we use and again, for those in the

1540
01:17:25,454 --> 01:17:27,836
active inference group here, you can use

1541
01:17:27,858 --> 01:17:30,588
the complete class theorem and look at

1542
01:17:30,674 --> 01:17:31,996
the prior preference of saying, oh,

1543
01:17:32,018 --> 01:17:33,912
well, the reward is actually technically

1544
01:17:33,976 --> 01:17:35,676
a prior preference, right? It's like a

1545
01:17:35,698 --> 01:17:39,148
log probability. So with that in mind,

1546
01:17:39,234 --> 01:17:41,356
you could squint at ant colony

1547
01:17:41,388 --> 01:17:43,232
optimization. And I guess the big

1548
01:17:43,286 --> 01:17:45,164
benefit comes from that massive

1549
01:17:45,212 --> 01:17:47,228
parallelization that you wouldn't

1550
01:17:47,324 --> 01:17:49,632
actually very easily, if at all, get

1551
01:17:49,686 --> 01:17:52,372
with our single agent neural based

1552
01:17:52,426 --> 01:17:54,724
approaches. And that might be an

1553
01:17:54,762 --> 01:17:57,136
interesting place to build on and I'll

1554
01:17:57,168 --> 01:17:58,756
stop rambling at this point. I'm not

1555
01:17:58,778 --> 01:18:00,020
sure if that was helpful.

1556
01:18:03,000 --> 01:18:05,688
That was awesome. Even earlier today,

1557
01:18:05,774 --> 01:18:08,216
Chris Fields in the Physics as

1558
01:18:08,238 --> 01:18:10,824
information processing course was

1559
01:18:10,862 --> 01:18:13,000
talking about the classical information

1560
01:18:13,150 --> 01:18:15,096
inscribed on the blanket, which is like

1561
01:18:15,118 --> 01:18:17,716
the pheromone perception and deposition,

1562
01:18:17,828 --> 01:18:20,164
pheromone modification and perception

1563
01:18:20,212 --> 01:18:22,444
sense making an action which we can

1564
01:18:22,482 --> 01:18:25,192
associate with the nest mate cognitive

1565
01:18:25,256 --> 01:18:27,964
system. So then there could be as simple

1566
01:18:28,002 --> 01:18:29,916
as a pass through for the nest mate,

1567
01:18:30,028 --> 01:18:32,400
could be any arbitrary relationship

1568
01:18:32,550 --> 01:18:34,956
described with a blanket, simple nest

1569
01:18:34,988 --> 01:18:38,048
mate, sophisticated nest mate, like

1570
01:18:38,134 --> 01:18:40,888
another level of time series modeling,

1571
01:18:41,084 --> 01:18:43,444
whereas there's the environmental time

1572
01:18:43,482 --> 01:18:46,516
series modeling and that's just in the

1573
01:18:46,538 --> 01:18:50,592
ants. And then the fourth

1574
01:18:50,656 --> 01:18:53,564
dimension is like that quantum rotation

1575
01:18:53,712 --> 01:18:56,228
which goes from the lower dimensional

1576
01:18:56,324 --> 01:18:58,936
classical stigmagic screen into the

1577
01:18:58,958 --> 01:19:01,624
quantum informational space. And so

1578
01:19:01,662 --> 01:19:03,896
that's one of the discussions ongoing in

1579
01:19:03,918 --> 01:19:06,180
Actimf right now is about well,

1580
01:19:06,270 --> 01:19:08,664
previous approaches to connect quantum

1581
01:19:08,792 --> 01:19:12,872
formalisms to macro,

1582
01:19:13,016 --> 01:19:15,756
let's just say neural phenomena based it

1583
01:19:15,778 --> 01:19:17,628
upon the plausibility of like a

1584
01:19:17,634 --> 01:19:21,200
molecular electronic bubbling up.

1585
01:19:21,350 --> 01:19:24,684
Whereas just with research from decision

1586
01:19:24,732 --> 01:19:27,072
making and statistics and just

1587
01:19:27,126 --> 01:19:29,808
multiperspectival modeling and all the

1588
01:19:29,814 --> 01:19:31,824
issues associated with the physicality

1589
01:19:31,872 --> 01:19:33,744
of information transfer, the finiteness

1590
01:19:33,792 --> 01:19:37,104
of it, the quantum formalism becomes

1591
01:19:37,152 --> 01:19:39,380
useful just by itself with or without

1592
01:19:39,530 --> 01:19:42,336
reliance on some other electronic

1593
01:19:42,368 --> 01:19:45,108
phenomena. So it's just a lot of very

1594
01:19:45,194 --> 01:19:46,968
interesting connections like having the

1595
01:19:46,974 --> 01:19:49,544
degrees of freedom on the blanket which

1596
01:19:49,582 --> 01:19:51,816
could be noiseless and four bits or it

1597
01:19:51,838 --> 01:19:53,896
could be noisy with this really specific

1598
01:19:53,998 --> 01:19:57,704
thing. But in Silico you get to play

1599
01:19:57,742 --> 01:20:00,396
it from both sides and scale things up

1600
01:20:00,418 --> 01:20:02,476
and down and do these meta heuristics on

1601
01:20:02,498 --> 01:20:04,668
top of that arbitrary space could be

1602
01:20:04,674 --> 01:20:07,084
really simple for learning, or it could

1603
01:20:07,122 --> 01:20:10,096
be however much. And then just like the

1604
01:20:10,118 --> 01:20:12,172
ant colony algorithm is ultimately

1605
01:20:12,236 --> 01:20:15,248
federated through embodiment, that

1606
01:20:15,334 --> 01:20:17,724
property makes it a really useful

1607
01:20:17,772 --> 01:20:19,968
candidate for biomimicry. So a lot of

1608
01:20:19,974 --> 01:20:21,276
times when people think about collective

1609
01:20:21,308 --> 01:20:22,736
behavior, they're thinking about like

1610
01:20:22,758 --> 01:20:24,148
the flock of birds and the school of

1611
01:20:24,154 --> 01:20:27,088
fish and those are of course collective

1612
01:20:27,264 --> 01:20:29,348
systems and collective behavioral and

1613
01:20:29,434 --> 01:20:31,056
all these kinds of complex systems

1614
01:20:31,088 --> 01:20:33,236
properties can be studied in that type

1615
01:20:33,258 --> 01:20:36,452
of system. But it is also neglecting

1616
01:20:36,596 --> 01:20:38,676
at least an analytical degree of freedom

1617
01:20:38,708 --> 01:20:41,752
with the stigma g. So that really

1618
01:20:41,806 --> 01:20:45,096
opens up both the quantum and the

1619
01:20:45,118 --> 01:20:47,848
classical information or both niche

1620
01:20:47,864 --> 01:20:51,852
modification and behavioral and

1621
01:20:51,906 --> 01:20:55,116
cognitive modeling. So just to add on

1622
01:20:55,138 --> 01:20:57,996
because I think and then also what the

1623
01:20:58,018 --> 01:21:00,044
node is could even be heterogeneous or

1624
01:21:00,082 --> 01:21:03,224
unknown or fit in different ways or fix

1625
01:21:03,282 --> 01:21:05,296
through design processes. So just like

1626
01:21:05,318 --> 01:21:07,404
the ant colonies are flexible, enabling

1627
01:21:07,452 --> 01:21:10,252
them to live in all kinds of places,

1628
01:21:10,396 --> 01:21:12,716
make all these kinds of nested decisions

1629
01:21:12,828 --> 01:21:16,084
that interact with each other, that

1630
01:21:16,122 --> 01:21:17,908
flexibility is just like the tip of the

1631
01:21:17,914 --> 01:21:19,972
iceberg, of what we could even just

1632
01:21:20,026 --> 01:21:22,052
describe, because there would always be

1633
01:21:22,106 --> 01:21:23,844
real environments we hadn't yet tried

1634
01:21:23,882 --> 01:21:25,144
with ant colonies. So we really would

1635
01:21:25,182 --> 01:21:27,816
never know the full extent of all the

1636
01:21:27,838 --> 01:21:30,552
repertoire and the dynamics of the ant

1637
01:21:30,606 --> 01:21:34,468
system. But then we can just abduct

1638
01:21:34,564 --> 01:21:37,900
into new mathematical statistical

1639
01:21:38,400 --> 01:21:40,476
distributional frameworks, pull back to

1640
01:21:40,498 --> 01:21:41,788
different levels of the learning and the

1641
01:21:41,794 --> 01:21:43,452
meta learning process and just start

1642
01:21:43,506 --> 01:21:47,032
there. And then almost ironically,

1643
01:21:47,176 --> 01:21:49,376
or maybe the opposite of that, it.

1644
01:21:49,398 --> 01:21:51,888
Could be applied to ant colony video

1645
01:21:51,974 --> 01:21:55,244
data or movement data or foraging

1646
01:21:55,292 --> 01:21:58,850
activity itself. But it kind of

1647
01:21:59,700 --> 01:22:01,776
takes inspiration and develops in

1648
01:22:01,798 --> 01:22:03,876
parallel or in conversation. So it's not

1649
01:22:03,898 --> 01:22:06,564
like bound to what real ants can do. Or

1650
01:22:06,602 --> 01:22:08,772
you could constrain it so that there are

1651
01:22:08,826 --> 01:22:11,812
properties that real ants have, like

1652
01:22:11,866 --> 01:22:13,316
they can only interact within this

1653
01:22:13,338 --> 01:22:15,384
certain way or there really are only

1654
01:22:15,422 --> 01:22:17,992
this many pheromones do model

1655
01:22:18,046 --> 01:22:20,568
comparison. So it's a lot of degrees of

1656
01:22:20,574 --> 01:22:22,884
freedom. I feel like you all are opening

1657
01:22:22,932 --> 01:22:25,544
up with the ant collie modeling. And

1658
01:22:25,582 --> 01:22:27,496
also one of the challenging pieces of

1659
01:22:27,518 --> 01:22:29,736
multi agent simulation is kind of like

1660
01:22:29,758 --> 01:22:31,100
the open endedness with the design

1661
01:22:31,170 --> 01:22:34,892
space. So then it's very hard for

1662
01:22:34,946 --> 01:22:36,636
even creative ideas like sometimes to

1663
01:22:36,658 --> 01:22:39,344
find the right compute resources that

1664
01:22:39,382 --> 01:22:41,696
obviously are still even needed for what

1665
01:22:41,718 --> 01:22:44,716
he discovered with the analysis.

1666
01:22:44,828 --> 01:22:47,808
Here I'll ask a question from Bert. In

1667
01:22:47,814 --> 01:22:50,688
the to before we move on to that, I just

1668
01:22:50,694 --> 01:22:53,392
want to clarify just so make sure that I

1669
01:22:53,446 --> 01:22:56,048
got the right term and certainly Abdul

1670
01:22:56,064 --> 01:22:57,636
Rahman and Travis might want to look it

1671
01:22:57,658 --> 01:22:59,268
up is when you were saying blanket, you

1672
01:22:59,274 --> 01:23:00,816
were referring to a Markov blanket,

1673
01:23:00,848 --> 01:23:03,664
correct? Yes. So the technical

1674
01:23:03,792 --> 01:23:06,868
definition of Markov blanket is when you

1675
01:23:06,874 --> 01:23:08,888
have a Bayesian graph where nodes are

1676
01:23:08,894 --> 01:23:10,024
the variables and edges are

1677
01:23:10,062 --> 01:23:11,812
relationships amongst these variables.

1678
01:23:11,956 --> 01:23:14,068
For any given node of interest, we'll

1679
01:23:14,084 --> 01:23:15,608
just call it internal states. So these

1680
01:23:15,614 --> 01:23:16,936
are not features of the world. It's not

1681
01:23:16,958 --> 01:23:19,156
tagged onto some tissue of a real nest

1682
01:23:19,188 --> 01:23:20,684
mate in the world. This is something

1683
01:23:20,722 --> 01:23:22,456
that's tagged onto or like a perspective

1684
01:23:22,488 --> 01:23:24,376
we could take on any node in a Bayes

1685
01:23:24,408 --> 01:23:26,396
graph and then all the nodes that

1686
01:23:26,418 --> 01:23:29,736
insulate it and the co parents are known

1687
01:23:29,768 --> 01:23:32,076
as the blanket in the sense that it's

1688
01:23:32,108 --> 01:23:35,008
like one layer insulator. And there's a

1689
01:23:35,014 --> 01:23:36,528
lot of more discussion on it and the

1690
01:23:36,534 --> 01:23:38,608
philosophical implications and all these

1691
01:23:38,694 --> 01:23:42,096
generalizations of that. But broadly,

1692
01:23:42,128 --> 01:23:44,224
the Markov blanket is just the inbound

1693
01:23:44,272 --> 01:23:46,484
dependencies which we associate with

1694
01:23:46,522 --> 01:23:48,676
sense making and perception learning

1695
01:23:48,778 --> 01:23:51,024
attention and then the outgoing

1696
01:23:51,072 --> 01:23:53,896
dependencies for the agent which we

1697
01:23:53,918 --> 01:23:57,210
associate with action influence in

1698
01:23:57,740 --> 01:24:00,856
some downstream pointing way. Thank you.

1699
01:24:00,878 --> 01:24:03,448
I just wanted to clarify that because I

1700
01:24:03,454 --> 01:24:05,044
don't think Abdul Rahman and Travis

1701
01:24:05,092 --> 01:24:06,468
might be familiar just with that

1702
01:24:06,494 --> 01:24:10,184
terminology. It's very know, very active

1703
01:24:10,232 --> 01:24:12,588
inferency kind of jargon. So I wanted to

1704
01:24:12,594 --> 01:24:14,396
make sure that they got that from the

1705
01:24:14,418 --> 01:24:17,832
physics point of thanks. Yeah, totally

1706
01:24:17,896 --> 01:24:21,584
great point. And ask a question now

1707
01:24:21,622 --> 01:24:24,210
from Bert. So Bert says.

1708
01:24:25,060 --> 01:24:27,292
Very impressive. Solving generative

1709
01:24:27,356 --> 01:24:29,244
models with more generative models

1710
01:24:29,292 --> 01:24:31,232
sounds very promising. What about

1711
01:24:31,286 --> 01:24:33,760
replacing ants with convolutions?

1712
01:24:37,610 --> 01:24:40,426
Talking about like a meta learning

1713
01:24:40,528 --> 01:24:42,902
algorithm, like having a neural network

1714
01:24:42,966 --> 01:24:44,966
that learns how to optimize other neural

1715
01:24:44,998 --> 01:24:45,770
networks?

1716
01:24:48,290 --> 01:24:51,374
Yeah, this concept, I think is

1717
01:24:51,412 --> 01:24:53,546
introduced at some point in machine

1718
01:24:53,578 --> 01:24:56,910
learning learn. But we wanted to apply

1719
01:24:56,980 --> 01:25:00,410
for nature based method

1720
01:25:00,490 --> 01:25:03,934
that mimics like the Mother Nature,

1721
01:25:03,982 --> 01:25:07,506
which is nature is the

1722
01:25:07,608 --> 01:25:09,902
most efficient optimization evolution

1723
01:25:09,966 --> 01:25:13,906
system. So looking at the results that

1724
01:25:14,088 --> 01:25:16,546
are there in nature applying nature

1725
01:25:16,658 --> 01:25:19,446
based methods they were superior to any

1726
01:25:19,468 --> 01:25:22,374
other method. And the results we got

1727
01:25:22,412 --> 01:25:25,610
also which just pointed that out that we

1728
01:25:25,680 --> 01:25:27,994
saw good performance coming out from

1729
01:25:28,032 --> 01:25:29,866
these results from our results and the

1730
01:25:29,888 --> 01:25:31,834
previous results from other methods as

1731
01:25:31,872 --> 01:25:35,514
well. Hop in here a little bit

1732
01:25:35,552 --> 01:25:39,274
too. In the case of neural architecture

1733
01:25:39,322 --> 01:25:40,686
search there's kind of a couple of

1734
01:25:40,708 --> 01:25:42,414
classes of approaches. One is

1735
01:25:42,452 --> 01:25:44,446
constructive, so kind of like where you

1736
01:25:44,468 --> 01:25:47,534
build larger and larger networks and try

1737
01:25:47,572 --> 01:25:49,658
and keep your network size minimal to

1738
01:25:49,684 --> 01:25:51,822
try and find your optimal solution.

1739
01:25:51,966 --> 01:25:53,854
Other types of neural architecture

1740
01:25:53,902 --> 01:25:55,426
search approaches use like a

1741
01:25:55,448 --> 01:25:57,266
superstructure and this is kind of how

1742
01:25:57,368 --> 01:25:59,442
the earlier iterations of this were

1743
01:25:59,496 --> 01:26:01,714
where you have a bound of your search

1744
01:26:01,752 --> 01:26:03,326
space and you try and find the optimal

1745
01:26:03,358 --> 01:26:04,806
network within that bound. So one is

1746
01:26:04,828 --> 01:26:06,486
like trying to build things from the

1747
01:26:06,508 --> 01:26:08,280
ground up and the other one is trying to

1748
01:26:08,650 --> 01:26:11,014
trim down a big network to a small

1749
01:26:11,052 --> 01:26:13,994
network. As to your question about

1750
01:26:14,032 --> 01:26:17,226
convolutions, there's been

1751
01:26:17,248 --> 01:26:19,318
a fair bit of research lately in what's

1752
01:26:19,334 --> 01:26:21,994
called graph based neural networks which

1753
01:26:22,032 --> 01:26:24,826
can use convolutions over like a

1754
01:26:24,848 --> 01:26:27,134
discrete graph search space and can

1755
01:26:27,172 --> 01:26:28,798
potentially produce other graphs. And I

1756
01:26:28,804 --> 01:26:30,490
believe there's been some neural

1757
01:26:30,650 --> 01:26:33,070
architecture search work using this.

1758
01:26:33,220 --> 01:26:36,414
But one of the main and I think cool

1759
01:26:36,452 --> 01:26:38,814
things about the approach here, which is

1760
01:26:38,852 --> 01:26:40,238
different from those is if even if you

1761
01:26:40,244 --> 01:26:41,950
have a graph based neural network and

1762
01:26:42,020 --> 01:26:44,626
you have your search base defined as

1763
01:26:44,648 --> 01:26:46,546
some kind of matrix where things are off

1764
01:26:46,568 --> 01:26:47,858
and on depending on which nodes are

1765
01:26:47,864 --> 01:26:49,298
connected to each other. You have a

1766
01:26:49,304 --> 01:26:51,906
fixed search space which may not be big

1767
01:26:51,928 --> 01:26:53,862
enough or it might not be the correct

1768
01:26:53,916 --> 01:26:55,154
search space for this neural

1769
01:26:55,202 --> 01:26:57,014
architecture search problem where this

1770
01:26:57,052 --> 01:26:58,854
method here is all continuous. Right?

1771
01:26:58,892 --> 01:27:01,798
So within this continuous search space

1772
01:27:01,964 --> 01:27:03,782
it gives the algorithm a lot of freedom,

1773
01:27:03,846 --> 01:27:06,714
maybe too much freedom but a really open

1774
01:27:06,752 --> 01:27:09,494
ended way of generating a wide variety

1775
01:27:09,542 --> 01:27:11,994
of neural architectures which if you

1776
01:27:12,032 --> 01:27:15,274
preconstrain your algorithm to work

1777
01:27:15,312 --> 01:27:17,322
within a fixed discrete superstructure

1778
01:27:17,386 --> 01:27:19,200
you may not even find them because

1779
01:27:19,650 --> 01:27:21,230
they're not even a possibility.

1780
01:27:22,130 --> 01:27:25,274
So that's one of the reasons we didn't

1781
01:27:25,322 --> 01:27:28,142
go that route. But there are graph based

1782
01:27:28,196 --> 01:27:30,286
neural architecture search algorithms

1783
01:27:30,318 --> 01:27:31,746
out there where basically you take the

1784
01:27:31,768 --> 01:27:34,818
architecture as a graph, you train a

1785
01:27:34,824 --> 01:27:36,494
neural network to spit out another graph

1786
01:27:36,542 --> 01:27:40,694
that it might think is better and those

1787
01:27:40,732 --> 01:27:43,990
use convolutions. Sometimes that helps.

1788
01:27:44,330 --> 01:27:46,134
That's awesome. So convolution sounds

1789
01:27:46,172 --> 01:27:49,014
like yes. And one thought on that is

1790
01:27:49,212 --> 01:27:52,178
yes, the ants solve all these incredible

1791
01:27:52,274 --> 01:27:55,882
patterns and kind of do amazing

1792
01:27:55,936 --> 01:27:58,534
things amidst informational and physical

1793
01:27:58,662 --> 01:28:06,846
limitations like we all do having

1794
01:28:06,948 --> 01:28:12,080
the ants be able to just

1795
01:28:13,570 --> 01:28:17,600
make trade offs within a task space

1796
01:28:18,310 --> 01:28:21,906
and then have a dial as modelers to

1797
01:28:21,928 --> 01:28:24,162
make that task space. Kind of like

1798
01:28:24,296 --> 01:28:27,860
touching the pheromone distribution or

1799
01:28:28,310 --> 01:28:32,022
metacognitive ants or something

1800
01:28:32,076 --> 01:28:35,494
emulating essentially that. And for

1801
01:28:35,532 --> 01:28:37,350
example, the active inference forward

1802
01:28:37,420 --> 01:28:39,590
looking and thinking through other minds

1803
01:28:40,250 --> 01:28:42,322
that there could be a kind of cognitive

1804
01:28:42,386 --> 01:28:46,090
colony. So then that enables in silico

1805
01:28:47,150 --> 01:28:50,380
total thought, experiment colonies and

1806
01:28:50,750 --> 01:28:54,330
through data driven processes also

1807
01:28:54,400 --> 01:28:56,910
kind of keep continuity with that model,

1808
01:28:56,980 --> 01:28:58,686
perhaps literally continuity with the

1809
01:28:58,708 --> 01:29:01,630
model and then connect it to empirical,

1810
01:29:02,130 --> 01:29:05,662
which is something that is

1811
01:29:05,716 --> 01:29:08,270
very hard for agent based modeling

1812
01:29:08,630 --> 01:29:11,490
which, as you kind of pointed to, often

1813
01:29:11,640 --> 01:29:14,354
sets certain fixed axes, performs like

1814
01:29:14,392 --> 01:29:17,438
asweep looks at one mechanism,

1815
01:29:17,534 --> 01:29:19,074
doesn't look at all these possible

1816
01:29:19,192 --> 01:29:23,046
mechanisms of learning and intra and

1817
01:29:23,148 --> 01:29:25,494
intergenerational and all these time

1818
01:29:25,532 --> 01:29:28,998
effects. So how do you see this

1819
01:29:29,084 --> 01:29:31,894
being used in different research or

1820
01:29:31,932 --> 01:29:33,830
application domains?

1821
01:29:38,400 --> 01:29:39,150
Well,

1822
01:29:42,580 --> 01:29:45,984
the main use case that we're thinking of

1823
01:29:46,022 --> 01:29:50,516
was neural architecture search to

1824
01:29:50,538 --> 01:29:53,380
apply it for other domains other than

1825
01:29:53,450 --> 01:29:55,604
neural architecture. We didn't figure

1826
01:29:55,642 --> 01:29:59,172
out that yet. I think Alex can explain

1827
01:29:59,226 --> 01:30:02,816
on that. I can hop in a little bit mean.

1828
01:30:02,858 --> 01:30:05,896
So basically this type of algorithm, if

1829
01:30:05,918 --> 01:30:08,696
you need to generate graphs and you

1830
01:30:08,718 --> 01:30:11,044
don't necessarily have a fixed structure

1831
01:30:11,092 --> 01:30:13,176
for that graph and when I say graph, I

1832
01:30:13,198 --> 01:30:14,588
mean like a computer science graph where

1833
01:30:14,594 --> 01:30:17,916
you have nodes in a different so pretty

1834
01:30:17,938 --> 01:30:19,672
much anything involving graph

1835
01:30:19,736 --> 01:30:21,484
construction. I think these types of

1836
01:30:21,522 --> 01:30:23,628
methods work. Neural networks kind of

1837
01:30:23,634 --> 01:30:26,096
under the hood can be represented as

1838
01:30:26,118 --> 01:30:28,988
graphs and usually are. So I think we're

1839
01:30:29,004 --> 01:30:30,496
using it for neural networks because

1840
01:30:30,518 --> 01:30:34,624
it's really popular but there's other

1841
01:30:34,662 --> 01:30:36,304
algorithms out there like the

1842
01:30:36,342 --> 01:30:39,148
traditional traveling salesman, there's

1843
01:30:39,164 --> 01:30:40,996
like routing problems, all that, any

1844
01:30:41,018 --> 01:30:42,068
type of stuff where you might need to

1845
01:30:42,074 --> 01:30:45,476
generate a graph in a smart way to

1846
01:30:45,498 --> 01:30:48,148
do that. To your other point though, I

1847
01:30:48,154 --> 01:30:49,268
think what's really cool here and I

1848
01:30:49,274 --> 01:30:50,468
don't want to steal Alto Robin's

1849
01:30:50,484 --> 01:30:52,472
thunder, but his last point on future

1850
01:30:52,526 --> 01:30:56,468
direction is while a particular version

1851
01:30:56,484 --> 01:30:58,264
of this ant colony optimization search

1852
01:30:58,302 --> 01:30:59,796
is running to find an optimal neural

1853
01:30:59,828 --> 01:31:03,464
network, a colony has fixed parameters

1854
01:31:03,512 --> 01:31:06,476
that it operates within. But if you

1855
01:31:06,498 --> 01:31:08,396
think of the colony as an organism as

1856
01:31:08,418 --> 01:31:10,184
opposed to the ants being an organism,

1857
01:31:10,312 --> 01:31:13,604
you can evolve colonies that optimize

1858
01:31:13,752 --> 01:31:17,168
how the ants themselves act.

1859
01:31:17,254 --> 01:31:19,810
So you can have evolving colonies that

1860
01:31:20,180 --> 01:31:23,264
in a smaller sense also evolve or

1861
01:31:23,302 --> 01:31:25,984
optimize what they're doing within their

1862
01:31:26,022 --> 01:31:29,300
prescripted parameters for the agents

1863
01:31:29,370 --> 01:31:30,628
they're generating. So you can have like

1864
01:31:30,634 --> 01:31:31,780
a meta meta.

1865
01:31:34,280 --> 01:31:36,864
It's awesome. I mean the evolutionary

1866
01:31:36,912 --> 01:31:39,864
account of a why question for ant

1867
01:31:39,902 --> 01:31:42,472
behavior today, one part of that answer

1868
01:31:42,526 --> 01:31:45,140
is like because colonies that couldn't

1869
01:31:45,300 --> 01:31:47,748
under that regularity or constraint

1870
01:31:47,924 --> 01:31:50,648
survive. We've had a long, long time to

1871
01:31:50,654 --> 01:31:52,636
wipe those off the table. And so every

1872
01:31:52,658 --> 01:31:54,348
biological system has to have that kind

1873
01:31:54,354 --> 01:31:56,140
of multiscale ordering.

1874
01:31:56,960 --> 01:32:01,032
In 2021 we made the active imper ants

1875
01:32:01,176 --> 01:32:03,036
paper which was modified from an

1876
01:32:03,058 --> 01:32:06,560
epistemic foraging visual attention task

1877
01:32:08,260 --> 01:32:11,392
about scanning around and then learning

1878
01:32:11,446 --> 01:32:13,984
a cicade policy that had to do with

1879
01:32:14,022 --> 01:32:16,016
epistemic foraging but not leaving a

1880
01:32:16,038 --> 01:32:17,936
trace. And then the main modification to

1881
01:32:17,958 --> 01:32:19,964
bring that active inference epistemic

1882
01:32:20,012 --> 01:32:22,684
visual foraging model into the active

1883
01:32:22,812 --> 01:32:25,876
inference ant setting was to add a

1884
01:32:25,898 --> 01:32:27,536
pheromone rule, just like you described,

1885
01:32:27,568 --> 01:32:28,576
even though, of course, again, that's

1886
01:32:28,608 --> 01:32:30,300
not the only pheromone rule, but that's

1887
01:32:30,320 --> 01:32:31,928
just the most simple pheromone rule that

1888
01:32:31,934 --> 01:32:33,736
we can generalize from, as you

1889
01:32:33,758 --> 01:32:37,048
definitely have. And there are just many

1890
01:32:37,214 --> 01:32:39,016
emerging ways of modeling those

1891
01:32:39,038 --> 01:32:41,352
multiscale active inference models. So

1892
01:32:41,406 --> 01:32:43,804
composing across layers, which we might

1893
01:32:43,842 --> 01:32:45,196
associate more with the kind of

1894
01:32:45,218 --> 01:32:48,844
laterality of things that happen through

1895
01:32:48,882 --> 01:32:51,656
interactions. And then also, as Mike

1896
01:32:51,688 --> 01:32:53,500
Levin shows, with kind of the time

1897
01:32:53,570 --> 01:32:57,740
diamond systems that have a memory

1898
01:32:57,900 --> 01:33:02,300
retention component of some shape,

1899
01:33:02,460 --> 01:33:05,200
cognitive shape, and then a protention

1900
01:33:06,760 --> 01:33:10,544
awareness or agency or other attributes

1901
01:33:10,592 --> 01:33:13,456
you can use to describe that. And that's

1902
01:33:13,488 --> 01:33:16,592
a statistically amenable way to describe

1903
01:33:16,736 --> 01:33:20,056
things. And then there's a variety of

1904
01:33:20,078 --> 01:33:22,852
implementations on a given statistical

1905
01:33:22,916 --> 01:33:24,548
problem, or like Federated Compute

1906
01:33:24,564 --> 01:33:26,744
architecture, it might be the case that

1907
01:33:26,782 --> 01:33:29,428
you're not running the pure matrix

1908
01:33:29,524 --> 01:33:32,636
multiplications that are shown in the

1909
01:33:32,658 --> 01:33:35,176
early MATLAB code of active inference.

1910
01:33:35,288 --> 01:33:36,972
Different components of machine learning

1911
01:33:37,026 --> 01:33:38,344
systems might be kind of composed

1912
01:33:38,392 --> 01:33:42,364
together, also ways kind of abiding by

1913
01:33:42,402 --> 01:33:44,796
those patterns of communication. But

1914
01:33:44,818 --> 01:33:47,776
then there's a level of abstraction that

1915
01:33:47,878 --> 01:33:49,308
we can still describe, but it doesn't

1916
01:33:49,324 --> 01:33:51,488
mean active inference is going to be

1917
01:33:51,574 --> 01:33:54,336
kind of causing it. So that's what gives

1918
01:33:54,358 --> 01:33:56,724
a lot of flexibility. And it's really

1919
01:33:56,762 --> 01:33:59,408
cool that through your background,

1920
01:33:59,504 --> 01:34:03,028
Alexander and work and these kinds of

1921
01:34:03,194 --> 01:34:05,536
collaborations that like the active

1922
01:34:05,568 --> 01:34:07,220
inference perspective on multi agent

1923
01:34:07,290 --> 01:34:10,216
modeling with all these other views can

1924
01:34:10,238 --> 01:34:14,040
at least come together comparatively.

1925
01:34:14,540 --> 01:34:17,144
And then that is going to, I think, be

1926
01:34:17,182 --> 01:34:19,976
quite an interesting interchange to

1927
01:34:19,998 --> 01:34:23,576
apply this entire tissue

1928
01:34:23,688 --> 01:34:27,452
type or colony type thinking

1929
01:34:27,586 --> 01:34:29,676
above and within the models. Just a lot

1930
01:34:29,698 --> 01:34:31,564
of degrees of freedom, like you said,

1931
01:34:31,602 --> 01:34:34,664
could be too. And I think there's

1932
01:34:34,712 --> 01:34:37,184
different ways too, I think depends on

1933
01:34:37,222 --> 01:34:40,108
how you want to take the ant metaphor.

1934
01:34:40,204 --> 01:34:42,784
And again, it's kind of interesting,

1935
01:34:42,982 --> 01:34:44,736
some of the questions or comments that

1936
01:34:44,758 --> 01:34:46,656
you're making, Daniel, and some from the

1937
01:34:46,678 --> 01:34:50,036
audience about how does this think

1938
01:34:50,058 --> 01:34:51,684
about it from a cognitive point of view.

1939
01:34:51,722 --> 01:34:53,312
I mean, I do work in cognitive

1940
01:34:53,376 --> 01:34:55,716
architectures, of course, again, kind of

1941
01:34:55,738 --> 01:34:58,960
from the whole single agent or single

1942
01:34:59,050 --> 01:35:02,856
entity and modeling a single brain in

1943
01:35:02,878 --> 01:35:05,384
its different regions. But I think if

1944
01:35:05,422 --> 01:35:08,680
you take a nature inspired

1945
01:35:09,340 --> 01:35:11,820
optimization approach like the ant

1946
01:35:11,890 --> 01:35:15,496
metaphor that Abdel Rahman latched

1947
01:35:15,528 --> 01:35:17,308
onto, and that's sort of like the way in

1948
01:35:17,314 --> 01:35:20,556
which he formalizes how takes a

1949
01:35:20,578 --> 01:35:22,956
principle of how ants interact with

1950
01:35:22,978 --> 01:35:25,296
their world, interact with each other,

1951
01:35:25,478 --> 01:35:28,672
and then mathematically model those

1952
01:35:28,726 --> 01:35:32,256
particular concepts step by step. And I

1953
01:35:32,278 --> 01:35:34,416
think if you bend the metaphor and say,

1954
01:35:34,438 --> 01:35:35,916
well, okay, could the ant colony

1955
01:35:35,948 --> 01:35:39,060
metaphor apply to multi human agent

1956
01:35:39,130 --> 01:35:41,140
systems, right, or other entities?

1957
01:35:41,960 --> 01:35:44,404
Does the ant necessarily can it be

1958
01:35:44,442 --> 01:35:47,904
generalized beyond the physical creature

1959
01:35:48,032 --> 01:35:51,076
upon which Abdul Rahman based his

1960
01:35:51,098 --> 01:35:52,368
initial metaphor? And that's an

1961
01:35:52,394 --> 01:35:54,312
interesting philosophical kind of take

1962
01:35:54,366 --> 01:35:56,872
to it. And then how do you apply that

1963
01:35:56,926 --> 01:35:58,968
to, let's say, building a multi agent

1964
01:35:59,054 --> 01:36:00,664
cognitive system. And then, of course,

1965
01:36:00,702 --> 01:36:03,176
as Travis was discussing with you and

1966
01:36:03,198 --> 01:36:05,828
you were mentioning metacognition know,

1967
01:36:05,854 --> 01:36:07,484
you could think of ant colonies, of ant

1968
01:36:07,522 --> 01:36:09,276
colonies, but you could even replace the

1969
01:36:09,298 --> 01:36:11,516
word ant and just say, well, we have

1970
01:36:11,698 --> 01:36:14,476
clusters of intelligent agents,

1971
01:36:14,578 --> 01:36:17,208
or whatever degree of modeling we're

1972
01:36:17,224 --> 01:36:19,136
doing. Because again, I do want to

1973
01:36:19,158 --> 01:36:21,216
emphasize that at least the cans and

1974
01:36:21,238 --> 01:36:23,088
ants agents that I have worked with in

1975
01:36:23,094 --> 01:36:25,680
the context of the Raman, they are not

1976
01:36:25,750 --> 01:36:28,212
each and of themselves, even, I would

1977
01:36:28,266 --> 01:36:31,344
argue, if nothing else, a very extremely

1978
01:36:31,392 --> 01:36:34,470
simplified generative model or a very

1979
01:36:34,840 --> 01:36:37,936
simple control system. There's no neural

1980
01:36:37,968 --> 01:36:39,428
network under each one because then

1981
01:36:39,434 --> 01:36:41,408
you'd have to simulate computationally

1982
01:36:41,504 --> 01:36:43,096
each one of these ants within the

1983
01:36:43,118 --> 01:36:44,232
framework. So I think there's always

1984
01:36:44,286 --> 01:36:47,256
that practical machine learning kind of

1985
01:36:47,278 --> 01:36:49,448
viewpoint of, well, there's always how

1986
01:36:49,454 --> 01:36:51,496
do you simulate that? And Alderam is

1987
01:36:51,518 --> 01:36:53,516
working with CPUs. It's not like he has

1988
01:36:53,538 --> 01:36:56,444
an army of GPUs to replace them with

1989
01:36:56,482 --> 01:36:58,236
convolutional networks. Again, if you

1990
01:36:58,258 --> 01:36:59,676
had the resources, this would be

1991
01:36:59,698 --> 01:37:02,604
awesome. But expense and money is

1992
01:37:02,642 --> 01:37:05,728
another constraint on this planet. But I

1993
01:37:05,734 --> 01:37:07,280
think there's interesting views and

1994
01:37:07,350 --> 01:37:10,192
interesting directions one could go by

1995
01:37:10,246 --> 01:37:13,132
taking inspiration from the ant metaphor

1996
01:37:13,196 --> 01:37:15,968
and the concept of pheromones and

1997
01:37:16,054 --> 01:37:19,172
translate them to other real world

1998
01:37:19,226 --> 01:37:20,948
signals and how, for example,

1999
01:37:21,034 --> 01:37:23,444
communication patterns among other

2000
01:37:23,482 --> 01:37:25,620
animal entities or other human agents.

2001
01:37:25,690 --> 01:37:27,156
And I think that that opens up an

2002
01:37:27,178 --> 01:37:29,056
interesting perspective. And if you're

2003
01:37:29,088 --> 01:37:31,770
constantly trying to connect it back to

2004
01:37:32,140 --> 01:37:34,616
free energy minimization and trying to

2005
01:37:34,638 --> 01:37:37,112
say, well, how are we balancing the

2006
01:37:37,166 --> 01:37:39,656
terms that you can decompose it into an

2007
01:37:39,678 --> 01:37:41,996
epistemic and an instrumental, and how

2008
01:37:42,018 --> 01:37:44,652
are these balancing out and how are

2009
01:37:44,706 --> 01:37:47,740
these physical processes that we specify

2010
01:37:48,720 --> 01:37:50,556
balancing those terms? That's just a

2011
01:37:50,578 --> 01:37:52,236
very interesting place to be. And you

2012
01:37:52,258 --> 01:37:54,476
mentioned active inference versions of

2013
01:37:54,498 --> 01:37:56,576
Hans, and that's fascinating and of

2014
01:37:56,598 --> 01:37:59,072
itself. Last comment I have is, again,

2015
01:37:59,126 --> 01:38:01,632
the degree of modeling and what you are

2016
01:38:01,686 --> 01:38:03,568
modeling. Like, if you're modeling a

2017
01:38:03,574 --> 01:38:06,176
society or organization, that's one way.

2018
01:38:06,198 --> 01:38:09,308
You could use the ant colony framework,

2019
01:38:09,404 --> 01:38:12,328
if you will, or metaheuristic

2020
01:38:12,444 --> 01:38:16,996
optimization frameworks to then cast a

2021
01:38:17,018 --> 01:38:19,560
system, any type of complex multi agent

2022
01:38:19,630 --> 01:38:22,490
system, as an active inference kind of

2023
01:38:22,860 --> 01:38:25,912
engaging process. Or you could go really

2024
01:38:25,966 --> 01:38:28,408
low level and think about cells in a

2025
01:38:28,414 --> 01:38:31,976
body or units that make up organs or

2026
01:38:31,998 --> 01:38:33,736
organelles and trying to say, well, can

2027
01:38:33,758 --> 01:38:36,028
we use this to model that level of

2028
01:38:36,034 --> 01:38:37,916
granularity within, like a human or an

2029
01:38:37,938 --> 01:38:39,388
animal entity? Right? And I think

2030
01:38:39,394 --> 01:38:41,308
there's some fascinating questions about

2031
01:38:41,394 --> 01:38:43,740
how does this metaphor manifest itself

2032
01:38:43,890 --> 01:38:46,012
at different timescales and different

2033
01:38:46,066 --> 01:38:49,516
degrees of perspective about how you're

2034
01:38:49,548 --> 01:38:50,816
modeling? What are you looking at?

2035
01:38:50,838 --> 01:38:52,640
What's the picture that you want to

2036
01:38:52,710 --> 01:38:54,192
emulate? And of course, there's always

2037
01:38:54,246 --> 01:38:55,564
under the hood, this practical

2038
01:38:55,612 --> 01:38:57,664
consideration of, well, okay, the

2039
01:38:57,702 --> 01:38:59,824
computational expense that you allocate,

2040
01:38:59,952 --> 01:39:02,164
and are you able to actually run that

2041
01:39:02,202 --> 01:39:04,036
simulation long enough? Because I think

2042
01:39:04,058 --> 01:39:06,496
Abdul Rahman, you can correct me if I'm

2043
01:39:06,528 --> 01:39:08,244
wrong. You mentioned one of the

2044
01:39:08,282 --> 01:39:10,496
experiments I think was for the bigger

2045
01:39:10,528 --> 01:39:12,424
systems took a month. Right. Of course

2046
01:39:12,462 --> 01:39:16,488
this was on CPUs. That can get pretty

2047
01:39:16,654 --> 01:39:18,472
prohibitive if you want to go even

2048
01:39:18,526 --> 01:39:20,696
bigger than that. But again, I think it

2049
01:39:20,718 --> 01:39:22,616
just depends on what hardware you have

2050
01:39:22,638 --> 01:39:25,050
to simulate this on. Yeah.

2051
01:39:25,420 --> 01:39:27,924
And also using high performance

2052
01:39:27,972 --> 01:39:30,268
computing, it is not always feasible for

2053
01:39:30,354 --> 01:39:33,416
smaller. So if we try to model the brain

2054
01:39:33,448 --> 01:39:35,580
of Aims like small neural network,

2055
01:39:36,160 --> 01:39:39,884
using a GPU might not be feasible

2056
01:39:39,932 --> 01:39:41,904
solution. And Travis is a high

2057
01:39:41,942 --> 01:39:43,488
performance computing specialist here,

2058
01:39:43,574 --> 01:39:47,392
expert tell you that sending data

2059
01:39:47,446 --> 01:39:50,132
to a GPU and getting it back is very

2060
01:39:50,186 --> 01:39:52,896
time consuming and resources consuming.

2061
01:39:52,928 --> 01:39:56,932
So it will worsen the time

2062
01:39:56,986 --> 01:39:59,160
consumption rather than solving it,

2063
01:39:59,230 --> 01:40:01,800
because communicating between the main

2064
01:40:01,870 --> 01:40:06,484
memory and the GPU had an overhead.

2065
01:40:06,612 --> 01:40:09,944
So it has to be a big enough problem to

2066
01:40:09,982 --> 01:40:13,532
actually utilize this GPU in such

2067
01:40:13,666 --> 01:40:14,700
solutions.

2068
01:40:16,880 --> 01:40:19,390
Sorry, go ahead.

2069
01:40:21,040 --> 01:40:23,764
When you have the super large language

2070
01:40:23,832 --> 01:40:26,560
models, or large models for computer

2071
01:40:26,630 --> 01:40:30,300
vision, they do a lot of just massive

2072
01:40:30,460 --> 01:40:32,096
operations on Tensors, which are

2073
01:40:32,118 --> 01:40:33,884
basically multidimensional matrices,

2074
01:40:33,932 --> 01:40:36,760
right? And when you have really big wide

2075
01:40:36,860 --> 01:40:39,476
Tensors, you can parallelize the

2076
01:40:39,498 --> 01:40:41,860
operation really nicely across the GPU.

2077
01:40:42,680 --> 01:40:45,908
A lot of this work is based on doing

2078
01:40:46,074 --> 01:40:48,016
time series forecasting, time series

2079
01:40:48,048 --> 01:40:50,776
classification on sensor data, stuff

2080
01:40:50,798 --> 01:40:54,136
from like power systems. So the

2081
01:40:54,158 --> 01:40:56,616
input to a large language model might be

2082
01:40:56,638 --> 01:41:00,436
1000 or more length

2083
01:41:00,468 --> 01:41:03,080
of a word embedding, which is actually

2084
01:41:03,230 --> 01:41:05,852
not huge. But if you go up to a computer

2085
01:41:05,906 --> 01:41:07,948
vision model, the input image may be

2086
01:41:08,114 --> 01:41:10,380
1000 by 1000 pixels and that gives you

2087
01:41:10,450 --> 01:41:12,716
actually a million inputs. Right. When

2088
01:41:12,738 --> 01:41:14,076
you're working with sensor systems, off

2089
01:41:14,098 --> 01:41:16,188
of aircraft, power plants, that type of

2090
01:41:16,194 --> 01:41:18,100
thing, you may have 50 to 100 inputs.

2091
01:41:18,200 --> 01:41:19,536
And when you're working with this type

2092
01:41:19,558 --> 01:41:21,296
of time series data, you don't need a

2093
01:41:21,318 --> 01:41:24,416
massive super wide neural network. And

2094
01:41:24,438 --> 01:41:26,176
then if you add in recurrency where you

2095
01:41:26,198 --> 01:41:28,096
have to do backprop over time and other

2096
01:41:28,118 --> 01:41:29,196
things like this, you actually can't

2097
01:41:29,228 --> 01:41:31,272
really parallelize it nicely on a GPU.

2098
01:41:31,356 --> 01:41:33,348
So for us, the CPU is actually more

2099
01:41:33,434 --> 01:41:35,588
efficient. We tried Abdul Rahman wrote a

2100
01:41:35,594 --> 01:41:37,524
bunch of code a long time ago to put

2101
01:41:37,562 --> 01:41:39,204
this stuff on GPUs and we found it was

2102
01:41:39,242 --> 01:41:41,316
quite a bit slower. So, depending on

2103
01:41:41,338 --> 01:41:42,516
what you're doing with a neural network,

2104
01:41:42,548 --> 01:41:44,330
a GPU actually isn't the right answer.

2105
01:41:45,180 --> 01:41:46,744
But the other cool thing about this,

2106
01:41:46,782 --> 01:41:48,344
which I think does have maybe even

2107
01:41:48,382 --> 01:41:52,216
potential for there, is that one

2108
01:41:52,238 --> 01:41:54,856
of the big not talked about problems in

2109
01:41:54,878 --> 01:41:56,024
machine learning is that back

2110
01:41:56,062 --> 01:41:58,604
propagation is the fastest thing we

2111
01:41:58,642 --> 01:42:01,500
know, but it's inherently not scalable.

2112
01:42:02,080 --> 01:42:04,684
You can get a bigger, better GPU to do

2113
01:42:04,722 --> 01:42:06,588
your bits of your network in parallel to

2114
01:42:06,594 --> 01:42:08,304
speed things up, but that only gets

2115
01:42:08,342 --> 01:42:10,256
better if you have a bigger network. If

2116
01:42:10,278 --> 01:42:11,392
you want to speed up the training

2117
01:42:11,446 --> 01:42:13,820
process, you can't just add another CPU

2118
01:42:13,980 --> 01:42:17,104
or another GPU and make back prop go

2119
01:42:17,142 --> 01:42:19,188
faster. You can make the forward and

2120
01:42:19,194 --> 01:42:20,336
backward pass through your neural

2121
01:42:20,368 --> 01:42:22,628
network faster. But you still have to do

2122
01:42:22,714 --> 01:42:26,356
every epoch of backprop. Iteratively a

2123
01:42:26,378 --> 01:42:29,060
method like this where we're generating

2124
01:42:29,400 --> 01:42:31,204
one, it's backprop free. So it's not

2125
01:42:31,242 --> 01:42:33,676
using backprop. We can use a nature

2126
01:42:33,728 --> 01:42:35,956
inspired or other method to use hundreds

2127
01:42:35,988 --> 01:42:37,544
of computers and you can throw twice as

2128
01:42:37,582 --> 01:42:39,112
many computers at it and get a result

2129
01:42:39,166 --> 01:42:41,396
twice as back, twice as fast, whereas

2130
01:42:41,508 --> 01:42:43,548
backprop you can't do that. So if you

2131
01:42:43,554 --> 01:42:45,372
think about actually being able to train

2132
01:42:45,426 --> 01:42:48,456
a neural network backprop,

2133
01:42:48,488 --> 01:42:51,496
actually it's got a pretty low speed

2134
01:42:51,528 --> 01:42:53,896
limit for what we need to do. And it's

2135
01:42:53,928 --> 01:42:54,936
kind of a big problem with the machine

2136
01:42:54,968 --> 01:42:55,892
learning community that people don't

2137
01:42:55,896 --> 01:42:56,928
like to talk about because they're like

2138
01:42:56,934 --> 01:42:58,396
oh, I'll just buy the next big Nvidia

2139
01:42:58,428 --> 01:43:01,200
GPU and that'll do things faster.

2140
01:43:02,580 --> 01:43:04,752
That's super interesting. Does this

2141
01:43:04,806 --> 01:43:07,188
maybe even bring up a kind of

2142
01:43:07,354 --> 01:43:09,876
relationship where things like a

2143
01:43:09,898 --> 01:43:12,608
graphics visualization of course a GPU

2144
01:43:12,704 --> 01:43:14,980
does well and that's like the screen

2145
01:43:15,130 --> 01:43:18,016
changing through time with a classical

2146
01:43:18,208 --> 01:43:22,120
process that can be massively unfolded.

2147
01:43:22,860 --> 01:43:25,604
And then the cognitive models,

2148
01:43:25,652 --> 01:43:27,656
ultimately, of the nest mates, which

2149
01:43:27,678 --> 01:43:29,768
again, can be nested. But the thing

2150
01:43:29,774 --> 01:43:31,584
that's more quantum, more cognitive

2151
01:43:31,652 --> 01:43:35,020
model, like you can do in parallel,

2152
01:43:35,680 --> 01:43:37,992
because the minds are not influencing

2153
01:43:38,056 --> 01:43:40,476
each other except through stigma g. So

2154
01:43:40,498 --> 01:43:42,956
then that is CPU bound, the size of the

2155
01:43:42,978 --> 01:43:46,144
colony. And then you could use

2156
01:43:46,182 --> 01:43:48,064
different graphics techniques, like

2157
01:43:48,102 --> 01:43:51,584
there are colonies, organisms. So one

2158
01:43:51,622 --> 01:43:54,924
ant being or it's a philosophical

2159
01:43:54,972 --> 01:43:57,796
question. What is the scale at which A

2160
01:43:57,898 --> 01:44:01,040
exists? But all throughout California

2161
01:44:01,120 --> 01:44:02,964
with the Argentine ant, for example.

2162
01:44:03,082 --> 01:44:04,848
And so how do we deal with those kinds

2163
01:44:04,864 --> 01:44:08,356
of meshwork cognitive systems all the

2164
01:44:08,378 --> 01:44:10,920
way on through 50 in an Acorn?

2165
01:44:11,340 --> 01:44:13,556
There's just all these different trade

2166
01:44:13,588 --> 01:44:16,136
offs that are being made and like in the

2167
01:44:16,158 --> 01:44:19,160
featureless deserts there's different

2168
01:44:19,230 --> 01:44:22,344
wayfinding pathfinding sensor

2169
01:44:22,392 --> 01:44:24,204
integration, polarization of light,

2170
01:44:24,242 --> 01:44:26,328
like different cognitive strategies

2171
01:44:26,504 --> 01:44:28,892
because they might be going out long

2172
01:44:28,946 --> 01:44:31,244
distance and dragging something home,

2173
01:44:31,282 --> 01:44:32,856
not leaving any pheromone because it's

2174
01:44:32,888 --> 01:44:35,088
not any more likely to have food there.

2175
01:44:35,174 --> 01:44:36,784
So in that case, the stigma g is

2176
01:44:36,822 --> 01:44:39,840
basically minimal to essentially none.

2177
01:44:40,180 --> 01:44:42,176
And then in other situations you could

2178
01:44:42,198 --> 01:44:45,104
have something that's very adherent to

2179
01:44:45,142 --> 01:44:47,844
distributions to the point of being fit

2180
01:44:48,042 --> 01:44:51,632
to a very kind of normative

2181
01:44:51,696 --> 01:44:54,740
path. But that's happening at a level

2182
01:44:54,810 --> 01:44:58,400
that allows the different compute

2183
01:44:58,480 --> 01:45:01,304
architectures, different information

2184
01:45:01,422 --> 01:45:04,692
architectures and ultimately

2185
01:45:04,756 --> 01:45:07,944
different biological embodiments to

2186
01:45:07,982 --> 01:45:10,900
really engage fruitfully.

2187
01:45:11,060 --> 01:45:13,348
Again looking at the variability, the

2188
01:45:13,374 --> 01:45:15,436
diversity of biological algorithms for

2189
01:45:15,458 --> 01:45:17,996
collective behavior which have been

2190
01:45:18,018 --> 01:45:20,044
studied by Professor Gordon and others

2191
01:45:20,082 --> 01:45:21,964
in so many different angles. Yet

2192
01:45:22,002 --> 01:45:23,404
sometimes it can feel like multi agent

2193
01:45:23,442 --> 01:45:24,976
models always start kind of like at

2194
01:45:24,998 --> 01:45:27,456
square one, demonstrate some proof of

2195
01:45:27,478 --> 01:45:30,064
concept phenomena, and then that is

2196
01:45:30,102 --> 01:45:31,676
utilized as part of a bigger

2197
01:45:31,708 --> 01:45:34,096
perspective. But it's not like that

2198
01:45:34,198 --> 01:45:37,716
model was ever claimed to have been

2199
01:45:37,898 --> 01:45:40,148
tuned to maximum performance. It's like

2200
01:45:40,154 --> 01:45:42,100
well, we got decision making behavior.

2201
01:45:42,600 --> 01:45:43,972
You could transfer this to group

2202
01:45:44,026 --> 01:45:46,144
decision making with Honeybee decision

2203
01:45:46,192 --> 01:45:48,964
making or something like that, but

2204
01:45:49,162 --> 01:45:52,196
there's still a big gap there. But I

2205
01:45:52,218 --> 01:45:54,004
think what you're describing with the

2206
01:45:54,042 --> 01:45:55,864
Kant, which is funny because it could be

2207
01:45:55,902 --> 01:45:58,760
cannot, but also the Kant is the dialect

2208
01:45:59,420 --> 01:46:01,836
which is spoken. It was very funny when

2209
01:46:01,858 --> 01:46:05,192
we came up with it. Great choice.

2210
01:46:05,256 --> 01:46:07,016
And it's just like yeah, because there's

2211
01:46:07,048 --> 01:46:10,350
multiple perspectives to swap from on

2212
01:46:10,880 --> 01:46:13,272
the classical screen because the meaning

2213
01:46:13,336 --> 01:46:15,756
of the word is something that's

2214
01:46:15,788 --> 01:46:17,308
happening. That fourth dimension,

2215
01:46:17,484 --> 01:46:20,432
cognitively, the meaning of the word

2216
01:46:20,486 --> 01:46:24,012
isn't to be found just on the blanket,

2217
01:46:24,156 --> 01:46:25,868
just on the interface itself. That's

2218
01:46:25,884 --> 01:46:28,996
just the communication. And that's like

2219
01:46:29,018 --> 01:46:30,628
a bounded system. Then if you model a

2220
01:46:30,634 --> 01:46:32,036
cognitive system that doesn't have that

2221
01:46:32,058 --> 01:46:35,428
kind of a constraint, so represented by

2222
01:46:35,434 --> 01:46:38,576
a map that has some kind of blanket

2223
01:46:38,768 --> 01:46:41,528
index, some kind of blanketing if you

2224
01:46:41,534 --> 01:46:43,016
don't embody that constraint in the

2225
01:46:43,038 --> 01:46:45,348
statistical model, the map, you're

2226
01:46:45,364 --> 01:46:47,012
ignoring one of the fundamental

2227
01:46:47,156 --> 01:46:50,584
constraints of modeling the way

2228
01:46:50,622 --> 01:46:51,956
that things happen in an embodied

2229
01:46:51,988 --> 01:46:54,024
fashion. Maybe there's some abstract

2230
01:46:54,072 --> 01:46:56,076
space for a certain problem that's just

2231
01:46:56,098 --> 01:46:59,324
like a total slam dunk. However, for

2232
01:46:59,362 --> 01:47:01,500
full generality, at least to the space

2233
01:47:01,570 --> 01:47:04,396
that we know of, biological life forms

2234
01:47:04,428 --> 01:47:06,800
and their engagements and ecological

2235
01:47:08,660 --> 01:47:10,944
engagements, not just like within one

2236
01:47:10,982 --> 01:47:14,880
behavior, that space is so vast

2237
01:47:15,380 --> 01:47:16,992
and there's so much to learn across

2238
01:47:17,046 --> 01:47:19,588
different systems within two. Again, to

2239
01:47:19,674 --> 01:47:22,916
abduce away into different information

2240
01:47:23,018 --> 01:47:25,892
architectures and active inference being

2241
01:47:25,946 --> 01:47:28,230
some subset or type of those.

2242
01:47:29,260 --> 01:47:31,656
So it's awesome work. Do you have any

2243
01:47:31,758 --> 01:47:32,920
last comments?

2244
01:47:37,970 --> 01:47:40,974
Well, giving ability for brands to be

2245
01:47:41,012 --> 01:47:42,942
self aware and aware about its

2246
01:47:42,996 --> 01:47:44,146
environment is actually something that

2247
01:47:44,168 --> 01:47:47,970
we implemented in our last work

2248
01:47:48,040 --> 01:47:51,634
with the BP free camps. They are

2249
01:47:51,672 --> 01:47:53,854
indirectly aware about their environment

2250
01:47:53,902 --> 01:47:57,798
and they are adapting to the changes

2251
01:47:57,884 --> 01:47:59,638
in their clinical environment by

2252
01:47:59,804 --> 01:48:02,470
indirectly meaning that they are

2253
01:48:02,620 --> 01:48:06,390
evolving using a genetic based algorithm

2254
01:48:07,370 --> 01:48:09,814
to just change their behavior, like how

2255
01:48:09,852 --> 01:48:13,366
they sense the

2256
01:48:13,388 --> 01:48:15,562
hormones when they take the steps and

2257
01:48:15,616 --> 01:48:16,826
some other times or some other

2258
01:48:16,848 --> 01:48:18,826
characteristics they have. So they are

2259
01:48:18,848 --> 01:48:21,920
adapting, but kind of like not in

2260
01:48:22,450 --> 01:48:27,566
an intelligent way, but through

2261
01:48:27,588 --> 01:48:29,246
evolution. If you want to say,

2262
01:48:29,348 --> 01:48:33,286
actually, we considered putting a brain

2263
01:48:33,338 --> 01:48:35,314
in each one of these agents. But then

2264
01:48:35,352 --> 01:48:38,562
again, as Travis and

2265
01:48:38,616 --> 01:48:41,966
Alex mentioned, we found that it won't

2266
01:48:41,998 --> 01:48:44,062
be practical. Actually, it would hinder

2267
01:48:44,126 --> 01:48:47,366
our Asynchronous design

2268
01:48:47,468 --> 01:48:50,326
because we couldn't prolize that it will

2269
01:48:50,348 --> 01:48:52,294
take time to train each one of these

2270
01:48:52,332 --> 01:48:54,790
brains as we evolve the neural networks.

2271
01:48:58,560 --> 01:48:59,310
Awesome.

2272
01:49:01,140 --> 01:49:03,760
Alexander or Travis, any last thoughts?

2273
01:49:12,670 --> 01:49:13,420
Okay,

2274
01:49:16,510 --> 01:49:18,622
I'm just really happy. I mean, I think

2275
01:49:18,676 --> 01:49:20,206
this work is really interesting and it

2276
01:49:20,228 --> 01:49:23,290
opens up a lot of pretty cool avenues.

2277
01:49:23,370 --> 01:49:26,878
Again, if we can get to the point where

2278
01:49:27,044 --> 01:49:28,926
we're evolving colonies that are

2279
01:49:28,948 --> 01:49:32,194
producing ants and can see where that

2280
01:49:32,232 --> 01:49:34,466
can go. So one of the big issues in

2281
01:49:34,488 --> 01:49:36,514
neural architecture search is the whole

2282
01:49:36,552 --> 01:49:38,046
question of what is an optimal neural

2283
01:49:38,078 --> 01:49:39,486
network and what an optimal neural

2284
01:49:39,518 --> 01:49:43,014
network is. Could be different for what

2285
01:49:43,052 --> 01:49:44,514
will be different for different tasks.

2286
01:49:44,562 --> 01:49:46,934
But not only that, even if it's the same

2287
01:49:46,972 --> 01:49:49,106
data set, how you're using that neural

2288
01:49:49,138 --> 01:49:51,686
network could lead to a lesser, more

2289
01:49:51,708 --> 01:49:52,918
optimal neural network, right?

2290
01:49:53,004 --> 01:49:54,258
Depending on what you're doing with it,

2291
01:49:54,284 --> 01:49:55,930
maybe you need one that's more energy

2292
01:49:56,000 --> 01:49:58,234
efficient. Maybe you don't care about

2293
01:49:58,272 --> 01:50:00,634
energy efficiency or performance, and

2294
01:50:00,672 --> 01:50:01,846
you'll take a slower neural network,

2295
01:50:01,878 --> 01:50:04,634
but you need more accuracy. So being

2296
01:50:04,672 --> 01:50:06,730
able to have algorithms which can

2297
01:50:06,880 --> 01:50:09,054
automate this whole process for us and

2298
01:50:09,092 --> 01:50:10,638
tune them to what we actually want to

2299
01:50:10,644 --> 01:50:11,886
use the neural network for is really

2300
01:50:11,908 --> 01:50:14,606
important. And I think, one, just having

2301
01:50:14,628 --> 01:50:16,366
ant colony optimization be able to

2302
01:50:16,388 --> 01:50:17,886
optimize a network for a problem is

2303
01:50:17,908 --> 01:50:19,394
great. But two, if we can make it such

2304
01:50:19,432 --> 01:50:21,362
that the algorithm itself is self

2305
01:50:21,416 --> 01:50:24,254
optimizing, it really can streamline

2306
01:50:24,302 --> 01:50:27,234
this whole process where right now,

2307
01:50:27,272 --> 01:50:28,706
if you're doing machine learning, it can

2308
01:50:28,728 --> 01:50:30,846
be kind of miserable. You make a neural

2309
01:50:30,878 --> 01:50:32,326
network architecture, you try it out,

2310
01:50:32,348 --> 01:50:34,434
see how well it does. Oh no, that didn't

2311
01:50:34,562 --> 01:50:35,958
do so well. Let me tweak a couple of

2312
01:50:35,964 --> 01:50:38,840
knobs automating that process.

2313
01:50:39,450 --> 01:50:42,006
My whole life as a computer scientist is

2314
01:50:42,028 --> 01:50:43,734
about being lazy, but being smart about

2315
01:50:43,772 --> 01:50:47,498
it. So whatever I can optimize so that

2316
01:50:47,504 --> 01:50:48,714
I don't have to do it over and over

2317
01:50:48,752 --> 01:50:51,450
again seems like a good use of my time.

2318
01:50:51,520 --> 01:50:53,466
So I'll be smart about having to do as

2319
01:50:53,488 --> 01:50:55,120
little as possible in the future.

2320
01:50:59,410 --> 01:51:01,358
I don't have too much to add to that. I

2321
01:51:01,364 --> 01:51:04,174
think a lot of the good discussion has

2322
01:51:04,212 --> 01:51:06,570
happened already, and we talked about

2323
01:51:06,660 --> 01:51:10,430
various implications and ways of viewing

2324
01:51:10,590 --> 01:51:13,282
ant colony optimization from other

2325
01:51:13,336 --> 01:51:14,846
perspectives, including an active

2326
01:51:14,878 --> 01:51:17,282
inference point of view. So I guess

2327
01:51:17,336 --> 01:51:19,894
really more from a closing thought on my

2328
01:51:19,932 --> 01:51:23,014
end, is that it will be interesting, or

2329
01:51:23,052 --> 01:51:25,654
it is an interesting direction to think

2330
01:51:25,692 --> 01:51:28,146
about, like I said earlier or suggested,

2331
01:51:28,178 --> 01:51:31,830
about the adaptation of the metaphor to

2332
01:51:31,900 --> 01:51:34,122
other systems. And what are you trying

2333
01:51:34,176 --> 01:51:37,290
to model and what's your goals from a

2334
01:51:37,360 --> 01:51:39,626
scientific and philosophical point of

2335
01:51:39,648 --> 01:51:41,706
view? What are questions you seek to

2336
01:51:41,728 --> 01:51:43,594
answer? And I think it might be very

2337
01:51:43,632 --> 01:51:46,202
interesting, again, given other

2338
01:51:46,256 --> 01:51:48,560
developments and computing technology

2339
01:51:49,250 --> 01:51:52,062
and ways in which you implement. The

2340
01:51:52,116 --> 01:51:54,586
parallelization that, I think, is that's

2341
01:51:54,618 --> 01:51:57,198
what attracted me the most to a lot of

2342
01:51:57,204 --> 01:51:59,538
these meta heuristic algorithms, even

2343
01:51:59,624 --> 01:52:02,546
things like particle swarm. And when I

2344
01:52:02,568 --> 01:52:04,866
worked with Travis many years ago on the

2345
01:52:04,888 --> 01:52:07,186
exam algorithms, you saw our names on

2346
01:52:07,208 --> 01:52:10,102
that and working on that type of stuff.

2347
01:52:10,156 --> 01:52:11,794
The part that always caught my attention

2348
01:52:11,842 --> 01:52:13,526
is, again, that ability to say, I can

2349
01:52:13,548 --> 01:52:15,800
put these entities on different

2350
01:52:16,170 --> 01:52:18,370
processing, computing, processing

2351
01:52:18,530 --> 01:52:21,720
resources or devices, and then they will

2352
01:52:22,110 --> 01:52:24,394
interact and exchange their results in

2353
01:52:24,432 --> 01:52:28,714
some way to try to optimize some often

2354
01:52:28,832 --> 01:52:31,658
complex multi objective cost function.

2355
01:52:31,744 --> 01:52:34,442
And so I think the part that we'll see

2356
01:52:34,496 --> 01:52:37,734
or that would encourage the wider spread

2357
01:52:37,782 --> 01:52:40,338
adoption of even like meta heuristic

2358
01:52:40,374 --> 01:52:41,998
algorithms in general, not to say that

2359
01:52:42,004 --> 01:52:43,710
they aren't used a lot in, for example,

2360
01:52:43,780 --> 01:52:46,286
the engineering domains is again the

2361
01:52:46,308 --> 01:52:48,646
development of parallel computing

2362
01:52:48,698 --> 01:52:50,082
processing systems and I think

2363
01:52:50,136 --> 01:52:52,094
exploiting things like asynchronous

2364
01:52:52,142 --> 01:52:53,646
computing. That was again another angle

2365
01:52:53,678 --> 01:52:55,758
that caught my attention from Travis.

2366
01:52:55,854 --> 01:52:58,206
He's done a lot of work on genetic

2367
01:52:58,238 --> 01:53:00,326
optimization and neurovolution from an

2368
01:53:00,348 --> 01:53:02,166
asynchronous point of view. And how can

2369
01:53:02,188 --> 01:53:04,614
we allocate, whatever resources are

2370
01:53:04,652 --> 01:53:07,926
available and distributing them across

2371
01:53:08,028 --> 01:53:10,086
global networks? I think that might be

2372
01:53:10,108 --> 01:53:13,626
the best shot to scaling up, let's say,

2373
01:53:13,648 --> 01:53:15,274
with what we got right now. There might

2374
01:53:15,312 --> 01:53:17,238
be again, you've mentioned, Daniel,

2375
01:53:17,334 --> 01:53:20,106
quantum technology. Quantum computing is

2376
01:53:20,128 --> 01:53:21,738
another interesting place that sort of

2377
01:53:21,744 --> 01:53:25,626
like changes the barring

2378
01:53:25,738 --> 01:53:28,282
technologies that we don't necessarily

2379
01:53:28,346 --> 01:53:30,414
have exactly at their best at this

2380
01:53:30,452 --> 01:53:33,054
moment. How can we take advantage of

2381
01:53:33,252 --> 01:53:36,742
citizen science or distributed computing

2382
01:53:36,906 --> 01:53:39,214
or peer to peer type of communication

2383
01:53:39,262 --> 01:53:42,126
and building massive active inference

2384
01:53:42,158 --> 01:53:45,458
systems that embody like the multi agent

2385
01:53:45,624 --> 01:53:48,466
metaphor of ant colony optimization or

2386
01:53:48,488 --> 01:53:51,000
other nature inspired frameworks? And

2387
01:53:51,450 --> 01:53:55,158
can this system evolve over very

2388
01:53:55,244 --> 01:53:57,730
long spans of time, just like evolution

2389
01:53:57,810 --> 01:53:59,862
really worked? Another piece, my final

2390
01:53:59,916 --> 01:54:02,906
comp and is that why I'm interested

2391
01:54:03,008 --> 01:54:06,394
sometimes in evolution is that to

2392
01:54:06,432 --> 01:54:09,082
me it is the inductive bias that

2393
01:54:09,136 --> 01:54:11,370
provided us with structures that allow,

2394
01:54:11,440 --> 01:54:13,846
for example, a human agent. Babies can

2395
01:54:13,968 --> 01:54:15,694
to operate already. Babies can already

2396
01:54:15,732 --> 01:54:18,462
recognize faces, right? And we have

2397
01:54:18,516 --> 01:54:20,654
certain instinctual reactions and

2398
01:54:20,692 --> 01:54:22,974
certain mechanisms that evolution has

2399
01:54:23,012 --> 01:54:25,002
endowed us with. And so a fascinating

2400
01:54:25,066 --> 01:54:28,594
question is what is the interplay of the

2401
01:54:28,632 --> 01:54:31,586
idea of simulating an artificial form of

2402
01:54:31,608 --> 01:54:34,382
evolution? Maybe building DNA structures

2403
01:54:34,446 --> 01:54:36,782
or very simplified computational

2404
01:54:36,846 --> 01:54:38,366
structures? Which would answer Abdel

2405
01:54:38,398 --> 01:54:40,546
Rahman's concern about, well, maybe we

2406
01:54:40,568 --> 01:54:42,270
don't want the agents to be too smart in

2407
01:54:42,280 --> 01:54:44,022
of themselves because I can't really

2408
01:54:44,076 --> 01:54:46,326
simulate that unless you give me like a

2409
01:54:46,348 --> 01:54:48,886
decade to run the simulator. But you

2410
01:54:48,908 --> 01:54:50,294
could maybe come up with a more

2411
01:54:50,332 --> 01:54:52,166
fundamental primitive and then use that

2412
01:54:52,188 --> 01:54:54,166
as a starting point for your neural

2413
01:54:54,198 --> 01:54:56,298
network. Let's say, Daniel, you want to

2414
01:54:56,304 --> 01:54:59,434
do some task in image segmentation and

2415
01:54:59,552 --> 01:55:01,354
like okay, but what can your

2416
01:55:01,392 --> 01:55:03,226
evolutionary framework give me? Well,

2417
01:55:03,248 --> 01:55:05,102
I'll say here here's a template to start

2418
01:55:05,156 --> 01:55:07,694
from. This is a kernel on which you

2419
01:55:07,732 --> 01:55:10,154
build your framework and it's like a DNA

2420
01:55:10,282 --> 01:55:13,198
structure. And this has evolved across

2421
01:55:13,364 --> 01:55:16,374
many years of distributed peer to peer

2422
01:55:16,442 --> 01:55:18,094
computing. And you could imagine

2423
01:55:18,142 --> 01:55:20,654
building this mammoth evolving,

2424
01:55:20,782 --> 01:55:24,290
continual learning style evolutionary

2425
01:55:24,630 --> 01:55:26,818
algorithm, whether it is based on

2426
01:55:26,984 --> 01:55:29,474
genetic algorithms or ant colony. And

2427
01:55:29,512 --> 01:55:30,798
you could imagine that might be an

2428
01:55:30,824 --> 01:55:32,726
interesting way to think about. And by

2429
01:55:32,748 --> 01:55:35,106
the way, I am spitballing and generating

2430
01:55:35,218 --> 01:55:38,086
an idea of how I could envision a

2431
01:55:38,108 --> 01:55:40,694
scalable form without inventing a new

2432
01:55:40,732 --> 01:55:43,002
computing system that I don't know will

2433
01:55:43,056 --> 01:55:46,506
or will not exist because quantum has a

2434
01:55:46,528 --> 01:55:48,234
lot of problems still to solve too,

2435
01:55:48,272 --> 01:55:50,442
like superconducting or super

2436
01:55:50,496 --> 01:55:53,690
temperatures or trapping photons,

2437
01:55:54,030 --> 01:55:56,906
as I have learned. So that might be an

2438
01:55:56,928 --> 01:55:58,494
interesting direction. I think the

2439
01:55:58,532 --> 01:56:00,654
scaling of this, especially from the

2440
01:56:00,692 --> 01:56:02,894
practical end, is going to be the most

2441
01:56:02,932 --> 01:56:04,302
important. We're going to. Need to pull

2442
01:56:04,356 --> 01:56:06,974
together all the tools that we have, as

2443
01:56:07,012 --> 01:56:09,614
I mentioned before. So I'll stop there,

2444
01:56:09,652 --> 01:56:11,066
too, because I'll let Abraham blame

2445
01:56:11,098 --> 01:56:12,960
more, so hopefully that made sense.

2446
01:56:13,970 --> 01:56:17,146
Well, this was very epic and inspiring,

2447
01:56:17,258 --> 01:56:20,310
so good luck with the work. You're all

2448
01:56:20,380 --> 01:56:24,134
welcome to suggest another piece

2449
01:56:24,172 --> 01:56:26,726
that we might focus on or continue the

2450
01:56:26,748 --> 01:56:28,454
discussion however you see fit, because

2451
01:56:28,492 --> 01:56:32,086
it's super interesting direction. So

2452
01:56:32,188 --> 01:56:34,822
thank you. Till next time. Thank you.

2453
01:56:34,956 --> 01:56:36,120
Thank you so much.



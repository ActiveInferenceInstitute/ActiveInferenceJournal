start	end	startTime	summary	headline	gist
170	33560	00:00	On August 10, 2023, we have Ahmed Elsaid, Alexander Arorbia and Travis Desel. Ahmed is going to give a presentation and following we will have some reflections and discussions.	Ahmed Elsaid will give a presentation in 2023	Presentation
34170	971154	00:34	Today I'm going to discuss the methods that we came up to solve new art, texture, search and neuralvolution problems. The methods that are ant colony optimization based solutions. And later I will discuss three of the points that we are considering for future work.	Ant colony optimization for neural text research and neurovolution	Ant colony optimization for neural network research and neurovolution
971192	1370660	16:11	In the next slide, I will discuss how we exploited or use this characteristic of anonym optimization to accelerate the solution that we came up with. The scheme of ants applying ants or ant collectimization in neural architecture search is depicted or illustrated in this flowchart.	In the next slide, I will discuss how we exploited anonym optimization to accelerate neural network optimization	Ants and Canes: Neural Architecture Search
1372470	1585640	22:52	This slide will discuss the different heuristics of the methods of ants. The first heuristic is superstructure itself. The superstructure consists of a neural network that is massively connected. Every node in the structure is connected to the other nodes via or through edges.	The superstructure consists of a neural network that is massively connected	Neural network
1587930	2302190	26:27	The second heuristic in Ash is the colony weight sharing. The third meta heuristic is multiple memory cells. The fourth mineristic is the multiple ant species. The fifth characteristic is the regularization of formal basement. And the sixth is jumping ants which is we want to experiment with.	We used the train weights to initialize the weights of newly generated neural networks	Generating a neural network with social ants
2306050	3388370	38:26	Ans was the first meta method to involve the core of ACO and column optimization and recurring neural networks. Exploiting recurrent connection proved successful because regularization component or the regularization heuristics give us better results. But the main drawback of Ans was the discrete search space.	Ans was the first meta method to involve the core of ACO and column optimization	Anatomy 2, Control the wandering ants
3389670	3581730	56:29	The future directions that we are future points that we're concerning for are to turn cans to a complete continuous search space. The third one is to actually consider one of the concepts that was coined in a book about ants. We want to see if this would give us a better performance.	The future directions that we are future points that we're concerning for	Future directions for RNNs
3584470	3842890	59:44	Alex Rorbia: How about Travis and Alexander? Either of you which want to go first, please feel free to give an introduction. Think about what does ant colony optimization how do you view it from an active inference perspective?	Alex Rorbia: Wow, what a great presentation	Ant-Based Optimization
3843820	3979430	1:04:03	Travis Sell: I thought this work was really very interesting in a lot of ways. The new version of ant colony optimization makes the search space continuous. And it's more like a real simulation of how ants would move around in the real world. Ahmed: The last thing that I discussed in the future directions is something that I actually started working on.	Travis Sell: I thought this work was very interesting	Ant colony optimization
3979800	4680020	1:06:19	Daniel: Why did you pursue foraging type algorithms overall? Does this class include the interaction based methods with direct agent contacts that Professor Gordon highlights in the Ant Encounters book? How do you see similarities and differences between neural network based approaches and active inference based approaches?	Daniel Rahman: Why did you pursue foraging type algorithms overall	Ant Foraging Algorithm
4683000	4964716	1:18:03	The quantum formalism becomes useful just by itself with or without reliance on some other electronic phenomena. That really opens up both the quantum and the classical information or both niche modification and behavioral and cognitive modeling. And then just like the ant colony algorithm is ultimately federated through embodiment, that property makes it a really useful candidate for biomimicry.	Silico allows you to combine quantum and classical information for multi agent modeling	Quantum and classical information in the brain
4964828	5058552	1:22:44	When you were saying blanket, you were referring to a Markov blanket, correct? Yes. The technical definition is when you have a Bayesian graph where nodes are the variables and edges are relationships amongst these variables. I just wanted to make sure that they got that from the physics point of thanks.	When you were saying blanket, you were referring to a Markov blanket	Markov Blanket
5058626	5132460	1:24:18	Solving generative models with more Generative models sounds very promising. What about replacing ants with convolutions? Talking about like a meta learning algorithm. Could a neural network learn how to optimize other neural networks?	Using nature based methods to optimize generative models sounds very promising	Machine Learning: Nature-based optimization
5134030	5877216	1:25:34	The method here is all continuous. So within this continuous search space it gives the algorithm a lot of freedom. How do you see this being used in different research or application domains? The main use case that we're thinking of was neural architecture search to apply it for other domains.	In the case of neural architecture search there's a couple of approaches	Neural Architecture Search: Convolution and Graph Based Search
5877398	6450760	1:37:57	Using high performance computing, it is not always feasible for smaller. If we try to model the brain of Aims like small neural network, using a GPU might not be feasible solution. For us, the CPU is actually more efficient.	Depending on what you're doing with a neural network, a GPU actually isn't the right answer	Will a GPU speed up neural network training?
6450830	6996770	1:47:30	Daniel: Giving ability for brands to be self aware and aware about its environment is actually something that we implemented in our last work with the BP free camps. How you're using that neural network could lead to a lesser, more optimal neural network. The parallelization that is that's what attracted me the most to a lot of these meta heuristic algorithms.	Daniel: I think this work is really interesting and opens up a lot of possibilities	Optimal neural networks: the future of AI

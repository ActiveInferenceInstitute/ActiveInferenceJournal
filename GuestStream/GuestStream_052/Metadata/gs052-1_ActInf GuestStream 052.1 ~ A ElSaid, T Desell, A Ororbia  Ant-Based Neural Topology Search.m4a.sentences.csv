start	end	sentNum	speaker	confidence	text
6850	7854	2	B	0.99799	Hello and welcome.
7972	10462	3	B	0.99992	This is ActInf Guest Stream 52.1.
10516	17070	4	B	0.76258	On August 10, 2023, we have Ahmed ElSaid, Alexander Ororbia, and Travis Desell.
17410	23562	5	B	0.57125	Ahmed is going to give a presentation and following we will have some reflections and discussions.
23626	28150	6	B	0.99976	So thank you all for joining and Ahmed to you for the presentation.
30490	33560	7	A	0.71364	Okay, thanks so much for having me.
34170	45180	8	A	0.99989	Today I'm going to discuss the methods that we came up to solve new art, texture, search and neuralvolution problems.
46110	51470	9	A	1	The methods that are ant colony optimization based solutions.
51970	60030	10	A	0.96901	So as the title here is mentioning, “ant colony optimization for neural text research and neuroevolution.”
61010	68094	11	A	0.99972	This work is collaboration between me, my previous advisor, Dr.
68212	74210	12	A	0.57125	Travis Desell, and my co advisor Dr. Alexander Ororbia.
75370	84230	14	A	0.6139	I'm currently an assistant professor in University of North Carolina, Wilmington.
86570	89110	15	A	0.99	And moving on to the next slide.
89610	102462	16	A	0.984	So as an overview, the things that I'm going to discuss today, I'll try to give Bird Eyes view for what is neurovolution, why we what, why we need it.
102516	115538	17	A	0.99	And from there, I'll just try to discuss our methods that is based on end calling optimization which is called N based neurotropology search or ants for short.
115704	135062	18	A	1	And after that I will discuss how we advanced with this idea by introducing continuous ants or cans for short, which could spread off the discrete search space and replace it with a continuous search space.
135196	145174	19	A	1	And after that I'm going to also say what we did with the three dimensions and the continuous ants to have back propagation free cans.
145302	152240	20	A	1	And later I will discuss three of the points that we are considering for future work.
153970	167406	21	A	0.50092	So in the machine learning learn, as the neural network structures got deeper and deeper, people were trying to optimize the structures to have better performance.
167518	184774	22	A	0.82	And people in different realms or different problem domain used to borrow the best performing architectures or structures and try to modify a little bit to work for their problem.
184892	193614	23	A	1	And they try to tweak some of the features of the structure and compare these different tweaks.
193682	196726	24	A	1	And then they say that we found the best performing structure.
196838	216500	25	A	0.99689	But to actually find an absolute optimum structure concerning that solution, it would be an NP hard problem because to reach that solution, they had to try all the combinations of the different structural elements, right?
218310	242454	26	A	0.99999	Because we have massive structures in these deep neural networks, it could be an NPR problem because we don't have computational power or enough time to actually train and test all these structures, all these structures constructed from these different combinations of structure elements.
242582	259422	27	A	0.68695	So the alternative way to do this is to actually try to apply a meteoristic method to convert to a near to upper solution that is much better than not optimizing the structure or relying on kind of like a random search, right?
259476	270610	28	A	0.88	A random search can get us better performing neural network, but it's not going to give us a near to optimal solution or to converge to a near to optimal solution.
274630	287320	29	A	0.93484	So a minoristic method would give us an automated method and also would converge to optimum structure new to optimum solution to this structure problem.
289950	299194	30	A	0.94091	So the way that people approached Nas was by trying to mimic how optimization is done in nature, right?
299312	319090	31	A	0.99131	So the first way they thought of it was trying to mimic how living organisms evolve in nature using genetic based algorithm like the Darwinian Genetic Evolution.
320710	350478	32	A	0.77026	It started with Neat, it's a short for Neuralution of Augmenting topologies and it relied on genetic algorithms where also that concept also applied in most of the Nas and Neural evolution methods and Exam is one of them.
350564	361310	33	A	0.85133	Travis came up with this method and it became one of the state of the art methods in Nas.
362450	379730	34	A	0.91228	So in such methods we tried to again mimic genetic evolution by introducing new structural elements or removing structural elements or altering the structure through the evolution process, through the evolution iterations and through the evolution generations.
379890	398794	35	A	0.99989	So we can apply mutations by splitting edges or adding edges or disabling edges, and we disable edges or disable some structure elements so that we don't lose that component, so that we can later on use it.
398912	402782	36	A	0.99997	Kind of like a dormant gene in a genome, right?
402836	405322	37	A	1	So that it can appear in later generations.
405386	406382	38	A	1	Not to get rid of.
406436	407194	39	A	0.98905	Totally.
407322	412806	40	A	0.99827	So in mutations we can disable edges, we can enable them in later generations.
412858	424450	41	A	0.61976	If we found that we want to try this option, we can also add recurrent edges or remove or enable or disable recurrent edges.
425190	445610	42	A	1	We can split nodes, we can take some of the nodes in a previous generation and we can just split it to two nodes and then take the edges connected to that node and try to divide it between the nodes that was generated from the previous node in the previous generation.
447870	452618	43	A	0.99999	Also we can add nodes to the structure.
452714	455486	44	A	0.99705	So all of these are part of the mutation process.
455588	465006	45	A	0.99797	In the genetic process we can also disable a node if we want to try to just get rid of one of the nodes and disabling a node.
465038	472078	46	A	1	Or multiple nodes will also disable the edges connected to that node beside mutations.
472254	492534	47	A	1	The other side of a genetic process is to do crossovers where we have two of the best performance population meet together to bring an offspring and the offspring will have collection of the characteristics coming from their parents.
492662	506110	48	A	0.97435	So it will take some of the characteristics from that parent, some other characteristics from the other parent, hoping that this would give us a better performing neural network or better performing generation.
508450	536502	49	A	0.99739	So the main problem sorry, so the main problem with genetic based algorithms is that they start with minimal structures like we can see there, meaning that inputs and outputs and starting from the optimization with this minimal search space can trap the method in a local minima through the optimization process.
536636	550926	50	A	0.94533	So we were thinking how to get rid of this obstacle by having a bigger or larger search space to start with, and then we can sample some solutions from that large surf space.
551108	562254	51	A	1	And we were looking around and we concerned end calling optimization, and I will say why, but I'll try to introduce the method first.
562292	569986	52	A	0.99984	So, the method was first introduced as graphic optimization method, graph optimization method, sorry.
570168	573570	53	A	0.57614	It was introduced in mid 90s by Marco DeRego.
573910	580838	54	A	0.67564	Marco DeRego applied this method on a travel salesman's problem.
580924	589962	55	A	1	And the problem is mainly about a travel salesman who wants to visit a number of cities in a country using the shortest staff and considering this problem.
590016	600810	56	A	0.5281	If we have different number, if the number of cities grows, then the permutations of these numbers of these cities that we have to consider to find the optimal solution.
601310	619422	57	A	0.99972	If this number of cities grows, then we will end up having an NPR problem because we won't have enough time or computational power to have this exclusive solution done or this exclusive search done.
619556	651610	58	A	0.92461	So, from his observations to answer in nature, he found that he can apply how they forage to find food in nature and then take this concept, this observation, and apply it in an algorithm to find the optimum path that leads from one point and to visit all the cities in the shortest path.
653790	675686	59	A	0.77437	So this observation, this slide and the coming slides will just try to give us a picture of how hence forage in nature to find food and then how Marcus Rigor took that concept to apply it in the travel systems problem.
675788	688440	60	A	0.94501	So, ants observers found that ants go out from their nest to find food and they try different directions, right?
689130	701690	61	A	1	And when they find food, eventually find food, they will take some of that food and then they will go back to their nest and in the way back, they will deposit some other substance, cholophone.
704050	709930	62	A	0.99902	So they deposit that substance so that they communicate that path to the food resource with the other ants.
710090	729110	63	A	0.99825	So actually, other ants do exploit this other substance, and when they sense it, they follow the path that the first ant took from the food resource to the nest, hoping that they will find food at the end of that path.
729930	735734	64	A	0.57206	When they actually find food at the end of the path, they will take some of the food and do the same thing.
735772	744006	65	A	0.99997	They will deposit some morpher mole on the same path, making it more appealing to other ants to take it, so that they can bring more food to the nest.
744198	750566	66	A	0.98697	So this process shows us the exploitation behavior of ants.
750678	773490	67	A	0.99994	But again, from time to time, ants also try to exploit some other food resources, potential food resources, and they kind of resist following the hormone traces, and they try to go away and find some new food resources for the nest.
774310	798700	68	A	0.96548	So the ants are not only exploiters, they're also explorers and these two concepts were used by Marco Derago to kind of like balance the search for the better or faster path between the cities for the travel sales management problem.
801710	811418	69	A	1	The third thing that we observed also in how I spoke in nature is that the older substance, the pheromone, also evaporates.
811514	829414	70	A	0.99866	So whenever a path to a food resource is not appealing anymore or the food resources are excluded, no more ants will take that path or when they take it and reach their food resources that is exposed, they will not take the same path to the nest again.
829452	833026	71	A	0.99995	They will try to wonder and to find more new food resources.
833138	846780	72	A	1	And because they not going that path again and not depositing any pheromone on that path, the pheromone will eventually evaporate and disappears and making it less and less appearing for other ants to take.
848510	858160	73	A	0.50836	So that's what Marco DeRego was looking at when he thought of the travel salesman's problem.
859890	871758	74	A	0.99927	He applied that for the travel salesman's problem by making one agent try these different paths and then comparing through each iteration.
871854	876486	75	A	0.97629	So that agent will take a path between the cities, right?
876588	890502	76	A	1	And then it will compare the length of that path to the previous experience with other paths and if it's shorter, it will try to deposit hormones on the segments of that path.
890646	894758	77	A	1	And eventually he was hoping, and he was right about what he was hoping.
894854	915170	78	A	0.99996	He was hoping that eventually the shorter path, shorter segments that give the ultimate shorter path has more and more hormone deposits making it more and more appealing for the agent to take it through the iterations.
915990	937918	79	A	0.8744	So we thought about this concept and we thought that it's very appealing to apply it for an Nas problem because Drago's solution was applied for a graph optimization problem and neural networks are in their sense direction graphs.
938114	953070	80	A	0.99174	So we also considered endcon optimization because it's full torrent, decentralized and scalable and it's also easily traceable going back to being decentralized.
955410	968660	81	A	0.89811	It made this method a perfect candidate for a pal and high performance computing solution which will eventually accelerate the optimization problem.
970630	971154	82	A	1	I think.
971192	988790	83	A	0.99973	In the next slide, after the next one, I will discuss how we exploited or use this characteristic of anonym optimization to accelerate the solution that we came up with or the method that we came up with, which is ants and cans.
989310	1006858	84	A	0.99208	So this scheme, or the scheme of ants applying ants or ant collectimization in neural architecture search is depicted or illustrated in this flowchart.
1007034	1035990	85	A	0.99972	So we start off with a massive search space expressed in superstructure which expresses or embodies a neural network that is massively connected, meaning that each node in that superstructure is connected with the other nodes through edges and recurrent edges, backwards and forward and backward recurrent edges.
1037550	1047814	86	A	1	And then we let a number of agents swarm over the structure from an input node to an output node.
1047862	1065582	87	A	0.99998	So each one of these agents will pick an input node and then it wanders from that node through the connection recurrent edges and recurrent edges and between the nodes and between the hidden layers till it gets and picks one of the output nodes.
1065646	1076370	88	A	1	And then we take all these paths of the different agents, and we put them together to form a structure, neural network structure.
1076450	1086914	89	A	0.96	And we take that structure and train it and test it and then compare its performance to a population of best performing neural networks, best performance structures.
1087042	1109360	90	A	0.97	And if it's better than the worst in the population, then we reward the path that the ants took, the agents took over in the superstructure, reward with hormone so that it makes these paths appealing for later iterations through the evolution process or the optimization process.
1109890	1126340	91	A	0.99553	That's if the generated structure is best than the worst in the population, if not, if it actually was worse than the worst in the population, then we discard that structure or that neural network, and we don't reward Any of the path that ants took.
1126650	1140090	92	A	1	And also the thermo evaporation will help us get rid of the pheromones that were deposited on the edges that are not giving us better and better structures.
1143710	1153230	93	A	1	And again, because the M colony is decentralized we exploited this by having an asynchronous solution or asynchronous evolution.
1154290	1168126	94	A	0.99991	We had a main process that took care of generating the new structures and also updating the population the best performing structures and updating the hormone on the superstructure.
1168238	1173934	95	A	0.99988	So the main process will generate structures and send them to worker processes.
1173982	1186870	96	A	1	The worker processes will train and test the neural network on the data that we have for the problem and then send the results back or the fitness of the neural network to the main process.
1186940	1198342	97	A	1	And based on that fitness the main process will either discard it or we'll take this fitness and compare it to the best performing in the population.
1198406	1204514	98	A	0.99979	If it's better than the worst it will reward the path that ends took on the superstructure by depositing more hormone.
1204582	1224034	99	A	0.99679	Or if it's worse than the worst in the population, we'll just start it and it will keep generating new structures and sending them to processes because the training which relies on that propagation is the most computationally expensive part in this process.
1224232	1247370	100	A	0.99998	If we have a number of worker processes that can work in parallel to train and evaluate these neural networks, these new structures we can speed up the process, right by training and evaluating different structures at the same time in parallel in an asynchronous evolution scheme.
1251890	1274782	101	A	0.99449	This is an animation but it's not working in this version of the slides because we're using a PDF but it's mainly a structure where you'll see edges or connections between these nodes fading or having darker colors based on the pheromone values through the iterations.
1274846	1301870	102	A	0.99983	So each frame in this animation is kind of an update for the pheromone value of the edges based on the performance of the version of the neural network that were generated by the agents when they swarmed from the start node taking one of the input nodes in the middle layer and then from there going to one of the hidden layers in this one hidden layer that we have here.
1301940	1303950	103	A	1	And from there going to the output.
1307090	1319940	104	A	0.99204	So that was the concept of applying ACO and con optimization in and this now I'm going to talk about the actual method that we came up with.
1322070	1327786	105	A	0.99651	So it's more generic and more powerful neural texture search method.
1327918	1351050	106	A	0.99989	More comprehensive if I may say that we opted to apply the methods on recurrent neural networks because they tend to be potentially larger than other neural networks structures because of their recurrent connections.
1351390	1370660	107	A	0.99982	So we thought that if we started this problem though the method or the concept applies to any neural network but applying it to recurrent neural networks made it more appealing challenge for measuring the performance of the method that we thought of.
1372470	1379506	108	A	0.99248	So this slide in the common slide will discuss the different heuristics of the methods of ants.
1379618	1395274	109	A	1	The first heuristic is superstructure itself and as I mentioned before it's a massive search space as massive as possible to be handled with the machine or the hardware that we're working on.
1395472	1406538	110	A	0.95	The superstructure consists of a neural network that is massively connected meaning that every node in the structure is connected to the other nodes via or through edges.
1406714	1409306	111	A	0.99917	Forward edges and recurrent edges.
1409418	1412270	112	A	0.56959	Backward forward recurrent edges and backward recurrent edges.
1412770	1421390	113	A	0.99978	This simple structure that we have here represents one of just the concept of the superstructure that we apply in ants.
1421550	1428306	114	A	0.97756	Here we have three input nodes, three hidden layers each have three nodes and one output node in the output layer.
1428418	1449658	115	A	1	And we are just showing one node connected to the other nodes through edges which are the ones representing green forward recurrent sorry, the edges are the one in gray and then forward recurrent and backward recurrent in the green and in the red.
1449824	1472100	116	A	1	And the concept of recurrent edges might be a little bit confusing if we look at it in this example because how can an edge be recurrent coming in and out from the same node, the nodes in the same timestamp but this structure might make it more clear.
1472470	1479142	117	A	0.99979	So here we have structure that is pretty much similar to the one we saw before.
1479196	1489446	118	A	0.99993	But here we have three input nodes, two hin layers still three hin layers each have three nodes and then alpha layer have two nodes and then we have also three time steps.
1489478	1503790	119	A	1	The current time step t zero and the previous time step t minus one, the one before that t minus two edges here are illustrated using the solid black lines.
1506130	1512590	120	A	0.85	Of course these edges are present in the current time step that is going to propagate through the neural network.
1513890	1516930	121	A	0.6554	Then the current connections.
1517270	1532370	122	A	0.99993	These ones are going to bring information or provide information from the previous time steps, the previous inputs or previous data that was fired to the nodes in the previous time steps.
1532530	1538050	123	A	0.82	And these recurrent edges are depicted here in red and orange.
1538130	1541398	124	A	1	The forward ones are depicted in red and orange.
1541494	1545980	125	A	1	The reds are coming from T minus one and the oranges are coming from T minus two.
1546990	1554080	126	A	1	And the backward current edges are the dot lines in blue and green.
1555170	1557482	127	A	1	And we can see that they are going backwards.
1557546	1562830	128	A	0.99988	So they go backward to backward layers.
1567190	1574318	129	A	0.97783	But because they are recurrent, we can do that because they process information, they bring information that already processed.
1574414	1582274	130	A	0.99997	So we don't have to worry about propagating information back through time or back through the structure.
1582322	1585640	131	A	0.99794	But we can do that if it's coming from previous time step.
1587930	1594780	132	A	0.96	The second heuristic in Ash is the colony weight sharing we wanted to use.
1596190	1614558	133	A	0.94769	So instead of Brandon initialized the weights or the snapped weights and generated neural networks, we wanted to use the train weights to initialize the weights of the newly generated neural networks.
1614654	1618530	134	A	0.99998	We did that by saving these neural networks on the superstructure.
1619510	1625226	135	A	0.99999	We used the last equation here to do this update.
1625278	1634546	136	A	0.99967	So we balanced between the weights that were saved previously and the weights that are coming from the trained or evaluated neural networks.
1634738	1638294	137	A	1	And we also used two strategies to do this update.
1638342	1645690	138	A	0.99987	We used a fixed parameter phi or we left phi.
1646270	1647526	139	A	0.99967	That was the first strategy.
1647558	1662186	140	A	1	The second strategy was to get phi by applying these two equations which relies on the performance of the neural network that was previously generated sorry, the previously generated and trained.
1662218	1669250	141	A	0.99908	So it relies on the performance of the neural network that was trained and validated or tested.
1669750	1687454	142	A	0.99152	So if the performance was good, then we will let the weights of that neural network to contribute more to the initialization of the weights of the newly generated RNS.
1687522	1694460	143	A	0.9311	If it's not performing that well, then this equation will not allow it to contribute that much.
1696190	1699254	144	A	1	The third meta heuristic is multiple memory cells.
1699302	1722606	145	A	0.99047	So at each node, when an agent or an ant reach to a node in the superstructure it will do a local search to pick the type of the neuron or the type of the node from these three different types of memory cells.
1722718	1729622	146	A	0.9981	So in the generated RNN, the generated structure, the nodes in the structure is not all the same.
1729756	1737206	147	A	0.9999	They will be different based on the local search that the agent will do or the ant will do.
1737228	1742890	148	A	0.93	At each node they reach through their path from the input to an output in the superstructure.
1744990	1748214	149	A	1	The fourth mineristic is the multiple ant species.
1748262	1753014	150	A	0.99995	So we applied different species or came up with different species.
1753062	1759738	151	A	0.99999	For the ants, the first species was the ones that will traverse over the edges, only the edges.
1759834	1768850	152	A	0.56325	So they will go only forward through the edges of the neural network of the superstructure.
1769190	1774990	153	A	0.56	And these ones are going to define the number of nodes in the generated structure.
1775070	1779122	154	A	0.9999	Also define the types of the nodes in the superstructure.
1779186	1789366	155	A	1	And when they done with their work, then the social or the second species, the social ants, will traverse between these nodes.
1789478	1796486	156	A	0.99987	But they will use the recurrent edges to move between these nodes.
1796518	1802314	157	A	0.99983	So they will create the recurrent edges for the newly generated RNN.
1802442	1808554	158	A	1	And we have two different species for these social ants or two different subtypes subspecies.
1808602	1814350	159	A	1	One is the forward social ends.
1814770	1819614	160	A	0.99642	These ones traverse only over the forward kind of connections.
1819662	1829946	161	A	0.99378	They go from the input to the output but only over the forward recurrent edges and then the backward recurrent edges or the backward social ends.
1829998	1836550	162	A	0.9185	These ones go from the output to the input and they traverse over the recurrent connections.
1837550	1854190	163	A	1	The reason we thought about these different species is that we wanted to control the tendency of the ants to wander around in the superstructure exploiting the convoluted mesh of recurrent connections.
1854850	1857022	164	A	0.98751	So we wanted to control that.
1857076	1869970	165	A	0.99993	So we came up with this strategy so that we can just define the structure using the explorer ants and then the recurrent connections can be defined after that using the social ants.
1872390	1877870	166	A	1	The fifth characteristic is the regularization of formal basement.
1877950	1899290	167	A	0.99978	Again, we wanted to give the ants in an incentive to bring sparser and also well performing neural networks by just penalizing them if they constructed denser or bigger structures.
1900370	1907920	168	A	0.99968	So we added this regularization term and formula that updated the hormone value.
1909890	1913998	169	A	1	And as you see, the regularization term relies on the performance.
1914094	1920770	170	A	1	The data here is the performance of the neural network and it also relies on the size of the structure.
1926070	1934440	171	A	0.74	The last, the 6th and last one is jumping ants which is we want to experiment with.
1934810	1957710	172	A	0.95056	If we let the ants if we let the ants jump over the layers when they move through the superstructure if we let them jump over these layers to construct the neural networks compared to if we restrict their movement to jump one layer at a time.
1957860	1967310	173	A	0.99998	How would this end up performance wise if they will give us parser and well performing structures?
1967390	1978018	174	A	0.72966	Or this jumping will hurt the performance by giving us weaker structures, weaker neural networks.
1978194	1978920	175	A	0.9987	So.
1980810	1987170	176	A	0.99994	We use a timespheres data that belong to Coal Fire Power plant.
1987330	2004986	177	A	1	We divide the data to have 7200 records for training and testing and here the plot shows that the data and we can see that it's nonlinear and it's acyclic and non seasonal.
2005018	2014430	178	A	0.99996	So it's a hard problem for a non neural network solution or a regression linear regression solution.
2014590	2018638	179	A	0.99964	So the input consists of twelve parameters.
2018814	2034870	180	A	0.99527	When we were trying to predict only one parameter the flame intensity experiments covered all of the heuristics of ants giving different values for these different parameters.
2036650	2039798	181	A	0.58	The superstructures consist of twelve input nodes.
2039814	2044810	182	A	1	Three hidden layers each have twelve nodes and one output node in the output layer.
2045150	2051130	183	A	0.92	The recurrent connection span over three times steps one, two and three times steps.
2051570	2061658	184	A	0.87825	In total, the superstructure had 49 nodes, 924 edges and almost 3.5 thousand recurrent edges.
2061754	2081750	185	A	0.99998	So if you unroll the structure over 72 time steps in back propagation through time will have about 352,000 nodes, about 6.5 million edges and about 26 million recurring edges.
2082970	2088134	186	A	0.9999	In the experiments, we also compared performance of ants using the same data set.
2088332	2091774	187	A	0.79677	We compared it to exam and neat.
2091922	2102966	188	A	0.98112	So Exam is the state of the art in neural texture search which is genetic based method.
2103158	2110510	189	A	0.99973	We also compared it to NEET because it's like a benchmark in the neural revolution and nas realm.
2110930	2126210	190	A	0.99	And also we compared it to fixed structures, unoptimized structures that had one and two and three hidden layers and also different types of memory based cells.
2127990	2134790	191	A	0.96	The experiments covered 1600 experiments to cover all the combinations of the meta heuristics of the heuristics of ants.
2135770	2142970	192	A	0.99301	Each one of these experiments was repeated ten times for statistical analysis.
2147310	2151722	193	A	0.89319	Ants generated 2000 RNNs for each experiment.
2151786	2154350	194	A	0.99991	Each trained for ten epics.
2155490	2163870	195	A	0.99988	In total, ants generated about generated train and evaluated 32 million RNNs.
2164030	2169170	196	A	0.99992	It took a month and 1000 CPUs to finish the experiments.
2170790	2192858	197	A	1	The results that we got showed that of course outperformed the unoptimized structures and it also outperformed Neat and then some of the combinations of ends outperformed Exam.
2192944	2207520	198	A	0.99752	So Exam here is the fourth from the left and we can see that the mean absolute error for some of the versions or the combinations of ant heuristics outperformed Exam.
2209890	2218654	199	A	0.99618	So we tried to look to do some statistical study for the results we got from Ads.
2218782	2226658	200	A	0.99988	So we tried to look at the top performing neural networks coming in our results.
2226754	2243850	201	A	0.99998	So we tried to look at the top ten, 2522 hundred and 55 hundred results and we look at the contribution of these heuristics in these best performing neural networks or structures.
2244910	2252830	202	A	0.99999	We found that these heuristics contributed effectively in most of these results.
2253890	2281590	203	A	0.99862	But the thing that was really intriguing for us or surprise us is that we saw that the recurrent connections disappeared in risk results because the best performing neural networks didn't have that much of recurrent connections.
2284250	2292838	204	A	0.99554	That meant for us that the memory based cells did the job for recurrent information coming from previous time steps.
2292934	2296618	205	A	0.9992	But we wanted to expand on this later.
2296704	2302190	206	A	0.95401	So it's on our list discussing on our future investigations.
2306050	2313390	207	A	0.99147	So this is just a summary for the achievements of Ans based on the results that we got from our experiments.
2316770	2325630	208	A	0.98473	So ANZ was the first meta method to involve the core of ACO and column optimization and recurring neural networks.
2325790	2332150	209	A	0.63948	Nas or neural heuristics to control ants tendency to wander around superstructure.
2332570	2346118	210	A	0.96059	Exploiting recurrent connection proved successful because the regularization component or the regularization heuristics give us better results.
2346214	2356830	211	A	0.98142	Showing here in this table and also the jumping ants here gave us better performance compared to non jumping ants.
2359170	2365466	212	A	0.91	And the realization.
2365578	2376510	213	A	0.9986	Also, the weight sharing strategy also proved effective.
2377110	2382370	214	A	0.999	If we look at it, the results here compared to if we don't apply weight sharing.
2394710	2410700	215	A	0.9879	So autopromyzing strategies are generic so the strategies that we use are generic enough to apply it for any problem or solution, that is end colony optimization based.
2412350	2420730	216	A	1	The Vermont deposition that the method that we came up with is also Novalu, which wasn't introduced in any previous literature.
2421810	2429470	217	A	0.97	And the performance of ants compared to the other benchmark and state of the art in the realm is also remarkable.
2430530	2437106	218	A	0.83772	So going forward, we thought that ant gave us a good result.
2437208	2443700	219	A	0.98932	But the main drawback of Ans was the discrete search space.
2445030	2457590	220	A	0.9791	So ans worked on this massively connected massively connected superstructure, but it's massive gas, but it's still discrete.
2458010	2462170	221	A	0.5128	Ants can move freely between these nodes.
2462910	2473310	222	A	0.51219	They are forced to move over between these nodes, over these predefined connections, whether they are forward edges or recurrent edges.
2474690	2483760	223	A	0.50347	So we thought of removing that continuous discrete search space and replacing it with a continuous search space.
2485430	2497010	224	A	0.99992	So we designed a 3D search space where the search space had like layers representing the lag, the time lags.
2497690	2505826	225	A	1	And then ants can jump over between these layers to give us the recurrent connections.
2505938	2516090	226	A	1	And on each of these layers, ants will just give us the nodes and the edges between the nodes.
2518590	2527690	227	A	0.99866	So in this slide, in the coming slides, we'll show an example of how ants move in cans or continuous ant, or continuous ends.
2527850	2534730	228	A	0.99035	So an agent or an ant will just start by picking up one of the layers that will move on.
2534900	2543662	229	A	0.82461	This is done in a discrete fashion and once this is done, it will then decide if it's going to do an exploitation or exploration movement.
2543806	2548594	230	A	0.99	And in this example, it decided to do an exploration movement.
2548642	2557590	231	A	0.99998	So it will decide the angle and the radius of its next allocation on that layer.
2558570	2569354	232	A	0.69359	Once that's done, it's going to go forward to that location and then decides if it's going to do an exploitation or if the next move will be exploitation or exploration move.
2569472	2573038	233	A	0.77	And this example will be an exploitation move.
2573124	2582958	234	A	0.99991	So it will try to exploit the pheromone traces, the hormone that was previously deposited by other ants in the search space.
2583124	2588962	235	A	0.9995	So it will use its sensing radius, that's something that is previously defined for each ant.
2589096	2596974	236	A	1	And then it will find the center of mass of the hormone traces.
2597102	2609278	237	A	1	And then when it calculates that center of mass of the hormone, it will consider it as its next location and then it will move to that spot.
2609474	2621002	238	A	0.7	And then it will decide about if its next move will be an exploitation exploration move at each location before it's deciding about the type of the step.
2621056	2631470	239	A	0.99976	If it's exploitation exploration, it will also decide if it's going to stay at the same level at the same time lag or jump to a next time lag.
2634310	2656870	240	A	0.98742	If the jump or the movement is on the same level, if the movement is on the same level, if the ants is doing an exploitation movement it will only consider the promontory that is ahead of it because it can't move backwards on the same time lag.
2657850	2664650	241	A	0.94368	Otherwise it will be doing a backwards step which is not allowed in neural networks.
2666510	2677150	242	A	0.99996	But if it's going to another time lag above layer then it's going to do a recurrent edge.
2679890	2687194	243	A	0.9984	Current edges can go back in time sorry, back in the structure in the previous layer.
2687322	2697026	244	A	0.95778	So now the end can consider all the promote traces within radius, the ones that are ahead of it and the ones that are behind it.
2697128	2709850	245	A	0.99977	So in this example he is going to consider in this step is going to consider all the promote traces within its sensing radius, calculate the center of mass and then consider it as its next position.
2709920	2716790	246	A	1	And it keep doing this till it reaches to the proximity of the output node.
2716870	2728510	247	A	0.54	And once this is done, it will decide which output node it will consider as its final position in its path from an input to the output.
2729650	2734160	248	A	1	And then other ends will do the same.
2734710	2740654	249	A	0.87	And then we'll have different paths from some input and some output.
2740782	2752710	250	A	0.98	And then Cans will take these paths and then try to condense the nodes so that we don't have so much nodes that are very close to each other.
2752780	2763606	251	A	0.99944	So the nodes that are within certain proximity will be clustered together using DB scan to have less number of nodes.
2763798	2773754	252	A	0.91	And then the paths will be taken, collected and put in a structure, a neural network structure.
2773882	2781946	253	A	1	And then sent to a worker process to train and test and then compare it to the population, the best performing RNNs.
2781978	2791214	254	A	1	And the process will be almost the same from this point will be almost the same as ants once the structure is constructed.
2791342	2798070	255	A	0.6	And the training and testing process will be the same as the ones that we discussed in ants.
2800810	2809734	256	A	0.99981	So this was also an animation which shows how these tasks are taken by the ants.
2809862	2831978	257	A	0.61108	Look from an input to an output in the 3D space 3D search space so ants, if we look at ants, ants have only eight tunable hyperparameters compared to when comparing this to ants and XM, it's half of the number of hyperparameters in the other methods.
2832154	2840962	258	A	0.99996	These hyperparameters are the number of layers of the search space, the number of time of lags, how many of them we have in search space.
2841096	2869820	259	A	0.99993	Number of agents, number of ants, which is similar to what we have a similar hyperparameter in ants the sensing radius of the agents or the ants the agents probability to create a new node which presents the exploration instead of the exploitation of the parameters or the hormone back traces in the space.
2870510	2909270	260	A	0.49509	Node condensation parameters or factors the variables that represent in the DB scan are considered also hyperparameters in cans for one updating parameter and for one volatility parameter and these are also present in ants and ACO based problems solutions sorry the experiments used three times years different number of parameters and different sizes.
2910890	2923260	261	A	1	The results showed that the results that we got from Cans were very competing with ants and Exam.
2924270	2928694	262	A	0.57268	They weren't necessarily better, but they competed.
2928742	2939518	263	A	0.99867	They weren't the same level, but it also did didn't do very well in one of the database, the data sets.
2939694	2958120	264	A	0.93088	But that was comparing the performance as the mean absolute error, but comparing Kent's results from the sorry.
2966390	2981142	265	A	1	The size of the neural networks that we got from Cairns, we saw that Cain's structures were sparser, so the performance was competing with ants and exam.
2981276	2989820	266	A	0.9999	But the structures compared to ants were much sparser, similar to exam.
2990670	2993818	267	A	0.53	The size of the structures that we're getting from Exam.
2993984	3014578	268	A	0.99998	Remember, exam is genetic based, meaning that we start the optimization process using the minimal structure elements but it was susceptible to local minimum traps and Tense is not susceptible to this problem.
3014744	3022366	269	A	0.99978	But it also gave us smaller structures with performances that were competing with other methods.
3022478	3023830	270	A	0.92943	Exam and ants.
3028750	3044080	271	A	0.96896	So the advantages of Cans is that it has an unbalanced space compared to ants compared the results that came out were good compared to ants and Exam.
3045170	3055438	272	A	0.97	The tunable hyperparameters are half of these in Exam and ants and also it indirectly encoded the neural topology to 3D thrash spaces.
3055534	3083660	273	A	0.99954	So this is one of maybe the contributions of important contributions cans then so far ants and Cans are solutions that applied neural topology architecture search meaning that they did not optimize the synaptic parameters of the neural networks during the optimization process or during the evolution process.
3084030	3091630	274	A	0.994	So we thought of actually making ends capable of doing this as well.
3091700	3104514	275	A	0.79015	So to train the neural network or optimize the synaptic weights, the weights of the structure of the neural networks during the optimization process, the structural optimization process.
3104632	3122550	276	A	0.99966	So we added a fourth dimension to the search space which we embedded the weights of snapdragon parameters in that map, these parameters in that new dimension.
3123850	3144910	277	A	0.99998	We also wanted the ants to be self aware and try to evolve themselves through the evolution process so that they can adapt to the changes or adapt themselves so that they can give us better performance.
3146210	3159886	278	A	0.99971	We want them to change their behavior, their characteristics like the sensing radius they had before, for example, that can be a variable that can change through the iteration thing and evolve through the iterations.
3160078	3173750	279	A	0.99982	Each end can change these characteristics as the evolution goes on on progress based on the performance of the neural networks that they are generating.
3181030	3200390	280	A	0.9908	So the advantage of doing this was that it eliminated the back propagation process which is the most computationally expensive part in the evolution process in the methods that we use so far in exam ends and Cans.
3200890	3208220	281	A	0.99988	So eliminating the back propagation gave us much faster evolution process.
3210350	3231886	282	A	0.55	The results we got I will discuss the graph on the right hand side first which discusses the fitness or the Ms main absolute error of the results we got from back propagation free cans, the four dimension cans compared to the normal cans, the back propagation cans and ants.
3232078	3250390	283	A	0.99962	So the results showed us that again on a particular database that that propagation cans and that propagation free cans did quite a similar job but they were both better than ants on this particular database.
3251050	3260506	284	A	0.99588	But the actually main contribution or the main advantage of applying Cans shows up in the graph on the left hand side.
3260688	3299430	285	A	0.99945	Because we can see that if you compare the results based on the time of the evolution based on the evolution time, we see that it took much less time than the back propagation version of cans and the ants using these different number of ants, Also this graph shows how fast back propagation precans compared to the normal cans.
3302010	3315110	286	A	0.99996	In this figure the curves and the dollar lines shows the time that back propagation free cans and cans took to prepare or generate the neural networks.
3315270	3325850	287	A	0.99997	We can see that back propagation frequency took more time to prepare or generate neural networks compared to back propagation cans or the normal cans.
3325930	3334820	288	A	1	And that's because the fourth dimension also has to evolve the ants or the agents through the iteration, so needs more time to do that.
3335830	3352578	289	A	0.99843	But the other two curves in dashed lines, here are the lines that shows the amount of time it took for the two methods to train and validate the neural networks.
3352754	3360490	290	A	0.99724	So backpropagation free cans doesn't have to train the neural network doesn't do backpropagation.
3361150	3370010	291	A	0.99735	So you can see that the time it took is in an order of magnitude less than the back propagation version of camps.
3370350	3388370	292	A	1	And these sort of lines shows the cumulative time took for both methods and of course it shows that back propagation precances took much much less time than the back propagation precance took much less time than the back propagation cans.
3389670	3403174	293	A	1	The future directions that we are future points that we're concerning for sorry, the points that we're concerning for future work are to turn cans to a complete continuous search space.
3403292	3418858	294	A	0.93273	So as you saw, the search space for Kents is not purely continuous because the time lags are represented by discrete layers.
3419034	3427850	295	A	0.99997	We want to replace this with the continuous dimension continuous layer representing the time lag in RNNs.
3428010	3453282	296	A	1	However, this is a little bit challenging because the time lags has to be known prior to any optimization process because other than that we will be mapping the whole time series as time lags and then picking the time lags from them which is not feasible.
3453426	3460266	297	A	0.99948	So this is the first point we're considering to investigate for in future work.
3460368	3477790	298	A	0.57	The second one is to investigate this finding that we found in the results in ants where the recurrent connections disappeared from the best performing structures.
3478930	3494830	299	A	1	The theory that we have is that the memory based cells replace these connections, give us the information that we need from the pastime steps so that they were more effective, more efficient in doing this compared to the recycle connections.
3494910	3499560	300	A	0.99998	So we have to expand on this and investigate it more.
3500650	3506490	301	A	0.97	The last thing we want to consider also, this is one of the top things that we want.
3506560	3510074	302	A	0.99824	These three points are the top on our list.
3510272	3535822	303	A	1	The third one is to actually consider one of the concepts that was coined in a book by Dr. Deborah Gordon where she mentioned that the living organism in an ant’s world are not the ants themselves, but they are the colonies.
3535886	3545762	305	A	0.97	The colonies are the organisms that they start, and they grow, and they interact with the environment and the ecosystem.
3545826	3549830	306	A	0.99995	They interact with other colonies, and they die at some point.
3549980	3553106	307	A	1	And the ants themselves are not organisms.
3553138	3556830	308	A	0.99955	They are the cells of these organisms, this colony organisms.
3556930	3569920	309	A	0.99976	So we want to take that concept and apply it in our methods to have number of colonies living together in parallel, evolving and communicating with each other.
3570770	3581730	310	A	0.94	And we want to see if this would give us a better performance, because, after all, we're trying to mimic nature in the solutions we're trying to investigate.
3584470	3590580	311	A	0.99374	So with this, I'm done with my presentation, and if you have any questions.
3595760	3596428	312	B	0.96257	All right.
3596514	3597004	313	B	0.9999	Awesome.
3597122	3599980	314	B	0.99442	Wow, what a great presentation.
3600320	3603336	315	B	0.98373	How about Travis and Alexander?
3603368	3609970	316	B	0.99854	Either of you which want to go first, please feel free to give an introduction and any primary remarks that you have on that.
3616660	3617120	317	D	0.69812	Okay.
3617190	3618192	318	D	1	I can say something.
3618246	3619504	319	D	1	I don't have too much to add.
3619542	3625412	320	D	1	I think Abdul Rahman did a pretty good job going over the core bits of the work.
3625546	3627024	321	D	0.92096	I'm Alex Ororbia.
3627152	3638570	322	D	0.99533	I'm an assistant professor in computer science, an affiliate professor in psychology, and affiliate faculty in computational neuroscience at the Rochester Institute of Technology.
3639420	3648810	323	D	0.99	And I work on a lot of stuff, but primarily predictive coding, active inference variational, free energy, a lot of the stuff that's actually of interest to this.
3650540	3662320	324	D	0.90521	Yeah, and this was a particularly interesting project for me because a branch of my own research is working in neurovolutionary methods or even just nature inspired metahuristic optimization.
3662660	3683264	325	D	0.9	And when I got the pleasure of working with Abdul Rahman when he was a PhD student at Know, we talked a lot about the ant colony based optimization approaches, and I encouraged him to look into sort of, like, the origins and also try to understand actually how physical ants behave.
3683312	3685350	326	D	0.99999	So that was always fascinating to me.
3686120	3688996	327	D	1	I don't have too much to add in terms of the technical parts.
3689028	3691028	328	D	1	I think he covered all the core results.
3691124	3693112	329	D	0.99	The only thing I like to think about.
3693166	3703852	330	D	0.97	And I'm actually more fascinated, too, to hear from the active inference institute their interest in the ant colony methods and particularly what was interesting to them.
3703906	3714944	331	D	0.99406	Because thinking about what does ant colony optimization how do you view it from an active inference perspective is, I think, particularly interesting and thinking.
3715062	3719504	332	D	1	And I even had a thought, I don't know if this was a question among the audience's mind.
3719702	3734420	333	D	0.9997	Do the little ant agents or the cant agents that run and described what is there a way to start to view them as a multi agent system that's optimizing some variational free energy quantity?
3735160	3745844	334	D	0.55226	Because a lot of even Carl's work I mean, I collaborate with him, sort of touches into this area of, like, well, what happens with collective intelligence and societal organization?
3745972	3754264	335	D	0.99	And you can kind of look at free energy from these very high level viewpoints all the way down to fine grained cellular activity.
3754312	3765932	336	D	0.95	And so I'm actually more curious to know Daniel and anyone else in the active inferences who can sort of explain their particular interest in ant based optimization and meta heuristic optimization.
3765996	3767024	337	D	0.99987	I'm curious to know that.
3767062	3772192	338	D	0.97178	But there could be some interesting viewpoints, like, what is the free energy?
3772246	3776404	339	D	1	I think the ants themselves are very mean.
3776442	3777936	340	D	0.99999	We have made them more intelligent.
3777968	3785304	341	D	1	I know Abdul Raman and I talked at length about, well, what if we even made them, for example, have, like, a reinforcement learning control system?
3785422	3793688	342	D	0.9	And you could even imagine, well, now, what if the cans themselves engage in a form of active inference themselves?
3793854	3795752	343	D	0.99983	What would that look like for the system?
3795806	3798510	344	D	0.98	And they are optimizing their own free energy.
3798960	3801480	345	D	0.99993	Those are just fun little thought experiments.
3801560	3803692	346	D	0.9998	We have obviously not worked on them.
3803826	3812572	347	D	1	At least Abdul Rahman was never exposed by me to that part of my world and biomimetic intelligence.
3812636	3814092	348	D	0.92787	So those are my comments.
3814156	3825024	349	D	0.5243	I'm not sure if they're particularly helpful, but they're very general, and I'm actually more curious to know from the active inference institute, their interest in it and where does that maybe perhaps intersect?
3825072	3830870	350	D	0.99828	Or this is just like, oh, we know interesting topics and yeah.
3832280	3832804	351	B	0.99991	Thank you.
3832842	3833840	352	B	0.56095	Alexander.
3834000	3837690	353	B	0.9995	Travis, you want to say hello and give any reflections on the talk?
3840220	3841304	354	C	0.65958	Sure, I'll come in.
3841342	3842890	355	C	0.99983	Hopefully you can see me a little bit.
3843820	3844888	356	C	0.89983	My jungle here.
3844974	3846072	357	C	0.9913	Hi, I'm Travis is Sell.
3846126	3848132	358	C	0.98542	I'm an associate professor at RIT.
3848276	3851576	359	C	0.66878	I'm also the graduate program director for a master's in data science.
3851608	3856110	360	C	0.99169	So if any of you are interested in any of this or data science, please shoot me an email.
3857200	3863330	361	C	0.62987	No, I thought this work was really very interesting in a lot of ways while Abdul Rahman was working on it.
3864180	3880100	362	C	1	I think one of the coolest parts about it is a popular neuroevolution algorithm that came after neat is called hyperneet, and it transforms the discrete search space of neural architecture search into a continuous one.
3880250	3883056	363	C	1	And it's shown to be a pretty powerful method.
3883088	3887496	364	C	0.9973	Well, this version of ant colony optimization, the new one, does the same thing.
3887598	3889284	365	C	0.38291	It makes the search space continuous.
3889332	3902844	366	C	0.99948	But what I found was really cool about it was that as opposed to traditional ant colony optimization where you have a graph and you just send the ants along the edges of the graph and you take the best paths and construct a graph after it.
3902962	3913436	367	C	0.61411	Here the ants are actually working like ants in the real world where they'll move a continuous amount of space, not just from point A to point B and actually dropping down pheromones.
3913468	3919010	368	C	1	And it's more like a real simulation of how ants would move around in the real world.
3919700	3923536	369	C	1	And we're getting better results from it than some of the older methods.
3923568	3933190	370	C	0.84299	So I think that really made me kind of happy to see that and thought it made it as very interesting work.
3935900	3936920	371	B	0.96779	That's awesome.
3937070	3942760	372	B	0.98074	Ahmed, want to add anything or I'm happy to give a thought on the answer and ask some questions from the live chat.
3943740	3944776	373	A	0.88331	I'm good.
3944958	3956300	374	A	0.98175	Travis, I think, covered all the things that I wanted to say about, I think just one thing that Alex mentioned that we give higher intelligence to the agent.
3956370	3960444	375	A	0.80659	This is something that we discussed and I'm pretty much open to that.
3960482	3964000	376	A	0.97	I started actually to think about it, but I didn't implement anything yet.
3964150	3973440	377	A	1	However, the last thing that I discussed in the future directions is something that I actually started working on, but I didn't start the experimentations yet.
3973510	3976690	378	A	0.99993	So hopefully we will see something out of that.
3978680	3979430	379	B	0.77511	Awesome.
3979800	3997660	380	B	0.77468	Well, there's a ton of ways to go and isn't that kind of one of the fundamental questions like in an interactive setting, either pure agent stigma g interaction or multi agent, but ultimately mediated through multiple stigma g's with like reading and writing and error correcting code.
3997730	4017660	381	B	0.99956	So in that communication setting, I felt like the work generalizes along multiple dimensions that previously approaches to multi agent just didn't have those kinds of flexibilities like the continuous time feature and several other features.
4017740	4034564	382	B	0.86	And I guess with respect to the ants themselves, I did five summers of field work with ants in the USA Southwest in Arizona and observed a lot of foraging activity.
4034692	4049356	383	B	0.99978	So that problem or that context or setting is really a fun one and a pervasive one across any kind of living system, anything that's going to be active and living.
4049458	4056380	384	B	0.90807	So why did you pursue foraging type algorithms overall?
4056720	4069730	385	B	1	And does this class include the interaction based methods with direct agent contacts that Professor Gordon highlights in the Ant Encounters book?
4070420	4088340	386	A	0.97434	Yeah, I will try to respond to this, but I just want to mention that the first thought about this actually Travis started this idea about using N colon optimization and neural sector search by applying this method in simpler neural networks.
4088420	4096368	387	A	0.51289	Elman and Gordon neural networks required neural networks and I took the leap from there and started working on my thesis, my PhD thesis.
4096564	4098684	388	A	0.99862	So we thought about this idea.
4098722	4126028	389	A	1	I think I mentioned a little bit about why we used an optimization in previous slide, but just repeating that for the audience to make sure that they got we thought about this idea because Nko optimization was applied as a graphic solution, and we thought that why not neural networks since they are investments?
4126124	4129264	390	A	0.96424	Neural networks, they are directed graphs.
4129312	4134708	391	A	0.78495	Neural networks are directed graphs because the flow of information goes from one direction to another direction.
4134724	4138964	392	A	0.85911	So they are directed, but they also graph because they are nodes connected with edges.
4139092	4149050	393	A	0.93	And our ultimate aim or goal was to optimize the structure by removing and adding elements to it so that it gives us better.
4151120	4151724	394	B	0.55376	Awesome.
4151842	4169196	395	B	0.65	And one way that Professor Gordon and others have talked about that bi directional learning relationship between the computer science and the math and the analytical formulations and then the field work and the actual behavior is because the thousands of species of ants.
4169228	4180736	396	B	0.99367	They're working amidst a huge range of ecologies with all these different patterns of regularities, all these different resource distributions and foraging.
4180848	4182500	397	B	0.99115	It's amazing how general it is.
4182570	4194692	398	B	0.99998	Yet it's also just one of the functionalities that need to occur in terms of these even slower processes, like allocation of tissue to faster processes, even of response to alarm pheromone.
4194756	4202412	399	B	0.99994	So it's like this one class of algorithms clearly scales across from few.
4202546	4208888	400	B	0.99619	Ultimately one some of these foraging algorithms are lone foragers they don't leave pheromone trails.
4208904	4227380	401	B	0.99994	So it's like even the idea of leaving a single positive pheromone or leaving but also there's things models can do that ants can't do, like the time travel, pheromone, lossless perception, high dimensional signaling profiles that can't occur just with like finite amounts of molecules.
4228520	4241316	402	B	0.99559	Now they are just persisting on their path as active agents within one generation with a variational free energy at the behavioral scale, then across generations with that evolutionary layer.
4241428	4246696	403	B	1	And the relationships between the neural network implementation and the active inference model.
4246798	4250060	404	B	0.95969	They're kind of like two ways of describing, implementing.
4250880	4251628	405	B	0.9975	Yeah.
4251794	4265692	406	B	0.89412	I'd be curious to hear any of your thoughts on where you see active inference coming into play or how do you see ultimately similarities and differences between neural network based approaches and active inference based approaches.
4265756	4270080	407	B	1	Are they the same complementary, overlapping.
4272760	4273172	408	A	0.44519	Guys?
4273226	4274550	409	A	0.62516	Want to take this one?
4276360	4279190	410	C	0.65	I think this might be more of an Alex question, to be honest.
4280520	4281670	411	A	0.56817	That's the test.
4282840	4286816	412	D	0.99274	Well, yeah, I mean, if you want to keep it on active inference.
4286848	4299820	413	D	0.99966	But I think, Travis, you're going to need to tag in when you want to get into the very specifics of the actual ant colony details because, again, I kind of see the ant colony optimization from a more global point of view.
4299970	4305292	414	D	0.93	I would say so to be mean this particular work.
4305426	4314800	415	D	0.99209	So that way Abdul Rahman is not also completely blindsided by your question, Daniel, even though the name is in the institute itself, this isn't an active inference.
4315140	4342760	416	D	0.99305	You know, again, while there are obviously, as you pointed out, lots of interesting elements like, for example, the fact that the ants when they conduct their exploration along let's just think about the recurrent networks and they're figuring out what nodes and what as Abdul Rahman explained the superstructure, right as they iterate across with their pheromone trails and figure out what nodes I want to recurrently, connect, feed, forward, connect, skip, connect, so on and so forth.
4344060	4344376	417	D	0.9997	You.
4344398	4344744	418	D	0.99994	Would say.
4344782	4345144	419	D	0.99921	Well, okay.
4345182	4352360	420	D	0.99995	What these ants are doing is they're engaging in epistemic foraging, which is a key concept in active inference or idid.
4352440	4359480	421	D	0.99975	So that way Abdulrahman and Travis are not also left behind by a jargon epistemic foraging and active inference.
4359640	4361212	422	D	0.99579	It's a big general framework.
4361276	4365228	423	D	0.99967	It's like a neurobiological process to RL.
4365324	4369504	424	D	0.92	And epistemic just refers to kind of like the uncertainty Travis that you and I work on.
4369542	4374676	425	D	0.97	And the idea is it's saying, well, OK, I want to understand my world.
4374858	4380884	426	D	0.98	And the more that I explore my world, right, there will be things that surprise me less.
4381002	4391256	427	D	0.91058	But if I encounter some information that is really weird when I build a generative world model or a predictive world model, that's very surprising, I should probably explore that.
4391438	4403484	428	D	1	And so, of course, I'm condensing the concept down into sort of like the exploration part of the explore exploit trade off that I know, you know, but that characterizes reinforcement learning.
4403522	4407464	429	D	0.99821	So that's just what we mean when we say epistemic or epistemic foraging.
4407512	4412416	430	D	0.99	And obviously, Abdul Rahman foraging can be likened to what the ants are doing, right?
4412438	4414028	431	D	0.99992	They're exploring their environment.
4414204	4416592	432	D	0.88	And so I guess with that in mind.
4416646	4427264	433	D	0.61191	So that way everyone's sort of on the same page here to get to your question about the differences, about how does this work versus your typical neural based approaches to active inference.
4427392	4433280	434	D	0.5	And I would fall into that category of, oh, I build neural models, biological process models.
4433440	4439096	435	D	0.99998	Those are very much focused, you could say, at the individual level, at least the ones that I am aware of.
4439118	4449644	436	D	0.99999	When you're building, for example, even a back propagation based, partially observable Markov decision process in active inference, that's like a single agent, right?
4449682	4467232	437	D	0.99836	You're trying to build this construct that is trying to balance the epistemic quantity with its instrumental term, which, by the way, another jargon term for you, Abdul Rahman and Travis, that's just like your reward signal or your prior preference or a prior distribution over goal states.
4467366	4472992	438	D	0.99	And so these agents sort of like deal with that trade off, but at an individual agent level.
4473046	4488564	439	D	0.99745	Now, again, I'm sure that there's interpretations of these from other perspectives, the ant based approach, even though I would not argue per se that this has at least an explicit form or a connection, at least that Abdul Rahman has made clear to active inference.
4488612	4493172	440	D	0.99264	But the idea is that this is like a multi agent approach to active inference.
4493236	4516284	441	D	0.92	And so the ants, when they conduct their epistemic foraging, which arguably is a very simple model, each ant and of themselves is essentially a bunch of coefficients and some hard coded rules because their job is essentially to work together with their pheromone trails to figure out oh, what parts of the superstructure are useful.
4516332	4521040	442	D	0.99831	So I'd say that that's different and it lends itself in some ways.
4521110	4527604	443	D	1	Of course, you can make the ants more complicated and lose the benefit of I'm just about to say you could massively paralyze this.
4527642	4531408	444	D	0.99	And this is one of the key strengths that I think is natural.
4531504	4539416	445	D	0.92994	In, for example, a lot of actually nature inspired optimization algorithms and ant colony optimization is one of them.
4539438	4553528	446	D	0.57104	Is Abdul Rahman has been using hundreds of CPUs, and you can put these ants on their own individual processor, and the communication that's occurring across them as they exchange information is through the pheromone trails.
4553544	4557336	447	D	0.99859	There's an indirect mechanism, it's not terribly complex to facilitate.
4557368	4564944	448	D	0.99	And I'm sure that there's even better ways to go about doing asynchronous forms of communication and further, further optimize this.
4564982	4579008	449	D	1	I know Travis and Travis can add to this has done things on like, citizen science and distributed computing through volunteer computing and how you can distribute this through a massive global Asynchronous network.
4579024	4594440	450	D	0.98466	So you can imagine adapting the ant form sorry, the ant colony optimization approach to some distributed, massively distributed version of active inference, where you essentially have to write down that the variational free energy.
4594590	4606200	451	D	0.99	And I'm putting quotes around this because, again, there is no concrete term written in Raman's work because at least we haven't viewed this from the active inference perspective directly.
4606280	4617490	452	D	0.99995	Each ant is optimizing its own variational free energy, but then there's probably a global quantity that sort of is related as a function of those pheromone trails in the individual ant agents.
4618020	4634336	453	D	1	And then of course with the exploitation term or the instrumental or what is the reward signal to give an RL term that's sort of driven by the performance of the actual agent on each candidate agent on the task right of the ram.
4634368	4638212	454	D	0.98	And you compute like mean squared error when you're doing time series prediction.
4638276	4655116	455	D	0.98338	So in some sense we have built in a reward function that we use and again, for those in the active inference group here, you can use the complete class theorem and look at the prior preference of saying, oh, well, the reward is actually technically a prior preference, right?
4655138	4657180	456	D	0.80425	It's like a log probability.
4657520	4662124	457	D	0.99979	So with that in mind, you could squint at ant colony optimization.
4662252	4673380	458	D	1	And I guess the big benefit comes from that massive parallelization that you wouldn't actually very easily, if at all, get with our single agent neural based approaches.
4673720	4678404	459	D	1	And that might be an interesting place to build on and I'll stop rambling at this point.
4678442	4680020	460	D	0.80863	I'm not sure if that was helpful.
4683000	4684184	461	B	0.99942	That was awesome.
4684302	4705820	462	B	0.9997	Even earlier today, Chris Fields in the Physics as information processing course was talking about the classical information inscribed on the blanket, which is like the pheromone perception and deposition, pheromone modification and perception sense making an action which we can associate with the nest mate cognitive system.
4705970	4726896	463	B	0.99934	So then there could be as simple as a pass through for the nest mate, could be any arbitrary relationship described with a blanket, simple nest mate, sophisticated nest mate, like another level of time series modeling, whereas there's the environmental time series modeling and that's just in the ants.
4726928	4740888	464	B	0.99	And then the fourth dimension is like that quantum rotation which goes from the lower dimensional classical stigmagic screen into the quantum informational space.
4741054	4761200	465	B	0.99	And so that's one of the discussions ongoing in Actimf right now is about well, previous approaches to connect quantum formalisms to macro, let's just say neural phenomena based it upon the plausibility of like a molecular electronic bubbling up.
4761350	4783300	466	B	0.93231	Whereas just with research from decision making and statistics and just multiperspectival modeling and all the issues associated with the physicality of information transfer, the finiteness of it, the quantum formalism becomes useful just by itself with or without reliance on some other electronic phenomena.
4783640	4794360	467	B	0.9995	So it's just a lot of very interesting connections like having the degrees of freedom on the blanket which could be noiseless and four bits or it could be noisy with this really specific thing.
4794430	4808268	468	B	0.99918	But in Silico you get to play it from both sides and scale things up and down and do these meta heuristics on top of that arbitrary space could be really simple for learning, or it could be however much.
4808354	4819420	469	B	0.98	And then just like the ant colony algorithm is ultimately federated through embodiment, that property makes it a really useful candidate for biomimicry.
4819500	4833672	470	B	0.99981	So a lot of times when people think about collective behavior, they're thinking about like the flock of birds and the school of fish and those are of course collective systems and collective behavioral and all these kinds of complex systems properties can be studied in that type of system.
4833726	4840010	471	B	0.99993	But it is also neglecting at least an analytical degree of freedom with the stigma g.
4840380	4853448	472	B	0.99992	So that really opens up both the quantum and the classical information or both niche modification and behavioral and cognitive modeling.
4853624	4864364	473	B	0.61256	So just to add on because I think and then also what the node is could even be heterogeneous or unknown or fit in different ways or fix through design processes.
4864412	4884516	474	B	0.99785	So just like the ant colonies are flexible, enabling them to live in all kinds of places, make all these kinds of nested decisions that interact with each other, that flexibility is just like the tip of the iceberg, of what we could even just describe, because there would always be real environments we hadn't yet tried with ant colonies.
4884548	4891208	475	B	0.9999	So we really would never know the full extent of all the repertoire and the dynamics of the ant system.
4891374	4904108	476	B	0.99994	But then we can just abduct into new mathematical statistical distributional frameworks, pull back to different levels of the learning and the meta learning process and just start there.
4904274	4909376	477	B	1	And then almost ironically, or maybe the opposite of that, it.
4909398	4916288	478	B	0.99885	Could be applied to ant colony video data or movement data or foraging activity itself.
4916454	4923132	479	B	0.57986	But it kind of takes inspiration and develops in parallel or in conversation.
4923196	4926100	480	B	0.99997	So it's not like bound to what real ants can do.
4926250	4938644	481	B	0.99993	Or you could constrain it so that there are properties that real ants have, like they can only interact within this certain way or there really are only this many pheromones do model comparison.
4938692	4941252	482	B	0.99867	So it's a lot of degrees of freedom.
4941396	4945156	483	B	1	I feel like you all are opening up with the ant collie modeling.
4945268	4951790	484	B	1	And also one of the challenging pieces of multi agent simulation is kind of like the open endedness with the design space.
4952960	4964716	485	B	0.99215	So then it's very hard for even creative ideas like sometimes to find the right compute resources that obviously are still even needed for what he discovered with the analysis.
4964828	4967644	486	B	0.9735	Here I'll ask a question from Bert.
4967692	4981430	487	D	0.99998	In the to before we move on to that, I just want to clarify just so make sure that I got the right term and certainly Abdul Rahman and Travis might want to look it up is when you were saying blanket, you were referring to a Markov blanket, correct?
4981800	4982356	488	B	0.99972	Yes.
4982458	4991812	489	B	0.99797	So the technical definition of Markov blanket is when you have a Bayesian graph where nodes are the variables and edges are relationships amongst these variables.
4991956	4995304	490	B	0.99993	For any given node of interest, we'll just call it internal states.
4995342	4996584	491	B	0.97829	So these are not features of the world.
4996622	5000056	492	B	0.84529	It's not tagged onto some tissue of a real nest mate in the world.
5000158	5014284	493	B	0.99992	This is something that's tagged onto or like a perspective we could take on any node in a Bayes graph and then all the nodes that insulate it and the co parents are known as the blanket in the sense that it's like one layer insulator.
5014412	5020770	494	B	1	And there's a lot of more discussion on it and the philosophical implications and all these generalizations of that.
5021320	5039850	495	B	0.99527	But broadly, the Markov blanket is just the inbound dependencies which we associate with sense making and perception learning attention; and then the outgoing dependencies for the agent which we associate with action influence in some downstream pointing way.
5040300	5040856	496	D	0.99985	Thank you.
5040878	5047032	497	D	1	I just wanted to clarify that because I don't think Abdul Rahman and Travis might be familiar just with that terminology.
5047096	5051976	498	D	0.96216	It's very know, very active inferenc-y kind of jargon.
5052008	5056188	499	D	0.98805	So I wanted to make sure that they got that from the physics point of thanks.
5056354	5058552	500	B	0.99598	Yeah, totally great point.
5058626	5062364	501	B	0.88	And ask a question now from Bert.
5062412	5064210	502	B	0.97192	So Bert says.
5065060	5066108	503	B	0.99967	Very impressive.
5066204	5070572	504	B	0.5726	Solving generative models with more generative models sounds very promising.
5070716	5073760	505	B	0.99995	What about replacing ants with convolutions?
5077610	5085770	506	A	0.92463	Talking about like a meta learning algorithm, like having a neural network that learns how to optimize other neural networks?
5088290	5094190	507	A	0.99457	Yeah, this concept, I think is introduced at some point in machine learning learn.
5094260	5110290	508	A	0.67929	But we wanted to apply for nature based method that mimics like the Mother Nature, which is… nature is the most efficient optimization evolution system.
5110360	5120470	509	A	0.99088	So looking at the results that are there in nature applying nature based methods they were superior to any other method.
5121210	5132460	510	A	1	And the results we got also which just pointed that out that we saw good performance coming out from these results from our results and the previous results from other methods as well.
5134030	5136140	511	C	0.88086	Hop in here a little bit too.
5137550	5141866	512	C	0.99753	In the case of neural architecture search there's kind of a couple of classes of approaches.
5141978	5151822	513	C	1	One is constructive, so kind of like where you build larger and larger networks and try and keep your network size minimal to try and find your optimal solution.
5151966	5164354	514	C	0.99995	Other types of neural architecture search approaches use like a superstructure and this is kind of how the earlier iterations of this were where you have a bound of your search space and you try and find the optimal network within that bound.
5164402	5171750	515	C	0.9996	So one is like trying to build things from the ground up and the other one is trying to trim down a big network to a small network.
5172750	5188586	516	C	0.51554	As to your question about convolutions, there's been a fair bit of research lately in what's called graph based neural networks which can use convolutions over like a discrete graph search space and can potentially produce other graphs.
5188618	5193070	517	C	1	And I believe there's been some neural architecture search work using this.
5193220	5208770	518	C	0.99987	But one of the main and I think cool things about the approach here, which is different from those is if even if you have a graph based neural network and you have your search base defined as some kind of matrix where things are off and on depending on which nodes are connected to each other.
5208840	5218594	519	C	1	You have a fixed search space which may not be big enough or it might not be the correct search space for this neural architecture search problem where this method here is all continuous.
5218642	5218854	520	C	0.99958	Right?
5218892	5241230	521	C	0.99979	So within this continuous search space it gives the algorithm a lot of freedom, maybe too much freedom but a really open ended way of generating a wide variety of neural architectures which if you preconstrain your algorithm to work within a fixed discrete superstructure you may not even find them because they're not even a possibility.
5242130	5246026	522	C	0.99784	So that's one of the reasons we didn't go that route.
5246058	5261458	523	C	0.99999	But there are graph based neural architecture search algorithms out there where basically you take the architecture as a graph, you train a neural network to spit out another graph that it might think is better and those use convolutions.
5261474	5263990	524	C	0.99867	Sometimes that helps.
5264330	5264982	525	B	0.82786	That's awesome.
5265036	5266854	526	B	0.70794	So convolution sounds like yes.
5266972	5303266	527	B	1	And one thought on that is yes, the ants solve all these incredible patterns and kind of do amazing things amidst informational and physical limitations like we all do having the ants be able to just make trade offs within a task space and then have a dial as modelers to make that task space.
5303368	5313880	528	B	0.99994	Kind of like touching the pheromone distribution or metacognitive ants or something emulating essentially that.
5314490	5323214	529	B	0.72	And for example, the active inference forward looking and thinking through other minds that there could be a kind of cognitive colony.
5323362	5366390	530	B	0.99998	So then that enables in silico total thought, experiment colonies and through data driven processes also kind of keep continuity with that model, perhaps literally continuity with the model and then connect it to empirical, which is something that is very hard for agent based modeling which, as you kind of pointed to, often sets certain fixed axes, performs like asweep looks at one mechanism, doesn't look at all these possible mechanisms of learning and intra and intergenerational and all these time effects.
5366890	5373830	531	B	0.99961	So how do you see this being used in different research or application domains?
5378400	5394592	532	A	0.64382	Well, the main use case that we're thinking of was neural architecture search to apply it for other domains other than neural architecture.
5394736	5396710	533	A	0.9948	We didn't figure out that yet.
5397160	5399990	534	A	1	I think Alex can explain on that.
5400840	5402816	535	C	0.68	I can hop in a little bit mean.
5402858	5420264	536	C	0.89622	So basically this type of algorithm, if you need to generate graphs and you don't necessarily have a fixed structure for that graph and when I say graph, I mean like a computer science graph where you have nodes in a different so pretty much anything involving graph construction.
5420312	5422332	537	C	1	I think these types of methods work.
5422466	5427536	538	C	0.9961	Neural networks kind of under the hood can be represented as graphs and usually are.
5427638	5446230	539	C	0.99941	So I think we're using it for neural networks because it's really popular but there's other algorithms out there like the traditional traveling salesman, there's like routing problems, all that, any type of stuff where you might need to generate a graph in a smart way to do that.
5446920	5465004	540	C	0.94	To your other point though, I think what's really cool here and I don't want to steal Alto Robin's thunder, but his last point on future direction is while a particular version of this ant colony optimization search is running to find an optimal neural network, a colony has fixed parameters that it operates within.
5465202	5477168	541	C	0.99997	But if you think of the colony as an organism as opposed to the ants being an organism, you can evolve colonies that optimize how the ants themselves act.
5477254	5489984	542	C	1	So you can have evolving colonies that in a smaller sense also evolve or optimize what they're doing within their prescripted parameters for the agents they're generating.
5490032	5491780	543	C	0.93364	So you can have like a meta meta.
5494280	5495060	544	B	0.9243	It's awesome.
5495130	5508900	545	B	0.99	I mean the evolutionary account of a why question for ant behavior today, one part of that answer is like because colonies that couldn't under that regularity or constraint survive.
5509060	5511924	546	B	0.98976	We've had a long, long time to wipe those off the table.
5512052	5516140	547	B	1	And so every biological system has to have that kind of multiscale ordering.
5516960	5536460	548	B	1	In 2021 we made the active imper ants paper which was modified from an epistemic foraging visual attention task about scanning around and then learning a cicade policy that had to do with epistemic foraging but not leaving a trace.
5536540	5554856	549	B	1	And then the main modification to bring that active inference epistemic visual foraging model into the active inference ant setting was to add a pheromone rule, just like you described, even though, of course, again, that's not the only pheromone rule, but that's just the most simple pheromone rule that we can generalize from, as you definitely have.
5555038	5561076	550	B	0.98	And there are just many emerging ways of modeling those multiscale active inference models.
5561108	5569656	551	B	0.99271	So composing across layers, which we might associate more with the kind of laterality of things that happen through interactions.
5569768	5592100	552	B	1	And then also, as Mike Levin shows, with kind of the time diamond systems that have a memory retention component of some shape, cognitive shape, and then a protention awareness or agency or other attributes you can use to describe that.
5592250	5597430	553	B	1	And that's a statistically amenable way to describe things.
5598440	5615176	554	B	1	And then there's a variety of implementations on a given statistical problem, or like Federated Compute architecture, it might be the case that you're not running the pure matrix multiplications that are shown in the early MATLAB code of active inference.
5615288	5624584	555	B	0.99999	Different components of machine learning systems might be kind of composed together, also ways kind of abiding by those patterns of communication.
5624632	5633408	556	B	0.99466	But then there's a level of abstraction that we can still describe, but it doesn't mean active inference is going to be kind of causing it.
5633574	5635520	557	B	0.98413	So that's what gives a lot of flexibility.
5635860	5654040	558	B	1	And it's really cool that through your background, Alexander and work and these kinds of collaborations that like the active inference perspective on multi agent modeling with all these other views can at least come together comparatively.
5654540	5669144	559	B	1	And then that is going to, I think, be quite an interesting interchange to apply this entire tissue type or colony type thinking above and within the models.
5669192	5672510	560	B	0.99991	Just a lot of degrees of freedom, like you said, could be too.
5673440	5680108	561	D	0.97	And I think there's different ways too, I think depends on how you want to take the ant metaphor.
5680204	5691684	562	D	0.92	And again, it's kind of interesting, some of the questions or comments that you're making, Daniel, and some from the audience about how does this think about it from a cognitive point of view.
5691722	5703940	563	D	0.94	I mean, I do work in cognitive architectures, of course, again, kind of from the whole single agent or single entity and modeling a single brain in its different regions.
5704020	5731616	564	D	0.98472	But I think if you take a nature inspired optimization approach like the ant metaphor that Abdel Rahman latched onto, and that's sort of like the way in which he formalizes how takes a principle of how ants interact with their world, interact with each other, and then mathematically model those particular concepts step by step.
5731798	5741140	565	D	0.89	And I think if you bend the metaphor and say, well, okay, could the ant colony metaphor apply to multi human agent systems, right, or other entities?
5741960	5751856	566	D	0.84954	Does the ant necessarily can it be generalized beyond the physical creature upon which Abdul Rahman based his initial metaphor?
5751888	5754968	567	D	0.99	And that's an interesting philosophical kind of take to it.
5755054	5760008	568	D	0.98	And then how do you apply that to, let's say, building a multi agent cognitive system.
5760094	5777452	569	D	1	And then, of course, as Travis was discussing with you and you were mentioning metacognition know, you could think of ant colonies, of ant colonies, but you could even replace the word ant and just say, well, we have clusters of intelligent agents, or whatever degree of modeling we're doing.
5777506	5796836	570	D	0.99064	Because again, I do want to emphasize that at least the cans and ants agents that I have worked with in the context of the Raman, they are not each and of themselves, even, I would argue, if nothing else, a very extremely simplified generative model or a very simple control system.
5797018	5803444	571	D	0.80293	There's no neural network under each one because then you'd have to simulate computationally each one of these ants within the framework.
5803492	5810344	572	D	0.92003	So I think there's always that practical machine learning kind of viewpoint of, well, there's always how do you simulate that?
5810382	5812548	573	D	0.66	And Alderam is working with CPUs.
5812644	5817624	574	D	0.99923	It's not like he has an army of GPUs to replace them with convolutional networks.
5817672	5820076	575	D	0.99997	Again, if you had the resources, this would be awesome.
5820178	5824284	576	D	0.99988	But expense and money is another constraint on this planet.
5824332	5845620	577	D	0.99935	But I think there's interesting views and interesting directions one could go by taking inspiration from the ant metaphor and the concept of pheromones and translate them to other real world signals and how, for example, communication patterns among other animal entities or other human agents.
5845690	5848368	578	D	1	And I think that that opens up an interesting perspective.
5848464	5869836	579	D	0.98	And if you're constantly trying to connect it back to free energy minimization and trying to say, well, how are we balancing the terms that you can decompose it into an epistemic and an instrumental, and how are these balancing out and how are these physical processes that we specify balancing those terms?
5869938	5871804	580	D	0.85023	That's just a very interesting place to be.
5871842	5877216	581	D	0.99	And you mentioned active inference versions of Hans, and that's fascinating and of itself.
5877398	5882396	582	D	0.99999	Last comment I have is, again, the degree of modeling and what you are modeling.
5882508	5886176	583	D	0.99683	Like, if you're modeling a society or organization, that's one way.
5886198	5904808	584	D	0.98303	You could use the ant colony framework, if you will, or metaheuristic optimization frameworks to then cast a system, any type of complex multi agent system, as an active inference kind of engaging process.
5904894	5918712	585	D	0.99999	Or you could go really low level and think about cells in a body or units that make up organs or organelles and trying to say, well, can we use this to model that level of granularity within, like a human or an animal entity?
5918776	5919004	586	C	0.99941	Right?
5919042	5929964	587	D	1	And I think there's some fascinating questions about how does this metaphor manifest itself at different timescales and different degrees of perspective about how you're modeling?
5930012	5930816	588	D	0.99996	What are you looking at?
5930838	5933244	589	D	0.99992	What's the picture that you want to emulate?
5933292	5943524	590	D	0.99	And of course, there's always under the hood, this practical consideration of, well, okay, the computational expense that you allocate, and are you able to actually run that simulation long enough?
5943562	5946916	591	D	0.97335	Because I think Abdul Rahman, you can correct me if I'm wrong.
5947018	5951688	592	D	0.99792	You mentioned one of the experiments I think was for the bigger systems took a month.
5951774	5952072	593	D	0.99571	Right.
5952126	5953800	594	D	1	Of course this was on CPUs.
5954860	5959224	595	D	0.99564	That can get pretty prohibitive if you want to go even bigger than that.
5959262	5964136	596	D	0.52258	But again, I think it just depends on what hardware you have to simulate this on.
5964318	5965050	597	A	0.87704	Yeah.
5965420	5971048	598	A	0.64	And also using high performance computing, it is not always feasible for smaller.
5971144	5980332	599	A	0.97694	So if we try to model the brain of Aims like small neural network, using a GPU might not be feasible solution.
5980396	5992896	600	A	0.69	And Travis is a high performance computing specialist here, expert tell you that sending data to a GPU and getting it back is very time consuming and resources consuming.
5992928	6006484	601	A	0.99043	So it will worsen the time consumption rather than solving it, because communicating between the main memory and the GPU had an overhead.
6006612	6014700	602	A	0.99958	So it has to be a big enough problem to actually utilize this GPU in such solutions.
6016880	6019390	603	C	0.84644	Sorry, go ahead.
6021040	6034384	604	C	0.99993	When you have the super large language models, or large models for computer vision, they do a lot of just massive operations on Tensors, which are basically multidimensional matrices, right?
6034502	6041860	605	C	1	And when you have really big wide Tensors, you can parallelize the operation really nicely across the GPU.
6042680	6052148	606	C	1	A lot of this work is based on doing time series forecasting, time series classification on sensor data, stuff from like power systems.
6052244	6063800	607	C	0.94516	So the input to a large language model might be 1000 or more length of a word embedding, which is actually not huge.
6063870	6071416	608	C	0.98958	But if you go up to a computer vision model, the input image may be 1000 by 1000 pixels and that gives you actually a million inputs.
6071448	6072030	609	C	0.9998	Right.
6072400	6078100	610	C	1	When you're working with sensor systems, off of aircraft, power plants, that type of thing, you may have 50 to 100 inputs.
6078200	6084204	611	C	1	And when you're working with this type of time series data, you don't need a massive super wide neural network.
6084252	6091272	612	C	1	And then if you add in recurrency where you have to do backprop over time and other things like this, you actually can't really parallelize it nicely on a GPU.
6091356	6093824	613	C	0.9908	So for us, the CPU is actually more efficient.
6093872	6100096	614	C	0.99107	We tried Abdul Rahman wrote a bunch of code a long time ago to put this stuff on GPUs and we found it was quite a bit slower.
6100208	6104330	615	C	0.97496	So, depending on what you're doing with a neural network, a GPU actually isn't the right answer.
6105180	6121500	616	C	0.99996	But the other cool thing about this, which I think does have maybe even potential for there, is that one of the big not talked about problems in machine learning is that back propagation is the fastest thing we know, but it's inherently not scalable.
6122080	6129884	617	C	0.99999	You can get a bigger, better GPU to do your bits of your network in parallel to speed things up, but that only gets better if you have a bigger network.
6130012	6137692	618	C	0.99995	If you want to speed up the training process, you can't just add another CPU or another GPU and make back prop go faster.
6137756	6141328	619	C	0.98792	You can make the forward and backward pass through your neural network faster.
6141424	6144176	620	C	0.99996	But you still have to do every epoch of backprop.
6144208	6150644	621	C	0.91979	Iteratively a method like this where we're generating one, it's backprop free.
6150682	6152448	622	C	0.99995	So it's not using backprop.
6152624	6162764	623	C	0.99998	We can use a nature inspired or other method to use hundreds of computers and you can throw twice as many computers at it and get a result twice as back, twice as fast, whereas backprop you can't do that.
6162802	6173324	624	C	0.95432	So if you think about actually being able to train a neural network backprop, actually it's got a pretty low speed limit for what we need to do.
6173442	6181200	625	C	0.99	And it's kind of a big problem with the machine learning community that people don't like to talk about because they're like oh, I'll just buy the next big Nvidia GPU and that'll do things faster.
6182580	6183936	626	B	0.99979	That's super interesting.
6184118	6202120	627	B	0.99997	Does this maybe even bring up a kind of relationship where things like a graphics visualization of course a GPU does well and that's like the screen changing through time with a classical process that can be massively unfolded.
6202860	6208596	628	B	1	And then the cognitive models, ultimately, of the nest mates, which again, can be nested.
6208628	6220092	629	B	0.99983	But the thing that's more quantum, more cognitive model, like you can do in parallel, because the minds are not influencing each other except through stigma g.
6220226	6223820	630	B	0.99959	So then that is CPU bound, the size of the colony.
6224820	6230924	631	B	0.77	And then you could use different graphics techniques, like there are colonies, organisms.
6230972	6235376	632	B	0.99064	So one ant being or it's a philosophical question.
6235478	6238528	633	B	0.99993	What is the scale at which A exists?
6238624	6242964	634	B	0.99915	But all throughout California with the Argentine ant, for example.
6243082	6250920	635	B	1	And so how do we deal with those kinds of meshwork cognitive systems all the way on through 50 in an Acorn?
6251340	6275088	636	B	0.97937	There's just all these different trade offs that are being made and like in the featureless deserts there's different wayfinding pathfinding sensor integration, polarization of light, like different cognitive strategies because they might be going out long distance and dragging something home, not leaving any pheromone because it's not any more likely to have food there.
6275174	6279840	637	B	0.9999	So in that case, the stigma g is basically minimal to essentially none.
6280180	6292500	638	B	1	And then in other situations you could have something that's very adherent to distributions to the point of being fit to a very kind of normative path.
6293160	6310900	639	B	0.99998	But that's happening at a level that allows the different compute architectures, different information architectures and ultimately different biological embodiments to really engage fruitfully.
6311060	6321544	640	B	0.99985	Again looking at the variability, the diversity of biological algorithms for collective behavior which have been studied by Professor Gordon and others in so many different angles.
6321672	6332572	641	B	0.9985	Yet sometimes it can feel like multi agent models always start kind of like at square one, demonstrate some proof of concept phenomena, and then that is utilized as part of a bigger perspective.
6332716	6339744	642	B	0.99992	But it's not like that model was ever claimed to have been tuned to maximum performance.
6339792	6342100	643	B	0.64391	It's like well, we got decision making behavior.
6342600	6351476	644	B	0.99998	You could transfer this to group decision making with Honeybee decision making or something like that, but there's still a big gap there.
6351658	6360532	645	B	0.6017	But I think what you're describing with the Kant, which is funny because it could be cannot, but also the Kant is the dialect which is spoken.
6360676	6363070	646	C	0.96693	It was very funny when we came up with it.
6364400	6365192	647	B	0.99987	Great choice.
6365256	6375936	648	B	1	And it's just like yeah, because there's multiple perspectives to swap from on the classical screen because the meaning of the word is something that's happening.
6375958	6385632	649	B	0.63522	That fourth dimension, cognitively, the meaning of the word isn't to be found just on the blanket, just on the interface itself.
6385686	6387040	650	B	0.96716	That's just the communication.
6388180	6389828	651	B	1	And that's like a bounded system.
6389914	6412656	652	B	0.99812	Then if you model a cognitive system that doesn't have that kind of a constraint, so represented by a map that has some kind of blanket index, some kind of blanketing if you don't embody that constraint in the statistical model, the map, you're ignoring one of the fundamental constraints of modeling the way that things happen in an embodied fashion.
6412788	6417896	653	B	0.99999	Maybe there's some abstract space for a certain problem that's just like a total slam dunk.
6418088	6438716	654	B	1	However, for full generality, at least to the space that we know of, biological life forms and their engagements and ecological engagements, not just like within one behavior, that space is so vast and there's so much to learn across different systems within two.
6438838	6448230	655	B	0.99826	Again, to abduce away into different information architectures and active inference being some subset or type of those.
6449260	6450760	656	B	0.96596	So it's awesome work.
6450830	6452920	657	B	0.9997	Do you have any last comments?
6457970	6470370	658	A	0.57823	Well, giving ability for brands to be self aware and aware about its environment is actually something that we implemented in our last work with the BP free camps.
6471030	6498026	659	A	0.9999	They are indirectly aware about their environment and they are adapting to the changes in their clinical environment by indirectly meaning that they are evolving using a genetic based algorithm to just change their behavior, like how they sense the hormones when they take the steps and some other times or some other characteristics they have.
6498128	6507994	660	A	0.99996	So they are adapting, but kind of like not in an intelligent way, but through evolution.
6508042	6514738	661	A	0.92634	If you want to say, actually, we considered putting a brain in each one of these agents.
6514824	6522910	662	A	0.9991	But then again, as Travis and Alex mentioned, we found that it won't be practical.
6523070	6534790	663	A	0.99771	Actually, it would hinder our Asynchronous design because we couldn't prolize that it will take time to train each one of these brains as we evolve the neural networks.
6538560	6539310	664	B	0.99891	Awesome.
6541140	6543760	665	B	0.89216	Alexander or Travis, any last thoughts?
6552670	6557950	666	C	0.607	Okay, I'm just really happy.
6558020	6563290	667	C	0.86	I mean, I think this work is really interesting and it opens up a lot of pretty cool avenues.
6563370	6572738	668	C	0.99886	Again, if we can get to the point where we're evolving colonies that are producing ants and can see where that can go.
6572824	6580500	669	C	0.99888	So one of the big issues in neural architecture search is the whole question of what is an optimal neural network and what an optimal neural network is.
6580950	6584514	670	C	0.99972	Could be different for what will be different for different tasks.
6584562	6592918	671	C	0.99999	But not only that, even if it's the same data set, how you're using that neural network could lead to a lesser, more optimal neural network, right?
6593004	6596694	672	C	0.99993	Depending on what you're doing with it, maybe you need one that's more energy efficient.
6596822	6603174	673	C	1	Maybe you don't care about energy efficiency or performance, and you'll take a slower neural network, but you need more accuracy.
6603302	6612382	674	C	0.99976	So being able to have algorithms which can automate this whole process for us and tune them to what we actually want to use the neural network for is really important.
6612516	6618046	675	C	1	And I think, one, just having ant colony optimization be able to optimize a network for a problem is great.
6618068	6629454	676	C	1	But two, if we can make it such that the algorithm itself is self optimizing, it really can streamline this whole process where right now, if you're doing machine learning, it can be kind of miserable.
6629502	6633222	677	C	0.99978	You make a neural network architecture, you try it out, see how well it does.
6633276	6635174	678	C	1	Oh no, that didn't do so well.
6635212	6638840	679	C	0.99998	Let me tweak a couple of knobs automating that process.
6639450	6643926	680	C	1	My whole life as a computer scientist is about being lazy, but being smart about it.
6643948	6651450	681	C	0.71043	So whatever I can optimize so that I don't have to do it over and over again seems like a good use of my time.
6651520	6655120	682	C	0.98324	So I'll be smart about having to do as little as possible in the future.
6659410	6661214	683	D	1	I don't have too much to add to that.
6661252	6676210	684	D	1	I think a lot of the good discussion has happened already, and we talked about various implications and ways of viewing ant colony optimization from other perspectives, including an active inference point of view.
6676360	6693042	685	D	0.99997	So I guess really more from a closing thought on my end, is that it will be interesting, or it is an interesting direction to think about, like I said earlier or suggested, about the adaptation of the metaphor to other systems.
6693106	6700218	686	D	1	And what are you trying to model and what's your goals from a scientific and philosophical point of view?
6700384	6701914	687	D	0.89858	What are questions you seek to answer?
6701952	6711498	688	D	0.96	And I think it might be very interesting, again, given other developments and computing technology and ways in which you implement.
6711674	6721134	689	D	1	The parallelization that, I think, is that's what attracted me the most to a lot of these meta heuristic algorithms, even things like particle swarm.
6721182	6730102	690	D	0.83	And when I worked with Travis many years ago on the exam algorithms, you saw our names on that and working on that type of stuff.
6730156	6751658	691	D	1	The part that always caught my attention is, again, that ability to say, I can put these entities on different processing, computing, processing resources or devices, and then they will interact and exchange their results in some way to try to optimize some often complex multi objective cost function.
6751744	6772446	692	D	1	And so I think the part that we'll see or that would encourage the wider spread adoption of even like meta heuristic algorithms in general, not to say that they aren't used a lot in, for example, the engineering domains is again the development of parallel computing processing systems and I think exploiting things like asynchronous computing.
6772478	6775758	693	D	0.99481	That was again another angle that caught my attention from Travis.
6775854	6781622	694	D	0.99977	He's done a lot of work on genetic optimization and neurovolution from an asynchronous point of view.
6781676	6789154	695	D	1	And how can we allocate, whatever resources are available and distributing them across global networks?
6789202	6794842	696	D	1	I think that might be the best shot to scaling up, let's say, with what we got right now.
6794896	6798460	697	D	0.99781	There might be again, you've mentioned, Daniel, quantum technology.
6798990	6811038	698	D	0.99987	Quantum computing is another interesting place that sort of like changes the barring technologies that we don't necessarily have exactly at their best at this moment.
6811204	6830386	699	D	1	How can we take advantage of citizen science or distributed computing or peer to peer type of communication and building massive active inference systems that embody like the multi agent metaphor of ant colony optimization or other nature inspired frameworks?
6830418	6838294	700	D	0.95	And can this system evolve over very long spans of time, just like evolution really worked?
6838332	6852794	701	D	0.96824	Another piece, my final comp and is that why I'm interested sometimes in evolution is that to me it is the inductive bias that provided us with structures that allow, for example, a human agent.
6852912	6854894	702	D	0.98401	Babies can to operate already.
6854932	6857022	703	D	0.99937	Babies can already recognize faces, right?
6857076	6863774	704	D	0.88	And we have certain instinctual reactions and certain mechanisms that evolution has endowed us with.
6863812	6872158	705	D	0.99	And so a fascinating question is what is the interplay of the idea of simulating an artificial form of evolution?
6872254	6877246	706	D	0.99881	Maybe building DNA structures or very simplified computational structures?
6877278	6888242	707	D	0.99995	Which would answer Abdel Rahman's concern about, well, maybe we don't want the agents to be too smart in of themselves because I can't really simulate that unless you give me like a decade to run the simulator.
6888386	6894534	708	D	0.99016	But you could maybe come up with a more fundamental primitive and then use that as a starting point for your neural network.
6894582	6903002	709	D	0.99932	Let's say, Daniel, you want to do some task in image segmentation and like okay, but what can your evolutionary framework give me?
6903056	6905566	710	D	0.82627	Well, I'll say here here's a template to start from.
6905668	6911114	711	D	0.99998	This is a kernel on which you build your framework and it's like a DNA structure.
6911242	6917070	712	D	1	And this has evolved across many years of distributed peer to peer computing.
6917150	6929246	713	D	0.6	And you could imagine building this mammoth evolving, continual learning style evolutionary algorithm, whether it is based on genetic algorithms or ant colony.
6929278	6932262	714	D	0.99	And you could imagine that might be an interesting way to think about.
6932316	6955558	715	D	0.98	And by the way, I am spitballing and generating an idea of how I could envision a scalable form without inventing a new computing system that I don't know will or will not exist because quantum has a lot of problems still to solve too, like superconducting or super temperatures or trapping photons, as I have learned.
6955734	6957782	716	D	0.99987	So that might be an interesting direction.
6957846	6963230	717	D	0.55	I think the scaling of this, especially from the practical end, is going to be the most important.
6963300	6963726	718	C	0.99562	We're going to.
6963748	6968206	719	D	0.99998	Need to pull together all the tools that we have, as I mentioned before.
6968308	6972960	720	D	0.97309	So I'll stop there, too, because I'll let Abraham blame more, so hopefully that made sense.
6973970	6979702	721	B	0.99803	Well, this was very epic and inspiring, so good luck with the work.
6979756	6991474	722	B	0.99474	You're all welcome to suggest another piece that we might focus on or continue the discussion however you see fit, because it's super interesting direction.
6991602	6992790	723	B	0.99207	So thank you.
6992860	6993942	724	B	0.72023	Till next time.
6994076	6994822	725	D	0.87295	Thank you.
6994956	6996120	726	C	0.99247	Thank you so much.
6996490	6996770	727	A	0.90331	Bye.

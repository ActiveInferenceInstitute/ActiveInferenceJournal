start	end	speaker	sentiment	confidence	text
170	720	A	0.546256959438324	You.
6850	7854	B	0.8720964789390564	Hello and welcome.
7972	10462	B	0.8687039017677307	This is active guest room 52.1.
10516	17070	B	0.9416112303733826	On August 10, 2023, we have Ahmed Elsaid, Alexander Arorbia and Travis Desel.
17410	23562	B	0.894409716129303	Ahmed is going to give a presentation and following we will have some reflections and discussions.
23626	28150	B	0.9746382236480713	So thank you all for joining and Ahmed to you for the presentation.
30490	33560	A	0.9767558574676514	Okay, thanks so much for having me.
34170	45180	A	0.7176217436790466	Today I'm going to discuss the methods that we came up to solve new art, texture, search and neuralvolution problems.
46110	51470	A	0.8020115494728088	The methods that are ant colony optimization based solutions.
51970	60030	A	0.8906620740890503	So as the title here is mentioning, ant colony optimization for neural text research and neurovolution.
61010	68094	A	0.6254230737686157	This work is collaboration between me, my previous advisor, Dr.
68212	72530	A	0.8839370012283325	Travis Desel, and my co advisor Dr.
72680	74210	A	0.7187786102294922	Alexander Auroravian.
75370	84230	A	0.9480363726615906	I'm currently an assistant professor in University of North Carolina, wilmington.
86570	89110	A	0.7428593635559082	And moving on to the next slide.
89610	102462	A	0.8723122477531433	So as an overview, the things that I'm going to discuss today, I'll try to give Bird Eyes view for what is neurovolution, why we what, why we need it.
102516	115538	A	0.8794140815734863	And from there, I'll just try to discuss our methods that is based on end calling optimization which is called N based neurotropology search or ants for short.
115704	135062	A	0.8048208355903625	And after that I will discuss how we advanced with this idea by introducing continuous ants or cans for short, which could spread off the discrete search space and replace it with a continuous search space.
135196	145174	A	0.8969705700874329	And after that I'm going to also say what we did with the three dimensions and the continuous ants to have back propagation free cans.
145302	152240	A	0.8567488789558411	And later I will discuss three of the points that we are considering for future work.
153970	167406	A	0.6029775738716125	So in the machine learning learn, as the neural network structures got deeper and deeper, people were trying to optimize the structures to have better performance.
167518	184774	A	0.8186290264129639	And people in different realms or different problem domain used to borrow the best performing architectures or structures and try to modify a little bit to work for their problem.
184892	193614	A	0.8760941624641418	And they try to tweak some of the features of the structure and compare these different tweaks.
193682	196726	A	0.6389060020446777	And then they say that we found the best performing structure.
196838	216500	A	0.7062495350837708	But to actually find an absolute optimum structure concerning that solution, it would be an NP hard problem because to reach that solution, they had to try all the combinations of the different structural elements, right?
218310	242454	A	0.5227691531181335	Because we have massive structures in these deep neural networks, it could be an NPR problem because we don't have computational power or enough time to actually train and test all these structures, all these structures constructed from these different combinations of structure elements.
242582	259422	A	0.601681649684906	So the alternative way to do this is to actually try to apply a meteoristic method to convert to a near to upper solution that is much better than not optimizing the structure or relying on kind of like a random search, right?
259476	270610	A	0.5402832627296448	A random search can get us better performing neural network, but it's not going to give us a near to optimal solution or to converge to a near to optimal solution.
274630	287320	A	0.6406749486923218	So a minoristic method would give us an automated method and also would converge to optimum structure new to optimum solution to this structure problem.
289950	299194	A	0.831215500831604	So the way that people approached Nas was by trying to mimic how optimization is done in nature, right?
299312	319090	A	0.8647468090057373	So the first way they thought of it was trying to mimic how living organisms evolve in nature using genetic based algorithm like the Darwinian Genetic Evolution.
320710	350478	A	0.84830641746521	It started with Neat, it's a short for Neuralution of Augmenting topologies and it relied on genetic algorithms where also that concept also applied in most of the Nas and Neural evolution methods and Exam is one of them.
350564	361310	A	0.7406415939331055	Travis came up with this method and it became one of the state of the art methods in Nas.
362450	379730	A	0.9078728556632996	So in such methods we tried to again mimic genetic evolution by introducing new structural elements or removing structural elements or altering the structure through the evolution process, through the evolution iterations and through the evolution generations.
379890	398794	A	0.8087157607078552	So we can apply mutations by splitting edges or adding edges or disabling edges, and we disable edges or disable some structure elements so that we don't lose that component, so that we can later on use it.
398912	402782	A	0.720120906829834	Kind of like a dormant gene in a genome, right?
402836	405322	A	0.7978723645210266	So that it can appear in later generations.
405386	406382	A	0.505843997001648	Not to get rid of.
406436	407194	A	0.6381548643112183	Totally.
407322	412806	A	0.7966686487197876	So in mutations we can disable edges, we can enable them in later generations.
412858	424450	A	0.8074617385864258	If we found that we want to try this option, we can also add recurrent edges or remove or enable or disable recurrent edges.
425190	445610	A	0.8827223181724548	We can split nodes, we can take some of the nodes in a previous generation and we can just split it to two nodes and then take the edges connected to that node and try to divide it between the nodes that was generated from the previous node in the previous generation.
447870	452618	A	0.7786903381347656	Also we can add nodes to the structure.
452714	455486	A	0.7553937435150146	So all of these are part of the mutation process.
455588	465006	A	0.7422989010810852	In the genetic process we can also disable a node if we want to try to just get rid of one of the nodes and disabling a node.
465038	472078	A	0.7412267923355103	Or multiple nodes will also disable the edges connected to that node beside mutations.
472254	492534	A	0.638709306716919	The other side of a genetic process is to do crossovers where we have two of the best performance population meet together to bring an offspring and the offspring will have collection of the characteristics coming from their parents.
492662	506110	A	0.593706488609314	So it will take some of the characteristics from that parent, some other characteristics from the other parent, hoping that this would give us a better performing neural network or better performing generation.
508450	536502	A	0.7584419846534729	So the main problem sorry, so the main problem with genetic based algorithms is that they start with minimal structures like we can see there, meaning that inputs and outputs and starting from the optimization with this minimal search space can trap the method in a local minima through the optimization process.
536636	550926	A	0.6750718951225281	So we were thinking how to get rid of this obstacle by having a bigger or larger search space to start with, and then we can sample some solutions from that large surf space.
551108	562254	A	0.8684030175209045	And we were looking around and we concerned end calling optimization, and I will say why, but I'll try to introduce the method first.
562292	569986	A	0.6175010204315186	So, the method was first introduced as graphic optimization method, graph optimization method, sorry.
570168	573570	A	0.9133661985397339	It was introduced in mid 90s by Marco DeRego.
573910	580838	A	0.8589833378791809	Marco DeRego applied this method on a travel salesman's problem.
580924	589962	A	0.6164606809616089	And the problem is mainly about a travel salesman who wants to visit a number of cities in a country using the shortest staff and considering this problem.
590016	600810	A	0.855839192867279	If we have different number, if the number of cities grows, then the permutations of these numbers of these cities that we have to consider to find the optimal solution.
601310	619422	A	0.6089651584625244	If this number of cities grows, then we will end up having an NPR problem because we won't have enough time or computational power to have this exclusive solution done or this exclusive search done.
619556	651610	A	0.7999641299247742	So, from his observations to answer in nature, he found that he can apply how they forage to find food in nature and then take this concept, this observation, and apply it in an algorithm to find the optimum path that leads from one point and to visit all the cities in the shortest path.
653790	675686	A	0.8947140574455261	So this observation, this slide and the coming slides will just try to give us a picture of how hence forage in nature to find food and then how Marcus Rigor took that concept to apply it in the travel systems problem.
675788	688440	A	0.7723203897476196	So, ants observers found that ants go out from their nest to find food and they try different directions, right?
689130	701690	A	0.8874495625495911	And when they find food, eventually find food, they will take some of that food and then they will go back to their nest and in the way back, they will deposit some other substance, cholophone.
704050	709930	A	0.9069641828536987	So they deposit that substance so that they communicate that path to the food resource with the other ants.
710090	729110	A	0.7426449060440063	So actually, other ants do exploit this other substance, and when they sense it, they follow the path that the first ant took from the food resource to the nest, hoping that they will find food at the end of that path.
729930	735734	A	0.9121420979499817	When they actually find food at the end of the path, they will take some of the food and do the same thing.
735772	744006	A	0.5178900361061096	They will deposit some morpher mole on the same path, making it more appealing to other ants to take it, so that they can bring more food to the nest.
744198	750566	A	0.5761698484420776	So this process shows us the exploitation behavior of ants.
750678	773490	A	0.6431959271430969	But again, from time to time, ants also try to exploit some other food resources, potential food resources, and they kind of resist following the hormone traces, and they try to go away and find some new food resources for the nest.
774310	798700	A	0.7952151894569397	So the ants are not only exploiters, they're also explorers and these two concepts were used by Marco Derago to kind of like balance the search for the better or faster path between the cities for the travel sales management problem.
801710	811418	A	0.8893467783927917	The third thing that we observed also in how I spoke in nature is that the older substance, the pheromone, also evaporates.
811514	829414	A	0.6217780113220215	So whenever a path to a food resource is not appealing anymore or the food resources are excluded, no more ants will take that path or when they take it and reach their food resources that is exposed, they will not take the same path to the nest again.
829452	833026	A	0.6739838123321533	They will try to wonder and to find more new food resources.
833138	846780	A	0.5875450372695923	And because they not going that path again and not depositing any pheromone on that path, the pheromone will eventually evaporate and disappears and making it less and less appearing for other ants to take.
848510	858160	A	0.7417957186698914	So that's what Marco DeRego was looking at when he thought of the travel salesman's problem.
859890	871758	A	0.8511437177658081	He applied that for the travel salesman's problem by making one agent try these different paths and then comparing through each iteration.
871854	876486	A	0.903652012348175	So that agent will take a path between the cities, right?
876588	890502	A	0.8868989944458008	And then it will compare the length of that path to the previous experience with other paths and if it's shorter, it will try to deposit hormones on the segments of that path.
890646	894758	A	0.5740200877189636	And eventually he was hoping, and he was right about what he was hoping.
894854	915170	A	0.6081862449645996	He was hoping that eventually the shorter path, shorter segments that give the ultimate shorter path has more and more hormone deposits making it more and more appealing for the agent to take it through the iterations.
915990	937918	A	0.8437440395355225	So we thought about this concept and we thought that it's very appealing to apply it for an Nas problem because Drago's solution was applied for a graph optimization problem and neural networks are in their sense direction graphs.
938114	953070	A	0.7167341113090515	So we also considered endcon optimization because it's full torrent, decentralized and scalable and it's also easily traceable going back to being decentralized.
955410	968660	A	0.8875174522399902	It made this method a perfect candidate for a pal and high performance computing solution which will eventually accelerate the optimization problem.
970630	971154	A	0.6888490915298462	I think.
971192	988790	A	0.7854864001274109	In the next slide, after the next one, I will discuss how we exploited or use this characteristic of anonym optimization to accelerate the solution that we came up with or the method that we came up with, which is ants and cans.
989310	1006858	A	0.9227740168571472	So this scheme, or the scheme of ants applying ants or ant collectimization in neural architecture search is depicted or illustrated in this flowchart.
1007034	1035990	A	0.8397648334503174	So we start off with a massive search space expressed in superstructure which expresses or embodies a neural network that is massively connected, meaning that each node in that superstructure is connected with the other nodes through edges and recurrent edges, backwards and forward and backward recurrent edges.
1037550	1047814	A	0.8886124491691589	And then we let a number of agents swarm over the structure from an input node to an output node.
1047862	1065582	A	0.9118849039077759	So each one of these agents will pick an input node and then it wanders from that node through the connection recurrent edges and recurrent edges and between the nodes and between the hidden layers till it gets and picks one of the output nodes.
1065646	1076370	A	0.8496828675270081	And then we take all these paths of the different agents, and we put them together to form a structure, neural network structure.
1076450	1086914	A	0.6023218035697937	And we take that structure and train it and test it and then compare its performance to a population of best performing neural networks, best performance structures.
1087042	1109360	A	0.5052764415740967	And if it's better than the worst in the population, then we reward the path that the ants took, the agents took over in the superstructure, reward with hormone so that it makes these paths appealing for later iterations through the evolution process or the optimization process.
1109890	1126340	A	0.6584060788154602	That's if the generated structure is best than the worst in the population, if not, if it actually was worse than the worst in the population, then we discard that structure or that neural network, and we don't reward Any of the path that ants took.
1126650	1140090	A	0.5793678760528564	And also the thermo evaporation will help us get rid of the pheromones that were deposited on the edges that are not giving us better and better structures.
1143710	1153230	A	0.5545777082443237	And again, because the M colony is decentralized we exploited this by having an asynchronous solution or asynchronous evolution.
1154290	1168126	A	0.8107395768165588	We had a main process that took care of generating the new structures and also updating the population the best performing structures and updating the hormone on the superstructure.
1168238	1173934	A	0.9168365597724915	So the main process will generate structures and send them to worker processes.
1173982	1186870	A	0.9076529741287231	The worker processes will train and test the neural network on the data that we have for the problem and then send the results back or the fitness of the neural network to the main process.
1186940	1198342	A	0.7630330324172974	And based on that fitness the main process will either discard it or we'll take this fitness and compare it to the best performing in the population.
1198406	1204514	A	0.6152248382568359	If it's better than the worst it will reward the path that ends took on the superstructure by depositing more hormone.
1204582	1224034	A	0.7216748595237732	Or if it's worse than the worst in the population, we'll just start it and it will keep generating new structures and sending them to processes because the training which relies on that propagation is the most computationally expensive part in this process.
1224232	1247370	A	0.5122935175895691	If we have a number of worker processes that can work in parallel to train and evaluate these neural networks, these new structures we can speed up the process, right by training and evaluating different structures at the same time in parallel in an asynchronous evolution scheme.
1251890	1274782	A	0.5168905258178711	This is an animation but it's not working in this version of the slides because we're using a PDF but it's mainly a structure where you'll see edges or connections between these nodes fading or having darker colors based on the pheromone values through the iterations.
1274846	1301870	A	0.9190514087677002	So each frame in this animation is kind of an update for the pheromone value of the edges based on the performance of the version of the neural network that were generated by the agents when they swarmed from the start node taking one of the input nodes in the middle layer and then from there going to one of the hidden layers in this one hidden layer that we have here.
1301940	1303950	A	0.8067400455474854	And from there going to the output.
1307090	1319940	A	0.8918206691741943	So that was the concept of applying ACO and con optimization in and this now I'm going to talk about the actual method that we came up with.
1322070	1327786	A	0.545961856842041	So it's more generic and more powerful neural texture search method.
1327918	1351050	A	0.5226882100105286	More comprehensive if I may say that we opted to apply the methods on recurrent neural networks because they tend to be potentially larger than other neural networks structures because of their recurrent connections.
1351390	1370660	A	0.5096343755722046	So we thought that if we started this problem though the method or the concept applies to any neural network but applying it to recurrent neural networks made it more appealing challenge for measuring the performance of the method that we thought of.
1372470	1379506	A	0.9324923753738403	So this slide in the common slide will discuss the different heuristics of the methods of ants.
1379618	1395274	A	0.7804028391838074	The first heuristic is superstructure itself and as I mentioned before it's a massive search space as massive as possible to be handled with the machine or the hardware that we're working on.
1395472	1406538	A	0.7531836032867432	The superstructure consists of a neural network that is massively connected meaning that every node in the structure is connected to the other nodes via or through edges.
1406714	1409306	A	0.8426589965820312	Forward edges and recurrent edges.
1409418	1412270	A	0.8525116443634033	Backward forward recurrent edges and backward recurrent edges.
1412770	1421390	A	0.8965063095092773	This simple structure that we have here represents one of just the concept of the superstructure that we apply in ants.
1421550	1428306	A	0.899071455001831	Here we have three input nodes, three hidden layers each have three nodes and one output node in the output layer.
1428418	1449658	A	0.8234084844589233	And we are just showing one node connected to the other nodes through edges which are the ones representing green forward recurrent sorry, the edges are the one in gray and then forward recurrent and backward recurrent in the green and in the red.
1449824	1472100	A	0.5722984671592712	And the concept of recurrent edges might be a little bit confusing if we look at it in this example because how can an edge be recurrent coming in and out from the same node, the nodes in the same timestamp but this structure might make it more clear.
1472470	1479142	A	0.6618568301200867	So here we have structure that is pretty much similar to the one we saw before.
1479196	1489446	A	0.9068892002105713	But here we have three input nodes, two hin layers still three hin layers each have three nodes and then alpha layer have two nodes and then we have also three time steps.
1489478	1503790	A	0.9037261009216309	The current time step t zero and the previous time step t minus one, the one before that t minus two edges here are illustrated using the solid black lines.
1506130	1512590	A	0.8839453458786011	Of course these edges are present in the current time step that is going to propagate through the neural network.
1513890	1516930	A	0.7239630222320557	Then the current connections.
1517270	1532370	A	0.9141960144042969	These ones are going to bring information or provide information from the previous time steps, the previous inputs or previous data that was fired to the nodes in the previous time steps.
1532530	1538050	A	0.8831586241722107	And these recurrent edges are depicted here in red and orange.
1538130	1541398	A	0.8735620975494385	The forward ones are depicted in red and orange.
1541494	1545980	A	0.8682552576065063	The reds are coming from T minus one and the oranges are coming from T minus two.
1546990	1554080	A	0.8597107529640198	And the backward current edges are the dot lines in blue and green.
1555170	1557482	A	0.8285020589828491	And we can see that they are going backwards.
1557546	1562830	A	0.544181764125824	So they go backward to backward layers.
1567190	1574318	A	0.8005520105361938	But because they are recurrent, we can do that because they process information, they bring information that already processed.
1574414	1582274	A	0.7334494590759277	So we don't have to worry about propagating information back through time or back through the structure.
1582322	1585640	A	0.8270468711853027	But we can do that if it's coming from previous time step.
1587930	1594780	A	0.8993070125579834	The second heuristic in Ash is the colony weight sharing we wanted to use.
1596190	1614558	A	0.8292268514633179	So instead of Brandon initialized the weights or the snapped weights and generated neural networks, we wanted to use the train weights to initialize the weights of the newly generated neural networks.
1614654	1618530	A	0.549314558506012	We did that by saving these neural networks on the superstructure.
1619510	1625226	A	0.9091095328330994	We used the last equation here to do this update.
1625278	1634546	A	0.9052574634552002	So we balanced between the weights that were saved previously and the weights that are coming from the trained or evaluated neural networks.
1634738	1638294	A	0.8799786567687988	And we also used two strategies to do this update.
1638342	1645690	A	0.9020683765411377	We used a fixed parameter phi or we left phi.
1646270	1647526	A	0.7930688261985779	That was the first strategy.
1647558	1662186	A	0.8815874457359314	The second strategy was to get phi by applying these two equations which relies on the performance of the neural network that was previously generated sorry, the previously generated and trained.
1662218	1669250	A	0.901737630367279	So it relies on the performance of the neural network that was trained and validated or tested.
1669750	1687454	A	0.5555884838104248	So if the performance was good, then we will let the weights of that neural network to contribute more to the initialization of the weights of the newly generated RNS.
1687522	1694460	A	0.7441951036453247	If it's not performing that well, then this equation will not allow it to contribute that much.
1696190	1699254	A	0.7858447432518005	The third meta heuristic is multiple memory cells.
1699302	1722606	A	0.8995774984359741	So at each node, when an agent or an ant reach to a node in the superstructure it will do a local search to pick the type of the neuron or the type of the node from these three different types of memory cells.
1722718	1729622	A	0.7095064520835876	So in the generated RNN, the generated structure, the nodes in the structure is not all the same.
1729756	1737206	A	0.8610818982124329	They will be different based on the local search that the agent will do or the ant will do.
1737228	1742890	A	0.8713706731796265	At each node they reach through their path from the input to an output in the superstructure.
1744990	1748214	A	0.7454009652137756	The fourth mineristic is the multiple ant species.
1748262	1753014	A	0.8334746956825256	So we applied different species or came up with different species.
1753062	1759738	A	0.842864990234375	For the ants, the first species was the ones that will traverse over the edges, only the edges.
1759834	1768850	A	0.8983310461044312	So they will go only forward through the edges of the neural network of the superstructure.
1769190	1774990	A	0.8783383369445801	And these ones are going to define the number of nodes in the generated structure.
1775070	1779122	A	0.808737576007843	Also define the types of the nodes in the superstructure.
1779186	1789366	A	0.9081880450248718	And when they done with their work, then the social or the second species, the social ants, will traverse between these nodes.
1789478	1796486	A	0.880567729473114	But they will use the recurrent edges to move between these nodes.
1796518	1802314	A	0.9216341376304626	So they will create the recurrent edges for the newly generated RNN.
1802442	1808554	A	0.8069653511047363	And we have two different species for these social ants or two different subtypes subspecies.
1808602	1814350	A	0.7963920831680298	One is the forward social ends.
1814770	1819614	A	0.8512707948684692	These ones traverse only over the forward kind of connections.
1819662	1829946	A	0.8692948818206787	They go from the input to the output but only over the forward recurrent edges and then the backward recurrent edges or the backward social ends.
1829998	1836550	A	0.8545066714286804	These ones go from the output to the input and they traverse over the recurrent connections.
1837550	1854190	A	0.8503443002700806	The reason we thought about these different species is that we wanted to control the tendency of the ants to wander around in the superstructure exploiting the convoluted mesh of recurrent connections.
1854850	1857022	A	0.747374951839447	So we wanted to control that.
1857076	1869970	A	0.8559354543685913	So we came up with this strategy so that we can just define the structure using the explorer ants and then the recurrent connections can be defined after that using the social ants.
1872390	1877870	A	0.817182719707489	The fifth characteristic is the regularization of formal basement.
1877950	1899290	A	0.7198060154914856	Again, we wanted to give the ants in an incentive to bring sparser and also well performing neural networks by just penalizing them if they constructed denser or bigger structures.
1900370	1907920	A	0.8910269737243652	So we added this regularization term and formula that updated the hormone value.
1909890	1913998	A	0.8450470566749573	And as you see, the regularization term relies on the performance.
1914094	1920770	A	0.896822452545166	The data here is the performance of the neural network and it also relies on the size of the structure.
1926070	1934440	A	0.871135950088501	The last, the 6th and last one is jumping ants which is we want to experiment with.
1934810	1957710	A	0.886038064956665	If we let the ants if we let the ants jump over the layers when they move through the superstructure if we let them jump over these layers to construct the neural networks compared to if we restrict their movement to jump one layer at a time.
1957860	1967310	A	0.8027111887931824	How would this end up performance wise if they will give us parser and well performing structures?
1967390	1978018	A	0.8699033260345459	Or this jumping will hurt the performance by giving us weaker structures, weaker neural networks.
1978194	1978920	C	0.617027759552002	So.
1980810	1987170	A	0.929847002029419	We use a timespheres data that belong to Coal Fire Power plant.
1987330	2004986	A	0.8502811193466187	We divide the data to have 7200 records for training and testing and here the plot shows that the data and we can see that it's nonlinear and it's acyclic and non seasonal.
2005018	2014430	A	0.4854098856449127	So it's a hard problem for a non neural network solution or a regression linear regression solution.
2014590	2018638	A	0.8684097528457642	So the input consists of twelve parameters.
2018814	2034870	A	0.8702439069747925	When we were trying to predict only one parameter the flame intensity experiments covered all of the heuristics of ants giving different values for these different parameters.
2036650	2039798	A	0.8008044362068176	The superstructures consist of twelve input nodes.
2039814	2044810	A	0.8639433979988098	Three hidden layers each have twelve nodes and one output node in the output layer.
2045150	2051130	A	0.9034406542778015	The recurrent connection span over three times steps one, two and three times steps.
2051570	2061658	A	0.9245557188987732	In total, the superstructure had 49 nodes, 924 edges and almost 3.5 thousand recurrent edges.
2061754	2081750	A	0.8966063857078552	So if you unroll the structure over 72 time steps in back propagation through time will have about 352,000 nodes, about 6.5 million edges and about 26 million recurring edges.
2082970	2088134	A	0.8940668106079102	In the experiments, we also compared performance of ants using the same data set.
2088332	2091774	A	0.671970784664154	We compared it to exam and neat.
2091922	2102966	A	0.7956104874610901	So Exam is the state of the art in neural texture search which is genetic based method.
2103158	2110510	A	0.8118227124214172	We also compared it to NEET because it's like a benchmark in the neural revolution and nas realm.
2110930	2126210	A	0.8554331064224243	And also we compared it to fixed structures, unoptimized structures that had one and two and three hidden layers and also different types of memory based cells.
2127990	2134790	A	0.8880420923233032	The experiments covered 1600 experiments to cover all the combinations of the meta heuristics of the heuristics of ants.
2135770	2142970	A	0.8893318176269531	Each one of these experiments was repeated ten times for statistical analysis.
2147310	2151722	A	0.8358305096626282	Ants generated 2000 RNNs for each experiment.
2151786	2154350	A	0.804469645023346	Each trained for ten epics.
2155490	2163870	A	0.8634838461875916	In total, ants generated about generated train and evaluated 32 million RNNs.
2164030	2169170	A	0.5587982535362244	It took a month and 1000 CPUs to finish the experiments.
2170790	2192858	A	0.8739153146743774	The results that we got showed that of course outperformed the unoptimized structures and it also outperformed Neat and then some of the combinations of ends outperformed Exam.
2192944	2207520	A	0.7358710765838623	So Exam here is the fourth from the left and we can see that the mean absolute error for some of the versions or the combinations of ant heuristics outperformed Exam.
2209890	2218654	A	0.8919849991798401	So we tried to look to do some statistical study for the results we got from Ads.
2218782	2226658	A	0.8757181763648987	So we tried to look at the top performing neural networks coming in our results.
2226754	2243850	A	0.7641650438308716	So we tried to look at the top ten, 2522 hundred and 55 hundred results and we look at the contribution of these heuristics in these best performing neural networks or structures.
2244910	2252830	A	0.8126765489578247	We found that these heuristics contributed effectively in most of these results.
2253890	2281590	A	0.7858776450157166	But the thing that was really intriguing for us or surprise us is that we saw that the recurrent connections disappeared in risk results because the best performing neural networks didn't have that much of recurrent connections.
2284250	2292838	A	0.8076769709587097	That meant for us that the memory based cells did the job for recurrent information coming from previous time steps.
2292934	2296618	A	0.7721750736236572	But we wanted to expand on this later.
2296704	2302190	A	0.8888576030731201	So it's on our list discussing on our future investigations.
2306050	2313390	A	0.6847444772720337	So this is just a summary for the achievements of Ans based on the results that we got from our experiments.
2316770	2325630	A	0.8740511536598206	So ANZ was the first meta method to involve the core of ACO and column optimization and recurring neural networks.
2325790	2332150	A	0.8645021319389343	Nas or neural heuristics to control ants tendency to wander around superstructure.
2332570	2346118	A	0.9151378273963928	Exploiting recurrent connection proved successful because the regularization component or the regularization heuristics give us better results.
2346214	2356830	A	0.8752831816673279	Showing here in this table and also the jumping ants here gave us better performance compared to non jumping ants.
2359170	2365466	A	0.7698291540145874	And the realization.
2365578	2376510	A	0.9046909809112549	Also, the weight sharing strategy also proved effective.
2377110	2382370	A	0.814884603023529	If we look at it, the results here compared to if we don't apply weight sharing.
2394710	2410700	A	0.7390671968460083	So autopromyzing strategies are generic so the strategies that we use are generic enough to apply it for any problem or solution, that is end colony optimization based.
2412350	2420730	A	0.864614725112915	The Vermont deposition that the method that we came up with is also Novalu, which wasn't introduced in any previous literature.
2421810	2429470	A	0.8390992879867554	And the performance of ants compared to the other benchmark and state of the art in the realm is also remarkable.
2430530	2437106	A	0.8611388802528381	So going forward, we thought that ant gave us a good result.
2437208	2443700	A	0.6555768251419067	But the main drawback of Ans was the discrete search space.
2445030	2457590	A	0.7069142460823059	So ans worked on this massively connected massively connected superstructure, but it's massive gas, but it's still discrete.
2458010	2462170	A	0.8477239012718201	Ants can move freely between these nodes.
2462910	2473310	A	0.8490031361579895	They are forced to move over between these nodes, over these predefined connections, whether they are forward edges or recurrent edges.
2474690	2483760	A	0.8511263132095337	So we thought of removing that continuous discrete search space and replacing it with a continuous search space.
2485430	2497010	A	0.7569922208786011	So we designed a 3D search space where the search space had like layers representing the lag, the time lags.
2497690	2505826	A	0.8395982384681702	And then ants can jump over between these layers to give us the recurrent connections.
2505938	2516090	A	0.8585882186889648	And on each of these layers, ants will just give us the nodes and the edges between the nodes.
2518590	2527690	A	0.9024263620376587	So in this slide, in the coming slides, we'll show an example of how ants move in cans or continuous ant, or continuous ends.
2527850	2534730	A	0.8215472102165222	So an agent or an ant will just start by picking up one of the layers that will move on.
2534900	2543662	A	0.9302915334701538	This is done in a discrete fashion and once this is done, it will then decide if it's going to do an exploitation or exploration movement.
2543806	2548594	A	0.8750754594802856	And in this example, it decided to do an exploration movement.
2548642	2557590	A	0.8578673005104065	So it will decide the angle and the radius of its next allocation on that layer.
2558570	2569354	A	0.9295428991317749	Once that's done, it's going to go forward to that location and then decides if it's going to do an exploitation or if the next move will be exploitation or exploration move.
2569472	2573038	A	0.5648880004882812	And this example will be an exploitation move.
2573124	2582958	A	0.8120425939559937	So it will try to exploit the pheromone traces, the hormone that was previously deposited by other ants in the search space.
2583124	2588962	A	0.9160498976707458	So it will use its sensing radius, that's something that is previously defined for each ant.
2589096	2596974	A	0.8840024471282959	And then it will find the center of mass of the hormone traces.
2597102	2609278	A	0.9210212230682373	And then when it calculates that center of mass of the hormone, it will consider it as its next location and then it will move to that spot.
2609474	2621002	A	0.9228237271308899	And then it will decide about if its next move will be an exploitation exploration move at each location before it's deciding about the type of the step.
2621056	2631470	A	0.8619305491447449	If it's exploitation exploration, it will also decide if it's going to stay at the same level at the same time lag or jump to a next time lag.
2634310	2656870	A	0.7066523432731628	If the jump or the movement is on the same level, if the movement is on the same level, if the ants is doing an exploitation movement it will only consider the promontory that is ahead of it because it can't move backwards on the same time lag.
2657850	2664650	A	0.7599750757217407	Otherwise it will be doing a backwards step which is not allowed in neural networks.
2666510	2677150	A	0.8451510071754456	But if it's going to another time lag above layer then it's going to do a recurrent edge.
2679890	2687194	A	0.7762155532836914	Current edges can go back in time sorry, back in the structure in the previous layer.
2687322	2697026	A	0.8638884425163269	So now the end can consider all the promote traces within radius, the ones that are ahead of it and the ones that are behind it.
2697128	2709850	A	0.9219162464141846	So in this example he is going to consider in this step is going to consider all the promote traces within its sensing radius, calculate the center of mass and then consider it as its next position.
2709920	2716790	A	0.8872480988502502	And it keep doing this till it reaches to the proximity of the output node.
2716870	2728510	A	0.9108216166496277	And once this is done, it will decide which output node it will consider as its final position in its path from an input to the output.
2729650	2734160	A	0.6367726922035217	And then other ends will do the same.
2734710	2740654	A	0.829695999622345	And then we'll have different paths from some input and some output.
2740782	2752710	A	0.8166187405586243	And then Cans will take these paths and then try to condense the nodes so that we don't have so much nodes that are very close to each other.
2752780	2763606	A	0.8991863131523132	So the nodes that are within certain proximity will be clustered together using DB scan to have less number of nodes.
2763798	2773754	A	0.8782804608345032	And then the paths will be taken, collected and put in a structure, a neural network structure.
2773882	2781946	A	0.8648847341537476	And then sent to a worker process to train and test and then compare it to the population, the best performing RNNs.
2781978	2791214	A	0.836236834526062	And the process will be almost the same from this point will be almost the same as ants once the structure is constructed.
2791342	2798070	A	0.8882870674133301	And the training and testing process will be the same as the ones that we discussed in ants.
2800810	2809734	A	0.8801261186599731	So this was also an animation which shows how these tasks are taken by the ants.
2809862	2831978	A	0.7705405354499817	Look from an input to an output in the 3D space 3D search space so ants, if we look at ants, ants have only eight tunable hyperparameters compared to when comparing this to ants and XM, it's half of the number of hyperparameters in the other methods.
2832154	2840962	A	0.8117619156837463	These hyperparameters are the number of layers of the search space, the number of time of lags, how many of them we have in search space.
2841096	2869820	A	0.8734037280082703	Number of agents, number of ants, which is similar to what we have a similar hyperparameter in ants the sensing radius of the agents or the ants the agents probability to create a new node which presents the exploration instead of the exploitation of the parameters or the hormone back traces in the space.
2870510	2909270	A	0.7722580432891846	Node condensation parameters or factors the variables that represent in the DB scan are considered also hyperparameters in cans for one updating parameter and for one volatility parameter and these are also present in ants and ACO based problems solutions sorry the experiments used three times years different number of parameters and different sizes.
2910890	2923260	A	0.6851944327354431	The results showed that the results that we got from Cans were very competing with ants and Exam.
2924270	2928694	A	0.7039844393730164	They weren't necessarily better, but they competed.
2928742	2939518	A	0.7763519883155823	They weren't the same level, but it also did didn't do very well in one of the database, the data sets.
2939694	2958120	A	0.7038602232933044	But that was comparing the performance as the mean absolute error, but comparing Kent's results from the sorry.
2966390	2981142	A	0.7154881954193115	The size of the neural networks that we got from Cairns, we saw that Cain's structures were sparser, so the performance was competing with ants and exam.
2981276	2989820	A	0.765378475189209	But the structures compared to ants were much sparser, similar to exam.
2990670	2993818	A	0.769290030002594	The size of the structures that we're getting from Exam.
2993984	3014578	A	0.7379426956176758	Remember, exam is genetic based, meaning that we start the optimization process using the minimal structure elements but it was susceptible to local minimum traps and Tense is not susceptible to this problem.
3014744	3022366	A	0.8215828537940979	But it also gave us smaller structures with performances that were competing with other methods.
3022478	3023830	A	0.6858598589897156	Exam and ants.
3028750	3044080	A	0.722308874130249	So the advantages of Cans is that it has an unbalanced space compared to ants compared the results that came out were good compared to ants and Exam.
3045170	3055438	A	0.8898285031318665	The tunable hyperparameters are half of these in Exam and ants and also it indirectly encoded the neural topology to 3D thrash spaces.
3055534	3083660	A	0.603080689907074	So this is one of maybe the contributions of important contributions cans then so far ants and Cans are solutions that applied neural topology architecture search meaning that they did not optimize the synaptic parameters of the neural networks during the optimization process or during the evolution process.
3084030	3091630	A	0.8012701869010925	So we thought of actually making ends capable of doing this as well.
3091700	3104514	A	0.9020232558250427	So to train the neural network or optimize the synaptic weights, the weights of the structure of the neural networks during the optimization process, the structural optimization process.
3104632	3122550	A	0.9085542559623718	So we added a fourth dimension to the search space which we embedded the weights of snapdragon parameters in that map, these parameters in that new dimension.
3123850	3144910	A	0.5446590185165405	We also wanted the ants to be self aware and try to evolve themselves through the evolution process so that they can adapt to the changes or adapt themselves so that they can give us better performance.
3146210	3159886	A	0.8265205025672913	We want them to change their behavior, their characteristics like the sensing radius they had before, for example, that can be a variable that can change through the iteration thing and evolve through the iterations.
3160078	3173750	A	0.8879194855690002	Each end can change these characteristics as the evolution goes on on progress based on the performance of the neural networks that they are generating.
3181030	3200390	A	0.5233383774757385	So the advantage of doing this was that it eliminated the back propagation process which is the most computationally expensive part in the evolution process in the methods that we use so far in exam ends and Cans.
3200890	3208220	A	0.5550125241279602	So eliminating the back propagation gave us much faster evolution process.
3210350	3231886	A	0.8482519388198853	The results we got I will discuss the graph on the right hand side first which discusses the fitness or the Ms main absolute error of the results we got from back propagation free cans, the four dimension cans compared to the normal cans, the back propagation cans and ants.
3232078	3250390	A	0.7498310804367065	So the results showed us that again on a particular database that that propagation cans and that propagation free cans did quite a similar job but they were both better than ants on this particular database.
3251050	3260506	A	0.7104933857917786	But the actually main contribution or the main advantage of applying Cans shows up in the graph on the left hand side.
3260688	3299430	A	0.6206793189048767	Because we can see that if you compare the results based on the time of the evolution based on the evolution time, we see that it took much less time than the back propagation version of cans and the ants using these different number of ants, Also this graph shows how fast back propagation precans compared to the normal cans.
3302010	3315110	A	0.8997692465782166	In this figure the curves and the dollar lines shows the time that back propagation free cans and cans took to prepare or generate the neural networks.
3315270	3325850	A	0.7270839214324951	We can see that back propagation frequency took more time to prepare or generate neural networks compared to back propagation cans or the normal cans.
3325930	3334820	A	0.7767504453659058	And that's because the fourth dimension also has to evolve the ants or the agents through the iteration, so needs more time to do that.
3335830	3352578	A	0.8623526692390442	But the other two curves in dashed lines, here are the lines that shows the amount of time it took for the two methods to train and validate the neural networks.
3352754	3360490	A	0.6601522564888	So backpropagation free cans doesn't have to train the neural network doesn't do backpropagation.
3361150	3370010	A	0.7328574061393738	So you can see that the time it took is in an order of magnitude less than the back propagation version of camps.
3370350	3388370	A	0.6791242361068726	And these sort of lines shows the cumulative time took for both methods and of course it shows that back propagation precances took much much less time than the back propagation precance took much less time than the back propagation cans.
3389670	3403174	A	0.7210965156555176	The future directions that we are future points that we're concerning for sorry, the points that we're concerning for future work are to turn cans to a complete continuous search space.
3403292	3418858	A	0.7672315835952759	So as you saw, the search space for Kents is not purely continuous because the time lags are represented by discrete layers.
3419034	3427850	A	0.7729750871658325	We want to replace this with the continuous dimension continuous layer representing the time lag in RNNs.
3428010	3453282	A	0.6817672252655029	However, this is a little bit challenging because the time lags has to be known prior to any optimization process because other than that we will be mapping the whole time series as time lags and then picking the time lags from them which is not feasible.
3453426	3460266	A	0.8787479400634766	So this is the first point we're considering to investigate for in future work.
3460368	3477790	A	0.7866097688674927	The second one is to investigate this finding that we found in the results in ants where the recurrent connections disappeared from the best performing structures.
3478930	3494830	A	0.516791045665741	The theory that we have is that the memory based cells replace these connections, give us the information that we need from the pastime steps so that they were more effective, more efficient in doing this compared to the recycle connections.
3494910	3499560	A	0.8458967208862305	So we have to expand on this and investigate it more.
3500650	3506490	A	0.47845587134361267	The last thing we want to consider also, this is one of the top things that we want.
3506560	3510074	A	0.8035682439804077	These three points are the top on our list.
3510272	3521694	A	0.8863244652748108	The third one is to actually consider one of the concepts that was coined in a book by Dr.
3521812	3535822	A	0.8553711771965027	Deborah Gordon where she mentioned that the living organism in ants world are not the ants themselves, but they are the colonies.
3535886	3545762	A	0.7722224593162537	The colonies are the organisms that they start, and they grow, and they interact with the environment and the ecosystem.
3545826	3549830	A	0.6203886866569519	They interact with other colonies, and they die at some point.
3549980	3553106	A	0.6065725088119507	And the ants themselves are not organisms.
3553138	3556830	A	0.8070921301841736	They are the cells of these organisms, this colony organisms.
3556930	3569920	A	0.6255825757980347	So we want to take that concept and apply it in our methods to have number of colonies living together in parallel, evolving and communicating with each other.
3570770	3581730	A	0.6760944724082947	And we want to see if this would give us a better performance, because, after all, we're trying to mimic nature in the solutions we're trying to investigate.
3584470	3590580	A	0.8184862732887268	So with this, I'm done with my presentation, and if you have any questions.
3595760	3596428	B	0.4896697998046875	All right.
3596514	3597004	B	0.918424665927887	Awesome.
3597122	3599980	B	0.985027551651001	Wow, what a great presentation.
3600320	3603336	B	0.914292573928833	How about Travis and Alexander?
3603368	3609970	B	0.8249416947364807	Either of you which want to go first, please feel free to give an introduction and any primary remarks that you have on that.
3616660	3617120	C	0.584351658821106	Okay.
3617190	3618192	D	0.770581841468811	I can say something.
3618246	3619504	D	0.6795206665992737	I don't have too much to add.
3619542	3625412	D	0.9466996192932129	I think Abdul Rahman did a pretty good job going over the core bits of the work.
3625546	3627024	D	0.8506953120231628	I'm Alex Rorbia.
3627152	3638570	D	0.8765153884887695	I'm an assistant professor in computer science, an affiliate professor in psychology, and affiliate faculty in computational neuroscience at the Rochester Institute of Technology.
3639420	3648810	D	0.5756596326828003	And I work on a lot of stuff, but primarily predictive coding, active inference variational, free energy, a lot of the stuff that's actually of interest to this.
3650540	3662320	D	0.9732493162155151	Yeah, and this was a particularly interesting project for me because a branch of my own research is working in neurovolutionary methods or even just nature inspired metahuristic optimization.
3662660	3683264	D	0.7977556586265564	And when I got the pleasure of working with Abdul Rahman when he was a PhD student at Know, we talked a lot about the ant colony based optimization approaches, and I encouraged him to look into sort of, like, the origins and also try to understand actually how physical ants behave.
3683312	3685350	D	0.9154576659202576	So that was always fascinating to me.
3686120	3688996	D	0.7181930541992188	I don't have too much to add in terms of the technical parts.
3689028	3691028	D	0.7769468426704407	I think he covered all the core results.
3691124	3693112	D	0.8381019830703735	The only thing I like to think about.
3693166	3703852	D	0.9591981768608093	And I'm actually more fascinated, too, to hear from the active inference institute their interest in the ant colony methods and particularly what was interesting to them.
3703906	3714944	D	0.8984396457672119	Because thinking about what does ant colony optimization how do you view it from an active inference perspective is, I think, particularly interesting and thinking.
3715062	3719504	D	0.6869280934333801	And I even had a thought, I don't know if this was a question among the audience's mind.
3719702	3734420	D	0.8871341347694397	Do the little ant agents or the cant agents that run and described what is there a way to start to view them as a multi agent system that's optimizing some variational free energy quantity?
3735160	3745844	D	0.6883658766746521	Because a lot of even Carl's work I mean, I collaborate with him, sort of touches into this area of, like, well, what happens with collective intelligence and societal organization?
3745972	3754264	D	0.6803868412971497	And you can kind of look at free energy from these very high level viewpoints all the way down to fine grained cellular activity.
3754312	3765932	D	0.5198641419410706	And so I'm actually more curious to know Daniel and anyone else in the active inferences who can sort of explain their particular interest in ant based optimization and meta heuristic optimization.
3765996	3767024	D	0.7711523771286011	I'm curious to know that.
3767062	3772192	D	0.5836153626441956	But there could be some interesting viewpoints, like, what is the free energy?
3772246	3776404	D	0.9600427746772766	I think the ants themselves are very mean.
3776442	3777936	D	0.6319735050201416	We have made them more intelligent.
3777968	3785304	D	0.8888795375823975	I know Abdul Raman and I talked at length about, well, what if we even made them, for example, have, like, a reinforcement learning control system?
3785422	3793688	D	0.7155794501304626	And you could even imagine, well, now, what if the cans themselves engage in a form of active inference themselves?
3793854	3795752	D	0.8586918711662292	What would that look like for the system?
3795806	3798510	D	0.7195690870285034	And they are optimizing their own free energy.
3798960	3801480	D	0.7402679324150085	Those are just fun little thought experiments.
3801560	3803692	D	0.8632822036743164	We have obviously not worked on them.
3803826	3812572	D	0.7180578708648682	At least Abdul Rahman was never exposed by me to that part of my world and biomimetic intelligence.
3812636	3814092	D	0.8241952061653137	So those are my comments.
3814156	3825024	D	0.6595587134361267	I'm not sure if they're particularly helpful, but they're very general, and I'm actually more curious to know from the active inference institute, their interest in it and where does that maybe perhaps intersect?
3825072	3830870	D	0.6961826086044312	Or this is just like, oh, we know interesting topics and yeah.
3832280	3832804	B	0.8529649972915649	Thank you.
3832842	3833840	B	0.6541420817375183	Alexander.
3834000	3837690	B	0.9173532128334045	Travis, you want to say hello and give any reflections on the talk?
3840220	3841304	C	0.5569122433662415	Sure, I'll come in.
3841342	3842890	C	0.7003263831138611	Hopefully you can see me a little bit.
3843820	3844888	C	0.7256720662117004	My jungle here.
3844974	3846072	C	0.7990649938583374	Hi, I'm Travis is Sell.
3846126	3848132	C	0.9047358632087708	I'm an associate professor at RIT.
3848276	3851576	C	0.8642810583114624	I'm also the graduate program director for a master's in data science.
3851608	3856110	C	0.652808666229248	So if any of you are interested in any of this or data science, please shoot me an email.
3857200	3863330	C	0.9673497080802917	No, I thought this work was really very interesting in a lot of ways while Abdul Rahman was working on it.
3864180	3880100	C	0.9503167271614075	I think one of the coolest parts about it is a popular neuroevolution algorithm that came after neat is called hyperneet, and it transforms the discrete search space of neural architecture search into a continuous one.
3880250	3883056	C	0.8748853206634521	And it's shown to be a pretty powerful method.
3883088	3887496	C	0.7362076044082642	Well, this version of ant colony optimization, the new one, does the same thing.
3887598	3889284	C	0.7037051320075989	It makes the search space continuous.
3889332	3902844	C	0.894381582736969	But what I found was really cool about it was that as opposed to traditional ant colony optimization where you have a graph and you just send the ants along the edges of the graph and you take the best paths and construct a graph after it.
3902962	3913436	C	0.7488630414009094	Here the ants are actually working like ants in the real world where they'll move a continuous amount of space, not just from point A to point B and actually dropping down pheromones.
3913468	3919010	C	0.6186507344245911	And it's more like a real simulation of how ants would move around in the real world.
3919700	3923536	C	0.8309850692749023	And we're getting better results from it than some of the older methods.
3923568	3933190	C	0.9898084998130798	So I think that really made me kind of happy to see that and thought it made it as very interesting work.
3935900	3936920	B	0.9799191951751709	That's awesome.
3937070	3942760	B	0.5183826088905334	Ahmed, want to add anything or I'm happy to give a thought on the answer and ask some questions from the live chat.
3943740	3944776	A	0.9117157459259033	I'm good.
3944958	3956300	A	0.7309033274650574	Travis, I think, covered all the things that I wanted to say about, I think just one thing that Alex mentioned that we give higher intelligence to the agent.
3956370	3960444	A	0.8532377481460571	This is something that we discussed and I'm pretty much open to that.
3960482	3964000	A	0.6010326743125916	I started actually to think about it, but I didn't implement anything yet.
3964150	3973440	A	0.8302413821220398	However, the last thing that I discussed in the future directions is something that I actually started working on, but I didn't start the experimentations yet.
3973510	3976690	A	0.7799566984176636	So hopefully we will see something out of that.
3978680	3979430	B	0.918424665927887	Awesome.
3979800	3997660	B	0.7092989087104797	Well, there's a ton of ways to go and isn't that kind of one of the fundamental questions like in an interactive setting, either pure agent stigma g interaction or multi agent, but ultimately mediated through multiple stigma g's with like reading and writing and error correcting code.
3997730	4017660	B	0.6642206907272339	So in that communication setting, I felt like the work generalizes along multiple dimensions that previously approaches to multi agent just didn't have those kinds of flexibilities like the continuous time feature and several other features.
4017740	4034564	B	0.677639901638031	And I guess with respect to the ants themselves, I did five summers of field work with ants in the USA Southwest in Arizona and observed a lot of foraging activity.
4034692	4049356	B	0.7590813636779785	So that problem or that context or setting is really a fun one and a pervasive one across any kind of living system, anything that's going to be active and living.
4049458	4056380	B	0.8603987097740173	So why did you pursue foraging type algorithms overall?
4056720	4069730	B	0.9103543162345886	And does this class include the interaction based methods with direct agent contacts that Professor Gordon highlights in the Ant Encounters book?
4070420	4088340	A	0.7897807359695435	Yeah, I will try to respond to this, but I just want to mention that the first thought about this actually Travis started this idea about using N colon optimization and neural sector search by applying this method in simpler neural networks.
4088420	4096368	A	0.8047076463699341	Elman and Gordon neural networks required neural networks and I took the leap from there and started working on my thesis, my PhD thesis.
4096564	4098684	A	0.8314693570137024	So we thought about this idea.
4098722	4126028	A	0.8193104863166809	I think I mentioned a little bit about why we used an optimization in previous slide, but just repeating that for the audience to make sure that they got we thought about this idea because Nko optimization was applied as a graphic solution, and we thought that why not neural networks since they are investments?
4126124	4129264	A	0.8063027858734131	Neural networks, they are directed graphs.
4129312	4134708	A	0.8673946261405945	Neural networks are directed graphs because the flow of information goes from one direction to another direction.
4134724	4138964	A	0.8939545154571533	So they are directed, but they also graph because they are nodes connected with edges.
4139092	4149050	A	0.5968086123466492	And our ultimate aim or goal was to optimize the structure by removing and adding elements to it so that it gives us better.
4151120	4151724	B	0.918424665927887	Awesome.
4151842	4169196	B	0.8629580140113831	And one way that Professor Gordon and others have talked about that bi directional learning relationship between the computer science and the math and the analytical formulations and then the field work and the actual behavior is because the thousands of species of ants.
4169228	4180736	B	0.8096747994422913	They're working amidst a huge range of ecologies with all these different patterns of regularities, all these different resource distributions and foraging.
4180848	4182500	B	0.8330934047698975	It's amazing how general it is.
4182570	4194692	B	0.6236888766288757	Yet it's also just one of the functionalities that need to occur in terms of these even slower processes, like allocation of tissue to faster processes, even of response to alarm pheromone.
4194756	4202412	B	0.7642099261283875	So it's like this one class of algorithms clearly scales across from few.
4202546	4208888	B	0.5412013530731201	Ultimately one some of these foraging algorithms are lone foragers they don't leave pheromone trails.
4208904	4227380	B	0.5992386341094971	So it's like even the idea of leaving a single positive pheromone or leaving but also there's things models can do that ants can't do, like the time travel, pheromone, lossless perception, high dimensional signaling profiles that can't occur just with like finite amounts of molecules.
4228520	4241316	B	0.8571153879165649	Now they are just persisting on their path as active agents within one generation with a variational free energy at the behavioral scale, then across generations with that evolutionary layer.
4241428	4246696	B	0.8301749229431152	And the relationships between the neural network implementation and the active inference model.
4246798	4250060	B	0.824561595916748	They're kind of like two ways of describing, implementing.
4250880	4251628	B	0.5491447448730469	Yeah.
4251794	4265692	B	0.8437676429748535	I'd be curious to hear any of your thoughts on where you see active inference coming into play or how do you see ultimately similarities and differences between neural network based approaches and active inference based approaches.
4265756	4270080	B	0.7987699508666992	Are they the same complementary, overlapping.
4272760	4273172	A	0.8000365495681763	Guys?
4273226	4274550	A	0.8352904915809631	Want to take this one?
4276360	4279190	C	0.8179374933242798	I think this might be more of an Alex question, to be honest.
4280520	4281670	A	0.5924021601676941	That's the test.
4282840	4286816	D	0.8539726138114929	Well, yeah, I mean, if you want to keep it on active inference.
4286848	4299820	D	0.8463352918624878	But I think, Travis, you're going to need to tag in when you want to get into the very specifics of the actual ant colony details because, again, I kind of see the ant colony optimization from a more global point of view.
4299970	4305292	D	0.6247351169586182	I would say so to be mean this particular work.
4305426	4314800	D	0.7954416871070862	So that way Abdul Rahman is not also completely blindsided by your question, Daniel, even though the name is in the institute itself, this isn't an active inference.
4315140	4342760	D	0.7253676652908325	You know, again, while there are obviously, as you pointed out, lots of interesting elements like, for example, the fact that the ants when they conduct their exploration along let's just think about the recurrent networks and they're figuring out what nodes and what as Abdul Rahman explained the superstructure, right as they iterate across with their pheromone trails and figure out what nodes I want to recurrently, connect, feed, forward, connect, skip, connect, so on and so forth.
4344060	4344376	D	0.546256959438324	You.
4344398	4344744	D	0.6277148723602295	Would say.
4344782	4345144	D	0.5966773629188538	Well, okay.
4345182	4352360	D	0.7688791155815125	What these ants are doing is they're engaging in epistemic foraging, which is a key concept in active inference or idid.
4352440	4359480	D	0.7859242558479309	So that way Abdulrahman and Travis are not also left behind by a jargon epistemic foraging and active inference.
4359640	4361212	D	0.7953022718429565	It's a big general framework.
4361276	4365228	D	0.7989445924758911	It's like a neurobiological process to RL.
4365324	4369504	D	0.8214030265808105	And epistemic just refers to kind of like the uncertainty Travis that you and I work on.
4369542	4374676	D	0.7083019018173218	And the idea is it's saying, well, OK, I want to understand my world.
4374858	4380884	D	0.5664058923721313	And the more that I explore my world, right, there will be things that surprise me less.
4381002	4391256	D	0.586316704750061	But if I encounter some information that is really weird when I build a generative world model or a predictive world model, that's very surprising, I should probably explore that.
4391438	4403484	D	0.8748868107795715	And so, of course, I'm condensing the concept down into sort of like the exploration part of the explore exploit trade off that I know, you know, but that characterizes reinforcement learning.
4403522	4407464	D	0.6078852415084839	So that's just what we mean when we say epistemic or epistemic foraging.
4407512	4412416	D	0.6297248005867004	And obviously, Abdul Rahman foraging can be likened to what the ants are doing, right?
4412438	4414028	D	0.6615906357765198	They're exploring their environment.
4414204	4416592	D	0.833100438117981	And so I guess with that in mind.
4416646	4427264	D	0.807732880115509	So that way everyone's sort of on the same page here to get to your question about the differences, about how does this work versus your typical neural based approaches to active inference.
4427392	4433280	D	0.8111013174057007	And I would fall into that category of, oh, I build neural models, biological process models.
4433440	4439096	D	0.8486735820770264	Those are very much focused, you could say, at the individual level, at least the ones that I am aware of.
4439118	4449644	D	0.8772624135017395	When you're building, for example, even a back propagation based, partially observable Markov decision process in active inference, that's like a single agent, right?
4449682	4467232	D	0.8704696297645569	You're trying to build this construct that is trying to balance the epistemic quantity with its instrumental term, which, by the way, another jargon term for you, Abdul Rahman and Travis, that's just like your reward signal or your prior preference or a prior distribution over goal states.
4467366	4472992	D	0.8805175423622131	And so these agents sort of like deal with that trade off, but at an individual agent level.
4473046	4488564	D	0.8517924547195435	Now, again, I'm sure that there's interpretations of these from other perspectives, the ant based approach, even though I would not argue per se that this has at least an explicit form or a connection, at least that Abdul Rahman has made clear to active inference.
4488612	4493172	D	0.8425687551498413	But the idea is that this is like a multi agent approach to active inference.
4493236	4516284	D	0.811649739742279	And so the ants, when they conduct their epistemic foraging, which arguably is a very simple model, each ant and of themselves is essentially a bunch of coefficients and some hard coded rules because their job is essentially to work together with their pheromone trails to figure out oh, what parts of the superstructure are useful.
4516332	4521040	D	0.8507351875305176	So I'd say that that's different and it lends itself in some ways.
4521110	4527604	D	0.6651607751846313	Of course, you can make the ants more complicated and lose the benefit of I'm just about to say you could massively paralyze this.
4527642	4531408	D	0.8871650099754333	And this is one of the key strengths that I think is natural.
4531504	4539416	D	0.782371461391449	In, for example, a lot of actually nature inspired optimization algorithms and ant colony optimization is one of them.
4539438	4553528	D	0.879978597164154	Is Abdul Rahman has been using hundreds of CPUs, and you can put these ants on their own individual processor, and the communication that's occurring across them as they exchange information is through the pheromone trails.
4553544	4557336	D	0.6105385422706604	There's an indirect mechanism, it's not terribly complex to facilitate.
4557368	4564944	D	0.7538203597068787	And I'm sure that there's even better ways to go about doing asynchronous forms of communication and further, further optimize this.
4564982	4579008	D	0.5381240844726562	I know Travis and Travis can add to this has done things on like, citizen science and distributed computing through volunteer computing and how you can distribute this through a massive global Asynchronous network.
4579024	4594440	D	0.8034871220588684	So you can imagine adapting the ant form sorry, the ant colony optimization approach to some distributed, massively distributed version of active inference, where you essentially have to write down that the variational free energy.
4594590	4606200	D	0.6064750552177429	And I'm putting quotes around this because, again, there is no concrete term written in Raman's work because at least we haven't viewed this from the active inference perspective directly.
4606280	4617490	D	0.8374256491661072	Each ant is optimizing its own variational free energy, but then there's probably a global quantity that sort of is related as a function of those pheromone trails in the individual ant agents.
4618020	4634336	D	0.9064955115318298	And then of course with the exploitation term or the instrumental or what is the reward signal to give an RL term that's sort of driven by the performance of the actual agent on each candidate agent on the task right of the ram.
4634368	4638212	D	0.676577091217041	And you compute like mean squared error when you're doing time series prediction.
4638276	4655116	D	0.832946240901947	So in some sense we have built in a reward function that we use and again, for those in the active inference group here, you can use the complete class theorem and look at the prior preference of saying, oh, well, the reward is actually technically a prior preference, right?
4655138	4657180	D	0.7654106020927429	It's like a log probability.
4657520	4662124	D	0.8295066356658936	So with that in mind, you could squint at ant colony optimization.
4662252	4673380	D	0.6054341197013855	And I guess the big benefit comes from that massive parallelization that you wouldn't actually very easily, if at all, get with our single agent neural based approaches.
4673720	4678404	D	0.8151413202285767	And that might be an interesting place to build on and I'll stop rambling at this point.
4678442	4680020	D	0.7004764676094055	I'm not sure if that was helpful.
4683000	4684184	B	0.9809349179267883	That was awesome.
4684302	4705820	B	0.8811697363853455	Even earlier today, Chris Fields in the Physics as information processing course was talking about the classical information inscribed on the blanket, which is like the pheromone perception and deposition, pheromone modification and perception sense making an action which we can associate with the nest mate cognitive system.
4705970	4726896	B	0.7925035357475281	So then there could be as simple as a pass through for the nest mate, could be any arbitrary relationship described with a blanket, simple nest mate, sophisticated nest mate, like another level of time series modeling, whereas there's the environmental time series modeling and that's just in the ants.
4726928	4740888	B	0.8558889031410217	And then the fourth dimension is like that quantum rotation which goes from the lower dimensional classical stigmagic screen into the quantum informational space.
4741054	4761200	B	0.8650780320167542	And so that's one of the discussions ongoing in Actimf right now is about well, previous approaches to connect quantum formalisms to macro, let's just say neural phenomena based it upon the plausibility of like a molecular electronic bubbling up.
4761350	4783300	B	0.5438972115516663	Whereas just with research from decision making and statistics and just multiperspectival modeling and all the issues associated with the physicality of information transfer, the finiteness of it, the quantum formalism becomes useful just by itself with or without reliance on some other electronic phenomena.
4783640	4794360	B	0.7937241196632385	So it's just a lot of very interesting connections like having the degrees of freedom on the blanket which could be noiseless and four bits or it could be noisy with this really specific thing.
4794430	4808268	B	0.5934171080589294	But in Silico you get to play it from both sides and scale things up and down and do these meta heuristics on top of that arbitrary space could be really simple for learning, or it could be however much.
4808354	4819420	B	0.808666467666626	And then just like the ant colony algorithm is ultimately federated through embodiment, that property makes it a really useful candidate for biomimicry.
4819500	4833672	B	0.8366169929504395	So a lot of times when people think about collective behavior, they're thinking about like the flock of birds and the school of fish and those are of course collective systems and collective behavioral and all these kinds of complex systems properties can be studied in that type of system.
4833726	4840010	B	0.5050605535507202	But it is also neglecting at least an analytical degree of freedom with the stigma g.
4840380	4853448	B	0.5648216009140015	So that really opens up both the quantum and the classical information or both niche modification and behavioral and cognitive modeling.
4853624	4864364	B	0.8528852462768555	So just to add on because I think and then also what the node is could even be heterogeneous or unknown or fit in different ways or fix through design processes.
4864412	4884516	B	0.6154534816741943	So just like the ant colonies are flexible, enabling them to live in all kinds of places, make all these kinds of nested decisions that interact with each other, that flexibility is just like the tip of the iceberg, of what we could even just describe, because there would always be real environments we hadn't yet tried with ant colonies.
4884548	4891208	B	0.5534974336624146	So we really would never know the full extent of all the repertoire and the dynamics of the ant system.
4891374	4904108	B	0.8734318614006042	But then we can just abduct into new mathematical statistical distributional frameworks, pull back to different levels of the learning and the meta learning process and just start there.
4904274	4909376	B	0.6292397379875183	And then almost ironically, or maybe the opposite of that, it.
4909398	4916288	B	0.9057897925376892	Could be applied to ant colony video data or movement data or foraging activity itself.
4916454	4923132	B	0.6408315896987915	But it kind of takes inspiration and develops in parallel or in conversation.
4923196	4926100	B	0.7532085180282593	So it's not like bound to what real ants can do.
4926250	4938644	B	0.8443254232406616	Or you could constrain it so that there are properties that real ants have, like they can only interact within this certain way or there really are only this many pheromones do model comparison.
4938692	4941252	B	0.743165135383606	So it's a lot of degrees of freedom.
4941396	4945156	B	0.6473388671875	I feel like you all are opening up with the ant collie modeling.
4945268	4951790	B	0.6631069779396057	And also one of the challenging pieces of multi agent simulation is kind of like the open endedness with the design space.
4952960	4964716	B	0.520542323589325	So then it's very hard for even creative ideas like sometimes to find the right compute resources that obviously are still even needed for what he discovered with the analysis.
4964828	4967644	B	0.8756452798843384	Here I'll ask a question from Bert.
4967692	4981430	D	0.8624963760375977	In the to before we move on to that, I just want to clarify just so make sure that I got the right term and certainly Abdul Rahman and Travis might want to look it up is when you were saying blanket, you were referring to a Markov blanket, correct?
4981800	4982356	B	0.46103888750076294	Yes.
4982458	4991812	B	0.8997446298599243	So the technical definition of Markov blanket is when you have a Bayesian graph where nodes are the variables and edges are relationships amongst these variables.
4991956	4995304	B	0.8982478380203247	For any given node of interest, we'll just call it internal states.
4995342	4996584	B	0.5371702313423157	So these are not features of the world.
4996622	5000056	B	0.48873239755630493	It's not tagged onto some tissue of a real nest mate in the world.
5000158	5014284	B	0.9071669578552246	This is something that's tagged onto or like a perspective we could take on any node in a Bayes graph and then all the nodes that insulate it and the co parents are known as the blanket in the sense that it's like one layer insulator.
5014412	5020770	B	0.8469890356063843	And there's a lot of more discussion on it and the philosophical implications and all these generalizations of that.
5021320	5039850	B	0.8948707580566406	But broadly, the Markov blanket is just the inbound dependencies which we associate with sense making and perception learning attention and then the outgoing dependencies for the agent which we associate with action influence in some downstream pointing way.
5040300	5040856	D	0.8529649972915649	Thank you.
5040878	5047032	D	0.6489266753196716	I just wanted to clarify that because I don't think Abdul Rahman and Travis might be familiar just with that terminology.
5047096	5051976	D	0.7405436635017395	It's very know, very active inferency kind of jargon.
5052008	5056188	D	0.8597400784492493	So I wanted to make sure that they got that from the physics point of thanks.
5056354	5058552	B	0.8993580341339111	Yeah, totally great point.
5058626	5062364	B	0.8555986285209656	And ask a question now from Bert.
5062412	5064210	B	0.7210191488265991	So Bert says.
5065060	5066108	B	0.9372386336326599	Very impressive.
5066204	5070572	B	0.9573215842247009	Solving generative models with more generative models sounds very promising.
5070716	5073760	B	0.8669263124465942	What about replacing ants with convolutions?
5077610	5085770	A	0.8623703718185425	Talking about like a meta learning algorithm, like having a neural network that learns how to optimize other neural networks?
5088290	5094190	A	0.7499128580093384	Yeah, this concept, I think is introduced at some point in machine learning learn.
5094260	5110290	A	0.5347716808319092	But we wanted to apply for nature based method that mimics like the Mother Nature, which is nature is the most efficient optimization evolution system.
5110360	5120470	A	0.7018067836761475	So looking at the results that are there in nature applying nature based methods they were superior to any other method.
5121210	5132460	A	0.9501581192016602	And the results we got also which just pointed that out that we saw good performance coming out from these results from our results and the previous results from other methods as well.
5134030	5136140	C	0.8150961995124817	Hop in here a little bit too.
5137550	5141866	C	0.8723023533821106	In the case of neural architecture search there's kind of a couple of classes of approaches.
5141978	5151822	C	0.5649964809417725	One is constructive, so kind of like where you build larger and larger networks and try and keep your network size minimal to try and find your optimal solution.
5151966	5164354	C	0.8757030367851257	Other types of neural architecture search approaches use like a superstructure and this is kind of how the earlier iterations of this were where you have a bound of your search space and you try and find the optimal network within that bound.
5164402	5171750	C	0.6763558387756348	So one is like trying to build things from the ground up and the other one is trying to trim down a big network to a small network.
5172750	5188586	C	0.5737462043762207	As to your question about convolutions, there's been a fair bit of research lately in what's called graph based neural networks which can use convolutions over like a discrete graph search space and can potentially produce other graphs.
5188618	5193070	C	0.5666893124580383	And I believe there's been some neural architecture search work using this.
5193220	5208770	C	0.7229681611061096	But one of the main and I think cool things about the approach here, which is different from those is if even if you have a graph based neural network and you have your search base defined as some kind of matrix where things are off and on depending on which nodes are connected to each other.
5208840	5218594	C	0.615185022354126	You have a fixed search space which may not be big enough or it might not be the correct search space for this neural architecture search problem where this method here is all continuous.
5218642	5218854	C	0.7360090017318726	Right?
5218892	5241230	C	0.5252906680107117	So within this continuous search space it gives the algorithm a lot of freedom, maybe too much freedom but a really open ended way of generating a wide variety of neural architectures which if you preconstrain your algorithm to work within a fixed discrete superstructure you may not even find them because they're not even a possibility.
5242130	5246026	C	0.505199670791626	So that's one of the reasons we didn't go that route.
5246058	5261458	C	0.7928469777107239	But there are graph based neural architecture search algorithms out there where basically you take the architecture as a graph, you train a neural network to spit out another graph that it might think is better and those use convolutions.
5261474	5263990	C	0.6219189167022705	Sometimes that helps.
5264330	5264982	B	0.9799191951751709	That's awesome.
5265036	5266854	B	0.5442473292350769	So convolution sounds like yes.
5266972	5303266	B	0.8853639960289001	And one thought on that is yes, the ants solve all these incredible patterns and kind of do amazing things amidst informational and physical limitations like we all do having the ants be able to just make trade offs within a task space and then have a dial as modelers to make that task space.
5303368	5313880	B	0.8872518539428711	Kind of like touching the pheromone distribution or metacognitive ants or something emulating essentially that.
5314490	5323214	B	0.8273549675941467	And for example, the active inference forward looking and thinking through other minds that there could be a kind of cognitive colony.
5323362	5366390	B	0.6713954210281372	So then that enables in silico total thought, experiment colonies and through data driven processes also kind of keep continuity with that model, perhaps literally continuity with the model and then connect it to empirical, which is something that is very hard for agent based modeling which, as you kind of pointed to, often sets certain fixed axes, performs like asweep looks at one mechanism, doesn't look at all these possible mechanisms of learning and intra and intergenerational and all these time effects.
5366890	5373830	B	0.9062544107437134	So how do you see this being used in different research or application domains?
5378400	5394592	A	0.822545051574707	Well, the main use case that we're thinking of was neural architecture search to apply it for other domains other than neural architecture.
5394736	5396710	A	0.7149720788002014	We didn't figure out that yet.
5397160	5399990	A	0.8816981911659241	I think Alex can explain on that.
5400840	5402816	C	0.6242215633392334	I can hop in a little bit mean.
5402858	5420264	C	0.779831051826477	So basically this type of algorithm, if you need to generate graphs and you don't necessarily have a fixed structure for that graph and when I say graph, I mean like a computer science graph where you have nodes in a different so pretty much anything involving graph construction.
5420312	5422332	C	0.6392378807067871	I think these types of methods work.
5422466	5427536	C	0.8883969187736511	Neural networks kind of under the hood can be represented as graphs and usually are.
5427638	5446230	C	0.49688422679901123	So I think we're using it for neural networks because it's really popular but there's other algorithms out there like the traditional traveling salesman, there's like routing problems, all that, any type of stuff where you might need to generate a graph in a smart way to do that.
5446920	5465004	C	0.7148452997207642	To your other point though, I think what's really cool here and I don't want to steal Alto Robin's thunder, but his last point on future direction is while a particular version of this ant colony optimization search is running to find an optimal neural network, a colony has fixed parameters that it operates within.
5465202	5477168	C	0.7680290341377258	But if you think of the colony as an organism as opposed to the ants being an organism, you can evolve colonies that optimize how the ants themselves act.
5477254	5489984	C	0.8649764060974121	So you can have evolving colonies that in a smaller sense also evolve or optimize what they're doing within their prescripted parameters for the agents they're generating.
5490032	5491780	C	0.861121654510498	So you can have like a meta meta.
5494280	5495060	B	0.9781022667884827	It's awesome.
5495130	5508900	B	0.6120116114616394	I mean the evolutionary account of a why question for ant behavior today, one part of that answer is like because colonies that couldn't under that regularity or constraint survive.
5509060	5511924	B	0.5030360221862793	We've had a long, long time to wipe those off the table.
5512052	5516140	B	0.7713148593902588	And so every biological system has to have that kind of multiscale ordering.
5516960	5536460	B	0.8881044983863831	In 2021 we made the active imper ants paper which was modified from an epistemic foraging visual attention task about scanning around and then learning a cicade policy that had to do with epistemic foraging but not leaving a trace.
5536540	5554856	B	0.6718851327896118	And then the main modification to bring that active inference epistemic visual foraging model into the active inference ant setting was to add a pheromone rule, just like you described, even though, of course, again, that's not the only pheromone rule, but that's just the most simple pheromone rule that we can generalize from, as you definitely have.
5555038	5561076	B	0.5790184140205383	And there are just many emerging ways of modeling those multiscale active inference models.
5561108	5569656	B	0.7433763146400452	So composing across layers, which we might associate more with the kind of laterality of things that happen through interactions.
5569768	5592100	B	0.8954205513000488	And then also, as Mike Levin shows, with kind of the time diamond systems that have a memory retention component of some shape, cognitive shape, and then a protention awareness or agency or other attributes you can use to describe that.
5592250	5597430	B	0.5622299313545227	And that's a statistically amenable way to describe things.
5598440	5615176	B	0.6763266324996948	And then there's a variety of implementations on a given statistical problem, or like Federated Compute architecture, it might be the case that you're not running the pure matrix multiplications that are shown in the early MATLAB code of active inference.
5615288	5624584	B	0.833212673664093	Different components of machine learning systems might be kind of composed together, also ways kind of abiding by those patterns of communication.
5624632	5633408	B	0.6546740531921387	But then there's a level of abstraction that we can still describe, but it doesn't mean active inference is going to be kind of causing it.
5633574	5635520	B	0.5398686528205872	So that's what gives a lot of flexibility.
5635860	5654040	B	0.9664149284362793	And it's really cool that through your background, Alexander and work and these kinds of collaborations that like the active inference perspective on multi agent modeling with all these other views can at least come together comparatively.
5654540	5669144	B	0.8489207029342651	And then that is going to, I think, be quite an interesting interchange to apply this entire tissue type or colony type thinking above and within the models.
5669192	5672510	B	0.7481157183647156	Just a lot of degrees of freedom, like you said, could be too.
5673440	5680108	D	0.859138548374176	And I think there's different ways too, I think depends on how you want to take the ant metaphor.
5680204	5691684	D	0.8860231041908264	And again, it's kind of interesting, some of the questions or comments that you're making, Daniel, and some from the audience about how does this think about it from a cognitive point of view.
5691722	5703940	D	0.8478432893753052	I mean, I do work in cognitive architectures, of course, again, kind of from the whole single agent or single entity and modeling a single brain in its different regions.
5704020	5731616	D	0.8299663066864014	But I think if you take a nature inspired optimization approach like the ant metaphor that Abdel Rahman latched onto, and that's sort of like the way in which he formalizes how takes a principle of how ants interact with their world, interact with each other, and then mathematically model those particular concepts step by step.
5731798	5741140	D	0.741518497467041	And I think if you bend the metaphor and say, well, okay, could the ant colony metaphor apply to multi human agent systems, right, or other entities?
5741960	5751856	D	0.8791968822479248	Does the ant necessarily can it be generalized beyond the physical creature upon which Abdul Rahman based his initial metaphor?
5751888	5754968	D	0.782742977142334	And that's an interesting philosophical kind of take to it.
5755054	5760008	D	0.8598984479904175	And then how do you apply that to, let's say, building a multi agent cognitive system.
5760094	5777452	D	0.7812086939811707	And then, of course, as Travis was discussing with you and you were mentioning metacognition know, you could think of ant colonies, of ant colonies, but you could even replace the word ant and just say, well, we have clusters of intelligent agents, or whatever degree of modeling we're doing.
5777506	5796836	D	0.5844216346740723	Because again, I do want to emphasize that at least the cans and ants agents that I have worked with in the context of the Raman, they are not each and of themselves, even, I would argue, if nothing else, a very extremely simplified generative model or a very simple control system.
5797018	5803444	D	0.6106185913085938	There's no neural network under each one because then you'd have to simulate computationally each one of these ants within the framework.
5803492	5810344	D	0.7914940118789673	So I think there's always that practical machine learning kind of viewpoint of, well, there's always how do you simulate that?
5810382	5812548	D	0.8769920468330383	And Alderam is working with CPUs.
5812644	5817624	D	0.5336165428161621	It's not like he has an army of GPUs to replace them with convolutional networks.
5817672	5820076	D	0.9638451933860779	Again, if you had the resources, this would be awesome.
5820178	5824284	D	0.7518846392631531	But expense and money is another constraint on this planet.
5824332	5845620	D	0.8484973311424255	But I think there's interesting views and interesting directions one could go by taking inspiration from the ant metaphor and the concept of pheromones and translate them to other real world signals and how, for example, communication patterns among other animal entities or other human agents.
5845690	5848368	D	0.924793004989624	And I think that that opens up an interesting perspective.
5848464	5869836	D	0.8438974618911743	And if you're constantly trying to connect it back to free energy minimization and trying to say, well, how are we balancing the terms that you can decompose it into an epistemic and an instrumental, and how are these balancing out and how are these physical processes that we specify balancing those terms?
5869938	5871804	D	0.9539803862571716	That's just a very interesting place to be.
5871842	5877216	D	0.8964772820472717	And you mentioned active inference versions of Hans, and that's fascinating and of itself.
5877398	5882396	D	0.7796868085861206	Last comment I have is, again, the degree of modeling and what you are modeling.
5882508	5886176	D	0.7924960851669312	Like, if you're modeling a society or organization, that's one way.
5886198	5904808	D	0.8748942613601685	You could use the ant colony framework, if you will, or metaheuristic optimization frameworks to then cast a system, any type of complex multi agent system, as an active inference kind of engaging process.
5904894	5918712	D	0.7236903309822083	Or you could go really low level and think about cells in a body or units that make up organs or organelles and trying to say, well, can we use this to model that level of granularity within, like a human or an animal entity?
5918776	5919004	C	0.7360090017318726	Right?
5919042	5929964	D	0.7498931884765625	And I think there's some fascinating questions about how does this metaphor manifest itself at different timescales and different degrees of perspective about how you're modeling?
5930012	5930816	D	0.8173394203186035	What are you looking at?
5930838	5933244	D	0.8599386811256409	What's the picture that you want to emulate?
5933292	5943524	D	0.5690773725509644	And of course, there's always under the hood, this practical consideration of, well, okay, the computational expense that you allocate, and are you able to actually run that simulation long enough?
5943562	5946916	D	0.7855244874954224	Because I think Abdul Rahman, you can correct me if I'm wrong.
5947018	5951688	D	0.8681478500366211	You mentioned one of the experiments I think was for the bigger systems took a month.
5951774	5952072	D	0.5664746165275574	Right.
5952126	5953800	D	0.6403098702430725	Of course this was on CPUs.
5954860	5959224	D	0.479075163602829	That can get pretty prohibitive if you want to go even bigger than that.
5959262	5964136	D	0.8259913325309753	But again, I think it just depends on what hardware you have to simulate this on.
5964318	5965050	A	0.5491447448730469	Yeah.
5965420	5971048	A	0.4850451648235321	And also using high performance computing, it is not always feasible for smaller.
5971144	5980332	A	0.505763590335846	So if we try to model the brain of Aims like small neural network, using a GPU might not be feasible solution.
5980396	5992896	A	0.484836220741272	And Travis is a high performance computing specialist here, expert tell you that sending data to a GPU and getting it back is very time consuming and resources consuming.
5992928	6006484	A	0.721541702747345	So it will worsen the time consumption rather than solving it, because communicating between the main memory and the GPU had an overhead.
6006612	6014700	A	0.5801640748977661	So it has to be a big enough problem to actually utilize this GPU in such solutions.
6016880	6019390	C	0.5147984027862549	Sorry, go ahead.
6021040	6034384	C	0.7524291276931763	When you have the super large language models, or large models for computer vision, they do a lot of just massive operations on Tensors, which are basically multidimensional matrices, right?
6034502	6041860	C	0.8073846101760864	And when you have really big wide Tensors, you can parallelize the operation really nicely across the GPU.
6042680	6052148	C	0.872783362865448	A lot of this work is based on doing time series forecasting, time series classification on sensor data, stuff from like power systems.
6052244	6063800	C	0.6791167259216309	So the input to a large language model might be 1000 or more length of a word embedding, which is actually not huge.
6063870	6071416	C	0.7909103035926819	But if you go up to a computer vision model, the input image may be 1000 by 1000 pixels and that gives you actually a million inputs.
6071448	6072030	C	0.5664746165275574	Right.
6072400	6078100	C	0.8617501854896545	When you're working with sensor systems, off of aircraft, power plants, that type of thing, you may have 50 to 100 inputs.
6078200	6084204	C	0.6600123643875122	And when you're working with this type of time series data, you don't need a massive super wide neural network.
6084252	6091272	C	0.7045120000839233	And then if you add in recurrency where you have to do backprop over time and other things like this, you actually can't really parallelize it nicely on a GPU.
6091356	6093824	C	0.7192680239677429	So for us, the CPU is actually more efficient.
6093872	6100096	C	0.5635643005371094	We tried Abdul Rahman wrote a bunch of code a long time ago to put this stuff on GPUs and we found it was quite a bit slower.
6100208	6104330	C	0.5072086453437805	So, depending on what you're doing with a neural network, a GPU actually isn't the right answer.
6105180	6121500	C	0.511040210723877	But the other cool thing about this, which I think does have maybe even potential for there, is that one of the big not talked about problems in machine learning is that back propagation is the fastest thing we know, but it's inherently not scalable.
6122080	6129884	C	0.8154938817024231	You can get a bigger, better GPU to do your bits of your network in parallel to speed things up, but that only gets better if you have a bigger network.
6130012	6137692	C	0.5910931825637817	If you want to speed up the training process, you can't just add another CPU or another GPU and make back prop go faster.
6137756	6141328	C	0.6094449162483215	You can make the forward and backward pass through your neural network faster.
6141424	6144176	C	0.6682463884353638	But you still have to do every epoch of backprop.
6144208	6150644	C	0.58157879114151	Iteratively a method like this where we're generating one, it's backprop free.
6150682	6152448	C	0.7403213381767273	So it's not using backprop.
6152624	6162764	C	0.6213732361793518	We can use a nature inspired or other method to use hundreds of computers and you can throw twice as many computers at it and get a result twice as back, twice as fast, whereas backprop you can't do that.
6162802	6173324	C	0.5801709890365601	So if you think about actually being able to train a neural network backprop, actually it's got a pretty low speed limit for what we need to do.
6173442	6181200	C	0.6555010676383972	And it's kind of a big problem with the machine learning community that people don't like to talk about because they're like oh, I'll just buy the next big Nvidia GPU and that'll do things faster.
6182580	6183936	B	0.971924364566803	That's super interesting.
6184118	6202120	B	0.5334761142730713	Does this maybe even bring up a kind of relationship where things like a graphics visualization of course a GPU does well and that's like the screen changing through time with a classical process that can be massively unfolded.
6202860	6208596	B	0.8654696941375732	And then the cognitive models, ultimately, of the nest mates, which again, can be nested.
6208628	6220092	B	0.7709711194038391	But the thing that's more quantum, more cognitive model, like you can do in parallel, because the minds are not influencing each other except through stigma g.
6220226	6223820	B	0.812471866607666	So then that is CPU bound, the size of the colony.
6224820	6230924	B	0.8494318127632141	And then you could use different graphics techniques, like there are colonies, organisms.
6230972	6235376	B	0.813210666179657	So one ant being or it's a philosophical question.
6235478	6238528	B	0.8697471618652344	What is the scale at which A exists?
6238624	6242964	B	0.814094603061676	But all throughout California with the Argentine ant, for example.
6243082	6250920	B	0.7790800929069519	And so how do we deal with those kinds of meshwork cognitive systems all the way on through 50 in an Acorn?
6251340	6275088	B	0.5268782377243042	There's just all these different trade offs that are being made and like in the featureless deserts there's different wayfinding pathfinding sensor integration, polarization of light, like different cognitive strategies because they might be going out long distance and dragging something home, not leaving any pheromone because it's not any more likely to have food there.
6275174	6279840	B	0.8024976253509521	So in that case, the stigma g is basically minimal to essentially none.
6280180	6292500	B	0.5882715582847595	And then in other situations you could have something that's very adherent to distributions to the point of being fit to a very kind of normative path.
6293160	6310900	B	0.540591835975647	But that's happening at a level that allows the different compute architectures, different information architectures and ultimately different biological embodiments to really engage fruitfully.
6311060	6321544	B	0.7884699106216431	Again looking at the variability, the diversity of biological algorithms for collective behavior which have been studied by Professor Gordon and others in so many different angles.
6321672	6332572	B	0.8190737962722778	Yet sometimes it can feel like multi agent models always start kind of like at square one, demonstrate some proof of concept phenomena, and then that is utilized as part of a bigger perspective.
6332716	6339744	B	0.5986546277999878	But it's not like that model was ever claimed to have been tuned to maximum performance.
6339792	6342100	B	0.682284951210022	It's like well, we got decision making behavior.
6342600	6351476	B	0.6597488522529602	You could transfer this to group decision making with Honeybee decision making or something like that, but there's still a big gap there.
6351658	6360532	B	0.6214169859886169	But I think what you're describing with the Kant, which is funny because it could be cannot, but also the Kant is the dialect which is spoken.
6360676	6363070	C	0.926388680934906	It was very funny when we came up with it.
6364400	6365192	B	0.9075248837471008	Great choice.
6365256	6375936	B	0.769777774810791	And it's just like yeah, because there's multiple perspectives to swap from on the classical screen because the meaning of the word is something that's happening.
6375958	6385632	B	0.7198389172554016	That fourth dimension, cognitively, the meaning of the word isn't to be found just on the blanket, just on the interface itself.
6385686	6387040	B	0.7835487127304077	That's just the communication.
6388180	6389828	B	0.7457089424133301	And that's like a bounded system.
6389914	6412656	B	0.5413556694984436	Then if you model a cognitive system that doesn't have that kind of a constraint, so represented by a map that has some kind of blanket index, some kind of blanketing if you don't embody that constraint in the statistical model, the map, you're ignoring one of the fundamental constraints of modeling the way that things happen in an embodied fashion.
6412788	6417896	B	0.542022705078125	Maybe there's some abstract space for a certain problem that's just like a total slam dunk.
6418088	6438716	B	0.5683347582817078	However, for full generality, at least to the space that we know of, biological life forms and their engagements and ecological engagements, not just like within one behavior, that space is so vast and there's so much to learn across different systems within two.
6438838	6448230	B	0.8166266083717346	Again, to abduce away into different information architectures and active inference being some subset or type of those.
6449260	6450760	B	0.9632188081741333	So it's awesome work.
6450830	6452920	B	0.865382194519043	Do you have any last comments?
6457970	6470370	A	0.5285149216651917	Well, giving ability for brands to be self aware and aware about its environment is actually something that we implemented in our last work with the BP free camps.
6471030	6498026	A	0.8947610855102539	They are indirectly aware about their environment and they are adapting to the changes in their clinical environment by indirectly meaning that they are evolving using a genetic based algorithm to just change their behavior, like how they sense the hormones when they take the steps and some other times or some other characteristics they have.
6498128	6507994	A	0.7441954016685486	So they are adapting, but kind of like not in an intelligent way, but through evolution.
6508042	6514738	A	0.8499450087547302	If you want to say, actually, we considered putting a brain in each one of these agents.
6514824	6522910	A	0.5544058084487915	But then again, as Travis and Alex mentioned, we found that it won't be practical.
6523070	6534790	A	0.6694099307060242	Actually, it would hinder our Asynchronous design because we couldn't prolize that it will take time to train each one of these brains as we evolve the neural networks.
6538560	6539310	B	0.918424665927887	Awesome.
6541140	6543760	B	0.8911897540092468	Alexander or Travis, any last thoughts?
6552670	6557950	C	0.9848214983940125	Okay, I'm just really happy.
6558020	6563290	C	0.986241340637207	I mean, I think this work is really interesting and it opens up a lot of pretty cool avenues.
6563370	6572738	C	0.7671994566917419	Again, if we can get to the point where we're evolving colonies that are producing ants and can see where that can go.
6572824	6580500	C	0.6995998620986938	So one of the big issues in neural architecture search is the whole question of what is an optimal neural network and what an optimal neural network is.
6580950	6584514	C	0.8518660068511963	Could be different for what will be different for different tasks.
6584562	6592918	C	0.5702821612358093	But not only that, even if it's the same data set, how you're using that neural network could lead to a lesser, more optimal neural network, right?
6593004	6596694	C	0.7643466591835022	Depending on what you're doing with it, maybe you need one that's more energy efficient.
6596822	6603174	C	0.6020755767822266	Maybe you don't care about energy efficiency or performance, and you'll take a slower neural network, but you need more accuracy.
6603302	6612382	C	0.7508975267410278	So being able to have algorithms which can automate this whole process for us and tune them to what we actually want to use the neural network for is really important.
6612516	6618046	C	0.823332667350769	And I think, one, just having ant colony optimization be able to optimize a network for a problem is great.
6618068	6629454	C	0.46031877398490906	But two, if we can make it such that the algorithm itself is self optimizing, it really can streamline this whole process where right now, if you're doing machine learning, it can be kind of miserable.
6629502	6633222	C	0.5126059055328369	You make a neural network architecture, you try it out, see how well it does.
6633276	6635174	C	0.9377281069755554	Oh no, that didn't do so well.
6635212	6638840	C	0.8850817084312439	Let me tweak a couple of knobs automating that process.
6639450	6643926	C	0.4598853290081024	My whole life as a computer scientist is about being lazy, but being smart about it.
6643948	6651450	C	0.7136743664741516	So whatever I can optimize so that I don't have to do it over and over again seems like a good use of my time.
6651520	6655120	C	0.5939708948135376	So I'll be smart about having to do as little as possible in the future.
6659410	6661214	D	0.6816338896751404	I don't have too much to add to that.
6661252	6676210	D	0.9446322321891785	I think a lot of the good discussion has happened already, and we talked about various implications and ways of viewing ant colony optimization from other perspectives, including an active inference point of view.
6676360	6693042	D	0.8755918741226196	So I guess really more from a closing thought on my end, is that it will be interesting, or it is an interesting direction to think about, like I said earlier or suggested, about the adaptation of the metaphor to other systems.
6693106	6700218	D	0.8940774202346802	And what are you trying to model and what's your goals from a scientific and philosophical point of view?
6700384	6701914	D	0.8758599162101746	What are questions you seek to answer?
6701952	6711498	D	0.9533483982086182	And I think it might be very interesting, again, given other developments and computing technology and ways in which you implement.
6711674	6721134	D	0.8668292164802551	The parallelization that, I think, is that's what attracted me the most to a lot of these meta heuristic algorithms, even things like particle swarm.
6721182	6730102	D	0.8448652625083923	And when I worked with Travis many years ago on the exam algorithms, you saw our names on that and working on that type of stuff.
6730156	6751658	D	0.5027425289154053	The part that always caught my attention is, again, that ability to say, I can put these entities on different processing, computing, processing resources or devices, and then they will interact and exchange their results in some way to try to optimize some often complex multi objective cost function.
6751744	6772446	D	0.5556894540786743	And so I think the part that we'll see or that would encourage the wider spread adoption of even like meta heuristic algorithms in general, not to say that they aren't used a lot in, for example, the engineering domains is again the development of parallel computing processing systems and I think exploiting things like asynchronous computing.
6772478	6775758	D	0.580466628074646	That was again another angle that caught my attention from Travis.
6775854	6781622	D	0.5162315368652344	He's done a lot of work on genetic optimization and neurovolution from an asynchronous point of view.
6781676	6789154	D	0.8719802498817444	And how can we allocate, whatever resources are available and distributing them across global networks?
6789202	6794842	D	0.7853069305419922	I think that might be the best shot to scaling up, let's say, with what we got right now.
6794896	6798460	D	0.8670910000801086	There might be again, you've mentioned, Daniel, quantum technology.
6798990	6811038	D	0.7203356623649597	Quantum computing is another interesting place that sort of like changes the barring technologies that we don't necessarily have exactly at their best at this moment.
6811204	6830386	D	0.6757499575614929	How can we take advantage of citizen science or distributed computing or peer to peer type of communication and building massive active inference systems that embody like the multi agent metaphor of ant colony optimization or other nature inspired frameworks?
6830418	6838294	D	0.7454239130020142	And can this system evolve over very long spans of time, just like evolution really worked?
6838332	6852794	D	0.6871394515037537	Another piece, my final comp and is that why I'm interested sometimes in evolution is that to me it is the inductive bias that provided us with structures that allow, for example, a human agent.
6852912	6854894	D	0.704952597618103	Babies can to operate already.
6854932	6857022	D	0.7524411082267761	Babies can already recognize faces, right?
6857076	6863774	D	0.8645569086074829	And we have certain instinctual reactions and certain mechanisms that evolution has endowed us with.
6863812	6872158	D	0.6459686160087585	And so a fascinating question is what is the interplay of the idea of simulating an artificial form of evolution?
6872254	6877246	D	0.9043378233909607	Maybe building DNA structures or very simplified computational structures?
6877278	6888242	D	0.5081458687782288	Which would answer Abdel Rahman's concern about, well, maybe we don't want the agents to be too smart in of themselves because I can't really simulate that unless you give me like a decade to run the simulator.
6888386	6894534	D	0.8437294363975525	But you could maybe come up with a more fundamental primitive and then use that as a starting point for your neural network.
6894582	6903002	D	0.8929011225700378	Let's say, Daniel, you want to do some task in image segmentation and like okay, but what can your evolutionary framework give me?
6903056	6905566	D	0.8446776866912842	Well, I'll say here here's a template to start from.
6905668	6911114	D	0.8293855786323547	This is a kernel on which you build your framework and it's like a DNA structure.
6911242	6917070	D	0.5117009282112122	And this has evolved across many years of distributed peer to peer computing.
6917150	6929246	D	0.677251935005188	And you could imagine building this mammoth evolving, continual learning style evolutionary algorithm, whether it is based on genetic algorithms or ant colony.
6929278	6932262	D	0.830467164516449	And you could imagine that might be an interesting way to think about.
6932316	6955558	D	0.5224774479866028	And by the way, I am spitballing and generating an idea of how I could envision a scalable form without inventing a new computing system that I don't know will or will not exist because quantum has a lot of problems still to solve too, like superconducting or super temperatures or trapping photons, as I have learned.
6955734	6957782	D	0.8398436903953552	So that might be an interesting direction.
6957846	6963230	D	0.792689323425293	I think the scaling of this, especially from the practical end, is going to be the most important.
6963300	6963726	C	0.5953644514083862	We're going to.
6963748	6968206	D	0.8037808537483215	Need to pull together all the tools that we have, as I mentioned before.
6968308	6972960	D	0.5873117446899414	So I'll stop there, too, because I'll let Abraham blame more, so hopefully that made sense.
6973970	6979702	B	0.9899235367774963	Well, this was very epic and inspiring, so good luck with the work.
6979756	6991474	B	0.9721391201019287	You're all welcome to suggest another piece that we might focus on or continue the discussion however you see fit, because it's super interesting direction.
6991602	6992790	B	0.9191112518310547	So thank you.
6992860	6993942	B	0.6996943354606628	Till next time.
6994076	6994822	D	0.8529649972915649	Thank you.
6994956	6996120	C	0.9793016910552979	Thank you so much.
6996490	6996770	A	0.5137446522712708	Bye.

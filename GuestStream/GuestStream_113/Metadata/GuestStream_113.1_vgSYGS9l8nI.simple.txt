SPEAKER_00:
Hello.

Welcome.

It is June 26th, 2025, and we are live in Active Inference Guest Stream 113.1.

Welcome, Jack, as well.

We are with Sergio Jaques, and we will be discussing Bayesian mechanics of economic choice, computational foundations of economic behavior,

There will be a presentation followed by a discussion.

So thank you again for joining and looking forward to the presentation.


SPEAKER_02:
Thank you for joining, Jack.

And thank you for the Active Inference community for inviting me to share my current research.

As Daniel said, I am Samuel or Sergio.

I have both names.

I am residing in Mexico City.

I am a second year PhD candidate in artificial intelligence.

And my current research basically focuses on applying the Active Inference framework

to understand economic choice and financial decisions.

Just to notice that this is joint work with Ernesto Moya, PhD, my thesis supervisor, and Luis Alberto Quesada, PhD as well, which are two Mexican academics as well.

Well, let's begin.

basically i'm going to be speaking a lot about this paper that we recently published but i'm also going to give you a brief introduction about the main things that we're trying to bring together which are basically topics from neuroeconomics topics naturally from basin mechanics and the active influence framework and topics from

classical or neoclassical economics and psychology so we're given that we are trying to merge all of those fields and as you probably know here in the community the free energy principle is very interdisciplinary itself so i think i would take some time to explain the intellectual journey of these different fields and how we think they nicely come together

So we'll start with economics, then we would move to how psychology meets economics, then a little bit of the thermodynamics of information, both for neuroeconomics and active inference, and then how the divisive normalization model from neuroeconomics can be derived formally from the free energy principle.

So that's the basic or the key insight from our paper.

And finally, I'll share some of the future directions for the future work that we intend to do for this year or the end of this year.

So let's start.

So I would like to start by sharing the

let's say the intellectual journey of how economists usually think about valuation and about encoding value.

So let's start with a Swiss mathematician, Daniel Bernoulli.

And as you probably know, he's very well known for creating classical utility theory or expect or classical expected utility theory, which basically said that that

Economic agents or people don't encode absolute monetary values.

We don't encode absolute monetary values.

We encode what he called moral value.

And moral value was just a way of saying subjective value.

so monetary values are i mean are subjective on how much we want something on the time we are on the on the place we are so basically that's what he formalized and he formalized it as a weighted sum of expected probabilities and payoffs

Now, later on, very famously, when Neumann and Morgenstern published their famous work, Theory of Games and Economic Behavior, where many of you listening probably have heard it because that was the foundation of game theory.

But one key insight was that when they published this work, they just expanded Bernoulli's expected utility theory to account for the specific axioms that economic agents should abide to.

in order to be what we usually came to refer to as rational, to make rational decisions or rational choices.

So these axioms were that our decisions should be complete, they should be transitive, and this is very, very important.

when i prefer a to b and b to c uh we should follow this this this ordinal uh property and our choices should be value independent independent of the other choices independent of past choices so that was another another choice

And if our rational agent or our economic agent abides by these axioms, the key point here is that what naturally follows and can be derived mathematically is that this agent would be maximizing its expected utility.

And this is key because this is another way, for example, in the active inference slang or language that

this economic agent would be pursuing a goal so it's another way of saying it it's going to be focused on goal directedness so so just keep that in mind and we will see how that impacts us later so this this framework uh basically impacted

all of neoclassical economics and modern economic development and another key aspect of von neumann and morganston's theory is that they thought of the probabilities of events and the payoffs as objective so

What do we refer with that?

For example, if we are going to roll a die, we know the exact probabilities and we can compute them exactly, analytically.

So that's pretty much what they meant, that we know the objective probabilities of specific economic outcomes and we know the payoffs in advance.

However, naturally, you can start thinking that that's rarely the case, with the exception of, for example, specific random games or casino-like games.

Later on, Paul Samuelson, which

probably one of the, let's say, leading figures in economics, in your case in economics, came to say that we really don't know the preferences of economic agents or economic agents.

I mean, we cannot get inside their brains to know their preferences.

So in his paper, Consumption Theory, in terms of revealed preference, he basically said, we cannot know the preferences in advance, but we can observe their choices and infer their preferences.

So that was the first type of a bit of Bayesian approach to economic decision making.

But later on, Leonard Savage, or better known as Jimmy Savage, famously published the Foundations of Statistics in the 1950s, where he established subjective expected utility theory.

meaning that he established that we definitely don't know the preferences in advance and we don't know the objective probabilities.

The only thing we have is beliefs, beliefs like in the active inference framework.

So those beliefs are subjective probabilities.

However, one important aspect here is that even though he established that we will have these beliefs and these objective probabilities, these would be a normative theory.

So normative theory is a way for economists to say rational.

because many many people not coming not acquainted with the theory of economics uh hear rational and they just think of i don't know like captain spock or something like that so when we say rational we really say normative and what we refer to is that the economic agent is making logically consistent choices abiding by these axioms transitivity independence uh completeness and we can also add continuity all right so that's basically how

rational choice theory evolved from its beginning and pretty much up to nowadays.

Now, then

many, many other researchers and economists came to say, all right, so that's the theory, but let's observe how do people actually behave.

So let's build a descriptive theory, not a normative theory of how people should behave.

And the first key figure in this story, it's Maurice Alai.

uh which was a french economist uh he also was a nobel laureate in economics and he came up what later became known for uh as that the allied paradox so the allied paradox basically said that

many sophisticated decision makers including leading economists like jimmy savage consistently violated the independence axiom that we just referred to so so what what did that mean that basically meant that

when facing different choices and rearranging the menu of choices or the opportunity set of choices, even Nobel laureate economists didn't abide by the independence axiom.

So that was one of the first, let's say, empirical validations that the axioms of rational choice theory were consistently violated, even by the people that came up or that developed a theory for them.

Later on, and most famously, Kahneman and Tversky introduced psychology into economics.

So that was the point where psychology formally met economics.

And they came up with a formal descriptive theory, which was later called prospect theory in the 1980s.

So basically, prospect theory was

an empirical theory contrary to the previous theories, which were mainly theoretical.

And they literally observed how people make economic decisions on their uncertainty.

And they came up with the graph here that we have in the right-hand side corner,

which you can see that has a sigmoid function.

So let's contrast that function with a classic utility theory function from Bernoulli.

So you can see that for the classic utility function from Bernoulli is basically explaining what we usually learn about in Economics 101, which is diminishing marginal returns.

we experience diminishing marginal returns.

That's a basic utility function.

But what Kahneman and Tversky find out was that that was not how people behave in reality.

They observed that they behave like this with this utility function, which is this sigmoid shape utility function.

And the key aspect of this utility function is that there's a discontinuity between gains and losses.

They more or less found that

uh people read or the brain registers with twice the intensity losses to gains so that was a key key insight and a key violation of traditional rational choice axioms uh so we can speak about a discontinuity between gains and losses uh and

Popularly, this theory came to be known as the, let's say, irrational argument for economic behavior or economic decision making.

But the thing was that many economists pushed back against this and many other decision scientists pushed back against this because this theory essentially said people behave and take irrational decisions very often.

They are subject to biases.

They take a lot of their decisions with heuristics that make them fall into traps.

So essentially, it was saying that people are not very smart.

People are idiots, even.

So that's what they said.

Now, later on, Richard Thaler and Loewenstein and other relevant figures who will also would be well in the case of Richard Thaler was also awarded the Nobel laureate in economics, pretty much imported prospect theory formally into economics and established what we now know as a field of behavioral economics, which is accounting for these

uh biases and heuristics into economic decision making and pretty much checking how consistently these biases and heuristics make us violate the traditional axioms of neoclassical economics however however

They, I mean, everything was like Paul Samuelson said, we infer the preferences, we infer what is happening in the brain by observing behavior.

It was still not a neurophysiological theory or anything like that.

So that was pretty much how we have been

uh evolving now let's move let's switch breaks and let's move to some other things happening specifically in computational neuroscience so in computational neuroscience more or less in the same along the same time there were a lot of very interesting discoveries so famously simon laughlin's 1981 study

of fly photoreceptor neurons demonstrated that the optimal computation on their constraints on their energy constraints would be one that looked like this where where the fly or in this case that the flies neurons photoreceptors would spend more on

representational resources on the parts that were more common or more likely to appear.

Meaning that, for example, for certain light intensity that was more common to appear in its environment, the fly or the flight photoreceptor neurons would encode those, would give a higher probability of encoding that level of light more often than very rare events in its environment.

So basically what we are seeing is that the fly photoreceptors are mapping

the environmental statistics in order to optimize or to compress the amount of limited energy it has in its brain.

So as we know, neural computation is very expensive.

The brain, even though it's only more or less 2% of the body's weight, it uses about a fifth of the body's energy.

So naturally, that means that it's very metabolically costly, and it needs to be very, very efficient on using that energy for its representational capacity.

And also, we can bring evolution and adaptiveness to the mix because naturally, the fly, as long as it performs this type of action and other type of actions better, is going to be able to survive.

I mean, it's just going to be able to adapt

more optically.

Then we can also introduce Landauer's principle from Rolf Landauer, who said, decreasing stochasticity in choices decreases information entropy.

and layer the famous InfoMax principle, which says the brain optimizes mutual information between output and input.

So that basically said, we have, for example, for the fly, the fly has an unlimited number of pixels or the stimuli that it has to encode, but it has a very limited

input capacity so it has to compress that and it compresses that by assigning different weights and assigning higher probabilities to the in this case the light intensities that that came that came up more often and when we take the integral of that function we can see that we're going to see this sigmoidal this sigmoidal shape or the sigmoidal cdf so just remember that because that's going to come up later

All right.

So how can we

take these insights and bring them back to economic decision-making.

So if we assume what rational choice theory said to us, that we essentially make decisions with uniform precision, we don't have information asymmetries, as neoclassical economics said, then what that means in terms of information processing is that neoclassical economies like von Neumann and Morgenstern assumed that there was no cost

in representation so and we can translate that uh geometrically as uniform precision and you can see as you can see in the graph so without depending on the reward magnitude the fine rate is going to increase just linearly

However, the reality is very different.

The reality is that if we put some noise into the mix and the noise is represented by that gray area, you can see that the reward magnitude by adding just a little noise is going to be huge.

It's going to be huge.

So it can be 20 or it can be 40.

So it's huge.

So what this means in terms of Laughlin's fly experiment is that

assuming that this was the brain how the brain was encoding value the brain would be very wasteful on encoding all of the possible rewards at the same at the same rate or at the same probability so that would be very wasteful in terms of energy for the brain so the reality as we came to know is that

that the optimal way of allocating energy resources for the brain is to allocate them for those events that came up more likely.

So that's a key aspect.

Now, another important thing is that this also tells us that the violations commonly discovered by Alai, Kahneman, Tversky, and other behavioral economists perhaps weren't actually violations to rational choice theory.

Perhaps rational choice theory was wrong.

And in reality, those decisions are in fact optimal, are in fact normative, but normative under the assumptions that we have thermodynamic constraints, that we have a limited representational capacity.

So if we assume that we have those limits on representation, then those decisions become rational.

All right.

So then how should value be represented in the brain?

Let's remember from Barlow's efficient coding hypothesis, neural systems evolve to maximize information transmission while minimizing metabolic cost.

So let's see the sigmoidal curve in the panel C.

So that's basically the sigmoid function that we just showed you.

And notice that it's remarkably similar to the value function that Kenem and Tversky discovered through the behavioral experiments, the very well-known loss aversion sigmoidal function.

And also notice that the optimal precision allocation produced a curve that is a reference-dependent response function.

um so basically what what this means is that what behavioral economies interpret as cognitive biases might actually be uh an optimal representation or an optimal computation of the brain of value under biological constraints so basically we can see that notice the what i just told you the gray area if we assume uniform precision in the lower right hand side corner in b

the noise made that the reward magnitude was huge.

So the amount of energy expenditure is going to be huge.

However, if we assume the sigmoidal function, which that would be the function that we would get as we saw with Laughlin's under thermodynamic constraints, the noise is very small.

The reward magnitude is very small.

So that's basically the evidence or the, yeah, I mean, the demonstration that the brain

is very efficient when using that sigmoidal shape instead of using that uniform shape.

All right, now moving to neuroeconomics.

Basically, neuroeconomics arise mainly from combining neuroscience, psychology, and economics.

And one of the leading figures I'm going to tell you about, well, some of the leading figures, let's start with David Hager.

David Hager, which was, if I remember correctly, at NYU, came up with other authors like Simon Shelley and Carandini.

with the famously divisive normalization model.

So what is divisive normalization?

So divisive normalization is basically a computational algorithm that the brain uses to, well, originally it was used to explain how the brain or how the

that V1 primary visual cortex encode the visual information.

So basically, if we look into the graph, what Higer and colleagues pretty much found is that neurons in the retina basically encode information, but they don't encode it, as we saw with Laughlin's, equally or uniformly.

What they do is that they divide the specific input or the excitatory input that we want to see

it divides them by that pool of surrounding neurons so so by dividing them by the that pool of surrounding activity of of neurons what it's basically doing is it's subtracting it's taking out that redundant information so that's pretty much what it's doing when dividing it by that by those surrounding neurons so that makes the again that makes the the brain or in this case the neurons in the retina much more effective and much more efficient

And if we see how the value encoding looks like, so we can see that the graph in the right looks pretty much the same as Laughlin's graph for fly photoreceptors.

Later on,

other neuroeconomists came to see that that divisive normalization algorithm not only worked for accounting for information in the primary visual cortex, it also worked for olfactory information, for sound,

and also for encoding value in the posterior parietal cortex.

So basically, what later came to be known is that it appears that device normalization is a canonical neural computation and not only for vision.

And later on, Louis, Ken Louis, showed that value representations in the brain can be well-described via divisive normalizations.

Also by this time, neuroeconomists have founded the main brain areas that are probably responsible for the main activities of encoding value, specifically the ventromedial prefrontal cortex and the striatum.

So even though there are other brain areas that also play a role, for example, the amygdala lines of cases and the like, the ventromedial prefrontal cortex and the striatum definitely have been shown to be the key areas involving value representation in the brain.

Now, later on, Paul Glitchmer here, which is also a professor at NYU, he actually directs the Neural Institute at NYU, came to, I mean, just kept working on this work also with David Hager and pretty much imported divisive normalization from neuroscience, from vision neuroscience into economics.

And later, with joint work with Timula, which is now, I think, in Australia or in Israel, later they published their work on expected subjective value theory.

Now, notice that subjective value theory looks a lot like Savage's subjective value theory.

But this is a neuroscience-based subjective value theory.

So even though it looks a little the same, it's not the same.

so basically they he established he took this device normalization model and established that and he used this formula to say that the brain can basically

well, that value representation in the brain can be shown to follow this exact formula.

So what this formula is saying is subjective value, SD, is dependent on the excitatory value, the value that I'm trying to evaluate or the payoff, divided by a fixed constant,

plus what we call the payoff expectation, which is the normalization factor, which in the case of vision neuroscience was the sum activity of the pool of neurons.

But in an economic interpretation, it could be the alternative options.

So if we have a menu of economic options, it will be the average sum of those options.

If we are speaking about intertemporal choice,

it could be the weighted average of those payoffs across time.

So basically, again, what we are saying is,

is that the value representation in the brain is context dependent.

So what this model is basically bringing to economics is context modulation, context modulation in economic decision making in the brain.

And I mean, just for you to know, in the standard divisive normalization model, Higer used usually FR,

instead of ST to represent that ST can be interpreted as neuronal firing.

So it's literally a neurophysiological model for economic value representation.

All right.

Now, moving on.

If we dig deeper into the model, we can see that it has two key parameters.

It has alpha and M. So what alpha is in this type of model, it's considered a free parameter, but in an economic context, it can be considered, for example, the risk tolerance.

Because that parameter alpha controls the shape or the curvature of the function.

So notice that if we increase alpha,

we get pretty much in the in in panel a we get bernoulli's utility function the same function that daniel bernoulli discovered like 200 years ago if we increase alpha we get that's that type of shape and then if we decrease it a bit we get kahneman taversky sigmoidal function

So the key aspect here is that this model is a more general model because it allows us to account for different utility functions that have come up in both neoclassical economics and behavioral economics.

But again, this is a

neuroscientific neurophysiological representation uh it just happens to look like neoclassical and behavioral economics and also for m which again is the pay of expectation uh what it does is that it moves the the mean the center of the of the of the utility function so we can move it by increasing m

And what that means in terms of Bayesian decision-making is that depending on my experience, on past outcomes, on my past preferences, that's going to shape, that's going to move the shape or the center point of my objective value utility function, right?

Yep.

Now, moving on into the active inference framework.

I mean, this is going to be pretty familiar to many people here listening to a talk in the community.

But for those maybe not so well acquainted with the free energy principle, we can trace back, let's say, the intellectual origins of the free energy principle to Helmholtz, who basically

discover that that we have as we have as individual unconscious inference and this might might sound very similar to what paul samuelson said that we cannot look inside brains to see what preference of people the people have that was pretty much at the same time i think but he was speaking about neurophysiological processes later on hinton and mumford uh came up with the first energy-based models

which later became very famous, for example, with the Boltzmann machine.

And those were the very first models that were energy-based and that considered hidden or latent variables.

So that's a key aspect.

And specifically, Moonforth came up with the Bayesian approach to vision.

Later on, predictive coding with Rao and Ballard, pretty much, or predictive processing, laid out the ground for later on Professor Friston to come up with a free energy principle and its, let's say, practical framework, which is the active inference framework, and later on with Bayesian mechanics.

So the key aspect here is that we can regard economic choices essentially as predictions about future outcomes or beliefs about states on their uncertainty.

Because even if we are going to make an economic decision in the present,

we're gonna see we're gonna think about future outcomes about how much would we like this chocolate this coffee what if i spend my salary in this or that how is going to impact that on my finances so almost all economic choices naturally inevitably have to do about how to do with the future and have some uncertainty embedded in them now

For those who maybe are not acquainted with the free energy principle, but for those who are, I think it's never wasteful to go again to explain it.

So basically, what the free energy principle claims or says is that if we observe that things exist, if something persists over time and its identity can be re-identifiable in the environment, it has a given structure, then

It must be true that it must encode or instantiate a statistical model of its environment, or we can call it a generative model, an internal model of its environment.

There we have the famous free energy principle formula.

And the key aspect here is that

the free energy principle has two aspects.

It has an accuracy term, which basically that could be rendered as we're going to see later as the expected utility in economics, expected utility theory, the log evidence.

And that's basically how accurate are my predictions.

I wanted to have the most accurate predictions.

So that's basically the reward in terms of reinforcement learning.

And in terms of the complexity term, the complexity terms brings into the model an entropic term.

So what does entropy bring into the model?

It brings flexibility.

it brings flexibility for example in the economic context flexibility for preferences we have to we want to be flexible in our preference because we have uncertainty in the world so perhaps now i like something but later i will not like that and i would prefer x to y so basically the model can account for

for biases that were regarded in behavioral economics, like intransitive decisions or just flexibility in preferences.

Now, the question would be, does the expected values theory that uses the devising normalization model from neuroeconomics can emerge from the perceptual inference part of the free energy principle?

So that's the question that we are asking.

And why specifically the perceptual inference in the perception action cycle of the free energy principle?

Because even though

active inference is well the active part or the action the action part of the cycle is very important because when we have references we have preferred outcomes we're going to be looking for information we're going to engage in information gathering activities

However, the initial value representation in the brain is going to be perceptual.

It's going to be perception.

So we can say that if the model exists, it's going to be living in the perceptual inference section.

And later on, we can add the information gathering aspect through the action component in the model.

So as we mentioned, what active inference does is that it assumes that agents, and in this case, economic agents, are not passive observers.

They're going to look to fulfill their preferences, their outcomes, and to have better information to make better economic choices.

Now, as we just saw, if we take out the complexity or the entropic term from the free energy principle, what we get is just the expected log probability of our preferred outcomes that can be more or less approximated by the expected utility in economics.

So that formula, that expected utility in economics, was basically what von Neumann and Morgenstern formulated almost 100 years ago.

So basically, if we think about this model in an economic context, what we call preferences in economics can be regarded as prior beliefs or preference distributions under the free energy framework.

Now,

What is that?

I mean, how is active inference useful for economics?

Well, the first thing is that it can unify the epistemic value or the information gathering aspect of economic behavior or strategic behavior and decision making.

Also, as I just mentioned, epistemic versus pragmatic actions.

So, I mean, economic agents and strategic agents need to evaluate the cost of gathering information versus increasing their position.

So again, as we mentioned, having a limited precision is not viable.

Having more

well, less stochastic or less noisy decisions is costly.

So we need to evaluate the cost or just weight the cost of those decisions.

And finally, as we also know, there's a lot of research coming out on the free energy principle and active inference applications to artificial intelligence systems that this framework allows us to track back

all of the specific decisions and all of the specific preferences and pretty much all of the parameters in the model.

So contrary to other machine learning techniques like neural networks, this is not a black box.

This is a transparent model that allows us to trace back every decision made in the model to the specific parameters of the model.

So it's a very transparent and explainable decision system if implemented specifically in an AI system.

all right so moving on to the key uh key let's say contribution of the paper so what we just we just uh we showed is that if we if you start from the free energy principle and expect free energy you can derive from that

the divisive normalization model as assumed by the expected subjective value theory of Klinschmer and Timula.

And basically, if you do all of the math, you can get to the free energy minimization and get M, like the internal model or the mean on their perceptual inference,

as that formula.

And we can see in the lower box that that formula and each component of the formula can be mapped directly to the expected subjective value theory or the divisive normalization model.

So what the lemma says is under a hierarchical generative model with Gaussian priors and likelihoods,

perceptual inference through the free energy principle converges to our representation, which is that representation there, which is equivalent to divisive normalization.

So what is going on?

As we said, the upper part of the model is basically the predicted value, the excitatory value that we are trying to encode.

And we are dividing it by a baseline plus

weighted average, which in this case is a precision weighted prediction error minimization term.

And again, what we are doing is we are subtracting, we are taking out all of the irrelevant information that we already know.

We already know from our experience, from past outcomes, or we just don't care.

It's irrelevant information.

So we are taking it out by making that division in order to just focus on the specific value that we are trying to encode.

so in short what we are doing is that we can show that the free energy principles allows us to account for context-dependent economic decisions and also reference-dependent economic decisions as posited by behavioral economic theory but in this case again is not

a bias in is not a heuristics per se is actually an optimal decision it's a normative model so in the case of uh of the economic uh let's say language this is a rational model of economic decision making but it's rational by assuming thermodynamic constraints uh so in that sense it becomes rational

so we're really we're really excited about it about this because what it says is that from neuroscience we can derive some of the let's say of the the the oldest paradigms in economic theory but we can test them and validate them both mathematically in this case but also empirically because i didn't mention that but the divisive normalization algorithm

has been tested empirically in humans and in non-human animals as well from I don't know like monkeys and I think even bees so basically we can see that the device normalization model is a very robust model and as many people in the community here in the activist community know the free energy principle as well can be mapped into a neural process theory so moving into that

Basically, the future directions of our work is that we want to operationalize this work into the practical version of the active inference framework, which is into a graph, into a partially observable mark of decision process.

Basically, what we want to build is an agent that makes economic or financial decisions in the market.

And how is this model better than, for example, a transformer type of architecture that does this type of economic decision making?

Number one, it's the traceability, as we just mentioned.

When we use transformer-like architectures or neural network architectures for things like financial decision-making or things like health decision-making, it's very sensible because we cannot really account for what the model is doing or how the model is coming up with its output.

In this case, with this type of graph in focus, we can have traceability of the system.

Number two, everything is explicit.

All of the parameters are explicit.

So that's another thing.

We can, for example, make beta and gamma and the rest of the parameters very, very explicit.

For example, we can use gamma as investor confidence.

We can use beta as decision uncertainty in the model.

And finally, we can use the Bayesian update belief equations.

who account for adaptive learning because naturally when you become an investor or for any other type of economic decision setting you learn as we just mentioned you learn about past outcomes if you have bad outcomes while investing you usually become more conservative uh your risk tolerance changes your priority and financial objectives change with time so

So basically, this model also allows us to have that flexibility for how the agent is adapting in the environment.

So basically, this is a future work that we're currently building, and we hope to have empiric results perhaps later this year or at the beginning of 2026.

So this is pretty much the future work we're working on.

And yeah, and that's pretty much the end of the talk.

We'll open up the space for questions.

Thank you, Daniel.

And here's my contact and my LinkedIn as well.

If anybody listening wants to add me or contact me, I'm all ears.

Thank you, Daniel.

And thank you to all for listening.


SPEAKER_00:
Thank you.

Very cool.

All right.

Well, those watching live can write a question in the live chat.

Let us begin with Jack.

Would you like to say hello and give any initial reflection or a question?


SPEAKER_01:
Hey there.

Yeah, I mean, I really find it fascinating that what we might consider biases in a lot of economics kind of turn out to be optimal.

You know, one thing I...

want to understand more is why is it important that you can derive some of these up-to-date economics models from active inference like what does that say that they are so related um and um


SPEAKER_02:
it natural to think that they would be like when you set out you think maybe like maybe this won't work like tell me a bit more about the fact that they are so related yeah well i think that um a long-standing inspiration in the economics field is to make economics as scientific as possible within the social sciences so historically economics imported a lot of things from physics

like thermodynamic equilibrium and many other things, just precisely trying to make it as scientific as possible.

But what happened is that it became highly technical and highly mathematical, but always assuming that economic agents behave as if, so theoretically, so just like a highly theoretical field.

So what happened initially with behavioral economics, and I think can happen now with neuroeconomics and by importing tools like the free energy principle or methods, is that we can make economic theory, we can validate or we can test economic theory in real life experiments.

Or in my case, also mathematically,

but mathematically like the Pythagorean theorem.

I mean, even though we can demonstrate the Pythagorean theorem mathematically, we can also test it in reality.

What happened with economic theories is that we prove them mathematically, assuming certain very implausible axioms, and later we just said, well, it's demonstrated, but assuming those axioms.

So what this allows us to do is to

prove it mathematically as well but by accounting for a in this case for a narrow process theory that can be testable in uh in empirically so that I think that that's a key a key insight and in this case a free energy principle uh I mean if we as myself and I think of many people listening in the community assume I I don't know I mean I don't want to use the the word true but it's a very good model to explain how uh

to explain self-organization and how things exist if we think that that's a very useful model then it should apply to this as well so that i think that's another thing on the second part of the question thank you i'll ask a question then jack you can you can go again or i'll read one live chat so


SPEAKER_00:
does this help us think about the microeconomic macroeconomic relationship like what what do we get from this cognitive approach or from bayesian statistical approaches that might connect those two differently than how they're understood to be connected now yeah yeah


SPEAKER_02:
I mean, a recent trend that I think has been going on in the last decade perhaps in just standard economics is that there has been, let's say, I mean, let's just say that macroeconomics has,

has decreased in popularity.

So for example, famously, MIT is no longer requiring to take microeconomics as a mandatory course, even within the economics major.

So that tells you that, in a sense, we are migrating to, yeah, I mean, just to explain economic in the individual level and then trying to go up to add them up.

So I mean, at least from my perspective,

The future of macroeconomics, and again, that's my personal opinion, would be starting with these type of empirically grounded microeconomic models.

So we can regard these type of approaches like a microeconomic model, and then moving it up in a microeconomic setting, but relying on complex systems, in complex adaptive systems.

So in that sense, again, I think that the pre-energy principle blends very nicely with that type of approach.

So that would be my opinion on that.


SPEAKER_01:
Go ahead, Jack.

maybe I have a more general question and it might not be answerable by you but I wonder why biology would be so optimal um I guess um because you know when economists do experimental research and they find oh maybe people have biases and stuff maybe maybe that makes sense like why would we

be optimal but then kind of what you're saying is well we are optimal given the constraints that we have the fact that we're biological and we don't have infinite energy and things like this so um i don't know if that's a question but more a comment of um i just don't understand too much about biology and like how optimal is it given the constraints that we have


SPEAKER_02:
Well, I mean, what I would, I mean, I think Daniel is very quick to answer that, but what I would say about that is, for example, the other day I was reading a paper from Maxwell Ramstead and he said that

that we can regard the free energy principle as a sort of generalization of the second law of thermodynamics.

So in that sense, what we could say is that for things to exist and not to dissipate, they need to be extremely effective.

so it's very i mean just for the loss of physics is very unlikely for very complex systems like us and brains like us to exist who came to be and to and to actually i mean kept existing it's very unlikely so in that sense i think that uh i mean we can regard uh

our structure our this decision making systems i mean just how we function as very optimal given the the the unlikelihood of us to to exist uh specifically given that the complexity that i mean the complexity of our structure so that would be my claim but but but surely i'm not a biologist so i don't know daniel if you have something to say about that


SPEAKER_00:
Yes.

So that history you told of the decision-making and the way that that was great as a non-economist, and it reminded me of optimal foraging theory, where sort of a simple version of optimal foraging theory would be like, how much...

resources are being obtained per unit time then once you start to bring in different trade-offs like water loss or the costs of movement you start to get closer and closer to empiricism and then the sort of death blow to that just seeds per minute

measurement is the subjectivity of beliefs like well there's predation risk so you could ask is this behavior optimal given predation risk but then in a way when you get to the subjectivity of predation risk

That's like kind of asking, well, is this an optimal investment strategy given the volatility profile?

And it may or may not be.

There may be a better performing trade strategy given a market condition.

But in a way, once you get to subjectivity, which is the sort of nucleus of decision-making, you get to that tautology.

which is like, well, they're acting as if these are the conditions.

So given their subjective evaluation, they are acting in this Bayes optimal way.

And so in a way that sort of like,

fake out almost like oh it's rational but that's so much of the debate that at least i've heard from afar is this debate what is rationality are are people in their decision making rational or not and it's like when we're doing bayes optimal modeling even if it's going to be a pathological state it's like there's something about the parameterization

where it is optimal for this repetitive behavior to arise.

It's not that everyone around this person or the person themselves wants to be having this thought or action per se, but optimal doesn't mean that.

It means like the ball is at the bottom of the bowl, but the bowl shape.

So it's like the starting point for Bayes optimal modeling

is that by taking constraints into account including interior constraints subjective constraints and evaluations then it's tautological and not really an account to say that it's optimal it's just sort of things are how they are and all the work goes into structuring and doing the inference on what those constraints are and so it's like the framework says less about what's optimal

but it lets us look at traders with a variety of portfolios and say we're already going to take it as a granted that they are acting in their own base optimal way so what is the structure and the parameterization that enables that and in that way feels like it moves forward not

trying to imagine a time before there was this debate of rational or irrational, but just as you structured the presentation, moving to a place where we can empirically ask, what are those objective and subjective constraints that make decision-making actors act the way they act?

You're in the AI program.

So just how did that come to be?

And how did you come to this economics topic in that program?


SPEAKER_02:
Right.

Basically, my background originally is in finance or let's say financial economics.

I later pursued graduate studies in engineering mathematics.

And while at that, I did my thesis on complex adaptive systems or complex systems engineering.

And that inevitably or eventually led me to active inference on the free energy principle.

And at the same time, well, when I became very excited about that, like, I don't know, like three, four years ago,

I also pursued the graduate studies in psychology and focused on mathematical psychology.

So that was like the natural bridge for me to go into the free energy principle.

So later when thinking about

what field I wanted to pursue for my PhD I thought well I mean usually economists are very I don't know I don't know why but they usually work like a lot in silos I mean they work on things like I don't know like law and political science and sociology but not so much in other areas like I don't know like not not so much in technology and stuff like that usually

uh also in finance i mean they just don't work in these type of things uh and in psychology i mean we i didn't have a i mean at least here in mexico we didn't have we i don't think we have a very strong mathematical psychology program so the only place i thought i could merge decision sciences cognitive science and and my interest in economics was in ai

And given that the recent literature, or I mean, I don't know, the last five years, the literature that has been coming up about applying the active inference framework for practical applications, I don't know, like inverses and stuff like that.

I just thought that that was like the perfect route for me.

So that's pretty much the story.


SPEAKER_00:
which economic actors or firms would have a different day-to-day experience how if these kinds of relationships were better understood well i think that for example on the type of agent i am i intend to develop


SPEAKER_02:
one one reason why i mean i don't know if you you have a different opinion but uh i haven't seen many ai agents for economic advisory i mean there's some there's some there's some robo advisors but they're very let's say very simple uh and again the reason why they're they're very simple i think is that it has been proven and it has been tested that llms cannot plan

so for example the other day i was listening to a talk from professor carl friston and he said that at least for him an agent for it to be an agent needs to be able to plan so i i completely share that insight so if he if it could mean if it has been proven that llms cannot plan then at least for me they are not very useful for financial advisory because the key for financial advisory is financial planning

So in that sense, I think that looking for other type of architectures, in this case, the partially observable market position process that is proposed by the Active Influence Framework, could be better for that.

I mean, just for the fact that it can plan.

and second because it's explainable and as i was saying a key aspect also about or the key disadvantage of using transformer-like architecture for some sort of financial advisory is that it's not explainable so if somebody loses all of their savings so what's going to be the explanation or just hallucinated or something like that so i think that that's a practical application


SPEAKER_00:
There will be some very interesting economic arenas and futures with all these different kinds of agents making different belief updating and policy selection.


SPEAKER_02:
Yeah.

Yeah, policy selection as well.


SPEAKER_00:
Jack, do you have anything else you want to ask?


SPEAKER_01:
No, not really.

Thank you.

It was really interesting.


SPEAKER_00:
So what else will you do in your PhD?

Hypothetically, where would you like to take in the second half of the PhD?


SPEAKER_02:
Well, basically, finish building this agent and hopefully get to test it with real people.

So that would be my intent for the next couple of years.


SPEAKER_00:
Anything else you want to add?


SPEAKER_02:
Well, just again, if somebody wants to get in contact for, I don't know, any type of research, opportunity, collaboration, or anything, there's my email, my academic email, and my LinkedIn profile as well.

So happy to collaborate.


SPEAKER_00:
Cool.

Well, thank you again for the very cool presentation.

Absolutely.

Thanks a lot.

Okay.

Until next time.

See you.


SPEAKER_02:
Thank you.

Bye.
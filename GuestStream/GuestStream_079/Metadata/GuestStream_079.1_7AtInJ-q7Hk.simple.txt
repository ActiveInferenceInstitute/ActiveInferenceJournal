SPEAKER_00:
hello and welcome this is active inference guest stream 79.1 on april 3rd 2024 we're here with reoto kanai talking about meta representations as representations of processes we'll have a presentation then a discussion so thank you very much for joining and looking forward to hearing the presentation and talking


SPEAKER_01:
so go for it yes okay right yeah yeah thank you very much for this opportunity to uh present our recent work uh yeah and thank you for supporting our very recent paper which uh just uh uploaded preprint so um so in this talk i want to talk about what meta representations are and but uh but to

Explain my motivation.

I want to start from my frustration in theories of consciousness.

A lot of theories of consciousness tend to be described semantically.

For example, in today's talk, I mainly focus on the higher order theories of consciousness.

But it kind of makes sense at the semantic level, but when we think about how we can implement such high-level theories with neural networks, we realize there are a lot of uncertainties about how we might be able to implement them.

But I think this is something we can call a constructivist approach.

So by thinking about how to implement theories, we can actually reveal vagueness and make the concepts more precise.

So that's the main topic.

and so so as i said uh so i want to talk about higher order theories of consciousness maybe many of you might have heard of it uh but just uh broadly speaking uh a higher order theory of consciousness uh claims that uh mental state is conscious uh not just by having

the processing but it becomes conscious if it's represented by a higher order mental state so by higher order mental state or higher order representation generally it means it's a representation about another representation so

so intuitively i think this makes sense so when uh so for example when your part of your brain is processing something red uh you know people ask whether that's sufficient for consciousness conscious experience of seeing red

but but when you have an additional representation that represent that you are processing something red so that kind of meta representation seems to be functionally related to awareness so that kind of makes an intuitive sense but

But once we start thinking about how we can implement this sort of meta representation, it's very unclear what it really means.

So we try to find a way

where we can sort of represent the meta representations.

But let me explain why it's difficult.

Okay, so this is a very naive way to think about

constructive meta representation so let's say you have a like image as an input and then there's some processing in the brain or deep neural networks then you can represent uh the contents of the input or an image but but if you transform this first order representation by another neural network uh that's like really naive way to implement meta representation

but but with this uh you can easily have like many meta representations so you know you uh let's say like uh if we think of this as the uh like visual information processing in the brain maybe you know this could be the retina and this this could be lgn this could be v1 v2 and so on and but but this

But with this, somehow everything is just a representation of the first input.

So somehow this doesn't work.

Or if you think this is enough for meta representation, then everything is a meta representation of something else.

So that seemed a little bit strange.

Another way to conceive metarepresentation is to think about confidence.

In the context of metacognition and cognitive neuroscience, we often take confidence reports from participants in an experiment.

So, for example, we could present some visual stimulus and then people report what they saw.

If it's a color discrimination task, they could say blue versus red or something like that.

But then you use this first order information to make a perceptual decision, but also you can report the confidence of seeing that stimulus.

So this is probably a bit better than just the very simplistic, so any transformation as a meta representation.

But on the other hand, if we think

about implementing this.

This is a very simple operation.

So people who are working in artificial neural networks easily use something called softmax, where you convert this kind of representation to some sort of probability interpretation.

So basically, if you convert activation patterns

of this layer so that it normalizes to one, then it is confident.

so so in a way this is just one very simple uh transformation of first of the representation so okay now maybe now this is still meaningful when we think about how the brain process this kind of uncertainty uh but but but this may not be uh basically it was not really satisfying for me uh as a way to uh so uh characterize metal representations

So the main solution we came up with this idea.

So you can

so we always tended to think about transforming first-order representation to something else but here what we are proposing is to construct a representation of processes rather than this kind of states so the idea is okay so like like not any neural network can be conceived as some sort of function from input to output but

But there is a way to represent these functions or processes.

So instead of re-representing first-order representation,

we are proposing we can if we represent this function f to some meta representation uh you know you know we can have a potentially uh better interpretation of meta representation and because this makes a qualitative uh difference so but maybe you know this might be somehow confusing so i wanted to give you an example using

artificial neural networks.

So that's the main topic of the paper.

OK, so here I just want to briefly sort of introduce the concept of latent space or embedding for people who are not familiar with this kind of stuff.

So in artificial neural networks, there is a concept of encoder and decoder.

And so, for example, this is let's say you train an autoencoder neural networks.

uh where the input is an image and then output is an image so so basically uh you want to reconstruct the input uh using encoder and decoder but there's a low dimensional bottleneck in the middle and and then usually uh you know when you have this kind of construction you find

interesting features where the vector in this space captures the important characteristics of the dataset, in this case, images.

So that's the idea.

So basically, it's a compressed nice representation of the dataset.

But what we are proposing is to create some sort of meta autoencoder where the input and outputs are neural networks.

So you can use

a neural network which can be parameterized by or represented by the weights and biases.

You can use many different ways to sort of convert neural networks into some vectors.

And then you reconstruct the neural network.

So maybe you can already see this is somewhat meta.

And so with this construction, you can have a representation of neural networks.

So in this case, each point in this space corresponds to some sort of neural network or function.

so yeah so so in a way this is a space of functions uh parameterized some you know like latent uh axes so so that's the general idea so i hope you get the idea so far

Yeah.

And so but from here, you know, I want to explain the actual experiments we did.

And so this is a little bit more detailed construction of what we did.

So we first created a lot of encoders by training autoencoders for all kind of different stimuli.

So, for example, like in one case, you know, let's say that we trained this kind of

first order your network on just on cars or just on flowers or sometimes just

the sound of dog barking and so on.

So, each person in your network is specialized in some kind of stimulus category.

And we use both visual and audio stimuli.

But each network is trained on just one type of stimulus, one category.

And then this encoder can be represented as 256 by 16 elements.

So each column corresponds.

So here, first of all, the latent space, we have 16 dimensions.

And then input image is 256 pixels.

Yeah, so basically, you can see this number as the sort of parameter for the dispersal of the neural network.

And then each column corresponds to... Okay, so now you can see that this is a very simple neural network, so just one transformation, right?

And then we took each of these as the input, as a filter, and then we get

like this kind of, so okay, so this one column here is a theater in this network, and then we use this as input.

So we trained another autoencoder to represent individual theaters, and then we computed representation of this whole network using these two networks.

So this might be slightly confusing, but the main thing is

we get representation of a network at this stage.

And then we created another meta-autoencoder using this

kind of later representation of the first network.

So it might be a little bit confusing compared to the previous image, but this is more to deal with the technical aspect.

But the main thing is we trained a lot of small networks on the specialized stimulus category, and then we tried to get representations of those networks.

So that's the idea.

OK, and so then we applied the t-SNE to visualize latent representations.

And so here, all the blue dots correspond to some visual stimuli.

some sort of category, and then the orange points correspond to some auditory stimuli.

So clearly, you can see there's a separation between visual and the audio stimuli.

So that's kind of interesting, right?

So because this suggests that when you have a meta representation, you can already tell whether the network is specialized in

visual stimulus or auditory stimulus so that's a um that's uh that's what we were uh excited about yeah but also you might be able to see like some clustering of same shapes it might be a bit difficult to see this in this image but uh but there might be something like you know like some

you might be able to tell whether a network is specialized in cars or buses or things like that.

So we tested that by

uh trying to predict uh the original data set just by looking at their meta representations so yeah so as you can see like there's a clear separation between modalities but but there's a little bit brighter like diagonal elements which suggests that you know sometimes we succeed in predicting like really specific

uh stimulus category so so that's uh that's really exciting um yeah okay so so so based on this uh it seems we can uh you know there's already some information about the structure of the weight of the network uh which tells us about what kind of information

they were trained on or specialized in.

That's an interesting observation.

But also, we tried this in the original weight space, but in that case, we cannot really predict which category the network was trained on.

So in that sense, you actually have to have this kind of meta representation to be able to make all these predictions.

okay so um but maybe one uh personal question is you know why is it possible um so especially uh when we try to compare like vision versus audition maybe the key difference is they may have different kind of invariancies or equivalencies uh so

For example, in images, the object category or labels do not change if we make

translation or a change in scale and so on so like the classical convolution neural networks are kind of uh rely on like this kind of uh invariances and but but maybe for auditory stimuli uh you know if you uh sort of move the

If you translate the input image or the sound differently, you experience different kinds of sounds and also the identity of the sound changes.

So there might be some modality-specific invariances.

So maybe after training a network, they learn to capture that kind of invariances.

metarepresentation may also find this kind of representation of invariancies or equivalencies in the structure of the weight.

So that's our current explanation why this works.

okay and i'll come back to this later so and uh so so in a way um like our experiment uh was also motivated by the the classical uh question of what it's like to be a bat and uh the interesting thing is oh we don't so you know bats

can do echolocation by emitting sound and then receiving sound but they use it for navigation so maybe it's like vision for them but it's hard to say like whether they are experiencing visual quality or auditory quality so we've been

We want to answer that kind of question eventually in consciousness research.

But to introduce an interesting context, so this is a very exciting series of research from American Ancestors Group at MIT in the early 2000s.

and so maybe many of you are already familiar with this experiment but they managed to do an experiment where they rewired the input from the retina to the auditory cortex so yeah I think it's like really amazing that they can actually do this sort of thing but yeah so but the idea is that

Their auditory cortex developed by receiving input from visual images, the data coming from the retina.

They had several Nature papers and they were all very interesting.

One of the key findings is that after auditory cortex was trained on visual input, their connectivity pattern looked more like normal visual cortex.

So this kind of connectivity to other neurons probably with similar orientation properties.

and so so somehow the connectivity seemed to reflect the sort of like statistics or a structure in the data they learned and but there's an additional interesting experiment behavior experiment they did and the the main question they asked is whether they

see or hear the activation of this rewired auditory cortex.

The short answer to that is that the animals seem to see the brain activities in the rewired auditory cortex.

That's a really amazing result.

Yeah, so based on this, I thought this could be a really useful way to think about whether we are, you know, whether some activities or some neural networks is more like for hearing or seeing.

And so, yeah, so that's something I always wanted to ask.

So.

Yeah, so maybe

I've been thinking about testing IIT using this kind of visual versus auditory coria.

So the question is whether we can tell whether a piece of cortex is visual or auditory just by looking at their connectivity.

Of course, IEIT is very difficult to apply and we don't know yet whether IEIT is true or not.

But at least we need to think about how we can test implications of IEIT by empirical research.

okay so so this was what i was thinking originally so let's say you know you take some you know like anatomy from visual cortex or auditory cortex and then there are like many techniques to look at

anatomical connections and functional connections, it's still very difficult to read out the weights from the actual brain.

But if we have this, we should be able to compute information structures as suggested by IIT.

And we can probably match the structure to the report of, you know,

visual qualia or auditory qualia so that's the ideal experiment I wanted to do but there are a lot of challenges so the first one is in the actual brain it's it's very difficult to have like complete characterization of anatomical connections and you know it's also impossible to look at the causal

patterns in the activation from all the neurons in question.

So that's very hard.

It seems impossible.

Another difficulty is the

the computation of phi in 980 is very hard so uh i know yeah we've been also trying to compute some surrogate of phi or earlier version of phi but it's still very hard if we want to compute phi from you know let's say tensor than neurons so that's that's pretty hard and so

And also maybe a third thing is, how can we match the structure of the report of experience to the structure of information?

So that's, again, a difficult question.

So with the approach I showed you earlier, we can kind of

replace some of the problems.

So instead of looking at the complete anatomy of the human visual cortex and human auditory cortex, we can create artificial neural networks by training them on specific stimuli, in this case, sounds and images.

uh no the great thing about artificial neural networks is we can see all the weights and connectivity patterns so of course you know we don't know whether they're conscious or not but but this uh this approach gives us something concrete to work with and another thing is uh about uh computing phi in iid uh we can

there's something slightly different.

So there's a really interesting paper by Mediano and colleagues where they propose the concept of weak IIDs.

So if you take

the original IIT, it has both mathematical and philosophical aspects.

So that's my interpretation.

But if we can use the mathematical implications, or if we use pi as an index of complexity or something, then

we can use iit in a more pragmatic manner so so that's their uh proposal weak iit so i think i think you know they're they're having like a lot of empirical research driven by iit so i think that's a a good thing but but here uh what i'm proposing is like more like conceptual iit so so in the um

uh meta representation work i presented today i i also took some inspiration from iit as well so the idea is like in iit whether that you know some network generates uh visual or auditory qualia should be fully described in terms of their connectivity patterns so that's

a kind of conceptual implication of iit so but instead of applying the mathematical formalisms of iit we just use autoencoder to find or embed neural networks in a practical computationally tractable manner so

So I think this kind of the conceptual idea approach could be also a useful way to make progress in consciousness research.

Okay, so the final points.

So maybe one question is whether meta representations, as we presented today, actually exist in biological brains?

That's highly questionable.

For example, today I embedded neural networks just by the weights of first-order networks.

But in the brain, it seems impossible to read out the weights from other brain regions.

well it may not be impossible but it seems like very unlikely and so there might so maybe in the brain there might be a different mechanism to achieve this so so instead of using weights if you have like many sort of input output pairs you can also

construct a representation of that kind of relationship so maybe without some figure of or equations it might be a bit difficult to understand this but let's say you have two brain regions like you know v1 and v5 and but if a third region receives the input from both regions

the third region can learn to represent the pair of representations so that kind of meta representation may exist in the brain and also now we've been thinking how we may be able to find such representations with fMRI so that's a topic for our future study yeah and another

question is what what's the point of having this kind of meta representation so what's the functional role so our current interpretation is when you have this kind of representation you can uh sort of

compare different networks or you can have some sort of a qualitative characterization of first-order networks so let's say you know you have a meta representation of uh

you know red neuron red color processing network or like oh no you could have that kind of meta representation like many many different uh first of the neural networks but then in this space you know you can talk about whether two processes are similar or different so for example in that space

where you can say that visual experiences are very different from auditory experiences but within the visual experiences there are many different types of experiences and here you know you can talk about the distances and similarities uh so maybe it's a bit similar to the uh idea of like what to back you know where

words are embedded in a latent vector space and when you have that kind of space you can actually have some reasonable representations of semantics of the words but but here if you embed neural networks you can have some sort of semantic

or representations of those networks so so that might be the potential functional role and so so in a way like you know this has like very this already has a a flavor of something like quality so when we talk about korea we always

compare certain experiences to other experiences and we can talk about whether you know two experiences are similar and so on so so in that sense maybe you know this kind of meta representation might be very important for us to be able to report the qualitative aspects of our own experience okay yeah that's all and thank you very much


SPEAKER_00:
thank you awesome cool great i wrote down some questions and also anyone watching live can can write questions um well

thanks for sharing it and it's definitely very striking finding about the visual and the audio differing so just kind of a preliminary question here how was time handled in the audio setting oh good question well we just converted the raw signal into an image so it's basically time and frequency uh so so that we can use the same network for handling two different devices

yeah interesting because it's also a difference between those two features and it made me wonder about video video chatting or watching a video perhaps as being a meta representation with audio and visual

because if there's like a lag or if there's a disconnect or any other number of relations it can um be noticeable as a contrast like oh they're talking louder than it looks like they're talking or something like that so I mean have you looked at that kind of Fusion or how could you look at it with the architecture that you had here


SPEAKER_01:
Oh, that's a great question.

So we have very simplistic architecture since this was just a peripheral concept.

But I think in the brain, we must have really multimodal representations at the same time.

So that way,

we should be able to compare different experiences at the same time and so so that that's where we get somehow close to the idea of global workspace so um yeah so

We are not directly addressing your question, but we are on a separate project.

We are actually training global workspace like neural networks so that they learn

kind of multimodal representations.

And also, so in that context, we believe it's very important to look at the latent space structures from different modalities and then see how they are related to each other.

So, with Rufan Van Ruren, we wrote a paper in Trending Neuroscience where we proposed that global workspace

maybe a kind of shared latent space across different specialized modules so yeah so so i think that part is also like important uh like next step to sort of to understand uh theories of consciousness from the perspective of deep learning interesting


SPEAKER_00:
uh i found it also very interesting how you began with the uncertainty estimate because in active inference a lot gets loaded on to uncertainty estimates of different parameters and that made me think about for a yeah thanks for the slide it made me think about for if you only have two parameters

to encode then you can encode like the mean and the variance however with the neural network autoencoder concept you could project down to just two or it could be more

and so then there's a much richer palette for and more bandwidth than just a statistical distribution even though it's also composed of statistical distributions but the minimal and the simplest and like the kind of most essential is really the statistical single distribution but this is basically just talking about the connectivity of multiple distributions yeah that's an interesting comment


SPEAKER_01:
Yeah, I guess maybe a crucial question is whether and how uncertainties are represented in the brain.

So when we think of uncertainty estimation in terms of two parameters, that's kind of a mathematical notion.

But uncertainties

we don't use just a single neuron to represent all uncertainty.

So there might be also population representation for uncertainty.

So in the actual implementation in the brain, you may actually use many neurons as well.

but i've been also like really interested in the this topic and especially uh like uncertainty estimation uh in the uh thalamus so you know my uh friend and colleague uh called uh komra uh he found uh

like uncertainty estimation neurons in the palpiter in the thalamus for visual experiences so I thought that was a really cool study and I thought you know there seems to be like a close link between having high confidence and

conscious experience so as I thought this must be like like a really key ingredient but but somehow like when we think about uncertainty in terms of deep learning it it's kind of it seems like really trivial so I feel like there's a gap there um that's very interesting um


SPEAKER_00:
there's a lot of ways to go with that um in the the last two points that you had up there you return to this kind of functionalist question or at least just perspective like what is

it doing so that made me wonder when you look at the meta representations for the networks you construct do they seem to convey something like summary statistics of the network like the the sparseness overall the connection or like some kind of network descriptions you also mentioned the differences in the stimuli type so like does the what parts of the representation of the meta representation reflect


SPEAKER_01:
um what the network is and then also like what it does input output wise uh yeah so we don't know really what's in the meta representations of our uh networks um but maybe this slide might be relevant so uh so each neural network the first order neural network uh tries to find like good

basis functions or have good filters so that they can efficiently encode the images or sounds and but they they really come from the statistical patterns in the stimulus so so that so that should be somehow

reflected in the you know weight structure so so that may be related to you know this kind of invariancies or good variances uh that's our speculation but also you know for like like really specific type of things so for example like these are all like different

kind of specific stimulus that maybe this is air conditioning or car sound and so on.

So they may have like really specific sort of structure.

So then they may be

embedded in the neural networks but the interesting thing is when you train a neural network uh you get different things every time uh but but they have something common across every training but on the surface they look very different right so yeah but across the results of many different training

uh different networks trying to have the same stimulus there is something common across them and so that's the kind of uh features we wanted to sort of capture with metal water encoders


SPEAKER_00:
okay this may be reading too much into the image but the um it's v for visual a for audio oh yeah yeah that's right yeah the big the big off diagonal blocks are just the differences between the audio and visual that are solid purple but then um and then um the

audio stimuli has more a little bit with other audio but then like in the visual it's I don't know it's like related to the syllable air so it's kind of like a natural Association even though it wasn't trained or maybe just because it's not a diagonal uh


SPEAKER_01:
yeah well there might be some bias uh yeah so this one yeah somehow the kind of predictor thought like all the images come from this one um yeah but it seems like you know from this figure like a different audio stimuli are very distinct whereas like in the visual things well they seem to kind of similar yeah but probably the

But this may be more due to technical constraints.

So here, we have to use really small patches, so maybe

But instead of looking at object categories per se, maybe those networks looked at the texture patterns across different categories.

So there might be still some information, but maybe they're not very specific to individual stimulus categories.


SPEAKER_00:
yeah i think i mean you're you're shooting for the moon with the consciousness component but even making simpler networks to understand what aspects latent spaces learn is a very useful method that i think this provides a strong example of yeah yeah i hope so and but also


SPEAKER_01:
Since we work in the domain of AI research as well, we encounter something called mechanistic interpretability.

It's a kind of field of AI research where people try to understand what's happening

inside neural networks because you know they got a lot of people playing deep learning is black box and we don't understand what's going on inside but but i think eventually we want to understand what's happening in neural networks as well and it should be still uh easier uh compared to understanding the brain so you know maybe you know in the context i idea i talked about

is a little bit but in deep learning we know all the weights we can do like any experiment and we can do ablation or we can look at the function of part of the network

and we can give them millions of trials.

So it's like an ideal experimental setup for a neuroscientist.

So if we don't understand deep learning networks, it's hopeless to understand the brain because

it's much harder to do experiments in the brain.

We cannot know the weights and connectivity.

So we only have very limited access to the actual material.

So in a way, we can practice how to understand systems using simple neural networks and then try to sort of use that experience to understand the brain.

So I think that kind of cooperation is potentially very interesting.


SPEAKER_00:
Yeah, very well said.

It reminds me of some work by Jonas and Cording from 2017.

The paper's called Could a Neuroscientist Understand a Microprocessor?

And they had an in silico simulation of a whole microprocessor doing different operations so they could do the lesions and the double lesions, make all the recordings.

And so that kind of revealed, on one hand, the limitations of different methods that people often use to ascribe function to the brain.

However, there's always this component where it's like, well, maybe that's just because the processor is a weird architecture or because the operations that they do are very synthetic and they don't really have a natural component.

whereas when you're proposing that the base neural networks must deal with the symmetries of audio visual and so on it's the kinds of challenges that organisms actually have to solve with nervous systems as opposed to being software which could have been written in an arbitrary way and so the results are a little bit different but it kind of makes the proof

points which is that to have the in silica version that you can do the digital simulations on can help you identify like where your studies are well powered or not and do all these other useful functions even if they don't directly answer the question itself yeah yeah i'm also a big fan of that uh like coding


SPEAKER_01:
paper.

So I think they made a really important point.

So even if we have access to all the things, we still may not understand it.

And it's hard to say whether the organic systems are easier to understand compared to synthetic

systems.

So I guess we just don't know.

But without making any assumption, I think it's just very hard to sort of understand any computation happening in many different scales, not just in the brain, but inside individual cells, or maybe at the smaller scale or

you know larger scale like galaxy and so yeah i think we still lack the science to connect physics to computation so and so on a different paper uh we also propose something called universality so the idea is uh well

The point is a theory of consciousness should be also universal in the sense that we can apply it to non-biological brains.

A lot of times people ask whether current LLMs are conscious or not, but it's kind of

it depends on the theory you subscribe to so but the current theories like higher order theory or global workspace theory do not tell us whether you know ai's are conscious or not but i think the reason is we need to define theories based on some physical system so

For example, it's hard to say whether an artificial neural network has a global workspace.

We have those concepts, but when we try to analyze physical systems and then try to see whether there's a global workspace inside, it's very hard.

make such theories universally applicable so that they can tell us about computations happening at many different scales, correspond to the concepts in those theories.


SPEAKER_00:
Yeah.

that definitely uh kind of calls back to the alien slide and if we had a tissue sample with only the static topology with no function what could be assessed um

on a different note um inactive inference I mean half of it is action so when we're thinking about variational auto encoders and the transformation um from observations like down into state spaces often that's in terms of the policy rather than only a compression which is kind of like a sense making and reverse sense making layer

that's the predictive coding predictive processing origins and then also to bring in the action representations so that's kind of like being able to distinguish the primary audio and the primary visual from the motor cortices and those may have very simple or very very um sophisticated representations


SPEAKER_01:
they're also ones that could be directly correlated with like bodily movement or muscle activity rather than being correlated at some level if you go that way like with something you can't measure yeah um yeah i think you know like here like we will only talk about sensory experience but but but like you said i think some of the meaning

uh of sensory experience comes from action or in relation to your body so for example uh you know without body there's no up or down or left or right so there's no direction but you get this kind of you know

So direction in the visual image, or reactivity via body, for example.

And also maybe you get kind of representations related to affordances.

So whether something is actionable, that also relies on some combination of sort of action and extensory representations.

I think it's very interesting to think about how we can extend our current research into more agent-based architecture.


SPEAKER_00:
that makes me think of um the possibility that the meta representations could be higher judgments whether those are experienced so like a very cross-modal judgment would be like can a human make this and then that could be asked across modalities or could reflect multiple modalities but it it

it has to draw information from also not just the sensory data but like other kinds of memory yeah

okay yeah it connects with with the broader cognitive modeling and the agent architecture whereas here it's really reducing it to just the streams of Vision and audio which is the right starting place and then there's going to be other streams that aren't just the sensory


SPEAKER_01:
Oh, okay.

Yeah, that's a great comment.

So this reminded me of something I had in mind and forgot to mention.

So we are thinking of this kind of meta representation as one of the modules attached to the global workspace.

So when you have something like global workspace, you want to use

functions not just from like sensory inputs but you want to combine that with action and reward and so on and so in principle okay so we want to use representations of potential actions or even like mental operations so like you know adding two numbers is a kind of mental action so

when you try so so we are actually like thinking about connecting like this kind of meta representation to general intelligence so when you are have a new program you have a representation of the task and

But to solve a new task, you want to find a potential combination of specialized modules you can use to solve that task.

So in a way, this kind of meta representation can be used to match

the tools you have in your brain with the current task.

So in that toolbox, you don't just have this kind of visual representations, but you may also want to use potential future actions you can use.

But you can also use the same meta representation approach to represent the repertoire of actions you can make.

So that's a connection.


SPEAKER_00:
Yeah, potentially in the active inference model, like you had the slices with the different filters.

the b matrix in active inference or just the transition matrix it's the policy dependent transitions on the world so that's kind of like the policy dependent filter applied yeah yeah yeah yeah oh yeah that's a good connection too yeah and then also on the thalamus i'm not familiar with the mammal neuroanatomy very much but when you said that clarity was important did you mean like


SPEAKER_01:
what um yeah so so i think a lot of interest uh in confidence or metacognition comes from the literature blindside or blindside like uh phenomena and so it seems that even when you have

you have the ability to report these stimulus without confidence, you know, you report, you didn't see it, right?

So it seems, you know, this kind of confidence is explicitly present in the brain.

So for example, like in the thalamus, and but so when you deactivate those neurons, somehow, you know, on this, you know, monkeys, they lose the confidence as they

So in terms of confidence rating, they seem to become like blindside patients.

So in my introduction, I kind of criticized the kind of simplicity of confidence as meta representation.

but at the same time in the literature of consciousness research it's also very important right so somehow like without confidence uh no you don't really report it but i think maybe you know we need to have like further studies about how confidence or like lack of confidence

changes the way information enters something like global workspace or i don't know some stage of consciousness so but i think that's like really likely in the context of like a precision weighting and stuff right so without

confidence maybe confidence works as a way for way to get information to something like lower workspace i don't know maybe i i talk about global workspace too much but yeah but i think it's a good way to like think about systems


SPEAKER_00:
yeah i i agree there these are just like kind of the labels and the the models that can then be applied so what that made me think of was the again an active inference if we're thinking about attention or precision confidence it's usually just one variable in the minimal case just changing the variance on a statistical distribution latent spaces makes it much richer um

So then let's just think about what sources are we paying attention to?

Which sources are we allowing to ignite the global workspace or have more of a percolating effect?

And then there could be like, it's like the dinner party problem.

so one part of the problem is just distinguishing and interpreting the sounds so that might be a simpler like left side louder or quieter and or or moving something like that but then you would have a higher order evaluation space

with like this person you can trust on this topic and that topic but not this one and vice versa here so that's invoking more of the world model and then that could control to what extent different things that those people said after they'd been parsed out so like the primary sensory mapping is simpler because this is just a very sensor actuator type problem whereas the higher order latent spaces could be just very large


SPEAKER_01:
um yeah i guess maybe for like a simple like presence versus absence kind of judgment uh this confidence may be like a critical for well a stimulus to enter conscious awareness uh whereas probably meta representation i talked about

maybe more related to sort of evaluation of the quality of the content of consciousness so if we want to make comparisons across like different sensory experiences you know we need this kind of meta representation i think that's a kind of

different aspect of metacognition um yeah so i guess maybe like like in like earlier days and consciousness research like people did a lot of like uh present versus absent report and so it's all about detection but i think detection is very different from kind of

qualitative assessment of experience and so i think there's a kind of transition in the focus of uh consciousness research from you know like present versus absent

uh report versus more like structured report you know by comparing like two experiences are similar or not so i know uh now it's gs group um has been like you know developing this method to sort of capture for your structure so

so i think you know we want to understand why certain experiences feel the way they do right and then that those things seem to be very difficult to approach but but by looking you know at you know sort of comparisons of different experiences so we seem to be able to capture

the overall structure of the experience.

So maybe, you know, our meta representation approach here is also more about capturing more sort of structured structure of the quality of experience rather than, you know, whether we consciously perceive something or just subliminally process the information.


SPEAKER_00:
That's a very interesting point.

It's almost like the early global workspace and Ignition were focused on the extent of the causal efficacy and the binding, but it was sort of left for later what the semantics or the contents were, because the question was

least from how i've seen it more like whether it percolated to awareness or not or like whether there was a binding or a reportable versus unreportable stimuli and so it totally makes sense that within the space of of the aware

the question of how to bring it to awareness is kind of already taken so then it requires a much richer state space like you have here to even approach it because it's just not an all-or-none question at this point


SPEAKER_01:
Yeah, I agree.

I think that's a really interesting tradition.

Although I feel like we haven't really solved this reportable versus unreportable question yet, because we still don't know where consciousness is really happening in the brain.

But I think we need just several different approaches to tackle the same problem.


SPEAKER_00:
Cool.

Well, I guess sort of in closing, what are you going to continue working on or what are you excited about?


SPEAKER_01:
So we want to do empirical research based on this.

So I think here we are proposing a new way to look at metarepresentation.

So one thing we're interested in doing is to find whether such metarepresentations exist in the brain.

So that's an open question, but we have some ideas about how to try that.

And another thing is, this is kind of beyond my ability, but there might be a way to do this better using mathematical tools.

so intuitively you know so now like some people talk about category theories and things like things like that but so it seems like what we are doing is um can be made more abstract and

formalized so that we can have better insight about what we are trying to do.

So here, what we did was kind of like naive way to express our thought about the representations.

But here,

Yeah, so we are kind of transforming functions into an object.

So basically one function is a point in some functional space.

And this point in functional space seems to be

somehow related to Quaria, so that's my current intuition.

But I think in mathematics, there must be useful tools to sort of deal with this sort of idea.

So I'm curious if there's any mathematician who can formulate this nicely.

I think that would be an interesting thing to do next.


SPEAKER_00:
very interesting and also to the little i know about it and people have discussed it that is sort of a category theoretic move to have a handle like a point around a mapping so that you can have the space of the models so you took a very constructivist

engineering empirical approach and then it's an exciting open question like what are the formal structures that generalize that and then you could spin up a million experiments just like the one that you probably built by hands and then learn from that pick up that as a point yeah any last comments


SPEAKER_01:
oh i have just one announcement which is this one um yeah so sorry about advertising but but we have a consciousness conference uh called assc i think that's one of the like main consciousness conferences and that's happening in tokyo this july so like if you're interested please come i'm one of the main organizers that sounds awesome cool


SPEAKER_00:
Thank you very much for this presentation.

Good luck with the work in the conference and hope to talk to you again.


SPEAKER_01:
Yeah, thank you.

That was fun.

Yep.


SPEAKER_00:
See you.

Thank you.
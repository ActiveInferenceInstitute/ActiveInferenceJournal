1
00:00:05,759 --> 00:00:08,400
hallo en welkom iedereen bij het Octave

2
00:00:08,400 --> 00:00:10,860
Inference Institute dit is actieve

3
00:00:10,860 --> 00:00:15,839
gaststream nummer 41.1 op 25 april 2023

4
00:00:15,839 --> 00:00:18,480
we zijn hier met Elliot Murphy en

5
00:00:18,480 --> 00:00:20,939
Stephen piantadosi dit wordt

6
00:00:20,939 --> 00:00:23,039
een behoorlijke discussie we beginnen met

7
00:00:23,039 --> 00:00:24,779
openingsverklaringen van Stephen en

8
00:00:24,779 --> 00:00:27,359
Elliot Elliott zal  begin dan met een paar

9
00:00:27,359 --> 00:00:28,920
vragen en we zullen

10
00:00:28,920 --> 00:00:32,759
aan het einde een open discussie hebben, dus Steven,

11
00:00:32,759 --> 00:00:34,739
bedankt voor je deelname en voor je

12
00:00:34,739 --> 00:00:36,960
openingsverklaring

13
00:00:36,960 --> 00:00:39,780
cool hallo, dus ik ben Steve piantadosi, ik ben een

14
00:00:39,780 --> 00:00:42,239
professor in psychologie en neurowetenschappen

15
00:00:42,239 --> 00:00:44,700
aan UC Berkeley

16
00:00:44,700 --> 00:00:46,860
um en  uh Ik denk dat een deel van de reden

17
00:00:46,860 --> 00:00:48,899
dat we hier zijn, is dat ik onlangs

18
00:00:48,899 --> 00:00:51,719
een paper heb geschreven over grote taalmodellen,

19
00:00:51,719 --> 00:00:53,940
deels om enig enthousiasme over te brengen

20
00:00:53,940 --> 00:00:55,199
over

21
00:00:55,199 --> 00:00:57,300
wat ze hebben bereikt op het gebied

22
00:00:57,300 --> 00:00:59,100
van het leren van syntaxis en

23
00:00:59,100 --> 00:01:00,719
semantiek,

24
00:01:00,719 --> 00:01:03,059
eh en  deels erop wijzend dat ik denk dat

25
00:01:03,059 --> 00:01:04,979
deze modellen echt veranderen hoe we

26
00:01:04,979 --> 00:01:06,780
over taal zouden moeten denken,

27
00:01:06,780 --> 00:01:08,520
eh, hoe we zouden moeten denken over theorieën over

28
00:01:08,520 --> 00:01:11,939
taalkundige representatie en en uh

29
00:01:11,939 --> 00:01:13,560
theorieën over grammatica,

30
00:01:13,560 --> 00:01:17,220
eh en waarschijnlijk ook theorieën over leren

31
00:01:17,220 --> 00:01:19,080
hallo

32
00:01:19,080 --> 00:01:21,960
geweldig ja dus ik ben Elliot  meffee Ik ben een

33
00:01:21,960 --> 00:01:23,040
postdoc op de afdeling

34
00:01:23,040 --> 00:01:25,799
neurochirurgie van UT Health in Texas

35
00:01:25,799 --> 00:01:27,240
eh ik heb Steven's paper met grote

36
00:01:27,240 --> 00:01:29,759
interesse gelezen Ik heb veel mensen gesproken en er

37
00:01:29,759 --> 00:01:31,619
waren enkele gebieden die samenvielen, maar de

38
00:01:31,619 --> 00:01:32,820
dingen waar ik me vandaag op wil concentreren

39
00:01:32,820 --> 00:01:34,979
in  reageren op Stephen en een beetje

40
00:01:34,979 --> 00:01:36,600
zoeken naar gebieden van

41
00:01:36,600 --> 00:01:38,640
Divergentie, misschien

42
00:01:38,640 --> 00:01:41,700
eh, dus je weet dat Steven's paper is gebaseerd

43
00:01:41,700 --> 00:01:43,619
op het idee dat moderne machine learning

44
00:01:43,619 --> 00:01:46,220
het hele

45
00:01:46,220 --> 00:01:48,119
theoretische raamwerk van Chomsky's

46
00:01:48,119 --> 00:01:49,860
benadering heeft ondermijnd en omzeild, dus ik wilde

47
00:01:49,860 --> 00:01:51,479
op sommige reageren  van deze hoofdargumenten en enkele

48
00:01:51,479 --> 00:01:52,560
andere gerelateerde argumenten in de

49
00:01:52,560 --> 00:01:54,060
literatuur uh dat zijn sommige

50
00:01:54,060 --> 00:01:55,799
luisterende mensen die misschien wat inzicht

51
00:01:55,799 --> 00:01:58,500
en gedachten hebben, dus het is een veel voorkomende

52
00:01:58,500 --> 00:02:00,720
kritiek om te zeggen dat grote

53
00:02:00,720 --> 00:02:03,180
taalmodellen gewoon het volgende token voorspellen en

54
00:02:03,180 --> 00:02:04,079
dat

55
00:02:04,079 --> 00:02:05,579
er duidelijk een beetje een  cliché klopt

56
00:02:05,579 --> 00:02:07,619
het is niet helemaal waar en ze

57
00:02:07,619 --> 00:02:09,419
voorspellen niet alleen het volgende teken ze

58
00:02:09,419 --> 00:02:11,700
lijken ook te confabuleren ze lijken te

59
00:02:11,700 --> 00:02:14,340
hallucineren ze liegen misschien ze

60
00:02:14,340 --> 00:02:16,440
geven willekeurig verschillende antwoorden op dezelfde

61
00:02:16,440 --> 00:02:18,300
vraag en ze lijken

62
00:02:18,300 --> 00:02:20,220
stochastisch taalachtige

63
00:02:20,220 --> 00:02:22,020
structuren na te bootsen ze  corrigeren

64
00:02:22,020 --> 00:02:23,819
zichzelf soms soms wanneer ze dat niet zouden moeten doen, als

65
00:02:23,819 --> 00:02:25,440
je ze een beetje pusht,

66
00:02:25,440 --> 00:02:27,000
veranderen ze van gedachten soms

67
00:02:27,000 --> 00:02:28,560
eh, in feite als Fox News momenteel

68
00:02:28,560 --> 00:02:30,060
op zoek is naar een vervanger voor Tucker

69
00:02:30,060 --> 00:02:31,800
Carlson, zouden ze minder kunnen doen, ze zouden

70
00:02:31,800 --> 00:02:33,680
zeker slechter kunnen doen dan

71
00:02:33,680 --> 00:02:36,120
tussendoor gebruiken als  ze zijn op zoek naar een

72
00:02:36,120 --> 00:02:38,040
vergelijkbaar kaliber, dus deze modellen

73
00:02:38,040 --> 00:02:40,620
lijken allerlei wilde dingen te doen

74
00:02:40,620 --> 00:02:41,940
eh en in de afgelopen 10 jaar is er

75
00:02:41,940 --> 00:02:43,500
een opeenvolging van verschillende, weet je,

76
00:02:43,500 --> 00:02:45,900
systemen ontwikkeld alsof we tabaksbed zijn

77
00:02:45,900 --> 00:02:47,580
en elk van hen is  gebaseerd op een andere

78
00:02:47,580 --> 00:02:49,620
neurale netbenadering Maar uiteindelijk

79
00:02:49,620 --> 00:02:51,180
lijken ze allemaal woorden te nemen en

80
00:02:51,180 --> 00:02:53,220
ze te karakteriseren door lijsten van uh honderden of

81
00:02:53,220 --> 00:02:56,220
duizenden getallen, dus het G23-netwerk

82
00:02:56,220 --> 00:03:00,000
heeft 175 miljard gewichten en 96

83
00:03:00,000 --> 00:03:02,160
aandachtshoofden in zijn architectuur en voor zover

84
00:03:02,160 --> 00:03:03,780
ik weet misschien  Steven kan

85
00:03:03,780 --> 00:03:06,060
me hier corrigeren, we hebben niet echt een goed

86
00:03:06,060 --> 00:03:07,319
idee van wat deze verschillende delen

87
00:03:07,319 --> 00:03:09,180
echt betekenen, het lijkt gewoon op

88
00:03:09,180 --> 00:03:10,920
die manier te werken, zoals aandachtshoofden en

89
00:03:10,920 --> 00:03:13,680
gpt3 kan aandacht besteden aan veel eerdere

90
00:03:13,680 --> 00:03:15,360
tokens in de string om te helpen

91
00:03:15,360 --> 00:03:17,040
ze voorspellen het volgende token, maar de

92
00:03:17,040 --> 00:03:18,659
hele architectuur van begin tot eind

93
00:03:18,659 --> 00:03:21,599
is een soort op engineering gebaseerde motivaties

94
00:03:21,599 --> 00:03:22,500
eh

95
00:03:22,500 --> 00:03:24,659
en ik vraag me altijd een beetje af hoe het zit met

96
00:03:24,659 --> 00:03:27,360
alle modellen die min of meer faalden van

97
00:03:27,360 --> 00:03:28,800
deze llms van de verschillende

98
00:03:28,800 --> 00:03:30,300
technologiebedrijven. Het is alsof deze bedrijven dat

99
00:03:30,300 --> 00:03:31,980
vaak lijken te doen

100
00:03:31,980 --> 00:03:33,060
eh weet je, laat het lijken alsof ze

101
00:03:33,060 --> 00:03:34,980
deze modellen hebben die echt heel goed werken,

102
00:03:34,980 --> 00:03:36,720
rechtstreeks uit de doos

103
00:03:36,720 --> 00:03:38,459
eh en ze lijken allemaal vernoemd te zijn naar

104
00:03:38,459 --> 00:03:40,739
een soort beroemde artiesten, ja,

105
00:03:40,739 --> 00:03:42,780
ze hebben Darley na salade of Dali,

106
00:03:42,780 --> 00:03:45,360
ze hebben Da Vinci, misschien mooi  binnenkort

107
00:03:45,360 --> 00:03:46,980
zal een van deze bedrijven een groot

108
00:03:46,980 --> 00:03:49,500
taalmodel uitbrengen, uh genaamd Jesus of

109
00:03:49,500 --> 00:03:50,879
zoiets, ik weet

110
00:03:50,879 --> 00:03:53,040
het niet, maar ze zeggen altijd dat dit ons

111
00:03:53,040 --> 00:03:54,900
New Foundation-model is, het heet Picasso,

112
00:03:54,900 --> 00:03:56,280
het is de eerste die we probeerden, we zijn

113
00:03:56,280 --> 00:03:58,140
gewoon geweldig, geen problemen, hetero  out of

114
00:03:58,140 --> 00:03:59,819
the box, maar ik vraag me altijd af hoe het zit met

115
00:03:59,819 --> 00:04:01,680
Oliver Black-boxen die

116
00:04:01,680 --> 00:04:03,659
elke keer min of meer hebben gefaald, er lijkt geen

117
00:04:03,659 --> 00:04:05,760
soort van een erg open en duidelijke

118
00:04:05,760 --> 00:04:07,379
structuur te zijn voor het soort wetenschappelijke

119
00:04:07,379 --> 00:04:09,840
redenering achter het selecteren, weet je, het ene

120
00:04:09,840 --> 00:04:11,939
of het andere model  uh maar nogmaals, misschien sta ik ervoor

121
00:04:11,939 --> 00:04:14,280
open om daarover gecorrigeerd te worden,

122
00:04:14,280 --> 00:04:16,798
dus zelfs basistaalmodellen doen het

123
00:04:16,798 --> 00:04:18,298
redelijk goed op basis van

124
00:04:18,298 --> 00:04:20,699
webvoorspelling, dus de

125
00:04:20,699 --> 00:04:22,139
vraag is of deze tools

126
00:04:22,139 --> 00:04:23,520
inzicht bieden in traditionele

127
00:04:23,520 --> 00:04:25,320
psycholinguïstische begrippen zoals grammatica

128
00:04:25,320 --> 00:04:27,300
en doorgeven, dus  dit is echt waarom ik de

129
00:04:27,300 --> 00:04:29,040
voorkeur geef aan het 10 Focus-model in plaats van het

130
00:04:29,040 --> 00:04:30,960
taalmodel, uh, voorgesteld door mensen

131
00:04:30,960 --> 00:04:32,880
als cyberveras,

132
00:04:32,880 --> 00:04:34,560
dus er is erop gewezen dat niemand

133
00:04:34,560 --> 00:04:36,479
echt denkt dat llms ons iets

134
00:04:36,479 --> 00:04:38,580
diepgaands over python vertellen als ze

135
00:04:38,580 --> 00:04:40,080
python-code net zo goed landen als natuurlijke

136
00:04:40,080 --> 00:04:42,120
taal  python is een symbolische

137
00:04:42,120 --> 00:04:43,740
taal met een zinsbouwgrammatica

138
00:04:43,740 --> 00:04:46,620
en niemand zegt dat llms de

139
00:04:46,620 --> 00:04:48,600
geheimen van python onthullen.

140
00:04:48,600 --> 00:04:50,940


141
00:04:50,940 --> 00:04:52,620


142
00:04:52,620 --> 00:04:54,000


143
00:04:54,000 --> 00:04:56,040
de

144
00:04:56,040 --> 00:04:57,540
afwezigheid van tegenargumenten zouden ook

145
00:04:57,540 --> 00:04:58,919
goede verklarende theorieën voor

146
00:04:58,919 --> 00:05:01,139
computertaal moeten zijn, daarom kunnen

147
00:05:01,139 --> 00:05:02,699
succesvolle modellen van natuurlijke

148
00:05:02,699 --> 00:05:04,620
taal niet worden gebruikt als bewijs

149
00:05:04,620 --> 00:05:06,540
tegen de generatieve frasestructuur van

150
00:05:06,540 --> 00:05:07,800
Amazon-taal,

151
00:05:07,800 --> 00:05:09,720
dus corpusmodel is ook om

152
00:05:09,720 --> 00:05:11,280
andere redenen echt een geschiktere term voor

153
00:05:11,280 --> 00:05:13,500
mensen  zoals Emily Bender en enkele anderen

154
00:05:13,500 --> 00:05:15,300
hebben laten zien dat kenmerken van de training

155
00:05:15,300 --> 00:05:16,979
Corpus in feite, ik denk dat Stephen

156
00:05:16,979 --> 00:05:17,940
dit citeert, je citeert dit in je paper

157
00:05:17,940 --> 00:05:19,860
eigenlijk als een beperking

158
00:05:19,860 --> 00:05:21,120
eh, ze laten zien dat kenmerken van de

159
00:05:21,120 --> 00:05:22,680
training Corpus het legproces sterk kunnen beïnvloeden,

160
00:05:22,680 --> 00:05:24,600
dus het is aangetoond

161
00:05:24,600 --> 00:05:25,800
dat de prestaties van grote

162
00:05:25,800 --> 00:05:27,780
taalmodellen in taallessen echt

163
00:05:27,780 --> 00:05:29,699
sterk worden beïnvloed door de diversiteit

164
00:05:29,699 --> 00:05:31,440
van de training Corpus

165
00:05:31,440 --> 00:05:33,180
um, maar natuurlijke taal zelf is niet

166
00:05:33,180 --> 00:05:35,340
bevooroordeeld, het is gewoon

167
00:05:35,340 --> 00:05:37,199
een computersysteem.

168
00:05:37,199 --> 00:05:39,120


169
00:05:39,120 --> 00:05:41,039
en maar natuurlijke taal zelf

170
00:05:41,039 --> 00:05:43,020
is niet bevooroordeeld, dus grote

171
00:05:43,020 --> 00:05:45,180
taalmodellen, daarom lijkt het

172
00:05:45,180 --> 00:05:47,880
me moeilijk om te weten dat ze het ermee eens zijn dat ze

173
00:05:47,880 --> 00:05:49,500
onderhevig zijn aan allerlei vooroordelen, ze kunnen

174
00:05:49,500 --> 00:05:51,000
daarom niet echt taalmodellen zijn,

175
00:05:51,000 --> 00:05:52,020
ze zijn modellen van iets

176
00:05:52,020 --> 00:05:53,880
anders, om dit argument af te ronden,

177
00:05:53,880 --> 00:05:55,620


178
00:05:55,620 --> 00:05:58,139
weet je, ook al worden llms duidelijk

179
00:05:58,139 --> 00:05:59,759
blootgesteld aan veel meer taalkundige

180
00:05:59,759 --> 00:06:01,320
ervaring bij kinderen, nogmaals, dit is

181
00:06:01,320 --> 00:06:02,699
iets anders dat Stephen kan zien

182
00:06:02,699 --> 00:06:04,680
en waarover hij praat in zijn paper en toch

183
00:06:04,680 --> 00:06:06,360
kunnen hun leerresultaten nog steeds

184
00:06:06,360 --> 00:06:08,160
relevant zijn bij het aanpakken van wat grammaticale

185
00:06:08,160 --> 00:06:09,840
generalisaties in principe leerbaar zijn,

186
00:06:09,840 --> 00:06:11,580
dus ik ben het eens met deze

187
00:06:11,580 --> 00:06:12,660
stelling hier weet je dat

188
00:06:12,660 --> 00:06:14,100
ze ons in principe iets kunnen vertellen

189
00:06:14,100 --> 00:06:16,080
over leerbaarheid in plaats van dingen

190
00:06:16,080 --> 00:06:17,639
zoals je weet brede

191
00:06:17,639 --> 00:06:20,520
acquisitiekaders en maar dat is ongeveer net zoveel als

192
00:06:20,520 --> 00:06:21,900
ik denk dat je kunt  misschien nu laten

193
00:06:21,900 --> 00:06:24,539
zien dat sommige inductieve vooroordelen

194
00:06:24,539 --> 00:06:26,819
eh niet nodig zijn om te leren, is niet

195
00:06:26,819 --> 00:06:28,139
echt hetzelfde als laten zien dat het

196
00:06:28,139 --> 00:06:30,060
niet aanwezig is bij kinderen, dus er is

197
00:06:30,060 --> 00:06:31,560
een lang debat geweest over de vraag of je

198
00:06:31,560 --> 00:06:32,639
negatief bewijs en instructie

199
00:06:32,639 --> 00:06:34,500
en correctie en feedback kent tijdens  het

200
00:06:34,500 --> 00:06:36,720
leren van talen is noodzakelijk of zelfs

201
00:06:36,720 --> 00:06:39,120
nuttig voor baby's en kinderen

202
00:06:39,120 --> 00:06:40,440
eh, maar op dit moment ben ik het meer eens

203
00:06:40,440 --> 00:06:42,479
met Eugene Choi en Gary Marcus en

204
00:06:42,479 --> 00:06:44,340
anderen die hebben benadrukt hoe llms

205
00:06:44,340 --> 00:06:45,660
momenteel erg duur zijn om te

206
00:06:45,660 --> 00:06:46,440
trainen,

207
00:06:46,440 --> 00:06:48,600
dat is duidelijk een voorbeeld van een

208
00:06:48,600 --> 00:06:50,400
geconcentreerde privé  macht in handen

209
00:06:50,400 --> 00:06:51,960
van een paar technologiebedrijven, hun

210
00:06:51,960 --> 00:06:54,539
impact op het milieu is enorm

211
00:06:54,539 --> 00:06:56,039
eh en je weet dat veel mensen hier

212
00:06:56,039 --> 00:06:57,300
minder beperkt en conservatief zijn geweest in

213
00:06:57,300 --> 00:06:58,979
hun beoordeling,

214
00:06:58,979 --> 00:07:00,660
veel minder dan Gary

215
00:07:00,660 --> 00:07:02,880
Marcus en Eugene, dus Bill Gates

216
00:07:02,880 --> 00:07:05,100
schreef onlangs dat chat-GPT eh is

217
00:07:05,100 --> 00:07:06,960
de grootste technische ontwikkeling sinds

218
00:07:06,960 --> 00:07:11,400
de uh grafische gebruikersinterface de GUI

219
00:07:11,400 --> 00:07:13,020
um en Henry Kissinger schreven in februari

220
00:07:13,020 --> 00:07:15,240
in de Wall Street Journal dat naarmate de

221
00:07:15,240 --> 00:07:17,400
capaciteit van chat gbt groter is geworden, ze de

222
00:07:17,400 --> 00:07:19,560
menselijke kennis opnieuw zullen definiëren,

223
00:07:19,560 --> 00:07:21,780
veranderingen in de structuur van onze realiteit zullen versnellen en

224
00:07:21,780 --> 00:07:23,220
politiek en samenleving zullen reorganiseren

225
00:07:23,220 --> 00:07:25,620
generatieve AI wordt gepubliceerd om

226
00:07:25,620 --> 00:07:28,319
nieuwe vormen van menselijk bewustzijn te genereren, dus er

227
00:07:28,319 --> 00:07:30,419
gebeuren momenteel zeer radicale beweringen. Ik

228
00:07:30,419 --> 00:07:32,220
vraag me af of

229
00:07:32,220 --> 00:07:35,099
je soms of de AI-hype misschien hebt opgeslagen

230
00:07:35,099 --> 00:07:36,840
in bepaalde delen van de academische wereld,

231
00:07:36,840 --> 00:07:39,240
mogelijk worden er veel grootse plannen

232
00:07:39,240 --> 00:07:40,680
gemaakt, maar ik denk dat je  meer

233
00:07:40,680 --> 00:07:42,180
concreet weten om het hier terug te leggen aan Steven

234
00:07:42,180 --> 00:07:44,280
Ik wilde misschien de kwestie aan de orde stellen

235
00:07:44,280 --> 00:07:46,919
van eh er is een kritiek van rorski en

236
00:07:46,919 --> 00:07:49,259
Beaumont die ik denk dat hij um heeft gelezen op

237
00:07:49,259 --> 00:07:50,520
lingbuzz

238
00:07:50,520 --> 00:07:51,240
eh

239
00:07:51,240 --> 00:07:53,280
ik denk dat je op Twitter hebt gezien dat je

240
00:07:53,280 --> 00:07:54,419
de reactie niet leuk vindt  ze maakten

241
00:07:54,419 --> 00:07:56,819
omdat het bezwaar dat ze maakten is

242
00:07:56,819 --> 00:07:58,500
dat je weet dat wetenschap een voorbeeld is van

243
00:07:58,500 --> 00:08:00,479
deductieve logica, je bezwaar is dat

244
00:08:00,479 --> 00:08:02,340
wetenschap niet deductief is, het is inductief

245
00:08:02,340 --> 00:08:04,380
juist en maar ik denk dat het algemene punt

246
00:08:04,380 --> 00:08:06,900
misschien nauwkeuriger is, namelijk dat

247
00:08:06,900 --> 00:08:08,520
je het niet kunt.  je kunt niet het feit gebruiken dat

248
00:08:08,520 --> 00:08:10,620
taalmodellen goed zijn in het voorspellen van bepaald

249
00:08:10,620 --> 00:08:13,020
taalgedrag bij mensen en sommige

250
00:08:13,020 --> 00:08:15,300
neurale beeldvormingsreacties, je kunt

251
00:08:15,300 --> 00:08:17,460
dat niet alleen gebruiken om te beweren dat ze

252
00:08:17,460 --> 00:08:19,199
een theorie van menselijke taal kunnen opleveren,

253
00:08:19,199 --> 00:08:21,000
dus in je paper Stephen weet je dat

254
00:08:21,000 --> 00:08:23,220
het lijkt alsof  bepaalde structuren werken

255
00:08:23,220 --> 00:08:25,020
beter dan andere het juiste

256
00:08:25,020 --> 00:08:26,819
aandachtsmechanisme is belangrijk voorspelling is

257
00:08:26,819 --> 00:08:28,680
belangrijk semantische representaties zijn

258
00:08:28,680 --> 00:08:30,180
belangrijk en daarom kunnen we op

259
00:08:30,180 --> 00:08:32,219
dit moment informatie verzamelen op basis van deze modellen,

260
00:08:32,219 --> 00:08:34,260
maar tot nu toe is dat eigenlijk alles wat ik heb

261
00:08:34,260 --> 00:08:36,000
kunnen verzamelen in de literatuur die ik heb

262
00:08:36,000 --> 00:08:37,559
niet zeker of je hier meer inzichten hebt,

263
00:08:37,559 --> 00:08:39,899
dus Rosky en Boomer gebruikten het voorbeeld van

264
00:08:39,899 --> 00:08:42,719
slechte voorspelling maar sterke uitleg

265
00:08:42,719 --> 00:08:44,760
juiste verklarende kracht en niet

266
00:08:44,760 --> 00:08:46,500
voorspellende nauwkeurigheid vormt de basis van de

267
00:08:46,500 --> 00:08:48,120
moderne wetenschap en ik wil

268
00:08:48,120 --> 00:08:49,440
dit later onderzoeken, misschien

269
00:08:49,440 --> 00:08:50,820
eh maar moderne taalmodellen  kan

270
00:08:50,820 --> 00:08:52,440
nauwkeurig delen van menselijke taal modelleren,

271
00:08:52,440 --> 00:08:54,180
maar ze kunnen ook heel goed presteren op

272
00:08:54,180 --> 00:08:55,920
onmogelijke talen en onnatuurlijke

273
00:08:55,920 --> 00:08:58,200
structuren die mensen niet kunnen leren en die

274
00:08:58,200 --> 00:09:00,360
grote moeite hebben met verwerken en ik

275
00:09:00,360 --> 00:09:01,560
weet dat je hiermee bekend bent met

276
00:09:01,560 --> 00:09:03,060
deze kritiek,

277
00:09:03,060 --> 00:09:04,380
maar je bent  zeker niet alleen hier

278
00:09:04,380 --> 00:09:08,339
op hetzelfde moment dus uh Elia

279
00:09:08,339 --> 00:09:10,320
um de hoofdwetenschapper bij open AI hij

280
00:09:10,320 --> 00:09:12,180
zei onlangs in een interview dat wat

281
00:09:12,180 --> 00:09:13,680
het betekent om het volgende token

282
00:09:13,680 --> 00:09:15,540
goed genoeg te voorspellen, het betekent dat je

283
00:09:15,540 --> 00:09:17,940
de onderliggende realiteit begrijpt die leidde tot de

284
00:09:17,940 --> 00:09:19,920
creatie van  dat teken

285
00:09:19,920 --> 00:09:21,779
dat nogal afwijkt van veel

286
00:09:21,779 --> 00:09:23,100
meer conservatieve beweringen in de

287
00:09:23,100 --> 00:09:24,540
literatuur hier

288
00:09:24,540 --> 00:09:26,519
eh en ook weet je, ik zou gewoon zeggen als

289
00:09:26,519 --> 00:09:27,839
reactie daarop dat verschillende

290
00:09:27,839 --> 00:09:30,019
componenten van wetenschap zowel

291
00:09:30,019 --> 00:09:32,580
inductief als deductief kunnen zijn, het zit niet

292
00:09:32,580 --> 00:09:34,140
echt in een van beide of je hebt  een bestaande

293
00:09:34,140 --> 00:09:36,300
theorie je formuleert hyperhypothese

294
00:09:36,300 --> 00:09:38,519
je verzamelt gegevens je analyseert het en

295
00:09:38,519 --> 00:09:40,200
dat is een soort deductief een deductief

296
00:09:40,200 --> 00:09:41,880
proces maar er zijn ook gevallen waarin je

297
00:09:41,880 --> 00:09:43,680
begint met een specifieke observatie je

298
00:09:43,680 --> 00:09:44,940
vindt enkele patronen en je trekt

299
00:09:44,940 --> 00:09:46,860
algemene conclusies goed en dan

300
00:09:46,860 --> 00:09:49,380
is er ontvoering waarbij je op magische wijze

301
00:09:49,380 --> 00:09:52,019
uitvindt  hypothesen en en verklein de

302
00:09:52,019 --> 00:09:53,760
hypotheseruimte je zou niet echt zeggen

303
00:09:53,760 --> 00:09:55,620
dat deductief redeneren onwetenschappelijk is

304
00:09:55,620 --> 00:09:58,200
of inductief redeneren onwetenschappelijk

305
00:09:58,200 --> 00:10:00,360
of abductief redeneren is onwetenschappelijk

306
00:10:00,360 --> 00:10:01,800
juist dit zijn allemaal gewoon verschillende manieren

307
00:10:01,800 --> 00:10:03,540
om dingen te doen

308
00:10:03,540 --> 00:10:05,459
eh ik bedoel, in je paper geef je de

309
00:10:05,459 --> 00:10:08,399
voorbeelden van het gebruik  modellen om uh

310
00:10:08,399 --> 00:10:09,779
orkanen en pandemieën te voorspellen als

311
00:10:09,779 --> 00:10:12,060
voorbeelden van dingen die zo rigoureus zijn als de

312
00:10:12,060 --> 00:10:13,860
wetenschap maar kan en dan smeek je je

313
00:10:13,860 --> 00:10:16,019
lezer om te concluderen dat de situatie

314
00:10:16,019 --> 00:10:18,120
niet anders is voor taalmodellen eh,

315
00:10:18,120 --> 00:10:19,920
maar ik denk dat het probleem voor mij is dat

316
00:10:19,920 --> 00:10:22,200
modellen die orkanen voorspellen dat niet zijn  in

317
00:10:22,200 --> 00:10:23,940
de business van het beantwoorden van de vraag

318
00:10:23,940 --> 00:10:25,740
wat is de orkaan

319
00:10:25,740 --> 00:10:27,420
juiste modellen die nauwkeurig het

320
00:10:27,420 --> 00:10:29,040
weer voorspellen zijn zeer nauwkeurig, maar ze zijn

321
00:10:29,040 --> 00:10:30,600
niet, weet je, ze zijn afgestemd op de

322
00:10:30,600 --> 00:10:32,700
meteorologieafdeling, maar ze zijn geen

323
00:10:32,700 --> 00:10:34,380
vervanging voor het

324
00:10:34,380 --> 00:10:35,760
eh, dus ik denk dat ik '  Ik

325
00:10:35,760 --> 00:10:37,620


326
00:10:37,620 --> 00:10:41,820


327
00:10:41,820 --> 00:10:43,800
denk dat ik zou kunnen beginnen met gewoon te

328
00:10:43,800 --> 00:10:45,240
zeggen dat

329
00:10:45,240 --> 00:10:47,579
eh ik ben het eens met veel van deze

330
00:10:47,579 --> 00:10:51,300
kritieken over uh deze

331
00:10:51,300 --> 00:10:54,000
modellen worden gecontroleerd door uh weet je

332
00:10:54,000 --> 00:10:56,220
een of twee bedrijven eh

333
00:10:56,220 --> 00:10:59,160
dat is heel erg problematisch

334
00:10:59,160 --> 00:11:01,320
eh weet je, ze hebben allerlei

335
00:11:01,320 --> 00:11:03,480
vooroordelen die ze hebben opgedaan omdat

336
00:11:03,480 --> 00:11:04,980
ze zijn getraind op tekst van

337
00:11:04,980 --> 00:11:06,180
internet

338
00:11:06,180 --> 00:11:08,640
eh dat is enorm problematisch

339
00:11:08,640 --> 00:11:10,980
eh weet je, ik ben het er zeker mee eens dat er

340
00:11:10,980 --> 00:11:13,079
eh dingen zijn  tenminste op dit moment

341
00:11:13,079 --> 00:11:16,500
doen de modellen het niet goed, dus eh

342
00:11:16,500 --> 00:11:18,420
ik denk dat het gemakkelijk is om

343
00:11:18,420 --> 00:11:20,640
voorbeelden te vinden van je weet vragen en

344
00:11:20,640 --> 00:11:23,100
problemen waardoor ze struikelen

345
00:11:23,100 --> 00:11:25,500
eh ik denk waarom ik enthousiast over

346
00:11:25,500 --> 00:11:26,760
ze ben geweest, hoewel

347
00:11:26,760 --> 00:11:29,820
eh niet  uh niet noodzakelijkerwijs in die

348
00:11:29,820 --> 00:11:32,399
termen, maar in termen van

349
00:11:32,399 --> 00:11:34,920
taalprestaties,

350
00:11:34,920 --> 00:11:38,459
eh specifiek syntaxis en semantiek,

351
00:11:38,459 --> 00:11:40,980
eh, ik denk dat ze veel verder gaan dan

352
00:11:40,980 --> 00:11:43,019
elke andere theorie in elk ander

353
00:11:43,019 --> 00:11:46,920
domein, dus er is geen andere

354
00:11:46,920 --> 00:11:49,380
theorie uit  taalkunde of

355
00:11:49,380 --> 00:11:53,100
computerwetenschap die kan leiden tot lange

356
00:11:53,100 --> 00:11:56,700
coherente uh grammaticale uh passages van

357
00:11:56,700 --> 00:11:58,500
tekst

358
00:11:58,500 --> 00:12:01,140
um en zo min of meer al hun

359
00:12:01,140 --> 00:12:04,920
problemen toegeven, aangezien uh je kent tools of of

360
00:12:04,920 --> 00:12:08,220
dingen die door bedrijven worden ingezet

361
00:12:08,220 --> 00:12:09,959
um um er is nog steeds deze kwestie

362
00:12:09,959 --> 00:12:12,899
van zoiets  hoe gaan ze om met

363
00:12:12,899 --> 00:12:14,760
taal en

364
00:12:14,760 --> 00:12:16,320
eh ik denk dat dit is waar veel van het

365
00:12:16,320 --> 00:12:17,760
enthousiasme vandaan komt, dat er echt

366
00:12:17,760 --> 00:12:20,160
niets is geweest dat ook maar in de verste verte op

367
00:12:20,160 --> 00:12:23,700
hen lijkt in termen van taalvaardigheid

368
00:12:23,700 --> 00:12:24,899
eh en dat is het ding waarvan ik

369
00:12:24,899 --> 00:12:27,660
denk dat het is  is opwindend, dus ja, ik ben het eens

370
00:12:27,660 --> 00:12:29,220
met een aantal van deze dingen waarmee je bent

371
00:12:29,220 --> 00:12:31,320
begonnen

372
00:12:31,320 --> 00:12:33,300
eh, maar desalniettemin, zoals ik denk in

373
00:12:33,300 --> 00:12:35,100
termen van syntaxis en semantiek,

374
00:12:35,100 --> 00:12:37,079
is er gewoon geen andere theorie die

375
00:12:37,079 --> 00:12:39,240
vergelijkbaar is met hen

376
00:12:39,240 --> 00:12:40,140
eh,

377
00:12:40,140 --> 00:12:42,120
maar dus laat me, laat me pushen  dat toen,

378
00:12:42,120 --> 00:12:44,940
dus ja, ik zou het belangrijkste

379
00:12:44,940 --> 00:12:46,320
bezwaar zijn van veel mensen met wie ik heb

380
00:12:46,320 --> 00:12:48,000
gesproken op de afdelingen taalkunde,

381
00:12:48,000 --> 00:12:50,760
die net als veel van de generaal zijn,

382
00:12:50,760 --> 00:12:53,100
weet je, het eerste van je paper is om echt te

383
00:12:53,100 --> 00:12:54,120
zeggen '

384
00:12:54,120 --> 00:12:55,620
nou, je kent je'  klopt, ze doen het

385
00:12:55,620 --> 00:12:57,360
geweldig, ze modelleren nauwkeurig

386
00:12:57,360 --> 00:12:58,920
alle aspecten van veel aspecten van

387
00:12:58,920 --> 00:13:01,380
syntaxis en semantiek, maar

388
00:13:01,380 --> 00:13:03,300
eh, ik ken er geen echt, zoals

389
00:13:03,300 --> 00:13:04,920
je weet Chomsky praat over feiten over

390
00:13:04,920 --> 00:13:06,660
taal, wat een ouderwets

391
00:13:06,660 --> 00:13:09,000
begrip is  en maar ik denk echt dat dat

392
00:13:09,000 --> 00:13:10,260
een nogal belangrijk idee is, te goed,

393
00:13:10,260 --> 00:13:12,899
zoals is er een ontdekking over

394
00:13:12,899 --> 00:13:16,860
taal zelf die llms op unieke wijze kan

395
00:13:16,860 --> 00:13:19,200
bieden, dus als llm een ​​voorspelling heeft gedaan

396
00:13:19,200 --> 00:13:21,899
over laten we zeggen dat je een

397
00:13:21,899 --> 00:13:24,600
zinsstructuur hebt Type X is

398
00:13:24,600 --> 00:13:26,279
moeilijker te verwerken dan zin  type

399
00:13:26,279 --> 00:13:28,620
Y en dit is een unieke voorspelling dat

400
00:13:28,620 --> 00:13:31,200
alleen zij het zouden genereren en geen menselijke

401
00:13:31,200 --> 00:13:33,899
taalkundige Chomsky homestein geen van deze

402
00:13:33,899 --> 00:13:34,980
mensen had dat ooit eerder voorspeld,

403
00:13:34,980 --> 00:13:37,260
maar het blijkt waar te zijn je doet eye

404
00:13:37,260 --> 00:13:38,760
tracking experimenten je doet allerlei

405
00:13:38,760 --> 00:13:40,800
verschillende gedragservaringen  en ach

406
00:13:40,800 --> 00:13:42,480
weet je het blijkt toch waar te zijn

407
00:13:42,480 --> 00:13:44,279
dit is een nieuw inzicht over

408
00:13:44,279 --> 00:13:45,839
taalverwerking het is een nieuw inzicht

409
00:13:45,839 --> 00:13:47,940
over taal weet je Gedrag Ik

410
00:13:47,940 --> 00:13:49,560
vraag me alleen af ​​Ik zeg niet Ik zeg niet

411
00:13:49,560 --> 00:13:51,240
dat dit in principe niet mogelijk is

412
00:13:51,240 --> 00:13:52,620
omdat het in de nabije toekomst zou kunnen gebeuren,

413
00:13:52,620 --> 00:13:54,540
maar dat is voor mij denk ik de

414
00:13:54,540 --> 00:13:57,240
kern waarom veel taalkundigen spreken over

415
00:13:57,240 --> 00:13:59,519
spreken namens de hele

416
00:13:59,519 --> 00:14:02,040
taalgemeenschap hier en weet je,

417
00:14:02,040 --> 00:14:03,180
ik denk dat dat een van de belangrijkste bezwaren zou zijn

418
00:14:03,180 --> 00:14:04,800


419
00:14:04,800 --> 00:14:08,279
ja, ik bedoel ik ik  weet niet van uh ik

420
00:14:08,279 --> 00:14:10,019
denk ik ik denk aan de inzichten die ze hebben

421
00:14:10,019 --> 00:14:12,420
gegeven als een soort algemene principes,

422
00:14:12,420 --> 00:14:14,459
dus eh,

423
00:14:14,459 --> 00:14:16,200
ik denk aan deze dingen zoals

424
00:14:16,200 --> 00:14:18,720
de kracht van het onthouden van stukjes

425
00:14:18,720 --> 00:14:20,820
taal, dus alsof ze

426
00:14:20,820 --> 00:14:22,500
lijken  om bijvoorbeeld heel goed te zijn in constructies

427
00:14:22,500 --> 00:14:24,540
en er zijn veel

428
00:14:24,540 --> 00:14:26,160
taaltheorieën, met name Chomsky's,

429
00:14:26,160 --> 00:14:28,740
die gaan over het

430
00:14:28,740 --> 00:14:30,899
proberen om een ​​soort van minimale hoeveelheden

431
00:14:30,899 --> 00:14:33,660
structuur te vinden om te onthouden.

432
00:14:33,660 --> 00:14:36,480


433
00:14:36,480 --> 00:14:38,279
verzameling

434
00:14:38,279 --> 00:14:40,440
van bewerkingen

435
00:14:40,440 --> 00:14:42,300
eh en ik denk dat dat niet goed is gegaan voor

436
00:14:42,300 --> 00:14:44,459
die theorieën, juist eh, terwijl dit

437
00:14:44,459 --> 00:14:46,740
heel goed gaat, dus eh,

438
00:14:46,740 --> 00:14:48,360
als we nadenken over iets dat

439
00:14:48,360 --> 00:14:50,040
de memorisatiecapaciteiten heeft, als we

440
00:14:50,040 --> 00:14:51,300
nadenken over theorieën over grammatica,

441
00:14:51,300 --> 00:14:54,240
bijvoorbeeld die uh bouwen  op

442
00:14:54,240 --> 00:14:56,040
eh, weet je, mensen houden van een echt

443
00:14:56,040 --> 00:14:58,139
opmerkelijk vermogen om

444
00:14:58,139 --> 00:14:59,880
verschillende constructies te onthouden, juiste of

445
00:14:59,880 --> 00:15:01,380
verschillende woorden, we kennen

446
00:15:01,380 --> 00:15:02,940
tienduizenden woorden, tienduizenden

447
00:15:02,940 --> 00:15:04,380
verschillende constructies, sorry,

448
00:15:04,380 --> 00:15:06,420
tienduizenden verschillende idiomen, misschien

449
00:15:06,420 --> 00:15:07,920
moet onze grammaticatheorie daarmee worden geïntegreerd

450
00:15:07,920 --> 00:15:10,260
en zij  zijn in zekere zin een

451
00:15:10,260 --> 00:15:12,480
soort bewijs van het principe dat dat

452
00:15:12,480 --> 00:15:15,660
soort benadering goed kan werken, goed,

453
00:15:15,660 --> 00:15:17,279
uhm kan erover nadenken om er andere soorten

454
00:15:17,279 --> 00:15:19,380
voorspellingen mee te doen, sommige

455
00:15:19,380 --> 00:15:21,779
mensen doen uh momenteel, maar

456
00:15:21,779 --> 00:15:23,339
ze proberen ze bijvoorbeeld te gebruiken om te

457
00:15:23,339 --> 00:15:25,860
meten  uh verwerkingsmoeilijkheid meet

458
00:15:25,860 --> 00:15:27,899
bijvoorbeeld verrassing van deze

459
00:15:27,899 --> 00:15:29,160
modellen

460
00:15:29,160 --> 00:15:30,600
eh er zijn verrassingsmetingen die juist

461
00:15:30,600 --> 00:15:31,699


462
00:15:31,699 --> 00:15:34,260
veel beter zijn dan bijvoorbeeld contextvrije

463
00:15:34,260 --> 00:15:36,120
grammatica's of andere soorten

464
00:15:36,120 --> 00:15:37,440
taalmodellen en dan is het een interessante

465
00:15:37,440 --> 00:15:39,899
vraag hoe die uh verrassingen of

466
00:15:39,899 --> 00:15:41,940
voorspelbaarheden zich verhouden tot menselijke

467
00:15:41,940 --> 00:15:44,399
verwerkingsrechten en het  kan een deel ervan vangen

468
00:15:44,399 --> 00:15:46,620
of kan niet-lineair zijn of het kan,

469
00:15:46,620 --> 00:15:49,079
weet je, slechts een klein beetje ervan vangen

470
00:15:49,079 --> 00:15:51,420
of wat dan ook, dat is een interessant

471
00:15:51,420 --> 00:15:53,940
soort andere wetenschappelijke vraag, maar ik

472
00:15:53,940 --> 00:15:55,500
denk dat ze in principe gelijk hebben, ze kunnen

473
00:15:55,500 --> 00:15:57,779
voorspellingen doen over bijvoorbeeld de

474
00:15:57,779 --> 00:15:59,880
verbanden  tussen zinnen, dus

475
00:15:59,880 --> 00:16:02,279
in de krant gaf ik dit voorbeeld van

476
00:16:02,279 --> 00:16:05,459
weet je, een verklaring omzetten in een

477
00:16:05,459 --> 00:16:07,920
vraag op 10 verschillende manieren, goed en

478
00:16:07,920 --> 00:16:10,620
vermoedelijk als je weet dat GPT of

479
00:16:10,620 --> 00:16:12,540
iets aan het doen is, dat het

480
00:16:12,540 --> 00:16:15,420
10 verschillende vragen vindt die allemaal uh

481
00:16:15,420 --> 00:16:18,060
in zijn  op de een of andere manier gerelateerd soort van dichtbij in

482
00:16:18,060 --> 00:16:20,339
de modellen die ten grondslag liggen aan semantische of

483
00:16:20,339 --> 00:16:22,139
syntactische ruimte

484
00:16:22,139 --> 00:16:24,660
eh en dus dat soort dingen zijn eh

485
00:16:24,660 --> 00:16:26,459
van het type waarvan ik denk eh uh

486
00:16:26,459 --> 00:16:28,560
weet je, sommige taalkundigen willen misschien

487
00:16:28,560 --> 00:16:30,180
juist, wat hier een verborgen

488
00:16:30,180 --> 00:16:32,220
verband is tussen zinnen of hun of

489
00:16:32,220 --> 00:16:34,320
hun  structuren, maar voor zover ik weet

490
00:16:34,320 --> 00:16:36,120
zijn ze nog niet empirisch geëvalueerd,

491
00:16:36,120 --> 00:16:39,660
dus ja ja, ik bedoel, dit

492
00:16:39,660 --> 00:16:41,220
soort modellen zijn nog maar een paar jaar oud,

493
00:16:41,220 --> 00:16:43,440
dus ik denk dat

494
00:16:43,440 --> 00:16:45,000
het redelijk is om er enthousiast over te zijn,

495
00:16:45,000 --> 00:16:46,259
ook al heeft dit soort werk

496
00:16:46,259 --> 00:16:48,540
't is nog niet gedaan nee dat klopt nee

497
00:16:48,540 --> 00:16:50,880
helemaal helemaal ik bedoel maar ik denk dat ik

498
00:16:50,880 --> 00:16:51,720
denk dat dat het juiste

499
00:16:51,720 --> 00:16:52,920
perspectief is om te nemen, maar ik denk dat dit

500
00:16:52,920 --> 00:16:54,779
de kwestie raakt van de

501
00:16:54,779 --> 00:16:56,579
eh die je noemde verrassing je noemde

502
00:16:56,579 --> 00:16:58,139
laneability

503
00:16:58,139 --> 00:17:00,839
eh je kent LMS en wat syntaxis maar  ze

504
00:17:00,839 --> 00:17:03,060
doen dit met duidelijk veel meer gegevens

505
00:17:03,060 --> 00:17:04,439
dan baby's doen

506
00:17:04,439 --> 00:17:06,240
um zodanig dat observaties van potentiële

507
00:17:06,240 --> 00:17:09,000
structuur op zichzelf geen

508
00:17:09,000 --> 00:17:10,799
reputatie zijn van de armoede van de

509
00:17:10,799 --> 00:17:12,540
stimulus, nou ja, de zwakkere versie zou ik

510
00:17:12,540 --> 00:17:13,500
moeten zeggen van de armoede van

511
00:17:13,500 --> 00:17:15,059
onderscheidend argument, dus het simpele feit

512
00:17:15,059 --> 00:17:17,579
dat LMS kan doen wat ze doen met onze

513
00:17:17,579 --> 00:17:19,559
grammaticale prijs is heel opvallend. Ik ben het

514
00:17:19,559 --> 00:17:20,939
ermee eens en in feite zou je

515
00:17:20,939 --> 00:17:22,500
dat misschien vijf of zes of

516
00:17:22,500 --> 00:17:23,699
zeven jaar geleden niet hebben voorspeld,

517
00:17:23,699 --> 00:17:25,260
maar het doet niets af aan de

518
00:17:25,260 --> 00:17:28,199
bewering dat mensen verrassing hebben en wij

519
00:17:28,199 --> 00:17:30,120
breng die gebeden mee en dus

520
00:17:30,120 --> 00:17:31,080
om te zien of

521
00:17:31,080 --> 00:17:33,299
computerlinguïstiek hypothesen en

522
00:17:33,299 --> 00:17:34,740
theoretische taalkunde kan beperken, wat ik denk dat het

523
00:17:34,740 --> 00:17:36,480
kan, trouwens, dit moet worden gedaan

524
00:17:36,480 --> 00:17:38,039
met je weet wel, zorgvuldige experimenten

525
00:17:38,039 --> 00:17:39,720
waarin verschillende leerparameters worden

526
00:17:39,720 --> 00:17:43,080
gecontroleerd en gigantische taalmodellen

527
00:17:43,080 --> 00:17:45,299
zoals gbt gratis zijn eigenlijk

528
00:17:45,299 --> 00:17:47,520
nutteloos hier en dit dus dit wordt

529
00:17:47,520 --> 00:17:49,559
een beetje tegellens en klachten dat we

530
00:17:49,559 --> 00:17:51,960
zoiets als een baby LM-project nodig hebben

531
00:17:51,960 --> 00:17:53,700
waarvan ik weet dat je erin geïnteresseerd bent en

532
00:17:53,700 --> 00:17:55,320
waar we meer hebben weet je ecologisch

533
00:17:55,320 --> 00:17:56,940
verantwoorde trainingssets voor jou  doe de

534
00:17:56,940 --> 00:17:58,380
voorspelling in je paper dat er enige

535
00:17:58,380 --> 00:17:59,640
structuur uit zal worden geleerd waarvan ik

536
00:17:59,640 --> 00:18:01,679
vermoed dat je daar misschien gelijk hebt

537
00:18:01,679 --> 00:18:03,059
eh, maar je weet dat zelfs met de

538
00:18:03,059 --> 00:18:04,679
baby LM-uitdaging er nog steeds het soort

539
00:18:04,679 --> 00:18:07,260
niet-triviale probleem is om meer traditionele problemen aan te pakken,

540
00:18:07,260 --> 00:18:09,419
zoals wanneer  de kinderen

541
00:18:09,419 --> 00:18:11,400
beginnen te generaliseren op basis van de hoeveelheid

542
00:18:11,400 --> 00:18:13,200
huidige input op basis van verschillende

543
00:18:13,200 --> 00:18:15,539
factoren cross-linguïstisch en dat

544
00:18:15,539 --> 00:18:17,700
vereist gewoon traditioneel, je kent

545
00:18:17,700 --> 00:18:18,720
psycholinguïstiek en

546
00:18:18,720 --> 00:18:21,539
taalverwerving, dus LMS, weet je, geeft

547
00:18:21,539 --> 00:18:22,919
om zaken als frequentie en

548
00:18:22,919 --> 00:18:24,600
verrassing, zoals je zei, maar er is

549
00:18:24,600 --> 00:18:26,160
echt een leuke  papier van Sophie [ __ ] en

550
00:18:26,160 --> 00:18:27,600
Andrea Martin het echt mooie papier

551
00:18:27,600 --> 00:18:30,000
en dat je volgens mij misschien hebt gezien dat

552
00:18:30,000 --> 00:18:31,620
heel mooi laat zien dat

553
00:18:31,620 --> 00:18:34,080
verdelingsstatistieken soms een aanwijzing kunnen zijn voor

554
00:18:34,080 --> 00:18:36,240
momenten van structuuropbouw, maar het

555
00:18:36,240 --> 00:18:37,919
vervangt deze noties met betrekking tot

556
00:18:37,919 --> 00:18:39,660
compositie, dus ik zal gewoon  lees een citaat

557
00:18:39,660 --> 00:18:42,360
van Chomsky 57 dat veel lijkt op

558
00:18:42,360 --> 00:18:45,240
wat um slots en meer en zeg ondanks

559
00:18:45,240 --> 00:18:47,820
onmiskenbare belangen en belang

560
00:18:47,820 --> 00:18:49,440
um van semantische en statistische

561
00:18:49,440 --> 00:18:51,360
taalmodellen dat ze geen directe

562
00:18:51,360 --> 00:18:52,919
relevantie lijken te hebben voor het probleem van het bepalen

563
00:18:52,919 --> 00:18:54,600
of karakteriseren van de reeks grammaticale

564
00:18:54,600 --> 00:18:56,340
verschillen  Ik denk dat we gedwongen zijn

565
00:18:56,340 --> 00:18:57,840
te concluderen dat grammatica autonoom is

566
00:18:57,840 --> 00:18:59,520
en onafhankelijk van betekenis en dat

567
00:18:59,520 --> 00:19:01,320
probabilistische modellen geen specifiek

568
00:19:01,320 --> 00:19:03,179
inzicht geven in enkele van de basisproblemen

569
00:19:03,179 --> 00:19:05,880
van de syntactische structuur, zodat de tweede uh

570
00:19:05,880 --> 00:19:07,980
hedge van de van de van de tweede

571
00:19:07,980 --> 00:19:10,380
zin bleek te zijn  onjuist, maar

572
00:19:10,380 --> 00:19:11,580
het is dus waar dat je weet wat er

573
00:19:11,580 --> 00:19:13,740
duidelijk wordt gezegd dat statistische modellen

574
00:19:13,740 --> 00:19:16,559
in 57 beschikbaar zijn, niet langer nauwkeurig zijn wanneer ze worden toegepast

575
00:19:16,559 --> 00:19:18,600
op modellen van vandaag en dat

576
00:19:18,600 --> 00:19:20,340
abstracte generalisaties kunnen maken over nieuwe

577
00:19:20,340 --> 00:19:21,900
strings en verdelingscategorieën, zoals

578
00:19:21,900 --> 00:19:23,640
je al zei, maar de prestaties

579
00:19:23,640 --> 00:19:25,380
van een enkel model  levert geen

580
00:19:25,380 --> 00:19:27,480
direct bewijs voor of tegen de

581
00:19:27,480 --> 00:19:29,760
landbaarheid van een bepaalde structuur door

582
00:19:29,760 --> 00:19:31,440
de enorme afstand te geven tussen elk

583
00:19:31,440 --> 00:19:33,900
computermodel dat vandaag beschikbaar is en

584
00:19:33,900 --> 00:19:36,240
het menselijk brein. Succes van het model

585
00:19:36,240 --> 00:19:38,100
betekent niet dat de structuur noodzakelijkerwijs

586
00:19:38,100 --> 00:19:40,679
land is en het falen van het model betekent ook niet

587
00:19:40,679 --> 00:19:42,539
dat de  structuur is niet leerbaar,

588
00:19:42,539 --> 00:19:44,520
ja

589
00:19:44,520 --> 00:19:47,100
ja, dus ik bedoel, ik denk dat het misschien de

590
00:19:47,100 --> 00:19:49,380
moeite waard is om een ​​paar

591
00:19:49,380 --> 00:19:51,000
verschillende versies van

592
00:19:51,000 --> 00:19:52,620
leerbaarheidsargumenten uit te pakken die mensen hebben gemaakt, omdat

593
00:19:52,620 --> 00:19:55,679
er eh, heel, heel sterke onmogelijkheidsclaims zijn geweest, die

594
00:19:55,679 --> 00:19:57,720


595
00:19:57,720 --> 00:19:59,160
uit een soort Chomsky-

596
00:19:59,160 --> 00:20:01,260
traditie komen, toch  dat we nooit

597
00:20:01,260 --> 00:20:04,140
beweringen zijn over de hoeveelheid gegevens die

598
00:20:04,140 --> 00:20:05,820
nodig was, er waren beweringen over

599
00:20:05,820 --> 00:20:08,220
het logische probleem van het leren van talen

600
00:20:08,220 --> 00:20:10,740
en dat het gewoon onmogelijk was,

601
00:20:10,740 --> 00:20:12,120
goed, het was onmogelijk zonder

602
00:20:12,120 --> 00:20:15,720
uh uh uh soort substantiële

603
00:20:15,720 --> 00:20:17,580
beperkingen op de  de klasse van

604
00:20:17,580 --> 00:20:19,380
talen of de klasse van grammatica's

605
00:20:19,380 --> 00:20:21,660
die je zou willen verwerven

606
00:20:21,660 --> 00:20:23,220
eh en mensen hebben lang

607
00:20:23,220 --> 00:20:25,380
gediscussieerd tegen die versie van

608
00:20:25,380 --> 00:20:26,340
dingen

609
00:20:26,340 --> 00:20:27,120
eh eh

610
00:20:27,120 --> 00:20:28,860
je weet dat er oud werk is van by

611
00:20:28,860 --> 00:20:30,419
by gold en dan is er een hele vorm van

612
00:20:30,419 --> 00:20:32,580
grammatica  verwervingstheorieën

613
00:20:32,580 --> 00:20:35,220
gebouwd op die traditie uh die zich veel zorgen maken

614
00:20:35,220 --> 00:20:37,980
over het soort uh volgorde

615
00:20:37,980 --> 00:20:39,240
waarin je verschillende

616
00:20:39,240 --> 00:20:40,860
hypothesen doorloopt en verschillende

617
00:20:40,860 --> 00:20:42,600
opties en dingen overweegt

618
00:20:42,600 --> 00:20:43,380
um

619
00:20:43,380 --> 00:20:45,059
um en mijn favoriete referentie hierin

620
00:20:45,059 --> 00:20:46,919
is dit artikel van

621
00:20:46,919 --> 00:20:49,799
um Nick jader en  Paul Vetan um noemde

622
00:20:49,799 --> 00:20:51,539
zoiets als ideaal leren van

623
00:20:51,539 --> 00:20:53,039
natuurlijke taal

624
00:20:53,039 --> 00:20:54,840
um dat in feite aantoont dat een

625
00:20:54,840 --> 00:20:57,539
ongedwongen leerling eh met

626
00:20:57,539 --> 00:21:00,480
voldoende gegevens het eh het soort

627
00:21:00,480 --> 00:21:01,860
genererende regels of het

628
00:21:01,860 --> 00:21:03,360
genererende grammatica

629
00:21:03,360 --> 00:21:05,640
um gewoon door het observeren van strings kan verwerven, maar dat

630
00:21:05,640 --> 00:21:08,160
dat papier  was eigenlijk

631
00:21:08,160 --> 00:21:10,200
in reactie op dit enorme oeuvre

632
00:21:10,200 --> 00:21:13,140
dat argumenteerde dat leren

633
00:21:13,140 --> 00:21:15,120
van positieve voorbeelden, dus alleen het

634
00:21:15,120 --> 00:21:17,820
observeren van strings, logisch

635
00:21:17,820 --> 00:21:20,520
onmogelijk was, dus eh,

636
00:21:20,520 --> 00:21:23,460
natuurlijk weet je dat mensen in

637
00:21:23,460 --> 00:21:25,679
Chomsky's Tradition echt van die

638
00:21:25,679 --> 00:21:28,140
vorm van  argument omdat het er een was dat

639
00:21:28,140 --> 00:21:30,419
zei uh je moest iets

640
00:21:30,419 --> 00:21:33,299
aangeboren uh gespecificeerd hebben om

641
00:21:33,299 --> 00:21:34,919
taalverwerving te laten werken het was als een

642
00:21:34,919 --> 00:21:36,600
soort wiskundig argument juist

643
00:21:36,600 --> 00:21:39,480
dat je een soort

644
00:21:39,480 --> 00:21:41,220
aangeboren grammatica of aangeboren ordening van

645
00:21:41,220 --> 00:21:42,900
hypothesen moest hebben of  iets en dat

646
00:21:42,900 --> 00:21:45,659
bleek gewoon helemaal verkeerd te zijn,

647
00:21:45,659 --> 00:21:48,299
dus als je, je weet wel, naar iets

648
00:21:48,299 --> 00:21:50,520
meer realistische leeromgevingen gaat,

649
00:21:50,520 --> 00:21:53,340
wat Tater en Vatani doen,

650
00:21:53,340 --> 00:21:55,500
eh, dan blijkt dat je als een

651
00:21:55,500 --> 00:21:57,240
geïdealiseerde leerling dingen kunt verwerven en

652
00:21:57,240 --> 00:21:59,100
er is geen  uitspraken over

653
00:21:59,100 --> 00:22:00,600
eh de hoeveelheid gegevens die

654
00:22:00,600 --> 00:22:02,700
zelfs daar nodig is, dat is het soort

655
00:22:02,700 --> 00:22:06,780
puur logische eh vermogen om te leren eh

656
00:22:06,780 --> 00:22:08,940
en dat vermogen is wat ik denk dat

657
00:22:08,940 --> 00:22:11,460
de eh grote versies van grote

658
00:22:11,460 --> 00:22:13,679
taalmodellen ook spreken, dus Chader

659
00:22:13,679 --> 00:22:15,600
invitani en andere andere  werk een beetje in

660
00:22:15,600 --> 00:22:17,460
die geest

661
00:22:17,460 --> 00:22:19,500
eh is je weet wiskundig en soort van

662
00:22:19,500 --> 00:22:21,480
argumenteren in principe, maar

663
00:22:21,480 --> 00:22:24,720
creëerde nooit iets dat echt een eh

664
00:22:24,720 --> 00:22:27,840
een grammaticarecht was of een echt soort

665
00:22:27,840 --> 00:22:30,000
geïmplementeerd taalmodel

666
00:22:30,000 --> 00:22:32,640
eh dus zelfs jij kent een model dat is

667
00:22:32,640 --> 00:22:35,400
getraind  op 100 miljoen of 100 miljard of

668
00:22:35,400 --> 00:22:38,340
hoeveel tokens ook, eh,

669
00:22:38,340 --> 00:22:41,159
zelfs dat soort model is volgens mij

670
00:22:41,159 --> 00:22:43,080
relevant voor die versie van het

671
00:22:43,080 --> 00:22:46,140
debat.

672
00:22:46,140 --> 00:22:48,659


673
00:22:48,659 --> 00:22:51,360


674
00:22:51,360 --> 00:22:53,640
en dan is er een tweede versie,

675
00:22:53,640 --> 00:22:56,760
goed, dat is uh, kunnen we taal leren

676
00:22:56,760 --> 00:22:59,760
met de specifieke gegevens die kinderen

677
00:22:59,760 --> 00:23:02,340
goed krijgen en dat is zowel de hoeveelheid gegevens

678
00:23:02,340 --> 00:23:04,620
als de vorm van de gegevens

679
00:23:04,620 --> 00:23:06,360
um en dus voor mensen die

680
00:23:06,360 --> 00:23:08,520
de baby LM-uitdaging niet kennen

681
00:23:08,520 --> 00:23:10,440
um uh is um uh

682
00:23:10,440 --> 00:23:12,840
dit uh

683
00:23:12,840 --> 00:23:14,340
um sorry om het een

684
00:23:14,340 --> 00:23:16,620
wedstrijd te noemen of een

685
00:23:16,620 --> 00:23:20,580
um a uh uh Ik denk dat het een uitdaging is om

686
00:23:20,580 --> 00:23:22,380
mensen zover te krijgen dat ze

687
00:23:22,380 --> 00:23:24,539
taalmodellen trainen op menselijke hoeveelheden

688
00:23:24,539 --> 00:23:27,179
data eh dus dat is meer zoiets

689
00:23:27,179 --> 00:23:28,679
eh ik denk dat er twee verschillende

690
00:23:28,679 --> 00:23:31,140
versies zijn 10 of 100 miljoen verschillende

691
00:23:31,140 --> 00:23:33,120
um tot 10 of 100 miljoen verschillende woorden

692
00:23:33,120 --> 00:23:35,640
in de trainingsset

693
00:23:35,640 --> 00:23:38,100
eh wat is zoals je weet 100ste of

694
00:23:38,100 --> 00:23:41,159
1000ste of zoiets groots als

695
00:23:41,159 --> 00:23:43,140
eh deze grote AI-bedrijven gebruiken voor

696
00:23:43,140 --> 00:23:46,620
hun taal  modellen eh en

697
00:23:46,620 --> 00:23:48,720
um Ik denk eigenlijk dat het is alsof

698
00:23:48,720 --> 00:23:50,340
dat precies het juiste soort

699
00:23:50,340 --> 00:23:52,020
ding is en precies wat het veld nodig heeft,

700
00:23:52,020 --> 00:23:54,780
want je zou kunnen ontdekken dat je op eh

701
00:23:54,780 --> 00:23:57,059
een hoeveelheid gegevens ter grootte van een kind

702
00:23:57,059 --> 00:23:58,980
eh in wezen de juiste syntaxis kunt leren,

703
00:23:58,980 --> 00:24:01,140
wat ik denk dat dat zou doen  zou het

704
00:24:01,140 --> 00:24:02,880
sterkste argument zijn tegen deze

705
00:24:02,880 --> 00:24:04,260
beweringen over armoede aan prikkels, je zou als

706
00:24:04,260 --> 00:24:06,299
alternatief kunnen ontdekken dat

707
00:24:06,299 --> 00:24:08,220
eh, misschien kun je niet veel leren,

708
00:24:08,220 --> 00:24:10,740
eh, misschien weet je, je weet dat je

709
00:24:10,740 --> 00:24:12,840
een veel krommer soort taalmodel bedenkt

710
00:24:12,840 --> 00:24:14,700
of dat het een aantal syntactische of

711
00:24:14,700 --> 00:24:16,799
semantische vaardigheden mist, eh

712
00:24:16,799 --> 00:24:17,520


713
00:24:17,520 --> 00:24:18,720
eh, ik denk eigenlijk dat de

714
00:24:18,720 --> 00:24:20,220
mislukkingen daar een beetje moeilijk te

715
00:24:20,220 --> 00:24:22,620
interpreteren zijn, omdat

716
00:24:22,620 --> 00:24:24,720
eh, kinderen uh gegevens wanneer ze daadwerkelijk

717
00:24:24,720 --> 00:24:26,400
taal leren, krijgen ze veel meer

718
00:24:26,400 --> 00:24:28,919
gegevens dan alleen uh alleen reeksen

719
00:24:28,919 --> 00:24:31,200
zinnen, waar ze interactie mee hebben

720
00:24:31,200 --> 00:24:33,059
in een omgeving

721
00:24:33,059 --> 00:24:34,440
eh, dus er zijn dingen in de wereld waar

722
00:24:34,440 --> 00:24:36,539
ze bij zijn eh hun uitingen zijn

723
00:24:36,539 --> 00:24:38,520
ook interactief, dus je kunt

724
00:24:38,520 --> 00:24:39,960
iets zeggen en zien of je

725
00:24:39,960 --> 00:24:41,400
ouder je bijvoorbeeld het ding brengt waar je om vraagt,

726
00:24:41,400 --> 00:24:43,799
dat is al lang

727
00:24:43,799 --> 00:24:46,020
beweerd door mensen

728
00:24:46,020 --> 00:24:49,020
um um als  a a je kent belangrijke cue

729
00:24:49,020 --> 00:24:51,240
in taalverwerving

730
00:24:51,240 --> 00:24:52,799
um dus

731
00:24:52,799 --> 00:24:55,440
um uh in de baby LM-uitdaging

732
00:24:55,440 --> 00:24:58,200
is er een mogelijkheid om deze

733
00:24:58,200 --> 00:25:00,960
modellen te trainen uh met een soort multimodale input

734
00:25:00,960 --> 00:25:02,340
Ik denk dat je ze net zoveel videogegevens kunt geven

735
00:25:02,340 --> 00:25:05,039
als je wilt geven

736
00:25:05,039 --> 00:25:07,140
um  uh maar waarschijnlijk is het moeilijk om

737
00:25:07,140 --> 00:25:09,000
precies het type opstelling

738
00:25:09,000 --> 00:25:11,220
en en feedback te repliceren dat kinderen daadwerkelijk krijgen,

739
00:25:11,220 --> 00:25:12,840
dus eh

740
00:25:12,840 --> 00:25:14,700
uh ik weet niet, weet je, ik ben

741
00:25:14,700 --> 00:25:17,700
opgewonden om te zien, uh, waar dat heen gaat

742
00:25:17,700 --> 00:25:19,799
en hoe dingen  pan out there

743
00:25:19,799 --> 00:25:20,760
um um

744
00:25:20,760 --> 00:25:23,700
uh weet je ik denk dat er een

745
00:25:23,700 --> 00:25:26,580
interessante verwante vraag is voor grote

746
00:25:26,580 --> 00:25:28,320
taalmodellen

747
00:25:28,320 --> 00:25:30,299
eh die is zoals wat het

748
00:25:30,299 --> 00:25:31,919
precies begrijpt wat alle

749
00:25:31,919 --> 00:25:34,200
gegevens doen, dus

750
00:25:34,200 --> 00:25:36,659
eh het zou kunnen zijn dat je zoveel gegevens nodig hebt

751
00:25:36,659 --> 00:25:38,279
voor  deze modellen omdat

752
00:25:38,279 --> 00:25:40,620
ze in feite een vorm

753
00:25:40,620 --> 00:25:43,919
van semantiek intern uitvinden, dus eh,

754
00:25:43,919 --> 00:25:45,600
ze ontdekken allebei de

755
00:25:45,600 --> 00:25:47,460
syntaxisregel en daar lijken ze

756
00:25:47,460 --> 00:25:49,380
nogal wat te leren over woordbetekenissen

757
00:25:49,380 --> 00:25:50,940


758
00:25:50,940 --> 00:25:52,200
ehen

759
00:25:52,200 --> 00:25:54,059
eh, het is niet helemaal onduidelijk, ik denk

760
00:25:54,059 --> 00:25:56,220
hoe  veel van de gegevens in deze moderne

761
00:25:56,220 --> 00:25:58,919
modellen uh is nodig voor syntaxis versus

762
00:25:58,919 --> 00:26:00,240
semantiek

763
00:26:00,240 --> 00:26:02,159
eh mijn eigen gok denk ik dat zou zijn

764
00:26:02,159 --> 00:26:05,580
dat de syntactische kant eh waarschijnlijk

765
00:26:05,580 --> 00:26:07,860
veel minder gegevens vereist dan

766
00:26:07,860 --> 00:26:09,600
de semantische kant

767
00:26:09,600 --> 00:26:11,159
um eigenlijk een student een voormalige student

768
00:26:11,159 --> 00:26:12,960
van  de mijne Frank Malika en ik schreven

769
00:26:12,960 --> 00:26:15,120
een paar jaar geleden een paper waarin we probeerden in te schatten

770
00:26:15,120 --> 00:26:17,400
hoeveel informatie een leerling

771
00:26:17,400 --> 00:26:19,679
noodzakelijkerwijs zou moeten verwerven

772
00:26:19,679 --> 00:26:22,140
om de verschillende aspecten

773
00:26:22,140 --> 00:26:23,820
van taal te leren, dus je moet alle

774
00:26:23,820 --> 00:26:25,320
woorden leren en je leert hun forums die je

775
00:26:25,320 --> 00:26:26,880
leert  hun betekenissen, je kent waarschijnlijk

776
00:26:26,880 --> 00:26:28,320
hun frequenties, je moet de

777
00:26:28,320 --> 00:26:32,100
syntaxis leren en wat we eigenlijk

778
00:26:32,100 --> 00:26:34,080
in die analyse vonden, dat was, weet je,

779
00:26:34,080 --> 00:26:35,640
eigenlijk gewoon een soort berekening van de achterkant van de

780
00:26:35,640 --> 00:26:37,440
envelop voor elk van deze

781
00:26:37,440 --> 00:26:40,679
domeinen, is dat syntaxis eigenlijk heel

782
00:26:40,679 --> 00:26:42,779
weinig stukjes informatie is  er is niet

783
00:26:42,779 --> 00:26:46,400
zoveel informatie voor nodig om syntaxis te leren,

784
00:26:46,400 --> 00:26:49,320
terwijl zoals de meeste informatie die

785
00:26:49,320 --> 00:26:52,740
je verkrijgt eigenlijk voor semantiek is, dus als je

786
00:26:52,740 --> 00:26:55,740
specificeert, ken je 30 tot 50.000

787
00:26:55,740 --> 00:26:57,720
verschillende woordbetekenissen, je weet of zelfs

788
00:26:57,720 --> 00:27:00,000
als elke betekenis

789
00:27:00,000 --> 00:27:02,340
um uh maar een paar bits is  zo goed,

790
00:27:02,340 --> 00:27:04,620
dat vereist veel informatie en

791
00:27:04,620 --> 00:27:06,360
waarschijnlijk is elke vergadering meer dan een paar

792
00:27:06,360 --> 00:27:08,460
stukjes goed, dus

793
00:27:08,460 --> 00:27:11,279
eh, het zou zo kunnen zijn, dat zou

794
00:27:11,279 --> 00:27:12,659
me doen vermoeden dat wat er gebeurt

795
00:27:12,659 --> 00:27:14,820
met grote taalmodellen, is dat de meeste van

796
00:27:14,820 --> 00:27:16,320
hun trainingsgegevens over woorden gaan

797
00:27:16,320 --> 00:27:18,600
semantiek en je kunt nadenken over andere

798
00:27:18,600 --> 00:27:20,760
manieren waarop kinderen woordsemantiek goed krijgen,

799
00:27:20,760 --> 00:27:22,140
dat is niet alleen een soort gelijktijdig

800
00:27:22,140 --> 00:27:24,600
voorkomend patroon in

801
00:27:24,600 --> 00:27:27,360
tekst, maar ik ben het ermee eens dat dat allemaal in

802
00:27:27,360 --> 00:27:29,039
de lucht hangt en echt opwindend om te zien

803
00:27:29,039 --> 00:27:30,960
wat er zal gebeuren dus  ja, ik weet dat

804
00:27:30,960 --> 00:27:32,640
sommige van de eerdere resultaten ervan uit het

805
00:27:32,640 --> 00:27:35,279
laboratorium van Lindsay suggereren dat ze in ieder geval

806
00:27:35,279 --> 00:27:37,740
beperkt zijn tot ecologisch valide, je

807
00:27:37,740 --> 00:27:40,919
weet wel, trainingssites uh modellen lijken te

808
00:27:40,919 --> 00:27:42,840
generaliseren, je kent lineaire regels voor

809
00:27:42,840 --> 00:27:44,940
Engels ja, geen vraagvorming ROM

810
00:27:44,940 --> 00:27:46,620
anders dan de hiërarchische regel, de

811
00:27:46,620 --> 00:27:48,120
juiste hiërarchie  regel, dus ik denk dat er

812
00:27:48,120 --> 00:27:49,980
een echte betekenis is waarin je weet dat

813
00:27:49,980 --> 00:27:52,020
de ruimte van de juiste

814
00:27:52,020 --> 00:27:54,000
syntactische prijs en inductieve vooroordelen

815
00:27:54,000 --> 00:27:56,220
werkelijk is, moet nog echt worden vastgesteld,

816
00:27:56,220 --> 00:27:57,960
maar het lijkt mij in ieder geval vrij

817
00:27:57,960 --> 00:27:59,400
duidelijk dat er een aantal zo moet zijn

818
00:27:59,400 --> 00:28:01,320
er is ook enig bewijs

819
00:28:01,320 --> 00:28:02,700
eh dat kinderen die in het Engels teruggaan

820
00:28:02,700 --> 00:28:04,679
naar deze frequentiekwestie dat kinderen in het

821
00:28:04,679 --> 00:28:06,360
Engels soms een

822
00:28:06,360 --> 00:28:08,100
tussenliggende kopie spellen van beweging in de

823
00:28:08,100 --> 00:28:10,140
gespecificeerde positie van de lagere

824
00:28:10,140 --> 00:28:11,880
voegwoordpositie van een lange

825
00:28:11,880 --> 00:28:13,980
afstand, wat vraag je je af, dus er is een proefschrift

826
00:28:13,980 --> 00:28:15,360
van Thornton en enkele  andere kranten

827
00:28:15,360 --> 00:28:18,120
hierover, dus ze zeggen eh welke persoon

828
00:28:18,120 --> 00:28:19,919
denk je die dat heeft gedaan in plaats van welke

829
00:28:19,919 --> 00:28:21,900
persoon denk je dat dat heeft gedaan, dus dit is

830
00:28:21,900 --> 00:28:23,159
een interessante weet je wel Miss-instelling

831
00:28:23,159 --> 00:28:25,020
omdat sommige talen

832
00:28:25,020 --> 00:28:26,760
deze tussenliggende kopieën daadwerkelijk spellen, maar

833
00:28:26,760 --> 00:28:28,799
Engels niet zo  het kind maakt de

834
00:28:28,799 --> 00:28:30,779
fout bij het instellen van zijn grammatica, maar de

835
00:28:30,779 --> 00:28:32,820
frequentie van de invoer is eigenlijk nul

836
00:28:32,820 --> 00:28:35,159
en dus heeft onze gemeenschappelijke vriend Gary Marcus

837
00:28:35,159 --> 00:28:36,779
ook een argument tegen het

838
00:28:36,779 --> 00:28:39,000
bepalen van de frequentie van de uitvoer van een kind in het geval

839
00:28:39,000 --> 00:28:41,159
van Duitse zelfstandige naamwoorden meervoud een meer regelmatige

840
00:28:41,159 --> 00:28:42,779
vorm van een bepaald soort  heeft de voorkeur,

841
00:28:42,779 --> 00:28:44,580
niet de frequente en er zijn veel

842
00:28:44,580 --> 00:28:46,559
voorbeelden zoals, dus er wordt soms beweerd

843
00:28:46,559 --> 00:28:49,080
dat proefpersonen passieve dingen ervaren waarbij

844
00:28:49,080 --> 00:28:50,460
de proefpersoon passief

845
00:28:50,460 --> 00:28:52,559
iets ervaart of erg vertraagd zijn bij kinderen in

846
00:28:52,559 --> 00:28:54,299
begripsstudies tot ongeveer acht

847
00:28:54,299 --> 00:28:56,039
omdat ze niet erg vaak voorkomen in de

848
00:28:56,039 --> 00:28:56,880
invoer.

849
00:28:56,880 --> 00:28:59,220
Maar Ken  Wexler en collega's hebben

850
00:28:59,220 --> 00:29:01,260
um subject experience Double H-

851
00:29:01,260 --> 00:29:03,539
vragen doorgenomen, zoals wie houdt van Mary en ze

852
00:29:03,539 --> 00:29:05,820
ontdekten dat deze net zo zeldzaam zijn

853
00:29:05,820 --> 00:29:07,500
in de invoer als subject en ervaring

854
00:29:07,500 --> 00:29:09,659
van passieve woorden, maar kinderen hebben geen probleem met het

855
00:29:09,659 --> 00:29:10,980
begrijpen van deze vragen,

856
00:29:10,980 --> 00:29:13,860
maar ze hebben wel problemen met het begrijpen

857
00:29:13,860 --> 00:29:16,020
onderwerpervaring van verbale passieven, dus

858
00:29:16,020 --> 00:29:17,880
frequentie lijkt opnieuw

859
00:29:17,880 --> 00:29:19,380
niet relevant of het is in ieder geval niet

860
00:29:19,380 --> 00:29:20,940
verklarend, toch? Ik denk dat het niet

861
00:29:20,940 --> 00:29:22,320
verklarend is met betrekking tot

862
00:29:22,320 --> 00:29:25,080
theorievorming, dus hoe kan LMS hierbij helpen,

863
00:29:25,080 --> 00:29:27,059
weet je, uiteenlopende gevallen wanneer er

864
00:29:27,059 --> 00:29:28,440
duidelijk iets anders aan de hand is dan

865
00:29:28,440 --> 00:29:30,960
frequentie  dus alums, je weet dat ze lijken te

866
00:29:30,960 --> 00:29:33,419
generaliseren, ga gewoon weer terug naar dit

867
00:29:33,419 --> 00:29:35,100
nummer van de cases die je in

868
00:29:35,100 --> 00:29:36,899
je paper hebt en je laat zien dat ze

869
00:29:36,899 --> 00:29:38,399
de structuur van kleurloze

870
00:29:38,399 --> 00:29:41,279
schermideeën generaliseren, wat vaak erg cool is,

871
00:29:41,279 --> 00:29:43,080
maar de positieve stimulans is nooit

872
00:29:43,080 --> 00:29:44,520
echt  ging over het niet

873
00:29:44,520 --> 00:29:46,380
statistisch kunnen leren van taal. Ik weet dat je

874
00:29:46,380 --> 00:29:48,059
die bewering juist hebt gedaan, maar Chomsky's

875
00:29:48,059 --> 00:29:49,559
punt in de jaren 50 over statistische

876
00:29:49,559 --> 00:29:51,779
modellen van de dag is niet waar voor

877
00:29:51,779 --> 00:29:53,700
commerciële LMS in 2023 en dat

878
00:29:53,700 --> 00:29:55,440
klopt, maar we kunnen dat ene punt niet gebruiken

879
00:29:55,440 --> 00:29:57,539
om te ondermijnen  je kent de hele

880
00:29:57,539 --> 00:29:59,520
geometrie Het basispunt van Enterprise Chomsky

881
00:29:59,520 --> 00:30:01,080
was dat je een grammaticale structuur zou kunnen hebben

882
00:30:01,080 --> 00:30:02,700
waarin elk

883
00:30:02,700 --> 00:30:04,980
diagram geen frequentie heeft en het ook

884
00:30:04,980 --> 00:30:06,899
geen duidelijk interpreteerbare

885
00:30:06,899 --> 00:30:08,039
instructies geeft aan de conceptuele

886
00:30:08,039 --> 00:30:09,840
interfaces, dus interfaces van andere

887
00:30:09,840 --> 00:30:11,580
systemen van de geest, dus zoals je laat zien

888
00:30:11,580 --> 00:30:13,980
je papieren GPT bootst voorbeelden na zoals pull

889
00:30:13,980 --> 00:30:16,559
the screen-ideeën en maar weet je,

890
00:30:16,559 --> 00:30:18,840
deze zin levert meer dan 150.000

891
00:30:18,840 --> 00:30:20,700
resultaten op Google op en het wordt

892
00:30:20,700 --> 00:30:22,620
uitgebreid besproken in de literatuur het kan

893
00:30:22,620 --> 00:30:24,480
het feit nabootsen dat het dit kan nabootsen, dit

894
00:30:24,480 --> 00:30:26,640
zegt ons eigenlijk niet veel  we kunnen in ieder geval

895
00:30:26,640 --> 00:30:27,840
niet echt iets met veel

896
00:30:27,840 --> 00:30:30,899
vertrouwen zeggen, dus je weet dat abiba achter

897
00:30:30,899 --> 00:30:32,580
University College Dublin dit citaat

898
00:30:32,580 --> 00:30:34,380
onlangs heeft uh, verwar je eigen

899
00:30:34,380 --> 00:30:36,840
goedgelovigheid niet met de intelligentie van een film en

900
00:30:36,840 --> 00:30:38,880
zelfs jong schreef de Coon vorig

901
00:30:38,880 --> 00:30:40,500
jaar dat critici gelijk hebben  beschuldig

902
00:30:40,500 --> 00:30:42,480
LMS ervan betrokken te zijn bij een soort

903
00:30:42,480 --> 00:30:43,919
mimiek

904
00:30:43,919 --> 00:30:46,860
eh en de voorbeeldzinnen van gbt

905
00:30:46,860 --> 00:30:48,840
die je in de krant geeft,

906
00:30:48,840 --> 00:30:50,700
doen het eigenlijk niet goed, want zoals je zegt,

907
00:30:50,700 --> 00:30:52,320
is het waarschijnlijk dat je betekenisloze

908
00:30:52,320 --> 00:30:54,240
talen kent die zeldzaam zijn in de trainingsgegevens, maar

909
00:30:54,240 --> 00:30:55,740
ze kunnen  doe het of ze kunnen het niet, maar er

910
00:30:55,740 --> 00:30:57,120
is geen middenweg om

911
00:30:57,120 --> 00:30:59,880
ons 10 voorbeelden als deze te geven, dus je

912
00:30:59,880 --> 00:31:02,880
hebt kleurloze groene ideeën die

913
00:31:02,880 --> 00:31:04,679
heel andere semantische objecten zijn dan

914
00:31:04,679 --> 00:31:06,840
zaken als glinsterende bruine konijnen

915
00:31:06,840 --> 00:31:09,899
witte glinsterende beren uh zwarte glanzende

916
00:31:09,899 --> 00:31:12,360
kangoeroes groene glinsterende apen

917
00:31:12,360 --> 00:31:15,120
geel oogverblindende leeuwen rode glinsterende

918
00:31:15,120 --> 00:31:16,320
elementen juist, dit zijn allemaal

919
00:31:16,320 --> 00:31:18,899
semantisch, semantisch raar en een beetje

920
00:31:18,899 --> 00:31:20,820
vreemd, maar het zijn nog steeds juridische

921
00:31:20,820 --> 00:31:22,440
structuren, het zijn een soort van betekenisvolle

922
00:31:22,440 --> 00:31:25,320
synthetische semantische objecten, eh, ja, ja, ik

923
00:31:25,320 --> 00:31:27,179


924
00:31:27,179 --> 00:31:29,580
zei net ja ja, ik, ik

925
00:31:29,580 --> 00:31:33,179
bedoel, dus ik, misschien kan ik ik  kan

926
00:31:33,179 --> 00:31:34,799
eerst op het eerste punt reageren,

927
00:31:34,799 --> 00:31:36,120
dus

928
00:31:36,120 --> 00:31:37,799
eh uh je begon te praten over

929
00:31:37,799 --> 00:31:40,140
deze andere uh soorten

930
00:31:40,140 --> 00:31:42,179
verwervingspatronen die misschien niet direct aansluiten

931
00:31:42,179 --> 00:31:44,279
op de frequentie

932
00:31:44,279 --> 00:31:47,159
eh en ik denk dat het eigenlijk een vergissing is

933
00:31:47,159 --> 00:31:50,640
om te denken dat eh soort moderne

934
00:31:50,640 --> 00:31:53,039
leermodellen  zou gewoon gebaseerd moeten zijn op frequentie,

935
00:31:53,039 --> 00:31:54,899
want

936
00:31:54,899 --> 00:31:57,120
eh, ze leren duidelijk als behoorlijk

937
00:31:57,120 --> 00:31:59,580
gecompliceerde families van regels of

938
00:31:59,580 --> 00:32:02,399
constructies of zoiets en

939
00:32:02,399 --> 00:32:03,899
eh, ik denk dat het zeer waarschijnlijk is dat wanneer

940
00:32:03,899 --> 00:32:06,600
ze leren, ze

941
00:32:06,600 --> 00:32:08,580
eh in zekere zin op zoek zijn naar een  eenvoudige

942
00:32:08,580 --> 00:32:10,559
of karige

943
00:32:10,559 --> 00:32:12,360
uitleg van de gegevens die

944
00:32:12,360 --> 00:32:13,980
ze goed hebben gezien en hoe dat in een cache

945
00:32:13,980 --> 00:32:16,080
in een neuraal netwerk wordt opgeslagen, is misschien

946
00:32:16,080 --> 00:32:19,200
ingewikkeld en je weet dat het afhangt van

947
00:32:19,200 --> 00:32:20,880
eh je kent parameters en de details

948
00:32:20,880 --> 00:32:22,679
van het leeralgoritme en en en

949
00:32:22,679 --> 00:32:24,480
dat soort dingen

950
00:32:24,480 --> 00:32:27,000
eh maar ik denk dat het is het is uh ik denk dat ik zou

951
00:32:27,000 --> 00:32:29,279
vermoeden misschien dat het waarschijnlijk

952
00:32:29,279 --> 00:32:31,740
het geval is dat

953
00:32:31,740 --> 00:32:35,640
um uh zoals ze zijn ze

954
00:32:35,640 --> 00:32:38,399
leren over een ingewikkelde reeks

955
00:32:38,399 --> 00:32:40,799
dingen juist een ingewikkeld soort  van

956
00:32:40,799 --> 00:32:43,799
familie van regels en constructies

957
00:32:43,799 --> 00:32:46,919
eh en dat betekent dat ik denk dat

958
00:32:46,919 --> 00:32:49,380
eh hun generalisaties kunnen zijn zoals de

959
00:32:49,380 --> 00:32:52,500
voorbeelden van mensen die je gaf

960
00:32:52,500 --> 00:32:55,440
um misschien een beetje discontinu zijn in de

961
00:32:55,440 --> 00:32:57,720
invoer, dus soms

962
00:32:57,720 --> 00:33:00,000
kun je je voorstellen dat je een aantal strings ziet die

963
00:33:00,000 --> 00:33:02,220
leiden  je naar een grammatica en de eenvoudigste

964
00:33:02,220 --> 00:33:03,840
grammatica van de gegevens die je

965
00:33:03,840 --> 00:33:06,059
tot nu toe hebt gezien, is er een die een

966
00:33:06,059 --> 00:33:09,120
ongeziene string voorspelt, en

967
00:33:09,120 --> 00:33:12,360
als dat gebeurt,

968
00:33:12,360 --> 00:33:14,100
neem je de gegevens en leer je een

969
00:33:14,100 --> 00:33:16,860
representatie die generaliseert in een

970
00:33:16,860 --> 00:33:19,440
nieuwe ongeziene  manier tot nu toe

971
00:33:19,440 --> 00:33:21,720
um puur omdat die generalisatie

972
00:33:21,720 --> 00:33:23,460
zo'n beetje de eenvoudigste weergave is van de

973
00:33:23,460 --> 00:33:25,019
gegevens die je tot nu toe hebt gezien,

974
00:33:25,019 --> 00:33:25,980
goed. Ik denk dat dat is wat

975
00:33:25,980 --> 00:33:28,320
taalkundigen probeerden goed te doen, probeer

976
00:33:28,320 --> 00:33:29,700
naar de gegevens te kijken en de

977
00:33:29,700 --> 00:33:31,260
theorie ervan en soms

978
00:33:31,260 --> 00:33:33,419
voorspelt die theorie een nieuw fenomeen

979
00:33:33,419 --> 00:33:35,840
of een nieuw type zin

980
00:33:35,840 --> 00:33:38,159
en dus als ze leren over een

981
00:33:38,159 --> 00:33:41,100
voldoende rijke ruimte aan theorieën

982
00:33:41,100 --> 00:33:42,899
eh, dan zou het

983
00:33:42,899 --> 00:33:45,000
voor hen niet onredelijk of onverwacht zijn  laat

984
00:33:45,000 --> 00:33:47,039
nu ook dat soort patronen zien

985
00:33:47,039 --> 00:33:48,899
of ze dat doen of niet, ik denk dat het

986
00:33:48,899 --> 00:33:51,299
nog steeds een open empirische vraag is,

987
00:33:51,299 --> 00:33:52,740


988
00:33:52,740 --> 00:33:54,120
juist omdat we ze moeten trainen op

989
00:33:54,120 --> 00:33:55,620
kleine hoeveelheden gegevens en hun

990
00:33:55,620 --> 00:33:57,480
generalisaties moeten testen en dit soort

991
00:33:57,480 --> 00:33:58,260
dingen,

992
00:33:58,260 --> 00:34:00,120
maar dat doe ik niet  Het feit

993
00:34:00,120 --> 00:34:01,620
dat je

994
00:34:01,620 --> 00:34:03,899
weet dat mensen dingen doen die

995
00:34:03,899 --> 00:34:06,059
niet puur op frequentie zijn gebaseerd, is hoe dan ook enig

996
00:34:06,059 --> 00:34:07,860
bewijs, want als

997
00:34:07,860 --> 00:34:09,300
je eenmaal over rijke en

998
00:34:09,300 --> 00:34:10,980
interessante klassen van theorieën leert, dan

999
00:34:10,980 --> 00:34:13,980
is dat het verwachte gedrag.

1000
00:34:13,980 --> 00:34:16,980
Ik had ongeveer een

1001
00:34:16,980 --> 00:34:18,719
jaar geleden een paper waarvan ik denk dat je

1002
00:34:18,719 --> 00:34:20,040
bekend bent met

1003
00:34:20,040 --> 00:34:22,560
um uh Yang en en pianta dosi waar

1004
00:34:22,560 --> 00:34:24,179
we waren

1005
00:34:24,179 --> 00:34:26,879
um uh aan het kijken naar eh

1006
00:34:26,879 --> 00:34:29,280
uh wat er gebeurt als je

1007
00:34:29,280 --> 00:34:32,639
een programma een leermodel geeft waarvan de strings van  uh

1008
00:34:32,639 --> 00:34:34,679
verschillende formele talen, dus denk aan

1009
00:34:34,679 --> 00:34:35,820


1010
00:34:35,820 --> 00:34:38,760
het geven van een algemeen model, alleen

1011
00:34:38,760 --> 00:34:41,159
weet je 10 of 20 misschien eenvoudige strings die

1012
00:34:41,159 --> 00:34:43,500
een bepaald patroon volgen en dan vragen om

1013
00:34:43,500 --> 00:34:46,199
een ​​programma te vinden dat uh die gegevens kan uitleggen,

1014
00:34:46,199 --> 00:34:49,320
wat vaak betekent dat je weet dat je

1015
00:34:49,320 --> 00:34:50,940
eh eh vindt wat  een soort van

1016
00:34:50,940 --> 00:34:52,679
programmatisch opschrijven van het

1017
00:34:52,679 --> 00:34:54,839
patroon in de strings

1018
00:34:54,839 --> 00:34:56,280
eh en in die figuur hebben we een paper

1019
00:34:56,280 --> 00:34:58,020
die echt relevant is voor dit

1020
00:34:58,020 --> 00:34:59,820
punt waar

1021
00:34:59,820 --> 00:35:02,640
eh de uh generalisaties die dat

1022
00:35:02,640 --> 00:35:04,200
soort model maakt

1023
00:35:04,200 --> 00:35:06,660
eh uh zijn denk ik een beetje  kwalitatief

1024
00:35:06,660 --> 00:35:08,160
zoals degene die je beschrijft voor

1025
00:35:08,160 --> 00:35:10,560
mensen precies waar

1026
00:35:10,560 --> 00:35:12,060
eh uh je ze een kleine hoeveelheid gegevens kunt geven

1027
00:35:12,060 --> 00:35:13,980
en het zal ongeziene

1028
00:35:13,980 --> 00:35:16,740
reeksen voorspellen met een zeer hoge waarschijnlijkheid

1029
00:35:16,740 --> 00:35:18,420
eh, ook al is er geen frequentie in

1030
00:35:18,420 --> 00:35:20,280
de trainingsinvoer, juist en de reden dat

1031
00:35:20,280 --> 00:35:22,740
het dat doet  is dat vaak de meest

1032
00:35:22,740 --> 00:35:24,660
beknopte computationele beschrijving van de

1033
00:35:24,660 --> 00:35:26,640
gegevens die je hebt gezien er een is die

1034
00:35:26,640 --> 00:35:29,640
een bepaalde uh nieuwe ongeziene

1035
00:35:29,640 --> 00:35:32,940
output voorspelt, zodat dat model

1036
00:35:32,940 --> 00:35:34,800
in wezen een implementatie is van het

1037
00:35:34,800 --> 00:35:36,839
soort Chader en Vitani-programma dat een

1038
00:35:36,839 --> 00:35:38,160


1039
00:35:38,160 --> 00:35:40,619
idee leert dat ik  eerder opgevoed

1040
00:35:40,619 --> 00:35:42,900
eh, maar het is er een waarvan ik denk dat je het

1041
00:35:42,900 --> 00:35:43,980
weet als je nadenkt in de context

1042
00:35:43,980 --> 00:35:45,660
van deze argumenten van kinderen die

1043
00:35:45,660 --> 00:35:48,599
ongebruikelijke of onverwachte dingen zeggen, zoals

1044
00:35:48,599 --> 00:35:50,280
wordt voorspeld door al dit soort

1045
00:35:50,280 --> 00:35:52,859
accounts, juist omdat zolang

1046
00:35:52,859 --> 00:35:54,180
zolang als  deze dingen

1047
00:35:54,180 --> 00:35:55,680
vergelijken effectief een interessante ruimte van

1048
00:35:55,680 --> 00:35:56,760
grammatica's

1049
00:35:56,760 --> 00:35:58,079
eh, dan zullen ze laten zien dat dat

1050
00:35:58,079 --> 00:35:59,760
soort gedrag ik denk

1051
00:35:59,760 --> 00:36:04,680
uh ja dus oké, dus ik denk dat je weet dat

1052
00:36:04,680 --> 00:36:06,960
het argument zou zijn dat de syntaxis in ieder geval vanuit

1053
00:36:06,960 --> 00:36:10,140
het genderperspectief

1054
00:36:10,140 --> 00:36:13,140
afzonderlijk functioneert  maar het is nog steeds

1055
00:36:13,140 --> 00:36:15,359
toegewezen aan semantiek, het informeert

1056
00:36:15,359 --> 00:36:17,040
pragmatiek, dus in het Midden-Oosten is de

1057
00:36:17,040 --> 00:36:18,480
syntaxis van programma's duidelijk zinloos.

1058
00:36:18,480 --> 00:36:20,160


1059
00:36:20,160 --> 00:36:22,320


1060
00:36:22,320 --> 00:36:24,180


1061
00:36:24,180 --> 00:36:26,220


1062
00:36:26,220 --> 00:36:27,900
categorisatie-

1063
00:36:27,900 --> 00:36:30,480
algoritme bij de at the um Center at the

1064
00:36:30,480 --> 00:36:32,220
conceptual systems

1065
00:36:32,220 --> 00:36:33,839
eh dus de architectuur van chomsky is een beetje

1066
00:36:33,839 --> 00:36:35,940
afhankelijk van het proces van het afbeelden van syntaxis

1067
00:36:35,940 --> 00:36:37,560
naar semantiek.

1068
00:36:37,560 --> 00:36:39,960


1069
00:36:39,960 --> 00:36:42,180


1070
00:36:42,180 --> 00:36:43,680
mapping proces precies

1071
00:36:43,680 --> 00:36:45,180
zoals waar is de mapping naar semantiek

1072
00:36:45,180 --> 00:36:47,160
en als er een mapping is, hoe

1073
00:36:47,160 --> 00:36:48,480
ziet het mappingproces eruit

1074
00:36:48,480 --> 00:36:50,099
wat zijn de eigenschappen van de semantiek

1075
00:36:50,099 --> 00:36:52,320
uh weet je wat doen deze wat de

1076
00:36:52,320 --> 00:36:54,000
eigenschappen van de semantiek plaatsen op

1077
00:36:54,000 --> 00:36:55,500
hun eigen sets van  beperkingen op het

1078
00:36:55,500 --> 00:36:57,180
marketingproces zoals ze doen voor

1079
00:36:57,180 --> 00:36:58,980
natuurlijke taal zijn ze een soort

1080
00:36:58,980 --> 00:37:01,079
weet je uh doen dit soort beperkingen

1081
00:37:01,079 --> 00:37:02,640
elkaar informeren is ze een soort heen

1082
00:37:02,640 --> 00:37:05,579
en weer proces net alsof elementen

1083
00:37:05,579 --> 00:37:07,260
deze vorm niet echt lijken te beschrijven, wat

1084
00:37:07,260 --> 00:37:09,359
betekent paren  correct zoals welke

1085
00:37:09,359 --> 00:37:11,339
betekenissen welke strings bijvoorbeeld goed nou

1086
00:37:11,339 --> 00:37:14,640
sorry zeg je dat

1087
00:37:14,640 --> 00:37:16,740
eh dat ze helemaal geen semantiek hebben

1088
00:37:16,740 --> 00:37:18,599
of zeg je dat er

1089
00:37:18,599 --> 00:37:21,180
gewoon geen duidelijke uh afbakening is tussen

1090
00:37:21,180 --> 00:37:23,400
hoe de structuren in kaart worden gebracht op de

1091
00:37:23,400 --> 00:37:25,500
semantiek ja de laatste  goed, dus ze

1092
00:37:25,500 --> 00:37:27,300
hebben duidelijk een potentieel een soort

1093
00:37:27,300 --> 00:37:28,800
semantiek. Ik weet dat je hebt gepleit voor een

1094
00:37:28,800 --> 00:37:30,180
conceptuele rol. Theorie is

1095
00:37:30,180 --> 00:37:31,920
hier relevant. De rest is misschien een

1096
00:37:31,920 --> 00:37:33,900
beetje mysterieuzer, maar de eigenlijke soja-

1097
00:37:33,900 --> 00:37:35,339
taalkunde. Er is een theorie

1098
00:37:35,339 --> 00:37:36,900
van het mappingproces.  zelf is het

1099
00:37:36,900 --> 00:37:39,060
expliciet en je kunt het in actie zien

1100
00:37:39,060 --> 00:37:40,440
en je kunt er verschillende theorieën over testen

1101
00:37:40,440 --> 00:37:42,000
in Psych taalkundige modellen en wat

1102
00:37:42,000 --> 00:37:44,700
heb je de eigenlijke regulering het

1103
00:37:44,700 --> 00:37:46,079
soort weet je beperkte ambiguïteit

1104
00:37:46,079 --> 00:37:48,119
ambiguïteit in de zin dat je één

1105
00:37:48,119 --> 00:37:50,400
woord meerdere betekenissen kent of één  structuur

1106
00:37:50,400 --> 00:37:53,040
meerdere interpretaties Enz, ja, ja,

1107
00:37:53,040 --> 00:37:55,200
ik bedoel, als je denkt dat ze

1108
00:37:55,200 --> 00:37:57,599
semantiek hebben, dan denk ik dat ze

1109
00:37:57,599 --> 00:37:59,640
een mapping moeten hebben van de syntaxis naar de

1110
00:37:59,640 --> 00:38:00,960
semantiek

1111
00:38:00,960 --> 00:38:03,599
eh, ik ben het ermee eens dat het niet zo is dat niemand

1112
00:38:03,599 --> 00:38:04,980
echt begrijpt hoe ze

1113
00:38:04,980 --> 00:38:07,560
op een diep niveau werken  klopt, dus ik ben het ermee eens dat

1114
00:38:07,560 --> 00:38:10,140
het niet zo duidelijk is als

1115
00:38:10,140 --> 00:38:11,940
zeg maar in generatieve syntaxis en

1116
00:38:11,940 --> 00:38:13,859
semantiek, waar

1117
00:38:13,859 --> 00:38:15,180
weet je dat je

1118
00:38:15,180 --> 00:38:17,880
de regels van compositie opschrijft en

1119
00:38:17,880 --> 00:38:19,920
een compositorische betekenis kunt ontlenen aan een

1120
00:38:19,920 --> 00:38:21,480
zin uit de samenstellende delen  of

1121
00:38:21,480 --> 00:38:23,339
iets goed zoals dat is eh

1122
00:38:23,339 --> 00:38:24,660
weet je dat is niet hoe ze

1123
00:38:24,660 --> 00:38:27,060
werken goed maar

1124
00:38:27,060 --> 00:38:28,859
eh ik ik gewoon ik ik zou niet als

1125
00:38:28,859 --> 00:38:32,160
vanzelfsprekend aannemen dat het zo moet zijn zoals

1126
00:38:32,160 --> 00:38:34,619
um uh het zou kunnen zijn dat hoe ze

1127
00:38:34,619 --> 00:38:36,420
werken eigenlijk is hoe we  werk goed

1128
00:38:36,420 --> 00:38:38,760
dat uh alles wordt weergegeven in

1129
00:38:38,760 --> 00:38:40,800
een of andere hoogdimensionale vectorruimte en

1130
00:38:40,800 --> 00:38:43,740
er is een ingewikkelde uh manier waarop

1131
00:38:43,740 --> 00:38:46,200
die vectorsemantiek wordt bijgewerkt met

1132
00:38:46,200 --> 00:38:48,240
elk extra woord of wat dan ook

1133
00:38:48,240 --> 00:38:51,480
in een taalstroom,

1134
00:38:51,480 --> 00:38:53,940
maar zoals ik denk dat het duidelijk is

1135
00:38:53,940 --> 00:38:55,320
dat ze wat hebben  soort

1136
00:38:55,320 --> 00:38:57,660
weergave van de semantiek van een

1137
00:38:57,660 --> 00:38:59,099
zin, zoals ze

1138
00:38:59,099 --> 00:39:01,260
bijvoorbeeld vragen kunnen beantwoorden, in ieder geval

1139
00:39:01,260 --> 00:39:02,760
ongeveer. Ik bedoel, het is niet

1140
00:39:02,760 --> 00:39:04,380
perfect, maar het

1141
00:39:04,380 --> 00:39:06,720
is niet zoals een engrammodel of

1142
00:39:06,720 --> 00:39:07,740
iets goeds dat echt

1143
00:39:07,740 --> 00:39:10,500
geen semantiek heeft, dus

1144
00:39:10,500 --> 00:39:12,780
eh ik denk dat ze eh ze zijn

1145
00:39:12,780 --> 00:39:14,040
zeker

1146
00:39:14,040 --> 00:39:16,920
semantiek vertegenwoordigen en

1147
00:39:16,920 --> 00:39:20,040
eh uh weet je dat bij te werken terwijl ze

1148
00:39:20,040 --> 00:39:21,599
taal verwerken, het toevallig

1149
00:39:21,599 --> 00:39:23,400
niet lijkt op deze andere formele

1150
00:39:23,400 --> 00:39:24,599
theorieën

1151
00:39:24,599 --> 00:39:26,400
eh en ik denk dat ik het niet zie  waarom

1152
00:39:26,400 --> 00:39:27,720
dat een probleem is, net als die andere

1153
00:39:27,720 --> 00:39:29,640
formele theorieën, zou gewoon kunnen zijn, weet je,

1154
00:39:29,640 --> 00:39:31,920
slechte benaderingen of of gewoon helemaal

1155
00:39:31,920 --> 00:39:33,720
verkeerd, ja, ja,

1156
00:39:33,720 --> 00:39:35,520
nee, helemaal helemaal, ik bedoel,

1157
00:39:35,520 --> 00:39:37,260
er zijn ook manieren waarop sommige van de

1158
00:39:37,260 --> 00:39:39,359
formele theorieën in de semantiek al

1159
00:39:39,359 --> 00:39:41,520
potentieel compatibel zijn met wat sommige  van

1160
00:39:41,520 --> 00:39:42,780
deze dingen gaat goed, dus een andere

1161
00:39:42,780 --> 00:39:45,240
manier om hierover na te denken, is dat je weet dat LMS

1162
00:39:45,240 --> 00:39:47,760
in orde is LMS zijn compressie-algoritmen,

1163
00:39:47,760 --> 00:39:49,740
maar begrip van natuurlijke taal gaat

1164
00:39:49,740 --> 00:39:51,540
meer over decompressie

1165
00:39:51,540 --> 00:39:54,180
het is ondubbelzinnig betekenis X Geen

1166
00:39:54,180 --> 00:39:56,099
betekenissen XYZ het gaat allemaal om het maken van

1167
00:39:56,099 --> 00:39:58,020
gevolgtrekkingen over weet je  meta-relaties

1168
00:39:58,020 --> 00:39:59,579
tussen concepten die niet in de

1169
00:39:59,579 --> 00:40:01,859
trainingsgegevens staan, dus sommige voorbeelden die

1170
00:40:01,859 --> 00:40:03,180
Melanie Mitchell geeft, dingen zoals

1171
00:40:03,180 --> 00:40:06,119
je weet dat ze er weer bovenop zit

1172
00:40:06,119 --> 00:40:08,700
uh het staat bovenop de doos, dit

1173
00:40:08,700 --> 00:40:10,140
varieert allemaal met de context, dus er is veel

1174
00:40:10,140 --> 00:40:11,880
andere dingen die goed gaan

1175
00:40:11,880 --> 00:40:13,260
eh en ik denk dat je enkele van die

1176
00:40:13,260 --> 00:40:16,440
voorbeelden in je paper bespreekt, dus je weet

1177
00:40:16,440 --> 00:40:18,119
eh, maar het taalvermogen valt nog steeds

1178
00:40:18,119 --> 00:40:20,940
niet in ieder geval weer onder deze

1179
00:40:20,940 --> 00:40:22,920
taaltheorie en het gaat niet om het

1180
00:40:22,920 --> 00:40:25,200
genereren van snaren, het gaat om deze vorm  wat betekent

1181
00:40:25,200 --> 00:40:27,540
koppelingsmachine, dus soms denkt dit in de

1182
00:40:27,540 --> 00:40:29,400
genitieftraditie zelfs dat alles wat er

1183
00:40:29,400 --> 00:40:31,859
is aan semantiek rechtvaardig en juist is, dus

1184
00:40:31,859 --> 00:40:33,540
Paul Petrovsky's conjunctief is dat er

1185
00:40:33,540 --> 00:40:36,000
een deel is dat menselijke semantiek rechtvaardig is en

1186
00:40:36,000 --> 00:40:37,500
dat is het

1187
00:40:37,500 --> 00:40:39,900
eh wat weer is, is heel eenvoudig elegant

1188
00:40:39,900 --> 00:40:42,000
het is het is het is interpreteerbaar  het is

1189
00:40:42,000 --> 00:40:43,920
compatibel met veel van de dingen die

1190
00:40:43,920 --> 00:40:46,320
je weet of misschien in je

1191
00:40:46,320 --> 00:40:47,940
nek van de woorden gebeurt, toch, maar hoe dan ook,

1192
00:40:47,940 --> 00:40:49,859
je weet nog steeds dat natuurlijke taal

1193
00:40:49,859 --> 00:40:51,540
nog meer compositorisch is,

1194
00:40:51,540 --> 00:40:54,060
die dingen zoals uh, je kent formele

1195
00:40:54,060 --> 00:40:55,260
talen, gewoon om een ​​duidelijk

1196
00:40:55,260 --> 00:40:56,820
onderscheid te maken, dat is  zijn gemaakt ze hebben een

1197
00:40:56,820 --> 00:40:58,260
veel rijkere samenstelling van structuur

1198
00:40:58,260 --> 00:41:00,839
er is meer aan de hand uh misschien dus

1199
00:41:00,839 --> 00:41:02,160
er is al eerder op gewezen dat je

1200
00:41:02,160 --> 00:41:03,480
dingen weet als op aandacht gebaseerde

1201
00:41:03,480 --> 00:41:05,640
machinemechanismen en Transformers

1202
00:41:05,640 --> 00:41:07,800
um staat combinaties van discrete

1203
00:41:07,800 --> 00:41:10,320
tokenbindingen toe die meer in de buurt komen

1204
00:41:10,320 --> 00:41:12,359
van een samenvoeging  zoals operator dan eenvoudige

1205
00:41:12,359 --> 00:41:14,640
terugkerende matrixvermenigvuldiging

1206
00:41:14,640 --> 00:41:16,140
um maar je kent het probleem van binaire

1207
00:41:16,140 --> 00:41:17,460
vertakking binaire vertakking overheid om

1208
00:41:17,460 --> 00:41:19,260
hier een ander voorbeeld te kiezen om te

1209
00:41:19,260 --> 00:41:20,640
praten over de volledige betekenisregulering

1210
00:41:20,640 --> 00:41:23,640
één principe binaire vertakking afbeelding is

1211
00:41:23,640 --> 00:41:24,839
een interessante vraag, maar geometrische

1212
00:41:24,839 --> 00:41:26,700
grammatica staat altijd open voor

1213
00:41:26,700 --> 00:41:28,740
verschillende  Oorsprong en locaties van deze

1214
00:41:28,740 --> 00:41:30,660
schijnbare beperking in synthetische

1215
00:41:30,660 --> 00:41:31,800
berekeningen, zoals waar

1216
00:41:31,800 --> 00:41:33,540
komt het vandaan, misschien is het een voorwaarde voor

1217
00:41:33,540 --> 00:41:34,980
samenvoegen, misschien wordt het opgelegd door een soepel

1218
00:41:34,980 --> 00:41:37,079
systeem, misschien is het een soort Prior,

1219
00:41:37,079 --> 00:41:39,060
weet je, wie weet en in feite is het een

1220
00:41:39,060 --> 00:41:40,500
recentere werkende generatieve grammatica

1221
00:41:40,500 --> 00:41:43,859
heeft geprobeerd alle theoretische aannames van het huwelijksrecht te onderbouwen en af ​​te schaffen

1222
00:41:43,859 --> 00:41:46,440


1223
00:41:46,440 --> 00:41:47,700
misschien is de verzamelingenleer niet de beste

1224
00:41:47,700 --> 00:41:48,780
manier om

1225
00:41:48,780 --> 00:41:50,339
eh de generatieve grammatica te modelleren misschien

1226
00:41:50,339 --> 00:41:51,540
zijn de logische verslagen van Maria geschikter er zijn

1227
00:41:51,540 --> 00:41:53,520
veel andere recente ideeën

1228
00:41:53,520 --> 00:41:55,020
die allemaal compatibel zijn met  de

1229
00:41:55,020 --> 00:41:57,540
met Chomsky's benadering klopt,

1230
00:41:57,540 --> 00:41:58,680
ja, een van de dingen waar Trump

1231
00:41:58,680 --> 00:42:00,060
het meest van houdt, is wanneer hij het bij het

1232
00:42:00,060 --> 00:42:01,380
verkeerde eind heeft.

1233
00:42:01,380 --> 00:42:03,720


1234
00:42:03,720 --> 00:42:06,300


1235
00:42:06,300 --> 00:42:08,640


1236
00:42:08,640 --> 00:42:11,839
levendig veld de mensen die

1237
00:42:11,839 --> 00:42:15,119
Bornstein zijn, weet je petrovsky uh uh

1238
00:42:15,119 --> 00:42:17,520
hajipura ze zijn het op fundamentele

1239
00:42:17,520 --> 00:42:19,380
manieren oneens met veel van wat de heersende stroming

1240
00:42:19,380 --> 00:42:20,940
van de chemische grammatica zou zeggen, maar er

1241
00:42:20,940 --> 00:42:21,960
is nog meer ruimte voor

1242
00:42:21,960 --> 00:42:24,240
onenigheid, maar het is nog steeds compatibel

1243
00:42:24,240 --> 00:42:26,400
met het rechtzetten van kernaannames, dus

1244
00:42:26,400 --> 00:42:27,599
veel  David, ik draag bijvoorbeeld

1245
00:42:27,599 --> 00:42:29,400
een beetje afwijkingen in dit kernrespect,

1246
00:42:29,400 --> 00:42:31,920
maar het probeert nog steeds deze

1247
00:42:31,920 --> 00:42:33,240
intuïties in verschillende formele

1248
00:42:33,240 --> 00:42:34,500
systemen te verankeren,

1249
00:42:34,500 --> 00:42:36,000
dus je weet dat

1250
00:42:36,000 --> 00:42:38,099
het een soort van is,

1251
00:42:38,099 --> 00:42:40,320
ik wil je gedachten weer krijgen over eh,

1252
00:42:40,320 --> 00:42:42,240
ik noemde Mitchell goed, dus Michelin

1253
00:42:42,240 --> 00:42:44,579
Bowers uh  2020 ze hebben deze papieren

1254
00:42:44,579 --> 00:42:47,040
proeflijst met terugkerende netwerken die

1255
00:42:47,040 --> 00:42:48,359
merkwaardig genoeg liggen waarvan ik denk dat je je er misschien

1256
00:42:48,359 --> 00:42:49,859
bewust van bent, dus het is een heel goed

1257
00:42:49,859 --> 00:42:51,060
voorbeeld om tot de kern

1258
00:42:51,060 --> 00:42:53,099
van het probleem door te dringen, dus het

1259
00:42:53,099 --> 00:42:54,780
is aangetoond dat terugkerende neurale netwerken nauwkeurig

1260
00:42:54,780 --> 00:42:56,520
modelleren, weet je niet  -werkwoord nummerovereenkomst,

1261
00:42:56,520 --> 00:42:58,020
maar Mitchell en Barrow toonden aan dat

1262
00:42:58,020 --> 00:43:00,060
deze netwerken ook een nummerovereenkomst zullen krijgen

1263
00:43:00,060 --> 00:43:01,680
met onnatuurlijke

1264
00:43:01,680 --> 00:43:03,359
zinsstructuren, dus structuren die niet

1265
00:43:03,359 --> 00:43:04,859
voorkomen in natuurlijke taal en die

1266
00:43:04,859 --> 00:43:06,540
mensen moeilijk kunnen verwerken,

1267
00:43:06,540 --> 00:43:09,359
dus de manier van leren voor rnns is

1268
00:43:09,359 --> 00:43:11,880
tenminste  want rnn is positief verschillend

1269
00:43:11,880 --> 00:43:14,339
van baby, je kent baby Homo sapiens,

1270
00:43:14,339 --> 00:43:16,260
dus het verhaal is Mitchell en

1271
00:43:16,260 --> 00:43:18,359
Bowers laten zien dat hoewel het lstl-model

1272
00:43:18,359 --> 00:43:20,040
een goede weergave heeft van enkelvoud

1273
00:43:20,040 --> 00:43:22,140
versus meervoud voor individuele zinnen,

1274
00:43:22,140 --> 00:43:24,359
er geen generalisatie gaande is,

1275
00:43:24,359 --> 00:43:25,800
ze kunnen op individueel

1276
00:43:25,800 --> 00:43:27,359
niveau weergeven  dus het model heeft geen

1277
00:43:27,359 --> 00:43:28,859
representatie van getal als een

1278
00:43:28,859 --> 00:43:31,200
abstractie welk getal alleen concrete

1279
00:43:31,200 --> 00:43:34,020
voorbeelden zijn van enkelvoud basis meervoud

1280
00:43:34,020 --> 00:43:35,400
um dus het succesvol voorspellen van

1281
00:43:35,400 --> 00:43:38,579
taalgedrag via LM of het succesvol

1282
00:43:38,579 --> 00:43:40,800
voorspellen van neurale reacties op een vergelijkbare

1283
00:43:40,800 --> 00:43:42,480
manier is duidelijk geweldig en misschien kunnen we

1284
00:43:42,480 --> 00:43:43,920
ingaan op  dat probleem later, maar er is

1285
00:43:43,920 --> 00:43:45,240
hier maar één kant van de medaille, de

1286
00:43:45,240 --> 00:43:47,099
andere kant van de medaille legt uit waarom

1287
00:43:47,099 --> 00:43:48,780
dit soort gedrag en niet een ander

1288
00:43:48,780 --> 00:43:50,339
gedrag waarom deze structuur ik niet

1289
00:43:50,339 --> 00:43:52,740
vergelijkbaar ben en dat is misschien Chomsky's meest

1290
00:43:52,740 --> 00:43:55,380
en zoals je weet zijn meest  belangrijk

1291
00:43:55,380 --> 00:43:56,760
punt echt waarom dit niet een ander

1292
00:43:56,760 --> 00:43:59,400
systeem is, zo taalkundig Theorie

1293
00:43:59,400 --> 00:44:00,720
geeft je dat of het begin van de munt

1294
00:44:00,720 --> 00:44:02,760
goed, terwijl LM echt klaar is, dus de

1295
00:44:02,760 --> 00:44:03,839
beschaamde Mitchell doet

1296
00:44:03,839 --> 00:44:05,819
zoiets, hij doet het

1297
00:44:05,819 --> 00:44:09,420
goed ja dus zoals neem um Yael Le  crets

1298
00:44:09,420 --> 00:44:11,400
en stanislash de Haynes hadden vanaf 2019

1299
00:44:11,400 --> 00:44:13,319
gelijk, ze keken naar nummerovereenkomst in

1300
00:44:13,319 --> 00:44:15,480
een lstm en vonden twee gespecialiseerde eenheden

1301
00:44:15,480 --> 00:44:17,640
die nummerovereenkomst codeerden, maar de

1302
00:44:17,640 --> 00:44:19,020
algehele bijdrage aan de prestaties was

1303
00:44:19,020 --> 00:44:21,839
laag en toen in 2021 uh ja corrects

1304
00:44:21,839 --> 00:44:24,000
hadden dit papier waarin ze laten zien dat

1305
00:44:24,000 --> 00:44:26,040
um in  hun neurale taalmodel het

1306
00:44:26,040 --> 00:44:28,079
bereikte geen echte recursieve verwerking

1307
00:44:28,079 --> 00:44:30,540
van geneste langeafstandsovereenkomst geslachtsmarkering

1308
00:44:30,540 --> 00:44:32,160
in het Italiaans.

1309
00:44:32,160 --> 00:44:34,020


1310
00:44:34,020 --> 00:44:35,819


1311
00:44:35,819 --> 00:44:37,380


1312
00:44:37,380 --> 00:44:39,960
is

1313
00:44:39,960 --> 00:44:41,280
het de juiste mapping is het de juiste

1314
00:44:41,280 --> 00:44:42,900
soort hiërarchie ze ontdekten dat

1315
00:44:42,900 --> 00:44:45,119
lstn-gebaseerde modellen onderwerpwebovereenkomst konden krijgen

1316
00:44:45,119 --> 00:44:47,220
over korte overspanningen van één graad van

1317
00:44:47,220 --> 00:44:49,260
inbedding, maar ze faalden bij een aantal langere

1318
00:44:49,260 --> 00:44:51,359
afhankelijkheden en in de meest recente

1319
00:44:51,359 --> 00:44:53,700
paper uh La crêpe satell met  de hand

1320
00:44:53,700 --> 00:44:56,760
en toonde aan dat ze moderne

1321
00:44:56,760 --> 00:45:00,180
Transformer LMS inclusief gpt2 XL op

1322
00:45:00,180 --> 00:45:01,980
dezelfde taak evalueerden en dat de Transformers

1323
00:45:01,980 --> 00:45:04,260
meer op mensen lijken dan LSM

1324
00:45:04,260 --> 00:45:06,300
en in het algemeen boven de overdracht presteerden, maar

1325
00:45:06,300 --> 00:45:08,040
ze presteren nog steeds onder de kans in één

1326
00:45:08,040 --> 00:45:09,660
sleutelconditie, namelijk de zoals ik al

1327
00:45:09,660 --> 00:45:11,099
zei de  meerdere inbedding van een van de

1328
00:45:11,099 --> 00:45:13,020
moeilijke geschriften en dus de reden

1329
00:45:13,020 --> 00:45:14,400
waarom ik deze studies noemde, is omdat

1330
00:45:14,400 --> 00:45:17,040
je weet dat het niet alleen is om de

1331
00:45:17,040 --> 00:45:18,540
grenzen van OMS te verkennen, wat een interessante vraag is,

1332
00:45:18,540 --> 00:45:19,500


1333
00:45:19,500 --> 00:45:21,540
maar beschouw het werk van mensen als Neil

1334
00:45:21,540 --> 00:45:24,180
Smith aan de UCL goed en hij werkte in

1335
00:45:24,180 --> 00:45:26,579
de jaren 90 met een polyglot Savant en

1336
00:45:26,579 --> 00:45:28,740
neurotypische controles die ze vergeleken, dus

1337
00:45:28,740 --> 00:45:30,540
onderzocht hij het leren

1338
00:45:30,540 --> 00:45:32,520
van een tweede taal van een kunstmatige taal die

1339
00:45:32,520 --> 00:45:34,500
zowel natuurlijke als onnatuurlijke grafische

1340
00:45:34,500 --> 00:45:35,880
structuren bevat, zoals het Michelin-viruspapier.

1341
00:45:35,880 --> 00:45:37,079
Het hele raamwerk is natuurlijk

1342
00:45:37,079 --> 00:45:39,119
versus onnatuurlijk en ze ontdekten dat

1343
00:45:39,119 --> 00:45:41,000
terwijl zowel de savant

1344
00:45:41,000 --> 00:45:43,319
als  de bedieningselementen konden de

1345
00:45:43,319 --> 00:45:45,480
taalkundig natuurlijke aspecten beheersen alleen de

1346
00:45:45,480 --> 00:45:46,920
bedieningselementen konden uiteindelijk omgaan met de

1347
00:45:46,920 --> 00:45:48,660
structuurafhankelijke onnatuurlijke fenomenen

1348
00:45:48,660 --> 00:45:50,460
en geen van beiden kon de

1349
00:45:50,460 --> 00:45:52,560
structuuronafhankelijke aspecten beheersen, dus enkele

1350
00:45:52,560 --> 00:45:53,880
rare regels waarbij het is alsof je weet dat je

1351
00:45:53,880 --> 00:45:55,440
de nadruk legt op het derde woord van

1352
00:45:55,440 --> 00:45:56,940
de zin dingen  zo

1353
00:45:56,940 --> 00:45:58,740
beweren ze dat de capaciteiten van Christopher

1354
00:45:58,740 --> 00:46:00,900
volledig te danken zijn aan zijn intacte taalkundige

1355
00:46:00,900 --> 00:46:03,480
vermogens, maar de besturing zou meer domein kunnen gebruiken.  een

1356
00:46:03,480 --> 00:46:05,579


1357
00:46:05,579 --> 00:46:07,319


1358
00:46:07,319 --> 00:46:09,900


1359
00:46:09,900 --> 00:46:11,520


1360
00:46:11,520 --> 00:46:13,319
minuut

1361
00:46:13,319 --> 00:46:16,079
geleden dat de lstm in de

1362
00:46:16,079 --> 00:46:18,359
gênante paper van Mitchell natuurlijke en

1363
00:46:18,359 --> 00:46:19,920
onnatuurlijke structuren op vrijwel

1364
00:46:19,920 --> 00:46:22,319
dezelfde manier benadert, dus het is niet je weet dat het geen

1365
00:46:22,319 --> 00:46:24,240
psychologisch plausibel model is dat ik zou

1366
00:46:24,240 --> 00:46:26,400
betogen en voor wat mensen ook doen

1367
00:46:26,400 --> 00:46:28,200
en soortgelijke observaties kunnen van toepassing zijn op

1368
00:46:28,200 --> 00:46:30,060
de grenzen van  Transformatormodellen in het

1369
00:46:30,060 --> 00:46:31,920
werk van La creta en al deze thema's zijn

1370
00:46:31,920 --> 00:46:33,780
zo dat ze bij

1371
00:46:33,780 --> 00:46:35,819
ons blijven tot het heden, dus

1372
00:46:35,819 --> 00:46:37,560
een andere recente krant van Talins die hij

1373
00:46:37,560 --> 00:46:39,240
een paar weken geleden plaatste, waarin hij naar door kinderen

1374
00:46:39,240 --> 00:46:41,880
gerichte spraak keek, toonde aan dat eh  lstms en

1375
00:46:41,880 --> 00:46:43,740
Transformers beperkt tot ecologisch

1376
00:46:43,740 --> 00:46:46,380
plausibele hoeveelheden gegeneraliseerde gegevens, aangezien

1377
00:46:46,380 --> 00:46:47,640
ik de lineaire regels voor Engels

1378
00:46:47,640 --> 00:46:49,920
rechts noemde in plaats van de abstracte regels en

1379
00:46:49,920 --> 00:46:51,780
in feite meer recent werk van Linton's

1380
00:46:51,780 --> 00:46:54,599
Lab vorige week, kijkend naar uh nou ja, vorig

1381
00:46:54,599 --> 00:46:56,220
jaar zou ik moeten zeggen, laat zien dat kijken naar

1382
00:46:56,220 --> 00:46:58,160
Garden  paden verrassing verklaart niet

1383
00:46:58,160 --> 00:47:01,319
uh syntactische ondubbelzinnigheid moeilijkheid

1384
00:47:01,319 --> 00:47:02,280
juist

1385
00:47:02,280 --> 00:47:03,900
um verrassingen zullen de grootte

1386
00:47:03,900 --> 00:47:05,400
van het Garden Path-effect voor alle

1387
00:47:05,400 --> 00:47:06,780
constructies onderschatten en dit raakt aan dit

1388
00:47:06,780 --> 00:47:08,099
probleem dat je noemde voordat je het weet,

1389
00:47:08,099 --> 00:47:10,140
misschien verrast dit alles gerelateerd aan

1390
00:47:10,140 --> 00:47:11,520
sommige aspecten van syntaxis, maar misschien ook niet

1391
00:47:11,520 --> 00:47:12,960
andere het is een soort van het is een zeer

1392
00:47:12,960 --> 00:47:14,819
non-tribute-kwestie die heel erg

1393
00:47:14,819 --> 00:47:16,800
openstaat voor discussie het is niet het is

1394
00:47:16,800 --> 00:47:18,720
nog niet opgelost maar dus liet Linton zien

1395
00:47:18,720 --> 00:47:20,640
dat Garden Path-effecten gewoon

1396
00:47:20,640 --> 00:47:21,720
veel moeilijker zijn dan je

1397
00:47:21,720 --> 00:47:24,359
van mij zou verwachten  onvoorspelbaarheid dus een andere manier

1398
00:47:24,359 --> 00:47:26,160
om dit argument te formuleren

1399
00:47:26,160 --> 00:47:29,160
eh is het citaat een recent argument

1400
00:47:29,160 --> 00:47:30,660
met Chomsky's om tot deze natuurlijke

1401
00:47:30,660 --> 00:47:32,940
basis onnatuurlijke kwestie te komen, zegt hij, stel dat we

1402
00:47:32,940 --> 00:47:34,560
een uitgebreid periodiek systeem hebben dat

1403
00:47:34,560 --> 00:47:36,180
alle elementen bevat die wel bestaan

1404
00:47:36,180 --> 00:47:38,819
of de elementen die mogelijk kunnen  bestaan

1405
00:47:38,819 --> 00:47:40,740
en alle elementen die

1406
00:47:40,740 --> 00:47:42,660
onmogelijk kunnen bestaan ​​en laten we zeggen dat je

1407
00:47:42,660 --> 00:47:44,880
een model hebt uh een kunstmatig model dat

1408
00:47:44,880 --> 00:47:46,560
geen onderscheid maakt tussen deze drie

1409
00:47:46,560 --> 00:47:48,780
categorieën wat dit model ook doet het

1410
00:47:48,780 --> 00:47:50,640
helpt ons niet om scheikunde

1411
00:47:50,640 --> 00:47:52,020
goed te begrijpen het doet iets anders

1412
00:47:52,020 --> 00:47:53,940
het doet iets voor  zeker, maar

1413
00:47:53,940 --> 00:47:55,020
of het al dan niet scheikunde moeten begrijpen

1414
00:47:55,020 --> 00:47:57,180
is iets aparts en ik

1415
00:47:57,180 --> 00:47:58,560
weet dat je hebt gezegd in reactie op

1416
00:47:58,560 --> 00:47:59,579
sommige van deze onderzoeken, ik denk dat je hebt

1417
00:47:59,579 --> 00:48:02,400
gezegd dat je weet en om aan te tonen

1418
00:48:02,400 --> 00:48:03,540
dat iets waarschijnlijk

1419
00:48:03,540 --> 00:48:04,920
ergens onmogelijk is  in je paper

1420
00:48:04,920 --> 00:48:06,240
denk ik dat je

1421
00:48:06,240 --> 00:48:07,859
eh zegt om aan te tonen dat iets

1422
00:48:07,859 --> 00:48:09,720
onmogelijk is met een normale balans van

1423
00:48:09,720 --> 00:48:12,300
politiek op valse positieven, moet

1424
00:48:12,300 --> 00:48:13,440
je laten zien dat je naar zoiets als

1425
00:48:13,440 --> 00:48:15,780
500 onafhankelijk gesamplede talen moet kijken, dus

1426
00:48:15,780 --> 00:48:17,880
citeer je dit in de paper, goed eh

1427
00:48:17,880 --> 00:48:19,020
wat je  dat kan waarschijnlijk niet, dat is

1428
00:48:19,020 --> 00:48:20,579
gewoon niet, het is niet haalbaar om te doen

1429
00:48:20,579 --> 00:48:23,880
en dus weet je dat ik dat niet ben.

1430
00:48:23,880 --> 00:48:25,800


1431
00:48:25,800 --> 00:48:27,119


1432
00:48:27,119 --> 00:48:29,040


1433
00:48:29,040 --> 00:48:30,839
een argument over onmogelijkheid

1434
00:48:30,839 --> 00:48:32,220
in principe niet

1435
00:48:32,220 --> 00:48:33,599
eh in een soort van extensionele zin,

1436
00:48:33,599 --> 00:48:34,980
weet je, net als zoeken in

1437
00:48:34,980 --> 00:48:37,440
talen van de wereld om te zien om in

1438
00:48:37,440 --> 00:48:38,579
elke taal te bewijzen dat het

1439
00:48:38,579 --> 00:48:40,260
onmogelijk is, dat is een soort van, het is een

1440
00:48:40,260 --> 00:48:41,280
ander argument of het

1441
00:48:41,280 --> 00:48:43,740
onmogelijk is in een willekeurige taal  in

1442
00:48:43,740 --> 00:48:45,180
de Amazone vergeleken met eigenlijk

1443
00:48:45,180 --> 00:48:47,220
onmogelijk op basis van de principes van

1444
00:48:47,220 --> 00:48:48,420
wat het taalsysteem eigenlijk

1445
00:48:48,420 --> 00:48:50,160
doet en wat het kan doen, dus ik zou

1446
00:48:50,160 --> 00:48:53,339
gewoon zeggen dat ja, ik denk dat

1447
00:48:53,339 --> 00:48:55,980
dat punt is dat je eigenlijk niet

1448
00:48:55,980 --> 00:48:58,560
weet wat typologisch niet is  mogelijk,

1449
00:48:58,560 --> 00:49:00,480
dus sommige mensen zeggen graag dingen

1450
00:49:00,480 --> 00:49:02,520
zoals jij weet dat er geen taal is die

1451
00:49:02,520 --> 00:49:04,859
X goed doet, daarom moeten we die

1452
00:49:04,859 --> 00:49:06,960
beperking in onze statistische

1453
00:49:06,960 --> 00:49:09,119
modellen inbouwen, maar als het niet

1454
00:49:09,119 --> 00:49:11,220
statistisch verantwoord is dat er geen

1455
00:49:11,220 --> 00:49:13,079
taal is die X goed doet als je dat hebt gedaan  ik heb

1456
00:49:13,079 --> 00:49:15,119
alleen naar 20 of 20 Europese

1457
00:49:15,119 --> 00:49:16,920
talen gekeken of zoiets goed ik bedoel

1458
00:49:16,920 --> 00:49:19,020
het is niet

1459
00:49:19,020 --> 00:49:22,380
eh uh zoals dat zou niet moeten motiveren om

1460
00:49:22,380 --> 00:49:24,599
iets met de modellen te doen goed

1461
00:49:24,599 --> 00:49:26,280
um uh als het is als het geen

1462
00:49:26,280 --> 00:49:28,200
statistisch gerechtvaardigde universele is

1463
00:49:28,200 --> 00:49:28,980
denk ik

1464
00:49:28,980 --> 00:49:30,180
eh

1465
00:49:30,180 --> 00:49:32,760
goed weet je ik ik denk dat je  weet dat

1466
00:49:32,760 --> 00:49:33,960
je helemaal gelijk hebt, maar dat

1467
00:49:33,960 --> 00:49:35,339
geldt meer in het algemeen voor de sociale

1468
00:49:35,339 --> 00:49:36,960
wetenschappen en psychologische wetenschappen,

1469
00:49:36,960 --> 00:49:39,180
typologisch ja, het is erg

1470
00:49:39,180 --> 00:49:40,380
moeilijk om deze dingen goed vast te stellen,

1471
00:49:40,380 --> 00:49:43,140
dus ik denk dat je, ik denk dat je gewoon een

1472
00:49:43,140 --> 00:49:44,640
beetje muffe man bent, je bent een beetje  je

1473
00:49:44,640 --> 00:49:47,339
zegt dat de sterke bewering erg

1474
00:49:47,339 --> 00:49:49,859
moeilijk te bewijzen is,

1475
00:49:49,859 --> 00:49:52,619
zoals er geen taal is die X heeft de

1476
00:49:52,619 --> 00:49:54,480
sterke bewering dat iets niet is

1477
00:49:54,480 --> 00:49:56,339
toegestaan ​​in natuurlijke taal is volgens mij

1478
00:49:56,339 --> 00:49:58,800
heel erg moeilijk te bewijzen eh eh

1479
00:49:58,800 --> 00:49:59,880


1480
00:49:59,880 --> 00:50:01,980
en weet je, ik denk dat  dat er

1481
00:50:01,980 --> 00:50:05,460
uh velen van jullie sterke

1482
00:50:05,460 --> 00:50:08,460
pogingen zijn geweest er zijn veel sterke

1483
00:50:08,460 --> 00:50:10,380
beweringen geweest van

1484
00:50:10,380 --> 00:50:12,720
eh uh vaak van generatieve syntaxis

1485
00:50:12,720 --> 00:50:16,800
juist over wat alle talen doen

1486
00:50:16,800 --> 00:50:19,140
eh en ik denk dat je weet dat mensen

1487
00:50:19,140 --> 00:50:21,119
heel goed zijn geweest in het vinden van

1488
00:50:21,119 --> 00:50:22,740
tegenvoorbeelden  op veel van die

1489
00:50:22,740 --> 00:50:24,720
dingen citeer ik dit artikel van Evans

1490
00:50:24,720 --> 00:50:26,579
en en Levinson

1491
00:50:26,579 --> 00:50:28,500
um waarvan je eigenlijk weet dat ik

1492
00:50:28,500 --> 00:50:30,660
al jaren had gehoord over hoe geen enkele taal X doet

1493
00:50:30,660 --> 00:50:32,160
en dat is wat we gebruiken om

1494
00:50:32,160 --> 00:50:33,599
onze theorieën te construeren en die van Evans

1495
00:50:33,599 --> 00:50:35,460
en Levin  paper Evans en Levinson

1496
00:50:35,460 --> 00:50:37,859
paper zijn echt van gedachten veranderd

1497
00:50:37,859 --> 00:50:40,680
over dit recht dat taal

1498
00:50:40,680 --> 00:50:43,260
eigenlijk veel diverser is dan

1499
00:50:43,260 --> 00:50:44,940
ik denk dat de meeste

1500
00:50:44,940 --> 00:50:47,760
eh de meeste syntactici zullen proberen

1501
00:50:47,760 --> 00:50:50,700
theorieën te construeren voor iets dus

1502
00:50:50,700 --> 00:50:53,579
eh eh weet je ik ik denk dat we gaan  terug

1503
00:50:53,579 --> 00:50:54,839
naar het begin van wat je

1504
00:50:54,839 --> 00:50:57,660
zei, ik denk dat we het erover eens zijn dat

1505
00:50:57,660 --> 00:50:59,880
je taalarchitecturen nodig hebt die

1506
00:50:59,880 --> 00:51:01,619
de dingen leren die kinderen leren en

1507
00:51:01,619 --> 00:51:03,660
het leerden van gegevens die ze leren, en het is

1508
00:51:03,660 --> 00:51:05,940


1509
00:51:05,940 --> 00:51:09,000
onwaarschijnlijk dat die architecturen dingen zijn  zoals lstms of je

1510
00:51:09,000 --> 00:51:10,559
kent eenvoudige terugkerende netwerken of wat dan

1511
00:51:10,559 --> 00:51:12,300
ook, zoals eh

1512
00:51:12,300 --> 00:51:14,400
ik denk dat al dat werk erg

1513
00:51:14,400 --> 00:51:16,680
nuttig is om de

1514
00:51:16,680 --> 00:51:19,200
juiste architectuur aan te scherpen

1515
00:51:19,200 --> 00:51:20,460
um

1516
00:51:20,460 --> 00:51:21,420
um

1517
00:51:21,420 --> 00:51:23,819
uh dus ik probeer me gewoon alles te herinneren

1518
00:51:23,819 --> 00:51:25,200
de punten die je aan het maken was

1519
00:51:25,200 --> 00:51:26,700
oh ja dus eh

1520
00:51:26,700 --> 00:51:29,160
maar ik denk dat

1521
00:51:29,160 --> 00:51:31,500
dit een soort keerzijde heeft, namelijk dat

1522
00:51:31,500 --> 00:51:32,760


1523
00:51:32,760 --> 00:51:34,380
eh ik denk dat de ruimte van dingen die

1524
00:51:34,380 --> 00:51:37,619
mensen kunnen leren eigenlijk een beetje wordt

1525
00:51:37,619 --> 00:51:39,599
onderschat, zoals er is deze

1526
00:51:39,599 --> 00:51:41,760
vooringenomenheid  om te zeggen dat je weet dat mensen

1527
00:51:41,760 --> 00:51:43,800
x y en z niet kunnen leren,

1528
00:51:43,800 --> 00:51:46,020
maar mensen uh in ieder geval buiten de

1529
00:51:46,020 --> 00:51:47,460
taal hebben dit echt

1530
00:51:47,460 --> 00:51:49,680
opmerkelijke vermogen om verschillende

1531
00:51:49,680 --> 00:51:51,240
soorten patronen te leren, zoals de

1532
00:51:51,240 --> 00:51:53,160
patronen die je bijvoorbeeld in muziek of wiskunde aantreft

1533
00:51:53,160 --> 00:51:55,619


1534
00:51:55,619 --> 00:51:57,839
um uh  we kunnen geavanceerde soorten

1535
00:51:57,839 --> 00:52:00,240
algoritmen leren, we kunnen

1536
00:52:00,240 --> 00:52:03,119
je leren een spaceshuttle vliegen of je

1537
00:52:03,119 --> 00:52:05,760
weet wel knopen leggen voor rotsklimmen of

1538
00:52:05,760 --> 00:52:07,319
wat dan ook, daar is

1539
00:52:07,319 --> 00:52:09,540
allerlei soorten procedurele en

1540
00:52:09,540 --> 00:52:11,280
algoritmische kennis die

1541
00:52:11,280 --> 00:52:13,440
structureel is dat dat  mensen kunnen

1542
00:52:13,440 --> 00:52:16,680
verwerven en ik denk dat dat uh

1543
00:52:16,680 --> 00:52:19,680
idee uh heel terecht motiveert om op zoek te gaan

1544
00:52:19,680 --> 00:52:21,359
naar leersystemen die kunnen

1545
00:52:21,359 --> 00:52:24,059
werken in vrij onbeperkte ruimtes,

1546
00:52:24,059 --> 00:52:26,160
dus

1547
00:52:26,160 --> 00:52:28,859
eh uh je kent je, je zou kunnen zeggen

1548
00:52:28,859 --> 00:52:30,119
dat oké, taal is anders

1549
00:52:30,119 --> 00:52:33,420
omdat taal is  een beperkte ruimte

1550
00:52:33,420 --> 00:52:35,040
eh uh en het kan waar zijn dat die

1551
00:52:35,040 --> 00:52:36,540
taal beperkt is, maar het kan ook

1552
00:52:36,540 --> 00:52:37,859
waar zijn dat de dingen die we in

1553
00:52:37,859 --> 00:52:39,960
taal zien uit andere bronnen komen.

1554
00:52:39,960 --> 00:52:42,180


1555
00:52:42,180 --> 00:52:44,099


1556
00:52:44,099 --> 00:52:47,040


1557
00:52:47,040 --> 00:52:48,720
juist en dat soort pragmatische

1558
00:52:48,720 --> 00:52:50,040
beperkingen

1559
00:52:50,040 --> 00:52:51,480
eh zijn de dingen die de

1560
00:52:51,480 --> 00:52:53,400
vorm van taal beperken, recht of taal is

1561
00:52:53,400 --> 00:52:54,660
communicatief, het is waarschijnlijk meer

1562
00:52:54,660 --> 00:52:56,760
communicatief dan

1563
00:52:56,760 --> 00:52:58,440
bijvoorbeeld muziek en dat zou de

1564
00:52:58,440 --> 00:53:01,140
vorm van dingen kunnen beperken, dus ik bedoel, zoals je weet,

1565
00:53:01,140 --> 00:53:02,700
dit is erg  oud debat in de

1566
00:53:02,700 --> 00:53:05,520
taalkunde over waar de uh

1567
00:53:05,520 --> 00:53:07,140
waar de eigenschappen van natuurlijke

1568
00:53:07,140 --> 00:53:09,059
taal vandaan komen eh

1569
00:53:09,059 --> 00:53:11,160
en eh ik denk dat wat ik probeer te zeggen

1570
00:53:11,160 --> 00:53:12,660
is dat er één soort perspectief is

1571
00:53:12,660 --> 00:53:15,119
waar eh je kijkt naar alle dingen die

1572
00:53:15,119 --> 00:53:16,980
mensen kunnen  doe zelfs buiten taal

1573
00:53:16,980 --> 00:53:18,599
alle rijke structuren en

1574
00:53:18,599 --> 00:53:20,819
algoritmen en processen die we konden

1575
00:53:20,819 --> 00:53:23,760
leren over en internaliseren en

1576
00:53:23,760 --> 00:53:25,559
je zegt oké, misschien is taal zo

1577
00:53:25,559 --> 00:53:27,420
en ja, taal heeft ook enkele van

1578
00:53:27,420 --> 00:53:29,700
deze andere grappige kleine eigenschappen

1579
00:53:29,700 --> 00:53:31,380
eh, maar weet je misschien die  komen uit

1580
00:53:31,380 --> 00:53:34,020
een aantal andere delen van waar

1581
00:53:34,020 --> 00:53:36,720
taal vandaan komt juist het is uh het is

1582
00:53:36,720 --> 00:53:38,400
weet je, we hebben een behoorlijk verfijnde

1583
00:53:38,400 --> 00:53:40,800
pragmatische redenering eh

1584
00:53:40,800 --> 00:53:42,720
we gebruiken het om bepaalde

1585
00:53:42,720 --> 00:53:45,359
communicatieve doelen te bereiken je kunt allerlei soorten

1586
00:53:45,359 --> 00:53:47,160
communicatieve kenmerken vinden

1587
00:53:47,160 --> 00:53:49,559
uh binnen de de  taalsysteem zelf

1588
00:53:49,559 --> 00:53:51,180
en dus misschien zijn sommige van deze andere

1589
00:53:51,180 --> 00:53:53,819
eigenschappen eigenschappen die

1590
00:53:53,819 --> 00:53:55,619
een andere oorsprong hebben

1591
00:53:55,619 --> 00:53:57,059
eh en dat die mening volgens mij

1592
00:53:57,059 --> 00:53:59,579
verkeerd kan zijn, maar het is er een waarvan ik

1593
00:53:59,579 --> 00:54:01,800
denk dat er naar moet worden gekeken om te zien

1594
00:54:01,800 --> 00:54:04,200
of het verkeerd is zoals  Ik denk dat het eh eh een

1595
00:54:04,200 --> 00:54:05,400


1596
00:54:05,400 --> 00:54:10,020
beetje is afgewezen eh door eh grote

1597
00:54:10,020 --> 00:54:12,900
delen van taalkundigen,

1598
00:54:12,900 --> 00:54:14,579
weet je, ik heb mensen dingen horen zeggen als oh

1599
00:54:14,579 --> 00:54:15,720
nou, communicatie verklaart niet echt

1600
00:54:15,720 --> 00:54:17,760
iets over taal,

1601
00:54:17,760 --> 00:54:20,040
en wat ze vaak bedoelen is dat het dat niet doet  Ik

1602
00:54:20,040 --> 00:54:22,200
leg het niet uit zoals de specifieke

1603
00:54:22,200 --> 00:54:23,760
eilandbeperkingen of iets waar ze aan

1604
00:54:23,760 --> 00:54:25,319
werken, maar er

1605
00:54:25,319 --> 00:54:26,700
zijn allerlei andere dingen in de

1606
00:54:26,700 --> 00:54:28,319
taal die communicatieve druk

1607
00:54:28,319 --> 00:54:30,359
waarschijnlijk wel verklaart eh

1608
00:54:30,359 --> 00:54:31,500
dus

1609
00:54:31,500 --> 00:54:33,540
eh ik denk dat mijn pitch altijd

1610
00:54:33,540 --> 00:54:36,359
voor een soort van breedte is  in termen van breedte

1611
00:54:36,359 --> 00:54:39,119
rekening houdend met uh de krachten die

1612
00:54:39,119 --> 00:54:41,460
taal kunnen vormen en niet

1613
00:54:41,460 --> 00:54:43,680
alles in een of andere vorm van aangeboren

1614
00:54:43,680 --> 00:54:45,359
beperkingen of iets dergelijks hoeven te stoppen, nee

1615
00:54:45,359 --> 00:54:46,680
helemaal en ik denk dat veel van

1616
00:54:46,680 --> 00:54:48,420
dat spul ermee compatibel is

1617
00:54:48,420 --> 00:54:49,980
ziekteprogramma

1618
00:54:49,980 --> 00:54:51,960
omdat het midden van dit programma wil dat de

1619
00:54:51,960 --> 00:54:53,339
syntaxis minimaal is, het wil niet dat het

1620
00:54:53,339 --> 00:54:54,660
ingewikkeld wordt, het wil niet dat het

1621
00:54:54,660 --> 00:54:56,220
ingewikkeld wordt, je weet dat het ingewikkelder moet

1622
00:54:56,220 --> 00:54:57,960
zijn, dus er waren er een paar die je noemde

1623
00:54:57,960 --> 00:54:59,460
de Curious-eigenschappen goed  dus

1624
00:54:59,460 --> 00:55:00,480
er zijn enkele van de eigenschappen waarmee

1625
00:55:00,480 --> 00:55:02,579
rekening moet worden gehouden in elk

1626
00:55:02,579 --> 00:55:04,200
taalmodel, ik zal je een

1627
00:55:04,200 --> 00:55:05,400
voorbeeld geven van de instelling van een persoonskenmerken

1628
00:55:05,400 --> 00:55:06,599


1629
00:55:06,599 --> 00:55:08,880
en deze persoonskenmerken vertonen zeer

1630
00:55:08,880 --> 00:55:10,440
niet-triviale verschillende generalisaties

1631
00:55:10,440 --> 00:55:12,480
die niet lijken te zijn  worden verantwoord Via

1632
00:55:12,480 --> 00:55:14,220
domein Algemeen leermechanisme dus ik

1633
00:55:14,220 --> 00:55:16,020
zit hier het werk van Daniel Harper

1634
00:55:16,020 --> 00:55:17,700
bij Queen Mary, dus bijvoorbeeld de

1635
00:55:17,700 --> 00:55:19,740
morfologische samenstelling van de persoon, de

1636
00:55:19,740 --> 00:55:21,720
interactie met getallen, de verbinding

1637
00:55:21,720 --> 00:55:24,059
met de ruimte, uh, eigenschappen van de semantiek

1638
00:55:24,059 --> 00:55:26,040
en de linearisatie, ze lijken allemaal

1639
00:55:26,040 --> 00:55:27,240
sterke kandidaten zijn voor onze

1640
00:55:27,240 --> 00:55:28,619
kennis van taal, precies wat we bedoelen

1641
00:55:28,619 --> 00:55:30,359
met taalkennis, maar aan de

1642
00:55:30,359 --> 00:55:32,280
andere kant hebben we zaken als

1643
00:55:32,280 --> 00:55:34,319
hoofdlettergebruik en hoofdbewegingen en dit

1644
00:55:34,319 --> 00:55:36,420
zijn allemaal structurele verschijnselen, maar

1645
00:55:36,420 --> 00:55:39,319
ze lijken zich te verzetten tegen een puur op

1646
00:55:39,319 --> 00:55:42,420
betekenis gebaseerde verklaring uh in

1647
00:55:42,420 --> 00:55:44,339
theoretische taalkunde klopt, het zou

1648
00:55:44,339 --> 00:55:45,839
geweldig zijn als syntaxis niets anders was dan een

1649
00:55:45,839 --> 00:55:47,520
computationele motor die

1650
00:55:47,520 --> 00:55:49,440
gestructureerde betekenis opbouwt en dat is het

1651
00:55:49,440 --> 00:55:51,240
minimalistische programma het doel, maar dat is

1652
00:55:51,240 --> 00:55:52,800
niet wat we daadwerkelijk vinden dat niet in

1653
00:55:52,800 --> 00:55:54,720
een echt minimalistisch, zoals concreet

1654
00:55:54,720 --> 00:55:57,240
model, een concrete mineralistische theorie is, het

1655
00:55:57,240 --> 00:55:59,040
doel is gewoon  alsof het programma is,

1656
00:55:59,040 --> 00:56:01,260
is taal perfect, oké, dat

1657
00:56:01,260 --> 00:56:03,119
is het programma, wat we vinden, nee,

1658
00:56:03,119 --> 00:56:05,160
duidelijk, niet oké, nee, geen taalkundige

1659
00:56:05,160 --> 00:56:07,680
gelooft dat echt en dus zou het

1660
00:56:07,680 --> 00:56:09,900
geweldig zijn als de syntaxis zo was, maar ik

1661
00:56:09,900 --> 00:56:11,280
denk dat je weet dat het programma is om

1662
00:56:11,280 --> 00:56:13,740
naar perfectie te zoeken, maar  vind het niet altijd,

1663
00:56:13,740 --> 00:56:15,839
dus een overeenkomst en hoofdbewegingen zijn

1664
00:56:15,839 --> 00:56:17,640
morfologisch meer voor fonologisch

1665
00:56:17,640 --> 00:56:19,319
fenomenaal de eigenschappen van de

1666
00:56:19,319 --> 00:56:20,700
uitvoeringssystemen wat

1667
00:56:20,700 --> 00:56:22,440
uitvoeringssystemen worden genoemd en dus

1668
00:56:22,440 --> 00:56:23,760
is het minimalistische programma zelf echt

1669
00:56:23,760 --> 00:56:24,839
compatibel met veel van wat je

1670
00:56:24,839 --> 00:56:26,880
zegt over je kent taal taal

1671
00:56:26,880 --> 00:56:28,500
er zijn aspecten van taal die

1672
00:56:28,500 --> 00:56:31,200
um kunnen worden geperfectioneerd en geoptimaliseerd voor

1673
00:56:31,200 --> 00:56:32,700
communicatieve efficiëntie, daar bestaat

1674
00:56:32,700 --> 00:56:35,520
absoluut geen twijfel over, maar waar is

1675
00:56:35,520 --> 00:56:38,160
die locus van efficiëntie in de

1676
00:56:38,160 --> 00:56:39,960
syntaxis zelf of is het een soort

1677
00:56:39,960 --> 00:56:42,000
extra-linguïstisch systeem is het in

1678
00:56:42,000 --> 00:56:43,680
pragmatiek weet je dat het is  het zit in de sensorische

1679
00:56:43,680 --> 00:56:45,540
motoriek zit het in de spraak

1680
00:56:45,540 --> 00:56:47,640
um waarschijnlijk de spraak en fonologie weet

1681
00:56:47,640 --> 00:56:50,400
je waarschijnlijk ik bedoel wie weet maar ik

1682
00:56:50,400 --> 00:56:52,740
denk dat veel van deze dingen veel

1683
00:56:52,740 --> 00:56:56,059
meer vereisen weet je serieuze overweging in

1684
00:56:56,059 --> 00:56:57,839
ouderwetse begrippen zoals structuur

1685
00:56:57,839 --> 00:56:59,700
afhankelijkheid compositorisch thema wat heb

1686
00:56:59,700 --> 00:57:01,140
je  dat soort dingen die je misschien

1687
00:57:01,140 --> 00:57:03,720
ergens in de literatuur kunt vinden, maar zelfs

1688
00:57:03,720 --> 00:57:06,720
gewoon basisonderwerpen zoals je weet

1689
00:57:06,720 --> 00:57:08,640
um kwantor die uitgebreide projecties opheft

1690
00:57:08,640 --> 00:57:09,900


1691
00:57:09,900 --> 00:57:12,059
um bijwoordelijke zoals bijwoordelijke hiërarchieën

1692
00:57:12,059 --> 00:57:13,920
al deze dingen in het minimalistische

1693
00:57:13,920 --> 00:57:16,619
programma kunnen extra taalkundig zijn, toch

1694
00:57:16,619 --> 00:57:18,180
kunnen ze eigenlijk buiten staan  syntaxis

1695
00:57:18,180 --> 00:57:20,579
en query zeer vreemde eigenschappen van de

1696
00:57:20,579 --> 00:57:23,099
semantische uh conceptuele systemen die

1697
00:57:23,099 --> 00:57:24,559
op zichzelf een soort domein zijn Algemene

1698
00:57:24,559 --> 00:57:27,420
rare overblijfselen van oude primaire

1699
00:57:27,420 --> 00:57:29,220
cognitie, juist de kenmerken van de manier waarop we

1700
00:57:29,220 --> 00:57:31,260
gebeurtenissen doorgeven zoals we doorgeven, weet je,

1701
00:57:31,260 --> 00:57:32,520
agenten en patiënten dat soort dingen dat

1702
00:57:32,520 --> 00:57:33,960
is zeker niet  dat is niet mensspecifiek,

1703
00:57:33,960 --> 00:57:35,520


1704
00:57:35,520 --> 00:57:37,260
maar je kent de manier waarop de syntaxis

1705
00:57:37,260 --> 00:57:39,059
instructies geeft aan deze systemen die

1706
00:57:39,059 --> 00:57:42,480
je goed kent, lijkt te zijn, dus je

1707
00:57:42,480 --> 00:57:43,800
weet dat generatieve taalkundigen

1708
00:57:43,800 --> 00:57:45,839
ook verschillende theorieën hebben over taalproductie.

1709
00:57:45,839 --> 00:57:47,040
Ik zal het alleen hebben over taalproductie

1710
00:57:47,040 --> 00:57:49,380
op basis van of we lemma's opslaan

1711
00:57:49,380 --> 00:57:51,240
of dat we woorden op precies dezelfde

1712
00:57:51,240 --> 00:57:52,619
manier bouwen als we zinnen en zinnen zullen formuleren, dus ik

1713
00:57:52,619 --> 00:57:53,819
weet dat jij het onderscheid maakt

1714
00:57:53,819 --> 00:57:55,440
tussen constructiegrammatica en een soort

1715
00:57:55,440 --> 00:57:57,180
generatieve grammatica en je weet hoeveel

1716
00:57:57,180 --> 00:57:58,680
gewicht ze hechten aan het onthouden van

1717
00:57:58,680 --> 00:58:00,119
vernauwingen, terwijl het gewoon

1718
00:58:00,119 --> 00:58:01,559
dingen bouwen van uit  van onder naar boven vanaf de

1719
00:58:01,559 --> 00:58:04,440
grond naar rechts en dus weet je dat in sommige

1720
00:58:04,440 --> 00:58:06,480
generatief geïnspireerde modellen mechanismen

1721
00:58:06,480 --> 00:58:08,460
die syntactische structuur genereren

1722
00:58:08,460 --> 00:58:10,200
geen onderscheid maken tussen processen die

1723
00:58:10,200 --> 00:58:12,200
boven of onder het woordniveau van toepassing zijn

1724
00:58:12,200 --> 00:58:14,640
er is geen aanwijzing welke betekenis syntaxis

1725
00:58:14,640 --> 00:58:16,500
en vorm allemaal samen worden opgeslagen in een

1726
00:58:16,500 --> 00:58:18,660
enkele atomaire  representaties elke fase

1727
00:58:18,660 --> 00:58:20,099
in lexicale toegang is een overgang

1728
00:58:20,099 --> 00:58:21,960
tussen verschillende soorten

1729
00:58:21,960 --> 00:58:23,760
datastructuren, daar is betekenis, er is

1730
00:58:23,760 --> 00:58:25,740
vorm en er is syntaxis, deze drie

1731
00:58:25,740 --> 00:58:27,480
kenmerken gaan min of meer samen en

1732
00:58:27,480 --> 00:58:28,920
ze overlappen niet altijd verschillende

1733
00:58:28,920 --> 00:58:31,440
talen, realiseer ze op verschillende manieren

1734
00:58:31,440 --> 00:58:34,800
en dus weet je  een raar de

1735
00:58:34,800 --> 00:58:36,839
basisdefinitie van een woord is gewoon deze rare

1736
00:58:36,839 --> 00:58:39,720
multi-systeemdefinitie en waar

1737
00:58:39,720 --> 00:58:41,040
veel dingen, veel verschillende cognitieve

1738
00:58:41,040 --> 00:58:42,900
systemen de basis van elk

1739
00:58:42,900 --> 00:58:44,940
elektrisch item verrijken.

1740
00:58:44,940 --> 00:58:46,680


1741
00:58:46,680 --> 00:58:48,299


1742
00:58:48,299 --> 00:58:50,099
linguïstische theorie

1743
00:58:50,099 --> 00:58:52,200
goed of in ieder geval in wat llms doen,

1744
00:58:52,200 --> 00:58:53,220


1745
00:58:53,220 --> 00:58:55,859
dus ik denk wat ik denk dat ik je zal vragen

1746
00:58:55,859 --> 00:58:58,799
wat jouw definitie van een woord is,

1747
00:58:58,799 --> 00:59:01,559
goed en wat kan llms echt

1748
00:59:01,559 --> 00:59:03,780
inzicht geven in woord Hood, goed, want als

1749
00:59:03,780 --> 00:59:04,680
je een beetje als je  heb geen

1750
00:59:04,680 --> 00:59:06,720
bestemming van wat een woord is, dan

1751
00:59:06,720 --> 00:59:07,920
zit je echt in de problemen, alsof we op zijn

1752
00:59:07,920 --> 00:59:10,319
minst LMS of kunstmatige

1753
00:59:10,319 --> 00:59:12,780
systemen moeten gebruiken om te informeren wat we bedoelen met een woord,

1754
00:59:12,780 --> 00:59:14,400
of misschien hebben we dat niet meer nodig.

1755
00:59:14,400 --> 00:59:16,859
niet zeker wat denk je ik ik ben niet ik weet

1756
00:59:16,859 --> 00:59:18,599
niet zeker wat je bedoelt ik bedoel

1757
00:59:18,599 --> 00:59:20,339
um

1758
00:59:20,339 --> 00:59:24,059
um ik heb geen wat is een woord waarom maakt

1759
00:59:24,059 --> 00:59:25,740
dat uit ik bedoel dat dat

1760
00:59:25,740 --> 00:59:27,359
gewoon een conventie is over hoe we de gebruiken

1761
00:59:27,359 --> 00:59:29,819
term woord goed hoe

1762
00:59:29,819 --> 00:59:31,920
bedoel ik dat je zou kunnen gebruiken weet je lemma's of

1763
00:59:31,920 --> 00:59:34,440
woordfirma's of wat dan ook dat

1764
00:59:34,440 --> 00:59:35,819
voelt gewoon als een conventionele keuze

1765
00:59:35,819 --> 00:59:38,040


1766
00:59:38,040 --> 00:59:39,180


1767
00:59:39,180 --> 00:59:41,880
zeg dat ik het ermee

1768
00:59:41,880 --> 00:59:43,920
eens ben dat woord een conventionering is,

1769
00:59:43,920 --> 00:59:45,599
weet je, iconen zijn geen intuïtief concept van

1770
00:59:45,599 --> 00:59:47,819
waar het vaak vertekend is door spelling,

1771
00:59:47,819 --> 00:59:50,760
de manier waarop we ruimten dingen rechtzetten, dus ik ben het

1772
00:59:50,760 --> 00:59:52,559
eens met die kritiek,

1773
00:59:52,559 --> 00:59:54,240
weet je, woord in intuïtieve zin is niet

1774
00:59:54,240 --> 00:59:56,339
echt wetenschappelijk  Hoe

1775
00:59:56,339 --> 00:59:58,260


1776
00:59:58,260 --> 01:00:00,359
zou je, weet je, het

1777
01:00:00,359 --> 01:00:02,160
intuïtieve concept van woord ontbinden in iets

1778
01:00:02,160 --> 01:00:03,660
dat vriendelijker is, weet je,

1779
01:00:03,660 --> 01:00:04,740
wetenschappelijk vatbaar of

1780
01:00:04,740 --> 01:00:06,540
psychologisch plausibel, en dat is

1781
01:00:06,540 --> 01:00:08,280
precies wat geometrische primaire probeert te

1782
01:00:08,280 --> 01:00:10,559
doen door woorden te ontbinden in weet je

1783
01:00:10,559 --> 01:00:12,599
onderscheidende kenmerken uh morfologische

1784
01:00:12,599 --> 01:00:15,180
categorieën conceptueel Wortels worden samengevoegd

1785
01:00:15,180 --> 01:00:17,579
met categorische kenmerken je weet dat je

1786
01:00:17,579 --> 01:00:20,700
een concept krijgt dat je kent en je hebt

1787
01:00:20,700 --> 01:00:22,440
het gemaakt met een zelfstandig naamwoord of een categorie om een ​​zelfstandig

1788
01:00:22,440 --> 01:00:24,119
naamwoord of Affaire te krijgen deze verschillende modellen

1789
01:00:24,119 --> 01:00:26,220
maken verschillende voorspellingen, ja, ik

1790
01:00:26,220 --> 01:00:28,859
bedoel, ik denk  dat algemene idee is

1791
01:00:28,859 --> 01:00:30,839
waarschijnlijk juist voor grote

1792
01:00:30,839 --> 01:00:32,880
taalmodellen, zoals ik denk dat ze

1793
01:00:32,880 --> 01:00:34,380
dingen moeten hebben die een beetje op

1794
01:00:34,380 --> 01:00:36,660
spraakcategorieën lijken, bijvoorbeeld

1795
01:00:36,660 --> 01:00:38,819
eh en ik denk dat ze in

1796
01:00:38,819 --> 01:00:42,900
staat moeten zijn om die hun

1797
01:00:42,900 --> 01:00:45,240
categorieën bij te werken  gebaseerd op de taal die

1798
01:00:45,240 --> 01:00:48,119
ze tot nu toe hebben gezien, dus zoals

1799
01:00:48,119 --> 01:00:50,520
je weet zet GPT zelfstandige naamwoorden en werkwoorden op de

1800
01:00:50,520 --> 01:00:53,400
juiste plaatsen en om dat te doen heb je

1801
01:00:53,400 --> 01:00:55,319
een soort weergave nodig van de zelfstandige naamwoorden

1802
01:00:55,319 --> 01:00:57,000
versus de werkwoorden en je hebt enige

1803
01:00:57,000 --> 01:01:00,839
vaardigheid nodig om uh  eh zoek jezelf op in een

1804
01:01:00,839 --> 01:01:02,579
reeks andere woorden en zoek uit of er

1805
01:01:02,579 --> 01:01:04,680
waarschijnlijk een zelfstandig naamwoord of een werkwoord

1806
01:01:04,680 --> 01:01:05,400
naast

1807
01:01:05,400 --> 01:01:07,559
eh staat, dus ik denk dat op dat

1808
01:01:07,559 --> 01:01:09,319
niveau dat soort eigenschappen van woorden

1809
01:01:09,319 --> 01:01:12,480
eh zeer waarschijnlijk goed zullen zijn en

1810
01:01:12,480 --> 01:01:15,780
daar  er zijn ook dingen die uh

1811
01:01:15,780 --> 01:01:17,640
um zeer waarschijnlijk zullen vinden in

1812
01:01:17,640 --> 01:01:19,619
de interne representaties van deze

1813
01:01:19,619 --> 01:01:21,420
modellen.

1814
01:01:21,420 --> 01:01:24,180


1815
01:01:24,180 --> 01:01:26,339


1816
01:01:26,339 --> 01:01:29,579
niet waar de uh uh dat is niet

1817
01:01:29,579 --> 01:01:31,500
waar de belangrijkste debatten of

1818
01:01:31,500 --> 01:01:35,220
meningsverschillen zijn, denk ik, zoals

1819
01:01:35,220 --> 01:01:38,280
eh uh ja, ik denk dat alle

1820
01:01:38,280 --> 01:01:40,079
taaltheorieën moeten zeggen dat er

1821
01:01:40,079 --> 01:01:41,400
verschillende soorten woorden zijn die

1822
01:01:41,400 --> 01:01:43,140
op verschillende plaatsen kunnen verschijnen of

1823
01:01:43,140 --> 01:01:44,339
zoiets

1824
01:01:44,339 --> 01:01:45,240
eh ja

1825
01:01:45,240 --> 01:01:47,160
oké, dus hoe zit het met het probleem, je weet dat je

1826
01:01:47,160 --> 01:01:49,020
communicatie goed noemde,

1827
01:01:49,020 --> 01:01:51,299
dus je weet het en je hebt helemaal gelijk

1828
01:01:51,299 --> 01:01:53,099
als Trump dingen zegt als taal

1829
01:01:53,099 --> 01:01:54,960
een denksysteem is of je weet dat taal

1830
01:01:54,960 --> 01:01:57,780
niet is geëvolueerd, hij is een

1831
01:01:57,780 --> 01:01:58,799
beetje een beetje  brutaal, hij bedoelt niet echt

1832
01:01:58,799 --> 01:02:00,180
dat hij in een heel specifieke

1833
01:02:00,180 --> 01:02:01,619
zin bedoelt,

1834
01:02:01,619 --> 01:02:03,540
toch, maar weet je, als we zeggen dat taal

1835
01:02:03,540 --> 01:02:05,640
een denksysteem is, bedoelen we

1836
01:02:05,640 --> 01:02:06,780
eh, we proberen er een

1837
01:02:06,780 --> 01:02:08,339
architectonische claim van te maken, dus als je kijkt naar

1838
01:02:08,339 --> 01:02:09,599
de architectuur van het minimalistische

1839
01:02:09,599 --> 01:02:10,380
programma

1840
01:02:10,380 --> 01:02:12,240
de syntactische afleiding en de

1841
01:02:12,240 --> 01:02:13,740
conceptuele systemen zijn letterlijk

1842
01:02:13,740 --> 01:02:15,960
verschillende systemen, juist de conceptuele

1843
01:02:15,960 --> 01:02:18,180
systemen halen dingen uit de syntaxis en vervolgens

1844
01:02:18,180 --> 01:02:19,799
naar hun eigen zaken ermee en de CI-

1845
01:02:19,799 --> 01:02:21,900
systemen hebben hun eigen bijzondere regels

1846
01:02:21,900 --> 01:02:23,700
en principes, daarom

1847
01:02:23,700 --> 01:02:25,319
dacht ik  in taal zijn beide vergelijkbare

1848
01:02:25,319 --> 01:02:27,000
symbolische compositorische systemen, maar op

1849
01:02:27,000 --> 01:02:29,160
verschillende manieren wordt slechts een deel van het denken

1850
01:02:29,160 --> 01:02:32,339
terecht het CI-interfacesysteem genoemd,

1851
01:02:32,339 --> 01:02:34,619
aangezien de CI-systemen per

1852
01:02:34,619 --> 01:02:36,480
definitie je weet welke conceptuele

1853
01:02:36,480 --> 01:02:38,819
systemen de mens heeft die geen toegang heeft tot

1854
01:02:38,819 --> 01:02:40,859
instructies uit de syntaxis en deze kan uitlezen  en we

1855
01:02:40,859 --> 01:02:42,540
weten niet wat ze volledig zijn ze lijken

1856
01:02:42,540 --> 01:02:43,920
iets te maken te hebben met gebeurtenissen in

1857
01:02:43,920 --> 01:02:45,540
grammaticale referentie en bepaaldheid

1858
01:02:45,540 --> 01:02:47,099
ze lijken de belangrijkste categorieën te zijn waar de

1859
01:02:47,099 --> 01:02:48,359
taal die je kent

1860
01:02:48,359 --> 01:02:49,440
conceptueel om geeft,

1861
01:02:49,440 --> 01:02:50,940
maar we weten niet echt dat dat een beetje

1862
01:02:50,940 --> 01:02:52,859
een  hypothese klopt

1863
01:02:52,859 --> 01:02:53,940
eh maar wat we wel weten is dat ze

1864
01:02:53,940 --> 01:02:56,220
niet zoveel gebruik lijken te maken van kleur

1865
01:02:56,220 --> 01:02:58,440
of eh dus geen taal

1866
01:02:58,440 --> 01:03:00,240
morfologisch markeert je kent kleurschakeringen

1867
01:03:00,240 --> 01:03:01,140


1868
01:03:01,140 --> 01:03:03,900
um of andere conceptuele kenmerken zoals um um

1869
01:03:03,900 --> 01:03:06,359
zorgen of bezorgdheid zoals geen taal

1870
01:03:06,359 --> 01:03:07,980
morfologisch  markeert een zekere mate van bezorgdheid

1871
01:03:07,980 --> 01:03:10,079
of bezorgdheid over een kwestie, maar we maken wel

1872
01:03:10,079 --> 01:03:11,960
gebruik van soortgelijke

1873
01:03:11,960 --> 01:03:13,500
epistemologische begrippen zoals

1874
01:03:13,500 --> 01:03:15,599
bewijskracht en dat soort dingen, dus

1875
01:03:15,599 --> 01:03:17,640
je kent er een goed. Ik denk dat wat ik

1876
01:03:17,640 --> 01:03:19,500
bedoel is dat het minimalistische programma

1877
01:03:19,500 --> 01:03:21,720
goed probeert te achterhalen  uit te zoeken met welke

1878
01:03:21,720 --> 01:03:23,400
aspecten van gedachtetaal

1879
01:03:23,400 --> 01:03:25,619
nauw verbonden is en met welke aspecten van

1880
01:03:25,619 --> 01:03:27,900
denken het niet is verbonden, dus het Midwest-

1881
01:03:27,900 --> 01:03:29,280
programma stelt ons in staat om dat

1882
01:03:29,280 --> 01:03:31,680
heel netjes op te splitsen en dit is een veel

1883
01:03:31,680 --> 01:03:33,180
genuanceerder raamwerk dan je weet

1884
01:03:33,180 --> 01:03:34,559
wanneer Chomski zegt talen denken

1885
01:03:34,559 --> 01:03:36,960
nogmaals, hij meent het misschien

1886
01:03:36,960 --> 01:03:38,339
niet, maar dat is niet wat de

1887
01:03:38,339 --> 01:03:40,319
feitelijke architectuur van zijn theorie zegt,

1888
01:03:40,319 --> 01:03:42,480
het is een retorisch apparaat dat heel

1889
01:03:42,480 --> 01:03:43,920
nuttig en interessant is om een

1890
01:03:43,920 --> 01:03:46,559
niet-gegradueerd publiek aan te trekken, ongeacht

1891
01:03:46,559 --> 01:03:48,839
of je kijkt naar echte theorieën die dat wel zijn  als je

1892
01:03:48,839 --> 01:03:50,700
uit het Mentalist-programma komt,

1893
01:03:50,700 --> 01:03:52,380
gelooft niemand echt dat taal gelijk is aan goed

1894
01:03:52,380 --> 01:03:53,760
denken. Het taalsysteem lijkt

1895
01:03:53,760 --> 01:03:55,920
zijn best te doen om toegang te krijgen tot

1896
01:03:55,920 --> 01:03:57,599
verschillende conceptuele systemen, deze opnieuw te formatteren en te manipuleren,

1897
01:03:57,599 --> 01:03:59,220
maar het heeft zijn grenzen.

1898
01:03:59,220 --> 01:04:01,440


1899
01:04:01,440 --> 01:04:02,819


1900
01:04:02,819 --> 01:04:05,160
met betrekking tot de syntaxis-engine en

1901
01:04:05,160 --> 01:04:07,260
welke niet

1902
01:04:07,260 --> 01:04:09,480
um, dus je weet dat dit soort teruggaat naar

1903
01:04:09,480 --> 01:04:11,579
het idee dat lexisering van een concept

1904
01:04:11,579 --> 01:04:14,099
het op de een of andere manier lijkt te veranderen, het

1905
01:04:14,099 --> 01:04:16,200
doordrenkt het met elementen die er

1906
01:04:16,200 --> 01:04:17,819
niet zijn in de  concept zelf, dus als

1907
01:04:17,819 --> 01:04:19,319
je lezing een concept is,

1908
01:04:19,319 --> 01:04:21,240
transformeer je het plotseling een beetje, je geeft het een

1909
01:04:21,240 --> 01:04:22,680
beetje extra, je strooit er iets anders

1910
01:04:22,680 --> 01:04:24,299
bovenop en dat lijkt te variëren

1911
01:04:24,299 --> 01:04:26,700
tussen verschillende nanotypes, maar dit

1912
01:04:26,700 --> 01:04:29,160
zijn allemaal heel duidelijke architecturale

1913
01:04:29,160 --> 01:04:31,619
claims binnen geometrische  grammatica die

1914
01:04:31,619 --> 01:04:34,980
heel duidelijke empirische voorspellingen doet, dus

1915
01:04:34,980 --> 01:04:36,420
met andere woorden, ik denk dat wat ik bedoel

1916
01:04:36,420 --> 01:04:37,140


1917
01:04:37,140 --> 01:04:38,940
al deze neuropsychologische onderzoeken zijn die worden

1918
01:04:38,940 --> 01:04:42,119
aangewakkerd, weet je, er is veel werk aan

1919
01:04:42,119 --> 01:04:43,980
eh in deze geest, wat laat het echt zien,

1920
01:04:43,980 --> 01:04:45,420
ik denk dat het laat zien dat je weet  wanneer

1921
01:04:45,420 --> 01:04:47,819
taal beschadigd is in de hersenen,

1922
01:04:47,819 --> 01:04:50,040
verliest het deze specifieke invloed of manier om

1923
01:04:50,040 --> 01:04:51,960
die systemen te beïnvloeden, maar er is geen

1924
01:04:51,960 --> 01:04:54,299
echte voorspelling vanuit de Gen gram

1925
01:04:54,299 --> 01:04:56,040
Enterprise, maar die niet-linguïstische

1926
01:04:56,040 --> 01:04:57,540
systemen zouden aangetast moeten zijn of zou

1927
01:04:57,540 --> 01:04:59,579
plotseling moeten stoppen als het kerntaalsysteem

1928
01:04:59,579 --> 01:05:01,200


1929
01:05:01,200 --> 01:05:02,760
eh is in feite gecompromitteerd, als er

1930
01:05:02,760 --> 01:05:05,099
iets is dat alleen maar de

1931
01:05:05,099 --> 01:05:07,859
belangrijkste scheiding tussen het syntactische

1932
01:05:07,859 --> 01:05:09,900
systeem en niet-linguïstische systemen benadrukt, klopt,

1933
01:05:09,900 --> 01:05:11,880
dus ik denk dat de vele voorspellingen hier

1934
01:05:11,880 --> 01:05:14,339
van de taal en communicatie uh

1935
01:05:14,339 --> 01:05:16,020
weet je, literatuur mist

1936
01:05:16,020 --> 01:05:19,380
het punt van de  architecturale claims

1937
01:05:19,380 --> 01:05:21,540
eh ik kan gewoon geven of Daniel

1938
01:05:21,540 --> 01:05:24,180
wil je gaan uh geef een beetje

1939
01:05:24,180 --> 01:05:25,799
achtergrond daar dus dus er zijn deze

1940
01:05:25,799 --> 01:05:26,760
papieren

1941
01:05:26,760 --> 01:05:30,119
um uh van uh uh EV federenko en en

1942
01:05:30,119 --> 01:05:32,880
rozemarijn Varley die dat zijn

1943
01:05:32,880 --> 01:05:34,500
um onderzoekt

1944
01:05:34,500 --> 01:05:37,799
um uh uh in een deel van  ze afasiepatiënt

1945
01:05:37,799 --> 01:05:40,200
dus dus mensen met een verminderde uh

1946
01:05:40,200 --> 01:05:41,880
taalvaardigheid

1947
01:05:41,880 --> 01:05:43,140
um

1948
01:05:43,140 --> 01:05:45,000
um laten in feite zien dat je met een verminderde

1949
01:05:45,000 --> 01:05:46,980
taalvaardigheid je

1950
01:05:46,980 --> 01:05:49,020
um Can nog steeds eh

1951
01:05:49,020 --> 01:05:50,579
behouden soort van

1952
01:05:50,579 --> 01:05:52,859
redeneervermogen kunt hebben, dus mensen zoals Chess Masters

1953
01:05:52,859 --> 01:05:55,020
chess Grand Masters bijvoorbeeld die

1954
01:05:55,020 --> 01:05:58,140
duidelijk erg goed zijn  bij redeneren

1955
01:05:58,140 --> 01:06:00,780
um uh um heeft misschien geen uh soort van

1956
01:06:00,780 --> 01:06:02,640
intacte taalkundige vaardigheden

1957
01:06:02,640 --> 01:06:04,079
eh en als aanvulling op dat soort

1958
01:06:04,079 --> 01:06:06,119
geduldig werk is er ook uh werk

1959
01:06:06,119 --> 01:06:08,520
van eb's Lab dat laat zien dat

1960
01:06:08,520 --> 01:06:11,220
eh uh de uh delen van de hersenen

1961
01:06:11,220 --> 01:06:13,859
die om uh taal geven

1962
01:06:13,859 --> 01:06:14,640
um

1963
01:06:14,640 --> 01:06:16,319
um  zijn te scheiden van de delen van de

1964
01:06:16,319 --> 01:06:17,700
hersenen die om andere andere

1965
01:06:17,700 --> 01:06:19,619
domeinen geven, zelfs die met dezelfde soort

1966
01:06:19,619 --> 01:06:21,599
taal, dus dingen als muziek

1967
01:06:21,599 --> 01:06:23,819
en wiskunde

1968
01:06:23,819 --> 01:06:25,859
eh eh komen niet voor in de

1969
01:06:25,859 --> 01:06:27,240
taalgebieden

1970
01:06:27,240 --> 01:06:29,400
eh dus EV en anderen hebben dat wel  betoogde

1971
01:06:29,400 --> 01:06:30,539
dat

1972
01:06:30,539 --> 01:06:33,539
eh, dit is eigenlijk bewijs tegen

1973
01:06:33,539 --> 01:06:36,660
de Chomsky en beweert dat eh,

1974
01:06:36,660 --> 01:06:38,579
taal het medium is om

1975
01:06:38,579 --> 01:06:40,440
goed te denken, omdat er denken kan

1976
01:06:40,440 --> 01:06:42,420
gebeuren in de afwezigheid van taal en

1977
01:06:42,420 --> 01:06:44,160
hersengebieden die om taal geven,

1978
01:06:44,160 --> 01:06:46,020
lijken niet de hersengebieden te zijn die ze  geef om

1979
01:06:46,020 --> 01:06:48,359
zorg om denken

1980
01:06:48,359 --> 01:06:50,099
eh ik denk dat Elliot je zegt

1981
01:06:50,099 --> 01:06:51,780
dat mensen niet echt geloven

1982
01:06:51,780 --> 01:06:53,039
dat eh

1983
01:06:53,039 --> 01:06:56,220
uh ze geloven niet dat dat

1984
01:06:56,220 --> 01:06:57,720
onderscheid ik bedoel eh

1985
01:06:57,720 --> 01:06:59,880
dat uh

1986
01:06:59,880 --> 01:07:01,740
nee het is het en er is ook veel

1987
01:07:01,740 --> 01:07:03,480
zelfs binnen

1988
01:07:03,480 --> 01:07:04,799
deze argumenten klopt zo dus in je

1989
01:07:04,799 --> 01:07:06,839
paper zeg je soms dat Chomsky

1990
01:07:06,839 --> 01:07:08,220
denkt dat taal een denksysteem is,

1991
01:07:08,220 --> 01:07:10,319
maar dan een paar pagina's later zul je zeggen dat

1992
01:07:10,319 --> 01:07:12,480
Chomsky ook gelooft dat syntaxis

1993
01:07:12,480 --> 01:07:13,799
een totaal ander systeem is dan

1994
01:07:13,799 --> 01:07:15,480
al het andere  juist, autonomie van

1995
01:07:15,480 --> 01:07:18,900
syntaxis enz. dus wat Chomsky is, dat is

1996
01:07:18,900 --> 01:07:20,460
niet mijn tegenspraak.

1997
01:07:20,460 --> 01:07:22,140


1998
01:07:22,140 --> 01:07:24,780


1999
01:07:24,780 --> 01:07:26,280


2000
01:07:26,280 --> 01:07:28,319


2001
01:07:28,319 --> 01:07:30,420
van de

2002
01:07:30,420 --> 01:07:32,520
architectuur goed, dus gewoon zeggen, gewoon

2003
01:07:32,520 --> 01:07:34,619
zeggen dat taal een denksysteem is, wat

2004
01:07:34,619 --> 01:07:35,579
betekent dat, dat betekent niets, het is

2005
01:07:35,579 --> 01:07:36,780
maar een heel vage

2006
01:07:36,780 --> 01:07:38,640
bewering, de vraag is hoe

2007
01:07:38,640 --> 01:07:41,280
taal precies bijdraagt ​​​​aan Thor en hoe

2008
01:07:41,280 --> 01:07:43,500
draagt ​​​​het niet bij

2009
01:07:43,500 --> 01:07:45,839
uh ja, ik bedoel  Ik denk dat zijn bewering

2010
01:07:45,839 --> 01:07:48,420
voornamelijk evolutionair is of zoiets klopt

2011
01:07:48,420 --> 01:07:51,180
dat uh dit is de oorsprong van

2012
01:07:51,180 --> 01:07:52,859
het systeem waarvan ik denk dat het min of

2013
01:07:52,859 --> 01:07:55,559
meer moeilijk te rijmen is met eh

2014
01:07:55,559 --> 01:07:57,599
uh het soort patiënt en en

2015
01:07:57,599 --> 01:07:59,520
neuroimaging-gegevens

2016
01:07:59,520 --> 01:08:00,660
um

2017
01:08:00,660 --> 01:08:04,079
um maar weet je of  als hij dat niet denkt,

2018
01:08:04,079 --> 01:08:07,079
moet hij het niet zeggen, anders reageren mensen op wat hij zei.

2019
01:08:07,079 --> 01:08:08,579


2020
01:08:08,579 --> 01:08:11,280


2021
01:08:11,280 --> 01:08:13,619


2022
01:08:13,619 --> 01:08:15,420


2023
01:08:15,420 --> 01:08:16,920
sommige aspecten van het denken

2024
01:08:16,920 --> 01:08:19,020
die duidelijk uniek zijn voor mensen, maar er

2025
01:08:19,020 --> 01:08:21,000
niet intrinsiek of causaal

2026
01:08:21,000 --> 01:08:22,920
mee verbonden zijn. De architectuur van het

2027
01:08:22,920 --> 01:08:24,600
systeem is heel anders dan het

2028
01:08:24,600 --> 01:08:26,399
soort generalisaties.

2029
01:08:26,399 --> 01:08:28,979


2030
01:08:28,979 --> 01:08:30,960


2031
01:08:30,960 --> 01:08:32,698
patiënten die geen tekortkomingen vertonen in complexe

2032
01:08:32,698 --> 01:08:33,960
redeneringen, zoals u zojuist noemde

2033
01:08:33,960 --> 01:08:35,759
schaken enzovoort, we zouden dit eigenlijk verwachten

2034
01:08:35,759 --> 01:08:37,380
onder een soort van u kent het

2035
01:08:37,380 --> 01:08:39,179
niet-lexicalistische raamwerk van

2036
01:08:39,179 --> 01:08:41,160
geometriesyntaxis waar betekenis, zoals ik zei, betekenis

2037
01:08:41,160 --> 01:08:43,799
syntaxis en vorm betekent gewoon

2038
01:08:43,799 --> 01:08:45,600
alles wat u kunt

2039
01:08:45,600 --> 01:08:46,979
taal veruitwendigen en al deze dingen zijn

2040
01:08:46,979 --> 01:08:48,540
afzonderlijke kenmerken en afzonderlijke systemen.

2041
01:08:48,540 --> 01:08:51,000
juist de autonomie van syntaxis betekent niet dat

2042
01:08:51,000 --> 01:08:52,020


2043
01:08:52,020 --> 01:08:53,819
je weet wat veel mensen denken,

2044
01:08:53,819 --> 01:08:54,719
het betekent gewoon ofwel een

2045
01:08:54,719 --> 01:08:56,160
bepaalde syntactische bewerkingen

2046
01:08:56,160 --> 01:08:58,439
die niet semantisch zijn er zijn bepaalde

2047
01:08:58,439 --> 01:09:00,299
dingen  je kunt doen met syntaxis dat je

2048
01:09:00,299 --> 01:09:01,920
alleen syntaxis kunt doen en je kunt geen

2049
01:09:01,920 --> 01:09:03,420
semantiek doen, dus dit komt terug op het

2050
01:09:03,420 --> 01:09:05,279
verschil tussen je weet uh

2051
01:09:05,279 --> 01:09:07,259
petrovsky's theorie dat semantiek

2052
01:09:07,259 --> 01:09:10,319
juist en juist is versus de uh staat de

2053
01:09:10,319 --> 01:09:11,698
overtuiging van de syntheticus toe dat er

2054
01:09:11,698 --> 01:09:13,080
bepaalde eigenaardige rare dingen zijn  je kunt doen

2055
01:09:13,080 --> 01:09:15,719
met syntaxis die gewoon syntactisch is, dus

2056
01:09:15,719 --> 01:09:17,759
er is een scheiding, zelfs binnen het

2057
01:09:17,759 --> 01:09:20,100
soort architectonisch kader en het is dus

2058
01:09:20,100 --> 01:09:22,439
niet zo verwonderlijk dat je

2059
01:09:22,439 --> 01:09:24,359
die scheiding ook op neuropsychologisch

2060
01:09:24,359 --> 01:09:25,979
niveau vindt.

2061
01:09:25,979 --> 01:09:28,319


2062
01:09:28,319 --> 01:09:30,960
voorspelling van de taal wordt

2063
01:09:30,960 --> 01:09:34,140
gedacht evolutionair idee dan goed dus

2064
01:09:34,140 --> 01:09:36,960
als dat niet zo is als je zegt

2065
01:09:36,960 --> 01:09:39,060
dat dat niet voorspelt dat het denken

2066
01:09:39,060 --> 01:09:41,160
afhankelijk is van taal

2067
01:09:41,160 --> 01:09:44,399
eh dan denk ik dat wie van

2068
01:09:44,399 --> 01:09:45,899
die theorie houdt, met wat voorspellingen moet komen

2069
01:09:45,899 --> 01:09:47,520


2070
01:09:47,520 --> 01:09:49,979
eh ongeveer uh je weet wat dat wat die

2071
01:09:49,979 --> 01:09:51,660
theorie eigenlijk betekent Ik bedoel, ik heb het gevoel dat

2072
01:09:51,660 --> 01:09:53,819
dat soort voorspellingen vaak

2073
01:09:53,819 --> 01:09:55,440
echt nodig zijn om

2074
01:09:55,440 --> 01:09:57,360
de inhoud van een voorspelling te begrijpen

2075
01:09:57,360 --> 01:09:58,320
um

2076
01:09:58,320 --> 01:10:00,540
um sorry Daniel je bent al

2077
01:10:00,540 --> 01:10:03,660
een tijdje omhoog uh nee  het is allemaal goed, ik

2078
01:10:03,660 --> 01:10:05,880
wilde gewoon

2079
01:10:05,880 --> 01:10:08,040
een

2080
01:10:08,040 --> 01:10:12,660
beetje ademen en een kans voor

2081
01:10:12,660 --> 01:10:15,480
iedereen om andere

2082
01:10:15,480 --> 01:10:17,520
vragen te stellen, maar wauw,

2083
01:10:17,520 --> 01:10:20,460
bedankt allebei voor de vele onderwerpen die we hebben

2084
01:10:20,460 --> 01:10:21,780
behandeld,

2085
01:10:21,780 --> 01:10:22,500


2086
01:10:22,500 --> 01:10:25,199
we zullen de laatste minuten een beetje hebben

2087
01:10:25,199 --> 01:10:27,120
conclusie en volgende stappen maar Dave

2088
01:10:27,120 --> 01:10:29,820
zou je een vraag willen stellen of gewoon

2089
01:10:29,820 --> 01:10:32,840
een korte reflectie willen geven

2090
01:10:36,900 --> 01:10:38,880
oké nee

2091
01:10:38,880 --> 01:10:39,960
um

2092
01:10:39,960 --> 01:10:42,239
er zijn veel reacties in de chat dus ik

2093
01:10:42,239 --> 01:10:45,120
hoop dat jullie ze allebei in

2094
01:10:45,120 --> 01:10:47,460
je eigen tijd kunnen lezen om te zien wat iedereen

2095
01:10:47,460 --> 01:10:49,440
heeft toegevoegd

2096
01:10:49,440 --> 01:10:52,860
waar  gaan we vanaf hier terwijl we

2097
01:10:52,860 --> 01:10:57,179
brullen naar mei 2023 en daarna

2098
01:10:57,179 --> 01:11:00,960
wat kunnen taalkundigen grote

2099
01:11:00,960 --> 01:11:02,940
taalmodelontwikkelaars en gebruikers cognitieve

2100
01:11:02,940 --> 01:11:05,159
wetenschappers wat zijn volgens jullie allemaal

2101
01:11:05,159 --> 01:11:06,960
enkele van de meest vruchtbare wegen

2102
01:11:06,960 --> 01:11:08,159
voorwaarts

2103
01:11:08,159 --> 01:11:10,080
oke

2104
01:11:10,080 --> 01:11:13,260
ik zou zeggen eh, je weet wel, de meest

2105
01:11:13,260 --> 01:11:14,699
vruchtbare weg voorwaarts  is om

2106
01:11:14,699 --> 01:11:15,600


2107
01:11:15,600 --> 01:11:17,460
eh echt als cognitieve psychologie te nemen, nee

2108
01:11:17,460 --> 01:11:18,659
serieus, er is de laatste tijd veel leuk werk

2109
01:11:18,659 --> 01:11:21,000
gedaan om dingen op één lijn te brengen, zoals

2110
01:11:21,000 --> 01:11:24,060
ja, je weet wel, Church EBT wolf van alfa-

2111
01:11:24,060 --> 01:11:25,739
plug-ins, de manier waarop chat gbt kan

2112
01:11:25,739 --> 01:11:28,080
communiceren met verschillende soorten modules,

2113
01:11:28,080 --> 01:11:30,179
eh, de manier om een ​​legitieme  soort

2114
01:11:30,179 --> 01:11:32,580
AGI-systeem, het hoeft niet noodzakelijkerwijs

2115
01:11:32,580 --> 01:11:34,380
psychologisch

2116
01:11:34,380 --> 01:11:35,880
afhankelijk te zijn van het soort modules dat

2117
01:11:35,880 --> 01:11:37,500
mensen hebben, maar ik denk dat het er

2118
01:11:37,500 --> 01:11:38,820
baat bij zal hebben, dus er zijn

2119
01:11:38,820 --> 01:11:41,040
enkele beweringen dat grote taalmodellen

2120
01:11:41,040 --> 01:11:42,540
misschien alles kunnen weten  allerlei

2121
01:11:42,540 --> 01:11:43,860
dingen goed alles alles wat je

2122
01:11:43,860 --> 01:11:44,760
leuk vindt

2123
01:11:44,760 --> 01:11:46,620
eh maar ik denk dat het op de lange termijn zeer

2124
01:11:46,620 --> 01:11:47,880
waarschijnlijk zal zijn dat llms

2125
01:11:47,880 --> 01:11:49,380
iets heel belangrijks en heel interessants kan doen,

2126
01:11:49,380 --> 01:11:51,000
maar het zal maar

2127
01:11:51,000 --> 01:11:53,100
een stukje van de puzzel zijn, dus in feite zelfs

2128
01:11:53,100 --> 01:11:55,440
open  AI-CEO Sam Altman zei vorige week

2129
01:11:55,440 --> 01:11:56,400
dat

2130
01:11:56,400 --> 01:11:58,440
eh, weet je wat we met llms kunnen doen,

2131
01:11:58,440 --> 01:12:00,120
echt een beetje uitgeput is, we hebben

2132
01:12:00,120 --> 01:12:02,520
nieuwe richtingen nodig, nieuwe nieuwe nieuwe wegen,

2133
01:12:02,520 --> 01:12:04,860


2134
01:12:04,860 --> 01:12:07,440
enzovoort.

2135
01:12:07,440 --> 01:12:09,060
studenten hier, maar ik

2136
01:12:09,060 --> 01:12:11,159
denk dat hij ook gelijk heeft, weet je, llms kunnen

2137
01:12:11,159 --> 01:12:12,300
iets spectaculairs doen, maar ze zullen

2138
01:12:12,300 --> 01:12:14,760
waarschijnlijk een klein onderdeel vormen van

2139
01:12:14,760 --> 01:12:17,760
de algemene AGI-architectuur.

2140
01:12:17,760 --> 01:12:19,679


2141
01:12:19,679 --> 01:12:21,900


2142
01:12:21,900 --> 01:12:25,679
Ik denk veel van de dus

2143
01:12:25,679 --> 01:12:28,020
laat me me hier nog een voorbeeld geven, dus eh

2144
01:12:28,020 --> 01:12:29,820
Anna zelfs over

2145
01:12:29,820 --> 01:12:31,920
eh die een zeer goede productieve

2146
01:12:31,920 --> 01:12:34,140
wetenschapper is, ze heeft onlangs een paper waarin ze

2147
01:12:34,140 --> 01:12:35,400
pleit voor een soort modulaire

2148
01:12:35,400 --> 01:12:37,800
architectuur voor llms en wat een

2149
01:12:37,800 --> 01:12:39,060
heel mooi raamwerk is, toch?  zeer

2150
01:12:39,060 --> 01:12:40,860
cognitief plausibel, het is precies het

2151
01:12:40,860 --> 01:12:41,880
soort ding waar we naar zouden moeten streven, het is

2152
01:12:41,880 --> 01:12:43,679
compatibel met Howard

2153
01:12:43,679 --> 01:12:45,239
Gardner's je weet wel begrip van meervoudige

2154
01:12:45,239 --> 01:12:46,800
intelligenties enzovoort,

2155
01:12:46,800 --> 01:12:48,000
maar ik denk tegelijkertijd

2156
01:12:48,000 --> 01:12:49,620
om deze opmerking af te maken,

2157
01:12:49,620 --> 01:12:51,360
er was laatst een technisch gesprek  week

2158
01:12:51,360 --> 01:12:54,840
denk ik of misschien een paar dagen geleden waar eh een

2159
01:12:54,840 --> 01:12:56,880
heleboel andere dingen

2160
01:12:56,880 --> 01:12:59,520
op onproductieve manieren kunnen worden samengevoegd met AI-hype, dus Greg

2161
01:12:59,520 --> 01:13:02,100
Brockman van openai gaf een van zijn

2162
01:13:02,100 --> 01:13:04,199
uh een van deze grote Ted Talks waar hij

2163
01:13:04,199 --> 01:13:06,179
verschillende plug-ins liet zien die GPD chatten

2164
01:13:06,179 --> 01:13:08,159
kan doen Ik noemde Wolfram bedienen, maar

2165
01:13:08,159 --> 01:13:09,300
er zijn ook dingen zoals het

2166
01:13:09,300 --> 01:13:11,820
genereren van afbeeldingen instacart shopping waar je

2167
01:13:11,820 --> 01:13:13,560
chat-tv kunt krijgen om dingen voor je te kopen en

2168
01:13:13,560 --> 01:13:15,060
wat heb je

2169
01:13:15,060 --> 01:13:17,040
um en nogmaals, dit brengt je terug naar

2170
01:13:17,040 --> 01:13:18,960
het idee dat meerdere subsystemen

2171
01:13:18,960 --> 01:13:20,940
verschillende subfuncties kunnen doen, dus Brockovich  liet

2172
01:13:20,940 --> 01:13:22,620
ook een voorbeeld zien van het geven van chat-

2173
01:13:22,620 --> 01:13:26,820
GPT een Excel-bestand een CSV-bestand en uit een

2174
01:13:26,820 --> 01:13:28,739
archiefdatabase met academische papers

2175
01:13:28,739 --> 01:13:30,060
waar het gewoon een heleboel papers opsomde

2176
01:13:30,060 --> 01:13:31,739
en dan titels en wat heb je

2177
01:13:31,739 --> 01:13:32,880
goed

2178
01:13:32,880 --> 01:13:34,500
eh en hij zei dat je weet dat

2179
01:13:34,500 --> 01:13:37,440
het chatipati gebruikt  Wereldkennis om

2180
01:13:37,440 --> 01:13:39,060
af te leiden wat de titels van de kolommen

2181
01:13:39,060 --> 01:13:40,860
betekenen, dus we begrepen dat je weet dat

2182
01:13:40,860 --> 01:13:42,780
titel de titel van het artikel betekent, het

2183
01:13:42,780 --> 01:13:44,640
begreep dat auteurs het aantal

2184
01:13:44,640 --> 01:13:47,280
auteurs per artikel bedoelden, het begreep dat

2185
01:13:47,280 --> 01:13:48,840
gemaakt de datum betekent waarop het artikel

2186
01:13:48,840 --> 01:13:50,820
correct is ingediend en omdat het een  TED-

2187
01:13:50,820 --> 01:13:52,080
talk, weet je, het hele publiek

2188
01:13:52,080 --> 01:13:54,120
gaf ons een staande ovatie,

2189
01:13:54,120 --> 01:13:56,340
maar de mogelijkheid om labels in

2190
01:13:56,340 --> 01:13:57,900
een Excel-bestand te beschrijven

2191
01:13:57,900 --> 01:14:01,679
is denk ik leuk, maar ik weet niet zeker of

2192
01:14:01,679 --> 01:14:03,120
je het echt Wereldkennis zou noemen, dus

2193
01:14:03,120 --> 01:14:04,800
ik denk dat er veel is  Ik zou alleen willen zeggen dat er

2194
01:14:04,800 --> 01:14:06,719
veel vooruitgang moet worden

2195
01:14:06,719 --> 01:14:08,880
geboekt naast het verminderen van antropomosoom

2196
01:14:08,880 --> 01:14:11,640
en je hebt de juiste balans ervan, dus

2197
01:14:11,640 --> 01:14:12,659
zoals ik al zei, je moet de juiste

2198
01:14:12,659 --> 01:14:13,920
balans hebben van

2199
01:14:13,920 --> 01:14:15,360
een psychologisch plausibele soort

2200
01:14:15,360 --> 01:14:17,100
modulaire architectuur, maar je kunt niet te

2201
01:14:17,100 --> 01:14:18,480
veel hebben  veel

2202
01:14:18,480 --> 01:14:19,920
eh antropomorfisme, want dan laat je je

2203
01:14:19,920 --> 01:14:21,540
meeslepen, je moet vinden, we

2204
01:14:21,540 --> 01:14:22,860
moeten de juiste balans vinden tussen het

2205
01:14:22,860 --> 01:14:25,739
modelleren van een soort mensachtige uh modulaire

2206
01:14:25,739 --> 01:14:27,780
systemen, maar het niet doen in een mate

2207
01:14:27,780 --> 01:14:29,280
die een beetje is, weet je,

2208
01:14:29,280 --> 01:14:31,140
um ongeloofwaardig of wetenschappelijk

2209
01:14:31,140 --> 01:14:33,679
onbehulpzaam

2210
01:14:35,040 --> 01:14:37,500
Ik bedoel, ik denk dat ik het met alles eens ben dat ik

2211
01:14:37,500 --> 01:14:40,199
erg enthousiast ben over deze uh manieren

2212
01:14:40,199 --> 01:14:42,739
om taalmodellen te verbinden met

2213
01:14:42,739 --> 01:14:45,120
uh andere vormen van

2214
01:14:45,120 --> 01:14:47,640
informatieverwerking uh wat lijkt op

2215
01:14:47,640 --> 01:14:49,679
wat mensen hebben, denk ik dat ik heb, ik bedoel

2216
01:14:49,679 --> 01:14:52,620
ik'  Ik ben erg verrast geweest over de uh

2217
01:14:52,620 --> 01:14:54,840
de dingen die ze kunnen doen,

2218
01:14:54,840 --> 01:14:58,260
net als taalmodellering, dus

2219
01:14:58,260 --> 01:15:00,120
u kent verschillende soorten

2220
01:15:00,120 --> 01:15:01,920
redeneerpuzzels en dingen die ze kunnen oplossen. Ik

2221
01:15:01,920 --> 01:15:05,340
denk dat het uh echt fascinerend is en

2222
01:15:05,340 --> 01:15:07,920
en weet je, misschien zal het  van ons eisen dat we

2223
01:15:07,920 --> 01:15:09,719
onze u kent de

2224
01:15:09,719 --> 01:15:11,760
relaties tussen taal en

2225
01:15:11,760 --> 01:15:13,500
denken heroverwegen en proberen een manier te vinden

2226
01:15:13,500 --> 01:15:15,239
om specifiek te zijn over wat het betekent dat

2227
01:15:15,239 --> 01:15:17,760
iets een

2228
01:15:17,760 --> 01:15:19,380
representatie heeft of om over

2229
01:15:19,380 --> 01:15:21,060
die representatie te redeneren Maar uiteindelijk

2230
01:15:21,060 --> 01:15:23,820
denk ik dat ik het ermee eens ben  dat eh eh

2231
01:15:23,820 --> 01:15:26,219
weet je dat mensen verschillende manieren

2232
01:15:26,219 --> 01:15:28,560
van denken over dingen hebben en dat dat

2233
01:15:28,560 --> 01:15:32,400
belangrijk lijkt voor eh voor intelligentie

2234
01:15:32,400 --> 01:15:35,040
eh ik ben ook super enthousiast over de baby

2235
01:15:35,040 --> 01:15:37,620
LM-uitdaging, dus ik denk aan de soort

2236
01:15:37,620 --> 01:15:39,960
taalkundige kant, juist eh

2237
01:15:39,960 --> 01:15:42,300
eh dat is precies de de  goed om te

2238
01:15:42,300 --> 01:15:45,300
zien hoe ver we kunnen komen met eh

2239
01:15:45,300 --> 01:15:47,219
kleinere datasets eh en misschien

2240
01:15:47,219 --> 01:15:50,820
weet je uiteindelijk daarna dat je probeert eh eh

2241
01:15:50,820 --> 01:15:53,699
wat meer te begrijpen over

2242
01:15:53,699 --> 01:15:55,739
het soort semantiek dat kinderen

2243
01:15:55,739 --> 01:15:57,840
verwerven en en waar ze het vandaan halen

2244
01:15:57,840 --> 01:15:59,820
en en hoe  soort externe semantiek

2245
01:15:59,820 --> 01:16:02,520
kan uh leren van talen informeren of

2246
01:16:02,520 --> 01:16:04,560
specifiek misschien grammatica en

2247
01:16:04,560 --> 01:16:06,600
syntaxis leren

2248
01:16:06,600 --> 01:16:09,480
eh ik denk dat mijn andere uh pad voorwaarts

2249
01:16:09,480 --> 01:16:12,179
punt zou zijn eh

2250
01:16:12,179 --> 01:16:15,840
dat er eh zoals ik Ik heb het gevoel dat

2251
01:16:15,840 --> 01:16:18,300
dit soort modellen eh

2252
01:16:18,300 --> 01:16:21,179
eh echt ver zijn gegaan  buiten de verwachtingen van mensen

2253
01:16:21,179 --> 01:16:23,340
voor dit soort

2254
01:16:23,340 --> 01:16:25,500
modelklassen, het juiste soort

2255
01:16:25,500 --> 01:16:27,960
statistisch leren op de grond,

2256
01:16:27,960 --> 01:16:29,940
patronen ontdekken in de tekst

2257
01:16:29,940 --> 01:16:30,719
um

2258
01:16:30,719 --> 01:16:32,640
um lijkt echt

2259
01:16:32,640 --> 01:16:35,040
behoorlijk opmerkelijke resultaten te geven eh

2260
01:16:35,040 --> 01:16:37,020
en dat heeft voor mij in de toekomst volgens mij

2261
01:16:37,020 --> 01:16:39,360
zojuist een enorme golf van

2262
01:16:39,360 --> 01:16:42,000
onzekerheid over theorieën, dus ik denk

2263
01:16:42,000 --> 01:16:43,620
dat onze theorieën van eigenlijk

2264
01:16:43,620 --> 01:16:46,739
alles in taal zeker

2265
01:16:46,739 --> 01:16:48,960
eh maar cognitie waarschijnlijk Neurowetenschap,

2266
01:16:48,960 --> 01:16:50,640
zoals al die dingen waarvan ik denk dat ze

2267
01:16:50,640 --> 01:16:53,100
zullen worden herwerkt als we,

2268
01:16:53,100 --> 01:16:54,420
wanneer we echt

2269
01:16:54,420 --> 01:16:56,219
het

2270
01:16:56,219 --> 01:16:58,560
eh uh het vermogen gaan begrijpen  van echt algemene

2271
01:16:58,560 --> 01:17:02,219
soorten leersystemen zoals deze, dus eh

2272
01:17:02,219 --> 01:17:03,900
dat maakt het dat je weet dat aan de ene

2273
01:17:03,900 --> 01:17:05,820
kant uh

2274
01:17:05,820 --> 01:17:07,440
um nogal een spelbreker is voor

2275
01:17:07,440 --> 01:17:09,360
theorieën uit het verleden, juist, vooral theorieën die

2276
01:17:09,360 --> 01:17:11,640
vertrouwden op

2277
01:17:11,640 --> 01:17:14,100
um uh je weet dat leren niet

2278
01:17:14,100 --> 01:17:15,659
goed kan werken

2279
01:17:15,659 --> 01:17:18,000
um  maar aan de positieve kant denk ik dat het het

2280
01:17:18,000 --> 01:17:20,460
een zeer opwindende tijd maakt, zowel voor

2281
01:17:20,460 --> 01:17:22,620
AI en cognitieve wetenschap als voor

2282
01:17:22,620 --> 01:17:24,000
taalkunde

2283
01:17:24,000 --> 01:17:25,560
eh waar zijn er nu deze echt

2284
01:17:25,560 --> 01:17:27,420
heel krachtige tools eh

2285
01:17:27,420 --> 01:17:29,719
dat lijkt een kwalitatief

2286
01:17:29,719 --> 01:17:32,940
eh verschillende grootte stap in de richting van menselijke

2287
01:17:32,940 --> 01:17:34,620
menselijke capaciteiten

2288
01:17:34,620 --> 01:17:36,120
um  en ik denk dat ik ze een beetje integreer

2289
01:17:36,120 --> 01:17:39,060
en zowel het soort

2290
01:17:39,060 --> 01:17:41,280
technische lessen als het soort

2291
01:17:41,280 --> 01:17:43,679
filosofische lessen neem over hoe ze

2292
01:17:43,679 --> 01:17:45,540
gemaakt zijn en wat voor soort principes er komen kijken

2293
01:17:45,540 --> 01:17:47,460
bij het ontwerpen van intelligente systemen. Ik

2294
01:17:47,460 --> 01:17:49,679
denk dat die dingen zullen

2295
01:17:49,679 --> 01:17:52,080
zal het gevoel echt bepalen in de komende

2296
01:17:52,080 --> 01:17:53,699
vijf of tien jaar

2297
01:17:53,699 --> 01:17:54,900
eh

2298
01:17:54,900 --> 01:17:57,000
en ook zoals ik gewoon zou zeggen in de

2299
01:17:57,000 --> 01:17:58,500
context van bredere thema's hier,

2300
01:17:58,500 --> 01:17:59,760
zoals je helemaal gelijk hebt, zoals ik me

2301
01:17:59,760 --> 01:18:01,620
herinner toen ik las over wanneer

2302
01:18:01,620 --> 01:18:04,920
diepblauw is uh Casper van  was het het

2303
01:18:04,920 --> 01:18:07,260
schaken, eh, ja, en er waren

2304
01:18:07,260 --> 01:18:08,940
enkele commentatoren die zeiden: je weet dat

2305
01:18:08,940 --> 01:18:12,480
schaken voorbij is, als een AI een mens kan zijn,

2306
01:18:12,480 --> 01:18:13,739
dan is het spel voorbij, wat heeft het voor zin om

2307
01:18:13,739 --> 01:18:15,420
schaken te studeren, je weet dat saai niet meer nodig is

2308
01:18:15,420 --> 01:18:16,920


2309
01:18:16,920 --> 01:18:18,900
eh en ik denk  als AI

2310
01:18:18,900 --> 01:18:20,640
schijnbaar alles heeft bereikt wat mensen moeten

2311
01:18:20,640 --> 01:18:22,140
doen om te schaken, wat heeft het dan voor zin om

2312
01:18:22,140 --> 01:18:23,100
het te spelen,

2313
01:18:23,100 --> 01:18:24,540
maar ik denk dat je weet dat het

2314
01:18:24,540 --> 01:18:26,219
de populariteit van

2315
01:18:26,219 --> 01:18:27,900
schaken heeft vergroot, toch zitten ze in onze mini-

2316
01:18:27,900 --> 01:18:29,580
schaakberoemdheden en ook in wereldwijde

2317
01:18:29,580 --> 01:18:31,199
toernooien  en ik zou voorspellen dat

2318
01:18:31,199 --> 01:18:32,520
hetzelfde waarschijnlijk ook zal gebeuren met

2319
01:18:32,520 --> 01:18:34,620
taal, weet je, llms betekent niet dat het

2320
01:18:34,620 --> 01:18:36,360
het einde van de taal is, geen

2321
01:18:36,360 --> 01:18:37,800
taal meer, niet meer. Taalkunde. Ik zou

2322
01:18:37,800 --> 01:18:39,179
eigenlijk terugdringen en zeggen dat het misschien

2323
01:18:39,179 --> 01:18:40,620
het tegenovergestelde zou zijn.

2324
01:18:40,620 --> 01:18:42,540
LMS zal de

2325
01:18:42,540 --> 01:18:44,400
algemene belangstelling voor taaltheorie vergroten

2326
01:18:44,400 --> 01:18:46,199
vanwege hun koppeling, je kent

2327
01:18:46,199 --> 01:18:48,060
rare beperkingen en schijnbare

2328
01:18:48,060 --> 01:18:50,100
beperkingen, toch, want ik zou ook

2329
01:18:50,100 --> 01:18:52,380
zeggen dat je op dit moment de schaal kent, de

2330
01:18:52,380 --> 01:18:55,500
schaal voor stresskwesties is zeker

2331
01:18:55,500 --> 01:18:57,420
verre van alles wat nodig is, wat

2332
01:18:57,420 --> 01:18:59,820
ontbreekt is  een vermogen van LMS om u te kennen,

2333
01:18:59,820 --> 01:19:01,260
abstraheert hun kennis en

2334
01:19:01,260 --> 01:19:03,239
ervaringen echt om

2335
01:19:03,239 --> 01:19:04,739
voorspellingen en generalisaties voor groepswassingen te maken, enzovoort.

2336
01:19:04,739 --> 01:19:06,780


2337
01:19:06,780 --> 01:19:07,860


2338
01:19:07,860 --> 01:19:09,179


2339
01:19:09,179 --> 01:19:10,739
ga van misschien

2340
01:19:10,739 --> 01:19:13,620
bepaalde soorten tokens en maar ik zou willen dat

2341
01:19:13,620 --> 01:19:15,060
ik denk dat mijn laatste mijn laatste

2342
01:19:15,060 --> 01:19:17,100
bewering zou zijn dat je de taalverwervingsliteratuur kent

2343
01:19:17,100 --> 01:19:19,080


2344
01:19:19,080 --> 01:19:21,480
eh heeft niet noodzakelijkerwijs llms nodig hoewel

2345
01:19:21,480 --> 01:19:22,860
je weet dat cognitieve wetenschappers niet

2346
01:19:22,860 --> 01:19:26,219
echt llms nodig hebben, we zouden mogelijk kunnen  uh

2347
01:19:26,219 --> 01:19:27,420
weet je, herstel en ben het

2348
01:19:27,420 --> 01:19:29,640
hier duidelijk niet mee eens, maar eh, ik zou zeggen dat grote

2349
01:19:29,640 --> 01:19:32,040
technologiebedrijven die profiteren van llms llms nodig hebben,

2350
01:19:32,040 --> 01:19:33,360
juist zij zijn de enigen die het

2351
01:19:33,360 --> 01:19:35,100
echt doen, het kan zijn dat de

2352
01:19:35,100 --> 01:19:37,320
geest een heel ik zal zeggen dat je weet dat de

2353
01:19:37,320 --> 01:19:39,000
geest is  in een zeer diverse ruimte kan het zijn

2354
01:19:39,000 --> 01:19:40,560
dat er bepaalde vormen van gedrag

2355
01:19:40,560 --> 01:19:42,420
en leren zijn die kunnen worden vastgelegd door

2356
01:19:42,420 --> 01:19:44,219
processen die vergelijkbaar zijn met wat llms doen,

2357
01:19:44,219 --> 01:19:45,540
dus Stephen heeft

2358
01:19:45,540 --> 01:19:47,159
in zijn artikelen enkele interessante voorbeelden gegeven over magnetisme

2359
01:19:47,159 --> 01:19:49,320
en vreemde leerregels

2360
01:19:49,320 --> 01:19:51,060
die zijn  heel domein Algemeen en heel

2361
01:19:51,060 --> 01:19:52,920
snel en heel mysterieus, dus je weet

2362
01:19:52,920 --> 01:19:54,780
misschien voor dat soort dingen dat

2363
01:19:54,780 --> 01:19:56,100
dat soort leren relevant zal zijn

2364
01:19:56,100 --> 01:19:57,780
en maar ik denk nog steeds dat het

2365
01:19:57,780 --> 01:19:59,580
onwaarschijnlijk is dat een van de kandidaten

2366
01:19:59,580 --> 01:20:02,100
natuurlijke taal zal zijn en in ieder geval de manier waarop

2367
01:20:02,100 --> 01:20:03,480
natuurlijk  taal werkt en het is volle

2368
01:20:03,480 --> 01:20:05,219
glorie in termen van de vorm, voornamelijk

2369
01:20:05,219 --> 01:20:07,380
regulering en wat heb je, dus ik denk dat ik

2370
01:20:07,380 --> 01:20:09,300
zou willen dat je weet dat het me een beetje doet denken

2371
01:20:09,300 --> 01:20:11,520
aan waar je weet dat je dit

2372
01:20:11,520 --> 01:20:13,320
beeld hebt van ik zag John Wick hoofdstuk vier

2373
01:20:13,320 --> 01:20:14,940
onlangs goed en hij heeft  dit is

2374
01:20:14,940 --> 01:20:16,140
een scène waarin hij door de

2375
01:20:16,140 --> 01:20:17,580
woestijn loopt en hij niet zeker weet of hij deze man heeft gezien

2376
01:20:17,580 --> 01:20:19,260


2377
01:20:19,260 --> 01:20:21,540


2378
01:20:21,540 --> 01:20:22,739


2379
01:20:22,739 --> 01:20:24,840


2380
01:20:24,840 --> 01:20:26,460
alsof hij hem wil vermoorden.  Je

2381
01:20:26,460 --> 01:20:28,199
hallucineert, maar dan realiseer je je dat

2382
01:20:28,199 --> 01:20:29,820
je soms voordat het te laat is weet dat

2383
01:20:29,820 --> 01:20:31,679
je echt hallucineert, dat

2384
01:20:31,679 --> 01:20:33,239
je geen oase ziet, dat je nog steeds

2385
01:20:33,239 --> 01:20:35,040
in de woestijn bent en ik denk dat dat

2386
01:20:35,040 --> 01:20:37,199
misschien de situatie is waarin we ons nu bevinden

2387
01:20:37,199 --> 01:20:39,360
met taalvaardigheid van veel

2388
01:20:39,360 --> 01:20:41,400
taalmodellen hebben we de illusie

2389
01:20:41,400 --> 01:20:44,940
van uh taalvaardigheid, want weet je,

2390
01:20:44,940 --> 01:20:46,260
je ziet altijd de illusie voordat

2391
01:20:46,260 --> 01:20:48,360
je de oase goed vindt, dus ik denk dat ik

2392
01:20:48,360 --> 01:20:49,440
denk dat we ons nu in de

2393
01:20:49,440 --> 01:20:51,420
hallucinerende staat van de woestijn bevinden waar

2394
01:20:51,420 --> 01:20:53,699
we  Ik zie potentiële vonken van

2395
01:20:53,699 --> 01:20:55,380
taalvaardigheid, maar het is nog steeds niet

2396
01:20:55,380 --> 01:20:57,420
erg duidelijk en robuust.

2397
01:20:57,420 --> 01:20:59,640
We hebben de Oasis nog niet bereikt.

2398
01:20:59,640 --> 01:21:02,480


2399
01:21:02,820 --> 01:21:06,360


2400
01:21:06,360 --> 01:21:09,540


2401
01:21:09,540 --> 01:21:11,219


2402
01:21:11,219 --> 01:21:13,440
om te zeggen

2403
01:21:13,440 --> 01:21:15,120
dat grote taalmodellen geen priors hebben,

2404
01:21:15,120 --> 01:21:17,540


2405
01:21:18,480 --> 01:21:21,300
hebben grote taalmodellen priors Ik zou

2406
01:21:21,300 --> 01:21:23,820
zeggen ja, dat doen ze zeker

2407
01:21:23,820 --> 01:21:25,620
eh en

2408
01:21:25,620 --> 01:21:28,440
eh, ik denk dat het het verschil is met

2409
01:21:28,440 --> 01:21:30,360
hoe mensen die je kent gewend zijn te denken

2410
01:21:30,360 --> 01:21:32,340
over priors en in Bayesiaanse gevolgtrekking

2411
01:21:32,340 --> 01:21:33,840
voor  bijvoorbeeld als je een

2412
01:21:33,840 --> 01:21:36,300
Bayesiaans statistisch model wilt opschrijven, zeg je

2413
01:21:36,300 --> 01:21:37,860
alsof je weet dat hier de parameters zijn en

2414
01:21:37,860 --> 01:21:39,300
dit zijn de priors op de

2415
01:21:39,300 --> 01:21:40,860
parameters

2416
01:21:40,860 --> 01:21:42,360
um grote taalmodellen. Ik denk dat de

2417
01:21:42,360 --> 01:21:44,340
priors zijn en misschien neurale noten in het

2418
01:21:44,340 --> 01:21:45,900
algemeen. Ik denk dat de dat de  de

2419
01:21:45,900 --> 01:21:47,880
prioren zijn veel meer impliciet, dus

2420
01:21:47,880 --> 01:21:49,679
er zijn enkele functies die ze

2421
01:21:49,679 --> 01:21:52,080
gemakkelijker te leren vinden dan uh dan andere

2422
01:21:52,080 --> 01:21:53,760
functies en er is zelfs wat werk

2423
01:21:53,760 --> 01:21:55,860
om te ontdekken dat je een

2424
01:21:55,860 --> 01:21:57,540
verklaring weet van wat dat soort impliciete

2425
01:21:57,540 --> 01:21:59,040
prioren zijn

2426
01:21:59,040 --> 01:22:00,840
eh, maar dat is eigenlijk hoe ik denk

2427
01:22:00,840 --> 01:22:02,040
over eh um

2428
01:22:02,040 --> 01:22:02,940


2429
01:22:02,940 --> 01:22:04,620
je weet vergelijking van verschillende

2430
01:22:04,620 --> 01:22:07,199
neurale netwerkarchitecturen juist eh eh

2431
01:22:07,199 --> 01:22:09,060
wat misschien iets opgetogen is

2432
01:22:09,060 --> 01:22:10,500
en ik zou het misschien eens kunnen zijn, zoals

2433
01:22:10,500 --> 01:22:12,600
je priors moet vinden waarmee ze

2434
01:22:12,600 --> 01:22:14,400
de dingen kunnen leren die kinderen

2435
01:22:14,400 --> 01:22:16,140
goed leren en

2436
01:22:16,140 --> 01:22:19,080
eh niet alle architecturen zullen dat doen  doe dat eh,

2437
01:22:19,080 --> 01:22:21,360
zelfs onder architecturen die

2438
01:22:21,360 --> 01:22:23,280
compleet worden of in staat zijn om

2439
01:22:23,280 --> 01:22:24,719
welke functie dan ook te leren, ze

2440
01:22:24,719 --> 01:22:27,540
zullen het niet allemaal doen, eh zelfs bij een soort

2441
01:22:27,540 --> 01:22:30,000
enorme dataset, dus

2442
01:22:30,000 --> 01:22:31,800
eh, ik denk aan dit soort zoeken over

2443
01:22:31,800 --> 01:22:34,080
neurale net-architecturen als  echt een van

2444
01:22:34,080 --> 01:22:36,360
een zoekopdracht over priors

2445
01:22:36,360 --> 01:22:38,580
um maar het zijn geen priors of ik bedoel, je zou

2446
01:22:38,580 --> 01:22:39,719
het kunnen zien als een zoekopdracht over

2447
01:22:39,719 --> 01:22:41,699
universele grammatica of zoiets, maar

2448
01:22:41,699 --> 01:22:44,340
het is het is geen priors of universele

2449
01:22:44,340 --> 01:22:46,560
grammatica in de zin dat mensen

2450
01:22:46,560 --> 01:22:48,540
erover hebben gesproken als zoiets  een expliciete

2451
01:22:48,540 --> 01:22:50,159
verklaring over wat voor soort regels zijn

2452
01:22:50,159 --> 01:22:51,780
toegestaan ​​of een expliciete verklaring over

2453
01:22:51,780 --> 01:22:53,880
wat voor soort functies zeer

2454
01:22:53,880 --> 01:22:55,320
waarschijnlijk zijn of iets dergelijks het is

2455
01:22:55,320 --> 01:22:57,360
daar allemaal impliciet gecodeerd eh ja ja

2456
01:22:57,360 --> 01:22:58,560
helemaal ik denk dat ik denk dat dat klopt ik bedoel,

2457
01:22:58,560 --> 01:23:00,540
je weet dat de echte

2458
01:23:00,540 --> 01:23:02,880
vraag is om te verminderen  de ruimte van wat

2459
01:23:02,880 --> 01:23:05,100
degenen die gelijk waarderen en als het ook maar in de

2460
01:23:05,100 --> 01:23:07,020
verte lijkt op wat mensen

2461
01:23:07,020 --> 01:23:09,300
doen, dan zou ik willen

2462
01:23:09,300 --> 01:23:11,219
zeggen dat dingen als qpt3

2463
01:23:11,219 --> 01:23:13,380
bestaand bewijs zijn dat je weet dat

2464
01:23:13,380 --> 01:23:15,960
het bouwen van volledig functionerende syntactische

2465
01:23:15,960 --> 01:23:18,000
categorieën op basis van

2466
01:23:18,000 --> 01:23:21,420
oppervlakteverdelingsanalyse  dit alleen is mogelijk dat is

2467
01:23:21,420 --> 01:23:24,120
ja, dat is correct, maar toch

2468
01:23:24,120 --> 01:23:27,300
zou ik zeggen dat de meeste beoefenaars niet

2469
01:23:27,300 --> 01:23:29,460
echt geloven dat syntactische categorieën

2470
01:23:29,460 --> 01:23:31,380
aangeboren zijn, dus het eerdere probleem is

2471
01:23:31,380 --> 01:23:33,060
iets minder irrelevant hier zijn het de

2472
01:23:33,060 --> 01:23:35,219
bewerkingen die zijn ingesteld om aangeboren te zijn, dus

2473
01:23:35,219 --> 01:23:37,500
de in  het syntaxisdomein het zijn bepaalde

2474
01:23:37,500 --> 01:23:39,659
taalkundige berekeningen waarvan wordt gezegd dat ze

2475
01:23:39,659 --> 01:23:41,159
in een en categorieën zelf vallen, in

2476
01:23:41,159 --> 01:23:42,719
feite heeft zelfs Charles Yang

2477
01:23:42,719 --> 01:23:44,100
um de afgelopen paar jaar toegegeven

2478
01:23:44,100 --> 01:23:46,800
dat ze er misschien in zitten, maar misschien niet,

2479
01:23:46,800 --> 01:23:49,679
dus mensen hebben een andere

2480
01:23:49,679 --> 01:23:51,540
relevante prioriteit gegeven, zijn dingen  zoals

2481
01:23:51,540 --> 01:23:53,100
eh, je kent mij en Gary Markets hebben

2482
01:23:53,100 --> 01:23:55,080
het gehad over compositie die

2483
01:23:55,080 --> 01:23:56,640
een groot probleem lijkt te zijn, dus mensen hebben

2484
01:23:56,640 --> 01:23:59,520
chat gbt BBC News-artikelen gegeven met het verzoek om het te

2485
01:23:59,520 --> 01:24:02,280
comprimeren en het vervolgens opnieuw uit te leggen uh dus

2486
01:24:02,280 --> 01:24:04,920
een voorbeeld dat ik zag was Peter Smith 58 is

2487
01:24:04,920 --> 01:24:06,480
gearresteerd worden op beschuldiging van

2488
01:24:06,480 --> 01:24:09,060
doodslag en je laat het comprimeren

2489
01:24:09,060 --> 01:24:10,920
en opnieuw uitleggen en het komt eruit alsof

2490
01:24:10,920 --> 01:24:12,420
58 mensen worden beschuldigd van

2491
01:24:12,420 --> 01:24:14,040
doodslag, dat is een vrij duidelijk

2492
01:24:14,040 --> 01:24:15,659
voorbeeld van een gebrek aan compositie dat

2493
01:24:15,659 --> 01:24:17,219
wordt ingebouwd in welke compressie

2494
01:24:17,219 --> 01:24:19,260
het ook doet en er is  nog een voorbeeld

2495
01:24:19,260 --> 01:24:20,940
waar er enkele voorbeelden zijn

2496
01:24:20,940 --> 01:24:23,159
van mogelijke analoge redenering, dus in

2497
01:24:23,159 --> 01:24:24,840
Bing-chat weet je dat Bing deze valstrikfunctie heeft.

2498
01:24:24,840 --> 01:24:26,100


2499
01:24:26,100 --> 01:24:28,080
De vraag is of het alleen

2500
01:24:28,080 --> 01:24:29,460
metarelaties vindt die al

2501
01:24:29,460 --> 01:24:31,080
door mensen zijn gedocumenteerd of creëert het echt

2502
01:24:31,080 --> 01:24:33,420
nieuwe relaties de nieuwe  dingen

2503
01:24:33,420 --> 01:24:35,100
die worden gebouwd,

2504
01:24:35,100 --> 01:24:38,520
dus weet je, iemand vroeg, uh, tekende

2505
01:24:38,520 --> 01:24:41,760
een tafel voor me waarin Jezus Christus werd vergeleken met de

2506
01:24:41,760 --> 01:24:44,640
Nokia 9910, precies de mobiele telefoon Nokia

2507
01:24:44,640 --> 01:24:46,080
9910,

2508
01:24:46,080 --> 01:24:47,520
en er stond dat je weet dat ze

2509
01:24:47,520 --> 01:24:49,860
de releasedatums vergeleken, het vergeleek de grootte,

2510
01:24:49,860 --> 01:24:53,280
het gewicht waarmee het de CPU vergeleek

2511
01:24:53,280 --> 01:24:55,140
Jezus 'almachtige kennis

2512
01:24:55,140 --> 01:24:57,600
vergeleek het geheugen van de telefoon met

2513
01:24:57,600 --> 01:25:00,600
de alwetende aard van God.

2514
01:25:00,600 --> 01:25:02,760


2515
01:25:02,760 --> 01:25:04,560


2516
01:25:04,560 --> 01:25:06,239


2517
01:25:06,239 --> 01:25:08,159
geweldig antwoord

2518
01:25:08,159 --> 01:25:10,560
wat is er mis mee dat is oké het

2519
01:25:10,560 --> 01:25:12,120
kan zijn dat het misschien veel op

2520
01:25:12,120 --> 01:25:13,860
analoge redenering lijkt, maar dan had het ook

2521
01:25:13,860 --> 01:25:15,420
een aantal nogal rare dingen waar het was

2522
01:25:15,420 --> 01:25:17,100
zoals je weet voor de camera zei het nee

2523
01:25:17,100 --> 01:25:19,679
het gaf alleen de beschrijving van Jezus of het is

2524
01:25:19,679 --> 01:25:21,659
niet echt wat  een camera is er zijn een aantal

2525
01:25:21,659 --> 01:25:24,000
dingen die lijken op analoog

2526
01:25:24,000 --> 01:25:27,860
redeneren misschien, maar het is onduidelijk ja hey

2527
01:25:27,860 --> 01:25:31,440
ik denk dat dat klinkt als een

2528
01:25:31,440 --> 01:25:33,020
geweldig antwoord voor mij ik ik wilde

2529
01:25:33,020 --> 01:25:36,360
zeggen zoals jij zei je zei

2530
01:25:36,360 --> 01:25:38,100
grote taalmodellen leren hun

2531
01:25:38,100 --> 01:25:39,719
bestaan ​​bewijs van een deel van meningsuiting

2532
01:25:39,719 --> 01:25:41,520
categorieën, maar alsof ze niet alleen

2533
01:25:41,520 --> 01:25:43,380
woordsoortcategorieën uitvoeren,

2534
01:25:43,380 --> 01:25:45,719
alsof ze veel grammaticale

2535
01:25:45,719 --> 01:25:47,640
syntactische kennis hebben

2536
01:25:47,640 --> 01:25:50,880
eh, en bovendien alsof ze

2537
01:25:50,880 --> 01:25:52,980
veel semantische kennis hebben en waarschijnlijk wat

2538
01:25:52,980 --> 01:25:55,080
pragmatische kennis en je weet dat ze dat zijn

2539
01:25:55,080 --> 01:25:58,080
niet slecht in vertalen en alsof het

2540
01:25:58,080 --> 01:25:59,880
veel meer is dat ze

2541
01:25:59,880 --> 01:26:01,139


2542
01:26:01,139 --> 01:26:03,840
eh hebben ontdekt dan alleen woordsoorten categorieën

2543
01:26:03,840 --> 01:26:07,020
eh eh nou het spijt me dat ik zei sorry

2544
01:26:07,020 --> 01:26:08,940
het is een technische categorie

2545
01:26:08,940 --> 01:26:11,040
juist ja nou sorry dus ja ja maar

2546
01:26:11,040 --> 01:26:13,080
ze hebben ontdekt  veel meer dan dat eh

2547
01:26:13,080 --> 01:26:13,980


2548
01:26:13,980 --> 01:26:15,600
ja um

2549
01:26:15,600 --> 01:26:18,780
ik ga als een um teaser slash

2550
01:26:18,780 --> 01:26:20,699
motivator voor hopelijk jullie beiden om

2551
01:26:20,699 --> 01:26:23,040
in de toekomst weer mee te doen met of

2552
01:26:23,040 --> 01:26:24,540
zonder andere gasten een paar van de

2553
01:26:24,540 --> 01:26:26,520
spannende vragen die alleen voor ons

2554
01:26:26,520 --> 01:26:28,679
in dit transcript kunnen worden opgenomen en  dan

2555
01:26:28,679 --> 01:26:30,060
bedankt zowel Ellie als Steven voor het

2556
01:26:30,060 --> 01:26:31,260
meedoen, dus slechts een paar van de laatste

2557
01:26:31,260 --> 01:26:33,719
vragen die werden gesteld. Juan vroeg hoe

2558
01:26:33,719 --> 01:26:35,820
kleine Transformers Jang in het geheel 2020

2559
01:26:35,820 --> 01:26:38,580
vergeleken met kinderen die taal leren

2560
01:26:38,580 --> 01:26:40,860
um 96 vroeg wat is uw mening over

2561
01:26:40,860 --> 01:26:42,800
impliciete priors versus dierlijk instinct

2562
01:26:42,800 --> 01:26:45,780
vroeg rojda  welke beperkingen die ruimte

2563
01:26:45,780 --> 01:26:48,420
in llms krijgen ze daar niet door te trainen,

2564
01:26:48,420 --> 01:26:50,699
dus ontdekken ze het, dat is niet

2565
01:26:50,699 --> 01:26:52,500
wat ze in het begin implementeren, misschien

2566
01:26:52,500 --> 01:26:54,780
en er zijn nog veel meer vragen, dus ik

2567
01:26:54,780 --> 01:26:57,540
hoop dat we allemaal elkaars werken kunnen beoordelen en

2568
01:26:57,540 --> 01:27:00,600
herlezen en  kom

2569
01:27:00,600 --> 01:27:04,020
samen voor 41.2 in de toekomst

2570
01:27:04,020 --> 01:27:06,060
bedankt Elliot en Steven voor deze

2571
01:27:06,060 --> 01:27:08,280
uitstekende stream bedankt Dave bedankt

2572
01:27:08,280 --> 01:27:10,739
allebei ja heel erg bedankt tot ziens tot

2573
01:27:10,739 --> 01:27:15,379
ziens


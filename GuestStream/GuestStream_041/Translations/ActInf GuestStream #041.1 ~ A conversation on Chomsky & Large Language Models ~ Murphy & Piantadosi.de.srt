1
00:00:05,759 --> 00:00:08,400
Hallo und willkommen alle im Octave

2
00:00:08,400 --> 00:00:10,860
Inference Institute. Dies ist der aktive

3
00:00:10,860 --> 00:00:15,839
Gaststream Nummer 41.1. Am 25. April 2023

4
00:00:15,839 --> 00:00:18,480
sind wir hier mit Elliot Murphy und

5
00:00:18,480 --> 00:00:20,939
Stephen Piantadosi. Dies wird

6
00:00:20,939 --> 00:00:23,039
eine ziemliche Diskussion. Wir werden mit

7
00:00:23,039 --> 00:00:24,779
Eröffnungsstatements von Stephen und

8
00:00:24,779 --> 00:00:27,359
Elliot Elliott beginnen  Dann leiten Sie mit einigen

9
00:00:27,359 --> 00:00:28,920
Fragen ein und wir werden

10
00:00:28,920 --> 00:00:32,759
am Ende eine offene Diskussion führen. Also Steven,

11
00:00:32,759 --> 00:00:34,739
danke, dass Sie sich angeschlossen haben, und zu Ihrer

12
00:00:34,739 --> 00:00:36,960
Eröffnungsrede.

13
00:00:36,960 --> 00:00:39,780
Cool, hallo, ich bin Steve Piantadosi

14
00:00:39,780 --> 00:00:42,239


15
00:00:42,239 --> 00:00:44,700


16
00:00:44,700 --> 00:00:46,860
ähm, ich schätze, ein Teil des Grundes,

17
00:00:46,860 --> 00:00:48,899
warum wir hier sind, ist, dass ich kürzlich

18
00:00:48,899 --> 00:00:51,719
eine Arbeit über große Sprachmodelle geschrieben habe, ähm,

19
00:00:51,719 --> 00:00:53,940
teilweise versuchte ich, etwas Enthusiasmus darüber zu vermitteln,

20
00:00:53,940 --> 00:00:55,199


21
00:00:55,199 --> 00:00:57,300
ähm, was sie in

22
00:00:57,300 --> 00:00:59,100
Bezug auf das Lernen von Syntax und

23
00:00:59,100 --> 00:01:00,719
Semantik

24
00:01:00,719 --> 00:01:03,059
ähm und erreicht haben  teilweise darauf hinweisend, denke ich, dass

25
00:01:03,059 --> 00:01:04,979
diese Modelle wirklich verändern, wie wir

26
00:01:04,979 --> 00:01:06,780
über Sprache denken sollten,

27
00:01:06,780 --> 00:01:08,520
wie wir über Theorien

28
00:01:08,520 --> 00:01:11,939
der sprachlichen Repräsentation denken sollten und und äh

29
00:01:11,939 --> 00:01:13,560
Theorien der Grammatik,

30
00:01:13,560 --> 00:01:17,220
ähm und wahrscheinlich auch Theorien des Lernens.

31
00:01:17,220 --> 00:01:19,080
Hallo,

32
00:01:19,080 --> 00:01:21,960
großartig, ja, also ich bin Elliot  Meffee Ich bin

33
00:01:21,960 --> 00:01:23,040
Postdoc in der Abteilung für

34
00:01:23,040 --> 00:01:25,799
Neurochirurgie am UT Health in Texas,

35
00:01:25,799 --> 00:01:27,240
ähm, ich habe Stevens Artikel mit großem

36
00:01:27,240 --> 00:01:29,759
Interesse gelesen. Ich habe viele Leute behandelt und es

37
00:01:29,759 --> 00:01:31,619
gab einige Bereiche,

38
00:01:31,619 --> 00:01:32,820
auf die ich mich heute konzentrieren möchte

39
00:01:32,820 --> 00:01:34,979
Ich antworte auf Stephen und

40
00:01:34,979 --> 00:01:36,600
probiere es mit Bereichen der

41
00:01:36,600 --> 00:01:38,640
Divergenz aus, vielleicht

42
00:01:38,640 --> 00:01:41,700
ähm, damit Sie wissen, dass Stevens Artikel

43
00:01:41,700 --> 00:01:43,619
auf der Idee basiert, dass modernes maschinelles Lernen

44
00:01:43,619 --> 00:01:46,220
den gesamten

45
00:01:46,220 --> 00:01:48,119
theoretischen Rahmen von Chomskys

46
00:01:48,119 --> 00:01:49,860
Ansatz untergraben und umgangen hat, also wollte ich

47
00:01:49,860 --> 00:01:51,479
auf einige antworten  Von diesen Hauptargumenten und einigen

48
00:01:51,479 --> 00:01:52,560
anderen verwandten Argumenten in der

49
00:01:52,560 --> 00:01:54,060
Literatur, äh, das sind einige Leute, die

50
00:01:54,060 --> 00:01:55,799
zuhören, haben vielleicht einige Einsichten

51
00:01:55,799 --> 00:01:58,500
und Gedanken dazu, also ist es eine sehr häufige

52
00:01:58,500 --> 00:02:00,720
Kritik zu sagen, dass große

53
00:02:00,720 --> 00:02:03,180
Sprachmodelle nur das nächste Token vorhersagen und

54
00:02:03,180 --> 00:02:04,079
was

55
00:02:04,079 --> 00:02:05,579
offensichtlich ein bisschen ein ist  Klischee

56
00:02:05,579 --> 00:02:07,619
richtig, es ist nicht ganz wahr und sie

57
00:02:07,619 --> 00:02:09,419
sagen nicht nur den nächsten Token voraus, sie

58
00:02:09,419 --> 00:02:11,700
scheinen auch zu konfabulieren, sie scheinen zu

59
00:02:11,700 --> 00:02:14,340
halluzinieren, sie lügen vielleicht, sie

60
00:02:14,340 --> 00:02:16,440
geben zufällig unterschiedliche Antworten auf dieselbe

61
00:02:16,440 --> 00:02:18,300
Frage, und sie scheinen

62
00:02:18,300 --> 00:02:20,220
sprachähnliche

63
00:02:20,220 --> 00:02:22,020
Strukturen stochastisch nachzuahmen  manchmal korrigieren sie

64
00:02:22,020 --> 00:02:23,819
sich selbst manchmal, wenn sie es nicht sollten,

65
00:02:23,819 --> 00:02:25,440
wenn man sie ein wenig drängt,

66
00:02:25,440 --> 00:02:27,000
ändern sie ihre Meinung manchmal, ähm,

67
00:02:27,000 --> 00:02:28,560
tatsächlich, wenn Fox News derzeit

68
00:02:28,560 --> 00:02:30,060
nach einem Ersatz für Tucker

69
00:02:30,060 --> 00:02:31,800
Carlson sucht, könnten sie weniger tun,

70
00:02:31,800 --> 00:02:33,680
als sie zwischendurch zu verwenden,

71
00:02:33,680 --> 00:02:36,120
wenn  Sie suchen nach einem

72
00:02:36,120 --> 00:02:38,040
ähnlichen Kaliber, also scheinen diese Modelle

73
00:02:38,040 --> 00:02:40,620
alle möglichen wilden Dinge zu tun,

74
00:02:40,620 --> 00:02:41,940
ähm, und in den letzten 10 Jahren

75
00:02:41,940 --> 00:02:43,500
wurde eine Reihe verschiedener, Sie wissen schon

76
00:02:43,500 --> 00:02:45,900
Systeme entwickelt, wie wir Tabakbetten sind

77
00:02:45,900 --> 00:02:47,580
und jedes von ihnen ist  basierend auf einem anderen

78
00:02:47,580 --> 00:02:49,620
neuronalen Netzansatz Aber letztendlich

79
00:02:49,620 --> 00:02:51,180
scheinen sie alle Wörter zu nehmen und

80
00:02:51,180 --> 00:02:53,220
sie durch Listen von Hunderten oder

81
00:02:53,220 --> 00:02:56,220
Tausenden von Zahlen zu charakterisieren, so dass das G23-Netzwerk

82
00:02:56,220 --> 00:03:00,000
175 Milliarden Gewichte und 96

83
00:03:00,000 --> 00:03:02,160
Aufmerksamkeitsköpfe in seiner Architektur hat und soweit

84
00:03:02,160 --> 00:03:03,780
ich weiß vielleicht  Steven kann

85
00:03:03,780 --> 00:03:06,060
mich hier korrigieren, wir haben keine wirklich gute

86
00:03:06,060 --> 00:03:07,319
Vorstellung davon, was diese verschiedenen Teile

87
00:03:07,319 --> 00:03:09,180
wirklich bedeuten, es scheint nur so zu

88
00:03:09,180 --> 00:03:10,920
funktionieren, wie Aufmerksamkeitsköpfe und

89
00:03:10,920 --> 00:03:13,680
gpt3 viel frühere

90
00:03:13,680 --> 00:03:15,360
Token in der Zeichenfolge beachten können, um zu helfen

91
00:03:15,360 --> 00:03:17,040
Sie sagen das nächste Token voraus, aber die

92
00:03:17,040 --> 00:03:18,659
gesamte Architektur von Anfang bis Ende

93
00:03:18,659 --> 00:03:21,599
ist eine Art ingenieurbasierter Motivation,

94
00:03:21,599 --> 00:03:22,500
ähm,

95
00:03:22,500 --> 00:03:24,659
und ich frage mich immer irgendwie, was mit

96
00:03:24,659 --> 00:03:27,360
all den Modellen ist, die von

97
00:03:27,360 --> 00:03:28,800
diesen llms von den verschiedenen

98
00:03:28,800 --> 00:03:30,300
Technologieunternehmen gescheitert sind, so wie es diese Unternehmen

99
00:03:30,300 --> 00:03:31,980
oft zu tun scheinen

100
00:03:31,980 --> 00:03:33,060
ähm, wissen Sie, lassen Sie es so aussehen, als hätten sie

101
00:03:33,060 --> 00:03:34,980
diese Modelle, die wirklich sehr gut funktionieren,

102
00:03:34,980 --> 00:03:36,720
direkt aus der Box, ähm,

103
00:03:36,720 --> 00:03:38,459
und sie scheinen alle nach

104
00:03:38,459 --> 00:03:40,739
einer Art berühmter Künstler benannt zu sein, richtig, richtig,

105
00:03:40,739 --> 00:03:42,780
sie haben Darley nach Salat oder Dali,

106
00:03:42,780 --> 00:03:45,360
sie haben Da Vinci, vielleicht hübsch  Bald

107
00:03:45,360 --> 00:03:46,980
wird eines dieser Unternehmen ein großes

108
00:03:46,980 --> 00:03:49,500
Sprachmodell herausbringen, äh, genannt Jesus oder

109
00:03:49,500 --> 00:03:50,879
so, ich weiß nicht,

110
00:03:50,879 --> 00:03:53,040
ähm, aber sie sagen immer, das hier ist unser

111
00:03:53,040 --> 00:03:54,900
New Foundation-Modell, es heißt Picasso,

112
00:03:54,900 --> 00:03:56,280
es ist das erste, das wir ausprobiert haben, wir sind

113
00:03:56,280 --> 00:03:58,140
einfach großartig, keine Probleme, geradeaus  Out of

114
00:03:58,140 --> 00:03:59,819
the Box, aber ich frage mich immer, was mit

115
00:03:59,819 --> 00:04:01,680
Oliver Black Boxen ist, die

116
00:04:01,680 --> 00:04:03,659
jedes Mal irgendwie versagt haben. Es scheint keine

117
00:04:03,659 --> 00:04:05,760
sehr offene und klare

118
00:04:05,760 --> 00:04:07,379
Struktur für die Art von wissenschaftlicher

119
00:04:07,379 --> 00:04:09,840
Argumentation zu geben, die hinter der Auswahl des einen

120
00:04:09,840 --> 00:04:11,939
oder anderen Modells steht  ähm, aber vielleicht bin

121
00:04:11,939 --> 00:04:14,280
ich offen dafür, diesbezüglich korrigiert zu werden, also

122
00:04:14,280 --> 00:04:16,798
machen selbst einfache Sprachmodelle

123
00:04:16,798 --> 00:04:18,298
ziemlich gut auf ähm

124
00:04:18,298 --> 00:04:20,699
wie grundlegende Webvorhersagen, also

125
00:04:20,699 --> 00:04:22,139
ist die Frage, ob diese Tools einen

126
00:04:22,139 --> 00:04:23,520
Einblick in traditionelle

127
00:04:23,520 --> 00:04:25,320
psycholinguistische Begriffe wie Grammatik

128
00:04:25,320 --> 00:04:27,300
und Passing geben  Das ist wirklich der Grund, warum ich

129
00:04:27,300 --> 00:04:29,040
das 10 Focus-Modell eher bevorzuge als das

130
00:04:29,040 --> 00:04:30,960
Sprachmodell, das von Leuten wie Cyberveras vorgeschlagen wird,

131
00:04:30,960 --> 00:04:32,880


132
00:04:32,880 --> 00:04:34,560
also wurde darauf hingewiesen, dass niemand

133
00:04:34,560 --> 00:04:36,479
wirklich glaubt, dass llms uns etwas

134
00:04:36,479 --> 00:04:38,580
Tiefgründiges über Python sagen, wenn sie

135
00:04:38,580 --> 00:04:40,080
Python-Code genauso gut landen wie natürliche

136
00:04:40,080 --> 00:04:42,120
Sprache gut  Python ist eine symbolische

137
00:04:42,120 --> 00:04:43,740
Sprache mit einer Phrasenstrukturgrammatik,

138
00:04:43,740 --> 00:04:46,620
und niemand sagt, dass llms die

139
00:04:46,620 --> 00:04:48,600
Geheimnisse von Python richtig enthüllen, also

140
00:04:48,600 --> 00:04:50,940
sagt er, um nur einige hier anzuführen, ob ein n-Modell

141
00:04:50,940 --> 00:04:52,620
als erklärende Theorien für

142
00:04:52,620 --> 00:04:54,000
natürliche Sprache auf der Grundlage ihrer

143
00:04:54,000 --> 00:04:56,040
Erfolge bei Sprachaufgaben ausgelegt werden kann  das

144
00:04:56,040 --> 00:04:57,540
Fehlen von Gegenargumenten sollten sie auch

145
00:04:57,540 --> 00:04:58,919
gute erklärende Theorien für die

146
00:04:58,919 --> 00:05:01,139
Computersprache sein, äh, daher können

147
00:05:01,139 --> 00:05:02,699
erfolgreiche Modelle der natürlichen

148
00:05:02,699 --> 00:05:04,620
Sprache nicht als Beweis

149
00:05:04,620 --> 00:05:06,540
gegen die generative Phrasenstruktur der

150
00:05:06,540 --> 00:05:07,800
Amazonensprache verwendet werden,

151
00:05:07,800 --> 00:05:09,720
so dass das Korpusmodell auch

152
00:05:09,720 --> 00:05:11,280
aus anderen Gründen ein geeigneterer Begriff ist

153
00:05:11,280 --> 00:05:13,500
Menschen  Wie Emily Bender und einige andere

154
00:05:13,500 --> 00:05:15,300
haben gezeigt, dass Merkmale des

155
00:05:15,300 --> 00:05:16,979
Trainingskorpus tatsächlich ich glaube, Stephen zitiert

156
00:05:16,979 --> 00:05:17,940
dies, Sie zitieren dies in Ihrem Artikel

157
00:05:17,940 --> 00:05:19,860
tatsächlich als Einschränkung,

158
00:05:19,860 --> 00:05:21,120
ähm, sie zeigen, dass Merkmale des

159
00:05:21,120 --> 00:05:22,680
Trainingskorpus den Legeprozess stark beeinflussen können,

160
00:05:22,680 --> 00:05:24,600
also wurde es gezeigt

161
00:05:24,600 --> 00:05:25,800
dass die Leistung großer

162
00:05:25,800 --> 00:05:27,780
Sprachmodelle im Sprachunterricht wirklich

163
00:05:27,780 --> 00:05:29,699
stark von der Vielfalt

164
00:05:29,699 --> 00:05:31,440
des Trainingskorpus beeinflusst wird,

165
00:05:31,440 --> 00:05:33,180
aber die natürliche Sprache selbst ist nicht

166
00:05:33,180 --> 00:05:35,340
voreingenommen, es ist nur

167
00:05:35,340 --> 00:05:37,199
ein Computersystem, das Menschen

168
00:05:37,199 --> 00:05:39,120
in dem, was sie sagen und wie sie handeln, voreingenommen sein können

169
00:05:39,120 --> 00:05:41,039
und aber die natürliche Sprache selbst

170
00:05:41,039 --> 00:05:43,020
ist nicht voreingenommen, so große

171
00:05:43,020 --> 00:05:45,180
Sprachmodelle, daher scheint es

172
00:05:45,180 --> 00:05:47,880
mir schwierig zu sein, dass Sie zustimmen, dass sie allen

173
00:05:47,880 --> 00:05:49,500
möglichen Vorurteilen unterliegen, sie

174
00:05:49,500 --> 00:05:51,000
können daher nicht wirklich Sprachmodelle sein,

175
00:05:51,000 --> 00:05:52,020
sie sind Modelle von etwas

176
00:05:52,020 --> 00:05:53,880
sonst, um dieses Argument irgendwie abzuschließen,

177
00:05:53,880 --> 00:05:55,620


178
00:05:55,620 --> 00:05:58,139
ähm, wissen Sie, obwohl llms

179
00:05:58,139 --> 00:05:59,759
deutlich mehr sprachlichen

180
00:05:59,759 --> 00:06:01,320
Erfahrungen bei Kindern wieder ausgesetzt sind, ist dies

181
00:06:01,320 --> 00:06:02,699
etwas anderes, was Stephen sehen kann

182
00:06:02,699 --> 00:06:04,680
und worüber er in seiner Arbeit spricht, und trotzdem

183
00:06:04,680 --> 00:06:06,360
können ihre Lernergebnisse immer noch sein

184
00:06:06,360 --> 00:06:08,160
relevant sein, wenn es darum geht, welche grammatikalischen

185
00:06:08,160 --> 00:06:09,840
Verallgemeinerungen im Prinzip lernbar sind,

186
00:06:09,840 --> 00:06:11,580
also stimme ich dieser

187
00:06:11,580 --> 00:06:12,660
Aussage hier zu, Sie wissen, dass

188
00:06:12,660 --> 00:06:14,100
sie uns im Prinzip etwas

189
00:06:14,100 --> 00:06:16,080
über Lernbarkeit sagen können, anstatt Dinge

190
00:06:16,080 --> 00:06:17,639
wie Sie allgemeine Erwerbsrahmen kennen,

191
00:06:17,639 --> 00:06:20,520
und das ist ungefähr so ​​​​viel, wie

192
00:06:20,520 --> 00:06:21,900
ich denke, dass Sie können  Vielleicht sagen wir jetzt,

193
00:06:21,900 --> 00:06:24,539
zu zeigen, dass einige induktive Vorurteile

194
00:06:24,539 --> 00:06:26,819
zum Lernen nicht notwendig sind, ist nicht

195
00:06:26,819 --> 00:06:28,139
wirklich dasselbe wie zu zeigen, dass sie

196
00:06:28,139 --> 00:06:30,060
bei Kindern nicht vorhanden sind, also

197
00:06:30,060 --> 00:06:31,560
gab es eine lange Debatte darüber, ob Sie

198
00:06:31,560 --> 00:06:32,639
negative Beweise und Anweisungen

199
00:06:32,639 --> 00:06:34,500
und Korrekturen und Rückmeldungen kennen

200
00:06:34,500 --> 00:06:36,720
Sprachenlernen ist notwendig oder sogar

201
00:06:36,720 --> 00:06:39,120
nützlich für Säuglinge und Kinder,

202
00:06:39,120 --> 00:06:40,440
ähm, aber im Moment stimme ich eher

203
00:06:40,440 --> 00:06:42,479
Eugene Choi und Gary Marcus und

204
00:06:42,479 --> 00:06:44,340
anderen zu, die hervorgehoben haben, dass LLMs

205
00:06:44,340 --> 00:06:45,660
derzeit sehr teuer zu

206
00:06:45,660 --> 00:06:46,440
trainieren sind,

207
00:06:46,440 --> 00:06:48,600
was eindeutig ein Beispiel für ein

208
00:06:48,600 --> 00:06:50,400
konzentriertes Privatleben ist  Macht in den Händen

209
00:06:50,400 --> 00:06:51,960
einiger Technologieunternehmen, ihre

210
00:06:51,960 --> 00:06:54,539
Umweltauswirkungen sind massiv,

211
00:06:54,539 --> 00:06:56,039
und Sie wissen, dass viele Menschen

212
00:06:56,039 --> 00:06:57,300


213
00:06:57,300 --> 00:06:58,979
hier weniger eingeschränkt und konservativ in ihrer Einschätzung waren,

214
00:06:58,979 --> 00:07:00,660
viel weniger als Gary

215
00:07:00,660 --> 00:07:02,880
Marcus und Eugene, also schrieb Bill Gates kürzlich,

216
00:07:02,880 --> 00:07:05,100
dass Chat GPT

217
00:07:05,100 --> 00:07:06,960
ähm ist  Die größte Tech-Entwicklung seit

218
00:07:06,960 --> 00:07:11,400
der äh grafischen Benutzeroberfläche, der GUI,

219
00:07:11,400 --> 00:07:13,020
und Henry Kissinger schrieben im Februar

220
00:07:13,020 --> 00:07:15,240
im Wall Street Journal, dass mit der

221
00:07:15,240 --> 00:07:17,400
Erweiterung der Kapazität von Chat GBT das

222
00:07:17,400 --> 00:07:19,560
menschliche Wissen neu definiert wird,

223
00:07:19,560 --> 00:07:21,780
Veränderungen in der Struktur unserer Realität beschleunigt und

224
00:07:21,780 --> 00:07:23,220
Politik und Gesellschaft neu organisiert werden

225
00:07:23,220 --> 00:07:25,620
generative KI wird veröffentlicht, um

226
00:07:25,620 --> 00:07:28,319
neue Formen des menschlichen Bewusstseins zu erzeugen, so dass

227
00:07:28,319 --> 00:07:30,419
im Moment sehr radikale Behauptungen auftauchen. Ich

228
00:07:30,419 --> 00:07:32,220
frage mich, ob manchmal

229
00:07:32,220 --> 00:07:35,099
oder der KI-Hype Sie wissen lässt, dass

230
00:07:35,099 --> 00:07:36,840
in bestimmten Teilen der Wissenschaft

231
00:07:36,840 --> 00:07:39,240
möglicherweise viele große Pläne

232
00:07:39,240 --> 00:07:40,680
geschmiedet werden, aber ich denke, Sie  Konkreter wissen,

233
00:07:40,680 --> 00:07:42,180
nur um es Steven

234
00:07:42,180 --> 00:07:44,280
hier wiederzugeben. Ich wollte vielleicht das Problem ansprechen, ähm, es

235
00:07:44,280 --> 00:07:46,919
gibt eine Kritik von Rorski und

236
00:07:46,919 --> 00:07:49,259
Beaumont, dass ich glaube, er hat ähm auf Lingbuzz gelesen,

237
00:07:49,259 --> 00:07:50,520


238
00:07:50,520 --> 00:07:51,240
ähm, ich

239
00:07:51,240 --> 00:07:53,280
glaube, Sie haben auf Twitter gesehen, dass Ihnen

240
00:07:53,280 --> 00:07:54,419
die Antwort nicht gefällt  Sie gaben an,

241
00:07:54,419 --> 00:07:56,819
weil sie den Einwand machten,

242
00:07:56,819 --> 00:07:58,500
dass Sie wissen, dass die Wissenschaft ein Beispiel für

243
00:07:58,500 --> 00:08:00,479
deduktive Logik ist. Ihr Einwand ist, dass die

244
00:08:00,479 --> 00:08:02,340
Wissenschaft nicht deduktiv, sondern induktiv ist,

245
00:08:02,340 --> 00:08:04,380
und ich denke, der allgemeine Punkt

246
00:08:04,380 --> 00:08:06,900
könnte genauer sein, nämlich dass

247
00:08:06,900 --> 00:08:08,520
Sie nicht können  Wenn Sie nicht die Tatsache verwenden, dass

248
00:08:08,520 --> 00:08:10,620
Sprachmodelle einiges an

249
00:08:10,620 --> 00:08:13,020
sprachlichem Verhalten bei Menschen und einige

250
00:08:13,020 --> 00:08:15,300
neuronale Bildgebungsreaktionen gut vorhersagen, können Sie

251
00:08:15,300 --> 00:08:17,460
das allein nicht verwenden, um zu behaupten, dass sie

252
00:08:17,460 --> 00:08:19,199
eine Theorie der menschlichen Sprache liefern können,

253
00:08:19,199 --> 00:08:21,000
also wissen Sie in Ihrer Arbeit, Stephen, dass

254
00:08:21,000 --> 00:08:23,220
es so aussieht  Bestimmte Strukturen funktionieren

255
00:08:23,220 --> 00:08:25,020
besser als andere Der richtige

256
00:08:25,020 --> 00:08:26,819
Aufmerksamkeitsmechanismus ist wichtig Vorhersage ist

257
00:08:26,819 --> 00:08:28,680
wichtig Semantische Repräsentationen sind

258
00:08:28,680 --> 00:08:30,180
wichtig und deshalb können wir

259
00:08:30,180 --> 00:08:32,219
derzeit auf der Grundlage dieser Modelle richtig

260
00:08:32,219 --> 00:08:34,260
ähm nachlesen, aber bisher ist das wirklich alles, was

261
00:08:34,260 --> 00:08:36,000
ich in der Literatur nachlesen konnte  Ich bin mir

262
00:08:36,000 --> 00:08:37,559
nicht sicher, ob Sie hier mehr Einsichten haben,

263
00:08:37,559 --> 00:08:39,899
also haben Rosky und Boomer das Beispiel der

264
00:08:39,899 --> 00:08:42,719
schlechten Vorhersage verwendet, aber die starke Erklärung, die

265
00:08:42,719 --> 00:08:44,760
richtige Erklärungskraft und nicht die

266
00:08:44,760 --> 00:08:46,500
Vorhersagegenauigkeit, bildet die Grundlage der

267
00:08:46,500 --> 00:08:48,120
modernen Wissenschaft, und ich möchte

268
00:08:48,120 --> 00:08:49,440
dies etwas später untersuchen, vielleicht

269
00:08:49,440 --> 00:08:50,820
aber moderne Sprachmodelle  können

270
00:08:50,820 --> 00:08:52,440
Teile der menschlichen Sprache genau modellieren,

271
00:08:52,440 --> 00:08:54,180
aber sie können auch sehr gut bei

272
00:08:54,180 --> 00:08:55,920
unmöglichen Sprachen und unnatürlichen

273
00:08:55,920 --> 00:08:58,200
Strukturen abschneiden, die Menschen nicht lernen können und

274
00:08:58,200 --> 00:09:00,360
große Schwierigkeiten haben, sie zu verarbeiten, und ich

275
00:09:00,360 --> 00:09:01,560
weiß, dass Sie mit diesen Kritikpunkten vertraut sind,

276
00:09:01,560 --> 00:09:03,060


277
00:09:03,060 --> 00:09:04,380
richtig, aber Sie sind es  definitiv nicht allein hier

278
00:09:04,380 --> 00:09:08,339
zur gleichen Zeit, also äh Elia

279
00:09:08,339 --> 00:09:10,320
ähm, der leitende Wissenschaftler bei Open AI,

280
00:09:10,320 --> 00:09:12,180
sagte er kürzlich in einem Interview, was es

281
00:09:12,180 --> 00:09:13,680
bedeutet, das nächste Token

282
00:09:13,680 --> 00:09:15,540
gut genug vorherzusagen, bedeutet, dass Sie

283
00:09:15,540 --> 00:09:17,940
die zugrunde liegende Realität verstehen, die zur

284
00:09:17,940 --> 00:09:19,920
Erstellung von führte  Dieses Zeichen,

285
00:09:19,920 --> 00:09:21,779
das von vielen

286
00:09:21,779 --> 00:09:23,100
konservativeren Behauptungen in der

287
00:09:23,100 --> 00:09:24,540
Literatur

288
00:09:24,540 --> 00:09:26,519
hier ziemlich abweicht, und Sie wissen auch, dass ich nur darauf antworten würde,

289
00:09:26,519 --> 00:09:27,839
dass verschiedene

290
00:09:27,839 --> 00:09:30,019
Komponenten der Wissenschaft entweder

291
00:09:30,019 --> 00:09:32,580
induktiv oder deduktiv sein können, richtig, es ist nicht

292
00:09:32,580 --> 00:09:34,140
wirklich in beiden oder Sie haben  eine bestehende

293
00:09:34,140 --> 00:09:36,300
Theorie formulieren Sie eine Hyperhypothese Sie

294
00:09:36,300 --> 00:09:38,519
sammeln Daten Sie analysieren sie und

295
00:09:38,519 --> 00:09:40,200
das ist eine Art deduktiver deduktiver

296
00:09:40,200 --> 00:09:41,880
Prozess, aber es gibt auch Fälle, in denen Sie

297
00:09:41,880 --> 00:09:43,680
mit einer bestimmten Beobachtung beginnen Sie

298
00:09:43,680 --> 00:09:44,940
finden einige Muster und Sie ziehen

299
00:09:44,940 --> 00:09:46,860
allgemeine Schlussfolgerungen richtig und dann

300
00:09:46,860 --> 00:09:49,380
gibt es eine Entführung, bei der Sie auf magische Weise

301
00:09:49,380 --> 00:09:52,019
erfinden  Hypothesen und reduzieren Sie den

302
00:09:52,019 --> 00:09:53,760
Hypothesenraum Sie würden nicht wirklich sagen,

303
00:09:53,760 --> 00:09:55,620
dass deduktives Denken unwissenschaftlich ist

304
00:09:55,620 --> 00:09:58,200
oder induktives Denken unwissenschaftlich ist

305
00:09:58,200 --> 00:10:00,360
oder abduktives Denken unwissenschaftlich ist,

306
00:10:00,360 --> 00:10:01,800
richtig, das sind alles nur verschiedene Möglichkeiten,

307
00:10:01,800 --> 00:10:03,540
Dinge zu tun

308
00:10:03,540 --> 00:10:05,459


309
00:10:05,459 --> 00:10:08,399
Modelle, um

310
00:10:08,399 --> 00:10:09,779
Hurrikane und Pandemien vorherzusagen, sind

311
00:10:09,779 --> 00:10:12,060
Beispiele für Dinge, die so streng sind, wie die

312
00:10:12,060 --> 00:10:13,860
Wissenschaft nur kann, und dann flehen Sie Ihren

313
00:10:13,860 --> 00:10:16,019
Leser an, zu dem Schluss zu kommen, dass die Situation

314
00:10:16,019 --> 00:10:18,120
bei Sprachmodellen nicht anders ist,

315
00:10:18,120 --> 00:10:19,920
ähm, aber ich denke, für mich ist das Problem, dass

316
00:10:19,920 --> 00:10:22,200
Modelle, die Hurrikane vorhersagen, dies nicht sind  Im

317
00:10:22,200 --> 00:10:23,940
Geschäft der Beantwortung der Frage,

318
00:10:23,940 --> 00:10:25,740
was der Hurrikan ist,

319
00:10:25,740 --> 00:10:27,420
richtige Modelle, die das Wetter genau vorhersagen,

320
00:10:27,420 --> 00:10:29,040
sind sehr genau, aber

321
00:10:29,040 --> 00:10:30,600
Sie wissen nicht, dass sie mit der Meteorologieabteilung abgestimmt sind,

322
00:10:30,600 --> 00:10:32,700
aber sie sind kein

323
00:10:32,700 --> 00:10:34,380
Ersatz dafür,

324
00:10:34,380 --> 00:10:35,760
ähm, also denke ich.  Ich weiß nur, dass ich

325
00:10:35,760 --> 00:10:37,620
dir das übergebe.

326
00:10:37,620 --> 00:10:41,820
Ja, okay, äh, da gibt es eine Menge,

327
00:10:41,820 --> 00:10:43,800
ähm, ich denke, ich könnte einfach damit beginnen, dass ich

328
00:10:43,800 --> 00:10:45,240
sagen würde,

329
00:10:45,240 --> 00:10:47,579
ähm, ich stimme vielen dieser

330
00:10:47,579 --> 00:10:51,300
Kritiken zu, dass diese

331
00:10:51,300 --> 00:10:54,000
Modelle von ähm kontrolliert werden

332
00:10:54,000 --> 00:10:56,220
Ein oder zwei Unternehmen, ähm,

333
00:10:56,220 --> 00:10:59,160
das ist sehr, sehr problematisch,

334
00:10:59,160 --> 00:11:01,320
ähm, Sie wissen, dass sie alle Arten von

335
00:11:01,320 --> 00:11:03,480
Vorurteilen haben, die sie sich angeeignet haben, weil

336
00:11:03,480 --> 00:11:04,980
sie mit Texten aus dem Internet trainiert sind,

337
00:11:04,980 --> 00:11:06,180


338
00:11:06,180 --> 00:11:08,640
ähm, das ist enorm problematisch, ähm,

339
00:11:08,640 --> 00:11:10,980
wissen Sie, ich stimme sicherlich zu, dass

340
00:11:10,980 --> 00:11:13,079
es äh-Dinge gibt  Zumindest im Moment,

341
00:11:13,079 --> 00:11:16,500
dass die Modelle nicht gut abschneiden, also ähm,

342
00:11:16,500 --> 00:11:18,420
ich denke, es ist einfach,

343
00:11:18,420 --> 00:11:20,640
Beispiele von Ihnen bekannten Fragen und

344
00:11:20,640 --> 00:11:23,100
Problemen zu finden, die

345
00:11:23,100 --> 00:11:25,500


346
00:11:25,500 --> 00:11:26,760
sie zum

347
00:11:26,760 --> 00:11:29,820
Stolpern bringen  ähm, nicht unbedingt in diesen

348
00:11:29,820 --> 00:11:32,399
Begriffen richtig, aber in Bezug auf die

349
00:11:32,399 --> 00:11:34,920
Leistung in Bezug auf Sprache,

350
00:11:34,920 --> 00:11:38,459
insbesondere Syntax und Semantik,

351
00:11:38,459 --> 00:11:40,980
ähm, ich denke, sie sind weit über

352
00:11:40,980 --> 00:11:43,019
jede andere Theorie in jedem anderen

353
00:11:43,019 --> 00:11:46,920
Bereich hinaus, also gibt es keine andere

354
00:11:46,920 --> 00:11:49,380
Theorie  Linguistik oder

355
00:11:49,380 --> 00:11:53,100
Informatik, die können Sie lange

356
00:11:53,100 --> 00:11:56,700
zusammenhängende äh grammatikalische äh Passagen von

357
00:11:56,700 --> 00:11:58,500
Texten

358
00:11:58,500 --> 00:12:01,140
äh und so irgendwie alle ihre

359
00:12:01,140 --> 00:12:04,920
Probleme zugeben, wie äh Sie kennen Tools oder oder

360
00:12:04,920 --> 00:12:08,220
Dinge, die von Unternehmen eingesetzt werden

361
00:12:08,220 --> 00:12:09,959


362
00:12:09,959 --> 00:12:12,899
wie gehen sie mit

363
00:12:12,899 --> 00:12:14,760
Sprache um und ähm,

364
00:12:14,760 --> 00:12:16,320
ich denke, daher kommt viel Enthusiasmus. Es

365
00:12:16,320 --> 00:12:17,760


366
00:12:17,760 --> 00:12:20,160
gab wirklich nichts, ähm, auch nur annähernd

367
00:12:20,160 --> 00:12:23,700
wie sie in Bezug auf sprachliche Fähigkeiten,

368
00:12:23,700 --> 00:12:24,899
ähm, und das ist es, was ich

369
00:12:24,899 --> 00:12:27,660
denke  ist aufregend, also ja, ich stimme

370
00:12:27,660 --> 00:12:29,220
einigen dieser Dinge zu,

371
00:12:29,220 --> 00:12:31,320
mit denen Sie angefangen haben,

372
00:12:31,320 --> 00:12:33,300
ähm, aber nichtsdestotrotz denke ich, dass es in

373
00:12:33,300 --> 00:12:35,100
Bezug auf Syntax und Semantik

374
00:12:35,100 --> 00:12:37,079
einfach keine andere Theorie gibt, die

375
00:12:37,079 --> 00:12:39,240
mit ihnen vergleichbar ist, ähm,

376
00:12:39,240 --> 00:12:40,140


377
00:12:40,140 --> 00:12:42,120
aber lassen Sie mich drängen  Das war damals

378
00:12:42,120 --> 00:12:44,940
richtig, also ja, ich würde den

379
00:12:44,940 --> 00:12:46,320
Haupteinwand von vielen Leuten,

380
00:12:46,320 --> 00:12:48,000
mit denen ich in den Abteilungen für Linguistik gesprochen habe,

381
00:12:48,000 --> 00:12:50,760
die wie viele der Allgemein sind, die Sie

382
00:12:50,760 --> 00:12:53,100
kennen, zuerst in Ihrer Arbeit

383
00:12:53,100 --> 00:12:54,120
sagen: „

384
00:12:54,120 --> 00:12:55,620
Nun, du kennst dich.  Richtig, sie machen

385
00:12:55,620 --> 00:12:57,360
einen wunderbaren Job, ähm, sie modellieren

386
00:12:57,360 --> 00:12:58,920
alle Aspekte von vielen Aspekten der

387
00:12:58,920 --> 00:13:01,380
Syntax und Semantik genau, aber ähm,

388
00:13:01,380 --> 00:13:03,300
ich kenne keine wirklichen, so wie

389
00:13:03,300 --> 00:13:04,920
Sie wissen, dass Chomsky über Fakten über

390
00:13:04,920 --> 00:13:06,660
Sprache spricht, was eine altmodische

391
00:13:06,660 --> 00:13:09,000
Vorstellung ist  und aber ich denke wirklich, dass das

392
00:13:09,000 --> 00:13:10,260
auch ein wichtiger Gedanke ist,

393
00:13:10,260 --> 00:13:12,899
wie gibt es eine Entdeckung über die

394
00:13:12,899 --> 00:13:16,860
Sprache selbst, die llms auf einzigartige Weise

395
00:13:16,860 --> 00:13:19,200
liefern kann, so wie wenn llms eine

396
00:13:19,200 --> 00:13:21,899
Vorhersage darüber gemacht hat, sagen wir, Sie haben eine

397
00:13:21,899 --> 00:13:24,600
Satzstruktur Typ X, die

398
00:13:24,600 --> 00:13:26,279
schwieriger zu verarbeiten ist als Satz  Typ

399
00:13:26,279 --> 00:13:28,620
Y und dies ist eine einzigartige Vorhersage, dass

400
00:13:28,620 --> 00:13:31,200
nur sie es erzeugen würden und kein menschlicher

401
00:13:31,200 --> 00:13:33,899
Linguist Chomsky Homestein, keiner dieser

402
00:13:33,899 --> 00:13:34,980
Leute hatte das jemals vorhergesagt,

403
00:13:34,980 --> 00:13:37,260
aber es stellt sich als wahr heraus, dass Sie Eye-

404
00:13:37,260 --> 00:13:38,760
Tracking-Experimente machen, Sie machen alle möglichen

405
00:13:38,760 --> 00:13:40,800
unterschiedlichen Verhaltenserfahrungen  und ach

406
00:13:40,800 --> 00:13:42,480
weißt du, es stellt sich schließlich als

407
00:13:42,480 --> 00:13:44,279
wahr heraus das ist eine neue Einsicht über die

408
00:13:44,279 --> 00:13:45,839
Sprachverarbeitung es ist eine neue Einsicht

409
00:13:45,839 --> 00:13:47,940
über die Sprache du weißt schon Verhalten Ich frage mich nur Ich sage

410
00:13:47,940 --> 00:13:49,560
nicht Ich sage nicht,

411
00:13:49,560 --> 00:13:51,240
dass dies im Prinzip nicht möglich ist

412
00:13:51,240 --> 00:13:52,620
weil es in naher Zukunft passieren könnte,

413
00:13:52,620 --> 00:13:54,540
aber das ist meiner Meinung nach der springende

414
00:13:54,540 --> 00:13:57,240
Punkt, warum viele Linguisten davon sprechen,

415
00:13:57,240 --> 00:13:59,519


416
00:13:59,519 --> 00:14:02,040
hier im Namen der gesamten Linguistik-Community zu sprechen, und Sie wissen,

417
00:14:02,040 --> 00:14:03,180
ich denke, das wäre einer der Haupteinwände,

418
00:14:03,180 --> 00:14:04,800


419
00:14:04,800 --> 00:14:08,279
ja, ich meine, ich  Ich weiß nicht von äh Ich

420
00:14:08,279 --> 00:14:10,019
schätze, ich denke an die Einsichten, die sie

421
00:14:10,019 --> 00:14:12,420
geliefert haben, als eine Art allgemeine Prinzipien,

422
00:14:12,420 --> 00:14:14,459
also ähm,

423
00:14:14,459 --> 00:14:16,200
ich denke über diese Dinge wie

424
00:14:16,200 --> 00:14:18,720
die Kraft, sich

425
00:14:18,720 --> 00:14:20,820
Sprachbrocken so gut zu merken, wie sie

426
00:14:20,820 --> 00:14:22,500
scheinen  zum Beispiel sehr gut in Konstruktionen zu sein,

427
00:14:22,500 --> 00:14:24,540
und es gibt viele

428
00:14:24,540 --> 00:14:26,160
linguistische Theorien,

429
00:14:26,160 --> 00:14:28,740
insbesondere von Chomsky, bei denen es darum geht, eine

430
00:14:28,740 --> 00:14:30,899
Art minimale Menge

431
00:14:30,899 --> 00:14:33,660
an Struktur zu finden, die man sich merken kann, und zu versuchen,

432
00:14:33,660 --> 00:14:36,480
so viel wie möglich aus

433
00:14:36,480 --> 00:14:38,279
ähm, einigen kleinen Mengen abzuleiten  Sammlung

434
00:14:38,279 --> 00:14:40,440
von Operationen

435
00:14:40,440 --> 00:14:42,300
ähm und ich denke, das ist für diese Theorien nicht gut gelaufen,

436
00:14:42,300 --> 00:14:44,459
ähm, während das hier

437
00:14:44,459 --> 00:14:46,740
wirklich gut läuft, also ähm,

438
00:14:46,740 --> 00:14:48,360
wenn wir an etwas denken, das

439
00:14:48,360 --> 00:14:50,040
die Fähigkeit zum Auswendiglernen hat, wenn wir

440
00:14:50,040 --> 00:14:51,300
zum Beispiel an Grammatiktheorien denken,

441
00:14:51,300 --> 00:14:54,240
die äh bauen  On

442
00:14:54,240 --> 00:14:56,040
ähm, Sie wissen, dass Menschen eine wirklich

443
00:14:56,040 --> 00:14:58,139
bemerkenswerte Fähigkeit haben, sich

444
00:14:58,139 --> 00:14:59,880
verschiedene Konstruktionen richtig oder

445
00:14:59,880 --> 00:15:01,380
verschiedene Wörter zu merken. Wir kennen

446
00:15:01,380 --> 00:15:02,940
Zehntausende von Wörtern. Zehntausende von

447
00:15:02,940 --> 00:15:04,380
verschiedenen Konstruktionen. Entschuldigung, Zehntausende von

448
00:15:04,380 --> 00:15:06,420
verschiedenen Redewendungen  Sie sind

449
00:15:06,420 --> 00:15:07,920


450
00:15:07,920 --> 00:15:10,260
in gewissem Sinne eine

451
00:15:10,260 --> 00:15:12,480
Art Beweis des Prinzips, dass diese

452
00:15:12,480 --> 00:15:15,660
Art von Ansatz gut funktionieren kann,

453
00:15:15,660 --> 00:15:17,279
ähm kann darüber nachdenken, andere Arten von

454
00:15:17,279 --> 00:15:19,380
Vorhersagen mit ihnen zu machen, ähm, einige davon, die

455
00:15:19,380 --> 00:15:21,779
die Leute derzeit machen, aber

456
00:15:21,779 --> 00:15:23,339
zum Beispiel versuchen, sie zum Messen zu verwenden

457
00:15:23,339 --> 00:15:25,860
äh Verarbeitungsschwierigkeiten messen

458
00:15:25,860 --> 00:15:27,899
zum Beispiel Überraschungen aus diesen

459
00:15:27,899 --> 00:15:29,160
Modellen

460
00:15:29,160 --> 00:15:30,600
ähm, es gibt überraschende Maßnahmen, die

461
00:15:30,600 --> 00:15:31,699


462
00:15:31,699 --> 00:15:34,260
viel besser sind als sagen wir kontextfreie

463
00:15:34,260 --> 00:15:36,120
Grammatiken oder andere Arten von

464
00:15:36,120 --> 00:15:37,440
Sprachmodellen, und dann ist es eine interessante

465
00:15:37,440 --> 00:15:39,899
Frage, wie sich diese äh Überraschungen oder

466
00:15:39,899 --> 00:15:41,940
Vorhersagbarkeiten auf die menschliche

467
00:15:41,940 --> 00:15:44,399
Verarbeitung beziehen, richtig und es  kann etwas

468
00:15:44,399 --> 00:15:46,620
davon erfassen oder nichtlinear sein oder es könnte,

469
00:15:46,620 --> 00:15:49,079
wissen Sie, nur ein wenig davon erfassen

470
00:15:49,079 --> 00:15:51,420
oder oder was auch immer, das ist eine interessante

471
00:15:51,420 --> 00:15:53,940
Art von anderer wissenschaftlicher Frage, aber ich

472
00:15:53,940 --> 00:15:55,500
denke im Prinzip richtig, dass sie

473
00:15:55,500 --> 00:15:57,779
Vorhersagen über zum Beispiel die

474
00:15:57,779 --> 00:15:59,880
Verbindungen machen können  zwischen den Sätzen richtig, also

475
00:15:59,880 --> 00:16:02,279
in der Arbeit habe ich dieses Beispiel gegeben, wie

476
00:16:02,279 --> 00:16:05,459
Sie wissen, wie man eine Deklaration

477
00:16:05,459 --> 00:16:07,920
auf 10 verschiedene Arten richtig in eine Frage umwandelt, und

478
00:16:07,920 --> 00:16:10,620
vermutlich, wenn Sie wissen, dass GPT oder

479
00:16:10,620 --> 00:16:12,540
etwas tut, dass es

480
00:16:12,540 --> 00:16:15,420
10 verschiedene Fragen findet, die alle äh

481
00:16:15,420 --> 00:16:18,060
in sind  Irgendwie verwandt in der Nähe in

482
00:16:18,060 --> 00:16:20,339
den Modellen, die dem semantischen oder syntaktischen Raum zugrunde liegen,

483
00:16:20,339 --> 00:16:22,139


484
00:16:22,139 --> 00:16:24,660
ähm und so sind diese Dinge

485
00:16:24,660 --> 00:16:26,459
von der Art, von der ich denke,

486
00:16:26,459 --> 00:16:28,560
ähm, Sie wissen, dass einige Linguisten Recht haben möchten,

487
00:16:28,560 --> 00:16:30,180
was hier eine versteckte

488
00:16:30,180 --> 00:16:32,220
Verbindung zwischen Sätzen oder ihrem oder

489
00:16:32,220 --> 00:16:34,320
ihrem ist  Strukturen, aber soweit ich weiß,

490
00:16:34,320 --> 00:16:36,120
wurden sie noch nicht empirisch evaluiert,

491
00:16:36,120 --> 00:16:39,660
also richtig, ja ja, ich meine, diese

492
00:16:39,660 --> 00:16:41,220
Art von Modellen ist erst ein paar Jahre alt,

493
00:16:41,220 --> 00:16:43,440
also denke ich, dass es

494
00:16:43,440 --> 00:16:45,000
vernünftig ist, sich für sie zu begeistern,

495
00:16:45,000 --> 00:16:46,259
obwohl diese Art von Arbeit nicht

496
00:16:46,259 --> 00:16:48,540
existiert  noch nicht gemacht nein das ist richtig nein total

497
00:16:48,540 --> 00:16:50,880
total ich meine aber ich denke ich

498
00:16:50,880 --> 00:16:51,720
denke das ist die richtige

499
00:16:51,720 --> 00:16:52,920
Perspektive aber ich denke das

500
00:16:52,920 --> 00:16:54,779
kommt auf das Thema an das

501
00:16:54,779 --> 00:16:56,579
du erwähnt hast Surprisal du hast Laneability erwähnt

502
00:16:56,579 --> 00:16:58,139


503
00:16:58,139 --> 00:17:00,839
ähm du kennst LMS und etwas Syntax aber  Sie

504
00:17:00,839 --> 00:17:03,060
tun dies mit offensichtlich viel mehr Daten

505
00:17:03,060 --> 00:17:04,439
als Säuglinge,

506
00:17:04,439 --> 00:17:06,240
ähm, so dass Beobachtungen der potenziellen

507
00:17:06,240 --> 00:17:09,000
Struktur an und für sich kein

508
00:17:09,000 --> 00:17:10,799
Ruf der Armut des

509
00:17:10,799 --> 00:17:12,540
Stimulus sind, sondern die schwächere Version, die ich

510
00:17:12,540 --> 00:17:13,500
sagen sollte, der Armut der

511
00:17:13,500 --> 00:17:15,059
herausragenden Argumente, also die bloße Tatsache

512
00:17:15,059 --> 00:17:17,579
Dass LMS mit unserem grammatikalischen Preis tun können, was sie tun,

513
00:17:17,579 --> 00:17:19,559
ist sehr bemerkenswert. Ich

514
00:17:19,559 --> 00:17:20,939
stimme zu, und tatsächlich hätten Sie

515
00:17:20,939 --> 00:17:22,500
das vor fünf oder sechs oder

516
00:17:22,500 --> 00:17:23,699
sieben Jahren nicht vorhergesagt,

517
00:17:23,699 --> 00:17:25,260
aber es entkräftet nicht die

518
00:17:25,260 --> 00:17:28,199
Behauptung, dass Menschen Überraschung haben und wir

519
00:17:28,199 --> 00:17:30,120
Bringen Sie diese Gebete mit und

520
00:17:30,120 --> 00:17:31,080
um zu sehen, ob

521
00:17:31,080 --> 00:17:33,299
Computerlinguistik Hypothesen und

522
00:17:33,299 --> 00:17:34,740
theoretische Linguistik einschränken kann, was meiner Meinung nach

523
00:17:34,740 --> 00:17:36,480
möglich ist, muss dies

524
00:17:36,480 --> 00:17:38,039
mit sorgfältigen Experimenten geschehen, bei

525
00:17:38,039 --> 00:17:39,720
denen verschiedene Lernparameter

526
00:17:39,720 --> 00:17:43,080
kontrolliert werden, und mit gigantischen Sprachmodellen

527
00:17:43,080 --> 00:17:45,299
wie gbt free sind hier im Grunde

528
00:17:45,299 --> 00:17:47,520
nutzlos, und das hier kommt zu

529
00:17:47,520 --> 00:17:49,559
einigen Kachellinsen und Beschwerden, dass wir so

530
00:17:49,559 --> 00:17:51,960
etwas wie ein Baby-LM-Projekt brauchen,

531
00:17:51,960 --> 00:17:53,700
an dem ich weiß, dass Sie daran interessiert sind und

532
00:17:53,700 --> 00:17:55,320
wo wir mehr ökologisch

533
00:17:55,320 --> 00:17:56,940
gültige Trainingssets für Sie haben  Machen Sie

534
00:17:56,940 --> 00:17:58,380
in Ihrem Aufsatz die Vorhersage, dass eine

535
00:17:58,380 --> 00:17:59,640
Struktur gelernt wird, von der ich

536
00:17:59,640 --> 00:18:01,679
vermute, dass Sie genau dort liegen könnten,

537
00:18:01,679 --> 00:18:03,059
aber Sie wissen, dass es selbst bei der

538
00:18:03,059 --> 00:18:04,679
Baby-LM-Herausforderung immer noch die Art

539
00:18:04,679 --> 00:18:07,260
von nicht trivialem Problem gibt,

540
00:18:07,260 --> 00:18:09,419
traditionellere Probleme wie wann anzugehen  Die Kinder

541
00:18:09,419 --> 00:18:11,400
beginnen zu verallgemeinern, basierend auf der Menge

542
00:18:11,400 --> 00:18:13,200
des aktuellen Inputs, basierend auf verschiedenen

543
00:18:13,200 --> 00:18:15,539
sprachübergreifenden Faktoren, und das

544
00:18:15,539 --> 00:18:17,700
erfordert nur traditionelle Kenntnisse in

545
00:18:17,700 --> 00:18:18,720
Psycholinguistik und

546
00:18:18,720 --> 00:18:21,539
Spracherwerb, also wissen Sie, dass LMS sich

547
00:18:21,539 --> 00:18:22,919
um Dinge wie Häufigkeit und

548
00:18:22,919 --> 00:18:24,600
Überraschung kümmert, wie Sie sagten, aber es gibt eine

549
00:18:24,600 --> 00:18:26,160
wirklich nette  Papier von Sophie Sluts und

550
00:18:26,160 --> 00:18:27,600
Andrea Martin, das wirklich schöne Papier,

551
00:18:27,600 --> 00:18:30,000
das Sie vielleicht gesehen haben, das

552
00:18:30,000 --> 00:18:31,620
sehr schön zeigt, dass

553
00:18:31,620 --> 00:18:34,080
Verteilungsstatistiken manchmal ein Hinweis auf

554
00:18:34,080 --> 00:18:36,240
Momente des Strukturaufbaus sein können, aber es

555
00:18:36,240 --> 00:18:37,919
ersetzt diese Begriffe in Bezug auf die

556
00:18:37,919 --> 00:18:39,660
Zusammensetzung, also werde ich es nur tun  Lesen Sie ein Zitat

557
00:18:39,660 --> 00:18:42,360
aus Chomsky 57, das sehr

558
00:18:42,360 --> 00:18:45,240
nach Slots und mehr klingt, und sagen Sie, dass sie trotz

559
00:18:45,240 --> 00:18:47,820
unbestreitbarer Interessen und Bedeutung

560
00:18:47,820 --> 00:18:49,440
von semantischen und statistischen

561
00:18:49,440 --> 00:18:51,360
Sprachmodellen keine direkte

562
00:18:51,360 --> 00:18:52,919
Relevanz für das Problem der Bestimmung

563
00:18:52,919 --> 00:18:54,600
oder Charakterisierung der Menge grammatikalischer

564
00:18:54,600 --> 00:18:56,340
Unterschiede zu haben scheinen  Ich denke, dass wir

565
00:18:56,340 --> 00:18:57,840
zu dem Schluss gezwungen sind, dass Grammatik autonom

566
00:18:57,840 --> 00:18:59,520
und bedeutungsunabhängig ist und dass

567
00:18:59,520 --> 00:19:01,320
probabilistische Modelle keinen besonderen

568
00:19:01,320 --> 00:19:03,179
Einblick in einige der grundlegenden Probleme

569
00:19:03,179 --> 00:19:05,880
der syntaktischen Struktur geben, so dass sich herausstellte, dass der zweite Satz der

570
00:19:05,880 --> 00:19:07,980
des zweiten

571
00:19:07,980 --> 00:19:10,380
Satzes war  falsch, aber

572
00:19:10,380 --> 00:19:11,580
es ist wahr, dass Sie wissen, was

573
00:19:11,580 --> 00:19:13,740
offensichtlich gesagt wird, dass verfügbare Statistikmodelle

574
00:19:13,740 --> 00:19:16,559
in 57 nicht mehr genau sind, wenn sie

575
00:19:16,559 --> 00:19:18,600
auf heutige Modelle angewendet werden, und das kann

576
00:19:18,600 --> 00:19:20,340
abstrakte Verallgemeinerungen über neuartige

577
00:19:20,340 --> 00:19:21,900
Zeichenfolgen und Verteilungskategorien machen, wie

578
00:19:21,900 --> 00:19:23,640
Sie richtig erwähnt haben, aber die Leistung

579
00:19:23,640 --> 00:19:25,380
eines einzelnen Modells  liefert keine

580
00:19:25,380 --> 00:19:27,480
direkten Beweise für oder gegen die

581
00:19:27,480 --> 00:19:29,760
Landbarkeit einer bestimmten Struktur, indem

582
00:19:29,760 --> 00:19:31,440
der große Abstand zwischen jedem

583
00:19:31,440 --> 00:19:33,900
heute verfügbaren Computermodell und

584
00:19:33,900 --> 00:19:36,240
dem menschlichen Gehirnmodell angegeben wird Erfolg

585
00:19:36,240 --> 00:19:38,100
bedeutet nicht, dass die Struktur notwendigerweise

586
00:19:38,100 --> 00:19:40,679
Land ist, und Modellversagen bedeutet auch nicht,

587
00:19:40,679 --> 00:19:42,539
dass dies der Fall ist  Struktur ist nicht erlernbar, ja

588
00:19:42,539 --> 00:19:44,520


589
00:19:44,520 --> 00:19:47,100
ja, also ich meine, ich denke, es

590
00:19:47,100 --> 00:19:49,380
lohnt sich vielleicht, ein paar

591
00:19:49,380 --> 00:19:51,000
verschiedene Versionen von

592
00:19:51,000 --> 00:19:52,620
Erlernbarkeitsargumenten zu entpacken, die Leute vorgebracht haben, weil es

593
00:19:52,620 --> 00:19:55,679
äh, sehr, sehr starke Art

594
00:19:55,679 --> 00:19:57,720
von Unmöglichkeitsansprüchen gegeben hat, die

595
00:19:57,720 --> 00:19:59,160
aus Art von Chomskys

596
00:19:59,160 --> 00:20:01,260
Tradition kommen, richtig  dass wir nie

597
00:20:01,260 --> 00:20:04,140
Behauptungen über die Datenmenge haben, die

598
00:20:04,140 --> 00:20:05,820
erforderlich war, genau da gab es Behauptungen über

599
00:20:05,820 --> 00:20:08,220
das logische Problem des Sprachenlernens

600
00:20:08,220 --> 00:20:10,740
und dass es einfach unmöglich war, ähm, richtig,

601
00:20:10,740 --> 00:20:12,120
es war unmöglich, ohne äh äh

602
00:20:12,120 --> 00:20:15,720
äh Art von erheblichen

603
00:20:15,720 --> 00:20:17,580
Einschränkungen zu haben  die Klasse der

604
00:20:17,580 --> 00:20:19,380
Sprachen oder die Klasse der Grammatik,

605
00:20:19,380 --> 00:20:21,660
die Sie sich aneignen würden,

606
00:20:21,660 --> 00:20:23,220
und die Leute haben lange Zeit

607
00:20:23,220 --> 00:20:25,380
dagegen gestritten, dass diese Version der

608
00:20:25,380 --> 00:20:26,340
Dinge,

609
00:20:26,340 --> 00:20:27,120
ähm,

610
00:20:27,120 --> 00:20:28,860
Sie wissen, es gibt alte Arbeiten

611
00:20:28,860 --> 00:20:30,419
von Gold und dann eine ganze Art von

612
00:20:30,419 --> 00:20:32,580
Grammatik  Erwerbstheorien, die

613
00:20:32,580 --> 00:20:35,220
auf dieser Tradition aufbauen, ähm, die sich viel Gedanken

614
00:20:35,220 --> 00:20:37,980
über die Art von äh-Reihenfolge machen, in

615
00:20:37,980 --> 00:20:39,240
der Sie verschiedene

616
00:20:39,240 --> 00:20:40,860
Hypothesen durchgehen und verschiedene

617
00:20:40,860 --> 00:20:42,600
Optionen und Dinge in Betracht ziehen, ähm,

618
00:20:42,600 --> 00:20:43,380


619
00:20:43,380 --> 00:20:45,059
und meine Lieblingsreferenz darin

620
00:20:45,059 --> 00:20:46,919
ist dieses Papier von

621
00:20:46,919 --> 00:20:49,799
ähm Nick Jader und  Paulus Veterinum nannte so

622
00:20:49,799 --> 00:20:51,539
etwas wie ideales Lernen

623
00:20:51,539 --> 00:20:53,039
natürlicher Sprache,

624
00:20:53,039 --> 00:20:54,840
was im Grunde zeigt, dass ein

625
00:20:54,840 --> 00:20:57,539
unbeschränkter Lernender mit

626
00:20:57,539 --> 00:21:00,480
genügend Daten die Art

627
00:21:00,480 --> 00:21:01,860
der Generierungsregeln oder die

628
00:21:01,860 --> 00:21:03,360
Generierungsgrammatik

629
00:21:03,360 --> 00:21:05,640
nur durch das richtige Beobachten von Zeichenfolgen erwerben könnte, aber

630
00:21:05,640 --> 00:21:08,160
das war dieses Papier  Das war wirklich eine

631
00:21:08,160 --> 00:21:10,200
Reaktion auf diese riesige Menge an Arbeit,

632
00:21:10,200 --> 00:21:13,140
die argumentierte, dass das Lernen

633
00:21:13,140 --> 00:21:15,120
aus positiven Beispielen, also durch das bloße

634
00:21:15,120 --> 00:21:17,820
Beobachten von Saiten, logisch unmöglich sei, also ähm, natürlich

635
00:21:17,820 --> 00:21:20,520


636
00:21:20,520 --> 00:21:23,460
wissen Sie, dass die Leute in

637
00:21:23,460 --> 00:21:25,679
Chomskys Tradition diese Form wirklich mochten

638
00:21:25,679 --> 00:21:28,140
Argument, weil es eines war, das

639
00:21:28,140 --> 00:21:30,419
sagte, äh, Sie müssten

640
00:21:30,419 --> 00:21:33,299


641
00:21:33,299 --> 00:21:34,919


642
00:21:34,919 --> 00:21:36,600


643
00:21:36,600 --> 00:21:39,480
etwas von

644
00:21:39,480 --> 00:21:41,220
Natur aus äh spezifiziert haben, damit der Spracherwerb funktioniert

645
00:21:41,220 --> 00:21:42,900
Irgendetwas und all das hat sich

646
00:21:42,900 --> 00:21:45,659
einfach als völlig falsch herausgestellt, also ähm,

647
00:21:45,659 --> 00:21:48,299
wenn Sie, ähm, wissen Sie, gehen Sie zu

648
00:21:48,299 --> 00:21:50,520
etwas realistischeren Lernumgebungen über,

649
00:21:50,520 --> 00:21:53,340
die Tater und Vatani tun,

650
00:21:53,340 --> 00:21:55,500
ähm, dann stellt sich heraus, dass Sie wie ein

651
00:21:55,500 --> 00:21:57,240
idealisierter Lernender Dinge lernen können, und es

652
00:21:57,240 --> 00:21:59,100
gibt keine  Aussagen über

653
00:21:59,100 --> 00:22:00,600
die Menge an Daten, die

654
00:22:00,600 --> 00:22:02,700
selbst dort erforderlich sind, das ist die Art von

655
00:22:02,700 --> 00:22:06,780
rein logischer Fähigkeit zu

656
00:22:06,780 --> 00:22:08,940
lernen, und diese Fähigkeit ist es, was meiner Meinung nach

657
00:22:08,940 --> 00:22:11,460
die großen Versionen großer

658
00:22:11,460 --> 00:22:13,679
Sprachmodelle auch richtig ansprechen, also Chader

659
00:22:13,679 --> 00:22:15,600
invitani und andere  Arbeite irgendwie darin, dass

660
00:22:15,600 --> 00:22:17,460
dieser Geist

661
00:22:17,460 --> 00:22:19,500
ähm ist, du weißt mathematisch und

662
00:22:19,500 --> 00:22:21,480
prinzipiell argumentieren, aber nie

663
00:22:21,480 --> 00:22:24,720
etwas geschaffen, das wirklich

664
00:22:24,720 --> 00:22:27,840
ein Grammatikrecht oder oder eine wirkliche Art von

665
00:22:27,840 --> 00:22:30,000
implementiertem Sprachmodell

666
00:22:30,000 --> 00:22:32,640
ähm war, also kennst du sogar ein Modell, das

667
00:22:32,640 --> 00:22:35,400
trainiert ist  auf 100 Millionen oder 100 Milliarden oder

668
00:22:35,400 --> 00:22:38,340
wie viele viele Token richtig, äh,

669
00:22:38,340 --> 00:22:41,159
sogar diese Art von Modell, denke ich, ist

670
00:22:41,159 --> 00:22:43,080
relevant für diese Version der

671
00:22:43,080 --> 00:22:46,140
Debatte, richtig, und und und und zu zeigen, dass äh,

672
00:22:46,140 --> 00:22:48,659
Sprachenlernen nicht unmöglich ist, äh,

673
00:22:48,659 --> 00:22:51,360
aus einem sehr uneingeschränkten Raum, okay

674
00:22:51,360 --> 00:22:53,640
und dann gibt es eine zweite Version

675
00:22:53,640 --> 00:22:56,760
richtig, die lautet: Können wir Sprache

676
00:22:56,760 --> 00:22:59,760
mit den spezifischen Daten lernen, die Kinder

677
00:22:59,760 --> 00:23:02,340
richtig verstehen, und das ist sowohl die Datenmenge als

678
00:23:02,340 --> 00:23:04,620
auch die Form der Daten,

679
00:23:04,620 --> 00:23:06,360
ähm und so für Leute, die

680
00:23:06,360 --> 00:23:08,520
die Baby-LM-Herausforderung nicht kennen  äh äh

681
00:23:08,520 --> 00:23:10,440
ist äh äh

682
00:23:10,440 --> 00:23:12,840
das äh äh tut mir

683
00:23:12,840 --> 00:23:14,340
leid, es einen

684
00:23:14,340 --> 00:23:16,620
Wettbewerb oder äh äh äh zu nennen,

685
00:23:16,620 --> 00:23:20,580
ich schätze, es ist eine Herausforderung, ähm zu

686
00:23:20,580 --> 00:23:22,380
versuchen, Leute dazu zu bringen,

687
00:23:22,380 --> 00:23:24,539
Sprachmodelle mit menschengroßen

688
00:23:24,539 --> 00:23:27,179
Datenmengen zu trainieren, ähm, das ist also eher so etwas wie

689
00:23:27,179 --> 00:23:28,679
ähm, ich denke, es gibt zwei verschiedene

690
00:23:28,679 --> 00:23:31,140
Versionen, 10 oder 100 Millionen verschiedene

691
00:23:31,140 --> 00:23:33,120
ähm bis 10 oder 100 Millionen verschiedene Wörter

692
00:23:33,120 --> 00:23:35,640
im Trainingsset

693
00:23:35,640 --> 00:23:38,100
ähm, das ist, wie Sie wissen, 100. oder

694
00:23:38,100 --> 00:23:41,159
1000. oder etwas so Großes wie

695
00:23:41,159 --> 00:23:43,140
ähm, das diese großen KI-Unternehmen für

696
00:23:43,140 --> 00:23:46,620
ihre Sprache verwenden  Modelle ähm und

697
00:23:46,620 --> 00:23:48,720
ähm Ich denke, eigentlich ist es so, als wäre

698
00:23:48,720 --> 00:23:50,340
das genau das Richtige

699
00:23:50,340 --> 00:23:52,020
und genau das, was das Feld

700
00:23:52,020 --> 00:23:54,780
richtig braucht, denn Sie könnten feststellen, dass Sie

701
00:23:54,780 --> 00:23:57,059
mit einer kindgerechten Menge an Daten im

702
00:23:57,059 --> 00:23:58,980
Wesentlichen die richtige Syntax lernen können,

703
00:23:58,980 --> 00:24:01,140
was ich denke würde  Wäre das

704
00:24:01,140 --> 00:24:02,880
stärkste Argument gegen diese Armut

705
00:24:02,880 --> 00:24:04,260
an Stimulus-Behauptungen, könnten Sie

706
00:24:04,260 --> 00:24:06,299
alternativ feststellen, dass

707
00:24:06,299 --> 00:24:08,220
Sie vielleicht nicht viel lernen können,

708
00:24:08,220 --> 00:24:10,740
vielleicht haben Sie, Sie wissen schon,

709
00:24:10,740 --> 00:24:12,840
ein viel schäbigeres Sprachmodell entwickelt

710
00:24:12,840 --> 00:24:14,700
oder ihm fehlen einige syntaktische oder

711
00:24:14,700 --> 00:24:16,799
semantische Fähigkeiten ähm

712
00:24:16,799 --> 00:24:17,520


713
00:24:17,520 --> 00:24:18,720
ähm, ich denke tatsächlich, dass die

714
00:24:18,720 --> 00:24:20,220
Fehler dort ein bisschen schwer zu

715
00:24:20,220 --> 00:24:22,620
interpretieren sind, weil

716
00:24:22,620 --> 00:24:24,720
ähm, Kinder, äh Daten, wenn sie tatsächlich eine

717
00:24:24,720 --> 00:24:26,400
Sprache lernen, bekommen sie viel mehr

718
00:24:26,400 --> 00:24:28,919
Daten als nur äh, nur Reihen von

719
00:24:28,919 --> 00:24:31,200
Sätzen, richtig, in denen sie

720
00:24:31,200 --> 00:24:33,059
in einer Umgebung interagieren

721
00:24:33,059 --> 00:24:34,440
ähm, also gibt es Sachen in der Welt

722
00:24:34,440 --> 00:24:36,539
vor ihnen, ähm, ihre Äußerungen sind

723
00:24:36,539 --> 00:24:38,520
auch interaktiv, also kannst du

724
00:24:38,520 --> 00:24:39,960
etwas sagen und sehen, ob deine

725
00:24:39,960 --> 00:24:41,400
Eltern dir das bringen, wonach du fragst,

726
00:24:41,400 --> 00:24:43,799
zum Beispiel, richtig, dass lange

727
00:24:43,799 --> 00:24:46,020
von Leuten argumentiert wurde, ähm

728
00:24:46,020 --> 00:24:49,020
wie  a a a wissen Sie, wichtiger Hinweis

729
00:24:49,020 --> 00:24:51,240
im Spracherwerb,

730
00:24:51,240 --> 00:24:52,799
ähm, ähm,

731
00:24:52,799 --> 00:24:55,440
in der Baby-LM-Herausforderung

732
00:24:55,440 --> 00:24:58,200
gibt es eine Möglichkeit, diese Modelle zu trainieren,

733
00:24:58,200 --> 00:25:00,960
ähm, mit einer Art multimodalen Eingaben.

734
00:25:00,960 --> 00:25:02,340
Ich denke, Sie können ihnen so viele Videodaten geben,

735
00:25:02,340 --> 00:25:05,039
wie Sie geben möchten, ähm

736
00:25:05,039 --> 00:25:07,140
äh, aber wahrscheinlich ist es schwer,

737
00:25:07,140 --> 00:25:09,000
genau die Art von Setup

738
00:25:09,000 --> 00:25:11,220
und Feedback zu replizieren, die Kinder tatsächlich bekommen, also ähm, ähm,

739
00:25:11,220 --> 00:25:12,840


740
00:25:12,840 --> 00:25:14,700
ich weiß nicht, du weißt, ich bin

741
00:25:14,700 --> 00:25:17,700
gespannt, ähm, wo das hinführt

742
00:25:17,700 --> 00:25:19,799
und wie die Dinge  Pan da draußen, ähm,

743
00:25:19,799 --> 00:25:20,760


744
00:25:20,760 --> 00:25:23,700
wissen Sie, ich denke, es gibt eine

745
00:25:23,700 --> 00:25:26,580
interessante verwandte Frage für große

746
00:25:26,580 --> 00:25:28,320
Sprachmodelle,

747
00:25:28,320 --> 00:25:30,299
ähm, was es ist,

748
00:25:30,299 --> 00:25:31,919
genau zu verstehen, was all die

749
00:25:31,919 --> 00:25:34,200
Daten tun, also

750
00:25:34,200 --> 00:25:36,659
ähm, es könnte sein, dass Sie so

751
00:25:36,659 --> 00:25:38,279
viele Daten brauchen  diese Modelle, weil

752
00:25:38,279 --> 00:25:40,620
sie intern effektiv irgendeine Form

753
00:25:40,620 --> 00:25:43,919
von Semantik erfinden, also ähm,

754
00:25:43,919 --> 00:25:45,600
sie entdecken beide die Regel der

755
00:25:45,600 --> 00:25:47,460
Syntax und dort scheinen sie

756
00:25:47,460 --> 00:25:49,380
ziemlich viel über Wortbedeutungen zu lernen,

757
00:25:49,380 --> 00:25:50,940


758
00:25:50,940 --> 00:25:52,200
ähm und

759
00:25:52,200 --> 00:25:54,059
ähm, es ist nicht, es ist völlig unklar, denke ich,

760
00:25:54,059 --> 00:25:56,220
wie  Viele der Daten in diesen modernen

761
00:25:56,220 --> 00:25:58,919
Modellen werden für Syntax versus Semantik benötigt.

762
00:25:58,919 --> 00:26:00,240


763
00:26:00,240 --> 00:26:02,159
Meine eigene Vermutung wäre,

764
00:26:02,159 --> 00:26:05,580
dass die syntaktische Seite wahrscheinlich

765
00:26:05,580 --> 00:26:07,860
viel weniger Daten benötigt als

766
00:26:07,860 --> 00:26:09,600
die semantische Seite

767
00:26:09,600 --> 00:26:11,159


768
00:26:11,159 --> 00:26:12,960
Mine Frank Malika und ich haben

769
00:26:12,960 --> 00:26:15,120
vor ein paar Jahren eine Arbeit geschrieben, in der wir versuchten, die

770
00:26:15,120 --> 00:26:17,400
Menge an Informationen abzuschätzen, die ein Lernender

771
00:26:17,400 --> 00:26:19,679
notwendigerweise erwerben müsste,

772
00:26:19,679 --> 00:26:22,140
um die verschiedenen Aspekte

773
00:26:22,140 --> 00:26:23,820
der Sprache zu lernen, also müssen Sie alle

774
00:26:23,820 --> 00:26:25,320
Wörter lernen und Sie lernen ihre Foren, die Sie

775
00:26:25,320 --> 00:26:26,880
lernen  Ihre Bedeutung kennen Sie wahrscheinlich

776
00:26:26,880 --> 00:26:28,320
ihre Frequenzen Sie müssen die

777
00:26:28,320 --> 00:26:32,100
Syntax lernen und im Grunde haben wir

778
00:26:32,100 --> 00:26:34,080
in dieser Analyse herausgefunden, dass Sie wissen im Grunde

779
00:26:34,080 --> 00:26:35,640
nur eine Art

780
00:26:35,640 --> 00:26:37,440
Berechnung der Rückseite der Hüllkurve für jede dieser

781
00:26:37,440 --> 00:26:40,679
Domänen war, dass die Syntax tatsächlich sehr

782
00:26:40,679 --> 00:26:42,779
wenige Informationen enthält  Es braucht nicht

783
00:26:42,779 --> 00:26:46,400
so viele Informationen, um die Syntax zu lernen,

784
00:26:46,400 --> 00:26:49,320
während die meisten Informationen, die

785
00:26:49,320 --> 00:26:52,740
Sie erwerben, eigentlich der Semantik dienen. Wenn Sie also

786
00:26:52,740 --> 00:26:55,740
angeben, dass Sie 30 bis 50.000

787
00:26:55,740 --> 00:26:57,720
verschiedene Wortbedeutungen kennen, wissen Sie, ob selbst

788
00:26:57,720 --> 00:27:00,000
wenn jede Bedeutung

789
00:27:00,000 --> 00:27:02,340
nur ein paar Bits ist  Richtig so,

790
00:27:02,340 --> 00:27:04,620
das erfordert eine Menge Informationen und

791
00:27:04,620 --> 00:27:06,360
wahrscheinlich hat jedes Treffen mehr als ein paar

792
00:27:06,360 --> 00:27:08,460
Bits richtig,

793
00:27:08,460 --> 00:27:11,279
also könnte es so sein, dass

794
00:27:11,279 --> 00:27:12,659
ich vermuten würde, dass sich die

795
00:27:12,659 --> 00:27:14,820
meisten

796
00:27:14,820 --> 00:27:16,320
ihrer Trainingsdaten auf Wörter beziehen

797
00:27:16,320 --> 00:27:18,600
Semantik und Sie können über andere

798
00:27:18,600 --> 00:27:20,760
Möglichkeiten nachdenken, wie Kinder die Wortsemantik richtig hinbekommen, das

799
00:27:20,760 --> 00:27:22,140
ist nicht nur eine Art von

800
00:27:22,140 --> 00:27:24,600
Mustern des gleichzeitigen Auftretens im Text, ähm,

801
00:27:24,600 --> 00:27:27,360
aber ich stimme zu, dass all das in

802
00:27:27,360 --> 00:27:29,039
der Luft liegt und es wirklich aufregend ist zu sehen,

803
00:27:29,039 --> 00:27:30,960
was so passieren wird  Ja, ich weiß, dass

804
00:27:30,960 --> 00:27:32,640
einige der früheren Ergebnisse daraus aus

805
00:27:32,640 --> 00:27:35,279
Lindsays Labor darauf hindeuten, dass zumindest

806
00:27:35,279 --> 00:27:37,740
beschränkt auf ökologisch gültige

807
00:27:37,740 --> 00:27:40,919
Trainingsstätten äh, Modelle scheinen zu

808
00:27:40,919 --> 00:27:42,840
verallgemeinern. Sie kennen lineare Regeln für

809
00:27:42,840 --> 00:27:44,940
Englisch. Ja, keine Frage, Bildung, ROM,

810
00:27:44,940 --> 00:27:46,620
andere als die hierarchische Regel, die

811
00:27:46,620 --> 00:27:48,120
richtige Hierarchie  Regel, also denke ich, dass es

812
00:27:48,120 --> 00:27:49,980
einen wirklichen Sinn gibt, in dem Sie wissen, dass

813
00:27:49,980 --> 00:27:52,020
der Raum des korrekten

814
00:27:52,020 --> 00:27:54,000
syntaktischen Preises und der induktiven Vorurteile

815
00:27:54,000 --> 00:27:56,220
wirklich noch wirklich festgelegt werden muss,

816
00:27:56,220 --> 00:27:57,960
aber es scheint zumindest für mich ziemlich

817
00:27:57,960 --> 00:27:59,400
offensichtlich, dass es so etwas geben muss

818
00:27:59,400 --> 00:28:01,320
Es gibt auch einige Hinweise

819
00:28:01,320 --> 00:28:02,700
darauf, dass Kinder auf Englisch

820
00:28:02,700 --> 00:28:04,679
auf dieses Frequenzproblem zurückgehen, dass Kinder auf

821
00:28:04,679 --> 00:28:06,360
Englisch manchmal eine

822
00:28:06,360 --> 00:28:08,100
Zwischenkopie der Bewegung in der

823
00:28:08,100 --> 00:28:10,140
angegebenen Position der unteren

824
00:28:10,140 --> 00:28:11,880
Komplementärposition einer langen

825
00:28:11,880 --> 00:28:13,980
Distanz buchstabieren, also gibt es eine These

826
00:28:13,980 --> 00:28:15,360
von Thornton und einigen  andere Papiere darüber,

827
00:28:15,360 --> 00:28:18,120
also sagen sie, ähm, welche Person,

828
00:28:18,120 --> 00:28:19,919
glauben Sie, wer das getan hat, anstatt welche

829
00:28:19,919 --> 00:28:21,900
Person, glauben Sie, hat das getan, also ist dies

830
00:28:21,900 --> 00:28:23,159
eine interessante Miss-Einstellung,

831
00:28:23,159 --> 00:28:25,020
weil einige Sprachen

832
00:28:25,020 --> 00:28:26,760
diese Zwischenkopien tatsächlich buchstabieren, aber

833
00:28:26,760 --> 00:28:28,799
Englisch nicht so  Das Kind macht den

834
00:28:28,799 --> 00:28:30,779
Fehler bei der Festlegung seiner Grammatik, aber die

835
00:28:30,779 --> 00:28:32,820
Häufigkeit der Eingabe ist tatsächlich null,

836
00:28:32,820 --> 00:28:35,159
und so hat unser gemeinsamer Freund Gary Marcus

837
00:28:35,159 --> 00:28:36,779
auch ein Argument gegen die Häufigkeit, mit der die

838
00:28:36,779 --> 00:28:39,000
Ausgabe eines Kindes im Fall

839
00:28:39,000 --> 00:28:41,159
von deutschen Substantivpluralen eine regelmäßigere

840
00:28:41,159 --> 00:28:42,779
Form der bestimmten Art bestimmt  ist bevorzugt

841
00:28:42,779 --> 00:28:44,580
nicht das häufigste und es gibt viele

842
00:28:44,580 --> 00:28:46,559
Beispiele, wie es manchmal behauptet wird,

843
00:28:46,559 --> 00:28:49,080
dass Subjekterfahrungspassive, bei denen

844
00:28:49,080 --> 00:28:50,460
das Subjekt

845
00:28:50,460 --> 00:28:52,559
bei Kindern in Verständnisstudien bis etwa acht passiv etwas erlebt oder sehr verzögert,

846
00:28:52,559 --> 00:28:54,299


847
00:28:54,299 --> 00:28:56,039
weil sie nicht sehr häufig in der Eingabe sind,

848
00:28:56,039 --> 00:28:56,880


849
00:28:56,880 --> 00:28:59,220
aber Ken  Wexler und Kollegen haben

850
00:28:59,220 --> 00:29:01,260
Doppel-H-Fragen zum Thema Erfahrung

851
00:29:01,260 --> 00:29:03,539
wie „Wer mag Maria“ durchlaufen und

852
00:29:03,539 --> 00:29:05,820
festgestellt, dass diese ebenso selten

853
00:29:05,820 --> 00:29:07,500
in der Eingabe sind wie Themen und Erfahrungen

854
00:29:07,500 --> 00:29:09,659
mit Passiven, aber Kinder haben kein Problem damit,

855
00:29:09,659 --> 00:29:10,980
diese Fragen zu verstehen,

856
00:29:10,980 --> 00:29:13,860
aber sie haben Probleme beim Verstehen

857
00:29:13,860 --> 00:29:16,020
Thema Erfahrung mit verbalen Passiven, so dass die

858
00:29:16,020 --> 00:29:17,880
Häufigkeit wieder einmal irrelevant zu sein scheint

859
00:29:17,880 --> 00:29:19,380
oder zumindest nicht

860
00:29:19,380 --> 00:29:20,940
erklärend ist. Ich denke, es ist nicht

861
00:29:20,940 --> 00:29:22,320
erklärend in Bezug auf den

862
00:29:22,320 --> 00:29:25,080
Theorieaufbau. Wie kann LMS also bei diesen

863
00:29:25,080 --> 00:29:27,059
unterschiedlichen Fällen helfen, in denen

864
00:29:27,059 --> 00:29:28,440
eindeutig etwas anderes vor sich geht als die

865
00:29:28,440 --> 00:29:30,960
Häufigkeit?  Also Alaune, Sie wissen, dass sie scheinen zu

866
00:29:30,960 --> 00:29:33,419
verallgemeinern, wenn Sie auf diese

867
00:29:33,419 --> 00:29:35,100
Ausgabe der Fälle zurückkommen, die Sie in

868
00:29:35,100 --> 00:29:36,899
Ihrer Arbeit haben, und Sie zeigen, dass sie

869
00:29:36,899 --> 00:29:38,399
die Struktur farbloser

870
00:29:38,399 --> 00:29:41,279
Bildschirmideen verallgemeinern, was oft sehr cool ist,

871
00:29:41,279 --> 00:29:43,080
ähm, aber den positiven Stimulus nie wirklich hat

872
00:29:43,080 --> 00:29:44,520


873
00:29:44,520 --> 00:29:46,380
Ich weiß, dass Sie

874
00:29:46,380 --> 00:29:48,059
diese Behauptung richtig gemacht haben, aber Chomskys

875
00:29:48,059 --> 00:29:49,559
Argument in den 50er Jahren über statistische

876
00:29:49,559 --> 00:29:51,779
Modelle des Tages gilt nicht für

877
00:29:51,779 --> 00:29:53,700
kommerzielle LMS im Jahr 2023, und das ist

878
00:29:53,700 --> 00:29:55,440
richtig, aber wir können diesen einzelnen Punkt nicht verwenden,

879
00:29:55,440 --> 00:29:57,539
um zu untergraben  Sie kennen die gesamte

880
00:29:57,539 --> 00:29:59,520
Geometrie. Der grundlegende Punkt von Enterprise Chomsky

881
00:29:59,520 --> 00:30:01,080
war, dass Sie eine

882
00:30:01,080 --> 00:30:02,700
grammatikalische Struktur haben könnten, in der jedes

883
00:30:02,700 --> 00:30:04,980
Diagramm eine Nullfrequenz hat und es auch

884
00:30:04,980 --> 00:30:06,899
keine klar interpretierbaren

885
00:30:06,899 --> 00:30:08,039
Anweisungen für die konzeptionellen

886
00:30:08,039 --> 00:30:09,840
Schnittstellen gibt, also Schnittstellen anderer

887
00:30:09,840 --> 00:30:11,580
Systeme des Geistes, wie Sie es zeigen

888
00:30:11,580 --> 00:30:13,980
Ihre Papier-GPT ahmt Beispiele wie Pull-

889
00:30:13,980 --> 00:30:16,559
the-Screen-Ideen nach, und Sie wissen noch einmal, dass

890
00:30:16,559 --> 00:30:18,840
dieser Satz über 150.000

891
00:30:18,840 --> 00:30:20,700
Ergebnisse bei Google liefert und

892
00:30:20,700 --> 00:30:22,620
in der Literatur ausführlich diskutiert wird. Er kann

893
00:30:22,620 --> 00:30:24,480
die Tatsache nachahmen, dass er dies nachahmen kann, was

894
00:30:24,480 --> 00:30:26,640
uns nicht wirklich viel sagt  Zumindest können wir

895
00:30:26,640 --> 00:30:27,840
nichts mit viel

896
00:30:27,840 --> 00:30:30,899
Zuversicht sagen, also wissen Sie, dass Abiba Behind am

897
00:30:30,899 --> 00:30:32,580
University College Dublin dieses Zitat

898
00:30:32,580 --> 00:30:34,380
kürzlich hat, ähm, verwechseln Sie Ihre eigene

899
00:30:34,380 --> 00:30:36,840
Leichtgläubigkeit nicht mit der Intelligenz eines Films, und

900
00:30:36,840 --> 00:30:38,880
tatsächlich schrieb sogar Young The Coon letztes

901
00:30:38,880 --> 00:30:40,500
Jahr, dass Kritiker recht haben  werfen Sie

902
00:30:40,500 --> 00:30:42,480
LMS vor, sich mit einer Art

903
00:30:42,480 --> 00:30:43,919


904
00:30:43,919 --> 00:30:46,860
Nachahmung zu beschäftigen, und die Beispielsätze von gbt,

905
00:30:46,860 --> 00:30:48,840
die Sie in der Arbeit geben, leisten eigentlich

906
00:30:48,840 --> 00:30:50,700
keinen guten Job, denn wie Sie sagen,

907
00:30:50,700 --> 00:30:52,320
ist es wahrscheinlich, dass Sie bedeutungslose

908
00:30:52,320 --> 00:30:54,240
Sprachen kennen, die in den Trainingsdaten selten sind, aber

909
00:30:54,240 --> 00:30:55,740
sie können  Entweder tun sie es oder sie können es nicht, aber

910
00:30:55,740 --> 00:30:57,120
es gibt keinen Mittelweg, wenn es darum geht,

911
00:30:57,120 --> 00:30:59,880
uns 10 Beispiele wie dieses zu geben, also

912
00:30:59,880 --> 00:31:02,880
haben Sie farblose grüne Ideen, die

913
00:31:02,880 --> 00:31:04,679
ganz andere semantische Objekte sind als

914
00:31:04,679 --> 00:31:06,840
Dinge wie braun schimmernde Kaninchen,

915
00:31:06,840 --> 00:31:09,899
weiße glitzernde Bären, äh, schwarze glänzende

916
00:31:09,899 --> 00:31:12,360
Kängurus, grüne glitzernde Affen

917
00:31:12,360 --> 00:31:15,120
gelbe, schillernde Löwen, rot schimmernde

918
00:31:15,120 --> 00:31:16,320
Elemente, richtig, das ist alles wie

919
00:31:16,320 --> 00:31:18,899
semantisch, semantisch seltsam und ein bisschen

920
00:31:18,899 --> 00:31:20,820
seltsam, aber sie sind immer noch wie rechtliche

921
00:31:20,820 --> 00:31:22,440
Strukturen, sie sind irgendwie bedeutungsvolle

922
00:31:22,440 --> 00:31:25,320
synthetische semantische Objekte, ähm,

923
00:31:25,320 --> 00:31:27,179


924
00:31:27,179 --> 00:31:29,580


925
00:31:29,580 --> 00:31:33,179
richtig  Ich kann

926
00:31:33,179 --> 00:31:34,799
auf den ersten Punkt zuerst richtig antworten,

927
00:31:34,799 --> 00:31:36,120
also ähm,

928
00:31:36,120 --> 00:31:37,799
Sie haben angefangen, über

929
00:31:37,799 --> 00:31:40,140
diese anderen ähm Arten von

930
00:31:40,140 --> 00:31:42,179
Erwerbsmustern zu sprechen, die vielleicht nicht direkt

931
00:31:42,179 --> 00:31:44,279
auf die Frequenz abgebildet werden, ähm,

932
00:31:44,279 --> 00:31:47,159
und ich denke, es ist eigentlich ein Fehler

933
00:31:47,159 --> 00:31:50,640
zu denken, dass äh Art von modernen

934
00:31:50,640 --> 00:31:53,039
Lernmodellen  sollte nur auf der Häufigkeit basieren,

935
00:31:53,039 --> 00:31:54,899
denn

936
00:31:54,899 --> 00:31:57,120
ähm, sie lernen eindeutig wie ziemlich

937
00:31:57,120 --> 00:31:59,580
komplizierte Familien von Regeln oder

938
00:31:59,580 --> 00:32:02,399
Konstruktionen oder so etwas, und

939
00:32:02,399 --> 00:32:03,899
ähm, ich denke, es ist sehr wahrscheinlich, dass

940
00:32:03,899 --> 00:32:06,600
sie, wenn sie lernen,

941
00:32:06,600 --> 00:32:08,580
in gewissem Sinne nach a suchen  Einfache

942
00:32:08,580 --> 00:32:10,559
oder sparsame

943
00:32:10,559 --> 00:32:12,360
Erklärung der Daten, die

944
00:32:12,360 --> 00:32:13,980
sie richtig gesehen haben und wie das

945
00:32:13,980 --> 00:32:16,080
in einem neuronalen Netzwerk zwischengespeichert wird, ist vielleicht

946
00:32:16,080 --> 00:32:19,200
kompliziert und Sie wissen, dass es von

947
00:32:19,200 --> 00:32:20,880
Parametern und den Besonderheiten

948
00:32:20,880 --> 00:32:22,679
des Lernalgorithmus abhängt, und und und

949
00:32:22,679 --> 00:32:24,480
Solche Dinge

950
00:32:24,480 --> 00:32:27,000
ähm, aber ich denke, es ist äh, ich denke,

951
00:32:27,000 --> 00:32:29,279
ich würde vielleicht vermuten, dass es wahrscheinlich

952
00:32:29,279 --> 00:32:31,740
der Fall ist, dass

953
00:32:31,740 --> 00:32:35,640
ähm, wie sie sind, sie

954
00:32:35,640 --> 00:32:38,399
lernen über eine komplizierte Reihe von

955
00:32:38,399 --> 00:32:40,799
Dingen, richtig, eine komplizierte Art  der

956
00:32:40,799 --> 00:32:43,799
Familie von Regeln und Konstruktionen

957
00:32:43,799 --> 00:32:46,919
ähm und das bedeutet, ich denke, dass

958
00:32:46,919 --> 00:32:49,380
ähm ihre Verallgemeinerungen wie die

959
00:32:49,380 --> 00:32:52,500
Beispiele von Leuten sein könnten, die Sie ähm gegeben haben,

960
00:32:52,500 --> 00:32:55,440
könnte in der Eingabe rechts irgendwie diskontinuierlich sein,

961
00:32:55,440 --> 00:32:57,720
also könnten Sie sich manchmal

962
00:32:57,720 --> 00:33:00,000
vorstellen, einige Zeichenfolgen zu sehen, die

963
00:33:00,000 --> 00:33:02,220
führen  Sie zu einer Grammatik und die einfachste

964
00:33:02,220 --> 00:33:03,840
Grammatik der Daten, die Sie

965
00:33:03,840 --> 00:33:06,059
bisher gesehen haben, ist eine, die eine

966
00:33:06,059 --> 00:33:09,120
unsichtbare Zeichenfolge richtig vorhersagt, und

967
00:33:09,120 --> 00:33:12,360
ähm, wenn das passiert, dann werden Sie

968
00:33:12,360 --> 00:33:14,100
die Daten nehmen und eine

969
00:33:14,100 --> 00:33:16,860
Darstellung lernen, die sich in einem

970
00:33:16,860 --> 00:33:19,440
Roman unsichtbar verallgemeinert  So weit, ähm,

971
00:33:19,440 --> 00:33:21,720
nur weil diese Verallgemeinerung

972
00:33:21,720 --> 00:33:23,460
so etwas wie die einfachste Darstellung der

973
00:33:23,460 --> 00:33:25,019
Daten ist, die Sie bisher gesehen haben

974
00:33:25,019 --> 00:33:25,980


975
00:33:25,980 --> 00:33:28,320


976
00:33:28,320 --> 00:33:29,700


977
00:33:29,700 --> 00:33:31,260
Theorie davon und dann

978
00:33:31,260 --> 00:33:33,419
sagt diese Theorie manchmal ein neues Phänomen

979
00:33:33,419 --> 00:33:35,840
oder eine neue Art von Satz voraus,

980
00:33:35,840 --> 00:33:38,159
und wenn sie also über einen

981
00:33:38,159 --> 00:33:41,100
ausreichend reichen Raum von Theorien lernen,

982
00:33:41,100 --> 00:33:42,899
dann wäre es nicht

983
00:33:42,899 --> 00:33:45,000
unvernünftig oder unerwartet für sie

984
00:33:45,000 --> 00:33:47,039
Zeigen Sie jetzt auch diese Art von Mustern,

985
00:33:47,039 --> 00:33:48,899
ob sie es tun oder nicht. Ich denke, das

986
00:33:48,899 --> 00:33:51,299
ist immer noch eine offene empirische Frage,

987
00:33:51,299 --> 00:33:52,740


988
00:33:52,740 --> 00:33:54,120
richtig, weil wir sie mit

989
00:33:54,120 --> 00:33:55,620
kleinen Datenmengen trainieren und ihre

990
00:33:55,620 --> 00:33:57,480
Verallgemeinerungen testen müssen, und solche

991
00:33:57,480 --> 00:33:58,260
Dinge,

992
00:33:58,260 --> 00:34:00,120
ähm, aber ich tue es nicht.  Denken Sie nicht, dass nur die Tatsache,

993
00:34:00,120 --> 00:34:01,620
dass

994
00:34:01,620 --> 00:34:03,899
Menschen Dinge tun, die

995
00:34:03,899 --> 00:34:06,059
nicht nur auf Frequenz basieren,

996
00:34:06,059 --> 00:34:07,860
überhaupt ein Beweis dafür ist, denn

997
00:34:07,860 --> 00:34:09,300
sobald Sie über reichhaltige und

998
00:34:09,300 --> 00:34:10,980
interessante Klassen von Theorien gelernt haben,

999
00:34:10,980 --> 00:34:13,980
ist dies das eigentlich erwartete Verhalten

1000
00:34:13,980 --> 00:34:16,980
Ich hatte vor ungefähr einem Jahr ein Papier,

1001
00:34:16,980 --> 00:34:18,719
von dem ich glaube, dass Sie

1002
00:34:18,719 --> 00:34:20,040
mit

1003
00:34:20,040 --> 00:34:22,560
äh Yang und Pianta Dosi vertraut sind,

1004
00:34:22,560 --> 00:34:24,179
wo wir äh äh

1005
00:34:24,179 --> 00:34:26,879
geschaut haben,

1006
00:34:26,879 --> 00:34:29,280
was passiert, wenn Sie

1007
00:34:29,280 --> 00:34:32,639
einem Programm Lernmodellzeichenfolgen geben  äh,

1008
00:34:32,639 --> 00:34:34,679
verschiedene formale Sprachen, also denken Sie an

1009
00:34:34,679 --> 00:34:35,820


1010
00:34:35,820 --> 00:34:38,760
ein allgemeines Modell, nur Sie

1011
00:34:38,760 --> 00:34:41,159
kennen 10 oder 20, vielleicht einfache Zeichenfolgen, die einem

1012
00:34:41,159 --> 00:34:43,500
Muster gehorchen, und bitten Sie es dann,

1013
00:34:43,500 --> 00:34:46,199
ein Programm zu finden, das diese Daten erklären kann,

1014
00:34:46,199 --> 00:34:49,320
was oft bedeutet, dass Sie wissen, dass

1015
00:34:49,320 --> 00:34:50,940
Sie einige finden  Art, das

1016
00:34:50,940 --> 00:34:52,679


1017
00:34:52,679 --> 00:34:54,839
Muster in den Strings programmgesteuert niederzuschreiben,

1018
00:34:54,839 --> 00:34:56,280
ähm, und in dieser Abbildung haben wir ein Papier,

1019
00:34:56,280 --> 00:34:58,020
das für diesen Punkt wirklich relevant ist,

1020
00:34:58,020 --> 00:34:59,820
wo ähm, die

1021
00:34:59,820 --> 00:35:02,640
Verallgemeinerungen, die diese Art

1022
00:35:02,640 --> 00:35:04,200
von Modell macht,

1023
00:35:04,200 --> 00:35:06,660
ähm, sind, denke ich, irgendwie  qualitativ

1024
00:35:06,660 --> 00:35:08,160
wie die, die Sie für

1025
00:35:08,160 --> 00:35:10,560
Leute beschreiben, wo

1026
00:35:10,560 --> 00:35:12,060
Sie ihnen eine kleine Menge

1027
00:35:12,060 --> 00:35:13,980
an Daten geben können, und es wird

1028
00:35:13,980 --> 00:35:16,740
mit sehr hoher Wahrscheinlichkeit unsichtbare Zeichenfolgen vorhersagen,

1029
00:35:16,740 --> 00:35:18,420
obwohl die Trainingseingabe null Frequenz enthält,

1030
00:35:18,420 --> 00:35:20,280
und der Grund dafür

1031
00:35:20,280 --> 00:35:22,740
ist, dass die

1032
00:35:22,740 --> 00:35:24,660
prägnanteste rechnerische Beschreibung der

1033
00:35:24,660 --> 00:35:26,640
Daten, die Sie gesehen haben, oft eine ist, die

1034
00:35:26,640 --> 00:35:29,640
eine bestimmte, äh, neue, unsichtbare

1035
00:35:29,640 --> 00:35:32,940
Ausgabe vorhersagt, so dass dieses Modell

1036
00:35:32,940 --> 00:35:34,800
im Wesentlichen eine Implementierung der

1037
00:35:34,800 --> 00:35:36,839
Art von Chader- und Vitani-Programmen ist, von denen

1038
00:35:36,839 --> 00:35:38,160


1039
00:35:38,160 --> 00:35:40,619
ich lerne  Früher angesprochen, ähm,

1040
00:35:40,619 --> 00:35:42,900
aber es ist eines, von dem ich denke, dass Sie es

1041
00:35:42,900 --> 00:35:43,980
wissen, wenn Sie im Zusammenhang

1042
00:35:43,980 --> 00:35:45,660
mit diesen Argumenten von Kindern denken, die

1043
00:35:45,660 --> 00:35:48,599
ungewöhnliche oder unerwartete Dinge sagen, wie das

1044
00:35:48,599 --> 00:35:50,280
von all diesen Arten von

1045
00:35:50,280 --> 00:35:52,859
Berichten vorhergesagt wird, richtig, weil so

1046
00:35:52,859 --> 00:35:54,180
lange so lange  Diese Dinge

1047
00:35:54,180 --> 00:35:55,680
vergleichen effektiv einen interessanten Bereich von

1048
00:35:55,680 --> 00:35:56,760
Grammatiken,

1049
00:35:56,760 --> 00:35:58,079
ähm, dann werden sie zeigen, dass diese

1050
00:35:58,079 --> 00:35:59,760
Art von Verhalten ich denke, äh ja,

1051
00:35:59,760 --> 00:36:04,680
also okay, also schätze du, du weißt,

1052
00:36:04,680 --> 00:36:06,960
dass das Argument wäre, dass zumindest aus

1053
00:36:06,960 --> 00:36:10,140
der Perspektive der Geschlechter die Syntax

1054
00:36:10,140 --> 00:36:13,140
separat funktioniert  aber es ist immer noch

1055
00:36:13,140 --> 00:36:15,359
auf Semantik abgebildet, es informiert

1056
00:36:15,359 --> 00:36:17,040
Pragmatik richtig, also ist die Programmsyntax im Nahen Osten

1057
00:36:17,040 --> 00:36:18,480
offensichtlich bedeutungslos, es ist

1058
00:36:18,480 --> 00:36:20,160
sehr klein, es ist nur eine

1059
00:36:20,160 --> 00:36:22,320
Linearisierung und Kennzeichnung, sie sind die

1060
00:36:22,320 --> 00:36:24,180
beiden einzigen Operationen, die Sie haben, einen

1061
00:36:24,180 --> 00:36:26,220
Linearisierungsalgorithmus für zentromotorische

1062
00:36:26,220 --> 00:36:27,900
Systeme und irgendeine Art von

1063
00:36:27,900 --> 00:36:30,480
Kategorisierungsalgorithmus am um Center at the

1064
00:36:30,480 --> 00:36:32,220
Conceptual Systems

1065
00:36:32,220 --> 00:36:33,839
ähm, also Chomskys Architektur ist irgendwie

1066
00:36:33,839 --> 00:36:35,940
abhängig von dem Prozess der Zuordnung von Syntax

1067
00:36:35,940 --> 00:36:37,560
zu Semantik, richtig, es ist Schaum, was

1068
00:36:37,560 --> 00:36:39,960
Regulierung bedeutet, es ist nicht nur Struktur und

1069
00:36:39,960 --> 00:36:42,180
es ist nicht nur Bedeutung, also

1070
00:36:42,180 --> 00:36:43,680
haben LMS das nicht wirklich  Mapping-Prozess genau

1071
00:36:43,680 --> 00:36:45,180
wie Wo ist die Zuordnung zur Semantik

1072
00:36:45,180 --> 00:36:47,160
und wenn es eine Zuordnung gibt, wie

1073
00:36:47,160 --> 00:36:48,480
sieht der Mapping-Prozess aus,

1074
00:36:48,480 --> 00:36:50,099


1075
00:36:50,099 --> 00:36:52,320
was sind die

1076
00:36:52,320 --> 00:36:54,000
Eigenschaften

1077
00:36:54,000 --> 00:36:55,500
seiner Semantik?  Beschränkungen des

1078
00:36:55,500 --> 00:36:57,180
Marketingprozesses, wie sie es für

1079
00:36:57,180 --> 00:36:58,980
natürliche Sprache tun, wissen Sie irgendwie,

1080
00:36:58,980 --> 00:37:01,079
äh, informieren sich diese Art von Beschränkungen

1081
00:37:01,079 --> 00:37:02,640
gegenseitig, sind sie eine Art Hin-

1082
00:37:02,640 --> 00:37:05,579
und Her-Prozess, genau wie Elemente

1083
00:37:05,579 --> 00:37:07,260
scheinen diese Form nicht wirklich zu beschreiben, was

1084
00:37:07,260 --> 00:37:09,359
Paarung bedeutet  richtig wie welche

1085
00:37:09,359 --> 00:37:11,339
bedeutungen welche strings zum beispiel richtig tut

1086
00:37:11,339 --> 00:37:14,640
mir leid sagst du das

1087
00:37:14,640 --> 00:37:16,740
ähm dass sie überhaupt keine semantik haben

1088
00:37:16,740 --> 00:37:18,599
oder sagst du dass es

1089
00:37:18,599 --> 00:37:21,180
einfach keine klare abgrenzung gibt zwischen

1090
00:37:21,180 --> 00:37:23,400
wie die strukturen auf die semantik abgebildet werden

1091
00:37:23,400 --> 00:37:25,500
ja letzteres  Richtig, also

1092
00:37:25,500 --> 00:37:27,300
haben sie eindeutig eine potenzielle Art

1093
00:37:27,300 --> 00:37:28,800
von Semantik. Ich weiß, dass Sie für eine

1094
00:37:28,800 --> 00:37:30,180
konzeptionelle Rollentheorie argumentiert haben, die hier relevant ist.

1095
00:37:30,180 --> 00:37:31,920
Der Rest ist vielleicht

1096
00:37:31,920 --> 00:37:33,900
etwas mysteriöser, aber die eigentliche Soja-

1097
00:37:33,900 --> 00:37:35,339
Linguistik ganz am Anfang, es gibt eine Theorie

1098
00:37:35,339 --> 00:37:36,900
des Kartierungsprozesses  An sich ist es

1099
00:37:36,900 --> 00:37:39,060
explizit und Sie können es in Aktion sehen

1100
00:37:39,060 --> 00:37:40,440
und Sie können verschiedene Theorien darüber

1101
00:37:40,440 --> 00:37:42,000
in Psych-Sprachmodellen testen und was

1102
00:37:42,000 --> 00:37:44,700
haben Sie die eigentliche Regulierung, die Sie

1103
00:37:44,700 --> 00:37:46,079
kennen, eingeschränkte Mehrdeutigkeit

1104
00:37:46,079 --> 00:37:48,119
Mehrdeutigkeit in dem Sinne, dass Sie ein

1105
00:37:48,119 --> 00:37:50,400
Wort mit mehreren Bedeutungen oder einer kennen

1106
00:37:50,400 --> 00:37:53,040
mehrere Interpretationen strukturieren usw. richtig

1107
00:37:53,040 --> 00:37:55,200
ja, ich meine, wenn Sie denken, dass sie

1108
00:37:55,200 --> 00:37:57,599
Semantik haben, dann denke ich, dass sie

1109
00:37:57,599 --> 00:37:59,640
eine Zuordnung von der Syntax zur Semantik haben müssen,

1110
00:37:59,640 --> 00:38:00,960


1111
00:38:00,960 --> 00:38:03,599
ähm, ich stimme zu, es ist nicht so, als würde niemand

1112
00:38:03,599 --> 00:38:04,980
wirklich verstehen, wie sie

1113
00:38:04,980 --> 00:38:07,560
auf irgendeiner tiefen Ebene arbeiten  Richtig, also stimme ich zu,

1114
00:38:07,560 --> 00:38:10,140
es ist nicht so klar wie ähm

1115
00:38:10,140 --> 00:38:11,940
sagen in der generativen Syntax und

1116
00:38:11,940 --> 00:38:13,859
Semantik, wo

1117
00:38:13,859 --> 00:38:15,180
ähm Sie wissen, dass Sie

1118
00:38:15,180 --> 00:38:17,880
die Regeln der Komposition aufschreiben und

1119
00:38:17,880 --> 00:38:19,920
eine kompositorische Bedeutung aus einem

1120
00:38:19,920 --> 00:38:21,480
Satz aus den Bestandteilen ableiten können  oder so

1121
00:38:21,480 --> 00:38:23,339
etwas Richtiges ist, ähm,

1122
00:38:23,339 --> 00:38:24,660
du weißt, so arbeiten sie nicht

1123
00:38:24,660 --> 00:38:27,060
richtig, aber

1124
00:38:27,060 --> 00:38:28,859
ähm, ich, ich, ich, ich würde es nicht als

1125
00:38:28,859 --> 00:38:32,160
selbstverständlich ansehen, dass es so sein muss, ähm,

1126
00:38:32,160 --> 00:38:34,619
es könnte sein, dass die Art und Weise, wie sie

1127
00:38:34,619 --> 00:38:36,420
arbeiten, tatsächlich so ist, wie wir  funktioniert richtig, dass

1128
00:38:36,420 --> 00:38:38,760
alles in

1129
00:38:38,760 --> 00:38:40,800
einem hochdimensionalen Vektorraum dargestellt wird und es

1130
00:38:40,800 --> 00:38:43,740
gibt eine komplizierte Art und Weise, wie

1131
00:38:43,740 --> 00:38:46,200
diese Vektorsemantik mit

1132
00:38:46,200 --> 00:38:48,240
jedem zusätzlichen Wort oder was

1133
00:38:48,240 --> 00:38:51,480
auch immer in einem linguistischen Strom aktualisiert wird,

1134
00:38:51,480 --> 00:38:53,940
aber ich denke, es ist klar,

1135
00:38:53,940 --> 00:38:55,320
dass sie einige haben  Art der

1136
00:38:55,320 --> 00:38:57,660
Darstellung der Semantik eines

1137
00:38:57,660 --> 00:38:59,099
Satzes richtig, als ob sie

1138
00:38:59,099 --> 00:39:01,260
zum Beispiel Fragen zumindest

1139
00:39:01,260 --> 00:39:02,760
annähernd beantworten können. Ich meine, es ist nicht

1140
00:39:02,760 --> 00:39:04,380
perfekt, aber

1141
00:39:04,380 --> 00:39:06,720
ähm, es ist nicht wie ein Engrammmodell oder

1142
00:39:06,720 --> 00:39:07,740
etwas Richtiges, das wirklich

1143
00:39:07,740 --> 00:39:10,500
keine Semantik hat

1144
00:39:10,500 --> 00:39:12,780
ähm, ich denke, dass sie ähm, sie

1145
00:39:12,780 --> 00:39:14,040


1146
00:39:14,040 --> 00:39:16,920
repräsentieren definitiv Semantik und

1147
00:39:16,920 --> 00:39:20,040
ähm, ähm, wissen Sie, aktualisieren Sie das, während

1148
00:39:20,040 --> 00:39:21,599
sie Sprache verarbeiten, sieht es einfach

1149
00:39:21,599 --> 00:39:23,400
nicht so aus wie diese anderen formalen

1150
00:39:23,400 --> 00:39:24,599
Theorien, ähm,

1151
00:39:24,599 --> 00:39:26,400
und ich glaube, ich verstehe es nicht  Warum

1152
00:39:26,400 --> 00:39:27,720
das ein Problem ist, genau wie diese anderen

1153
00:39:27,720 --> 00:39:29,640
formalen Theorien, könnten nur Sie wissen,

1154
00:39:29,640 --> 00:39:31,920
schlechte Annäherungen oder oder einfach völlig

1155
00:39:31,920 --> 00:39:33,720
falsch sein, ja ja ja

1156
00:39:33,720 --> 00:39:35,520
nein völlig völlig

1157
00:39:35,520 --> 00:39:37,260


1158
00:39:37,260 --> 00:39:39,359


1159
00:39:39,359 --> 00:39:41,520


1160
00:39:41,520 --> 00:39:42,780
dieser Dinge funktionieren richtig, also ist eine andere

1161
00:39:42,780 --> 00:39:45,240
Möglichkeit, darüber nachzudenken, dass Sie wissen, dass LMS

1162
00:39:45,240 --> 00:39:47,760
gut sind LMS sind Komprimierungsalgorithmen,

1163
00:39:47,760 --> 00:39:49,740
aber das Verständnis natürlicher Sprache dreht sich

1164
00:39:49,740 --> 00:39:51,540
mehr um Dekomprimierung,

1165
00:39:51,540 --> 00:39:54,180
es geht um die Begriffsklärung von Bedeutung X Aus

1166
00:39:54,180 --> 00:39:56,099
Bedeutungen XYZ geht es darum,

1167
00:39:56,099 --> 00:39:58,020
Rückschlüsse auf Sie zu ziehen  Meta-Beziehungen

1168
00:39:58,020 --> 00:39:59,579
zwischen Konzepten, die nicht in den

1169
00:39:59,579 --> 00:40:01,859
Trainingsdaten enthalten sind, also einige Beispiele, die

1170
00:40:01,859 --> 00:40:03,180
Melanie Mitchell Dinge ausgibt, wie

1171
00:40:03,180 --> 00:40:06,119
oben drauf weißt du, dass sie wieder oben ist, äh,

1172
00:40:06,119 --> 00:40:08,700
es ist oben auf der Box, all diese

1173
00:40:08,700 --> 00:40:10,140
variieren je nach Kontext, also gibt es eine Menge davon

1174
00:40:10,140 --> 00:40:11,880
andere Dinge, die richtig ablaufen,

1175
00:40:11,880 --> 00:40:13,260
und ich denke, Sie diskutieren einige dieser

1176
00:40:13,260 --> 00:40:16,440
Beispiele in Ihrer Arbeit, damit Sie wissen,

1177
00:40:16,440 --> 00:40:18,119
aber die Sprachfähigkeit ist immer noch

1178
00:40:18,119 --> 00:40:20,940
nicht zumindest wieder unter dieser

1179
00:40:20,940 --> 00:40:22,920
Sprachtheorie und es geht nicht um die

1180
00:40:22,920 --> 00:40:25,200
Generierung von Zeichenfolgen, sondern um diese Form  Das bedeutet

1181
00:40:25,200 --> 00:40:27,540
Paarungsmaschine, also

1182
00:40:27,540 --> 00:40:29,400
denken Sie manchmal sogar, dass alles, was

1183
00:40:29,400 --> 00:40:31,859
Semantik ausmacht, gerecht und richtig ist, also ist

1184
00:40:31,859 --> 00:40:33,540
Paul Petrovskys Konjunktiv,

1185
00:40:33,540 --> 00:40:36,000
dass die menschliche Semantik gerecht ist, und das ist es,

1186
00:40:36,000 --> 00:40:37,500


1187
00:40:37,500 --> 00:40:39,900
was wiederum sehr einfach, elegant ist, es ist, es ist,

1188
00:40:39,900 --> 00:40:42,000
es ist interpretierbar  Es ist

1189
00:40:42,000 --> 00:40:43,920
mit vielen Dingen kompatibel, die

1190
00:40:43,920 --> 00:40:46,320
Sie wissen oder die Ihnen vielleicht im

1191
00:40:46,320 --> 00:40:47,940
Hals der Wörter richtig vor sich gehen, aber

1192
00:40:47,940 --> 00:40:49,859
Sie wissen immer noch, dass natürliche Sprache

1193
00:40:49,859 --> 00:40:51,540
immer noch kompositorischer ist,

1194
00:40:51,540 --> 00:40:54,060
Dinge wie äh, Sie kennen formale

1195
00:40:54,060 --> 00:40:55,260
Sprachen, nur um eine klare

1196
00:40:55,260 --> 00:40:56,820
Unterscheidung zu treffen das ist  Sie haben eine

1197
00:40:56,820 --> 00:40:58,260
viel reichhaltigere Strukturzusammensetzung, es

1198
00:40:58,260 --> 00:41:00,839
gibt mehr Dinge, die passieren, ähm, vielleicht wurde also

1199
00:41:00,839 --> 00:41:02,160
bereits darauf hingewiesen, dass Sie

1200
00:41:02,160 --> 00:41:03,480
Dinge wie aufmerksamkeitsbasierte

1201
00:41:03,480 --> 00:41:05,640
Maschinenmechanismen und Transformatoren kennen,

1202
00:41:05,640 --> 00:41:07,800
um Kombinationen von diskreten

1203
00:41:07,800 --> 00:41:10,320
Token-Bindungen zu ermöglichen, was eher

1204
00:41:10,320 --> 00:41:12,359
einer Verschmelzung entspricht  wie Operator als einfache

1205
00:41:12,359 --> 00:41:14,640
rekurrente Matrixmultiplikation

1206
00:41:14,640 --> 00:41:16,140
ähm, aber Sie kennen das Problem der binären

1207
00:41:16,140 --> 00:41:17,460
Verzweigung binäre Verzweigung Regierung,

1208
00:41:17,460 --> 00:41:19,260
nur um hier ein weiteres Beispiel zu wählen, um über

1209
00:41:19,260 --> 00:41:20,640
die volle Bedeutung zu sprechen

1210
00:41:20,640 --> 00:41:23,640


1211
00:41:23,640 --> 00:41:24,839


1212
00:41:24,839 --> 00:41:26,700


1213
00:41:26,700 --> 00:41:28,740
Ursprünge und Orte dieser

1214
00:41:28,740 --> 00:41:30,660
offensichtlichen Einschränkung in der synthetischen

1215
00:41:30,660 --> 00:41:31,800
Berechnung, genau wie woher sie

1216
00:41:31,800 --> 00:41:33,540
kommt, vielleicht ist es eine Bedingung für die

1217
00:41:33,540 --> 00:41:34,980
Zusammenführung, vielleicht wird sie von einem reibungslosen

1218
00:41:34,980 --> 00:41:37,079
System auferlegt, vielleicht ist es eine Art Prior,

1219
00:41:37,079 --> 00:41:39,060
wissen Sie, wer weiß, und tatsächlich ist es eine

1220
00:41:39,060 --> 00:41:40,500
neuere funktionierende generative Grammatik

1221
00:41:40,500 --> 00:41:43,859
hat versucht, alle

1222
00:41:43,859 --> 00:41:46,440
theoretischen Annahmen des

1223
00:41:46,440 --> 00:41:47,700
Eherechts zu erden und zu beseitigen. Vielleicht ist die Mengenlehre nicht der beste

1224
00:41:47,700 --> 00:41:48,780
Weg, um

1225
00:41:48,780 --> 00:41:50,339
die generative Grammatik zu modellieren. Vielleicht

1226
00:41:50,339 --> 00:41:51,540
sind Marias logische Konten angemessener, es

1227
00:41:51,540 --> 00:41:53,520
gibt viele andere neuere Ideen,

1228
00:41:53,520 --> 00:41:55,020
die alle kompatibel sind  Das

1229
00:41:55,020 --> 00:41:57,540
mit Chomskys Ansatz ist richtig,

1230
00:41:57,540 --> 00:41:58,680
ja, eines der Dinge, die Trump

1231
00:41:58,680 --> 00:42:00,060
am meisten mag, ist, wenn er sich als

1232
00:42:00,060 --> 00:42:01,380
falsch erwiesen hat. Viele dieser

1233
00:42:01,380 --> 00:42:03,720
Theorien richten sich gegen die

1234
00:42:03,720 --> 00:42:06,300
minimalistische Kernarchitektur des Mainstreams, aber

1235
00:42:06,300 --> 00:42:08,640
das, ja, ich denke, es ist sehr unterschiedlich

1236
00:42:08,640 --> 00:42:11,839
wie  Lebendiges Feld Die Leute, die

1237
00:42:11,839 --> 00:42:15,119
Bornstein sind, wissen Sie, Petrovsky uh uh

1238
00:42:15,119 --> 00:42:17,520
hajipura, sie stimmen in grundlegender

1239
00:42:17,520 --> 00:42:19,380
Weise nicht mit dem überein, was der Mainstream

1240
00:42:19,380 --> 00:42:20,940
der chemischen Grammatik sagen würde, aber es

1241
00:42:20,940 --> 00:42:21,960
gibt noch mehr Spielraum für

1242
00:42:21,960 --> 00:42:24,240
Meinungsverschiedenheiten, aber es ist immer noch kompatibel,

1243
00:42:24,240 --> 00:42:26,400
so viele Grundannahmen richtig zu stellen

1244
00:42:26,400 --> 00:42:27,599
David Ich trage zum Beispiel nur

1245
00:42:27,599 --> 00:42:29,400
gewisse Abweichungen in dieser Kernbetrachtung,

1246
00:42:29,400 --> 00:42:31,920
aber es versucht immer noch, diese

1247
00:42:31,920 --> 00:42:33,240
Intuitionen in verschiedenen formalen

1248
00:42:33,240 --> 00:42:34,500
Systemen zu verankern, ähm, also

1249
00:42:34,500 --> 00:42:36,000
weißt du,

1250
00:42:36,000 --> 00:42:38,099
es ist irgendwie so,

1251
00:42:38,099 --> 00:42:40,320
ich möchte deine Gedanken noch einmal auf ähm,

1252
00:42:40,320 --> 00:42:42,240
ich habe Mitchell richtig erwähnt, also Michelin

1253
00:42:42,240 --> 00:42:44,579
Bowers, ähm  2020 haben sie diese

1254
00:42:44,579 --> 00:42:47,040
Papierversuchsliste mit wiederkehrenden Netzwerken, die

1255
00:42:47,040 --> 00:42:48,359
merkwürdigerweise liegen, von denen ich denke, dass Sie sich

1256
00:42:48,359 --> 00:42:49,859
dessen bewusst sind, also ist es ein wirklich gutes

1257
00:42:49,859 --> 00:42:51,060
Beispiel, um das Problem irgendwie auf den Punkt zu bringen,

1258
00:42:51,060 --> 00:42:53,099
so dass sich

1259
00:42:53,099 --> 00:42:54,780
gezeigt hat, dass wiederkehrende neuronale Netzwerke genau

1260
00:42:54,780 --> 00:42:56,520
modellieren, Sie wissen nicht  -Verb-Zahlenübereinstimmung,

1261
00:42:56,520 --> 00:42:58,020
aber Mitchell und Barrow zeigten, dass

1262
00:42:58,020 --> 00:43:00,060
diese Netzwerke auch eine

1263
00:43:00,060 --> 00:43:01,680
Zahlenübereinstimmung mit unnatürlichen

1264
00:43:01,680 --> 00:43:03,359
Satzstrukturen landen werden, also Strukturen, die

1265
00:43:03,359 --> 00:43:04,859
in der natürlichen Sprache nicht zu finden sind und die

1266
00:43:04,859 --> 00:43:06,540
Menschen nur schwer verarbeiten können,

1267
00:43:06,540 --> 00:43:09,359
so dass der Lernmodus für rnns zumindest richtig ist

1268
00:43:09,359 --> 00:43:11,880
denn rnn unterscheidet sich positiv von

1269
00:43:11,880 --> 00:43:14,339
Säugling, Sie kennen den Säugling Homo sapiens

1270
00:43:14,339 --> 00:43:16,260
richtig, also ist die Geschichte Mitchell und

1271
00:43:16,260 --> 00:43:18,359
Bowers zeigen, dass das lstl-Modell zwar

1272
00:43:18,359 --> 00:43:20,040
eine gute Darstellung von Singular

1273
00:43:20,040 --> 00:43:22,140
und Plural für einzelne Sätze hat, aber

1274
00:43:22,140 --> 00:43:24,359
keine Verallgemeinerung stattfindet, die

1275
00:43:24,359 --> 00:43:25,800
sie auf individueller Ebene darstellen können

1276
00:43:25,800 --> 00:43:27,359
Das Modell hat also keine

1277
00:43:27,359 --> 00:43:28,859
Darstellung der Zahl als

1278
00:43:28,859 --> 00:43:31,200
Abstraktion, welche Zahl nur konkrete

1279
00:43:31,200 --> 00:43:34,020
Instanzen des Singulars Basis Plural

1280
00:43:34,020 --> 00:43:35,400
um sind, also ist die erfolgreiche Vorhersage des

1281
00:43:35,400 --> 00:43:38,579
Sprachverhaltens über LM oder die erfolgreiche

1282
00:43:38,579 --> 00:43:40,800
Vorhersage neuronaler Reaktionen auf ähnliche

1283
00:43:40,800 --> 00:43:42,480
Weise offensichtlich großartig und vielleicht können wir uns darauf

1284
00:43:42,480 --> 00:43:43,920
einlassen  dieses Thema später, aber es gibt

1285
00:43:43,920 --> 00:43:45,240
hier nur eine Seite der Medaille, genau die

1286
00:43:45,240 --> 00:43:47,099
andere Seite der Medaille erklärt, warum

1287
00:43:47,099 --> 00:43:48,780
diese Art von Verhalten und nicht irgendein anderes

1288
00:43:48,780 --> 00:43:50,339
Verhalten, warum diese Struktur mir nicht

1289
00:43:50,339 --> 00:43:52,740
ähnlich ist und das vielleicht Chomskys am meisten ist

1290
00:43:52,740 --> 00:43:55,380
und wie Sie sein am meisten kennen  Wichtiger

1291
00:43:55,380 --> 00:43:56,760
Punkt wirklich, warum dies nicht irgendein anderes

1292
00:43:56,760 --> 00:43:59,400
System ist, so dass die Sprachtheorie

1293
00:43:59,400 --> 00:44:00,720
Ihnen irgendwie das oder den Anfang der Münze

1294
00:44:00,720 --> 00:44:02,760
richtig gibt, während LM wirklich fertig ist, also

1295
00:44:02,760 --> 00:44:03,839
macht der mitchell verlegene Artikel

1296
00:44:03,839 --> 00:44:05,819
so etwas, er macht es

1297
00:44:05,819 --> 00:44:09,420
gut, ja, also nehmen Sie ähm Yael Le  crets

1298
00:44:09,420 --> 00:44:11,400
und stanislash the Haynes hatten ab 2019

1299
00:44:11,400 --> 00:44:13,319
Recht, sie haben sich die Nummernvereinbarung in

1300
00:44:13,319 --> 00:44:15,480
einem LSTM angesehen und zwei spezialisierte Einheiten gefunden,

1301
00:44:15,480 --> 00:44:17,640
die die Nummernvereinbarung codierten, aber der

1302
00:44:17,640 --> 00:44:19,020
Gesamtbeitrag zur Leistung war

1303
00:44:19,020 --> 00:44:21,839
gering, und dann hatten 2021, äh ja, die Korrekten

1304
00:44:21,839 --> 00:44:24,000
dieses Papier, in dem sie zeigen, dass

1305
00:44:24,000 --> 00:44:26,040
ähm drin ist  Ihr neuronales Sprachmodell hat

1306
00:44:26,040 --> 00:44:28,079
keine echte rekursive Verarbeitung

1307
00:44:28,079 --> 00:44:30,540
der verschachtelten Geschlechtskennzeichnung für weitreichende Vereinbarungen

1308
00:44:30,540 --> 00:44:32,160
auf Italienisch erreicht. Ich denke,

1309
00:44:32,160 --> 00:44:34,020
ähm, selbst wenn eine hierarchische Verarbeitung,

1310
00:44:34,020 --> 00:44:35,819
was Sie wissen, erreicht wurde, wie Sie

1311
00:44:35,819 --> 00:44:37,380
zuvor argumentiert haben, war eine gewisse Hierarchie

1312
00:44:37,380 --> 00:44:39,960
übrig, es war da, aber die Frage  Ist

1313
00:44:39,960 --> 00:44:41,280
es die richtige Zuordnung, ist es die richtige

1314
00:44:41,280 --> 00:44:42,900
Art von Hierarchie, fanden sie heraus, dass

1315
00:44:42,900 --> 00:44:45,119
LSTN-basierte Modelle

1316
00:44:45,119 --> 00:44:47,220
über kurze Spannen eine

1317
00:44:47,220 --> 00:44:49,260
Einbettungsstufe erreichen konnten, aber sie scheiterten an einigen längeren

1318
00:44:49,260 --> 00:44:51,359
Abhängigkeiten und in der neuesten

1319
00:44:51,359 --> 00:44:53,700
Veröffentlichung uh La crepe satell with  die Hand

1320
00:44:53,700 --> 00:44:56,760
und zeigten, dass sie moderne

1321
00:44:56,760 --> 00:45:00,180
Transformer LMS einschließlich gpt2 XL bei der

1322
00:45:00,180 --> 00:45:01,980
gleichen Aufgabe bewertet haben und die Transformers

1323
00:45:01,980 --> 00:45:04,260
ähnlicher wie Menschen funktionieren als LSM

1324
00:45:04,260 --> 00:45:06,300
und insgesamt eine überdurchschnittliche Leistung erbracht haben, aber

1325
00:45:06,300 --> 00:45:08,040
in einer

1326
00:45:08,040 --> 00:45:09,660
Schlüsselbedingung, die, wie ich

1327
00:45:09,660 --> 00:45:11,099
erwähnt habe, immer noch unterdurchschnittlich sind  mehrfache Einbettung in die

1328
00:45:11,099 --> 00:45:13,020
schwierigen Schriften, und der Grund,

1329
00:45:13,020 --> 00:45:14,400
warum ich diese Studien erwähnt habe, ist, weil

1330
00:45:14,400 --> 00:45:17,040
Sie wissen, dass es nicht nur darum geht, die

1331
00:45:17,040 --> 00:45:18,540
Grenzen von OMS zu erkunden, was eine interessante Frage ist,

1332
00:45:18,540 --> 00:45:19,500


1333
00:45:19,500 --> 00:45:21,540
sondern dass Sie die Arbeit von Leuten wie Neil

1334
00:45:21,540 --> 00:45:24,180
Smith an der UCL für richtig halten, und er hat darin gearbeitet  in

1335
00:45:24,180 --> 00:45:26,579
den 90er Jahren mit einem polyglotten Savant und

1336
00:45:26,579 --> 00:45:28,740
neurotypischen Kontrollen, die sie verglichen, also

1337
00:45:28,740 --> 00:45:30,540
untersuchte er das Erlernen

1338
00:45:30,540 --> 00:45:32,520
einer künstlichen Sprache in der zweiten Sprache, die

1339
00:45:32,520 --> 00:45:34,500
sowohl natürliche als auch unnatürliche grafische

1340
00:45:34,500 --> 00:45:35,880
Strukturen wie das Michelin-Viruspapier enthielt,

1341
00:45:35,880 --> 00:45:37,079
richtig, das gesamte Framework ist natürlich

1342
00:45:37,079 --> 00:45:39,119
gegen unnatürlich, und sie fanden heraus, dass

1343
00:45:39,119 --> 00:45:41,000
sowohl der Savant

1344
00:45:41,000 --> 00:45:43,319
als auch  die Steuerelemente könnten die

1345
00:45:43,319 --> 00:45:45,480
sprachlich natürlichen Aspekte beherrschen, nur die

1346
00:45:45,480 --> 00:45:46,920
Steuerelemente könnten schließlich die

1347
00:45:46,920 --> 00:45:48,660
strukturabhängigen unnatürlichen Phänomene bewältigen,

1348
00:45:48,660 --> 00:45:50,460
und keine von ihnen könnte die

1349
00:45:50,460 --> 00:45:52,560
strukturunabhängigen Aspekte beherrschen, also einige

1350
00:45:52,560 --> 00:45:53,880
seltsame Regeln, bei denen es so ist, als ob Sie wissen, dass Sie

1351
00:45:53,880 --> 00:45:55,440
die Betonung auf dem dritten Wort

1352
00:45:55,440 --> 00:45:56,940
der Satzsachen setzen  So

1353
00:45:56,940 --> 00:45:58,740
argumentieren sie, dass Christophers Fähigkeiten

1354
00:45:58,740 --> 00:46:00,900
ausschließlich auf seine intakten sprachlichen

1355
00:46:00,900 --> 00:46:03,480
Fähigkeiten zurückzuführen sind, aber die Kontrollen könnten

1356
00:46:03,480 --> 00:46:05,579
mehr Domäne Allgemeine Art von kognitiven

1357
00:46:05,579 --> 00:46:07,319
Ressourcen wie Aufmerksamkeitskontrolle

1358
00:46:07,319 --> 00:46:09,900
usw. einsetzen, weshalb sie

1359
00:46:09,900 --> 00:46:11,520
mit schwierigen Prozessen umgehen könnten,

1360
00:46:11,520 --> 00:46:13,319
aber ich habe gerade erwähnt, dass Sie a wissen

1361
00:46:13,319 --> 00:46:16,079
Vor einer Minute, dass das lstm in dem

1362
00:46:16,079 --> 00:46:18,359
verlegenen Artikel von Mitchell natürliche und

1363
00:46:18,359 --> 00:46:19,920
unnatürliche Strukturen auf ziemlich

1364
00:46:19,920 --> 00:46:22,319
dieselbe Weise angeht, also wissen Sie nicht, dass es kein

1365
00:46:22,319 --> 00:46:24,240
psychologisch plausibles Modell ist, würde ich

1366
00:46:24,240 --> 00:46:26,400
argumentieren, und für alles, was Menschen tun,

1367
00:46:26,400 --> 00:46:28,200
und ähnliche Beobachtungen können auf

1368
00:46:28,200 --> 00:46:30,060
die Grenzen von zutreffen  Transformer-Modelle in La

1369
00:46:30,060 --> 00:46:31,920
Cretas Arbeit und all diese Themen sind

1370
00:46:31,920 --> 00:46:33,780
so gut, dass sie

1371
00:46:33,780 --> 00:46:35,819
uns den ganzen Weg bis in die Gegenwart begleiten, also zeigte eine andere

1372
00:46:35,819 --> 00:46:37,560
von Talins jüngsten Papieren, die er

1373
00:46:37,560 --> 00:46:39,240
vor ein paar Wochen veröffentlichte und die sich mit kindlicher

1374
00:46:39,240 --> 00:46:41,880
Sprache befasste, das ähm  lstms und

1375
00:46:41,880 --> 00:46:43,740
Transformers sind auf ökologisch

1376
00:46:43,740 --> 00:46:46,380
plausible Datenmengen beschränkt, verallgemeinert, da

1377
00:46:46,380 --> 00:46:47,640
ich eher die linearen Regeln für englisches

1378
00:46:47,640 --> 00:46:49,920
Recht als die abstrakten Regeln erwähnt habe, und

1379
00:46:49,920 --> 00:46:51,780
tatsächlich zeigt die neuere Arbeit von Linton's

1380
00:46:51,780 --> 00:46:54,599
Lab letzte Woche, ähm, letztes

1381
00:46:54,599 --> 00:46:56,220
Jahr, ich sollte sagen, dass ein Blick auf

1382
00:46:56,220 --> 00:46:58,160
Garden zeigt  Paths Surprisal erklärt nicht die

1383
00:46:58,160 --> 00:47:01,319
Schwierigkeit der syntaktischen Begriffsklärung,

1384
00:47:01,319 --> 00:47:02,280
richtig

1385
00:47:02,280 --> 00:47:03,900
um Überraschungen wird die Größe

1386
00:47:03,900 --> 00:47:05,400
des Garden Path-Effekts über alle

1387
00:47:05,400 --> 00:47:06,780
Konstruktionen hinweg unterschätzt, und das kommt zu diesem

1388
00:47:06,780 --> 00:47:08,099
Problem, das Sie erwähnt haben, bevor Sie wissen, dass

1389
00:47:08,099 --> 00:47:10,140
dies alles vielleicht überrascht hat, was mit

1390
00:47:10,140 --> 00:47:11,520
einigen Aspekten der Syntax zusammenhängt, aber vielleicht auch nicht  Bei

1391
00:47:11,520 --> 00:47:12,960
anderen ist es eine Art

1392
00:47:12,960 --> 00:47:14,819
Nicht-Tribut-Problem, das sehr

1393
00:47:14,819 --> 00:47:16,800
offen für Diskussionen ist, es ist nicht so, dass es

1394
00:47:16,800 --> 00:47:18,720
noch nicht geklärt ist, aber Linton hat gezeigt,

1395
00:47:18,720 --> 00:47:20,640
dass Garden Path-Effekte

1396
00:47:20,640 --> 00:47:21,720
viel schwieriger sind, als Sie es

1397
00:47:21,720 --> 00:47:24,359
von mir erwarten würden  Unvorhersagbarkeit. Eine andere Art,

1398
00:47:24,359 --> 00:47:26,160
dieses Argument zu formulieren,

1399
00:47:26,160 --> 00:47:29,160
ist das Zitat. Ein kürzlich geführter Streit

1400
00:47:29,160 --> 00:47:30,660
mit Chomsky, um zu dieser natürlichen

1401
00:47:30,660 --> 00:47:32,940
Grundlage zu gelangen. Unnatürliches Problem. Er sagt, nehmen wir an, wir

1402
00:47:32,940 --> 00:47:34,560
haben ein erweitertes Periodensystem, das

1403
00:47:34,560 --> 00:47:36,180
alle Elemente enthält, die existieren,

1404
00:47:36,180 --> 00:47:38,819
oder die Elemente, die möglicherweise vorhanden sind  existieren

1405
00:47:38,819 --> 00:47:40,740
und all die Elemente, die

1406
00:47:40,740 --> 00:47:42,660
unmöglich existieren können, und sagen wir, Sie haben

1407
00:47:42,660 --> 00:47:44,880
ein Modell, äh, ein künstliches Modell, das

1408
00:47:44,880 --> 00:47:46,560
nicht zwischen diesen drei Kategorien unterscheiden kann,

1409
00:47:46,560 --> 00:47:48,780
was auch immer dieses Modell tut, es

1410
00:47:48,780 --> 00:47:50,640
hilft uns nicht, die Chemie richtig zu verstehen,

1411
00:47:50,640 --> 00:47:52,020
es tut etwas anderes,

1412
00:47:52,020 --> 00:47:53,940
es tut etwas für etwas  sicher, aber

1413
00:47:53,940 --> 00:47:55,020
ob es darum geht, Chemie zu verstehen oder nicht,

1414
00:47:55,020 --> 00:47:57,180
ist etwas anderes, und ich

1415
00:47:57,180 --> 00:47:58,560
weiß, dass Sie als Antwort auf

1416
00:47:58,560 --> 00:47:59,579
einige dieser Studien

1417
00:47:59,579 --> 00:48:02,400
gesagt haben, dass Sie es wissen und um zu zeigen,

1418
00:48:02,400 --> 00:48:03,540
dass etwas wahrscheinlich

1419
00:48:03,540 --> 00:48:04,920
irgendwo unmöglich ist  Ich glaube, Sie sagen in Ihrem Aufsatz

1420
00:48:04,920 --> 00:48:06,240


1421
00:48:06,240 --> 00:48:07,859
ähm, um zu zeigen, dass etwas

1422
00:48:07,859 --> 00:48:09,720
mit normaler Ausgewogenheit der

1423
00:48:09,720 --> 00:48:12,300
Politik bei Fehlalarmen unmöglich ist. Sie müssen

1424
00:48:12,300 --> 00:48:13,440
zeigen, dass Sie sich etwa

1425
00:48:13,440 --> 00:48:15,780
500 unabhängig abgetastete Sprachen ansehen müssen, also

1426
00:48:15,780 --> 00:48:17,880
zitieren Sie dies in dem Aufsatz richtig, ähm,

1427
00:48:17,880 --> 00:48:19,020
was Sie  wahrscheinlich nicht, das ist

1428
00:48:19,020 --> 00:48:20,579
einfach nicht möglich,

1429
00:48:20,579 --> 00:48:23,880
und Sie wissen, ich bin mir nicht sicher,

1430
00:48:23,880 --> 00:48:25,800
ob dies wirklich das Hauptargument widerlegt,

1431
00:48:25,800 --> 00:48:27,119
das ich hier vorbringe,

1432
00:48:27,119 --> 00:48:29,040
weil Leute wie Michelin Bowers es

1433
00:48:29,040 --> 00:48:30,839
vorbringen  Ein Argument über die Unmöglichkeit

1434
00:48:30,839 --> 00:48:32,220
im Prinzip nicht

1435
00:48:32,220 --> 00:48:33,599
ähm in einem erweiterten Sinne, den Sie

1436
00:48:33,599 --> 00:48:34,980
kennen, genau wie das Durchsuchen der

1437
00:48:34,980 --> 00:48:37,440
Weltsprachen, um in

1438
00:48:37,440 --> 00:48:38,579
jeder einzelnen Sprache zu beweisen, dass es

1439
00:48:38,579 --> 00:48:40,260
unmöglich ist, richtig, das ist irgendwie ein

1440
00:48:40,260 --> 00:48:41,280
anderes Argument, ob es

1441
00:48:41,280 --> 00:48:43,740
in einer zufälligen Sprache unmöglich ist  im

1442
00:48:43,740 --> 00:48:45,180
Amazonas im Vergleich zu tatsächlich

1443
00:48:45,180 --> 00:48:47,220
unmöglich, basierend auf den Prinzipien dessen, was

1444
00:48:47,220 --> 00:48:48,420
das Sprachsystem tatsächlich

1445
00:48:48,420 --> 00:48:50,160
tut, was es kann, also würde ich

1446
00:48:50,160 --> 00:48:53,339
nur sagen, ja, ich denke,

1447
00:48:53,339 --> 00:48:55,980
dieser Punkt ist, dass Sie nicht wirklich

1448
00:48:55,980 --> 00:48:58,560
wissen, was typologisch nicht ist  möglich

1449
00:48:58,560 --> 00:49:00,480
richtig, also sagen manche Leute gerne Dinge

1450
00:49:00,480 --> 00:49:02,520
wie Sie wissen, dass es keine Sprache gibt, die

1451
00:49:02,520 --> 00:49:04,859
X richtig macht, deshalb müssen wir diese

1452
00:49:04,859 --> 00:49:06,960
Einschränkung in unsere statistischen

1453
00:49:06,960 --> 00:49:09,119
Modelle einbauen, richtig, aber wenn es nicht

1454
00:49:09,119 --> 00:49:11,220
statistisch gerechtfertigt ist, dass es keine

1455
00:49:11,220 --> 00:49:13,079
Sprache gibt, die X richtig macht, wenn Sie haben  Ich habe mir

1456
00:49:13,079 --> 00:49:15,119
nur 20 oder 20 europäische

1457
00:49:15,119 --> 00:49:16,920
Sprachen angesehen oder so, richtig. Ich meine,

1458
00:49:16,920 --> 00:49:19,020
es ist nicht

1459
00:49:19,020 --> 00:49:22,380
ähm, das sollte nicht motivieren,

1460
00:49:22,380 --> 00:49:24,599
etwas mit den Modellen zu tun, richtig

1461
00:49:24,599 --> 00:49:26,280
ähm, wenn es ist, wenn es kein

1462
00:49:26,280 --> 00:49:28,200
statistisch gerechtfertigtes Universal ist  Ich

1463
00:49:28,200 --> 00:49:28,980


1464
00:49:28,980 --> 00:49:30,180


1465
00:49:30,180 --> 00:49:32,760
weiß, dass

1466
00:49:32,760 --> 00:49:33,960
Sie völlig Recht haben, aber das

1467
00:49:33,960 --> 00:49:35,339
gilt allgemeiner für die

1468
00:49:35,339 --> 00:49:36,960
Sozialwissenschaften und die psychologischen Wissenschaften,

1469
00:49:36,960 --> 00:49:39,180
genau wie typologisch. Ja, es ist sehr

1470
00:49:39,180 --> 00:49:40,380
schwierig, diese Dinge richtig zu machen,

1471
00:49:40,380 --> 00:49:43,140
also schätze ich, Sie sind nur

1472
00:49:43,140 --> 00:49:44,640
ein bisschen altbacken  Sie

1473
00:49:44,640 --> 00:49:47,339
sagen, dass die starke Behauptung sehr

1474
00:49:47,339 --> 00:49:49,859
schwer zu beweisen ist,

1475
00:49:49,859 --> 00:49:52,619
wie es keine Sprache gibt, die X hat

1476
00:49:52,619 --> 00:49:54,480


1477
00:49:54,480 --> 00:49:56,339


1478
00:49:56,339 --> 00:49:58,800


1479
00:49:58,800 --> 00:49:59,880


1480
00:49:59,880 --> 00:50:01,980
dass es

1481
00:50:01,980 --> 00:50:05,460
äh viele von Ihnen starke

1482
00:50:05,460 --> 00:50:08,460
Versuche gab, es gab viele starke

1483
00:50:08,460 --> 00:50:10,380
Behauptungen von

1484
00:50:10,380 --> 00:50:12,720
äh äh oft von der generativen Syntax,

1485
00:50:12,720 --> 00:50:16,800
was alle Sprachen tun, äh

1486
00:50:16,800 --> 00:50:19,140
und ich denke, dass Sie wissen, dass die Leute

1487
00:50:19,140 --> 00:50:21,119
sehr gut darin waren,

1488
00:50:21,119 --> 00:50:22,740
Gegenbeispiele zu finden  Zu vielen dieser

1489
00:50:22,740 --> 00:50:24,720
Dinge zitiere ich diesen Aufsatz von Evans

1490
00:50:24,720 --> 00:50:26,579
und Levinson,

1491
00:50:26,579 --> 00:50:28,500
ähm, wissen Sie eigentlich, ich habe

1492
00:50:28,500 --> 00:50:30,660
jahrelang davon gehört, dass keine Sprache X macht,

1493
00:50:30,660 --> 00:50:32,160
und das verwenden wir, um

1494
00:50:32,160 --> 00:50:33,599
unsere Theorien zu konstruieren, und die von Evans

1495
00:50:33,599 --> 00:50:35,460
und Levin  Papier Evans und Levinson

1496
00:50:35,460 --> 00:50:37,859
Papier haben wirklich meine Meinung

1497
00:50:37,859 --> 00:50:40,680
darüber geändert, dass Sprache

1498
00:50:40,680 --> 00:50:43,260
tatsächlich viel vielfältiger ist als

1499
00:50:43,260 --> 00:50:44,940
ich denke, die

1500
00:50:44,940 --> 00:50:47,760
meisten Syntaktiker werden versuchen,

1501
00:50:47,760 --> 00:50:50,700
Theorien für etwas zu konstruieren, also ähm,

1502
00:50:50,700 --> 00:50:53,579
wissen Sie, ich, ich, ich denke, wir gehen  Zurück

1503
00:50:53,579 --> 00:50:54,839
zum Anfang von dem, was Sie

1504
00:50:54,839 --> 00:50:57,660
gesagt haben. Ich denke, wir sind uns einig, dass

1505
00:50:57,660 --> 00:50:59,880
Sie Spracharchitekturen brauchen, die

1506
00:50:59,880 --> 00:51:01,619
die Dinge lernen, die Kinder lernen, und die

1507
00:51:01,619 --> 00:51:03,660
es aus Daten lernen, die sie lernen, und

1508
00:51:03,660 --> 00:51:05,940
diese Architekturen sind möglicherweise

1509
00:51:05,940 --> 00:51:09,000
keine Dinge  wie lstms oder du

1510
00:51:09,000 --> 00:51:10,559
kennst einfache rekurrente Netzwerke oder

1511
00:51:10,559 --> 00:51:12,300
was auch immer,

1512
00:51:12,300 --> 00:51:14,400
ich denke, all diese Arbeit ist sehr

1513
00:51:14,400 --> 00:51:16,680
nützlich, um die

1514
00:51:16,680 --> 00:51:19,200
richtige Architektur zu verfeinern,

1515
00:51:19,200 --> 00:51:20,460
ähm,

1516
00:51:20,460 --> 00:51:21,420


1517
00:51:21,420 --> 00:51:23,819
also versuche ich nur, mich an alles zu erinnern

1518
00:51:23,819 --> 00:51:25,200
Die Punkte, die Sie gemacht haben,

1519
00:51:25,200 --> 00:51:26,700
oh ja, ähm,

1520
00:51:26,700 --> 00:51:29,160
aber ich denke, dass es

1521
00:51:29,160 --> 00:51:31,500
eine Art Kehrseite dazu gibt, nämlich dass ähm,

1522
00:51:31,500 --> 00:51:32,760


1523
00:51:32,760 --> 00:51:34,380
ich denke, dass der Raum der Dinge, die

1524
00:51:34,380 --> 00:51:37,619
Menschen lernen können, tatsächlich irgendwie

1525
00:51:37,619 --> 00:51:39,599
unterschätzt wird, genau wie es diese

1526
00:51:39,599 --> 00:51:41,760
Voreingenommenheit gibt  to to zu sagen, Sie wissen, dass Menschen

1527
00:51:41,760 --> 00:51:43,800
x, y und z

1528
00:51:43,800 --> 00:51:46,020
um nicht lernen können, aber Menschen, äh, zumindest außerhalb der

1529
00:51:46,020 --> 00:51:47,460
Sprache, haben diese wirklich

1530
00:51:47,460 --> 00:51:49,680
bemerkenswerte Fähigkeit, verschiedene

1531
00:51:49,680 --> 00:51:51,240
Arten von Mustern zu lernen, genau wie die

1532
00:51:51,240 --> 00:51:53,160
Muster, die Sie zum Beispiel in Musik oder Mathematik finden,

1533
00:51:53,160 --> 00:51:55,619


1534
00:51:55,619 --> 00:51:57,839
ähm  Wir können ausgefeilte Arten

1535
00:51:57,839 --> 00:52:00,240
von Algorithmen lernen, wir können lernen, wie man

1536
00:52:00,240 --> 00:52:03,119
ein Space Shuttle fliegt oder wie man

1537
00:52:03,119 --> 00:52:05,760
Knoten zum Klettern bindet oder

1538
00:52:05,760 --> 00:52:07,319
was auch immer, da gibt es alle

1539
00:52:07,319 --> 00:52:09,540
Arten von prozeduralem und

1540
00:52:09,540 --> 00:52:11,280
algorithmischem Wissen, das

1541
00:52:11,280 --> 00:52:13,440
strukturell ist  Menschen können sich etwas

1542
00:52:13,440 --> 00:52:16,680
aneignen, und ich denke, dass diese äh

1543
00:52:16,680 --> 00:52:19,680
Vorstellung äh sehr zu Recht dazu motiviert, nach

1544
00:52:19,680 --> 00:52:21,359
Lernsystemen zu suchen, die

1545
00:52:21,359 --> 00:52:24,059
in ziemlich uneingeschränkten Räumen funktionieren können, also ähm,

1546
00:52:24,059 --> 00:52:26,160


1547
00:52:26,160 --> 00:52:28,859
Sie kennen Sie, Sie könnten sagen,

1548
00:52:28,859 --> 00:52:30,119
dass Sprache anders ist,

1549
00:52:30,119 --> 00:52:33,420
weil Sprache anders ist  ein begrenzter Raum

1550
00:52:33,420 --> 00:52:35,040
ähm und es mag wahr sein, dass diese

1551
00:52:35,040 --> 00:52:36,540
Sprache eingeschränkt ist, aber es könnte auch

1552
00:52:36,540 --> 00:52:37,859
wahr sein, dass die Dinge, die wir in

1553
00:52:37,859 --> 00:52:39,960
Sprache sehen, aus anderen Quellen stammen, richtig,

1554
00:52:39,960 --> 00:52:42,180
es könnte sein, dass äh Sprache

1555
00:52:42,180 --> 00:52:44,099
besonders pragmatisch ist, zum Beispiel im

1556
00:52:44,099 --> 00:52:47,040
Vergleich zu äh Musik oder Mathematik

1557
00:52:47,040 --> 00:52:48,720
Recht und diese Art von pragmatischen

1558
00:52:48,720 --> 00:52:50,040
Zwängen

1559
00:52:50,040 --> 00:52:51,480
sind die Dinge, die die

1560
00:52:51,480 --> 00:52:53,400
Form der Sprache einschränken Recht oder Sprache ist

1561
00:52:53,400 --> 00:52:54,660
kommunikativ, sie ist wahrscheinlich

1562
00:52:54,660 --> 00:52:56,760
kommunikativer als zum

1563
00:52:56,760 --> 00:52:58,440
Beispiel Musik und das könnte die

1564
00:52:58,440 --> 00:53:01,140
Form der Dinge einschränken, also meine ich, wie Sie wissen,

1565
00:53:01,140 --> 00:53:02,700
ist das sehr  alte Debatte in der

1566
00:53:02,700 --> 00:53:05,520
Linguistik darüber, woher das äh,

1567
00:53:05,520 --> 00:53:07,140
woher die Eigenschaften der natürlichen

1568
00:53:07,140 --> 00:53:09,059
Sprache kommen, ähm

1569
00:53:09,059 --> 00:53:11,160
und ähm, ich denke, ich versuche zu sagen,

1570
00:53:11,160 --> 00:53:12,660
dass es eine Art Perspektive gibt,

1571
00:53:12,660 --> 00:53:15,119
wo man sich all die Dinge anschaut, die

1572
00:53:15,119 --> 00:53:16,980
Menschen können  tun sogar außerhalb der Sprache

1573
00:53:16,980 --> 00:53:18,599
all die reichen Strukturen und

1574
00:53:18,599 --> 00:53:20,819
Algorithmen und Prozesse, die wir lernen

1575
00:53:20,819 --> 00:53:23,760
und verinnerlichen konnten, und

1576
00:53:23,760 --> 00:53:25,559
Sie sagen, okay, vielleicht ist Sprache so

1577
00:53:25,559 --> 00:53:27,420
und dann ja, Sprache hat auch einige

1578
00:53:27,420 --> 00:53:29,700
dieser anderen lustigen kleinen Eigenschaften,

1579
00:53:29,700 --> 00:53:31,380
ähm, aber Sie kennen vielleicht diese  kommen von

1580
00:53:31,380 --> 00:53:34,020
einigen anderen Teilen, wo

1581
00:53:34,020 --> 00:53:36,720
Sprache herkommt, richtig, es ist äh,

1582
00:53:36,720 --> 00:53:38,400
du weißt, wir haben ziemlich ausgefeilte

1583
00:53:38,400 --> 00:53:40,800
pragmatische Argumente,

1584
00:53:40,800 --> 00:53:42,720
ähm, wir verwenden es, um bestimmte

1585
00:53:42,720 --> 00:53:45,359
kommunikative Ziele zu erreichen

1586
00:53:45,359 --> 00:53:47,160


1587
00:53:47,160 --> 00:53:49,559
Sprachsystem selbst

1588
00:53:49,559 --> 00:53:51,180
und so sind vielleicht einige dieser anderen

1589
00:53:51,180 --> 00:53:53,819
Eigenschaften Eigenschaften, die

1590
00:53:53,819 --> 00:53:55,619
einen anderen Ursprung haben,

1591
00:53:55,619 --> 00:53:57,059
und dass diese Ansicht meiner Meinung nach falsch sein könnte,

1592
00:53:57,059 --> 00:53:59,579
aber es ist eine, von der

1593
00:53:59,579 --> 00:54:01,800
ich denke, dass sie überprüft werden muss, um zu sehen,

1594
00:54:01,800 --> 00:54:04,200
ob sie falsch ist richtig wie  Ich denke, es

1595
00:54:04,200 --> 00:54:05,400
wurde ähm

1596
00:54:05,400 --> 00:54:10,020
irgendwie abgetan von ähm großen

1597
00:54:10,020 --> 00:54:12,900
Teilen von Linguisten, genau,

1598
00:54:12,900 --> 00:54:14,579
wissen Sie, ich habe gehört, wie Leute Sachen sagen wie na

1599
00:54:14,579 --> 00:54:15,720
ja, Kommunikation erklärt nicht wirklich

1600
00:54:15,720 --> 00:54:17,760
etwas über die richtige Sprache,

1601
00:54:17,760 --> 00:54:20,040
und was sie oft meinen, ist, dass es nicht so ist '

1602
00:54:20,040 --> 00:54:22,200
Erklären Sie nicht die besonderen

1603
00:54:22,200 --> 00:54:23,760
Inselbeschränkungen oder etwas, an dem sie

1604
00:54:23,760 --> 00:54:25,319
gerade arbeiten, aber es

1605
00:54:25,319 --> 00:54:26,700
gibt alle möglichen anderen Dinge in der

1606
00:54:26,700 --> 00:54:28,319
Sprache, die der kommunikative Druck

1607
00:54:28,319 --> 00:54:30,359
wahrscheinlich erklärt

1608
00:54:30,359 --> 00:54:31,500


1609
00:54:31,500 --> 00:54:33,540


1610
00:54:33,540 --> 00:54:36,359
in

1611
00:54:36,359 --> 00:54:39,119
Bezug auf die Begriffsbreite in Anbetracht der Kräfte, die die

1612
00:54:39,119 --> 00:54:41,460
Sprache formen können und nicht

1613
00:54:41,460 --> 00:54:43,680
alles in irgendeine Form von angeborenen

1614
00:54:43,680 --> 00:54:45,359
Einschränkungen oder ähnlichem bringen zu müssen, nein,

1615
00:54:45,359 --> 00:54:46,680
ganz und ich denke, ich denke, dass vieles von

1616
00:54:46,680 --> 00:54:48,420
diesem Zeug mit ihnen kompatibel ist

1617
00:54:48,420 --> 00:54:49,980
Krankheitsprogramm,

1618
00:54:49,980 --> 00:54:51,960
weil die Mitte dieses Programms eine

1619
00:54:51,960 --> 00:54:53,339
minimale Syntax haben will, es will nicht

1620
00:54:53,339 --> 00:54:54,660
kompliziert sein, es will nicht, dass es

1621
00:54:54,660 --> 00:54:56,220
noch komplizierter ist, es muss

1622
00:54:56,220 --> 00:54:57,960
sein, also gab es einige, die Sie richtig erwähnt haben,

1623
00:54:57,960 --> 00:54:59,460
die Curious-Eigenschaften  Es

1624
00:54:59,460 --> 00:55:00,480
gibt also einige der Eigenschaften, die

1625
00:55:00,480 --> 00:55:02,579
in jedem Sprachmodell berücksichtigt werden müssen,

1626
00:55:02,579 --> 00:55:04,200
ähm, ich gebe Ihnen ein

1627
00:55:04,200 --> 00:55:05,400
Beispiel, direkt die Einstellung von Personenmerkmalen,

1628
00:55:05,400 --> 00:55:06,599


1629
00:55:06,599 --> 00:55:08,880
und diese Personenmerkmale weisen sehr

1630
00:55:08,880 --> 00:55:10,440
nicht triviale unterschiedliche Verallgemeinerungen auf,

1631
00:55:10,440 --> 00:55:12,480
die nicht so scheinen  werden für Via-

1632
00:55:12,480 --> 00:55:14,220
Domäne berücksichtigt. Allgemeiner Lernmechanismus, also

1633
00:55:14,220 --> 00:55:16,020
sitze ich hier, die Arbeit von Daniel Harper

1634
00:55:16,020 --> 00:55:17,700
bei Queen Mary, also zum Beispiel die

1635
00:55:17,700 --> 00:55:19,740
morphologische Zusammensetzung der Person, ihre

1636
00:55:19,740 --> 00:55:21,720
Interaktion mit der Zahl, ihre Verbindung

1637
00:55:21,720 --> 00:55:24,059
zum Raum, äh, Eigenschaften ihrer Semantik

1638
00:55:24,059 --> 00:55:26,040
und ihre Linearisierung, die sie alle zu haben scheinen

1639
00:55:26,040 --> 00:55:27,240
starke Kandidaten für unsere

1640
00:55:27,240 --> 00:55:28,619
Sprachkenntnisse sein, genau das, was wir

1641
00:55:28,619 --> 00:55:30,359
unter Sprachkenntnissen verstehen, aber auf der

1642
00:55:30,359 --> 00:55:32,280
anderen Seite haben wir Dinge wie Fall und

1643
00:55:32,280 --> 00:55:34,319
Übereinstimmung und Kopfbewegung, und das

1644
00:55:34,319 --> 00:55:36,420
sind alles strukturelle Phänomene, aber

1645
00:55:36,420 --> 00:55:39,319
sie scheinen sich einer rein

1646
00:55:39,319 --> 00:55:42,420
bedeutungsbasierten Erklärung zu widersetzen, ähm

1647
00:55:42,420 --> 00:55:44,339
Theoretische Linguistik, richtig, es

1648
00:55:44,339 --> 00:55:45,839
wäre großartig, wenn die Syntax nichts anderes wäre als eine

1649
00:55:45,839 --> 00:55:47,520
Rechenmaschine, die

1650
00:55:47,520 --> 00:55:49,440
strukturierte Bedeutungen aufbaut, und das ist das

1651
00:55:49,440 --> 00:55:51,240
minimalistische Programm das Ziel, aber das ist

1652
00:55:51,240 --> 00:55:52,800
nicht das, was wir tatsächlich finden, das ist in keinem

1653
00:55:52,800 --> 00:55:54,720
wirklich minimalistischen wie konkreten

1654
00:55:54,720 --> 00:55:57,240
Modell, sondern in irgendeiner konkreten mineralischen Theorie

1655
00:55:57,240 --> 00:55:59,040
ist das Ziel nur  wie das Programm ist die

1656
00:55:59,040 --> 00:56:01,260
Sprache ist perfekt okay das ist das

1657
00:56:01,260 --> 00:56:03,119
Programm das was wir finden nein

1658
00:56:03,119 --> 00:56:05,160
offensichtlich nicht okay nein kein Linguist

1659
00:56:05,160 --> 00:56:07,680
glaubt das tatsächlich und daher wäre es

1660
00:56:07,680 --> 00:56:09,900
großartig, wenn die Syntax so wäre, aber ich

1661
00:56:09,900 --> 00:56:11,280
denke, Sie wissen, dass das Programm

1662
00:56:11,280 --> 00:56:13,740
nach Perfektion suchen soll, aber  nicht immer findet man so

1663
00:56:13,740 --> 00:56:15,839
eine stimme und kopfbewegungen sind

1664
00:56:15,839 --> 00:56:17,640
morphologisch eher für phonologisch

1665
00:56:17,640 --> 00:56:19,319
phänomenal die eigenschaften der

1666
00:56:19,319 --> 00:56:20,700
aufführungssysteme was man

1667
00:56:20,700 --> 00:56:22,440
aufführungssysteme nennt und so

1668
00:56:22,440 --> 00:56:23,760
ist das minimalistische programm an sich wirklich

1669
00:56:23,760 --> 00:56:24,839
kompatibel mit vielem was man

1670
00:56:24,839 --> 00:56:26,880
über sprache sprache sagt

1671
00:56:26,880 --> 00:56:28,500
Es gibt Aspekte der Sprache, die

1672
00:56:28,500 --> 00:56:31,200
für kommunikative Effizienz perfektioniert und optimiert werden können,

1673
00:56:31,200 --> 00:56:32,700


1674
00:56:32,700 --> 00:56:35,520
absolut kein Zweifel, aber wo ist

1675
00:56:35,520 --> 00:56:38,160
dieser Ort der Effizienz in der

1676
00:56:38,160 --> 00:56:39,960
Syntax selbst oder ist es eine Art

1677
00:56:39,960 --> 00:56:42,000
zusätzliches sprachliches System in der

1678
00:56:42,000 --> 00:56:43,680
Pragmatik, die Sie kennen?  es in der

1679
00:56:43,680 --> 00:56:45,540
sensomotorik ist es in der sprache

1680
00:56:45,540 --> 00:56:47,640
ähm wahrscheinlich die sprache und die phonologie

1681
00:56:47,640 --> 00:56:50,400
wahrscheinlich weißt du ich meine wer weiß aber ich

1682
00:56:50,400 --> 00:56:52,740
denke viele dieser dinge erfordern viel

1683
00:56:52,740 --> 00:56:56,059
mehr ernsthafte überlegungen zu

1684
00:56:56,059 --> 00:56:57,839
altmodischen begriffen wie

1685
00:56:57,839 --> 00:56:59,700
strukturabhängigkeit kompositionsthema was hast

1686
00:56:59,700 --> 00:57:01,140
du  Dinge wie die, die Sie vielleicht

1687
00:57:01,140 --> 00:57:03,720
irgendwo in der Literatur finden können, aber ähm, auch

1688
00:57:03,720 --> 00:57:06,720
nur grundlegende Themen wie Sie wissen,

1689
00:57:06,720 --> 00:57:08,640
Quantifizierer, die erweiterte Projektionen anheben,

1690
00:57:08,640 --> 00:57:09,900


1691
00:57:09,900 --> 00:57:12,059
ähm, adverbiale wie adverbiale Hierarchien,

1692
00:57:12,059 --> 00:57:13,920
all diese Dinge im minimalistischen

1693
00:57:13,920 --> 00:57:16,619
Programm können außersprachlich sein, richtig,

1694
00:57:16,619 --> 00:57:18,180
sie können tatsächlich außerhalb sein  Syntax

1695
00:57:18,180 --> 00:57:20,579
und Abfrage sehr seltsame Eigenschaften der

1696
00:57:20,579 --> 00:57:23,099
semantischen äh konzeptuellen Systeme, die

1697
00:57:23,099 --> 00:57:24,559
an sich eine Art Domäne sind. Allgemeine

1698
00:57:24,559 --> 00:57:27,420
seltsame Überbleibsel aus der alten primären

1699
00:57:27,420 --> 00:57:29,220
Kognition, genau die Merkmale der Art und

1700
00:57:29,220 --> 00:57:31,260
Weise, wie wir Ereignisse weitergeben

1701
00:57:31,260 --> 00:57:32,520


1702
00:57:32,520 --> 00:57:33,960
Das ist nicht menschenspezifisch,

1703
00:57:33,960 --> 00:57:35,520


1704
00:57:35,520 --> 00:57:37,260
aber Sie wissen, wie die Syntax

1705
00:57:37,260 --> 00:57:39,059
diesen Systemen Anweisungen gibt, von denen

1706
00:57:39,059 --> 00:57:42,480
Sie wissen, dass sie richtig zu sein scheinen, also

1707
00:57:42,480 --> 00:57:43,800
wissen Sie, dass generative Linguisten

1708
00:57:43,800 --> 00:57:45,839
auch unterschiedliche Theorien zur Sprachproduktion haben.

1709
00:57:45,839 --> 00:57:47,040
Ich werde nur über die Sprachproduktion sprechen,

1710
00:57:47,040 --> 00:57:49,380
basierend darauf, ob wir Lemmata speichern  oder

1711
00:57:49,380 --> 00:57:51,240
ob wir Wörter genauso aufbauen wie

1712
00:57:51,240 --> 00:57:52,619
Phrasen und Sätze, damit ich

1713
00:57:52,619 --> 00:57:53,819
weiß, dass Sie

1714
00:57:53,819 --> 00:57:55,440
zwischen Konstruktionsgrammatik und einer Art

1715
00:57:55,440 --> 00:57:57,180
generativer Grammatik unterscheiden, und Sie wissen, welches

1716
00:57:57,180 --> 00:57:58,680
Gewicht sie auf das Auswendiglernen von

1717
00:57:58,680 --> 00:58:00,119
Engpässen legen, während es nur darum geht,

1718
00:58:00,119 --> 00:58:01,559
Dinge aus ihnen aufzubauen  von Grund auf von Grund

1719
00:58:01,559 --> 00:58:04,440
auf richtig und Sie wissen, dass in einigen

1720
00:58:04,440 --> 00:58:06,480
generativen inspirierten Modellen Mechanismen,

1721
00:58:06,480 --> 00:58:08,460
die eine syntaktische Struktur erzeugen,

1722
00:58:08,460 --> 00:58:10,200
keinen Unterschied zwischen Prozessen machen, die

1723
00:58:10,200 --> 00:58:12,200
über oder unter der Wortebene angewendet werden.

1724
00:58:12,200 --> 00:58:14,640
Es gibt keinen Hinweis darauf, welche Bedeutung Syntax

1725
00:58:14,640 --> 00:58:16,500
und Form alle zusammen in einem

1726
00:58:16,500 --> 00:58:18,660
einzigen Atomic gespeichert sind  Repräsentationen Jede Phase

1727
00:58:18,660 --> 00:58:20,099
des lexikalischen Zugriffs ist ein Übergang

1728
00:58:20,099 --> 00:58:21,960
zwischen verschiedenen Arten von

1729
00:58:21,960 --> 00:58:23,760
Datenstrukturen, genau das bedeutet, dass es eine

1730
00:58:23,760 --> 00:58:25,740
Form und eine Syntax gibt. Diese drei

1731
00:58:25,740 --> 00:58:27,480
Merkmale vermischen sich irgendwie miteinander und

1732
00:58:27,480 --> 00:58:28,920
sie überschneiden sich nicht immer. Verschiedene

1733
00:58:28,920 --> 00:58:31,440
Sprachen realisieren sie auf unterschiedliche Weise,

1734
00:58:31,440 --> 00:58:34,800
und Sie wissen es also  Eine seltsame grundlegende

1735
00:58:34,800 --> 00:58:36,839
Definition eines Wortes ist einfach diese seltsame

1736
00:58:36,839 --> 00:58:39,720
Multi-System-Definition und wo

1737
00:58:39,720 --> 00:58:41,040
viele Dinge, viele verschiedene kognitive

1738
00:58:41,040 --> 00:58:42,900
Systeme die Basis jedes

1739
00:58:42,900 --> 00:58:44,940
elektrischen Gegenstands bereichern, richtig, Sie haben,

1740
00:58:44,940 --> 00:58:46,680
ähm, so etwas gibt es nicht wirklich, diesen

1741
00:58:46,680 --> 00:58:48,299
Anreicherungsprozess,

1742
00:58:48,299 --> 00:58:50,099
ähm, irgendwo anders drin  Sprachtheorie

1743
00:58:50,099 --> 00:58:52,200
richtig oder zumindest was llms so machen,

1744
00:58:52,200 --> 00:58:53,220


1745
00:58:53,220 --> 00:58:55,859
also denke ich, was ich schätze, ich frage

1746
00:58:55,859 --> 00:58:58,799
Sie, was ist Ihre Definition eines Wortes

1747
00:58:58,799 --> 00:59:01,559
richtig und was können llms wirklich

1748
00:59:01,559 --> 00:59:03,780
Einblicke in das Wort Hood rechts geben, denn wenn

1749
00:59:03,780 --> 00:59:04,680
Sie irgendwie, wenn Sie  Wenn Sie kein

1750
00:59:04,680 --> 00:59:06,720
Ziel haben, was ein Wort ist, dann

1751
00:59:06,720 --> 00:59:07,920
sind Sie wirklich in Schwierigkeiten, genau wie wir

1752
00:59:07,920 --> 00:59:10,319
zumindest LMS oder künstliche

1753
00:59:10,319 --> 00:59:12,780
Systeme verwenden müssen, um zu informieren, was wir mit einem Wort meinen,

1754
00:59:12,780 --> 00:59:14,400
oder vielleicht brauchen wir das nicht mehr

1755
00:59:14,400 --> 00:59:16,859
nicht sicher was denkst du ich bin nicht ich bin mir

1756
00:59:16,859 --> 00:59:18,599
nicht sicher was du meinst ich meine äh äh ich ich

1757
00:59:18,599 --> 00:59:20,339


1758
00:59:20,339 --> 00:59:24,059
habe kein was ist ein wort warum ist das wichtig

1759
00:59:24,059 --> 00:59:25,740
ich meine dass das

1760
00:59:25,740 --> 00:59:27,359
nur eine konvention darüber ist wie wir das verwenden

1761
00:59:27,359 --> 00:59:29,819
Begriff Wort richtig, was

1762
00:59:29,819 --> 00:59:31,920
ich meine, Sie könnten Lemmata oder

1763
00:59:31,920 --> 00:59:34,440
Wortfirmen oder was auch immer verwenden, das

1764
00:59:34,440 --> 00:59:35,819
fühlt sich einfach wie eine konventionelle Wahl an

1765
00:59:35,819 --> 00:59:38,040


1766
00:59:38,040 --> 00:59:39,180


1767
00:59:39,180 --> 00:59:41,880
Sagen Sie, ich

1768
00:59:41,880 --> 00:59:43,920
stimme zu, dass Wort eine Konventionalisierung ist. Sie

1769
00:59:43,920 --> 00:59:45,599
wissen, dass Symbole kein intuitives Konzept sind,

1770
00:59:45,599 --> 00:59:47,819
wo es oft durch die Orthographie voreingenommen ist,

1771
00:59:47,819 --> 00:59:50,760
wie wir Leerzeichen richtig setzen, so

1772
00:59:50,760 --> 00:59:52,559
dass ich dieser Kritik zustimme. Sie

1773
00:59:52,559 --> 00:59:54,240
wissen, dass Wort im intuitiven Sinne nicht

1774
00:59:54,240 --> 00:59:56,339
wirklich wissenschaftlich ist  Konstruieren, aber ich

1775
00:59:56,339 --> 00:59:58,260
schätze, lassen Sie mich meine Frage umformulieren, wie Sie

1776
00:59:58,260 --> 01:00:00,359
das

1777
01:00:00,359 --> 01:00:02,160
intuitive Konzept des Wortes in etwas zerlegen würden, das

1778
01:00:02,160 --> 01:00:03,660


1779
01:00:03,660 --> 01:00:04,740
wissenschaftlich zugänglicher oder

1780
01:00:04,740 --> 01:00:06,540
psychologisch plausibler ist

1781
01:00:06,540 --> 01:00:08,280


1782
01:00:08,280 --> 01:00:10,559


1783
01:00:10,559 --> 01:00:12,599
Unterscheidungsmerkmale äh morphologische

1784
01:00:12,599 --> 01:00:15,180
Kategorien konzeptionelle Wurzeln, die

1785
01:00:15,180 --> 01:00:17,579
mit kategorialen Merkmalen verschmolzen werden Sie wissen, dass Sie

1786
01:00:17,579 --> 01:00:20,700
ein Konzept erhalten, das Sie kennen, und Sie haben

1787
01:00:20,700 --> 01:00:22,440
es mit einem Substantiv oder einer Kategorie geschafft, ein

1788
01:00:22,440 --> 01:00:24,119
Substantiv oder eine Affäre zu erhalten. Diese verschiedenen Modelle

1789
01:00:24,119 --> 01:00:26,220
machen unterschiedliche Vorhersagen, ja, ich

1790
01:00:26,220 --> 01:00:28,859
meine, ich denke  Diese allgemeine Idee ist

1791
01:00:28,859 --> 01:00:30,839
wahrscheinlich richtig für große

1792
01:00:30,839 --> 01:00:32,880
Sprachmodelle, wie ich denke, dass sie

1793
01:00:32,880 --> 01:00:34,380
Dinge haben müssen, die Art

1794
01:00:34,380 --> 01:00:36,660
von Wortartkategorien sind, zum Beispiel

1795
01:00:36,660 --> 01:00:38,819
ähm, und ich denke, dass sie in der

1796
01:00:38,819 --> 01:00:42,900
Lage sein müssen, diese Kategorien zu aktualisieren

1797
01:00:42,900 --> 01:00:45,240
basierend auf der Sprache, die

1798
01:00:45,240 --> 01:00:48,119
sie bisher richtig gesehen haben, also wie

1799
01:00:48,119 --> 01:00:50,520
Sie wissen, setzt GPT Substantive und Verben an die

1800
01:00:50,520 --> 01:00:53,400
richtigen Stellen, und um dies zu tun,

1801
01:00:53,400 --> 01:00:55,319
benötigen Sie eine gewisse Darstellung der Substantive

1802
01:00:55,319 --> 01:00:57,000
im Vergleich zu den Verben, und Sie benötigen eine gewisse

1803
01:00:57,000 --> 01:01:00,839
Fähigkeit, äh  äh, finden Sie sich in einer

1804
01:01:00,839 --> 01:01:02,579
Reihe anderer Wörter und finden Sie heraus, ob

1805
01:01:02,579 --> 01:01:04,680
es wahrscheinlich ein Substantiv oder ein Verb

1806
01:01:04,680 --> 01:01:05,400
als nächstes gibt,

1807
01:01:05,400 --> 01:01:07,559
also denke ich, dass auf dieser

1808
01:01:07,559 --> 01:01:09,319
Ebene diese Art von Eigenschaften von Wörtern

1809
01:01:09,319 --> 01:01:12,480
äh sehr wahrscheinlich richtig und

1810
01:01:12,480 --> 01:01:15,780
vorhanden sind  Es gibt auch Dinge, die

1811
01:01:15,780 --> 01:01:17,640
sehr wahrscheinlich in

1812
01:01:17,640 --> 01:01:19,619
den internen Darstellungen dieser

1813
01:01:19,619 --> 01:01:21,420
Modelle zu finden sind. Ich sehe nicht, wie es anders sein könnte,

1814
01:01:21,420 --> 01:01:24,180
als das,

1815
01:01:24,180 --> 01:01:26,339
aber so weit ich weiß,

1816
01:01:26,339 --> 01:01:29,579
ist das so  nicht wo das äh äh das ist nicht

1817
01:01:29,579 --> 01:01:31,500
wo die Hauptdebatten oder

1818
01:01:31,500 --> 01:01:35,220
Meinungsverschiedenheiten sind Ich denke, es ist richtig wie äh äh

1819
01:01:35,220 --> 01:01:38,280
ja Ich denke, alle

1820
01:01:38,280 --> 01:01:40,079
Sprachtheorien müssen sagen, dass es

1821
01:01:40,079 --> 01:01:41,400
verschiedene Arten von Wörtern gibt, die

1822
01:01:41,400 --> 01:01:43,140
an verschiedenen Stellen auftauchen können oder

1823
01:01:43,140 --> 01:01:44,339
so

1824
01:01:44,339 --> 01:01:45,240
Ähm, ja,

1825
01:01:45,240 --> 01:01:47,160
okay, wie sieht es mit dem Thema aus, von dem Sie wissen, dass Sie

1826
01:01:47,160 --> 01:01:49,020
Kommunikation richtig erwähnt haben,

1827
01:01:49,020 --> 01:01:51,299
ähm, also wissen Sie, und Sie haben völlig Recht,

1828
01:01:51,299 --> 01:01:53,099
wenn Trump sagt, dass Dinge wie Sprache

1829
01:01:53,099 --> 01:01:54,960
ein Denksystem sind oder Sie wissen, dass sich Sprache

1830
01:01:54,960 --> 01:01:57,780
nicht weiterentwickelt hat, er ist irgendwie ein

1831
01:01:57,780 --> 01:01:58,799
bisschen  Frech, er meint nicht wirklich,

1832
01:01:58,799 --> 01:02:00,180
dass er es in einem ganz bestimmten

1833
01:02:00,180 --> 01:02:01,619
Sinne meint, richtig

1834
01:02:01,619 --> 01:02:03,540
ähm, aber wissen Sie, wenn wir sagen, dass Sprache

1835
01:02:03,540 --> 01:02:05,640
ein Denksystem ist, meinen wir,

1836
01:02:05,640 --> 01:02:06,780
ähm, wir versuchen, ihm einen

1837
01:02:06,780 --> 01:02:08,339
architektonischen Anspruch zu geben, also wenn Sie sich das ansehen

1838
01:02:08,339 --> 01:02:09,599
Die Architektur des minimalistischen

1839
01:02:09,599 --> 01:02:10,380
Programms,

1840
01:02:10,380 --> 01:02:12,240
die syntaktische Ableitung und die

1841
01:02:12,240 --> 01:02:13,740
konzeptionellen Systeme sind buchstäblich

1842
01:02:13,740 --> 01:02:15,960
unterschiedliche Systeme, richtig, die konzeptionellen

1843
01:02:15,960 --> 01:02:18,180
Systeme nehmen Sachen aus der Syntax und machen sich dann

1844
01:02:18,180 --> 01:02:19,799
damit selbst vertraut, und die CI-

1845
01:02:19,799 --> 01:02:21,900
Systeme haben ihre eigenen besonderen Regeln

1846
01:02:21,900 --> 01:02:23,700
und Prinzipien, weshalb ich

1847
01:02:23,700 --> 01:02:25,319
dachte  In der Sprache sind beide ähnliche

1848
01:02:25,319 --> 01:02:27,000
symbolische Kompositionssysteme, aber auf

1849
01:02:27,000 --> 01:02:29,160
unterschiedliche Weise wird nur eine Teilmenge des Denkens

1850
01:02:29,160 --> 01:02:32,339
richtig als das CI-Schnittstellensystem bezeichnet,

1851
01:02:32,339 --> 01:02:34,619
da die CI-Systeme per

1852
01:02:34,619 --> 01:02:36,480
Definition alle konzeptionellen

1853
01:02:36,480 --> 01:02:38,819
Systeme sind, die Menschen haben, die nicht auf

1854
01:02:38,819 --> 01:02:40,859
Anweisungen aus der Syntax zugreifen und diese auslesen können  und wir

1855
01:02:40,859 --> 01:02:42,540
wissen nicht, was sie genau sind, sie scheinen

1856
01:02:42,540 --> 01:02:43,920
etwas mit Ereignissen in

1857
01:02:43,920 --> 01:02:45,540
grammatikalischer Referenz und Bestimmtheit zu tun zu haben,

1858
01:02:45,540 --> 01:02:47,099
sie scheinen die Hauptkategorien zu sein, um die sich die

1859
01:02:47,099 --> 01:02:48,359
Sprache konzeptionell kümmert,

1860
01:02:48,359 --> 01:02:49,440


1861
01:02:49,440 --> 01:02:50,940
aber wir wissen nicht wirklich, dass das

1862
01:02:50,940 --> 01:02:52,859
nur eine Art ist  Hypothese richtig

1863
01:02:52,859 --> 01:02:53,940
ähm, aber was wir wissen ist, dass sie

1864
01:02:53,940 --> 01:02:56,220
nicht so viel Farbe zu verwenden scheinen

1865
01:02:56,220 --> 01:02:58,440
oder ähm, also

1866
01:02:58,440 --> 01:03:00,240
markiert keine Sprache morphologisch Farbschattierungen

1867
01:03:00,240 --> 01:03:01,140


1868
01:03:01,140 --> 01:03:03,900
oder andere konzeptionelle Merkmale wie ähm

1869
01:03:03,900 --> 01:03:06,359
Sorge oder Sorge wie keine Sprache

1870
01:03:06,359 --> 01:03:07,980
morphologisch  markiert ein gewisses Maß an Sorge

1871
01:03:07,980 --> 01:03:10,079
oder Besorgnis über ein Problem, aber wir

1872
01:03:10,079 --> 01:03:11,960
verwenden solche

1873
01:03:11,960 --> 01:03:13,500
erkenntnistheoretischen Begriffe wie

1874
01:03:13,500 --> 01:03:15,599
Beweiskraft und ähnliches, also

1875
01:03:15,599 --> 01:03:17,640
kennen Sie einen

1876
01:03:17,640 --> 01:03:19,500


1877
01:03:19,500 --> 01:03:21,720
gut  herauszufinden, mit welchen

1878
01:03:21,720 --> 01:03:23,400
Aspekten des Denkens die Sprache

1879
01:03:23,400 --> 01:03:25,619
eng verbunden ist und mit welchen Aspekten des

1880
01:03:25,619 --> 01:03:27,900
Denkens sie nicht verbunden ist, also

1881
01:03:27,900 --> 01:03:29,280
erlaubt uns das Midwest-Programm, das

1882
01:03:29,280 --> 01:03:31,680
recht ordentlich aufzuteilen, und dies ist ein viel

1883
01:03:31,680 --> 01:03:33,180
nuancierterer Rahmen, als Sie wissen,

1884
01:03:33,180 --> 01:03:34,559
wenn Chomski sagt, Sprachen dachten

1885
01:03:34,559 --> 01:03:36,960
Wieder tut er es nicht, vielleicht meint er es, vielleicht tut er es

1886
01:03:36,960 --> 01:03:38,339
nicht, aber das ist nicht, was die

1887
01:03:38,339 --> 01:03:40,319
eigentliche Architektur seiner Theorie sagt,

1888
01:03:40,319 --> 01:03:42,480
es ist ein rhetorisches Mittel, das sehr

1889
01:03:42,480 --> 01:03:43,920
nützlich und interessant ist, um

1890
01:03:43,920 --> 01:03:46,559
Studentenpublikum anzuziehen, was auch immer,

1891
01:03:46,559 --> 01:03:48,839
wenn man sich die tatsächlichen Theorien ansieht, die es sind

1892
01:03:48,839 --> 01:03:50,700
Aus dem Mentalist-Programm herauskommend

1893
01:03:50,700 --> 01:03:52,380
glaubt niemand wirklich, dass Sprache gleich

1894
01:03:52,380 --> 01:03:53,760
Denken ist, richtig, das Sprachsystem scheint

1895
01:03:53,760 --> 01:03:55,920
sein Bestes zu geben, um auf verschiedene konzeptionelle Systeme zuzugreifen und sie

1896
01:03:55,920 --> 01:03:57,599
neu zu formatieren und zu manipulieren,

1897
01:03:57,599 --> 01:03:59,220
aber es hat seine Grenzen,

1898
01:03:59,220 --> 01:04:01,440
richtig, wir wissen, was Systeme buchstabieren. Schlüssel-

1899
01:04:01,440 --> 01:04:02,819
Kernwissenssysteme sind miteinander verbunden

1900
01:04:02,819 --> 01:04:05,160
in Bezug auf die Syntax-Engine und

1901
01:04:05,160 --> 01:04:07,260
welche sind nicht

1902
01:04:07,260 --> 01:04:09,480
äh, also wissen Sie, dass diese Art auf

1903
01:04:09,480 --> 01:04:11,579
die Idee zurückkommt, dass die Lexisierung eines Konzepts es

1904
01:04:11,579 --> 01:04:14,099
vielleicht auf irgendeine Weise zu verändern scheint, es

1905
01:04:14,099 --> 01:04:16,200
durchdringt es irgendwie mit Elementen, die

1906
01:04:16,200 --> 01:04:17,819
nicht in der vorhanden sind  Konzept selbst, wenn

1907
01:04:17,819 --> 01:04:19,319
Sie also ein Konzept vortragen,

1908
01:04:19,319 --> 01:04:21,240
verändern Sie es plötzlich ein wenig, geben ihm ein

1909
01:04:21,240 --> 01:04:22,680
kleines Extra, streuen etwas anderes

1910
01:04:22,680 --> 01:04:24,299
darüber, und das scheint

1911
01:04:24,299 --> 01:04:26,700
zwischen verschiedenen Nanotypen zu variieren, und aber dies

1912
01:04:26,700 --> 01:04:29,160
sind alle wie sehr klare architektonische

1913
01:04:29,160 --> 01:04:31,619
Behauptungen innerhalb der Geometrie  Grammatik, die

1914
01:04:31,619 --> 01:04:34,980
sehr klare empirische Vorhersagen machen,

1915
01:04:34,980 --> 01:04:36,420
mit anderen Worten, ich denke, was ich sage,

1916
01:04:36,420 --> 01:04:37,140
sind

1917
01:04:37,140 --> 01:04:38,940
all diese neuropsychologischen Studien, die

1918
01:04:38,940 --> 01:04:42,119
Sie wissen lassen, in einer Menge Arbeit,

1919
01:04:42,119 --> 01:04:43,980
ähm, was zeigt es wirklich?

1920
01:04:43,980 --> 01:04:45,420
Ich denke, es zeigt, dass Sie es wissen  Wenn die

1921
01:04:45,420 --> 01:04:47,819
Sprache im Gehirn beschädigt ist,

1922
01:04:47,819 --> 01:04:50,040
verliert sie diese besondere Einflussnahme oder Art der

1923
01:04:50,040 --> 01:04:51,960
Beeinflussung dieser Systeme, aber es gibt keine

1924
01:04:51,960 --> 01:04:54,299
wirkliche Vorhersage innerhalb des Gengram

1925
01:04:54,299 --> 01:04:56,040
Enterprise, aber diese nicht-sprachlichen

1926
01:04:56,040 --> 01:04:57,540
Systeme sollten beeinträchtigt werden oder sollten

1927
01:04:57,540 --> 01:04:59,579
plötzlich heruntergefahren werden, wenn das Kernsprachsystem

1928
01:04:59,579 --> 01:05:01,200


1929
01:05:01,200 --> 01:05:02,760
ähm ist in der Tat kompromittiert, wenn

1930
01:05:02,760 --> 01:05:05,099
überhaupt, was nur die

1931
01:05:05,099 --> 01:05:07,859
prinzipielle Trennung zwischen dem syntaktischen

1932
01:05:07,859 --> 01:05:09,900
System und nicht-sprachlichen Systemen betont,

1933
01:05:09,900 --> 01:05:11,880
also denke ich, dass die vielen Vorhersagen hier

1934
01:05:11,880 --> 01:05:14,339
aus der Sprache und der Kommunikation, äh,

1935
01:05:14,339 --> 01:05:16,020
Sie wissen schon, Literatur

1936
01:05:16,020 --> 01:05:19,380
den Sinn der Sache verfehlen  architektonische Behauptungen

1937
01:05:19,380 --> 01:05:21,540
ähm, ich kann nur geben oder Daniel,

1938
01:05:21,540 --> 01:05:24,180
willst du gehen ähm, gib ein bisschen

1939
01:05:24,180 --> 01:05:25,799
Hintergrundwissen, also gibt es diese

1940
01:05:25,799 --> 01:05:26,760
Papiere

1941
01:05:26,760 --> 01:05:30,119
ähm von EV Federenko und

1942
01:05:30,119 --> 01:05:32,880
Rosmarin Varley, die ähm, ähm,

1943
01:05:32,880 --> 01:05:34,500


1944
01:05:34,500 --> 01:05:37,799
teilweise untersuchen  Die aphasischen Patienten

1945
01:05:37,799 --> 01:05:40,200
also so Leute mit eingeschränkten

1946
01:05:40,200 --> 01:05:41,880
sprachlichen Fähigkeiten,

1947
01:05:41,880 --> 01:05:43,140


1948
01:05:43,140 --> 01:05:45,000
ähm, zeigen im Grunde, dass Sie mit eingeschränkten

1949
01:05:45,000 --> 01:05:46,980
sprachlichen Fähigkeiten immer

1950
01:05:46,980 --> 01:05:49,020
noch ähm

1951
01:05:49,020 --> 01:05:50,579
erhaltene

1952
01:05:50,579 --> 01:05:52,859
Denkfähigkeiten haben können, also Leute wie Chess Masters

1953
01:05:52,859 --> 01:05:55,020
Schachgroßmeister zum Beispiel, die

1954
01:05:55,020 --> 01:05:58,140
offensichtlich sehr gut sind  at at argumentation

1955
01:05:58,140 --> 01:06:00,780
äh äh hat vielleicht keine

1956
01:06:00,780 --> 01:06:02,640
intakten sprachlichen Fähigkeiten

1957
01:06:02,640 --> 01:06:04,079
äh und dann ergänzend zu dieser Art

1958
01:06:04,079 --> 01:06:06,119
von geduldiger Arbeit gibt es auch äh Arbeiten

1959
01:06:06,119 --> 01:06:08,520
von ebb's Lab, die zeigen, dass äh äh

1960
01:06:08,520 --> 01:06:11,220
die äh Teile des Gehirns sich

1961
01:06:11,220 --> 01:06:13,859
um äh Sprache kümmern äh

1962
01:06:13,859 --> 01:06:14,640


1963
01:06:14,640 --> 01:06:16,319
äh  sind trennbar von den Teilen des

1964
01:06:16,319 --> 01:06:17,700
Gehirns, die sich um andere

1965
01:06:17,700 --> 01:06:19,619
Bereiche kümmern, sogar solche mit der gleichen Art von

1966
01:06:19,619 --> 01:06:21,599
Sprache, so dass Dinge wie Musik

1967
01:06:21,599 --> 01:06:23,819
und Mathematik,

1968
01:06:23,819 --> 01:06:25,859
ähm, in den

1969
01:06:25,859 --> 01:06:27,240
Sprachbereichen, ähm,

1970
01:06:27,240 --> 01:06:29,400
also EV und andere eher nicht passieren  argumentierte,

1971
01:06:29,400 --> 01:06:30,539
dass

1972
01:06:30,539 --> 01:06:33,539
ähm, das ist im Grunde genommen ein Beweis gegen

1973
01:06:33,539 --> 01:06:36,660
die Chomskys und behaupten, dass ähm, dass

1974
01:06:36,660 --> 01:06:38,579
Sprache das Medium für richtiges Denken ist,

1975
01:06:38,579 --> 01:06:40,440
weil es Denken gibt, das

1976
01:06:40,440 --> 01:06:42,420
in Abwesenheit von Sprache stattfinden kann, und

1977
01:06:42,420 --> 01:06:44,160
Gehirnbereiche, die sich um Sprache kümmern,

1978
01:06:44,160 --> 01:06:46,020
scheinen nicht die Gehirnbereiche zu sein, die sie interessieren

1979
01:06:46,020 --> 01:06:48,359
kümmere dich darum kümmere dich darum zu denken ähm,

1980
01:06:48,359 --> 01:06:50,099
ich schätze Elliot du sagst,

1981
01:06:50,099 --> 01:06:51,780
dass die Leute nicht wirklich glauben,

1982
01:06:51,780 --> 01:06:53,039
dass

1983
01:06:53,039 --> 01:06:56,220
ähm, sie glauben nicht, dass diese

1984
01:06:56,220 --> 01:06:57,720
Unterscheidung ich meine ähm, äh,

1985
01:06:57,720 --> 01:06:59,880


1986
01:06:59,880 --> 01:07:01,740
nein, es ist es und außerdem gibt es eine Menge  von

1987
01:07:01,740 --> 01:07:03,480
Selbstwidersprüchen sogar Innerhalb

1988
01:07:03,480 --> 01:07:04,799
dieser Argumente richtig so, in Ihrem

1989
01:07:04,799 --> 01:07:06,839
Aufsatz sagen Sie manchmal, dass Chomsky

1990
01:07:06,839 --> 01:07:08,220
denkt, dass Sprache ein Denksystem ist,

1991
01:07:08,220 --> 01:07:10,319
aber dann, ein paar Seiten später, werden Sie sagen, dass

1992
01:07:10,319 --> 01:07:12,480
Chomsky auch glaubt, dass die Syntax

1993
01:07:12,480 --> 01:07:13,799
ein völlig von

1994
01:07:13,799 --> 01:07:15,480
allem anderen getrenntes System ist  Richtig, Sie Autonomie der

1995
01:07:15,480 --> 01:07:18,900
Syntax usw. Also, was ist Chomsky-Sache, das

1996
01:07:18,900 --> 01:07:20,460
ist nicht mein Widerspruch. Ich meine, er

1997
01:07:20,460 --> 01:07:22,140
hat beide Dinge gesagt, ähm,

1998
01:07:22,140 --> 01:07:24,780
genau so, also sollten Sie

1999
01:07:24,780 --> 01:07:26,280
sich fragen, ob er

2000
01:07:26,280 --> 01:07:28,319
diese Dinge wirklich glaubt oder woraus sich das

2001
01:07:28,319 --> 01:07:30,420
Physische ergibt  von der

2002
01:07:30,420 --> 01:07:32,520
Architektur her, also nur zu

2003
01:07:32,520 --> 01:07:34,619
sagen, Sprache ist ein Denksystem, was

2004
01:07:34,619 --> 01:07:35,579
bedeutet das, das bedeutet

2005
01:07:35,579 --> 01:07:36,780
nichts, es ist nur eine sehr vage

2006
01:07:36,780 --> 01:07:38,640
Aussage. Die Frage ist, wie genau die

2007
01:07:38,640 --> 01:07:41,280
Sprache zu Thor beiträgt und wie

2008
01:07:41,280 --> 01:07:43,500
nicht, äh

2009
01:07:43,500 --> 01:07:45,839
ja, ich meine  Ich denke, seine Behauptung ist

2010
01:07:45,839 --> 01:07:48,420
hauptsächlich evolutionär oder etwas Richtiges,

2011
01:07:48,420 --> 01:07:51,180
dass dies der Ursprung

2012
01:07:51,180 --> 01:07:52,859
des Systems ist, von dem ich denke, dass es

2013
01:07:52,859 --> 01:07:55,559
genauso schwer mit

2014
01:07:55,559 --> 01:07:57,599
der Art von Patienten- und

2015
01:07:57,599 --> 01:07:59,520
Neuroimaging-Daten in Einklang zu bringen ist,

2016
01:07:59,520 --> 01:08:00,660


2017
01:08:00,660 --> 01:08:04,079
ähm, aber wissen Sie, ob  wenn er das nicht denkt,

2018
01:08:04,079 --> 01:08:07,079
dann sollte er es nicht sagen, sonst

2019
01:08:07,079 --> 01:08:08,579
reagieren die Leute auf das, was er gesagt hat. Ich denke,

2020
01:08:08,579 --> 01:08:11,280
nein, nein, denn das Argument ist, dass

2021
01:08:11,280 --> 01:08:13,619
Sprache eine Art Denksystem ist, das

2022
01:08:13,619 --> 01:08:15,420
einige Aspekte davon reguliert, und es

2023
01:08:15,420 --> 01:08:16,920
und es gibt nach  Einige Aspekte des Denkens

2024
01:08:16,920 --> 01:08:19,020
sind eindeutig einzigartig für Menschen, aber

2025
01:08:19,020 --> 01:08:21,000
sie sind nicht intrinsisch oder kausal

2026
01:08:21,000 --> 01:08:22,920
damit verbunden. Die Architektur des

2027
01:08:22,920 --> 01:08:24,600
Systems unterscheidet sich stark von der Art

2028
01:08:24,600 --> 01:08:26,399
von Verallgemeinerungen, die Sie rhetorisch

2029
01:08:26,399 --> 01:08:28,979
aus der Architektur ableiten können, z.

2030
01:08:28,979 --> 01:08:30,960
B. wenn Sie aphasisch vor Ort arbeiten

2031
01:08:30,960 --> 01:08:32,698
Patienten, die keine Defizite im komplexen

2032
01:08:32,698 --> 01:08:33,960
Denken zeigen, wie Sie gerade das Schachspielen usw. erwähnt haben,

2033
01:08:33,960 --> 01:08:35,759
würden wir eigentlich erwarten, dass

2034
01:08:35,759 --> 01:08:37,380
dies unter einer Art von Sie wissen,

2035
01:08:37,380 --> 01:08:39,179
nicht-lexikalistischem Rahmen der

2036
01:08:39,179 --> 01:08:41,160
Geometriesyntax, wo Bedeutung, wie ich sagte,

2037
01:08:41,160 --> 01:08:43,799
Syntax bedeutet und Form Form einfach

2038
01:08:43,799 --> 01:08:45,600
alles bedeutet, was Sie können

2039
01:08:45,600 --> 01:08:46,979
Sprache auslagern und all diese Dinge sind

2040
01:08:46,979 --> 01:08:48,540
getrennte Merkmale und getrennte Systeme.

2041
01:08:48,540 --> 01:08:51,000
Richtig, die Autonomie der Syntax bedeutet nicht, dass

2042
01:08:51,000 --> 01:08:52,020


2043
01:08:52,020 --> 01:08:53,819
Sie wissen, was viele Leute denken,

2044
01:08:53,819 --> 01:08:54,719
es bedeutet nur, dass es

2045
01:08:54,719 --> 01:08:56,160
bestimmte syntaktische Operationen gibt,

2046
01:08:56,160 --> 01:08:58,439
die nicht semantisch sind, es gibt bestimmte

2047
01:08:58,439 --> 01:09:00,299
Dinge  Sie können mit Syntax tun, dass Sie

2048
01:09:00,299 --> 01:09:01,920
nur Syntax und keine Semantik machen können,

2049
01:09:01,920 --> 01:09:03,420
also kommen wir zurück auf den

2050
01:09:03,420 --> 01:09:05,279
Unterschied zwischen

2051
01:09:05,279 --> 01:09:07,259
Petrovskys Theorie, dass Semantik

2052
01:09:07,259 --> 01:09:10,319
gerecht und richtig ist, und dem

2053
01:09:10,319 --> 01:09:11,698
Glauben des Synthetikers, dass es

2054
01:09:11,698 --> 01:09:13,080
bestimmte seltsame Dinge gibt  Sie können

2055
01:09:13,080 --> 01:09:15,719
mit einer Syntax auskommen, die nur syntaktisch ist, so dass

2056
01:09:15,719 --> 01:09:17,759
es sogar innerhalb des architektonischen Rahmens eine Trennung gibt,

2057
01:09:17,759 --> 01:09:20,100
und daher ist es

2058
01:09:20,100 --> 01:09:22,439
nicht allzu überraschend, dass Sie

2059
01:09:22,439 --> 01:09:24,359
diese Trennung auch auf neuropsychologischer

2060
01:09:24,359 --> 01:09:25,979
Ebene finden  Die

2061
01:09:25,979 --> 01:09:28,319


2062
01:09:28,319 --> 01:09:30,960
Vorhersage der Sprache ist eine

2063
01:09:30,960 --> 01:09:34,140
gedachte evolutionäre Idee, dann richtig, also

2064
01:09:34,140 --> 01:09:36,960
wie äh, wenn das nicht so ist, wenn Sie sagen,

2065
01:09:36,960 --> 01:09:39,060
dass das nicht vorhersagt, dass das Denken

2066
01:09:39,060 --> 01:09:41,160
auf der Sprache beruht, ähm,

2067
01:09:41,160 --> 01:09:44,399
dann ähm, ich denke, wer

2068
01:09:44,399 --> 01:09:45,899
diese Theorie mag, sollte sich

2069
01:09:45,899 --> 01:09:47,520
einige Vorhersagen einfallen lassen

2070
01:09:47,520 --> 01:09:49,979
Ähm, ähm, du weißt, was das

2071
01:09:49,979 --> 01:09:51,660
bedeutet, was diese Theorie eigentlich bedeutet. Ich meine, ich habe das Gefühl, dass

2072
01:09:51,660 --> 01:09:53,819
diese Art von Vorhersagen oft

2073
01:09:53,819 --> 01:09:55,440
wirklich notwendig sind, um

2074
01:09:55,440 --> 01:09:57,360
den Inhalt einer Vorhersage zu verstehen

2075
01:09:57,360 --> 01:09:58,320


2076
01:09:58,320 --> 01:10:00,540


2077
01:10:00,540 --> 01:10:03,660
Es ist alles gut, ich

2078
01:10:03,660 --> 01:10:05,880
wollte nur irgendwie

2079
01:10:05,880 --> 01:10:08,040
ein

2080
01:10:08,040 --> 01:10:12,660
ähm Luft holen und ähm eine Gelegenheit für

2081
01:10:12,660 --> 01:10:15,480
alle, ähm weitere Fragen zu stellen,

2082
01:10:15,480 --> 01:10:17,520
aber wow,

2083
01:10:17,520 --> 01:10:20,460
danke euch beiden für die vielen Themen, die wir behandelt haben, ähm,

2084
01:10:20,460 --> 01:10:21,780


2085
01:10:21,780 --> 01:10:22,500


2086
01:10:22,500 --> 01:10:25,199
wir werden in den letzten Minuten, ähm,

2087
01:10:25,199 --> 01:10:27,120
irgendwie  Fazit und nächste Schritte, aber Dave,

2088
01:10:27,120 --> 01:10:29,820
möchten Sie eine Frage stellen oder nur

2089
01:10:29,820 --> 01:10:32,840
kurz darüber nachdenken?

2090
01:10:36,900 --> 01:10:38,880
Ok, nein, es

2091
01:10:38,880 --> 01:10:39,960


2092
01:10:39,960 --> 01:10:42,239
gibt viele Kommentare im Chat, also

2093
01:10:42,239 --> 01:10:45,120
hoffe ich, dass Sie beide sie in

2094
01:10:45,120 --> 01:10:47,460
Ihrer Freizeit lesen können, um zu sehen, was jeder

2095
01:10:47,460 --> 01:10:49,440


2096
01:10:49,440 --> 01:10:52,860
wo hinzugefügt hat  Gehen wir von hier aus, während wir

2097
01:10:52,860 --> 01:10:57,179
in den Mai 2023 und darüber hinaus brüllen,

2098
01:10:57,179 --> 01:11:00,960
was können Linguisten, große

2099
01:11:00,960 --> 01:11:02,940
Sprachmodellentwickler und Benutzer,

2100
01:11:02,940 --> 01:11:05,159
Kognitionswissenschaftler, was sind Ihrer Meinung nach

2101
01:11:05,159 --> 01:11:06,960
einige der fruchtbarsten Wege

2102
01:11:06,960 --> 01:11:08,159
nach vorne?

2103
01:11:08,159 --> 01:11:10,080
Okay,

2104
01:11:10,080 --> 01:11:13,260
ich würde sagen, ähm, Sie kennen den

2105
01:11:13,260 --> 01:11:14,699
fruchtbarsten Weg nach vorne  ist, ähm

2106
01:11:14,699 --> 01:11:15,600


2107
01:11:15,600 --> 01:11:17,460
wie kognitive Psychologie wirklich

2108
01:11:17,460 --> 01:11:18,659
ernst zu nehmen, nein, es gibt eine Menge netter Arbeit

2109
01:11:18,659 --> 01:11:21,000
in letzter Zeit, die versucht, Dinge auszurichten, wie

2110
01:11:21,000 --> 01:11:24,060
ja, Sie kennen Church EBT Wolf von Alpha-

2111
01:11:24,060 --> 01:11:25,739
Plugins, die Art und Weise, wie Chat-GBT

2112
01:11:25,739 --> 01:11:28,080
mit verschiedenen Arten von Modulen verbunden werden kann,

2113
01:11:28,080 --> 01:11:30,179
um eine legitime zu erstellen  Eine Art

2114
01:11:30,179 --> 01:11:32,580
AGI-System muss nicht unbedingt

2115
01:11:32,580 --> 01:11:34,380
psychologisch

2116
01:11:34,380 --> 01:11:35,880
auf die Art von Modulen angewiesen sein, die

2117
01:11:35,880 --> 01:11:37,500
Menschen haben, aber ich denke, es wird

2118
01:11:37,500 --> 01:11:38,820
davon profitieren, also gab es

2119
01:11:38,820 --> 01:11:41,040
einige Behauptungen, dass große Sprachmodelle

2120
01:11:41,040 --> 01:11:42,540
vielleicht alles können  irgendetwas

2121
01:11:42,540 --> 01:11:43,860
richtig alles alles was man mag

2122
01:11:43,860 --> 01:11:44,760


2123
01:11:44,760 --> 01:11:46,620
ähm aber ich denke langfristig wird es

2124
01:11:46,620 --> 01:11:47,880
wohl so sein dass llms

2125
01:11:47,880 --> 01:11:49,380
etwas sehr wichtiges und sehr interessantes machen kann

2126
01:11:49,380 --> 01:11:51,000
aber es wird nur

2127
01:11:51,000 --> 01:11:53,100
ein teil des puzzles sein also eigentlich sogar

2128
01:11:53,100 --> 01:11:55,440
offen  AI-CEO Sam Altman sagte letzte Woche,

2129
01:11:55,440 --> 01:11:56,400
dass,

2130
01:11:56,400 --> 01:11:58,440
ähm, wissen Sie, was wir mit llms machen können,

2131
01:11:58,440 --> 01:12:00,120
wirklich irgendwie erschöpft ist, wir brauchen

2132
01:12:00,120 --> 01:12:02,520
neue Richtungen, neue, neue Wege und so weiter

2133
01:12:02,520 --> 01:12:04,860


2134
01:12:04,860 --> 01:12:07,440


2135
01:12:07,440 --> 01:12:09,060
Studenten hier, aber ich

2136
01:12:09,060 --> 01:12:11,159
denke, er hat auch Recht, Sie wissen, dass llms

2137
01:12:11,159 --> 01:12:12,300
etwas Spektakuläres tun können, aber sie werden

2138
01:12:12,300 --> 01:12:14,760
wahrscheinlich einen kleinen Teil

2139
01:12:14,760 --> 01:12:17,760
der allgemeinen AGI-Architektur bilden, richtig, und

2140
01:12:17,760 --> 01:12:19,679
wenn Sie über AGI als

2141
01:12:19,679 --> 01:12:21,900
potenzielles potenzielles Ziel nachdenken möchten,

2142
01:12:21,900 --> 01:12:25,679
wissen Sie es  Ich denke, viele davon, also

2143
01:12:25,679 --> 01:12:28,020
lassen Sie mich hier ein weiteres Beispiel geben, also ähm

2144
01:12:28,020 --> 01:12:29,820
Anna, sogar über

2145
01:12:29,820 --> 01:12:31,920
ähm, die eine sehr gute produktive

2146
01:12:31,920 --> 01:12:34,140
Wissenschaftlerin ist. Sie hat kürzlich einen Artikel veröffentlicht, in dem sie

2147
01:12:34,140 --> 01:12:35,400
für eine Art modulare

2148
01:12:35,400 --> 01:12:37,800
Architektur für llms argumentiert, und das ist ein

2149
01:12:37,800 --> 01:12:39,060
sehr schönes Framework, richtig

2150
01:12:39,060 --> 01:12:40,860
kognitiv sehr plausibel, es ist genau das,

2151
01:12:40,860 --> 01:12:41,880
worauf wir drängen sollten,

2152
01:12:41,880 --> 01:12:43,679
es ist kompatibel mit Howard

2153
01:12:43,679 --> 01:12:45,239
Gardners Vorstellung von multiplen

2154
01:12:45,239 --> 01:12:46,800
Intelligenzen und so

2155
01:12:46,800 --> 01:12:48,000
weiter, aber ich denke gleichzeitig, nur um

2156
01:12:48,000 --> 01:12:49,620
diesen Kommentar zu beenden,

2157
01:12:49,620 --> 01:12:51,360
gab es zuletzt einen Technikvortrag  Woche,

2158
01:12:51,360 --> 01:12:54,840
glaube ich, oder vielleicht vor ein paar Tagen, wo

2159
01:12:54,840 --> 01:12:56,880
viele andere Dinge

2160
01:12:56,880 --> 01:12:59,520
auf unproduktive Weise mit KI-Hype in Verbindung gebracht werden können, also

2161
01:12:59,520 --> 01:13:02,100
hielt Greg Brockman von Openai einen seiner

2162
01:13:02,100 --> 01:13:04,199
äh einen dieser großen Ted Talks, in denen er

2163
01:13:04,199 --> 01:13:06,179
verschiedene Plugins zeigte, die GPD chatten

2164
01:13:06,179 --> 01:13:08,159
Ich habe erwähnt, dass Wolfram funktionieren kann, aber es

2165
01:13:08,159 --> 01:13:09,300
gibt auch Dinge wie

2166
01:13:09,300 --> 01:13:11,820
Bilderzeugung, Instacart-Shopping, wo Sie

2167
01:13:11,820 --> 01:13:13,560
Chat-TV bekommen können, um Ihnen Dinge zu kaufen, und

2168
01:13:13,560 --> 01:13:15,060
was Sie haben,

2169
01:13:15,060 --> 01:13:17,040
ähm, und das bringt Sie wieder zurück zu

2170
01:13:17,040 --> 01:13:18,960
der Idee, dass mehrere Subsysteme

2171
01:13:18,960 --> 01:13:20,940
verschiedene Subfunktionen ausführen können, so Brockovich

2172
01:13:20,940 --> 01:13:22,620
zeigte auch ein Beispiel für das Geben von Chat

2173
01:13:22,620 --> 01:13:26,820
GPT, einer Excel-Datei, einer CSV-Datei und aus einer

2174
01:13:26,820 --> 01:13:28,739
Archivdatenbank mit akademischen Arbeiten,

2175
01:13:28,739 --> 01:13:30,060
in der nur eine Reihe von Arbeiten

2176
01:13:30,060 --> 01:13:31,739
und dann Titel und was haben Sie

2177
01:13:31,739 --> 01:13:32,880
Recht,

2178
01:13:32,880 --> 01:13:34,500
ähm, und er sagte, dass Sie wissen, wie

2179
01:13:34,500 --> 01:13:37,440
es Chatipati verwendet  Weltwissen, um

2180
01:13:37,440 --> 01:13:39,060
abzuleiten, was die Titel der Kolumnen

2181
01:13:39,060 --> 01:13:40,860
bedeuten, also haben wir verstanden, dass Sie wissen, dass

2182
01:13:40,860 --> 01:13:42,780
Titel der Titel des Papiers ist, es hat

2183
01:13:42,780 --> 01:13:44,640
verstanden, dass Autoren die Anzahl

2184
01:13:44,640 --> 01:13:47,280
der Autoren pro Papier bedeuten, es hat verstanden, dass das

2185
01:13:47,280 --> 01:13:48,840
erstellte Datum das Datum bedeutet, an dem das Papier

2186
01:13:48,840 --> 01:13:50,820
richtig eingereicht wurde, und weil es ein ist  TED-

2187
01:13:50,820 --> 01:13:52,080
Talk, wissen Sie, das ganze Publikum

2188
01:13:52,080 --> 01:13:54,120
gab uns Standing Ovations,

2189
01:13:54,120 --> 01:13:56,340
ähm, aber die Möglichkeit, Labels in

2190
01:13:56,340 --> 01:13:57,900
einer Excel-Datei zu beschreiben,

2191
01:13:57,900 --> 01:14:01,679
ist wohl nett, aber ähm, ich bin mir nicht sicher, ob

2192
01:14:01,679 --> 01:14:03,120
Sie es wirklich Weltwissen nennen würden, also

2193
01:14:03,120 --> 01:14:04,800
denke ich, es gibt eine Menge  Ich würde nur sagen, dass

2194
01:14:04,800 --> 01:14:06,719


2195
01:14:06,719 --> 01:14:08,880
neben der Reduzierung der Anthroposomen noch viel Fortschritt gemacht werden muss,

2196
01:14:08,880 --> 01:14:11,640
und Sie haben das richtige Gleichgewicht, also

2197
01:14:11,640 --> 01:14:12,659
müssen Sie, wie ich sagte, das richtige

2198
01:14:12,659 --> 01:14:13,920
Gleichgewicht

2199
01:14:13,920 --> 01:14:15,360
einer psychologisch plausiblen Art von

2200
01:14:15,360 --> 01:14:17,100
modularer Architektur haben, aber Sie können es nicht haben

2201
01:14:17,100 --> 01:14:18,480
viel

2202
01:14:18,480 --> 01:14:19,920
äh Anthropomorphismus, denn dann wirst du

2203
01:14:19,920 --> 01:14:21,540
mitgerissen, du musst finden, wir

2204
01:14:21,540 --> 01:14:22,860
müssen die richtige Balance finden zwischen der

2205
01:14:22,860 --> 01:14:25,739
Modellierung von menschenähnlichen äh modularen

2206
01:14:25,739 --> 01:14:27,780
Systemen, aber nicht in einem Maße,

2207
01:14:27,780 --> 01:14:29,280
das ein bisschen, du weißt schon,

2208
01:14:29,280 --> 01:14:31,140
unplausibel oder wissenschaftlich

2209
01:14:31,140 --> 01:14:33,679
nicht hilfreich ist

2210
01:14:35,040 --> 01:14:37,500
Ich meine, ich denke, ich stimme allem zu, dass

2211
01:14:37,500 --> 01:14:40,199
ich wirklich begeistert bin von diesen äh Möglichkeiten,

2212
01:14:40,199 --> 01:14:42,739
Sprachmodelle mit

2213
01:14:42,739 --> 01:14:45,120
äh anderen Formen der

2214
01:14:45,120 --> 01:14:47,640
Informationsverarbeitung zu verbinden, äh, das scheint so zu sein, was die

2215
01:14:47,640 --> 01:14:49,679
Leute haben. Ich denke, ich meine,

2216
01:14:49,679 --> 01:14:52,620
ich '  Ich war sehr überrascht über

2217
01:14:52,620 --> 01:14:54,840
die Dinge, die sie tun können, ähm,

2218
01:14:54,840 --> 01:14:58,260
genauso wie Sprachmodellierung, also

2219
01:14:58,260 --> 01:15:00,120
ähm, Sie kennen verschiedene Arten von

2220
01:15:00,120 --> 01:15:01,920
Denkrätseln und Dinge, die sie lösen können. Ich

2221
01:15:01,920 --> 01:15:05,340
denke, das ist wirklich faszinierend

2222
01:15:05,340 --> 01:15:07,920
und Sie wissen vielleicht  von uns verlangen,

2223
01:15:07,920 --> 01:15:09,719
unsere Sie kennen die

2224
01:15:09,719 --> 01:15:11,760
Beziehungen zwischen Sprache und

2225
01:15:11,760 --> 01:15:13,500
Denken zu überdenken und zu versuchen, einen Weg zu finden, um

2226
01:15:13,500 --> 01:15:15,239
genau zu sagen, was es bedeutet, dass

2227
01:15:15,239 --> 01:15:17,760
etwas eine

2228
01:15:17,760 --> 01:15:19,380
Repräsentation hat, oder über

2229
01:15:19,380 --> 01:15:21,060
diese Repräsentation nachzudenken, aber letztendlich

2230
01:15:21,060 --> 01:15:23,820
denke ich, dass ich zustimme  dass ähm,

2231
01:15:23,820 --> 01:15:26,219
du weißt, dass Menschen unterschiedliche Denkweisen haben

2232
01:15:26,219 --> 01:15:28,560
und das

2233
01:15:28,560 --> 01:15:32,400
scheint wichtig zu sein für ähm für Intelligenz

2234
01:15:32,400 --> 01:15:35,040
ähm. Ich bin auch super aufgeregt wegen der Baby-

2235
01:15:35,040 --> 01:15:37,620
LM-Herausforderung, also denke ich, auf der

2236
01:15:37,620 --> 01:15:39,960
sprachlichen Seite richtig

2237
01:15:39,960 --> 01:15:42,300
ähm, das ist genau das Richtige  Es ist richtig,

2238
01:15:42,300 --> 01:15:45,300
zu sehen, wie weit wir mit äh

2239
01:15:45,300 --> 01:15:47,219
kleineren Datensätzen kommen können, und vielleicht

2240
01:15:47,219 --> 01:15:50,820
weißt du danach, dass du versuchst, äh äh

2241
01:15:50,820 --> 01:15:53,699
etwas mehr über

2242
01:15:53,699 --> 01:15:55,739
die Arten von Semantik zu verstehen, die Kinder

2243
01:15:55,739 --> 01:15:57,840
erwerben und woher sie sie bekommen

2244
01:15:57,840 --> 01:15:59,820
und und wie  Eine Art externe Semantik

2245
01:15:59,820 --> 01:16:02,520
kann das Sprachenlernen oder

2246
01:16:02,520 --> 01:16:04,560
speziell vielleicht Grammatik- und

2247
01:16:04,560 --> 01:16:06,600
Syntaxlernen informieren.

2248
01:16:06,600 --> 01:16:09,480
Ich denke, mein anderer Weg nach vorne

2249
01:16:09,480 --> 01:16:12,179
wäre,

2250
01:16:12,179 --> 01:16:15,840
dass es äh gibt, wie ich das Gefühl habe, dass

2251
01:16:15,840 --> 01:16:18,300
diese Art von Modellen äh

2252
01:16:18,300 --> 01:16:21,179
äh wirklich weit gegangen ist  Übertrifft die Erwartungen der Menschen

2253
01:16:21,179 --> 01:16:23,340
an diese Art

2254
01:16:23,340 --> 01:16:25,500
von Modellmodellen, die Art von grundlegendem

2255
01:16:25,500 --> 01:16:27,960
statistischem Lernen, das Entdecken von

2256
01:16:27,960 --> 01:16:29,940
Mustern in Texten,

2257
01:16:29,940 --> 01:16:30,719


2258
01:16:30,719 --> 01:16:32,640
ähm, scheint wirklich

2259
01:16:32,640 --> 01:16:35,040
ziemlich bemerkenswerte Ergebnisse zu liefern, ähm,

2260
01:16:35,040 --> 01:16:37,020
und das hat für mich,

2261
01:16:37,020 --> 01:16:39,360
glaube ich, gerade eine große Welle von ausgelöst

2262
01:16:39,360 --> 01:16:42,000
Unsicherheit über Theorien, also denke ich,

2263
01:16:42,000 --> 01:16:43,620
dass unsere Theorien von im Grunde

2264
01:16:43,620 --> 01:16:46,739
allem in der Sprache sicher

2265
01:16:46,739 --> 01:16:48,960
ähm, aber Kognition wahrscheinlich Neurowissenschaften

2266
01:16:48,960 --> 01:16:50,640
genau so wie all diese Dinge, die ich denke,

2267
01:16:50,640 --> 01:16:53,100
überarbeitet werden,

2268
01:16:53,100 --> 01:16:54,420
wenn wir wirklich

2269
01:16:54,420 --> 01:16:56,219
die

2270
01:16:56,219 --> 01:16:58,560
ähm die Fähigkeit irgendwie verstehen  von wirklich allgemeinen

2271
01:16:58,560 --> 01:17:02,219
Arten von Lernsystemen wie diesen, also ähm,

2272
01:17:02,219 --> 01:17:03,900
das macht es auf der einen

2273
01:17:03,900 --> 01:17:05,820
Seite

2274
01:17:05,820 --> 01:17:07,440
ähm irgendwie schade für frühere

2275
01:17:07,440 --> 01:17:09,360
Theorien, besonders Theorien, die sich

2276
01:17:09,360 --> 01:17:11,640
darauf stützten,

2277
01:17:11,640 --> 01:17:14,100
ähm, Sie wissen, Lernen kann nicht

2278
01:17:14,100 --> 01:17:15,659
gut funktionieren

2279
01:17:15,659 --> 01:17:18,000
ähm  Aber auf der positiven Seite denke ich, dass es

2280
01:17:18,000 --> 01:17:20,460
eine sehr aufregende Zeit sowohl für die

2281
01:17:20,460 --> 01:17:22,620
KI als auch für die Kognitionswissenschaft und die

2282
01:17:22,620 --> 01:17:24,000
Linguistik ist,

2283
01:17:24,000 --> 01:17:25,560
wo es jetzt diese wirklich wirklich

2284
01:17:25,560 --> 01:17:27,420
mächtigen Werkzeuge gibt,

2285
01:17:27,420 --> 01:17:29,719
die wie ein qualitativ

2286
01:17:29,719 --> 01:17:32,940
äh anders großer Schritt in Richtung menschlicher

2287
01:17:32,940 --> 01:17:34,620
menschlicher Fähigkeiten erscheinen ähm

2288
01:17:34,620 --> 01:17:36,120
und ich denke, sie zu integrieren

2289
01:17:36,120 --> 01:17:39,060
und sowohl die Art von

2290
01:17:39,060 --> 01:17:41,280
Ingenieursunterricht als auch die Art von

2291
01:17:41,280 --> 01:17:43,679
philosophischem Unterricht darüber zu nehmen, wie sie

2292
01:17:43,679 --> 01:17:45,540
hergestellt werden und welche Prinzipien

2293
01:17:45,540 --> 01:17:47,460
in das Design intelligenter Systeme einfließen,

2294
01:17:47,460 --> 01:17:49,679
denke ich, dass das äh diese Dinge werden

2295
01:17:49,679 --> 01:17:52,080
wird das Gefühl in den nächsten

2296
01:17:52,080 --> 01:17:53,699
fünf oder zehn Jahren wirklich prägen, ähm, und

2297
01:17:53,699 --> 01:17:54,900


2298
01:17:54,900 --> 01:17:57,000
ich würde auch sagen, im

2299
01:17:57,000 --> 01:17:58,500
Zusammenhang mit breiteren Themen hier, dass

2300
01:17:58,500 --> 01:17:59,760
Sie völlig recht haben, wie ich mich

2301
01:17:59,760 --> 01:18:01,620
erinnere, als ich darüber las, wann

2302
01:18:01,620 --> 01:18:04,920
Deep Blue be uh Casper of war  war es das

2303
01:18:04,920 --> 01:18:07,260
Schach, uh, ja, und es gab

2304
01:18:07,260 --> 01:18:08,940
einige Kommentatoren, die sagten, dass

2305
01:18:08,940 --> 01:18:12,480
Schach vorbei ist, wenn eine KI ein Mensch sein kann,

2306
01:18:12,480 --> 01:18:13,739
dann ist das Spiel vorbei, was ist der

2307
01:18:13,739 --> 01:18:15,420


2308
01:18:15,420 --> 01:18:16,920


2309
01:18:16,920 --> 01:18:18,900
Sinn des Schachstudiums?  Wenn die KI

2310
01:18:18,900 --> 01:18:20,640
anscheinend alles erreicht hat, was Menschen

2311
01:18:20,640 --> 01:18:22,140
tun müssen, um Schach zu spielen, was ist der Sinn,

2312
01:18:22,140 --> 01:18:23,100
es zu spielen,

2313
01:18:23,100 --> 01:18:24,540
ähm, aber ich denke, Sie wissen, wenn sich

2314
01:18:24,540 --> 01:18:26,219
herausstellte, dass es die Popularität von

2315
01:18:26,219 --> 01:18:27,900
Schach steigerte, richtig, sie sind in unseren Mini-Schach-

2316
01:18:27,900 --> 01:18:29,580
Prominenten sowie in weltweiten

2317
01:18:29,580 --> 01:18:31,199
Turnieren  und ich würde vorhersagen, dass

2318
01:18:31,199 --> 01:18:32,520
das gleiche wahrscheinlich

2319
01:18:32,520 --> 01:18:34,620
auch mit der Sprache passieren wird. Wissen Sie, llms bedeuten nicht, dass

2320
01:18:34,620 --> 01:18:36,360
es das Ende der Sprache ist, keine

2321
01:18:36,360 --> 01:18:37,800
Sprache mehr, keine Linguistik mehr. Ich würde

2322
01:18:37,800 --> 01:18:39,179
tatsächlich zurückdrängen und sagen, vielleicht

2323
01:18:39,179 --> 01:18:40,620
wäre es das Gegenteil,

2324
01:18:40,620 --> 01:18:42,540
ähm, Sie kennen den Erfolg  LMS wird das

2325
01:18:42,540 --> 01:18:44,400
allgemeine Interesse an linguistischer

2326
01:18:44,400 --> 01:18:46,199
Theorie aufgrund ihrer Paarung steigern. Sie kennen

2327
01:18:46,199 --> 01:18:48,060
seltsame Einschränkungen und scheinbare

2328
01:18:48,060 --> 01:18:50,100
Einschränkungen, denn ich würde auch

2329
01:18:50,100 --> 01:18:52,380
sagen, dass Sie die Skala an dieser Stelle kennen, die

2330
01:18:52,380 --> 01:18:55,500
Stressproblemskala ist definitiv

2331
01:18:55,500 --> 01:18:57,420
weit von allem entfernt, was

2332
01:18:57,420 --> 01:18:59,820
fehlt  eine Fähigkeit von LMS, wie Sie wissen,

2333
01:18:59,820 --> 01:19:01,260
ihr Wissen und ihre Erfahrungen wirklich zu abstrahieren,

2334
01:19:01,260 --> 01:19:03,239
um

2335
01:19:03,239 --> 01:19:04,739
Gruppenwaschvorhersagen und Verallgemeinerungen zu machen und so

2336
01:19:04,739 --> 01:19:06,780
weiter. Ich habe einige Beispiele gegeben, aber es gibt einige

2337
01:19:06,780 --> 01:19:07,860
andere in der Literatur, bei denen es

2338
01:19:07,860 --> 01:19:09,179
nicht wirklich gut darin zu sein scheint,

2339
01:19:09,179 --> 01:19:10,739
es zu verallgemeinern  Gehen Sie von vielleicht

2340
01:19:10,739 --> 01:19:13,620
bestimmten Token-Typen aus und aber ich würde

2341
01:19:13,620 --> 01:19:15,060
Sie wissen, dass ich meine letzte meine letzte

2342
01:19:15,060 --> 01:19:17,100
Behauptung wäre, dass Sie wissen, dass die

2343
01:19:17,100 --> 01:19:19,080
Spracherwerbsliteratur

2344
01:19:19,080 --> 01:19:21,480
nicht unbedingt llms braucht, obwohl

2345
01:19:21,480 --> 01:19:22,860
Sie wissen, dass Kognitionswissenschaftler keine

2346
01:19:22,860 --> 01:19:26,219
llms brauchen, die wir möglicherweise könnten  ähm,

2347
01:19:26,219 --> 01:19:27,420
Sie wissen, wiedereinsetzen und

2348
01:19:27,420 --> 01:19:29,640
hier offensichtlich anderer Meinung zu sein, aber ähm, ich würde sagen, große

2349
01:19:29,640 --> 01:19:32,040
Technologieunternehmen, die von llms profitieren, brauchen

2350
01:19:32,040 --> 01:19:33,360
llms, richtig, sie sind die einzigen, die es

2351
01:19:33,360 --> 01:19:35,100
wirklich tun.

2352
01:19:35,100 --> 01:19:37,320
Es kann sein, dass der Verstand ein sehr

2353
01:19:37,320 --> 01:19:39,000
ist  In einem sehr unterschiedlichen Raum kann es sein,

2354
01:19:39,000 --> 01:19:40,560
dass es bestimmte Formen des Verhaltens

2355
01:19:40,560 --> 01:19:42,420
und des Lernens gibt, die durch Prozesse erfasst werden können, die

2356
01:19:42,420 --> 01:19:44,219
denen von llms ähneln.

2357
01:19:44,219 --> 01:19:45,540
Stephen hat

2358
01:19:45,540 --> 01:19:47,159
in seinen Artikeln einige interessante Beispiele über Magnetismus

2359
01:19:47,159 --> 01:19:49,320
und seltsame Lernregeln gegeben

2360
01:19:49,320 --> 01:19:51,060
Sehr Domäne Allgemein und sehr

2361
01:19:51,060 --> 01:19:52,920
schnell und sehr mysteriös, also wissen Sie

2362
01:19:52,920 --> 01:19:54,780
vielleicht, dass diese Art des Lernens für diese Art von Dingen

2363
01:19:54,780 --> 01:19:56,100


2364
01:19:56,100 --> 01:19:57,780
relevant sein wird, aber ich halte es immer noch für

2365
01:19:57,780 --> 01:19:59,580
unwahrscheinlich, dass einer der Kandidaten

2366
01:19:59,580 --> 01:20:02,100
natürliche Sprache und zumindest die Art und Weise

2367
01:20:02,100 --> 01:20:03,480
natürlich ist  Sprache funktioniert und es ist voller

2368
01:20:03,480 --> 01:20:05,219
Ruhm in Bezug auf die Form, hauptsächlich

2369
01:20:05,219 --> 01:20:07,380
Regulierung und was haben Sie, also denke ich,

2370
01:20:07,380 --> 01:20:09,300
ich würde Sie wissen, dass es mich irgendwie

2371
01:20:09,300 --> 01:20:11,520
daran erinnert, wo Sie wissen, dass

2372
01:20:11,520 --> 01:20:13,320
Sie dieses Bild von John Wick, Kapitel vier, kürzlich gesehen haben,

2373
01:20:13,320 --> 01:20:14,940
richtig und er hat  Das ist

2374
01:20:14,940 --> 01:20:16,140
diese Szene, in der er in der Wüste spazieren geht

2375
01:20:16,140 --> 01:20:17,580
und sich nicht sicher ist, ob er

2376
01:20:17,580 --> 01:20:19,260
diesen Typen gesehen hat,

2377
01:20:19,260 --> 01:20:21,540


2378
01:20:21,540 --> 01:20:22,739


2379
01:20:22,739 --> 01:20:24,840


2380
01:20:24,840 --> 01:20:26,460
als würde er ihn ermorden wollen.  Sie

2381
01:20:26,460 --> 01:20:28,199
halluzinieren, aber dann erkennen Sie, dass

2382
01:20:28,199 --> 01:20:29,820
Sie manchmal, bevor es zu spät ist, wissen, dass

2383
01:20:29,820 --> 01:20:31,679
Sie tatsächlich halluzinieren, dass

2384
01:20:31,679 --> 01:20:33,239
Sie keine Oase sehen, dass Sie immer noch

2385
01:20:33,239 --> 01:20:35,040
in der Wüste sind, und ich denke, das ist

2386
01:20:35,040 --> 01:20:37,199
vielleicht die Situation, in der wir uns gerade befinden

2387
01:20:37,199 --> 01:20:39,360
mit sprachlicher Kompetenz von vielen

2388
01:20:39,360 --> 01:20:41,400
Sprachmodellen haben wir die Illusion

2389
01:20:41,400 --> 01:20:44,940
von äh sprachlicher Kompetenz, denn Sie wissen,

2390
01:20:44,940 --> 01:20:46,260
ähm, Sie sehen immer die Illusion, bevor

2391
01:20:46,260 --> 01:20:48,360
Sie die Oase richtig finden, also denke ich, dass

2392
01:20:48,360 --> 01:20:49,440
wir uns gerade im

2393
01:20:49,440 --> 01:20:51,420
halluzinierenden Zustand der Wüste befinden, wo

2394
01:20:51,420 --> 01:20:53,699
wir  Ich sehe potenzielle Funken von

2395
01:20:53,699 --> 01:20:55,380
sprachlicher Kompetenz, aber es ist immer noch nicht

2396
01:20:55,380 --> 01:20:57,420
sehr klar und robust.

2397
01:20:57,420 --> 01:20:59,640


2398
01:20:59,640 --> 01:21:02,480
Ähm, wir haben die Oase noch nicht erreicht

2399
01:21:02,820 --> 01:21:06,360


2400
01:21:06,360 --> 01:21:09,540


2401
01:21:09,540 --> 01:21:11,219


2402
01:21:11,219 --> 01:21:13,440
zu sagen,

2403
01:21:13,440 --> 01:21:15,120
dass große Sprachmodelle keine

2404
01:21:15,120 --> 01:21:17,540
Prioritäten

2405
01:21:18,480 --> 01:21:21,300
haben. Haben große Sprachmodelle Prioritäten? Ich würde

2406
01:21:21,300 --> 01:21:23,820
sagen, ja, sie tun es definitiv

2407
01:21:23,820 --> 01:21:25,620


2408
01:21:25,620 --> 01:21:28,440


2409
01:21:28,440 --> 01:21:30,360


2410
01:21:30,360 --> 01:21:32,340


2411
01:21:32,340 --> 01:21:33,840
Beispiel, wenn Sie gerne ein

2412
01:21:33,840 --> 01:21:36,300
Bayes'sches statistisches Modell aufschreiben, sagen Sie,

2413
01:21:36,300 --> 01:21:37,860
wie Sie wissen, hier sind die Parameter und

2414
01:21:37,860 --> 01:21:39,300
hier ist, was die Priors auf den Parametern sind.

2415
01:21:39,300 --> 01:21:40,860


2416
01:21:40,860 --> 01:21:42,360
Um große Sprachmodelle. Ich denke, die

2417
01:21:42,360 --> 01:21:44,340
Priors sind und vielleicht neuronale Nüsse im

2418
01:21:44,340 --> 01:21:45,900
Allgemeinen  Die

2419
01:21:45,900 --> 01:21:47,880
Priors sind viel impliziter, also

2420
01:21:47,880 --> 01:21:49,679
gibt es einige Funktionen, die sie

2421
01:21:49,679 --> 01:21:52,080
leichter zu lernen finden als andere

2422
01:21:52,080 --> 01:21:53,760
Funktionen, und es gibt sogar einige Arbeit, die

2423
01:21:53,760 --> 01:21:55,860
versucht, herauszufinden, dass Sie eine

2424
01:21:55,860 --> 01:21:57,540
Aussage darüber kennen, was diese Art von impliziten

2425
01:21:57,540 --> 01:21:59,040
Priors sind,

2426
01:21:59,040 --> 01:22:00,840
aber das ist eigentlich so, wie ich denke

2427
01:22:00,840 --> 01:22:02,040
Über

2428
01:22:02,040 --> 01:22:02,940
ähm,

2429
01:22:02,940 --> 01:22:04,620
Sie kennen den Vergleich verschiedener

2430
01:22:04,620 --> 01:22:07,199
neuronaler Netzwerkarchitekturen,

2431
01:22:07,199 --> 01:22:09,060
ähm, das ist vielleicht etwas Beschwingtes,

2432
01:22:09,060 --> 01:22:10,500
und ich könnte dem zustimmen, dass

2433
01:22:10,500 --> 01:22:12,600
Sie Priori finden müssen, die es ihnen ermöglichen,

2434
01:22:12,600 --> 01:22:14,400
die Dinge zu lernen, die Kinder

2435
01:22:14,400 --> 01:22:16,140
richtig lernen, und

2436
01:22:16,140 --> 01:22:19,080
ähm, nicht alle Architekturen werden es tun  Tun Sie das ähm

2437
01:22:19,080 --> 01:22:21,360
sogar unter Architekturen, die

2438
01:22:21,360 --> 01:22:23,280
vollständig werden oder in der Lage sind,

2439
01:22:23,280 --> 01:22:24,719
jede Art von Funktion zu lernen, nicht alle von ihnen

2440
01:22:24,719 --> 01:22:27,540
werden es tun, ähm, selbst bei

2441
01:22:27,540 --> 01:22:30,000
riesigen Datensatzgrößen, also

2442
01:22:30,000 --> 01:22:31,800
ähm, ich denke an diese Art der Suche über

2443
01:22:31,800 --> 01:22:34,080
neuronale Netzarchitekturen als  wirklich eine

2444
01:22:34,080 --> 01:22:36,360
Suche nach Priors,

2445
01:22:36,360 --> 01:22:38,580
ähm, aber es sind keine Priors, oder ich meine, man

2446
01:22:38,580 --> 01:22:39,719
könnte es sich als eine Suche nach

2447
01:22:39,719 --> 01:22:41,699
universeller Grammatik oder so etwas vorstellen, aber

2448
01:22:41,699 --> 01:22:44,340
es ist keine Priors oder universelle

2449
01:22:44,340 --> 01:22:46,560
Grammatik in dem Sinne, dass die Leute

2450
01:22:46,560 --> 01:22:48,540
so darüber gesprochen haben  eine explizite

2451
01:22:48,540 --> 01:22:50,159
Aussage darüber, welche Arten von Regeln

2452
01:22:50,159 --> 01:22:51,780
erlaubt sind oder eine explizite Aussage darüber,

2453
01:22:51,780 --> 01:22:53,880
welche Arten von Funktionen mit hoher

2454
01:22:53,880 --> 01:22:55,320
Wahrscheinlichkeit oder so etwas sind, es ist

2455
01:22:55,320 --> 01:22:57,360
alles implizit dort codiert,

2456
01:22:57,360 --> 01:22:58,560
ähm, ja, total, ich denke, ich denke,

2457
01:22:58,560 --> 01:23:00,540
das ist richtig. Ich meine, Sie wissen, dass die eigentliche

2458
01:23:00,540 --> 01:23:02,880
Frage reduziert wird  Der Raum dessen, was

2459
01:23:02,880 --> 01:23:05,100
diejenigen, die gleichermaßen schätzen, und wenn es auch nur

2460
01:23:05,100 --> 01:23:07,020
annähernd so ist wie das, was Menschen

2461
01:23:07,020 --> 01:23:09,300
tun, würde ich

2462
01:23:09,300 --> 01:23:11,219
zumindest sagen, dass Dinge wie qpt3 ein

2463
01:23:11,219 --> 01:23:13,380
Beweis dafür sind, dass Sie wissen, dass das

2464
01:23:13,380 --> 01:23:15,960
Erstellen voll funktionsfähiger syntaktischer

2465
01:23:15,960 --> 01:23:18,000
Kategorien aus der Oberflächenverteilungsanalyse

2466
01:23:18,000 --> 01:23:21,420
Dies allein ist möglich, das ist

2467
01:23:21,420 --> 01:23:24,120
ja, das ist richtig, aber Sie wissen, obwohl

2468
01:23:24,120 --> 01:23:27,300
ich sagen würde, dass die meisten Praktiker nicht

2469
01:23:27,300 --> 01:23:29,460
wirklich glauben, dass syntaktische Kategorien

2470
01:23:29,460 --> 01:23:31,380
angeboren sind, also ist das vorherige Problem

2471
01:23:31,380 --> 01:23:33,060
etwas weniger irrelevant, hier sind die

2472
01:23:33,060 --> 01:23:35,219
Operationen, die als angeboren festgelegt sind, also

2473
01:23:35,219 --> 01:23:37,500
das in  Die Syntaxdomäne sind bestimmte

2474
01:23:37,500 --> 01:23:39,659
linguistische Berechnungen, von denen gesagt wird, dass

2475
01:23:39,659 --> 01:23:41,159


2476
01:23:41,159 --> 01:23:42,719


2477
01:23:42,719 --> 01:23:44,100


2478
01:23:44,100 --> 01:23:46,800
sie sich in einer Kategorie befinden. Tatsächlich hat sogar Charles Yang um in den letzten Jahren zugegeben, dass sie vielleicht darin enthalten sind, aber vielleicht auch nicht,

2479
01:23:46,800 --> 01:23:49,679
so dass die Leute anderen

2480
01:23:49,679 --> 01:23:51,540
Dingen eine relevante Priorität eingeräumt haben  Wie

2481
01:23:51,540 --> 01:23:53,100
ähm, Sie kennen mich und Gary Markets haben

2482
01:23:53,100 --> 01:23:55,080
über Kompositionalität gesprochen, die

2483
01:23:55,080 --> 01:23:56,640
ein großes Problem zu sein scheint, also haben die Leute

2484
01:23:56,640 --> 01:23:59,520
Chat-GBT-Artikel in den BBC-Nachrichten gegeben, in denen sie darum gebeten haben, sie zu

2485
01:23:59,520 --> 01:24:02,280
komprimieren und sie dann erneut zu erklären.

2486
01:24:02,280 --> 01:24:04,920
Ein Beispiel, das ich gesehen habe, war Peter Smith 58 ist  Sie

2487
01:24:04,920 --> 01:24:06,480
werden wegen

2488
01:24:06,480 --> 01:24:09,060
Totschlags verhaftet und Sie bringen es dazu, es zu komprimieren

2489
01:24:09,060 --> 01:24:10,920
und erneut zu erklären, und es kommt heraus, dass

2490
01:24:10,920 --> 01:24:12,420
58 Personen wegen Totschlags angeklagt werden,

2491
01:24:12,420 --> 01:24:14,040
richtig, das ist ein ziemlich klares

2492
01:24:14,040 --> 01:24:15,659
Beispiel für einen Mangel an Kompositionalität, der

2493
01:24:15,659 --> 01:24:17,219
in jede Komprimierung eingebaut

2494
01:24:17,219 --> 01:24:19,260
ist und es gibt  Ein weiteres Beispiel,

2495
01:24:19,260 --> 01:24:20,940
bei dem es einige Beispiele

2496
01:24:20,940 --> 01:24:23,159
für potenzielle analoge Argumente gab, also

2497
01:24:23,159 --> 01:24:24,840
wissen Sie im Bing-Chat, dass Bing diese Fallenfunktion hat.

2498
01:24:24,840 --> 01:24:26,100


2499
01:24:26,100 --> 01:24:28,080
Die Frage ist, ob es nur

2500
01:24:28,080 --> 01:24:29,460
Metabeziehungen findet, die bereits

2501
01:24:29,460 --> 01:24:31,080
von Menschen dokumentiert wurden, oder ob es wirklich

2502
01:24:31,080 --> 01:24:33,420
neue Beziehungen schafft  Zeug,

2503
01:24:33,420 --> 01:24:35,100
das gebaut wird,

2504
01:24:35,100 --> 01:24:38,520
ähm, also weißt du, jemand hat mich gefragt, äh, hat mir

2505
01:24:38,520 --> 01:24:41,760
eine Tabelle gezeichnet, in der Jesus Christus mit dem

2506
01:24:41,760 --> 01:24:44,640
Nokia 9910 verglichen wurde, genau wie das Handy Nokia

2507
01:24:44,640 --> 01:24:46,080
9910,

2508
01:24:46,080 --> 01:24:47,520
und da stand, du weißt, dass sie

2509
01:24:47,520 --> 01:24:49,860
die Veröffentlichungsdaten verglichen, es hat die Größe und

2510
01:24:49,860 --> 01:24:53,280
das Gewicht verglichen, mit dem es die CPU verglichen hat

2511
01:24:53,280 --> 01:24:55,140
Jesu allmächtiges Wissen, es

2512
01:24:55,140 --> 01:24:57,600
verglich die Erinnerung an das Telefon mit

2513
01:24:57,600 --> 01:25:00,600
der allwissenden Natur Gottes, richtig, ähm,

2514
01:25:00,600 --> 01:25:02,760
ich glaube, es sagte auch, dass sie

2515
01:25:02,760 --> 01:25:04,560
beide auferstanden seien, weil das Nokia

2516
01:25:04,560 --> 01:25:06,239
ein paar Mal neu aufgelegt wurde, richtig, also

2517
01:25:06,239 --> 01:25:08,159
klingt das Nokia wie ein  großartige Antwort, was daran

2518
01:25:08,159 --> 01:25:10,560
falsch ist, das ist in Ordnung, es

2519
01:25:10,560 --> 01:25:12,120
mag sein, dass es sich sehr nach

2520
01:25:12,120 --> 01:25:13,860
analogem Denken anhört, aber dann gab es auch

2521
01:25:13,860 --> 01:25:15,420
einige ziemlich seltsame, bei denen es

2522
01:25:15,420 --> 01:25:17,100
so war, als ob Sie wissen, dass die Kamera nein sagte,

2523
01:25:17,100 --> 01:25:19,679
es gab nur die Beschreibung von Jesus oder es ist

2524
01:25:19,679 --> 01:25:21,659
nicht wirklich was  Eine Kamera ist, es gibt

2525
01:25:21,659 --> 01:25:24,000
einige Dinge, die wie analoges

2526
01:25:24,000 --> 01:25:27,860
Denken aussehen, aber es ist unklar, ja,

2527
01:25:27,860 --> 01:25:31,440
hey, ich denke, das klingt nach einer

2528
01:25:31,440 --> 01:25:33,020
großartigen Antwort für mich. Ich wollte

2529
01:25:33,020 --> 01:25:36,360
sagen, wie Sie sagten,

2530
01:25:36,360 --> 01:25:38,100
große Sprachmodelle lernen ihre

2531
01:25:38,100 --> 01:25:39,719
Existenz Beweis der Wortart

2532
01:25:39,719 --> 01:25:41,520
Kategorien, aber als ob sie nicht nur

2533
01:25:41,520 --> 01:25:43,380
Wortart-Kategorien richtig ausgeben,

2534
01:25:43,380 --> 01:25:45,719
als ob sie eine Menge grammatikalisch-

2535
01:25:45,719 --> 01:25:47,640
syntaktisches Wissen haben,

2536
01:25:47,640 --> 01:25:50,880
ähm, und außerdem haben sie eine

2537
01:25:50,880 --> 01:25:52,980
Menge semantisches Wissen und wahrscheinlich etwas

2538
01:25:52,980 --> 01:25:55,080
pragmatisches Wissen, und Sie wissen, dass sie es sind

2539
01:25:55,080 --> 01:25:58,080
nicht schlecht im Übersetzen und als wäre es

2540
01:25:58,080 --> 01:25:59,880
viel mehr, was sie

2541
01:25:59,880 --> 01:26:01,139
entdeckt haben,

2542
01:26:01,139 --> 01:26:03,840
als nur Wortarten Kategorien ähm äh,

2543
01:26:03,840 --> 01:26:07,020
nun, es tut mir leid, ich sagte, es tut mir leid, es ist

2544
01:26:07,020 --> 01:26:08,940
eine technische Kategorie,

2545
01:26:08,940 --> 01:26:11,040
ja, tja, tut mir leid, ja, ja, aber

2546
01:26:11,040 --> 01:26:13,080
sie haben es entdeckt  viel mehr als das

2547
01:26:13,080 --> 01:26:13,980
ähm ja ähm

2548
01:26:13,980 --> 01:26:15,600


2549
01:26:15,600 --> 01:26:18,780
Ich werde als äh Teaser Slash

2550
01:26:18,780 --> 01:26:20,699
Motivator für Sie beide hoffentlich

2551
01:26:20,699 --> 01:26:23,040
in Zukunft mit oder

2552
01:26:23,040 --> 01:26:24,540
ohne andere Gäste ein paar der

2553
01:26:24,540 --> 01:26:26,520
spannenden Fragen nur für uns

2554
01:26:26,520 --> 01:26:28,679
in dieses Transkript aufnehmen und  dann vielen

2555
01:26:28,679 --> 01:26:30,060
Dank an Ellie und Steven, dass Sie sich

2556
01:26:30,060 --> 01:26:31,260
angeschlossen haben. Nur ein paar der letzten

2557
01:26:31,260 --> 01:26:33,719
Fragen, die Juan gestellt wurden, fragte, wie

2558
01:26:33,719 --> 01:26:35,820
kleine Transformers Jang überhaupt 2020 im

2559
01:26:35,820 --> 01:26:38,580
Vergleich zu Kindern lernen, die Sprache lernen,

2560
01:26:38,580 --> 01:26:40,860
um 96, fragte, was Sie über

2561
01:26:40,860 --> 01:26:42,800
implizite Prioren im Vergleich zu tierischem Instinkt denken,

2562
01:26:42,800 --> 01:26:45,780
fragte Rojda  Welche Beschränkungen, die Platz

2563
01:26:45,780 --> 01:26:48,420
in llms haben, bekommen sie nicht durch Training,

2564
01:26:48,420 --> 01:26:50,699
entdecken sie es, das ist nicht das, was

2565
01:26:50,699 --> 01:26:52,500
sie am Anfang vielleicht implementieren,

2566
01:26:52,500 --> 01:26:54,780
und es gibt noch viele weitere Fragen, also

2567
01:26:54,780 --> 01:26:57,540
hoffe ich, dass wir alle die

2568
01:26:57,540 --> 01:27:00,600
Werke des anderen überprüfen und erneut lesen können  Kommen Sie

2569
01:27:00,600 --> 01:27:04,020
in Zukunft für 41.2 zusammen.

2570
01:27:04,020 --> 01:27:06,060
Danke Elliot und Steven für diesen

2571
01:27:06,060 --> 01:27:08,280
exzellenten Stream. Danke Dave. Danke

2572
01:27:08,280 --> 01:27:10,739
euch beiden. Ja, vielen Dank. Auf Wiedersehen

2573
01:27:10,739 --> 01:27:15,379



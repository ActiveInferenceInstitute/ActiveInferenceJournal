1
00:00:05,759 --> 00:00:08,400
大家好，欢迎大家来到 Octave

2
00:00:08,400 --> 00:00:10,860
Inference Institute，这是

3
00:00:10,860 --> 00:00:15,839
2023 年 4 月 25 日的第 41.1 号活跃访客流，

4
00:00:15,839 --> 00:00:18,480
我们与 Elliot Murphy 和

5
00:00:18,480 --> 00:00:20,939
Stephen piantadosi 一起来到这里，这将是

6
00:00:20,939 --> 00:00:23,039
一场相当大的讨论，我们将从

7
00:00:23,039 --> 00:00:24,779
Stephen 和

8
00:00:24,779 --> 00:00:27,359
Elliot 的开场白开始 Elliott 将 然后提出一些

9
00:00:27,359 --> 00:00:28,920
问题，最后我们将进行公开

10
00:00:28,920 --> 00:00:32,759
讨论所以史蒂文，请

11
00:00:32,759 --> 00:00:34,739
感谢您的加入，并感谢您的

12
00:00:34,739 --> 00:00:36,960
开场白，

13
00:00:36,960 --> 00:00:39,780
嗨，我是史蒂夫·皮安塔多西，我是

14
00:00:39,780 --> 00:00:42,239


15
00:00:42,239 --> 00:00:44,700
加州大学伯克利分校的心理学和神经科学教授，

16
00:00:44,700 --> 00:00:46,860
嗯， 呃，我想

17
00:00:46,860 --> 00:00:48,899
我们来这里的部分原因是我最近写了

18
00:00:48,899 --> 00:00:51,719
一篇关于大型语言模型的论文，

19
00:00:51,719 --> 00:00:53,940
部分是为了表达

20
00:00:53,940 --> 00:00:55,199
对

21
00:00:55,199 --> 00:00:57,300
他们在

22
00:00:57,300 --> 00:00:59,100
学习语法和

23
00:00:59,100 --> 00:01:00,719
语义方面取得的成就的热情。

24
00:01:00,719 --> 00:01:03,059
在某种程度上指出，我认为

25
00:01:03,059 --> 00:01:04,979
这些模型确实改变了我们

26
00:01:04,979 --> 00:01:06,780
应该如何思考语言，

27
00:01:06,780 --> 00:01:08,520
嗯，我们应该如何思考

28
00:01:08,520 --> 00:01:11,939
语言表征理论和呃

29
00:01:11,939 --> 00:01:13,560
语法理论，嗯，

30
00:01:13,560 --> 00:01:17,220
可能还有学习理论

31
00:01:17,220 --> 00:01:19,080


32
00:01:19,080 --> 00:01:21,960
meffee 我是

33
00:01:21,960 --> 00:01:23,040


34
00:01:23,040 --> 00:01:25,799
得克萨斯州 UT Health 神经外科系的博士后，

35
00:01:25,799 --> 00:01:27,240
嗯，我非常感兴趣地阅读了 Steven 的论文，

36
00:01:27,240 --> 00:01:29,759
我读了很多人，

37
00:01:29,759 --> 00:01:31,619
有一些趋同的领域，但

38
00:01:31,619 --> 00:01:32,820
我今天想关注的是

39
00:01:32,820 --> 00:01:34,979
回应 Stephen 并尝试

40
00:01:34,979 --> 00:01:36,600
与 Divergence 领域做一些事情，

41
00:01:36,600 --> 00:01:38,640


42
00:01:38,640 --> 00:01:41,700
嗯，所以你知道 Steven 的论文是

43
00:01:41,700 --> 00:01:43,619
基于这样的想法，即现代机器学习

44
00:01:43,619 --> 00:01:46,220
已经颠覆并绕过了

45
00:01:46,220 --> 00:01:48,119
乔姆斯基方法的整个理论框架，

46
00:01:48,119 --> 00:01:49,860
所以我想回应

47
00:01:49,860 --> 00:01:51,479
一些 这些主要论点和

48
00:01:51,479 --> 00:01:52,560
文献中的其他一些相关论点，

49
00:01:52,560 --> 00:01:54,060
嗯，有些人在

50
00:01:54,060 --> 00:01:55,799
听可能有一些见解

51
00:01:55,799 --> 00:01:58,500
和想法，所以这是一个非常普遍的

52
00:01:58,500 --> 00:02:00,720
批评，说大型语言

53
00:02:00,720 --> 00:02:03,180
模型只是预测下一个标记，而

54
00:02:03,180 --> 00:02:04,079


55
00:02:04,079 --> 00:02:05,579
这显然有点 陈词滥调是对的，

56
00:02:05,579 --> 00:02:07,619
嗯，不太正确，他们

57
00:02:07,619 --> 00:02:09,419
不只是预测下一个标记，他们似乎也在虚构

58
00:02:09,419 --> 00:02:11,700
他们似乎产生

59
00:02:11,700 --> 00:02:14,340
幻觉，他们可能在撒谎，他们随机地

60
00:02:14,340 --> 00:02:16,440
对同一个问题提供不同的答案，

61
00:02:16,440 --> 00:02:18,300
而且他们似乎

62
00:02:18,300 --> 00:02:20,220
随机模仿类似语言的

63
00:02:20,220 --> 00:02:22,020
结构他们

64
00:02:22,020 --> 00:02:23,819
有时他们不应该纠正自己，

65
00:02:23,819 --> 00:02:25,440
如果你稍微推动他们，他们

66
00:02:25,440 --> 00:02:27,000
有时会改变主意

67
00:02:27,000 --> 00:02:28,560


68
00:02:28,560 --> 00:02:30,060


69
00:02:30,060 --> 00:02:31,800


70
00:02:31,800 --> 00:02:33,680


71
00:02:33,680 --> 00:02:36,120
他们正在寻找一种您

72
00:02:36,120 --> 00:02:38,040
知道的类似口径，因此这些模型

73
00:02:38,040 --> 00:02:40,620
似乎可以做各种疯狂的事情，

74
00:02:40,620 --> 00:02:41,940
嗯，在过去的 10 年中，已经

75
00:02:41,940 --> 00:02:43,500


76
00:02:43,500 --> 00:02:45,900
开发出一系列不同的系统，就像我们是烟草床一样，

77
00:02:45,900 --> 00:02:47,580
它们中的每一个都是 基于不同的

78
00:02:47,580 --> 00:02:49,620
神经网络方法但最终它们

79
00:02:49,620 --> 00:02:51,180
似乎都采用单词并

80
00:02:51,180 --> 00:02:53,220
通过数百或

81
00:02:53,220 --> 00:02:56,220
数千个数字列表来表征它们，因此 G23 网络

82
00:02:56,220 --> 00:03:00,000


83
00:03:00,000 --> 00:03:02,160
在其架构中具有 1750 亿个权重和 96 个注意力头，据

84
00:03:02,160 --> 00:03:03,780
我所知 史蒂文可以

85
00:03:03,780 --> 00:03:06,060
在这里纠正我，我们真的不太清楚

86
00:03:06,060 --> 00:03:07,319
这些不同部分的

87
00:03:07,319 --> 00:03:09,180
真正含义它只是看起来

88
00:03:09,180 --> 00:03:10,920
像注意力头一样工作，

89
00:03:10,920 --> 00:03:13,680
gpt3 可以关注

90
00:03:13,680 --> 00:03:15,360
字符串中更早的标记以提供帮助

91
00:03:15,360 --> 00:03:17,040
他们预测下一个标记，但

92
00:03:17,040 --> 00:03:18,659
整个架构从头到尾

93
00:03:18,659 --> 00:03:21,599
都是基于工程的动机，

94
00:03:21,599 --> 00:03:22,500
嗯，

95
00:03:22,500 --> 00:03:24,659
我总是想知道

96
00:03:24,659 --> 00:03:27,360


97
00:03:27,360 --> 00:03:28,800
来自不同科技公司的这些 llms 的所有模型都失败了，

98
00:03:28,800 --> 00:03:30,300
就像这些公司

99
00:03:30,300 --> 00:03:31,980
经常出现的那样

100
00:03:31,980 --> 00:03:33,060
嗯，你知道，让他们看起来好像拥有

101
00:03:33,060 --> 00:03:34,980
这些模型，开箱即用，

102
00:03:34,980 --> 00:03:36,720


103
00:03:36,720 --> 00:03:38,459


104
00:03:38,459 --> 00:03:40,739


105
00:03:40,739 --> 00:03:42,780


106
00:03:42,780 --> 00:03:45,360
效果很好 很快，其中一家

107
00:03:45,360 --> 00:03:46,980
公司将发布一个大型

108
00:03:46,980 --> 00:03:49,500
语言模型，呃，叫做 Jesus 或

109
00:03:49,500 --> 00:03:50,879
其他我不知道的东西，

110
00:03:50,879 --> 00:03:53,040
嗯，但他们总是说这是我们的

111
00:03:53,040 --> 00:03:54,900
新基础模型，它叫做 Picasso，

112
00:03:54,900 --> 00:03:56,280
这是我们尝试的第一个模型，我们很棒，

113
00:03:56,280 --> 00:03:58,140
没有问题

114
00:03:58,140 --> 00:03:59,819
开箱即用，但我总是想知道

115
00:03:59,819 --> 00:04:01,680
Oliver Black boxes

116
00:04:01,680 --> 00:04:03,659
每次都失败了，似乎没有

117
00:04:03,659 --> 00:04:05,760
一种非常开放和清晰的

118
00:04:05,760 --> 00:04:07,379
结构来

119
00:04:07,379 --> 00:04:09,840
选择你知道的一种

120
00:04:09,840 --> 00:04:11,939
或另一种模型背后的科学推理 呃，但我可能会再次接受

121
00:04:11,939 --> 00:04:14,280
纠正，

122
00:04:14,280 --> 00:04:16,798
嗯，所以即使是基本的语言模型也能

123
00:04:16,798 --> 00:04:18,298


124
00:04:18,298 --> 00:04:20,699
像基本的网络预测一样在嗯上做得很好，所以

125
00:04:20,699 --> 00:04:22,139
问题是这些工具是否提供了

126
00:04:22,139 --> 00:04:23,520
对传统

127
00:04:23,520 --> 00:04:25,320
心理语言学概念（如语法

128
00:04:25,320 --> 00:04:27,300
和传递）的洞察力所以 这就是为什么我

129
00:04:27,300 --> 00:04:29,040
更喜欢 10 Focus 模型而不是

130
00:04:29,040 --> 00:04:30,960


131
00:04:30,960 --> 00:04:32,880
像 cyberveras 这样的人建议的语言模型，

132
00:04:32,880 --> 00:04:34,560
所以有人指出，当他们将 python 代码和自然语言一样好时，没有人

133
00:04:34,560 --> 00:04:36,479
真正认为 llms 告诉我们任何关于

134
00:04:36,479 --> 00:04:38,580
python 的深刻知识

135
00:04:38,580 --> 00:04:40,080


136
00:04:40,080 --> 00:04:42,120
python 是一种

137
00:04:42,120 --> 00:04:43,740
具有短语结构语法的符号语言

138
00:04:43,740 --> 00:04:46,620
，没有人说 llms 正在揭开

139
00:04:46,620 --> 00:04:48,600
python 的秘密，所以只是

140
00:04:48,600 --> 00:04:50,940
在这里放各种，他说如果 n 个模型可以

141
00:04:50,940 --> 00:04:52,620


142
00:04:52,620 --> 00:04:54,000
基于它们

143
00:04:54,000 --> 00:04:56,040
在语言任务上的成功被解释为自然语言的解释性理论，那么在

144
00:04:56,040 --> 00:04:57,540
没有反驳论点，它们也应该

145
00:04:57,540 --> 00:04:58,919
是计算机语言的良好解释理论，

146
00:04:58,919 --> 00:05:01,139
呃，因此，

147
00:05:01,139 --> 00:05:02,699
成功的自然

148
00:05:02,699 --> 00:05:04,620
语言模型不能用作

149
00:05:04,620 --> 00:05:06,540
反对亚马逊语言生成短语结构的证据，

150
00:05:06,540 --> 00:05:07,800


151
00:05:07,800 --> 00:05:09,720
所以语料库模型确实是一个更

152
00:05:09,720 --> 00:05:11,280
合适的术语，出于其他原因，

153
00:05:11,280 --> 00:05:13,500
人们 像 Emily bender 和其他一些人

154
00:05:13,500 --> 00:05:15,300
已经表明训练语料库的特征

155
00:05:15,300 --> 00:05:16,979
事实上我认为斯蒂芬引用了

156
00:05:16,979 --> 00:05:17,940
你在你的论文中引用这个

157
00:05:17,940 --> 00:05:19,860
实际上作为限制

158
00:05:19,860 --> 00:05:21,120
嗯他们表明

159
00:05:21,120 --> 00:05:22,680
训练语料库的特征可以严重影响

160
00:05:22,680 --> 00:05:24,600
铺设过程所以它已经被证明

161
00:05:24,600 --> 00:05:25,800
大型语言

162
00:05:25,800 --> 00:05:27,780
模型在语言课程上的表现确实

163
00:05:27,780 --> 00:05:29,699
受到训练语料库多样性的严重影响，

164
00:05:29,699 --> 00:05:31,440


165
00:05:31,440 --> 00:05:33,180
嗯，但自然语言本身并没有

166
00:05:33,180 --> 00:05:35,340
偏见，

167
00:05:35,340 --> 00:05:37,199
它只是一个计算系统，人类

168
00:05:37,199 --> 00:05:39,120
可能会在他们所说的和他们的行为上产生偏见

169
00:05:39,120 --> 00:05:41,039
但是自然语言本身并

170
00:05:41,039 --> 00:05:43,020
没有偏见 如此大的语言

171
00:05:43,020 --> 00:05:45,180
模型 因此我似乎很难

172
00:05:45,180 --> 00:05:47,880
同意它们受到

173
00:05:47,880 --> 00:05:49,500
各种偏见的影响

174
00:05:49,500 --> 00:05:51,000
因此它们不能真正成为

175
00:05:51,000 --> 00:05:52,020
语言模型 它们是某种事物的模型

176
00:05:52,020 --> 00:05:53,880
其他所以只是为了结束这个

177
00:05:53,880 --> 00:05:55,620
论点，

178
00:05:55,620 --> 00:05:58,139
嗯，你知道，即使 llms 显然再次

179
00:05:58,139 --> 00:05:59,759
接触到儿童的更多语言

180
00:05:59,759 --> 00:06:01,320
经验，这是

181
00:06:01,320 --> 00:06:02,699
斯蒂芬可以看到

182
00:06:02,699 --> 00:06:04,680
并在他的论文中谈论的其他东西，即使如此，

183
00:06:04,680 --> 00:06:06,360
他们的学习成果可能仍然

184
00:06:06,360 --> 00:06:08,160
与解决原则上可以学习的语法概括相关，

185
00:06:08,160 --> 00:06:09,840


186
00:06:09,840 --> 00:06:11,580
所以我同意

187
00:06:11,580 --> 00:06:12,660
这里的声明你知道

188
00:06:12,660 --> 00:06:14,100
原则上他们可以告诉我们一些关于

189
00:06:14,100 --> 00:06:16,080
可学习性的东西，而不是

190
00:06:16,080 --> 00:06:17,639
像你知道的广泛的习得

191
00:06:17,639 --> 00:06:20,520
框架那样的东西，但

192
00:06:20,520 --> 00:06:21,900
我认为你可以做到这一点 也许现在就说，

193
00:06:21,900 --> 00:06:24,539
表明一些归纳性偏见

194
00:06:24,539 --> 00:06:26,819
对学习来说不是必需的，这与

195
00:06:26,819 --> 00:06:28,139
表明它

196
00:06:28,139 --> 00:06:30,060
不存在于儿童身上并不是一回事，所以

197
00:06:30,060 --> 00:06:31,560
关于你是否

198
00:06:31,560 --> 00:06:32,639
知道负面证据、指导

199
00:06:32,639 --> 00:06:34,500
、纠正和反馈一直存在着长期的争论

200
00:06:34,500 --> 00:06:36,720
语言学习

201
00:06:36,720 --> 00:06:39,120
对婴儿和儿童来说是必要的，甚至是有用的，

202
00:06:39,120 --> 00:06:40,440
嗯，但现在我有点同意

203
00:06:40,440 --> 00:06:42,479
Eugene Choi 和 Gary Marcus 以及

204
00:06:42,479 --> 00:06:44,340
其他人的观点，他们强调了

205
00:06:44,340 --> 00:06:45,660
你知道 llms 目前的培训成本非常昂贵，

206
00:06:45,660 --> 00:06:46,440


207
00:06:46,440 --> 00:06:48,600
这显然是一个

208
00:06:48,600 --> 00:06:50,400
集中私人的例子 权力掌握在

209
00:06:50,400 --> 00:06:51,960
一些科技公司手中，他们对

210
00:06:51,960 --> 00:06:54,539
环境的影响是巨大的，

211
00:06:54,539 --> 00:06:56,039
嗯，你知道，很多人

212
00:06:56,039 --> 00:06:57,300
在这里的评估中没有那么拘束和保守，

213
00:06:57,300 --> 00:06:58,979


214
00:06:58,979 --> 00:07:00,660
嗯，除了加里马库斯和尤金之外，这要少得多，

215
00:07:00,660 --> 00:07:02,880
所以比尔盖茨最近

216
00:07:02,880 --> 00:07:05,100
写道，聊天 GPT 是

217
00:07:05,100 --> 00:07:06,960
嗯 自

218
00:07:06,960 --> 00:07:11,400
图形用户界面图形用户界面 GUI

219
00:07:11,400 --> 00:07:13,020
um 和 Henry Kissinger 二月份

220
00:07:13,020 --> 00:07:15,240
在华尔街日报上写道，随着聊天

221
00:07:15,240 --> 00:07:17,400
gbt 的容量变得更广泛，他们

222
00:07:17,400 --> 00:07:19,560
将重新定义人类知识，加速

223
00:07:19,560 --> 00:07:21,780
我们现实结构的变化，并

224
00:07:21,780 --> 00:07:23,220
重组政治和社会

225
00:07:23,220 --> 00:07:25,620
生成人工智能的发布是为了产生

226
00:07:25,620 --> 00:07:28,319
新的人类意识形式，所以

227
00:07:28,319 --> 00:07:30,419
目前正在发生非常激进的主张

228
00:07:30,419 --> 00:07:32,220


229
00:07:32,220 --> 00:07:35,099


230
00:07:35,099 --> 00:07:36,840


231
00:07:36,840 --> 00:07:39,240


232
00:07:39,240 --> 00:07:40,680
更具体

233
00:07:40,680 --> 00:07:42,180
地知道，只是把它放回史蒂文

234
00:07:42,180 --> 00:07:44,280
这里我想提出这个问题，嗯，

235
00:07:44,280 --> 00:07:46,919
rorski 和 Beaumont 的批评，

236
00:07:46,919 --> 00:07:49,259
我认为他在 lingbuzz 上读过嗯，

237
00:07:49,259 --> 00:07:50,520


238
00:07:50,520 --> 00:07:51,240


239
00:07:51,240 --> 00:07:53,280
我想你在 Twitter 上看到你

240
00:07:53,280 --> 00:07:54,419
不喜欢这个回应 他们

241
00:07:54,419 --> 00:07:56,819
之所以反对，是因为他们提出的反对意见是

242
00:07:56,819 --> 00:07:58,500
你知道科学是演绎逻辑的一个例子，

243
00:07:58,500 --> 00:08:00,479
你的反对意见是

244
00:08:00,479 --> 00:08:02,340
科学不是演绎的，它是归纳的，

245
00:08:02,340 --> 00:08:04,380
但我认为一般观点

246
00:08:04,380 --> 00:08:06,900
可能更准确，即你

247
00:08:06,900 --> 00:08:08,520
不能，你可以 使用

248
00:08:08,520 --> 00:08:10,620
语言模型可以很好地预测

249
00:08:10,620 --> 00:08:13,020
人类的某些语言行为和某些

250
00:08:13,020 --> 00:08:15,300
神经成像反应这一事实，您不能

251
00:08:15,300 --> 00:08:17,460
单独使用它来声称它们可以产生

252
00:08:17,460 --> 00:08:19,199
人类语言理论，

253
00:08:19,199 --> 00:08:21,000
所以在您的论文 Stephen 中，您知道

254
00:08:21,000 --> 00:08:23,220
这似乎 某些结构

255
00:08:23,220 --> 00:08:25,020
比其他结构更好 正确的注意力

256
00:08:25,020 --> 00:08:26,819
机制很重要 预测很

257
00:08:26,819 --> 00:08:28,680
重要 语义表示很

258
00:08:28,680 --> 00:08:30,180
重要 因此我们

259
00:08:30,180 --> 00:08:32,219
目前可以基于这些模型收集正确的

260
00:08:32,219 --> 00:08:34,260
嗯但到目前为止，这真的是我

261
00:08:34,260 --> 00:08:36,000
在文献中能够收集到的全部

262
00:08:36,000 --> 00:08:37,559
不确定你在这里是否有更多见解，

263
00:08:37,559 --> 00:08:39,899
所以 Rosky 和 ​​Boomer 使用了

264
00:08:39,899 --> 00:08:42,719
预测不佳但强有力的解释的例子

265
00:08:42,719 --> 00:08:44,760
正确的解释力而不是

266
00:08:44,760 --> 00:08:46,500
预测准确性构成了

267
00:08:46,500 --> 00:08:48,120
现代科学的基础，我想

268
00:08:48,120 --> 00:08:49,440
稍后再探讨这个，也许

269
00:08:49,440 --> 00:08:50,820
嗯但是现代语言模型 可以

270
00:08:50,820 --> 00:08:52,440
准确地模拟人类语言的一部分，

271
00:08:52,440 --> 00:08:54,180
但它们也可以在人类无法学习并且难以处理的

272
00:08:54,180 --> 00:08:55,920
不可能的语言和不自然的

273
00:08:55,920 --> 00:08:58,200
结构上表现得非常好，

274
00:08:58,200 --> 00:09:00,360
我

275
00:09:00,360 --> 00:09:01,560
知道你对这些批评很熟悉，嗯，

276
00:09:01,560 --> 00:09:03,060


277
00:09:03,060 --> 00:09:04,380
但你是 绝对不是一个人同时在这里，

278
00:09:04,380 --> 00:09:08,339
所以 uh Elia

279
00:09:08,339 --> 00:09:10,320
um 是 open AI 的首席科学家，他

280
00:09:10,320 --> 00:09:12,180
最近在接受采访时说，足够好地

281
00:09:12,180 --> 00:09:13,680
预测下一个标记意味着什么，

282
00:09:13,680 --> 00:09:15,540
这意味着你了解

283
00:09:15,540 --> 00:09:17,940
导致

284
00:09:17,940 --> 00:09:19,920
创建的潜在现实 这个标志

285
00:09:19,920 --> 00:09:21,779
与这里的文献

286
00:09:21,779 --> 00:09:23,100
中许多更保守的主张完全不同，

287
00:09:23,100 --> 00:09:24,540


288
00:09:24,540 --> 00:09:26,519
嗯，而且你知道我只是

289
00:09:26,519 --> 00:09:27,839
回应说

290
00:09:27,839 --> 00:09:30,019
科学的不同组成部分可以是

291
00:09:30,019 --> 00:09:32,580
归纳的也可以是演绎的，对吧，它不是

292
00:09:32,580 --> 00:09:34,140
真的，或者你有 一个现有的

293
00:09:34,140 --> 00:09:36,300
理论，你制定超假设，

294
00:09:36,300 --> 00:09:38,519
你收集数据，你分析它，

295
00:09:38,519 --> 00:09:40,200
这是一种演绎的演绎

296
00:09:40,200 --> 00:09:41,880
过程，但也有一些情况，你

297
00:09:41,880 --> 00:09:43,680
从一个特定的观察开始，你会

298
00:09:43,680 --> 00:09:44,940
发现一些模式，然后你得出正确的

299
00:09:44,940 --> 00:09:46,860
一般结论，然后是

300
00:09:46,860 --> 00:09:49,380
你神奇地

301
00:09:49,380 --> 00:09:52,019
发明的绑架 假设并减少

302
00:09:52,019 --> 00:09:53,760
假设空间，你不会真的说

303
00:09:53,760 --> 00:09:55,620
演绎推理是不科学的

304
00:09:55,620 --> 00:09:58,200
或归纳推理是不科学的

305
00:09:58,200 --> 00:10:00,360
或归纳推理是不科学的，

306
00:10:00,360 --> 00:10:01,800
这些都是不同的做事方式，

307
00:10:01,800 --> 00:10:03,540


308
00:10:03,540 --> 00:10:05,459
嗯，我的意思是在你的论文中你给出了

309
00:10:05,459 --> 00:10:08,399
使用的例子 预测飓风

310
00:10:08,399 --> 00:10:09,779
和大流行病的模型作为与

311
00:10:09,779 --> 00:10:12,060


312
00:10:12,060 --> 00:10:13,860
科学一样严格的例子，然后你恳求你的

313
00:10:13,860 --> 00:10:16,019
读者得出结论，

314
00:10:16,019 --> 00:10:18,120
语言模型的情况没有什么不同，

315
00:10:18,120 --> 00:10:19,920
嗯，但我想对我来说，问题是

316
00:10:19,920 --> 00:10:22,200
预测飓风的模型没有 在

317
00:10:22,200 --> 00:10:23,940
回答

318
00:10:23,940 --> 00:10:25,740
什么是飓风的问题时，

319
00:10:25,740 --> 00:10:27,420
准确预测天气的正确模型

320
00:10:27,420 --> 00:10:29,040
非常准确，但他们

321
00:10:29,040 --> 00:10:30,600
不是你知道他们与

322
00:10:30,600 --> 00:10:32,700
气象部门保持一致，但他们不能替代

323
00:10:32,700 --> 00:10:34,380
它，

324
00:10:34,380 --> 00:10:35,760
嗯，所以我想我' 只是你知道把它

325
00:10:35,760 --> 00:10:37,620
交给你是的

326
00:10:37,620 --> 00:10:41,820
好吧呃那里有很多嗯

327
00:10:41,820 --> 00:10:43,800
我想我可以先

328
00:10:43,800 --> 00:10:45,240
说嗯嗯

329
00:10:45,240 --> 00:10:47,579
我同意很多这些

330
00:10:47,579 --> 00:10:51,300
批评是正确的关于呃这些

331
00:10:51,300 --> 00:10:54,000
模型被呃你知道

332
00:10:54,000 --> 00:10:56,220
一两家公司，嗯，

333
00:10:56,220 --> 00:10:59,160
这是非常非常有问题的，

334
00:10:59,160 --> 00:11:01,320
嗯，你知道他们有各种各样的

335
00:11:01,320 --> 00:11:03,480
偏见，因为

336
00:11:03,480 --> 00:11:04,980
他们接受了来自互联网的文本培训，

337
00:11:04,980 --> 00:11:06,180


338
00:11:06,180 --> 00:11:08,640
嗯，这是非常有问题的，

339
00:11:08,640 --> 00:11:10,980
嗯，你知道，我当然同意，

340
00:11:10,980 --> 00:11:13,079
呃事情在 至少目前

341
00:11:13,079 --> 00:11:16,500
这些模型做得不好所以

342
00:11:16,500 --> 00:11:18,420
嗯我认为很容易找到

343
00:11:18,420 --> 00:11:20,640
你知道的问题的例子和

344
00:11:20,640 --> 00:11:23,100
会绊倒他们的问题

345
00:11:23,100 --> 00:11:25,500
嗯我想为什么我对它们感到兴奋

346
00:11:25,500 --> 00:11:26,760
虽然

347
00:11:26,760 --> 00:11:29,820
嗯不是 呃不一定在这些

348
00:11:29,820 --> 00:11:32,399
方面是正确的，但在

349
00:11:32,399 --> 00:11:34,920
语言性能方面，

350
00:11:34,920 --> 00:11:38,459
特别是语法和语义，

351
00:11:38,459 --> 00:11:40,980
嗯，我认为它们远远超出

352
00:11:40,980 --> 00:11:43,019
任何其他领域的任何其他理论，

353
00:11:43,019 --> 00:11:46,920
所以没有其他

354
00:11:46,920 --> 00:11:49,380
理论 语言学或计算机

355
00:11:49,380 --> 00:11:53,100
科学可以让你知道长

356
00:11:53,100 --> 00:11:56,700
连贯的语法呃文本段落

357
00:11:56,700 --> 00:11:58,500


358
00:11:58,500 --> 00:12:01,140
嗯所以有点承认他们所有的

359
00:12:01,140 --> 00:12:04,920
问题因为你知道

360
00:12:04,920 --> 00:12:08,220
公司部署的工具或东西

361
00:12:08,220 --> 00:12:09,959
嗯嗯还有这个问题

362
00:12:09,959 --> 00:12:12,899
他们在处理语言方面表现如何，

363
00:12:12,899 --> 00:12:14,760


364
00:12:14,760 --> 00:12:16,320
嗯，我认为这是很多

365
00:12:16,320 --> 00:12:17,760
热情的来源，就

366
00:12:17,760 --> 00:12:20,160


367
00:12:20,160 --> 00:12:23,700
语言能力而言，真的没有什么比他们更遥远的了，

368
00:12:23,700 --> 00:12:24,899
嗯，这就是我

369
00:12:24,899 --> 00:12:27,660
认为的 是令人兴奋的，所以是的，我

370
00:12:27,660 --> 00:12:29,220
同意你开始的这些事情，

371
00:12:29,220 --> 00:12:31,320


372
00:12:31,320 --> 00:12:33,300
嗯，但是就像我认为在

373
00:12:33,300 --> 00:12:35,100
语法和语义方面一样，

374
00:12:35,100 --> 00:12:37,079
没有其他理论可以

375
00:12:37,079 --> 00:12:39,240
与它们相提并论，嗯，

376
00:12:39,240 --> 00:12:40,140


377
00:12:40,140 --> 00:12:42,120
但让我来推动 那时候是的，

378
00:12:42,120 --> 00:12:44,940
是的，我会反对我在语言学系采访过的

379
00:12:44,940 --> 00:12:46,320
很多人的主要反对意见，

380
00:12:46,320 --> 00:12:48,000


381
00:12:48,000 --> 00:12:50,760
他们就像很多一般人一样，你

382
00:12:50,760 --> 00:12:53,100
首先知道你的论文是真的

383
00:12:53,100 --> 00:12:54,120
说得好，

384
00:12:54,120 --> 00:12:55,620
嗯，你知道你' 没错，他们确实做得

385
00:12:55,620 --> 00:12:57,360
很好，嗯，准确地模拟了

386
00:12:57,360 --> 00:12:58,920


387
00:12:58,920 --> 00:13:01,380
句法和语义的许多方面的所有方面，但是，

388
00:13:01,380 --> 00:13:03,300
嗯，我不知道任何真实的，就像

389
00:13:03,300 --> 00:13:04,920
你知道乔姆斯基谈论关于

390
00:13:04,920 --> 00:13:06,660
语言的事实，这是一个老式的

391
00:13:06,660 --> 00:13:09,000
概念 但我真的认为这是

392
00:13:09,000 --> 00:13:10,260
一个非常重要的概念，

393
00:13:10,260 --> 00:13:12,899
就像

394
00:13:12,899 --> 00:13:16,860
llms 可以独特地提供关于语言本身的一些发现，

395
00:13:16,860 --> 00:13:19,200
就像 llm 做出了一些

396
00:13:19,200 --> 00:13:21,899
预测，假设你有一个

397
00:13:21,899 --> 00:13:24,600
句子结构类型 X

398
00:13:24,600 --> 00:13:26,279
比句子更难处理 类型

399
00:13:26,279 --> 00:13:28,620
Y，这是一个独特的预测，

400
00:13:28,620 --> 00:13:31,200
只有他们会产生它，而没有人类

401
00:13:31,200 --> 00:13:33,899
语言学家乔姆斯基·霍姆斯坦，这些

402
00:13:33,899 --> 00:13:34,980
人之前都没有预测到，

403
00:13:34,980 --> 00:13:37,260
但事实证明这是真的你做了眼动

404
00:13:37,260 --> 00:13:38,760
追踪实验，你做了各种

405
00:13:38,760 --> 00:13:40,800
不同的行为体验 哦，

406
00:13:40,800 --> 00:13:42,480
你知道，毕竟它是

407
00:13:42,480 --> 00:13:44,279
真的，这是关于

408
00:13:44,279 --> 00:13:45,839
语言处理的新见解，这是

409
00:13:45,839 --> 00:13:47,940
关于语言的新见解，你知道行为我只是

410
00:13:47,940 --> 00:13:49,560
想知道我不是说我不是说

411
00:13:49,560 --> 00:13:51,240
这在原则上是不可能的

412
00:13:51,240 --> 00:13:52,620
因为它可能会在

413
00:13:52,620 --> 00:13:54,540
不久的将来发生，但我想这就是

414
00:13:54,540 --> 00:13:57,240
为什么很多语言学家

415
00:13:57,240 --> 00:13:59,519


416
00:13:59,519 --> 00:14:02,040
在这里代表整个语言学社区发言的关键，你知道

417
00:14:02,040 --> 00:14:03,180
我想这将是主要的

418
00:14:03,180 --> 00:14:04,800
反对意见之一

419
00:14:04,800 --> 00:14:08,279
是的我的意思是我我 不知道，嗯，我

420
00:14:08,279 --> 00:14:10,019
想我认为他们

421
00:14:10,019 --> 00:14:12,420
提供的见解是一种普遍原则，

422
00:14:12,420 --> 00:14:14,459
所以

423
00:14:14,459 --> 00:14:16,200
嗯，我认为这些事情就像

424
00:14:16,200 --> 00:14:18,720
记忆大块

425
00:14:18,720 --> 00:14:20,820
语言的力量一样，就像它们

426
00:14:20,820 --> 00:14:22,500
看起来一样 例如，要非常擅长结构，

427
00:14:22,500 --> 00:14:24,540
并且有很多

428
00:14:24,540 --> 00:14:26,160
语言学理论，

429
00:14:26,160 --> 00:14:28,740
特别是乔姆斯基的权利，这是关于

430
00:14:28,740 --> 00:14:30,899
试图找到一种最少量

431
00:14:30,899 --> 00:14:33,660
的结构来记住权利，试图

432
00:14:33,660 --> 00:14:36,480
从

433
00:14:36,480 --> 00:14:38,279
一些小的集合中尽可能多地推导一些小的

434
00:14:38,279 --> 00:14:40,440
操作的集合，

435
00:14:40,440 --> 00:14:42,300
嗯，我认为对于

436
00:14:42,300 --> 00:14:44,459
那些理论来说并不顺利，嗯，而这

437
00:14:44,459 --> 00:14:46,740
确实很好，所以

438
00:14:46,740 --> 00:14:48,360
嗯，如果我们考虑一些

439
00:14:48,360 --> 00:14:50,040
具有记忆能力的东西，

440
00:14:50,040 --> 00:14:51,300
例如我们考虑构建的语法理论

441
00:14:51,300 --> 00:14:54,240


442
00:14:54,240 --> 00:14:56,040
嗯，你知道人类非常喜欢

443
00:14:56,040 --> 00:14:58,139
记住

444
00:14:58,139 --> 00:14:59,880
不同结构或

445
00:14:59,880 --> 00:15:01,380
不同单词的非凡能力，我们知道数万

446
00:15:01,380 --> 00:15:02,940
个单词数万个

447
00:15:02,940 --> 00:15:04,380
不同的结构对不起

448
00:15:04,380 --> 00:15:06,420
数万个不同的习语也许我们的

449
00:15:06,420 --> 00:15:07,920
语法理论应该

450
00:15:07,920 --> 00:15:10,260
与此相结合，他们 从某种意义上说，这是

451
00:15:10,260 --> 00:15:12,480
一种原则证明，这种

452
00:15:12,480 --> 00:15:15,660
方法可以很好地发挥作用，

453
00:15:15,660 --> 00:15:17,279
嗯，可以考虑用它们做出其他类型的

454
00:15:17,279 --> 00:15:19,380
预测，嗯，其中一些

455
00:15:19,380 --> 00:15:21,779
人目前正在做，但

456
00:15:21,779 --> 00:15:23,339
例如试图用它们来

457
00:15:23,339 --> 00:15:25,860
衡量

458
00:15:25,860 --> 00:15:27,899
例如，从这些模型中，处理难度测量出乎意料，

459
00:15:27,899 --> 00:15:29,160


460
00:15:29,160 --> 00:15:30,600
嗯，有些出乎意料的措施

461
00:15:30,600 --> 00:15:31,699


462
00:15:31,699 --> 00:15:34,260
比上下文无关

463
00:15:34,260 --> 00:15:36,120
语法或其他类型的语言

464
00:15:36,120 --> 00:15:37,440
模型要好得多，然后有趣的

465
00:15:37,440 --> 00:15:39,899
问题是，这些出人意料或可

466
00:15:39,899 --> 00:15:41,940
预测性与人类

467
00:15:41,940 --> 00:15:44,399
处理权有何关系，它 可能捕获其中的一些

468
00:15:44,399 --> 00:15:46,620
或者可能是非线性的或者它可能

469
00:15:46,620 --> 00:15:49,079
你只知道捕获它的一点点

470
00:15:49,079 --> 00:15:51,420
或者或者任何

471
00:15:51,420 --> 00:15:53,940
其他有趣的科学问题但我

472
00:15:53,940 --> 00:15:55,500
认为原则上他们可以

473
00:15:55,500 --> 00:15:57,779
做出关于例如

474
00:15:57,779 --> 00:15:59,880
连接的预测是正确的 句子之间正确所以

475
00:15:59,880 --> 00:16:02,279
在论文中我给出了这个例子，

476
00:16:02,279 --> 00:16:05,459
你知道

477
00:16:05,459 --> 00:16:07,920
以 10 种不同的方式将声明转换为问题，并且

478
00:16:07,920 --> 00:16:10,620
大概当你知道 GPT 或

479
00:16:10,620 --> 00:16:12,540
正在做的事情时，它会找到

480
00:16:12,540 --> 00:16:15,420
10 个不同的问题，这些都是呃

481
00:16:15,420 --> 00:16:18,060
在语义或句法空间底层的模型中某种方式附近的某种方式，

482
00:16:18,060 --> 00:16:20,339


483
00:16:20,339 --> 00:16:22,139


484
00:16:22,139 --> 00:16:24,660
嗯，所以这些东西是

485
00:16:24,660 --> 00:16:26,459
我认为的那种类型，

486
00:16:26,459 --> 00:16:28,560
嗯，你知道一些语言学家可能想要

487
00:16:28,560 --> 00:16:30,180
正确的，这是

488
00:16:30,180 --> 00:16:32,220
句子之间或他们或

489
00:16:32,220 --> 00:16:34,320
他们的句子之间的一些隐藏联系 结构，但据我所知，

490
00:16:34,320 --> 00:16:36,120
它们还没有经过经验评估，

491
00:16:36,120 --> 00:16:39,660
所以是的，是的，是的，我的意思是这些

492
00:16:39,660 --> 00:16:41,220
模型只有几年的历史，

493
00:16:41,220 --> 00:16:43,440
所以我认为

494
00:16:43,440 --> 00:16:45,000


495
00:16:45,000 --> 00:16:46,259
即使这种工作已经

496
00:16:46,259 --> 00:16:48,540
存在，但对它们感到兴奋是合理的 还没有完成，不，那是对的，不

497
00:16:48,540 --> 00:16:50,880
完全，我的意思是，但我

498
00:16:50,880 --> 00:16:51,720
认为这是正确的

499
00:16:51,720 --> 00:16:52,920
观点，但我认为这

500
00:16:52,920 --> 00:16:54,779
涉及到

501
00:16:54,779 --> 00:16:56,579
你提到的令人惊讶的问题，你提到了

502
00:16:56,579 --> 00:16:58,139
laneability

503
00:16:58,139 --> 00:17:00,839
嗯，你知道 LMS 和一些语法但是 他们

504
00:17:00,839 --> 00:17:03,060
这样做的数据显然

505
00:17:03,060 --> 00:17:04,439
比婴儿多得多，

506
00:17:04,439 --> 00:17:06,240
因此对潜在结构的观察

507
00:17:06,240 --> 00:17:09,000
本身并不能

508
00:17:09,000 --> 00:17:10,799
说明刺激的贫乏，

509
00:17:10,799 --> 00:17:12,540
我应该说的是较弱的版本，即

510
00:17:12,540 --> 00:17:13,500


511
00:17:13,500 --> 00:17:15,059
杰出论证的贫乏，所以仅仅是事实

512
00:17:15,059 --> 00:17:17,579
LMS 可以用我们的语法奖完成他们所做的事情，

513
00:17:17,579 --> 00:17:19,559
这非常惊人，我

514
00:17:19,559 --> 00:17:20,939
同意，事实上你可能不会

515
00:17:20,939 --> 00:17:22,500
预料到

516
00:17:22,500 --> 00:17:23,699
五六七年前，

517
00:17:23,699 --> 00:17:25,260
嗯，但这并不能否定

518
00:17:25,260 --> 00:17:28,199
人类有惊喜的说法，我们

519
00:17:28,199 --> 00:17:30,120
带上这些祈祷，

520
00:17:30,120 --> 00:17:31,080
以便看看计算

521
00:17:31,080 --> 00:17:33,299
语言学是否可以限制假设和

522
00:17:33,299 --> 00:17:34,740
理论语言学，我认为它

523
00:17:34,740 --> 00:17:36,480
可以通过这种方式做到这一点，这需要

524
00:17:36,480 --> 00:17:38,039
与你一起完成，你知道控制

525
00:17:38,039 --> 00:17:39,720
不同学习参数的仔细实验

526
00:17:39,720 --> 00:17:43,080
和巨大的语言模型

527
00:17:43,080 --> 00:17:45,299
像 gbt free 基本上你知道

528
00:17:45,299 --> 00:17:47,520
在这里没用，所以这会引起

529
00:17:47,520 --> 00:17:49,559
一些 tile lens 和抱怨我们

530
00:17:49,559 --> 00:17:51,960
需要像婴儿 LM 项目这样的东西

531
00:17:51,960 --> 00:17:53,700
我知道你感兴趣并且

532
00:17:53,700 --> 00:17:55,320
我们有更多你知道生态

533
00:17:55,320 --> 00:17:56,940
有效的训练集你

534
00:17:56,940 --> 00:17:58,380
在你的论文中预测

535
00:17:58,380 --> 00:17:59,640
将从中学习到一些结构，我

536
00:17:59,640 --> 00:18:01,679
怀疑你可能是对的，

537
00:18:01,679 --> 00:18:03,059
嗯，但你知道即便如此，即使有

538
00:18:03,059 --> 00:18:04,679
婴儿 LM 挑战，仍然存在

539
00:18:04,679 --> 00:18:07,260
解决更传统问题的非平凡问题，

540
00:18:07,260 --> 00:18:09,419
例如什么时候 孩子们

541
00:18:09,419 --> 00:18:11,400
开始根据

542
00:18:11,400 --> 00:18:13,200


543
00:18:13,200 --> 00:18:15,539
跨语言的不同因素根据当前输入量进行概括，这只

544
00:18:15,539 --> 00:18:17,700
需要传统的

545
00:18:17,700 --> 00:18:18,720
心理语言学和语言

546
00:18:18,720 --> 00:18:21,539
习得，所以你知道 LMS 确实关心

547
00:18:21,539 --> 00:18:22,919
频率和

548
00:18:22,919 --> 00:18:24,600
意外之类的事情，就像你说的那样，但

549
00:18:24,600 --> 00:18:26,160
真的很好 Sophie sluts 和

550
00:18:26,160 --> 00:18:27,600
Andrea Martin 的论文非常漂亮，

551
00:18:27,600 --> 00:18:30,000
我想你可能已经看到它

552
00:18:30,000 --> 00:18:31,620
很好地表明分布

553
00:18:31,620 --> 00:18:34,080
统计有时可以作为

554
00:18:34,080 --> 00:18:36,240
结构构建时刻的线索，但它

555
00:18:36,240 --> 00:18:37,919
确实取代了这些与组合有关的概念，

556
00:18:37,919 --> 00:18:39,660
所以我只是 阅读

557
00:18:39,660 --> 00:18:42,360
Chomsky 57 的一句话，这听起来很像

558
00:18:42,360 --> 00:18:45,240
um slots and more 并说尽管

559
00:18:45,240 --> 00:18:47,820


560
00:18:47,820 --> 00:18:49,440


561
00:18:49,440 --> 00:18:51,360
语言的语义和统计模型具有不可否认的兴趣和重要性，但它们似乎

562
00:18:51,360 --> 00:18:52,919
与确定

563
00:18:52,919 --> 00:18:54,600
或表征语法

564
00:18:54,600 --> 00:18:56,340
差异集的问题没有直接关系 我认为我们不得不

565
00:18:56,340 --> 00:18:57,840
得出这样的结论：语法是自主的

566
00:18:57,840 --> 00:18:59,520
，独立于意义，

567
00:18:59,520 --> 00:19:01,320
概率模型没有

568
00:19:01,320 --> 00:19:03,179
对句法结构的一些基本问题给出特别的洞察力，

569
00:19:03,179 --> 00:19:05,880
所以

570
00:19:05,880 --> 00:19:07,980
第二句的 the 的第二个呃对冲结果

571
00:19:07,980 --> 00:19:10,380
是 不正确，但确实

572
00:19:10,380 --> 00:19:11,580
如此，您

573
00:19:11,580 --> 00:19:13,740
确实知道

574
00:19:13,740 --> 00:19:16,559
57 中的可用统计模型在今天应用于模型时不再准确，

575
00:19:16,559 --> 00:19:18,600
并且可以

576
00:19:18,600 --> 00:19:20,340
对您提到的新

577
00:19:20,340 --> 00:19:21,900
字符串和分布类别进行抽象概括，

578
00:19:21,900 --> 00:19:23,640
但

579
00:19:23,640 --> 00:19:25,380
单个模型的性能 没有通过

580
00:19:25,380 --> 00:19:27,480


581
00:19:27,480 --> 00:19:29,760


582
00:19:29,760 --> 00:19:31,440
给出

583
00:19:31,440 --> 00:19:33,900
当今可用的任何计算模型与

584
00:19:33,900 --> 00:19:36,240
人脑模型之间的巨大距离来提供支持或反对特定结构的着陆性的直接证据，成功

585
00:19:36,240 --> 00:19:38,100


586
00:19:38,100 --> 00:19:40,679
并不意味着该结构必然是陆地，模型失败也不意味着

587
00:19:40,679 --> 00:19:42,539
结构是不可学习的，

588
00:19:42,539 --> 00:19:44,520


589
00:19:44,520 --> 00:19:47,100
是的，是的，所以我的意思是，我认为可能

590
00:19:47,100 --> 00:19:49,380
值得拆开人们提出的几个

591
00:19:49,380 --> 00:19:51,000
不同版本的可学习性

592
00:19:51,000 --> 00:19:52,620
论点，因为有

593
00:19:52,620 --> 00:19:55,679
非常非常强烈

594
00:19:55,679 --> 00:19:57,720
的不可能性主张，

595
00:19:57,720 --> 00:19:59,160
嗯，来自乔姆斯基的

596
00:19:59,160 --> 00:20:01,260
传统，对吧 我们从来没有

597
00:20:01,260 --> 00:20:04,140
声称需要的数据量是

598
00:20:04,140 --> 00:20:05,820
正确的 有关于

599
00:20:05,820 --> 00:20:08,220
语言学习的逻辑问题的说法

600
00:20:08,220 --> 00:20:10,740


601
00:20:10,740 --> 00:20:12,120
这是不可能的

602
00:20:12,120 --> 00:20:15,720


603
00:20:15,720 --> 00:20:17,580


604
00:20:17,580 --> 00:20:19,380


605
00:20:19,380 --> 00:20:21,660
你会获得的语言类或语法类，嗯，

606
00:20:21,660 --> 00:20:23,220
人们很长一段时间一直在

607
00:20:23,220 --> 00:20:25,380
反对那个版本的

608
00:20:25,380 --> 00:20:26,340
东西，

609
00:20:26,340 --> 00:20:27,120


610
00:20:27,120 --> 00:20:28,860
嗯，你知道有 gold 的旧作品

611
00:20:28,860 --> 00:20:30,419
，然后是整个

612
00:20:30,419 --> 00:20:32,580
语法

613
00:20:32,580 --> 00:20:35,220
建立在传统之上的习得理论非常担心

614
00:20:35,220 --> 00:20:37,980


615
00:20:37,980 --> 00:20:39,240
你遍历不同的

616
00:20:39,240 --> 00:20:40,860
假设并考虑不同的

617
00:20:40,860 --> 00:20:42,600
选择和事物的顺序，

618
00:20:42,600 --> 00:20:43,380


619
00:20:43,380 --> 00:20:45,059
嗯，我最喜欢的参考

620
00:20:45,059 --> 00:20:46,919
是尼克贾德的这篇论文

621
00:20:46,919 --> 00:20:49,799
和 Paul vetani um 称之为

622
00:20:49,799 --> 00:20:51,539


623
00:20:51,539 --> 00:20:53,039
自然语言的理想学习

624
00:20:53,039 --> 00:20:54,840
um 这基本上表明，一个

625
00:20:54,840 --> 00:20:57,539
不受约束的学习者可以 uh 有

626
00:20:57,539 --> 00:21:00,480
足够的数据 uh 获得 uh

627
00:21:00,480 --> 00:21:01,860
生成规则的种类或

628
00:21:01,860 --> 00:21:03,360
生成语法

629
00:21:03,360 --> 00:21:05,640
um 仅通过观察字符串是正确的，但

630
00:21:05,640 --> 00:21:08,160
那篇论文 确实是

631
00:21:08,160 --> 00:21:10,200
为了回应大量的工作，这些工作

632
00:21:10,200 --> 00:21:13,140
争辩说

633
00:21:13,140 --> 00:21:15,120
从正例中学习，因此仅通过

634
00:21:15,120 --> 00:21:17,820
观察字符串在逻辑上是

635
00:21:17,820 --> 00:21:20,520
不可能的，所以

636
00:21:20,520 --> 00:21:23,460
嗯，当然，你知道

637
00:21:23,460 --> 00:21:25,679
乔姆斯基传统中的人们真的很喜欢这种

638
00:21:25,679 --> 00:21:28,140
形式 论证是因为它

639
00:21:28,140 --> 00:21:30,419
说，呃，你必须天生就有

640
00:21:30,419 --> 00:21:33,299
特定的东西，才能使

641
00:21:33,299 --> 00:21:34,919
语言习得起作用，这就像

642
00:21:34,919 --> 00:21:36,600
一种数学论证，

643
00:21:36,600 --> 00:21:39,480
你必须有某种

644
00:21:39,480 --> 00:21:41,220
天生的语法或假设的天生顺序，

645
00:21:41,220 --> 00:21:42,900
或者 有些事情和所有这些都

646
00:21:42,900 --> 00:21:45,659
被证明是完全错误的所以

647
00:21:45,659 --> 00:21:48,299
嗯，如果你知道转移到

648
00:21:48,299 --> 00:21:50,520
更现实的学习环境，

649
00:21:50,520 --> 00:21:53,340
Tater 和 vatani 做的

650
00:21:53,340 --> 00:21:55,500
嗯嗯然后事实证明你就像一个

651
00:21:55,500 --> 00:21:57,240
理想化的学习者可以获得东西并且

652
00:21:57,240 --> 00:21:59,100
没有 关于

653
00:21:59,100 --> 00:22:00,600
um 即使在那里也需要的数据量的陈述

654
00:22:00,600 --> 00:22:02,700
是那种

655
00:22:02,700 --> 00:22:06,780
纯逻辑的 uh 学习能力

656
00:22:06,780 --> 00:22:08,940
um 而这种能力是我认为

657
00:22:08,940 --> 00:22:11,460
大型语言模型的 uh 大版本

658
00:22:11,460 --> 00:22:13,679
也正确所以 Chader

659
00:22:13,679 --> 00:22:15,600
invitani 和其他 这种精神的工作方式

660
00:22:15,600 --> 00:22:17,460


661
00:22:17,460 --> 00:22:19,500
是，你知道数学和

662
00:22:19,500 --> 00:22:21,480
原则上的争论，但从来没有

663
00:22:21,480 --> 00:22:24,720
创造出真正的

664
00:22:24,720 --> 00:22:27,840
语法正确的东西，或者真正

665
00:22:27,840 --> 00:22:30,000
实现的语言模型，

666
00:22:30,000 --> 00:22:32,640
所以即使你知道一个

667
00:22:32,640 --> 00:22:35,400
经过训练的模型 在 1 亿或 1000 亿或

668
00:22:35,400 --> 00:22:38,340
许多代币上，

669
00:22:38,340 --> 00:22:41,159
嗯，我认为即使是那种模型也

670
00:22:41,159 --> 00:22:43,080
与辩论的那个版本相关，

671
00:22:43,080 --> 00:22:46,140
并且表明

672
00:22:46,140 --> 00:22:48,659
语言学习并非不可能，

673
00:22:48,659 --> 00:22:51,360
嗯，从一个非常不受约束的空间好吧

674
00:22:51,360 --> 00:22:53,640
然后还有第二个版本，

675
00:22:53,640 --> 00:22:56,760
呃，我们可以用

676
00:22:56,760 --> 00:22:59,760
孩子们正确的特定数据来学习语言吗？

677
00:22:59,760 --> 00:23:02,340
这既是数据量

678
00:23:02,340 --> 00:23:04,620
又是数据形式，

679
00:23:04,620 --> 00:23:06,360
嗯，所以对于那些不了解

680
00:23:06,360 --> 00:23:08,520
婴儿 LM 挑战的人来说

681
00:23:08,520 --> 00:23:10,440
嗯嗯是嗯

682
00:23:10,440 --> 00:23:12,840
这个嗯嗯

683
00:23:12,840 --> 00:23:14,340
抱歉把它称为

684
00:23:14,340 --> 00:23:16,620
竞赛或

685
00:23:16,620 --> 00:23:20,580
嗯嗯嗯嗯我想这是一个挑战

686
00:23:20,580 --> 00:23:22,380
嗯试图让人们

687
00:23:22,380 --> 00:23:24,539
在人类规模的数据量上训练语言模型

688
00:23:24,539 --> 00:23:27,179
嗯所以这更像是

689
00:23:27,179 --> 00:23:28,679
嗯我认为有两个不同的

690
00:23:28,679 --> 00:23:31,140
版本 10 或 1 亿个不同的

691
00:23:31,140 --> 00:23:33,120
um 到 10 或 1 亿个不同的单词

692
00:23:33,120 --> 00:23:35,640
在训练集中

693
00:23:35,640 --> 00:23:38,100
um 这就像你知道的第 100 个或第

694
00:23:38,100 --> 00:23:41,159
1000 个或与

695
00:23:41,159 --> 00:23:43,140
这些大 AI 公司正在使用的

696
00:23:43,140 --> 00:23:46,620
语言一样大 模型 um 和

697
00:23:46,620 --> 00:23:48,720
um 我认为实际上它就像是

698
00:23:48,720 --> 00:23:50,340
正确的

699
00:23:50,340 --> 00:23:52,020
事情并且正是该领域需要的东西，

700
00:23:52,020 --> 00:23:54,780
因为你可能会发现

701
00:23:54,780 --> 00:23:57,059
um 一个儿童大小的数据

702
00:23:57,059 --> 00:23:58,980
um 你基本上可以学习

703
00:23:58,980 --> 00:24:01,140
正确的语法，我认为会 将是

704
00:24:01,140 --> 00:24:02,880
反对这些刺激贫困声明的最有力论据，

705
00:24:02,880 --> 00:24:04,260
您可能会

706
00:24:04,260 --> 00:24:06,299
发现

707
00:24:06,299 --> 00:24:08,220
嗯，也许您学不到很多东西，

708
00:24:08,220 --> 00:24:10,740
也许您知道想出

709
00:24:10,740 --> 00:24:12,840
一种更糟糕的语言模型，

710
00:24:12,840 --> 00:24:14,700
或者它缺乏一些句法或

711
00:24:14,700 --> 00:24:16,799
语义能力，

712
00:24:16,799 --> 00:24:17,520
嗯

713
00:24:17,520 --> 00:24:18,720
嗯，我实际上认为

714
00:24:18,720 --> 00:24:20,220
那里的失败有点难以

715
00:24:20,220 --> 00:24:22,620
解释，因为

716
00:24:22,620 --> 00:24:24,720
嗯，孩子们，嗯，数据，当他们真正

717
00:24:24,720 --> 00:24:26,400
学习语言时，他们获得的

718
00:24:26,400 --> 00:24:28,919
数据要多得多，而不仅仅是

719
00:24:28,919 --> 00:24:31,200
他们在环境中互动的句子串

720
00:24:31,200 --> 00:24:33,059


721
00:24:33,059 --> 00:24:34,440
嗯，所以他们面前的世界上有一些东西，

722
00:24:34,440 --> 00:24:36,539
他们的话语也是

723
00:24:36,539 --> 00:24:38,520
互动的，所以你可以说点什么，

724
00:24:38,520 --> 00:24:39,960
然后看看你的

725
00:24:39,960 --> 00:24:41,400
父母是否给你带来了你要求的东西，

726
00:24:41,400 --> 00:24:43,799
例如人们长期以来一直争论的权利

727
00:24:43,799 --> 00:24:46,020


728
00:24:46,020 --> 00:24:49,020
你知道在语言习得中的重要线索

729
00:24:49,020 --> 00:24:51,240


730
00:24:51,240 --> 00:24:52,799
嗯所以

731
00:24:52,799 --> 00:24:55,440
嗯嗯在婴儿 LM 挑战中有

732
00:24:55,440 --> 00:24:58,200
能力训练这些

733
00:24:58,200 --> 00:25:00,960
模型呃使用多模式输入

734
00:25:00,960 --> 00:25:02,340
我认为你可以给他们尽可能多的视频

735
00:25:02,340 --> 00:25:05,039
数据

736
00:25:05,039 --> 00:25:07,140
呃，但可能很难

737
00:25:07,140 --> 00:25:09,000
完全复制

738
00:25:09,000 --> 00:25:11,220
孩子们实际得到的设置类型和反馈，

739
00:25:11,220 --> 00:25:12,840
所以

740
00:25:12,840 --> 00:25:14,700
嗯，呃，我不知道你知道我

741
00:25:14,700 --> 00:25:17,700
很高兴看到呃，事情进展到哪里

742
00:25:17,700 --> 00:25:19,799
以及事情如何 在那里，

743
00:25:19,799 --> 00:25:20,760
嗯，嗯，

744
00:25:20,760 --> 00:25:23,700
你知道，我认为

745
00:25:23,700 --> 00:25:26,580
对于大型语言模型有一个有趣的相关问题，

746
00:25:26,580 --> 00:25:28,320


747
00:25:28,320 --> 00:25:30,299
嗯，就像它是什么，它

748
00:25:30,299 --> 00:25:31,919
准确地理解所有

749
00:25:31,919 --> 00:25:34,200
数据在做什么，所以

750
00:25:34,200 --> 00:25:36,659
嗯，可能是你需要这么

751
00:25:36,659 --> 00:25:38,279
多数据 这些模型是因为

752
00:25:38,279 --> 00:25:40,620
它们在内部有效地发明了某种形式

753
00:25:40,620 --> 00:25:43,919
的语义，所以

754
00:25:43,919 --> 00:25:45,600
嗯，它们都在发现

755
00:25:45,600 --> 00:25:47,460
语法规则，而且它们似乎在

756
00:25:47,460 --> 00:25:49,380
学习很多关于词义的知识，

757
00:25:49,380 --> 00:25:50,940


758
00:25:50,940 --> 00:25:52,200
嗯，

759
00:25:52,200 --> 00:25:54,059
嗯，这不是完全不清楚，我

760
00:25:54,059 --> 00:25:56,220
想 这些现代模型中的大部分数据

761
00:25:56,220 --> 00:25:58,919
呃是语法与语义所需要的

762
00:25:58,919 --> 00:26:00,240


763
00:26:00,240 --> 00:26:02,159
我自己的猜测我认为可能

764
00:26:02,159 --> 00:26:05,580
是语法方面呃可能

765
00:26:05,580 --> 00:26:07,860
需要比语义方面更少的数据

766
00:26:07,860 --> 00:26:09,600


767
00:26:09,600 --> 00:26:11,159
嗯实际上是学生以前

768
00:26:11,159 --> 00:26:12,960
的学生 我的 Frank Malika 和我几年前写了一篇论文，

769
00:26:12,960 --> 00:26:15,120
试图估计

770
00:26:15,120 --> 00:26:17,400
一个学习者必须获得的信息量，嗯，

771
00:26:17,400 --> 00:26:19,679


772
00:26:19,679 --> 00:26:22,140
学习

773
00:26:22,140 --> 00:26:23,820
语言的不同方面，所以你必须学习所有的

774
00:26:23,820 --> 00:26:25,320
单词，你学习他们的论坛，你

775
00:26:25,320 --> 00:26:26,880
学习 它们的含义，你可能知道

776
00:26:26,880 --> 00:26:28,320
它们的频率，你必须学习

777
00:26:28,320 --> 00:26:32,100
语法，基本上我们

778
00:26:32,100 --> 00:26:34,080
在那个分析中发现，你知道

779
00:26:34,080 --> 00:26:35,640
基本上只是

780
00:26:35,640 --> 00:26:37,440
对这些域中的每一个的一种粗略计算，

781
00:26:37,440 --> 00:26:40,679
语法实际上是非常

782
00:26:40,679 --> 00:26:42,779
少的信息

783
00:26:42,779 --> 00:26:46,400
学习语法不需要那么多信息，

784
00:26:46,400 --> 00:26:49,320
而

785
00:26:49,320 --> 00:26:52,740
你获得的大部分信息实际上是关于语义的，所以

786
00:26:52,740 --> 00:26:55,740
指定你知道 30 到 50 000 个

787
00:26:55,740 --> 00:26:57,720
不同的单词含义，即使

788
00:26:57,720 --> 00:27:00,000
每个含义

789
00:27:00,000 --> 00:27:02,340
只是几个位 就像

790
00:27:02,340 --> 00:27:04,620
那样需要大量信息，

791
00:27:04,620 --> 00:27:06,360
可能每次会议都不止

792
00:27:06,360 --> 00:27:08,460
于此，所以

793
00:27:08,460 --> 00:27:11,279
嗯嗯，这可能会让

794
00:27:11,279 --> 00:27:12,659
我猜测

795
00:27:12,659 --> 00:27:14,820
大型语言模型正在发生的事情是

796
00:27:14,820 --> 00:27:16,320
他们的大部分训练数据都是关于单词的

797
00:27:16,320 --> 00:27:18,600
语义，你可以考虑让

798
00:27:18,600 --> 00:27:20,760
孩子们正确理解单词语义的其他方式，

799
00:27:20,760 --> 00:27:22,140
这不仅仅是

800
00:27:22,140 --> 00:27:24,600
文本中的一种共现模式，

801
00:27:24,600 --> 00:27:27,360
嗯，但我同意所有这些都悬

802
00:27:27,360 --> 00:27:29,039
而未决，看到

803
00:27:29,039 --> 00:27:30,960
会发生什么真的很令人兴奋所以 是的，我知道

804
00:27:30,960 --> 00:27:32,640


805
00:27:32,640 --> 00:27:35,279
Lindsay 实验室的一些早期结果表明，至少

806
00:27:35,279 --> 00:27:37,740
仅限于生态有效你

807
00:27:37,740 --> 00:27:40,919
知道培训网站呃模型似乎

808
00:27:40,919 --> 00:27:42,840
概括你知道英语的线性规则是的

809
00:27:42,840 --> 00:27:44,940
没有问题形成

810
00:27:44,940 --> 00:27:46,620
除了层次规则之外的 ROM

811
00:27:46,620 --> 00:27:48,120
正确的层次结构 规则所以我认为

812
00:27:48,120 --> 00:27:49,980
有一种真正的意义，你知道

813
00:27:49,980 --> 00:27:52,020
正确的

814
00:27:52,020 --> 00:27:54,000
句法价格和归纳偏差的空间

815
00:27:54,000 --> 00:27:56,220
确实还没有真正解决，

816
00:27:56,220 --> 00:27:57,960
但至少在我看来很

817
00:27:57,960 --> 00:27:59,400
明显必须有一些所以

818
00:27:59,400 --> 00:28:01,320
还有一些证据

819
00:28:01,320 --> 00:28:02,700
表明，英语儿童会回到

820
00:28:02,700 --> 00:28:04,679
这个频率问题，即

821
00:28:04,679 --> 00:28:06,360
英语儿童有时会在

822
00:28:06,360 --> 00:28:08,100


823
00:28:08,100 --> 00:28:10,140


824
00:28:10,140 --> 00:28:11,880


825
00:28:11,880 --> 00:28:13,980
长距离 wh 问题的较低补码位置的指定位置拼出运动的中间副本，所以

826
00:28:13,980 --> 00:28:15,360
Thornton 和一些人有一篇论文 其他关于

827
00:28:15,360 --> 00:28:18,120
这个的论文所以他们说嗯你认为哪个人

828
00:28:18,120 --> 00:28:19,919
做了那个而不是

829
00:28:19,919 --> 00:28:21,900
你认为哪个人做了那个所以这是

830
00:28:21,900 --> 00:28:23,159
一个有趣的你知道小姐设置

831
00:28:23,159 --> 00:28:25,020
因为有些语言确实拼出

832
00:28:25,020 --> 00:28:26,760
了这些中间副本但

833
00:28:26,760 --> 00:28:28,799
英语不是这样 孩子

834
00:28:28,799 --> 00:28:30,779
在设置他们的语法时犯了错误，但

835
00:28:30,779 --> 00:28:32,820
输入的频率实际上是零

836
00:28:32,820 --> 00:28:35,159
，所以我们共同的朋友加里马库斯

837
00:28:35,159 --> 00:28:36,779
也反对频率

838
00:28:36,779 --> 00:28:39,000
决定孩子的输出在

839
00:28:39,000 --> 00:28:41,159
德语名词复数的情况下是某种更规则的

840
00:28:41,159 --> 00:28:42,779
形式 首选而

841
00:28:42,779 --> 00:28:44,580
不是常见的，并且有很多这样的

842
00:28:44,580 --> 00:28:46,559
例子，所以有时声称

843
00:28:46,559 --> 00:28:49,080
受试者经历被动，

844
00:28:49,080 --> 00:28:50,460
受试者被动地经历

845
00:28:50,460 --> 00:28:52,559
某事或在理解研究中非常延迟的孩子

846
00:28:52,559 --> 00:28:54,299
直到八岁左右，

847
00:28:54,299 --> 00:28:56,039
因为他们在输入中不是很频繁

848
00:28:56,039 --> 00:28:56,880


849
00:28:56,880 --> 00:28:59,220
但是肯 Wexler 和他的同事

850
00:28:59,220 --> 00:29:01,260
经历了 um subject experience Double H

851
00:29:01,260 --> 00:29:03,539
问题，比如谁喜欢 Mary，他们

852
00:29:03,539 --> 00:29:05,820
发现这些问题

853
00:29:05,820 --> 00:29:07,500
在输入中与 subject 和 experience

854
00:29:07,500 --> 00:29:09,659
of passives 一样不常见，但是孩子们在

855
00:29:09,659 --> 00:29:10,980
理解这些问题的研究中没有问题，

856
00:29:10,980 --> 00:29:13,860
但他们确实有理解问题 口头

857
00:29:13,860 --> 00:29:16,020
被动的主题体验，所以

858
00:29:16,020 --> 00:29:17,880
频率再次似乎是

859
00:29:17,880 --> 00:29:19,380
无关紧要的，或者至少它不是

860
00:29:19,380 --> 00:29:20,940
解释性的，我想这

861
00:29:20,940 --> 00:29:22,320
对理论

862
00:29:22,320 --> 00:29:25,080
构建没有解释性，所以 LMS 如何帮助解决这些

863
00:29:25,080 --> 00:29:27,059
你知道的不同情况，当

864
00:29:27,059 --> 00:29:28,440
除了频率之外显然还有其他事情发生时

865
00:29:28,440 --> 00:29:30,960
所以校友们，你知道他们似乎

866
00:29:30,960 --> 00:29:33,419
再次概括回到

867
00:29:33,419 --> 00:29:35,100
你论文中的案例，

868
00:29:35,100 --> 00:29:36,899
你表明他们

869
00:29:36,899 --> 00:29:38,399
概括了无色

870
00:29:38,399 --> 00:29:41,279
屏幕想法的结构，这通常很酷，

871
00:29:41,279 --> 00:29:43,080
嗯，但积极的刺激从来没有

872
00:29:43,080 --> 00:29:44,520
真正 关于无法从

873
00:29:44,520 --> 00:29:46,380
统计学上学习语言我知道你的

874
00:29:46,380 --> 00:29:48,059
说法是正确的，但乔姆斯基

875
00:29:48,059 --> 00:29:49,559
在 50 年代关于

876
00:29:49,559 --> 00:29:51,779
当今统计模型的观点不适用于

877
00:29:51,779 --> 00:29:53,700
2023 年的商业 LMS，这是

878
00:29:53,700 --> 00:29:55,440
正确的，但我们不能用那个单一的

879
00:29:55,440 --> 00:29:57,539
观点来破坏 你知道整个

880
00:29:57,539 --> 00:29:59,520
几何企业乔姆斯基的基本

881
00:29:59,520 --> 00:30:01,080
观点是你可以有一个

882
00:30:01,080 --> 00:30:02,700
语法结构，其中每个

883
00:30:02,700 --> 00:30:04,980
图表都有零频率并且它也

884
00:30:04,980 --> 00:30:06,899
无法为概念界面提供清晰可解释的

885
00:30:06,899 --> 00:30:08,039
说明，

886
00:30:08,039 --> 00:30:09,840
因此其他

887
00:30:09,840 --> 00:30:11,580
思维系统的界面就像你展示的那样

888
00:30:11,580 --> 00:30:13,980
你的论文 GPT 模仿了像 pull

889
00:30:13,980 --> 00:30:16,559
the screen ideas 这样的例子，但是你又知道

890
00:30:16,559 --> 00:30:18,840
这句话在谷歌上产生了超过 150 000 个

891
00:30:18,840 --> 00:30:20,700
结果并且它

892
00:30:20,700 --> 00:30:22,620
在文献中被广泛讨论它能够

893
00:30:22,620 --> 00:30:24,480
模仿它可以模仿的事实并

894
00:30:24,480 --> 00:30:26,640
不能真正告诉我们太多在 至少我们真的

895
00:30:26,640 --> 00:30:27,840
不能很有信心地说任何话

896
00:30:27,840 --> 00:30:30,899
所以你知道

897
00:30:30,899 --> 00:30:32,580
都柏林大学背后的阿比巴最近引用了这句话

898
00:30:32,580 --> 00:30:34,380
呃不要把你自己的

899
00:30:34,380 --> 00:30:36,840
轻信误认为是电影的智慧

900
00:30:36,840 --> 00:30:38,880
事实上甚至年轻的浣熊侠联盟去年也写道

901
00:30:38,880 --> 00:30:40,500
评论家是正确的 指责

902
00:30:40,500 --> 00:30:42,480
LMS 从事一种

903
00:30:42,480 --> 00:30:43,919
模仿

904
00:30:43,919 --> 00:30:46,860
um 和

905
00:30:46,860 --> 00:30:48,840
你在论文中给出的来自 gbt 的例句实际上

906
00:30:48,840 --> 00:30:50,700
做得不好，因为正如你所说，

907
00:30:50,700 --> 00:30:52,320
你可能知道

908
00:30:52,320 --> 00:30:54,240
训练数据中很少见的无意义语言，但

909
00:30:54,240 --> 00:30:55,740
它们可以 要么做，要么他们做不到，但是就给

910
00:30:55,740 --> 00:30:57,120


911
00:30:57,120 --> 00:30:59,880
我们 10 个这样的例子而言，没有中间立场，所以你

912
00:30:59,880 --> 00:31:02,880
有无色的绿色想法，这些想法

913
00:31:02,880 --> 00:31:04,679
与

914
00:31:04,679 --> 00:31:06,840
诸如棕色闪闪发光的兔子、白色闪闪

915
00:31:06,840 --> 00:31:09,899
发光的兔子、黑色闪亮的

916
00:31:09,899 --> 00:31:12,360
袋鼠、绿色闪闪发光的猴子之类的语义对象截然不同

917
00:31:12,360 --> 00:31:15,120
黄色令人眼花缭乱的狮子 红色填充

918
00:31:15,120 --> 00:31:16,320
元素 对，这些都像是

919
00:31:16,320 --> 00:31:18,899
语义上的怪异，

920
00:31:18,899 --> 00:31:20,820
有点奇怪，但它们仍然像法律

921
00:31:20,820 --> 00:31:22,440
结构，它们是一种有意义的

922
00:31:22,440 --> 00:31:25,320
合成语义对象，

923
00:31:25,320 --> 00:31:27,179
嗯，对，

924
00:31:27,179 --> 00:31:29,580
我只是说是的，是的，我，我，我

925
00:31:29,580 --> 00:31:33,179
的意思是，所以我也许我可以 可以

926
00:31:33,179 --> 00:31:34,799
先对第一点做出正确回应，

927
00:31:34,799 --> 00:31:36,120
所以

928
00:31:36,120 --> 00:31:37,799
嗯，嗯，你开始谈论

929
00:31:37,799 --> 00:31:40,140
这些其他呃类型的习得

930
00:31:40,140 --> 00:31:42,179
模式，这些模式可能不会直接映射

931
00:31:42,179 --> 00:31:44,279
到频率，

932
00:31:44,279 --> 00:31:47,159
嗯，我认为

933
00:31:47,159 --> 00:31:50,640
认为现代学习

934
00:31:50,640 --> 00:31:53,039
模型实际上是错误的 应该只是基于频率，

935
00:31:53,039 --> 00:31:54,899
因为

936
00:31:54,899 --> 00:31:57,120
嗯，他们显然正在学习非常

937
00:31:57,120 --> 00:31:59,580
复杂的规则或

938
00:31:59,580 --> 00:32:02,399
结构或其他东西，

939
00:32:02,399 --> 00:32:03,899
嗯，我认为当

940
00:32:03,899 --> 00:32:06,600
他们正在学习时，他们很可能正在学习，他们

941
00:32:06,600 --> 00:32:08,580
在某种意义上正在寻找一个 简单

942
00:32:08,580 --> 00:32:10,559
或简约的

943
00:32:10,559 --> 00:32:12,360
嗯嗯，对

944
00:32:12,360 --> 00:32:13,980
他们正确看到的数据的解释以及这些数据如何

945
00:32:13,980 --> 00:32:16,080
缓存在神经网络中可能很

946
00:32:16,080 --> 00:32:19,200
复杂，你知道这取决于

947
00:32:19,200 --> 00:32:20,880
嗯你知道参数和

948
00:32:20,880 --> 00:32:22,679
学习算法的细节以及和和

949
00:32:22,679 --> 00:32:24,480
那些事情

950
00:32:24,480 --> 00:32:27,000
嗯，但我认为它是呃我想我可能会

951
00:32:27,000 --> 00:32:29,279
怀疑情况可能是这样嗯嗯

952
00:32:29,279 --> 00:32:31,740


953
00:32:31,740 --> 00:32:35,640
他们是他们正在

954
00:32:35,640 --> 00:32:38,399
学习一组复杂的

955
00:32:38,399 --> 00:32:40,799
东西正确的一种复杂的东西

956
00:32:40,799 --> 00:32:43,799
一系列的规则和构造

957
00:32:43,799 --> 00:32:46,919
um 这意味着我认为

958
00:32:46,919 --> 00:32:49,380
um 他们的概括可能就像

959
00:32:49,380 --> 00:32:52,500
你给的人的例子

960
00:32:52,500 --> 00:32:55,440
um 可能在输入权中有点不连续

961
00:32:55,440 --> 00:32:57,720
所以有时你

962
00:32:57,720 --> 00:33:00,000
可以想象看到一些字符串

963
00:33:00,000 --> 00:33:02,220
导致 你的语法和数据的最简单的

964
00:33:02,220 --> 00:33:03,840
语法，你

965
00:33:03,840 --> 00:33:06,059
到目前为止看到的是一个预测一个

966
00:33:06,059 --> 00:33:09,120
看不见的字符串的正确和

967
00:33:09,120 --> 00:33:12,360
嗯，如果发生这种情况，那么你

968
00:33:12,360 --> 00:33:14,100
将把数据学习一种

969
00:33:14,100 --> 00:33:16,860
表示，它在一些

970
00:33:16,860 --> 00:33:19,440
看不见的小说中概括 到目前为止，

971
00:33:19,440 --> 00:33:21,720
嗯，纯粹是因为这种概括

972
00:33:21,720 --> 00:33:23,460
是对数据的最简单描述，

973
00:33:23,460 --> 00:33:25,019
你已经看到了迄今为​​止的场景，

974
00:33:25,019 --> 00:33:25,980
我认为这就是

975
00:33:25,980 --> 00:33:28,320
语言学家试图做的事情，尝试呃

976
00:33:28,320 --> 00:33:29,700
看看数据并提出

977
00:33:29,700 --> 00:33:31,260
它的理论，然后有时该

978
00:33:31,260 --> 00:33:33,419
理论正确地预测了一些新现象

979
00:33:33,419 --> 00:33:35,840
或一些新类型的句子，

980
00:33:35,840 --> 00:33:38,159
因此，如果他们正在学习

981
00:33:38,159 --> 00:33:41,100
足够丰富的理论空间，

982
00:33:41,100 --> 00:33:42,899
那么你就不会知道

983
00:33:42,899 --> 00:33:45,000
他们这样做是不合理或出乎意料的

984
00:33:45,000 --> 00:33:47,039
现在也展示这些模式，

985
00:33:47,039 --> 00:33:48,899
无论他们是否这样做，我认为这

986
00:33:48,899 --> 00:33:51,299
仍然是一个开放的经验问题，

987
00:33:51,299 --> 00:33:52,740


988
00:33:52,740 --> 00:33:54,120
嗯，因为我们必须用

989
00:33:54,120 --> 00:33:55,620
少量数据训练他们并测试他们的

990
00:33:55,620 --> 00:33:57,480
概括，以及这些

991
00:33:57,480 --> 00:33:58,260
事情，

992
00:33:58,260 --> 00:34:00,120
嗯，但我不知道 不要认为

993
00:34:00,120 --> 00:34:01,620


994
00:34:01,620 --> 00:34:03,899
你知道人类做的事情

995
00:34:03,899 --> 00:34:06,059
并非纯粹基于频率这一事实是任何

996
00:34:06,059 --> 00:34:07,860
证据都是正确的，因为

997
00:34:07,860 --> 00:34:09,300
一旦你学习了丰富而

998
00:34:09,300 --> 00:34:10,980
有趣的理论类别，那么

999
00:34:10,980 --> 00:34:13,980
这就是预期的行为，

1000
00:34:13,980 --> 00:34:16,980
嗯实际上 我大约一年前有一篇论文，

1001
00:34:16,980 --> 00:34:18,719
我认为你很

1002
00:34:18,719 --> 00:34:20,040
熟悉

1003
00:34:20,040 --> 00:34:22,560
um uh Yang 和 pianta dosi

1004
00:34:22,560 --> 00:34:24,179
我们在哪里

1005
00:34:24,179 --> 00:34:26,879
um uh 看着

1006
00:34:26,879 --> 00:34:29,280
um uh 当你给

1007
00:34:29,280 --> 00:34:32,639
程序学习模型字符串时会发生什么 呃

1008
00:34:32,639 --> 00:34:34,679
不同的形式语言所以想像

1009
00:34:34,679 --> 00:34:35,820


1010
00:34:35,820 --> 00:34:38,760
嗯给出一个通用模型只是你

1011
00:34:38,760 --> 00:34:41,159
知道 10 或 20 个可能

1012
00:34:41,159 --> 00:34:43,500
遵循某种模式的简单字符串然后要求它

1013
00:34:43,500 --> 00:34:46,199
找到一个程序可以解释该

1014
00:34:46,199 --> 00:34:49,320
数据这通常意味着你知道找到

1015
00:34:49,320 --> 00:34:50,940
嗯嗯找到一些 一种以编程方式

1016
00:34:50,940 --> 00:34:52,679


1017
00:34:52,679 --> 00:34:54,839
在字符串中写下模式的方法，

1018
00:34:54,839 --> 00:34:56,280
嗯，在那个图中，我们有一篇论文，

1019
00:34:56,280 --> 00:34:58,020
它与这一点真正相关，

1020
00:34:58,020 --> 00:34:59,820


1021
00:34:59,820 --> 00:35:02,640
嗯，那种

1022
00:35:02,640 --> 00:35:04,200
模型做出的

1023
00:35:04,200 --> 00:35:06,660
呃概括，嗯，我认为有点 定性地

1024
00:35:06,660 --> 00:35:08,160
就像你为人们描述的那样，

1025
00:35:08,160 --> 00:35:10,560


1026
00:35:10,560 --> 00:35:12,060
嗯，你可以给他们少量

1027
00:35:12,060 --> 00:35:13,980
的数据，它会以

1028
00:35:13,980 --> 00:35:16,740
很高的概率预测看不见的字符串，

1029
00:35:16,740 --> 00:35:18,420
即使在训练输入中的频率为零，

1030
00:35:18,420 --> 00:35:20,280
也是

1031
00:35:20,280 --> 00:35:22,740
如此的原因 通常，您所看到的对数据的最

1032
00:35:22,740 --> 00:35:24,660
简洁的计算描述

1033
00:35:24,660 --> 00:35:26,640
是

1034
00:35:26,640 --> 00:35:29,640
预测一些特定的，嗯，新的看不见的

1035
00:35:29,640 --> 00:35:32,940
输出，因此该模型本质上是

1036
00:35:32,940 --> 00:35:34,800


1037
00:35:34,800 --> 00:35:36,839
Chader 和 Vitani 程序

1038
00:35:36,839 --> 00:35:38,160
学习的一种实现，

1039
00:35:38,160 --> 00:35:40,619
我的想法 较早提出来，

1040
00:35:40,619 --> 00:35:42,900
嗯，但

1041
00:35:42,900 --> 00:35:43,980
如果你在

1042
00:35:43,980 --> 00:35:45,660
这些争论的背景下思考孩子们说不

1043
00:35:45,660 --> 00:35:48,599
寻常或意想不到的事情，就像

1044
00:35:48,599 --> 00:35:50,280
所有这些类型的

1045
00:35:50,280 --> 00:35:52,859
账户都预测的那样，我认为你知道这一点，因为

1046
00:35:52,859 --> 00:35:54,180
只要 这些东西有效地

1047
00:35:54,180 --> 00:35:55,680
比较了一个有趣的语法空间，

1048
00:35:55,680 --> 00:35:56,760


1049
00:35:56,760 --> 00:35:58,079
嗯，然后他们将展示那种

1050
00:35:58,079 --> 00:35:59,760
行为，我认为

1051
00:35:59,760 --> 00:36:04,680
嗯，是的，很好，所以我想你知道

1052
00:36:04,680 --> 00:36:06,960
这个论点是至少从

1053
00:36:06,960 --> 00:36:10,140
性别的角度来看，语法是分开

1054
00:36:10,140 --> 00:36:13,140
运作的 但它仍然

1055
00:36:13,140 --> 00:36:15,359
映射到语义 它

1056
00:36:15,359 --> 00:36:17,040
正确地告知语用学 所以在中东

1057
00:36:17,040 --> 00:36:18,480
程序语法显然是没有意义的

1058
00:36:18,480 --> 00:36:20,160
它非常小 它只是它只是一个

1059
00:36:20,160 --> 00:36:22,320
线性化和标记 它们是

1060
00:36:22,320 --> 00:36:24,180
仅有的两个操作你有一个

1061
00:36:24,180 --> 00:36:26,220
线性化算法到中心电机

1062
00:36:26,220 --> 00:36:27,900
系统和某种

1063
00:36:27,900 --> 00:36:30,480
在概念系统的嗯中心的分类算法

1064
00:36:30,480 --> 00:36:32,220


1065
00:36:32,220 --> 00:36:33,839
嗯所以乔姆斯基的架构有点

1066
00:36:33,839 --> 00:36:35,940
依赖于将句法映射

1067
00:36:35,940 --> 00:36:37,560
到语义的过程，它是泡沫意思

1068
00:36:37,560 --> 00:36:39,960
规则它不仅仅是结构而且

1069
00:36:39,960 --> 00:36:42,180
它不仅仅是意义所以 LMS 真的

1070
00:36:42,180 --> 00:36:43,680
没有这个 映射过程

1071
00:36:43,680 --> 00:36:45,180
就像映射到语义在哪里

1072
00:36:45,180 --> 00:36:47,160
，如果有映射，

1073
00:36:47,160 --> 00:36:48,480
映射过程看起来像什么，

1074
00:36:48,480 --> 00:36:50,099
它的语义属性是什么，

1075
00:36:50,099 --> 00:36:52,320
你知道这些是什么，

1076
00:36:52,320 --> 00:36:54,000
语义属性放在

1077
00:36:54,000 --> 00:36:55,500
它们自己的集合上是什么 对

1078
00:36:55,500 --> 00:36:57,180
营销过程的限制，就像他们对

1079
00:36:57,180 --> 00:36:58,980
自然语言所做的那样，你

1080
00:36:58,980 --> 00:37:01,079
知道吗，嗯，做这些限制相互通知，

1081
00:37:01,079 --> 00:37:02,640
他们是一种

1082
00:37:02,640 --> 00:37:05,579
来回过程，就像元素似乎并

1083
00:37:05,579 --> 00:37:07,260
没有真正描述这种形式

1084
00:37:07,260 --> 00:37:09,359
意味着配对 正确，例如

1085
00:37:09,359 --> 00:37:11,339
哪些字符串的含义

1086
00:37:11,339 --> 00:37:14,640
正确 抱歉，您是说

1087
00:37:14,640 --> 00:37:16,740
嗯，它们根本没有语义，

1088
00:37:16,740 --> 00:37:18,599
还是说

1089
00:37:18,599 --> 00:37:21,180


1090
00:37:21,180 --> 00:37:23,400
结构如何映射到

1091
00:37:23,400 --> 00:37:25,500
语义之间没有明确的描述 是的，后者 是的，所以他们

1092
00:37:25,500 --> 00:37:27,300
显然有一些潜在的某种

1093
00:37:27,300 --> 00:37:28,800
语义 我知道你已经争论过一个

1094
00:37:28,800 --> 00:37:30,180
概念角色 理论在这里是相关的 对

1095
00:37:30,180 --> 00:37:31,920
吧，它的其余部分可能

1096
00:37:31,920 --> 00:37:33,900
有点神秘，但实际的大豆

1097
00:37:33,900 --> 00:37:35,339
语言学在

1098
00:37:35,339 --> 00:37:36,900
映射过程的理论 它本身是

1099
00:37:36,900 --> 00:37:39,060
明确的，你可以在行动中看到它，你

1100
00:37:39,060 --> 00:37:40,440
可以

1101
00:37:40,440 --> 00:37:42,000
在 Psych 语言模型中测试它的不同理论，你有什么

1102
00:37:42,000 --> 00:37:44,700
实际的规则你

1103
00:37:44,700 --> 00:37:46,079
知道的那种你知道的约束

1104
00:37:46,079 --> 00:37:48,119
歧义在你知道一个

1105
00:37:48,119 --> 00:37:50,400
词的多重含义或一个意义上的歧义 构造

1106
00:37:50,400 --> 00:37:53,040
多重解释等等，是

1107
00:37:53,040 --> 00:37:55,200
的，我的意思是，如果你认为他们有

1108
00:37:55,200 --> 00:37:57,599
语义，那么我认为他们必须

1109
00:37:57,599 --> 00:37:59,640
有从语法到语义的映射，嗯，

1110
00:37:59,640 --> 00:38:00,960


1111
00:38:00,960 --> 00:38:03,599
我同意，这并不是说没有人

1112
00:38:03,599 --> 00:38:04,980
真正理解他们是如何

1113
00:38:04,980 --> 00:38:07,560
在任何深层次上工作的 是的，所以我同意，

1114
00:38:07,560 --> 00:38:10,140
它不像

1115
00:38:10,140 --> 00:38:11,940
生成语法和语义中所说的那样清楚

1116
00:38:11,940 --> 00:38:13,859


1117
00:38:13,859 --> 00:38:15,180


1118
00:38:15,180 --> 00:38:17,880


1119
00:38:17,880 --> 00:38:19,920


1120
00:38:19,920 --> 00:38:21,480
或者像

1121
00:38:21,480 --> 00:38:23,339
这样的事情是

1122
00:38:23,339 --> 00:38:24,660
嗯你知道那不是他们

1123
00:38:24,660 --> 00:38:27,060
工作的方式但

1124
00:38:27,060 --> 00:38:28,859
嗯我我只是我我不会

1125
00:38:28,859 --> 00:38:32,160
想当然地认为它必须像那样

1126
00:38:32,160 --> 00:38:34,619
嗯嗯这可能是他们的

1127
00:38:34,619 --> 00:38:36,420
工作方式实际上是我们的方式 正确地工作，

1128
00:38:36,420 --> 00:38:38,760
嗯，一切都在

1129
00:38:38,760 --> 00:38:40,800
一些高维向量空间中表示，并且

1130
00:38:40,800 --> 00:38:43,740
有一些复杂的呃方式，

1131
00:38:43,740 --> 00:38:46,200
其中向量语义随着

1132
00:38:46,200 --> 00:38:48,240
每个额外的单词或

1133
00:38:48,240 --> 00:38:51,480
语言流中的任何东西而更新，

1134
00:38:51,480 --> 00:38:53,940
但就像我一样，我认为很明显

1135
00:38:53,940 --> 00:38:55,320
他们有一些 一种

1136
00:38:55,320 --> 00:38:57,660


1137
00:38:57,660 --> 00:38:59,099
句子语义的表示，就像他们可以

1138
00:38:59,099 --> 00:39:01,260
至少大约回答问题一样

1139
00:39:01,260 --> 00:39:02,760


1140
00:39:02,760 --> 00:39:04,380


1141
00:39:04,380 --> 00:39:06,720


1142
00:39:06,720 --> 00:39:07,740


1143
00:39:07,740 --> 00:39:10,500


1144
00:39:10,500 --> 00:39:12,780
嗯，我认为他们是，

1145
00:39:12,780 --> 00:39:14,040
嗯，他们肯定

1146
00:39:14,040 --> 00:39:16,920
代表语义，

1147
00:39:16,920 --> 00:39:20,040
嗯，你知道更新它，因为他们

1148
00:39:20,040 --> 00:39:21,599
处理语言，它恰好

1149
00:39:21,599 --> 00:39:23,400
看起来不像其他正式

1150
00:39:23,400 --> 00:39:24,599
理论，

1151
00:39:24,599 --> 00:39:26,400
嗯，我想我不明白 为什么

1152
00:39:26,400 --> 00:39:27,720
这是一个像其他

1153
00:39:27,720 --> 00:39:29,640
形式理论一样的问题 可能只是你知道

1154
00:39:29,640 --> 00:39:31,920
近似值很差或者完全

1155
00:39:31,920 --> 00:39:33,720
错误

1156
00:39:33,720 --> 00:39:35,520


1157
00:39:35,520 --> 00:39:37,260


1158
00:39:37,260 --> 00:39:39,359


1159
00:39:39,359 --> 00:39:41,520


1160
00:39:41,520 --> 00:39:42,780
这些事情做对了，所以另一种

1161
00:39:42,780 --> 00:39:45,240
思考方式是你知道 LMS

1162
00:39:45,240 --> 00:39:47,760
很好 LMS 是压缩算法，

1163
00:39:47,760 --> 00:39:49,740
但自然语言理解

1164
00:39:49,740 --> 00:39:51,540
更像是解压，

1165
00:39:51,540 --> 00:39:54,180
它消除了 X 的歧义

1166
00:39:54,180 --> 00:39:56,099
XYZ 的意义，这一切都是关于

1167
00:39:56,099 --> 00:39:58,020
对你知道的进行推断

1168
00:39:58,020 --> 00:39:59,579
不在

1169
00:39:59,579 --> 00:40:01,859
训练数据中的概念之间的元关系，所以

1170
00:40:01,859 --> 00:40:03,180
Melanie Mitchell 给出了一些例子，比如

1171
00:40:03,180 --> 00:40:06,119
在上面，你知道她又在上面，

1172
00:40:06,119 --> 00:40:08,700
呃，在框的顶部，所有这些都

1173
00:40:08,700 --> 00:40:10,140
随着上下文而变化，所以有很多

1174
00:40:10,140 --> 00:40:11,880
其他事情正在发生，

1175
00:40:11,880 --> 00:40:13,260
嗯，我想你

1176
00:40:13,260 --> 00:40:16,440
在论文中讨论了其中的一些例子，所以你知道，

1177
00:40:16,440 --> 00:40:18,119
嗯，但是语言的能力

1178
00:40:18,119 --> 00:40:20,940
至少仍然不是在这种语言理论下，

1179
00:40:20,940 --> 00:40:22,920
它不是关于字符串

1180
00:40:22,920 --> 00:40:25,200
生成，而是关于这种形式 意思是

1181
00:40:25,200 --> 00:40:27,540
配对机器所以有时这在

1182
00:40:27,540 --> 00:40:29,400
属格传统中甚至认为

1183
00:40:29,400 --> 00:40:31,859
语义的一切都是公正和正确的所以

1184
00:40:31,859 --> 00:40:33,540
保罗彼得罗夫斯基的联合是有

1185
00:40:33,540 --> 00:40:36,000
一些是人类语义是公正的而且就是这样嗯这又

1186
00:40:36,000 --> 00:40:37,500


1187
00:40:37,500 --> 00:40:39,900
是非常简单优雅

1188
00:40:39,900 --> 00:40:42,000
它是它是它是可解释的 它

1189
00:40:42,000 --> 00:40:43,920
与你知道的很多事情兼容，

1190
00:40:43,920 --> 00:40:46,320
或者可能会在你的

1191
00:40:46,320 --> 00:40:47,940
词的脖子上发生，但不管

1192
00:40:47,940 --> 00:40:49,859
它仍然是你知道自然语言

1193
00:40:49,859 --> 00:40:51,540
仍然更具组合性，

1194
00:40:51,540 --> 00:40:54,060
比如呃你知道形式

1195
00:40:54,060 --> 00:40:55,260
语言只是为了做出明确的

1196
00:40:55,260 --> 00:40:56,820
区分 已经使它们具有

1197
00:40:56,820 --> 00:40:58,260
更丰富的结构组合，有

1198
00:40:58,260 --> 00:41:00,839
更多的东西在发生，嗯，也许所以

1199
00:41:00,839 --> 00:41:02,160
之前有人指出，你

1200
00:41:02,160 --> 00:41:03,480
知道诸如基于注意力的机器

1201
00:41:03,480 --> 00:41:05,640
机制和变形金刚之类的东西，

1202
00:41:05,640 --> 00:41:07,800
嗯，允许离散

1203
00:41:07,800 --> 00:41:10,320
令牌绑定的组合，这更接近于

1204
00:41:10,320 --> 00:41:12,359
合并 like operator than simple

1205
00:41:12,359 --> 00:41:14,640
recurrent matrix multiplication

1206
00:41:14,640 --> 00:41:16,140
um but you know the issue of binary

1207
00:41:16,140 --> 00:41:17,460
branching binary branching government

1208
00:41:17,460 --> 00:41:19,260
只是在这里选择另一个例子来

1209
00:41:19,260 --> 00:41:20,640
谈论完整的含义规则

1210
00:41:20,640 --> 00:41:23,640
一个原则二元分支图像是

1211
00:41:23,640 --> 00:41:24,839
一个有趣的问题，但几何

1212
00:41:24,839 --> 00:41:26,700
语法一直对

1213
00:41:26,700 --> 00:41:28,740
不同的人开放 合成计算中这种明显约束的起源和位置

1214
00:41:28,740 --> 00:41:30,660


1215
00:41:30,660 --> 00:41:31,800
就像它

1216
00:41:31,800 --> 00:41:33,540
来自哪里可能是

1217
00:41:33,540 --> 00:41:34,980
合并的条件可能是由平滑

1218
00:41:34,980 --> 00:41:37,079
系统强加的可能是一种你

1219
00:41:37,079 --> 00:41:39,060
知道谁知道的先验实际上它是一些

1220
00:41:39,060 --> 00:41:40,500
最近的工作生成语法

1221
00:41:40,500 --> 00:41:43,859
试图建立并消除所有关于

1222
00:41:43,859 --> 00:41:46,440
婚姻

1223
00:41:46,440 --> 00:41:47,700
权利的理论假设也许集合论不是

1224
00:41:47,700 --> 00:41:48,780


1225
00:41:48,780 --> 00:41:50,339
建模生成语法的最佳方法也许玛丽亚

1226
00:41:50,339 --> 00:41:51,540
逻辑解释更合适

1227
00:41:51,540 --> 00:41:53,520
那里有很多其他最近的想法都与

1228
00:41:53,520 --> 00:41:55,020


1229
00:41:55,020 --> 00:41:57,540
乔姆斯基的方法是正确的事实上

1230
00:41:57,540 --> 00:41:58,680
是的特朗普

1231
00:41:58,680 --> 00:42:00,060
最喜欢的事情之一就是当他被

1232
00:42:00,060 --> 00:42:01,380
证明是错误的时候很多这些

1233
00:42:01,380 --> 00:42:03,720
理论都反对核心

1234
00:42:03,720 --> 00:42:06,300
主流极简主义架构但是

1235
00:42:06,300 --> 00:42:08,640
是的我认为这是一个非常多样化的

1236
00:42:08,640 --> 00:42:11,839
喜欢 充满活力的领域

1237
00:42:11,839 --> 00:42:15,119
Bornstein 的人，你认识 petrovsky uh uh

1238
00:42:15,119 --> 00:42:17,520
hajipura 他们在根本上不同意

1239
00:42:17,520 --> 00:42:19,380


1240
00:42:19,380 --> 00:42:20,940
化学语法的主流观点，但

1241
00:42:20,940 --> 00:42:21,960
仍有更多的

1242
00:42:21,960 --> 00:42:24,240
分歧空间，但它仍然

1243
00:42:24,240 --> 00:42:26,400
与正确设置核心假设兼容，所以

1244
00:42:26,400 --> 00:42:27,599
很多 大卫我只是

1245
00:42:27,599 --> 00:42:29,400
在这个核心方面穿着例如有点偏离

1246
00:42:29,400 --> 00:42:31,920
但它仍然试图

1247
00:42:31,920 --> 00:42:33,240
在不同的正式系统中建立这些直觉

1248
00:42:33,240 --> 00:42:34,500


1249
00:42:34,500 --> 00:42:36,000
嗯所以你知道

1250
00:42:36,000 --> 00:42:38,099


1251
00:42:38,099 --> 00:42:40,320
我想再次了解你的想法嗯

1252
00:42:40,320 --> 00:42:42,240
我提到米切尔是对的所以米其林

1253
00:42:42,240 --> 00:42:44,579
鲍尔斯呃 2020 年，他们有这篇论文

1254
00:42:44,579 --> 00:42:47,040
试验列出了递归网络，

1255
00:42:47,040 --> 00:42:48,359
我想你可能

1256
00:42:48,359 --> 00:42:49,859
知道这是对的，所以这是一个很好的

1257
00:42:49,859 --> 00:42:51,060
例子，只是为了触及

1258
00:42:51,060 --> 00:42:53,099
问题的核心，所以递归神经

1259
00:42:53,099 --> 00:42:54,780
网络已经被证明可以准确地

1260
00:42:54,780 --> 00:42:56,520
模拟你所知道的非 - 动词数协议，

1261
00:42:56,520 --> 00:42:58,020
但 Mitchell 和 Barrow 表明，

1262
00:42:58,020 --> 00:43:00,060
这些网络还将

1263
00:43:00,060 --> 00:43:01,680
与不自然的句子

1264
00:43:01,680 --> 00:43:03,359
结构达成数字协议，因此

1265
00:43:03,359 --> 00:43:04,859
自然语言中没有的结构以及

1266
00:43:04,859 --> 00:43:06,540
人类很难正确处理的结构，

1267
00:43:06,540 --> 00:43:09,359
因此 rnns 的学习模式至少是

1268
00:43:09,359 --> 00:43:11,880
对于 rnn 与婴儿的肯定区别，

1269
00:43:11,880 --> 00:43:14,339
你知道婴儿智人是

1270
00:43:14,339 --> 00:43:16,260
对的，所以故事是米切尔和

1271
00:43:16,260 --> 00:43:18,359
鲍尔斯表明，虽然 lstl 模型

1272
00:43:18,359 --> 00:43:20,040
很好地表示了

1273
00:43:20,040 --> 00:43:22,140
单个句子的单数与复数，但

1274
00:43:22,140 --> 00:43:24,359
没有进行正确的概括，

1275
00:43:24,359 --> 00:43:25,800
它们可以在个体

1276
00:43:25,800 --> 00:43:27,359
层面上表示 因此该模型没有将

1277
00:43:27,359 --> 00:43:28,859
数字表示为

1278
00:43:28,859 --> 00:43:31,200
抽象，什么数字只是

1279
00:43:31,200 --> 00:43:34,020
单数基础复数的具体实例，

1280
00:43:34,020 --> 00:43:35,400
所以通过 LM 成功预测语言

1281
00:43:35,400 --> 00:43:38,579
行为或

1282
00:43:38,579 --> 00:43:40,800
以类似方式成功预测神经反应

1283
00:43:40,800 --> 00:43:42,480
显然很棒，也许我们可以

1284
00:43:42,480 --> 00:43:43,920
进入 那个问题稍后但

1285
00:43:43,920 --> 00:43:45,240
这里只有硬币的一面硬币的

1286
00:43:45,240 --> 00:43:47,099
另一面正在解释为什么

1287
00:43:47,099 --> 00:43:48,780
这种行为而不是其他一些

1288
00:43:48,780 --> 00:43:50,339
行为为什么这个结构我不

1289
00:43:50,339 --> 00:43:52,740
相似而且这可能是乔姆斯基最

1290
00:43:52,740 --> 00:43:55,380
喜欢你最了解他的 重要的

1291
00:43:55,380 --> 00:43:56,760
一点真的为什么这不是其他

1292
00:43:56,760 --> 00:43:59,400
系统所以语言理论

1293
00:43:59,400 --> 00:44:00,720
给了你那个或硬币的开始，

1294
00:44:00,720 --> 00:44:02,760
而 LM 真的完成了所以

1295
00:44:02,760 --> 00:44:03,839
米切尔尴尬的论文做了

1296
00:44:03,839 --> 00:44:05,819
类似的事情他做得

1297
00:44:05,819 --> 00:44:09,420
很好是的所以就像拿 um Yael Le

1298
00:44:09,420 --> 00:44:11,400
Haynes 的 crets 和 stanislash 是 2019 年的，

1299
00:44:11,400 --> 00:44:13,319
对吧，他们在 lstm 中查看了数字协议，

1300
00:44:13,319 --> 00:44:15,480
发现了两个专门

1301
00:44:15,480 --> 00:44:17,640
编码数字协议的单元，但对

1302
00:44:17,640 --> 00:44:19,020
性能的总体贡献

1303
00:44:19,020 --> 00:44:21,839
很低，然后在 2021 年，嗯，是的，更正

1304
00:44:21,839 --> 00:44:24,000
了这篇论文，他们

1305
00:44:24,000 --> 00:44:26,040
在其中展示了嗯 他们的神经语言模型

1306
00:44:26,040 --> 00:44:28,079
没有实现意大利语中

1307
00:44:28,079 --> 00:44:30,540
嵌套远程协议性别

1308
00:44:30,540 --> 00:44:32,160
标记的真正递归处理，我想

1309
00:44:32,160 --> 00:44:34,020
嗯，即使

1310
00:44:34,020 --> 00:44:35,819
你所知道的一些分层处理已经实现，正如你

1311
00:44:35,819 --> 00:44:37,380
之前所说的那样，一些层次结构仍然

1312
00:44:37,380 --> 00:44:39,960
存在，但问题是 它是

1313
00:44:39,960 --> 00:44:41,280
正确的映射吗？它是正确

1314
00:44:41,280 --> 00:44:42,900
的层次结构吗？他们发现基于

1315
00:44:42,900 --> 00:44:45,119
lstn 的模型可以

1316
00:44:45,119 --> 00:44:47,220
在短跨度内达成主题网络协议，一个

1317
00:44:47,220 --> 00:44:49,260
嵌入度，但他们在一些较长的依赖关系上失败了，

1318
00:44:49,260 --> 00:44:51,359
在最近的

1319
00:44:51,359 --> 00:44:53,700
论文中，uh La crepe satell with 手

1320
00:44:53,700 --> 00:44:56,760
并表明他们在相同任务上评估了

1321
00:44:56,760 --> 00:45:00,180
包括 gpt2 XL 在内的现代 Transformer LMS，

1322
00:45:00,180 --> 00:45:01,980
并且 Transformer 的表现

1323
00:45:01,980 --> 00:45:04,260
比 LSM 与人类更相似，

1324
00:45:04,260 --> 00:45:06,300
并且总体上表现优于转移，但

1325
00:45:06,300 --> 00:45:08,040
他们在一个关键条件下的表现仍然低于机会，

1326
00:45:08,040 --> 00:45:09,660
正如我

1327
00:45:09,660 --> 00:45:11,099
提到的那样 多重嵌入一个

1328
00:45:11,099 --> 00:45:13,020
困难的经文所以

1329
00:45:13,020 --> 00:45:14,400
我提到这些研究的原因是因为

1330
00:45:14,400 --> 00:45:17,040
你知道这不仅仅是探索

1331
00:45:17,040 --> 00:45:18,540
OMS 的局限性这是一个有趣的

1332
00:45:18,540 --> 00:45:19,500
问题

1333
00:45:19,500 --> 00:45:21,540
嗯但考虑像 UCL 的 Neil Smith 这样的人的工作

1334
00:45:21,540 --> 00:45:24,180
，他确实在

1335
00:45:24,180 --> 00:45:26,579
上世纪 90 年代，通晓多种语言的 Savant 和典型的

1336
00:45:26,579 --> 00:45:28,740
神经控制者将它们进行比较，因此

1337
00:45:28,740 --> 00:45:30,540
他研究了

1338
00:45:30,540 --> 00:45:32,520
包含

1339
00:45:32,520 --> 00:45:34,500
自然和非自然图形

1340
00:45:34,500 --> 00:45:35,880
结构的人工语言的第二语言学习，例如米其林病毒论文，

1341
00:45:35,880 --> 00:45:37,079
整个框架是自然的

1342
00:45:37,079 --> 00:45:39,119
与非自然的，他们发现

1343
00:45:39,119 --> 00:45:41,000
虽然学者

1344
00:45:41,000 --> 00:45:43,319
和 控件可以掌握

1345
00:45:43,319 --> 00:45:45,480
语言上的自然方面，只有

1346
00:45:45,480 --> 00:45:46,920
控件最终可以处理

1347
00:45:46,920 --> 00:45:48,660
依赖于结构的非自然现象，

1348
00:45:48,660 --> 00:45:50,460
而且它们都不能掌握

1349
00:45:50,460 --> 00:45:52,560
结构独立方面，所以一些

1350
00:45:52,560 --> 00:45:53,880
奇怪的规则就像你知道你

1351
00:45:53,880 --> 00:45:55,440
把重点放在句子的第三个词上

1352
00:45:55,440 --> 00:45:56,940
像这样，所以他们

1353
00:45:56,940 --> 00:45:58,740
争辩说克里斯托弗的能力

1354
00:45:58,740 --> 00:46:00,900
完全归功于他完整的语言

1355
00:46:00,900 --> 00:46:03,480
能力，但控制可以使用

1356
00:46:03,480 --> 00:46:05,579
更多领域一般的认知

1357
00:46:05,579 --> 00:46:07,319
资源，比如你知道的注意力

1358
00:46:07,319 --> 00:46:09,900
控制等等，这就是为什么他们可以处理

1359
00:46:09,900 --> 00:46:11,520
困难的过程，

1360
00:46:11,520 --> 00:46:13,319
但我刚才提到你知道一个 一分钟

1361
00:46:13,319 --> 00:46:16,079
前，米切尔尴尬论文中的 lstm

1362
00:46:16,079 --> 00:46:18,359


1363
00:46:18,359 --> 00:46:19,920
以几乎

1364
00:46:19,920 --> 00:46:22,319
相同的方式接近自然和非自然结构，所以你不知道这不是一个

1365
00:46:22,319 --> 00:46:24,240
心理上合理的模型，我认为

1366
00:46:24,240 --> 00:46:26,400
无论人类在做什么，

1367
00:46:26,400 --> 00:46:28,200
类似的观察都可以应用于

1368
00:46:28,200 --> 00:46:30,060
La creta 作品中的 Transformer 模型

1369
00:46:30,060 --> 00:46:31,920
和所有这些主题都

1370
00:46:31,920 --> 00:46:33,780
很好，它们一直伴随着

1371
00:46:33,780 --> 00:46:35,819
我们直到现在，所以 talins

1372
00:46:35,819 --> 00:46:37,560
最近发表的另一篇论文他在

1373
00:46:37,560 --> 00:46:39,240
几周前发表了一篇关于儿童

1374
00:46:39,240 --> 00:46:41,880
定向演讲的论文表明，嗯 Lstms 和

1375
00:46:41,880 --> 00:46:43,740
Transformers 仅限于生态学上

1376
00:46:43,740 --> 00:46:46,380
合理的数据量，因为

1377
00:46:46,380 --> 00:46:47,640
我提到了英语权利的线性规则

1378
00:46:47,640 --> 00:46:49,920
而不是抽象规则，

1379
00:46:49,920 --> 00:46:51,780
事实上上周林顿实验室最近的工作

1380
00:46:51,780 --> 00:46:54,599
看着呃

1381
00:46:54,599 --> 00:46:56,220
去年我应该说看

1382
00:46:56,220 --> 00:46:58,160
花园 意外路径并不能解释

1383
00:46:58,160 --> 00:47:01,319
语法消歧的困难，

1384
00:47:01,319 --> 00:47:02,280


1385
00:47:02,280 --> 00:47:03,900
嗯，意外会低估

1386
00:47:03,900 --> 00:47:05,400
所有结构中花园小径效应的大小，

1387
00:47:05,400 --> 00:47:06,780
这就涉及到

1388
00:47:06,780 --> 00:47:08,099
你之前提到的这个问题，你

1389
00:47:08,099 --> 00:47:10,140
可能会惊讶所有这些与

1390
00:47:10,140 --> 00:47:11,520
语法的某些方面有关，但也许不会

1391
00:47:11,520 --> 00:47:12,960
其他的，这是一个非常

1392
00:47:12,960 --> 00:47:14,819
非致敬的问题，非常

1393
00:47:14,819 --> 00:47:16,800
开放讨论，不是尚未

1394
00:47:16,800 --> 00:47:18,720
解决，但林惇表明

1395
00:47:18,720 --> 00:47:20,640
花园小径效果

1396
00:47:20,640 --> 00:47:21,720
比您对我的期望要困难得多

1397
00:47:21,720 --> 00:47:24,359
不可预测性，所以另一种

1398
00:47:24,359 --> 00:47:26,160
表达这个论点的方式

1399
00:47:26,160 --> 00:47:29,160
是引用

1400
00:47:29,160 --> 00:47:30,660
乔姆斯基最近关于解决这个自然

1401
00:47:30,660 --> 00:47:32,940
基础的不自然问题的论点，他说假设我们

1402
00:47:32,940 --> 00:47:34,560
有一个扩展的元素周期表，其中

1403
00:47:34,560 --> 00:47:36,180
包括所有确实存在的元素

1404
00:47:36,180 --> 00:47:38,819
或可能存在的元素 存在

1405
00:47:38,819 --> 00:47:40,740
和所有不可能

1406
00:47:40,740 --> 00:47:42,660
存在的元素，假设你有

1407
00:47:42,660 --> 00:47:44,880
一些模型，嗯，一些人工模型，

1408
00:47:44,880 --> 00:47:46,560
无法区分这三个

1409
00:47:46,560 --> 00:47:48,780
类别，无论这个模型在做什么，

1410
00:47:48,780 --> 00:47:50,640
它都不能帮助我们理解化学，对，它在

1411
00:47:50,640 --> 00:47:52,020
做其他事情，

1412
00:47:52,020 --> 00:47:53,940
它正在做一些事情 当然，但

1413
00:47:53,940 --> 00:47:55,020
是否必须了解

1414
00:47:55,020 --> 00:47:57,180
化学是另外一回事，我

1415
00:47:57,180 --> 00:47:58,560
知道你在回应其中

1416
00:47:58,560 --> 00:47:59,579
一些研究时说过，我想你

1417
00:47:59,579 --> 00:48:02,400
说过你知道，并且为了表明

1418
00:48:02,400 --> 00:48:03,540
某些事情

1419
00:48:03,540 --> 00:48:04,920
在某个地方可能是不可能的 在你的论文中，我

1420
00:48:04,920 --> 00:48:06,240
认为你说

1421
00:48:06,240 --> 00:48:07,859
嗯是为了表明在误报上政治

1422
00:48:07,859 --> 00:48:09,720
的正常平衡是不可能的，

1423
00:48:09,720 --> 00:48:12,300
你需要

1424
00:48:12,300 --> 00:48:13,440
表明你需要查看类似

1425
00:48:13,440 --> 00:48:15,780
500 种独立采样的语言，所以

1426
00:48:15,780 --> 00:48:17,880
你在论文中引用了这个，

1427
00:48:17,880 --> 00:48:19,020
嗯，你 可能不能那样做，

1428
00:48:19,020 --> 00:48:20,579
这不是一件可行的事情，

1429
00:48:20,579 --> 00:48:23,880
所以你知道我不是

1430
00:48:23,880 --> 00:48:25,800


1431
00:48:25,800 --> 00:48:27,119


1432
00:48:27,119 --> 00:48:29,040


1433
00:48:29,040 --> 00:48:30,839
关于原则上不可能的争论，而不是

1434
00:48:30,839 --> 00:48:32,220


1435
00:48:32,220 --> 00:48:33,599
你知道的某种扩展意义上的嗯，

1436
00:48:33,599 --> 00:48:34,980
就像在

1437
00:48:34,980 --> 00:48:37,440
世界各地搜索语言，以证明在

1438
00:48:37,440 --> 00:48:38,579
每一种语言中都是

1439
00:48:38,579 --> 00:48:40,260
不可能的，对吧，这是一个

1440
00:48:40,260 --> 00:48:41,280
不同的论点，它是否

1441
00:48:41,280 --> 00:48:43,740
在某种随机语言中是不可能的 在

1442
00:48:43,740 --> 00:48:45,180
亚马逊中，根据

1443
00:48:45,180 --> 00:48:47,220


1444
00:48:47,220 --> 00:48:48,420
语言系统实际

1445
00:48:48,420 --> 00:48:50,160
做的事情的原则，实际上是不可能的，就像它可以做的那样，所以我

1446
00:48:50,160 --> 00:48:53,339
只想说是的，我认为

1447
00:48:53,339 --> 00:48:55,980
那一点是你实际上不

1448
00:48:55,980 --> 00:48:58,560
知道什么是类型学上的 可能是对的，

1449
00:48:58,560 --> 00:49:00,480
所以有些人喜欢说

1450
00:49:00,480 --> 00:49:02,520
你知道没有语言可以

1451
00:49:02,520 --> 00:49:04,859
做 X，因此我们必须

1452
00:49:04,859 --> 00:49:06,960
在我们的统计

1453
00:49:06,960 --> 00:49:09,119
模型中正确建立这种限制，但是如果

1454
00:49:09,119 --> 00:49:11,220
统计上没有证明没有

1455
00:49:11,220 --> 00:49:13,079
语言可以做 X 是正确的，如果你有

1456
00:49:13,079 --> 00:49:15,119
只看了 20 或 20 种欧洲

1457
00:49:15,119 --> 00:49:16,920
语言或一些正确的东西我的意思

1458
00:49:16,920 --> 00:49:19,020
是它不是

1459
00:49:19,020 --> 00:49:22,380
嗯嗯那样不应该激励对

1460
00:49:22,380 --> 00:49:24,599
模型做任何事情

1461
00:49:24,599 --> 00:49:26,280
嗯嗯如果它不是

1462
00:49:26,280 --> 00:49:28,200
统计上合理的通用我

1463
00:49:28,200 --> 00:49:28,980
认为

1464
00:49:28,980 --> 00:49:30,180


1465
00:49:30,180 --> 00:49:32,760
嗯你知道我我我认为你 知道

1466
00:49:32,760 --> 00:49:33,960
你是完全正确的，但这只是

1467
00:49:33,960 --> 00:49:35,339
更普遍地适用于社会

1468
00:49:35,339 --> 00:49:36,960
科学和心理科学，

1469
00:49:36,960 --> 00:49:39,180
就像类型学一样，是的，

1470
00:49:39,180 --> 00:49:40,380
很难将这些事情建立

1471
00:49:40,380 --> 00:49:43,140
正确，所以我猜你我猜

1472
00:49:43,140 --> 00:49:44,640
你只是有点陈旧 你是

1473
00:49:44,640 --> 00:49:47,339
说强烈的主张

1474
00:49:47,339 --> 00:49:49,859
很难证明是正确的，

1475
00:49:49,859 --> 00:49:52,619
就像没有语言有 X

1476
00:49:52,619 --> 00:49:54,480


1477
00:49:54,480 --> 00:49:56,339
我认为自然语言中不允许出现某些东西的强烈主张是

1478
00:49:56,339 --> 00:49:58,800
很难证明的，

1479
00:49:58,800 --> 00:49:59,880


1480
00:49:59,880 --> 00:50:01,980
嗯，你知道我认为 已经

1481
00:50:01,980 --> 00:50:05,460
有很多你知道的强有力的

1482
00:50:05,460 --> 00:50:08,460
尝试有很多来自

1483
00:50:08,460 --> 00:50:10,380


1484
00:50:10,380 --> 00:50:12,720
嗯嗯经常来自生成语法的强烈主张

1485
00:50:12,720 --> 00:50:16,800
关于所有语言的作用

1486
00:50:16,800 --> 00:50:19,140
嗯我认为你知道人们

1487
00:50:19,140 --> 00:50:21,119
非常擅长寻找

1488
00:50:21,119 --> 00:50:22,740
反例 对于很多

1489
00:50:22,740 --> 00:50:24,720
事情，我引用了 Evans 和 Levinson 的这篇论文，

1490
00:50:24,720 --> 00:50:26,579


1491
00:50:26,579 --> 00:50:28,500
嗯，实际上你知道我

1492
00:50:28,500 --> 00:50:30,660
多年来一直听说没有语言是如何做 X 的，

1493
00:50:30,660 --> 00:50:32,160
这就是我们用来

1494
00:50:32,160 --> 00:50:33,599
构建我们的理论的东西，Evans

1495
00:50:33,599 --> 00:50:35,460
和 Levin 的 论文 Evans 和 Levinson

1496
00:50:35,460 --> 00:50:37,859
论文真的呃改变了我

1497
00:50:37,859 --> 00:50:40,680
对这个权利的看法就像语言

1498
00:50:40,680 --> 00:50:43,260
实际上比我想象的要多样化得多 嗯

1499
00:50:43,260 --> 00:50:44,940


1500
00:50:44,940 --> 00:50:47,760
大多数句法学家你会知道尝试

1501
00:50:47,760 --> 00:50:50,700
为某些东西构建理论所以嗯嗯

1502
00:50:50,700 --> 00:50:53,579
你知道我我我想我们去

1503
00:50:53,579 --> 00:50:54,839
回到你

1504
00:50:54,839 --> 00:50:57,660
所说的开头，我想我们会同意，呃，

1505
00:50:57,660 --> 00:50:59,880
你需要语言架构来

1506
00:50:59,880 --> 00:51:01,619
学习孩子们学习的东西，并

1507
00:51:01,619 --> 00:51:03,660
从他们学习的数据中学到这些东西，而

1508
00:51:03,660 --> 00:51:05,940
这些架构可能

1509
00:51:05,940 --> 00:51:09,000
不太可能成为事物 像 lstms 或你

1510
00:51:09,000 --> 00:51:10,559
知道简单的循环网络或其他

1511
00:51:10,559 --> 00:51:12,300
任何正确的东西，比如

1512
00:51:12,300 --> 00:51:14,400
嗯我认为所有这些工作

1513
00:51:14,400 --> 00:51:16,680
对于磨练

1514
00:51:16,680 --> 00:51:19,200
正确的架构非常有用

1515
00:51:19,200 --> 00:51:20,460


1516
00:51:20,460 --> 00:51:21,420


1517
00:51:21,420 --> 00:51:23,819
嗯嗯嗯所以我只是想记住所有的

1518
00:51:23,819 --> 00:51:25,200
一切 你提出的观点

1519
00:51:25,200 --> 00:51:26,700
哦，是的，嗯，

1520
00:51:26,700 --> 00:51:29,160
但我认为这有

1521
00:51:29,160 --> 00:51:31,500
一种反面，那

1522
00:51:31,500 --> 00:51:32,760
就是，嗯，

1523
00:51:32,760 --> 00:51:34,380
我认为

1524
00:51:34,380 --> 00:51:37,619
人们可以学习的东西的空间实际上被

1525
00:51:37,619 --> 00:51:39,599
低估了，就像有这种

1526
00:51:39,599 --> 00:51:41,760
偏见一样 要说你知道人们无法

1527
00:51:41,760 --> 00:51:43,800
学习 x y 和 z

1528
00:51:43,800 --> 00:51:46,020
嗯但是人们呃至少在

1529
00:51:46,020 --> 00:51:47,460
语言之外有这种非常

1530
00:51:47,460 --> 00:51:49,680
了不起的能力来学习不同

1531
00:51:49,680 --> 00:51:51,240
种类的模式就像

1532
00:51:51,240 --> 00:51:53,160
你在音乐或数学中找到的模式一样

1533
00:51:53,160 --> 00:51:55,619


1534
00:51:55,619 --> 00:51:57,839
嗯嗯 我们可以学习复杂类型的

1535
00:51:57,839 --> 00:52:00,240
算法，我们可以学习

1536
00:52:00,240 --> 00:52:03,119
你知道驾驶航天飞机，或者你

1537
00:52:03,119 --> 00:52:05,760
知道打结攀岩

1538
00:52:05,760 --> 00:52:07,319
等等，就像那里有

1539
00:52:07,319 --> 00:52:09,540
各种各样的程序和

1540
00:52:09,540 --> 00:52:11,280
算法知识，

1541
00:52:11,280 --> 00:52:13,440
这些知识是结构化的 人们能够

1542
00:52:13,440 --> 00:52:16,680
获得并且我认为那个呃

1543
00:52:16,680 --> 00:52:19,680
概念呃非常正确地激励

1544
00:52:19,680 --> 00:52:21,359
寻找可以在

1545
00:52:21,359 --> 00:52:24,059
非常不受限制的空间工作的学习系统

1546
00:52:24,059 --> 00:52:26,160
所以

1547
00:52:26,160 --> 00:52:28,859
嗯嗯你知道你你可能会说

1548
00:52:28,859 --> 00:52:30,119
好吧语言是不同的

1549
00:52:30,119 --> 00:52:33,420
因为语言是 一个受限的空间

1550
00:52:33,420 --> 00:52:35,040
嗯嗯，

1551
00:52:35,040 --> 00:52:36,540
语言是受限的，但也可能

1552
00:52:36,540 --> 00:52:37,859
是我们在语言中看到的东西

1553
00:52:37,859 --> 00:52:39,960
来自其他来源，对吧，

1554
00:52:39,960 --> 00:52:42,180
嗯，语言可能

1555
00:52:42,180 --> 00:52:44,099
特别实用，例如

1556
00:52:44,099 --> 00:52:47,040
与音乐或数学相比

1557
00:52:47,040 --> 00:52:48,720
是的，那些语用

1558
00:52:48,720 --> 00:52:50,040
限制

1559
00:52:50,040 --> 00:52:51,480
嗯是限制

1560
00:52:51,480 --> 00:52:53,400
语言形式的东西，或者语言是

1561
00:52:53,400 --> 00:52:54,660
交际的，它可能

1562
00:52:54,660 --> 00:52:56,760
比音乐更具交际性，

1563
00:52:56,760 --> 00:52:58,440
例如，这可能会限制

1564
00:52:58,440 --> 00:53:01,140
事物的形式，所以我的意思是，正如你所知，

1565
00:53:01,140 --> 00:53:02,700
这是非常

1566
00:53:02,700 --> 00:53:05,520
语言学中关于

1567
00:53:05,520 --> 00:53:07,140
自然语言的属性

1568
00:53:07,140 --> 00:53:09,059
从何而来的古老争论，嗯，

1569
00:53:09,059 --> 00:53:11,160
嗯，我想我想说的

1570
00:53:11,160 --> 00:53:12,660
是，有一种观点，

1571
00:53:12,660 --> 00:53:15,119
呃，你可以看待人类可以看到的所有事物

1572
00:53:15,119 --> 00:53:16,980
即使在语言之外，

1573
00:53:16,980 --> 00:53:18,599
所有丰富的结构、

1574
00:53:18,599 --> 00:53:20,819
算法和过程都能够

1575
00:53:20,819 --> 00:53:23,760
了解和内化，

1576
00:53:23,760 --> 00:53:25,559
你说好吧，也许语言就是这样，

1577
00:53:25,559 --> 00:53:27,420
然后是的，语言也有一些

1578
00:53:27,420 --> 00:53:29,700
其他有趣的小属性，

1579
00:53:29,700 --> 00:53:31,380
嗯，但你知道也许那些 来自

1580
00:53:31,380 --> 00:53:34,020
其他一些

1581
00:53:34,020 --> 00:53:36,720
语言来自哪里的其他部分，嗯，

1582
00:53:36,720 --> 00:53:38,400
你知道我们有非常复杂的语用

1583
00:53:38,400 --> 00:53:40,800
推理，

1584
00:53:40,800 --> 00:53:42,720
嗯，我们正在使用它来实现某些

1585
00:53:42,720 --> 00:53:45,359
交流目的，你可以

1586
00:53:45,359 --> 00:53:47,160


1587
00:53:47,160 --> 00:53:49,559
在 语言系统本身

1588
00:53:49,559 --> 00:53:51,180
等等，所以也许这些其他

1589
00:53:51,180 --> 00:53:53,819
属性中的一些是具有

1590
00:53:53,819 --> 00:53:55,619
其他来源的属性，嗯，

1591
00:53:55,619 --> 00:53:57,059
我认为那个观点可能是

1592
00:53:57,059 --> 00:53:59,579
错误的，但它是一个，

1593
00:53:59,579 --> 00:54:01,800
嗯，我认为需要看看

1594
00:54:01,800 --> 00:54:04,200
它是否是错误的，就像 我认为这已经被

1595
00:54:04,200 --> 00:54:05,400


1596
00:54:05,400 --> 00:54:10,020
很多

1597
00:54:10,020 --> 00:54:12,900
语言学家忽视了，对吧，你

1598
00:54:12,900 --> 00:54:14,579
知道，我听说人们说，哦，良好的

1599
00:54:14,579 --> 00:54:15,720
沟通并不能真正

1600
00:54:15,720 --> 00:54:17,760
解释任何关于语言的正确性

1601
00:54:17,760 --> 00:54:20,040
，他们的意思通常是它不能。 不能

1602
00:54:20,040 --> 00:54:22,200
像特定的岛屿

1603
00:54:22,200 --> 00:54:23,760
限制或他们正在

1604
00:54:23,760 --> 00:54:25,319
努力解决的问题那样解释，但

1605
00:54:25,319 --> 00:54:26,700


1606
00:54:26,700 --> 00:54:28,319
语言中有各种各样的其他事情，交际压力

1607
00:54:28,319 --> 00:54:30,359
可能会解释，

1608
00:54:30,359 --> 00:54:31,500
嗯，

1609
00:54:31,500 --> 00:54:33,540
嗯，我想我的推销总是

1610
00:54:33,540 --> 00:54:36,359
为了某种广度 在术语广度上，

1611
00:54:36,359 --> 00:54:39,119
考虑到

1612
00:54:39,119 --> 00:54:41,460
可以塑造语言的力量，而不需要将

1613
00:54:41,460 --> 00:54:43,680
其全部置于某种形式的先天

1614
00:54:43,680 --> 00:54:45,359
约束或类似的东西中，不

1615
00:54:45,359 --> 00:54:46,680
完全，我认为我认为很多

1616
00:54:46,680 --> 00:54:48,420
东西都与

1617
00:54:48,420 --> 00:54:49,980
它们兼容 illness program

1618
00:54:49,980 --> 00:54:51,960
因为这个程序的中间部分希望

1619
00:54:51,960 --> 00:54:53,339
语法最少 它不希望它

1620
00:54:53,339 --> 00:54:54,660
复杂 它不希望它变得

1621
00:54:54,660 --> 00:54:56,220
更复杂 它必须如此

1622
00:54:56,220 --> 00:54:57,960
所以你提到了一些

1623
00:54:57,960 --> 00:54:59,460
Curious 属性 所以

1624
00:54:59,460 --> 00:55:00,480


1625
00:55:00,480 --> 00:55:02,579
在任何语言模型中都需要考虑一些属性，

1626
00:55:02,579 --> 00:55:04,200
呃，我会给你一个关于

1627
00:55:04,200 --> 00:55:05,400
人物特征设置的例子，

1628
00:55:05,400 --> 00:55:06,599


1629
00:55:06,599 --> 00:55:08,880
这些人物特征表现出非常

1630
00:55:08,880 --> 00:55:10,440
重要的不同概括，这些概括

1631
00:55:10,440 --> 00:55:12,480
似乎并不 考虑到通过

1632
00:55:12,480 --> 00:55:14,220
领域一般学习机制，所以我

1633
00:55:14,220 --> 00:55:16,020
坐在这里是玛丽皇后学院丹尼尔哈珀的工作，

1634
00:55:16,020 --> 00:55:17,700
例如

1635
00:55:17,700 --> 00:55:19,740
人的形态构成，它

1636
00:55:19,740 --> 00:55:21,720
与数字的相互作用，它

1637
00:55:21,720 --> 00:55:24,059
与空间的联系，呃，它的语义属性

1638
00:55:24,059 --> 00:55:26,040
和线性化，它们似乎

1639
00:55:26,040 --> 00:55:27,240
都是 成为我们

1640
00:55:27,240 --> 00:55:28,619
语言知识的有力候选人，这就是我们

1641
00:55:28,619 --> 00:55:30,359
所说的语言知识，但

1642
00:55:30,359 --> 00:55:32,280
另一方面，我们有格子、

1643
00:55:32,280 --> 00:55:34,319
协议和头部运动等东西，这些

1644
00:55:34,319 --> 00:55:36,420
都是结构现象，但

1645
00:55:36,420 --> 00:55:39,319
它们似乎抵制纯粹

1646
00:55:39,319 --> 00:55:42,420
基于意义的解释，呃

1647
00:55:42,420 --> 00:55:44,339
理论语言学是对的，

1648
00:55:44,339 --> 00:55:45,839
如果句法只是一个

1649
00:55:45,839 --> 00:55:47,520
构建结构化意义的计算引擎，那就太好了，

1650
00:55:47,520 --> 00:55:49,440
这就是

1651
00:55:49,440 --> 00:55:51,240
极简主义程序的目标，但这

1652
00:55:51,240 --> 00:55:52,800
不是我们实际发现的，在

1653
00:55:52,800 --> 00:55:54,720
任何实际的极简主义中都没有，比如具体

1654
00:55:54,720 --> 00:55:57,240
模型，任何具体的矿物理论，

1655
00:55:57,240 --> 00:55:59,040
目标只是 就像这个程序是

1656
00:55:59,040 --> 00:56:01,260
语言是完美的好吧，这个

1657
00:56:01,260 --> 00:56:03,119
程序是我们发现的东西

1658
00:56:03,119 --> 00:56:05,160
显然不好没有语言学家

1659
00:56:05,160 --> 00:56:07,680
真的相信这一点，所以

1660
00:56:07,680 --> 00:56:09,900
如果语法是那样的话会很棒但是我

1661
00:56:09,900 --> 00:56:11,280
想你知道这个程序是为了

1662
00:56:11,280 --> 00:56:13,740
寻找完美但是 并不总能找到它，

1663
00:56:13,740 --> 00:56:15,839
所以协议和头部运动的形态

1664
00:56:15,839 --> 00:56:17,640
更多地是为了语音

1665
00:56:17,640 --> 00:56:19,319
现象

1666
00:56:19,319 --> 00:56:20,700
性能系统的属性，也就是所谓的

1667
00:56:20,700 --> 00:56:22,440
性能系统，所以

1668
00:56:22,440 --> 00:56:23,760
极简主义程序本身确实

1669
00:56:23,760 --> 00:56:24,839
与你

1670
00:56:24,839 --> 00:56:26,880
所说的很多你知道的语言兼容

1671
00:56:26,880 --> 00:56:28,500
语言的某些方面可以

1672
00:56:28,500 --> 00:56:31,200
um 完善和优化以提高

1673
00:56:31,200 --> 00:56:32,700
交际效率，

1674
00:56:32,700 --> 00:56:35,520
这绝对完全毫无疑问，但

1675
00:56:35,520 --> 00:56:38,160
效率的核心在哪里是

1676
00:56:38,160 --> 00:56:39,960
语法本身还是某种

1677
00:56:39,960 --> 00:56:42,000
额外的语言系统它是在语用

1678
00:56:42,000 --> 00:56:43,680
学中你知道的是 它在感觉

1679
00:56:43,680 --> 00:56:45,540
运动中 它在演讲中吗

1680
00:56:45,540 --> 00:56:47,640
嗯可能是演讲和音韵学

1681
00:56:47,640 --> 00:56:50,400
你知道我的意思是谁知道但我

1682
00:56:50,400 --> 00:56:52,740
认为很多这些东西需要

1683
00:56:52,740 --> 00:56:56,059
更多你知道认真考虑

1684
00:56:56,059 --> 00:56:57,839
老式的概念，比如结构

1685
00:56:57,839 --> 00:56:59,700
依赖性 构图主题 你有什么

1686
00:56:59,700 --> 00:57:01,140
诸如此类的东西，您也许可以

1687
00:57:01,140 --> 00:57:03,720
在文献中的某个地方找到，但是嗯，即使只是

1688
00:57:03,720 --> 00:57:06,720
像您所知道的基本主题，

1689
00:57:06,720 --> 00:57:08,640
量词提高了扩展的

1690
00:57:08,640 --> 00:57:09,900
预测，以及副词，

1691
00:57:09,900 --> 00:57:12,059
例如状语层次结构，

1692
00:57:12,059 --> 00:57:13,920
极简主义程序中的所有这些东西都

1693
00:57:13,920 --> 00:57:16,619
可以是额外的语言权利，

1694
00:57:16,619 --> 00:57:18,180
它们实际上可以在 句法

1695
00:57:18,180 --> 00:57:20,579
和查询 语义的非常奇怪的属性

1696
00:57:20,579 --> 00:57:23,099
呃概念系统

1697
00:57:23,099 --> 00:57:24,559
本身就是一种领域

1698
00:57:24,559 --> 00:57:27,420
古代初级认知的一般奇怪遗留物

1699
00:57:27,420 --> 00:57:29,220
正确的

1700
00:57:29,220 --> 00:57:31,260
我们传递事件的方式的特征 我们传递你知道

1701
00:57:31,260 --> 00:57:32,520
代理人和病人的方式 这样的事情

1702
00:57:32,520 --> 00:57:33,960
绝对不是 这不是人类

1703
00:57:33,960 --> 00:57:35,520
特有的

1704
00:57:35,520 --> 00:57:37,260
嗯，但你知道语法

1705
00:57:37,260 --> 00:57:39,059
为这些系统提供指令的方式

1706
00:57:39,059 --> 00:57:42,480
你知道正确的似乎是这样你

1707
00:57:42,480 --> 00:57:43,800
知道生成语言学家也有不同的

1708
00:57:43,800 --> 00:57:45,839
语言生成理论

1709
00:57:45,839 --> 00:57:47,040
我将只

1710
00:57:47,040 --> 00:57:49,380
根据我们是否存储引理来谈论语言生成

1711
00:57:49,380 --> 00:57:51,240
或者我们构建单词的方式是否与

1712
00:57:51,240 --> 00:57:52,619
我们构建短语和句子的方式完全相同，所以我

1713
00:57:52,619 --> 00:57:53,819
知道你区分

1714
00:57:53,819 --> 00:57:55,440
了构造语法和

1715
00:57:55,440 --> 00:57:57,180
生成语法，你知道

1716
00:57:57,180 --> 00:57:58,680
它们对记忆限制的重视程度，

1717
00:57:58,680 --> 00:58:00,119
而只是

1718
00:58:00,119 --> 00:58:01,559
从

1719
00:58:01,559 --> 00:58:04,440
自下而上，所以你知道在一些

1720
00:58:04,440 --> 00:58:06,480
生成句法结构的启发模型机制中，在

1721
00:58:06,480 --> 00:58:08,460


1722
00:58:08,460 --> 00:58:10,200


1723
00:58:10,200 --> 00:58:12,200
单词级别之上或之下应用的过程之间没有区别

1724
00:58:12,200 --> 00:58:14,640
没有指针，这意味着语法

1725
00:58:14,640 --> 00:58:16,500
和形式都存储在一起一个

1726
00:58:16,500 --> 00:58:18,660
单一的原子

1727
00:58:18,660 --> 00:58:20,099
词汇访问中的每个阶段都是

1728
00:58:20,099 --> 00:58:21,960
不同类型数据

1729
00:58:21,960 --> 00:58:23,760
结构之间的转换，这意味着有

1730
00:58:23,760 --> 00:58:25,740
形式和语法这三个

1731
00:58:25,740 --> 00:58:27,480
特征混合在一起并且

1732
00:58:27,480 --> 00:58:28,920
它们并不总是重叠不同的

1733
00:58:28,920 --> 00:58:31,440
语言以不同的方式实现它们

1734
00:58:31,440 --> 00:58:34,800
所以你知道 一个奇怪的

1735
00:58:34,800 --> 00:58:36,839
词的基本定义就是这个奇怪的

1736
00:58:36,839 --> 00:58:39,720
多系统定义，其中很多

1737
00:58:39,720 --> 00:58:41,040
东西很多不同的认知

1738
00:58:41,040 --> 00:58:42,900
系统丰富了每个

1739
00:58:42,900 --> 00:58:44,940
电子产品的基础，你有

1740
00:58:44,940 --> 00:58:46,680
嗯没有什么比这更

1741
00:58:46,680 --> 00:58:48,299
丰富的过程了

1742
00:58:48,299 --> 00:58:50,099
嗯在其他任何地方 语言学理论是

1743
00:58:50,099 --> 00:58:52,200
正确的，或者至少在 llms 正在做的事情中，

1744
00:58:52,200 --> 00:58:53,220


1745
00:58:53,220 --> 00:58:55,859
所以我猜我猜我会问

1746
00:58:55,859 --> 00:58:58,799
你你对单词 right 的定义是什么，

1747
00:58:58,799 --> 00:59:01,559
以及 llms 能真正提供

1748
00:59:01,559 --> 00:59:03,780
对单词 Hood right 的见解，因为如果

1749
00:59:03,780 --> 00:59:04,680
你有点如果你 没有

1750
00:59:04,680 --> 00:59:06,720
一个词是什么的目的地那么

1751
00:59:06,720 --> 00:59:07,920
你真的有麻烦了就像我们

1752
00:59:07,920 --> 00:59:10,319
必须至少使用 LMS 或人工

1753
00:59:10,319 --> 00:59:12,780
系统来告知我们一个词的意思

1754
00:59:12,780 --> 00:59:14,400
或者我们可能不再需要它了我是

1755
00:59:14,400 --> 00:59:16,859
不知道你怎么想 我不是

1756
00:59:16,859 --> 00:59:18,599


1757
00:59:18,599 --> 00:59:20,339


1758
00:59:20,339 --> 00:59:24,059


1759
00:59:24,059 --> 00:59:25,740


1760
00:59:25,740 --> 00:59:27,359


1761
00:59:27,359 --> 00:59:29,819
term word right what like

1762
00:59:29,819 --> 00:59:31,920
I mean you could use you know lemmas or

1763
00:59:31,920 --> 00:59:34,440
word firms or or anything like that

1764
00:59:34,440 --> 00:59:35,819
just feels like a conventional Choice

1765
00:59:35,819 --> 00:59:38,040
我不确定它是什么，

1766
00:59:38,040 --> 00:59:39,180
那里有什么利害关系

1767
00:59:39,180 --> 00:59:41,880
所以你会怎么想我会 说我

1768
00:59:41,880 --> 00:59:43,920
同意这个词是一种约定俗成你

1769
00:59:43,920 --> 00:59:45,599
知道图标不是直观的概念

1770
00:59:45,599 --> 00:59:47,819
它经常受到正字法的影响

1771
00:59:47,819 --> 00:59:50,760
我们把空间放在正确的方式

1772
00:59:50,760 --> 00:59:52,559
所以我同意批评你

1773
00:59:52,559 --> 00:59:54,240
知道直觉意义上的词并不是

1774
00:59:54,240 --> 00:59:56,339
真正的科学 然而，我

1775
00:59:56,339 --> 00:59:58,260
想让我重新表述我的问题，嗯，

1776
00:59:58,260 --> 01:00:00,359
你知道如何将

1777
01:00:00,359 --> 01:00:02,160
单词的直观概念分解成

1778
01:00:02,160 --> 01:00:03,660
你知道的更

1779
01:00:03,660 --> 01:00:04,740
科学或

1780
01:00:04,740 --> 01:00:06,540
心理上合理的东西，这正是

1781
01:00:06,540 --> 01:00:08,280
几何初级试图

1782
01:00:08,280 --> 01:00:10,559
通过将单词分解成你知道的来做的

1783
01:00:10,559 --> 01:00:12,599
独特的特征呃形态类别概念根

1784
01:00:12,599 --> 01:00:15,180


1785
01:00:15,180 --> 01:00:17,579
与分类特征合并你知道你

1786
01:00:17,579 --> 01:00:20,700
得到一个你知道的概念并且你已经

1787
01:00:20,700 --> 01:00:22,440
用名词或类别来获得

1788
01:00:22,440 --> 01:00:24,119
名词或事件这些不同的模型

1789
01:00:24,119 --> 01:00:26,220
做出不同的预测是的我的

1790
01:00:26,220 --> 01:00:28,859
意思是我认为 这个一般想法

1791
01:00:28,859 --> 01:00:30,839
可能适用于大型语言

1792
01:00:30,839 --> 01:00:32,880
模型，比如我认为它们必须

1793
01:00:32,880 --> 01:00:34,380
具有类似于

1794
01:00:34,380 --> 01:00:36,660
词性类别的东西，例如

1795
01:00:36,660 --> 01:00:38,819
嗯，我认为它们

1796
01:00:38,819 --> 01:00:42,900
必须能够更新它们的

1797
01:00:42,900 --> 01:00:45,240
类别 基于

1798
01:00:45,240 --> 01:00:48,119
他们到目前为止所看到的正确语言，就像

1799
01:00:48,119 --> 01:00:50,520
你知道 GPT 将名词和动词放在

1800
01:00:50,520 --> 01:00:53,400
正确的位置一样，要做到这一点，你

1801
01:00:53,400 --> 01:00:55,319
需要一些名词

1802
01:00:55,319 --> 01:00:57,000
与动词的表示，你需要一些

1803
01:00:57,000 --> 01:01:00,839
能力，嗯 uh 将自己定位在

1804
01:01:00,839 --> 01:01:02,579
一串其他单词中并弄清楚下一个是否

1805
01:01:02,579 --> 01:01:04,680
可能是名词或动词

1806
01:01:04,680 --> 01:01:05,400


1807
01:01:05,400 --> 01:01:07,559
um 所以我认为在那个

1808
01:01:07,559 --> 01:01:09,319
级别上 uh 的那些词的属性

1809
01:01:09,319 --> 01:01:12,480
很可能是正确的并且

1810
01:01:12,480 --> 01:01:15,780
那里 还有一些东西

1811
01:01:15,780 --> 01:01:17,640
很可能在

1812
01:01:17,640 --> 01:01:19,619
这些模型的内部表示中被发现，

1813
01:01:19,619 --> 01:01:21,420
我看不出它怎么可能是

1814
01:01:21,420 --> 01:01:24,180
其他方式，呃，除了那个，

1815
01:01:24,180 --> 01:01:26,339
嗯，但据我所知，

1816
01:01:26,339 --> 01:01:29,579
那是 不是那个呃呃那不是

1817
01:01:29,579 --> 01:01:31,500
主要辩论或

1818
01:01:31,500 --> 01:01:35,220
分歧我认为是正确的就像

1819
01:01:35,220 --> 01:01:38,280
嗯嗯是的我认为所有

1820
01:01:38,280 --> 01:01:40,079
语言理论都必须说

1821
01:01:40,079 --> 01:01:41,400
有不同种类的词

1822
01:01:41,400 --> 01:01:43,140
可以出现在不同的地方或

1823
01:01:43,140 --> 01:01:44,339
类似的东西

1824
01:01:44,339 --> 01:01:45,240
嗯，

1825
01:01:45,240 --> 01:01:47,160
好吧，那么你知道你提到沟通的问题怎么样，

1826
01:01:47,160 --> 01:01:49,020


1827
01:01:49,020 --> 01:01:51,299
嗯，所以你知道，

1828
01:01:51,299 --> 01:01:53,099
当特朗普看到说语言

1829
01:01:53,099 --> 01:01:54,960
是一种思想系统或者你知道语言

1830
01:01:54,960 --> 01:01:57,780
没有进化时，你是完全正确的他

1831
01:01:57,780 --> 01:01:58,799
有点 厚颜无耻，他并不是真的意味着

1832
01:01:58,799 --> 01:02:00,180
他在某种非常具体的意义上意味着对，

1833
01:02:00,180 --> 01:02:01,619


1834
01:02:01,619 --> 01:02:03,540
嗯，但是你知道当我们说语言是

1835
01:02:03,540 --> 01:02:05,640
一种思想系统时，我们的意思是，

1836
01:02:05,640 --> 01:02:06,780
嗯，我们正试图让它成为一种

1837
01:02:06,780 --> 01:02:08,339
建筑主张，所以如果你看

1838
01:02:08,339 --> 01:02:09,599
极简程序的架构 句法

1839
01:02:09,599 --> 01:02:10,380


1840
01:02:10,380 --> 01:02:12,240
推导和

1841
01:02:12,240 --> 01:02:13,740
概念系统实际上是

1842
01:02:13,740 --> 01:02:15,960
不同的系统 概念

1843
01:02:15,960 --> 01:02:18,180
系统从语法中获取东西，然后

1844
01:02:18,180 --> 01:02:19,799
用它来处理自己的业务，CI

1845
01:02:19,799 --> 01:02:21,900
系统有自己独特的规则

1846
01:02:21,900 --> 01:02:23,700
和原则，这就是我

1847
01:02:23,700 --> 01:02:25,319
认为的原因 在语言中都是相似的

1848
01:02:25,319 --> 01:02:27,000
符号组合系统，但在

1849
01:02:27,000 --> 01:02:29,160
不同的方式中，只有思想的一个子集

1850
01:02:29,160 --> 01:02:32,339
被恰当地称为 CI 接口

1851
01:02:32,339 --> 01:02:34,619
系统，因为 CI 系统根据

1852
01:02:34,619 --> 01:02:36,480
定义你知道

1853
01:02:36,480 --> 01:02:38,819
人类拥有的任何无法访问和

1854
01:02:38,819 --> 01:02:40,859
读出语法指令的概念系统 我们

1855
01:02:40,859 --> 01:02:42,540
不完全知道它们是

1856
01:02:42,540 --> 01:02:43,920
什么它们似乎与

1857
01:02:43,920 --> 01:02:45,540
语法指称和确定性事件有关

1858
01:02:45,540 --> 01:02:47,099
它们似乎是

1859
01:02:47,099 --> 01:02:48,359
您所知道的语言在

1860
01:02:48,359 --> 01:02:49,440
概念上关心的主要类别

1861
01:02:49,440 --> 01:02:50,940
但我们真的不知道那只是

1862
01:02:50,940 --> 01:02:52,859
一种 假设是对的，

1863
01:02:52,859 --> 01:02:53,940
但我们所知道的是，他们

1864
01:02:53,940 --> 01:02:56,220
似乎并没有那么

1865
01:02:56,220 --> 01:02:58,440
多地使用颜色，或者嗯，所以没有语言在

1866
01:02:58,440 --> 01:03:00,240
形态上标记你知道颜色的深浅

1867
01:03:00,240 --> 01:03:01,140


1868
01:03:01,140 --> 01:03:03,900
嗯或其他概念特征，比如

1869
01:03:03,900 --> 01:03:06,359
嗯嗯担心或关注就像没有语言

1870
01:03:06,359 --> 01:03:07,980
形态上的 标志着对某个问题的某种程度的担忧

1871
01:03:07,980 --> 01:03:10,079
或关注，但我们确实

1872
01:03:10,079 --> 01:03:11,960
使用了诸如

1873
01:03:11,960 --> 01:03:13,500


1874
01:03:13,500 --> 01:03:15,599
证据性之类的认识论概念，所以

1875
01:03:15,599 --> 01:03:17,640
你很了解一个我想我

1876
01:03:17,640 --> 01:03:19,500
说的是极简主义程序在

1877
01:03:19,500 --> 01:03:21,720
试图弄清楚 找出

1878
01:03:21,720 --> 01:03:23,400
思想语言的哪些方面与思想语言

1879
01:03:23,400 --> 01:03:25,619
密切相关，哪些方面与

1880
01:03:25,619 --> 01:03:27,900
思想语言无关，因此中西部

1881
01:03:27,900 --> 01:03:29,280
计划使我们能够

1882
01:03:29,280 --> 01:03:31,680
相当巧妙地划分它，这是一个

1883
01:03:31,680 --> 01:03:33,180
比

1884
01:03:33,180 --> 01:03:34,559
乔姆斯基说语言思想时所知道的要微妙得多的框架

1885
01:03:34,559 --> 01:03:36,960
又一次，他不是，也许他的意思是，也许

1886
01:03:36,960 --> 01:03:38,339
他不是，但这不是

1887
01:03:38,339 --> 01:03:40,319
他理论的实际架构所说的，

1888
01:03:40,319 --> 01:03:42,480
这是一种修辞手段，如果你看一下实际的理论，

1889
01:03:42,480 --> 01:03:43,920
你就会知道它非常有用和有趣，可以

1890
01:03:43,920 --> 01:03:46,559
吸引本科生听众

1891
01:03:46,559 --> 01:03:48,839


1892
01:03:48,839 --> 01:03:50,700
从 The Mentalist 计划中走出来 没有

1893
01:03:50,700 --> 01:03:52,380
人真正相信语言等于

1894
01:03:52,380 --> 01:03:53,760
思想正确 语言系统似乎尽

1895
01:03:53,760 --> 01:03:55,920
最大努力访问、

1896
01:03:55,920 --> 01:03:57,599
重新格式化和操纵各种

1897
01:03:57,599 --> 01:03:59,220
概念系统，但它有其局限性

1898
01:03:59,220 --> 01:04:01,440
我们知道系统拼写什么 关键

1899
01:04:01,440 --> 01:04:02,819
核心知识系统已连接

1900
01:04:02,819 --> 01:04:05,160
关于语法引擎，

1901
01:04:05,160 --> 01:04:07,260
哪些不是，

1902
01:04:07,260 --> 01:04:09,480
嗯，所以你知道这回到了这样的

1903
01:04:09,480 --> 01:04:11,579
想法，即概念的词法化

1904
01:04:11,579 --> 01:04:14,099
似乎可能以某种方式改变它，它在某种程度上将

1905
01:04:14,099 --> 01:04:16,200
它注入了

1906
01:04:16,200 --> 01:04:17,819
不存在的元素 概念本身，所以如果

1907
01:04:17,819 --> 01:04:19,319
你讲的是一个概念，你会突然

1908
01:04:19,319 --> 01:04:21,240
改变它一点，给它

1909
01:04:21,240 --> 01:04:22,680
一点额外的东西，

1910
01:04:22,680 --> 01:04:24,299
在它上面撒一些东西，这似乎

1911
01:04:24,299 --> 01:04:26,700
因不同的纳米类型而异，但这些

1912
01:04:26,700 --> 01:04:29,160
都是几何结构中非常清晰的建筑

1913
01:04:29,160 --> 01:04:31,619
主张 语法可以

1914
01:04:31,619 --> 01:04:34,980
做出非常明确的经验预测所以

1915
01:04:34,980 --> 01:04:36,420
换句话说，我想我想说的

1916
01:04:36,420 --> 01:04:37,140
是

1917
01:04:37,140 --> 01:04:38,940
所有这些神经心理学研究都

1918
01:04:38,940 --> 01:04:42,119
被煽动了你知道在很多工作中

1919
01:04:42,119 --> 01:04:43,980
嗯在这方面它真正表明了什么

1920
01:04:43,980 --> 01:04:45,420
我认为它表明你知道 当

1921
01:04:45,420 --> 01:04:47,819
语言在大脑中受损时，它会

1922
01:04:47,819 --> 01:04:50,040
失去这种影响这些系统的特殊影响或模式，

1923
01:04:50,040 --> 01:04:51,960
但

1924
01:04:51,960 --> 01:04:54,299
Gengram Enterprise 内部没有真正的预测，

1925
01:04:54,299 --> 01:04:56,040
但那些非语言

1926
01:04:56,040 --> 01:04:57,540
系统应该会受损，或者

1927
01:04:57,540 --> 01:04:59,579
如果核心语言系统突然关闭，你会知道

1928
01:04:59,579 --> 01:05:01,200


1929
01:05:01,200 --> 01:05:02,760
嗯，事实上，如果

1930
01:05:02,760 --> 01:05:05,099
只是强调

1931
01:05:05,099 --> 01:05:07,859
句法

1932
01:05:07,859 --> 01:05:09,900
系统和非语言系统之间的主要分离是对的，

1933
01:05:09,900 --> 01:05:11,880
那么我认为这里的很多预测

1934
01:05:11,880 --> 01:05:14,339
来自语言和交流，嗯，

1935
01:05:14,339 --> 01:05:16,020
你知道文学有点遗漏了

1936
01:05:16,020 --> 01:05:19,380
重点 架构声明，

1937
01:05:19,380 --> 01:05:21,540
嗯，我可以给，或者 Daniel 你

1938
01:05:21,540 --> 01:05:24,180
想去吗，嗯，给点

1939
01:05:24,180 --> 01:05:25,799
背景知识，所以有这些

1940
01:05:25,799 --> 01:05:26,760
论文，嗯，嗯，

1941
01:05:26,760 --> 01:05:30,119
来自 EV federenko 和

1942
01:05:30,119 --> 01:05:32,880
Rosemary Varley，正在

1943
01:05:32,880 --> 01:05:34,500
检查

1944
01:05:34,500 --> 01:05:37,799
嗯，嗯，嗯，部分 他们是失语症患者

1945
01:05:37,799 --> 01:05:40,200
所以所以语言能力受损的人

1946
01:05:40,200 --> 01:05:41,880


1947
01:05:41,880 --> 01:05:43,140


1948
01:05:43,140 --> 01:05:45,000
嗯嗯基本上表明

1949
01:05:45,000 --> 01:05:46,980
语言能力受损你你

1950
01:05:46,980 --> 01:05:49,020
嗯可以仍然有

1951
01:05:49,020 --> 01:05:50,579
嗯嗯保留某种推理

1952
01:05:50,579 --> 01:05:52,859
能力所以像国际象棋大师

1953
01:05:52,859 --> 01:05:55,020
国际象棋大师这样的人

1954
01:05:55,020 --> 01:05:58,140
显然非常好 在推理方面，

1955
01:05:58,140 --> 01:06:00,780
嗯嗯嗯可能没有

1956
01:06:00,780 --> 01:06:02,640
完整的语言能力

1957
01:06:02,640 --> 01:06:04,079
嗯然后补充那种

1958
01:06:04,079 --> 01:06:06,119
耐心的工作还有

1959
01:06:06,119 --> 01:06:08,520
来自 ebb 实验室的呃工作表明

1960
01:06:08,520 --> 01:06:11,220
嗯嗯呃大脑中

1961
01:06:11,220 --> 01:06:13,859
关心呃语言嗯嗯的部分

1962
01:06:13,859 --> 01:06:14,640


1963
01:06:14,640 --> 01:06:16,319
与大脑中关心其他领域的部分是分开的，

1964
01:06:16,319 --> 01:06:17,700


1965
01:06:17,700 --> 01:06:19,619
即使是那些具有相同类型

1966
01:06:19,619 --> 01:06:21,599
语言的领域，比如音乐

1967
01:06:21,599 --> 01:06:23,819
和数学之类的东西，嗯，嗯，

1968
01:06:23,819 --> 01:06:25,859
往往不会发生在

1969
01:06:25,859 --> 01:06:27,240
语言领域，

1970
01:06:27,240 --> 01:06:29,400
所以 EV 和其他人都有 争辩说，

1971
01:06:29,400 --> 01:06:30,539


1972
01:06:30,539 --> 01:06:33,539
嗯，这基本上是反对

1973
01:06:33,539 --> 01:06:36,660
乔姆斯基的证据，并声称，嗯，

1974
01:06:36,660 --> 01:06:38,579
语言是正确思考的媒介，

1975
01:06:38,579 --> 01:06:40,440
因为

1976
01:06:40,440 --> 01:06:42,420
在没有语言的情况下可能会发生思考，而

1977
01:06:42,420 --> 01:06:44,160
关心语言的大脑区域

1978
01:06:44,160 --> 01:06:46,020
似乎不是他们认为的大脑区域

1979
01:06:46,020 --> 01:06:48,359
关心关心思考

1980
01:06:48,359 --> 01:06:50,099
嗯我猜艾略特你是说

1981
01:06:50,099 --> 01:06:51,780
人们并不真的相信那个

1982
01:06:51,780 --> 01:06:53,039


1983
01:06:53,039 --> 01:06:56,220
他们不相信那种

1984
01:06:56,220 --> 01:06:57,720
区别我的意思是

1985
01:06:57,720 --> 01:06:59,880
嗯

1986
01:06:59,880 --> 01:07:01,740
不就是这样而且还有很多

1987
01:07:01,740 --> 01:07:03,480
即使在这些论点中也有类似的自相矛盾

1988
01:07:03,480 --> 01:07:04,799
所以在你的

1989
01:07:04,799 --> 01:07:06,839
论文中你有时会说乔姆斯基

1990
01:07:06,839 --> 01:07:08,220
认为语言是一个思想系统

1991
01:07:08,220 --> 01:07:10,319
但是几页之后你会说

1992
01:07:10,319 --> 01:07:12,480
乔姆斯基也认为语法是

1993
01:07:12,480 --> 01:07:13,799
一个与

1994
01:07:13,799 --> 01:07:15,480
其他任何东西完全不同的系统 对，你语法的自主权

1995
01:07:15,480 --> 01:07:18,900
等等，这是乔姆斯基的事，这

1996
01:07:18,900 --> 01:07:20,460
不是我的矛盾

1997
01:07:20,460 --> 01:07:22,140


1998
01:07:22,140 --> 01:07:24,780


1999
01:07:24,780 --> 01:07:26,280


2000
01:07:26,280 --> 01:07:28,319


2001
01:07:28,319 --> 01:07:30,420
从

2002
01:07:30,420 --> 01:07:32,520
体系结构的角度来看，只是

2003
01:07:32,520 --> 01:07:34,619
说语言是一种思想系统，

2004
01:07:34,619 --> 01:07:35,579
这意味着什么，这并不意味着

2005
01:07:35,579 --> 01:07:36,780
什么，这只是一个非常模糊的

2006
01:07:36,780 --> 01:07:38,640
陈述，问题是

2007
01:07:38,640 --> 01:07:41,280
语言究竟对雷神有何贡献，以及

2008
01:07:41,280 --> 01:07:43,500
它如何没有贡献，嗯，

2009
01:07:43,500 --> 01:07:45,839
是的，我的意思是 我认为他的主张

2010
01:07:45,839 --> 01:07:48,420
主要是进化论或某些正确的东西，

2011
01:07:48,420 --> 01:07:51,180
呃这是系统的起源

2012
01:07:51,180 --> 01:07:52,859
，我认为这有点

2013
01:07:52,859 --> 01:07:55,559
同样难以与

2014
01:07:55,559 --> 01:07:57,599
嗯嗯那种病人和

2015
01:07:57,599 --> 01:07:59,520
神经影像学数据

2016
01:07:59,520 --> 01:08:00,660


2017
01:08:00,660 --> 01:08:04,079
嗯嗯嗯但你知道 如果他不这么认为，

2018
01:08:04,079 --> 01:08:07,079
那么他就不应该这么说，否则人们

2019
01:08:07,079 --> 01:08:08,579
会回应他所说的话，我认为

2020
01:08:08,579 --> 01:08:11,280
很好，不，不，因为争论是

2021
01:08:11,280 --> 01:08:13,619
语言是一种思想系统，它

2022
01:08:13,619 --> 01:08:15,420
调节思想的某些方面，

2023
01:08:15,420 --> 01:08:16,920
它会产生 思想的某些方面

2024
01:08:16,920 --> 01:08:19,020
显然是人类独有的，但在

2025
01:08:19,020 --> 01:08:21,000
本质上或因果关系上并没有与之相关

2026
01:08:21,000 --> 01:08:22,920


2027
01:08:22,920 --> 01:08:24,600
系统的架构与

2028
01:08:24,600 --> 01:08:26,399
您可以

2029
01:08:26,399 --> 01:08:28,979
从架构中修辞事件的那种概括非常不同，

2030
01:08:28,979 --> 01:08:30,960
例如当您从失语症现场工作时 正如

2031
01:08:30,960 --> 01:08:32,698


2032
01:08:32,698 --> 01:08:33,960
您刚才提到的

2033
01:08:33,960 --> 01:08:35,759
下棋等，患者在复杂推理方面没有表现出任何缺陷，我们实际上希望

2034
01:08:35,759 --> 01:08:37,380
在一种您知道的几何句法

2035
01:08:37,380 --> 01:08:39,179
的非词汇主义框架下出现这种情况，

2036
01:08:39,179 --> 01:08:41,160
其中我所说的意思是

2037
01:08:41,160 --> 01:08:43,799
句法和形式形式只是意味着

2038
01:08:43,799 --> 01:08:45,600
您可以 外部化

2039
01:08:45,600 --> 01:08:46,979
语言，所有这些东西都是

2040
01:08:46,979 --> 01:08:48,540
独立的功能和独立的系统，

2041
01:08:48,540 --> 01:08:51,000
正确的语法自主性并不

2042
01:08:51,000 --> 01:08:52,020
意味着

2043
01:08:52,020 --> 01:08:53,819
你知道很多人认为这

2044
01:08:53,819 --> 01:08:54,719
意味着它只是意味着要么

2045
01:08:54,719 --> 01:08:56,160
某些某些语法操作

2046
01:08:56,160 --> 01:08:58,439
不是语义的某些

2047
01:08:58,439 --> 01:09:00,299
东西 你可以用你

2048
01:09:00,299 --> 01:09:01,920
只能做语法而不能做

2049
01:09:01,920 --> 01:09:03,420
语义的语法来做，所以这又回到了

2050
01:09:03,420 --> 01:09:05,279
你知道呃

2051
01:09:05,279 --> 01:09:07,259
彼得罗夫斯基的理论语义是

2052
01:09:07,259 --> 01:09:10,319
公正和正确与呃允许

2053
01:09:10,319 --> 01:09:11,698
合成学家相信存在

2054
01:09:11,698 --> 01:09:13,080
某些奇怪的奇怪事物之间的区别 你可以

2055
01:09:13,080 --> 01:09:15,719
用句法的句法来做，所以

2056
01:09:15,719 --> 01:09:17,759
即使在那种架构框架内也有离婚

2057
01:09:17,759 --> 01:09:20,100
，所以

2058
01:09:20,100 --> 01:09:22,439
你也发现

2059
01:09:22,439 --> 01:09:24,359
神经心理学

2060
01:09:24,359 --> 01:09:25,979
层面的离婚并不奇怪我会说

2061
01:09:25,979 --> 01:09:28,319
我想我会

2062
01:09:28,319 --> 01:09:30,960
想要一个 语言的预测被

2063
01:09:30,960 --> 01:09:34,140
认为是进化的想法然后是正确的所以

2064
01:09:34,140 --> 01:09:36,960
就像呃如果那不是如果你说那

2065
01:09:36,960 --> 01:09:39,060
不能预测思想

2066
01:09:39,060 --> 01:09:41,160
依赖于语言

2067
01:09:41,160 --> 01:09:44,399
那么呃我认为喜欢

2068
01:09:44,399 --> 01:09:45,899
那个理论的人应该提出

2069
01:09:45,899 --> 01:09:47,520
一些预测

2070
01:09:47,520 --> 01:09:49,979
嗯，嗯，你知道那个

2071
01:09:49,979 --> 01:09:51,660
理论实际上意味着什么我的意思是我觉得

2072
01:09:51,660 --> 01:09:53,819
这些类型的预测

2073
01:09:53,819 --> 01:09:55,440
对于理解预测的内容通常是非常必要的

2074
01:09:55,440 --> 01:09:57,360


2075
01:09:57,360 --> 01:09:58,320


2076
01:09:58,320 --> 01:10:00,540
嗯嗯很抱歉丹尼尔你的手已经

2077
01:10:00,540 --> 01:10:03,660
举了一段时间呃不 一切都很好，

2078
01:10:03,660 --> 01:10:05,880
只是有点想吸

2079
01:10:05,880 --> 01:10:08,040


2080
01:10:08,040 --> 01:10:12,660
一口气，嗯，这是一个机会，让

2081
01:10:12,660 --> 01:10:15,480
任何人都可以问任何其他

2082
01:10:15,480 --> 01:10:17,520
问题，但是哇，

2083
01:10:17,520 --> 01:10:20,460
谢谢你们，我们已经讨论了很多话题，嗯，

2084
01:10:20,460 --> 01:10:21,780


2085
01:10:21,780 --> 01:10:22,500


2086
01:10:22,500 --> 01:10:25,199
我们将在最后几分钟讨论，嗯，

2087
01:10:25,199 --> 01:10:27,120
有点 结论和后续步骤 但 Dave

2088
01:10:27,120 --> 01:10:29,820
你想问一个问题还是

2089
01:10:29,820 --> 01:10:32,840
做一个简短的反思

2090
01:10:36,900 --> 01:10:38,880
好吧不嗯

2091
01:10:38,880 --> 01:10:39,960


2092
01:10:39,960 --> 01:10:42,239
聊天中有很多评论所以我

2093
01:10:42,239 --> 01:10:45,120
希望你们两个都可以在

2094
01:10:45,120 --> 01:10:47,460
自己的时间阅读它们以查看每个人都

2095
01:10:47,460 --> 01:10:49,440
添加了什么

2096
01:10:49,440 --> 01:10:52,860
当我们

2097
01:10:52,860 --> 01:10:57,179
咆哮到 2023 年 5 月时，我们会从这里走吗？超越

2098
01:10:57,179 --> 01:11:00,960
语言学家、大型语言模型

2099
01:11:00,960 --> 01:11:02,940
开发人员和用户认知

2100
01:11:02,940 --> 01:11:05,159
科学家，你们每个人认为

2101
01:11:05,159 --> 01:11:06,960
什么是最富有成效的前进道路

2102
01:11:06,960 --> 01:11:08,159


2103
01:11:08,159 --> 01:11:10,080
好吧，

2104
01:11:10,080 --> 01:11:13,260
我会说，嗯，你知道，嗯，最

2105
01:11:13,260 --> 01:11:14,699
富有成效的前进道路 是真的

2106
01:11:14,699 --> 01:11:15,600


2107
01:11:15,600 --> 01:11:17,460
像认知心理学一样对待 um 不

2108
01:11:17,460 --> 01:11:18,659
认真最近有很多不错的工作

2109
01:11:18,659 --> 01:11:21,000
试图调整事情，比如

2110
01:11:21,000 --> 01:11:24,060
是的，你知道来自 alpha 插件的 Church EBT wolf

2111
01:11:24,060 --> 01:11:25,739
聊天 gbt 可以

2112
01:11:25,739 --> 01:11:28,080
与不同类型的模块交互的方式

2113
01:11:28,080 --> 01:11:30,179
um 构建合法的方式

2114
01:11:30,179 --> 01:11:32,580
一种 AGI 系统，

2115
01:11:32,580 --> 01:11:34,380
你不一定必须知道它在心理上

2116
01:11:34,380 --> 01:11:35,880
依赖于

2117
01:11:35,880 --> 01:11:37,500
人类拥有的那种模块，但我认为它会

2118
01:11:37,500 --> 01:11:38,820
从中受益所以有人声称

2119
01:11:38,820 --> 01:11:41,040
大型语言模型

2120
01:11:41,040 --> 01:11:42,540
也许你知道吗 各种

2121
01:11:42,540 --> 01:11:43,860
事情都是正确的一切你

2122
01:11:43,860 --> 01:11:44,760
喜欢的一切

2123
01:11:44,760 --> 01:11:46,620
嗯但我认为从长远来看，最

2124
01:11:46,620 --> 01:11:47,880
有可能的情况是 llms

2125
01:11:47,880 --> 01:11:49,380
可以做一些非常重要和非常

2126
01:11:49,380 --> 01:11:51,000
有趣的事情，但它只是

2127
01:11:51,000 --> 01:11:53,100
拼图的一部分所以实际上什至是

2128
01:11:53,100 --> 01:11:55,440
开放的 AI CEO Sam Altman 上周说，

2129
01:11:55,440 --> 01:11:56,400


2130
01:11:56,400 --> 01:11:58,440
嗯，你知道我们可以用 llms 做什么

2131
01:11:58,440 --> 01:12:00,120
真的有点累了，我们需要

2132
01:12:00,120 --> 01:12:02,520
新的方向新的新途径

2133
01:12:02,520 --> 01:12:04,860
等等我想你可能

2134
01:12:04,860 --> 01:12:07,440
更了解与投资者交谈而不是

2135
01:12:07,440 --> 01:12:09,060
语言 这里的学生，但我

2136
01:12:09,060 --> 01:12:11,159
认为他也是对的，你知道 llms 可以

2137
01:12:11,159 --> 01:12:12,300
做一些惊人的事情，但它们

2138
01:12:12,300 --> 01:12:14,760
可能会构成通用 AGI 架构的一小部分，

2139
01:12:14,760 --> 01:12:17,760


2140
01:12:17,760 --> 01:12:19,679
如果你想在这里将 AGI 视为一个

2141
01:12:19,679 --> 01:12:21,900
潜在的潜在目标，

2142
01:12:21,900 --> 01:12:25,679
那么你知道 我想很多所以让

2143
01:12:25,679 --> 01:12:28,020
我在这里举另一个例子所以

2144
01:12:28,020 --> 01:12:29,820
嗯安娜甚至超过

2145
01:12:29,820 --> 01:12:31,920
嗯她是一位非常优秀的多产

2146
01:12:31,920 --> 01:12:34,140
科学家她最近有一篇论文

2147
01:12:34,140 --> 01:12:35,400
嗯争论一种

2148
01:12:35,400 --> 01:12:37,800
用于 llms 的模块化架构这是一个

2149
01:12:37,800 --> 01:12:39,060
非常好的框架它是 在

2150
01:12:39,060 --> 01:12:40,860
认知上非常合理，这正是

2151
01:12:40,860 --> 01:12:41,880
我们应该推动的事情，

2152
01:12:41,880 --> 01:12:43,679
因为它与霍华德

2153
01:12:43,679 --> 01:12:45,239
加德纳的多元智能概念兼容，等等，嗯，

2154
01:12:45,239 --> 01:12:46,800


2155
01:12:46,800 --> 01:12:48,000
但我认为同时只是

2156
01:12:48,000 --> 01:12:49,620
为了完成这个评论，

2157
01:12:49,620 --> 01:12:51,360
嗯，最后有一个技术谈话 我

2158
01:12:51,360 --> 01:12:54,840
想这周或者几天前，嗯，

2159
01:12:54,840 --> 01:12:56,880
很多其他的东西可以以

2160
01:12:56,880 --> 01:12:59,520
非生产性的方式与 AI 炒作混为一谈，所以

2161
01:12:59,520 --> 01:13:02,100
来自 openai 的 Greg Brockman 他给出了他的一个，

2162
01:13:02,100 --> 01:13:04,199
呃，其中一个 Ted Talks，他

2163
01:13:04,199 --> 01:13:06,179
展示了可以聊天 GPD 的不同插件

2164
01:13:06,179 --> 01:13:08,159
可以做我提到的 Wolfram 操作，但

2165
01:13:08,159 --> 01:13:09,300
也有图像

2166
01:13:09,300 --> 01:13:11,820
生成 instacart 购物之类的东西，在那里你

2167
01:13:11,820 --> 01:13:13,560
可以通过聊天电视给你买东西，

2168
01:13:13,560 --> 01:13:15,060
你有什么，

2169
01:13:15,060 --> 01:13:17,040
嗯，这再次让你回到

2170
01:13:17,040 --> 01:13:18,960
多个子系统可以执行

2171
01:13:18,960 --> 01:13:20,940
不同子功能的想法，所以 Brockovich

2172
01:13:20,940 --> 01:13:22,620
还展示了一个例子，给聊天

2173
01:13:22,620 --> 01:13:26,820
GPT 一个 Excel 文件一个 CSV 文件和一个

2174
01:13:26,820 --> 01:13:28,739
学术论文的存档数据库，

2175
01:13:28,739 --> 01:13:30,060
它只列出了一堆论文

2176
01:13:30,060 --> 01:13:31,739
，然后是标题，你有什么

2177
01:13:31,739 --> 01:13:32,880
权利

2178
01:13:32,880 --> 01:13:34,500
嗯，他说你知道使用

2179
01:13:34,500 --> 01:13:37,440
chatipati 它使用 World Knowledge 来

2180
01:13:37,440 --> 01:13:39,060
推断列标题的

2181
01:13:39,060 --> 01:13:40,860
含义，因此我们知道您知道

2182
01:13:40,860 --> 01:13:42,780
标题是指论文的标题，它

2183
01:13:42,780 --> 01:13:44,640
理解作者是指

2184
01:13:44,640 --> 01:13:47,280
每篇论文的作者人数，它理解

2185
01:13:47,280 --> 01:13:48,840
创建是指论文

2186
01:13:48,840 --> 01:13:50,820
正确提交的日期，因为它是 TED

2187
01:13:50,820 --> 01:13:52,080
演讲你知道所有的观众

2188
01:13:52,080 --> 01:13:54,120
都给了我们一个起立鼓掌的权利

2189
01:13:54,120 --> 01:13:56,340
嗯但是我想在 Excel 文件上描述标签的能力

2190
01:13:56,340 --> 01:13:57,900


2191
01:13:57,900 --> 01:14:01,679
很好但嗯我不确定

2192
01:14:01,679 --> 01:14:03,120
你真的称之为世界知识所以

2193
01:14:03,120 --> 01:14:04,800
我想有很多 我只想说，在

2194
01:14:04,800 --> 01:14:06,719


2195
01:14:06,719 --> 01:14:08,880
减少人类小体的同时还需要取得很多进展，

2196
01:14:08,880 --> 01:14:11,640
并且你有适当的平衡，所以

2197
01:14:11,640 --> 01:14:12,659
就像我说的，你必须有正确的

2198
01:14:12,659 --> 01:14:13,920
平衡，嗯，在

2199
01:14:13,920 --> 01:14:15,360
心理上合理的

2200
01:14:15,360 --> 01:14:17,100
模块化架构，但你不能有

2201
01:14:17,100 --> 01:14:18,480
太多 很多

2202
01:14:18,480 --> 01:14:19,920
um 拟人化 因为那样你会被

2203
01:14:19,920 --> 01:14:21,540
带走你必须发现我们

2204
01:14:21,540 --> 01:14:22,860
必须在

2205
01:14:22,860 --> 01:14:25,739
建模类人 uh 模块化

2206
01:14:25,739 --> 01:14:27,780
系统之间找到正确的平衡，但不要把它做到

2207
01:14:27,780 --> 01:14:29,280
你知道有点

2208
01:14:29,280 --> 01:14:31,140
难以置信或科学上

2209
01:14:31,140 --> 01:14:33,679
无用的程度

2210
01:14:35,040 --> 01:14:37,500
我的意思是，我认为我同意所有这些，

2211
01:14:37,500 --> 01:14:40,199
我真的很兴奋，因为这些将

2212
01:14:40,199 --> 01:14:42,739
语言模型连接到

2213
01:14:42,739 --> 01:14:45,120
其他信息

2214
01:14:45,120 --> 01:14:47,640
处理形式的方法，呃，这看起来确实像

2215
01:14:47,640 --> 01:14:49,679
人们所拥有的，我想我是说

2216
01:14:49,679 --> 01:14:52,620
我' 我对

2217
01:14:52,620 --> 01:14:54,840
他们能够做的事情感到非常惊讶，

2218
01:14:54,840 --> 01:14:58,260
就像语言建模一样，所以

2219
01:14:58,260 --> 01:15:00,120
你知道不同种类的推理

2220
01:15:00,120 --> 01:15:01,920
难题和他们可以解决的事情，

2221
01:15:01,920 --> 01:15:05,340
我认为这真的很迷人

2222
01:15:05,340 --> 01:15:07,920
而且你知道也许会 要求我们

2223
01:15:07,920 --> 01:15:09,719
重新思考我们所知道的

2224
01:15:09,719 --> 01:15:11,760
语言和思想之间的关系，

2225
01:15:11,760 --> 01:15:13,500
并尝试找出一种方法

2226
01:15:13,500 --> 01:15:15,239
来具体说明

2227
01:15:15,239 --> 01:15:17,760
某物具有

2228
01:15:17,760 --> 01:15:19,380
表征或推理

2229
01:15:19,380 --> 01:15:21,060
该表征意味着什么但最终我

2230
01:15:21,060 --> 01:15:23,820
想我同意 嗯，嗯，

2231
01:15:23,820 --> 01:15:26,219
你知道人们有不同的

2232
01:15:26,219 --> 01:15:28,560
思考方式，这对

2233
01:15:28,560 --> 01:15:32,400
智力来说似乎很重要，

2234
01:15:32,400 --> 01:15:35,040
嗯，我也对婴儿

2235
01:15:35,040 --> 01:15:37,620
LM 挑战感到非常兴奋，所以我认为在

2236
01:15:37,620 --> 01:15:39,960
语言方面是对的，

2237
01:15:39,960 --> 01:15:42,300
嗯，这正是 正确的做法是

2238
01:15:42,300 --> 01:15:45,300
看看我们能用

2239
01:15:45,300 --> 01:15:47,219
更小的数据集取得多远，嗯，也许

2240
01:15:47,219 --> 01:15:50,820
最终你会知道尝试嗯，嗯，

2241
01:15:50,820 --> 01:15:53,699
了解更多关于

2242
01:15:53,699 --> 01:15:55,739
孩子们获得的语义类型

2243
01:15:55,739 --> 01:15:57,840
以及他们从哪里获得

2244
01:15:57,840 --> 01:15:59,820
以及如何获得的语义 某种外部语义

2245
01:15:59,820 --> 01:16:02,520
可以为语言学习提供信息，或者

2246
01:16:02,520 --> 01:16:04,560
特别是语法和

2247
01:16:04,560 --> 01:16:06,600
句法学习，

2248
01:16:06,600 --> 01:16:09,480
嗯，我想我的另一个呃路径前进

2249
01:16:09,480 --> 01:16:12,179
点是

2250
01:16:12,179 --> 01:16:15,840
嗯，嗯，就像我觉得

2251
01:16:15,840 --> 01:16:18,300
这些模型已经有，嗯，嗯，

2252
01:16:18,300 --> 01:16:21,179
真的走了很远 超出人们

2253
01:16:21,179 --> 01:16:23,340
对这类模型的期望

2254
01:16:23,340 --> 01:16:25,500
正确的基础

2255
01:16:25,500 --> 01:16:27,960
统计学习 发现

2256
01:16:27,960 --> 01:16:29,940
文本中的模式

2257
01:16:29,940 --> 01:16:30,719


2258
01:16:30,719 --> 01:16:32,640
嗯 似乎给出了非常

2259
01:16:32,640 --> 01:16:35,040
非常显着的结果

2260
01:16:35,040 --> 01:16:37,020
嗯 对我来说，我

2261
01:16:37,020 --> 01:16:39,360
认为这刚刚引入了巨大的浪潮

2262
01:16:39,360 --> 01:16:42,000
理论的不确定性，所以我认为

2263
01:16:42,000 --> 01:16:43,620
我们的理论

2264
01:16:43,620 --> 01:16:46,739
基本上可以肯定是语言中的一切，

2265
01:16:46,739 --> 01:16:48,960
但认知可能是神经科学，

2266
01:16:48,960 --> 01:16:50,640
就像我认为的所有这些东西一样，

2267
01:16:50,640 --> 01:16:53,100


2268
01:16:53,100 --> 01:16:54,420
当我们真正开始

2269
01:16:54,420 --> 01:16:56,219
理解这种

2270
01:16:56,219 --> 01:16:58,560
能力时，我们将重新研究

2271
01:16:58,560 --> 01:17:02,219
像这样的真正通用的学习系统，所以

2272
01:17:02,219 --> 01:17:03,900
嗯，一方面让你知道，

2273
01:17:03,900 --> 01:17:05,820
嗯，

2274
01:17:05,820 --> 01:17:07,440
对过去的理论来说有点令人失望，

2275
01:17:07,440 --> 01:17:09,360
尤其是那些

2276
01:17:09,360 --> 01:17:11,640
依赖于

2277
01:17:11,640 --> 01:17:14,100
嗯你知道学习不能

2278
01:17:14,100 --> 01:17:15,659
很好地工作的理论

2279
01:17:15,659 --> 01:17:18,000
但从好的方面来说，我认为这

2280
01:17:18,000 --> 01:17:20,460
对

2281
01:17:20,460 --> 01:17:22,620
人工智能、认知科学和

2282
01:17:22,620 --> 01:17:24,000
语言学来说都是一个非常激动人心的时刻，嗯，

2283
01:17:24,000 --> 01:17:25,560
现在有这些

2284
01:17:25,560 --> 01:17:27,420
非常强大的工具，嗯，

2285
01:17:27,420 --> 01:17:29,719
这似乎是朝着

2286
01:17:29,719 --> 01:17:32,940
人类能力迈出的质上不同规模的一步，

2287
01:17:32,940 --> 01:17:34,620


2288
01:17:34,620 --> 01:17:36,120
嗯 我认为可以将它们整合起来，

2289
01:17:36,120 --> 01:17:39,060
并同时学习

2290
01:17:39,060 --> 01:17:41,280
工程课程和

2291
01:17:41,280 --> 01:17:43,679
哲学课程，了解它们是如何

2292
01:17:43,679 --> 01:17:45,540
制造的，以及

2293
01:17:45,540 --> 01:17:47,460
设计智能系统的原则是什么，我

2294
01:17:47,460 --> 01:17:49,679
认为那些东西会

2295
01:17:49,679 --> 01:17:52,080
将真正塑造未来

2296
01:17:52,080 --> 01:17:53,699
五到十年的感觉，

2297
01:17:53,699 --> 01:17:54,900


2298
01:17:54,900 --> 01:17:57,000
嗯，就像我只是在

2299
01:17:57,000 --> 01:17:58,500
更广泛的主题背景下说的那样，就

2300
01:17:58,500 --> 01:17:59,760
好像你是完全正确的，就像我

2301
01:17:59,760 --> 01:18:01,620
记得当我读到什么时候

2302
01:18:01,620 --> 01:18:04,920
深蓝色是呃卡斯珀一样 是国际

2303
01:18:04,920 --> 01:18:07,260
象棋吗，嗯，是的，有

2304
01:18:07,260 --> 01:18:08,940
一些评论员说你知道国际

2305
01:18:08,940 --> 01:18:12,480
象棋已经结束了，如果人工智能可以成为人类，

2306
01:18:12,480 --> 01:18:13,739
那么它就是关于

2307
01:18:13,739 --> 01:18:15,420
学习国际象棋的意义的游戏，你知道没有必要再

2308
01:18:15,420 --> 01:18:16,920
无聊了，

2309
01:18:16,920 --> 01:18:18,900
嗯，我想 如果人工智能似乎已经实现了

2310
01:18:18,900 --> 01:18:20,640
人类

2311
01:18:20,640 --> 01:18:22,140
下棋所需做的一切，那么下棋的意义是什么，

2312
01:18:22,140 --> 01:18:23,100


2313
01:18:23,100 --> 01:18:24,540
嗯，但我想你知道它是否会

2314
01:18:24,540 --> 01:18:26,219
增加国际象棋的受欢迎程度，

2315
01:18:26,219 --> 01:18:27,900
对吧，他们在我们的迷你国际象棋

2316
01:18:27,900 --> 01:18:29,580
名人以及全球

2317
01:18:29,580 --> 01:18:31,199
锦标赛中 而且我预测

2318
01:18:31,199 --> 01:18:32,520


2319
01:18:32,520 --> 01:18:34,620
语言也可能会发生同样的情况，你知道 llms 并不意味着

2320
01:18:34,620 --> 01:18:36,360
它是语言的终结，不再是

2321
01:18:36,360 --> 01:18:37,800
语言，不再是语言学，我

2322
01:18:37,800 --> 01:18:39,179
实际上会反驳说，也许它

2323
01:18:39,179 --> 01:18:40,620
会恰恰相反，嗯，

2324
01:18:40,620 --> 01:18:42,540
你知道成功的 LMS 将

2325
01:18:42,540 --> 01:18:44,400
增加对语言理论的普遍兴趣，

2326
01:18:44,400 --> 01:18:46,199
因为它们的配对你知道

2327
01:18:46,199 --> 01:18:48,060
奇怪的约束和明显的

2328
01:18:48,060 --> 01:18:50,100
限制，因为我还要

2329
01:18:50,100 --> 01:18:52,380
说你知道此时的规模，

2330
01:18:52,380 --> 01:18:55,500
压力问题的规模肯定

2331
01:18:55,500 --> 01:18:57,420
远远不够，所

2332
01:18:57,420 --> 01:18:59,820
缺乏的是 LMS 的一种能力，你知道它

2333
01:18:59,820 --> 01:19:01,260
确实抽象了他们的知识和

2334
01:19:01,260 --> 01:19:03,239
经验，以便进行群体清洗

2335
01:19:03,239 --> 01:19:04,739
预测和概括等

2336
01:19:04,739 --> 01:19:06,780
我举了一些例子，但

2337
01:19:06,780 --> 01:19:07,860
在文献中还有其他一些它

2338
01:19:07,860 --> 01:19:09,179
似乎并不擅长

2339
01:19:09,179 --> 01:19:10,739
概括 go of maybe

2340
01:19:10,739 --> 01:19:13,620
particular token types and but I would

2341
01:19:13,620 --> 01:19:15,060
you know I would you know I would guess my final 我最后的

2342
01:19:15,060 --> 01:19:17,100
主张是你知道

2343
01:19:17,100 --> 01:19:19,080
语言习得文献

2344
01:19:19,080 --> 01:19:21,480
um 不一定需要 llms 尽管

2345
01:19:21,480 --> 01:19:22,860
你知道认知科学家并不

2346
01:19:22,860 --> 01:19:26,219
真的需要 llms 我们可能 呃，

2347
01:19:26,219 --> 01:19:27,420
你知道恢复原状，但显然

2348
01:19:27,420 --> 01:19:29,640
不同意，但嗯，我想说的是，从

2349
01:19:29,640 --> 01:19:32,040
llms 中获利的大型科技公司需要

2350
01:19:32,040 --> 01:19:33,360
llms，对，他们是唯一

2351
01:19:33,360 --> 01:19:35,100


2352
01:19:35,100 --> 01:19:37,320
真正这样做的公司

2353
01:19:37,320 --> 01:19:39,000
一个非常多样化的空间可能

2354
01:19:39,000 --> 01:19:40,560
存在某些形式的行为

2355
01:19:40,560 --> 01:19:42,420
和学习可能会被

2356
01:19:42,420 --> 01:19:44,219
类似于 llms 正在做的过程所捕获，

2357
01:19:44,219 --> 01:19:45,540
所以斯蒂芬

2358
01:19:45,540 --> 01:19:47,159
在他的论文中给出了一些有趣的例子关于磁力

2359
01:19:47,159 --> 01:19:49,320
和奇怪的学习规则

2360
01:19:49,320 --> 01:19:51,060
非常普遍，非常

2361
01:19:51,060 --> 01:19:52,920
快速，非常神秘，所以你知道

2362
01:19:52,920 --> 01:19:54,780
也许对于那些事情，

2363
01:19:54,780 --> 01:19:56,100
这种学习将是

2364
01:19:56,100 --> 01:19:57,780
相关的，但我仍然认为

2365
01:19:57,780 --> 01:19:59,580
其中一个候选人不太可能

2366
01:19:59,580 --> 01:20:02,100
是自然语言，至少是

2367
01:20:02,100 --> 01:20:03,480
自然的方式 语言是有效的，它

2368
01:20:03,480 --> 01:20:05,219
在形式上非常荣耀，主要是

2369
01:20:05,219 --> 01:20:07,380
监管，你有什么，所以我想

2370
01:20:07,380 --> 01:20:09,300
你知道这让我想起了你

2371
01:20:09,300 --> 01:20:11,520
在哪里你知道你有这张

2372
01:20:11,520 --> 01:20:13,320
图片我最近看到了约翰维克第四章，

2373
01:20:13,320 --> 01:20:14,940
他有 这是

2374
01:20:14,940 --> 01:20:16,140
他在沙漠中行走的场景

2375
01:20:16,140 --> 01:20:17,580
，他不确定自己是否看到了

2376
01:20:17,580 --> 01:20:19,260
这个想要暗杀的人，

2377
01:20:19,260 --> 01:20:21,540
这有点像当你在沙漠中行走时，

2378
01:20:21,540 --> 01:20:22,739


2379
01:20:22,739 --> 01:20:24,840
嗯，你有一种看到

2380
01:20:24,840 --> 01:20:26,460
绿洲的错觉，因为事实证明你 重新产生

2381
01:20:26,460 --> 01:20:28,199
幻觉，但随后你意识到

2382
01:20:28,199 --> 01:20:29,820
你有时在为时已晚之前知道

2383
01:20:29,820 --> 01:20:31,679
你实际上正在产生幻觉它是

2384
01:20:31,679 --> 01:20:33,239
你没有看到绿洲你仍然

2385
01:20:33,239 --> 01:20:35,040
在沙漠中我认为这

2386
01:20:35,040 --> 01:20:37,199
可能就是我们现在所处的情况

2387
01:20:37,199 --> 01:20:39,360
有了很多语言模型的语言能力，

2388
01:20:39,360 --> 01:20:41,400
我们就有了

2389
01:20:41,400 --> 01:20:44,940
呃语言能力的错觉，因为你知道，

2390
01:20:44,940 --> 01:20:46,260
嗯，你总是在找到绿洲之前看到错觉，

2391
01:20:46,260 --> 01:20:48,360
所以我想我

2392
01:20:48,360 --> 01:20:49,440
认为现在我们处于

2393
01:20:49,440 --> 01:20:51,420
沙漠的幻觉状态

2394
01:20:51,420 --> 01:20:53,699
我看到了

2395
01:20:53,699 --> 01:20:55,380
语言能力的潜在火花，但它仍然不是

2396
01:20:55,380 --> 01:20:57,420
很清晰和强大，

2397
01:20:57,420 --> 01:20:59,640
嗯，我们还没有真正到达绿洲，

2398
01:20:59,640 --> 01:21:02,480
是的，

2399
01:21:02,820 --> 01:21:06,360
嗯，只是一个快速的问题，所以看看

2400
01:21:06,360 --> 01:21:09,540
你是否可以给出一个简短的回答，所以

2401
01:21:09,540 --> 01:21:11,219
svenochino

2402
01:21:11,219 --> 01:21:13,440
写了一个问题，它是正确的吗

2403
01:21:13,440 --> 01:21:15,120
要说大型语言模型没有

2404
01:21:15,120 --> 01:21:17,540
先验，

2405
01:21:18,480 --> 01:21:21,300
大型语言模型是否有先验，我会

2406
01:21:21,300 --> 01:21:23,820
说是的，他们肯定有，

2407
01:21:23,820 --> 01:21:25,620
嗯，

2408
01:21:25,620 --> 01:21:28,440
嗯，他们是我认为

2409
01:21:28,440 --> 01:21:30,360
你认识的人习惯于思考

2410
01:21:30,360 --> 01:21:32,340
先验和贝叶斯推理的区别

2411
01:21:32,340 --> 01:21:33,840
例如，如果你喜欢写下

2412
01:21:33,840 --> 01:21:36,300
贝叶斯统计模型，你会说

2413
01:21:36,300 --> 01:21:37,860
你知道这是参数，

2414
01:21:37,860 --> 01:21:39,300
这是参数上的先验是

2415
01:21:39,300 --> 01:21:40,860


2416
01:21:40,860 --> 01:21:42,360
大型语言模型，我认为

2417
01:21:42,360 --> 01:21:44,340
先验是

2418
01:21:44,340 --> 01:21:45,900
一般的神经坚果，我认为那是

2419
01:21:45,900 --> 01:21:47,880
先验是更隐含的，所以

2420
01:21:47,880 --> 01:21:49,679
有一些函数他们发现

2421
01:21:49,679 --> 01:21:52,080
比其他函数更容易学习，甚至还有

2422
01:21:52,080 --> 01:21:53,760
一些工作

2423
01:21:53,760 --> 01:21:55,860
试图发现你知道一些关于

2424
01:21:55,860 --> 01:21:57,540
那些隐含

2425
01:21:57,540 --> 01:21:59,040
先验是

2426
01:21:59,040 --> 01:22:00,840
什么的陈述，但这实际上是我的想法

2427
01:22:00,840 --> 01:22:02,040
关于

2428
01:22:02,040 --> 01:22:02,940
嗯，嗯，

2429
01:22:02,940 --> 01:22:04,620
你知道不同

2430
01:22:04,620 --> 01:22:07,199
神经网络架构的比较，

2431
01:22:07,199 --> 01:22:09,060
嗯，嗯，这可能是一件值得高兴的事情，

2432
01:22:09,060 --> 01:22:10,500
我可能会同意

2433
01:22:10,500 --> 01:22:12,600
你必须找到先验知识，让他们学习

2434
01:22:12,600 --> 01:22:14,400
孩子们正确学习的东西，

2435
01:22:14,400 --> 01:22:16,140


2436
01:22:16,140 --> 01:22:19,080
嗯，并不是所有的架构都会

2437
01:22:19,080 --> 01:22:21,360
即使在正在

2438
01:22:21,360 --> 01:22:23,280
变得完整或能够学习

2439
01:22:23,280 --> 01:22:24,719
任何类型的功能的架构中也这样做，

2440
01:22:24,719 --> 01:22:27,540
即使在

2441
01:22:27,540 --> 01:22:30,000
巨大的数据集大小上，也不是所有的架构都会这样做，所以

2442
01:22:30,000 --> 01:22:31,800
嗯，我认为这种对

2443
01:22:31,800 --> 01:22:34,080
神经网络架构的搜索是 确实是对

2444
01:22:34,080 --> 01:22:36,360
先验的搜索之一，

2445
01:22:36,360 --> 01:22:38,580
嗯，但它不是先验，或者我的意思是你

2446
01:22:38,580 --> 01:22:39,719
可以将其视为对

2447
01:22:39,719 --> 01:22:41,699
通用语法的搜索或正确的东西，但

2448
01:22:41,699 --> 01:22:44,340
它不是先验或通用

2449
01:22:44,340 --> 01:22:46,560
语法，就人们

2450
01:22:46,560 --> 01:22:48,540
所说的那样

2451
01:22:48,540 --> 01:22:50,159
关于允许哪些类型的规则的明确声明

2452
01:22:50,159 --> 01:22:51,780
或关于

2453
01:22:51,780 --> 01:22:53,880
哪些类型的函数是高

2454
01:22:53,880 --> 01:22:55,320
概率的明确声明或类似的东西它

2455
01:22:55,320 --> 01:22:57,360
都隐式编码在那里

2456
01:22:57,360 --> 01:22:58,560
嗯是的是的完全我认为我认为

2457
01:22:58,560 --> 01:23:00,540
这是正确的我的意思是你知道真正的

2458
01:23:00,540 --> 01:23:02,880
问题正在减少 那些

2459
01:23:02,880 --> 01:23:05,100
相似的人的空间，如果它与

2460
01:23:05,100 --> 01:23:07,020
人类

2461
01:23:07,020 --> 01:23:09,300
正在做的事情有很大的不同，那么我

2462
01:23:09,300 --> 01:23:11,219
至少会说像 qpt3 这样的东西

2463
01:23:11,219 --> 01:23:13,380
存在证明你知道

2464
01:23:13,380 --> 01:23:15,960


2465
01:23:15,960 --> 01:23:18,000
从表面分布

2466
01:23:18,000 --> 01:23:21,420
分析构建功能齐全的句法类别 仅此一项是可能的，

2467
01:23:21,420 --> 01:23:24,120
是的，这是正确的，但即使如此，

2468
01:23:24,120 --> 01:23:27,300
我会说大多数从业者并不

2469
01:23:27,300 --> 01:23:29,460
真正相信句法类别

2470
01:23:29,460 --> 01:23:31,380
是天生的，因此先前的问题

2471
01:23:31,380 --> 01:23:33,060
在这里稍微无关紧要，这是

2472
01:23:33,060 --> 01:23:35,219
设置为天生的操作，因此

2473
01:23:35,219 --> 01:23:37,500
在 语法领域 它是特定的

2474
01:23:37,500 --> 01:23:39,659
语言计算，据说

2475
01:23:39,659 --> 01:23:41,159
属于 a 和类别本身

2476
01:23:41,159 --> 01:23:42,719
事实上，甚至 Charles Yang

2477
01:23:42,719 --> 01:23:44,100
um 在过去几年中承认

2478
01:23:44,100 --> 01:23:46,800
他们可能在其中但也许不在其中，

2479
01:23:46,800 --> 01:23:49,679
所以人们已经给出了另一个

2480
01:23:49,679 --> 01:23:51,540
相关的优先级是事情 就像

2481
01:23:51,540 --> 01:23:53,100
嗯，你认识我，Gary markets

2482
01:23:53,100 --> 01:23:55,080
谈到了似乎

2483
01:23:55,080 --> 01:23:56,640
是一个大问题的组合性，所以人们给

2484
01:23:56,640 --> 01:23:59,520
聊天 gbt BBC 新闻文章要求它

2485
01:23:59,520 --> 01:24:02,280
压缩它然后重新解释它呃所以

2486
01:24:02,280 --> 01:24:04,920
我看到的一个例子是 Peter Smith 58 是 因过失

2487
01:24:04,920 --> 01:24:06,480


2488
01:24:06,480 --> 01:24:09,060
杀人罪被捕，你让它压缩

2489
01:24:09,060 --> 01:24:10,920
并重新解释它，结果是

2490
01:24:10,920 --> 01:24:12,420
58 人被指控过失

2491
01:24:12,420 --> 01:24:14,040
杀人权，这是一个非常明显的

2492
01:24:14,040 --> 01:24:15,659
例子，表明它正在做的任何压缩都缺乏组合性，

2493
01:24:15,659 --> 01:24:17,219


2494
01:24:17,219 --> 01:24:19,260
而且有 另一个例子，

2495
01:24:19,260 --> 01:24:20,940
那里有一些

2496
01:24:20,940 --> 01:24:23,159
潜在的类比推理的例子，所以在

2497
01:24:23,159 --> 01:24:24,840
Bing 聊天中你知道 Bing 有这个

2498
01:24:24,840 --> 01:24:26,100
陷阱功能

2499
01:24:26,100 --> 01:24:28,080
嗯问题是它只是找到

2500
01:24:28,080 --> 01:24:29,460
已经被

2501
01:24:29,460 --> 01:24:31,080
人类记录的元关系，还是它真的

2502
01:24:31,080 --> 01:24:33,420
创造了新的新关系

2503
01:24:33,420 --> 01:24:35,100
正在建造的东西，

2504
01:24:35,100 --> 01:24:38,520
嗯，所以你知道有人问我，嗯，给我画了

2505
01:24:38,520 --> 01:24:41,760
一张比较耶稣基督和

2506
01:24:41,760 --> 01:24:44,640
诺基亚 9910 的表格，对了，手机诺基亚

2507
01:24:44,640 --> 01:24:46,080
9910

2508
01:24:46,080 --> 01:24:47,520
嗯，它说你知道他们比较了

2509
01:24:47,520 --> 01:24:49,860
发布日期，比较了尺寸

2510
01:24:49,860 --> 01:24:53,280
和重量，比较了 CPU 和

2511
01:24:53,280 --> 01:24:55,140
耶稣无所不能的知识，它

2512
01:24:55,140 --> 01:24:57,600
把手机的记忆力与

2513
01:24:57,600 --> 01:25:00,600
上帝无所不知的本性相提并论，

2514
01:25:00,600 --> 01:25:02,760
嗯，我也认为它说他们

2515
01:25:02,760 --> 01:25:04,560
都复活了，因为诺基亚

2516
01:25:04,560 --> 01:25:06,239
重新发布了几次，所以

2517
01:25:06,239 --> 01:25:08,159
诺基亚听起来像 很好的答案

2518
01:25:08,159 --> 01:25:10,560
那有什么问题没关系

2519
01:25:10,560 --> 01:25:12,120
可能听起来很像

2520
01:25:12,120 --> 01:25:13,860
类比推理但是它也

2521
01:25:13,860 --> 01:25:15,420
有一些非常奇怪的地方

2522
01:25:15,420 --> 01:25:17,100
就像你知道相机它说不

2523
01:25:17,100 --> 01:25:19,679
它只是给出了耶稣的描述或者它

2524
01:25:19,679 --> 01:25:21,659
不是真的 相机是不是有些

2525
01:25:21,659 --> 01:25:24,000
东西看起来像是类比

2526
01:25:24,000 --> 01:25:27,860
推理，但还不清楚是的，嘿，

2527
01:25:27,860 --> 01:25:31,440
我认为这对我来说是一个

2528
01:25:31,440 --> 01:25:33,020
很棒的答案，

2529
01:25:33,020 --> 01:25:36,360
我想像你一样说

2530
01:25:36,360 --> 01:25:38,100
大型语言模型学习它们

2531
01:25:38,100 --> 01:25:39,719
存在的词性证明

2532
01:25:39,719 --> 01:25:41,520
类别，但就像他们不只是

2533
01:25:41,520 --> 01:25:43,380
输出词性类别一样，

2534
01:25:43,380 --> 01:25:45,719
就像他们有很多语法

2535
01:25:45,719 --> 01:25:47,640
句法知识，

2536
01:25:47,640 --> 01:25:50,880
嗯，而且他们有

2537
01:25:50,880 --> 01:25:52,980
很多语义知识，可能还有一些语

2538
01:25:52,980 --> 01:25:55,080
用知识，你知道

2539
01:25:55,080 --> 01:25:58,080
他们是 翻译还不错，而且

2540
01:25:58,080 --> 01:25:59,880
他们

2541
01:25:59,880 --> 01:26:01,139
发现的

2542
01:26:01,139 --> 01:26:03,840
不仅仅是词性类别，

2543
01:26:03,840 --> 01:26:07,020
嗯，嗯，我很抱歉，我说我很抱歉，

2544
01:26:07,020 --> 01:26:08,940
这是一个技术类别，对，对，对，对，对，对，对，

2545
01:26:08,940 --> 01:26:11,040
对，但是

2546
01:26:11,040 --> 01:26:13,080
他们发现了 远不止于此，

2547
01:26:13,080 --> 01:26:13,980
嗯，

2548
01:26:13,980 --> 01:26:15,600
嗯，嗯，

2549
01:26:15,600 --> 01:26:18,780
我将作为一个 um teaser slash

2550
01:26:18,780 --> 01:26:20,699
motivator 希望你们俩

2551
01:26:20,699 --> 01:26:23,040
在未来有或

2552
01:26:23,040 --> 01:26:24,540
没有其他客人的情况下再次加入 uh 一些

2553
01:26:24,540 --> 01:26:26,520
令人兴奋的问题只是为了让我们

2554
01:26:26,520 --> 01:26:28,679
包含在这份成绩单中和 然后

2555
01:26:28,679 --> 01:26:30,060
感谢 Ellie 和 Steven 的

2556
01:26:30,060 --> 01:26:31,260
加入，所以只是最后几个

2557
01:26:31,260 --> 01:26:33,719
被问到的问题 Juan 问了

2558
01:26:33,719 --> 01:26:35,820
2020 年小变形金刚 Jang

2559
01:26:35,820 --> 01:26:38,580
与儿童学习语言相比如何

2560
01:26:38,580 --> 01:26:40,860
96 问你对

2561
01:26:40,860 --> 01:26:42,800
隐性先验与动物本能

2562
01:26:42,800 --> 01:26:45,780
rojda 的看法 llms 中的空间限制是什么

2563
01:26:45,780 --> 01:26:48,420
他们没有通过培训到达那里，

2564
01:26:48,420 --> 01:26:50,699
所以他们是否发现它不是

2565
01:26:50,699 --> 01:26:52,500
他们在开始时实施的可能

2566
01:26:52,500 --> 01:26:54,780
还有更多问题所以我

2567
01:26:54,780 --> 01:26:57,540
希望我们都可以 um 回顾并

2568
01:26:57,540 --> 01:27:00,600
重新阅读彼此的作品和

2569
01:27:00,600 --> 01:27:04,020
在未来的某个时间一起参加 41.2

2570
01:27:04,020 --> 01:27:06,060
谢谢 Elliot 和 Steven 的

2571
01:27:06,060 --> 01:27:08,280
精彩直播 谢谢 Dave 谢谢你们

2572
01:27:08,280 --> 01:27:10,739
俩 是的 谢谢

2573
01:27:10,739 --> 01:27:15,379
你们 再见再见


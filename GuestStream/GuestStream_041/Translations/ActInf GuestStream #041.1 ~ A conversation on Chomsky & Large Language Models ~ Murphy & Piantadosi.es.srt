1
00:00:05,759 --> 00:00:08,400
hola y bienvenidos a todos al octave

2
00:00:08,400 --> 00:00:10,860
inference Institute este es el flujo de invitados activo

3
00:00:10,860 --> 00:00:15,839
número 41.1 el 25 de abril de 2023

4
00:00:15,839 --> 00:00:18,480
estamos aquí con Elliot Murphy y

5
00:00:18,480 --> 00:00:20,939
Stephen piantadosi esta va a ser

6
00:00:20,939 --> 00:00:23,039
una gran discusión comenzaremos con las

7
00:00:23,039 --> 00:00:24,779
declaraciones de apertura de Stephen y

8
00:00:24,779 --> 00:00:27,359
Elliot Elliott  luego dirija con algunas

9
00:00:27,359 --> 00:00:28,920
preguntas y tendremos una

10
00:00:28,920 --> 00:00:32,759
discusión abierta al final, así que Steven, por favor,

11
00:00:32,759 --> 00:00:34,739
gracias por unirse y por su

12
00:00:34,739 --> 00:00:36,960
declaración de apertura.

13
00:00:36,960 --> 00:00:39,780


14
00:00:39,780 --> 00:00:42,239


15
00:00:42,239 --> 00:00:44,700


16
00:00:44,700 --> 00:00:46,860
eh, supongo que parte de la razón por la

17
00:00:46,860 --> 00:00:48,899
que estamos aquí es que recientemente escribí

18
00:00:48,899 --> 00:00:51,719
un artículo sobre grandes modelos de lenguaje, en

19
00:00:51,719 --> 00:00:53,940
parte, tratando de transmitir algo de entusiasmo

20
00:00:53,940 --> 00:00:55,199
sobre,

21
00:00:55,199 --> 00:00:57,300
um, lo que han logrado en

22
00:00:57,300 --> 00:00:59,100
términos de aprendizaje de sintaxis y

23
00:00:59,100 --> 00:01:00,719
semántica,

24
00:01:00,719 --> 00:01:03,059
um y  en parte señalando que creo que

25
00:01:03,059 --> 00:01:04,979
estos modelos realmente cambian la forma en que

26
00:01:04,979 --> 00:01:06,780
deberíamos pensar sobre el lenguaje,

27
00:01:06,780 --> 00:01:08,520
cómo deberíamos pensar sobre las teorías de la

28
00:01:08,520 --> 00:01:11,939
representación lingüística y las

29
00:01:11,939 --> 00:01:13,560
teorías de la gramática,

30
00:01:13,560 --> 00:01:17,220
y probablemente también las teorías del aprendizaje.

31
00:01:17,220 --> 00:01:19,080
hola

32
00:01:19,080 --> 00:01:21,960
increíble, sí, entonces soy Elliot  meffee Soy un

33
00:01:21,960 --> 00:01:23,040
posdoctorado en el departamento de

34
00:01:23,040 --> 00:01:25,799
neurocirugía de UT Health en Texas

35
00:01:25,799 --> 00:01:27,240
um, leí el artículo de Steven con gran

36
00:01:27,240 --> 00:01:29,759
interés. Lo hice con mucha gente y

37
00:01:29,759 --> 00:01:31,619
había algunas áreas de convergencia, pero las

38
00:01:31,619 --> 00:01:32,820
cosas en las que quiero centrarme

39
00:01:32,820 --> 00:01:34,979
hoy en  respondiendo a Stephen e

40
00:01:34,979 --> 00:01:36,600
investigando sobre áreas de

41
00:01:36,600 --> 00:01:38,640
Divergencia, tal vez,

42
00:01:38,640 --> 00:01:41,700
para que sepas que el artículo de Steven se basa

43
00:01:41,700 --> 00:01:43,619
en la idea de que el aprendizaje automático moderno

44
00:01:43,619 --> 00:01:46,220
ha subvertido y pasado por alto todo el

45
00:01:46,220 --> 00:01:48,119
marco teórico del

46
00:01:48,119 --> 00:01:49,860
enfoque de Chomsky, así que quería responder

47
00:01:49,860 --> 00:01:51,479
a algunos  de estos argumentos principales y algunos

48
00:01:51,479 --> 00:01:52,560
otros argumentos relacionados en la

49
00:01:52,560 --> 00:01:54,060
literatura, eh, algunas personas que

50
00:01:54,060 --> 00:01:55,799
escuchan pueden tener alguna idea

51
00:01:55,799 --> 00:01:58,500
y pensamientos, por lo que es una crítica muy común

52
00:01:58,500 --> 00:02:00,720
decir que los modelos de lenguaje grande

53
00:02:00,720 --> 00:02:03,180
solo predicen el próximo token y

54
00:02:03,180 --> 00:02:04,079
que

55
00:02:04,079 --> 00:02:05,579
obviamente hay un poco de  cliché

56
00:02:05,579 --> 00:02:07,619
cierto, um, no es del todo cierto y

57
00:02:07,619 --> 00:02:09,419
no solo predicen el próximo token,

58
00:02:09,419 --> 00:02:11,700
también parecen confabularse, parecen

59
00:02:11,700 --> 00:02:14,340
alucinar, tal vez mienten,

60
00:02:14,340 --> 00:02:16,440
brindan aleatoriamente diferentes respuestas a la misma

61
00:02:16,440 --> 00:02:18,300
pregunta y parecen haber

62
00:02:18,300 --> 00:02:20,220
imitado estocásticamente

63
00:02:20,220 --> 00:02:22,020
estructuras similares al lenguaje.  a veces se corrigen a

64
00:02:22,020 --> 00:02:23,819
sí mismos, a veces cuando no deberían,

65
00:02:23,819 --> 00:02:25,440
si los presionas un poco,

66
00:02:25,440 --> 00:02:27,000
cambian de opinión a veces,

67
00:02:27,000 --> 00:02:28,560
de hecho, si Fox News está

68
00:02:28,560 --> 00:02:30,060
buscando un reemplazo para Tucker

69
00:02:30,060 --> 00:02:31,800
Carlson, podrían hacer menos,

70
00:02:31,800 --> 00:02:33,680
definitivamente podrían hacerlo peor que usar

71
00:02:33,680 --> 00:02:36,120
entre si  están buscando un

72
00:02:36,120 --> 00:02:38,040
calibre similar, ya sabes, por lo que estos modelos

73
00:02:38,040 --> 00:02:40,620
parecen hacer todo tipo de cosas salvajes,

74
00:02:40,620 --> 00:02:41,940
um, y en los últimos 10 años ha

75
00:02:41,940 --> 00:02:43,500
habido una secuencia de diferentes

76
00:02:43,500 --> 00:02:45,900
sistemas, ya sabes, desarrollados como si fuéramos una cama de tabaco

77
00:02:45,900 --> 00:02:47,580
y cada uno de ellos es  basado en un

78
00:02:47,580 --> 00:02:49,620
enfoque de red neuronal diferente Pero, en última instancia,

79
00:02:49,620 --> 00:02:51,180
todos parecen tomar palabras y

80
00:02:51,180 --> 00:02:53,220
caracterizarlas mediante listas de cientos o

81
00:02:53,220 --> 00:02:56,220
miles de números, por lo que la red G23

82
00:02:56,220 --> 00:03:00,000
tiene 175 mil millones de pesos y 96

83
00:03:00,000 --> 00:03:02,160
cabezas de atención en su arquitectura y, por lo que

84
00:03:02,160 --> 00:03:03,780
sé, tal vez  Steven puede

85
00:03:03,780 --> 00:03:06,060
corregirme aquí, realmente no tenemos una gran

86
00:03:06,060 --> 00:03:07,319
idea de lo que realmente significan estas diferentes partes,

87
00:03:07,319 --> 00:03:09,180
simplemente parece

88
00:03:09,180 --> 00:03:10,920
funcionar de esa manera, como las cabezas de atención y

89
00:03:10,920 --> 00:03:13,680
gpt3 pueden prestar atención a tokens mucho más antiguos

90
00:03:13,680 --> 00:03:15,360
en la cadena para ayudar.

91
00:03:15,360 --> 00:03:17,040
ellos predicen el próximo token, pero

92
00:03:17,040 --> 00:03:18,659
toda la arquitectura de principio a fin

93
00:03:18,659 --> 00:03:21,599
es una especie de motivaciones basadas en la ingeniería,

94
00:03:21,599 --> 00:03:22,500
um,

95
00:03:22,500 --> 00:03:24,659
y siempre me pregunto qué pasa con

96
00:03:24,659 --> 00:03:27,360
todos los modelos que fallaron de

97
00:03:27,360 --> 00:03:28,800
estas películas de las diferentes

98
00:03:28,800 --> 00:03:30,300
compañías tecnológicas. Es como si estas compañías

99
00:03:30,300 --> 00:03:31,980
a menudo parecieran

100
00:03:31,980 --> 00:03:33,060
um, ya sabes, haz que parezca que tienen

101
00:03:33,060 --> 00:03:34,980
estos modelos que realmente funcionan muy bien

102
00:03:34,980 --> 00:03:36,720
directamente de la caja,

103
00:03:36,720 --> 00:03:38,459
um, y todos parecen tener el nombre de

104
00:03:38,459 --> 00:03:40,739
algún tipo de artista famoso, correcto, correcto,

105
00:03:40,739 --> 00:03:42,780
tienen Darley después de la ensalada o Dali,

106
00:03:42,780 --> 00:03:45,360
tienen Da Vinci, tal vez bonito.  pronto una

107
00:03:45,360 --> 00:03:46,980
de estas compañías lanzará un

108
00:03:46,980 --> 00:03:49,500
modelo de lenguaje grande eh llamado Jesús o

109
00:03:49,500 --> 00:03:50,879
algo así no lo sé

110
00:03:50,879 --> 00:03:53,040
um pero siempre dicen que este es nuestro

111
00:03:53,040 --> 00:03:54,900
modelo New Foundation se llama Picasso

112
00:03:54,900 --> 00:03:56,280
es el primero que probamos somos

113
00:03:56,280 --> 00:03:58,140
simplemente geniales sin problemas directamente  fuera de

114
00:03:58,140 --> 00:03:59,819
la caja, pero siempre me pregunto qué pasa con las

115
00:03:59,819 --> 00:04:01,680
cajas de Oliver Black que han

116
00:04:01,680 --> 00:04:03,659
fallado cada vez que no parece

117
00:04:03,659 --> 00:04:05,760
haber una estructura muy abierta y clara

118
00:04:05,760 --> 00:04:07,379
para el tipo de

119
00:04:07,379 --> 00:04:09,840
razonamiento científico detrás de la selección de un

120
00:04:09,840 --> 00:04:11,939
modelo u otro.  eh, pero nuevamente, podría estar

121
00:04:11,939 --> 00:04:14,280
abierto a que me corrijan sobre eso, por lo que

122
00:04:14,280 --> 00:04:16,798
incluso los modelos de lenguaje básicos funcionan

123
00:04:16,798 --> 00:04:18,298
bastante bien en,

124
00:04:18,298 --> 00:04:20,699
como la predicción web básica, por lo que el

125
00:04:20,699 --> 00:04:22,139
problema es si estas herramientas brindan

126
00:04:22,139 --> 00:04:23,520
información sobre

127
00:04:23,520 --> 00:04:25,320
nociones psicolingüísticas tradicionales como gramática

128
00:04:25,320 --> 00:04:27,300
y aprobación.  esta es realmente la razón por la que

129
00:04:27,300 --> 00:04:29,040
prefiero el modelo 10 Focus en lugar del

130
00:04:29,040 --> 00:04:30,960
modelo de lenguaje sugerido por personas

131
00:04:30,960 --> 00:04:32,880
como cyberveras,

132
00:04:32,880 --> 00:04:34,560
por lo que se ha señalado que nadie

133
00:04:34,560 --> 00:04:36,479
realmente piensa que las películas nos dicen algo

134
00:04:36,479 --> 00:04:38,580
profundo sobre python cuando obtienen el

135
00:04:38,580 --> 00:04:40,080
código python tan bien como el

136
00:04:40,080 --> 00:04:42,120
lenguaje natural.  python es un

137
00:04:42,120 --> 00:04:43,740
lenguaje simbólico con una gramática de estructura de frase

138
00:04:43,740 --> 00:04:46,620
y nadie dice que las películas están revelando los

139
00:04:46,620 --> 00:04:48,600
secretos de python, así que solo para poner

140
00:04:48,600 --> 00:04:50,940
varios aquí, dice que si un modelo n puede

141
00:04:50,940 --> 00:04:52,620
interpretarse como teorías explicativas para el

142
00:04:52,620 --> 00:04:54,000
lenguaje natural en función de sus

143
00:04:54,000 --> 00:04:56,040
éxitos en las tareas del lenguaje, entonces en  la

144
00:04:56,040 --> 00:04:57,540
ausencia de contraargumentos también deberían

145
00:04:57,540 --> 00:04:58,919
ser buenas teorías explicativas para el

146
00:04:58,919 --> 00:05:01,139
lenguaje informático, por lo tanto,

147
00:05:01,139 --> 00:05:02,699
un modelo exitoso de

148
00:05:02,699 --> 00:05:04,620
lenguaje natural no se puede usar como evidencia

149
00:05:04,620 --> 00:05:06,540
contra la estructura de frase generativa del

150
00:05:06,540 --> 00:05:07,800
lenguaje de Amazon,

151
00:05:07,800 --> 00:05:09,720
por lo que el modelo Corpus es realmente un

152
00:05:09,720 --> 00:05:11,280
término más apropiado por otras razones también

153
00:05:11,280 --> 00:05:13,500
personas  como Emily Bender y algunos otros

154
00:05:13,500 --> 00:05:15,300
han demostrado que las características del

155
00:05:15,300 --> 00:05:16,979
Corpus de entrenamiento, de hecho, creo que Stephen cita

156
00:05:16,979 --> 00:05:17,940
esto, usted cita esto en su artículo

157
00:05:17,940 --> 00:05:19,860
en realidad como una limitación,

158
00:05:19,860 --> 00:05:21,120
um, muestran que las características del

159
00:05:21,120 --> 00:05:22,680
Corpus de entrenamiento pueden influir en gran medida en

160
00:05:22,680 --> 00:05:24,600
el proceso de colocación, por lo que se ha demostrado.

161
00:05:24,600 --> 00:05:25,800
que el rendimiento de los grandes

162
00:05:25,800 --> 00:05:27,780
modelos de lenguaje en las clases de idiomas está

163
00:05:27,780 --> 00:05:29,699
muy influenciado por la diversidad

164
00:05:29,699 --> 00:05:31,440
del corpus de entrenamiento,

165
00:05:31,440 --> 00:05:33,180
pero el lenguaje natural en sí mismo no está

166
00:05:33,180 --> 00:05:35,340
sesgado, es solo

167
00:05:35,340 --> 00:05:37,199
un sistema computacional, los seres humanos

168
00:05:37,199 --> 00:05:39,120
pueden estar sesgados en lo que dicen y cómo

169
00:05:39,120 --> 00:05:41,039
actúan.  y pero el lenguaje natural en sí mismo

170
00:05:41,039 --> 00:05:43,020
no está sesgado, así que los grandes

171
00:05:43,020 --> 00:05:45,180
modelos de lenguaje, por lo tanto, me parece difícil

172
00:05:45,180 --> 00:05:47,880
saber que están

173
00:05:47,880 --> 00:05:49,500
sujetos a todo tipo de sesgos, por lo

174
00:05:49,500 --> 00:05:51,000
tanto, en realidad no pueden ser modelos de

175
00:05:51,000 --> 00:05:52,020
lenguaje, son modelos de algo.  de lo

176
00:05:52,020 --> 00:05:53,880
contrario, solo para resumir este

177
00:05:53,880 --> 00:05:55,620
argumento,

178
00:05:55,620 --> 00:05:58,139
um, ya sabes, aunque las películas están claramente

179
00:05:58,139 --> 00:05:59,759
expuestas a una experiencia lingüística mucho mayor

180
00:05:59,759 --> 00:06:01,320
en los niños, nuevamente, esto es

181
00:06:01,320 --> 00:06:02,699
algo más que Stephen puede ver

182
00:06:02,699 --> 00:06:04,680
y habla en su artículo y, aun así,

183
00:06:04,680 --> 00:06:06,360
sus resultados de aprendizaje aún pueden  ser

184
00:06:06,360 --> 00:06:08,160
relevante al abordar qué

185
00:06:08,160 --> 00:06:09,840
generalizaciones gramaticales se pueden aprender en

186
00:06:09,840 --> 00:06:11,580
principio, así que estoy de acuerdo con esta

187
00:06:11,580 --> 00:06:12,660
afirmación aquí, sabe que, en

188
00:06:12,660 --> 00:06:14,100
principio, pueden decirnos algo

189
00:06:14,100 --> 00:06:16,080
sobre la capacidad de aprendizaje en lugar de cosas

190
00:06:16,080 --> 00:06:17,639
como usted conoce Marcos de Adquisiciones amplios

191
00:06:17,639 --> 00:06:20,520
y pero eso es todo lo que

192
00:06:20,520 --> 00:06:21,900
creo que puede  tal vez decir ahora que

193
00:06:21,900 --> 00:06:24,539
mostrar que algunos sesgos inductivos

194
00:06:24,539 --> 00:06:26,819
no son necesarios para el aprendizaje no es

195
00:06:26,819 --> 00:06:28,139
realmente lo mismo que mostrar que

196
00:06:28,139 --> 00:06:30,060
no está presente en los niños, por lo que ha

197
00:06:30,060 --> 00:06:31,560
habido un largo debate sobre si

198
00:06:31,560 --> 00:06:32,639
conoce evidencia negativa e instrucción

199
00:06:32,639 --> 00:06:34,500
y corrección y retroalimentación durante  el

200
00:06:34,500 --> 00:06:36,720
aprendizaje de idiomas es necesario o incluso

201
00:06:36,720 --> 00:06:39,120
útil para bebés y niños,

202
00:06:39,120 --> 00:06:40,440
pero en este momento estoy más de acuerdo

203
00:06:40,440 --> 00:06:42,479
con Eugene Choi y Gary Marcus y

204
00:06:42,479 --> 00:06:44,340
otros que han resaltado cómo las películas son

205
00:06:44,340 --> 00:06:45,660
actualmente muy caras de

206
00:06:45,660 --> 00:06:46,440
entrenar

207
00:06:46,440 --> 00:06:48,600
que claramente son un ejemplo de un

208
00:06:48,600 --> 00:06:50,400
privado concentrado.  el poder está en manos

209
00:06:50,400 --> 00:06:51,960
de unas pocas empresas de tecnología, su

210
00:06:51,960 --> 00:06:54,539
impacto ambiental es masivo,

211
00:06:54,539 --> 00:06:56,039
y sabes que muchas personas han sido

212
00:06:56,039 --> 00:06:57,300
menos restringidas y conservadoras en

213
00:06:57,300 --> 00:06:58,979
su evaluación aquí,

214
00:06:58,979 --> 00:07:00,660
mucho menos que Gary

215
00:07:00,660 --> 00:07:02,880
Marcus y Eugene, por lo que Bill Gates

216
00:07:02,880 --> 00:07:05,100
escribió recientemente que chat GPT es,

217
00:07:05,100 --> 00:07:06,960
um  el mayor desarrollo tecnológico desde

218
00:07:06,960 --> 00:07:11,400
la interfaz gráfica de usuario uh, la GUI

219
00:07:11,400 --> 00:07:13,020
um y Henry Kissinger escribieron en febrero

220
00:07:13,020 --> 00:07:15,240
en el Wall Street Journal que, a medida que la

221
00:07:15,240 --> 00:07:17,400
capacidad de chat gbt se haya ampliado,

222
00:07:17,400 --> 00:07:19,560
redefinirán el conocimiento humano, acelerarán los

223
00:07:19,560 --> 00:07:21,780
cambios en el tejido de nuestra realidad y

224
00:07:21,780 --> 00:07:23,220
reorganizarán la política y la sociedad.  La

225
00:07:23,220 --> 00:07:25,620
IA generativa se publica para generar

226
00:07:25,620 --> 00:07:28,319
nuevas formas de conciencia humana, por lo que

227
00:07:28,319 --> 00:07:30,419
en este momento se están produciendo afirmaciones muy radicales. Me

228
00:07:30,419 --> 00:07:32,220
pregunto si a veces o si

229
00:07:32,220 --> 00:07:35,099
la exageración de la IA puede haber guardado

230
00:07:35,099 --> 00:07:36,840
en ciertas partes de la Academia,

231
00:07:36,840 --> 00:07:39,240
potencialmente se están haciendo muchos grandes planes,

232
00:07:39,240 --> 00:07:40,680
pero creo que tú  saber más

233
00:07:40,680 --> 00:07:42,180
concretamente solo para devolvérselo a Steven

234
00:07:42,180 --> 00:07:44,280
aquí. Quería plantear el tema

235
00:07:44,280 --> 00:07:46,919
de que hay una crítica de rorski y

236
00:07:46,919 --> 00:07:49,259
Beaumont que creo que ha leído en

237
00:07:49,259 --> 00:07:50,520
lingbuzz.

238
00:07:50,520 --> 00:07:51,240


239
00:07:51,240 --> 00:07:53,280
creo que viste en Twitter que

240
00:07:53,280 --> 00:07:54,419
no te gusta la respuesta.  dieron

241
00:07:54,419 --> 00:07:56,819
porque la objeción que hicieron es

242
00:07:56,819 --> 00:07:58,500
que sabes que la ciencia es un ejemplo de

243
00:07:58,500 --> 00:08:00,479
lógica deductiva tu objeción es que la

244
00:08:00,479 --> 00:08:02,340
ciencia no es deductiva, es inductiva,

245
00:08:02,340 --> 00:08:04,380
pero creo que el punto general

246
00:08:04,380 --> 00:08:06,900
podría ser más preciso, a saber, que

247
00:08:06,900 --> 00:08:08,520
no puedes  No use el hecho de que los

248
00:08:08,520 --> 00:08:10,620
modelos de lenguaje predicen bien algunos

249
00:08:10,620 --> 00:08:13,020
comportamientos lingüísticos en humanos y algunas

250
00:08:13,020 --> 00:08:15,300
respuestas de imágenes neuronales, no puede usar

251
00:08:15,300 --> 00:08:17,460
eso solo para afirmar que pueden producir

252
00:08:17,460 --> 00:08:19,199
una teoría del lenguaje humano,

253
00:08:19,199 --> 00:08:21,000
por lo que en su artículo Stephen sabe que

254
00:08:21,000 --> 00:08:23,220
parece que  Ciertas estructuras funcionan

255
00:08:23,220 --> 00:08:25,020
mejor que otras. El mecanismo de atención correcto

256
00:08:25,020 --> 00:08:26,819
es

257
00:08:26,819 --> 00:08:28,680
importante. La predicción es importante.

258
00:08:28,680 --> 00:08:30,180


259
00:08:30,180 --> 00:08:32,219


260
00:08:32,219 --> 00:08:34,260


261
00:08:34,260 --> 00:08:36,000


262
00:08:36,000 --> 00:08:37,559
No estoy seguro de si tiene más ideas aquí,

263
00:08:37,559 --> 00:08:39,899
así que Rosky y Boomer usaron el ejemplo de una

264
00:08:39,899 --> 00:08:42,719
predicción deficiente pero una explicación sólida. El

265
00:08:42,719 --> 00:08:44,760
poder explicativo correcto y la

266
00:08:44,760 --> 00:08:46,500
precisión no predictiva forman la base de la

267
00:08:46,500 --> 00:08:48,120
ciencia moderna y quiero explorar

268
00:08:48,120 --> 00:08:49,440
esto un poco más tarde, tal vez

269
00:08:49,440 --> 00:08:50,820
um pero modelos de lenguaje moderno.  pueden

270
00:08:50,820 --> 00:08:52,440
modelar con precisión partes del lenguaje humano,

271
00:08:52,440 --> 00:08:54,180
pero también pueden funcionar muy bien en

272
00:08:54,180 --> 00:08:55,920
lenguajes imposibles y

273
00:08:55,920 --> 00:08:58,200
estructuras antinaturales que los humanos no pueden aprender y

274
00:08:58,200 --> 00:09:00,360
tienen una gran dificultad para procesar y

275
00:09:00,360 --> 00:09:01,560
sé que estás familiarizado con estos con

276
00:09:01,560 --> 00:09:03,060
estas críticas, ¿verdad,

277
00:09:03,060 --> 00:09:04,380
pero eres  Definitivamente no estoy solo aquí

278
00:09:04,380 --> 00:09:08,339
al mismo tiempo, así que, eh, Elia,

279
00:09:08,339 --> 00:09:10,320
um, el científico jefe de Open AI,

280
00:09:10,320 --> 00:09:12,180
dijo recientemente en una entrevista que ¿qué

281
00:09:12,180 --> 00:09:13,680
significa predecir el próximo token

282
00:09:13,680 --> 00:09:15,540
lo suficientemente bien? Significa que comprendes

283
00:09:15,540 --> 00:09:17,940
la realidad subyacente que condujo a la

284
00:09:17,940 --> 00:09:19,920
creación de  ese token

285
00:09:19,920 --> 00:09:21,779
que es bastante divergente de muchas

286
00:09:21,779 --> 00:09:23,100
afirmaciones más conservadoras en la

287
00:09:23,100 --> 00:09:24,540
literatura aquí

288
00:09:24,540 --> 00:09:26,519
um y también sabes que solo diría en

289
00:09:26,519 --> 00:09:27,839
respuesta a eso que los diferentes

290
00:09:27,839 --> 00:09:30,019
componentes de la ciencia pueden ser

291
00:09:30,019 --> 00:09:32,580
inductivos o deductivos, en realidad no está

292
00:09:32,580 --> 00:09:34,140
en ninguno de los dos o tienes  una

293
00:09:34,140 --> 00:09:36,300
teoría existente, formulas hiperhipótesis,

294
00:09:36,300 --> 00:09:38,519
recopilas datos, los analizas y

295
00:09:38,519 --> 00:09:40,200
eso es un proceso deductivo deductivo,

296
00:09:40,200 --> 00:09:41,880
pero también hay casos en los que

297
00:09:41,880 --> 00:09:43,680
comienzas con una observación específica,

298
00:09:43,680 --> 00:09:44,940
encuentras algunos patrones e induces

299
00:09:44,940 --> 00:09:46,860
conclusiones generales correctas y luego

300
00:09:46,860 --> 00:09:49,380
está la abducción donde mágicamente

301
00:09:49,380 --> 00:09:52,019
inventas  hipótesis y reducir el

302
00:09:52,019 --> 00:09:53,760
espacio de hipótesis, en realidad no dirías

303
00:09:53,760 --> 00:09:55,620
que el razonamiento deductivo no es científico

304
00:09:55,620 --> 00:09:58,200
o que el razonamiento inductivo no es científico

305
00:09:58,200 --> 00:10:00,360
o que el razonamiento abductivo no es científico, ¿verdad?

306
00:10:00,360 --> 00:10:01,800
Estas son solo formas diferentes

307
00:10:01,800 --> 00:10:03,540
de hacer las cosas,

308
00:10:03,540 --> 00:10:05,459
um, quiero decir que en tu artículo das

309
00:10:05,459 --> 00:10:08,399
ejemplos del uso  modelos para predecir

310
00:10:08,399 --> 00:10:09,779
huracanes y pandemias como

311
00:10:09,779 --> 00:10:12,060
ejemplos de cosas que son tan rigurosas como la

312
00:10:12,060 --> 00:10:13,860
ciencia y luego imploras a tu

313
00:10:13,860 --> 00:10:16,019
lector que concluya que la situación

314
00:10:16,019 --> 00:10:18,120
no es diferente para los modelos de lenguaje

315
00:10:18,120 --> 00:10:19,920
pero supongo que para mí el problema es que los

316
00:10:19,920 --> 00:10:22,200
modelos que predicen huracanes no lo son  en

317
00:10:22,200 --> 00:10:23,940
el negocio de responder a la pregunta ¿

318
00:10:23,940 --> 00:10:25,740
qué es el huracán? Los

319
00:10:25,740 --> 00:10:27,420
modelos correctos que predicen con precisión el

320
00:10:27,420 --> 00:10:29,040
clima son muy precisos,

321
00:10:29,040 --> 00:10:30,600


322
00:10:30,600 --> 00:10:32,700
pero no lo son

323
00:10:32,700 --> 00:10:34,380


324
00:10:34,380 --> 00:10:35,760
.  Ya sabes, te paso

325
00:10:35,760 --> 00:10:37,620
eso,

326
00:10:37,620 --> 00:10:41,820
sí, está bien, bueno, hay muchas cosas allí,

327
00:10:41,820 --> 00:10:43,800
supongo que podría comenzar simplemente

328
00:10:43,800 --> 00:10:45,240
diciendo que,

329
00:10:45,240 --> 00:10:47,579
estoy de acuerdo con muchas de estas

330
00:10:47,579 --> 00:10:51,300
críticas sobre estos

331
00:10:51,300 --> 00:10:54,000
modelos controlados por, ya sabes.

332
00:10:54,000 --> 00:10:56,220
una o dos empresas,

333
00:10:56,220 --> 00:10:59,160
um, eso es muy, muy problemático,

334
00:10:59,160 --> 00:11:01,320
um, sabes que tienen todo tipo de

335
00:11:01,320 --> 00:11:03,480
sesgos que han adquirido porque

336
00:11:03,480 --> 00:11:04,980
están capacitados en texto de

337
00:11:04,980 --> 00:11:06,180
Internet, um,

338
00:11:06,180 --> 00:11:08,640
eso es muy problemático,

339
00:11:08,640 --> 00:11:10,980
um, sabes, ciertamente estoy de acuerdo en que

340
00:11:10,980 --> 00:11:13,079
hay, uh, cosas en  al menos en la actualidad,

341
00:11:13,079 --> 00:11:16,500
los modelos no funcionan bien, así que,

342
00:11:16,500 --> 00:11:18,420
um, creo que es fácil encontrar

343
00:11:18,420 --> 00:11:20,640
ejemplos de preguntas y

344
00:11:20,640 --> 00:11:23,100
problemas que los harán

345
00:11:23,100 --> 00:11:25,500


346
00:11:25,500 --> 00:11:26,760


347
00:11:26,760 --> 00:11:29,820
tropezar.  uh, no necesariamente en esos

348
00:11:29,820 --> 00:11:32,399
términos, pero en términos de

349
00:11:32,399 --> 00:11:34,920
desempeño en el lenguaje,

350
00:11:34,920 --> 00:11:38,459
um específicamente, sintaxis y semántica,

351
00:11:38,459 --> 00:11:40,980
um, creo que están mucho más allá de

352
00:11:40,980 --> 00:11:43,019
cualquier otra teoría en cualquier otro

353
00:11:43,019 --> 00:11:46,920
dominio, así que no hay otra

354
00:11:46,920 --> 00:11:49,380
teoría fuera de  lingüística o ciencias de la computación

355
00:11:49,380 --> 00:11:53,100
que pueden generar que sepas largos y

356
00:11:53,100 --> 00:11:56,700
coherentes uh gramaticales uh pasajes de

357
00:11:56,700 --> 00:11:58,500
texto

358
00:11:58,500 --> 00:12:01,140
um y admitir todos sus

359
00:12:01,140 --> 00:12:04,920
problemas como uh sabes herramientas o

360
00:12:04,920 --> 00:12:08,220
cosas que implementan las empresas

361
00:12:08,220 --> 00:12:09,959
um um todavía hay esta esta cuestión

362
00:12:09,959 --> 00:12:12,899
de como  ¿cómo se las arreglan para lidiar con el

363
00:12:12,899 --> 00:12:14,760
lenguaje y

364
00:12:14,760 --> 00:12:16,320
creo que de ahí proviene gran parte del

365
00:12:16,320 --> 00:12:17,760
entusiasmo? Realmente

366
00:12:17,760 --> 00:12:20,160
no ha habido nada ni remotamente

367
00:12:20,160 --> 00:12:23,700
parecido a ellos en términos de capacidad lingüística

368
00:12:23,700 --> 00:12:24,899
y eso es lo que

369
00:12:24,899 --> 00:12:27,660
creo que es.  es emocionante, así que sí, estoy de acuerdo

370
00:12:27,660 --> 00:12:29,220
con un montón de estas cosas con las que

371
00:12:29,220 --> 00:12:31,320
empezaste

372
00:12:31,320 --> 00:12:33,300
um uh pero, sin embargo, creo que en

373
00:12:33,300 --> 00:12:35,100
términos de sintaxis y semántica

374
00:12:35,100 --> 00:12:37,079
simplemente no hay otra teoría que sea

375
00:12:37,079 --> 00:12:39,240
comparable a ellos

376
00:12:39,240 --> 00:12:40,140
um

377
00:12:40,140 --> 00:12:42,120
pero déjame déjame empujar  eso en ese entonces,

378
00:12:42,120 --> 00:12:44,940
así que sí, yo diría que la principal

379
00:12:44,940 --> 00:12:46,320
objeción de muchas personas con las que he

380
00:12:46,320 --> 00:12:48,000
hablado en los departamentos de lingüística

381
00:12:48,000 --> 00:12:50,760
que son como muchos de los generales, sabes,

382
00:12:50,760 --> 00:12:53,100
primero de tu trabajo es

383
00:12:53,100 --> 00:12:54,120
decir realmente bueno,

384
00:12:54,120 --> 00:12:55,620
um, tú te conoces.  cierto, lo hacen, hacen

385
00:12:55,620 --> 00:12:57,360
un trabajo maravilloso, modelando con precisión

386
00:12:57,360 --> 00:12:58,920
todos los aspectos de muchos aspectos de la

387
00:12:58,920 --> 00:13:01,380
sintaxis y la semántica, sin embargo,

388
00:13:01,380 --> 00:13:03,300
no conozco ninguno real, como

389
00:13:03,300 --> 00:13:04,920
tú sabes, Chomsky habla sobre hechos sobre el

390
00:13:04,920 --> 00:13:06,660
lenguaje, que es una noción pasada de moda.

391
00:13:06,660 --> 00:13:09,000
y pero realmente creo que es

392
00:13:09,000 --> 00:13:10,260
una noción importante también,

393
00:13:10,260 --> 00:13:12,899
como si hay algún descubrimiento sobre el

394
00:13:12,899 --> 00:13:16,860
lenguaje en sí mismo que las películas pueden proporcionar de manera única,

395
00:13:16,860 --> 00:13:19,200
como si la película hiciera alguna

396
00:13:19,200 --> 00:13:21,899
predicción sobre, digamos, que tienes una

397
00:13:21,899 --> 00:13:24,600
estructura de oración Tipo X que es más

398
00:13:24,600 --> 00:13:26,279
difícil de procesar que una oración  escriba

399
00:13:26,279 --> 00:13:28,620
Y y esta es una predicción única que

400
00:13:28,620 --> 00:13:31,200
solo ellos generarían y ningún

401
00:13:31,200 --> 00:13:33,899
lingüista humano Chomsky homestein ninguna de estas

402
00:13:33,899 --> 00:13:34,980
personas había predicho eso antes,

403
00:13:34,980 --> 00:13:37,260
pero resulta ser cierto usted hace

404
00:13:37,260 --> 00:13:38,760
experimentos de seguimiento ocular hace todo tipo de

405
00:13:38,760 --> 00:13:40,800
experiencia de comportamiento diferente  y, oh, ya

406
00:13:40,800 --> 00:13:42,480
sabes, después de todo, resulta ser

407
00:13:42,480 --> 00:13:44,279
cierto. Esta es una nueva perspectiva sobre el

408
00:13:44,279 --> 00:13:45,839
procesamiento del lenguaje. Es una nueva perspectiva

409
00:13:45,839 --> 00:13:47,940
sobre el lenguaje.

410
00:13:47,940 --> 00:13:49,560


411
00:13:49,560 --> 00:13:51,240
Ya sabes. Comportamiento.

412
00:13:51,240 --> 00:13:52,620
porque podría suceder en un

413
00:13:52,620 --> 00:13:54,540
futuro cercano, pero supongo que para mí ese es el

414
00:13:54,540 --> 00:13:57,240
quid de por qué muchos lingüistas hablan

415
00:13:57,240 --> 00:13:59,519
de hablar en nombre de toda la

416
00:13:59,519 --> 00:14:02,040
comunidad lingüística aquí y sabes,

417
00:14:02,040 --> 00:14:03,180
supongo que esa sería una de las principales

418
00:14:03,180 --> 00:14:04,800
objeciones,

419
00:14:04,800 --> 00:14:08,279
sí, quiero decir yo yo  no sé de eh,

420
00:14:08,279 --> 00:14:10,019
supongo que pienso en las ideas que han

421
00:14:10,019 --> 00:14:12,420
proporcionado como una especie de principios generales,

422
00:14:12,420 --> 00:14:14,459
así que, eh,

423
00:14:14,459 --> 00:14:16,200
pienso en estas cosas como el

424
00:14:16,200 --> 00:14:18,720
poder de memorizar fragmentos

425
00:14:18,720 --> 00:14:20,820
de lenguaje, así que

426
00:14:20,820 --> 00:14:22,500
parecen  ser muy bueno en construcciones,

427
00:14:22,500 --> 00:14:24,540
por ejemplo, y hay muchas

428
00:14:24,540 --> 00:14:26,160
teorías lingüísticas, la de Chomsky en

429
00:14:26,160 --> 00:14:28,740
particular, que

430
00:14:28,740 --> 00:14:30,899
tratan de encontrar cantidades mínimas

431
00:14:30,899 --> 00:14:33,660
de estructura para memorizar correctamente, tratando

432
00:14:33,660 --> 00:14:36,480
de derivar tanto como sea posible de

433
00:14:36,480 --> 00:14:38,279
um, un conjunto pequeño, algo pequeño  colección

434
00:14:38,279 --> 00:14:40,440
de operaciones

435
00:14:40,440 --> 00:14:42,300
um y creo que eso no ha ido bien para

436
00:14:42,300 --> 00:14:44,459
esas teorías cierto um mientras que esto

437
00:14:44,459 --> 00:14:46,740
va muy bien así que

438
00:14:46,740 --> 00:14:48,360
um uh si pensamos en algo que

439
00:14:48,360 --> 00:14:50,040
tiene la capacidad de memorización si

440
00:14:50,040 --> 00:14:51,300
pensamos en teorías de gramática, por

441
00:14:51,300 --> 00:14:54,240
ejemplo, que  en

442
00:14:54,240 --> 00:14:56,040
um, sabes que a los humanos les gusta la

443
00:14:56,040 --> 00:14:58,139
capacidad realmente notable de memorizar

444
00:14:58,139 --> 00:14:59,880
diferentes construcciones correctas o

445
00:14:59,880 --> 00:15:01,380
palabras diferentes sabemos decenas de

446
00:15:01,380 --> 00:15:02,940
miles de palabras decenas de miles de

447
00:15:02,940 --> 00:15:04,380
construcciones diferentes lo siento decenas de

448
00:15:04,380 --> 00:15:06,420
miles de expresiones idiomáticas diferentes tal vez nuestra

449
00:15:06,420 --> 00:15:07,920
teoría de la gramática debería integrarse

450
00:15:07,920 --> 00:15:10,260
con eso y ellos  en cierto sentido, son una

451
00:15:10,260 --> 00:15:12,480
especie de prueba de principio de que ese

452
00:15:12,480 --> 00:15:15,660
tipo de enfoque puede funcionar bien, ¿verdad?

453
00:15:15,660 --> 00:15:17,279
um puede pensar en hacer otros tipos de

454
00:15:17,279 --> 00:15:19,380
predicciones con ellos um algunas de las cuales

455
00:15:19,380 --> 00:15:21,779
la gente está haciendo actualmente pero,

456
00:15:21,779 --> 00:15:23,339
por ejemplo, tratando de usarlos para

457
00:15:23,339 --> 00:15:25,860
medir  uh dificultad de procesamiento medir

458
00:15:25,860 --> 00:15:27,899
sorpresa por ejemplo de estos

459
00:15:27,899 --> 00:15:29,160
modelos

460
00:15:29,160 --> 00:15:30,600
um hay medidas de sorpresa que

461
00:15:30,600 --> 00:15:31,699
son

462
00:15:31,699 --> 00:15:34,260
mucho mejores que decir gramáticas libres de contexto

463
00:15:34,260 --> 00:15:36,120
u otros tipos de

464
00:15:36,120 --> 00:15:37,440
modelos de lenguaje y luego es interesante

465
00:15:37,440 --> 00:15:39,899
preguntarse cómo esas uh sorpresas o

466
00:15:39,899 --> 00:15:41,940
previsibilidades se relacionan con el

467
00:15:41,940 --> 00:15:44,399
procesamiento humano correcto y  puede capturar

468
00:15:44,399 --> 00:15:46,620
algo o puede ser no lineal o puede que

469
00:15:46,620 --> 00:15:49,079
solo capture un poco

470
00:15:49,079 --> 00:15:51,420
o lo que sea que es un

471
00:15:51,420 --> 00:15:53,940
tipo interesante de otra pregunta científica, pero

472
00:15:53,940 --> 00:15:55,500
creo que, en principio, pueden

473
00:15:55,500 --> 00:15:57,779
hacer predicciones sobre, por ejemplo, las

474
00:15:57,779 --> 00:15:59,880
conexiones  entre oraciones, así que

475
00:15:59,880 --> 00:16:02,279
en el documento di este ejemplo de que

476
00:16:02,279 --> 00:16:05,459
sabe convertir una declaración en una

477
00:16:05,459 --> 00:16:07,920
pregunta de 10 maneras diferentes y

478
00:16:07,920 --> 00:16:10,620
presumiblemente cuando sabe GPT o

479
00:16:10,620 --> 00:16:12,540
algo está haciendo, está encontrando

480
00:16:12,540 --> 00:16:15,420
10 preguntas diferentes que están todas eh

481
00:16:15,420 --> 00:16:18,060
en  de alguna manera relacionado cerca en

482
00:16:18,060 --> 00:16:20,339
los modelos subyacentes del espacio semántico o

483
00:16:20,339 --> 00:16:22,139
sintáctico

484
00:16:22,139 --> 00:16:24,660
um y ese tipo de cosas son uh

485
00:16:24,660 --> 00:16:26,459
del tipo que creo um uh

486
00:16:26,459 --> 00:16:28,560
sabes que algunos lingüistas podrían querer

487
00:16:28,560 --> 00:16:30,180
bien que es aquí hay una

488
00:16:30,180 --> 00:16:32,220
conexión oculta entre oraciones o su o

489
00:16:32,220 --> 00:16:34,320
su  estructuras, pero hasta donde yo sé, aún

490
00:16:34,320 --> 00:16:36,120
no se han evaluado empíricamente,

491
00:16:36,120 --> 00:16:39,660
así que sí, sí, me refiero a que este

492
00:16:39,660 --> 00:16:41,220
tipo de modelos solo tienen unos pocos años,

493
00:16:41,220 --> 00:16:43,440
así que creo que es

494
00:16:43,440 --> 00:16:45,000
razonable estar entusiasmado con

495
00:16:45,000 --> 00:16:46,259
ellos a pesar de que este tipo de trabajo no lo

496
00:16:46,259 --> 00:16:48,540
ha hecho.  no se ha hecho todavía no eso es correcto no

497
00:16:48,540 --> 00:16:50,880
totalmente totalmente quiero decir pero

498
00:16:50,880 --> 00:16:51,720
creo que esa es la

499
00:16:51,720 --> 00:16:52,920
perspectiva correcta a tomar pero creo que esto

500
00:16:52,920 --> 00:16:54,779
llega al tema de la

501
00:16:54,779 --> 00:16:56,579
um que mencionaste sorpresa mencionaste

502
00:16:56,579 --> 00:16:58,139
laneability

503
00:16:58,139 --> 00:17:00,839
um sabes LMS y algo de sintaxis pero

504
00:17:00,839 --> 00:17:03,060
obviamente, lo hacen con mucha más información

505
00:17:03,060 --> 00:17:04,439
que los bebés,

506
00:17:04,439 --> 00:17:06,240
de modo que las observaciones de la

507
00:17:06,240 --> 00:17:09,000
estructura potencial en sí mismas no son una

508
00:17:09,000 --> 00:17:10,799
reputación de la pobreza del

509
00:17:10,799 --> 00:17:12,540
estímulo, bueno, la versión más débil,

510
00:17:12,540 --> 00:17:13,500
debería decir, de la pobreza del

511
00:17:13,500 --> 00:17:15,059
argumento distinguido, así que el mero hecho.

512
00:17:15,059 --> 00:17:17,579
que LMS pueda hacer lo que hace con nuestro

513
00:17:17,579 --> 00:17:19,559
premio gramatical es muy sorprendente. Estoy de

514
00:17:19,559 --> 00:17:20,939
acuerdo y, de hecho, no lo habrías

515
00:17:20,939 --> 00:17:22,500
predicho hace cinco, seis o

516
00:17:22,500 --> 00:17:23,699
siete años,

517
00:17:23,699 --> 00:17:25,260
pero no invalida la

518
00:17:25,260 --> 00:17:28,199
afirmación de que los humanos tienen sorpresa y nosotros

519
00:17:28,199 --> 00:17:30,120
traiga esas oraciones con nosotros

520
00:17:30,120 --> 00:17:31,080
para ver si la

521
00:17:31,080 --> 00:17:33,299
lingüística computacional puede restringir las hipótesis y la

522
00:17:33,299 --> 00:17:34,740
lingüística teórica, lo que creo que

523
00:17:34,740 --> 00:17:36,480
puede hacer, por cierto, esto debe hacerse

524
00:17:36,480 --> 00:17:38,039
con experimentos cuidadosos en

525
00:17:38,039 --> 00:17:39,720
los que se controlan diferentes parámetros de aprendizaje

526
00:17:39,720 --> 00:17:43,080
y modelos de lenguaje gigantes.

527
00:17:43,080 --> 00:17:45,299
como gbt gratis, básicamente son

528
00:17:45,299 --> 00:17:47,520
inútiles aquí y esto, por lo que esto llega a

529
00:17:47,520 --> 00:17:49,559
algunos de los lentes de mosaico y las quejas de que

530
00:17:49,559 --> 00:17:51,960
necesitamos algo como un proyecto de LM para bebés

531
00:17:51,960 --> 00:17:53,700
que sé que le interesa y

532
00:17:53,700 --> 00:17:55,320
donde tenemos más

533
00:17:55,320 --> 00:17:56,940
conjuntos de entrenamiento ecológicamente válidos.  haz la

534
00:17:56,940 --> 00:17:58,380
predicción en tu artículo de que se

535
00:17:58,380 --> 00:17:59,640
aprenderá alguna estructura de la que

536
00:17:59,640 --> 00:18:01,679
sospecho que podrías estar en lo correcto,

537
00:18:01,679 --> 00:18:03,059
um pero sabes que aun así, incluso con el

538
00:18:03,059 --> 00:18:04,679
desafío del bebé LM, todavía existe el tipo

539
00:18:04,679 --> 00:18:07,260
de problema no trivial de abordar

540
00:18:07,260 --> 00:18:09,419
problemas más tradicionales como cuándo  los niños

541
00:18:09,419 --> 00:18:11,400
comienzan a generalizar en función de la cantidad

542
00:18:11,400 --> 00:18:13,200
de entrada actual en función de diferentes

543
00:18:13,200 --> 00:18:15,539
factores interlingüísticos y eso

544
00:18:15,539 --> 00:18:17,700
requiere solo conocimientos tradicionales de

545
00:18:17,700 --> 00:18:18,720
psicolingüística y

546
00:18:18,720 --> 00:18:21,539
adquisición del lenguaje, por lo que LMS sabe que se preocupan

547
00:18:21,539 --> 00:18:22,919
por cosas como la frecuencia y

548
00:18:22,919 --> 00:18:24,600
la sorpresa, como dijiste, pero hay una

549
00:18:24,600 --> 00:18:26,160
muy buena  artículo de Sophie sluts y

550
00:18:26,160 --> 00:18:27,600
Andrea Martin, el artículo realmente hermoso

551
00:18:27,600 --> 00:18:30,000
y que creo que habrás visto

552
00:18:30,000 --> 00:18:31,620
muestra muy bien que las

553
00:18:31,620 --> 00:18:34,080
estadísticas de distribución a veces pueden ser una pista para los

554
00:18:34,080 --> 00:18:36,240
momentos de construcción de estructuras, pero

555
00:18:36,240 --> 00:18:37,919
reemplaza estas nociones relacionadas con

556
00:18:37,919 --> 00:18:39,660
la composición, así que simplemente  lea una cita

557
00:18:39,660 --> 00:18:42,360
de Chomsky 57 que se parece mucho a lo

558
00:18:42,360 --> 00:18:45,240
que um tragamonedas y más y diga a pesar de los

559
00:18:45,240 --> 00:18:47,820
innegables intereses e importancia

560
00:18:47,820 --> 00:18:49,440
um de los modelos semánticos y estadísticos del

561
00:18:49,440 --> 00:18:51,360
lenguaje, parecen no tener una

562
00:18:51,360 --> 00:18:52,919
relevancia directa para el problema de determinar

563
00:18:52,919 --> 00:18:54,600
o caracterizar el conjunto de

564
00:18:54,600 --> 00:18:56,340
diferencias gramaticales  Creo que nos vemos obligados

565
00:18:56,340 --> 00:18:57,840
a concluir que la gramática es autónoma

566
00:18:57,840 --> 00:18:59,520
e independiente del significado y que los

567
00:18:59,520 --> 00:19:01,320
modelos probabilísticos no dan una

568
00:19:01,320 --> 00:19:03,179
idea particular de algunos de los problemas básicos

569
00:19:03,179 --> 00:19:05,880
de la estructura sintáctica, por lo que la segunda

570
00:19:05,880 --> 00:19:07,980
cobertura de la segunda

571
00:19:07,980 --> 00:19:10,380
oración resultó ser  incorrecto, pero

572
00:19:10,380 --> 00:19:11,580
es tan cierto que sabe que lo que

573
00:19:11,580 --> 00:19:13,740
obviamente se dice son modelos de estadísticas disponibles

574
00:19:13,740 --> 00:19:16,559
en 57 ya no es preciso cuando se aplica

575
00:19:16,559 --> 00:19:18,600
a los modelos de hoy y eso puede hacer

576
00:19:18,600 --> 00:19:20,340
generalizaciones abstractas sobre

577
00:19:20,340 --> 00:19:21,900
cadenas novedosas y categorías de distribución como

578
00:19:21,900 --> 00:19:23,640
mencionó bien, pero el rendimiento

579
00:19:23,640 --> 00:19:25,380
de un solo modelo  no proporciona

580
00:19:25,380 --> 00:19:27,480
evidencia directa a favor o en contra de la

581
00:19:27,480 --> 00:19:29,760
capacidad de aterrizar de una estructura en particular al

582
00:19:29,760 --> 00:19:31,440
dar la gran distancia entre cualquier

583
00:19:31,440 --> 00:19:33,900
modelo computacional disponible en la actualidad y

584
00:19:33,900 --> 00:19:36,240
el modelo del cerebro humano el éxito no

585
00:19:36,240 --> 00:19:38,100
significa que la estructura sea necesariamente

586
00:19:38,100 --> 00:19:40,679
tierra y el fracaso del modelo tampoco significa

587
00:19:40,679 --> 00:19:42,539
que el  la estructura no se puede aprender,

588
00:19:42,539 --> 00:19:44,520
sí,

589
00:19:44,520 --> 00:19:47,100
sí, así que quiero decir, creo que tal vez valga la

590
00:19:47,100 --> 00:19:49,380
pena desempacar un par de

591
00:19:49,380 --> 00:19:51,000
versiones diferentes de argumentos de capacidad de aprendizaje

592
00:19:51,000 --> 00:19:52,620
que la gente ha hecho porque

593
00:19:52,620 --> 00:19:55,679
ha habido, uh, un tipo muy fuerte

594
00:19:55,679 --> 00:19:57,720
de afirmaciones de imposibilidad,

595
00:19:57,720 --> 00:19:59,160
um, saliendo de una especie de tradición de Chomsky, ¿

596
00:19:59,160 --> 00:20:01,260
verdad?  que nunca somos

597
00:20:01,260 --> 00:20:04,140
afirmaciones sobre la cantidad de datos que se

598
00:20:04,140 --> 00:20:05,820
requerían, hubo afirmaciones sobre

599
00:20:05,820 --> 00:20:08,220
el problema lógico del aprendizaje de idiomas

600
00:20:08,220 --> 00:20:10,740
y que era simplemente imposible,

601
00:20:10,740 --> 00:20:12,120
um, cierto, era imposible sin

602
00:20:12,120 --> 00:20:15,720
tener uh uh uh tipo de

603
00:20:15,720 --> 00:20:17,580
restricciones sustanciales en el  la clase de

604
00:20:17,580 --> 00:20:19,380
idiomas o la clase de gramáticas

605
00:20:19,380 --> 00:20:21,660
que usted adquiriría

606
00:20:21,660 --> 00:20:23,220
um y la gente durante mucho tiempo ha estado

607
00:20:23,220 --> 00:20:25,380
discutiendo en contra de esa versión de las

608
00:20:25,380 --> 00:20:26,340
cosas

609
00:20:26,340 --> 00:20:27,120
um

610
00:20:27,120 --> 00:20:28,860
um usted sabe que hay un trabajo antiguo de

611
00:20:28,860 --> 00:20:30,419
by gold y luego hay todo tipo de

612
00:20:30,419 --> 00:20:32,580
gramaticales  teorías de adquisición

613
00:20:32,580 --> 00:20:35,220
construidas sobre esa tradición uh que se preocupan

614
00:20:35,220 --> 00:20:37,980
mucho por el tipo de uh orden en el

615
00:20:37,980 --> 00:20:39,240
que atraviesas diferentes

616
00:20:39,240 --> 00:20:40,860
hipótesis y consideras diferentes

617
00:20:40,860 --> 00:20:42,600
opciones y cosas

618
00:20:42,600 --> 00:20:43,380
um

619
00:20:43,380 --> 00:20:45,059
um y mi referencia favorita en esto

620
00:20:45,059 --> 00:20:46,919
es este artículo de

621
00:20:46,919 --> 00:20:49,799
um Nick jader y  Paul vetani um llamó

622
00:20:49,799 --> 00:20:51,539
algo así como el aprendizaje ideal del

623
00:20:51,539 --> 00:20:53,039
lenguaje natural

624
00:20:53,039 --> 00:20:54,840
um que básicamente muestra que un

625
00:20:54,840 --> 00:20:57,539
estudiante sin restricciones podría uh con

626
00:20:57,539 --> 00:21:00,480
suficientes datos adquirir el uh el tipo

627
00:21:00,480 --> 00:21:01,860
de generación de reglas o la

628
00:21:01,860 --> 00:21:03,360
generación de gramática

629
00:21:03,360 --> 00:21:05,640
um simplemente observando cadenas, pero eso

630
00:21:05,640 --> 00:21:08,160
ese papel  fue realmente

631
00:21:08,160 --> 00:21:10,200
en respuesta a este enorme cuerpo de trabajo

632
00:21:10,200 --> 00:21:13,140
que argumentaba que aprender

633
00:21:13,140 --> 00:21:15,120
de ejemplos positivos, por lo que solo

634
00:21:15,120 --> 00:21:17,820
observando cadenas era como lógicamente

635
00:21:17,820 --> 00:21:20,520
imposible, cierto, así que

636
00:21:20,520 --> 00:21:23,460
um uh, por supuesto, sabes que a la gente en la

637
00:21:23,460 --> 00:21:25,679
Tradición de Chomsky realmente le gustó esa

638
00:21:25,679 --> 00:21:28,140
forma de  argumento porque era uno que

639
00:21:28,140 --> 00:21:30,419
decía, uh, tenías que tener algo

640
00:21:30,419 --> 00:21:33,299
especificado de forma innata para que la

641
00:21:33,299 --> 00:21:34,919
adquisición del lenguaje funcionara, era como una

642
00:21:34,919 --> 00:21:36,600
especie de argumento matemático, cierto,

643
00:21:36,600 --> 00:21:39,480
tenías que tener algún tipo

644
00:21:39,480 --> 00:21:41,220
de gramática innata u ordenación innata de

645
00:21:41,220 --> 00:21:42,900
hipótesis o  algo y todo eso

646
00:21:42,900 --> 00:21:45,659
resultó ser totalmente incorrecto, así que,

647
00:21:45,659 --> 00:21:48,299
um, si sabes que te mueves a

648
00:21:48,299 --> 00:21:50,520
entornos de aprendizaje un poco más realistas

649
00:21:50,520 --> 00:21:53,340
que Tater y Vatani hacen,

650
00:21:53,340 --> 00:21:55,500
um, entonces resulta que, como un

651
00:21:55,500 --> 00:21:57,240
estudiante idealizado, puede adquirir cosas y

652
00:21:57,240 --> 00:21:59,100
no hay  declaraciones sobre

653
00:21:59,100 --> 00:22:00,600
um, la cantidad de datos que se requieren,

654
00:22:00,600 --> 00:22:02,700
incluso allí, ese es el tipo de

655
00:22:02,700 --> 00:22:06,780
pura capacidad lógica uh para aprender,

656
00:22:06,780 --> 00:22:08,940
um, y esa capacidad es a lo que creo que

657
00:22:08,940 --> 00:22:11,460
las uh versiones grandes de modelos de lenguaje grande

658
00:22:11,460 --> 00:22:13,679
también hablan, así que Chader

659
00:22:13,679 --> 00:22:15,600
invitani y otros otros  trabajo en el sentido de

660
00:22:15,600 --> 00:22:17,460
que ese Espíritu,

661
00:22:17,460 --> 00:22:19,500
um, sabes matemáticas y una especie de

662
00:22:19,500 --> 00:22:21,480
argumento en principio, pero nunca

663
00:22:21,480 --> 00:22:24,720
creó algo que fuera realmente un eh,

664
00:22:24,720 --> 00:22:27,840
un derecho gramatical o un tipo real de

665
00:22:27,840 --> 00:22:30,000
modelo de lenguaje implementado,

666
00:22:30,000 --> 00:22:32,640
um, así que incluso tú conoces un modelo que está

667
00:22:32,640 --> 00:22:35,400
entrenado  en 100 millones o 100 mil millones o la

668
00:22:35,400 --> 00:22:38,340
cantidad de fichas, eh,

669
00:22:38,340 --> 00:22:41,159
incluso ese tipo de modelo, creo que es

670
00:22:41,159 --> 00:22:43,080
relevante para esa versión del

671
00:22:43,080 --> 00:22:46,140
debate, ¿

672
00:22:46,140 --> 00:22:48,659


673
00:22:48,659 --> 00:22:51,360
verdad?

674
00:22:51,360 --> 00:22:53,640
y luego hay una segunda versión, ¿

675
00:22:53,640 --> 00:22:56,760
podemos aprender el idioma

676
00:22:56,760 --> 00:22:59,760
con los datos específicos que los niños entienden

677
00:22:59,760 --> 00:23:02,340
bien y eso es tanto la cantidad de datos

678
00:23:02,340 --> 00:23:04,620
como la forma de los datos?

679
00:23:04,620 --> 00:23:06,360


680
00:23:06,360 --> 00:23:08,520


681
00:23:08,520 --> 00:23:10,440
um uh es

682
00:23:10,440 --> 00:23:12,840
um uh esto uh

683
00:23:12,840 --> 00:23:14,340
um siento llamarlo

684
00:23:14,340 --> 00:23:16,620
competencia o

685
00:23:16,620 --> 00:23:20,580
um a uh uh Supongo que es un desafío

686
00:23:20,580 --> 00:23:22,380
um tratar de hacer que la gente entrene

687
00:23:22,380 --> 00:23:24,539
modelos de lenguaje en cantidades

688
00:23:24,539 --> 00:23:27,179
de datos de tamaño humano um así que eso es algo más como

689
00:23:27,179 --> 00:23:28,679
um, creo que hay dos

690
00:23:28,679 --> 00:23:31,140
versiones diferentes, 10 o 100 millones diferentes

691
00:23:31,140 --> 00:23:33,120
um a 10 o 100 millones de palabras diferentes

692
00:23:33,120 --> 00:23:35,640
en el conjunto de capacitación

693
00:23:35,640 --> 00:23:38,100
um que es como si supieras 100 o

694
00:23:38,100 --> 00:23:41,159
1000 o algo tan grande como

695
00:23:41,159 --> 00:23:43,140
um estas grandes empresas de IA están usando para

696
00:23:43,140 --> 00:23:46,620
su idioma  modelos um y

697
00:23:46,620 --> 00:23:48,720
um, creo que en realidad es como si fuera

698
00:23:48,720 --> 00:23:50,340
exactamente el tipo correcto de

699
00:23:50,340 --> 00:23:52,020
cosas y exactamente lo que el campo necesita,

700
00:23:52,020 --> 00:23:54,780
porque es posible que encuentre que en

701
00:23:54,780 --> 00:23:57,059
um, una cantidad de datos del tamaño de un niño,

702
00:23:57,059 --> 00:23:58,980
um, esencialmente puede aprender la sintaxis

703
00:23:58,980 --> 00:24:01,140
correcta, lo cual creo que sería  sería el

704
00:24:01,140 --> 00:24:02,880
argumento más fuerte en contra de estas

705
00:24:02,880 --> 00:24:04,260
afirmaciones de Pobreza de estímulo,

706
00:24:04,260 --> 00:24:06,299
alternativamente podría encontrar que,

707
00:24:06,299 --> 00:24:08,220
um, tal vez no pueda aprender mucho,

708
00:24:08,220 --> 00:24:10,740
um, tal vez usted, usted sabe, cree

709
00:24:10,740 --> 00:24:12,840
un tipo de modelo de lenguaje mucho más desagradable

710
00:24:12,840 --> 00:24:14,700
o carece de algunas habilidades sintácticas o

711
00:24:14,700 --> 00:24:16,799
semánticas,

712
00:24:16,799 --> 00:24:17,520
um

713
00:24:17,520 --> 00:24:18,720
um, en realidad creo que las

714
00:24:18,720 --> 00:24:20,220
fallas son un poco difíciles de

715
00:24:20,220 --> 00:24:22,620
interpretar porque

716
00:24:22,620 --> 00:24:24,720
um niños uh datos cuando en realidad están

717
00:24:24,720 --> 00:24:26,400
aprendiendo un idioma obtienen muchos más

718
00:24:26,400 --> 00:24:28,919
datos que solo uh solo cadenas de

719
00:24:28,919 --> 00:24:31,200
oraciones en las que están interactuando

720
00:24:31,200 --> 00:24:33,059
en un entorno

721
00:24:33,059 --> 00:24:34,440
um, entonces hay cosas en el mundo

722
00:24:34,440 --> 00:24:36,539
frente a ellos, um, sus expresiones

723
00:24:36,539 --> 00:24:38,520
también son interactivas, así que puedes decir

724
00:24:38,520 --> 00:24:39,960
algo y ver si tu

725
00:24:39,960 --> 00:24:41,400
padre te trae lo que

726
00:24:41,400 --> 00:24:43,799
pides, por ejemplo,

727
00:24:43,799 --> 00:24:46,020


728
00:24:46,020 --> 00:24:49,020
¿verdad?  a a sabes una pista importante

729
00:24:49,020 --> 00:24:51,240
en la adquisición del lenguaje

730
00:24:51,240 --> 00:24:52,799
um así que

731
00:24:52,799 --> 00:24:55,440
um uh en el desafío de LM para bebés

732
00:24:55,440 --> 00:24:58,200
existe la capacidad de entrenar estos

733
00:24:58,200 --> 00:25:00,960
modelos uh con entradas multimodales creo que

734
00:25:00,960 --> 00:25:02,340
puedes darles tantos

735
00:25:02,340 --> 00:25:05,039
datos de video como quieras darles

736
00:25:05,039 --> 00:25:07,140
um  eh, pero probablemente sea difícil

737
00:25:07,140 --> 00:25:09,000
replicar exactamente el tipo de configuración

738
00:25:09,000 --> 00:25:11,220
y la retroalimentación que los niños realmente reciben.

739
00:25:11,220 --> 00:25:12,840


740
00:25:12,840 --> 00:25:14,700


741
00:25:14,700 --> 00:25:17,700


742
00:25:17,700 --> 00:25:19,799
pan ahí afuera

743
00:25:19,799 --> 00:25:20,760
um

744
00:25:20,760 --> 00:25:23,700
um uh sabes que creo que hay una

745
00:25:23,700 --> 00:25:26,580
pregunta interesante relacionada con

746
00:25:26,580 --> 00:25:28,320
modelos de lenguaje grandes

747
00:25:28,320 --> 00:25:30,299
um que es como

748
00:25:30,299 --> 00:25:31,919
entender exactamente lo que

749
00:25:31,919 --> 00:25:34,200
están haciendo todos los datos así que

750
00:25:34,200 --> 00:25:36,659
um podría ser que necesites

751
00:25:36,659 --> 00:25:38,279
tantos datos para  estos modelos porque

752
00:25:38,279 --> 00:25:40,620
efectivamente están inventando algún tipo

753
00:25:40,620 --> 00:25:43,919
de semántica internamente, así que,

754
00:25:43,919 --> 00:25:45,600
ambos están descubriendo la regla de la

755
00:25:45,600 --> 00:25:47,460
sintaxis y allí parecen estar

756
00:25:47,460 --> 00:25:49,380
aprendiendo un poco sobre el significado de las palabras,

757
00:25:49,380 --> 00:25:50,940


758
00:25:50,940 --> 00:25:52,200
umy

759
00:25:52,200 --> 00:25:54,059
um, no está del todo claro, creo que

760
00:25:54,059 --> 00:25:56,220
cómo  gran parte de los datos en estos

761
00:25:56,220 --> 00:25:58,919
modelos modernos uh son necesarios para la sintaxis versus la

762
00:25:58,919 --> 00:26:00,240
semántica

763
00:26:00,240 --> 00:26:02,159
um mi propia conjetura creo que sería

764
00:26:02,159 --> 00:26:05,580
que el lado sintáctico es uh probablemente

765
00:26:05,580 --> 00:26:07,860
requiere muchos menos datos que

766
00:26:07,860 --> 00:26:09,600
el lado semántico

767
00:26:09,600 --> 00:26:11,159
um en realidad un estudiante un ex estudiante

768
00:26:11,159 --> 00:26:12,960
de  el mío Frank Malika y yo escribimos un artículo

769
00:26:12,960 --> 00:26:15,120
hace unos años tratando de estimar la

770
00:26:15,120 --> 00:26:17,400
cantidad de información que un alumno

771
00:26:17,400 --> 00:26:19,679
necesariamente tendría que adquirir

772
00:26:19,679 --> 00:26:22,140
para aprender los diferentes aspectos

773
00:26:22,140 --> 00:26:23,820
del idioma, así que tienes que aprender todas las

774
00:26:23,820 --> 00:26:25,320
palabras y aprendes sus foros

775
00:26:25,320 --> 00:26:26,880
aprendes  sus significados, probablemente conozca

776
00:26:26,880 --> 00:26:28,320
sus frecuencias, tiene que aprender la

777
00:26:28,320 --> 00:26:32,100
sintaxis y, básicamente, lo que encontramos

778
00:26:32,100 --> 00:26:34,080
en ese análisis, que era

779
00:26:34,080 --> 00:26:35,640
básicamente una especie de

780
00:26:35,640 --> 00:26:37,440
cálculo de fondo para cada uno de estos

781
00:26:37,440 --> 00:26:40,679
dominios, es que la sintaxis es en realidad muy

782
00:26:40,679 --> 00:26:42,779
pocos bits de información.  no se necesita

783
00:26:42,779 --> 00:26:46,400
tanta información para aprender la sintaxis,

784
00:26:46,400 --> 00:26:49,320
mientras que la mayoría de la información que

785
00:26:49,320 --> 00:26:52,740
adquiere es en realidad para la semántica, por lo que

786
00:26:52,740 --> 00:26:55,740
especifica que conoce de 30 a 50 000

787
00:26:55,740 --> 00:26:57,720
significados de palabras diferentes que sabe si incluso

788
00:26:57,720 --> 00:27:00,000
si cada significado es

789
00:27:00,000 --> 00:27:02,340
um uh solo unos pocos bits  Así es,

790
00:27:02,340 --> 00:27:04,620
eso requiere mucha información y

791
00:27:04,620 --> 00:27:06,360
probablemente cada reunión es más que unos pocos

792
00:27:06,360 --> 00:27:08,460
bits, así que,

793
00:27:08,460 --> 00:27:11,279
eh, podría ser así, eso me

794
00:27:11,279 --> 00:27:12,659
haría suponer que lo que sucede

795
00:27:12,659 --> 00:27:14,820
con los modelos de lenguaje grandes es que la mayoría de

796
00:27:14,820 --> 00:27:16,320
sus datos de entrenamiento son sobre Word.

797
00:27:16,320 --> 00:27:18,600
semántica y puede pensar en otras

798
00:27:18,600 --> 00:27:20,760
formas en que los niños entienden correctamente la semántica de las palabras,

799
00:27:20,760 --> 00:27:22,140
eso no es solo un tipo de

800
00:27:22,140 --> 00:27:24,600
patrones de co-ocurrencia en el

801
00:27:24,600 --> 00:27:27,360
texto, pero estoy de acuerdo, todo eso está en

802
00:27:27,360 --> 00:27:29,039
el aire y es realmente emocionante ver

803
00:27:29,039 --> 00:27:30,960
lo que sucederá.  sí, sé que

804
00:27:30,960 --> 00:27:32,640
algunos de los resultados anteriores del

805
00:27:32,640 --> 00:27:35,279
laboratorio de Lindsay sugieren que, al menos,

806
00:27:35,279 --> 00:27:37,740
restringido a

807
00:27:37,740 --> 00:27:40,919
sitios de capacitación ecológicamente válidos, eh, los modelos parecen

808
00:27:40,919 --> 00:27:42,840
generalizarse, conoces reglas lineales para

809
00:27:42,840 --> 00:27:44,940
inglés, sí, no formación de preguntas ROM

810
00:27:44,940 --> 00:27:46,620
que no sea la regla jerárquica la

811
00:27:46,620 --> 00:27:48,120
jerarquía correcta  regla, así que creo que

812
00:27:48,120 --> 00:27:49,980
hay un sentido real en el que sabes que el

813
00:27:49,980 --> 00:27:52,020
espacio del

814
00:27:52,020 --> 00:27:54,000
precio sintáctico correcto y los sesgos inductivos

815
00:27:54,000 --> 00:27:56,220
realmente aún no se ha resuelto,

816
00:27:56,220 --> 00:27:57,960
pero al menos me parece bastante

817
00:27:57,960 --> 00:27:59,400
obvio que tiene que haber algo así

818
00:27:59,400 --> 00:28:01,320
también hay alguna evidencia

819
00:28:01,320 --> 00:28:02,700
de que los niños en inglés se remontan

820
00:28:02,700 --> 00:28:04,679
a este problema de frecuencia que los niños en

821
00:28:04,679 --> 00:28:06,360
inglés a veces deletrean una

822
00:28:06,360 --> 00:28:08,100
copia intermedia de movimiento en la

823
00:28:08,100 --> 00:28:10,140
posición especificada de la

824
00:28:10,140 --> 00:28:11,880
posición del complementador inferior de una

825
00:28:11,880 --> 00:28:13,980
pregunta de larga distancia, así que hay una tesis

826
00:28:13,980 --> 00:28:15,360
de Thornton y algunos  otros documentos sobre

827
00:28:15,360 --> 00:28:18,120
esto, por lo que dicen um, ¿qué persona

828
00:28:18,120 --> 00:28:19,919
crees que hizo eso en lugar de qué

829
00:28:19,919 --> 00:28:21,900
persona crees que hizo eso? Entonces, este es

830
00:28:21,900 --> 00:28:23,159
un ajuste interesante, ya sabes, Miss

831
00:28:23,159 --> 00:28:25,020
porque algunos idiomas en realidad

832
00:28:25,020 --> 00:28:26,760
deletrean estas copias intermedias, pero

833
00:28:26,760 --> 00:28:28,799
el inglés no lo hace.  el niño comete el

834
00:28:28,799 --> 00:28:30,779
error de establecer su gramática, pero la

835
00:28:30,779 --> 00:28:32,820
frecuencia de la entrada es en realidad cero,

836
00:28:32,820 --> 00:28:35,159
por lo que nuestro amigo común Gary Marcus

837
00:28:35,159 --> 00:28:36,779
también tiene un argumento en contra de que la frecuencia

838
00:28:36,779 --> 00:28:39,000
determine la salida de un niño en el caso

839
00:28:39,000 --> 00:28:41,159
de los plurales de sustantivos alemanes, una forma más regular

840
00:28:41,159 --> 00:28:42,779
de cierto tipo  Se prefiere

841
00:28:42,779 --> 00:28:44,580
no el frecuente y hay muchos

842
00:28:44,580 --> 00:28:46,559
ejemplos, por lo que a veces se afirma

843
00:28:46,559 --> 00:28:49,080
que el sujeto experimenta pasivos donde

844
00:28:49,080 --> 00:28:50,460
el sujeto está experimentando pasivamente

845
00:28:50,460 --> 00:28:52,559
algo o muy [ __ ] en los niños en

846
00:28:52,559 --> 00:28:54,299
estudios de comprensión hasta alrededor de los ocho

847
00:28:54,299 --> 00:28:56,039
porque no son muy frecuentes en la

848
00:28:56,039 --> 00:28:56,880
entrada

849
00:28:56,880 --> 00:28:59,220
Pero Ken  Wexler y sus colegas han pasado

850
00:28:59,220 --> 00:29:01,260
por um experiencia de sujeto Preguntas doble H

851
00:29:01,260 --> 00:29:03,539
como a quién le gusta Mary y

852
00:29:03,539 --> 00:29:05,820
descubrieron que estas son tan poco frecuentes

853
00:29:05,820 --> 00:29:07,500
en la entrada como el sujeto y la experiencia

854
00:29:07,500 --> 00:29:09,659
de los pasivos, pero los niños no tienen problemas en los

855
00:29:09,659 --> 00:29:10,980
estudios de comprensión de estas preguntas,

856
00:29:10,980 --> 00:29:13,860
pero sí tienen problemas para comprender.  la

857
00:29:13,860 --> 00:29:16,020
experiencia del sujeto de los pasivos verbales, por lo que la

858
00:29:16,020 --> 00:29:17,880
frecuencia una vez más parece ser

859
00:29:17,880 --> 00:29:19,380
irrelevante o al menos no es

860
00:29:19,380 --> 00:29:20,940
explicativa, ¿verdad?

861
00:29:20,940 --> 00:29:22,320


862
00:29:22,320 --> 00:29:25,080


863
00:29:25,080 --> 00:29:27,059


864
00:29:27,059 --> 00:29:28,440


865
00:29:28,440 --> 00:29:30,960
Entonces, ex alumnos, saben que parecen

866
00:29:30,960 --> 00:29:33,419
generalizar nuevamente volviendo a este

867
00:29:33,419 --> 00:29:35,100
tema de los casos que tienen en

868
00:29:35,100 --> 00:29:36,899
su artículo y muestran que

869
00:29:36,899 --> 00:29:38,399
generalizan la estructura de las

870
00:29:38,399 --> 00:29:41,279
ideas de pantalla incoloras, lo que a menudo es muy bueno,

871
00:29:41,279 --> 00:29:43,080
pero el estímulo positivo nunca ha sido

872
00:29:43,080 --> 00:29:44,520
realmente  se trata de no poder

873
00:29:44,520 --> 00:29:46,380
aprender el idioma estadísticamente. Sé que

874
00:29:46,380 --> 00:29:48,059
hizo esa afirmación correctamente, pero el punto de chomsky

875
00:29:48,059 --> 00:29:49,559
en los años 50 sobre los

876
00:29:49,559 --> 00:29:51,779
modelos estadísticos del día no es cierto para

877
00:29:51,779 --> 00:29:53,700
LMS comercial en 2023 y eso es

878
00:29:53,700 --> 00:29:55,440
correcto, pero no podemos usar ese

879
00:29:55,440 --> 00:29:57,539
punto único para socavar  conoces toda la

880
00:29:57,539 --> 00:29:59,520
geometría El punto básico de Enterprise Chomsky

881
00:29:59,520 --> 00:30:01,080
era que podrías tener una

882
00:30:01,080 --> 00:30:02,700
estructura gramatical en la que cada

883
00:30:02,700 --> 00:30:04,980
diagrama tiene una frecuencia cero y tampoco

884
00:30:04,980 --> 00:30:06,899
proporciona instrucciones claramente interpretables

885
00:30:06,899 --> 00:30:08,039
a las

886
00:30:08,039 --> 00:30:09,840
interfaces conceptuales, por lo que las interfaces de otros

887
00:30:09,840 --> 00:30:11,580
sistemas de la mente, como estás mostrando

888
00:30:11,580 --> 00:30:13,980
su GPT en papel imita ejemplos como tirar de

889
00:30:13,980 --> 00:30:16,559
las ideas de la pantalla y, pero sabe nuevamente,

890
00:30:16,559 --> 00:30:18,840
esta oración produce más de 150 000

891
00:30:18,840 --> 00:30:20,700
resultados en Google y se discute

892
00:30:20,700 --> 00:30:22,620
ampliamente en la literatura, es capaz

893
00:30:22,620 --> 00:30:24,480
de imitar el hecho de que puede imitar esto realmente

894
00:30:24,480 --> 00:30:26,640
no nos dice mucho en  al menos

895
00:30:26,640 --> 00:30:27,840
no podemos decir nada con mucha

896
00:30:27,840 --> 00:30:30,899
confianza, así que sabes que abiba detrás de la

897
00:30:30,899 --> 00:30:32,580
University College Dublin tiene esta cita

898
00:30:32,580 --> 00:30:34,380
recientemente, no confundas tu propia

899
00:30:34,380 --> 00:30:36,840
credulidad con la inteligencia de una película y,

900
00:30:36,840 --> 00:30:38,880
de hecho, incluso el joven Mapache escribió el

901
00:30:38,880 --> 00:30:40,500
año pasado que los críticos tienen razón al  acusar a

902
00:30:40,500 --> 00:30:42,480
LMS de estar involucrado en una especie de

903
00:30:42,480 --> 00:30:43,919
mimetismo

904
00:30:43,919 --> 00:30:46,860
um y las oraciones de ejemplo de gbt

905
00:30:46,860 --> 00:30:48,840
que das en el documento en realidad

906
00:30:48,840 --> 00:30:50,700
no hacen un buen trabajo porque, como dices,

907
00:30:50,700 --> 00:30:52,320
es probable que sepas

908
00:30:52,320 --> 00:30:54,240
idiomas sin sentido raros en los datos de entrenamiento pero

909
00:30:54,240 --> 00:30:55,740
pueden  o lo hacen o no pueden, pero

910
00:30:55,740 --> 00:30:57,120
no hay término medio en términos de

911
00:30:57,120 --> 00:30:59,880
darnos 10 ejemplos como este, por lo que

912
00:30:59,880 --> 00:31:02,880
tiene ideas verdes incoloras que son

913
00:31:02,880 --> 00:31:04,679
objetos semánticos muy diferentes de

914
00:31:04,679 --> 00:31:06,840
cosas como conejos marrones brillantes

915
00:31:06,840 --> 00:31:09,899
Osos blancos brillantes canguros negros brillantes monos

916
00:31:09,899 --> 00:31:12,360
verdes brillantes

917
00:31:12,360 --> 00:31:15,120
Leones deslumbrantes amarillos Elementos relucientes rojos

918
00:31:15,120 --> 00:31:16,320
Cierto, todos estos son como

919
00:31:16,320 --> 00:31:18,899
semánticos semánticamente extraños y un poco

920
00:31:18,899 --> 00:31:20,820
extraños, pero aún son como

921
00:31:20,820 --> 00:31:22,440
estructuras legales, son una especie de

922
00:31:22,440 --> 00:31:25,320
objetos semánticos sintéticos significativos.

923
00:31:25,320 --> 00:31:27,179


924
00:31:27,179 --> 00:31:29,580


925
00:31:29,580 --> 00:31:33,179
puede

926
00:31:33,179 --> 00:31:34,799
responder al primer punto primero a la derecha,

927
00:31:34,799 --> 00:31:36,120
así que

928
00:31:36,120 --> 00:31:37,799
empezaste hablando de

929
00:31:37,799 --> 00:31:40,140
estos otros tipos de

930
00:31:40,140 --> 00:31:42,179
patrones de adquisición que tal vez no se correspondan directamente

931
00:31:42,179 --> 00:31:44,279
con la frecuencia,

932
00:31:44,279 --> 00:31:47,159
y creo que en realidad es un error

933
00:31:47,159 --> 00:31:50,640
pensar en ese tipo de modelos de aprendizaje modernos.

934
00:31:50,640 --> 00:31:53,039
debería basarse solo en la frecuencia

935
00:31:53,039 --> 00:31:54,899
porque,

936
00:31:54,899 --> 00:31:57,120
um, claramente están aprendiendo como

937
00:31:57,120 --> 00:31:59,580
familias bastante complicadas de reglas o

938
00:31:59,580 --> 00:32:02,399
construcciones o algo así y,

939
00:32:02,399 --> 00:32:03,899
um, creo que es muy probable que cuando estén

940
00:32:03,899 --> 00:32:06,600
aprendiendo que,

941
00:32:06,600 --> 00:32:08,580
en cierto sentido, están buscando un  simple

942
00:32:08,580 --> 00:32:10,559
o parsimoniosa

943
00:32:10,559 --> 00:32:12,360
um uh explicación de los datos que

944
00:32:12,360 --> 00:32:13,980
han visto bien y cómo se almacenan en caché

945
00:32:13,980 --> 00:32:16,080
en una red neuronal es tal vez

946
00:32:16,080 --> 00:32:19,200
complicado y sabes que depende de

947
00:32:19,200 --> 00:32:20,880
um sabes parámetros y los detalles

948
00:32:20,880 --> 00:32:22,679
del algoritmo de aprendizaje y y y

949
00:32:22,679 --> 00:32:24,480
ese tipo de cosas

950
00:32:24,480 --> 00:32:27,000
um, pero creo que es uh, supongo que

951
00:32:27,000 --> 00:32:29,279
sospecharía que tal vez sea

952
00:32:29,279 --> 00:32:31,740
el caso de que

953
00:32:31,740 --> 00:32:35,640
um uh, como si estuvieran, están

954
00:32:35,640 --> 00:32:38,399
aprendiendo sobre un conjunto complicado de

955
00:32:38,399 --> 00:32:40,799
cosas, cierto, un tipo complicado  de la

956
00:32:40,799 --> 00:32:43,799
familia de reglas y construcciones

957
00:32:43,799 --> 00:32:46,919
um y eso significa que creo que

958
00:32:46,919 --> 00:32:49,380
um sus generalizaciones pueden ser como los

959
00:32:49,380 --> 00:32:52,500
ejemplos de personas que diste

960
00:32:52,500 --> 00:32:55,440
um podrían ser un poco discontinuos en la

961
00:32:55,440 --> 00:32:57,720
entrada, así que a veces

962
00:32:57,720 --> 00:33:00,000
podrías imaginarte viendo algunas cadenas que

963
00:33:00,000 --> 00:33:02,220
conducen  usted a una gramática y la gramática más simple

964
00:33:02,220 --> 00:33:03,840
de los datos que ha

965
00:33:03,840 --> 00:33:06,059
visto hasta ahora es la que predice una

966
00:33:06,059 --> 00:33:09,120
cadena invisible y,

967
00:33:09,120 --> 00:33:12,360
um, si eso sucede, entonces estará

968
00:33:12,360 --> 00:33:14,100
tomando los datos aprendiendo una

969
00:33:14,100 --> 00:33:16,860
representación que se generaliza en alguna

970
00:33:16,860 --> 00:33:19,440
novela invisible  hasta ahora,

971
00:33:19,440 --> 00:33:21,720
simplemente porque esa generalización

972
00:33:21,720 --> 00:33:23,460
es una especie de descripción más simple de los

973
00:33:23,460 --> 00:33:25,019
datos que has visto en escena hasta la fecha.

974
00:33:25,019 --> 00:33:25,980
Creo que eso es algo de lo que los

975
00:33:25,980 --> 00:33:28,320
lingüistas intentaron hacer.

976
00:33:28,320 --> 00:33:29,700


977
00:33:29,700 --> 00:33:31,260
teoría de la misma y, a veces, esa

978
00:33:31,260 --> 00:33:33,419
teoría predice algún fenómeno nuevo

979
00:33:33,419 --> 00:33:35,840
correcto o algún tipo nuevo de oración

980
00:33:35,840 --> 00:33:38,159
y, por lo tanto, si están aprendiendo sobre un

981
00:33:38,159 --> 00:33:41,100
espacio suficientemente rico de teorías,

982
00:33:41,100 --> 00:33:42,899
um, entonces no sería

983
00:33:42,899 --> 00:33:45,000
irrazonable o inesperado para ellos.

984
00:33:45,000 --> 00:33:47,039
También mostrar ese tipo de patrones ahora,

985
00:33:47,039 --> 00:33:48,899
ya sea que lo hagan o no, creo que

986
00:33:48,899 --> 00:33:51,299
todavía es una pregunta empírica abierta, ¿

987
00:33:51,299 --> 00:33:52,740
verdad?

988
00:33:52,740 --> 00:33:54,120
Porque tenemos que entrenarlos con

989
00:33:54,120 --> 00:33:55,620
pequeñas cantidades de datos y probar sus

990
00:33:55,620 --> 00:33:57,480
generalizaciones y este tipo de

991
00:33:57,480 --> 00:33:58,260
cosas,

992
00:33:58,260 --> 00:34:00,120
pero yo no.  No creo que el simple hecho de

993
00:34:00,120 --> 00:34:01,620
que,

994
00:34:01,620 --> 00:34:03,899
um, sabes que los humanos hacen cosas que

995
00:34:03,899 --> 00:34:06,059
no se basan únicamente en la frecuencia, sea una

996
00:34:06,059 --> 00:34:07,860
evidencia de cualquier manera, porque

997
00:34:07,860 --> 00:34:09,300
una vez que estás aprendiendo sobre ricas e

998
00:34:09,300 --> 00:34:10,980
interesantes clases de teorías,

999
00:34:10,980 --> 00:34:13,980
ese es el comportamiento esperado,

1000
00:34:13,980 --> 00:34:16,980
um, en realidad.  Yo tuve um un artículo hace aproximadamente un

1001
00:34:16,980 --> 00:34:18,719
año que creo que estás

1002
00:34:18,719 --> 00:34:20,040
familiarizado con

1003
00:34:20,040 --> 00:34:22,560
um uh Yang y pianta dosi donde

1004
00:34:22,560 --> 00:34:24,179
estábamos

1005
00:34:24,179 --> 00:34:26,879
um uh mirando

1006
00:34:26,879 --> 00:34:29,280
um uh algo de lo que sucede cuando le das a

1007
00:34:29,280 --> 00:34:32,639
un programa aprendizaje de cadenas de modelo de  uh,

1008
00:34:32,639 --> 00:34:34,679
diferentes lenguajes formales, así que piensa en

1009
00:34:34,679 --> 00:34:35,820


1010
00:34:35,820 --> 00:34:38,760
dar un modelo general, solo

1011
00:34:38,760 --> 00:34:41,159
sabes 10 o 20, tal vez cadenas simples que

1012
00:34:41,159 --> 00:34:43,500
obedezcan algún patrón y luego pedirle que

1013
00:34:43,500 --> 00:34:46,199
encuentre un programa que uh pueda explicar esos

1014
00:34:46,199 --> 00:34:49,320
datos, lo que a menudo significa que sabes encontrar

1015
00:34:49,320 --> 00:34:50,940
um uh encontrar algunos  forma de

1016
00:34:50,940 --> 00:34:52,679
escribir programáticamente el

1017
00:34:52,679 --> 00:34:54,839
patrón en las cadenas

1018
00:34:54,839 --> 00:34:56,280
um y en esa figura tenemos un documento

1019
00:34:56,280 --> 00:34:58,020
que es realmente relevante para este

1020
00:34:58,020 --> 00:34:59,820
punto donde

1021
00:34:59,820 --> 00:35:02,640
um las generalizaciones que hace ese tipo

1022
00:35:02,640 --> 00:35:04,200
de modelo

1023
00:35:04,200 --> 00:35:06,660
um uh son creo que tipo de  cualitativamente

1024
00:35:06,660 --> 00:35:08,160
como los que estás describiendo para

1025
00:35:08,160 --> 00:35:10,560
las personas justo donde,

1026
00:35:10,560 --> 00:35:12,060
eh, puedes darles una pequeña cantidad

1027
00:35:12,060 --> 00:35:13,980
de datos y predecirá

1028
00:35:13,980 --> 00:35:16,740
cadenas invisibles con una probabilidad muy alta,

1029
00:35:16,740 --> 00:35:18,420
aunque hay una frecuencia cero en

1030
00:35:18,420 --> 00:35:20,280
la entrada de entrenamiento correcta y la razón por la que lo

1031
00:35:20,280 --> 00:35:22,740
hace  es que, a menudo, la

1032
00:35:22,740 --> 00:35:24,660
descripción computacional más concisa de los

1033
00:35:24,660 --> 00:35:26,640
datos que ha visto es la que

1034
00:35:26,640 --> 00:35:29,640
predice algún nuevo resultado no visto en particular,

1035
00:35:29,640 --> 00:35:32,940
por lo que ese modelo es

1036
00:35:32,940 --> 00:35:34,800
esencialmente una implementación del

1037
00:35:34,800 --> 00:35:36,839
tipo de programa Chader y Vitani

1038
00:35:36,839 --> 00:35:38,160
aprendiendo

1039
00:35:38,160 --> 00:35:40,619
um idea que yo  mencioné anteriormente,

1040
00:35:40,619 --> 00:35:42,900
um, pero es uno que creo que sabes

1041
00:35:42,900 --> 00:35:43,980
si piensas en el contexto

1042
00:35:43,980 --> 00:35:45,660
de estos argumentos de niños que dicen

1043
00:35:45,660 --> 00:35:48,599
cosas inusuales o inesperadas como esa

1044
00:35:48,599 --> 00:35:50,280
es predicha por todos estos tipos de

1045
00:35:50,280 --> 00:35:52,859
cuentas, porque

1046
00:35:52,859 --> 00:35:54,180
mientras  estas cosas están

1047
00:35:54,180 --> 00:35:55,680
comparando efectivamente un espacio interesante de

1048
00:35:55,680 --> 00:35:56,760
gramáticas,

1049
00:35:56,760 --> 00:35:58,079
entonces mostrarán que ese

1050
00:35:58,079 --> 00:35:59,760
tipo de comportamiento creo que está

1051
00:35:59,760 --> 00:36:04,680
bien, así que supongo que saben que

1052
00:36:04,680 --> 00:36:06,960
el argumento sería que, al menos desde

1053
00:36:06,960 --> 00:36:10,140
la perspectiva de género, la sintaxis

1054
00:36:10,140 --> 00:36:13,140
funciona por separado.  pero todavía está

1055
00:36:13,140 --> 00:36:15,359
asignado a la semántica, informa la

1056
00:36:15,359 --> 00:36:17,040
pragmática correctamente, por lo que en el Medio Oriente, la

1057
00:36:17,040 --> 00:36:18,480
sintaxis del programa obviamente no tiene sentido, es

1058
00:36:18,480 --> 00:36:20,160
muy pequeña, es solo una

1059
00:36:20,160 --> 00:36:22,320
linealización y un etiquetado, son las

1060
00:36:22,320 --> 00:36:24,180
dos únicas operaciones, tiene un

1061
00:36:24,180 --> 00:36:26,220
algoritmo de linealización para

1062
00:36:26,220 --> 00:36:27,900
sistemas centromotores y algún tipo de

1063
00:36:27,900 --> 00:36:30,480
algoritmo de categorización en el centro de um en los

1064
00:36:30,480 --> 00:36:32,220
sistemas conceptuales

1065
00:36:32,220 --> 00:36:33,839
um, entonces la arquitectura de chomsky

1066
00:36:33,839 --> 00:36:35,940
depende del proceso de mapeo de sintaxis

1067
00:36:35,940 --> 00:36:37,560
a semántica, correcto, es regulación de significado de espuma,

1068
00:36:37,560 --> 00:36:39,960
no es solo estructura y

1069
00:36:39,960 --> 00:36:42,180
no es solo significado, por lo que LMS

1070
00:36:42,180 --> 00:36:43,680
realmente no tiene esto  proceso de mapeo

1071
00:36:43,680 --> 00:36:45,180
como dónde está el mapeo a la semántica

1072
00:36:45,180 --> 00:36:47,160
y si hay un mapeo, ¿qué hace? ¿Cómo se ve el

1073
00:36:47,160 --> 00:36:48,480
proceso de mapeo? ¿

1074
00:36:48,480 --> 00:36:50,099


1075
00:36:50,099 --> 00:36:52,320
Cuáles son las

1076
00:36:52,320 --> 00:36:54,000
propiedades de

1077
00:36:54,000 --> 00:36:55,500
su semántica?  Las restricciones en el

1078
00:36:55,500 --> 00:36:57,180
proceso de marketing como lo hacen para el

1079
00:36:57,180 --> 00:36:58,980
lenguaje natural. ¿

1080
00:36:58,980 --> 00:37:01,079
Saben? Este tipo de restricciones se

1081
00:37:01,079 --> 00:37:02,640
informan entre sí. Son una especie de

1082
00:37:02,640 --> 00:37:05,579
proceso de ida y vuelta, como los elementos.

1083
00:37:05,579 --> 00:37:07,260


1084
00:37:07,260 --> 00:37:09,359
correcto, como qué

1085
00:37:09,359 --> 00:37:11,339
significados, qué cadenas, por ejemplo,

1086
00:37:11,339 --> 00:37:14,640
bien, lo siento, ¿estás diciendo

1087
00:37:14,640 --> 00:37:16,740
que no tienen semántica

1088
00:37:16,740 --> 00:37:18,599
en absoluto o estás diciendo que

1089
00:37:18,599 --> 00:37:21,180
simplemente no hay una delimitación clara entre

1090
00:37:21,180 --> 00:37:23,400
cómo las estructuras se asignan a la

1091
00:37:23,400 --> 00:37:25,500
semántica, sí, lo último?  cierto,

1092
00:37:25,500 --> 00:37:27,300
claramente tienen algún tipo

1093
00:37:27,300 --> 00:37:28,800
de semántica potencial. Sé que has argumentado a favor de un

1094
00:37:28,800 --> 00:37:30,180
papel conceptual. La teoría es relevante

1095
00:37:30,180 --> 00:37:31,920
aquí. El resto tal vez sea un

1096
00:37:31,920 --> 00:37:33,900
poco más misterioso, pero la soya real.

1097
00:37:33,900 --> 00:37:35,339
La lingüística en sí es una teoría

1098
00:37:35,339 --> 00:37:36,900
del proceso de mapeo.  en sí mismo es

1099
00:37:36,900 --> 00:37:39,060
explícito y puedes verlo en acción

1100
00:37:39,060 --> 00:37:40,440
y puedes probar diferentes teorías

1101
00:37:40,440 --> 00:37:42,000
en modelos lingüísticos de Psych y cuál es la

1102
00:37:42,000 --> 00:37:44,700
regulación real el

1103
00:37:44,700 --> 00:37:46,079
tipo de ambigüedad restringida que conoces

1104
00:37:46,079 --> 00:37:48,119
ambigüedad en el sentido de que conoces una

1105
00:37:48,119 --> 00:37:50,400
palabra significados múltiples o uno  estructura

1106
00:37:50,400 --> 00:37:53,040
múltiples interpretaciones, etc., claro,

1107
00:37:53,040 --> 00:37:55,200
sí, quiero decir, si crees que tienen

1108
00:37:55,200 --> 00:37:57,599
semántica, entonces creo que tienen que

1109
00:37:57,599 --> 00:37:59,640
tener un mapeo de la sintaxis a la

1110
00:37:59,640 --> 00:38:00,960
semántica,

1111
00:38:00,960 --> 00:38:03,599
um, estoy de acuerdo, no es que nadie

1112
00:38:03,599 --> 00:38:04,980
realmente entienda cómo están trabajando

1113
00:38:04,980 --> 00:38:07,560
en un nivel profundo.  cierto, así que estoy de acuerdo,

1114
00:38:07,560 --> 00:38:10,140
no es tan claro como,

1115
00:38:10,140 --> 00:38:11,940
digamos, en sintaxis generativa y

1116
00:38:11,940 --> 00:38:13,859
semántica justo donde,

1117
00:38:13,859 --> 00:38:15,180
um, sabes que escribes

1118
00:38:15,180 --> 00:38:17,880
las reglas de composición y puedes

1119
00:38:17,880 --> 00:38:19,920
derivar un significado composicional de una

1120
00:38:19,920 --> 00:38:21,480
oración de las partes componentes  o

1121
00:38:21,480 --> 00:38:23,339
algo como eso,

1122
00:38:23,339 --> 00:38:24,660
um, sabes que no es así como

1123
00:38:24,660 --> 00:38:27,060
funcionan bien, pero, um, yo

1124
00:38:27,060 --> 00:38:28,859
solo, no daría por

1125
00:38:28,859 --> 00:38:32,160
sentado que tiene que ser así, um,

1126
00:38:32,160 --> 00:38:34,619
uh, podría ser que la forma en que están

1127
00:38:34,619 --> 00:38:36,420
trabajando es en realidad la forma en que nosotros  funciona bien,

1128
00:38:36,420 --> 00:38:38,760
todo está representado en

1129
00:38:38,760 --> 00:38:40,800
un espacio vectorial de alta dimensión y

1130
00:38:40,800 --> 00:38:43,740
hay una forma complicada en la que la

1131
00:38:43,740 --> 00:38:46,200
semántica del vector se actualiza con

1132
00:38:46,200 --> 00:38:48,240
cada palabra adicional o lo que sea

1133
00:38:48,240 --> 00:38:51,480
en un flujo lingüístico,

1134
00:38:51,480 --> 00:38:53,940
pero creo que está claro

1135
00:38:53,940 --> 00:38:55,320
que tienen algunos  tipo de

1136
00:38:55,320 --> 00:38:57,660
representación de la semántica de una

1137
00:38:57,660 --> 00:38:59,099
oración como si pudieran responder

1138
00:38:59,099 --> 00:39:01,260
preguntas, por ejemplo, al menos

1139
00:39:01,260 --> 00:39:02,760
aproximadamente, quiero decir que no es

1140
00:39:02,760 --> 00:39:04,380
perfecto, pero

1141
00:39:04,380 --> 00:39:06,720
um, no es como un modelo de engrama o

1142
00:39:06,720 --> 00:39:07,740
algo correcto que realmente

1143
00:39:07,740 --> 00:39:10,500
no tiene semántica entonces

1144
00:39:10,500 --> 00:39:12,780
um, creo que son, um,

1145
00:39:12,780 --> 00:39:14,040
definitivamente

1146
00:39:14,040 --> 00:39:16,920
representan la semántica y,

1147
00:39:16,920 --> 00:39:20,040
um, sabes, actualizan eso a medida que

1148
00:39:20,040 --> 00:39:21,599
procesan el lenguaje, simplemente

1149
00:39:21,599 --> 00:39:23,400
no se parece a estas otras teorías formales,

1150
00:39:23,400 --> 00:39:24,599


1151
00:39:24,599 --> 00:39:26,400
um, y supongo que no lo veo.  ¿Por qué

1152
00:39:26,400 --> 00:39:27,720
eso es un problema, como esas otras

1153
00:39:27,720 --> 00:39:29,640
teorías formales? Podría ser simplemente que conoces

1154
00:39:29,640 --> 00:39:31,920
aproximaciones deficientes o simplemente totalmente

1155
00:39:31,920 --> 00:39:33,720
incorrectas.

1156
00:39:33,720 --> 00:39:35,520


1157
00:39:35,520 --> 00:39:37,260


1158
00:39:37,260 --> 00:39:39,359


1159
00:39:39,359 --> 00:39:41,520
de

1160
00:39:41,520 --> 00:39:42,780
estas cosas están funcionando bien, así que otra

1161
00:39:42,780 --> 00:39:45,240
forma de pensar en esto es que sabes que los LMS

1162
00:39:45,240 --> 00:39:47,760
están bien. Los LMS son algoritmos de compresión,

1163
00:39:47,760 --> 00:39:49,740
pero la comprensión del lenguaje natural

1164
00:39:49,740 --> 00:39:51,540


1165
00:39:51,540 --> 00:39:54,180


1166
00:39:54,180 --> 00:39:56,099
tiene que ver más con la

1167
00:39:56,099 --> 00:39:58,020
descompresión.  metarelaciones

1168
00:39:58,020 --> 00:39:59,579
entre conceptos que no están en los

1169
00:39:59,579 --> 00:40:01,859
datos de entrenamiento, así que algunos ejemplos que

1170
00:40:01,859 --> 00:40:03,180
Melanie Mitchell da cosas como

1171
00:40:03,180 --> 00:40:06,119
encima de ti sabes que ella está encima de nuevo,

1172
00:40:06,119 --> 00:40:08,700
eh, está encima de la caja, todo esto

1173
00:40:08,700 --> 00:40:10,140
varía según el contexto, así que hay muchos

1174
00:40:10,140 --> 00:40:11,880
otras cosas que están sucediendo bien,

1175
00:40:11,880 --> 00:40:13,260
um, y creo que discute algunos de esos

1176
00:40:13,260 --> 00:40:16,440
ejemplos en su artículo para que sepa,

1177
00:40:16,440 --> 00:40:18,119
um, pero la facultad del lenguaje todavía

1178
00:40:18,119 --> 00:40:20,940
no está al menos nuevamente bajo esta teoría del

1179
00:40:20,940 --> 00:40:22,920
lenguaje y no se trata de la

1180
00:40:22,920 --> 00:40:25,200
generación de cadenas, se trata de esta forma  lo que significa

1181
00:40:25,200 --> 00:40:27,540
máquina de emparejamiento, así que a veces esto en la

1182
00:40:27,540 --> 00:40:29,400
tradición genitiva incluso piensa que todo lo que hay en la

1183
00:40:29,400 --> 00:40:31,859
semántica es justo y correcto, entonces la

1184
00:40:31,859 --> 00:40:33,540
conjuntiva de Paul petrovsky está ahí,

1185
00:40:33,540 --> 00:40:36,000
algo es que la semántica humana es justa y

1186
00:40:36,000 --> 00:40:37,500
eso es todo,

1187
00:40:37,500 --> 00:40:39,900
um, que de nuevo es muy simple elegante

1188
00:40:39,900 --> 00:40:42,000
es es es es interpretable  es

1189
00:40:42,000 --> 00:40:43,920
compatible con muchas de las cosas que

1190
00:40:43,920 --> 00:40:46,320
sabes o tal vez están pasando por tu

1191
00:40:46,320 --> 00:40:47,940
cuello de las palabras, pero independientemente de que

1192
00:40:47,940 --> 00:40:49,859
aún sepas que el lenguaje natural es

1193
00:40:49,859 --> 00:40:51,540
aún más compositivo,

1194
00:40:51,540 --> 00:40:54,060
cosas como, eh, sabes

1195
00:40:54,060 --> 00:40:55,260
lenguajes formales solo para hacer una

1196
00:40:55,260 --> 00:40:56,820
distinción clara eso es  se han hecho, tienen una

1197
00:40:56,820 --> 00:40:58,260
composición de estructura mucho más rica,

1198
00:40:58,260 --> 00:41:00,839
hay más cosas sucediendo, eh, tal vez, así que

1199
00:41:00,839 --> 00:41:02,160
se ha señalado antes que

1200
00:41:02,160 --> 00:41:03,480
sabes cosas como mecanismos de máquinas basados ​​​​en la atención

1201
00:41:03,480 --> 00:41:05,640
y Transformers,

1202
00:41:05,640 --> 00:41:07,800
um, permiten combinaciones de

1203
00:41:07,800 --> 00:41:10,320
enlaces de token discretos, lo que es más aproximado

1204
00:41:10,320 --> 00:41:12,359
a una fusión  como operador que simple

1205
00:41:12,359 --> 00:41:14,640
multiplicación de matriz recurrente

1206
00:41:14,640 --> 00:41:16,140
um, pero conoce el tema de la

1207
00:41:16,140 --> 00:41:17,460
ramificación binaria gobierno de ramificación binaria

1208
00:41:17,460 --> 00:41:19,260
solo para elegir otro ejemplo aquí para

1209
00:41:19,260 --> 00:41:20,640
hablar sobre el significado completo regulación

1210
00:41:20,640 --> 00:41:23,640
un principio imagen de ramificación binaria es

1211
00:41:23,640 --> 00:41:24,839
una pregunta interesante pero la

1212
00:41:24,839 --> 00:41:26,700
gramática de la geometría siempre ha estado abierta a

1213
00:41:26,700 --> 00:41:28,740
diferentes  Orígenes y ubicaciones de esta

1214
00:41:28,740 --> 00:41:30,660
aparente restricción en la

1215
00:41:30,660 --> 00:41:31,800
computación sintética, como de dónde

1216
00:41:31,800 --> 00:41:33,540
proviene, tal vez sea una condición para la

1217
00:41:33,540 --> 00:41:34,980
fusión, tal vez sea impuesta por un

1218
00:41:34,980 --> 00:41:37,079
sistema fluido, tal vez sea una especie de Prioridad, ya

1219
00:41:37,079 --> 00:41:39,060
sabes, quién sabe y, de hecho, es una

1220
00:41:39,060 --> 00:41:40,500
gramática generativa de trabajo más reciente.

1221
00:41:40,500 --> 00:41:43,859
ha tratado de fundamentar y eliminar todas las

1222
00:41:43,859 --> 00:41:46,440
suposiciones teóricas del matrimonio, ¿

1223
00:41:46,440 --> 00:41:47,700
verdad? Quizás la teoría de conjuntos no sea la mejor

1224
00:41:47,700 --> 00:41:48,780
manera de modelar.

1225
00:41:48,780 --> 00:41:50,339
La gramática generativa. Quizás las

1226
00:41:50,339 --> 00:41:51,540
cuentas lógicas de María sean más apropiadas.

1227
00:41:51,540 --> 00:41:53,520
Hay muchas otras ideas recientes

1228
00:41:53,520 --> 00:41:55,020
que son todas compatibles con  el

1229
00:41:55,020 --> 00:41:57,540
con el enfoque de chomsky correcto, de hecho,

1230
00:41:57,540 --> 00:41:58,680
sí, una de las cosas que

1231
00:41:58,680 --> 00:42:00,060
más le gusta a Trump es cuando

1232
00:42:00,060 --> 00:42:01,380
demuestra que está equivocado,

1233
00:42:01,380 --> 00:42:03,720


1234
00:42:03,720 --> 00:42:06,300


1235
00:42:06,300 --> 00:42:08,640


1236
00:42:08,640 --> 00:42:11,839
correcto  campo vibrante las personas que son

1237
00:42:11,839 --> 00:42:15,119
Bornstein, conoces a petrovsky uh uh

1238
00:42:15,119 --> 00:42:17,520
hajipura, no están de acuerdo de

1239
00:42:17,520 --> 00:42:19,380
manera fundamental con mucho de lo que diría la corriente principal

1240
00:42:19,380 --> 00:42:20,940
de la gramática química, pero

1241
00:42:20,940 --> 00:42:21,960
aún hay más posibilidades de

1242
00:42:21,960 --> 00:42:24,240
desacuerdo, pero aún es compatible

1243
00:42:24,240 --> 00:42:26,400
con establecer las suposiciones básicas correctas, así que

1244
00:42:26,400 --> 00:42:27,599
muchas  David, solo uso, por ejemplo, una

1245
00:42:27,599 --> 00:42:29,400
especie de desviación en este aspecto central,

1246
00:42:29,400 --> 00:42:31,920
pero todavía está tratando de fundamentar estas

1247
00:42:31,920 --> 00:42:33,240
intuiciones en diferentes sistemas formales,

1248
00:42:33,240 --> 00:42:34,500


1249
00:42:34,500 --> 00:42:36,000
um, así que sabes

1250
00:42:36,000 --> 00:42:38,099
que es como que

1251
00:42:38,099 --> 00:42:40,320
quiero que vuelvas a pensar en um,

1252
00:42:40,320 --> 00:42:42,240
mencioné a Mitchell, así que Michelin

1253
00:42:42,240 --> 00:42:44,579
Bowers, eh  2020 tienen esta

1254
00:42:44,579 --> 00:42:47,040
lista de prueba en papel de redes recurrentes

1255
00:42:47,040 --> 00:42:48,359
que, curiosamente, creo que usted podría

1256
00:42:48,359 --> 00:42:49,859
conocer bien, por lo que es un muy buen

1257
00:42:49,859 --> 00:42:51,060
ejemplo solo para llegar al meollo

1258
00:42:51,060 --> 00:42:53,099
del problema, por lo que se

1259
00:42:53,099 --> 00:42:54,780
ha demostrado que las redes neuronales recurrentes modelan con precisión lo que

1260
00:42:54,780 --> 00:42:56,520
no sabes.  -Concordancia numérica de verbos,

1261
00:42:56,520 --> 00:42:58,020
pero Mitchell y Barrow demostraron que

1262
00:42:58,020 --> 00:43:00,060
estas redes también generarán una

1263
00:43:00,060 --> 00:43:01,680
concordancia numérica con estructuras de oraciones no naturales,

1264
00:43:01,680 --> 00:43:03,359
por lo que estructuras que no se

1265
00:43:03,359 --> 00:43:04,859
encuentran en el lenguaje natural y que

1266
00:43:04,859 --> 00:43:06,540
los humanos tienen dificultades para procesar correctamente,

1267
00:43:06,540 --> 00:43:09,359
por lo que el modo de aprendizaje para rnns es al

1268
00:43:09,359 --> 00:43:11,880
menos  para rnn es positivamente distinto

1269
00:43:11,880 --> 00:43:14,339
de infant, conoces al homo sapiens infantil,

1270
00:43:14,339 --> 00:43:16,260
así que la historia es que Mitchell y

1271
00:43:16,260 --> 00:43:18,359
Bowers muestran que, si bien el modelo lstl

1272
00:43:18,359 --> 00:43:20,040
tiene una buena representación de singular

1273
00:43:20,040 --> 00:43:22,140
versus plural para oraciones individuales,

1274
00:43:22,140 --> 00:43:24,359
no hay una generalización correcta

1275
00:43:24,359 --> 00:43:25,800
que puedan representar a

1276
00:43:25,800 --> 00:43:27,359
nivel individual  por lo tanto, el modelo no tiene una

1277
00:43:27,359 --> 00:43:28,859
representación del número como una

1278
00:43:28,859 --> 00:43:31,200
abstracción, qué número es solo

1279
00:43:31,200 --> 00:43:34,020
instancias concretas de base singular plural

1280
00:43:34,020 --> 00:43:35,400
um, por lo que predecir con éxito el

1281
00:43:35,400 --> 00:43:38,579
comportamiento del lenguaje a través de LM o predecir con éxito las

1282
00:43:38,579 --> 00:43:40,800
respuestas neuronales de una

1283
00:43:40,800 --> 00:43:42,480
manera similar es obviamente excelente y tal vez podamos

1284
00:43:42,480 --> 00:43:43,920
entrar  ese problema más adelante, pero

1285
00:43:43,920 --> 00:43:45,240
solo hay un lado de la moneda aquí, el

1286
00:43:45,240 --> 00:43:47,099
otro lado de la moneda explica por qué

1287
00:43:47,099 --> 00:43:48,780
este tipo de comportamiento y no algún otro

1288
00:43:48,780 --> 00:43:50,339
comportamiento, por qué esta estructura no soy

1289
00:43:50,339 --> 00:43:52,740
similar y eso es quizás lo más de chomsky

1290
00:43:52,740 --> 00:43:55,380
y como tú lo conoces más

1291
00:43:55,380 --> 00:43:56,760
Punto importante realmente por qué este no es otro

1292
00:43:56,760 --> 00:43:59,400
sistema, por lo que la teoría lingüística

1293
00:43:59,400 --> 00:44:00,720
te da eso o el comienzo de la moneda

1294
00:44:00,720 --> 00:44:02,760
correctamente, mientras que LM realmente está terminado, por lo que el

1295
00:44:02,760 --> 00:44:03,839
papel avergonzado de Mitchell hace

1296
00:44:03,839 --> 00:44:05,819
algo así, lo hace

1297
00:44:05,819 --> 00:44:09,420
bien, sí, así que toma um Yael Le  crets

1298
00:44:09,420 --> 00:44:11,400
y stanislash the Haynes eran de 2019,

1299
00:44:11,400 --> 00:44:13,319
correcto, observaron la concordancia numérica en

1300
00:44:13,319 --> 00:44:15,480
un lstm y encontraron dos unidades especializadas

1301
00:44:15,480 --> 00:44:17,640
que codificaban la concordancia numérica, pero la

1302
00:44:17,640 --> 00:44:19,020
contribución general al rendimiento fue

1303
00:44:19,020 --> 00:44:21,839
baja y luego, en 2021, sí, las correcciones

1304
00:44:21,839 --> 00:44:24,000
tenían este documento donde muestran que

1305
00:44:24,000 --> 00:44:26,040
um en  su modelo de lenguaje neuronal

1306
00:44:26,040 --> 00:44:28,079
no logró un procesamiento recursivo genuino

1307
00:44:28,079 --> 00:44:30,540
del marcado de género de acuerdo de largo alcance anidado

1308
00:44:30,540 --> 00:44:32,160
en italiano.  ¿

1309
00:44:32,160 --> 00:44:34,020


1310
00:44:34,020 --> 00:44:35,819


1311
00:44:35,819 --> 00:44:37,380


1312
00:44:37,380 --> 00:44:39,960
Es

1313
00:44:39,960 --> 00:44:41,280
el mapeo correcto? ¿Es el

1314
00:44:41,280 --> 00:44:42,900
tipo correcto de jerarquía? Descubrieron que los

1315
00:44:42,900 --> 00:44:45,119
modelos basados ​​en lstn podían aterrizar un acuerdo web sujeto

1316
00:44:45,119 --> 00:44:47,220
en lapsos cortos con un grado de

1317
00:44:47,220 --> 00:44:49,260
incrustación, pero fallaron en algunas dependencias más largas

1318
00:44:49,260 --> 00:44:51,359
y en el artículo más reciente

1319
00:44:51,359 --> 00:44:53,700
uh La crepe satell con  la mano

1320
00:44:53,700 --> 00:44:56,760
y demostraron que evaluaron el

1321
00:44:56,760 --> 00:45:00,180
Transformer LMS moderno, incluido gpt2 XL, en la

1322
00:45:00,180 --> 00:45:01,980
misma tarea y que los Transformers se desempeñan

1323
00:45:01,980 --> 00:45:04,260
de manera más similar a los humanos que LSM

1324
00:45:04,260 --> 00:45:06,300
y se desempeñaron por encima de la transferencia en general, pero

1325
00:45:06,300 --> 00:45:08,040
aún se desempeñan por debajo de la probabilidad en una

1326
00:45:08,040 --> 00:45:09,660
condición clave que es, como

1327
00:45:09,660 --> 00:45:11,099
mencioné, el  múltiples incrustaciones en una de las

1328
00:45:11,099 --> 00:45:13,020
escrituras difíciles y, por lo tanto, la razón

1329
00:45:13,020 --> 00:45:14,400
por la que mencioné estos estudios es porque

1330
00:45:14,400 --> 00:45:17,040
saben que no se trata solo de explorar los

1331
00:45:17,040 --> 00:45:18,540
límites de OMS, que es una pregunta interesante,

1332
00:45:18,540 --> 00:45:19,500


1333
00:45:19,500 --> 00:45:21,540
pero considere el trabajo de personas como Neil

1334
00:45:21,540 --> 00:45:24,180
Smith en UCL correcto y él trabajó en

1335
00:45:24,180 --> 00:45:26,579
los años 90 con un sabio políglota y

1336
00:45:26,579 --> 00:45:28,740
controles neurotípicos comparándolos, así que

1337
00:45:28,740 --> 00:45:30,540
investigó el aprendizaje

1338
00:45:30,540 --> 00:45:32,520
de un segundo idioma de un lenguaje artificial que contiene

1339
00:45:32,520 --> 00:45:34,500


1340
00:45:34,500 --> 00:45:35,880
estructuras gráficas tanto naturales como no naturales como el papel del virus Michelin,

1341
00:45:35,880 --> 00:45:37,079
todo el marco es natural

1342
00:45:37,079 --> 00:45:39,119
versus antinatural y encontraron que

1343
00:45:39,119 --> 00:45:41,000
mientras tanto el sabio

1344
00:45:41,000 --> 00:45:43,319
como el  los controles podrían dominar los

1345
00:45:43,319 --> 00:45:45,480
aspectos lingüísticamente naturales, solo los

1346
00:45:45,480 --> 00:45:46,920
controles podrían eventualmente manejar los

1347
00:45:46,920 --> 00:45:48,660
fenómenos no naturales dependientes de la estructura

1348
00:45:48,660 --> 00:45:50,460
y ninguno de ellos podría dominar los

1349
00:45:50,460 --> 00:45:52,560
aspectos independientes de la estructura, por lo que algunas

1350
00:45:52,560 --> 00:45:53,880
reglas extrañas donde es como si supieras que

1351
00:45:53,880 --> 00:45:55,440
marcas el énfasis en la tercera palabra de

1352
00:45:55,440 --> 00:45:56,940
la oración cosas  así,

1353
00:45:56,940 --> 00:45:58,740
argumentan que las habilidades de Christopher se

1354
00:45:58,740 --> 00:46:00,900
deben por completo a sus facultades lingüísticas intactas,

1355
00:46:00,900 --> 00:46:03,480
pero los controles podrían emplear

1356
00:46:03,480 --> 00:46:05,579
más recursos cognitivos generales de dominio,

1357
00:46:05,579 --> 00:46:07,319
como el control de la atención,

1358
00:46:07,319 --> 00:46:09,900
etc., por lo que podrían lidiar

1359
00:46:09,900 --> 00:46:11,520
con procesos difíciles,

1360
00:46:11,520 --> 00:46:13,319
pero acabo de mencionar que conoces un

1361
00:46:13,319 --> 00:46:16,079
hace un minuto que el lstm en el

1362
00:46:16,079 --> 00:46:18,359
documento avergonzado de Mitchell se acerca a las estructuras naturales y

1363
00:46:18,359 --> 00:46:19,920
no naturales de la

1364
00:46:19,920 --> 00:46:22,319
misma manera, por lo que no es un

1365
00:46:22,319 --> 00:46:24,240
modelo psicológicamente plausible, diría yo

1366
00:46:24,240 --> 00:46:26,400
y para lo que sea que estén haciendo los humanos

1367
00:46:26,400 --> 00:46:28,200
y observaciones similares se pueden aplicar a

1368
00:46:28,200 --> 00:46:30,060
los límites de  Los modelos de transformadores en el

1369
00:46:30,060 --> 00:46:31,920
trabajo de La creta y todos estos temas son

1370
00:46:31,920 --> 00:46:33,780
como si se quedaran con

1371
00:46:33,780 --> 00:46:35,819
nosotros hasta el presente, por lo que

1372
00:46:35,819 --> 00:46:37,560
otro de los artículos recientes de Talin que

1373
00:46:37,560 --> 00:46:39,240
publicó hace unas semanas sobre el

1374
00:46:39,240 --> 00:46:41,880
discurso dirigido por niños mostró que um  lstms y

1375
00:46:41,880 --> 00:46:43,740
Transformers se limitan a

1376
00:46:43,740 --> 00:46:46,380
cantidades ecológicamente plausibles de datos generalizados, ya que

1377
00:46:46,380 --> 00:46:47,640
mencioné las reglas lineales para el

1378
00:46:47,640 --> 00:46:49,920
derecho en inglés en lugar de las reglas abstractas y,

1379
00:46:49,920 --> 00:46:51,780
de hecho, un trabajo más reciente del

1380
00:46:51,780 --> 00:46:54,599
laboratorio de Linton la semana pasada mirando, bueno, el

1381
00:46:54,599 --> 00:46:56,220
año pasado, debería decir que muestra que mirando

1382
00:46:56,220 --> 00:46:58,160
Garden  la sorpresa de las rutas no explica

1383
00:46:58,160 --> 00:47:01,319
la dificultad de desambiguación sintáctica, ¿verdad? Las

1384
00:47:01,319 --> 00:47:02,280


1385
00:47:02,280 --> 00:47:03,900
sorpresas subestimarán el tamaño

1386
00:47:03,900 --> 00:47:05,400
del efecto Garden Path en todas las

1387
00:47:05,400 --> 00:47:06,780
construcciones y esto llega a este

1388
00:47:06,780 --> 00:47:08,099
problema que mencionaste antes.

1389
00:47:08,099 --> 00:47:10,140


1390
00:47:10,140 --> 00:47:11,520


1391
00:47:11,520 --> 00:47:12,960
otros, es una especie de

1392
00:47:12,960 --> 00:47:14,819
problema que no es un tributo, está muy

1393
00:47:14,819 --> 00:47:16,800
abierto a la discusión, no se ha

1394
00:47:16,800 --> 00:47:18,720
resuelto todavía, pero Linton demostró

1395
00:47:18,720 --> 00:47:20,640
que los efectos de Garden Path son mucho

1396
00:47:20,640 --> 00:47:21,720
más difíciles de lo que esperaría

1397
00:47:21,720 --> 00:47:24,359
de mí.  imprevisibilidad, entonces otra forma

1398
00:47:24,359 --> 00:47:26,160
de expresar este argumento

1399
00:47:26,160 --> 00:47:29,160
es la cita de un argumento reciente

1400
00:47:29,160 --> 00:47:30,660
con chomsky para llegar a esta

1401
00:47:30,660 --> 00:47:32,940
base natural, un problema antinatural, dice, supongamos que

1402
00:47:32,940 --> 00:47:34,560
tenemos una tabla periódica ampliada que

1403
00:47:34,560 --> 00:47:36,180
incluye todos los elementos que existen

1404
00:47:36,180 --> 00:47:38,819
o los elementos que posiblemente pueden  existen

1405
00:47:38,819 --> 00:47:40,740
y todos los elementos que posiblemente no pueden

1406
00:47:40,740 --> 00:47:42,660
existir y digamos que tienes

1407
00:47:42,660 --> 00:47:44,880
algún modelo, eh, algún modelo artificial que

1408
00:47:44,880 --> 00:47:46,560
no distingue entre estas tres

1409
00:47:46,560 --> 00:47:48,780
categorías, sea lo que sea lo que esté haciendo este modelo,

1410
00:47:48,780 --> 00:47:50,640
no nos está ayudando a entender la química, ¿verdad? Está

1411
00:47:50,640 --> 00:47:52,020
haciendo otra cosa,

1412
00:47:52,020 --> 00:47:53,940
está haciendo algo por  claro, pero

1413
00:47:53,940 --> 00:47:55,020
si tiene que entender o no la

1414
00:47:55,020 --> 00:47:57,180
química es algo separado y sé

1415
00:47:57,180 --> 00:47:58,560
que ha dicho en respuesta a

1416
00:47:58,560 --> 00:47:59,579
algunos de estos estudios, creo que ha

1417
00:47:59,579 --> 00:48:02,400
dicho que sabe y para demostrar

1418
00:48:02,400 --> 00:48:03,540
que es probable que algo sea

1419
00:48:03,540 --> 00:48:04,920
imposible en algún lugar.  en su artículo,

1420
00:48:04,920 --> 00:48:06,240
creo que dice

1421
00:48:06,240 --> 00:48:07,859
um para demostrar que algo es

1422
00:48:07,859 --> 00:48:09,720
imposible con el equilibrio normal de

1423
00:48:09,720 --> 00:48:12,300
Política sobre falsos positivos, debe

1424
00:48:12,300 --> 00:48:13,440
demostrar que debe observar algo así como

1425
00:48:13,440 --> 00:48:15,780
500 idiomas muestreados de forma independiente, por lo que

1426
00:48:15,780 --> 00:48:17,880
cita esto en el artículo correcto, um,

1427
00:48:17,880 --> 00:48:19,020
que usted  probablemente no pueda hacerlo,

1428
00:48:19,020 --> 00:48:20,579
simplemente no, no es algo factible de hacer,

1429
00:48:20,579 --> 00:48:23,880
por lo que saben que no lo soy, no estoy muy seguro de

1430
00:48:23,880 --> 00:48:25,800
que esto realmente refute el

1431
00:48:25,800 --> 00:48:27,119
argumento principal que estoy presentando aquí,

1432
00:48:27,119 --> 00:48:29,040
porque personas como Michelin Bowers están

1433
00:48:29,040 --> 00:48:30,839
haciendo  un argumento sobre la imposibilidad

1434
00:48:30,839 --> 00:48:32,220
en principio, no

1435
00:48:32,220 --> 00:48:33,599
en algún tipo de sentido extensivo, ya

1436
00:48:33,599 --> 00:48:34,980
sabes, como buscar en los

1437
00:48:34,980 --> 00:48:37,440
idiomas del mundo para ver y probar en

1438
00:48:37,440 --> 00:48:38,579
cada idioma que es

1439
00:48:38,579 --> 00:48:40,260
imposible, cierto, es un

1440
00:48:40,260 --> 00:48:41,280
argumento diferente si es

1441
00:48:41,280 --> 00:48:43,740
imposible en algún idioma aleatorio  en

1442
00:48:43,740 --> 00:48:45,180
el Amazonas en comparación con algo realmente

1443
00:48:45,180 --> 00:48:47,220
imposible basado en los principios de lo

1444
00:48:47,220 --> 00:48:48,420
que el sistema lingüístico realmente está

1445
00:48:48,420 --> 00:48:50,160
haciendo, como lo que puede hacer, así que

1446
00:48:50,160 --> 00:48:53,339
solo diría que sí, creo que

1447
00:48:53,339 --> 00:48:55,980
ese punto es que en realidad no

1448
00:48:55,980 --> 00:48:58,560
sabes lo que tipológicamente no es  Es posible,

1449
00:48:58,560 --> 00:49:00,480
cierto, a algunas personas les gusta decir cosas

1450
00:49:00,480 --> 00:49:02,520
como que sabes que no hay un lenguaje que

1451
00:49:02,520 --> 00:49:04,859
haga X, por lo tanto, tenemos que incorporar esa

1452
00:49:04,859 --> 00:49:06,960
restricción en nuestros

1453
00:49:06,960 --> 00:49:09,119
modelos estadísticos, pero si no está

1454
00:49:09,119 --> 00:49:11,220
justificado estadísticamente que no hay un

1455
00:49:11,220 --> 00:49:13,079
lenguaje que haga X correctamente si has

1456
00:49:13,079 --> 00:49:15,119
solo miré 20 o 20

1457
00:49:15,119 --> 00:49:16,920
idiomas europeos o algo así, quiero decir

1458
00:49:16,920 --> 00:49:19,020
que no es

1459
00:49:19,020 --> 00:49:22,380
um uh, eso no debería motivar a hacer

1460
00:49:22,380 --> 00:49:24,599
nada con los modelos, ¿

1461
00:49:24,599 --> 00:49:26,280


1462
00:49:26,280 --> 00:49:28,200


1463
00:49:28,200 --> 00:49:28,980


1464
00:49:28,980 --> 00:49:30,180


1465
00:49:30,180 --> 00:49:32,760
verdad um uh si es si no es un universal estadísticamente justificado?  sé que

1466
00:49:32,760 --> 00:49:33,960
tienes toda la razón, pero eso solo

1467
00:49:33,960 --> 00:49:35,339
se aplica de manera más general a las

1468
00:49:35,339 --> 00:49:36,960
ciencias sociales y las ciencias psicológicas,

1469
00:49:36,960 --> 00:49:39,180
como tipológicamente, sí, es muy

1470
00:49:39,180 --> 00:49:40,380
difícil establecer estas cosas

1471
00:49:40,380 --> 00:49:43,140
correctamente, así que supongo que tú, supongo que eres un tipo

1472
00:49:43,140 --> 00:49:44,640
obsoleto, eres un poco  usted está

1473
00:49:44,640 --> 00:49:47,339
diciendo que es muy difícil demostrar que la afirmación fuerte es

1474
00:49:47,339 --> 00:49:49,859
correcta,

1475
00:49:49,859 --> 00:49:52,619
ya que no hay un idioma que tenga X,

1476
00:49:52,619 --> 00:49:54,480


1477
00:49:54,480 --> 00:49:56,339


1478
00:49:56,339 --> 00:49:58,800
creo que la afirmación fuerte de que algo no está permitido en el lenguaje natural es muy, muy difícil de probar,

1479
00:49:58,800 --> 00:49:59,880
um

1480
00:49:59,880 --> 00:50:01,980
um y sabes que creo  que ha

1481
00:50:01,980 --> 00:50:05,460
habido, eh, muchos de ustedes conocen

1482
00:50:05,460 --> 00:50:08,460
intentos sólidos, ha habido muchas

1483
00:50:08,460 --> 00:50:10,380
afirmaciones sólidas de,

1484
00:50:10,380 --> 00:50:12,720
eh, a menudo desde la sintaxis generativa

1485
00:50:12,720 --> 00:50:16,800
sobre lo que hacen todos los lenguajes,

1486
00:50:16,800 --> 00:50:19,140
y creo que saben que la gente ha

1487
00:50:19,140 --> 00:50:21,119
sido muy buena para encontrar tipos de

1488
00:50:21,119 --> 00:50:22,740
contraejemplos  a muchas de esas

1489
00:50:22,740 --> 00:50:24,720
cosas, cito este artículo de Evans

1490
00:50:24,720 --> 00:50:26,579
y Levinson,

1491
00:50:26,579 --> 00:50:28,500
um, que en realidad sabes que había escuchado

1492
00:50:28,500 --> 00:50:30,660
durante años sobre cómo ningún lenguaje hace X

1493
00:50:30,660 --> 00:50:32,160
y eso es lo que estamos usando para

1494
00:50:32,160 --> 00:50:33,599
construir nuestras teorías y que Evans

1495
00:50:33,599 --> 00:50:35,460
y Levin  paper Evans y Levinson

1496
00:50:35,460 --> 00:50:37,859
paper realmente me cambiaron de opinión

1497
00:50:37,859 --> 00:50:40,680
sobre este derecho de que el lenguaje es

1498
00:50:40,680 --> 00:50:43,260
en realidad mucho más uh diverso de lo que

1499
00:50:43,260 --> 00:50:44,940
creo que la

1500
00:50:44,940 --> 00:50:47,760
mayoría de los sintácticos, ¿sabes? Intenta

1501
00:50:47,760 --> 00:50:50,700
construir teorías para algo así que

1502
00:50:50,700 --> 00:50:53,579
um um sabes que creo que vamos  volviendo

1503
00:50:53,579 --> 00:50:54,839
al principio de lo que

1504
00:50:54,839 --> 00:50:57,660
dijiste, creo que estaríamos de acuerdo en que

1505
00:50:57,660 --> 00:50:59,880
necesitas arquitecturas de lenguaje que

1506
00:50:59,880 --> 00:51:01,619
aprendan las cosas que los niños aprenden y

1507
00:51:01,619 --> 00:51:03,660
aprendieron de los datos que aprenden y es

1508
00:51:03,660 --> 00:51:05,940


1509
00:51:05,940 --> 00:51:09,000
poco probable que esas arquitecturas sean cosas  como lstms o

1510
00:51:09,000 --> 00:51:10,559
conoce redes recurrentes simples o lo que sea

1511
00:51:10,559 --> 00:51:12,300
correcto como

1512
00:51:12,300 --> 00:51:14,400
um creo que todo ese trabajo es muy

1513
00:51:14,400 --> 00:51:16,680
útil para perfeccionar la

1514
00:51:16,680 --> 00:51:19,200
arquitectura correcta

1515
00:51:19,200 --> 00:51:20,460
um

1516
00:51:20,460 --> 00:51:21,420
um

1517
00:51:21,420 --> 00:51:23,819
uh así que solo estoy tratando de recordar todo

1518
00:51:23,819 --> 00:51:25,200
de todo  los puntos que estabas planteando,

1519
00:51:25,200 --> 00:51:26,700
oh,

1520
00:51:26,700 --> 00:51:29,160
sí, pero creo que hay

1521
00:51:29,160 --> 00:51:31,500
una especie de reverso de esto, que es que,

1522
00:51:31,500 --> 00:51:32,760


1523
00:51:32,760 --> 00:51:34,380
creo que el espacio de las cosas que

1524
00:51:34,380 --> 00:51:37,619
la gente puede aprender en realidad está

1525
00:51:37,619 --> 00:51:39,599
subestimado, como si existiera este

1526
00:51:39,599 --> 00:51:41,760
sesgo.  a decir que sabes que la gente no puede

1527
00:51:41,760 --> 00:51:43,800
aprender x y y z

1528
00:51:43,800 --> 00:51:46,020
um pero la gente uh al menos fuera del

1529
00:51:46,020 --> 00:51:47,460
lenguaje tiene esta

1530
00:51:47,460 --> 00:51:49,680
habilidad realmente notable de aprender diferentes

1531
00:51:49,680 --> 00:51:51,240
tipos de patrones como los

1532
00:51:51,240 --> 00:51:53,160
patrones que encuentras en la música o las

1533
00:51:53,160 --> 00:51:55,619
matemáticas, por ejemplo

1534
00:51:55,619 --> 00:51:57,839
um uh  podemos aprender tipos sofisticados

1535
00:51:57,839 --> 00:52:00,240
de algoritmos, ¿verdad? Podemos aprender a

1536
00:52:00,240 --> 00:52:03,119
volar un transbordador espacial o a

1537
00:52:03,119 --> 00:52:05,760
atar nudos para escalar rocas o

1538
00:52:05,760 --> 00:52:07,319
lo que sea, así como hay todo

1539
00:52:07,319 --> 00:52:09,540
tipo de conocimiento procedimental y

1540
00:52:09,540 --> 00:52:11,280
algorítmico que es

1541
00:52:11,280 --> 00:52:13,440
estructural que eso  las personas son capaces de

1542
00:52:13,440 --> 00:52:16,680
adquirir y creo que esa uh

1543
00:52:16,680 --> 00:52:19,680
noción uh con mucha razón motiva la

1544
00:52:19,680 --> 00:52:21,359
búsqueda de sistemas de aprendizaje que puedan

1545
00:52:21,359 --> 00:52:24,059
funcionar en espacios bastante ilimitados,

1546
00:52:24,059 --> 00:52:26,160
así que

1547
00:52:26,160 --> 00:52:28,859
um uh sabes que podrías decir

1548
00:52:28,859 --> 00:52:30,119
que está bien, bueno, el lenguaje es diferente

1549
00:52:30,119 --> 00:52:33,420
porque el lenguaje es  un espacio restringido

1550
00:52:33,420 --> 00:52:35,040
um uh y podría ser cierto que ese

1551
00:52:35,040 --> 00:52:36,540
idioma está restringido, pero también podría

1552
00:52:36,540 --> 00:52:37,859
ser cierto que las cosas que vemos en el

1553
00:52:37,859 --> 00:52:39,960
lenguaje provienen de otras fuentes, ¿verdad?

1554
00:52:39,960 --> 00:52:42,180
Podría ser que uh el lenguaje sea

1555
00:52:42,180 --> 00:52:44,099
especialmente pragmático, por ejemplo,

1556
00:52:44,099 --> 00:52:47,040
en comparación con uh la música o las matemáticas.

1557
00:52:47,040 --> 00:52:48,720
correcto y ese tipo de restricciones pragmáticas

1558
00:52:48,720 --> 00:52:50,040


1559
00:52:50,040 --> 00:52:51,480
um son las cosas que restringen la

1560
00:52:51,480 --> 00:52:53,400
forma del lenguaje correcto o el lenguaje es

1561
00:52:53,400 --> 00:52:54,660
comunicativo, probablemente sea más

1562
00:52:54,660 --> 00:52:56,760
comunicativo que la música, por

1563
00:52:56,760 --> 00:52:58,440
ejemplo, y eso podría restringir la

1564
00:52:58,440 --> 00:53:01,140
forma de las cosas, así que quiero decir, como saben,

1565
00:53:01,140 --> 00:53:02,700
esto es muy  viejo debate en

1566
00:53:02,700 --> 00:53:05,520
lingüística sobre de

1567
00:53:05,520 --> 00:53:07,140
dónde vienen las propiedades del lenguaje natural

1568
00:53:07,140 --> 00:53:09,059


1569
00:53:09,059 --> 00:53:11,160
y supongo que lo que estoy tratando de decir

1570
00:53:11,160 --> 00:53:12,660
es que hay un tipo de perspectiva

1571
00:53:12,660 --> 00:53:15,119
en la que miras todas las cosas que

1572
00:53:15,119 --> 00:53:16,980
los humanos pueden  incluso fuera del lenguaje,

1573
00:53:16,980 --> 00:53:18,599
todas las ricas estructuras y

1574
00:53:18,599 --> 00:53:20,819
algoritmos y procesos fueron capaces de

1575
00:53:20,819 --> 00:53:23,760
aprender e internalizar y

1576
00:53:23,760 --> 00:53:25,559
dices, está bien, tal vez el lenguaje es así

1577
00:53:25,559 --> 00:53:27,420
y luego sí, el lenguaje también tiene algunas de

1578
00:53:27,420 --> 00:53:29,700
estas otras pequeñas propiedades divertidas,

1579
00:53:29,700 --> 00:53:31,380
pero sabes, tal vez esas  provienen de

1580
00:53:31,380 --> 00:53:34,020
otras partes de donde proviene el

1581
00:53:34,020 --> 00:53:36,720
lenguaje, ¿sabes?,

1582
00:53:36,720 --> 00:53:38,400
tenemos un razonamiento pragmático bastante sofisticado, lo

1583
00:53:38,400 --> 00:53:40,800


1584
00:53:40,800 --> 00:53:42,720
estamos usando para lograr ciertos

1585
00:53:42,720 --> 00:53:45,359
fines comunicativos, puedes encontrar todo

1586
00:53:45,359 --> 00:53:47,160
tipo de características comunicativas,

1587
00:53:47,160 --> 00:53:49,559
eh, dentro del  sistema lingüístico en sí mismo,

1588
00:53:49,559 --> 00:53:51,180
y tal vez algunas de estas otras

1589
00:53:51,180 --> 00:53:53,819
propiedades son propiedades que tienen

1590
00:53:53,819 --> 00:53:55,619
algún otro origen,

1591
00:53:55,619 --> 00:53:57,059
um, y esa vista, creo, podría estar

1592
00:53:57,059 --> 00:53:59,579
equivocada, pero es una que,

1593
00:53:59,579 --> 00:54:01,800
um, creo que debe analizarse para ver

1594
00:54:01,800 --> 00:54:04,200
si está mal, como  Creo que ha

1595
00:54:04,200 --> 00:54:05,400
sido

1596
00:54:05,400 --> 00:54:10,020
um uh un poco descartado um por gran

1597
00:54:10,020 --> 00:54:12,900
parte de los lingüistas, ¿sabes? He

1598
00:54:12,900 --> 00:54:14,579
escuchado a la gente decir cosas como oh,

1599
00:54:14,579 --> 00:54:15,720
bueno, la comunicación realmente no

1600
00:54:15,720 --> 00:54:17,760
explica nada sobre el idioma, ¿verdad?

1601
00:54:17,760 --> 00:54:20,040
y lo que quieren decir a menudo es que no  No

1602
00:54:20,040 --> 00:54:22,200
explico como las limitaciones particulares de la isla

1603
00:54:22,200 --> 00:54:23,760
o algo en lo que están

1604
00:54:23,760 --> 00:54:25,319
trabajando correctamente, pero

1605
00:54:25,319 --> 00:54:26,700
hay todo tipo de otras cosas en el

1606
00:54:26,700 --> 00:54:28,319
lenguaje que las presiones comunicativas

1607
00:54:28,319 --> 00:54:30,359
probablemente sí explican,

1608
00:54:30,359 --> 00:54:31,500
um,

1609
00:54:31,500 --> 00:54:33,540
um, supongo que mi tono siempre es para una

1610
00:54:33,540 --> 00:54:36,359
especie de amplitud.  en amplitud de términos en

1611
00:54:36,359 --> 00:54:39,119
consideración de las fuerzas que

1612
00:54:39,119 --> 00:54:41,460
pueden dar forma al lenguaje y no tener que

1613
00:54:41,460 --> 00:54:43,680
ponerlo todo en algún tipo de

1614
00:54:43,680 --> 00:54:45,359
restricciones innatas o algo así no

1615
00:54:45,359 --> 00:54:46,680
totalmente y creo que mucho de

1616
00:54:46,680 --> 00:54:48,420
eso es compatible con

1617
00:54:48,420 --> 00:54:49,980
ellos  programa de enfermedad

1618
00:54:49,980 --> 00:54:51,960
porque la mitad de este programa quiere que la

1619
00:54:51,960 --> 00:54:53,339
sintaxis sea mínima, no quiere que

1620
00:54:53,339 --> 00:54:54,660
sea complicado, no quiere que sea, ya sabes,

1621
00:54:54,660 --> 00:54:56,220
más complicado, tiene que

1622
00:54:56,220 --> 00:54:57,960
ser, así que hubo algunos que mencionaste,

1623
00:54:57,960 --> 00:54:59,460
las propiedades curiosas, ¿verdad?  así que

1624
00:54:59,460 --> 00:55:00,480
hay algunas de las propiedades que

1625
00:55:00,480 --> 00:55:02,579
deben tenerse en cuenta en cualquier modelo de

1626
00:55:02,579 --> 00:55:04,200
lenguaje que, eh, les daré un

1627
00:55:04,200 --> 00:55:05,400
ejemplo, la configuración de las características de una persona

1628
00:55:05,400 --> 00:55:06,599


1629
00:55:06,599 --> 00:55:08,880
y estas características de la persona exhiben

1630
00:55:08,880 --> 00:55:10,440
generalizaciones diferentes muy no triviales

1631
00:55:10,440 --> 00:55:12,480
que no parecen  tener en cuenta el

1632
00:55:12,480 --> 00:55:14,220
mecanismo de aprendizaje general del dominio Via, así que estoy

1633
00:55:14,220 --> 00:55:16,020
sentado aquí, el trabajo de Daniel Harper

1634
00:55:16,020 --> 00:55:17,700
en Queen Mary, por ejemplo, la

1635
00:55:17,700 --> 00:55:19,740
composición morfológica de la persona, su

1636
00:55:19,740 --> 00:55:21,720
interacción con el número, su conexión

1637
00:55:21,720 --> 00:55:24,059
con el espacio, uh, las propiedades de su semántica

1638
00:55:24,059 --> 00:55:26,040
y su linealización.

1639
00:55:26,040 --> 00:55:27,240
ser buenos candidatos para nuestro

1640
00:55:27,240 --> 00:55:28,619
conocimiento del lenguaje, correcto, lo que entendemos

1641
00:55:28,619 --> 00:55:30,359
por conocimiento del lenguaje, pero por

1642
00:55:30,359 --> 00:55:32,280
otro lado tenemos cosas como el caso y el

1643
00:55:32,280 --> 00:55:34,319
acuerdo y el movimiento de la cabeza y

1644
00:55:34,319 --> 00:55:36,420
todos estos son fenómenos estructurales, sin embargo,

1645
00:55:36,420 --> 00:55:39,319
parecen resistir una

1646
00:55:39,319 --> 00:55:42,420
explicación puramente basada en el significado, eh en

1647
00:55:42,420 --> 00:55:44,339
Lingüística teórica, cierto,

1648
00:55:44,339 --> 00:55:45,839
sería genial si la sintaxis no fuera más que un

1649
00:55:45,839 --> 00:55:47,520
motor computacional que construye un

1650
00:55:47,520 --> 00:55:49,440
significado estructurado y ese es el

1651
00:55:49,440 --> 00:55:51,240
programa minimalista, el objetivo, pero eso

1652
00:55:51,240 --> 00:55:52,800
no es lo que realmente encontramos, eso no está en

1653
00:55:52,800 --> 00:55:54,720
ningún minimalista real como

1654
00:55:54,720 --> 00:55:57,240
modelo concreto, ninguna teoría mineralista concreta, el

1655
00:55:57,240 --> 00:55:59,040
objetivo es solo  por ejemplo, el programa es el

1656
00:55:59,040 --> 00:56:01,260
idioma es perfecto, está bien, ese es el

1657
00:56:01,260 --> 00:56:03,119
programa es que lo que encontramos no,

1658
00:56:03,119 --> 00:56:05,160
obviamente, no está bien, no, ningún lingüista

1659
00:56:05,160 --> 00:56:07,680
realmente cree eso, por lo que

1660
00:56:07,680 --> 00:56:09,900
sería genial si la sintaxis fuera así, pero

1661
00:56:09,900 --> 00:56:11,280
creo que sabes que el programa es para

1662
00:56:11,280 --> 00:56:13,740
buscar la perfección, pero  no siempre lo encuentro tan

1663
00:56:13,740 --> 00:56:15,839
caso un acuerdo y el movimiento de la cabeza son

1664
00:56:15,839 --> 00:56:17,640
morfológicos más para fonológico

1665
00:56:17,640 --> 00:56:19,319
fenomenal las propiedades de los

1666
00:56:19,319 --> 00:56:20,700
sistemas de rendimiento lo que se llama

1667
00:56:20,700 --> 00:56:22,440
sistemas de rendimiento y así el

1668
00:56:22,440 --> 00:56:23,760
programa minimalista en sí es realmente

1669
00:56:23,760 --> 00:56:24,839
compatible con mucho de lo que estás

1670
00:56:24,839 --> 00:56:26,880
diciendo sobre el idioma idioma

1671
00:56:26,880 --> 00:56:28,500
hay aspectos del lenguaje que pueden

1672
00:56:28,500 --> 00:56:31,200
ser um perfeccionados y optimizados para la

1673
00:56:31,200 --> 00:56:32,700
eficiencia comunicativa absolutamente

1674
00:56:32,700 --> 00:56:35,520
sin ninguna duda al respecto, pero ¿dónde está

1675
00:56:35,520 --> 00:56:38,160
ese lugar de eficiencia? ¿Está en la

1676
00:56:38,160 --> 00:56:39,960
sintaxis misma o es algún tipo de

1677
00:56:39,960 --> 00:56:42,000
sistema lingüístico adicional? ¿Está en la

1678
00:56:42,000 --> 00:56:43,680
pragmática?  en el sentido

1679
00:56:43,680 --> 00:56:45,540
motor está en el habla

1680
00:56:45,540 --> 00:56:47,640
um probablemente el habla y la fonología

1681
00:56:47,640 --> 00:56:50,400
probablemente sabes quiero decir quién sabe pero creo que

1682
00:56:50,400 --> 00:56:52,740
muchas de estas cosas exigen mucho

1683
00:56:52,740 --> 00:56:56,059
más sabes consideración seria en

1684
00:56:56,059 --> 00:56:57,839
nociones anticuadas como

1685
00:56:57,839 --> 00:56:59,700
dependencia de la estructura tema compositivo qué

1686
00:56:59,700 --> 00:57:01,140
tienes  cosas como esas que tal vez puedas

1687
00:57:01,140 --> 00:57:03,720
encontrar en algún lugar de la literatura, pero

1688
00:57:03,720 --> 00:57:06,720
um incluso solo temas básicos como tú sabes

1689
00:57:06,720 --> 00:57:08,640
um cuantificador elevando proyecciones extendidas

1690
00:57:08,640 --> 00:57:09,900


1691
00:57:09,900 --> 00:57:12,059
um adverbial como jerarquías adverbiales

1692
00:57:12,059 --> 00:57:13,920
todas estas cosas en el

1693
00:57:13,920 --> 00:57:16,619
programa minimalista pueden ser extra lingüísticas, en

1694
00:57:16,619 --> 00:57:18,180
realidad pueden estar fuera de  sintaxis

1695
00:57:18,180 --> 00:57:20,579
y consulta propiedades muy raras de la

1696
00:57:20,579 --> 00:57:23,099
semántica uh sistemas conceptuales que son

1697
00:57:23,099 --> 00:57:24,559
en sí mismos una especie de dominio

1698
00:57:24,559 --> 00:57:27,420
Restos extraños generales de la

1699
00:57:27,420 --> 00:57:29,220
cognición primaria antigua derecho las características de la forma en que

1700
00:57:29,220 --> 00:57:31,260
pasamos eventos la forma en que pasamos ya sabes

1701
00:57:31,260 --> 00:57:32,520
agentes y pacientes cosas así

1702
00:57:32,520 --> 00:57:33,960
eso definitivamente no es  eso no es específico de los humanos,

1703
00:57:33,960 --> 00:57:35,520


1704
00:57:35,520 --> 00:57:37,260
um, pero sabes que la forma en que la sintaxis

1705
00:57:37,260 --> 00:57:39,059
proporciona instrucciones a estos sistemas que

1706
00:57:39,059 --> 00:57:42,480
sabes correctamente parece serlo, así que

1707
00:57:42,480 --> 00:57:43,800
sabes que los lingüistas generativos también tienen diferentes

1708
00:57:43,800 --> 00:57:45,839
teorías sobre la producción del lenguaje.

1709
00:57:45,839 --> 00:57:47,040
Solo hablaré sobre la producción del lenguaje

1710
00:57:47,040 --> 00:57:49,380
en función de si almacenamos lemas.  o

1711
00:57:49,380 --> 00:57:51,240
si construimos palabras exactamente de la misma

1712
00:57:51,240 --> 00:57:52,619
manera que formularemos frases y oraciones, así que

1713
00:57:52,619 --> 00:57:53,819
sé que usted hace la distinción

1714
00:57:53,819 --> 00:57:55,440
entre la gramática de construcción y el tipo de

1715
00:57:55,440 --> 00:57:57,180
gramática generativa y sabe el

1716
00:57:57,180 --> 00:57:58,680
peso que le dan a memorizar

1717
00:57:58,680 --> 00:58:00,119
constricciones, mientras que solo construye

1718
00:58:00,119 --> 00:58:01,559
cosas a partir de  el fondo hacia arriba desde el

1719
00:58:01,559 --> 00:58:04,440
suelo hacia arriba a la derecha y, por lo tanto, en algunos

1720
00:58:04,440 --> 00:58:06,480
modelos generativos inspirados, los mecanismos

1721
00:58:06,480 --> 00:58:08,460
que generan la estructura sintáctica

1722
00:58:08,460 --> 00:58:10,200
no hacen distinciones entre los procesos que

1723
00:58:10,200 --> 00:58:12,200
se aplican por encima o por debajo del nivel de la palabra,

1724
00:58:12,200 --> 00:58:14,640
no hay un puntero que signifique que la sintaxis

1725
00:58:14,640 --> 00:58:16,500
y la forma se almacenan juntas en un

1726
00:58:16,500 --> 00:58:18,660
solo Atomic  representaciones cada etapa

1727
00:58:18,660 --> 00:58:20,099
en el acceso léxico es una transición

1728
00:58:20,099 --> 00:58:21,960
entre diferentes tipos de

1729
00:58:21,960 --> 00:58:23,760
estructuras de datos allí mismo hay significado hay

1730
00:58:23,760 --> 00:58:25,740
forma y hay sintaxis estas tres

1731
00:58:25,740 --> 00:58:27,480
características se entremezclan y

1732
00:58:27,480 --> 00:58:28,920
no siempre se superponen diferentes

1733
00:58:28,920 --> 00:58:31,440
idiomas se dan cuenta de diferentes maneras

1734
00:58:31,440 --> 00:58:34,800
y entonces ya sabes  extraño, la

1735
00:58:34,800 --> 00:58:36,839
definición básica de una palabra es solo esta

1736
00:58:36,839 --> 00:58:39,720
definición extraña de múltiples sistemas y donde muchas

1737
00:58:39,720 --> 00:58:41,040
cosas, muchos sistemas cognitivos diferentes

1738
00:58:41,040 --> 00:58:42,900
enriquecen la base de cada

1739
00:58:42,900 --> 00:58:44,940
elemento eléctrico, ¿verdad? No

1740
00:58:44,940 --> 00:58:46,680
hay nada como esto, realmente este

1741
00:58:46,680 --> 00:58:48,299
proceso de enriquecimiento.

1742
00:58:48,299 --> 00:58:50,099
Teoría lingüística

1743
00:58:50,099 --> 00:58:52,200
correcta o al menos en lo que están haciendo las películas

1744
00:58:52,200 --> 00:58:53,220


1745
00:58:53,220 --> 00:58:55,859
así que supongo que supongo que

1746
00:58:55,859 --> 00:58:58,799
te preguntaré cuál es tu definición de una palabra

1747
00:58:58,799 --> 00:59:01,559
correcta y qué películas realmente pueden proporcionar

1748
00:59:01,559 --> 00:59:03,780
información sobre la palabra Hood correcta porque si

1749
00:59:03,780 --> 00:59:04,680
no tiene un

1750
00:59:04,680 --> 00:59:06,720
destino de lo que es una palabra, entonces

1751
00:59:06,720 --> 00:59:07,920
está realmente en problemas, como si

1752
00:59:07,920 --> 00:59:10,319
al menos tuviéramos que usar LMS o

1753
00:59:10,319 --> 00:59:12,780
sistemas artificiales para informar lo que queremos decir con una palabra

1754
00:59:12,780 --> 00:59:14,400
o tal vez ya no lo necesitemos.

1755
00:59:14,400 --> 00:59:16,859
no estoy seguro de lo que piensas yo no estoy

1756
00:59:16,859 --> 00:59:18,599
no estoy seguro de lo que quieres decir quiero decir um

1757
00:59:18,599 --> 00:59:20,339


1758
00:59:20,339 --> 00:59:24,059
um yo no tengo una qué es una palabra por qué

1759
00:59:24,059 --> 00:59:25,740
importa eso quiero decir que eso es

1760
00:59:25,740 --> 00:59:27,359
solo una convención sobre cómo usamos el

1761
00:59:27,359 --> 00:59:29,819
término palabra correcto lo que me

1762
00:59:29,819 --> 00:59:31,920
refiero a que podrías usar sabes lemas o

1763
00:59:31,920 --> 00:59:34,440
palabras firmes o lo que sea que se

1764
00:59:34,440 --> 00:59:35,819
siente como una opción convencional Elección

1765
00:59:35,819 --> 00:59:38,040
estoy No estoy seguro de qué es lo que está en

1766
00:59:38,040 --> 00:59:39,180
juego allí,

1767
00:59:39,180 --> 00:59:41,880
así que ¿cómo lo haría? Supongo que lo haría  Digo, estoy de

1768
00:59:41,880 --> 00:59:43,920
acuerdo, la palabra es una convencionalización,

1769
00:59:43,920 --> 00:59:45,599
sabes que los iconos no son un concepto intuitivo de

1770
00:59:45,599 --> 00:59:47,819
donde a menudo está sesgado por la ortografía,

1771
00:59:47,819 --> 00:59:50,760
la forma en que arreglamos los espacios, de modo

1772
00:59:50,760 --> 00:59:52,559
que estoy de acuerdo con esa crítica,

1773
00:59:52,559 --> 00:59:54,240
sabes, la palabra en el sentido intuitivo no es

1774
00:59:54,240 --> 00:59:56,339
realmente científica.  construir, sin embargo,

1775
00:59:56,339 --> 00:59:58,260
supongo que déjame reformular mi pregunta, ¿cómo

1776
00:59:58,260 --> 01:00:00,359
podrías, um, ya sabes, descomponer el

1777
01:00:00,359 --> 01:00:02,160
concepto intuitivo de palabra en algo

1778
01:00:02,160 --> 01:00:03,660
que sea más amable

1779
01:00:03,660 --> 01:00:04,740
científicamente susceptible o

1780
01:00:04,740 --> 01:00:06,540
psicológicamente plausible, que es

1781
01:00:06,540 --> 01:00:08,280
exactamente lo que intenta hacer la primaria geométrica

1782
01:00:08,280 --> 01:00:10,559
al descomponer palabras en ya sabes?

1783
01:00:10,559 --> 01:00:12,599
características distintivas uh

1784
01:00:12,599 --> 01:00:15,180
categorías morfológicas raíces conceptuales que se fusionan

1785
01:00:15,180 --> 01:00:17,579
con características categóricas sabes que

1786
01:00:17,579 --> 01:00:20,700
obtienes un concepto que conoces y lo has hecho

1787
01:00:20,700 --> 01:00:22,440
con un sustantivo o una categoría para obtener un

1788
01:00:22,440 --> 01:00:24,119
sustantivo o Asunto estos diferentes modelos

1789
01:00:24,119 --> 01:00:26,220
hacen diferentes predicciones cierto sí quiero decir yo

1790
01:00:26,220 --> 01:00:28,859
creo  Es probable que esa idea general

1791
01:00:28,859 --> 01:00:30,839
sea adecuada para modelos de lenguaje grandes,

1792
01:00:30,839 --> 01:00:32,880
como creo que deben

1793
01:00:32,880 --> 01:00:34,380
tener cosas que son como parte

1794
01:00:34,380 --> 01:00:36,660
de las categorías del habla, por ejemplo,

1795
01:00:36,660 --> 01:00:38,819
y creo que

1796
01:00:38,819 --> 01:00:42,900
deben poder actualizar esas

1797
01:00:42,900 --> 01:00:45,240
categorías.  basado en el lenguaje que

1798
01:00:45,240 --> 01:00:48,119
han visto hasta ahora, como si

1799
01:00:48,119 --> 01:00:50,520
supieras que GPT coloca sustantivos y verbos en los

1800
01:00:50,520 --> 01:00:53,400
lugares correctos y para hacer eso,

1801
01:00:53,400 --> 01:00:55,319
necesitas alguna representación de los sustantivos

1802
01:00:55,319 --> 01:00:57,000
versus los verbos y necesitas alguna

1803
01:00:57,000 --> 01:01:00,839
habilidad para eh  ubíquese en una

1804
01:01:00,839 --> 01:01:02,579
cadena de otras palabras y descubra si

1805
01:01:02,579 --> 01:01:04,680
es probable que haya un sustantivo o un verbo

1806
01:01:04,680 --> 01:01:05,400
a continuación,

1807
01:01:05,400 --> 01:01:07,559
así que creo que en ese

1808
01:01:07,559 --> 01:01:09,319
nivel es muy probable que ese tipo de propiedades de las palabras

1809
01:01:09,319 --> 01:01:12,480
sean correctas y

1810
01:01:12,480 --> 01:01:15,780
allí  también hay cosas que es

1811
01:01:15,780 --> 01:01:17,640
muy probable que se encuentren en

1812
01:01:17,640 --> 01:01:19,619
las representaciones internas de estos

1813
01:01:19,619 --> 01:01:21,420
modelos.

1814
01:01:21,420 --> 01:01:24,180


1815
01:01:24,180 --> 01:01:26,339


1816
01:01:26,339 --> 01:01:29,579
no donde el uh uh ahí no es

1817
01:01:29,579 --> 01:01:31,500
donde los principales debates o

1818
01:01:31,500 --> 01:01:35,220
desacuerdos creo que es correcto como

1819
01:01:35,220 --> 01:01:38,280
um uh sí creo que todas las teorías del

1820
01:01:38,280 --> 01:01:40,079
lenguaje tienen que decir que

1821
01:01:40,079 --> 01:01:41,400
hay diferentes tipos de palabras que

1822
01:01:41,400 --> 01:01:43,140
pueden aparecer en diferentes lugares o

1823
01:01:43,140 --> 01:01:44,339
algo así

1824
01:01:44,339 --> 01:01:45,240
um, sí, está

1825
01:01:45,240 --> 01:01:47,160
bien, entonces, ¿qué pasa con el problema que sabes que

1826
01:01:47,160 --> 01:01:49,020
mencionaste sobre la comunicación?

1827
01:01:49,020 --> 01:01:51,299
um, así que sabes y tienes toda la razón

1828
01:01:51,299 --> 01:01:53,099
cuando Trump ve cosas como que el lenguaje

1829
01:01:53,099 --> 01:01:54,960
es un sistema de pensamiento o sabes que el lenguaje

1830
01:01:54,960 --> 01:01:57,780
no evolucionó, él está siendo un

1831
01:01:57,780 --> 01:01:58,799
poco  descarado, en realidad no quiere decir

1832
01:01:58,799 --> 01:02:00,180
que quiere decir en un sentido muy específico, ¿

1833
01:02:00,180 --> 01:02:01,619
verdad?

1834
01:02:01,619 --> 01:02:03,540
Pero sabes, cuando decimos que el lenguaje es

1835
01:02:03,540 --> 01:02:05,640
un sistema de pensamiento, lo que queremos decir es,

1836
01:02:05,640 --> 01:02:06,780
um, estamos tratando de obtener un

1837
01:02:06,780 --> 01:02:08,339
reclamo arquitectónico, así que si miras

1838
01:02:08,339 --> 01:02:09,599
la arquitectura del programa minimalista,

1839
01:02:09,599 --> 01:02:10,380


1840
01:02:10,380 --> 01:02:12,240
la derivación sintáctica y los

1841
01:02:12,240 --> 01:02:13,740
sistemas conceptuales son literalmente

1842
01:02:13,740 --> 01:02:15,960
sistemas diferentes, ¿

1843
01:02:15,960 --> 01:02:18,180


1844
01:02:18,180 --> 01:02:19,799


1845
01:02:19,799 --> 01:02:21,900


1846
01:02:21,900 --> 01:02:23,700
verdad?

1847
01:02:23,700 --> 01:02:25,319
en el lenguaje, ambos son

1848
01:02:25,319 --> 01:02:27,000
sistemas de composición simbólica similares, pero de

1849
01:02:27,000 --> 01:02:29,160
diferentes maneras, solo un subconjunto de pensamiento

1850
01:02:29,160 --> 01:02:32,339
se llama correctamente el sistema de interfaz de CI,

1851
01:02:32,339 --> 01:02:34,619
ya que los sistemas de CI son, por

1852
01:02:34,619 --> 01:02:36,480
definición, cualquiera que sea el

1853
01:02:36,480 --> 01:02:38,819
sistema conceptual que tienen los humanos que no pueden acceder y

1854
01:02:38,819 --> 01:02:40,859
leer instrucciones de sintaxis  y

1855
01:02:40,859 --> 01:02:42,540
no sabemos cuáles son completamente, parecen

1856
01:02:42,540 --> 01:02:43,920
tener algo que ver con eventos en

1857
01:02:43,920 --> 01:02:45,540
referencia gramatical y definición,

1858
01:02:45,540 --> 01:02:47,099
parecen ser las categorías principales que el

1859
01:02:47,099 --> 01:02:48,359
lenguaje que conoces se preocupa

1860
01:02:48,359 --> 01:02:49,440
conceptualmente,

1861
01:02:49,440 --> 01:02:50,940
pero en realidad no sabemos, eso es

1862
01:02:50,940 --> 01:02:52,859
solo un poco  hipótesis correcta

1863
01:02:52,859 --> 01:02:53,940
um pero lo que sí sabemos es que

1864
01:02:53,940 --> 01:02:56,220
no parecen hacer mucho uso del color

1865
01:02:56,220 --> 01:02:58,440
o um así que ningún lenguaje

1866
01:02:58,440 --> 01:03:00,240
marca morfológicamente ya sabes tonos de

1867
01:03:00,240 --> 01:03:01,140
color

1868
01:03:01,140 --> 01:03:03,900
um u otras características conceptuales como um

1869
01:03:03,900 --> 01:03:06,359
um preocupación o preocupación como ningún lenguaje

1870
01:03:06,359 --> 01:03:07,980
morfológicamente  marca un grado de preocupación

1871
01:03:07,980 --> 01:03:10,079
o inquietud sobre un problema, pero hacemos

1872
01:03:10,079 --> 01:03:11,960
uso de

1873
01:03:11,960 --> 01:03:13,500
nociones epistemológicas como la

1874
01:03:13,500 --> 01:03:15,599
evidencialidad y cosas así, así que lo

1875
01:03:15,599 --> 01:03:17,640
conoces bien. Supongo que lo que

1876
01:03:17,640 --> 01:03:19,500
digo es que el programa minimalista hace un

1877
01:03:19,500 --> 01:03:21,720
buen trabajo al tratar de descifrar  averiguar a qué

1878
01:03:21,720 --> 01:03:23,400
aspectos del pensamiento el lenguaje está

1879
01:03:23,400 --> 01:03:25,619
íntimamente ligado y a qué aspectos del

1880
01:03:25,619 --> 01:03:27,900
pensamiento no está ligado, por lo que el programa del Medio Oeste

1881
01:03:27,900 --> 01:03:29,280
nos permite repartir eso

1882
01:03:29,280 --> 01:03:31,680
bastante bien y este es un

1883
01:03:31,680 --> 01:03:33,180
marco mucho más matizado de lo que sabes

1884
01:03:33,180 --> 01:03:34,559
cuando chomski dice que los lenguajes pensaron

1885
01:03:34,559 --> 01:03:36,960
de nuevo, no lo hace, tal vez lo dice en serio, tal vez

1886
01:03:36,960 --> 01:03:38,339
no, pero eso no es lo que

1887
01:03:38,339 --> 01:03:40,319
dice la arquitectura real de su teoría, es

1888
01:03:40,319 --> 01:03:42,480
un dispositivo retórico que es muy

1889
01:03:42,480 --> 01:03:43,920
útil e interesante para

1890
01:03:43,920 --> 01:03:46,559
atraer audiencias de estudiantes universitarios, lo que sea si

1891
01:03:46,559 --> 01:03:48,839
observa teorías reales que son

1892
01:03:48,839 --> 01:03:50,700
Saliendo del programa The Mentalist,

1893
01:03:50,700 --> 01:03:52,380
nadie realmente cree que el lenguaje sea igual al

1894
01:03:52,380 --> 01:03:53,760
pensamiento. Correcto, el sistema lingüístico parece

1895
01:03:53,760 --> 01:03:55,920
hacer todo lo posible para acceder,

1896
01:03:55,920 --> 01:03:57,599
reformatear y manipular varios

1897
01:03:57,599 --> 01:03:59,220
sistemas conceptuales, pero tiene sus límites.

1898
01:03:59,220 --> 01:04:01,440


1899
01:04:01,440 --> 01:04:02,819


1900
01:04:02,819 --> 01:04:05,160
con respecto al motor de sintaxis y

1901
01:04:05,160 --> 01:04:07,260
cuáles no lo son,

1902
01:04:07,260 --> 01:04:09,480
así que sabes que esto vuelve a

1903
01:04:09,480 --> 01:04:11,579
la idea de que la lexización de un concepto

1904
01:04:11,579 --> 01:04:14,099
parece alterarlo de alguna manera,

1905
01:04:14,099 --> 01:04:16,200
lo impregna con elementos que

1906
01:04:16,200 --> 01:04:17,819
no están allí en el  el concepto en sí mismo, así que si

1907
01:04:17,819 --> 01:04:19,319
disertas es un concepto, de repente lo

1908
01:04:19,319 --> 01:04:21,240
transformas un poco, le das un

1909
01:04:21,240 --> 01:04:22,680
poco más, le rocías algo más

1910
01:04:22,680 --> 01:04:24,299
encima y eso parece variar

1911
01:04:24,299 --> 01:04:26,700
según los diferentes nanotipos, pero

1912
01:04:26,700 --> 01:04:29,160
todos estos son reclamos arquitectónicos muy claros

1913
01:04:29,160 --> 01:04:31,619
dentro de la geometría.  gramática que

1914
01:04:31,619 --> 01:04:34,980
hace predicciones empíricas muy claras,

1915
01:04:34,980 --> 01:04:36,420
en otras palabras, supongo que lo que estoy diciendo

1916
01:04:36,420 --> 01:04:37,140
son

1917
01:04:37,140 --> 01:04:38,940
todos estos estudios de neuropsicología que

1918
01:04:38,940 --> 01:04:42,119
están incitados, sabes, en mucho trabajo,

1919
01:04:42,119 --> 01:04:43,980
um, en este sentido, ¿qué muestra realmente?

1920
01:04:43,980 --> 01:04:45,420
Creo que muestra que sabes  cuando el

1921
01:04:45,420 --> 01:04:47,819
lenguaje se daña en el cerebro,

1922
01:04:47,819 --> 01:04:50,040
pierde esta influencia particular o el modo de

1923
01:04:50,040 --> 01:04:51,960
influir en esos sistemas, pero no hay una

1924
01:04:51,960 --> 01:04:54,299
predicción real desde dentro de Gen Gram

1925
01:04:54,299 --> 01:04:56,040
Enterprise, pero esos sistemas no lingüísticos

1926
01:04:56,040 --> 01:04:57,540
deberían verse afectados o debería

1927
01:04:57,540 --> 01:04:59,579
cerrarse repentinamente si el

1928
01:04:59,579 --> 01:05:01,200
sistema de lenguaje central

1929
01:05:01,200 --> 01:05:02,760
um está comprometido, de hecho, en todo caso,

1930
01:05:02,760 --> 01:05:05,099
eso solo enfatiza el

1931
01:05:05,099 --> 01:05:07,859
divorcio principal entre el

1932
01:05:07,859 --> 01:05:09,900
sistema sintáctico y los sistemas no lingüísticos,

1933
01:05:09,900 --> 01:05:11,880
así que creo que muchas predicciones aquí

1934
01:05:11,880 --> 01:05:14,339
del lenguaje y la comunicación, eh, ya

1935
01:05:14,339 --> 01:05:16,020
sabes, la literatura está perdiendo

1936
01:05:16,020 --> 01:05:19,380
el punto de la  reclamos arquitectónicos

1937
01:05:19,380 --> 01:05:21,540
um yo solo puedo dar o Daniel ¿

1938
01:05:21,540 --> 01:05:24,180
quieres ir uh dar un poco de

1939
01:05:24,180 --> 01:05:25,799
antecedentes allí así que están estos

1940
01:05:25,799 --> 01:05:26,760
papeles

1941
01:05:26,760 --> 01:05:30,119
um uh de uh uh EV federenko y y

1942
01:05:30,119 --> 01:05:32,880
rosemary Varley que están

1943
01:05:32,880 --> 01:05:34,500
um examinando

1944
01:05:34,500 --> 01:05:37,799
um uh uh en parte de  ellos pacientes afásicos,

1945
01:05:37,799 --> 01:05:40,200
por lo que las personas que tienen

1946
01:05:40,200 --> 01:05:41,880
habilidades lingüísticas disminuidas

1947
01:05:41,880 --> 01:05:43,140
um

1948
01:05:43,140 --> 01:05:45,000
um básicamente muestran que con

1949
01:05:45,000 --> 01:05:46,980
habilidades lingüísticas disminuidas usted

1950
01:05:46,980 --> 01:05:49,020
um puede todavía puede tener

1951
01:05:49,020 --> 01:05:50,579
um uh habilidades de razonamiento conservadas,

1952
01:05:50,579 --> 01:05:52,859
por lo que personas como Chess Masters

1953
01:05:52,859 --> 01:05:55,020
chess Grand Masters, por ejemplo, que

1954
01:05:55,020 --> 01:05:58,140
obviamente son muy buenos  en el razonamiento

1955
01:05:58,140 --> 01:06:00,780
um uh um podría no tener uh tipo de

1956
01:06:00,780 --> 01:06:02,640
habilidades lingüísticas intactas

1957
01:06:02,640 --> 01:06:04,079
um y luego complementando ese tipo

1958
01:06:04,079 --> 01:06:06,119
de trabajo paciente también hay uh trabajo

1959
01:06:06,119 --> 01:06:08,520
del laboratorio de ebb que muestra que

1960
01:06:08,520 --> 01:06:11,220
um uh las uh partes del cerebro

1961
01:06:11,220 --> 01:06:13,859
que se preocupan por uh lenguaje

1962
01:06:13,859 --> 01:06:14,640
um

1963
01:06:14,640 --> 01:06:16,319
um  son separables de las partes del

1964
01:06:16,319 --> 01:06:17,700
cerebro que se preocupan por otros

1965
01:06:17,700 --> 01:06:19,619
dominios, incluso aquellos con el mismo tipo de

1966
01:06:19,619 --> 01:06:21,599
lenguaje, por lo que cosas como la música

1967
01:06:21,599 --> 01:06:23,819
y las matemáticas,

1968
01:06:23,819 --> 01:06:25,859
um, uh, tienden a no ocurrir en las

1969
01:06:25,859 --> 01:06:27,240
áreas del lenguaje,

1970
01:06:27,240 --> 01:06:29,400
por lo que EV y otros han tenido  argumentó

1971
01:06:29,400 --> 01:06:30,539
que

1972
01:06:30,539 --> 01:06:33,539
esto es básicamente evidencia en contra

1973
01:06:33,539 --> 01:06:36,660
de Chomsky y afirma que el

1974
01:06:36,660 --> 01:06:38,579
lenguaje es el medio para pensar

1975
01:06:38,579 --> 01:06:40,440
correctamente porque hay un pensamiento que puede

1976
01:06:40,440 --> 01:06:42,420
ocurrir en ausencia de lenguaje y las

1977
01:06:42,420 --> 01:06:44,160
áreas del cerebro que se preocupan por el lenguaje

1978
01:06:44,160 --> 01:06:46,020
parecen no ser las áreas del cerebro que ellos  me importa preocuparme

1979
01:06:46,020 --> 01:06:48,359
por pensar

1980
01:06:48,359 --> 01:06:50,099
um supongo que Elliot estás diciendo

1981
01:06:50,099 --> 01:06:51,780
que la gente realmente no cree

1982
01:06:51,780 --> 01:06:53,039
eso

1983
01:06:53,039 --> 01:06:56,220
um uh ellos no creen que esa

1984
01:06:56,220 --> 01:06:57,720
distinción quiero decir

1985
01:06:57,720 --> 01:06:59,880
um que uh

1986
01:06:59,880 --> 01:07:01,740
no es todo y también hay mucho  de

1987
01:07:01,740 --> 01:07:03,480
autocontradicción incluso dentro de

1988
01:07:03,480 --> 01:07:04,799
estos argumentos, así que en su

1989
01:07:04,799 --> 01:07:06,839
artículo a veces dice que Chomsky

1990
01:07:06,839 --> 01:07:08,220
cree que el lenguaje es un sistema de pensamiento,

1991
01:07:08,220 --> 01:07:10,319
pero luego, unas páginas más adelante, dirá que

1992
01:07:10,319 --> 01:07:12,480
Chomsky también cree que la sintaxis es

1993
01:07:12,480 --> 01:07:13,799
un sistema totalmente separado de

1994
01:07:13,799 --> 01:07:15,480
cualquier otra cosa.  cierto, autonomía de la

1995
01:07:15,480 --> 01:07:18,900
sintaxis, etc. Entonces, ¿cuál es Chomsky? Esa

1996
01:07:18,900 --> 01:07:20,460
no es mi contradicción. Quiero decir, él

1997
01:07:20,460 --> 01:07:22,140
dijo ambas cosas,

1998
01:07:22,140 --> 01:07:24,780
um, exactamente, así que, por lo tanto, es posible que

1999
01:07:24,780 --> 01:07:26,280
desee preguntarse si realmente

2000
01:07:26,280 --> 01:07:28,319


2001
01:07:28,319 --> 01:07:30,420
cree en estas cosas.  de la

2002
01:07:30,420 --> 01:07:32,520
arquitectura, así que solo

2003
01:07:32,520 --> 01:07:34,619
digo que el lenguaje es un sistema de pensamiento. ¿Qué

2004
01:07:34,619 --> 01:07:35,579
significa eso? Eso no significa

2005
01:07:35,579 --> 01:07:36,780
nada. Es solo una afirmación muy vaga.

2006
01:07:36,780 --> 01:07:38,640
La pregunta es cómo exactamente

2007
01:07:38,640 --> 01:07:41,280
contribuye el lenguaje a Thor y cómo

2008
01:07:41,280 --> 01:07:43,500
no contribuye.

2009
01:07:43,500 --> 01:07:45,839
Creo que su afirmación es

2010
01:07:45,839 --> 01:07:48,420
principalmente evolutiva o algo correcto,

2011
01:07:48,420 --> 01:07:51,180
que este es el origen del

2012
01:07:51,180 --> 01:07:52,859
sistema que creo que es

2013
01:07:52,859 --> 01:07:55,559
igualmente difícil de cuadrar con

2014
01:07:55,559 --> 01:07:57,599
el tipo de paciente y los

2015
01:07:57,599 --> 01:07:59,520
datos de neuroimagen,

2016
01:07:59,520 --> 01:08:00,660


2017
01:08:00,660 --> 01:08:04,079
pero sabes si  si no piensa

2018
01:08:04,079 --> 01:08:07,079
eso, entonces no debería decirlo o la gente

2019
01:08:07,079 --> 01:08:08,579
responderá a lo que dijo. Creo que no,

2020
01:08:08,579 --> 01:08:11,280
no, porque el argumento es que el

2021
01:08:11,280 --> 01:08:13,619
lenguaje es una especie de sistema de pensamiento,

2022
01:08:13,619 --> 01:08:15,420
regula algunos aspectos del pensamiento

2023
01:08:15,420 --> 01:08:16,920
y produce.  algunos aspectos del pensamiento

2024
01:08:16,920 --> 01:08:19,020
que son claramente exclusivos de los humanos, pero que

2025
01:08:19,020 --> 01:08:21,000
no están intrínsecamente o causalmente ligados

2026
01:08:21,000 --> 01:08:22,920
a él, la arquitectura del

2027
01:08:22,920 --> 01:08:24,600
sistema es muy diferente del tipo

2028
01:08:24,600 --> 01:08:26,399
de generalizaciones que puedes retóricamente.

2029
01:08:26,399 --> 01:08:28,979


2030
01:08:28,979 --> 01:08:30,960


2031
01:08:30,960 --> 01:08:32,698
pacientes que no muestran deficiencias en el

2032
01:08:32,698 --> 01:08:33,960
razonamiento complejo como usted acaba de mencionar jugando al

2033
01:08:33,960 --> 01:08:35,759
ajedrez, etc., en realidad esperaríamos

2034
01:08:35,759 --> 01:08:37,380
esto bajo un tipo de

2035
01:08:37,380 --> 01:08:39,179
marco no lexicalista de

2036
01:08:39,179 --> 01:08:41,160
sintaxis geométrica donde el significado, como dije, significa

2037
01:08:41,160 --> 01:08:43,799
sintaxis y forma forma simplemente significa

2038
01:08:43,799 --> 01:08:45,600
cualquier cosa que pueda  externalizar el

2039
01:08:45,600 --> 01:08:46,979
lenguaje y todas estas cosas son

2040
01:08:46,979 --> 01:08:48,540
características separadas y sistemas separados,

2041
01:08:48,540 --> 01:08:51,000
la autonomía de la sintaxis no

2042
01:08:51,000 --> 01:08:52,020
significa que

2043
01:08:52,020 --> 01:08:53,819
sepas lo que mucha gente piensa que significa

2044
01:08:53,819 --> 01:08:54,719
simplemente significa que

2045
01:08:54,719 --> 01:08:56,160
hay ciertas operaciones sintácticas

2046
01:08:56,160 --> 01:08:58,439
que no son semánticas hay ciertas

2047
01:08:58,439 --> 01:09:00,299
cosas  puedes hacer con la sintaxis que

2048
01:09:00,299 --> 01:09:01,920
solo puedes hacer sintaxis y no puedes hacer

2049
01:09:01,920 --> 01:09:03,420
semántica, así que esto vuelve a la

2050
01:09:03,420 --> 01:09:05,279
diferencia entre la

2051
01:09:05,279 --> 01:09:07,259
teoría de petrovsky de que la semántica es

2052
01:09:07,259 --> 01:09:10,319
justa y correcta versus la

2053
01:09:10,319 --> 01:09:11,698
creencia de los sintéticos de que hay

2054
01:09:11,698 --> 01:09:13,080
ciertas cosas raras peculiares  puede hacerlo

2055
01:09:13,080 --> 01:09:15,719
con sintaxis que son simplemente sintácticas, por lo que

2056
01:09:15,719 --> 01:09:17,759
hay un divorcio incluso dentro del tipo

2057
01:09:17,759 --> 01:09:20,100
de marco arquitectónico, por lo que

2058
01:09:20,100 --> 01:09:22,439
no es demasiado sorprendente que también encuentre

2059
01:09:22,439 --> 01:09:24,359
ese divorcio a nivel neuropsicológico.  la

2060
01:09:24,359 --> 01:09:25,979


2061
01:09:25,979 --> 01:09:28,319


2062
01:09:28,319 --> 01:09:30,960
predicción del lenguaje es

2063
01:09:30,960 --> 01:09:34,140
pensamiento idea evolutiva entonces correcto

2064
01:09:34,140 --> 01:09:36,960
como uh si eso no es si estás diciendo que

2065
01:09:36,960 --> 01:09:39,060
eso no predice que el pensamiento

2066
01:09:39,060 --> 01:09:41,160
se basa en el lenguaje

2067
01:09:41,160 --> 01:09:44,399
um entonces entonces uh creo que a quien le guste

2068
01:09:44,399 --> 01:09:45,899
esa teoría debería hacer

2069
01:09:45,899 --> 01:09:47,520
algunas predicciones

2070
01:09:47,520 --> 01:09:49,979
um acerca de uh sabes lo que esa

2071
01:09:49,979 --> 01:09:51,660
teoría realmente significa quiero decir que siento que

2072
01:09:51,660 --> 01:09:53,819
ese tipo de predicciones a menudo son

2073
01:09:53,819 --> 01:09:55,440
realmente necesarias para comprender el

2074
01:09:55,440 --> 01:09:57,360
contenido de una predicción

2075
01:09:57,360 --> 01:09:58,320
um

2076
01:09:58,320 --> 01:10:00,540
um lo siento mucho Daniel tu mano ha

2077
01:10:00,540 --> 01:10:03,660
estado levantada por un tiempo uh no  todo está bien,

2078
01:10:03,660 --> 01:10:05,880
solo quería

2079
01:10:05,880 --> 01:10:08,040
traer

2080
01:10:08,040 --> 01:10:12,660
un respiro y una oportunidad para que

2081
01:10:12,660 --> 01:10:15,480
cualquiera pueda hacer otras

2082
01:10:15,480 --> 01:10:17,520
preguntas, pero wow,

2083
01:10:17,520 --> 01:10:20,460
gracias a ambos por los muchos temas que hemos

2084
01:10:20,460 --> 01:10:21,780
cubierto,

2085
01:10:21,780 --> 01:10:22,500


2086
01:10:22,500 --> 01:10:25,199
tendremos en los últimos minutos, eh, algo

2087
01:10:25,199 --> 01:10:27,120
así  conclusión y próximos pasos, pero Dave, ¿

2088
01:10:27,120 --> 01:10:29,820
te gustaría hacer una pregunta o simplemente

2089
01:10:29,820 --> 01:10:32,840
dar una breve reflexión? De

2090
01:10:36,900 --> 01:10:38,880


2091
01:10:38,880 --> 01:10:39,960
acuerdo, no,

2092
01:10:39,960 --> 01:10:42,239
hay muchos comentarios en el chat, así que

2093
01:10:42,239 --> 01:10:45,120
espero que ambos puedan leerlos en

2094
01:10:45,120 --> 01:10:47,460
su propio tiempo para ver lo que cada uno

2095
01:10:47,460 --> 01:10:49,440
agregó.

2096
01:10:49,440 --> 01:10:52,860
vamos desde aquí mientras

2097
01:10:52,860 --> 01:10:57,179
rugimos hacia mayo de 2023 y más allá, ¿

2098
01:10:57,179 --> 01:11:00,960
qué pueden hacer los lingüistas, desarrolladores y usuarios de grandes modelos de lenguaje,

2099
01:11:00,960 --> 01:11:02,940


2100
01:11:02,940 --> 01:11:05,159
científicos cognitivos, cuáles creen que son

2101
01:11:05,159 --> 01:11:06,960
algunos de

2102
01:11:06,960 --> 01:11:08,159


2103
01:11:08,159 --> 01:11:10,080


2104
01:11:10,080 --> 01:11:13,260
los

2105
01:11:13,260 --> 01:11:14,699
caminos a seguir más fructíferos?  es realmente

2106
01:11:14,699 --> 01:11:15,600
tomar,

2107
01:11:15,600 --> 01:11:17,460
um, como la psicología cognitiva, no en

2108
01:11:17,460 --> 01:11:18,659
serio, hay un montón de buen trabajo

2109
01:11:18,659 --> 01:11:21,000
recientemente tratando de alinear cosas como

2110
01:11:21,000 --> 01:11:24,060
sí, ya sabes, Church EBT wolf de los

2111
01:11:24,060 --> 01:11:25,739
complementos alfa, la forma en que chat gbt puede

2112
01:11:25,739 --> 01:11:28,080
interactuar con diferentes tipos de módulos,

2113
01:11:28,080 --> 01:11:30,179
um, la forma de construir un legítimo  tipo

2114
01:11:30,179 --> 01:11:32,580
de sistema AGI, no necesariamente

2115
01:11:32,580 --> 01:11:34,380
tiene que

2116
01:11:34,380 --> 01:11:35,880
depender psicológicamente del tipo de módulos que

2117
01:11:35,880 --> 01:11:37,500
tienen los seres humanos, pero creo que se

2118
01:11:37,500 --> 01:11:38,820
beneficiará de él, por lo que ha habido

2119
01:11:38,820 --> 01:11:41,040
algunas afirmaciones de que los modelos de lenguaje grandes

2120
01:11:41,040 --> 01:11:42,540
tal vez pueden saberlo todo.  tipo de

2121
01:11:42,540 --> 01:11:43,860
cosas bien todo todo todo lo que te

2122
01:11:43,860 --> 01:11:44,760
gusta

2123
01:11:44,760 --> 01:11:46,620
pero creo que a la larga lo más

2124
01:11:46,620 --> 01:11:47,880
probable es que las películas

2125
01:11:47,880 --> 01:11:49,380
puedan hacer algo muy importante y muy

2126
01:11:49,380 --> 01:11:51,000
interesante, pero solo será

2127
01:11:51,000 --> 01:11:53,100
una pieza del rompecabezas, así que, de hecho, incluso

2128
01:11:53,100 --> 01:11:55,440
abierto  El CEO de AI, Sam Altman, dijo la semana pasada

2129
01:11:55,440 --> 01:11:56,400
que,

2130
01:11:56,400 --> 01:11:58,440
um, sabes, lo que podemos hacer con las películas

2131
01:11:58,440 --> 01:12:00,120
realmente se ha agotado, necesitamos

2132
01:12:00,120 --> 01:12:02,520
nuevas direcciones, nuevas, nuevas avenidas, etc.

2133
01:12:02,520 --> 01:12:04,860


2134
01:12:04,860 --> 01:12:07,440


2135
01:12:07,440 --> 01:12:09,060
estudiantes aquí, pero

2136
01:12:09,060 --> 01:12:11,159
creo que también tiene razón, sabes que las películas pueden

2137
01:12:11,159 --> 01:12:12,300
hacer algo espectacular, pero

2138
01:12:12,300 --> 01:12:14,760
probablemente formarán una pequeña parte de

2139
01:12:14,760 --> 01:12:17,760
la arquitectura general de AGI, ¿

2140
01:12:17,760 --> 01:12:19,679


2141
01:12:19,679 --> 01:12:21,900


2142
01:12:21,900 --> 01:12:25,679
verdad?  Pienso mucho en eso, así que

2143
01:12:25,679 --> 01:12:28,020
déjame darme otro ejemplo aquí, así que,

2144
01:12:28,020 --> 01:12:29,820
um, Anna, incluso más,

2145
01:12:29,820 --> 01:12:31,920
um, que es una científica muy buena y productiva,

2146
01:12:31,920 --> 01:12:34,140
tiene un artículo recientemente, um, que

2147
01:12:34,140 --> 01:12:35,400
aboga por una especie de

2148
01:12:35,400 --> 01:12:37,800
arquitectura modular para películas y que es un

2149
01:12:37,800 --> 01:12:39,060
marco muy bueno, ¿verdad?  muy

2150
01:12:39,060 --> 01:12:40,860
plausible desde el punto de vista cognitivo, es exactamente el

2151
01:12:40,860 --> 01:12:41,880
tipo de cosa que deberíamos impulsar,

2152
01:12:41,880 --> 01:12:43,679
es compatible con la

2153
01:12:43,679 --> 01:12:45,239
noción de inteligencias múltiples de Howard Gardner, etc.

2154
01:12:45,239 --> 01:12:46,800


2155
01:12:46,800 --> 01:12:48,000


2156
01:12:48,000 --> 01:12:49,620


2157
01:12:49,620 --> 01:12:51,360


2158
01:12:51,360 --> 01:12:54,840
Creo que hace una semana o tal vez hace unos días, donde

2159
01:12:54,840 --> 01:12:56,880
muchas otras cosas se pueden combinar con la

2160
01:12:56,880 --> 01:12:59,520
exageración de la IA de manera improductiva, por lo que Greg

2161
01:12:59,520 --> 01:13:02,100
Brockman de openai dio

2162
01:13:02,100 --> 01:13:04,199
una de sus grandes charlas Ted donde

2163
01:13:04,199 --> 01:13:06,179
mostró diferentes complementos que chatean GPD.  ¿

2164
01:13:06,179 --> 01:13:08,159
Puedo hacer? Mencioné que Wolfram opera, pero

2165
01:13:08,159 --> 01:13:09,300
también hay cosas como la

2166
01:13:09,300 --> 01:13:11,820
generación de imágenes, compras instantáneas, donde

2167
01:13:11,820 --> 01:13:13,560
puede obtener chat TV para comprar cosas y

2168
01:13:13,560 --> 01:13:15,060
lo que tiene,

2169
01:13:15,060 --> 01:13:17,040
um y nuevamente, esto lo lleva de vuelta a

2170
01:13:17,040 --> 01:13:18,960
la idea de que múltiples subsistemas pueden hacer

2171
01:13:18,960 --> 01:13:20,940
diferentes subfunciones, así que Brockovich

2172
01:13:20,940 --> 01:13:22,620
también mostró un ejemplo de dar a chat

2173
01:13:22,620 --> 01:13:26,820
GPT un archivo de Excel un archivo CSV y de una

2174
01:13:26,820 --> 01:13:28,739
base de datos de archivo de trabajos académicos

2175
01:13:28,739 --> 01:13:30,060
donde solo enumeraba un montón de trabajos

2176
01:13:30,060 --> 01:13:31,739
y luego títulos y qué tenías

2177
01:13:31,739 --> 01:13:32,880
razón

2178
01:13:32,880 --> 01:13:34,500
um y dijo que sabes

2179
01:13:34,500 --> 01:13:37,440
usar chatipati usa  Conocimiento mundial para

2180
01:13:37,440 --> 01:13:39,060
inferir lo que significan los títulos de las columnas,

2181
01:13:39,060 --> 01:13:40,860
por lo que entendimos que sabe que

2182
01:13:40,860 --> 01:13:42,780
título significa el título del artículo

2183
01:13:42,780 --> 01:13:44,640
entendió que los autores significan el número

2184
01:13:44,640 --> 01:13:47,280
de autores por artículo entendió que

2185
01:13:47,280 --> 01:13:48,840
creado significa la fecha en que se

2186
01:13:48,840 --> 01:13:50,820
envió el artículo correctamente y porque es un

2187
01:13:50,820 --> 01:13:52,080
Charla de TED: toda la audiencia

2188
01:13:52,080 --> 01:13:54,120
nos dio una ovación de pie,

2189
01:13:54,120 --> 01:13:56,340
pero supongo que la capacidad de describir etiquetas en

2190
01:13:56,340 --> 01:13:57,900
un archivo de Excel

2191
01:13:57,900 --> 01:14:01,679
es agradable, pero no estoy seguro de que

2192
01:14:01,679 --> 01:14:03,120
realmente lo llames Conocimiento mundial, así que

2193
01:14:03,120 --> 01:14:04,800
supongo que hay mucho.  Solo diría que se

2194
01:14:04,800 --> 01:14:06,719
necesita mucho progreso

2195
01:14:06,719 --> 01:14:08,880
junto con la reducción del antropomosoma

2196
01:14:08,880 --> 01:14:11,640
y tienes el equilibrio correcto, así que, como

2197
01:14:11,640 --> 01:14:12,659
dije, debes tener el

2198
01:14:12,659 --> 01:14:13,920
equilibrio correcto de

2199
01:14:13,920 --> 01:14:15,360
um, tipo de

2200
01:14:15,360 --> 01:14:17,100
arquitectura modular psicológicamente plausible, pero no puedes tener

2201
01:14:17,100 --> 01:14:18,480
también  mucho

2202
01:14:18,480 --> 01:14:19,920
um antropomorfismo porque entonces te

2203
01:14:19,920 --> 01:14:21,540
dejarás llevar, tienes que encontrar que tenemos que

2204
01:14:21,540 --> 01:14:22,860
encontrar el equilibrio correcto entre

2205
01:14:22,860 --> 01:14:25,739
modelar una especie de sistemas modulares uh similares a los humanos,

2206
01:14:25,739 --> 01:14:27,780
pero no hacerlo en un grado

2207
01:14:27,780 --> 01:14:29,280
que es

2208
01:14:29,280 --> 01:14:31,140
un poco inverosímil o científicamente

2209
01:14:31,140 --> 01:14:33,679
inútil.

2210
01:14:35,040 --> 01:14:37,500
Quiero decir, creo que estoy de acuerdo con todo lo que

2211
01:14:37,500 --> 01:14:40,199
estoy realmente entusiasmado con estas formas

2212
01:14:40,199 --> 01:14:42,739
de conectar modelos de lenguaje con

2213
01:14:42,739 --> 01:14:45,120
otras formas de

2214
01:14:45,120 --> 01:14:47,640
procesamiento de información, lo que parece ser lo que

2215
01:14:47,640 --> 01:14:49,679
tiene la gente.  Me

2216
01:14:49,679 --> 01:14:52,620
han sorprendido mucho

2217
01:14:52,620 --> 01:14:54,840
las cosas que son capaces de hacer,

2218
01:14:54,840 --> 01:14:58,260
como modelar el lenguaje, así que

2219
01:14:58,260 --> 01:15:00,120
sabes diferentes tipos de

2220
01:15:00,120 --> 01:15:01,920
acertijos de razonamiento y cosas que pueden resolver.

2221
01:15:01,920 --> 01:15:05,340
Creo que es realmente fascinante

2222
01:15:05,340 --> 01:15:07,920
y sabes que tal vez lo haga.  requieren que

2223
01:15:07,920 --> 01:15:09,719
reconsideremos nuestras ya sabes las

2224
01:15:09,719 --> 01:15:11,760
relaciones entre el lenguaje y el

2225
01:15:11,760 --> 01:15:13,500
pensamiento y tratemos de encontrar una manera

2226
01:15:13,500 --> 01:15:15,239
de ser específicos sobre lo que significa que

2227
01:15:15,239 --> 01:15:17,760
algo tenga una

2228
01:15:17,760 --> 01:15:19,380
representación o razonar sobre

2229
01:15:19,380 --> 01:15:21,060
esa representación Pero, en última instancia,

2230
01:15:21,060 --> 01:15:23,820
creo que estoy de acuerdo  que um

2231
01:15:23,820 --> 01:15:26,219
um sabes que las personas tienen diferentes modos

2232
01:15:26,219 --> 01:15:28,560
de pensar acerca de las cosas y que eso

2233
01:15:28,560 --> 01:15:32,400
parece importante para la inteligencia

2234
01:15:32,400 --> 01:15:35,040
um también estoy muy entusiasmado con el

2235
01:15:35,040 --> 01:15:37,620
desafío de LM para bebés, así que creo que en el

2236
01:15:37,620 --> 01:15:39,960
lado lingüístico correcto um

2237
01:15:39,960 --> 01:15:42,300
uh eso es exactamente el  Lo correcto es

2238
01:15:42,300 --> 01:15:45,300
ver hasta dónde podemos llegar con

2239
01:15:45,300 --> 01:15:47,219
conjuntos de datos más pequeños y tal vez

2240
01:15:47,219 --> 01:15:50,820
eventualmente después de eso, trata de

2241
01:15:50,820 --> 01:15:53,699
entender un poco más sobre

2242
01:15:53,699 --> 01:15:55,739
los tipos de semántica que

2243
01:15:55,739 --> 01:15:57,840
adquieren los niños y de dónde la obtienen

2244
01:15:57,840 --> 01:15:59,820
y cómo.  el tipo de semántica externa

2245
01:15:59,820 --> 01:16:02,520
puede informar el aprendizaje de idiomas o,

2246
01:16:02,520 --> 01:16:04,560
específicamente, tal vez el aprendizaje de gramática y

2247
01:16:04,560 --> 01:16:06,600
sintaxis.

2248
01:16:06,600 --> 01:16:09,480


2249
01:16:09,480 --> 01:16:12,179


2250
01:16:12,179 --> 01:16:15,840


2251
01:16:15,840 --> 01:16:18,300


2252
01:16:18,300 --> 01:16:21,179
más allá de las expectativas de la gente

2253
01:16:21,179 --> 01:16:23,340
para este tipo de clase

2254
01:16:23,340 --> 01:16:25,500
de modelo correcto tipo de

2255
01:16:25,500 --> 01:16:27,960
aprendizaje estadístico básico descubrimiento de

2256
01:16:27,960 --> 01:16:29,940
patrones en el texto

2257
01:16:29,940 --> 01:16:30,719
um

2258
01:16:30,719 --> 01:16:32,640
um parece que da

2259
01:16:32,640 --> 01:16:35,040
resultados realmente notables

2260
01:16:35,040 --> 01:16:37,020
um y eso para mí en el futuro creo que

2261
01:16:37,020 --> 01:16:39,360
acaba de introducir una gran ola de

2262
01:16:39,360 --> 01:16:42,000
incertidumbre sobre las teorías, así que creo

2263
01:16:42,000 --> 01:16:43,620
que nuestras teorías de básicamente

2264
01:16:43,620 --> 01:16:46,739
todo en el lenguaje con seguridad,

2265
01:16:46,739 --> 01:16:48,960
pero la cognición probablemente Neurociencia,

2266
01:16:48,960 --> 01:16:50,640
como todas esas cosas, creo que

2267
01:16:50,640 --> 01:16:53,100
van a ser reelaboradas

2268
01:16:53,100 --> 01:16:54,420
cuando realmente lleguemos a

2269
01:16:54,420 --> 01:16:56,219
entender la

2270
01:16:56,219 --> 01:16:58,560
um uh la capacidad  de tipos realmente generales

2271
01:16:58,560 --> 01:17:02,219
de sistemas de aprendizaje como estos, así que eso

2272
01:17:02,219 --> 01:17:03,900
hace que, por un

2273
01:17:03,900 --> 01:17:05,820
lado, sea

2274
01:17:05,820 --> 01:17:07,440
un poco fastidioso para las

2275
01:17:07,440 --> 01:17:09,360
teorías pasadas, especialmente las teorías que se

2276
01:17:09,360 --> 01:17:11,640
basaron en

2277
01:17:11,640 --> 01:17:14,100
que el aprendizaje no puede

2278
01:17:14,100 --> 01:17:15,659
funcionar bien.

2279
01:17:15,659 --> 01:17:18,000
pero por el lado positivo, creo que lo convierte

2280
01:17:18,000 --> 01:17:20,460
en un momento muy emocionante tanto para la

2281
01:17:20,460 --> 01:17:22,620
IA como para la ciencia cognitiva y la

2282
01:17:22,620 --> 01:17:24,000
lingüística,

2283
01:17:24,000 --> 01:17:25,560
um, donde ahora están estas

2284
01:17:25,560 --> 01:17:27,420
herramientas realmente poderosas,

2285
01:17:27,420 --> 01:17:29,719
um, que parecen un

2286
01:17:29,719 --> 01:17:32,940
paso cualitativamente uh de diferente tamaño hacia las

2287
01:17:32,940 --> 01:17:34,620
habilidades humanas,

2288
01:17:34,620 --> 01:17:36,120
um  y pienso en integrarlos

2289
01:17:36,120 --> 01:17:39,060
y tomar tanto el tipo de

2290
01:17:39,060 --> 01:17:41,280
lecciones de ingeniería como el tipo de

2291
01:17:41,280 --> 01:17:43,679
lecciones filosóficas sobre cómo se

2292
01:17:43,679 --> 01:17:45,540
hacen y qué tipo de principios se incluyen

2293
01:17:45,540 --> 01:17:47,460
en el diseño de sistemas inteligentes.

2294
01:17:47,460 --> 01:17:49,679
Creo que esas cosas

2295
01:17:49,679 --> 01:17:52,080
realmente dará forma a la sensación en los próximos

2296
01:17:52,080 --> 01:17:53,699
cinco o diez años,

2297
01:17:53,699 --> 01:17:54,900
um

2298
01:17:54,900 --> 01:17:57,000
y también como diría en el

2299
01:17:57,000 --> 01:17:58,500
contexto de temas más amplios aquí,

2300
01:17:58,500 --> 01:17:59,760
como si tuvieras toda la razón, como

2301
01:17:59,760 --> 01:18:01,620
recuerdo cuando estaba leyendo sobre cuando el

2302
01:18:01,620 --> 01:18:04,920
azul profundo sea uh Casper de  fue el

2303
01:18:04,920 --> 01:18:07,260
ajedrez, eh, sí, sí, y hubo

2304
01:18:07,260 --> 01:18:08,940
algunos comentaristas que dijeron que sabes que el

2305
01:18:08,940 --> 01:18:12,480
ajedrez se acabó si una IA puede ser un humano,

2306
01:18:12,480 --> 01:18:13,739
entonces se acabó el juego cuál es el punto de

2307
01:18:13,739 --> 01:18:15,420
estudiar ajedrez sabes que ya no hay necesidad

2308
01:18:15,420 --> 01:18:16,920
de aburrir

2309
01:18:16,920 --> 01:18:18,900
um y supongo  si la IA aparentemente ha logrado

2310
01:18:18,900 --> 01:18:20,640
todo lo que los humanos necesitan

2311
01:18:20,640 --> 01:18:22,140
hacer para jugar al ajedrez, ¿cuál es el punto de

2312
01:18:22,140 --> 01:18:23,100
jugarlo?

2313
01:18:23,100 --> 01:18:24,540
um, pero creo que saben si algo

2314
01:18:24,540 --> 01:18:26,219
resultó para aumentar la popularidad del

2315
01:18:26,219 --> 01:18:27,900
ajedrez, ¿verdad? Están en nuestros mini-

2316
01:18:27,900 --> 01:18:29,580
celebridades del ajedrez, así como en

2317
01:18:29,580 --> 01:18:31,199
torneos mundiales.  y yo Predeciría que

2318
01:18:31,199 --> 01:18:32,520
probablemente también sucederá lo mismo con el

2319
01:18:32,520 --> 01:18:34,620
lenguaje, sabes que las películas no significan que sea

2320
01:18:34,620 --> 01:18:36,360
el fin del lenguaje, no más

2321
01:18:36,360 --> 01:18:37,800
lenguaje, no más lingüística.

2322
01:18:37,800 --> 01:18:39,179


2323
01:18:39,179 --> 01:18:40,620


2324
01:18:40,620 --> 01:18:42,540
LMS

2325
01:18:42,540 --> 01:18:44,400
aumentará el interés general en la

2326
01:18:44,400 --> 01:18:46,199
teoría lingüística debido a su emparejamiento, conoce

2327
01:18:46,199 --> 01:18:48,060
restricciones extrañas y

2328
01:18:48,060 --> 01:18:50,100
limitaciones aparentes, porque también

2329
01:18:50,100 --> 01:18:52,380
diría que conoce la escala en este punto, la

2330
01:18:52,380 --> 01:18:55,500
escala del problema del estrés está definitivamente

2331
01:18:55,500 --> 01:18:57,420
lejos de todo lo que se necesita lo que

2332
01:18:57,420 --> 01:18:59,820
falta es  una capacidad de LMS para que sepa

2333
01:18:59,820 --> 01:19:01,260
realmente abstraer su conocimiento y

2334
01:19:01,260 --> 01:19:03,239
experiencias para hacer

2335
01:19:03,239 --> 01:19:04,739
predicciones y generalizaciones de lavado de grupo, etc.

2336
01:19:04,739 --> 01:19:06,780
Di algunos ejemplos, pero hay algunos

2337
01:19:06,780 --> 01:19:07,860
otros en la literatura donde

2338
01:19:07,860 --> 01:19:09,179
no parece ser realmente bueno para

2339
01:19:09,179 --> 01:19:10,739
generalizar, puede  ir de tal vez

2340
01:19:10,739 --> 01:19:13,620
tipos de tokens particulares y, pero me gustaría que

2341
01:19:13,620 --> 01:19:15,060
supieras que supongo que mi

2342
01:19:15,060 --> 01:19:17,100
afirmación final sería que sabes que la

2343
01:19:17,100 --> 01:19:19,080
literatura de adquisición de idiomas,

2344
01:19:19,080 --> 01:19:21,480
um, no necesariamente necesita películas, aunque

2345
01:19:21,480 --> 01:19:22,860
sabes que los científicos cognitivos

2346
01:19:22,860 --> 01:19:26,219
realmente no necesitan películas que podríamos potencialmente  uh, ya

2347
01:19:26,219 --> 01:19:27,420
sabes, reincorpora y obviamente

2348
01:19:27,420 --> 01:19:29,640
no estás de acuerdo aquí, pero yo diría que las grandes

2349
01:19:29,640 --> 01:19:32,040
empresas de tecnología que se benefician de las películas necesitan

2350
01:19:32,040 --> 01:19:33,360
películas, ¿verdad? Son las únicas que

2351
01:19:33,360 --> 01:19:35,100
realmente lo hacen. Puede ser que la

2352
01:19:35,100 --> 01:19:37,320
mente sea muy. Diré que sabes que la

2353
01:19:37,320 --> 01:19:39,000
mente es.  En un espacio muy diverso, puede ser

2354
01:19:39,000 --> 01:19:40,560
que haya ciertas formas de comportamiento

2355
01:19:40,560 --> 01:19:42,420
y aprendizaje que puedan ser capturadas por

2356
01:19:42,420 --> 01:19:44,219
procesos similares a los que están haciendo las películas,

2357
01:19:44,219 --> 01:19:45,540
por lo que Stephen ha dado algunos

2358
01:19:45,540 --> 01:19:47,159
ejemplos interesantes en sus artículos sobre el magnetismo

2359
01:19:47,159 --> 01:19:49,320
y un tipo extraño de reglas de aprendizaje

2360
01:19:49,320 --> 01:19:51,060
que son  muy dominio General y muy

2361
01:19:51,060 --> 01:19:52,920
rápido y muy misterioso así que sabes que

2362
01:19:52,920 --> 01:19:54,780
tal vez para ese tipo de cosas

2363
01:19:54,780 --> 01:19:56,100
ese tipo de aprendizaje será

2364
01:19:56,100 --> 01:19:57,780
relevante y pero sigo pensando que es

2365
01:19:57,780 --> 01:19:59,580
poco probable que uno de los candidatos

2366
01:19:59,580 --> 01:20:02,100
sea el lenguaje natural y al menos de la manera

2367
01:20:02,100 --> 01:20:03,480
natural  el lenguaje funciona y es todo su

2368
01:20:03,480 --> 01:20:05,219
esplendor en términos de la forma, principalmente la

2369
01:20:05,219 --> 01:20:07,380
regulación y lo que tienes, así que supongo que

2370
01:20:07,380 --> 01:20:09,300
deberías saber que me recuerda un poco

2371
01:20:09,300 --> 01:20:11,520
dónde sabes que tienes esta

2372
01:20:11,520 --> 01:20:13,320
imagen de Vi a John Wick capítulo cuatro

2373
01:20:13,320 --> 01:20:14,940
recientemente y él tiene  esta hay

2374
01:20:14,940 --> 01:20:16,140
esta escena en la que está caminando en el

2375
01:20:16,140 --> 01:20:17,580
desierto y no está seguro de haber visto a

2376
01:20:17,580 --> 01:20:19,260
este tipo como si quisiera asesinarlo, es

2377
01:20:19,260 --> 01:20:21,540
como cuando caminas en el

2378
01:20:21,540 --> 01:20:22,739
desierto

2379
01:20:22,739 --> 01:20:24,840
um y tienes la ilusión de ver un

2380
01:20:24,840 --> 01:20:26,460
oasis porque resulta que tú'  estás

2381
01:20:26,460 --> 01:20:28,199
alucinando, pero luego te das cuenta de que,

2382
01:20:28,199 --> 01:20:29,820
a veces, antes de que sea demasiado tarde, sabes

2383
01:20:29,820 --> 01:20:31,679
que en realidad estás alucinando, es que

2384
01:20:31,679 --> 01:20:33,239
no estás viendo un oasis, todavía estás

2385
01:20:33,239 --> 01:20:35,040
en el desierto y creo que esa es

2386
01:20:35,040 --> 01:20:37,199
quizás la situación en la que nos encontramos ahora.

2387
01:20:37,199 --> 01:20:39,360
con la competencia lingüística de muchos

2388
01:20:39,360 --> 01:20:41,400
modelos lingüísticos, tenemos la ilusión

2389
01:20:41,400 --> 01:20:44,940
de la competencia lingüística, ya sabes,

2390
01:20:44,940 --> 01:20:46,260
siempre ves la ilusión antes de

2391
01:20:46,260 --> 01:20:48,360
encontrar el Oasis correcto, así que creo que creo que

2392
01:20:48,360 --> 01:20:49,440
ahora mismo estamos en el

2393
01:20:49,440 --> 01:20:51,420
estado alucinante del desierto donde

2394
01:20:51,420 --> 01:20:53,699
estoy viendo chispas potenciales de

2395
01:20:53,699 --> 01:20:55,380
competencia lingüística, pero aún no es

2396
01:20:55,380 --> 01:20:57,420
muy claro y sólido,

2397
01:20:57,420 --> 01:20:59,640
um, en realidad no hemos llegado al Oasis

2398
01:20:59,640 --> 01:21:02,480
todavía, sí,

2399
01:21:02,820 --> 01:21:06,360
um, solo una pregunta rápida, así que vea si

2400
01:21:06,360 --> 01:21:09,540
puede dar una respuesta breve para que

2401
01:21:09,540 --> 01:21:11,219
svenochino

2402
01:21:11,219 --> 01:21:13,440
escriba la pregunta, ¿es correcto?  para decir

2403
01:21:13,440 --> 01:21:15,120
que los modelos de lenguaje grande no tienen

2404
01:21:15,120 --> 01:21:17,540
antecedentes ¿los

2405
01:21:18,480 --> 01:21:21,300
modelos de lenguaje grande tienen antecedentes? Yo

2406
01:21:21,300 --> 01:21:23,820
diría que sí, definitivamente los tienen,

2407
01:21:23,820 --> 01:21:25,620
um y

2408
01:21:25,620 --> 01:21:28,440
um, creo que la diferencia en

2409
01:21:28,440 --> 01:21:30,360
cómo las personas que conoces están acostumbradas a pensar

2410
01:21:30,360 --> 01:21:32,340
en los antecedentes y en la inferencia bayesiana

2411
01:21:32,340 --> 01:21:33,840
para  ejemplo, si desea escribir un

2412
01:21:33,840 --> 01:21:36,300
modelo estadístico bayesiano, dice que sabe,

2413
01:21:36,300 --> 01:21:37,860
aquí están los parámetros y

2414
01:21:37,860 --> 01:21:39,300
aquí están los anteriores en los

2415
01:21:39,300 --> 01:21:40,860
parámetros,

2416
01:21:40,860 --> 01:21:42,360
um, modelos de lenguaje grandes, creo que los

2417
01:21:42,360 --> 01:21:44,340
anteriores son y tal vez nueces neuronales en

2418
01:21:44,340 --> 01:21:45,900
general, creo que el que el  los

2419
01:21:45,900 --> 01:21:47,880
anteriores son mucho más implícitos, así que

2420
01:21:47,880 --> 01:21:49,679
hay algunas funciones que encuentran

2421
01:21:49,679 --> 01:21:52,080
más fáciles de aprender que otras

2422
01:21:52,080 --> 01:21:53,760
funciones e incluso hay algo de trabajo

2423
01:21:53,760 --> 01:21:55,860
tratando de descubrir que conoces alguna

2424
01:21:55,860 --> 01:21:57,540
declaración de qué tipo de

2425
01:21:57,540 --> 01:21:59,040
previos implícitos son,

2426
01:21:59,040 --> 01:22:00,840
pero eso es realmente lo que pienso.

2427
01:22:00,840 --> 01:22:02,040
sobre um

2428
01:22:02,040 --> 01:22:02,940


2429
01:22:02,940 --> 01:22:04,620
um sabes comparación de diferentes

2430
01:22:04,620 --> 01:22:07,199
arquitecturas de redes neuronales cierto

2431
01:22:07,199 --> 01:22:09,060
um uh que tal vez sea algo eufórico

2432
01:22:09,060 --> 01:22:10,500
y podría estar de acuerdo en que

2433
01:22:10,500 --> 01:22:12,600
tienes que encontrar antecedentes que les permitan

2434
01:22:12,600 --> 01:22:14,400
aprender las cosas que los niños aprenden

2435
01:22:14,400 --> 01:22:16,140
correctamente y

2436
01:22:16,140 --> 01:22:19,080
um no todas las arquitecturas lo harán  haz eso,

2437
01:22:19,080 --> 01:22:21,360
um, incluso entre arquitecturas que se están

2438
01:22:21,360 --> 01:22:23,280
volviendo completas o capaces de aprender

2439
01:22:23,280 --> 01:22:24,719
cualquier tipo de función, no todas lo

2440
01:22:24,719 --> 01:22:27,540
harán, eh, incluso en

2441
01:22:27,540 --> 01:22:30,000
conjuntos de datos de gran tamaño, así que

2442
01:22:30,000 --> 01:22:31,800
um, creo que este tipo de búsqueda en

2443
01:22:31,800 --> 01:22:34,080
arquitecturas de redes neuronales es como  realmente uno de

2444
01:22:34,080 --> 01:22:36,360
una búsqueda sobre anteriores

2445
01:22:36,360 --> 01:22:38,580
um pero no son anteriores o quiero decir que

2446
01:22:38,580 --> 01:22:39,719
podrías pensar en ello como una búsqueda sobre

2447
01:22:39,719 --> 01:22:41,699
gramática Universal o algo así pero

2448
01:22:41,699 --> 01:22:44,340
es no es anterior o

2449
01:22:44,340 --> 01:22:46,560
gramática Universal en el sentido de que la gente ha

2450
01:22:46,560 --> 01:22:48,540
hablado de ello como  una

2451
01:22:48,540 --> 01:22:50,159
declaración explícita sobre qué tipo de reglas están

2452
01:22:50,159 --> 01:22:51,780
permitidas o una declaración explícita sobre

2453
01:22:51,780 --> 01:22:53,880
qué tipo de funciones son de alta

2454
01:22:53,880 --> 01:22:55,320
probabilidad o algo así,

2455
01:22:55,320 --> 01:22:57,360
todo está codificado implícitamente,

2456
01:22:57,360 --> 01:22:58,560
sí, sí, totalmente, creo, creo que

2457
01:22:58,560 --> 01:23:00,540
es correcto, quiero decir, sabes que la verdadera

2458
01:23:00,540 --> 01:23:02,880
pregunta es reducir  el espacio de lo que

2459
01:23:02,880 --> 01:23:05,100
aquellos que aprecian por igual y si es

2460
01:23:05,100 --> 01:23:07,020
algo remotamente parecido a lo que los seres humanos

2461
01:23:07,020 --> 01:23:09,300
están haciendo, entonces diría que

2462
01:23:09,300 --> 01:23:11,219
al menos diría que cosas como qpt3

2463
01:23:11,219 --> 01:23:13,380
existen como prueba de que usted sabe que

2464
01:23:13,380 --> 01:23:15,960
construir categorías sintácticas que funcionen completamente

2465
01:23:15,960 --> 01:23:18,000
a partir del análisis de distribución superficial

2466
01:23:18,000 --> 01:23:21,420
esto por sí solo es posible,

2467
01:23:21,420 --> 01:23:24,120
sí, eso es correcto, pero aún así

2468
01:23:24,120 --> 01:23:27,300
diría que la mayoría de los practicantes

2469
01:23:27,300 --> 01:23:29,460
realmente no creen que las categorías sintácticas

2470
01:23:29,460 --> 01:23:31,380
sean innatas, por lo que el problema anterior es

2471
01:23:31,380 --> 01:23:33,060
un poco menos irrelevante aquí, son las

2472
01:23:33,060 --> 01:23:35,219
operaciones las que están configuradas para ser innatas, por lo que

2473
01:23:35,219 --> 01:23:37,500
en  el dominio de la sintaxis son

2474
01:23:37,500 --> 01:23:39,659
cálculos lingüísticos particulares que se dice que

2475
01:23:39,659 --> 01:23:41,159
están en una y categorías en sí mismas, de

2476
01:23:41,159 --> 01:23:42,719
hecho, incluso Charles Yang

2477
01:23:42,719 --> 01:23:44,100
um ha admitido en los últimos dos años

2478
01:23:44,100 --> 01:23:46,800
que tal vez estén en él, pero tal vez no,

2479
01:23:46,800 --> 01:23:49,679
por lo que las personas han dado otra

2480
01:23:49,679 --> 01:23:51,540
prioridad relevante son cosas  como

2481
01:23:51,540 --> 01:23:53,100
um, me conoces y Gary Markets hemos

2482
01:23:53,100 --> 01:23:55,080
hablado sobre la composicionalidad que parece

2483
01:23:55,080 --> 01:23:56,640
ser un gran problema, por lo que la gente ha dado

2484
01:23:56,640 --> 01:23:59,520
artículos de chat gbt BBC News pidiéndole que

2485
01:23:59,520 --> 01:24:02,280
lo comprima y luego vuelva a explicarlo, así que

2486
01:24:02,280 --> 01:24:04,920
un ejemplo que vi fue Peter Smith 58 es

2487
01:24:04,920 --> 01:24:06,480
ser arrestado por cargos de

2488
01:24:06,480 --> 01:24:09,060
homicidio involuntario y lo comprimes

2489
01:24:09,060 --> 01:24:10,920
y vuelves a explicarlo y resulta que

2490
01:24:10,920 --> 01:24:12,420
58 personas están siendo acusadas de

2491
01:24:12,420 --> 01:24:14,040
homicidio involuntario, ese es un ejemplo bastante claro

2492
01:24:14,040 --> 01:24:15,659
de la falta de composición que se

2493
01:24:15,659 --> 01:24:17,219
está incorporando en cualquier compresión

2494
01:24:17,219 --> 01:24:19,260
que esté haciendo y hay  otro ejemplo

2495
01:24:19,260 --> 01:24:20,940
en el que ha habido algunos ejemplos

2496
01:24:20,940 --> 01:24:23,159
de razonamiento analógico potencial, por lo que en el

2497
01:24:23,159 --> 01:24:24,840
chat de Bing sabes que Bing tiene esta

2498
01:24:24,840 --> 01:24:26,100
función de trampa,

2499
01:24:26,100 --> 01:24:28,080
la pregunta es si solo encuentra

2500
01:24:28,080 --> 01:24:29,460
metarelaciones que ya han sido

2501
01:24:29,460 --> 01:24:31,080
documentadas por humanos o si realmente está

2502
01:24:31,080 --> 01:24:33,420
creando nuevas relaciones.  cosas

2503
01:24:33,420 --> 01:24:35,100
que se están construyendo

2504
01:24:35,100 --> 01:24:38,520
um así que sabes que alguien preguntó uh me dibujó

2505
01:24:38,520 --> 01:24:41,760
una tabla comparando a Jesucristo con el

2506
01:24:41,760 --> 01:24:44,640
Nokia 9910 justo el teléfono celular Nokia

2507
01:24:44,640 --> 01:24:46,080
9910

2508
01:24:46,080 --> 01:24:47,520
um y dijo que sabes que compararon

2509
01:24:47,520 --> 01:24:49,860
las fechas de lanzamiento comparó el tamaño

2510
01:24:49,860 --> 01:24:53,280
el peso comparó la CPU con  El

2511
01:24:53,280 --> 01:24:55,140
conocimiento todopoderoso de Jesús

2512
01:24:55,140 --> 01:24:57,600
comparó la memoria del teléfono con

2513
01:24:57,600 --> 01:25:00,600
la naturaleza omnisciente de Dios, ¿verdad?

2514
01:25:00,600 --> 01:25:02,760
También creo que dijo que

2515
01:25:02,760 --> 01:25:04,560
ambos resucitaron porque el Nokia fue

2516
01:25:04,560 --> 01:25:06,239
relanzado un par de veces, así que

2517
01:25:06,239 --> 01:25:08,159
el Nokia suena como un  gran respuesta

2518
01:25:08,159 --> 01:25:10,560
lo que está mal con eso está bien

2519
01:25:10,560 --> 01:25:12,120
puede ser tal vez suena mucho como un

2520
01:25:12,120 --> 01:25:13,860
razonamiento analógico, pero también

2521
01:25:13,860 --> 01:25:15,420
tenía algunos bastante extraños en los que era

2522
01:25:15,420 --> 01:25:17,100
como si supieras para la cámara, dijo que no,

2523
01:25:17,100 --> 01:25:19,679
solo dio la descripción de Jesús o

2524
01:25:19,679 --> 01:25:21,659
no es realmente lo que  una cámara es que hay algún

2525
01:25:21,659 --> 01:25:24,000
tipo de cosas que parecen un

2526
01:25:24,000 --> 01:25:27,860
razonamiento analógico, tal vez, pero no está claro, sí, oye,

2527
01:25:27,860 --> 01:25:31,440
creo que eso suena como una

2528
01:25:31,440 --> 01:25:33,020
respuesta increíble para mí.

2529
01:25:33,020 --> 01:25:36,360


2530
01:25:36,360 --> 01:25:38,100


2531
01:25:38,100 --> 01:25:39,719


2532
01:25:39,719 --> 01:25:41,520
categorías, pero como si no solo

2533
01:25:41,520 --> 01:25:43,380
generaran categorías de parte del discurso

2534
01:25:43,380 --> 01:25:45,719
como si tuvieran mucho

2535
01:25:45,719 --> 01:25:47,640
conocimiento sintáctico gramatical

2536
01:25:47,640 --> 01:25:50,880
um uh y además como si tuvieran

2537
01:25:50,880 --> 01:25:52,980
mucho conocimiento semántico y probablemente algo de

2538
01:25:52,980 --> 01:25:55,080
conocimiento pragmático y sabes que son

2539
01:25:55,080 --> 01:25:58,080
ellos  no está mal en la traducción y es

2540
01:25:58,080 --> 01:25:59,880
mucho más que han

2541
01:25:59,880 --> 01:26:01,139
descubierto

2542
01:26:01,139 --> 01:26:03,840
um que solo parte de las categorías del discurso

2543
01:26:03,840 --> 01:26:07,020
um uh bueno lo siento dije que lo siento es

2544
01:26:07,020 --> 01:26:08,940
una categoría técnica cierto sí bueno lo siento sí

2545
01:26:08,940 --> 01:26:11,040
sí pero

2546
01:26:11,040 --> 01:26:13,080
han descubierto  mucho más que eso

2547
01:26:13,080 --> 01:26:13,980
um

2548
01:26:13,980 --> 01:26:15,600
sí um

2549
01:26:15,600 --> 01:26:18,780
voy a ser un um teaser slash

2550
01:26:18,780 --> 01:26:20,699
motivador para que ambos se

2551
01:26:20,699 --> 01:26:23,040
unan uh nuevamente en el futuro con o

2552
01:26:23,040 --> 01:26:24,540
sin otros invitados algunas de las

2553
01:26:24,540 --> 01:26:26,520
preguntas emocionantes solo para que las

2554
01:26:26,520 --> 01:26:28,679
incluyamos en esta transcripción y  luego,

2555
01:26:28,679 --> 01:26:30,060
gracias a Ellie y Steven por

2556
01:26:30,060 --> 01:26:31,260
unirse, así que solo algunas de las últimas

2557
01:26:31,260 --> 01:26:33,719
preguntas que se hicieron. Juan preguntó cómo se

2558
01:26:33,719 --> 01:26:35,820
hacen los pequeños Transformers Jang en todo 2020

2559
01:26:35,820 --> 01:26:38,580
en comparación con los niños que aprenden un idioma.  ¿

2560
01:26:38,580 --> 01:26:40,860


2561
01:26:40,860 --> 01:26:42,800


2562
01:26:42,800 --> 01:26:45,780
Qué restricciones ese espacio

2563
01:26:45,780 --> 01:26:48,420
en las películas no logran mediante la capacitación?

2564
01:26:48,420 --> 01:26:50,699
Entonces, ¿están descubriendo que eso no es lo que

2565
01:26:50,699 --> 01:26:52,500
implementaron al principio? Quizás

2566
01:26:52,500 --> 01:26:54,780
y hay muchas más preguntas, así que

2567
01:26:54,780 --> 01:26:57,540
espero que todos podamos revisar y

2568
01:26:57,540 --> 01:27:00,600
releer los trabajos de los demás y

2569
01:27:00,600 --> 01:27:04,020
reunirse para 41.2 en algún momento futuro

2570
01:27:04,020 --> 01:27:06,060
gracias Elliot y Steven por esta

2571
01:27:06,060 --> 01:27:08,280
transmisión excelente gracias Dave gracias a

2572
01:27:08,280 --> 01:27:10,739
ambos sí muchas gracias

2573
01:27:10,739 --> 01:27:15,379
adiós nos vemos


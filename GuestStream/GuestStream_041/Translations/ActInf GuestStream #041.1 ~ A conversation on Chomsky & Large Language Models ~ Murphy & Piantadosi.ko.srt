1
00:00:05,759 --> 00:00:08,400
안녕하세요. 옥타브 추론 연구소에 오신 여러분을 환영합니다.

2
00:00:08,400 --> 00:00:10,860


3
00:00:10,860 --> 00:00:15,839
2023년 4월 25일 게스트 스트림 번호 41.1입니다.

4
00:00:15,839 --> 00:00:18,480
Elliot Murphy와 Stephen piantadosi와 함께 있습니다.

5
00:00:18,480 --> 00:00:20,939


6
00:00:20,939 --> 00:00:23,039
꽤 토론이 될 것입니다. 우리는

7
00:00:23,039 --> 00:00:24,779
Stephen과

8
00:00:24,779 --> 00:00:27,359
Elliot Elliott의 개회사로 시작할 것입니다.  그런 다음 몇 가지 질문으로 이어지고

9
00:00:27,359 --> 00:00:28,920


10
00:00:28,920 --> 00:00:32,759
마지막에 공개 토론을 할 것입니다. Steven은

11
00:00:32,759 --> 00:00:34,739
참여해 주셔서 감사합니다. 그리고

12
00:00:34,739 --> 00:00:36,960
시작 발언을 부탁드립니다.

13
00:00:36,960 --> 00:00:39,780
안녕하세요, 저는 Steve Piantadosi입니다. 저는 UC Berkeley의

14
00:00:39,780 --> 00:00:42,239
심리학 및 신경과학 교수입니다

15
00:00:42,239 --> 00:00:44,700


16
00:00:44,700 --> 00:00:46,860
음 그리고  어, 우리가 여기 있는 이유 중 일부는

17
00:00:46,860 --> 00:00:48,899
내가 최근에

18
00:00:48,899 --> 00:00:51,719
대규모 언어 모델에 관한 논문을 썼기 때문이라고 생각합니다. 음

19
00:00:51,719 --> 00:00:53,940
부분적으로는 그들이 구문 및 의미론 학습 측면에서 성취한 것에 대한 약간의 열정을 전달하려고 노력했습니다.

20
00:00:53,940 --> 00:00:55,199


21
00:00:55,199 --> 00:00:57,300


22
00:00:57,300 --> 00:00:59,100


23
00:00:59,100 --> 00:01:00,719


24
00:01:00,719 --> 00:01:03,059
음 그리고  부분적으로 저는

25
00:01:03,059 --> 00:01:04,979
이 모델들이 우리가

26
00:01:04,979 --> 00:01:06,780
언어에 대해 어떻게 생각해야 하는지, 음 언어 표현

27
00:01:06,780 --> 00:01:08,520
이론과 어 문법 이론에 대해 어떻게 생각해야 하는지를 정말로 바꾼다고 생각합니다.

28
00:01:08,520 --> 00:01:11,939


29
00:01:11,939 --> 00:01:13,560


30
00:01:13,560 --> 00:01:17,220


31
00:01:17,220 --> 00:01:19,080


32
00:01:19,080 --> 00:01:21,960
meffee 저는

33
00:01:21,960 --> 00:01:23,040


34
00:01:23,040 --> 00:01:25,799
텍사스에 있는 UT Health의 신경외과에서 박사후 연구원입니다

35
00:01:25,799 --> 00:01:27,240
음 Steven의 논문을 큰 관심을 가지고 읽었습니다.

36
00:01:27,240 --> 00:01:29,759


37
00:01:29,759 --> 00:01:31,619


38
00:01:31,619 --> 00:01:32,820


39
00:01:32,820 --> 00:01:34,979
Stephen에 응답하고 Divergence

40
00:01:34,979 --> 00:01:36,600
영역과 관련하여 조사하는 것일

41
00:01:36,600 --> 00:01:38,640
수도 있습니다.

42
00:01:38,640 --> 00:01:41,700
음 Steven의 논문은

43
00:01:41,700 --> 00:01:43,619
현대 기계 학습이

44
00:01:43,619 --> 00:01:46,220


45
00:01:46,220 --> 00:01:48,119
Chomsky 접근 방식의 전체 이론적 프레임워크를 전복하고 우회

46
00:01:48,119 --> 00:01:49,860
했다는 아이디어를 기반으로 합니다.

47
00:01:49,860 --> 00:01:51,479
이러한 주요 주장과

48
00:01:51,479 --> 00:01:52,560
문학의 다른 관련 주장에 대해

49
00:01:52,560 --> 00:01:54,060


50
00:01:54,060 --> 00:01:55,799
듣는 사람들이 약간의 통찰력과 생각을 가지고 있을 수 있으므로

51
00:01:55,799 --> 00:01:58,500


52
00:01:58,500 --> 00:02:00,720
대규모 언어

53
00:02:00,720 --> 00:02:03,180
모델이 다음 토큰을 예측할 뿐이라고 말하는 것은 매우 일반적인 비판입니다.

54
00:02:03,180 --> 00:02:04,079


55
00:02:04,079 --> 00:02:05,579
진부한

56
00:02:05,579 --> 00:02:07,619
맞습니다 그것은 사실이 아닙니다 그리고 그들은

57
00:02:07,619 --> 00:02:09,419
단지 다음 토큰을 예측하지 않습니다 그들은

58
00:02:09,419 --> 00:02:11,700
또한 조립하는 것 같습니다 그들은

59
00:02:11,700 --> 00:02:14,340
환각을 일으키는 것 같습니다 그들은 아마도 거짓말을 하는 것 같습니다 그들은

60
00:02:14,340 --> 00:02:16,440
같은 질문에 대해 무작위로 다른 답변을 제공하고

61
00:02:16,440 --> 00:02:18,300


62
00:02:18,300 --> 00:02:20,220
확률적으로 언어와 같은

63
00:02:20,220 --> 00:02:22,020
구조를 모방하는 것 같습니다 그들은  때때로 자신을 바로잡아야

64
00:02:22,020 --> 00:02:23,819
할 때

65
00:02:23,819 --> 00:02:25,440
당신이 그들을 조금 밀면 그들은

66
00:02:25,440 --> 00:02:27,000
때때로 그들의 마음을 바꿉니다

67
00:02:27,000 --> 00:02:28,560
음 사실 Fox News가 현재

68
00:02:28,560 --> 00:02:30,060
Tucker Carlson의 교체를 찾고 있다면

69
00:02:30,060 --> 00:02:31,800
그들은 덜 할 수 있습니다 그들은 사이를

70
00:02:31,800 --> 00:02:33,680
사용하는 것보다 확실히 더 나쁘게 할 수 있습니다

71
00:02:33,680 --> 00:02:36,120
if  그들은 당신이

72
00:02:36,120 --> 00:02:38,040
알고 있는 비슷한 구경을 찾고 있습니다 그래서 이 모델들은

73
00:02:38,040 --> 00:02:40,620
모든 종류의 거친 일을 하는 것처럼 보입니다

74
00:02:40,620 --> 00:02:41,940
음 그리고 지난 10년 동안

75
00:02:41,940 --> 00:02:43,500


76
00:02:43,500 --> 00:02:45,900
우리가 담배 침대인 것처럼 개발된 일련의 다른 시스템이 있었고

77
00:02:45,900 --> 00:02:47,580
그들 각각은  다른

78
00:02:47,580 --> 00:02:49,620
신경망 접근 방식을 기반으로 하지만 궁극적으로 그들은

79
00:02:49,620 --> 00:02:51,180
모두 단어를 취하고

80
00:02:51,180 --> 00:02:53,220
수백 또는

81
00:02:53,220 --> 00:02:56,220
수천 개의 숫자 목록으로 특성화하는 것처럼 보입니다. 따라서 G23 네트워크는

82
00:02:56,220 --> 00:03:00,000


83
00:03:00,000 --> 00:03:02,160
아키텍처에 1,750억 개의 가중치와 96개의 주의 헤드가 있으며

84
00:03:02,160 --> 00:03:03,780
내가 아는 한  Steven은

85
00:03:03,780 --> 00:03:06,060
여기에서 저를 바로잡을 수 있습니다. 우리는

86
00:03:06,060 --> 00:03:07,319
이러한 다른 부분이 실제로 무엇을 의미하는지에 대한 좋은 아이디어가 없습니다.

87
00:03:07,319 --> 00:03:09,180


88
00:03:09,180 --> 00:03:10,920
어텐션 헤드와

89
00:03:10,920 --> 00:03:13,680
gpt3가 도움을 주기 위해 문자열의 훨씬 이전 토큰에 주의를 기울일 수 있는 방식으로 작동하는 것 같습니다.

90
00:03:13,680 --> 00:03:15,360


91
00:03:15,360 --> 00:03:17,040
그들은 다음 토큰을 예측하지만

92
00:03:17,040 --> 00:03:18,659
처음부터 끝까지 전체 아키텍처는

93
00:03:18,659 --> 00:03:21,599
일종의 엔지니어링 기반 동기입니다

94
00:03:21,599 --> 00:03:22,500
음

95
00:03:22,500 --> 00:03:24,659
그리고 저는 항상

96
00:03:24,659 --> 00:03:27,360


97
00:03:27,360 --> 00:03:28,800
다른 기술

98
00:03:28,800 --> 00:03:30,300
회사의 이러한 영화에서 실패한 모든 모델에 대해 궁금합니다.

99
00:03:30,300 --> 00:03:31,980


100
00:03:31,980 --> 00:03:33,060
음 당신은 그들이 정말 잘 작동하는이 모델을 가지고있는 것처럼 보이게합니다.

101
00:03:33,060 --> 00:03:34,980


102
00:03:34,980 --> 00:03:36,720
즉시 사용 가능

103
00:03:36,720 --> 00:03:38,459
음 그리고 그들은 모두 유명한 예술가의 이름을 따서 명명 된 것 같습니다.

104
00:03:38,459 --> 00:03:40,739
바로

105
00:03:40,739 --> 00:03:42,780
그들은 샐러드 후 Darley가 있거나

106
00:03:42,780 --> 00:03:45,360
Da Vinci가있을 수 있습니다.  곧

107
00:03:45,360 --> 00:03:46,980
이 회사 중 하나가 대형

108
00:03:46,980 --> 00:03:49,500
언어 모델을 출시할 예정입니다 어 예수 또는

109
00:03:49,500 --> 00:03:50,879
제가 모르는 어떤 것입니다

110
00:03:50,879 --> 00:03:53,040
하지만 그들은 항상 이것이 이것이 우리의

111
00:03:53,040 --> 00:03:54,900
새로운 재단 모델이라고 말합니다 그것은 피카소라고 합니다

112
00:03:54,900 --> 00:03:56,280
그것은 우리가 그것을 시도한 첫 번째 모델입니다 우리는 단지

113
00:03:56,280 --> 00:03:58,140
훌륭합니다 아무 문제 없이 똑바로

114
00:03:58,140 --> 00:03:59,819
즉시 사용할 수 있지만 저는 매번 실패한 올리버 블랙박스에 대해 궁금합니다.

115
00:03:59,819 --> 00:04:01,680


116
00:04:01,680 --> 00:04:03,659


117
00:04:03,659 --> 00:04:05,760


118
00:04:05,760 --> 00:04:07,379


119
00:04:07,379 --> 00:04:09,840
하나의

120
00:04:09,840 --> 00:04:11,939
모델 또는 다른 모델을 선택하는 과학적 추론에 대해 매우 개방적이고 명확한 구조가 없는 것 같습니다.  어 하지만 다시 저는

121
00:04:11,939 --> 00:04:14,280
그것에 대해 수정할 수 있을 것 같습니다

122
00:04:14,280 --> 00:04:16,798
음 그래서 기본 언어 모델도

123
00:04:16,798 --> 00:04:18,298


124
00:04:18,298 --> 00:04:20,699
기본 웹 예측과 같이 음에서 꽤 잘 수행하므로

125
00:04:20,699 --> 00:04:22,139
문제는 이러한 도구가

126
00:04:22,139 --> 00:04:23,520


127
00:04:23,520 --> 00:04:25,320
문법 및 전달과 같은 전통적인 심리 언어학적 개념과 같은 통찰력을 제공하는지 여부입니다.

128
00:04:25,320 --> 00:04:27,300
이것이 제가 사이버베라스 같은 사람들이 제안한 언어

129
00:04:27,300 --> 00:04:29,040
모델보다 10 Focus 모델을 선호하는 이유입니다.

130
00:04:29,040 --> 00:04:30,960


131
00:04:30,960 --> 00:04:32,880


132
00:04:32,880 --> 00:04:34,560
그래서 아무도 llm이

133
00:04:34,560 --> 00:04:36,479


134
00:04:36,479 --> 00:04:38,580


135
00:04:38,580 --> 00:04:40,080
파이썬 코드와

136
00:04:40,080 --> 00:04:42,120
자연어를 포함할 때 파이썬에 대해 심오한 것을 알려주지 않는다고 생각한다는 지적이 있습니다.  파이썬은

137
00:04:42,120 --> 00:04:43,740
구문 구조 문법이 있는 상징적 언어

138
00:04:43,740 --> 00:04:46,620
이며 아무도 llms가 파이썬의 비밀을 밝히고 있다고 말하지 않습니다.

139
00:04:46,620 --> 00:04:48,600
그래서

140
00:04:48,600 --> 00:04:50,940
여기에 다양한 것을 넣기 위해 그는 n 모델이 언어 작업에 대한 성공에 기반한 자연 언어에

141
00:04:50,940 --> 00:04:52,620
대한 설명 이론으로 해석될 수 있다면

142
00:04:52,620 --> 00:04:54,000


143
00:04:54,000 --> 00:04:56,040
다음에서 말합니다.

144
00:04:56,040 --> 00:04:57,540
반론이 없으면 그들은 컴퓨터 언어

145
00:04:57,540 --> 00:04:58,919
에 대한 좋은 설명 이론이 되어야 합니다. 어

146
00:04:58,919 --> 00:05:01,139
따라서

147
00:05:01,139 --> 00:05:02,699
성공적인

148
00:05:02,699 --> 00:05:04,620
자연어 모델은 아마존 언어의 생성 구문 구조에 대한 증거로 사용할 수 없습니다.

149
00:05:04,620 --> 00:05:06,540


150
00:05:06,540 --> 00:05:07,800


151
00:05:07,800 --> 00:05:09,720


152
00:05:09,720 --> 00:05:11,280


153
00:05:11,280 --> 00:05:13,500
에밀리 벤더(Emily Bender)와 몇몇 다른 사람들은

154
00:05:13,500 --> 00:05:15,300
훈련 말뭉치의 기능이

155
00:05:15,300 --> 00:05:16,979
사실 제 생각에 스티븐이

156
00:05:16,979 --> 00:05:17,940
이것을 인용한다고 생각합니다 당신은 이것을 당신의 논문에서

157
00:05:17,940 --> 00:05:19,860
실제로 제한으로 인용합니다

158
00:05:19,860 --> 00:05:21,120
음 그들은

159
00:05:21,120 --> 00:05:22,680
훈련 말뭉치의 특징이 배치 과정에 큰 영향을 미칠 수 있다는 것을 보여줍니다.

160
00:05:22,680 --> 00:05:24,600


161
00:05:24,600 --> 00:05:25,800


162
00:05:25,800 --> 00:05:27,780
언어 수업에 대한 대규모 언어 모델의 성능은 교육 말뭉치의

163
00:05:27,780 --> 00:05:29,699
다양성에 크게 영향을 받지만

164
00:05:29,699 --> 00:05:31,440


165
00:05:31,440 --> 00:05:33,180
자연어 자체는 편향되지 않습니다.

166
00:05:33,180 --> 00:05:35,340


167
00:05:35,340 --> 00:05:37,199
인간이

168
00:05:37,199 --> 00:05:39,120
말하는 내용과 행동 방식에 편향될 수 있는 것은 컴퓨터 시스템일 뿐입니다.

169
00:05:39,120 --> 00:05:41,039
그러나 자연어 자체는

170
00:05:41,039 --> 00:05:43,020
편향되지 않습니다. 너무 큰 언어

171
00:05:43,020 --> 00:05:45,180
모델이므로

172
00:05:45,180 --> 00:05:47,880


173
00:05:47,880 --> 00:05:49,500
모든 종류의 편향에 종속되어 있으므로

174
00:05:49,500 --> 00:05:51,000
실제로 언어의 모델이 될 수 없습니다.

175
00:05:51,000 --> 00:05:52,020
그들은 무언가의 모델입니다.

176
00:05:52,020 --> 00:05:53,880
그렇지 않으면 이 주장을 마무리하기 위해

177
00:05:53,880 --> 00:05:55,620


178
00:05:55,620 --> 00:05:58,139
음, 영화가 어린이들에게 훨씬

179
00:05:58,139 --> 00:05:59,759
더 많은 언어적 경험에 분명히 노출되어 있음에도 불구하고

180
00:05:59,759 --> 00:06:01,320
이것은

181
00:06:01,320 --> 00:06:02,699
Stephen이 그것을 볼 수

182
00:06:02,699 --> 00:06:04,680
있고 그의 논문에서 이야기할 수 있는 또 다른 것이므로

183
00:06:04,680 --> 00:06:06,360
그들의 학습 결과는 여전히

184
00:06:06,360 --> 00:06:08,160
어떤 문법적 일반화가 원칙적으로 학습 가능한지 다루는 것과 관련이 있으므로

185
00:06:08,160 --> 00:06:09,840


186
00:06:09,840 --> 00:06:11,580


187
00:06:11,580 --> 00:06:12,660
여기서 이 진술에 동의합니다. 원칙적으로 광범위한 획득 프레임워크를 알고 있는 것과 같은

188
00:06:12,660 --> 00:06:14,100


189
00:06:14,100 --> 00:06:16,080
것보다 학습 가능성에 대해 무엇인가를 말할 수 있다는 것을 알고 있습니다.

190
00:06:16,080 --> 00:06:17,639


191
00:06:17,639 --> 00:06:20,520


192
00:06:20,520 --> 00:06:21,900


193
00:06:21,900 --> 00:06:24,539
귀납적 편향이

194
00:06:24,539 --> 00:06:26,819
학습에 필요하지 않다는 것을 지금 당장 보여주는 것은

195
00:06:26,819 --> 00:06:28,139
그것이

196
00:06:28,139 --> 00:06:30,060
아이들에게 존재하지 않는다는 것을 보여주는 것과 실제로는 같지 않습니다.

197
00:06:30,060 --> 00:06:31,560


198
00:06:31,560 --> 00:06:32,639


199
00:06:32,639 --> 00:06:34,500


200
00:06:34,500 --> 00:06:36,720
언어 학습은 영유아에게 필요하거나 심지어

201
00:06:36,720 --> 00:06:39,120
유용합니다.

202
00:06:39,120 --> 00:06:40,440
하지만 지금은

203
00:06:40,440 --> 00:06:42,479
Eugene Choi와 Gary Marcus 및

204
00:06:42,479 --> 00:06:44,340
다른 사람들이 현재 얼마나 많은 비용이 드는지 강조한 다른 사람들의 의견에 더 동의합니다.

205
00:06:44,340 --> 00:06:45,660


206
00:06:45,660 --> 00:06:46,440


207
00:06:46,440 --> 00:06:48,600


208
00:06:48,600 --> 00:06:50,400


209
00:06:50,400 --> 00:06:51,960
몇몇 기술 회사의 손에 있는 힘은

210
00:06:51,960 --> 00:06:54,539
환경에 미치는 영향이 엄청나고

211
00:06:54,539 --> 00:06:56,039
많은 사람들이 여기에서 평가에

212
00:06:56,039 --> 00:06:57,300
덜 제한적이고 보수적이라는 것을 알고 있습니다.

213
00:06:57,300 --> 00:06:58,979


214
00:06:58,979 --> 00:07:00,660
Gary

215
00:07:00,660 --> 00:07:02,880
Marcus와 Eugene 외에는 훨씬 적습니다. 그래서 Bill Gates는 최근

216
00:07:02,880 --> 00:07:05,100
채팅 GPT가

217
00:07:05,100 --> 00:07:06,960
음이라고 썼습니다.  uh 그래픽 사용자 인터페이스 이후 가장 큰 기술 개발

218
00:07:06,960 --> 00:07:11,400


219
00:07:11,400 --> 00:07:13,020
um과 Henry Kissinger는 2월

220
00:07:13,020 --> 00:07:15,240
월스트리트 저널에 채팅

221
00:07:15,240 --> 00:07:17,400
gbt의 용량이 더 넓어짐에 따라

222
00:07:17,400 --> 00:07:19,560
인간 지식을 재정의하고

223
00:07:19,560 --> 00:07:21,780
현실 구조의 변화를 가속화하고

224
00:07:21,780 --> 00:07:23,220
정치와 사회를 재구성할 것이라고 썼습니다.

225
00:07:23,220 --> 00:07:25,620
생성 AI는

226
00:07:25,620 --> 00:07:28,319
새로운 형태의 인간 의식을 생성하기 위해 게시되므로

227
00:07:28,319 --> 00:07:30,419
현재 매우 급진적인 주장이 일어나고 있습니다.

228
00:07:30,419 --> 00:07:32,220
때때로

229
00:07:32,220 --> 00:07:35,099
또는 AI 과대 광고가

230
00:07:35,099 --> 00:07:36,840
학계의 특정 부분에 저장되어

231
00:07:36,840 --> 00:07:39,240
잠재적으로 많은 웅장한 계획이

232
00:07:39,240 --> 00:07:40,680
만들어지고 있다는 것을 알고 있을지 궁금합니다.  좀 더

233
00:07:40,680 --> 00:07:42,180
구체적으로 말하자면 여기 Steven에게 다시 전달하기 위해

234
00:07:42,180 --> 00:07:44,280
문제를 제기하고 싶었습니다.

235
00:07:44,280 --> 00:07:46,919
음 rorski와

236
00:07:46,919 --> 00:07:49,259
Beaumont가 lingbuzz에서 읽은 것 같다는 비판이 있습니다. 음

237
00:07:49,259 --> 00:07:50,520


238
00:07:50,520 --> 00:07:51,240


239
00:07:51,240 --> 00:07:53,280
트위터에서

240
00:07:53,280 --> 00:07:54,419
반응이 마음에 들지 않는다는 것을 본 것 같습니다.  그들이

241
00:07:54,419 --> 00:07:56,819
만든 이의 제기는

242
00:07:56,819 --> 00:07:58,500
과학이 연역적 논리의 예라는 것을 알고 있다는 것입니다.

243
00:07:58,500 --> 00:08:00,479
귀하의 이의는

244
00:08:00,479 --> 00:08:02,340
과학이 연역적이지 않고 귀납적이라는 것입니다.

245
00:08:02,340 --> 00:08:04,380
그러나 일반적인 포인트가

246
00:08:04,380 --> 00:08:06,900
더 정확할 수 있다고 생각합니다. 즉, 할

247
00:08:06,900 --> 00:08:08,520
수 없다는 것입니다.

248
00:08:08,520 --> 00:08:10,620
언어 모델이

249
00:08:10,620 --> 00:08:13,020
인간의 일부 언어 행동과 일부

250
00:08:13,020 --> 00:08:15,300
신경 이미징 반응을 잘 예측한다는 사실을 사용하지 마십시오.

251
00:08:15,300 --> 00:08:17,460
그것만으로는 인간 언어 이론을 산출할 수 있다고 주장할 수 없습니다.

252
00:08:17,460 --> 00:08:19,199


253
00:08:19,199 --> 00:08:21,000
따라서 귀하의 논문에서 Stephen은

254
00:08:21,000 --> 00:08:23,220
특정 구조가

255
00:08:23,220 --> 00:08:25,020
다른 구조보다 더 잘 작동합니다. 올바른 주의

256
00:08:25,020 --> 00:08:26,819
메커니즘이 중요합니다. 예측이

257
00:08:26,819 --> 00:08:28,680
중요합니다. 의미론적 표현이

258
00:08:28,680 --> 00:08:30,180
중요하므로

259
00:08:30,180 --> 00:08:32,219
현재 이러한 모델을 기반으로 수집할 수 있습니다.

260
00:08:32,219 --> 00:08:34,260
하지만 지금까지 제가

261
00:08:34,260 --> 00:08:36,000
문헌에서 수집할 수 있었던 것은 그게 전부입니다.

262
00:08:36,000 --> 00:08:37,559
여기에 더 많은 통찰력이 있는지 확실하지 않으므로

263
00:08:37,559 --> 00:08:39,899
Rosky와 Boomer는

264
00:08:39,899 --> 00:08:42,719
잘못된 예측이지만 강력한 설명의 예를 사용했습니다. 올바른

265
00:08:42,719 --> 00:08:44,760
설명력과

266
00:08:44,760 --> 00:08:46,500
예측 정확도가 현대 과학의 기초를 형성

267
00:08:46,500 --> 00:08:48,120
하고

268
00:08:48,120 --> 00:08:49,440
나중에 이것을 탐구하고 싶습니다. 아마도

269
00:08:49,440 --> 00:08:50,820
음 그러나 현대 언어 모델

270
00:08:50,820 --> 00:08:52,440
인간 언어의 일부를 정확하게 모델링할 수

271
00:08:52,440 --> 00:08:54,180
있지만

272
00:08:54,180 --> 00:08:55,920


273
00:08:55,920 --> 00:08:58,200
인간이 배울 수 없고

274
00:08:58,200 --> 00:09:00,360
처리하는 데 큰 어려움이 있는 불가능한 언어와 부자연스러운 구조에서도 매우 잘 수행할 수 있습니다.

275
00:09:00,360 --> 00:09:01,560


276
00:09:01,560 --> 00:09:03,060


277
00:09:03,060 --> 00:09:04,380
분명히 동시에 여기에 혼자가 아니므

278
00:09:04,380 --> 00:09:08,339
로

279
00:09:08,339 --> 00:09:10,320
오픈 AI의 수석 과학자 Elia 음 그는

280
00:09:10,320 --> 00:09:12,180
최근 인터뷰에서

281
00:09:12,180 --> 00:09:13,680
다음 토큰을 충분히 잘 예측한다는 것이 무엇을 의미하는지에 대한

282
00:09:13,680 --> 00:09:15,540


283
00:09:15,540 --> 00:09:17,940


284
00:09:17,940 --> 00:09:19,920
생성으로 이어진 근본적인 현실을 이해한다는 것을 의미한다고 말했습니다.  여기 문헌에 있는

285
00:09:19,920 --> 00:09:21,779
훨씬

286
00:09:21,779 --> 00:09:23,100
더 보수적인 주장과는 꽤 다른 그 토큰은

287
00:09:23,100 --> 00:09:24,540


288
00:09:24,540 --> 00:09:26,519
음, 그리고

289
00:09:26,519 --> 00:09:27,839


290
00:09:27,839 --> 00:09:30,019
과학의 다른 구성 요소가

291
00:09:30,019 --> 00:09:32,580
귀납적이거나 연역적일 수 있다는 것에 대한 응답으로 말할 것입니다.

292
00:09:32,580 --> 00:09:34,140
기존

293
00:09:34,140 --> 00:09:36,300
이론 당신은 하이퍼 가설을 세웁니다 당신은

294
00:09:36,300 --> 00:09:38,519
데이터를 수집합니다 당신은 그것을 분석하고

295
00:09:38,519 --> 00:09:40,200
그것은 일종의 연역적

296
00:09:40,200 --> 00:09:41,880
과정이지만

297
00:09:41,880 --> 00:09:43,680
특정 관찰로 시작하여

298
00:09:43,680 --> 00:09:44,940
몇 가지 패턴을 찾고

299
00:09:44,940 --> 00:09:46,860
일반적인 결론을 유도한 다음 마술처럼 발명하는

300
00:09:46,860 --> 00:09:49,380
납치가 있는 경우도 있습니다.

301
00:09:49,380 --> 00:09:52,019


302
00:09:52,019 --> 00:09:53,760


303
00:09:53,760 --> 00:09:55,620
연역적 추론이 비과학적이거나

304
00:09:55,620 --> 00:09:58,200
귀납적 추론이 비과학적이거나

305
00:09:58,200 --> 00:10:00,360
귀납적 추론이 비과학적이라고 말하지 않을 가설 공간을 줄입니다.

306
00:10:00,360 --> 00:10:01,800


307
00:10:01,800 --> 00:10:03,540


308
00:10:03,540 --> 00:10:05,459


309
00:10:05,459 --> 00:10:08,399


310
00:10:08,399 --> 00:10:09,779
허리케인과 전염병을 예측하는 모델은

311
00:10:09,779 --> 00:10:12,060


312
00:10:12,060 --> 00:10:13,860
과학이 얻는 것만큼 엄격한 것들의 예이며

313
00:10:13,860 --> 00:10:16,019
독자에게 상황이

314
00:10:16,019 --> 00:10:18,120
언어 모델과 다르지 않다는 결론을 내리도록 간청합니다.

315
00:10:18,120 --> 00:10:19,920
음 하지만 저에게 문제는

316
00:10:19,920 --> 00:10:22,200
허리케인을 예측하는 모델이 그렇지 않다는 것입니다  허리케인이 무엇인지에 대한

317
00:10:22,200 --> 00:10:23,940
질문에 대답하는 비즈니스에서

318
00:10:23,940 --> 00:10:25,740


319
00:10:25,740 --> 00:10:27,420


320
00:10:27,420 --> 00:10:29,040
날씨를 정확하게 예측하는 올바른 모델은 매우 정확하지만 기상학과와

321
00:10:29,040 --> 00:10:30,600
일치한다는 것을 알지 못하지만 대신할 수는 없습니다.

322
00:10:30,600 --> 00:10:32,700


323
00:10:32,700 --> 00:10:34,380


324
00:10:34,380 --> 00:10:35,760
너만 알잖아 네게 넘겨줄게 그래

325
00:10:35,760 --> 00:10:37,620


326
00:10:37,620 --> 00:10:41,820


327
00:10:41,820 --> 00:10:43,800
알았어 음 어 거기에 많은게 있어 음 음

328
00:10:43,800 --> 00:10:45,240


329
00:10:45,240 --> 00:10:47,579
어 난 이런 비판들 중 많은 부분에 동의해

330
00:10:47,579 --> 00:10:51,300
어 이

331
00:10:51,300 --> 00:10:54,000
모델들이 어에 의해 통제되고 있다는 걸 알잖아

332
00:10:54,000 --> 00:10:56,220
하나 또는 두 개의 회사는

333
00:10:56,220 --> 00:10:59,160
음 매우 문제가 많습니다 음

334
00:10:59,160 --> 00:11:01,320
어 그들은 그들이 인터넷의 텍스트에 대해 교육을 받았기 때문에 그들이 얻은 모든 종류의 편견을 가지고 있다는 것을 알고 있습니다

335
00:11:01,320 --> 00:11:03,480


336
00:11:03,480 --> 00:11:04,980


337
00:11:04,980 --> 00:11:06,180


338
00:11:06,180 --> 00:11:08,640
음 그것은 매우 문제가 있습니다 음

339
00:11:08,640 --> 00:11:10,980
당신도 알고 있습니다 나는 확실히 동의합니다

340
00:11:10,980 --> 00:11:13,079
적어도 현재로서는

341
00:11:13,079 --> 00:11:16,500
모델이 제대로 작동하지 않기 때문에

342
00:11:16,500 --> 00:11:18,420
음

343
00:11:18,420 --> 00:11:20,640
당신이 알고 있는 질문과

344
00:11:20,640 --> 00:11:23,100
그들을 넘어뜨릴 문제의 예를 찾는 것이 쉬운 것 같아요 음 음 나는

345
00:11:23,100 --> 00:11:25,500
왜 내가 그들에 대해 흥분했는지 생각합니다 음은

346
00:11:25,500 --> 00:11:26,760


347
00:11:26,760 --> 00:11:29,820
그렇지 않습니다  어 반드시 그

348
00:11:29,820 --> 00:11:32,399
용어가 맞다는 것은 아니지만

349
00:11:32,399 --> 00:11:34,920
언어 성능 측면에서

350
00:11:34,920 --> 00:11:38,459
음 특히 구문 및 의미론

351
00:11:38,459 --> 00:11:40,980
음 내 생각에 그것들은 음

352
00:11:40,980 --> 00:11:43,019
다른 도메인의 다른 어떤 이론보다 훨씬 뛰어납니다.

353
00:11:43,019 --> 00:11:46,920
그래서 다른

354
00:11:46,920 --> 00:11:49,380
이론은 없습니다.  언어학 또는 컴퓨터

355
00:11:49,380 --> 00:11:53,100
과학은 길고

356
00:11:53,100 --> 00:11:56,700
일관된 어 문법적 어 텍스트 구절을 생성할 수

357
00:11:56,700 --> 00:11:58,500


358
00:11:58,500 --> 00:12:01,140
있고

359
00:12:01,140 --> 00:12:04,920
어 당신이 알고 있는 도구나

360
00:12:04,920 --> 00:12:08,220
회사에서 배포하는 것들을 알고 있기 때문에 모든 문제를 인정합니다 음

361
00:12:08,220 --> 00:12:09,959
음 여전히 이런 질문이 있습니다

362
00:12:09,959 --> 00:12:12,899
그들이 언어를 어떻게 다루는지

363
00:12:12,899 --> 00:12:14,760
그리고

364
00:12:14,760 --> 00:12:16,320
음 여기에서 많은

365
00:12:16,320 --> 00:12:17,760
열정이 나오는 것 같아요 정말 아무

366
00:12:17,760 --> 00:12:20,160
것도 없었어요 음

367
00:12:20,160 --> 00:12:23,700
언어 능력 측면에서 그들과 조금이라도 같은 것이

368
00:12:23,700 --> 00:12:24,899
음 그리고 그게 제가

369
00:12:24,899 --> 00:12:27,660
생각하는 것입니다  흥미진진합니다 그래서 네 저는

370
00:12:27,660 --> 00:12:29,220
당신이 시작했던 이러한 많은 것들에 동의합니다

371
00:12:29,220 --> 00:12:31,320


372
00:12:31,320 --> 00:12:33,300
음 어 하지만 그럼에도 불구하고 제가 생각하는 것처럼

373
00:12:33,300 --> 00:12:35,100
구문 및 의미론 측면에서

374
00:12:35,100 --> 00:12:37,079


375
00:12:37,079 --> 00:12:39,240
그들과 비교할 수 있는 다른 이론은 없습니다 음

376
00:12:39,240 --> 00:12:40,140


377
00:12:40,140 --> 00:12:42,120
하지만 그래서 제가 밀어보도록 하겠습니다  그 당시에는

378
00:12:42,120 --> 00:12:44,940
맞아요 그래서 저는 제가 언어학과에서

379
00:12:44,940 --> 00:12:46,320
제가 이야기를 나눈 많은 사람들의 주요 반대 의견이 있을 것입니다. 그들은

380
00:12:46,320 --> 00:12:48,000


381
00:12:48,000 --> 00:12:50,760


382
00:12:50,760 --> 00:12:53,100


383
00:12:53,100 --> 00:12:54,120


384
00:12:54,120 --> 00:12:55,620
여러분이 논문의 첫 번째로 알고 있는 일반적인 많은 사람들과 같습니다.  그들은

385
00:12:55,620 --> 00:12:57,360
훌륭한 일을 합니다 어 구문 및 의미론의

386
00:12:57,360 --> 00:12:58,920
많은 측면의 모든 측면을 정확하게 모델링

387
00:12:58,920 --> 00:13:01,380
하지만 음

388
00:13:01,380 --> 00:13:03,300


389
00:13:03,300 --> 00:13:04,920
촘스키가 구식 개념인 언어에 대한 사실에 대해 이야기하는 것을 알고 있는 것처럼 어떤 실제도 모릅니다

390
00:13:04,920 --> 00:13:06,660


391
00:13:06,660 --> 00:13:09,000
하지만

392
00:13:09,000 --> 00:13:10,260


393
00:13:10,260 --> 00:13:12,899


394
00:13:12,899 --> 00:13:16,860
llm이 고유하게 제공할 수 있는 언어 자체에 대한 발견이 있는지와 같이

395
00:13:16,860 --> 00:13:19,200
llm이

396
00:13:19,200 --> 00:13:21,899


397
00:13:21,899 --> 00:13:24,600
문장 구조를 가지고 있다고 가정해 봅시다 유형 X가

398
00:13:24,600 --> 00:13:26,279
문장보다 처리하기 더 어렵다는 점에 대해 예측한 경우와 같이 그것은 매우 중요한 개념이라고 생각합니다.  유형

399
00:13:26,279 --> 00:13:28,620
Y 그리고 이것은

400
00:13:28,620 --> 00:13:31,200
그들만이 생성할 수 있는 고유한 예측이며 인간

401
00:13:31,200 --> 00:13:33,899
언어학자인 Chomsky homestein은 아무도

402
00:13:33,899 --> 00:13:34,980
이전에 예측하지 못했지만

403
00:13:34,980 --> 00:13:37,260
시선

404
00:13:37,260 --> 00:13:38,760
추적 실험을 수행하고 모든 종류의

405
00:13:38,760 --> 00:13:40,800
다양한 행동 경험을 수행하는 것이 사실임이 밝혀졌습니다.  그리고 오

406
00:13:40,800 --> 00:13:42,480
결국 사실로 밝혀졌군요

407
00:13:42,480 --> 00:13:44,279
이것은 언어 처리에 대한 새로운 통찰입니다

408
00:13:44,279 --> 00:13:45,839


409
00:13:45,839 --> 00:13:47,940
여러분이 알고 있는 언어에 대한 새로운 통찰입니다

410
00:13:47,940 --> 00:13:49,560


411
00:13:49,560 --> 00:13:51,240


412
00:13:51,240 --> 00:13:52,620
가까운 장래에 일어날 수도 있기

413
00:13:52,620 --> 00:13:54,540
때문에

414
00:13:54,540 --> 00:13:57,240
많은 언어학자들이 여기 전체 언어학 커뮤니티를 대신하여 말하는 이유의 핵심이라고 생각

415
00:13:57,240 --> 00:13:59,519


416
00:13:59,519 --> 00:14:02,040
합니다.

417
00:14:02,040 --> 00:14:03,180
이것이 주요 반대 중 하나가 될 것 같습니다.

418
00:14:03,180 --> 00:14:04,800


419
00:14:04,800 --> 00:14:08,279
잘 모르겠어 어 나는

420
00:14:08,279 --> 00:14:10,019
그들이 제공 한 통찰력을 일종의 일반 원칙으로 생각하는 것 같아요

421
00:14:10,019 --> 00:14:12,420


422
00:14:12,420 --> 00:14:14,459
그래서

423
00:14:14,459 --> 00:14:16,200
음 어 나는 언어 덩어리를 암기하는 힘과 같은 것에 대해 생각합니다.

424
00:14:16,200 --> 00:14:18,720


425
00:14:18,720 --> 00:14:20,820


426
00:14:20,820 --> 00:14:22,500


427
00:14:22,500 --> 00:14:24,540
예를 들어 구성에 매우 능숙하고 특히

428
00:14:24,540 --> 00:14:26,160
촘스키의 언어 이론이 많이 있습니다. 어,

429
00:14:26,160 --> 00:14:28,740


430
00:14:28,740 --> 00:14:30,899


431
00:14:30,899 --> 00:14:33,660
암기할 수 있는 최소한의 구조를 찾으려고 노력하는 것과 관련하여 음에서

432
00:14:33,660 --> 00:14:36,480
가능한 한 많이 파생시키려고 노력하는 것입니다.

433
00:14:36,480 --> 00:14:38,279


434
00:14:38,279 --> 00:14:40,440
작업 모음

435
00:14:40,440 --> 00:14:42,300
음 그리고 제 생각에는 그 이론들에 대해서는 잘 되지 않았습니다. 음

436
00:14:42,300 --> 00:14:44,459
반면 이것은

437
00:14:44,459 --> 00:14:46,740
정말 잘 됩니다 그래서 음

438
00:14:46,740 --> 00:14:48,360
어 우리가 문법 이론에 대해 생각한다면

439
00:14:48,360 --> 00:14:50,040
암기 능력이 있는 것을

440
00:14:50,040 --> 00:14:51,300
생각한다면 예를 들어 어

441
00:14:51,300 --> 00:14:54,240


442
00:14:54,240 --> 00:14:56,040
음, 인간은 서로 다른 구문을

443
00:14:56,040 --> 00:14:58,139
암기하는 놀라운 능력을 좋아한다는 것을 알고 있습니다. 우리가

444
00:14:58,139 --> 00:14:59,880


445
00:14:59,880 --> 00:15:01,380
알고 있는

446
00:15:01,380 --> 00:15:02,940
수만 개의 단어

447
00:15:02,940 --> 00:15:04,380
수만 개의 서로 다른 구문

448
00:15:04,380 --> 00:15:06,420


449
00:15:06,420 --> 00:15:07,920
죄송합니다.

450
00:15:07,920 --> 00:15:10,260
어떤 의미에서

451
00:15:10,260 --> 00:15:12,480
그런

452
00:15:12,480 --> 00:15:15,660
종류의 접근 방식이 잘 작동할 수 있다는 일종의 원리 증명입니다 음

453
00:15:15,660 --> 00:15:17,279
다른 유형의 예측을 만드는 것에 대해 생각할 수 있습니다

454
00:15:17,279 --> 00:15:19,380
음

455
00:15:19,380 --> 00:15:21,779
사람들이 현재 수행하고 있지만

456
00:15:21,779 --> 00:15:23,339
예를 들어 측정에 사용하려고 하는 것입니다

457
00:15:23,339 --> 00:15:25,860


458
00:15:25,860 --> 00:15:27,899
예를 들어 이러한 모델에서 어 처리 난이도 측정은 놀랍습니다 음

459
00:15:27,899 --> 00:15:29,160


460
00:15:29,160 --> 00:15:30,600


461
00:15:30,600 --> 00:15:31,699


462
00:15:31,699 --> 00:15:34,260
문맥 없는

463
00:15:34,260 --> 00:15:36,120
문법이나 다른 종류의 언어

464
00:15:36,120 --> 00:15:37,440
모델보다 훨씬 나은 놀라운 측정이 있습니다.

465
00:15:37,440 --> 00:15:39,899


466
00:15:39,899 --> 00:15:41,940


467
00:15:41,940 --> 00:15:44,399


468
00:15:44,399 --> 00:15:46,620
그 중 일부를 포착하거나 비선형적일 수 있습니다. 또는

469
00:15:46,620 --> 00:15:49,079
일부만 포착한다는 것을 알고 있을 수도 있습니다.

470
00:15:49,079 --> 00:15:51,420
또는 흥미로운

471
00:15:51,420 --> 00:15:53,940
종류의 다른 과학적 질문일 수도 있지만

472
00:15:53,940 --> 00:15:55,500
원칙적으로 그들은 예를 들어 연결에 대해 예측할 수 있다고 생각합니다.

473
00:15:55,500 --> 00:15:57,779


474
00:15:57,779 --> 00:15:59,880
논문에서 문장 사이에

475
00:15:59,880 --> 00:16:02,279


476
00:16:02,279 --> 00:16:05,459


477
00:16:05,459 --> 00:16:07,920
10가지 다른 방법으로 선언을 질문으로 변환하는 예를 들었습니다.

478
00:16:07,920 --> 00:16:10,620
GPT나 무언가를 알고 있을 때

479
00:16:10,620 --> 00:16:12,540


480
00:16:12,540 --> 00:16:15,420
10가지 다른 질문을 찾고 있을 때 옳을 것입니다.  의미

481
00:16:15,420 --> 00:16:18,060


482
00:16:18,060 --> 00:16:20,339
론적 또는

483
00:16:20,339 --> 00:16:22,139
구문론적 공간의 근저에 있는 모델에서 어떤 방식으로 관련된 종류의 근처에

484
00:16:22,139 --> 00:16:24,660
음 그리고 그런 종류의 것들은

485
00:16:24,660 --> 00:16:26,459
제가 생각하는 유형입니다 음

486
00:16:26,459 --> 00:16:28,560


487
00:16:28,560 --> 00:16:30,180


488
00:16:30,180 --> 00:16:32,220
어 일부 언어학자들이 바로 원할 수도 있다는 것을 알고 있습니다.

489
00:16:32,220 --> 00:16:34,320
하지만 내가 아는 한

490
00:16:34,320 --> 00:16:36,120


491
00:16:36,120 --> 00:16:39,660
아직 경험적으로 평가되지 않았으니 맞아 맞아 맞아 이런

492
00:16:39,660 --> 00:16:41,220
종류의 모델은 몇 년 밖에 안 되었기 때문에

493
00:16:41,220 --> 00:16:43,440


494
00:16:43,440 --> 00:16:45,000
음

495
00:16:45,000 --> 00:16:46,259
이런 종류의 작업이

496
00:16:46,259 --> 00:16:48,540
없었음에도 불구하고 그것에 대해 흥분하는 것이 합리적이라고 생각합니다.  아직 완료되지 않았습니다 아니요 맞습니다 아니요

497
00:16:48,540 --> 00:16:50,880
전적으로 제 말은

498
00:16:50,880 --> 00:16:51,720
그게 올바른

499
00:16:51,720 --> 00:16:52,920
관점이라고 생각하지만 이것이

500
00:16:52,920 --> 00:16:54,779


501
00:16:54,779 --> 00:16:56,579
당신이 언급한 놀라운 점에 대한 문제에 도달하는 것 같습니다 차선

502
00:16:56,579 --> 00:16:58,139
가능성에 대해 언급했습니다

503
00:16:58,139 --> 00:17:00,839
음 LMS와 일부 구문을 알고 있지만  그들은

504
00:17:00,839 --> 00:17:03,060


505
00:17:03,060 --> 00:17:04,439
유아보다 분명히 훨씬 더 많은 데이터로

506
00:17:04,439 --> 00:17:06,240
그렇게 하여 잠재적인

507
00:17:06,240 --> 00:17:09,000
구조에 대한 관찰 자체가 자극

508
00:17:09,000 --> 00:17:10,799
의 빈곤에 대한 평판이 아니라

509
00:17:10,799 --> 00:17:12,540


510
00:17:12,540 --> 00:17:13,500


511
00:17:13,500 --> 00:17:15,059
저명한 주장의 빈곤에 대해 더 약한 버전을 말해야 하므로 단순한 사실입니다.

512
00:17:15,059 --> 00:17:17,579
LMS가 우리의

513
00:17:17,579 --> 00:17:19,559
문법상으로 할 수 있는 일을 할 수 있다는 것은 매우 놀랍습니다.

514
00:17:19,559 --> 00:17:20,939
동의합니다. 사실

515
00:17:20,939 --> 00:17:22,500
5년, 6년,

516
00:17:22,500 --> 00:17:23,699
7년 전에는 예측하지

517
00:17:23,699 --> 00:17:25,260
못했을 것입니다.

518
00:17:25,260 --> 00:17:28,199


519
00:17:28,199 --> 00:17:30,120


520
00:17:30,120 --> 00:17:31,080
컴퓨터

521
00:17:31,080 --> 00:17:33,299
언어학이 가설과

522
00:17:33,299 --> 00:17:34,740
이론적 언어학을 제한

523
00:17:34,740 --> 00:17:36,480
할 수 있는지 확인하기 위해 이러한 기도를 가져오세요.

524
00:17:36,480 --> 00:17:38,039


525
00:17:38,039 --> 00:17:39,720
다양한 학습 매개변수가

526
00:17:39,720 --> 00:17:43,080
제어되는 신중한 실험과 거대한 언어 모델을 알고 있어야 합니다.

527
00:17:43,080 --> 00:17:45,299
gbt free와 같은 무료는 기본적으로

528
00:17:45,299 --> 00:17:47,520
여기서 쓸모가 없다는 것을 알고 있습니다. 그래서 이것은

529
00:17:47,520 --> 00:17:49,559
타일 렌즈와 불만에 도달하여

530
00:17:49,559 --> 00:17:51,960


531
00:17:51,960 --> 00:17:53,700
귀하가 관심을 갖고 있고

532
00:17:53,700 --> 00:17:55,320
생태학적으로 유효한 훈련 세트를 더 많이 알고 있는 아기 LM 프로젝트와 같은 것이 필요하다는 불만을 제기합니다.

533
00:17:55,320 --> 00:17:56,940


534
00:17:56,940 --> 00:17:58,380
논문에서 어떤

535
00:17:58,380 --> 00:17:59,640
구조가 학습될 것이라고 예측하세요. 저는

536
00:17:59,640 --> 00:18:01,679
여러분이 바로 거기에 있을지도 모른다고 생각합니다.

537
00:18:01,679 --> 00:18:03,059
하지만

538
00:18:03,059 --> 00:18:04,679
아기 LM 챌린지에도 불구하고 여전히

539
00:18:04,679 --> 00:18:07,260
더 전통적인 문제를 해결하는 사소한 문제가 있습니다.

540
00:18:07,260 --> 00:18:09,419
아이들은

541
00:18:09,419 --> 00:18:11,400


542
00:18:11,400 --> 00:18:13,200


543
00:18:13,200 --> 00:18:15,539
교차 언어적으로 다양한 요소를 기반으로 현재 입력의 양을 기반으로 일반화하기 시작하며 이는

544
00:18:15,539 --> 00:18:17,700


545
00:18:17,700 --> 00:18:18,720
심리 언어학 및 언어

546
00:18:18,720 --> 00:18:21,539
습득을 알고 있는 전통적인 방식만 필요하므로 LMS는 당신이 말했듯이

547
00:18:21,539 --> 00:18:22,919
빈도 및 놀라움과 같은 것에 관심이 있지만

548
00:18:22,919 --> 00:18:24,600


549
00:18:24,600 --> 00:18:26,160
정말 좋은 점이 있습니다.  Sophie sluts와

550
00:18:26,160 --> 00:18:27,600
Andrea Martin의 논문은 정말 아름다운 논문입니다.

551
00:18:27,600 --> 00:18:30,000


552
00:18:30,000 --> 00:18:31,620
분배

553
00:18:31,620 --> 00:18:34,080
통계가 때때로

554
00:18:34,080 --> 00:18:36,240
구조 구축의 순간에 대한 단서가 될 수 있지만 구성과

555
00:18:36,240 --> 00:18:37,919
관련된 이러한 개념을 대체할 수 있음을 매우 훌륭하게 보여주는 것을 보셨을 것입니다.

556
00:18:37,919 --> 00:18:39,660


557
00:18:39,660 --> 00:18:42,360
Chomsky 57의 인용문을 읽으십시오.

558
00:18:42,360 --> 00:18:45,240
음 슬롯 등과 매우 비슷하게 들리며 언어의 의미론적 및 통계적 모델의

559
00:18:45,240 --> 00:18:47,820
부인할 수 없는 관심과 중요성에도 불구하고

560
00:18:47,820 --> 00:18:49,440


561
00:18:49,440 --> 00:18:51,360
그들은 문법적 차이 집합을

562
00:18:51,360 --> 00:18:52,919
결정하거나 특성화하는 문제와 직접적인 관련이 없는 것으로 보입니다.

563
00:18:52,919 --> 00:18:54,600


564
00:18:54,600 --> 00:18:56,340
우리는

565
00:18:56,340 --> 00:18:57,840
문법이 자율적이고

566
00:18:57,840 --> 00:18:59,520
의미와 독립적이며

567
00:18:59,520 --> 00:19:01,320
확률적 모델이 구문 구조의

568
00:19:01,320 --> 00:19:03,179
기본 문제 중 일부에 특별한 통찰력을 제공하지 않는다는 결론을 내릴 수

569
00:19:03,179 --> 00:19:05,880


570
00:19:05,880 --> 00:19:07,980


571
00:19:07,980 --> 00:19:10,380
밖에 없다고 생각합니다.  정확하지 않지만

572
00:19:10,380 --> 00:19:11,580


573
00:19:11,580 --> 00:19:13,740


574
00:19:13,740 --> 00:19:16,559
57에서 사용 가능한 통계 모델이라고 분명히 말한 것이 오늘날 모델에 적용될 때 더 이상 정확하지 않으며 올바르게 언급한 것처럼

575
00:19:16,559 --> 00:19:18,600


576
00:19:18,600 --> 00:19:20,340
새로운 문자열 및 분포 범주에 대한 추상적인 일반화를 만들 수

577
00:19:20,340 --> 00:19:21,900


578
00:19:21,900 --> 00:19:23,640
있지만

579
00:19:23,640 --> 00:19:25,380
단일 모델의 성능을 알고 있다는 것은 사실입니다.

580
00:19:25,380 --> 00:19:27,480


581
00:19:27,480 --> 00:19:29,760


582
00:19:29,760 --> 00:19:31,440


583
00:19:31,440 --> 00:19:33,900
오늘날 사용할 수 있는 계산 모델과

584
00:19:33,900 --> 00:19:36,240
인간 두뇌 모델의 성공 사이에 엄청난 거리를 제공함으로써 특정 구조의 착륙

585
00:19:36,240 --> 00:19:38,100


586
00:19:38,100 --> 00:19:40,679
가능성에 대한 직접적인 증거를 제공하지 않습니다.

587
00:19:40,679 --> 00:19:42,539
구조는 학습할 수 없습니다

588
00:19:42,539 --> 00:19:44,520
맞아 맞아 맞아

589
00:19:44,520 --> 00:19:47,100
맞아 그래서 제 말은

590
00:19:47,100 --> 00:19:49,380


591
00:19:49,380 --> 00:19:51,000


592
00:19:51,000 --> 00:19:52,620
사람들이 만든 몇 가지 다른 버전의 학습

593
00:19:52,620 --> 00:19:55,679


594
00:19:55,679 --> 00:19:57,720
가능성 주장을 풀어볼 가치가 있다고 생각한다는 것입니다.

595
00:19:57,720 --> 00:19:59,160


596
00:19:59,160 --> 00:20:01,260
우리는

597
00:20:01,260 --> 00:20:04,140
필요한 데이터의 양에 대해 결코 주장하지 않습니다

598
00:20:04,140 --> 00:20:05,820


599
00:20:05,820 --> 00:20:08,220
언어 학습의 논리적 문제에 대한 주장이 있었고

600
00:20:08,220 --> 00:20:10,740
그것은 단지 불가능했습니다

601
00:20:10,740 --> 00:20:12,120
음 맞습니다 그것은 어 어 어

602
00:20:12,120 --> 00:20:15,720
상당한

603
00:20:15,720 --> 00:20:17,580
제약 없이는 불가능했습니다  당신이 습득할 언어의 종류

604
00:20:17,580 --> 00:20:19,380
나 문법의 종류는

605
00:20:19,380 --> 00:20:21,660


606
00:20:21,660 --> 00:20:23,220
음 그리고 오랫동안 사람들은

607
00:20:23,220 --> 00:20:25,380
그 버전에 대해 논쟁을 벌였습니다

608
00:20:25,380 --> 00:20:26,340


609
00:20:26,340 --> 00:20:27,120
음 음 음

610
00:20:27,120 --> 00:20:28,860
당신은 금에 의한 오래된 작업이 있다는 것을 알고 있습니다.

611
00:20:28,860 --> 00:20:30,419


612
00:20:30,419 --> 00:20:32,580


613
00:20:32,580 --> 00:20:35,220
그 전통을 기반으로 한 획득 이론은

614
00:20:35,220 --> 00:20:37,980


615
00:20:37,980 --> 00:20:39,240
다양한

616
00:20:39,240 --> 00:20:40,860
가설을 통해 다양한

617
00:20:40,860 --> 00:20:42,600
옵션과 것들을 고려하는 순서에 대해 많이 걱정합니다.

618
00:20:42,600 --> 00:20:43,380
음 음,

619
00:20:43,380 --> 00:20:45,059
제가 가장 좋아하는 참조는

620
00:20:45,059 --> 00:20:46,919


621
00:20:46,919 --> 00:20:49,799
음 Nick jader의 이 논문입니다.  Paul vetani um은 자연어

622
00:20:49,799 --> 00:20:51,539
의 이상적인 학습과 같은 것을 불렀습니다.

623
00:20:51,539 --> 00:20:53,039


624
00:20:53,039 --> 00:20:54,840
기본적으로

625
00:20:54,840 --> 00:20:57,539
제약 없는 학습자가

626
00:20:57,539 --> 00:21:00,480
충분한 데이터를 가지고 어 일종의

627
00:21:00,480 --> 00:21:01,860
생성 규칙이나

628
00:21:01,860 --> 00:21:03,360
생성 문법을

629
00:21:03,360 --> 00:21:05,640
음 문자열을 관찰하는 것만으로도 얻을 수 있다는 것을 보여줍니다.

630
00:21:05,640 --> 00:21:08,160


631
00:21:08,160 --> 00:21:10,200


632
00:21:10,200 --> 00:21:13,140


633
00:21:13,140 --> 00:21:15,120
긍정적인 예에서 배우는 것, 즉

634
00:21:15,120 --> 00:21:17,820
문자열을 관찰하는 것에서 배우는 것은 논리적으로

635
00:21:17,820 --> 00:21:20,520
불가능하다고 주장하는 이 거대한 작업에 대한 응답이었습니다. 그래서

636
00:21:20,520 --> 00:21:23,460
음 어 물론

637
00:21:23,460 --> 00:21:25,679
촘스키의 전통에 있는 사람들이 그런 형식을 정말 좋아한다는 것을 알고 있습니다.

638
00:21:25,679 --> 00:21:28,140
논증은 언어 습득이 작동하려면 선천적으로

639
00:21:28,140 --> 00:21:30,419
무언가를 가져야 한다고 말했기 때문입니다.

640
00:21:30,419 --> 00:21:33,299


641
00:21:33,299 --> 00:21:34,919
그것은

642
00:21:34,919 --> 00:21:36,600
일종의 수학적 논증과 같았습니다.

643
00:21:36,600 --> 00:21:39,480
일종의

644
00:21:39,480 --> 00:21:41,220
선천적인 문법이나 선천적인 가설 순서

645
00:21:41,220 --> 00:21:42,900
또는  무언가와 그 모든 것이

646
00:21:42,900 --> 00:21:45,659
완전히 잘못된 것으로 밝혀졌습니다. 그래서

647
00:21:45,659 --> 00:21:48,299
음 만약 당신이 음 테이터와 바타니가 하는 약간

648
00:21:48,299 --> 00:21:50,520
더 현실적인 학습 설정으로 이동하는 것을 안다면

649
00:21:50,520 --> 00:21:53,340


650
00:21:53,340 --> 00:21:55,500
음 그러면

651
00:21:55,500 --> 00:21:57,240
이상적인 학습자가 무언가를 습득할 수 있고

652
00:21:57,240 --> 00:21:59,100
아무것도 없다는 것이 밝혀졌습니다

653
00:21:59,100 --> 00:22:00,600
음 거기에도 필요한 데이터의 양에 대한 진술은

654
00:22:00,600 --> 00:22:02,700
일종의

655
00:22:02,700 --> 00:22:06,780
순수한 논리적 어 학습 능력

656
00:22:06,780 --> 00:22:08,940
이며 그 능력은

657
00:22:08,940 --> 00:22:11,460
큰 언어 모델의 큰 버전

658
00:22:11,460 --> 00:22:13,679
도 올바르게 말하므로 Chader

659
00:22:13,679 --> 00:22:15,600
invitani 및 기타 다른

660
00:22:15,600 --> 00:22:17,460
그 정신은

661
00:22:17,460 --> 00:22:19,500
당신이 수학적이고

662
00:22:19,500 --> 00:22:21,480
원칙적으로 논쟁하는 것을 알고 있지만

663
00:22:21,480 --> 00:22:24,720
실제로 어

664
00:22:24,720 --> 00:22:27,840
문법적 권리 또는 실제 종류의

665
00:22:27,840 --> 00:22:30,000
구현된 언어 모델인 것을 만들지 않았다는 점에서 작동합니다.

666
00:22:30,000 --> 00:22:32,640
음 그래서 당신도 훈련된 모델을 알고 있습니다

667
00:22:32,640 --> 00:22:35,400
1억 또는 1000억 또는

668
00:22:35,400 --> 00:22:38,340
아무리 많은 토큰에 대해서도 음 어

669
00:22:38,340 --> 00:22:41,159
그런 종류의 모델도

670
00:22:41,159 --> 00:22:43,080
해당 버전의 토론과 관련이 있다고 생각하고

671
00:22:43,080 --> 00:22:46,140
어

672
00:22:46,140 --> 00:22:48,659
언어 학습이 불가능하지 않다는 것을 보여줍니다

673
00:22:48,659 --> 00:22:51,360
음 매우 제한되지 않은 공간에서 오케이

674
00:22:51,360 --> 00:22:53,640
그런 다음 두 번째 버전이 있습니다.

675
00:22:53,640 --> 00:22:56,760
어

676
00:22:56,760 --> 00:22:59,760
아이들이 올바르게 이해하는 특정 데이터로 언어를 배울 수

677
00:22:59,760 --> 00:23:02,340
있고 그것은 데이터의 양과 데이터의

678
00:23:02,340 --> 00:23:04,620
형식입니다

679
00:23:04,620 --> 00:23:06,360
음 아기 LM 챌린지를 모르는 사람들을 위해

680
00:23:06,360 --> 00:23:08,520


681
00:23:08,520 --> 00:23:10,440
음 음 음 음 어

682
00:23:10,440 --> 00:23:12,840
이건 음 음

683
00:23:12,840 --> 00:23:14,340


684
00:23:14,340 --> 00:23:16,620
대회라고 부르기 미안해 음 아 어 어

685
00:23:16,620 --> 00:23:20,580
도전인 것 같아 음

686
00:23:20,580 --> 00:23:22,380
사람들이

687
00:23:22,380 --> 00:23:24,539
인간 크기의 데이터로 언어 모델을 훈련시키도록 하는 것은

688
00:23:24,539 --> 00:23:27,179
음 그래서 그것은 더 비슷한 것입니다

689
00:23:27,179 --> 00:23:28,679
음 훈련 세트에는 1천만 또는 1억 개의 서로 다른 두 가지 버전이 있다고 생각합니다

690
00:23:28,679 --> 00:23:31,140


691
00:23:31,140 --> 00:23:33,120
음 100번째 또는

692
00:23:33,120 --> 00:23:35,640


693
00:23:35,640 --> 00:23:38,100


694
00:23:38,100 --> 00:23:41,159
1000번째 또는

695
00:23:41,159 --> 00:23:43,140
음 이 큰 AI 회사가

696
00:23:43,140 --> 00:23:46,620
그들의 언어에 사용하는 것과 같은 큰 것입니다  모델 음 그리고

697
00:23:46,620 --> 00:23:48,720
음 저는 실제로 그것이

698
00:23:48,720 --> 00:23:50,340
정확히 올바른 종류의

699
00:23:50,340 --> 00:23:52,020
일이고 정확히 현장에서 필요한 것과 같다고 생각합니다.

700
00:23:52,020 --> 00:23:54,780
왜냐하면

701
00:23:54,780 --> 00:23:57,059
음 어린이 크기의 데이터에서

702
00:23:57,059 --> 00:23:58,980
음 기본적으로 올바른 구문을 배울 수 있기 때문입니다.

703
00:23:58,980 --> 00:24:01,140


704
00:24:01,140 --> 00:24:02,880
이러한 자극의 빈곤 주장에 반대하는 가장 강력한 주장이 될 것입니다.

705
00:24:02,880 --> 00:24:04,260


706
00:24:04,260 --> 00:24:06,299
대안적으로

707
00:24:06,299 --> 00:24:08,220
음 아마도 당신은 아주 많이 배울 수 없을 것입니다 음

708
00:24:08,220 --> 00:24:10,740
아마도 당신이 알고 있는

709
00:24:10,740 --> 00:24:12,840
훨씬 더 조잡한 종류의 언어 모델을 제시하거나

710
00:24:12,840 --> 00:24:14,700
일부 통사적 또는

711
00:24:14,700 --> 00:24:16,799
의미론적 능력이 부족할 것입니다 음

712
00:24:16,799 --> 00:24:17,520


713
00:24:17,520 --> 00:24:18,720
음 저는 실제로

714
00:24:18,720 --> 00:24:20,220
거기에 있는 실패는 해석하기가 약간 어렵다고 생각합니다.

715
00:24:20,220 --> 00:24:22,620
왜냐하면

716
00:24:22,620 --> 00:24:24,720
음 아이들이 실제로 언어를 배울 때 데이터를 얻을 때

717
00:24:24,720 --> 00:24:26,400


718
00:24:26,400 --> 00:24:28,919


719
00:24:28,919 --> 00:24:31,200
그들이 환경에서 상호 작용하는 문장의 문자열보다 훨씬 더 많은 데이터를 얻기 때문입니다.

720
00:24:31,200 --> 00:24:33,059


721
00:24:33,059 --> 00:24:34,440
음 그래서 그들 앞에는 세상에 물건이 있습니다

722
00:24:34,440 --> 00:24:36,539
음 그들의 발화

723
00:24:36,539 --> 00:24:38,520
도 대화식입니다 그래서 당신이 무언가를 말할 수

724
00:24:38,520 --> 00:24:39,960
있고

725
00:24:39,960 --> 00:24:41,400
당신이 요청한 것을 당신의 부모가 당신에게 가져다 주는지 확인할 수 있습니다

726
00:24:41,400 --> 00:24:43,799
예를 들어 사람들이 오랫동안 주장해 온 권리

727
00:24:43,799 --> 00:24:46,020


728
00:24:46,020 --> 00:24:49,020
um um as  a a a a a a language Acquisition에서 중요한 단서를 알고 있습니다

729
00:24:49,020 --> 00:24:51,240


730
00:24:51,240 --> 00:24:52,799
음 음 음

731
00:24:52,799 --> 00:24:55,440
아기 LM 챌린지에는

732
00:24:55,440 --> 00:24:58,200
이러한 모델을 훈련시킬 수 있는 기능이 있습니다

733
00:24:58,200 --> 00:25:00,960
어 종류의 다중 모드 입력으로

734
00:25:00,960 --> 00:25:02,340


735
00:25:02,340 --> 00:25:05,039
원하는 만큼 비디오 데이터를 제공할 수 있다고 생각합니다 음

736
00:25:05,039 --> 00:25:07,140
uh 하지만

737
00:25:07,140 --> 00:25:09,000


738
00:25:09,000 --> 00:25:11,220
아이들이 실제로 받는 설정 및 피드백 유형을 정확하게 복제하는 것은 어려울 것입니다

739
00:25:11,220 --> 00:25:12,840


740
00:25:12,840 --> 00:25:14,700
um uh

741
00:25:14,700 --> 00:25:17,700
그게 어디로 가는지, 어떻게 일이 진행되는지 보고 싶습니다.

742
00:25:17,700 --> 00:25:19,799


743
00:25:19,799 --> 00:25:20,760
음 음 어 알다시피

744
00:25:20,760 --> 00:25:23,700


745
00:25:23,700 --> 00:25:26,580
대규모 언어 모델에 대한 흥미로운 관련 질문이 있다고 생각합니다 음

746
00:25:26,580 --> 00:25:28,320


747
00:25:28,320 --> 00:25:30,299


748
00:25:30,299 --> 00:25:31,919


749
00:25:31,919 --> 00:25:34,200


750
00:25:34,200 --> 00:25:36,659
그것은 모든

751
00:25:36,659 --> 00:25:38,279
데이터가 수행하는 작업을 정확히 이해하는 것과 같습니다.  이러한 모델은

752
00:25:38,279 --> 00:25:40,620


753
00:25:40,620 --> 00:25:43,919
내부적으로 어떤 형태의 의미론을 효과적으로 발명하고 있기 때문에 음

754
00:25:43,919 --> 00:25:45,600
둘 다 구문의 규칙을 발견하고

755
00:25:45,600 --> 00:25:47,460
거기서

756
00:25:47,460 --> 00:25:49,380
단어 의미에 대해 꽤 많이 배우는 것 같습니다

757
00:25:49,380 --> 00:25:50,940


758
00:25:50,940 --> 00:25:52,200
음그리고

759
00:25:52,200 --> 00:25:54,059
음 그것은 아닙니다 완전히 불분명합니다 제 생각

760
00:25:54,059 --> 00:25:56,220
에는  이 현대 모델의 많은 데이터는

761
00:25:56,220 --> 00:25:58,919
어 구문 대 의미론에 필요합니다.

762
00:25:58,919 --> 00:26:00,240


763
00:26:00,240 --> 00:26:02,159
음 제 생각에는

764
00:26:02,159 --> 00:26:05,580
통사론 측면이 어 아마도 의미론 측면

765
00:26:05,580 --> 00:26:07,860
보다 훨씬 적은 데이터가 필요할 것 같습니다.

766
00:26:07,860 --> 00:26:09,600


767
00:26:09,600 --> 00:26:11,159
음 실제로는 학생입니다.

768
00:26:11,159 --> 00:26:12,960
Frank Malika와 저는

769
00:26:12,960 --> 00:26:15,120
몇 년 전에

770
00:26:15,120 --> 00:26:17,400
학습자가 언어의

771
00:26:17,400 --> 00:26:19,679


772
00:26:19,679 --> 00:26:22,140
다양한 측면을 학습하기 위해 반드시 습득해야 하는 정보의 양을 추정하기 위해 논문을 작성했습니다.

773
00:26:22,140 --> 00:26:23,820


774
00:26:23,820 --> 00:26:25,320


775
00:26:25,320 --> 00:26:26,880
그들의 의미 당신은 아마도

776
00:26:26,880 --> 00:26:28,320
그들의 빈도를 알고 있을 것입니다. 당신은

777
00:26:28,320 --> 00:26:32,100
구문을 배워야 하고 기본적으로 우리가 그 분석에서 발견한 것은

778
00:26:32,100 --> 00:26:34,080


779
00:26:34,080 --> 00:26:35,640
기본적으로

780
00:26:35,640 --> 00:26:37,440
이러한 각 도메인에 대한 엔벨로프 계산의 일종에 불과하다는 것

781
00:26:37,440 --> 00:26:40,679
입니다. 구문은 실제로 매우

782
00:26:40,679 --> 00:26:42,779
적은 양의 정보입니다.

783
00:26:42,779 --> 00:26:46,400
구문을 배우는 데는 그다지 많은 정보가 필요하지 않지만 여러분이

784
00:26:46,400 --> 00:26:49,320


785
00:26:49,320 --> 00:26:52,740
얻는 대부분의 정보는 실제로 의미론을 위한 것이므로

786
00:26:52,740 --> 00:26:55,740


787
00:26:55,740 --> 00:26:57,720


788
00:26:57,720 --> 00:27:00,000
각 의미가

789
00:27:00,000 --> 00:27:02,340
음 어 단 몇 비트일지라도 30에서 50,000개의 서로 다른 단어 의미를 알고 있다고 지정하면 됩니다.  그렇게 하려면

790
00:27:02,340 --> 00:27:04,620
많은 정보가 필요하고

791
00:27:04,620 --> 00:27:06,360
아마도 각 회의는 몇 비트 이상일 것입니다.

792
00:27:06,360 --> 00:27:08,460
그래서 음 어

793
00:27:08,460 --> 00:27:11,279
그것은

794
00:27:11,279 --> 00:27:12,659


795
00:27:12,659 --> 00:27:14,820
대규모 언어 모델에서 일어나는 일이 대부분의

796
00:27:14,820 --> 00:27:16,320
훈련 데이터가 단어에 관한 것이라고 추측하게 할 수 있습니다.

797
00:27:16,320 --> 00:27:18,600
의미론과

798
00:27:18,600 --> 00:27:20,760
아이들이 단어 의미론을 올바르게 이해하는 다른 방법에 대해 생각할 수 있습니다.

799
00:27:20,760 --> 00:27:22,140
그것은

800
00:27:22,140 --> 00:27:24,600
텍스트에서 동시 발생 패턴의 종류가 아닙니다.

801
00:27:24,600 --> 00:27:27,360
하지만 저는 그 모든 것이 공중에 떠

802
00:27:27,360 --> 00:27:29,039
있고 무슨 일이 일어날지 정말 흥미진진하다는 데 동의합니다.

803
00:27:29,039 --> 00:27:30,960
예, Lindsay의 연구실

804
00:27:30,960 --> 00:27:32,640
에서 나온 초기 결과 중 일부는

805
00:27:32,640 --> 00:27:35,279
적어도

806
00:27:35,279 --> 00:27:37,740
생태학적으로 유효한 훈련 장소로 제한된다는 것을 알고 있습니다.

807
00:27:37,740 --> 00:27:40,919
어 모델은 영어

808
00:27:40,919 --> 00:27:42,840
에 대한 선형 규칙을 알고 있음을 일반화하는 것 같습니다

809
00:27:42,840 --> 00:27:44,940
예 아니오 질문 형성 ROM 계층적 규칙

810
00:27:44,940 --> 00:27:46,620
이외의

811
00:27:46,620 --> 00:27:48,120
올바른 계층  따라서

812
00:27:48,120 --> 00:27:49,980


813
00:27:49,980 --> 00:27:52,020
올바른

814
00:27:52,020 --> 00:27:54,000
구문 가격과 귀납적 편향의 공간이

815
00:27:54,000 --> 00:27:56,220
실제로 정착되지는 않았지만

816
00:27:56,220 --> 00:27:57,960
적어도 나에게는

817
00:27:57,960 --> 00:27:59,400
약간의 규칙이 있어야 한다는 것이 꽤 분명해 보입니다.

818
00:27:59,400 --> 00:28:01,320
또한

819
00:28:01,320 --> 00:28:02,700
영어로 된 아이들이 이

820
00:28:02,700 --> 00:28:04,679
빈도 문제로 돌아가서

821
00:28:04,679 --> 00:28:06,360
영어로 된 아이들이 때때로 장거리의 낮은 보완 위치의 지정된 위치에서

822
00:28:06,360 --> 00:28:08,100
움직임의 중간 사본을 철자한다는 몇 가지 증거가 있습니다.

823
00:28:08,100 --> 00:28:10,140


824
00:28:10,140 --> 00:28:11,880


825
00:28:11,880 --> 00:28:13,980
그래서

826
00:28:13,980 --> 00:28:15,360
Thornton과 일부의 논문이 있습니다.  이것에 대한 다른 논문들

827
00:28:15,360 --> 00:28:18,120
그래서 그들은 음 누가

828
00:28:18,120 --> 00:28:19,919
그렇게 했다고 생각하세요? 어떤

829
00:28:19,919 --> 00:28:21,900
사람이 그렇게 했다고 생각하기 보다는

830
00:28:21,900 --> 00:28:23,159


831
00:28:23,159 --> 00:28:25,020
어떤 언어가 실제로

832
00:28:25,020 --> 00:28:26,760
이러한 중간 사본을 철자하지만

833
00:28:26,760 --> 00:28:28,799
영어는 그렇지 않기 때문에 이것은 흥미로운 설정입니다.  그 아이는

834
00:28:28,799 --> 00:28:30,779
문법을 설정하는 데 오류를 범하지만

835
00:28:30,779 --> 00:28:32,820
입력 빈도는 실제로 0이므로

836
00:28:32,820 --> 00:28:35,159
우리의 공통 친구인 Gary Marcus

837
00:28:35,159 --> 00:28:36,779
도 독일어 명사 복수형(특정 종류의 더 규칙적인 형식)의 경우 아이의 출력을 결정하는 빈도에 대해 논쟁을 벌입니다.

838
00:28:36,779 --> 00:28:39,000


839
00:28:39,000 --> 00:28:41,159


840
00:28:41,159 --> 00:28:42,779


841
00:28:42,779 --> 00:28:44,580
빈번한 것이 아니라 선호되는

842
00:28:44,580 --> 00:28:46,559
예가 많기 때문에

843
00:28:46,559 --> 00:28:49,080
주제가 수동적으로 무언가를 경험하거나

844
00:28:49,080 --> 00:28:50,460


845
00:28:50,460 --> 00:28:52,559


846
00:28:52,559 --> 00:28:54,299
이해력 연구에서 아이들이 입력에서 자주 발생하지 않기 때문에 약 8 시까 지

847
00:28:54,299 --> 00:28:56,039
매우 지연되는 수동 경험이 있다고 주장하는 경우가 있습니다.

848
00:28:56,039 --> 00:28:56,880


849
00:28:56,880 --> 00:28:59,220
그러나 Ken  Wexler와 동료들은 누가

850
00:28:59,220 --> 00:29:01,260


851
00:29:01,260 --> 00:29:03,539
Mary를 좋아하는지와 같은 더블 H 질문을 경험해 보았고

852
00:29:03,539 --> 00:29:05,820
이들은 수동태의

853
00:29:05,820 --> 00:29:07,500
주제와 경험만큼 입력에서 드물다는 것을 발견했지만

854
00:29:07,500 --> 00:29:09,659
아이들은

855
00:29:09,659 --> 00:29:10,980
이러한 질문에 대한 이해 연구에 문제가 없지만

856
00:29:10,980 --> 00:29:13,860
이해하는 데 문제가 있습니다.

857
00:29:13,860 --> 00:29:16,020
언어 수동태의 주제 경험이므로

858
00:29:16,020 --> 00:29:17,880
빈도가 다시 한 번

859
00:29:17,880 --> 00:29:19,380
관련이 없는 것 같거나 적어도

860
00:29:19,380 --> 00:29:20,940
설명이 맞지 않는 것 같습니다.

861
00:29:20,940 --> 00:29:22,320
이론 구축과 관련하여 설명이 아닌 것 같습니다.

862
00:29:22,320 --> 00:29:25,080
따라서 빈도 외에 다른 일이 분명히 있을 때 LMS가 이러한 경우에 어떻게 도움이 될 수 있는지 알고 있습니다.

863
00:29:25,080 --> 00:29:27,059


864
00:29:27,059 --> 00:29:28,440


865
00:29:28,440 --> 00:29:30,960
그래서 명반 여러분은 그들이 논문에 있는 사례의 이 문제로

866
00:29:30,960 --> 00:29:33,419
돌아가서 다시 일반화하는 것 같다는 것을 알고

867
00:29:33,419 --> 00:29:35,100


868
00:29:35,100 --> 00:29:36,899
있으며 종종 매우 멋진

869
00:29:36,899 --> 00:29:38,399
무색 화면 아이디어의 구조를 일반화한다는 것을 보여줍니다

870
00:29:38,399 --> 00:29:41,279


871
00:29:41,279 --> 00:29:43,080
음 하지만 긍정적인 자극은 결코

872
00:29:43,080 --> 00:29:44,520
실제로

873
00:29:44,520 --> 00:29:46,380
통계적으로 언어를 배울 수 없다는 것에 대해 알고 있습니다. 그

874
00:29:46,380 --> 00:29:48,059
주장이 옳았다는 것을 알고 있지만 오늘날의

875
00:29:48,059 --> 00:29:49,559
통계 모델에 대한 50년대의 촘스키의 주장은

876
00:29:49,559 --> 00:29:51,779


877
00:29:51,779 --> 00:29:53,700
2023년의 상용 LMS에 대해서는 사실이

878
00:29:53,700 --> 00:29:55,440
아닙니다.

879
00:29:55,440 --> 00:29:57,539
당신은 전체

880
00:29:57,539 --> 00:29:59,520
기하학을 알고 있습니다 Enterprise Chomsky의 기본

881
00:29:59,520 --> 00:30:01,080
요점은

882
00:30:01,080 --> 00:30:02,700
모든

883
00:30:02,700 --> 00:30:04,980
다이어그램이 빈도가 0인

884
00:30:04,980 --> 00:30:06,899


885
00:30:06,899 --> 00:30:08,039
문법적 구조를 가질 수 있다는 것입니다.

886
00:30:08,039 --> 00:30:09,840


887
00:30:09,840 --> 00:30:11,580


888
00:30:11,580 --> 00:30:13,980
귀하의 논문 GPT는 스크린 아이디어 끌어오기와 같은 예를 모방

889
00:30:13,980 --> 00:30:16,559
하지만

890
00:30:16,559 --> 00:30:18,840
이 문장은 Google에서 150,000개 이상의 결과를 산출하고

891
00:30:18,840 --> 00:30:20,700


892
00:30:20,700 --> 00:30:22,620
문헌에서 광범위하게 논의됩니다.

893
00:30:22,620 --> 00:30:24,480
모방할 수 있다는 사실을 모방할 수 있습니다.

894
00:30:24,480 --> 00:30:26,640
적어도 우리는

895
00:30:26,640 --> 00:30:27,840


896
00:30:27,840 --> 00:30:30,899
확신을 가지고 아무 말도 할 수 없습니다. 그래서

897
00:30:30,899 --> 00:30:32,580
더블린 대학의 뒤에 있는 abiba가 최근에 이 인용문을 가지고 있다는 것을 알고 계실 것입니다.

898
00:30:32,580 --> 00:30:34,380
어, 영화의 지능에 대한 당신 자신의 속기를 착각하지 마세요.

899
00:30:34,380 --> 00:30:36,840


900
00:30:36,840 --> 00:30:38,880


901
00:30:38,880 --> 00:30:40,500


902
00:30:40,500 --> 00:30:42,480
LMS가 일종의 흉내를 내고 있다고 비난합니다.

903
00:30:42,480 --> 00:30:43,919


904
00:30:43,919 --> 00:30:46,860
음, 논문에서 제공한 gbt의 예문은

905
00:30:46,860 --> 00:30:48,840
실제로

906
00:30:48,840 --> 00:30:50,700
제대로 작동하지 않습니다.

907
00:30:50,700 --> 00:30:52,320


908
00:30:52,320 --> 00:30:54,240


909
00:30:54,240 --> 00:30:55,740
그렇게 하거나 할 수 없지만

910
00:30:55,740 --> 00:30:57,120


911
00:30:57,120 --> 00:30:59,880
우리에게 이와 같은 10가지 예를 제공하는 측면에서 중간 지점이 없으므로 갈색 반짝이는

912
00:30:59,880 --> 00:31:02,880


913
00:31:02,880 --> 00:31:04,679


914
00:31:04,679 --> 00:31:06,840
토끼, 흰색 반짝이는 곰,

915
00:31:06,840 --> 00:31:09,899
검은색 반짝이는

916
00:31:09,899 --> 00:31:12,360
캥거루 녹색 반짝이는 원숭이와 매우 다른 의미론적 개체인 무색의 녹색 아이디어를 갖게 됩니다.

917
00:31:12,360 --> 00:31:15,120
노란색 눈부신 라이온스 빨간색 shimming

918
00:31:15,120 --> 00:31:16,320
요소 맞아 이것들은 모두

919
00:31:16,320 --> 00:31:18,899
의미론적으로 이상하고 약간

920
00:31:18,899 --> 00:31:20,820
이상하지만 여전히 법적

921
00:31:20,820 --> 00:31:22,440
구조와 같습니다 그들은 일종의 의미 있는

922
00:31:22,440 --> 00:31:25,320
합성 의미론적 객체입니다.

923
00:31:25,320 --> 00:31:27,179


924
00:31:27,179 --> 00:31:29,580


925
00:31:29,580 --> 00:31:33,179


926
00:31:33,179 --> 00:31:34,799
첫 번째 포인트에 먼저 반응할 수 있습니다.

927
00:31:34,799 --> 00:31:36,120
그래서

928
00:31:36,120 --> 00:31:37,799
음 어 당신은 주파수에 직접 매핑되지 않을 수도 있는

929
00:31:37,799 --> 00:31:40,140
다른 어 종류의 획득

930
00:31:40,140 --> 00:31:42,179
패턴에 대해 이야기하기 시작했습니다 음

931
00:31:42,179 --> 00:31:44,279


932
00:31:44,279 --> 00:31:47,159
그리고 내 생각에

933
00:31:47,159 --> 00:31:50,640
어 종류의 현대 학습

934
00:31:50,640 --> 00:31:53,039
모델을 생각하는 것은 실제로 실수인 것 같습니다  빈도에 기반을 두어야 합니다.

935
00:31:53,039 --> 00:31:54,899
왜냐면

936
00:31:54,899 --> 00:31:57,120
음 그들은

937
00:31:57,120 --> 00:31:59,580
규칙이나 구조 등의 꽤 복잡한 가족처럼 분명히 배우고 있기 때문입니다.

938
00:31:59,580 --> 00:32:02,399


939
00:32:02,399 --> 00:32:03,899
음 저는

940
00:32:03,899 --> 00:32:06,600
그들이 배울 때

941
00:32:06,600 --> 00:32:08,580
어떤 의미에서 그들이 음 어떤 의미에서  간단

942
00:32:08,580 --> 00:32:10,559
하거나 인색한

943
00:32:10,559 --> 00:32:12,360
음 그들이 본 데이터에 대한 설명

944
00:32:12,360 --> 00:32:13,980
과 그것이

945
00:32:13,980 --> 00:32:16,080
신경망에서 어떻게 캐싱되는지는 아마도

946
00:32:16,080 --> 00:32:19,200
복잡할 수 있으며

947
00:32:19,200 --> 00:32:20,880
음 매개 변수와

948
00:32:20,880 --> 00:32:22,679
학습 알고리즘의 세부 사항을 알고 있고 그리고 그리고 그리고

949
00:32:22,679 --> 00:32:24,480
그런 종류의 것들이

950
00:32:24,480 --> 00:32:27,000
음 하지만 내 생각에는 어

951
00:32:27,000 --> 00:32:29,279
아마도 음 어

952
00:32:29,279 --> 00:32:31,740


953
00:32:31,740 --> 00:32:35,640
그들은 그들이

954
00:32:35,640 --> 00:32:38,399
복잡한 것들에 대해 배우는 것처럼 복잡한 종류의 것들에 대해 배우는 것일 가능성이 있다고 생각합니다

955
00:32:38,399 --> 00:32:40,799


956
00:32:40,799 --> 00:32:43,799
규칙 및 구성의 계열

957
00:32:43,799 --> 00:32:46,919
음 그리고 그것은

958
00:32:46,919 --> 00:32:49,380
음 그들의 일반화가

959
00:32:49,380 --> 00:32:52,500
당신이 준 사람들의 예와 같을 수 있다는 것을 의미합니다.

960
00:32:52,500 --> 00:32:55,440


961
00:32:55,440 --> 00:32:57,720
입력 오른쪽에서 일종의 불연속적 일 수 있으므로 때때로 당신은

962
00:32:57,720 --> 00:33:00,000


963
00:33:00,000 --> 00:33:02,220
다음으로 이어지는 문자열을 보는 것을 상상할 수 있습니다.  지금까지 본 데이터의 문법과 가장 간단한

964
00:33:02,220 --> 00:33:03,840
문법은

965
00:33:03,840 --> 00:33:06,059


966
00:33:06,059 --> 00:33:09,120
보이지 않는 문자열을 예측하는 것입니다.

967
00:33:09,120 --> 00:33:12,360
음, 그런 일이 발생하면

968
00:33:12,360 --> 00:33:14,100
데이터가 보이지 않는 소설

969
00:33:14,100 --> 00:33:16,860
에서 일반화되는 표현을 학습하게 될 것입니다.

970
00:33:16,860 --> 00:33:19,440
지금까지

971
00:33:19,440 --> 00:33:21,720
음 순전히 그 일반화는

972
00:33:21,720 --> 00:33:23,460


973
00:33:23,460 --> 00:33:25,019
여러분이 지금까지 본 데이터의 가장 간단한 설명이기 때문입니다.

974
00:33:25,019 --> 00:33:25,980
제 생각에는

975
00:33:25,980 --> 00:33:28,320
언어학자들이

976
00:33:28,320 --> 00:33:29,700
데이터를 보고

977
00:33:29,700 --> 00:33:31,260
그것의 이론 그리고 때때로 그

978
00:33:31,260 --> 00:33:33,419
이론은 어떤 새로운 현상을

979
00:33:33,419 --> 00:33:35,840
올바르게 예측하거나 어떤 새로운 유형의 문장을 예측합니다.

980
00:33:35,840 --> 00:33:38,159
그래서 그들이

981
00:33:38,159 --> 00:33:41,100
충분히 풍부한 이론 공간을 통해 배우고 있다면 음,

982
00:33:41,100 --> 00:33:42,899


983
00:33:42,899 --> 00:33:45,000
그들이 비합리적이거나 예상치 못한

984
00:33:45,000 --> 00:33:47,039
또한 그러한 종류의 패턴을 보여주든

985
00:33:47,039 --> 00:33:48,899
그렇지 않든

986
00:33:48,899 --> 00:33:51,299
여전히 열려 있는 경험적 질문입니다.

987
00:33:51,299 --> 00:33:52,740


988
00:33:52,740 --> 00:33:54,120
음,

989
00:33:54,120 --> 00:33:55,620
소량의 데이터로 그들을 교육하고 일반화를 테스트해야 하기 때문에

990
00:33:55,620 --> 00:33:57,480


991
00:33:57,480 --> 00:33:58,260


992
00:33:58,260 --> 00:34:00,120
음, 하지만 저는 그렇지 않습니다.

993
00:34:00,120 --> 00:34:01,620


994
00:34:01,620 --> 00:34:03,899
인간이 순전히

995
00:34:03,899 --> 00:34:06,059
빈도에 기반하지 않은 일을 한다는 사실이

996
00:34:06,059 --> 00:34:07,860
어느 쪽이든 옳다는 증거라고 생각하지 마세요.

997
00:34:07,860 --> 00:34:09,300
일단 풍부하고 흥미로운 이론에 대해 배우고 나면

998
00:34:09,300 --> 00:34:10,980


999
00:34:10,980 --> 00:34:13,980
그것이 실제로 예상되는 행동이기 때문입니다.

1000
00:34:13,980 --> 00:34:16,980
나는 음 음 1

1001
00:34:16,980 --> 00:34:18,719
년 전에 당신이 우리가 있었던

1002
00:34:18,719 --> 00:34:20,040


1003
00:34:20,040 --> 00:34:22,560
음 어 양과 피안타 도시에 대해 잘 알고 있을 것 같은 논문을 가지고 있었습니다.

1004
00:34:22,560 --> 00:34:24,179


1005
00:34:24,179 --> 00:34:26,879


1006
00:34:26,879 --> 00:34:29,280


1007
00:34:29,280 --> 00:34:32,639
어 서로

1008
00:34:32,639 --> 00:34:34,679
다른 공식 언어가 있으므로

1009
00:34:34,679 --> 00:34:35,820


1010
00:34:35,820 --> 00:34:38,760
일반적인 모델을 제공하고 일부 패턴을 따르는

1011
00:34:38,760 --> 00:34:41,159
간단한 문자열 10개 또는 20개를 제공한

1012
00:34:41,159 --> 00:34:43,500
다음

1013
00:34:43,500 --> 00:34:46,199
해당 데이터를 설명할 수 있는 프로그램을 찾

1014
00:34:46,199 --> 00:34:49,320


1015
00:34:49,320 --> 00:34:50,940
도록 요청하는 것과 같이 생각해보세요.

1016
00:34:50,940 --> 00:34:52,679
프로그래밍 방식으로

1017
00:34:52,679 --> 00:34:54,839
패턴을 문자열에 기록하는 방법

1018
00:34:54,839 --> 00:34:56,280
음 그리고 그 그림에서 우리는 이 시점과 정말 관련된 논문을 가지고 있습니다.

1019
00:34:56,280 --> 00:34:58,020


1020
00:34:58,020 --> 00:34:59,820


1021
00:34:59,820 --> 00:35:02,640


1022
00:35:02,640 --> 00:35:04,200


1023
00:35:04,200 --> 00:35:06,660
질적으로는

1024
00:35:06,660 --> 00:35:08,160


1025
00:35:08,160 --> 00:35:10,560
사람들을 위해 설명하는 것과 같습니다.

1026
00:35:10,560 --> 00:35:12,060
음 어 작은 양의

1027
00:35:12,060 --> 00:35:13,980
데이터를 제공하면 훈련 입력에 빈도가 0이더라도 매우 높은 확률로 보이지 않는 문자열을 예측할 수 있고

1028
00:35:13,980 --> 00:35:16,740


1029
00:35:16,740 --> 00:35:18,420


1030
00:35:18,420 --> 00:35:20,280


1031
00:35:20,280 --> 00:35:22,740
그렇게 하는 이유가 있습니다.  당신이 본

1032
00:35:22,740 --> 00:35:24,660
데이터에 대한 가장 간결한 계산 설명은 보이지

1033
00:35:24,660 --> 00:35:26,640


1034
00:35:26,640 --> 00:35:29,640
않는 특정

1035
00:35:29,640 --> 00:35:32,940
출력을 예측하여 해당 모델이

1036
00:35:32,940 --> 00:35:34,800
본질적으로

1037
00:35:34,800 --> 00:35:36,839
일종의 Chader 및 Vitani 프로그램

1038
00:35:36,839 --> 00:35:38,160
학습

1039
00:35:38,160 --> 00:35:40,619
음 아이디어의 구현이라는 것입니다.  이전에 자랐지

1040
00:35:40,619 --> 00:35:42,900
만

1041
00:35:42,900 --> 00:35:43,980


1042
00:35:43,980 --> 00:35:45,660
아이들이

1043
00:35:45,660 --> 00:35:48,599
비정상적이거나 예상치 못한 일을 말하는 이러한 주장의 맥락에서 생각한다면

1044
00:35:48,599 --> 00:35:50,280
이러한 모든 종류의 계정에 의해 예측된다는 것을 알고 있다고 생각합니다.

1045
00:35:50,280 --> 00:35:52,859


1046
00:35:52,859 --> 00:35:54,180
이런 것들은 문법

1047
00:35:54,180 --> 00:35:55,680
의 흥미로운 공간을 효과적으로 비교하고 있습니다

1048
00:35:55,680 --> 00:35:56,760


1049
00:35:56,760 --> 00:35:58,079
음 그러면 그들은 그런

1050
00:35:58,079 --> 00:35:59,760
종류의 행동이 제 생각에는

1051
00:35:59,760 --> 00:36:04,680
어 그래 괜찮아요 그래서

1052
00:36:04,680 --> 00:36:06,960
최소한

1053
00:36:06,960 --> 00:36:10,140
성별 관점에서 구문이

1054
00:36:10,140 --> 00:36:13,140
별도로 작동한다는 주장을 알 것 같아요  그러나 그것은 여전히 ​​의미론

1055
00:36:13,140 --> 00:36:15,359
에 매핑되어 있어

1056
00:36:15,359 --> 00:36:17,040
실용론을 올바르게 알려줍니다. 따라서 중동

1057
00:36:17,040 --> 00:36:18,480
프로그램 구문은 분명히 의미가 없습니다.

1058
00:36:18,480 --> 00:36:20,160
매우 작습니다. 단지

1059
00:36:20,160 --> 00:36:22,320
선형화 및 라벨링 일뿐입니다.

1060
00:36:22,320 --> 00:36:24,180


1061
00:36:24,180 --> 00:36:26,220
중심 운동 시스템에 대한 선형화 알고리즘

1062
00:36:26,220 --> 00:36:27,900
과 일종의  개념 시스템의

1063
00:36:27,900 --> 00:36:30,480
um 센터에 있는 분류 알고리즘

1064
00:36:30,480 --> 00:36:32,220


1065
00:36:32,220 --> 00:36:33,839
음 그래서 촘스키의 아키텍처는

1066
00:36:33,839 --> 00:36:35,940
구문을 시맨틱에 매핑하는 프로세스에 의존하는 것 같습니다.

1067
00:36:35,940 --> 00:36:37,560
거품 의미

1068
00:36:37,560 --> 00:36:39,960
규제입니다.

1069
00:36:39,960 --> 00:36:42,180
단순한 구조도 의미도 아니기 때문에 LMS에는

1070
00:36:42,180 --> 00:36:43,680
실제로 이것이 없습니다.  시맨틱에 대한 매핑이 어디에 있는지 매핑 프로세스

1071
00:36:43,680 --> 00:36:45,180


1072
00:36:45,180 --> 00:36:47,160
와 매핑이 있는 경우

1073
00:36:47,160 --> 00:36:48,480
매핑 프로세스가 어떻게 생겼는지

1074
00:36:48,480 --> 00:36:50,099
시맨틱의 속성이

1075
00:36:50,099 --> 00:36:52,320
무엇인지 알 수 있습니다.

1076
00:36:52,320 --> 00:36:54,000
의미론의 속성이

1077
00:36:54,000 --> 00:36:55,500
자체 세트에 배치하는 작업  자연어에 대해

1078
00:36:55,500 --> 00:36:57,180
하는 것과 같은 마케팅 프로세스에 대한 제약은

1079
00:36:57,180 --> 00:36:58,980
당신이

1080
00:36:58,980 --> 00:37:01,079
알고 있는 것입니다. 이러한 종류의 제약은

1081
00:37:01,079 --> 00:37:02,640
서로에게 알려주는 것입니다.

1082
00:37:02,640 --> 00:37:05,579
요소가 이

1083
00:37:05,579 --> 00:37:07,260
양식 의미 쌍을 실제로 설명하지 않는 것처럼 일종의 앞뒤 프로세스인지 여부입니다.

1084
00:37:07,260 --> 00:37:09,359


1085
00:37:09,359 --> 00:37:11,339
예를 들어 어떤 문자열이 어떤 의미를 가지고 있는지와 같이 정확합니다.

1086
00:37:11,339 --> 00:37:14,640
죄송합니다.

1087
00:37:14,640 --> 00:37:16,740
음, 의미론이

1088
00:37:16,740 --> 00:37:18,599
전혀 없다고 말하는 건가요 아니면 구조가 의미론에 매핑되는 방식

1089
00:37:18,599 --> 00:37:21,180
사이에 명확한 어 묘사가 없다고 말하는 건가요?

1090
00:37:21,180 --> 00:37:23,400


1091
00:37:23,400 --> 00:37:25,500
예 후자입니다.  그래서 그들은

1092
00:37:25,500 --> 00:37:27,300
잠재적으로 어떤 종류

1093
00:37:27,300 --> 00:37:28,800
의 의미론을 가지고 있습니다. 저는 당신이

1094
00:37:28,800 --> 00:37:30,180
개념적 역할 이론이 여기에 관련되어 있다고 주장한 것을 알고 있습니다.

1095
00:37:30,180 --> 00:37:31,920
나머지 부분은

1096
00:37:31,920 --> 00:37:33,900
조금 더 신비할 수 있지만 실제 대두

1097
00:37:33,900 --> 00:37:35,339
언어학은

1098
00:37:35,339 --> 00:37:36,900
매핑 프로세스의 이론이 있습니다.  그 자체는

1099
00:37:36,900 --> 00:37:39,060
명시적이며 실제로 작동하는 것을 볼 수 있으며

1100
00:37:39,060 --> 00:37:40,440


1101
00:37:40,440 --> 00:37:42,000
Psych 언어 모델에서 다양한 이론을 테스트할 수 있으며

1102
00:37:42,000 --> 00:37:44,700
실제 규정이 무엇인지 알 수 있습니다.

1103
00:37:44,700 --> 00:37:46,079


1104
00:37:46,079 --> 00:37:48,119


1105
00:37:48,119 --> 00:37:50,400
구조

1106
00:37:50,400 --> 00:37:53,040
다중 해석 기타 맞아요

1107
00:37:53,040 --> 00:37:55,200
네 제 말은

1108
00:37:55,200 --> 00:37:57,599
그들이 의미론을 가지고 있다고 생각한다면 구문에서 의미론으로의 매핑이 있어야 한다고 생각합니다 음

1109
00:37:57,599 --> 00:37:59,640


1110
00:37:59,640 --> 00:38:00,960


1111
00:38:00,960 --> 00:38:03,599
동의합니다 아무도

1112
00:38:03,599 --> 00:38:04,980
그들이

1113
00:38:04,980 --> 00:38:07,560
어떤 심층 수준에서 어떻게 작업하고 있는지 이해하지 못하는 것과 같지 않다는 데 동의합니다  맞아 그래서 나는 그것이

1114
00:38:07,560 --> 00:38:10,140


1115
00:38:10,140 --> 00:38:11,940
생성 구문과 의미론에서 말하는 것처럼 명확하지 않다는 데 동의합니다. 음

1116
00:38:11,940 --> 00:38:13,859


1117
00:38:13,859 --> 00:38:15,180


1118
00:38:15,180 --> 00:38:17,880
구성 규칙을 적어두고 구성 요소의 문장

1119
00:38:17,880 --> 00:38:19,920
에서 구성 의미를 파생할 수 있다는 것을 알고 있습니다.

1120
00:38:19,920 --> 00:38:21,480
또는 그런 게 맞아요

1121
00:38:21,480 --> 00:38:23,339


1122
00:38:23,339 --> 00:38:24,660
음 그게 그들이 일하는 방식이 아니라는 걸 알지만 음 난 난

1123
00:38:24,660 --> 00:38:27,060


1124
00:38:27,060 --> 00:38:28,859
그냥 난

1125
00:38:28,859 --> 00:38:32,160
그렇게 되어야 한다는 걸 당연하게 여기지 않을 거에요 음 어

1126
00:38:32,160 --> 00:38:34,619
그들이

1127
00:38:34,619 --> 00:38:36,420
일하는 방식이 실제로 우리가

1128
00:38:36,420 --> 00:38:38,760
어 모든 것이 고차원 벡터 공간에 표현되고

1129
00:38:38,760 --> 00:38:40,800


1130
00:38:40,800 --> 00:38:43,740


1131
00:38:43,740 --> 00:38:46,200
벡터 의미 체계가

1132
00:38:46,200 --> 00:38:48,240
각각의 추가 단어 또는

1133
00:38:48,240 --> 00:38:51,480
언어 흐름에 있는 무엇이든 업데이트되는 복잡한 방식이 있습니다.

1134
00:38:51,480 --> 00:38:53,940


1135
00:38:53,940 --> 00:38:55,320


1136
00:38:55,320 --> 00:38:57,660


1137
00:38:57,660 --> 00:38:59,099


1138
00:38:59,099 --> 00:39:01,260
예를 들어 적어도 대략적인 질문에 답할 수 있는 것처럼 문장의 의미 체계를 표현하는 것입니다.

1139
00:39:01,260 --> 00:39:02,760
제 말은 완벽하지는 않지만

1140
00:39:02,760 --> 00:39:04,380


1141
00:39:04,380 --> 00:39:06,720
엔그램 모델과 같지 않거나

1142
00:39:06,720 --> 00:39:07,740
실제로는 의미 체계가 없는 권리가 없다는 것입니다.

1143
00:39:07,740 --> 00:39:10,500


1144
00:39:10,500 --> 00:39:12,780
음 나는 그것들이

1145
00:39:12,780 --> 00:39:14,040
음 그것들이 확실히

1146
00:39:14,040 --> 00:39:16,920
의미론을 나타내고 있다는 것을 알고 있고 음 어

1147
00:39:16,920 --> 00:39:20,040
그들은 그들이 언어를 처리할 때 업데이트하는 것을 알고 있습니다.

1148
00:39:20,040 --> 00:39:21,599
그것은 우연히

1149
00:39:21,599 --> 00:39:23,400
다른 형식 이론처럼 보이지 않는 일이 발생합니다

1150
00:39:23,400 --> 00:39:24,599


1151
00:39:24,599 --> 00:39:26,400
음 그리고 나는 내가 보지 못하는 것 같습니다  왜

1152
00:39:26,400 --> 00:39:27,720
그것이 다른 형식 이론과 마찬가지로 문제가 되는지

1153
00:39:27,720 --> 00:39:29,640
당신이 형편

1154
00:39:29,640 --> 00:39:31,920
없는 근사치를 알고 있거나 완전히

1155
00:39:31,920 --> 00:39:33,720
틀렸을 수 있습니다 예 예 예

1156
00:39:33,720 --> 00:39:35,520
아니오 완전히 완전히 내 말은

1157
00:39:35,520 --> 00:39:37,260


1158
00:39:37,260 --> 00:39:39,359
의미론의 형식 이론 중

1159
00:39:39,359 --> 00:39:41,520
일부가 이미 잠재적으로 일부

1160
00:39:41,520 --> 00:39:42,780
이러한 것들이 올바르게 작동하고 있으므로

1161
00:39:42,780 --> 00:39:45,240
이것에 대해 생각하는 또 다른 방법은 LMS가 잘 알고 있다는 것입니다.

1162
00:39:45,240 --> 00:39:47,760
LMS는 압축 알고리즘

1163
00:39:47,760 --> 00:39:49,740
이지만 자연어 이해는 압축

1164
00:39:49,740 --> 00:39:51,540
해제에 관한 것입니다.

1165
00:39:51,540 --> 00:39:54,180
의미 X에서

1166
00:39:54,180 --> 00:39:56,099
의미 XYZ를 명확하게 하는 것입니다.

1167
00:39:56,099 --> 00:39:58,020
알고 있는 것에 대해 추론하는 것이 전부입니다.  교육 데이터

1168
00:39:58,020 --> 00:39:59,579
에 없는 개념 간의 메타 관계

1169
00:39:59,579 --> 00:40:01,859
그래서

1170
00:40:01,859 --> 00:40:03,180
Melanie Mitchell이 ​​제공하는 몇 가지 예는

1171
00:40:03,180 --> 00:40:06,119
그녀가 다시 정상에 있다는 것을 알고 있습니다.

1172
00:40:06,119 --> 00:40:08,700


1173
00:40:08,700 --> 00:40:10,140


1174
00:40:10,140 --> 00:40:11,880
잘 진행되고 있는 다른 일들

1175
00:40:11,880 --> 00:40:13,260
음 그리고 제 생각에 당신은 당신의 논문에서 그러한 예들 중 일부에 대해 논의하고 있다고 생각합니다.

1176
00:40:13,260 --> 00:40:16,440
그래서 당신은

1177
00:40:16,440 --> 00:40:18,119
음 하지만 언어 능력은 여전히

1178
00:40:18,119 --> 00:40:20,940
이 언어 이론 하에서 적어도 다시는 아니며

1179
00:40:20,940 --> 00:40:22,920
문자열 생성에 관한 것이 아니라

1180
00:40:22,920 --> 00:40:25,200
이 형식에 관한 것입니다.  의미

1181
00:40:25,200 --> 00:40:27,540
짝짓기 기계 그래서 때때로 이것은

1182
00:40:27,540 --> 00:40:29,400
속격 전통에서 심지어

1183
00:40:29,400 --> 00:40:31,859
의미론에 있는 모든 것이 정당하고 옳다고 생각하므로

1184
00:40:31,859 --> 00:40:33,540
Paul Petrovsky의 접속법은

1185
00:40:33,540 --> 00:40:36,000
인간의 의미론이 정당하다는 것입니다.

1186
00:40:36,000 --> 00:40:37,500


1187
00:40:37,500 --> 00:40:39,900


1188
00:40:39,900 --> 00:40:42,000
그것은

1189
00:40:42,000 --> 00:40:43,920


1190
00:40:43,920 --> 00:40:46,320
여러분이 알고 있거나 아마도 여러분의

1191
00:40:46,320 --> 00:40:47,940
목에 있는 단어들과 호환될 수 있지만 그럼에도 불구하고

1192
00:40:47,940 --> 00:40:49,859
여전히 자연어가

1193
00:40:49,859 --> 00:40:51,540
여전히 더 구성적이라는 것을 알고 있습니다.

1194
00:40:51,540 --> 00:40:54,060


1195
00:40:54,060 --> 00:40:55,260


1196
00:40:55,260 --> 00:40:56,820
그들은

1197
00:40:56,820 --> 00:40:58,260
훨씬 더 풍부한 구조 구성을 갖게 되었습니다.

1198
00:40:58,260 --> 00:41:00,839
더 많은 일이 진행되고 있습니다. 어 아마도 주의

1199
00:41:00,839 --> 00:41:02,160


1200
00:41:02,160 --> 00:41:03,480
기반 기계 메커니즘 및 트랜스포머와 같은 것을 알고 있기 전에 지적된 바 있습니다.

1201
00:41:03,480 --> 00:41:05,640


1202
00:41:05,640 --> 00:41:07,800
음 병합에 더 가까운 개별 토큰 바인딩의 조합을 허용합니다.

1203
00:41:07,800 --> 00:41:10,320


1204
00:41:10,320 --> 00:41:12,359
간단한 순환 행렬 곱셈보다 연산자와 비슷

1205
00:41:12,359 --> 00:41:14,640


1206
00:41:14,640 --> 00:41:16,140
하지만 이진 분기 정부의 문제를 알고 있습니다.

1207
00:41:16,140 --> 00:41:17,460


1208
00:41:17,460 --> 00:41:19,260
여기에서

1209
00:41:19,260 --> 00:41:20,640
전체 의미 규정에 대해 이야기하기 위해 다른 예를 선택하기 위해

1210
00:41:20,640 --> 00:41:23,640
이진 분기 이미지는

1211
00:41:23,640 --> 00:41:24,839
흥미로운 질문이지만 기하학

1212
00:41:24,839 --> 00:41:26,700
문법은 항상 다른 것에 열려 있습니다.

1213
00:41:26,700 --> 00:41:28,740


1214
00:41:28,740 --> 00:41:30,660
합성 계산에서 이 명백한 제약의 기원과 위치는

1215
00:41:30,660 --> 00:41:31,800
어디에서

1216
00:41:31,800 --> 00:41:33,540
왔는지, 병합에 대한 조건일 수도 있고,

1217
00:41:33,540 --> 00:41:34,980
원활한 시스템에 의해 부과될 수도 있으며,

1218
00:41:34,980 --> 00:41:37,079


1219
00:41:37,079 --> 00:41:39,060
누가 알고 있는지 알 수 있는 일종의 사전일 수도 있고 실제로는

1220
00:41:39,060 --> 00:41:40,500
더 최근에 작동하는 생성 문법일 수도 있습니다.

1221
00:41:40,500 --> 00:41:43,859


1222
00:41:43,859 --> 00:41:46,440
결혼에 대한 모든 이론적 가정을 근거로 삼고 없애려고 노력했습니다.

1223
00:41:46,440 --> 00:41:47,700
이론을 설정하는 것이 음 생성 문법을 모델링하는 가장 좋은 방법이 아닐 수도 있습니다.

1224
00:41:47,700 --> 00:41:48,780


1225
00:41:48,780 --> 00:41:50,339
마리아의

1226
00:41:50,339 --> 00:41:51,540
논리적 설명이 더 적절할 수도 있습니다.

1227
00:41:51,540 --> 00:41:53,520


1228
00:41:53,520 --> 00:41:55,020


1229
00:41:55,020 --> 00:41:57,540
사실 촘스키의 접근 방식이 맞습니다.

1230
00:41:57,540 --> 00:41:58,680
트럼프가

1231
00:41:58,680 --> 00:42:00,060
가장 좋아하는 것 중 하나는 그가

1232
00:42:00,060 --> 00:42:01,380
틀렸다는 것이 입증되었을 때입니다. 이러한 많은

1233
00:42:01,380 --> 00:42:03,720
이론은 핵심 주류

1234
00:42:03,720 --> 00:42:06,300
미니멀리스트 아키텍처에 반대하지만

1235
00:42:06,300 --> 00:42:08,640
네 저는 매우 다양하다고 생각합니다.

1236
00:42:08,640 --> 00:42:11,839
활기찬 분야 본슈타인 사람들은 아시

1237
00:42:11,839 --> 00:42:15,119
다시피 페트로프스키 어 하지

1238
00:42:15,119 --> 00:42:17,520
푸라 그들은 화학 문법의

1239
00:42:17,520 --> 00:42:19,380
주류가 말하는 많은 것에 근본적인 방식으로 동의하지 않지만

1240
00:42:19,380 --> 00:42:20,940


1241
00:42:20,940 --> 00:42:21,960
여전히

1242
00:42:21,960 --> 00:42:24,240
의견 불일치의 여지가 더 있지만 여전히

1243
00:42:24,240 --> 00:42:26,400
핵심 가정을 올바르게 설정하는 것과 호환되므로

1244
00:42:26,400 --> 00:42:27,599
많은  David 저는 예를 들어

1245
00:42:27,599 --> 00:42:29,400
이 핵심적인 측면에서 일종의 일탈을 입지

1246
00:42:29,400 --> 00:42:31,920
만 여전히

1247
00:42:31,920 --> 00:42:33,240
다른 형식 시스템에서 이러한 직관을 접지하려고 노력하고 있습니다

1248
00:42:33,240 --> 00:42:34,500


1249
00:42:34,500 --> 00:42:36,000
음 그래서 당신이 그것에 대해

1250
00:42:36,000 --> 00:42:38,099


1251
00:42:38,099 --> 00:42:40,320
다시 생각하고 싶다는 것을 알고 있습니다. 음

1252
00:42:40,320 --> 00:42:42,240
Mitchell을 올바르게 언급했기 때문에 Michelin

1253
00:42:42,240 --> 00:42:44,579
Bowers uh  2020 그들은 이 종이

1254
00:42:44,579 --> 00:42:47,040
시험 목록 순환 네트워크를

1255
00:42:47,040 --> 00:42:48,359
호기심에 배치했는데 당신이

1256
00:42:48,359 --> 00:42:49,859
옳다고 생각합니다. 문제의

1257
00:42:49,859 --> 00:42:51,060
핵심에 도달하기 위한 정말 좋은 예이므로

1258
00:42:51,060 --> 00:42:53,099
순환 신경망이

1259
00:42:53,099 --> 00:42:54,780


1260
00:42:54,780 --> 00:42:56,520
당신이 알고 있지 않은 것을 정확하게 모델링하는 것으로 나타났습니다.  -동사 수는 일치

1261
00:42:56,520 --> 00:42:58,020
하지만 Mitchell과 Barrow는

1262
00:42:58,020 --> 00:43:00,060
이러한 네트워크가 부자연스러운 문장 구조와도 일치할 수 있음을 보여주어

1263
00:43:00,060 --> 00:43:01,680


1264
00:43:01,680 --> 00:43:03,359


1265
00:43:03,359 --> 00:43:04,859
자연어에서 찾을 수 없고

1266
00:43:04,859 --> 00:43:06,540
인간이 제대로 처리하는 데 어려움을 겪는 구조이므로

1267
00:43:06,540 --> 00:43:09,359
rnns에 대한 학습 모드는

1268
00:43:09,359 --> 00:43:11,880
최소한  rnn은 유아와 긍정적으로 구별되므로

1269
00:43:11,880 --> 00:43:14,339
유아 호모 사피엔스를 알고 있으므로

1270
00:43:14,339 --> 00:43:16,260
이야기는 Mitchell이고

1271
00:43:16,260 --> 00:43:18,359
Bowers는 lstl 모델이

1272
00:43:18,359 --> 00:43:20,040


1273
00:43:20,040 --> 00:43:22,140
개별 문장에 대해 단수형과 복수형을 잘 표현하지만

1274
00:43:22,140 --> 00:43:24,359


1275
00:43:24,359 --> 00:43:25,800
개인 수준에서 나타낼 수 있는 일반화가 진행되지 않는다는 것을 보여줍니다.

1276
00:43:25,800 --> 00:43:27,359
따라서 모델은

1277
00:43:27,359 --> 00:43:28,859
숫자를

1278
00:43:28,859 --> 00:43:31,200
추상화로 표현하지 않습니다. 어떤 숫자는

1279
00:43:31,200 --> 00:43:34,020
단수형 복수형의 구체적인 인스턴스일 뿐이

1280
00:43:34,020 --> 00:43:35,400
므로

1281
00:43:35,400 --> 00:43:38,579
LM을 통해 언어 행동을 성공적으로 예측하거나

1282
00:43:38,579 --> 00:43:40,800
유사한 방식으로 신경 반응을 성공적으로 예측하는

1283
00:43:40,800 --> 00:43:42,480
것은 분명히 훌륭하고 아마도 우리는

1284
00:43:42,480 --> 00:43:43,920
그 문제는 나중에 나오지만

1285
00:43:43,920 --> 00:43:45,240
여기 동전의 한쪽면만 있습니다.

1286
00:43:45,240 --> 00:43:47,099
동전의 다른 쪽 면은 왜

1287
00:43:47,099 --> 00:43:48,780
이런 유형의 행동이 다른

1288
00:43:48,780 --> 00:43:50,339
행동이 아닌지를 설명하고 있습니다. 왜 이 구조가 제가 비슷하지 않은지

1289
00:43:50,339 --> 00:43:52,740
그리고 그것은 아마도 촘스키의 가장이고

1290
00:43:52,740 --> 00:43:55,380
당신이 그의 가장 잘 알고 있는 것처럼  중요한

1291
00:43:55,380 --> 00:43:56,760
점 정말 이것이 다른 시스템이 아닌 이유

1292
00:43:56,760 --> 00:43:59,400
는 언어 이론 종류는

1293
00:43:59,400 --> 00:44:00,720
당신에게 동전의 시작을 제공하는

1294
00:44:00,720 --> 00:44:02,760
반면 LM은 실제로 수행되므로

1295
00:44:02,760 --> 00:44:03,839
Mitchell 당황한 종이는

1296
00:44:03,839 --> 00:44:05,819
그가 잘하는 것과 같은 것을 수행합니다

1297
00:44:05,819 --> 00:44:09,420
예 그래서 음 Yael Le를 가져가는 것과 같습니다  crets

1298
00:44:09,420 --> 00:44:11,400
와 stanislash the Haynes는 2019년부터

1299
00:44:11,400 --> 00:44:13,319


1300
00:44:13,319 --> 00:44:15,480
lstm에서 숫자 일치를 살펴보고 숫자 일치를 인코딩하는 두 개의 특수 단위를 찾았지만

1301
00:44:15,480 --> 00:44:17,640


1302
00:44:17,640 --> 00:44:19,020
성능에 대한 전반적인 기여도는

1303
00:44:19,020 --> 00:44:21,839
낮았고 2021년에 어 예 수정이 이

1304
00:44:21,839 --> 00:44:24,000
문서에서

1305
00:44:24,000 --> 00:44:26,040
음이 있음을 보여줍니다.  그들의 신경 언어 모델은 이탈리아어로 된 중첩된 장거리 합의 성별 표시의

1306
00:44:26,040 --> 00:44:28,079
진정한 재귀 처리를 달성하지 못했습니다.

1307
00:44:28,079 --> 00:44:30,540


1308
00:44:30,540 --> 00:44:32,160


1309
00:44:32,160 --> 00:44:34,020


1310
00:44:34,020 --> 00:44:35,819


1311
00:44:35,819 --> 00:44:37,380
이전에 주장한 것처럼 일부 계층적 처리가 달성되었다고 해도 음, 일부 계층이

1312
00:44:37,380 --> 00:44:39,960
남아 있었지만 문제는

1313
00:44:39,960 --> 00:44:41,280


1314
00:44:41,280 --> 00:44:42,900


1315
00:44:42,900 --> 00:44:45,119
lstn 기반 모델이

1316
00:44:45,119 --> 00:44:47,220
1도의 짧은 범위에 걸쳐 주제 웹 계약을

1317
00:44:47,220 --> 00:44:49,260
체결할 수 있지만 더 긴

1318
00:44:49,260 --> 00:44:51,359
종속성 및 가장 최근

1319
00:44:51,359 --> 00:44:53,700
논문에서 실패했다는 것을 발견한 올바른 종류의 계층 구조입니다.  손을 잡고

1320
00:44:53,700 --> 00:44:56,760


1321
00:44:56,760 --> 00:45:00,180


1322
00:45:00,180 --> 00:45:01,980
동일한 작업에서 gpt2 XL을 포함한 현대 Transformer LMS를 평가했으며 Transformers는

1323
00:45:01,980 --> 00:45:04,260
LSM보다 인간과 더 유사하게 수행

1324
00:45:04,260 --> 00:45:06,300
하고 전반적으로 이전보다 높은 수행을 수행했지만 내가

1325
00:45:06,300 --> 00:45:08,040


1326
00:45:08,040 --> 00:45:09,660


1327
00:45:09,660 --> 00:45:11,099
언급한 한 가지 주요 조건에서는 여전히 확률보다 낮은 수행을 수행함을 보여주었습니다.  하나는 어려운 경전을 여러 개 삽입하는 것

1328
00:45:11,099 --> 00:45:13,020


1329
00:45:13,020 --> 00:45:14,400
입니다. 그래서 제가 이러한 연구를 언급한 이유는

1330
00:45:14,400 --> 00:45:17,040


1331
00:45:17,040 --> 00:45:18,540
흥미로운 질문인 OMS의 한계를 탐구하는 것이

1332
00:45:18,540 --> 00:45:19,500


1333
00:45:19,500 --> 00:45:21,540
아니라 UCL의 Neil Smith와 같은 사람들의 작업을 고려한다는 것을 알고 있기 때문입니다.

1334
00:45:21,540 --> 00:45:24,180


1335
00:45:24,180 --> 00:45:26,579
90년대에 여러 언어를 사용하는 Savant와

1336
00:45:26,579 --> 00:45:28,740
신경전형적 컨트롤을 비교하여

1337
00:45:28,740 --> 00:45:30,540


1338
00:45:30,540 --> 00:45:32,520


1339
00:45:32,520 --> 00:45:34,500


1340
00:45:34,500 --> 00:45:35,880
미쉐린 바이러스 논문과 같은 자연적이고 부자연스러운 그래픽 구조를 모두 포함하는 인공 언어의 제2 언어 학습을 조사했습니다.

1341
00:45:35,880 --> 00:45:37,079


1342
00:45:37,079 --> 00:45:39,119


1343
00:45:39,119 --> 00:45:41,000


1344
00:45:41,000 --> 00:45:43,319
컨트롤은

1345
00:45:43,319 --> 00:45:45,480
언어적으로 자연스러운 측면을 마스터할 수 있습니다.

1346
00:45:45,480 --> 00:45:46,920
컨트롤만 결국

1347
00:45:46,920 --> 00:45:48,660
구조에 의존하는 부자연스러운 현상을 처리할 수

1348
00:45:48,660 --> 00:45:50,460
있고 둘 다 구조 독립적인 측면을 마스터할 수 없으므로

1349
00:45:50,460 --> 00:45:52,560


1350
00:45:52,560 --> 00:45:53,880


1351
00:45:53,880 --> 00:45:55,440


1352
00:45:55,440 --> 00:45:56,940
문장의 세 번째 단어에 강조 표시를 하는 것과 같은 이상한 규칙이 있습니다.  그래서 그들은

1353
00:45:56,940 --> 00:45:58,740
Christopher의 능력이

1354
00:45:58,740 --> 00:46:00,900
전적으로 그의 온전한 언어

1355
00:46:00,900 --> 00:46:03,480
능력 때문이라고 주장하지만 컨트롤은

1356
00:46:03,480 --> 00:46:05,579
더 많은 영역을 사용할 수 있습니다.

1357
00:46:05,579 --> 00:46:07,319
주의력

1358
00:46:07,319 --> 00:46:09,900
제어 등을 알고 있기 때문에 그들이 어려운 프로세스를 처리할 수 있는 이유입니다.

1359
00:46:09,900 --> 00:46:11,520


1360
00:46:11,520 --> 00:46:13,319


1361
00:46:13,319 --> 00:46:16,079
Mitchell

1362
00:46:16,079 --> 00:46:18,359
당황 종이의 lstm이 거의 같은 방식으로 자연 구조와

1363
00:46:18,359 --> 00:46:19,920
부자연 구조에 접근하기 몇 분 전에

1364
00:46:19,920 --> 00:46:22,319


1365
00:46:22,319 --> 00:46:24,240
심리적으로 그럴듯한 모델이 아니라는 것을 알 수 있습니다.

1366
00:46:24,240 --> 00:46:26,400
인간이 무엇을 하든지

1367
00:46:26,400 --> 00:46:28,200
유사한 관찰이 다음의 한계에 적용될 수 있습니다.

1368
00:46:28,200 --> 00:46:30,060
La

1369
00:46:30,060 --> 00:46:31,920
creta의 작품에 나오는 트랜스포머 모델과 이 모든 테마는

1370
00:46:31,920 --> 00:46:33,780
마치 현재까지 우리와 함께 머물고 있는 것과 같습니다.

1371
00:46:33,780 --> 00:46:35,819
그래서

1372
00:46:35,819 --> 00:46:37,560
그가

1373
00:46:37,560 --> 00:46:39,240
몇 주 전에 게시한 아동

1374
00:46:39,240 --> 00:46:41,880
지시 연설을 살펴보는 또 다른 talin의 최근 논문 중 하나는 음  lstms 및

1375
00:46:41,880 --> 00:46:43,740
Transformers는

1376
00:46:43,740 --> 00:46:46,380
추상적인

1377
00:46:46,380 --> 00:46:47,640
규칙보다는 영어 권리에 대한 선형 규칙을 언급한 것처럼 일반화된 데이터의 생태학적으로 그럴듯한 양으로 제한되었으며

1378
00:46:47,640 --> 00:46:49,920


1379
00:46:49,920 --> 00:46:51,780
사실 지난주에 linton's

1380
00:46:51,780 --> 00:46:54,599
Lab에서 어 웰을 살펴본 지난

1381
00:46:54,599 --> 00:46:56,220
해의 작업은 정원을 바라보는 것을 보여줍니다.

1382
00:46:56,220 --> 00:46:58,160
경로 서프라이즈는 설명하지 않습니다

1383
00:46:58,160 --> 00:47:01,319
어 통사적 명확화 어려움

1384
00:47:01,319 --> 00:47:02,280
맞습니다

1385
00:47:02,280 --> 00:47:03,900
음 놀라움은

1386
00:47:03,900 --> 00:47:05,400
모든 구성에서 정원 경로 효과의 크기를 과소 예측할 것이며

1387
00:47:05,400 --> 00:47:06,780
이것은

1388
00:47:06,780 --> 00:47:08,099
당신이 알기 전에 언급한 이 문제에 도달합니다

1389
00:47:08,099 --> 00:47:10,140
어쩌면 이 모든 관련 어 어

1390
00:47:10,140 --> 00:47:11,520
구문의 일부 측면에 대해 놀랐지만 그렇지 않을 수도 있습니다

1391
00:47:11,520 --> 00:47:12,960
다른 문제는 일종의 공물이

1392
00:47:12,960 --> 00:47:14,819
아닌 문제로 논의의 여지가 매우 많습니다.

1393
00:47:14,819 --> 00:47:16,800


1394
00:47:16,800 --> 00:47:18,720
아직 해결되지 않은 것은 아니지만 Linton은

1395
00:47:18,720 --> 00:47:20,640
Garden Path 효과가

1396
00:47:20,640 --> 00:47:21,720
제게 기대하는 것보다 훨씬 더 어렵다는 것을 보여주었습니다.

1397
00:47:21,720 --> 00:47:24,359
예측불가능성 그래서

1398
00:47:24,359 --> 00:47:26,160
이 주장을 표현하는 또 다른 방법 음은

1399
00:47:26,160 --> 00:47:29,160


1400
00:47:29,160 --> 00:47:30,660
촘스키가 이 자연적

1401
00:47:30,660 --> 00:47:32,940
기초 부자연스러운 문제를 얻기 위해 최근 주장한 인용문입니다.

1402
00:47:32,940 --> 00:47:34,560


1403
00:47:34,560 --> 00:47:36,180


1404
00:47:36,180 --> 00:47:38,819
존재

1405
00:47:38,819 --> 00:47:40,740
하고 존재할 수 없는 모든 요소들

1406
00:47:40,740 --> 00:47:42,660
그리고 여러분이 어떤 모델을 가지고 있다고 가정해 봅시다.

1407
00:47:42,660 --> 00:47:44,880


1408
00:47:44,880 --> 00:47:46,560
이 세 가지 범주를 구별하지 못하는 인공 모델이 있습니다.

1409
00:47:46,560 --> 00:47:48,780
이 모델이

1410
00:47:48,780 --> 00:47:50,640


1411
00:47:50,640 --> 00:47:52,020
무엇을 하든지 그것은 우리가 화학을 이해하는 데 도움이 되지 않습니다.

1412
00:47:52,020 --> 00:47:53,940
확실하지만

1413
00:47:53,940 --> 00:47:55,020


1414
00:47:55,020 --> 00:47:57,180
화학을 이해해야 하는지 여부는 별개의 문제이며

1415
00:47:57,180 --> 00:47:58,560


1416
00:47:58,560 --> 00:47:59,579
이러한 연구 중 일부에 대한 응답으로 귀하가

1417
00:47:59,579 --> 00:48:02,400
말한 것을 알고 있습니다.

1418
00:48:02,400 --> 00:48:03,540


1419
00:48:03,540 --> 00:48:04,920
당신의 논문에서 나는

1420
00:48:04,920 --> 00:48:06,240


1421
00:48:06,240 --> 00:48:07,859


1422
00:48:07,859 --> 00:48:09,720


1423
00:48:09,720 --> 00:48:12,300
당신이

1424
00:48:12,300 --> 00:48:13,440


1425
00:48:13,440 --> 00:48:15,780
500개의 독립적으로 샘플링된 언어와 같은 것을 볼 필요가 있다는 것을 보여줄 필요가 있다는 것을 보여줄 필요가 있는 정치의 정상적인 균형으로는 무언가가 불가능하다는 것을 보여주기 위해 음이라고 말한 것 같아요.

1426
00:48:15,780 --> 00:48:17,880


1427
00:48:17,880 --> 00:48:19,020
아마 할 수 없을 것입니다.

1428
00:48:19,020 --> 00:48:20,579
그것은 실현 가능한 일이 아닙니다.

1429
00:48:20,579 --> 00:48:23,880
그래서 제가 그렇지 않다는 것을 알다시피 저는

1430
00:48:23,880 --> 00:48:25,800
이것이

1431
00:48:25,800 --> 00:48:27,119
제가 여기서 옳게 만들고 있는 주요 주장을 반박하는지 확신할 수 없습니다.

1432
00:48:27,119 --> 00:48:29,040
왜냐하면 Michelin Bowers와 같은 사람들이

1433
00:48:29,040 --> 00:48:30,839


1434
00:48:30,839 --> 00:48:32,220
원칙적으로 불가능성에 대한 논쟁은

1435
00:48:32,220 --> 00:48:33,599
음 어떤 확장적 의미가 아니라 전

1436
00:48:33,599 --> 00:48:34,980


1437
00:48:34,980 --> 00:48:37,440
세계 언어를 검색하여

1438
00:48:37,440 --> 00:48:38,579
모든 단일 언어에서

1439
00:48:38,579 --> 00:48:40,260
불가능하다는 것을 증명하는 것과 같습니다.

1440
00:48:40,260 --> 00:48:41,280


1441
00:48:41,280 --> 00:48:43,740


1442
00:48:43,740 --> 00:48:45,180


1443
00:48:45,180 --> 00:48:47,220


1444
00:48:47,220 --> 00:48:48,420
언어 시스템이 실제로 무엇을

1445
00:48:48,420 --> 00:48:50,160
할 수 있는지에 대한 원칙에 따라 아마존에서 실제로 불가능한 것과 비교했을 때 저는

1446
00:48:50,160 --> 00:48:53,339
그냥 그렇다고 말할 것입니다.

1447
00:48:53,339 --> 00:48:55,980
그 요점은

1448
00:48:55,980 --> 00:48:58,560
유형학적으로 무엇이 아닌지 실제로 알지 못한다는 것입니다.

1449
00:48:58,560 --> 00:49:00,480
그래서 일부 사람들은 X를 수행하는

1450
00:49:00,480 --> 00:49:02,520
언어가 없다는 것을 알고 있으므로 통계 모델에

1451
00:49:02,520 --> 00:49:04,859
해당 제한을 구축해야

1452
00:49:04,859 --> 00:49:06,960


1453
00:49:06,960 --> 00:49:09,119
하지만 통계적으로 그렇지 않은

1454
00:49:09,119 --> 00:49:11,220


1455
00:49:11,220 --> 00:49:13,079
경우 X를 올바르게 수행하는 언어가 없다는 것과 같은 것을 말하는 것을 좋아합니다.

1456
00:49:13,079 --> 00:49:15,119
20개 또는 20개의 유럽

1457
00:49:15,119 --> 00:49:16,920
언어 또는 무언가를 옳게 보았습니다. 제 말은

1458
00:49:16,920 --> 00:49:19,020


1459
00:49:19,020 --> 00:49:22,380
음 어 그게 모델에게 어떤 일을 하도록 동기를 부여해서는 안 된다는 것입니다

1460
00:49:22,380 --> 00:49:24,599


1461
00:49:24,599 --> 00:49:26,280
음 어 통계적으로 정당한 보편적인 언어가 아니라면 제

1462
00:49:26,280 --> 00:49:28,200


1463
00:49:28,200 --> 00:49:28,980
생각에는

1464
00:49:28,980 --> 00:49:30,180
음

1465
00:49:30,180 --> 00:49:32,760
잘 아시겠지만 제 생각에는

1466
00:49:32,760 --> 00:49:33,960
당신이 완전히 옳다는 것을 알지만 그것은

1467
00:49:33,960 --> 00:49:35,339
사회 과학과 심리 과학에 더 일반적으로 적용됩니다.

1468
00:49:35,339 --> 00:49:36,960


1469
00:49:36,960 --> 00:49:39,180


1470
00:49:39,180 --> 00:49:40,380


1471
00:49:40,380 --> 00:49:43,140


1472
00:49:43,140 --> 00:49:44,640


1473
00:49:44,640 --> 00:49:47,339
강한 주장은 X가 있는 언어가 없는 것처럼

1474
00:49:47,339 --> 00:49:49,859
옳다는 것을 증명하기가 매우 어렵다고 말하는 것입니다.

1475
00:49:49,859 --> 00:49:52,619


1476
00:49:52,619 --> 00:49:54,480


1477
00:49:54,480 --> 00:49:56,339
자연어에서 무언가가 허용되지 않는다는 강한 주장은

1478
00:49:56,339 --> 00:49:58,800
증명하기가 매우 어렵다고 생각

1479
00:49:58,800 --> 00:49:59,880


1480
00:49:59,880 --> 00:50:01,980
합니다.

1481
00:50:01,980 --> 00:50:05,460
많은 사람들이 강력한 시도가 있었다는 것을 알고 있습니다.

1482
00:50:05,460 --> 00:50:08,460


1483
00:50:08,460 --> 00:50:10,380


1484
00:50:10,380 --> 00:50:12,720


1485
00:50:12,720 --> 00:50:16,800
모든 언어가 하는 일에 대해 생성 구문에서 나오는 강력한 주장이 많이 있었습니다.

1486
00:50:16,800 --> 00:50:19,140


1487
00:50:19,140 --> 00:50:21,119


1488
00:50:21,119 --> 00:50:22,740
많은 것들에 대해

1489
00:50:22,740 --> 00:50:24,720
저는 Evans와 Levinson이 작성한 이 논문을 인용합니다.

1490
00:50:24,720 --> 00:50:26,579


1491
00:50:26,579 --> 00:50:28,500
음 실제로 제가

1492
00:50:28,500 --> 00:50:30,660
몇 년 동안 어떤 언어도 X를 수행하지 않는다는 것을 들었고

1493
00:50:30,660 --> 00:50:32,160
그것이 우리가

1494
00:50:32,160 --> 00:50:33,599
이론을 구성하는 데 사용하는 것이고 Evans

1495
00:50:33,599 --> 00:50:35,460
와 Levin의  논문 Evans와 Levinson

1496
00:50:35,460 --> 00:50:37,859
논문은 정말 어

1497
00:50:37,859 --> 00:50:40,680
일종의 언어가 내가 생각하는 것

1498
00:50:40,680 --> 00:50:43,260
보다 실제로 훨씬 더 어 다양하다는 이 권리에 대한 제 마음을 바꿨습니다.

1499
00:50:43,260 --> 00:50:44,940


1500
00:50:44,940 --> 00:50:47,760


1501
00:50:47,760 --> 00:50:50,700


1502
00:50:50,700 --> 00:50:53,579


1503
00:50:53,579 --> 00:50:54,839
당신이 말한 것의 시작으로 돌아가서

1504
00:50:54,839 --> 00:50:57,660
우리는

1505
00:50:57,660 --> 00:50:59,880


1506
00:50:59,880 --> 00:51:01,619
아이들이 배우는 것을 배우고

1507
00:51:01,619 --> 00:51:03,660
그들이 배우는 데이터로부터 배운 언어 아키텍처가 필요하다는 데 동의할 것이라고 생각합니다.

1508
00:51:03,660 --> 00:51:05,940
그러한 아키텍처는

1509
00:51:05,940 --> 00:51:09,000
사물이 아닐 수도 있습니다.  lstms와 같은

1510
00:51:09,000 --> 00:51:10,559
간단한 반복 네트워크 또는

1511
00:51:10,559 --> 00:51:12,300


1512
00:51:12,300 --> 00:51:14,400
음과 같은 모든 작업이 올바른 아키텍처를 연마하는 데 매우 유용하다고 생각합니다. 음 음 어

1513
00:51:14,400 --> 00:51:16,680


1514
00:51:16,680 --> 00:51:19,200


1515
00:51:19,200 --> 00:51:20,460


1516
00:51:20,460 --> 00:51:21,420


1517
00:51:21,420 --> 00:51:23,819
그래서

1518
00:51:23,819 --> 00:51:25,200
모든 것을 기억하려고합니다.  당신이 지적한 요점은

1519
00:51:25,200 --> 00:51:26,700
오 예 그래서 음

1520
00:51:26,700 --> 00:51:29,160
하지만 제 생각에는

1521
00:51:29,160 --> 00:51:31,500
이것에 대한 일종의 반전이 있다고 생각합니다.

1522
00:51:31,500 --> 00:51:32,760


1523
00:51:32,760 --> 00:51:34,380
음 제 생각에는

1524
00:51:34,380 --> 00:51:37,619
사람들이 배울 수 있는 것의 공간은 실제로 어 일종의 과소

1525
00:51:37,619 --> 00:51:39,599
평가된

1526
00:51:39,599 --> 00:51:41,760
편향이 있는 것처럼  말하자면 사람들은

1527
00:51:41,760 --> 00:51:43,800
xy와 z를 배울 수 없다는 걸 알지만 음

1528
00:51:43,800 --> 00:51:46,020
적어도

1529
00:51:46,020 --> 00:51:47,460
언어 밖에 있는 사람들은 예를 들어 음악이나 수학에서 찾을 수 있는 패턴과 같은

1530
00:51:47,460 --> 00:51:49,680
다양한 종류의 패턴을 배울 수 있는 정말 놀라운 능력을 가지고 있습니다

1531
00:51:49,680 --> 00:51:51,240


1532
00:51:51,240 --> 00:51:53,160


1533
00:51:53,160 --> 00:51:55,619


1534
00:51:55,619 --> 00:51:57,839
음 어  우리는 정교한 유형의 알고리즘을 배울 수 있습니다. 우주

1535
00:51:57,839 --> 00:52:00,240


1536
00:52:00,240 --> 00:52:03,119
왕복선을 조종하거나

1537
00:52:03,119 --> 00:52:05,760
암벽 등반을 위해 매듭을 짓는 등

1538
00:52:05,760 --> 00:52:07,319
모든

1539
00:52:07,319 --> 00:52:09,540
종류의 절차 및

1540
00:52:09,540 --> 00:52:11,280
알고리즘 지식이 있습니다.

1541
00:52:11,280 --> 00:52:13,440
구조적입니다.  사람들은

1542
00:52:13,440 --> 00:52:16,680
획득할 수 있고 제 생각에 그

1543
00:52:16,680 --> 00:52:19,680
개념은 어 꽤 제한이 없는 공간에서 작동

1544
00:52:19,680 --> 00:52:21,359
할 수 있는 학습 시스템을 찾는 동기를 부여합니다.

1545
00:52:21,359 --> 00:52:24,059


1546
00:52:24,059 --> 00:52:26,160


1547
00:52:26,160 --> 00:52:28,859
음 어 당신은 당신을 알고 있습니다.

1548
00:52:28,859 --> 00:52:30,119
언어가 다르기

1549
00:52:30,119 --> 00:52:33,420
때문에 언어는 다릅니다.  제한된 공간

1550
00:52:33,420 --> 00:52:35,040
음 어 그리고 그 언어가 제한된다는 것이 사실일 수도

1551
00:52:35,040 --> 00:52:36,540
있지만

1552
00:52:36,540 --> 00:52:37,859
우리가 언어에서 보는 것이

1553
00:52:37,859 --> 00:52:39,960
다른 출처에서 온다는 것도 사실일 수도 있습니다.

1554
00:52:39,960 --> 00:52:42,180
어 언어는

1555
00:52:42,180 --> 00:52:44,099
예를 들어

1556
00:52:44,099 --> 00:52:47,040
어 음악이나 수학에 비해 특히 실용적일 수 있습니다

1557
00:52:47,040 --> 00:52:48,720
권리와 그런 종류의 실용적인

1558
00:52:48,720 --> 00:52:50,040
제약은

1559
00:52:50,040 --> 00:52:51,480


1560
00:52:51,480 --> 00:52:53,400
언어의 형식을 제약하는 것입니다. 권리 또는 언어는

1561
00:52:53,400 --> 00:52:54,660
의사소통이 가능합니다. 예를

1562
00:52:54,660 --> 00:52:56,760
들어 음악보다 의사소통이 더 많을 수 있으며

1563
00:52:56,760 --> 00:52:58,440


1564
00:52:58,440 --> 00:53:01,140
사물의 형식을 제약할 수 있습니다. 아시다시피

1565
00:53:01,140 --> 00:53:02,700
이것은 매우  자연 언어의 특성이

1566
00:53:02,700 --> 00:53:05,520
어디에서 오는지에 대한 언어학의 오래된 논쟁 음

1567
00:53:05,520 --> 00:53:07,140


1568
00:53:07,140 --> 00:53:09,059


1569
00:53:09,059 --> 00:53:11,160
그리고 어 제가 말하려는 것은

1570
00:53:11,160 --> 00:53:12,660


1571
00:53:12,660 --> 00:53:15,119


1572
00:53:15,119 --> 00:53:16,980
인간이 할 수 있는 모든 것을 보는 한 가지 관점이 있다는 것입니다.  언어 밖에서도

1573
00:53:16,980 --> 00:53:18,599
모든 풍부한 구조와

1574
00:53:18,599 --> 00:53:20,819
알고리즘 및 프로세스에

1575
00:53:20,819 --> 00:53:23,760
대해 배우고 내면화할 수 있었고

1576
00:53:23,760 --> 00:53:25,559
언어가 그런 것일 수도 있고

1577
00:53:25,559 --> 00:53:27,420
언어도

1578
00:53:27,420 --> 00:53:29,700
이러한 다른 재미있는 작은 속성 중 일부를 가지고 있다고 말할 수

1579
00:53:29,700 --> 00:53:31,380
있지만 아마도 그것들을 알고 있을 것입니다.  언어가

1580
00:53:31,380 --> 00:53:34,020
어디에서 왔는지에 대한 다른 부분에서 옵니다 바로

1581
00:53:34,020 --> 00:53:36,720
그것은 어

1582
00:53:36,720 --> 00:53:38,400
우리가 꽤 정교한 실용적인 추론을 가지고 있다는 것을 알고 있습니다

1583
00:53:38,400 --> 00:53:40,800


1584
00:53:40,800 --> 00:53:42,720
음 어 우리는 특정 의사 소통 목적을 달성하기 위해 그것을 사용하고 있습니다

1585
00:53:42,720 --> 00:53:45,359
당신은 모든

1586
00:53:45,359 --> 00:53:47,160
종류의 의사 소통 기능을 찾을 수 있습니다

1587
00:53:47,160 --> 00:53:49,559
어 내에서  언어 시스템 자체

1588
00:53:49,559 --> 00:53:51,180
와 그래서 아마도 이러한 다른

1589
00:53:51,180 --> 00:53:53,819
속성 중 일부는 다른 기원을 가진 속성일 수 있으며

1590
00:53:53,819 --> 00:53:55,619


1591
00:53:55,619 --> 00:53:57,059


1592
00:53:57,059 --> 00:53:59,579


1593
00:53:59,579 --> 00:54:01,800
제가 생각하기에 그 관점이

1594
00:54:01,800 --> 00:54:04,200
틀릴 수 있다고 생각합니다.  나는 그것이

1595
00:54:04,200 --> 00:54:05,400


1596
00:54:05,400 --> 00:54:10,020
음 어 일종의 기각 된 것 같아요 음 어 많은

1597
00:54:10,020 --> 00:54:12,900
언어 학자들에 의해 음 그냥 당신도 알다시피

1598
00:54:12,900 --> 00:54:14,579
사람들이 오 잘 의사

1599
00:54:14,579 --> 00:54:15,720
소통은 실제로

1600
00:54:15,720 --> 00:54:17,760
언어에 대해 아무것도 설명하지 않으며

1601
00:54:17,760 --> 00:54:20,040
그들이 자주 의미하는 것은 그렇지 않다는 것입니다.

1602
00:54:20,040 --> 00:54:22,200
특정 섬

1603
00:54:22,200 --> 00:54:23,760
제약이나 그들이

1604
00:54:23,760 --> 00:54:25,319
올바르게 작업하고 있는 것과 같은 것을 설명하지 않지만

1605
00:54:25,319 --> 00:54:26,700


1606
00:54:26,700 --> 00:54:28,319
언어에는 의사 소통 압력이

1607
00:54:28,319 --> 00:54:30,359
아마도 설명할 수 있는 모든 종류의 다른 것들이 있습니다

1608
00:54:30,359 --> 00:54:31,500
음 음 음

1609
00:54:31,500 --> 00:54:33,540
내 피치는 항상

1610
00:54:33,540 --> 00:54:36,359
일종의 폭을 위한 것 같아요  용어의 폭에서

1611
00:54:36,359 --> 00:54:39,119


1612
00:54:39,119 --> 00:54:41,460
언어를 형성할 수 있는 힘을 고려하고

1613
00:54:41,460 --> 00:54:43,680
모든 것을 어떤 형태의 타고난

1614
00:54:43,680 --> 00:54:45,359
제약이나 그와 비슷한 것에 넣을 필요가 없으며

1615
00:54:45,359 --> 00:54:46,680


1616
00:54:46,680 --> 00:54:48,420
그 많은 것들이 그들과 호환된다고 생각합니다

1617
00:54:48,420 --> 00:54:49,980
질병 프로그램은

1618
00:54:49,980 --> 00:54:51,960
이 프로그램의 중간 부분이

1619
00:54:51,960 --> 00:54:53,339
구문이 최소화되기를 원하기 때문입니다. 복잡하기를 원하지 않습니다. 더 이상

1620
00:54:53,339 --> 00:54:54,660


1621
00:54:54,660 --> 00:54:56,220
복잡해지기를 원하지 않습니다. 더 이상

1622
00:54:56,220 --> 00:54:57,960


1623
00:54:57,960 --> 00:54:59,460
복잡하지 않아야 합니다.  따라서

1624
00:54:59,460 --> 00:55:00,480


1625
00:55:00,480 --> 00:55:02,579
어떤 언어 모델에서든 설명해야 할 몇 가지 속성이 있습니다.

1626
00:55:02,579 --> 00:55:04,200
어,

1627
00:55:04,200 --> 00:55:05,400
사람의 특징 설정에 대한 올바른 예를 들어 보겠습니다.

1628
00:55:05,400 --> 00:55:06,599


1629
00:55:06,599 --> 00:55:08,880
이러한 사람의 특징은 매우

1630
00:55:08,880 --> 00:55:10,440
사소한 다른 일반화를

1631
00:55:10,440 --> 00:55:12,480
보여줍니다.

1632
00:55:12,480 --> 00:55:14,220
도메인 일반 학습 메커니즘을 통해 설명되므로

1633
00:55:14,220 --> 00:55:16,020
여기 Queen Mary에서 Daniel Harper의 작업이 있습니다.

1634
00:55:16,020 --> 00:55:17,700
예를 들어

1635
00:55:17,700 --> 00:55:19,740
사람의 형태학적 구성은

1636
00:55:19,740 --> 00:55:21,720
숫자와의 상호 작용 공간과의 연결입니다.

1637
00:55:21,720 --> 00:55:24,059


1638
00:55:24,059 --> 00:55:26,040


1639
00:55:26,040 --> 00:55:27,240


1640
00:55:27,240 --> 00:55:28,619
언어 지식에 대한 강력한 후보가 되십시오. 우리가 언어 지식으로 의미하는 바는 맞지만

1641
00:55:28,619 --> 00:55:30,359


1642
00:55:30,359 --> 00:55:32,280
다른 한편으로는 사례 및

1643
00:55:32,280 --> 00:55:34,319
동의 및 머리 움직임과 같은 것이 있으며 이것들은

1644
00:55:34,319 --> 00:55:36,420
모두 구조적 현상이지만

1645
00:55:36,420 --> 00:55:39,319
순전히

1646
00:55:39,319 --> 00:55:42,420
의미 기반 설명에 저항하는 것 같습니다.

1647
00:55:42,420 --> 00:55:44,339
이론적인 언어학 맞습니다.

1648
00:55:44,339 --> 00:55:45,839
구문이 구조화된 의미를

1649
00:55:45,839 --> 00:55:47,520
구축하는 계산 엔진에 불과하고 그것이 목표인

1650
00:55:47,520 --> 00:55:49,440


1651
00:55:49,440 --> 00:55:51,240
미니멀리스트 프로그램이라면 좋을 것입니다.

1652
00:55:51,240 --> 00:55:52,800


1653
00:55:52,800 --> 00:55:54,720


1654
00:55:54,720 --> 00:55:57,240


1655
00:55:57,240 --> 00:55:59,040
프로그램이

1656
00:55:59,040 --> 00:56:01,260
언어가 완벽하다는 것과 같은

1657
00:56:01,260 --> 00:56:03,119
프로그램은 우리가 발견한 것이 아니오

1658
00:56:03,119 --> 00:56:05,160
분명히 괜찮지 않다는 것입니다 아니오 언어학자는

1659
00:56:05,160 --> 00:56:07,680
실제로 그것을 믿지 않으므로

1660
00:56:07,680 --> 00:56:09,900
구문이 이와 같으면 좋을 것입니다. 하지만

1661
00:56:09,900 --> 00:56:11,280
프로그램이 완벽함을 찾는 것이라는 것을 알고 있다고 생각합니다.

1662
00:56:11,280 --> 00:56:13,740
항상 그런 것은 아니므로

1663
00:56:13,740 --> 00:56:15,839
일치와 머리 움직임은

1664
00:56:15,839 --> 00:56:17,640
음운 현상에 대한 형태론적입니다.

1665
00:56:17,640 --> 00:56:19,319


1666
00:56:19,319 --> 00:56:20,700
성능

1667
00:56:20,700 --> 00:56:22,440
시스템이라고 하는 성능 시스템의 속성이므로

1668
00:56:22,440 --> 00:56:23,760
미니멀리스트 프로그램 자체는 언어 언어를 알고 있다고 말하는

1669
00:56:23,760 --> 00:56:24,839
많은 내용과 실제로 호환됩니다.  의사

1670
00:56:24,839 --> 00:56:26,880


1671
00:56:26,880 --> 00:56:28,500


1672
00:56:28,500 --> 00:56:31,200


1673
00:56:31,200 --> 00:56:32,700
소통 효율성을 위해 완벽하고 최적화될 수 있는 언어의 측면이 있습니다.

1674
00:56:32,700 --> 00:56:35,520
전적으로 의심의 여지가 없지만

1675
00:56:35,520 --> 00:56:38,160
효율성의 위치는

1676
00:56:38,160 --> 00:56:39,960
구문 자체에 있습니까 아니면 일종의

1677
00:56:39,960 --> 00:56:42,000
추가 언어 시스템입니까

1678
00:56:42,000 --> 00:56:43,680
?  그것은 감각

1679
00:56:43,680 --> 00:56:45,540
운동에서 그것은 연설에 있습니까

1680
00:56:45,540 --> 00:56:47,640
음 아마도 연설과 음운론

1681
00:56:47,640 --> 00:56:50,400
아마 당신은 알 것입니다. 누가 알겠습니까? 하지만 저는

1682
00:56:50,400 --> 00:56:52,740
이러한 많은 것들이 훨씬 더 많은 것을 요구한다고 생각합니다

1683
00:56:52,740 --> 00:56:56,059


1684
00:56:56,059 --> 00:56:57,839
구조

1685
00:56:57,839 --> 00:56:59,700
의존 구성 주제와 같은 구식 개념에 대한 진지한 고려 당신이 가지고 있는 것

1686
00:56:59,700 --> 00:57:01,140


1687
00:57:01,140 --> 00:57:03,720
문헌 어딘가에서 찾을 수 있는 것과 같은 것입니다. 하지만

1688
00:57:03,720 --> 00:57:06,720
음

1689
00:57:06,720 --> 00:57:08,640
한정사, 확장

1690
00:57:08,640 --> 00:57:09,900
투영법,

1691
00:57:09,900 --> 00:57:12,059
부사 계층과 같은 부사,

1692
00:57:12,059 --> 00:57:13,920
미니멀리스트 프로그램의 이러한 모든 것들은

1693
00:57:13,920 --> 00:57:16,619


1694
00:57:16,619 --> 00:57:18,180
실제로 외부에 있을 수 있는 추가 언어적 권리일 수 있습니다.  구문

1695
00:57:18,180 --> 00:57:20,579
및 쿼리는 그 자체로 도메인의 종류인 의미론적 어 개념 시스템의 매우 기묘한 속성입니다.

1696
00:57:20,579 --> 00:57:23,099


1697
00:57:23,099 --> 00:57:24,559


1698
00:57:24,559 --> 00:57:27,420
고대 기본 인식에서 일반적으로 이상한 잔재물입니다. 우리가

1699
00:57:27,420 --> 00:57:29,220


1700
00:57:29,220 --> 00:57:31,260
이벤트를 전달하는 방식의 특징 우리가 전달하는 방식

1701
00:57:31,260 --> 00:57:32,520
에이전트 및 환자와 같은 것은

1702
00:57:32,520 --> 00:57:33,960
확실히 그렇지 않습니다.  그것은 인간에게 특정한 것이 아닙니다.

1703
00:57:33,960 --> 00:57:35,520


1704
00:57:35,520 --> 00:57:37,260
하지만 구문이

1705
00:57:37,260 --> 00:57:39,059
이러한 시스템에 지침을 제공하는 방식을 알고 있으므로

1706
00:57:39,059 --> 00:57:42,480


1707
00:57:42,480 --> 00:57:43,800
생성 언어학자도

1708
00:57:43,800 --> 00:57:45,839
언어 생성에 대한 다른 이론을 가지고 있다는 것을 알고 있습니다.

1709
00:57:45,839 --> 00:57:47,040


1710
00:57:47,040 --> 00:57:49,380
우리가 기본형을 저장하는지 여부에 따라 언어 생성에 대해 이야기하겠습니다.

1711
00:57:49,380 --> 00:57:51,240
또는 우리가

1712
00:57:51,240 --> 00:57:52,619
구문과 문장을 만드는 것과 똑같은 방식으로 단어를

1713
00:57:52,619 --> 00:57:53,819
만드는지 여부를 알기 때문에

1714
00:57:53,819 --> 00:57:55,440
구성 문법과 일종의

1715
00:57:55,440 --> 00:57:57,180
생성 문법을 구분하고 수축을

1716
00:57:57,180 --> 00:57:58,680
암기하는 데 가중치를 두는 반면 단지

1717
00:57:58,680 --> 00:58:00,119


1718
00:58:00,119 --> 00:58:01,559
바닥에서 위로

1719
00:58:01,559 --> 00:58:04,440
오른쪽으로 그래서 구문 구조를 생성하는 일부

1720
00:58:04,440 --> 00:58:06,480
생성 영감 모델 메커니즘에서 단어 수준 위 또는 아래에 적용되는

1721
00:58:06,480 --> 00:58:08,460


1722
00:58:08,460 --> 00:58:10,200
프로세스 사이에 구분이 없음을 알고 있습니다.

1723
00:58:10,200 --> 00:58:12,200


1724
00:58:12,200 --> 00:58:14,640
구문

1725
00:58:14,640 --> 00:58:16,500
및 형식을 의미하는 포인터가 모두

1726
00:58:16,500 --> 00:58:18,660
하나의 Atomic  어휘 액세스의 각 단계 표현은 서로

1727
00:58:18,660 --> 00:58:20,099


1728
00:58:20,099 --> 00:58:21,960
다른 종류의 데이터 구조 사이의 전환입니다.

1729
00:58:21,960 --> 00:58:23,760


1730
00:58:23,760 --> 00:58:25,740
형식과 구문이 있다는 것을 의미합니다. 이 세 가지

1731
00:58:25,740 --> 00:58:27,480
기능은 함께 혼합되며

1732
00:58:27,480 --> 00:58:28,920
항상 중첩되지는 않습니다. 서로 다른

1733
00:58:28,920 --> 00:58:31,440
언어가 서로 다른 방식으로 인식

1734
00:58:31,440 --> 00:58:34,800
하므로  이상하게도

1735
00:58:34,800 --> 00:58:36,839
단어의 기본 정의는 이 이상한

1736
00:58:36,839 --> 00:58:39,720
다중 시스템 정의이며

1737
00:58:39,720 --> 00:58:41,040
많은 다른 인지

1738
00:58:41,040 --> 00:58:42,900
시스템이 모든 전기 항목의 기초를 풍부하게 하는 곳입니다.

1739
00:58:42,900 --> 00:58:44,940


1740
00:58:44,940 --> 00:58:46,680


1741
00:58:46,680 --> 00:58:48,299


1742
00:58:48,299 --> 00:58:50,099
언어 이론이

1743
00:58:50,099 --> 00:58:52,200
맞거나 적어도 llms가 하는 일이 무엇인지

1744
00:58:52,200 --> 00:58:53,220


1745
00:58:53,220 --> 00:58:55,859
추측합니다. 그래서 제가 추측하는 것이 무엇인지 추측합니다.

1746
00:58:55,859 --> 00:58:58,799


1747
00:58:58,799 --> 00:59:01,559
올바른 단어에 대한 정의가 무엇이며 llms가 단어 Hood에 대한 통찰력을 실제로 제공할 수 있는 것은 무엇인지 물어볼 것입니다.

1748
00:59:01,559 --> 00:59:03,780


1749
00:59:03,780 --> 00:59:04,680


1750
00:59:04,680 --> 00:59:06,720
단어가 무엇인지에 대한 목적지가 없다면

1751
00:59:06,720 --> 00:59:07,920
우리가 단어의 의미를 알리기 위해

1752
00:59:07,920 --> 00:59:10,319
적어도 LMS 또는 인공 시스템을 사용해야 하거나

1753
00:59:10,319 --> 00:59:12,780


1754
00:59:12,780 --> 00:59:14,400
더 이상 필요하지 않을 수 있는 것처럼 정말 문제에 처한 것입니다.

1755
00:59:14,400 --> 00:59:16,859
당신이 어떻게 생각하는지 모르겠어 내가 아니야

1756
00:59:16,859 --> 00:59:18,599


1757
00:59:18,599 --> 00:59:20,339


1758
00:59:20,339 --> 00:59:24,059
무슨 말인지 잘 모르겠어 내 말은 음 음 난 단어가 왜

1759
00:59:24,059 --> 00:59:25,740
중요해

1760
00:59:25,740 --> 00:59:27,359


1761
00:59:27,359 --> 00:59:29,819
용어 단어가 맞습니다 제 말은

1762
00:59:29,819 --> 00:59:31,920
당신이 알고 있는 보조정리나

1763
00:59:31,920 --> 00:59:34,440
단어 회사, 또는

1764
00:59:34,440 --> 00:59:35,819
기존의 선택처럼 느껴지는 것을 사용할 수 있다는 것을 의미합니다.

1765
00:59:35,819 --> 00:59:38,040
무엇이

1766
00:59:38,040 --> 00:59:39,180


1767
00:59:39,180 --> 00:59:41,880
문제인지 잘 모르겠습니다.  동의합니다.

1768
00:59:41,880 --> 00:59:43,920
단어는 관습화입니다.

1769
00:59:43,920 --> 00:59:45,599
아이콘은 철자법에 의해 종종 편향되는 직관적인 개념이 아닙니다.

1770
00:59:45,599 --> 00:59:47,819


1771
00:59:47,819 --> 00:59:50,760
우리가 공백을 올바르게 배치하는 방식이므로

1772
00:59:50,760 --> 00:59:52,559


1773
00:59:52,559 --> 00:59:54,240
직관적인 의미에서 단어가

1774
00:59:54,240 --> 00:59:56,339
실제로 과학적이지 않다는 비판에 동의합니다.  구성하지만

1775
00:59:56,339 --> 00:59:58,260
제 질문을 바꿔서 설명하겠습니다.

1776
00:59:58,260 --> 01:00:00,359


1777
01:00:00,359 --> 01:00:02,160
단어의 직관적인 개념을 어떻게 하면

1778
01:00:02,160 --> 01:00:03,660


1779
01:00:03,660 --> 01:00:04,740
과학적으로 수용 가능하거나

1780
01:00:04,740 --> 01:00:06,540
심리적으로 타당하다고 알고 있는

1781
01:00:06,540 --> 01:00:08,280
어떤 것으로 분해할 수 있는지 알 수 있습니다.

1782
01:00:08,280 --> 01:00:10,559


1783
01:00:10,559 --> 01:00:12,599
독특한 특징 uh 형태학적

1784
01:00:12,599 --> 01:00:15,180
범주 개념적 뿌리가

1785
01:00:15,180 --> 01:00:17,579
범주적 특징과 병합되는 중 당신은

1786
01:00:17,579 --> 01:00:20,700
당신이 알고 있는 개념을 얻었고

1787
01:00:20,700 --> 01:00:22,440
명사 또는

1788
01:00:22,440 --> 01:00:24,119
사건을 얻기 위해 명사 또는 범주로 만들었다는 것을 알고 있습니다. 이러한 다른 모델은

1789
01:00:24,119 --> 01:00:26,220
다른 예측을 합니다.

1790
01:00:26,220 --> 01:00:28,859
그 일반적인 아이디어는

1791
01:00:28,859 --> 01:00:30,839
대규모 언어 모델에 적합할 것

1792
01:00:30,839 --> 01:00:32,880
같습니다. 예를 들어 음성 범주의

1793
01:00:32,880 --> 01:00:34,380
일부와 같은 것이 있어야 한다고

1794
01:00:34,380 --> 01:00:36,660


1795
01:00:36,660 --> 01:00:38,819
생각합니다.

1796
01:00:38,819 --> 01:00:42,900


1797
01:00:42,900 --> 01:00:45,240


1798
01:00:45,240 --> 01:00:48,119
그들이 지금까지 본 언어를 기반으로

1799
01:00:48,119 --> 01:00:50,520
GPT가 명사와 동사를

1800
01:00:50,520 --> 01:00:53,400
올바른 위치에 배치하고 그렇게 하려면

1801
01:00:53,400 --> 01:00:55,319
명사와 동사의 표현이 필요하고

1802
01:00:55,319 --> 01:00:57,000


1803
01:00:57,000 --> 01:01:00,839
어  uh

1804
01:01:00,839 --> 01:01:02,579
다른 단어의 문자열에서 자신의 위치를 ​​파악하고 다음에

1805
01:01:02,579 --> 01:01:04,680
명사 또는 동사가 있을 가능성이 있는지 파악합니다.

1806
01:01:04,680 --> 01:01:05,400


1807
01:01:05,400 --> 01:01:07,559
그래서 저는 그

1808
01:01:07,559 --> 01:01:09,319
수준에서 단어의 이러한 종류의 속성이

1809
01:01:09,319 --> 01:01:12,480
옳을 가능성이 매우 높다고 생각합니다.

1810
01:01:12,480 --> 01:01:15,780
또한 음 음 이러한 모델의 내부 표현

1811
01:01:15,780 --> 01:01:17,640
에서 발견될 가능성이 매우 높은 것들이 있습니다.

1812
01:01:17,640 --> 01:01:19,619


1813
01:01:19,619 --> 01:01:21,420
어떻게 다른 방법이 있을 수 있는지 모르겠습니다. 음,

1814
01:01:21,420 --> 01:01:24,180


1815
01:01:24,180 --> 01:01:26,339
하지만 제가 아는 한

1816
01:01:26,339 --> 01:01:29,579
그것은  그게 어디가 아니라 uh uh 그게

1817
01:01:29,579 --> 01:01:31,500
주요 논쟁이나

1818
01:01:31,500 --> 01:01:35,220
의견 불일치가 있는 곳이 아니에요 제 생각에는

1819
01:01:35,220 --> 01:01:38,280
음 어 예 언어의 모든 이론은

1820
01:01:38,280 --> 01:01:40,079


1821
01:01:40,079 --> 01:01:41,400


1822
01:01:41,400 --> 01:01:43,140
다른 장소에 나타날 수 있는 다른 종류의 단어가 있다고 말해야 한다고 생각합니다

1823
01:01:43,140 --> 01:01:44,339


1824
01:01:44,339 --> 01:01:45,240
음 그래 알았어 그럼

1825
01:01:45,240 --> 01:01:47,160
당신이 의사 소통에 대해 언급 한 것을 알고있는 문제는 어떻습니까

1826
01:01:47,160 --> 01:01:49,020


1827
01:01:49,020 --> 01:01:51,299
음 그래서

1828
01:01:51,299 --> 01:01:53,099
Trump가 언어와 같은 것을 말할 때 당신이 완전히 옳다는 것을 알

1829
01:01:53,099 --> 01:01:54,960
거나 언어가

1830
01:01:54,960 --> 01:01:57,780
진화하지 않았다는 것을 알고 있습니다.

1831
01:01:57,780 --> 01:01:58,799
건방진 그는 실제로

1832
01:01:58,799 --> 01:02:00,180
그가 매우 특정한 의미에서 의미한다는 것을 의미하지는 않습니다.

1833
01:02:00,180 --> 01:02:01,619


1834
01:02:01,619 --> 01:02:03,540
하지만 우리가 언어가

1835
01:02:03,540 --> 01:02:05,640
사고 시스템이라고 말할 때 우리가 의미하는 것은

1836
01:02:05,640 --> 01:02:06,780
음 우리는 그것을 건축적 주장을 얻으려고 노력하고 있습니다.

1837
01:02:06,780 --> 01:02:08,339


1838
01:02:08,339 --> 01:02:09,599
미니멀리스트

1839
01:02:09,599 --> 01:02:10,380
프로그램의

1840
01:02:10,380 --> 01:02:12,240
아키텍처 구문 파생과

1841
01:02:12,240 --> 01:02:13,740
개념 시스템은 문자 그대로

1842
01:02:13,740 --> 01:02:15,960
다른 시스템입니다. 개념

1843
01:02:15,960 --> 01:02:18,180
시스템은 구문에서 물건을 가져온 다음

1844
01:02:18,180 --> 01:02:19,799
자체 비즈니스로 가져오고 CI

1845
01:02:19,799 --> 01:02:21,900
시스템에는 고유한 규칙

1846
01:02:21,900 --> 01:02:23,700
과 원칙이 있습니다. 이것이 내가 생각한 이유입니다.

1847
01:02:23,700 --> 01:02:25,319
언어에서 둘 다 유사한

1848
01:02:25,319 --> 01:02:27,000
상징적 구성 시스템이지만

1849
01:02:27,000 --> 01:02:29,160
다른 방식으로 생각의 하위 집합 만

1850
01:02:29,160 --> 01:02:32,339
CI 인터페이스 시스템이라고 적절하게 불립니다.

1851
01:02:32,339 --> 01:02:34,619
CI 시스템은 정의에 따라 구문에서 지침에

1852
01:02:34,619 --> 01:02:36,480


1853
01:02:36,480 --> 01:02:38,819
액세스하고 읽을 수없는 인간이 가진 개념적 시스템을 알고 있기 때문입니다.

1854
01:02:38,819 --> 01:02:40,859
그리고 우리는

1855
01:02:40,859 --> 01:02:42,540
그것들이 무엇인지 완전히 알지 못합니다. 그것들은 문법적 참조 및 명확성

1856
01:02:42,540 --> 01:02:43,920
에서 사건과 관련이 있는 것 같습니다.

1857
01:02:43,920 --> 01:02:45,540


1858
01:02:45,540 --> 01:02:47,099
그것들은

1859
01:02:47,099 --> 01:02:48,359
여러분이 알고 있는 언어가 개념적으로 관심을 갖는 주요 범주인 것 같습니다.

1860
01:02:48,359 --> 01:02:49,440


1861
01:02:49,440 --> 01:02:50,940


1862
01:02:50,940 --> 01:02:52,859
가설은 맞지만

1863
01:02:52,859 --> 01:02:53,940
우리가 아는 것은 그들이

1864
01:02:53,940 --> 01:02:56,220
색상을 그다지 많이 사용하지 않는 것 같다는 것입니다.

1865
01:02:56,220 --> 01:02:58,440
또는 음 그래서

1866
01:02:58,440 --> 01:03:00,240


1867
01:03:00,240 --> 01:03:01,140


1868
01:03:01,140 --> 01:03:03,900
어떤

1869
01:03:03,900 --> 01:03:06,359
언어도

1870
01:03:06,359 --> 01:03:07,980
형태학적으로 당신이 알고 있는 색상의 음영을 표시하지 않습니다.  문제에 대한 어느 정도의 걱정

1871
01:03:07,980 --> 01:03:10,079
이나 우려를 표시하지만 우리는

1872
01:03:10,079 --> 01:03:11,960


1873
01:03:11,960 --> 01:03:13,500


1874
01:03:13,500 --> 01:03:15,599
증거와 같은 인식론적 개념을 사용하므로

1875
01:03:15,599 --> 01:03:17,640
하나를 잘 알 수 있습니다.

1876
01:03:17,640 --> 01:03:19,500


1877
01:03:19,500 --> 01:03:21,720


1878
01:03:21,720 --> 01:03:23,400
사고 언어의 어떤 측면이

1879
01:03:23,400 --> 01:03:25,619
밀접하게 연결되어 있고 사고의 어떤 측면이

1880
01:03:25,619 --> 01:03:27,900
연결되어 있지 않은지 중서부

1881
01:03:27,900 --> 01:03:29,280
프로그램을 통해 우리는 그것을 아주 깔끔하게 조각할 수 있습니다.

1882
01:03:29,280 --> 01:03:31,680
이것은 Chomski가 언어 사고를 말할 때보다 훨씬

1883
01:03:31,680 --> 01:03:33,180
더 미묘한 프레임워크입니다.

1884
01:03:33,180 --> 01:03:34,559


1885
01:03:34,559 --> 01:03:36,960
다시 말하지만 그는 그가 의미하지 않을 수도 있지만

1886
01:03:36,960 --> 01:03:38,339


1887
01:03:38,339 --> 01:03:40,319
그의 이론의 실제 아키텍처가 말하는 것은 아닙니다.

1888
01:03:40,319 --> 01:03:42,480


1889
01:03:42,480 --> 01:03:43,920


1890
01:03:43,920 --> 01:03:46,559


1891
01:03:46,559 --> 01:03:48,839
실제 이론을 보면 학부 청중을 끌어들이는 데 매우 유용하고 흥미로운 수사학 장치입니다.

1892
01:03:48,839 --> 01:03:50,700
The Mentalist 프로그램에서 나오는

1893
01:03:50,700 --> 01:03:52,380
언어가 생각과 같다고 믿는 사람은 아무도 없습니다.

1894
01:03:52,380 --> 01:03:53,760
언어 시스템은 다양한 개념 시스템에

1895
01:03:53,760 --> 01:03:55,920
액세스하고 재구성하고 조작하기 위해 최선을 다하는 것 같지만

1896
01:03:55,920 --> 01:03:57,599


1897
01:03:57,599 --> 01:03:59,220
한계가 있습니다.

1898
01:03:59,220 --> 01:04:01,440


1899
01:04:01,440 --> 01:04:02,819


1900
01:04:02,819 --> 01:04:05,160
구문 엔진과 관련하여

1901
01:04:05,160 --> 01:04:07,260
어떤 엔진이

1902
01:04:07,260 --> 01:04:09,480
음이 아니므로 이런 종류의

1903
01:04:09,480 --> 01:04:11,579
개념을 어휘화하면

1904
01:04:11,579 --> 01:04:14,099
어떤 방식으로든 개념을 변경하는 것처럼 보이고 이는 개념

1905
01:04:14,099 --> 01:04:16,200


1906
01:04:16,200 --> 01:04:17,819
에 없는 요소로 스며들게 한다는 생각으로 되돌아갑니다.  개념 자체를

1907
01:04:17,819 --> 01:04:19,319
강의하는 경우 갑자기 개념을

1908
01:04:19,319 --> 01:04:21,240
조금 변형하고

1909
01:04:21,240 --> 01:04:22,680


1910
01:04:22,680 --> 01:04:24,299
그 위에 다른 것을 뿌립니다.

1911
01:04:24,299 --> 01:04:26,700
나노 유형에 따라 달라지는 것처럼 보이지만 이것들은

1912
01:04:26,700 --> 01:04:29,160
모두 기하학 내에서 매우 명확한 건축적 주장과 같습니다.

1913
01:04:29,160 --> 01:04:31,619


1914
01:04:31,619 --> 01:04:34,980
매우 명확한 경험적 예측을 하는 문법입니다.

1915
01:04:34,980 --> 01:04:36,420
즉, 제가 말하는 것은

1916
01:04:36,420 --> 01:04:37,140


1917
01:04:37,140 --> 01:04:38,940


1918
01:04:38,940 --> 01:04:42,119
많은 작업에서 여러분이 알고 있는 모든 신경심리학 연구입니다.

1919
01:04:42,119 --> 01:04:43,980


1920
01:04:43,980 --> 01:04:45,420


1921
01:04:45,420 --> 01:04:47,819
뇌에서 언어가 손상되면

1922
01:04:47,819 --> 01:04:50,040


1923
01:04:50,040 --> 01:04:51,960
이러한 시스템에 영향을 미치는 특정 방식이나 방식을 잃게 되지만

1924
01:04:51,960 --> 01:04:54,299
Gen gram Enterprise 내에서 실제 예측은 없지만

1925
01:04:54,299 --> 01:04:56,040
이러한 비언어적

1926
01:04:56,040 --> 01:04:57,540
시스템은 손상되거나

1927
01:04:57,540 --> 01:04:59,579
핵심 언어 시스템이 갑자기 종료되어야 합니다.

1928
01:04:59,579 --> 01:05:01,200


1929
01:05:01,200 --> 01:05:02,760
사실

1930
01:05:02,760 --> 01:05:05,099


1931
01:05:05,099 --> 01:05:07,859
통사

1932
01:05:07,859 --> 01:05:09,900
체계와 비언어적 체계 사이의 원칙적 분리를 강조하는 것이 있다면 음은 타협됩니다.

1933
01:05:09,900 --> 01:05:11,880
그래서 제 생각에는

1934
01:05:11,880 --> 01:05:14,339
언어와 의사소통에서 나오는 많은 예측이 어,

1935
01:05:14,339 --> 01:05:16,020
문학이

1936
01:05:16,020 --> 01:05:19,380
요점을 놓치고 있는 것 같습니다.  건축적 주장

1937
01:05:19,380 --> 01:05:21,540
음 제가 그냥 줄 수 있어요 아니면 Daniel이

1938
01:05:21,540 --> 01:05:24,180
가고 싶나요 어 거기에 약간의

1939
01:05:24,180 --> 01:05:25,799
배경을 알려주세요 그래서 이 서류가 있습니다

1940
01:05:25,799 --> 01:05:26,760


1941
01:05:26,760 --> 01:05:30,119
음 어 EV 페데렌코와

1942
01:05:30,119 --> 01:05:32,880
로즈마리 발리가 음 어 어 부분적으로 음

1943
01:05:32,880 --> 01:05:34,500
검토하고 있습니다

1944
01:05:34,500 --> 01:05:37,799
실어증 환자들

1945
01:05:37,799 --> 01:05:40,200
그래서 언어 능력이 손상된 사람들 음 음

1946
01:05:40,200 --> 01:05:41,880


1947
01:05:41,880 --> 01:05:43,140


1948
01:05:43,140 --> 01:05:45,000
기본적으로 언어 능력이 손상된 상태에서도

1949
01:05:45,000 --> 01:05:46,980


1950
01:05:46,980 --> 01:05:49,020
여전히

1951
01:05:49,020 --> 01:05:50,579
음 어 보존된 추리

1952
01:05:50,579 --> 01:05:52,859
능력을 가질 수 있음을 보여줍니다.

1953
01:05:52,859 --> 01:05:55,020


1954
01:05:55,020 --> 01:05:58,140
at at reasoning

1955
01:05:58,140 --> 01:06:00,780
음 음 음 음

1956
01:06:00,780 --> 01:06:02,640
온전한 언어 능력이 없을 수도 있습니다

1957
01:06:02,640 --> 01:06:04,079
음 그리고 그런 종류

1958
01:06:04,079 --> 01:06:06,119
의 환자 작업을 보완하는 것은

1959
01:06:06,119 --> 01:06:08,520
ebb의 연구실에서

1960
01:06:08,520 --> 01:06:11,220
음 음 어 언어에 관심을 갖는 뇌의 부분을 보여주는 작업입니다

1961
01:06:11,220 --> 01:06:13,859


1962
01:06:13,859 --> 01:06:14,640
음

1963
01:06:14,640 --> 01:06:16,319
음  같은 종류의 언어를 가진 영역이라도

1964
01:06:16,319 --> 01:06:17,700
다른 영역을 신경 쓰는 뇌의 부분과 분리 가능합니다.

1965
01:06:17,700 --> 01:06:19,619


1966
01:06:19,619 --> 01:06:21,599
그래서 음악과 수학 같은 것들은

1967
01:06:21,599 --> 01:06:23,819


1968
01:06:23,819 --> 01:06:25,859
음 어 언어 영역에서 발생하지 않는 경향이 있습니다

1969
01:06:25,859 --> 01:06:27,240


1970
01:06:27,240 --> 01:06:29,400
음 그래서 EV와 다른 사람들은

1971
01:06:29,400 --> 01:06:30,539


1972
01:06:30,539 --> 01:06:33,539
음 이것은 기본적으로 촘스키에 반하는 증거라고

1973
01:06:33,539 --> 01:06:36,660
주장하고

1974
01:06:36,660 --> 01:06:38,579
언어가 없을 때 일어날

1975
01:06:38,579 --> 01:06:40,440
수 있는 생각이 있기 때문에 언어가 올바른 생각을 위한 매체라고 주장하고

1976
01:06:40,440 --> 01:06:42,420


1977
01:06:42,420 --> 01:06:44,160
언어에 관심을 갖는 뇌 영역은

1978
01:06:44,160 --> 01:06:46,020
그들이 생각하는 뇌 영역이 아닌 것 같습니다.

1979
01:06:46,020 --> 01:06:48,359
생각에 대한 관심

1980
01:06:48,359 --> 01:06:50,099
음 내 생각에 Elliot 당신은

1981
01:06:50,099 --> 01:06:51,780
사람들이 실제로 그것을 믿지 않는다고 말하는 것 같아요 음 어

1982
01:06:51,780 --> 01:06:53,039


1983
01:06:53,039 --> 01:06:56,220
그들은 그

1984
01:06:56,220 --> 01:06:57,720
구별을 믿지 않습니다 내 말은

1985
01:06:57,720 --> 01:06:59,880
음 음

1986
01:06:59,880 --> 01:07:01,740
아니 그게 다이고 또한 많은

1987
01:07:01,740 --> 01:07:03,480


1988
01:07:03,480 --> 01:07:04,799
이러한 주장 내에서도 자기 모순과 같은 것입니다. 따라서 귀하의

1989
01:07:04,799 --> 01:07:06,839
논문에서 때때로 Chomsky는

1990
01:07:06,839 --> 01:07:08,220
언어가 사고 시스템이라고 생각한다고 말하지만

1991
01:07:08,220 --> 01:07:10,319
몇 페이지 후에

1992
01:07:10,319 --> 01:07:12,480
Chomsky는 또한 구문이 다른 것과

1993
01:07:12,480 --> 01:07:13,799
완전히 분리된 시스템이라고 믿는다고 말할 것입니다.

1994
01:07:13,799 --> 01:07:15,480
맞아요 당신은 구문의 자율성

1995
01:07:15,480 --> 01:07:18,900
등등 그래서 이것은 Chomsky 것입니다 그것은

1996
01:07:18,900 --> 01:07:20,460
내 모순이 아닙니다 제 말은 그가 그 두

1997
01:07:20,460 --> 01:07:22,140
가지를 모두

1998
01:07:22,140 --> 01:07:24,780
정확히 말했다는 것을 의미합니다 그래서

1999
01:07:24,780 --> 01:07:26,280
당신은 스스로에게 물어보고 싶을 것입니다.

2000
01:07:26,280 --> 01:07:28,319


2001
01:07:28,319 --> 01:07:30,420
아키텍처에서

2002
01:07:30,420 --> 01:07:32,520
바로

2003
01:07:32,520 --> 01:07:34,619
언어가 사고 시스템이라고 말하는 것은 아무

2004
01:07:34,619 --> 01:07:35,579
의미가 없습니다.

2005
01:07:35,579 --> 01:07:36,780
매우 모호한

2006
01:07:36,780 --> 01:07:38,640
진술 일뿐입니다.

2007
01:07:38,640 --> 01:07:41,280
언어가 정확히 Thor에 기여하는 방법과

2008
01:07:41,280 --> 01:07:43,500
기여하지 않는 방법은 무엇입니까

2009
01:07:43,500 --> 01:07:45,839
어 예 내 말은  내 생각에 그의 주장은

2010
01:07:45,839 --> 01:07:48,420
주로 진화적이거나 옳다고 생각합니다.

2011
01:07:48,420 --> 01:07:51,180
어 이것이

2012
01:07:51,180 --> 01:07:52,859
제가 생각하는 시스템의 기원입니다

2013
01:07:52,859 --> 01:07:55,559


2014
01:07:55,559 --> 01:07:57,599
음 어 종류의 환자 및

2015
01:07:57,599 --> 01:07:59,520
신경 영상 데이터 음 음 음 음 음 음 음 음.

2016
01:07:59,520 --> 01:08:00,660


2017
01:08:00,660 --> 01:08:04,079
그가 그렇게 생각하지 않는다면 그는

2018
01:08:04,079 --> 01:08:07,079
그것을 말하지 말아야 합니다. 그렇지 않으면 사람들이

2019
01:08:07,079 --> 01:08:08,579
그가 말한 것에 응답할 것입니다. 저는

2020
01:08:08,579 --> 01:08:11,280


2021
01:08:11,280 --> 01:08:13,619
언어가 생각

2022
01:08:13,619 --> 01:08:15,420
의 일부 측면을 규제하는 일종의 사고 시스템이라는 주장이 있기 때문에 아니오라고 생각합니다.

2023
01:08:15,420 --> 01:08:16,920


2024
01:08:16,920 --> 01:08:19,020
분명히 인간에게 고유한 생각의 일부 측면이지만

2025
01:08:19,020 --> 01:08:21,000
본질적으로 또는 인과적으로 연결되어 있지는 않습니다.

2026
01:08:21,000 --> 01:08:22,920
시스템의 아키텍처는

2027
01:08:22,920 --> 01:08:24,600


2028
01:08:24,600 --> 01:08:26,399


2029
01:08:26,399 --> 01:08:28,979
아키텍처에서 수사적으로 이벤트를 수행할 수 있는 일반화의 종류와 매우 다릅니다.

2030
01:08:28,979 --> 01:08:30,960


2031
01:08:30,960 --> 01:08:32,698


2032
01:08:32,698 --> 01:08:33,960
당신이 방금 체스 게임을 언급한 것처럼 복잡한 추론에 결함이 없는 환자들

2033
01:08:33,960 --> 01:08:35,759
우리는 실제로

2034
01:08:35,759 --> 01:08:37,380
당신이 알고 있는 일종의

2035
01:08:37,380 --> 01:08:39,179
비사전주의적 기하학 구문 프레임워크에서 이것을 기대할 것입니다.

2036
01:08:39,179 --> 01:08:41,160


2037
01:08:41,160 --> 01:08:43,799


2038
01:08:43,799 --> 01:08:45,600


2039
01:08:45,600 --> 01:08:46,979
언어를 외부화하고 이 모든 것들은

2040
01:08:46,979 --> 01:08:48,540
별도의 기능과 별도의 시스템입니다.

2041
01:08:48,540 --> 01:08:51,000
구문의 자율성은

2042
01:08:51,000 --> 01:08:52,020


2043
01:08:52,020 --> 01:08:53,819
많은 사람들이 생각하는 것을 의미하지 않습니다. 의미

2044
01:08:53,819 --> 01:08:54,719


2045
01:08:54,719 --> 01:08:56,160


2046
01:08:56,160 --> 01:08:58,439
하지 않는 특정 구문 작업이 있음을 의미합니다. 특정

2047
01:08:58,439 --> 01:09:00,299
사항이 있습니다.  당신은

2048
01:09:00,299 --> 01:09:01,920
문법만 할 수 있고

2049
01:09:01,920 --> 01:09:03,420
의미론은 할 수 없는 구문으로 할 수 있습니다. 그래서 이것은 의미론이 정당하고 옳다는

2050
01:09:03,420 --> 01:09:05,279
당신이 알고 있는 uh

2051
01:09:05,279 --> 01:09:07,259
petrovsky의 이론과

2052
01:09:07,259 --> 01:09:10,319
어 허용

2053
01:09:10,319 --> 01:09:11,698
합성학자의 믿음 사이의 차이점으로 되돌아갑니다.

2054
01:09:11,698 --> 01:09:13,080


2055
01:09:13,080 --> 01:09:15,719
통사론적인 구문으로 할 수 있으므로

2056
01:09:15,719 --> 01:09:17,759
일종의 구조적 틀 내에서도 이혼이 있습니다.

2057
01:09:17,759 --> 01:09:20,100
따라서

2058
01:09:20,100 --> 01:09:22,439


2059
01:09:22,439 --> 01:09:24,359
신경심리학적 수준에서도 이혼을 발견하는 것은 그리 놀라운 일이 아닙니다.

2060
01:09:24,359 --> 01:09:25,979


2061
01:09:25,979 --> 01:09:28,319


2062
01:09:28,319 --> 01:09:30,960
언어의 예측은

2063
01:09:30,960 --> 01:09:34,140
생각의 진화론적인 생각입니다 그럼

2064
01:09:34,140 --> 01:09:36,960
그렇죠 어 그게 아니라면

2065
01:09:36,960 --> 01:09:39,060
생각이

2066
01:09:39,060 --> 01:09:41,160
언어에 의존한다는 것을 예측하지 못한다면 음

2067
01:09:41,160 --> 01:09:44,399
그럼 어 그 이론을 좋아하는 사람은 예측을 좀 해야 할 것 같아요

2068
01:09:44,399 --> 01:09:45,899


2069
01:09:45,899 --> 01:09:47,520


2070
01:09:47,520 --> 01:09:49,979
음 대략 어 당신은 그

2071
01:09:49,979 --> 01:09:51,660
이론이 실제로 무엇을 의미하는지 알 것입니다 제 말은

2072
01:09:51,660 --> 01:09:53,819
그런 종류의 예측이 종종 예측의 내용을

2073
01:09:53,819 --> 01:09:55,440
이해하는 데 정말로 필요하다고 생각한다는 것입니다

2074
01:09:55,440 --> 01:09:57,360


2075
01:09:57,360 --> 01:09:58,320
음 음 음

2076
01:09:58,320 --> 01:10:00,540
정말 죄송합니다 다니엘 당신의 손은

2077
01:10:00,540 --> 01:10:03,660
잠시 동안 uh no  모두 좋습니다

2078
01:10:03,660 --> 01:10:05,880
그냥 숨을 쉬고 싶고 음

2079
01:10:05,880 --> 01:10:08,040


2080
01:10:08,040 --> 01:10:12,660


2081
01:10:12,660 --> 01:10:15,480
누구에게나 uh 다른

2082
01:10:15,480 --> 01:10:17,520
질문을 할 수 있는 기회지만 와우

2083
01:10:17,520 --> 01:10:20,460
우리가 다룬 많은 주제에 대해 둘 다 감사합니다

2084
01:10:20,460 --> 01:10:21,780


2085
01:10:21,780 --> 01:10:22,500
um

2086
01:10:22,500 --> 01:10:25,199
우리는 마지막 몇 분 안에 가질 것입니다 uh

2087
01:10:25,199 --> 01:10:27,120
일종의  결론 및 다음 단계이지만 Dave는

2088
01:10:27,120 --> 01:10:29,820
질문을 하시거나

2089
01:10:29,820 --> 01:10:32,840
짧은 반성을 하시겠습니까? 오케이 아니요 음

2090
01:10:36,900 --> 01:10:38,880


2091
01:10:38,880 --> 01:10:39,960


2092
01:10:39,960 --> 01:10:42,239
채팅에 많은 댓글이 있으므로

2093
01:10:42,239 --> 01:10:45,120
두 사람 모두

2094
01:10:45,120 --> 01:10:47,460
자신의 시간에 읽어서 모두가 어디에

2095
01:10:47,460 --> 01:10:49,440
추가했는지 확인할 수 있기를 바랍니다.

2096
01:10:49,440 --> 01:10:52,860


2097
01:10:52,860 --> 01:10:57,179
2023년 5월 이후에는

2098
01:10:57,179 --> 01:11:00,960
언어학자 대규모 언어 모델

2099
01:11:00,960 --> 01:11:02,940
개발자와 사용자

2100
01:11:02,940 --> 01:11:05,159
인지과학자 여러분 각자가 생각하는

2101
01:11:05,159 --> 01:11:06,960
가장

2102
01:11:06,960 --> 01:11:08,159


2103
01:11:08,159 --> 01:11:10,080


2104
01:11:10,080 --> 01:11:13,260


2105
01:11:13,260 --> 01:11:14,699
유익한 경로가 무엇이라고 생각하십니까?  음 인지심리학과 같은 것을 진지하게

2106
01:11:14,699 --> 01:11:15,600
받아들이는 것입니다 아니 진지하게

2107
01:11:15,600 --> 01:11:17,460


2108
01:11:17,460 --> 01:11:18,659


2109
01:11:18,659 --> 01:11:21,000
최근에

2110
01:11:21,000 --> 01:11:24,060
알파 플러그인의 교회 EBT 늑대

2111
01:11:24,060 --> 01:11:25,739
채팅 gbt가

2112
01:11:25,739 --> 01:11:28,080
다른 종류의 모듈과 인터페이스할 수 있는 방식 음

2113
01:11:28,080 --> 01:11:30,179
합법적인  일종의

2114
01:11:30,179 --> 01:11:32,580
AGI 시스템은 인간이 가지고 있는

2115
01:11:32,580 --> 01:11:34,380


2116
01:11:34,380 --> 01:11:35,880
종류의 모듈에 심리적으로 의존할 필요는 없지만

2117
01:11:35,880 --> 01:11:37,500


2118
01:11:37,500 --> 01:11:38,820
그것으로부터 이익을 얻을 것이라고 생각하므로

2119
01:11:38,820 --> 01:11:41,040
대규모 언어 모델이

2120
01:11:41,040 --> 01:11:42,540
모든 것을 알 수 있다는 주장이 있습니다.  여러 가지가

2121
01:11:42,540 --> 01:11:43,860
맞습니다 모든 것 당신이

2122
01:11:43,860 --> 01:11:44,760
좋아하는 모든 것

2123
01:11:44,760 --> 01:11:46,620
음 하지만 저는 장기적으로

2124
01:11:46,620 --> 01:11:47,880
llms가

2125
01:11:47,880 --> 01:11:49,380
매우 중요하고 매우 흥미로운 일을 할 수 있는 경우가 될 가능성이 가장 높다고 생각합니다.

2126
01:11:49,380 --> 01:11:51,000


2127
01:11:51,000 --> 01:11:53,100


2128
01:11:53,100 --> 01:11:55,440
AI CEO Sam Altman은 지난 주에

2129
01:11:55,440 --> 01:11:56,400


2130
01:11:56,400 --> 01:11:58,440
음 우리가 llms로 할 수 있는 일이

2131
01:11:58,440 --> 01:12:00,120
정말 지쳤다고 말했습니다. 우리는

2132
01:12:00,120 --> 01:12:02,520
새로운 방향이 필요합니다. 새롭고 새로운 새로운 방법 등이 있습니다.

2133
01:12:02,520 --> 01:12:04,860


2134
01:12:04,860 --> 01:12:07,440


2135
01:12:07,440 --> 01:12:09,060
llms가

2136
01:12:09,060 --> 01:12:11,159
멋진 일을 할 수 있다는 것을 알지만 아마

2137
01:12:11,159 --> 01:12:12,300


2138
01:12:12,300 --> 01:12:14,760


2139
01:12:14,760 --> 01:12:17,760
일반적인 AGI 아키텍처의 작은 부분을 형성할 것입니다.

2140
01:12:17,760 --> 01:12:19,679
AGI를

2141
01:12:19,679 --> 01:12:21,900
잠재적인 잠재적 목표로 생각하고 싶다면

2142
01:12:21,900 --> 01:12:25,679
음.  제 생각에는 많은 것들이

2143
01:12:25,679 --> 01:12:28,020
여기에 또 다른 예를 들겠습니다. 그래서

2144
01:12:28,020 --> 01:12:29,820
음 Anna는

2145
01:12:29,820 --> 01:12:31,920
아주 훌륭한 생산적인

2146
01:12:31,920 --> 01:12:34,140
과학자이기도 하고 최근에 논문을 가지고 있습니다.

2147
01:12:34,140 --> 01:12:35,400
음 llms를 위한 일종의 모듈식 아키텍처에 대해 주장하고 있는데

2148
01:12:35,400 --> 01:12:37,800


2149
01:12:37,800 --> 01:12:39,060
이것은 아주 좋은 프레임워크입니다.  인지

2150
01:12:39,060 --> 01:12:40,860
적으로 매우 타당합니다 그것은

2151
01:12:40,860 --> 01:12:41,880
우리가 추진해야 하는 것과 정확히 일치합니다 그것은

2152
01:12:41,880 --> 01:12:43,679
Howard

2153
01:12:43,679 --> 01:12:45,239
Gardner의 다중 지능 개념과 호환됩니다

2154
01:12:45,239 --> 01:12:46,800


2155
01:12:46,800 --> 01:12:48,000
음 하지만 동시에

2156
01:12:48,000 --> 01:12:49,620
이 논평을 마치기 위해

2157
01:12:49,620 --> 01:12:51,360
음 마지막으로 기술 대화가 있었습니다  몇 주

2158
01:12:51,360 --> 01:12:54,840
전 또는 아마도 며칠 전에 음

2159
01:12:54,840 --> 01:12:56,880
다른 많은 것들이

2160
01:12:56,880 --> 01:12:59,520
비생산적인 방식으로 AI 과대 광고와 결합될 수 있었기 때문에

2161
01:12:59,520 --> 01:13:02,100
openai의 Greg Brockman이 자신의

2162
01:13:02,100 --> 01:13:04,199
어, 이 큰 Ted Talks 중 하나를 주었고

2163
01:13:04,199 --> 01:13:06,179
GPD를 채팅하는 다른 플러그인을 보여주었습니다.

2164
01:13:06,179 --> 01:13:08,159
할 수 있는 것은 Wolfram이 작동한다고 언급했지만

2165
01:13:08,159 --> 01:13:09,300


2166
01:13:09,300 --> 01:13:11,820


2167
01:13:11,820 --> 01:13:13,560
채팅 TV를 통해 물건을 구매할 수 있는 이미지 생성 인스타카트 쇼핑과 같은 것도 있습니다.

2168
01:13:13,560 --> 01:13:15,060


2169
01:13:15,060 --> 01:13:17,040
음, 이것은

2170
01:13:17,040 --> 01:13:18,960
여러 하위 시스템이 다른 하위 기능을 수행할 수 있다는 생각으로 다시 돌아가게 합니다.

2171
01:13:18,960 --> 01:13:20,940
그래서 Brockovich

2172
01:13:20,940 --> 01:13:22,620
또한 채팅

2173
01:13:22,620 --> 01:13:26,820
GPT에 Excel 파일과 CSV 파일을 제공하는 예와

2174
01:13:26,820 --> 01:13:28,739
학술 논문의 보관 데이터베이스에서

2175
01:13:28,739 --> 01:13:30,060
많은 논문과

2176
01:13:30,060 --> 01:13:31,739
제목,

2177
01:13:31,739 --> 01:13:32,880
올바른

2178
01:13:32,880 --> 01:13:34,500


2179
01:13:34,500 --> 01:13:37,440
내용을 나열한 예를 보여주었습니다.  World Knowledge는

2180
01:13:37,440 --> 01:13:39,060
칼럼의 제목이 무엇을 의미하는지 추론하여

2181
01:13:39,060 --> 01:13:40,860


2182
01:13:40,860 --> 01:13:42,780
제목은 논문의 제목을 의미하고

2183
01:13:42,780 --> 01:13:44,640
저자는

2184
01:13:44,640 --> 01:13:47,280
논문당 저자 수를 의미하고

2185
01:13:47,280 --> 01:13:48,840
생성은 논문이

2186
01:13:48,840 --> 01:13:50,820
올바르게 제출된 날짜를 의미한다는 것을 이해했습니다.  TED

2187
01:13:50,820 --> 01:13:52,080
토크 관객분들이 다들

2188
01:13:52,080 --> 01:13:54,120
기립박수를 보내주셨죠

2189
01:13:54,120 --> 01:13:56,340
음 근데 엑셀파일에 라벨을 기술하는 능력은

2190
01:13:56,340 --> 01:13:57,900


2191
01:13:57,900 --> 01:14:01,679
좋은거 같은데 음

2192
01:14:01,679 --> 01:14:03,120
세계지식이라고 부르실지 잘 모르겠어서

2193
01:14:03,120 --> 01:14:04,800
많을거 같아요  저는 단지

2194
01:14:04,800 --> 01:14:06,719


2195
01:14:06,719 --> 01:14:08,880
안드로포모좀을 줄이는 것과 함께 많은 진전이 필요하다고 말하고 싶습니다.

2196
01:14:08,880 --> 01:14:11,640
그리고 여러분은 그것의 적절한 균형을 가지고 있습니다. 그래서

2197
01:14:11,640 --> 01:14:12,659
제가 말했듯이 여러분은 음

2198
01:14:12,659 --> 01:14:13,920


2199
01:14:13,920 --> 01:14:15,360
심리적으로 그럴듯한 종류의

2200
01:14:15,360 --> 01:14:17,100
모듈식 구조의 올바른 균형을 가져야 하지만 너무 가질 수는 없습니다.

2201
01:14:17,100 --> 01:14:18,480


2202
01:14:18,480 --> 01:14:19,920


2203
01:14:19,920 --> 01:14:21,540


2204
01:14:21,540 --> 01:14:22,860


2205
01:14:22,860 --> 01:14:25,739
인간과 같은 종류의 어 모듈식

2206
01:14:25,739 --> 01:14:27,780
시스템을 모델링하는 것 사이에서 올바른 균형을 찾아야 하지만

2207
01:14:27,780 --> 01:14:29,280
여러분이 알고 있는 약간

2208
01:14:29,280 --> 01:14:31,140
믿기지 않거나 과학적으로

2209
01:14:31,140 --> 01:14:33,679
도움이 되지 않는 정도까지 하지 않는 것 사이에서 올바른 균형을 찾아야 합니다.

2210
01:14:35,040 --> 01:14:37,500
제 말은 제 말은

2211
01:14:37,500 --> 01:14:40,199


2212
01:14:40,199 --> 01:14:42,739
언어 모델을

2213
01:14:42,739 --> 01:14:45,120
어 다른 형태의 정보

2214
01:14:45,120 --> 01:14:47,640
처리에 연결하는 이러한 방법에 대해 정말 흥분된다는 모든 것에 동의한다고 생각합니다. 어 그것은

2215
01:14:47,640 --> 01:14:49,679
사람들이 가지고 있는 것과 같습니다.

2216
01:14:49,679 --> 01:14:52,620


2217
01:14:52,620 --> 01:14:54,840


2218
01:14:54,840 --> 01:14:58,260
언어 모델링과 마찬가지로 그들이 할 수 있는 일에 매우 놀랐습니다 그래서 음

2219
01:14:58,260 --> 01:15:00,120
다양한 종류의 추리

2220
01:15:00,120 --> 01:15:01,920
퍼즐과 그들이 풀 수 있는 것들을 알고 있습니다 제

2221
01:15:01,920 --> 01:15:05,340
생각에는 어 정말 매력적

2222
01:15:05,340 --> 01:15:07,920
이며 아마도  당신이

2223
01:15:07,920 --> 01:15:09,719
알고 있는

2224
01:15:09,719 --> 01:15:11,760
언어와 생각 사이의 관계를 다시 생각

2225
01:15:11,760 --> 01:15:13,500
하고 무언가가 표현을

2226
01:15:13,500 --> 01:15:15,239


2227
01:15:15,239 --> 01:15:17,760
가지고 있거나

2228
01:15:17,760 --> 01:15:19,380


2229
01:15:19,380 --> 01:15:21,060
그 표현에 대해 추론하는 것이 무엇을 의미하는지에 대해 구체적으로 알아내도록 노력해야 합니다. 하지만 궁극적으로

2230
01:15:21,060 --> 01:15:23,820
나는 동의한다고 생각합니다.  음 음

2231
01:15:23,820 --> 01:15:26,219
사람들이

2232
01:15:26,219 --> 01:15:28,560
사물에 대해 생각하는 방식이 다르다는 걸 알고 그게

2233
01:15:28,560 --> 01:15:32,400
지능에 중요한 것 같아요 음

2234
01:15:32,400 --> 01:15:35,040
저도 아기 LM 챌린지에 대해 매우 흥분해서

2235
01:15:35,040 --> 01:15:37,620


2236
01:15:37,620 --> 01:15:39,960
언어적인 측면에서 생각해요 맞아요 음

2237
01:15:39,960 --> 01:15:42,300
어 그게 바로

2238
01:15:42,300 --> 01:15:45,300
우리가 어 작은 데이터 세트로 얼마나 멀리 갈 수 있는지 보는 것이 옳은 일 음

2239
01:15:45,300 --> 01:15:47,219
그리고 아마도 그

2240
01:15:47,219 --> 01:15:50,820
후에 여러분은 어

2241
01:15:50,820 --> 01:15:53,699


2242
01:15:53,699 --> 01:15:55,739
아이들이

2243
01:15:55,739 --> 01:15:57,840
습득하는 의미론의 종류와 그들이 그것을 어디서

2244
01:15:57,840 --> 01:15:59,820
어떻게 얻는지 좀 더 이해하려고 노력하는 것을 알게 될 것입니다  일종의 외부 의미 체계는

2245
01:15:59,820 --> 01:16:02,520
어 언어 학습 또는

2246
01:16:02,520 --> 01:16:04,560
특히 문법 및

2247
01:16:04,560 --> 01:16:06,600
구문 학습에 정보를 제공할 수 있습니다. 음

2248
01:16:06,600 --> 01:16:09,480
제 또 다른 어 앞으로 나아갈

2249
01:16:09,480 --> 01:16:12,179
지점은

2250
01:16:12,179 --> 01:16:15,840
어 다음과 같을 것 같습니다. 저는

2251
01:16:15,840 --> 01:16:18,300
이런 종류의 모델이

2252
01:16:18,300 --> 01:16:21,179
음 어 정말 멀리 갔다고 느낍니다.  이런 종류의 모델에 대한 사람들의 기대를 뛰어넘는

2253
01:16:21,179 --> 01:16:23,340


2254
01:16:23,340 --> 01:16:25,500
기초적인

2255
01:16:25,500 --> 01:16:27,960
통계 학습이

2256
01:16:27,960 --> 01:16:29,940
텍스트에서 패턴을 발견하는 것

2257
01:16:29,940 --> 01:16:30,719
음 음 음

2258
01:16:30,719 --> 01:16:32,640
정말

2259
01:16:32,640 --> 01:16:35,040
꽤 놀라운 결과를 주는 것 같습니다

2260
01:16:35,040 --> 01:16:37,020
음 그리고 앞으로 제

2261
01:16:37,020 --> 01:16:39,360
생각에는 엄청난 물결을 불러일으킨 것 같습니다.

2262
01:16:39,360 --> 01:16:42,000
이론에 대한 불확실성이 있기 때문에

2263
01:16:42,000 --> 01:16:43,620
기본적으로 언어의 모든 것에 대한 우리의 이론은

2264
01:16:43,620 --> 01:16:46,739
확실히

2265
01:16:46,739 --> 01:16:48,960
음 하지만 인지는 아마도 신경과학이

2266
01:16:48,960 --> 01:16:50,640
맞을 것이라고 생각합니다.

2267
01:16:50,640 --> 01:16:53,100


2268
01:16:53,100 --> 01:16:54,420


2269
01:16:54,420 --> 01:16:56,219


2270
01:16:56,219 --> 01:16:58,560


2271
01:16:58,560 --> 01:17:02,219
이와 같은 정말 일반적인 종류의 학습 시스템의 음

2272
01:17:02,219 --> 01:17:03,900
음, 한편으로는 음 음 음

2273
01:17:03,900 --> 01:17:05,820


2274
01:17:05,820 --> 01:17:07,440
과거 이론에 대한 일종의 안타까운 일이죠.

2275
01:17:07,440 --> 01:17:09,360


2276
01:17:09,360 --> 01:17:11,640


2277
01:17:11,640 --> 01:17:14,100


2278
01:17:14,100 --> 01:17:15,659


2279
01:17:15,659 --> 01:17:18,000
하지만 좋은 점은

2280
01:17:18,000 --> 01:17:20,460


2281
01:17:20,460 --> 01:17:22,620
AI와 인지 과학, 그리고

2282
01:17:22,620 --> 01:17:24,000
언어학 모두에게 매우 흥미로운 시간이 될 것이라고 생각합니다. 음

2283
01:17:24,000 --> 01:17:25,560
이제 이러한 정말

2284
01:17:25,560 --> 01:17:27,420
정말 강력한 도구가 있습니다.

2285
01:17:27,420 --> 01:17:29,719


2286
01:17:29,719 --> 01:17:32,940
음 질적으로 어 인간의 능력을 향한 차원이 다른 단계처럼 보입니다.

2287
01:17:32,940 --> 01:17:34,620


2288
01:17:34,620 --> 01:17:36,120
음  그리고 저는 그것들을 통합

2289
01:17:36,120 --> 01:17:39,060
하고

2290
01:17:39,060 --> 01:17:41,280


2291
01:17:41,280 --> 01:17:43,679
그것들이 어떻게

2292
01:17:43,679 --> 01:17:45,540
만들어지고 지능형 시스템을 설계하는 데 어떤 원리가 적용되는지에 대한 공학 수업과 철학 수업을 모두 듣는다고 생각합니다.

2293
01:17:45,540 --> 01:17:47,460


2294
01:17:47,460 --> 01:17:49,679


2295
01:17:49,679 --> 01:17:52,080
다음 5년 또는 10년 동안 느낌을 실제로 형성할 것입니다.

2296
01:17:52,080 --> 01:17:53,699


2297
01:17:53,699 --> 01:17:54,900


2298
01:17:54,900 --> 01:17:57,000
또한

2299
01:17:57,000 --> 01:17:58,500
더 넓은 주제의 맥락에서 제가 말하고 싶은 것처럼 여기에서

2300
01:17:58,500 --> 01:17:59,760
당신이 완전히 옳다는 것처럼 제가 언제

2301
01:17:59,760 --> 01:18:01,620


2302
01:18:01,620 --> 01:18:04,920
deep blue be uh Casper of에 대해 읽었을 때를 기억합니다.

2303
01:18:04,920 --> 01:18:07,260
체스였나 어 뭐 맞나 그래 그리고

2304
01:18:07,260 --> 01:18:08,940


2305
01:18:08,940 --> 01:18:12,480
AI가 인간이 될 수 있다면 체스는 끝났다는 걸 안다고 말하는 해설자들이 있었어

2306
01:18:12,480 --> 01:18:13,739
그러면 체스 공부의 요점이 무엇인지에 대한 게임이야

2307
01:18:13,739 --> 01:18:15,420


2308
01:18:15,420 --> 01:18:16,920
더 이상 지루할 필요가 없다는 것을 알고 음

2309
01:18:16,920 --> 01:18:18,900
그리고 내 생각에  AI가

2310
01:18:18,900 --> 01:18:20,640
인간이 체스를 두는 데 필요한 모든 것을 달성한 것처럼 보인다면

2311
01:18:20,640 --> 01:18:22,140
체스를 두는 것이 무슨 의미가 있겠습니까?

2312
01:18:22,140 --> 01:18:23,100


2313
01:18:23,100 --> 01:18:24,540
하지만 체스

2314
01:18:24,540 --> 01:18:26,219
의 인기를 높이는 데 도움이 된 것이 있다면 아시리라 생각합니다.

2315
01:18:26,219 --> 01:18:27,900
그들은 미니 체스

2316
01:18:27,900 --> 01:18:29,580
유명인뿐만 아니라 전 세계

2317
01:18:29,580 --> 01:18:31,199
토너먼트에 있습니다.  그리고 나는

2318
01:18:31,199 --> 01:18:32,520
아마 언어에서도 같은 일이 일어날 것이라고 예측할 것입니다.

2319
01:18:32,520 --> 01:18:34,620
llms가 언어의 끝이라는 것을 의미하지 않는다는 것을 알고 있습니다.

2320
01:18:34,620 --> 01:18:36,360
더 이상

2321
01:18:36,360 --> 01:18:37,800
언어가 더 이상 언어학이 아닙니다.

2322
01:18:37,800 --> 01:18:39,179


2323
01:18:39,179 --> 01:18:40,620


2324
01:18:40,620 --> 01:18:42,540
LMS는

2325
01:18:42,540 --> 01:18:44,400
언어 이론에 대한 일반적인 관심을 증가시킬 것입니다.

2326
01:18:44,400 --> 01:18:46,199


2327
01:18:46,199 --> 01:18:48,060
이상한 제약과 명백한

2328
01:18:48,060 --> 01:18:50,100
한계를 알고 있기 때문입니다.

2329
01:18:50,100 --> 01:18:52,380
이 시점에서 규모를 알고 있다고 말할 수 있기 때문입니다.

2330
01:18:52,380 --> 01:18:55,500


2331
01:18:55,500 --> 01:18:57,420


2332
01:18:57,420 --> 01:18:59,820


2333
01:18:59,820 --> 01:19:01,260


2334
01:19:01,260 --> 01:19:03,239
그룹 워시 예측 및 일반화 등을 수행하기 위해 지식과 경험을 추상화하는 LMS의 능력은

2335
01:19:03,239 --> 01:19:04,739


2336
01:19:04,739 --> 01:19:06,780
몇 가지 예를 제시했지만 일반화에 실제로 능숙하지 않은 것으로 보이는

2337
01:19:06,780 --> 01:19:07,860
문헌에 다른 몇 가지가 있습니다.

2338
01:19:07,860 --> 01:19:09,179


2339
01:19:09,179 --> 01:19:10,739
아마도

2340
01:19:10,739 --> 01:19:13,620
특정 토큰 유형으로 이동하지만

2341
01:19:13,620 --> 01:19:15,060
내 마지막

2342
01:19:15,060 --> 01:19:17,100
주장은

2343
01:19:17,100 --> 01:19:19,080
언어 습득 문헌을 알고 있다는 것입니다. 인지

2344
01:19:19,080 --> 01:19:21,480


2345
01:19:21,480 --> 01:19:22,860
과학자는 실제로 llms가 필요하지 않다는 것을 알고 있지만

2346
01:19:22,860 --> 01:19:26,219
잠재적으로 llms가 필요하지는 않습니다.  어

2347
01:19:26,219 --> 01:19:27,420
당신은 복직을 알고 분명히

2348
01:19:27,420 --> 01:19:29,640
여기에 동의하지 않지만 음 저는

2349
01:19:29,640 --> 01:19:32,040
llms에서 이익을 얻는 대기업이

2350
01:19:32,040 --> 01:19:33,360
llms가 필요하다고 말하고 싶습니다. 그들은 실제로 그것을 하는 유일한 회사입니다.

2351
01:19:33,360 --> 01:19:35,100


2352
01:19:35,100 --> 01:19:37,320


2353
01:19:37,320 --> 01:19:39,000
마음이 매우  매우 다양한 공간에서 영화가 하는 것과 유사한 프로세스

2354
01:19:39,000 --> 01:19:40,560


2355
01:19:40,560 --> 01:19:42,420
에 의해 포착될 수 있는 특정 형태의 행동과 학습이 있을 수 있습니다.

2356
01:19:42,420 --> 01:19:44,219


2357
01:19:44,219 --> 01:19:45,540
그래서 Stephen은

2358
01:19:45,540 --> 01:19:47,159
자신의 논문에서 자기

2359
01:19:47,159 --> 01:19:49,320
와 이상한 종류의 학습 규칙에 대한 몇 가지 흥미로운 예를 제시했습니다.

2360
01:19:49,320 --> 01:19:51,060
매우 도메인 일반적이고 매우

2361
01:19:51,060 --> 01:19:52,920
빠르고 매우 신비 스럽기 때문에

2362
01:19:52,920 --> 01:19:54,780


2363
01:19:54,780 --> 01:19:56,100
그런 종류의 학습이

2364
01:19:56,100 --> 01:19:57,780
관련이 있다는 것을 알 수 있지만

2365
01:19:57,780 --> 01:19:59,580
후보자 중 하나가

2366
01:19:59,580 --> 01:20:02,100
자연어가 될 가능성은 낮고 적어도

2367
01:20:02,100 --> 01:20:03,480
자연 스럽습니다.  언어가 작동하고

2368
01:20:03,480 --> 01:20:05,219
주로

2369
01:20:05,219 --> 01:20:07,380
규제와 무엇을 가지고 있는지의 측면에서 완전한 영광입니다 그래서 나는

2370
01:20:07,380 --> 01:20:09,300
그것이 당신이 당신이

2371
01:20:09,300 --> 01:20:11,520
알고 있는 이

2372
01:20:11,520 --> 01:20:13,320
이미지가 있는 곳을 생각나게 한다는 것을 알고 계실 것 같습니다. 최근에 John Wick 4장을 봤습니다.

2373
01:20:13,320 --> 01:20:14,940
이것은

2374
01:20:14,940 --> 01:20:16,140
그가

2375
01:20:16,140 --> 01:20:17,580
사막을 걷고 있는 장면인데 그가 암살하려는 것처럼 이 남자를 본 것인지 확실하지 않습니다.

2376
01:20:17,580 --> 01:20:19,260


2377
01:20:19,260 --> 01:20:21,540


2378
01:20:21,540 --> 01:20:22,739
사막을 걸을 때

2379
01:20:22,739 --> 01:20:24,840
음, 오아시스를 보는 환상을 갖게 되는 것과 같습니다.

2380
01:20:24,840 --> 01:20:26,460


2381
01:20:26,460 --> 01:20:28,199
환각을 다시 하고 있지만

2382
01:20:28,199 --> 01:20:29,820
너무 늦기 전에

2383
01:20:29,820 --> 01:20:31,679
실제로 환각을 하고 있다는 것을 가끔 알고 있다는 것을 깨닫습니다.

2384
01:20:31,679 --> 01:20:33,239
오아시스가 보이지 않고 여전히

2385
01:20:33,239 --> 01:20:35,040
사막에 있고 그것이 바로

2386
01:20:35,040 --> 01:20:37,199
지금 우리가 처한 상황일 수도 있다고 생각합니다.

2387
01:20:37,199 --> 01:20:39,360
많은 언어 모델의 언어 능력으로

2388
01:20:39,360 --> 01:20:41,400
우리는 어 언어 능력에 대한 환상을 가지고 있습니다.

2389
01:20:41,400 --> 01:20:44,940


2390
01:20:44,940 --> 01:20:46,260


2391
01:20:46,260 --> 01:20:48,360
오아시스를 찾기 전에 항상 환상을 보게 되기 때문입니다. 제

2392
01:20:48,360 --> 01:20:49,440
생각에 지금 우리는

2393
01:20:49,440 --> 01:20:51,420
사막의 환각 상태에 있습니다.

2394
01:20:51,420 --> 01:20:53,699
잠재적인 언어 능력의 불꽃이 보이지만

2395
01:20:53,699 --> 01:20:55,380
아직 명확하지 않고 강력하지 않습니다.

2396
01:20:55,380 --> 01:20:57,420


2397
01:20:57,420 --> 01:20:59,640
음 우리는 아직 오아시스에 실제로 도달하지 못했습니다.

2398
01:20:59,640 --> 01:21:02,480
예

2399
01:21:02,820 --> 01:21:06,360
음 그냥 속사포 질문입니다.

2400
01:21:06,360 --> 01:21:09,540
그래서 짧은 응답을 할 수 있는지 확인하여

2401
01:21:09,540 --> 01:21:11,219
svenochino가

2402
01:21:11,219 --> 01:21:13,440
질문을 씁니다.

2403
01:21:13,440 --> 01:21:15,120
큰 언어 모델에는 우선 순위가 없다고 말하면

2404
01:21:15,120 --> 01:21:17,540


2405
01:21:18,480 --> 01:21:21,300
큰 언어 모델에는 우선 순위가 있습니다. 저는

2406
01:21:21,300 --> 01:21:23,820
예라고 말하고 싶습니다. 그들은 확실히 그렇습니다 음

2407
01:21:23,820 --> 01:21:25,620
그리고

2408
01:21:25,620 --> 01:21:28,440
음 그들은

2409
01:21:28,440 --> 01:21:30,360
당신이 아는 사람들이

2410
01:21:30,360 --> 01:21:32,340
사전에 대해 생각하고 베이지안 추론에 대해 생각하는 데 사용되는 방식의 차이라고 생각합니다.

2411
01:21:32,340 --> 01:21:33,840
예를 들어

2412
01:21:33,840 --> 01:21:36,300
베이지안 통계 모델을 작성하고 싶다면

2413
01:21:36,300 --> 01:21:37,860
여기에 매개변수가 있고

2414
01:21:37,860 --> 01:21:39,300
여기에 매개변수에 대한 사전이 무엇인지 아는 것처럼 말합니다 음

2415
01:21:39,300 --> 01:21:40,860


2416
01:21:40,860 --> 01:21:42,360
큰 언어 모델 내 사전은

2417
01:21:42,360 --> 01:21:44,340


2418
01:21:44,340 --> 01:21:45,900
일반적으로 신경 너트라고 생각합니다.

2419
01:21:45,900 --> 01:21:47,880
이전 함수는 훨씬 더 암묵적 권리이므로

2420
01:21:47,880 --> 01:21:49,679


2421
01:21:49,679 --> 01:21:52,080
다른 함수보다 배우기 쉬운 일부 함수가 있으며

2422
01:21:52,080 --> 01:21:53,760


2423
01:21:53,760 --> 01:21:55,860


2424
01:21:55,860 --> 01:21:57,540
그러한 종류의 암시적 사전 순위가 무엇인지에 대한 설명을 알고 있음을 발견하려는 작업도 있지만

2425
01:21:57,540 --> 01:21:59,040


2426
01:21:59,040 --> 01:22:00,840
실제로는 그렇게 생각합니다.  에

2427
01:22:00,840 --> 01:22:02,040
대해 음 음 서로

2428
01:22:02,040 --> 01:22:02,940


2429
01:22:02,940 --> 01:22:04,620
다른 신경망 아키텍처의 비교를 알고 있습니다. 음 어 아마도 의기

2430
01:22:04,620 --> 01:22:07,199


2431
01:22:07,199 --> 01:22:09,060
양양한 것일 수도

2432
01:22:09,060 --> 01:22:10,500
있고 아이들이 올바르게 배우는 것을 배울 수 있는 사전을 찾아야 한다는 데 동의할 수도 있습니다.

2433
01:22:10,500 --> 01:22:12,600


2434
01:22:12,600 --> 01:22:14,400


2435
01:22:14,400 --> 01:22:16,140


2436
01:22:16,140 --> 01:22:19,080
음 모든 아키텍처가 그렇지는 않습니다.

2437
01:22:19,080 --> 01:22:21,360
음

2438
01:22:21,360 --> 01:22:23,280
완전하게 변하고 있거나

2439
01:22:23,280 --> 01:22:24,719
어떤 종류의 기능을 배울 수 있는 아키텍처 사이에서도 모든

2440
01:22:24,719 --> 01:22:27,540


2441
01:22:27,540 --> 01:22:30,000


2442
01:22:30,000 --> 01:22:31,800


2443
01:22:31,800 --> 01:22:34,080
아키텍처가 그렇게하지는 않을 것입니다.  실제로는

2444
01:22:34,080 --> 01:22:36,360
사전 검색 중 하나입니다.

2445
01:22:36,360 --> 01:22:38,580
음, 사전 검색이 아닙니다.

2446
01:22:38,580 --> 01:22:39,719


2447
01:22:39,719 --> 01:22:41,699
또는 보편 문법에 대한 검색이라고 생각할 수 있습니다. 하지만

2448
01:22:41,699 --> 01:22:44,340


2449
01:22:44,340 --> 01:22:46,560
사람들이 다음과 같이 이야기했다는 의미에서 사전 검색이나 보편 문법이 아닙니다.

2450
01:22:46,560 --> 01:22:48,540


2451
01:22:48,540 --> 01:22:50,159
어떤 종류의 규칙이 허용되는지에 대한 명시적 진술

2452
01:22:50,159 --> 01:22:51,780
또는

2453
01:22:51,780 --> 01:22:53,880
어떤 종류의 함수가 높은

2454
01:22:53,880 --> 01:22:55,320
확률인지에 대한 명시적 진술 또는

2455
01:22:55,320 --> 01:22:57,360
모든 것이 암묵적으로 코딩되어 있다는 것과 같은 것 음 예

2456
01:22:57,360 --> 01:22:58,560
예 전적으로 내 생각에

2457
01:22:58,560 --> 01:23:00,540
그것이 맞다고 생각합니다.  유사하게 평가하는 것의

2458
01:23:00,540 --> 01:23:02,880
공간

2459
01:23:02,880 --> 01:23:05,100
과

2460
01:23:05,100 --> 01:23:07,020
인간이 하는 것과 조금이라도 비슷하다면

2461
01:23:07,020 --> 01:23:09,300


2462
01:23:09,300 --> 01:23:11,219
적어도 qpt3와 같은 것이

2463
01:23:11,219 --> 01:23:13,380
존재한다는 증거는 표면 분포 분석에서

2464
01:23:13,380 --> 01:23:15,960
완전히 기능하는 구문 범주를 구축한다는 것을 알고 있다는 증거입니다.

2465
01:23:15,960 --> 01:23:18,000


2466
01:23:18,000 --> 01:23:21,420
이것만으로도 가능합니다.

2467
01:23:21,420 --> 01:23:24,120
맞습니다. 하지만

2468
01:23:24,120 --> 01:23:27,300
대부분의 실무자들은

2469
01:23:27,300 --> 01:23:29,460
구문 범주가 타고난 것이라고 실제로 믿지 않는다고 말하고 싶습니다.

2470
01:23:29,460 --> 01:23:31,380
따라서 이전 문제는

2471
01:23:31,380 --> 01:23:33,060
여기에서 약간 덜 관련이 있습니다.

2472
01:23:33,060 --> 01:23:35,219
타고난 것으로 설정된 작업이므로

2473
01:23:35,219 --> 01:23:37,500
in  구문 도메인에 있다고 하는 특정

2474
01:23:37,500 --> 01:23:39,659
언어 계산이며

2475
01:23:39,659 --> 01:23:41,159


2476
01:23:41,159 --> 01:23:42,719
실제로 Charles Yang

2477
01:23:42,719 --> 01:23:44,100
um조차도 지난 몇 년 동안

2478
01:23:44,100 --> 01:23:46,800
그들이 그 안에 있을 수도 있지만 그렇지 않을 수도 있음을 인정했기 때문에

2479
01:23:46,800 --> 01:23:49,679
사람들이 다른

2480
01:23:49,679 --> 01:23:51,540
관련 우선 순위를 부여한 것은 사물입니다.

2481
01:23:51,540 --> 01:23:53,100
저와 Gary 시장은

2482
01:23:53,100 --> 01:23:55,080


2483
01:23:55,080 --> 01:23:56,640
큰 문제인 것 같은 구성성에 대해 이야기했습니다. 그래서 사람들은

2484
01:23:56,640 --> 01:23:59,520
채팅을 제공했습니다.

2485
01:23:59,520 --> 01:24:02,280


2486
01:24:02,280 --> 01:24:04,920


2487
01:24:04,920 --> 01:24:06,480


2488
01:24:06,480 --> 01:24:09,060
살인 혐의로 체포되고

2489
01:24:09,060 --> 01:24:10,920
그것을 압축하고 다시 설명하면

2490
01:24:10,920 --> 01:24:12,420
58명이 과실

2491
01:24:12,420 --> 01:24:14,040
치사 혐의로 기소되는 것으로 나옵니다. 그것은 그것이 하고 있는 압축이 무엇이든 간에

2492
01:24:14,040 --> 01:24:15,659
구성성이 결여되어 있다는 꽤 분명한 예입니다.

2493
01:24:15,659 --> 01:24:17,219


2494
01:24:17,219 --> 01:24:19,260


2495
01:24:19,260 --> 01:24:20,940


2496
01:24:20,940 --> 01:24:23,159
잠재적인 유추적 추론의 몇 가지 예가 있는 또 다른 예는

2497
01:24:23,159 --> 01:24:24,840
Bing 채팅에서 Bing이 이 함정 기능을 가지고 있다는 것을 알고 있습니다

2498
01:24:24,840 --> 01:24:26,100


2499
01:24:26,100 --> 01:24:28,080
음 문제는 인간이

2500
01:24:28,080 --> 01:24:29,460
이미 문서화한 메타

2501
01:24:29,460 --> 01:24:31,080


2502
01:24:31,080 --> 01:24:33,420
관계를 찾는 것입니다.  만들어지고 있는 물건

2503
01:24:33,420 --> 01:24:35,100


2504
01:24:35,100 --> 01:24:38,520
음 그래서 누군가가 저에게

2505
01:24:38,520 --> 01:24:41,760
예수 그리스도와

2506
01:24:41,760 --> 01:24:44,640
노키아 9910을 비교하는 표를 그렸다는 것을 알고 계실 겁니다. 바로 휴대 전화인 노키아

2507
01:24:44,640 --> 01:24:46,080
9910

2508
01:24:46,080 --> 01:24:47,520
음과 그들이 CPU와

2509
01:24:47,520 --> 01:24:49,860
크기와 무게를 비교한 출시 날짜를 비교했다는 것을 알고 있다고 합니다.

2510
01:24:49,860 --> 01:24:53,280


2511
01:24:53,280 --> 01:24:55,140
예수의 전지전능한 지식

2512
01:24:55,140 --> 01:24:57,600
폰의 기억력과 신의 전지전능과 비교했지 뭐 음

2513
01:24:57,600 --> 01:25:00,600


2514
01:25:00,600 --> 01:25:02,760
또

2515
01:25:02,760 --> 01:25:04,560
노키아가 몇 번 재발매 되어서 둘 다 부활했다고 했던 것 같은데 맞아요

2516
01:25:04,560 --> 01:25:06,239
그래서

2517
01:25:06,239 --> 01:25:08,159
노키아가 짱구같음  훌륭한 답변이

2518
01:25:08,159 --> 01:25:10,560
뭐가 잘못된 건지 괜찮습니다. 유

2519
01:25:10,560 --> 01:25:12,120


2520
01:25:12,120 --> 01:25:13,860
추적 추론처럼 들릴 수도 있지만

2521
01:25:13,860 --> 01:25:15,420


2522
01:25:15,420 --> 01:25:17,100
카메라에 대해 알고 있는 것과 같은 꽤 이상한 것도 있었습니다. 그것은

2523
01:25:17,100 --> 01:25:19,679
단지 예수님의 설명을 제공했거나

2524
01:25:19,679 --> 01:25:21,659
실제로는 그렇지 않습니다.  카메라에는

2525
01:25:21,659 --> 01:25:24,000
유추적 추론처럼 보이는 것이 있을

2526
01:25:24,000 --> 01:25:27,860
수 있지만 불분명합니다.

2527
01:25:27,860 --> 01:25:31,440
예, 제 생각에는 그게 저에게 멋진 대답처럼 들리는 것 같아요.

2528
01:25:31,440 --> 01:25:33,020


2529
01:25:33,020 --> 01:25:36,360


2530
01:25:36,360 --> 01:25:38,100
큰 언어 모델이

2531
01:25:38,100 --> 01:25:39,719
품사의 존재 증명을 배운다고 말씀하신 것처럼 말하려고 했습니다.

2532
01:25:39,719 --> 01:25:41,520
범주이지만

2533
01:25:41,520 --> 01:25:43,380
음성 범주의 일부를 출력하지 않는 것처럼

2534
01:25:43,380 --> 01:25:45,719
문법적

2535
01:25:45,719 --> 01:25:47,640
통사적 지식이 많고

2536
01:25:47,640 --> 01:25:50,880
더 나아가

2537
01:25:50,880 --> 01:25:52,980
의미론적 지식과

2538
01:25:52,980 --> 01:25:55,080
실용적인 지식이 많은 것처럼 그들이 있다는 것을 알고 있습니다.

2539
01:25:55,080 --> 01:25:58,080
번역 솜씨가 나쁘지 않고

2540
01:25:58,080 --> 01:25:59,880
그들이 발견한 것이 훨씬 더 많다는 듯이

2541
01:25:59,880 --> 01:26:01,139


2542
01:26:01,139 --> 01:26:03,840


2543
01:26:03,840 --> 01:26:07,020
um uh well 미안하다고 말했는데

2544
01:26:07,020 --> 01:26:08,940
기술적인 범주라고 그렇죠 뭐 그래 미안해서 그래 그래

2545
01:26:08,940 --> 01:26:11,040
하지만

2546
01:26:11,040 --> 01:26:13,080
그들은 발견했어  그보다 훨씬 더

2547
01:26:13,080 --> 01:26:13,980
음

2548
01:26:13,980 --> 01:26:15,600
예

2549
01:26:15,600 --> 01:26:18,780
음 음 티저 슬래시

2550
01:26:18,780 --> 01:26:20,699
동기로 두 분 모두

2551
01:26:20,699 --> 01:26:23,040
나중에 다른 손님이 있든 없든 나중에 다시 합류하기를 바랍니다.

2552
01:26:23,040 --> 01:26:24,540


2553
01:26:24,540 --> 01:26:26,520


2554
01:26:26,520 --> 01:26:28,679
이 기록에 포함할 몇 가지 흥미로운 질문과  그런 다음

2555
01:26:28,679 --> 01:26:30,060
Ellie와 Steven 모두 참여해 주셔서 감사합니다.

2556
01:26:30,060 --> 01:26:31,260


2557
01:26:31,260 --> 01:26:33,719
Juan이 마지막으로 질문한 몇 가지 질문은

2558
01:26:33,719 --> 01:26:35,820
2020년에 어린이 학습 언어와 비교하여 작은 Transformers Jang이 어떻게 작동하는지

2559
01:26:35,820 --> 01:26:38,580


2560
01:26:38,580 --> 01:26:40,860
물었습니다.

2561
01:26:40,860 --> 01:26:42,800


2562
01:26:42,800 --> 01:26:45,780


2563
01:26:45,780 --> 01:26:48,420
llms의 공간에 어떤 제약이 있는지 교육을 통해 도달하지 못하므로

2564
01:26:48,420 --> 01:26:50,699


2565
01:26:50,699 --> 01:26:52,500
그들이 처음에 구현한 것이 아니라는 것을 발견하고 있는지도 모릅니다.

2566
01:26:52,500 --> 01:26:54,780
더 많은 질문이 있으므로

2567
01:26:54,780 --> 01:26:57,540
우리 모두

2568
01:26:57,540 --> 01:27:00,600
서로의 작업을 검토하고 다시 읽을 수 있기를 바랍니다.

2569
01:27:00,600 --> 01:27:04,020
나중에 41.2를 위해 모이세요.

2570
01:27:04,020 --> 01:27:06,060
이 훌륭한 스트림에 대해 Elliot과 Steven에게 감사합니다.

2571
01:27:06,060 --> 01:27:08,280
Dave에게 감사합니다. Dave에게 감사합니다.

2572
01:27:08,280 --> 01:27:10,739
둘 다 감사합니다.

2573
01:27:10,739 --> 01:27:15,379



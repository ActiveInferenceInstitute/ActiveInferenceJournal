SPEAKER_00:
Hello, everyone.

Welcome to the Active Inference Lab.

Today it is August 20th, 2021, and we are here in guest stream number 9.1 with some awesome guests.

So we'll just start with introductions and then head into a short presentation before a question and answer period.

So if you're watching live, definitely write questions in the chat.

So I'm Daniel.

I'm a researcher in California and blue.


SPEAKER_01:
I'm Blue Knight.

I'm an independent research consultant in New Mexico.

And I'll pass it to Rafael.


SPEAKER_03:
Hi, everyone.

Thank you guys for having us over.

I'm Rafael.

I'm also an independent researcher.

I have a day job at Google that has some but not a lot to do with this topic.

And I've been working with Pranav and Jacob on this topic for

about a year and a half now.

It's been a wild ride.

So Rana, go ahead.


SPEAKER_02:
Hello, everyone.

Thank you, Daniel and Pluto, for inviting us.

I'm a PhD student at Carnegie Mellon Tepper School of Business.

My areas of research are collective intelligence, where does it come from, building

causal model, social technical architecture, looking at correctives as adaptive systems.

You know, I was very interested in the active infantry work, we met Raphael, I and Jacob met, you know, across different CI conferences and got this project going.

So we're very excited to talk more about and learn from all the discussions that we have.

Back over to you, Wes.


SPEAKER_00:
Sounds cool.

So let's go to Rafael's presentation, and then we'll just have a bunch of time for questions.

So go for it.


SPEAKER_03:
All right.

Before I get started, can you all confirm that you can see me all right?

Yeah, we see the slides.

The slides, perfect.

All right, cool.

All right, so what are we doing here?

So as we mentioned, the interest here is to try to understand better the phenomenon of collective intelligence.

And so two examples here of what we're talking about, and they're pretty different examples.

One of them is a pretty classic example.

It's the murmuration of starlings, top left.

I know the bird that's solving a puzzle on the top right is a crow, not a starling, so I'm cheating a little bit, but that's fine.

So...

On the bottom, the example is the discovery of the Higgs boson, which is an amazing example of kind of large-scale, long-term scientific collaboration.

So those are both legit examples of collective intelligence and

yet the way that we approach analyzing these things tends to be a little bit different.

So if you're doing something around complex aerodynamic systems or computational biology, you tend to look primarily at the collective behavior and to be able to do that in a computable way, you assume that individual agents are following simple rules to react to the environment and to their peers.

And if you're doing something like anthropology or social science, you're probably thinking of collectives that look more at the bottom.

And so you're focused on these really sophisticated agents that have these complex social cognitive capabilities that are the thing that endows them with the ability to act as collectives.

And you try to build a picture of the collective behavior kind of bottom up from some of those interactions.

And both of them are valid perspectives, of course, but they're speaking pretty different languages.

And we think that there must be a missing link here, because if you think about us humans, I mean, I definitely feel like I am at the same time an autonomous individual and a member of not just one, but a bunch of collectives, right?

So I fluidly go in and out of companies or sports team or polity.

And of course, I'm part of my family.

So all these collectives can demonstrate intelligence behavior that cannot be explained exclusively by aggregation of the individual intelligence.

And yet the collective itself is a complex system that is composed of

components that are in and of themselves highly autonomous complex adaptive systems as well.

And that's how we frame the goal of our research.

We were looking at what we call the misting link, which is a plausible theoretical description of the functional relationship between these two scales.

In particular, we think that the missing link will be found by understanding what kinds of social cognitive features enable the formation of an effective collective within these kinds of autonomous agents.

All right.

To do that, we've been using Active Inference as a framework since the group here is probably way familiar with Active Inference already.

I'm not going to go a lot into it.

Just pointing out here that we're approaching this.

You might see the language that's a little bit different from...

from some of the canonical stuff that Fristin and team put out.

We are using some different nomenclature, but it's really the same principle.

We're all looking at an agent as a Markov blanket and trying to model exactly what's the generative distribution that

that creates behavior in that agent and look at what happens when you put it against some quote unquote true system dynamics that are happening in the environment.

And of course you run the system as an agent model.

Except that in our case, it's a multi-scale model because we're interested both in what happens at the scale of individual agents and at the global system.

For us, it's actually quite interesting to try to decouple things and look at

what's going on in a very specific way.

Modeling many-to-many relationships is really hard.

So we really focused on these pairwise subsystems.

So our model is really composed of a collection of pairs of agents.

They only ever interact with each other within a single pair.

And

We basically compose them, what we call the system scale model, bottom up from copies of these pairs of agents.

All right.

So let's look first at the individual scale where we have

each pair is operating on the same physical environment.

You can think of them as sharing one of these circles.

If you're familiar with the McGregor model that has been published in 2015, it's a very simple, intentionally

stylized active inference model that was originally designed to be analytically solvable, actually.

So it allows us to have a really clear view of what's going on on this system as a dynamical system.

And we just added some minimal capabilities to the agents in that model.

Each of the agents that are marked out as little can think of them as little parts within this circle environment.

Each of them can only do a couple of things, which is their capabilities.

They can only observe their own position.

They can't directly observe the environment, meaning they can't observe their own position.

They can only get a glue, which is an on-off sensor generated by a chemical signal on each cell.

I'm getting a ping here from a friendly viewer, also known as my wife, saying that my image may be blurred.

Now, it's good for others.


SPEAKER_00:
Yes?

It might look a little different, but I have a high resolution one saved locally, no matter what the final processed YouTube one looks like.

Thanks, YouTube.


SPEAKER_03:
Thanks, YouTube.

All right, cool.

So as I was saying, the main way that agents get information about their physical environment is by this sensory state, which is a binary, an on-off sensor.

You can think of it as being generated by a chemical signal or a kind of smell on each cell and it's emanating from this source position.

So that's pretty much it about the physical environment and agents can go left and right or stay standstill on that environment.

Now talking about the relationship between agents and their peer on a pair.

Their peer-on-peer is a funny thing to say, but the agent can observe their peer or their partner's position, relative position, meaning I'm one or two or however many hops to the left or to the right.

They can be actually in the same position as well.

And they can also observe their motion, right?

Their behavior.

you can observe whether the agent, the other agent, the partner has gone to the left driver or sits on the last iteration.

Right.

So let's talk a little bit about reward functions.

The agents have independent reward functions that are two peaks with one common peak and one private peak.

The reason why that support is gonna become clear a little bit later.

Cool.

So that's it.

That's pretty much it about the individual agents.

As I promised, it's a really simple model.

And the global scale, as I said, is number of agent pair.

You can think of like 80 copies of these two pairs of agents, meaning 160 agents in total or whatever.

And from that perspective of the higher scale, the Markov blanket is actually

composed of the collections of all of these pairs, right?

So they are each interacting with a global environment.

And the cool thing is that we're,

The way that the sensory inputs are relayed to each agent is as desires.

So you can think of it at the top level as kind of bringing in some nudges, some target states that are pretty much the sort of smells that the agent picks up.

All right, the other important thing is that the optimal state for the system corresponds to the agent's shared target only.

But so from the system's perspective, it, quote unquote, wants all the agents to be in that share target.

But the agents themselves, they don't know it.

Why it's important is that it means that there's no exogenous incentive to achieve the global goal.

So it's not like just we're nudging.

We're asking all the agents to do the same thing.

And lo and behold, they're going to do it.

All right.

And let's talk a little bit about how free energy minimization happens here.

on the individual level, on the level of the individual agent, we just assume that there's some process, some neurological process within each agent that does gradient descent.

That's the traditional technique.

But at the global level, what we're actually doing is that the behavior that we're simulating at the collective

of all these individual agents the the aggregate of that behavior is uh what we want what we would like to see uh approximate uh gradient descent so that's the thing that we actually want to demonstrate here we want to look at the behavior of the collective um calculate the the free energy the collective free energy which is f sigma here in this uh in this equation at the bottom and uh see if it's actually minimized if it's minimized then we uh

then we can say that the system, this group of very simple agents, as a collective, it's performing active inference at the global scale or not.

And shock review, what we find is that the extent to which the collective is actually able to do that free energy minimization depends on the social capabilities that we endow these simple agents with.

So we focused on two cognitive capabilities or features that are stylized versions of things that we traditionally look at in social science.

The first one is what we call theory of mind.

And what we define that as that is the ability of an agent to put itself in the shoes of another.

And we actually tried to model that in a pre-literal way.

So if you think of shallow versus deep representations of the other, you can think of a shallow representation as like a mechanistic, or if they're going this way, it means this.

A rule-based kind of way which you could think of being

internalization of a rule that i think somebody else is following uh but we actually did here is something quite different right so uh it was based on this philosophy of active inference that uh means that the agent has every agent has a self-actualization and what we said is hey uh the the self-actualization loop the internal part of it needs to be um

implemented by some kind of quote-unquote neurological circuits.

And what if exactly the same type of circuit, the same structure were being used to model the partners so that you actually also had a partner actualization loop?

So this is quite literally putting yourself in the shoes of your partner, right?

So you have your own beliefs about your position and about your desires, and you also have

the literally the same kind of belief about the partner's position and their desires and you can use that plus of course whatever sensor you have whatever information you have from the environment to predict

the partner's actions exactly the same way that you're predicting your own actions.

The only difference, of course, is that you don't actually control your partner, so you're predicting that that's the dotted line from A2 out or from A1 out in the right.

So you're predicting how you're creating counterfactuals about what your partner is going to do.

And then, of course, you can observe whether that prediction matches the outcome in the next round.

And each partner uses that to calibrate their model of their partner.

Um, and, uh, the other thing is you want, uh, those two predictions to be consistent between themselves, right?

You want to, you want the beliefs about yourself and the partner to, to have some consistency.

And that actually helps you, uh, do what we call a triangulation, right?

So, you know, uh, you know, very, maybe very little about your physical environment, but you're seeing your partner do stuff and, uh, and moving around.

And you're using that information to inform your belief about your own position.

So that's the power, really the power of Theory of Mind for our model.

And we're going to see how useful that is.

Now, the other one is goal alignment.

I now realize that I've been speaking for a long time, so I'm gonna go a little bit faster.

And so the other feature that we talked about is this thing called goal alignment.

And it's really about the ability for an agent to adapt its goals to match another agent's goals.

And this is a bottom-up incentivization mechanism.

And remember what I mentioned, that the system actually has an optimum but has no direct way to tell the agents about it.

So the assumption here is that to the extent that you have overlaps,

between the two agents' goals.

And it turns out that the overlapping goals are aligned with the system optimum that provides a bottom-up way for collectively optimal behavior to happen.

And we didn't model the dynamics of how the goal alignment happens.

We just assumed that it can be flipped on and off.

just again for simplicity.

And that's the thing that we'd like to explore further.

Cool.

So we did a virtual experiment.

We ran simulations with all four possible combinations of with or without theory of mind, with or without goal alignment.

And basically what we, and the baseline here, which is without theory of mind, without goal alignment corresponds to the scenario where these agents are pretty much blind to each other.

They're not interacting.

with them, with each other at all.

We can see each other but they're not using that information.

And we're gonna see how those features actually drive the behavior of individual agents and also the system itself.

Cool.

So basically what we find is that, I don't think I mentioned this before, but we define in each pair, there's a perceptually strong and perceptually weak agent.

The weak agent is pretty much following almost random noise that they have very, very poor smell receptors, but they can still use the partner's position motion to triangulate.

So the effect here that we see agent-wise are mostly what benefit do these social capabilities endow the weak agent with.

And so we see that the model with both together actually is the one where the performance of the weak agents consistently improves really well to the point where it gets almost as good at finding an optimum as the

as the strong agent.

And why is that?

So the basic intuition is that theory of mind by itself in this model is not able to improve outcomes that much, because remember, each agent has two goals, which correspond to the two peaks

middle drawing on the left side.

And because there's this great ambiguity, right?

If I'm going in a certain direction, it's hard to interpret that in terms of going to a specific goal if there's two possible goals where you're going.

So that limits how much useful information the agent can get from the strong.

And goal alignment itself doesn't help by itself, because now you have a single reward target, but you're still not super well-defined in where you're going.

But putting the two capabilities together actually does result in drastic improvement.

So I'll let that sink in for a second.

Theory of mind, knowing, putting yourself in the shoes of another, plus having a goal alignment, the capacity to share a goal, it actually creates benefits for the agents that are compensates, helps a lot to compensate for deficiencies

in the agent's ability to navigate its physical environment.

And that also is true looking at the collective.

So remember, we were talking about whether these models can minimize the collective free energy f sigma.

So we're interpreting these runs of

all these runs of optimization for the individual agents as part of one single Bayesian inference step for the collective, right?

And what you see here is, if we're doing this right, the graphs should look like

like gradient descent down the free energy gradient.

And this only really happens consistently towards an asymptote of zero in the fourth model where you have both of these social capabilities.

As we mentioned, you know, get some improvement.

You have some abilities from these agents that are coming from their individual.

but when you put the social cognitive together with the capabilities about the environment, that's when you get the ability to really minimize the system-free energy.

All right, so that's what we found in our paper.

And we think it's a pretty interesting and compelling example of being able to link, still in a very stylized way, but very clearly link social cognitive capabilities at the individual level that are not just simple rules, right?

This is not about simple rule following, but it actually has to do with interpreting the other and using that information.

and linking that directly to active inference at the collective level.

And where would we like to take that?

I think the four most exciting things that we've been discussing the last couple of months are, first of all, can you actually look at experiments with humans and

Because, you know, these capabilities, these social capabilities are things that you can measure.

Theory of mind and the ability to align goals are things that you can test out with human subjects.

And can you use our model to represent and simulate and contrast with what happens in groups of humans and see if differences in their abilities to

to have theory of mind and to align on goals matches up with what we're defining here as free energy minimization.

That's one idea.

Another one is collectives.

One of the things that collectives can have is behaviors around shared resources.

And these resources endow them with capabilities in different ways.

So that creates, we think that creates sort of an on-ramp to understanding the idea of economic value.

That's what we've been jokingly calling it for Sonomics.

The further down you go, the more speculative we get, but we think that

Active inference can actually help us understand this notion of where does value come from?

Is it really just related to scarcity or is actually also related to the capabilities that it's endowing with different agents, how these two things relate to each other?

So I think there's a powerful idea there.

Um, this notion of, uh, guyanomics, uh, I wrote a little bit about that.

I think it's, uh, it's been starting to pick up scene in some areas and, uh, you know, ultimately the.

All of us are embedded in one big collective, which is Gaia from Gaia Theory.

It's the planetary scale collective intelligence that sometimes works really well, sometimes it doesn't work really well.

Can we use these sorts of ideas of multi-scale active inference modeling to improve how we define and model the behavior of that Gaia system?

Finally going from the pretty big to the pretty small and individuals an individual humans brain We've been looking at Jeff Hawkins's

a thousand brain theory from his recent book.

And there's a lot of interesting ideas that look a lot like active inference, but as far as, you know, nobody's tried to model what he says is the fundamental unit of intelligence, which are these cortical columns in the neocortex.

Nobody tried to model those using active inference.

And as the name implies, the main thesis of this theory is that the intelligence in our brains comes from the collective behavior of all these

all these cortical columns interacting by a certain algorithm.

And so what would it look like to try to implement this here in the context of the brain?

What inter agent capabilities are required?

So I'm gonna pause there.

I've overstayed my welcome.

I'm really curious to hear what kind of questions or thoughts we have and yeah.


SPEAKER_00:
Thanks.

Awesome.

Thanks for the fascinating presentation.

You're muted then.

Thanks.

Sorry.

Great presentation.

So maybe we could start with just Pravnav and then Blue.

Just what is bringing you to being right here and what are your first thoughts either being involved on the work or on the collective behavior side more generally?


SPEAKER_02:
Blue, are you going first?

Go ahead.

Author's privilege.

Thank you.

Thank you.

No, I think this work in particular, I'll start with this work as a context to set up the bigger idea is a part of the thing that I'm interested in and we're interested together is figuring out one

understanding causality of where does, you know, collective intelligence emerge from, like what are the interactions between humans or humans and machines, those things.

While you can have causal theory from a management, social science perspective,

It's very hard to know that it is optimal.

Is that the outcome?

Like a statistical mechanics approach to looking at ideal gas behavior and saying that, oh, at the aggregate level, this is how the gas should behave.

But you can look at the micro interactions underneath.

So I think from my perspective, active inference is

I know this may not sound correct, but from a social scientist perspective, it is an a theoretical way of physics way of looking at the Markov blanket, looking at the statistical mechanics without worrying about how exactly does theory of mind come about.

So as Rafael explained, we had a partner actualization loop.

We think the analog in social science is theory of mind.

And what we start to demonstrate is that even if there are individual

uh active inference agents they will need some form of partner actualization and in humans you can observe that as the capability of theory of mind to be able to you know use each other as sources of information so if i cannot uh work in my environment because my physical skills like looking at you know smelling the uh different chemicals at different levels is low

but my social perceptiveness is high.

So I can look at my partner and infer something about my environment.

Then I can still work in the environment.

But that alone is not enough for us to work as a collective.

The free energy of the collective will not go down.

just if everybody can understand each other's perspective or step in each other's shoes.

So then we need something more.

So the second thing that we introduce and test out is some form of aligning goals.

There has to be a reason to find common peaks.

Again, the current model does not say how it comes about.

That is a social science type of question.

What's the causality?

Do we trust each other?

Do we have good communication processes or not?

accuracies and efficiency issues in communication themselves.

All of this is not, you know, AIF doesn't worry about that, but it tells us that these two capabilities at the individual agent level are important to get a collective outcome.

Like I think from, from our perspective, that is really the core contribution that you're saying that there are some cognitive capabilities that are beyond an individual AIF model, the way McGregor had set it up that are needed for it to show up as a collective.

Right.

There's much more to be done here, but we are starting to draw these parallels.

So the exciting thing that, uh, Rafael started, uh, you know, talk more about was can we map those, which is, uh, experimentally, if we run, uh, human agents in a similar environment.

and ask them to do a similar task.

And we have AIF agents working together.

Can we create a mapping of what is the theory of mind index, the score for this person on this scale?

Because that would help us preemptively say that, all right, if you have a collection of people with this distribution of theory of mind, their ability to understand each other's social perceptiveness, which is a standard scale in social sciences,

it's called rme reading the mind in the eye that's a standard scale so like if we can create a mapping from a real uh indicator of theory of mind onto these things we can start having some predictions that this team is likely to succeed more than this combination of people right so that starts to open up the idea all right now we have some interesting uh uh understanding of how the whole system works and can we draw it to real humans so this is very much the beginnings of a

grander research statement and research direction that we are trying to pursue and using AIF in tandem with causal theories or management or social science theories to figure out more that how can we help teams better or design things better.

Is that enough?

I didn't go much beyond that, but very happy to talk more on other kinds of holds.

Blav, over to you.


SPEAKER_01:
Thanks for that.

That was great.

Yeah, I really enjoyed this paper from my background is like biological sciences.

And so like I've done cellular and molecular neuroscience, like all the way through my Ph.D.

until I kind of took the jump off the diving board.

So for me, it's I'm not as well versed in the social science aspect, but I think a lot about collectives.

And I'm super drawn to collective intelligence and even psychology.

hierarchical systemic organization right like so how you know bodies are built right from like molecules to cells to tissues to organs etc um and even like to societies and ecosystems um like the gaia uh theory that you were talking about earlier so so this paper was awesome for me i do have like quite a bit of questions um especially at like the social science front like in the beginning of the paper you guys outlined like a whole lot of different um

you know, potential ways that we operate like as multi-system, um, multi-agent systems.

And so like a lot of them, you had theory of mind in there, um, and also the goal alignment, but you also had like shared norms and folk psychology.

And so what, what made you choose just those two as opposed to like testing all the other ones out?

Or did you try others or did you not figure out a way to like mathematically get it together?

Or, or what was your logic there?

Expedience?


SPEAKER_03:
Pranav, do you want to take that?


SPEAKER_02:
Yes.

I think we had about six months of discussion of what would be the scope of the paper.

And that's where, because it is like so many things.

As you can see, Rafael is always excited about looking at it at the level of Christonomics, at the level of Gyanomics.

But before we could jump onto any of those things, I think it was important to boil down as to what are the key pieces that are needed before we can go to specifics.

So the key piece was at this point, AIF does a very good job of explaining an individual interacting with an environment and how it operates.

So yes, we can go to norms.

Yes, we can go to these things, but these are details about how does norms emerge.

And at the end of the day, again, I could be misunderstanding some pieces of it because I am coming from social sciences to active inference.

So pardon the errors there and please correct me.

My understanding is that AIF does not give you a sort of a human level explanation.

It is the same kind of collective outcome that we would see if of cells clumping together.

We would not call it theory of mind in cells because I don't think we have a feature that says, oh, cells have theory of mind of other cells.

It could be like it is still an analogy.

So AIF is not telling you that this is the mechanism.

It's saying that you need this as a functional piece to it so that the energy minimization happens.

And that is demonstrated dramatically.

So the reason we did not go into a specific implementation was because I think that would be the next set of steps.

We first need to say that at the abstract level, at the baseline level, concept level, we need to have certain additions to an AIF framework when looking at collectives.

So one way of looking at collective that we do in the

systems-free energy, system-level free energy, is that we look at the entire system as an AI thing and plots free energy.

But that is very different from being able to say that, oh, how are these two members or these 15 members within this system interacting with each other?

What are the capabilities that these people have that is different from this whole system as an active inference system?

And that is where I think if I'm not, I don't mean to complicate this or sound vague.

I think we first wanted to establish, all right, what is the minimum mechanism that we need?

If we do that in one paper, the simplest level, then let's start digging into how does the goal alignment or norm formation come about?

So that can then be a parameter that certain teams or certain set of individuals or subgroups of individuals are better at forming norms while others are not.

you would expect these people to minimize their free energy as a subgroup better than this other subgroup.

So that would be an exciting extension in which you can go, but we didn't want to jump there because it would be doing too much.

At least that's where we were at.

Rafael, is that accurate or did I completely go off?

No, I think you caught on.


SPEAKER_03:
I just wanted to add that.

From my side, where I...

where I got to the beginnings of what we were discussing is I had this question in my mind of, you know, it takes a ton of cells to create collective behavior.

It's usually a pretty big discrepancy between the different scales at which, you know, different levels of hierarchy are

are set up, if you think about a cell and then an organ now, I'm sorry, an organ or, I'm clearly not a biologist, but even in these examples that we gave with starlings or other examples with other animals other than humans, it looks like it, like the,

the behaviors of ant colonies and so on.

It's a collective behavior, but it's really driven by the statistical, mechanical implications of these relatively simple rule-following behaviors.

Whereas the sort of, I don't claim to have demonstrated it, so I'll continue calling it a hunch,

The hunch that we had is because humans have this more sophisticated set of abilities that imply some ability to do things in a non-rule following way, meaning that they're not cause-effect relationships at the individual level.

that allows collectives of humans to be formed with fewer humans and with more of a fluid aspect, meaning the collective can also have more flexible behaviors.

Again, don't claim to have proven it, but that's where I feel like the two capabilities that we picked came from.

Because theory of mind, what distinguishes true theory of mind from just having a heuristic about how your partner is going to behave is really this ability to put what you're seeing in the world through the same

the same active inference loop as you use for yourself, which is what we call putting yourself in the shoes of another.

And that's inherently not a simple cause-effect thing.

The other one with goal alignment, even though we didn't demonstrate it quite as deeply, I think it follows the same principle of it being this kind of mutual adjustment of expectations that eventually gets you to a shared goal.

And so you feel like that has the kernel of an idea about what makes human collective special.


SPEAKER_00:
Awesome.

one comment and then I'll ask a question from the chat.

You kind of pointed out that there's a lot of modeling at the single agent level.

And then sometimes when you have a massive number of agents interacting, you can almost take these mean field approaches or statistical averaging.

And then the kind of complexity is in the middle where there's small groups and reconfiguring.

So that was really interesting.

And also just the challenge of different words used for the same phenomena across scales or across systems.

I understood it as sort of like you were saying, this is the continent of theory of mind.

These are some of the keywords or the questions, and then that's just a first connection.

And now you could reconfigure what the collective could do or do other experiments.

So here's a question from the chat.

They wrote a feature of the model is that order emerges through the bottom up self-organization rather than top down priors like most active inference models on morphogenesis.

However, the goals were pretty much hard coded into the model.

What self-organization did is only to select the collectively optimal decision among a set of possible decisions.

So I'd like to know more how the model improves on existing literature and importantly, what it says about intelligence, which is understood as the capability to understand the world rather than just optimize outcome given a set of possible decisions.


SPEAKER_03:
All right, I can.


SPEAKER_00:
Yeah.


SPEAKER_03:
Talk a little bit to that and then if you have anything to add.

I think this

This really points to another limitation presented by Time when we decided to publish this for Entropy.

We had this notion of being able to investigate how goals emerge at the collective level as well.

And that turned out to just not be feasible in our time frame.

But I totally agree with the critique that ultimately what you do want to understand is also how goals emerge.

We talk about that a little bit in the paper.

I think the one possible approach is this notion of

exploring states and as a collective and and seeing what happens to individual outcomes and if if there is an evolutionary nudge that would that would that would align incentives naturally you know from the from the perspective of the um of the various combinations of of

of individual incentives, as well as if you think about social incentives, they had these ideas of keeping up with the Joneses or however, there's a bunch of ways to model that.

And we think that given

Given that kind of evolutionary nudge towards goal alignment, we could see that emerge as a totally bottom-up model as well.

Having said that, I don't think that the model is completely top-down, given

it's not giving like such strong constraints.

And the way we try to tease apart that thing is that this notion of everybody has two different goals, but the system has a single,

single optimum state.

But it's not directly telling.

It's not directly giving incentives to the individuals to do it.

It's just like, quote unquote, by coincidence, the shared goal is also the optimum for the system.

Of course, as we showed, without a way to align on that shared goal, it's just as likely that each individual will just pursue their own private goal.

We did play around with some models where the shared goal was actually a lower peak in the reward function than the private goals.

And of course, that meant that absent goal alignment, the agents were more likely to pursue the...

the private goals than the shared goal.

And so you have a little bit of that flavor of, okay, I think we can, we can at least say something about that, about that relationship that is not just the traditional bottom-up imposing goals.

And of course they are gonna strive towards those goals.


SPEAKER_02:
You have, I think I'll jump in and add a couple of,

mostly how we arrived at this vision.

So I think, so, so it will, uh, uh, like, I think great question.

This is the kind of, we started with a complex model and, you know, evolved it into what's the simplest instantiation for the goal of this paper.

So let me walk you through a couple of different things, which we started off with and now planning ahead on.

So we first started with the idea that.

Okay, if you're looking for a minimal mechanism, we need something through which, and I'm talking in analogy in two agents, teams terms, not in AIF terms.

We need the two agents to not only understand each other's goals or distribution of desires, but also compile them.

So the simplest way we initially started with that, we gave each individual their own twin peat or three peat or how many peat distribution of coals in their environment and said, okay, you can just take them and combine them.

But what we started encountering in that piece was that should we be thinking about having an additional value?

for approaching shared goals.

That is when the peaks are combined, if my partner also has this similar peak, it should be given high value.

So there is like, there is a synergy bonus or there's a collective goal bonus that should be given that minimizes my, uh, you know, selfish desire to go after goals that is only mine, like my private goals.

So like we've played around this sort of decision space, but we found out that I think we'll have to create another parameter on

giving them how much is their preference for personal goals versus preference for collective goals.

So that's an additional parameter that can be added to the model to each individual that they have.

Uh, I forget the name that you're like selfish bias or like team bias, something that we created something of that sort, uh, and that could determine.

So you could also end up in a place where me as an agent is always going after

my personal goals, but Raphael as a team member is always going after team goals or shared goals.

That would be the distinction that we would be able to draw out.

That was not part of the scope of this paper.

That's why we removed away that thing and simplified it as everybody has only two peaks and it is very clearly known that this is the shared peak and this is the unshared peak to simplify the model and the explanation.

What you can do is make a general version of it that you can have a way of compiling them.

You can add on a parameter which can say that I have a team preference.

The social science equivalent would be my collectivism orientation or individualistic versus collectivistic orientations.

If I am very collectivistic, I'm very likely to choose a goal that is good for the team.

If I'm individualistic, I'm likely to use a goal that is good for me.

Right.

I'm not optimizing for the team.

So like those kinds of things can be built in and hopefully we'll be able to explore that further.

But we realized that that orientation by itself is not required to minimally explain the emergence of.

Right.

So one interesting piece, again, not explored in the paper was over here.

The idea was that both.

Agents should end up at the shared peak.

So they're trying to seek the common peak.

What if, uh, so that is how systems of high system performance was, uh, defined, but what if it is, uh, what you would call a disjunctive class that has, there are multiple goals in the environment.

The team as a whole should hit all the goals as against compile on a single goal.

What does that look like?

That is cooperation to achieve all the goals.

Our model currently cannot handle that.

It does not know how to coordinate separate goals that are contributing to a deal.

Over here, the assumption there is that

synchronizing or converging onto a position is what is good collective behavior.

That is not true.

As you try to do more analogies on which direction you want to grow the model, that would be an extension that one has to think about.

How are we defining what better collective outcome looks like?

That depends on the environment you're working in, depends on how the goals are broken down.

So those are the places that we would like to go.

So you're absolutely right that it

It is essentially right now an optimal decision-making situation because the simplest feature, the goal was not to make the optimization, the complexity of the paper.

The goal was to say that you need a heuristic to find the common ground.

That's it.

And here is the simplest one that we presented.

Does that make sense?


SPEAKER_00:
I think it brings us to this question about what does make the higher order group an active inference agent versus really what does make something specifically an active inference agent at any scale?

Is it just enough to have input of information and policy selection?

Isn't that just control theory more broadly?

So how do we...

Figure out who is and isn't the active inference agent at the level we designed it at explicitly, which was the lower level formally defined.

And then where really your claim is kind of building the bridge to, which is statements about collective phenomena and collective behavior.

So maybe to either of the authors and then...


SPEAKER_03:
Yeah, and I don't have a ton to add to that.

I just wanted to say that I don't have a strong background in complexity theory or even in dynamical systems.

But I was reflecting on those sorts of things, and it struck me that this assumption of gradient descent, the assumption that optimization is even possible is such an ingrained thing when you think about this kind of modeling.

Exploding that assumption a little bit and focusing on exactly this question of, is it even possible for a system that looks like this to actually, in practice, minimize this free energy functional?

Right?

Is it?

And I think, again, my day job has to do with some issues of organizational dynamics and so on.

And I think that has a lot actually to do with system dysfunctions, because the basic assumptions of

what needs to happen in order for things to fall into place are sometimes not trivial at all.

And the efficacy of a team of the collective at doing that kind of minimization, which we really understand as achieving some kind of shared interpretation of the world is really what's at stake here.

I also want to add really quickly that

uh we cheated a little bit our system is not an active the the global system is not an active interest model it's just an interest uh model uh it again came down to expedience so it's the the collective as a whole is not acting back into the into the uh environment so i think there are bigger questions there around how does that

bigger loop evolve and how does that influence the capabilities that that agents have right if you're not really trying to just to to get your system to to act optimally on the world and the levers you have are the kinds of of social cognitive capabilities that individual agents have would that select for more for more agents with more theory of mind

with more goal alignment, with a mix.

So that's the kind of things that we would have expected to see with a broader dynamical lens at the system scale.


SPEAKER_00:
Thanks.

Lu?


SPEAKER_01:
Yeah, so from the complexity

perspective and kind of piggybacking off of like the bigger system and what is the bigger system.

So I don't know if you're familiar with maybe integrated information theory of consciousness, but it really looks at like to what degree the whole is more than the sum of just its interconnected parts.

And then there's that like angle, but then there's also a recent paper by David Krakauer that talks about the information theory of individuality.

And they really talk in this paper about how in order for a unit to be a new individual, there has to be like, so, and this goes even back to like the origins of life.

So you start off as a collective of whatever molecules and then what makes you form a cell?

And so what has to happen in the integrated information theory paper, or sorry, the information theory of individuality paper is that there's this bi-directional information flow from both the bottom to the top and then from the top to the bottom, which you also see in human organizations when you get a group of individuals that align on some kind of collective goal

Like, so somehow a leader emerges, right?

Like, so everybody's kind of like, well, I don't know what to do.

And like, you know how people are, they act like that.

So then somebody will take charge and direct the group, even in an emergent unstructured way, there becomes this bi-directional information flow that makes something a collective unit.

And I just wonder, like, I just find myself questioning whether you have that, if you've built a new collective unit or not.

So, and do you think you could and how?


SPEAKER_02:
Can I jump in, Rafael, quickly?

Lou, you bring up fantastic points.

I think the way I have simplified and understood active inference is that you have an agent.

It is considered to have an active working in an active inference paradigm if it is interacting with its physical environment to reduce its surprise.

It is trying to act on it, build a hypothesis, and then act on it, as simple as that.

But when we try to move to collectives, what we are changing here, so for example, if you were to say, is this single human an active inference agent?

We ask, does it interact with its physical environment to optimize an outcome?

Then we ask, is this organization or a team acting as an active inference as a system?

You can say that, oh, this organization is interacting with its physical environment, with the resource environment, and doing things.

The distinction that we're trying to build is how do you go from cure to cure, which means when there are multiple individual AIF agents, in addition to having a physical environment, you also have a social environment.

And that is where you have to be interactive, right?

So like now your idea of theory of mind or goal alignment, all of this resides in the, from my perspective, in the social environment place, which leads to a system as a whole behaving such that it looks like it is an active influence agent.

It appears to be an active influence agent because it is able to

very dynamically work on its physical environment, but coordinate or combine its information that is coming from decentralized sources internally to get, you know, the minimum free energy type of action.

So now building from like, looking back from there, the idea of

So we don't do in our current paper, I think as Rafael said, we are not modeling the entire system as an AF agent.

What you're saying is this is simply a statistical look.

So if you look at all the different runs and compile them together, it would seem as if in most cases, the behavior of the system is such that it aligns with reducing free energy.

It is not actually actively doing things.

This is just an compilation or aggregation of the different simulation runs that we've done.

So because you're not trying to build an AI model at the whole, because at the whole level, if you were trying to build an AI model, the model would look different.

You would not have separate individuals in the model.

They will have to act as somewhere joint at the hip in which there is perfect or there's some form of information integration going on.

So the idea that I think you brought up was that in an unstructured environment, there's a leader developing and they bring it down.

Fascinating idea.

And I think the way we were thinking about this was, if we are going down the direction of when goals are merged, do we have a bonus for convergence?

Where does that bonus come from?

So I think one way of thinking about this would be that that is the leader's job.

The leader's job is to change the weightage that each member gives to pursuing a common goal.

So if left to my own devices, just carrying the previous example, I might be very strongly focused on my individual goal that is not shared with it.

But if we have a leader amongst ourselves, the leader is changing the way I weight these different goals in my perspective.

And if they're able to convince me that the common goal would be a better outcome for you.

in my type of theorization, I would say, oh, negotiation and goal alignment.

So that would be that we are talking and we are saying like, all right, let me understand what are you after?

Are you looking for status?

Are you looking for exciting job opportunities?

Are you looking for, you know, more higher pay?

Let me understand what your underlying desires and motivations are.

And let me try to get them closer to what the common goal is.

Like by pursuing the common goal, you will get X and that motivates you.

And, you know,

from like a very modeling perspective, it changes the weightage that you assign to pursuing common goals.

And that kind of individual interactions, I would not say that it doesn't have to be a single leader.

It could all be decentralized that people are helping each other understand why following the common goal is better for each one of them.

That is a discovery process.

In that sense, it is very directly looped that once a collective starts forming or once consensus start forming,

it becomes easier and easier or more profitable for other individuals to join into the group goal because there is very high chance of either success or some other resources that you get as a payout.

Those kinds of directions, we can think about doing.

Again, none of this was done in this model.

It's a fairly simplistic model to talk about

Let's think about social environment.

How do you go?

How do you bootstrap from AIF in a physical environment to what do the agents do such that it looks like at the high level it is AIF.

It is not at this point, but minimum cognitive architectures.


SPEAKER_01:
Can I ask you for power?

Yeah.


SPEAKER_02:
Yeah.


SPEAKER_01:
Um, so like in the global system, that's not an active inference agent, but like each individual agent is an active inference agent.

And then they form this like dyad.

So is the dyad like this meso at this meso level is the dyad and active inference agent also, or no.


SPEAKER_02:
I think, uh, Rafael feel free to, uh, you know,

disagree with me.

I don't think it is an active inference agent at the diet level because there is no, uh, there is no changes that are happening in how I infer about my goal states or how I do.

The only thing that is happening is

Now I have a secondary source of information.

So in the dyad, up until now, my only source of information was the physical environment.

If I was low-skilled, like I think when Rafael was presenting, like the orange line was the low-skilled person, they had a very low ability to understand where they are or are in that environment.

And hence, they were not very successful in achieving their goals.

most of the time, but the high skilled, the blue line, which came down really quickly, had a very accurate understanding of where they are on the map so they could quickly traverse to their goal.

The thing that changed was by having high social perceptiveness, now I have a secondary source of information about the environment.

My primary skill is gone, like I cannot infer from the chemicals, but I can infer from my partner's behavior.

So in that sense, there is additional information that is helping

the individual agent be a better AIF, right?

Because now they have a source of location of information that they can trust and build accurately on.

That said,

That sort of information is not changing their own weighting of their goals.

And that's why there's no top down.

As a collective, we are not trying to influence each other.

We are only taking independent decisions now assisted by a social environment also.

And that much is enough to see collective outcomes emerge.

But if you're looking for more complex scenarios in which you're supposed to make a decision of, should we break down across multiple tasks or how many of these people should do task A and do task B or understand who is good at what.

Those kinds of things will require more sophisticated machinery.

So I would not say it is a proper AIF.


SPEAKER_03:
well to that i would add my counterpoint that uh if you define a proper af model as just any markup blanket then yes the diets are are um markup blankets composed of the where the internal states are union of the two agents and uh yeah even the even the bigger collective is just a very boring uh

agent, right?

The collective is an AIF agent.

It has its inputs and it has its active state, which is the null state, right?

But, you know, proper there mean, is it useful to think of it in those terms?

I guess it depends, right?

We wanted to learn some salient facts about the collective.

We didn't quite ask that many questions about the dyad as its own unit, but we could.


SPEAKER_01:
So in the dyad formation, did you guys play with, like, I know that you did the strong and the weak, and you had, like, the strong one had strong environmental preferences, and you had some parameters that you played with, like, and the weak agent didn't have any, like, was really stupid about the environment, but, like, could lock on to the partner and infer based on the partner.

So did you ever play with parameters, like, what if you make one agent, like,

you know, weak in both and one agent strong in both or, or what, what happens?

Like, did you, did you mess around with those at all?

Is it like the scale of it?

Like as you were tuning to get like what was just right for you?


SPEAKER_03:
Uh, yes, I think we're not going to go into a lot of detail on that.

Uh, we did learn one thing very interesting, which is, uh, you could very easily replicate the phenomenon of the blind, the blind leading the blind.

Uh, we turned out, it turned out that if, uh,

If the strong agent was also very socially perceptive, it was very easy for that agent to lock into the partner's behavior and try to learn, try to infer more than was actually legitimate from what the partner was doing, which was, of course, almost Brownian motion.

And that actually, you know, it can throw a poor agent into a tailspin.

Uh, it is, it is when, when you start looking at these, uh, at these more, like, as I said, less, less rule following behaviors, they are pretty sensitive to, to, to kind of this balance of those four partners.


SPEAKER_02:
Well, that is exactly it.

I think a fantastic question blue, uh, and Rafael has captured the essence of it is.

there is a combination so like we had the entire like i was doing robustness things and like there's an entire space of what are the different combinations and how do they pan out the most striking example was when the strong agent has a very strong social perceptiveness also uh they even if they've achieved their target they start to move away from that because they're you know trying to get something they're getting confused about their own state because they think that the other member is still trying to move towards their goal state so either there must be some other goal state or like

This is me putting explanation on top of the behavior that we saw.

So like we did not see a stabilization coming up.

Now what this tells us, it tells us a very important thing that our model is not complete at all, right?

It simply says that you need these two features, but these two features are not enough to see a fairly sophisticated version of collective behavior in which there is a error correction at that level.

And from my perspective, again, and I know that Rafael and I, you know,

have been going at understanding active inference and how we define it.

And that's why I come out on the side of the collective is not completely an active inference model yet because it does not have error correction.

And how do we get there?

Like there's more nuance to be built into that.

How is it different from if you have both, all the agents in your team are very good in their physical environment.

So, so that's like having an all-star team.

We know sometimes all the all-star teams fail.

Can we bring up that kind of that niche of behavior?

Now, all of these questions are unanswered and extremely exciting, you know, potent areas to go into.

Uh, but all of that, and that's what I, you know, I come down and like, we started with norms and we thought that we should have this and we should have that, but we boiled down to, okay, what is the minimum thing that we need to say without this, nothing works.

So the collective does not emerge even in minimal circumstances.

without these two features in play.


SPEAKER_00:
If I could give one comment on that.

And it's such an interesting tension that your two answers brought out, where if the system is designed as an active inference agent, it's a no-brainer.

Yes, it is.

And then this debate, which we've seen play out

know increasingly across so many different areas okay it's a system that wasn't explicitly designed as an active inference agent whether it directly computationally emerges from a designed agent or whether it's a performer on a stage or some other social system governance and then there's the realism question is it actually quote an active inference agent versus the instrumentalism question can we use this framework

statistically to do inference and just like the distribution doesn't have to be perfectly normal in nature for the use of a t-test there's ways to use active inference very usefully so it's so cool to see like a sandbox for exploring some of these fundamental questions about collective behavior which oftentimes it feels like every single species or every system they have their own programming language or they have their own norms or

the edges are transient so they go down the rabbit hole with the math that way instead of stable edges so what's the framework that's going to have the flexibility to actually cross these different bridges within the collective intelligence or collective behavior area so it's an awesome question and i think that it goes to the heart of like look i don't i don't know if you guys have ever read um the three body problem where they have you know spoiler alerts there's a


SPEAKER_03:
and alien species and they have this big computer that's made up of soldiers with flags.

the soldiers with flags make up a computer.

By definition, it's a computer because it was designed.

They're following rules that caused it to compute certain functions.

And that's how we design computers.

Now, there may be certain Turing machines that emerge that we can sometimes even prove that they are universal Turing machines.

And that's pretty awesome, right?

And that's another way of saying that they are computers.

And I think there's the same sort of question going on here with Active Influence, where is it a Markov blanket?

Yeah.

and draw the lines around it, it's a Markov blanket.

To what extent it's performing free energy minimization depends on how you define the generative density, how you define the internal state so you can calculate the free energy functional, and yada, yada.

proof is inputting, right?

When you look at it, does it actually look like it's optimizing that free energy function?

Sometimes the answer to that is really hard to get or only feasible to get if you make some pretty aggressive assumptions, as we're saying about mean-to-yield and so on.

So at the heart, there is an epistemological question, almost an ontological one.


SPEAKER_00:
I'm going to ask a question from the chat.

This is from Dean who wrote, is there a difference between a self-actualization loop and what might be described as a sense of self-awareness?

And what is the pursuit of shared goals dependent on if common interpretation and translation such as symbolic manipulation is not generating a common context?

Thank you.


SPEAKER_03:
That's an awesome question.

I don't pretend to have any answers to it.

I'll give you my sort of opinion.

I'd also love to hear from the others.

I think my mental model of this is, hey, these agents are reasonable.

They only have, to the extent that you can say that they have beliefs or opinions about anything, they have beliefs or opinions about two things, right?

Their own position and their

their partner's position.

So they, in particular, they don't have, and these are literally just numbers from one to 16, right?

So there's two probability distributions.

In particular, they don't have anything that we could associate with the self as a concept.

right because they're only looking at two positions right now i think there's uh definitely uh it's it's not trivial to say well what is uh does self-awareness mean a symbolic sense of self or is it just the the epiphenomenon of having beliefs about some aspect of itself

Right?

I'm not qualified to judge that, but in my opinion, what the agent has is just information about a specific aspect of itself.

And it feels like without the symbolic,

without having a sort of more sophisticated causal model that has opinions about this, opinions about that, and opinions about how these other things connect.

And one of those things is the self, which representing is like a pointer to self.

I don't think that qualifies as self-awareness.


SPEAKER_00:
Awesome.

These are fun discussions.

Blue?

Daniel, you're muted.

Oh, it was on the stream, but not on Jitsi.

So Blue?


SPEAKER_01:
I will follow up to that question, actually.

So you have like this self-actualizing loop with the agent, and then you have this like other actualizing loop, right?

And I kind of looked at it like maybe your self-actualizing loop is like your IQ and your other actualizing loop is like your EQ, maybe, or either way.

But what I really thought about is like, you know, we are by nature, like such incredibly selfish creatures.

And so like, it seems that you like maybe gave equal weight in the model.

Like, can you maybe dig into like, because in reality we don't give equal weight, like we would give the self loop way more priority than the other loop.

And was that a parameter that you played with?


SPEAKER_03:
yes like i have a lot more to say but like rafael go first and then i can come but yes rafael go ahead please yeah no no i'm just gonna say yes i think the the the primary question that we were looking at when we're thinking about the these these loops if if you think about the um just the basic function of of taking some input from the

from the environment forming some belief states that are just really like a collection of numbers and then acting on it.

That can represent pretty much anything.

That's what we were... When we talk about a partner actualization loop, you can really just think of it as two copies of the same circuit.

One of them is wired,

wired out into my own actions and the other one is not.

Right, so the one that's wired out of my own actions is the self-actualization, just by virtue of the fact that it's completing the, it's the thing that's completing the markup blanket.

The other one is wired out into nothing, so it's just generating predictions about my partner's behavior.

But it has the same formal structure internally to the agent.

Now, I think the part where it gets interesting is that consistency requirement where we were talking about.

We have beliefs about the other and as the predictions that we're generating about their actions are related to those beliefs.

But I think what you're asking is really about what matters most to me, right?

Is it about...

Am I paying more attention to my actions and my goals or to my partner's actions and to their goals?

So I think the definition of matters is really about what's influencing my actions.

That is primarily about my beliefs, but they are informed about my beliefs

it's primarily about my beliefs of my position, but they're also informed by my beliefs about my partner's position, which is that process of triangulation that we talked about.

And that's the parameter, the theory of mind parameter that we were squeezing out.

So it is, of course,

You know, it's very simplified, but in principle, that's a range that can go anywhere.

I think we did play around even with having some anti-social behavior programmed in and, you know, limited amount of interpretability.


SPEAKER_02:
Yeah, I think Blue, very insightful question again.

And I think we were also thinking about the self as being IQ because it is individual skill.

and the partner actualization group being more social intelligence because it is about understanding the other person's interaction.

We sidestep that problem of

How do you weigh one over the other by choosing the space?

And again, I think this maps on very well with your previous question that, oh, what happens if both of them, if an agent is good at both, right?

We see blind leading the blind, which means that as soon as you see that diversity in agents that can be really good at both or really bad at both,

you'll need some form of waiting mechanism that to what extent do I value information that I have received from my primary skill versus I value information that I've received from my social environment.

and by social environment.

Now, as you can imagine, if you have more than one partner, you'll get, you will also get conflict information because the partners might be moving differently because they are differently abled, right?

So at that time, we cannot sidestep the question of how do I weight the social information coming in from different partners?

So there has to be a trust or a credibility index that I have to maintain internally.

So this is very close to a concept like a transactive memory system

in management literature.

where we are trying to set up that, oh, I know that Rafael is an expert at skill A and that's a credible source.

So even if I find a new team member who is equally skilled at the same skill A, I'm more likely to go to Rafael for help when I need it because he has credibility, he has some history.

So there is an index that I've maintained internally, a meta memory of mapping of who has what skills and how credible they are.

So you cannot sidestep that question as we start building and working towards bigger collectives.

So you are spot on with that.

And that will start to happen even at the simplistic level of this discrepancy can be created between my primary information source and my social information source.

played around with, oh, let's give it anti-social.

Like they always think that their physical thing is better.

This led us to having conversation like, do they have metamemory?

Which is like, do they have self-awareness that they are low skilled in the physical environment as compared to their partner?

Right.

So this all started building more complex things.

So like in my other stream of research, I build those kinds of agent based models, which are social cognitive architectures that you give them a memory.

They'll give them a meta memory.

You give them meta attention.

You give them meta reasoning, a way to map who is working with what, how much do I trust them?

So like, and that's where these ideas were flowing in from that all that complexity, we cannot sidestep that complexity.

If you want to build, you know, that sophisticated behavior.

That was not the goal of the paper.

And that's why we just removed all that.

We just sidestepped that complexity by saying, let's look at what is the minimum gains that can be had by doing X or Y. But the next step, really, if you are trying to build a more sophisticated behavior outcomes, not just minimizing system-level free energy, would be to poke in that direction.

So brilliant question.

This is actively of how we've been thinking and making

more complex and then simplifying.

This is the exact tension and interesting insights that we get there.

So very happy to, uh, you know, if you have more ideas on, oh, this would be something that would be nice to pursue.

I'm all yours.


SPEAKER_00:
You know, one thought that that brought to mind was when there's just a differential equation model, like some kind of smooth line that's gonna be generated, the model can't really fail.

Like you could fail to read in your dataset, but it's not like the line's ever gonna break.

So then people are wondering about how well does it fit and they kind of focus on the performance.

And then with agent-based modeling, it's always understood that there's actually only a few islands of parameters.

And of course the choice of which parameters to include

from which effective collective behavior is going to arise.

Like it's not just enough to be an ant forager, not every foraging strategy works.

And the one that works in the desert is not gonna work in the jungle.

So it's actually just these islands of stability.

And then you bring up all these questions like, well, what's the resilience of it to being perturbed?

Those are the kinds of questions that we care about with the real world systems.

And it's almost like the affordance from a scientific and a modeling communication perspective of having

a modeling approach like linear regression that pretty much accepts any kind of data set without really questioning, it leads to different questions that are being asked and it leads to different uses of the model.

So it's just so cool that those kinds of questions are arising and I'm sure to come up in interesting ways in your own research.

So we'll keep on reading about that.


SPEAKER_02:
Yeah, this is super exciting because that's the direction that... Sorry, did I speak over you, Rafa?

No, go ahead.

No, no, so this is...

clearly as it is communicated.

This is super exciting to me because I think a big question that we've not asked yet is what is the goal of this kind of a modeling?

There's one way to look at active inference can do X, Y, Z. From my perspective, the goal is, okay, how can we have these

social cognitive models and active inference models go hand in hand so that we understand something about real teams.

And real teams have, as you really elegantly put, they have nonlinear behaviors.

They have islands of places where they'll behave in a structured manner and suddenly it breaks because one parameter moved out of that island.

To be able to get there, we need some minimal machinery.

So I think that is the space where I live in when we, with Anita and with Chris Riedel,

And I think I've not worked directly with David Engel, but Anitha has pre-sourced with it.

That is the direction that we are pushing in in that space.

It's like, oh, let's find this island of area where meta attention and meta memory is reasonably developed, or there is diversity within the team.

Where are the stable coordination zones there?

So that is one thing.

But in AIF, at this point, we did not even have the minimal machinery that I think we could understand and interpret.

So we started at, what's the first building block that we need to build?

Confession is I don't think I completely understand AIF rules.

I think they're fluid.

Like, how do you define an AIF like Rafael was bringing up?

If it is only a Markov blanket, then yes, it is.

If it is this, then it is that.

So this could be my shortcoming because I've not read enough, or it could be a field is actually really updating itself.

So, so this is, that's why this is exciting.

Like let's build that.

But for me, my eye is always on the, like, how do we pull it out into, is this applicable?

Like we don't want only equations that can never be wrong.

In a manner of speaking, you don't get nonlinear dynamics out of it directly.

Sorry.

Yeah.

Awesome.

Thank you.


SPEAKER_03:
Yeah.

And 100% agree.

And I think this product, you alluded that and I want to pull it up.

It's really about the scientific endeavor as a collective active inference behavior, right?

To think about how...

how scientific modeling in the kind of linear paradigm works.

It's about, you know, establishing universal truths that are not contingent, you know, contingent only on a very, you know,

very specific, very explicit set of assumptions that are knowable.

And within that, you know, we pull it out.

You can explain it to an alien or you can pull it out and write it into stone tablets and the next civilization can read it and so on, right?

And that's not what we're doing here, right?

I think we're feeling our way as a collective towards a more useful, more...


SPEAKER_00:
interesting and applicable way to to understand the real systems that are around us and then to maybe to learn something to help them function better so that's that's probably working at it um i'll ask a quick question from the chat because i think it applies at then blue so dean wrote where would the authors see citizen learning organizational structures blind leading the blind or basic research at scale

No, it's not a BuzzFeed article from 2027.

Might be.


SPEAKER_03:
Maybe we're seeing a glimpse of the future.

I don't know enough to answer.

Pranav, Louis, you guys want to take this up?


SPEAKER_01:
Pranav, you're muted.


SPEAKER_02:
Yeah.

Sorry, yeah.

I was just responding to Dean's comment.

It's such an insightful comment.

One hopes that it is basically scale.

It brings up the...

thought experiment that will democracy thrive if you don't have independent thinkers right like so if it is blind leading the blind that nobody's actually thinking everybody is just based on secondary information then it would look one way and it would be the same at the AIF level it's the same behavior or it is independently thinking and basically such at scale and you get completely different outcomes in both cases so like I don't think I have good answers there but I think

very, very legitimate question, something that we can figure out.

I think that's model it.

That's model it.


SPEAKER_00:
And it relates to the discussion earlier about convergent collective behaviors versus divergent.

So whether it's like honeybees deciding on where to relocate or some other type of consensus protocol, you want convergence, but then you talked about an example where there was preference to be on separate niches.

And it's also important to have divergence like in discourse and in other areas and creativity.

So how to design that space is so critical.


SPEAKER_02:
I think, I think, uh,

You're spot on, but there is value in both of those.

See, convergence is important to get the outcome.

Like the bees have to decide on where to go finally.

But divergence is important to assess the accuracy of the information.

So to the extent, or rather to explore the space, right?

Like accuracy and exploration of the space itself.

So to the extent it is blind leading the blind, I would say that is biased towards convergence, which means the system is opening itself up to not like,

settling on sub-optimal outcomes because everybody agrees very quickly on what the solution should be.

Lack of creativity, that kind of thing.

If you are too divergent and you're not able to converge, then the system is opening itself to not getting the rewards because nobody's agreeing on anything.

So you need both.

And that's what I think the independent research and the blind-linear brand are fantastic things.

I don't think it can resolve the issue unless there is one agent that is always understood to be

they have the most accurate information always.

And the entire system understands that, right?

Like it could be like a small family unit where it says, Oh, whatever mom says is correct.

So all the children's are lined up because they are not trying to question it.

They agree and trust that that is true.

And in that case, convergence is great.

Family than mine.


SPEAKER_00:
Um, and actually to connect that to some of the analytical components of active inference in the free energy minimization, there's one term that's often framed as pragmatic value.

And then the other one is the exploratory, the epistemic component, you know, an accuracy minus complexity, these different framings that we've seen.

And so it was so cool that you described the need to have a balance of the converging and the diverging collective algorithms.

not worrying about whether it is a formal active inference at the higher scale, but just some of the same principles, which is that you have to have some target seeking or signal detecting aspect, and then some aspect that prevents at the very best premature convergence, if not actually keeping options open to remain resilient.

So those are some of the principles of collective systems that kind of go beyond even


SPEAKER_02:
Active Inference alone and just it doesn't matter at that higher level exactly what it is and so really interesting stuff yeah so if you look in the literature uh in the management so look at Megra and even collective and uh complex adapter systems from Holly Arrow like they have a book on groups as

complex adaptive systems.

And like one of the key things that, you know, in a separate line of research I build on that is there are two functions that any group has to do.

One is efficiency, the other is maintenance.

The idea of efficiency is that the resource that you have, you have to deploy very well and get the maximum out of it.

The idea of maintenance is that, oh, you need to choose the right goals so that the system can survive, right?

Because if you're very efficient, but you're going after low rewarding things, then the system will not thrive.

On the other hand, if you are choosing the right goals, but you're wasting away the resources and not applying them efficiently, the system will not survive.

So those two things together are functions of any

group or any collective.

So I think, again, like for me, it's like they're looking at the same phenomena from two different sides.

One is coming from what's the mechanics of it?

How do we achieve that?

So you look at all the social cognitive architecture, collective memory, collective attention, collective reasoning.

The other is like, does this look like an AIF and can we model it mathematically without worrying about what exactly is the mechanism, right?

So it can prove as a ground truth to

this team has way more to achieve than it is currently achieving because an ideal AIF model can be better performing, right?

And you're missing something.

So there is some losses happening in the processes, in the groups, right?

It could be, you're not understanding, you're not choosing the right strategy, or we are biased towards convergence, whatever that is, right?

Like that's something to be diagnosed about the team.

And that therein lies the practical application that you have to figure out from year to year, what could be going wrong.

So, yeah.


SPEAKER_03:
The thing that I wanted to add to that is if you look at Gaia theory and if you shake it hard enough and it looks like a dynamical system and you're throwing the ergodicity assumption, yeah, maybe from the whole...

whole system throughout it's like billions and billions of years time scale perspective and you can back out and say yeah it's a single it's a single active inference agent and therefore there is one you can back back it into like one big system goal

for all time that is only ever visible in retrospect from the perspective of like some of the timekeepers in the multiverse or whatever, right?

We hear simple agents trying to do our part, don't have access to that.

And we have to do with more proximate goals, right?

These are often gonna be at odds.

These are often gonna turn out to be wrong.

And there's always this tension between,

how you construct the goals, are you constructing them for exploitation or for exploration, at what level, how can you internalize the work of other systems around you to get towards more of a

uh, more bang for the buck for everybody and so on.

Uh, but it's all, it's, it's all really imperfect.

And I think that's what the question trying to finally tie it back to that question on, on, on citizen learning.

It's, you know, from a, from a long run perspective, probably it's to some definition, it works out.

Okay.

But what okay means and, uh, you know, in the long term, we're all dead.

It's, uh,

The devil was definitely in the details on that one.


SPEAKER_00:
Nice.

That reminds me a lot of Bucky Fuller and human's role as sense maker locally and making it work for everybody.

So it's really cool to see that kind of come back to active inference through design science and other areas.

So, Blue?


SPEAKER_01:
So I have like a small technical question and I'm just gonna load it up followed by my big question.

So Pranav, I am curious about your like social science mapping and metamapping and memory and metamemory, just like how much compute power that takes?

Like, is it monstrous?

But then I'll add a couple more things.

So I have done a little bit of agent-based modeling.

If you like need somebody to interrogate you over

your model parameters.

I'm super good at that.

So I will harass you anytime.

And then the real question that I have is maybe because of the nature of agent based modeling, maybe it's, you know, you always have to define the parameters of the system.

And so we're looking to create a model where

essentially the rules are broken, right?

Like when you make a collective, the collective has broken the rules.

It's done what the individual agents, it's not collective behavior, it's something new, right?

And so we don't know what that newness is going to be.

And so maybe agent-based modeling is maybe the wrong tool.

Like how do you allow in an agent-based model where you define all the parameters, how do you allow for this like newness to emerge?


SPEAKER_02:
Again, Luke, fantastic question.

I'm so happy to keep listening to your questions.

They're just very insightful.

I'm not sure if I've understood the spirit of your question, but I think based on what I understand, I would say the use of agent-based modeling is predicated on the goal that you have.

In a social science or social theory perspective, we are building a transactive systems model of collective intelligence.

That is, we are saying that collective intelligence emerges out of the formation and co-regulation of processes that involve collective memory.

collective attention and collective reasoning.

So intelligence is an outcome of these three functions, but these are not independent as in every person has that.

That is true, but there is coordinated.

So collective memory, which is called transactive memory in the literature, emerges out of the skill-based interactions, that coordination that people do.

Same thing with attention.

So we're building that kind of a model.

The role of an agent-based approach to this kind of theorizing is essentially to say that, okay, I have this fantastic narrative theory that people understand, are aware of each other's skills, metamemory, right?

And they use that information to make allocation choices.

That is, if I have a certain task and it requires help with X skill, who do I go to?

Or who do I assign this to in the team?

So if that is the theory,

how do we know that this is enough?

This is parsimony.

So this is the idea of generatist efficiency, right?

Like are these minimal processes enough to explain the emergence of your collective memory, right?

Like will a flock emerge if each bird, each void is given these three rules, right?

And that kind of research question or building of that model is important to basically give some credence to

These are the minimum socio cognitive processes that are important to give you this outcome.

So we are not trying to code flocking.

We are only giving individuals certain capacities and the flock is emerging.

So this is minimal enough.

Now, is that flock adaptive to, uh, let's say a predator approaching them?

Maybe not because you've not told them how to react to a predator.

Right?

So in that sense, we now know there is a boundary condition that with only X rules, it can handle high levels of workload, but it cannot handle

high levels of uncertainty, like those kinds of things.

So computationally, how difficult or how complex is the thing depends on how many features you are trying to build into it.

The model that I've developed for my dissertation and I'm presenting as I'm going on the market is fundamentally saying that at the simplistic level, give each individual agent three capacities of having skills and meta memory, understanding my team members, their skills.

having attention or focus, like ability to work on something and understand how other people are working on different things.

So like what priority that is, how important it is for them.

which leads to the meta reasoning, which is I have selfish motivations and I know that other people also have selfish motivations, but is the task aligned with each other motivation?

So I should send this task to somebody for whom this is already aligned.

So they'll get to it faster and that will benefit the team.

It's like none of this is a centralized, here's the leader who knows everything and is making the allocations.

This is not top-down command control.

This is very decentralized.

How will an intelligence at the collective level emerge to the extent these members are able to figure these things out?

That's the building of the model that's headed towards that direction.

What becomes useful is,

Under what circumstances, the useful question to us is under what circumstances is meta reasoning or collective memory important?

So this is the question that is very different from saying that, oh, do we need this parameter or not?

And how competition intensive it is.

The answer lies in, OK, what's the environment?

So as Daniel was pointing out, there are islands of operations.

Does this island in a phase space land in a place where these parameters are important or not?

So those are the kinds of questions you can answer with that.

And I think that would drive the choice of parameters.

I would not try to say that, oh, it should be an all-encompassing model.

It should be the simplest model.

If anything, I've tried to always say that we're trying to go to the simplest model that explains a part very clearly.

Does that answer your question?

I suspect it does not.


SPEAKER_01:
Yeah, so definitely like with flocking and like I've seen similar models of like fish schooling and stuff like that.

So we have that as a model of maybe collective behavior versus collective intelligence or like has something actually, has something new emerged in the flock or in the school of fish?

And so my point is, is that when you just give these agents their individual parameters, how do you know when there's another level?

Like we're looking for another level that does active inference.

So not like a yes to collective intelligence, you can eat collective intelligence and collective behavior can be modeled with a group versus the individual agents.

But when do you have another level?

Like in the hierarchy, when do you, when do your cells become a tissue?

When does your group of people become, you know, a squad or a team or something like this?

So that's what I don't know if agent-based modeling is suited to determine or not.


SPEAKER_02:
Well, that's a good question.

I think it depends on, so it's partly a definitional issue.

What do you mean by another level, right?

So intelligence, the way it is defined, at least in that literature, collective intelligence is an ability to adapt to changing environments.

So as the task changes in its complexity, that is all the uncertainty is changing or the level of,

knowledge interdependence or the level of workload is changing over time, the team is able to seamlessly adapt its internal coordination processes to maintain performance.

So it is not just because the task has changed in its complexity, so the environment is moving around, just because the environment moved, it should not be so specialized that it starts to fail.

It is able to adapt to it.

So if intelligence is defined that way, to what extent can the team

regulate internally to maintain performance at the highest level possible, then I would say, yes, it is emerging.

Because the diagram that you show, essentially the plot that you show, is a team that does not have these coordination mechanisms is failing, is dropping in performance anytime there is a perturbation.

While a team that has these mechanisms is seeing a military dip, but is able to maintain high levels of performance.

So it is a relative thing.

Now, as you change the composition,

The internal coordinations would be different because now it's a different set of people trying to coordinate, but it would not invalidate the idea that a different set of people still have corrective intelligence or not, because it's given by given our internal resources.

How are we adapting to the changes?

Like if you're able to track there.

So in that sense, I think agent based model does a fantastic job of showing you that we are able to see this emergent behavior of maintaining high level performance.

Or not, right?

And that's where you go into the mechanisms.

This mechanism is essential for this outcome versus not.

Does that help?


SPEAKER_01:
It does, it does.

But it's the question of at what scale is it an active inference agent, right?

So, like, is the team the active inference agent?

Or is it each individual's, is it like their collective active inference versus the team collective?

I can see Daniel like, oh, wait, I was just going to say that.


SPEAKER_00:
Yeah.

To speak to the question of novelty, there's the question of generating novelty and then responding to novelty.

And in the case of generating novelty, especially for humans, we haven't really talked about stigmergy, niche modification, cumulative culture.

So there's many avenues from collective behavior to more niche modifying algorithms that are

maybe explaining some of the phenomena that we are interested in here.

But then this question about responding to novelty and like, how do you deal with a threat that maybe hasn't been seen before or a failure mode that's never been observed or rare?

It kind of just reminded me of homeostasis, which is generalizing across specific situations to help you keep target variables within a preferred range.

And then

instead of just reacting to changes, it makes one move towards anticipating, which is the whole area of variously like cybernetics and signal processing.

And so those features of responding and anticipating and planning

agent-based modeling is a great place to computationally and physically test that.

And clearly active inference at the very least can be sketched out over a lot of these systems.

And then it will be a debate always like about the details and the mechanisms like you brought up, but it's just to have a framework that we can integrate scales and just nest them cleanly is at the very least a path towards answering some of these questions, if not philosophically, at least empirically.

Yeah.

Raphael.

Yeah.


SPEAKER_03:
Thank you.

Yeah.

And I think about the philosophical aspect also, uh, as backing down from, unless you're looking at the guy theory thing and you don't believe in such a thing as, uh, you know, um, the, uh, selection multiverse, uh, thing.

So if you back down to like subsystems of the big thing, whatever the big thing is,

then you can back down into functions, right?

And you can use that as your benchmark for what performance, for success, for intelligence looks like.

And we construct down and match that up with your get from the bottoms up, from the bottom up view.

And I guess the interesting question is when and where does it actually do the bottom up and the top down approach line up?

Right.

And, you know, there's, I think in general, there's no, there's no answer there from, from, from an ecological perspective.

I think there's some, some pretty clear patterns of functional specialization and, and some of them are hierarchical.

Some of them are, you know, tend to be like almost hierarchical, you know, hierarchical with a little,

little bit of sprinklings of horizontal conditions.

And maybe that's a useful philosophical approach to look at things, at least from the instantiation of all the kinds of systems that we're interested in.

It's when does a group of cells become a tissue, but it's also what are the conditions that a thing needs to meet to be

to be meeting the function of a tissue and to be, uh, worthy of being called the tissue in that, in that functional context.

Right.

And then you can talk about, well, what if that tissue is not made of, of cells, but of like the, uh, nano nanobots or whatever and so on.


SPEAKER_02:
Yeah.

I don't think it is a, it is either or type of question.

I think it is more of, uh, the two approaches of like coming at it from a mechanistic perspective versus coming at it from an active inference perspective.

are complimentary.

They're trying to look at the same phenomena, but estimate them differently.

So the idea of morphogenesis and morphostasis, I think that that's like when novelty happens, how do you respond to that novelty is part of one term in the AIF equation.

and it is also a way of looking at saying that oh will the system adapt to it or not right so there's a mechanism of how it adapts and then there is the equation version of it if it adapts well it would be reducing its uh free energy right because that is what we are able to estimate i think both of them are very complementary approaches what so one way i think i look at it is

how do you know that the adaptation from a mechanistic standpoint that is happening is correct or wrong, right?

Is it maladaptive?

Is it going to lead to a better outcome or is it going to lead to a worse outcome?

One way of looking at it is, is that reducing free energy or is that increasing free energy of the system, right?

Now, that is where you can marry the two things and work as complements that, well, there are certain adaptations because as an agent is sitting inside the system, you know, not knowing the bigger picture, there is no way you know whether these, you know,

uh variations that you're trying are good or bad like you need a system level outcome so i think that is how i think they're complementary again i don't have good answers yet because that's the area of research that you're trying to figure out but i suspect that that is where it could go you could combine the two approaches to get more insight on the both the mechanism and the overall outcome this has been great any last thoughts or


SPEAKER_00:
Anytime, you're always welcome to join again.

And it's just super interesting to hear about.

We hope that people watching it live and in replay could think about some of these questions.

But anyone is welcome to give a last thought.

Otherwise, this was just a great first appearance with you both, and we hope to see you again.


SPEAKER_01:
Yeah, just thanks.

This was super fun to read and to discuss.

And I think that the...

theory underlying the model is hot and super on point and building it out and fleshing it out into a more thoroughly developed model will be awesome.


SPEAKER_03:
Thank you.

I really appreciate it.

Thanks to both of you guys for inviting us.

Thanks, of course, to everybody who's watching this or reading transcripts or connected to the mind meld and getting an instant download of all of humanity's knowledge 200 years from now.

And yeah, I mean, as I said, I think the ultimate purpose of all of this is to be useful and it's an evolving conversation.

So we're really curious to hear thoughts, questions, other ideas that might pop up.

And yeah, I mean, I don't know if our emails are available, but if not, we can make them available or you can reach out to us on Twitter or to the

active inference labs, organizers, and they can connect us.

Yeah.


SPEAKER_02:
Yep.

Same here.

Thank you both of you.

And thank you to Sir Will and Dean for the questions.

We are, I think both of us are available on Twitter or otherwise to have questions.

Good luck to follow up on certain things, you know, and see if we can make interesting projects out of it.

Thank you folks.

It has been fun.

Bye.

Cheers.

Bye.


SPEAKER_00:
Good times.
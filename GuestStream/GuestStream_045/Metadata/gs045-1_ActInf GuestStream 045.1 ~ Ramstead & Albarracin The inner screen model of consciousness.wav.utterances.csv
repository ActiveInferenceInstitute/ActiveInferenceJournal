start	end	speaker	confidence	text
6570	36630	A	0.8914594366197185	Hello and welcome. It's June 19, 2023. We're going to have a presentation and a discussion with Maxwell Ramstead and Mal Abarasan here. So thank you, Maxwell and Mao, for joining. Looking forward to your presentation, setting some of the context, and then thank you to all of our guests. You'll have an opportunity to introduce yourself and give some background when you first give reflections. So onto the slides. Maxwell.
37130	64400	B	0.8839762068965517	Excellent. Well, maybe now and I can just briefly reintroduce ourselves for those of us watching at home. So, hey, folks, my name is Maxwell Ramstead. I am the director of Research versus AI. I am a scholar in the tradition of active inference and an expert on the free energy principle. Yeah. And mao. Would you please?
65570	100540	C	0.9336953846153844	Sure. So. My name is Malva Hasan. I work at Versus with Maxwell. I'm Director of Product for Research and development, but I'm also doing a PhD in cognitive computing. I worked in social sciences and in artificial intelligence in the artificial intelligence field. And I've specifically tried to apply active inference to more socially grounded questions and sometimes more abstract and representational questions, diving all the way down to the philosophy that can be attached to it.
103180	288710	B	0.9269944131455402	Thank you, Mal. So, yeah, I'm very pleased to present this work. Today, Mao and I will be discussing our new preprint called the Inner Screen Model of Consciousness. And it is about basically an application of the free energy principle to the problem, the conundrum of consciousness. So just to give a little bit of background and now I'll just be walking through the slides, if at any point you want to jump in and add complete correct, please feel free to do so. This is work that started in late last year and continued throughout the year. It's preoccupied us quite a bit, actually, over the last while. Mao and I are the lead authors on this, what has now become a series of papers, with one more preprint coming out, hopefully before the end of the month, and another in the works, hopefully before the end of the year, with the same core group of authors. So obviously Mao, as well as Adam Saffron, Alex Keefer, Brendan Klein, Chris Fields and Carl Friston. And the follow up papers include a bunch of other authors. So we've been working with some pretty cool folks on this, folks listed here and others. Yeah. So the overall aim across the two or three papers is to present something like a minimal, unifying model of consciousness premised on the free energy principle. In the paper that we're discussing today, we are presenting, as I was intimating a moment ago, a model of consciousness that follows directly from applying the free energy principle to well understood human neurophysiology. The take home message of today's talk is that, well, let me take one step back. There has been a lot of discussion in the literature over the last decade about whether the free energy principle has anything unique to teach us about consciousness. And there have been responses pro and contra. And I think that the question up until recently was just unsettled. We would like to suggest a positive response to that question. And as such, one of the take home messages is that actually, yes, a model of consciousness can be directly derived from the free energy principle applied to explain known neuroanatomy. And I guess the kind of technical message, the technical part of the takeaway is that consciousness has or is isomorphic to a nested holographic mathematical structure. So this sounds like a mouthful, might sound a little crazy. We are going to unpack this presently. Now, did you want to add anything to that?
289560	291240	C	0.9997666666666666	No, all good.
291390	911416	B	0.9322483522727288	Excellent. So the original paper that we wrote turned into, like a 24,000 word monstrosity of a paper, and it was elephantine both in the sense that it was very large and also in the sense that of the soupy parable of the blind men and the elephant. What we're trying to do is in some sense, identify a parsimonious mathematical structure that underwrites the different accounts of consciousness that have been developed based on the free energy principle. I'll spare you the original paper structure. I said a moment ago that we were developing a minimal unifying model. This is a notion that was proposed by our friend and colleague Vanya Visa, I believe, a few years ago now, in 2020. And Vanya at the time was noticing, well, the field of consciousness studies is replete with a bunch of different models and theories, and there seems to be no end in sight to the multiplication of these theories. And the idea is maybe what the field needs is not one additional N plus one theory of consciousness, but rather a minimal unifying model by which Vanya means a model that specifies only necessary properties of consciousness. So this doesn't come with, like, a strong sufficiency claim, really, like, what are the kind of rudimentary building blocks that are necessary for consciousness in general. This model would have determinable descriptions that can be made more specific. So it's kind of a base model that can be expanded to cover different varieties of conscious experience. And finally, that it's minimal and unifying in the sense that it integrates current approaches to consciousness in no small part by highlighting their common assumptions. So kind of pointing to a least common denominator of all existing accounts. And so what we have tried to do in this paper is to well, in this series of papers is to engage in this analysis and this mummy analysis, if you will, for theories of consciousness premised on the free energy principle directly. So it's worth quickly discussing the free energy principle. I'm almost embarrassed to do this at the Active Inference Institute, but it's always worth going through this very quickly. So there's a standard distinction in physics that one can leverage between dynamics, mechanics and principles. The free energy principle, as it says on the tin is a principle. So principles are kind of the foundation of a hierarchy of theory building in science, as it were. Top of the hierarchy are formal descriptions of behavior what are known as dynamics. So if you're applying dynamical systems theory to understand the time evolution of some system, you're in the realm of dynamics. Arguably, folks like Kepler and Galileo were also in the realm of dynamics. So I see some empirical phenomenon and I produce a formal description of that behavior. Mechanics comes into play when we move from Kepler and Galileo to Newton, when we move from merely describing some behavior to providing equations of motion that allow us to explain why that behavior has the particular shape that it does. So classical mechanics, for example, gives us an account of gravitational force and explains why orbits are shaped the way they do. So if galileo and kepler just noticed the shape of orbits and provided a formal description of them, newton explains where that shape comes from in some sense, and principles, in turn, explain where mechanics come from. So classical mechanics come from the principle of least action or stationary action, which, very roughly speaking, is the principle according to which no more or no less energy is used than necessary to perform some physical movements. That kinetic. And potential energies balance out such that the true paths of a system through its state space are those for which that balance equals zero. The principle of least action explains where classical mechanics comes from. In some sense. And similarly, the free energy principle is at the basis of another kind of mechanics that is becoming known as Bayesian mechanics. You can think of the free energy principle as having the same relation to Bayesian mechanics as the principle of least action has to classical mechanics. And Bayesian mechanics is the physics of probabilistic beliefs. So it's a physics that connects the properties of the system that you're considering the physical properties of the system. In particular, the information entropy of states of a system to the connects the thermodynamic entropy of the states of a system to the information entropy of the beliefs that a system has about the systems to which it is coupled. Here. A probabilistic belief means a probability density in a very general sense. We're not assuming some kind of contentful motion of belief like you might find in philosophy. Really. This is about kind of a tracking relation. But we'll get that to that in actually, just a few slides. There are two main formulations of the free energy principle in the literature. The classical and the quantum. Both rest on this apparatus of the Markov blanket. The Markov blanket definitionally is the degrees of freedom that separate. But couple two systems. Or rather, two things within a larger system. It's effectively a statistical boundary that allows us to say that, well, given this boundary, the inside is independent of the outside. The way that we express the dependencies. So taking one step back quickly, this is a dynamic systems approach to cognition, to self organizing systems, and to physical systems more generally. It is a dynamical systems approach in the sense that we're using the tools of dynamical systems theory to examine the time evolution of a system, its trajectory in its state space. And in this context, a generative model is used to encode the relations of dependence that obtain in the time evolution of the system. So this is important and I've been kind of making this point maybe ad nauseam for the last, I guess, half decade. But the generative model is not a model that you have in your head. It is a statistical model that represents the conditional dependence structure of the entire system that you're considering. So this is an environment agent system in the broadest sense. And the free energy principle says that if a specific sparseness structure obtains so if things are disconnected in a special way, then it'll look as if the subsets that are disconnected are tracking each other. So more technically, we partition our system into particles. Particles are basically internal states shrouded behind their Markov blanket, where the Markov blanket is composed of sensory states and active states. Sensory states affect internal states but are not affected by internal states and active states affect external states but are not affected by external states. This separation of causal dependence and this structure intervening between the two bulk subsets of the system is really critical. The free energy principle says if my generative model, which encodes all of the dependencies of the system, if my generative model contains a Markov blanket in the sense that we just defined, then we are basically licensed in our interpretation of internal states as basically tracking external states where tracking has a specific mathematical definition. In this context, tracking means to encode the parameters of some probabilistic belief about external states. So to the extent that we want to call this representation, we should note that this is not a representation in the typical tele semantic sense of having some internal states track some external features. What we're saying is that basically if I have a Markov blanket, then my internal states are basically shaping a belief about the external world. And I think that's really critical to understand. And this is where the inference and the modeling bit comes from. This kind of tracking relationship, formally speaking, is approximate daisy and inference. So basically, in a nutshell, the free energy principle says in our physical universe as we understand it, with the usual mechanics running in the background, right? Classical relativistic, quantum and statistical, if boundaries exist in the physical system, then the bulk across the boundary is going to be tracking whatever is beyond the boundary. So in a nutshell, this is what the free energy principle says. Now, I noticed you put some stuff in the chat. Would you mind using your voice to compliment.
911608	944744	C	0.9410472727272727	Yeah, I was just saying that this part that you were just explaining often seems kind of esoteric to people. But it's critical to understand the sparseness of the coupling because then you kind of start understanding where perspective comes from. And if you have perspective, you also have the capacity to assess causality starting from a point and extending outwards to something else. And this is true from any so that's what I wanted to add.
944942	1292968	B	0.9358012799043082	Thank you, Mala. I think that's useful addition. Yeah. So this is the classical formulation of the free energy principle. There is a new hip funky formulation that has become available over the last half decade due to Chris Fields, Jim Glazebrook and their colleagues, the so called quantum information theoretic formulation of the FEP. Fear not. This doesn't appeal directly to quantum mechanics. Rather, it appeals to the tools that have been used to extend information theory in the context of quantum mechanics, which allow you to calculate like all sorts of funky wave equations and propagation operators and all that. We don't have to get into the details. Basically, it's just a way to reframe what I just said. So what I just said is if there's a boundary in my physical system, then the things across the boundary will look as if they're tracking each other. Another way of saying that is, well, this boundary is a kind of screen green takes on a particular sense in quantum information theory. We don't necessarily have to get into the details of it, but basically you can think of these degrees of freedom that couple the two systems as a kind of probabilistic surface and you can think of the bulk across either side of the boundary as in alternation reading and writing information onto the blanket. So any Markov blanket whatsoever can be construed as a classical information channel or a screen and in effect, all of the classical information that you need to describe the way that things across the boundary. So the so called bulk of the system, couple to one another, is contained on the boundary. It encodes the classical information that's necessary to describe the couplings within the system. This is holography. This is the hologram. Not in the sense of Holography. Although we might discuss senses in which these things are intertwined, we're using the word Hologram in the sense of the Holographic principle from physics. The Holographic principle is a principle that was originally discovered in the context of black hole thermodynamics. And what it basically tells us is necessarily from the point of view of an external observer. Some bulk in physical space can only contain as much information as can fit onto its surface. So this echoes what we were just discussing about Markov blanket. The physical reason is that if a bulk collapses into a black hole and the bulk contained any more information than could be fit onto its boundary then we would basically be losing information. And that would violate the conservation of classical information also known as the principle of unitarity in quantum mechanics which basically says that information is never lost. So we're using the word Hologram in that sense. This is different from the sense of Hologram everyone has seen, like the Hologram of Tupac. Or does that make me look seem old at the Super Bowl a few years ago? Or maybe Pokemon cards, maybe that also makes me seem old now. I don't know what the kids are doing, but you've seen like red baseball cards and stuff like that with different superimposed picture layers that become visible as you move. That's not the sense of Hologram that's at stake. Although maybe so we'll get to why, but we'll focus on the Hologram theoretic formulation. So, to summarize, in the Holographic principle version of the free energy principle, the quantum information theoretic version of the principle, all of the classical information that I need as an external observer to characterize the coupling between two subsystems of a larger system exist on the boundary. And that's a pretty remarkable physical fact. And a few years ago, Chris Fields, Jim Glazebrook and Mike Levin proposed that maybe this kind of information concentrating bottleneck architecture also has application to consciousness in the form of an internal screen. So the idea was that the information that I bring to bear, to parse my sensory stream and make sense of what is constantly bombarding me, it has to live somewhere. And so the idea is, well, it lives in this inner screen. So you basically have an external Markov blanket, which is a Markov blanket just in the usual sense. And then the idea is well within the set of internal states that are kind of partitioned off from the organisms due to the organism's Markov blanket. Then you would have an internal Markov blanket structure within those internal states. This is prediction six from their great paper. Minimal physicalism internal awareness requires internal boundaries. And there's this interesting connection to integrated information theory I am agnostic with respect to IIT, so we're not going to go there today. But for those of you who are interested, this kind of inner loop mechanism seems to generate positive integrated information. So there's an interesting connection there. Before I move on to Neurophysiology, I'll hand things over to Mao. If you wanted to add anything.
1293134	1311470	C	0.8969572972972972	Yeah, actually I was literally about to type something that this is actually also connected to some neomaterialist formulations of folding onto itself. So maybe this is something worth digging into a little later in the talk.
1312480	1938916	B	0.9222862737127401	Awesome. Then we shall. And I believe that Ali is also a co author on the paper and we have him. So congratulations, by the way, on the preprint. I saw it released and Mao has told me about it. I'm really happy to see such serious philosophical work being done with the free energy principle as the basis. I think you should be very happy with the end result and proud. But now let me get back to my monologue. So rather our Mao and my diet. It's not really a dialogue. It's sort of like I don't know how to call it. Anyway, I'm thinking in French. I have the words in French, I promise. So now that we've, you know, reviewed in a very much too short, way too brutally fast fashion the free energy principle and its core formulations, we're now going to derive a model of consciousness by applying it to the known neurophysiology of the brain. Are you ready? You buckled up? Okay, good. So we have just said that the FEP can be used to model things that have boundaries. I submit to you that the brain is a thing with boundaries and importantly, a lot, many, several internal boundaries. So in particular, we know that the brain has a very sparse structure. So if you think of just the raw numbers, right? There are roughly on the order of 100 and 5160 billion neurons in the brain. But each neuron only makes about something like, I want to say five figures, because I've spent so long, ten to the five connections, right? My neurophysiology is a bit far behind me, but it's something on the order of a few thousand to maybe tens of thousands of connections per neuron. So obviously, if you just think about it, most neurons are not connected to each other, not directly. And furthermore, the brain has a hierarchical structure. And when you think about it, what is a hierarchy? A hierarchy is a sparse connectivity structure, right? So if you think about a hierarchy of connectivity, what having a hierarchical structure means, at the end of the day, this isn't some statement about power dynamics where things at the top of the hierarchy are more important than things at the bottom. Your cortical neurons don't drain more juice than other parts of the brain. The neurons in those parts. What we're talking about is a regular sparse pattern of connectivity, where I only make connections to the layers directly below me and to the layers directly above me. What I want to suggest now is that you can understand the sparseness as a set of nested Markov blankets, and that's going to do a lot of heavy lifting for us. So the discussions here are based on two papers. One by Fields and colleagues called neurons as hierarchies of quantum reference frames. It's really cool. I recommend it. And also a paper by Inesh Ipolito, myself and our colleagues on Markov blankets in the brain. So this is going to be a much too short summary of those discussions, but basically, you can start with individual neurons and then talk about their Markov blankets to bring it back to something that I was saying earlier, but that I didn't emphasize while we were going through the slides. The Markov blanket identifies dynamical dependencies, not necessarily physical boundaries. So it's a statistical boundary it's not necessarily like the boundaries of a wall, like a cell. In the case of formalizing individual neurons with Markov blanket we would associate the post synaptic conductance with the internal states, the post synaptic voltage with the active states, the presynaptic conductance to external states and the presynaptic voltage to sensory states. You have some equations of motion that tell you how each of these quantities change and what they depend on. And you'll notice that this has the dependency structure of a Markov blanket where the internal states only depend on sensory active and internal states, the active states only depend on internal states, and so on. So it respects the dependency structures that we were discussing a bit earlier, where again, the sensory states can only depend on other blanket states and external states and so on. So here we have a nice dynamic Markov blanket that we carve around our neurons. And then there are two directions that we can go. We can ascend the scales towards canonical microcircuits and then cortical columns, individual brain regions, whole brain networks and so on. And at each of these successive scales of self organization you can similarly identify Markov blanket. If you want to discuss this in more detail, you can go see the paper by Inesh and myself and colleagues. You can also go from the individual neuron down towards gated channels, towards synapses dendrites and the soma. So the body of the cell, all of these things have successive Markov blanket. And so all throughout the brain's organization at the different scales at which itself organizes. You have this nice structure of nested Markov blankets, of Markov blankets? Of Markov blankets. So that's one clear immediate sense in which there are these blankets. And you can start to think of these blankets as screens in the way that we just were talking about. A slightly more illustrative example of what we're talking about here is Markov blankets and predictive coding. This is a figure pretty typical in the predictive coding literature. What it tells you is that the brain has a series of levels or layers, where superordinate layers are basically passing predictions about sensory input onto subordinate layers and subordinate layers are shuffling prediction error back up to the superordinate layers. This is probably like I'm almost embarrassed again to discuss this at the active inference institute. I'm sure everyone is quite familiar with these things already. The novelty that we are introducing here is to say well, each of these layers is connected to each other via a Markov blanket. And so that's I think like a key insight is that each of these layers has internal states and communicates to other layers through what you can formalize as a Markov blanket. So you can understand at any given scale of brain activity, you can understand the different levels of message passing as successively nested screens. The cool thing is that what each of these screens is doing in some sense is course graining, the stuff in the screen below. So in some sense each screen is kind of implicitly contained in the following one. And what you end up having is a kind of nested structure of screens of screens of screens of screens, which is, I think, pretty cool. We recover, as you'll note in this figure, the inner screen hypothesis. Each of these, this is probably a better way to visualize it. You have at the left the kind of external Markov blanket of the organism. It is segregating or carving out a set of internal states. And what we're saying is these internal states have a layered level structure, a sparse structure that we can formalize using nested Markov blanket. And like I was saying, each screen successively coarse grains the next screen. And this gives some flesh to this notion of contextual computing, this idea that really what the brain is, is an organ of context where each layer is providing context to the layer below and where each layer is attuning to increasingly higher order, slower and more physically widespread regularities. So one thing that we're proposing here, which is a novel hypothesis that we intend to test empirically over the next few years, is that this architecture has two special layers. In some sense you can think about it as write only layers. So if you're thinking about the stack, the sensory periphery or the sensory bottom, in one sense can only write in one direction, it can only write into the system and the topmost level can only write downwards. This is a key architectural feature. One thing to point out is that in the paper we associated the topmost level to structures like the hippocampus. We don't need to suggest that the hippocampus is uniquely the top of this hierarchy. Rather there is probably a whole bulk of structures that basically cannot any longer be coarse grained. So you kind of have like an executive level top. And what's particular about this top is that it can only write down into the stack. So its function really is to be about and monitor the other layers of the stack, implementing a form of COVID action which trickles down throughout the stack. So now I saw you write some stuff. Would you like to add?
1939018	1977730	C	0.9132959090909089	Yeah, so I think one of the things that's important to highlight is why some things can only write up and some things can only write down. And what this means for irreducible, the irreducible mark of blanket, which is the part where experience happens because there's a counter that and some future questions that are going to happen that relates to superstructures and then why don't we experience the consciousness of a superstructure? So I think it might be important to dig into that a little bit.
1978360	2105364	B	0.9194328838951311	Absolutely. Thank you. So, yeah, to summarize then, there are basically two, right, only layers. As you ascend the layers which are successively coarse graining each other, there is an irreducible screen that can't further be coarse grained. We believe that this corresponds to something like a naturalized Homunculus. And we're calling this perspective with an intentional provocation. We're calling this perspective Neocartesian in the sense that we may have in some sense identified the processes that make it seem as if there was something like a Homunculus. Obviously we're not, or maybe not. Obviously it's worth saying we are not falling into the Homunculus fallacy because we're not claiming that the topmost layers perceive themselves. So the buck stops with the top layers and the top layers can only perceive themselves vicariously by acting through other layers. And we associate this action of layers of internal Markov blankets on each other with attention in particular and other neuromodulatory effects. So I mean the overall picture is one where consciousness is intrinsically related to overt and covert actions, to the overt acting and sensing associated with the external Markov blanket and with the COVID actions that are being deployed within the stack and in particular that are issued by the very top set of levels. As I said, this is not necessarily a strict hierarchy. It could be a hierarchy with several different tops. But this integrated top structure we believe could be associated with a naturalized Homunculus. I'm going to skip through the rest and thank you for your attention. Unless Malana wants to add something.
2105502	2151370	C	0.9440092592592592	Just one thing is, I think to put it in simpler terms as well, the two layers that can only write can't store memories. Because that's right, memories is not just an encoding, it's a process of retrieval. And it requires interactions between different levels, which is why the lowest levels are only interacting with things which are external and therefore can't necessarily store and can't make direct interactions with anything else within the entity itself and the highest level as well. So there is no memories being encoded at these two levels, which is why they're right only and not. I think that helps a little bit.
2152780	2169280	B	0.9384275675675676	Thank you. Yeah, I would agree. So thank you all for your attention. Thank you in particular, Dan, for the invitation. It's always great to be at the AWI and yeah, we looking forward to this discussion.
2174960	2183084	C	0.9441321428571429	I think Daniel said he wasn't going to do too much animation, so if.
2183122	2187410	B	0.9547933333333334	People have are you going to do any facilitation at all then.
2195400	2197348	C	0.779345	Yeah. Okay.
2197434	2247430	D	0.8908026530612249	So thanks Maxwell and Mao for the amazing talk. There are so many interesting things to dig on in this paper, I don't know even where to start. But I do have one question too, actually, in particular. So does this inner screen model imply a kind of protopenpsychism or a more restricted bioppsychism? I mean, do you think the type of nested Holographic structure described by the model all exists in biological systems? And regardless of the answer, would you think that it generalizes to larger systems comprising these systems, like populations and colonies and so on?
2247960	2377180	B	0.932709131832797	Maybe I can take. A quick stab at that. So we think that the free energy principle itself commits you to a kind of I don't know what the right word is. We're writing a paper now. We're struggling with the exact coinage, but it's not quite a pen psychism. It's a pen representationalism or a pen tracking ism, if you'll allow me, the absolutely disgusting neologism. Basically what the FEP implies is that anything that has a boundary, it looks as if it's tracking what's beyond the boundary. So at a very basic level, the FEP kind of licenses this talk about anything. So anything whatsoever looks as if it's tracking what's across the boundary by virtue of existing. So at a very basic level, if I see a rock outside, I know that it's not 1000 degrees Celsius outside. Right. The fact that a rock has a specific structure entails something about its environment. Likewise, I can see you, Maria, you are not wearing a winter parka. And I can infer from that that it's probably room temperature around you and not like you're not somewhere in Antarctica or wherever else. So this kind of basic tracking relationship applies to everything. What we've tried to do in this paper is to start spelling out more explicitly what the difference is between just rocks and rock stars in some sense. Like, what is the special kind of structure that you need to add to the Markov blanket? And what you'll notice is that you get consciousness by making the overall structure more sparse. So as you make more of the connectivity zero, you get all of these weird things that pop up, one of which is we think like this consciousness structure. Mao has thought about this maybe more than I have, so I'd be curious to get her perspective.
2377780	2484370	C	0.919296403508771	Well, so I know that people are a little bit wary of panpsychism, so we usually add the term proto in front of it. And so the idea is that we have to deconceptualize what consciousness means. And oftentimes what we mean is something that resembles what humans do. And we have to be a little bit if we take our formalism consciousness can become emergent at the scales where humans are. But it doesn't mean that it doesn't exist at lower scales, where the inertia or the entropy generated by policies is lower. So if you think of a rock, its policy is pretty much maintained bonds. You know what I mean? There's nothing else. The scale at which some kind of thing exists and changes is much different than you deciding to have coffee. But in effect, your cells could be something that we could consider exist, closer to something that a rock does and even lower just the proteins that make up your system. So if we understand that there is nestedness and that the conscious process happens once you have this capacity to exchange information across layers, anything that has the stacking of layers in a nested fashion is in effect, proto conscious. But we have to be very careful about what we load into. The term conscious, I think, is mainly the question.
2485300	2632160	B	0.9413384911242603	I'm sharing my screen here. This is from a paper on path integrals, particular kinds and strange particles. So it is a typology of different kinds of Markov blankets and different kinds of particles. So something like a rock, it still has a Markov blanket, but its Markov blanket is not partitioned into sensory and active states. There are no states that have this property that active states have that they affect the external world that are not affected by it. In return. A rock just has sensory states. If I kick a rock, unless it's radioactive, it's not going to act on me. It only has the inference part of active inference. It's only kind of changing its state as a response to the forces that are acting on it. Active particles are particles that have both sensory and active states. And you might think, for example, of a cell. A cell has this kind of structure. Particles like ourselves have additional sparseness in the Markov blanket, in particular, what we call strange particles. The internal states of strange particles do not have direct access to the active states of strange particles, and therefore they need to be inferred. And this is what gets you planning. At the end of the day, a strange particle can only exist by kind of inferring itself into existence precisely because there are no direct connections between the active and the internal states. So as you remove connectivity from the overall structure from here to here, you're adding sparseness. From here to here, you're adding sparseness. As you make the system more and more sparse, you get more and more interesting behavior to emerge from the system. And you can think of the topmost structure that we were describing as an instance of this broader architecture where the topmost layer doesn't have direct access to the active states of the system. And therefore, it kind of has to go through the entire bulk of the system to infer itself into existence.
2635420	2647390	C	0.9364567857142856	So there's a few hands up there on Google where you can see the people who share hand. Okay. So, Ali, I think you raised your hand first.
2650400	2712480	E	0.9293424999999995	Yes. Thank you again for your crystal clear and really illuminating talk. I've written down three questions, but maybe I'll begin with probably the most basic and naive one. So in some of the earlier literature on active inference and the FEP, there was this emphasis on distinguishing between generative process and generative model. But in this paper, in figure one, which you also shown in your slides, we don't see any generative process in the diagram. So where does generative process fit into this diagram? Because the way I see it, it's basically somehow included in the generative model or I don't know. What is the distinction between these two concepts?
2713060	2962028	B	0.9161620588235293	So you'll have to forgive some of the earlier literature for this, there's some terminological inconsistency. The only two constructs that are at play here are the generative model and the variational density that's encoded by internal states. That's it. So it is not the case that the FEP is about how the generative models the generative process. A lot of this language comes from previous Bayesian approaches and in particular, the helmholtz machine. In the helmholtz machine. So helmholtz machines are some of the earlier predictive coding machines. And in the helmholtz machine, basically you have a forward pass that connects the sensory end to basically the top of the predictive hierarchy and then a backwards pass connecting top of the model back to the sensory end. And in a helmholtz machine, you would call the forward pass a recognition model and the backwards pass a generative model. The forwards passes a recognition model because you're inputting some data and then basically by the time the information trickles up to the top, you have recognized what caused the data. And the backwards pass is called a generative model for the reasons that are sometimes invoked in the predictive coding literature where you're basically generating possible sensoria given a certain configuration of states. So we don't use recognition and generative in that sense anymore. And I apologize. In some previous publications, I've been a bit sloppy in the way that I use my language and I've used the term recognition density. What's really at stake is and I'll share my screen just to help make the point. It's what it says here. So you have your generative model, right? Your generative model is a joint probability density. So what is a joint probability density? Well, you have a bunch of different basically states that your system is defined over and you have a probability density that the shape of which captures the way that states can change over time. So, like, if I change this way, then I need to change in that way. And that's what this shape encodes. And what the FEP says is if my generative model has a certain sparseness, if it has Markov blanket, then there's this additional probability density that I can define. It is parameterized by the internal states of the system. It's called the variational density. And this is like my probabilistic best guess about what's causing my external states. At no point do you need to appeal to any other bits of generative process when you're actually coding these models up in MATLAB or in Python. What we call the generative process is just the equations of motion that govern external states. So if you actually hand code one of these one day, your generative process might include like, Newtonian physics and whatever else. It's sort of just like what is the process that actually animates the dynamics of external states? In some very simple models, like in the ones used by Miguel Aguilera and colleagues, you're looking at linear dissipative systems and they don't have any dynamics right. So basically, the system kind of just dissipates. A linear dissipative system is a dampened spring physically, right? So it just goes spring and then it dissipates to the fixed point. So similarly, you may or may not have dynamics to the external states and that's what gets harnessed in the generative process. This should be clear. Yeah. Sorry, ma'am.
2962124	3029392	C	0.9252248360655734	No, I mean, I think what the question that Ali is asking also has some important correlates that often get asked to us, like what's the difference between consciousness or dreaming? Or what's the difference between a dead system that decays or being psychotic? And I think what Maxwell is saying is our system currently doesn't say anything about your actual connection to the real world other than a thing, which is successful at being a thing, is probably well joined relative to its needs to the world. And therefore your relation to reality has probably got a better grasp if you are functional for a longer amount of time, this minus senescence. So yeah, does that kind of help also, Ali?
3029456	3037110	E	0.917406875	Yes. Thank you both. It really makes a lot more sense now. Thank you so much.
3037480	3060890	B	0.9265127083333334	Well, it's my pleasure. And remember, model just means joint probability density here. Right? There's a criminally underappreciated paper that Carl wrote in Entropy in 2012 called the Free Energy Principle for Biological Systems, which is where a lot of the Markov blanket story comes from. And in.

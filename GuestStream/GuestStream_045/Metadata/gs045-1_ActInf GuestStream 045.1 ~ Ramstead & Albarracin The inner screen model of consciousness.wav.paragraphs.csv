start	end	paragNum	speaker	confidence	startTime	wordCount	text
6570	26482	1	A	0.99868	00:06	50	Hello and welcome. It's June 19, 2023. We're going to have a presentation and a discussion with Maxwell Ramstead and Mal Abarasan here. So thank you, Maxwell and Mao, for joining. Looking forward to your presentation, setting some of the context, and then thank you to all of our guests.
26586	44586	2	A	0.9775	00:26	39	You'll have an opportunity to introduce yourself and give some background when you first give reflections. So onto the slides. Maxwell. Excellent. Well, maybe now and I can just briefly reintroduce ourselves for those of us watching at home.
44768	63194	3	B	0.99765	00:44	37	So, hey, folks, my name is Maxwell Ramstead. I am the director of Research versus AI. I am a scholar in the tradition of active inference and an expert on the free energy principle. Yeah. And mao.
63242	70686	4	B	0.99974	01:03	16	Would you please? Sure. So. My name is Malva Hasan. I work at Versus with Maxwell.
70718	100540	5	C	0.48287	01:10	65	I'm Director of Product for Research and development, but I'm also doing a PhD in cognitive computing. I worked in social sciences and in artificial intelligence in the artificial intelligence field. And I've specifically tried to apply active inference to more socially grounded questions and sometimes more abstract and representational questions, diving all the way down to the philosophy that can be attached to it.
103180	140890	6	B	0.99987	01:43	85	Thank you, Mal. So, yeah, I'm very pleased to present this work. Today, Mao and I will be discussing our new preprint called the Inner Screen Model of Consciousness. And it is about basically an application of the free energy principle to the problem, the conundrum of consciousness. So just to give a little bit of background and now I'll just be walking through the slides, if at any point you want to jump in and add complete correct, please feel free to do so.
142220	184128	7	B	0.99998	02:22	102	This is work that started in late last year and continued throughout the year. It's preoccupied us quite a bit, actually, over the last while. Mao and I are the lead authors on this, what has now become a series of papers, with one more preprint coming out, hopefully before the end of the month, and another in the works, hopefully before the end of the year, with the same core group of authors. So obviously Mao, as well as Adam Saffron, Alex Keefer, Brendan Klein, Chris Fields and Carl Friston. And the follow up papers include a bunch of other authors.
184224	224512	8	B	0.55667	03:04	95	So we've been working with some pretty cool folks on this, folks listed here and others. Yeah. So the overall aim across the two or three papers is to present something like a minimal, unifying model of consciousness premised on the free energy principle. In the paper that we're discussing today, we are presenting, as I was intimating a moment ago, a model of consciousness that follows directly from applying the free energy principle to well understood human neurophysiology. The take home message of today's talk is that, well, let me take one step back.
224566	257768	9	B	1.0	03:44	90	There has been a lot of discussion in the literature over the last decade about whether the free energy principle has anything unique to teach us about consciousness. And there have been responses pro and contra. And I think that the question up until recently was just unsettled. We would like to suggest a positive response to that question. And as such, one of the take home messages is that actually, yes, a model of consciousness can be directly derived from the free energy principle applied to explain known neuroanatomy.
257864	283780	10	B	0.77	04:17	45	And I guess the kind of technical message, the technical part of the takeaway is that consciousness has or is isomorphic to a nested holographic mathematical structure. So this sounds like a mouthful, might sound a little crazy. We are going to unpack this presently.
286200	322800	11	B	0.9012	04:46	91	Now, did you want to add anything to that? No, all good. Excellent. So the original paper that we wrote turned into, like a 24,000 word monstrosity of a paper, and it was elephantine both in the sense that it was very large and also in the sense that of the soupy parable of the blind men and the elephant. What we're trying to do is in some sense, identify a parsimonious mathematical structure that underwrites the different accounts of consciousness that have been developed based on the free energy principle.
323220	376540	12	B	0.99726	05:23	119	I'll spare you the original paper structure. I said a moment ago that we were developing a minimal unifying model. This is a notion that was proposed by our friend and colleague Vanya Visa, I believe, a few years ago now, in 2020. And Vanya at the time was noticing, well, the field of consciousness studies is replete with a bunch of different models and theories, and there seems to be no end in sight to the multiplication of these theories. And the idea is maybe what the field needs is not one additional N plus one theory of consciousness, but rather a minimal unifying model by which Vanya means a model that specifies only necessary properties of consciousness.
376620	413496	13	B	0.99999	06:16	96	So this doesn't come with, like, a strong sufficiency claim, really, like, what are the kind of rudimentary building blocks that are necessary for consciousness in general. This model would have determinable descriptions that can be made more specific. So it's kind of a base model that can be expanded to cover different varieties of conscious experience. And finally, that it's minimal and unifying in the sense that it integrates current approaches to consciousness in no small part by highlighting their common assumptions. So kind of pointing to a least common denominator of all existing accounts.
413688	453500	14	B	0.96	06:53	101	And so what we have tried to do in this paper is to well, in this series of papers is to engage in this analysis and this mummy analysis, if you will, for theories of consciousness premised on the free energy principle directly. So it's worth quickly discussing the free energy principle. I'm almost embarrassed to do this at the Active Inference Institute, but it's always worth going through this very quickly. So there's a standard distinction in physics that one can leverage between dynamics, mechanics and principles. The free energy principle, as it says on the tin is a principle.
454000	486740	15	B	0.78172	07:34	81	So principles are kind of the foundation of a hierarchy of theory building in science, as it were. Top of the hierarchy are formal descriptions of behavior what are known as dynamics. So if you're applying dynamical systems theory to understand the time evolution of some system, you're in the realm of dynamics. Arguably, folks like Kepler and Galileo were also in the realm of dynamics. So I see some empirical phenomenon and I produce a formal description of that behavior.
487080	548644	16	B	0.99542	08:07	140	Mechanics comes into play when we move from Kepler and Galileo to Newton, when we move from merely describing some behavior to providing equations of motion that allow us to explain why that behavior has the particular shape that it does. So classical mechanics, for example, gives us an account of gravitational force and explains why orbits are shaped the way they do. So if galileo and kepler just noticed the shape of orbits and provided a formal description of them, newton explains where that shape comes from in some sense, and principles, in turn, explain where mechanics come from. So classical mechanics come from the principle of least action or stationary action, which, very roughly speaking, is the principle according to which no more or no less energy is used than necessary to perform some physical movements. That kinetic.
548692	583620	17	B	1.0	09:08	87	And potential energies balance out such that the true paths of a system through its state space are those for which that balance equals zero. The principle of least action explains where classical mechanics comes from. In some sense. And similarly, the free energy principle is at the basis of another kind of mechanics that is becoming known as Bayesian mechanics. You can think of the free energy principle as having the same relation to Bayesian mechanics as the principle of least action has to classical mechanics.
583960	628112	18	B	1.0	09:43	83	And Bayesian mechanics is the physics of probabilistic beliefs. So it's a physics that connects the properties of the system that you're considering the physical properties of the system. In particular, the information entropy of states of a system to the connects the thermodynamic entropy of the states of a system to the information entropy of the beliefs that a system has about the systems to which it is coupled. Here. A probabilistic belief means a probability density in a very general sense.
628166	644148	19	B	0.99997	10:28	50	We're not assuming some kind of contentful motion of belief like you might find in philosophy. Really. This is about kind of a tracking relation. But we'll get that to that in actually, just a few slides. There are two main formulations of the free energy principle in the literature.
644244	662844	20	B	1.0	10:44	37	The classical and the quantum. Both rest on this apparatus of the Markov blanket. The Markov blanket definitionally is the degrees of freedom that separate. But couple two systems. Or rather, two things within a larger system.
662962	712670	21	B	0.99983	11:02	109	It's effectively a statistical boundary that allows us to say that, well, given this boundary, the inside is independent of the outside. The way that we express the dependencies. So taking one step back quickly, this is a dynamic systems approach to cognition, to self organizing systems, and to physical systems more generally. It is a dynamical systems approach in the sense that we're using the tools of dynamical systems theory to examine the time evolution of a system, its trajectory in its state space. And in this context, a generative model is used to encode the relations of dependence that obtain in the time evolution of the system.
713120	730390	22	B	0.69007	11:53	36	So this is important and I've been kind of making this point maybe ad nauseam for the last, I guess, half decade. But the generative model is not a model that you have in your head.
732520	781028	23	B	0.99941	12:12	96	It is a statistical model that represents the conditional dependence structure of the entire system that you're considering. So this is an environment agent system in the broadest sense. And the free energy principle says that if a specific sparseness structure obtains so if things are disconnected in a special way, then it'll look as if the subsets that are disconnected are tracking each other. So more technically, we partition our system into particles. Particles are basically internal states shrouded behind their Markov blanket, where the Markov blanket is composed of sensory states and active states.
781194	854140	24	B	0.99939	13:01	152	Sensory states affect internal states but are not affected by internal states and active states affect external states but are not affected by external states. This separation of causal dependence and this structure intervening between the two bulk subsets of the system is really critical. The free energy principle says if my generative model, which encodes all of the dependencies of the system, if my generative model contains a Markov blanket in the sense that we just defined, then we are basically licensed in our interpretation of internal states as basically tracking external states where tracking has a specific mathematical definition. In this context, tracking means to encode the parameters of some probabilistic belief about external states. So to the extent that we want to call this representation, we should note that this is not a representation in the typical tele semantic sense of having some internal states track some external features.
855840	889524	25	B	0.99983	14:15	84	What we're saying is that basically if I have a Markov blanket, then my internal states are basically shaping a belief about the external world. And I think that's really critical to understand. And this is where the inference and the modeling bit comes from. This kind of tracking relationship, formally speaking, is approximate daisy and inference. So basically, in a nutshell, the free energy principle says in our physical universe as we understand it, with the usual mechanics running in the background, right?
889562	917490	26	B	0.99978	14:49	78	Classical relativistic, quantum and statistical, if boundaries exist in the physical system, then the bulk across the boundary is going to be tracking whatever is beyond the boundary. So in a nutshell, this is what the free energy principle says. Now, I noticed you put some stuff in the chat. Would you mind using your voice to compliment. Yeah, I was just saying that this part that you were just explaining often seems kind of esoteric to people.
918180	947960	27	C	0.99998	15:18	65	But it's critical to understand the sparseness of the coupling because then you kind of start understanding where perspective comes from. And if you have perspective, you also have the capacity to assess causality starting from a point and extending outwards to something else. And this is true from any so that's what I wanted to add. Thank you, Mala. I think that's useful addition.
948860	976860	28	B	0.97992	15:48	56	Yeah. So this is the classical formulation of the free energy principle. There is a new hip funky formulation that has become available over the last half decade due to Chris Fields, Jim Glazebrook and their colleagues, the so called quantum information theoretic formulation of the FEP. Fear not. This doesn't appeal directly to quantum mechanics.
976940	1016540	29	B	0.9994	16:16	109	Rather, it appeals to the tools that have been used to extend information theory in the context of quantum mechanics, which allow you to calculate like all sorts of funky wave equations and propagation operators and all that. We don't have to get into the details. Basically, it's just a way to reframe what I just said. So what I just said is if there's a boundary in my physical system, then the things across the boundary will look as if they're tracking each other. Another way of saying that is, well, this boundary is a kind of screen green takes on a particular sense in quantum information theory.
1017120	1065810	30	B	1.0	16:57	122	We don't necessarily have to get into the details of it, but basically you can think of these degrees of freedom that couple the two systems as a kind of probabilistic surface and you can think of the bulk across either side of the boundary as in alternation reading and writing information onto the blanket. So any Markov blanket whatsoever can be construed as a classical information channel or a screen and in effect, all of the classical information that you need to describe the way that things across the boundary. So the so called bulk of the system, couple to one another, is contained on the boundary. It encodes the classical information that's necessary to describe the couplings within the system.
1068980	1095508	31	B	0.81708	17:48	55	This is holography. This is the hologram. Not in the sense of Holography. Although we might discuss senses in which these things are intertwined, we're using the word Hologram in the sense of the Holographic principle from physics. The Holographic principle is a principle that was originally discovered in the context of black hole thermodynamics.
1095604	1139940	32	B	0.76	18:15	106	And what it basically tells us is necessarily from the point of view of an external observer. Some bulk in physical space can only contain as much information as can fit onto its surface. So this echoes what we were just discussing about Markov blanket. The physical reason is that if a bulk collapses into a black hole and the bulk contained any more information than could be fit onto its boundary then we would basically be losing information. And that would violate the conservation of classical information also known as the principle of unitarity in quantum mechanics which basically says that information is never lost.
1141080	1168416	33	B	0.99051	19:01	83	So we're using the word Hologram in that sense. This is different from the sense of Hologram everyone has seen, like the Hologram of Tupac. Or does that make me look seem old at the Super Bowl a few years ago? Or maybe Pokemon cards, maybe that also makes me seem old now. I don't know what the kids are doing, but you've seen like red baseball cards and stuff like that with different superimposed picture layers that become visible as you move.
1168518	1221824	34	B	0.96681	19:28	113	That's not the sense of Hologram that's at stake. Although maybe so we'll get to why, but we'll focus on the Hologram theoretic formulation. So, to summarize, in the Holographic principle version of the free energy principle, the quantum information theoretic version of the principle, all of the classical information that I need as an external observer to characterize the coupling between two subsystems of a larger system exist on the boundary. And that's a pretty remarkable physical fact. And a few years ago, Chris Fields, Jim Glazebrook and Mike Levin proposed that maybe this kind of information concentrating bottleneck architecture also has application to consciousness in the form of an internal screen.
1221942	1258830	35	B	0.99986	20:21	101	So the idea was that the information that I bring to bear, to parse my sensory stream and make sense of what is constantly bombarding me, it has to live somewhere. And so the idea is, well, it lives in this inner screen. So you basically have an external Markov blanket, which is a Markov blanket just in the usual sense. And then the idea is well within the set of internal states that are kind of partitioned off from the organisms due to the organism's Markov blanket. Then you would have an internal Markov blanket structure within those internal states.
1259440	1286470	36	B	1.0	20:59	65	This is prediction six from their great paper. Minimal physicalism internal awareness requires internal boundaries. And there's this interesting connection to integrated information theory I am agnostic with respect to IIT, so we're not going to go there today. But for those of you who are interested, this kind of inner loop mechanism seems to generate positive integrated information. So there's an interesting connection there.
1286920	1313228	37	B	1.0	21:26	56	Before I move on to Neurophysiology, I'll hand things over to Mao. If you wanted to add anything. Yeah, actually I was literally about to type something that this is actually also connected to some neomaterialist formulations of folding onto itself. So maybe this is something worth digging into a little later in the talk. Awesome.
1313394	1332704	38	B	0.90387	21:53	58	Then we shall. And I believe that Ali is also a co author on the paper and we have him. So congratulations, by the way, on the preprint. I saw it released and Mao has told me about it. I'm really happy to see such serious philosophical work being done with the free energy principle as the basis.
1332752	1349992	39	B	1.0	22:12	45	I think you should be very happy with the end result and proud. But now let me get back to my monologue. So rather our Mao and my diet. It's not really a dialogue. It's sort of like I don't know how to call it.
1350046	1377392	40	B	0.94913	22:30	64	Anyway, I'm thinking in French. I have the words in French, I promise. So now that we've, you know, reviewed in a very much too short, way too brutally fast fashion the free energy principle and its core formulations, we're now going to derive a model of consciousness by applying it to the known neurophysiology of the brain. Are you ready? You buckled up?
1377446	1403628	41	B	0.99976	22:57	62	Okay, good. So we have just said that the FEP can be used to model things that have boundaries. I submit to you that the brain is a thing with boundaries and importantly, a lot, many, several internal boundaries. So in particular, we know that the brain has a very sparse structure. So if you think of just the raw numbers, right?
1403714	1425410	42	B	0.94338	23:23	40	There are roughly on the order of 100 and 5160 billion neurons in the brain. But each neuron only makes about something like, I want to say five figures, because I've spent so long, ten to the five connections, right?
1429300	1454524	43	B	0.99848	23:49	71	My neurophysiology is a bit far behind me, but it's something on the order of a few thousand to maybe tens of thousands of connections per neuron. So obviously, if you just think about it, most neurons are not connected to each other, not directly. And furthermore, the brain has a hierarchical structure. And when you think about it, what is a hierarchy? A hierarchy is a sparse connectivity structure, right?
1454562	1497290	44	B	0.99994	24:14	123	So if you think about a hierarchy of connectivity, what having a hierarchical structure means, at the end of the day, this isn't some statement about power dynamics where things at the top of the hierarchy are more important than things at the bottom. Your cortical neurons don't drain more juice than other parts of the brain. The neurons in those parts. What we're talking about is a regular sparse pattern of connectivity, where I only make connections to the layers directly below me and to the layers directly above me. What I want to suggest now is that you can understand the sparseness as a set of nested Markov blankets, and that's going to do a lot of heavy lifting for us.
1497660	1516780	45	B	0.99917	24:57	45	So the discussions here are based on two papers. One by Fields and colleagues called neurons as hierarchies of quantum reference frames. It's really cool. I recommend it. And also a paper by Inesh Ipolito, myself and our colleagues on Markov blankets in the brain.
1516860	1572656	46	B	0.99932	25:16	141	So this is going to be a much too short summary of those discussions, but basically, you can start with individual neurons and then talk about their Markov blankets to bring it back to something that I was saying earlier, but that I didn't emphasize while we were going through the slides. The Markov blanket identifies dynamical dependencies, not necessarily physical boundaries. So it's a statistical boundary it's not necessarily like the boundaries of a wall, like a cell. In the case of formalizing individual neurons with Markov blanket we would associate the post synaptic conductance with the internal states, the post synaptic voltage with the active states, the presynaptic conductance to external states and the presynaptic voltage to sensory states. You have some equations of motion that tell you how each of these quantities change and what they depend on.
1572838	1624640	47	B	1.0	26:12	113	And you'll notice that this has the dependency structure of a Markov blanket where the internal states only depend on sensory active and internal states, the active states only depend on internal states, and so on. So it respects the dependency structures that we were discussing a bit earlier, where again, the sensory states can only depend on other blanket states and external states and so on. So here we have a nice dynamic Markov blanket that we carve around our neurons. And then there are two directions that we can go. We can ascend the scales towards canonical microcircuits and then cortical columns, individual brain regions, whole brain networks and so on.
1624710	1665724	48	B	1.0	27:04	84	And at each of these successive scales of self organization you can similarly identify Markov blanket. If you want to discuss this in more detail, you can go see the paper by Inesh and myself and colleagues. You can also go from the individual neuron down towards gated channels, towards synapses dendrites and the soma. So the body of the cell, all of these things have successive Markov blanket. And so all throughout the brain's organization at the different scales at which itself organizes.
1665772	1692468	49	B	0.99999	27:45	64	You have this nice structure of nested Markov blankets, of Markov blankets? Of Markov blankets. So that's one clear immediate sense in which there are these blankets. And you can start to think of these blankets as screens in the way that we just were talking about. A slightly more illustrative example of what we're talking about here is Markov blankets and predictive coding.
1692564	1734500	50	B	0.99997	28:12	102	This is a figure pretty typical in the predictive coding literature. What it tells you is that the brain has a series of levels or layers, where superordinate layers are basically passing predictions about sensory input onto subordinate layers and subordinate layers are shuffling prediction error back up to the superordinate layers. This is probably like I'm almost embarrassed again to discuss this at the active inference institute. I'm sure everyone is quite familiar with these things already. The novelty that we are introducing here is to say well, each of these layers is connected to each other via a Markov blanket.
1734920	1781780	51	B	0.93	28:54	121	And so that's I think like a key insight is that each of these layers has internal states and communicates to other layers through what you can formalize as a Markov blanket. So you can understand at any given scale of brain activity, you can understand the different levels of message passing as successively nested screens. The cool thing is that what each of these screens is doing in some sense is course graining, the stuff in the screen below. So in some sense each screen is kind of implicitly contained in the following one. And what you end up having is a kind of nested structure of screens of screens of screens of screens, which is, I think, pretty cool.
1781850	1816380	52	B	0.99996	29:41	73	We recover, as you'll note in this figure, the inner screen hypothesis. Each of these, this is probably a better way to visualize it. You have at the left the kind of external Markov blanket of the organism. It is segregating or carving out a set of internal states. And what we're saying is these internal states have a layered level structure, a sparse structure that we can formalize using nested Markov blanket.
1817140	1887220	53	B	0.92	30:17	144	And like I was saying, each screen successively coarse grains the next screen. And this gives some flesh to this notion of contextual computing, this idea that really what the brain is, is an organ of context where each layer is providing context to the layer below and where each layer is attuning to increasingly higher order, slower and more physically widespread regularities. So one thing that we're proposing here, which is a novel hypothesis that we intend to test empirically over the next few years, is that this architecture has two special layers. In some sense you can think about it as write only layers. So if you're thinking about the stack, the sensory periphery or the sensory bottom, in one sense can only write in one direction, it can only write into the system and the topmost level can only write downwards.
1887560	1914108	54	B	0.999	31:27	68	This is a key architectural feature. One thing to point out is that in the paper we associated the topmost level to structures like the hippocampus. We don't need to suggest that the hippocampus is uniquely the top of this hierarchy. Rather there is probably a whole bulk of structures that basically cannot any longer be coarse grained. So you kind of have like an executive level top.
1914194	1950330	55	B	0.98	31:54	84	And what's particular about this top is that it can only write down into the stack. So its function really is to be about and monitor the other layers of the stack, implementing a form of COVID action which trickles down throughout the stack. So now I saw you write some stuff. Would you like to add? Yeah, so I think one of the things that's important to highlight is why some things can only write up and some things can only write down.
1951340	1988020	56	C	0.96	32:31	76	And what this means for irreducible, the irreducible mark of blanket, which is the part where experience happens because there's a counter that and some future questions that are going to happen that relates to superstructures and then why don't we experience the consciousness of a superstructure? So I think it might be important to dig into that a little bit. Absolutely. Thank you. So, yeah, to summarize then, there are basically two, right, only layers.
1988520	2024416	57	B	0.99999	33:08	79	As you ascend the layers which are successively coarse graining each other, there is an irreducible screen that can't further be coarse grained. We believe that this corresponds to something like a naturalized Homunculus. And we're calling this perspective with an intentional provocation. We're calling this perspective Neocartesian in the sense that we may have in some sense identified the processes that make it seem as if there was something like a Homunculus. Obviously we're not, or maybe not.
2024438	2081164	58	B	0.99945	33:44	131	Obviously it's worth saying we are not falling into the Homunculus fallacy because we're not claiming that the topmost layers perceive themselves. So the buck stops with the top layers and the top layers can only perceive themselves vicariously by acting through other layers. And we associate this action of layers of internal Markov blankets on each other with attention in particular and other neuromodulatory effects. So I mean the overall picture is one where consciousness is intrinsically related to overt and covert actions, to the overt acting and sensing associated with the external Markov blanket and with the COVID actions that are being deployed within the stack and in particular that are issued by the very top set of levels. As I said, this is not necessarily a strict hierarchy.
2081212	2097880	59	B	0.99994	34:41	36	It could be a hierarchy with several different tops. But this integrated top structure we believe could be associated with a naturalized Homunculus. I'm going to skip through the rest and thank you for your attention.
2103260	2148170	60	B	0.99934	35:03	107	Unless Malana wants to add something. Just one thing is, I think to put it in simpler terms as well, the two layers that can only write can't store memories. Because that's right, memories is not just an encoding, it's a process of retrieval. And it requires interactions between different levels, which is why the lowest levels are only interacting with things which are external and therefore can't necessarily store and can't make direct interactions with anything else within the entity itself and the highest level as well. So there is no memories being encoded at these two levels, which is why they're right only and not.
2149260	2160392	61	C	0.96	35:49	28	I think that helps a little bit. Thank you. Yeah, I would agree. So thank you all for your attention. Thank you in particular, Dan, for the invitation.
2160456	2169280	62	B	0.99955	36:00	16	It's always great to be at the AWI and yeah, we looking forward to this discussion.
2174960	2187410	63	C	1.0	36:14	26	I think Daniel said he wasn't going to do too much animation, so if. People have are you going to do any facilitation at all then.
2195400	2214810	64	C	0.63635	36:35	40	Yeah. Okay. So thanks Maxwell and Mao for the amazing talk. There are so many interesting things to dig on in this paper, I don't know even where to start. But I do have one question too, actually, in particular.
2215280	2250070	65	D	0.63526	36:55	69	So does this inner screen model imply a kind of protopenpsychism or a more restricted bioppsychism? I mean, do you think the type of nested Holographic structure described by the model all exists in biological systems? And regardless of the answer, would you think that it generalizes to larger systems comprising these systems, like populations and colonies and so on? Maybe I can take. A quick stab at that.
2250760	2286140	66	B	0.86142	37:30	80	So we think that the free energy principle itself commits you to a kind of I don't know what the right word is. We're writing a paper now. We're struggling with the exact coinage, but it's not quite a pen psychism. It's a pen representationalism or a pen tracking ism, if you'll allow me, the absolutely disgusting neologism. Basically what the FEP implies is that anything that has a boundary, it looks as if it's tracking what's beyond the boundary.
2286220	2314676	67	B	0.99749	38:06	67	So at a very basic level, the FEP kind of licenses this talk about anything. So anything whatsoever looks as if it's tracking what's across the boundary by virtue of existing. So at a very basic level, if I see a rock outside, I know that it's not 1000 degrees Celsius outside. Right. The fact that a rock has a specific structure entails something about its environment.
2314868	2351744	68	B	0.90748	38:34	92	Likewise, I can see you, Maria, you are not wearing a winter parka. And I can infer from that that it's probably room temperature around you and not like you're not somewhere in Antarctica or wherever else. So this kind of basic tracking relationship applies to everything. What we've tried to do in this paper is to start spelling out more explicitly what the difference is between just rocks and rock stars in some sense. Like, what is the special kind of structure that you need to add to the Markov blanket?
2351792	2393860	69	B	0.59	39:11	100	And what you'll notice is that you get consciousness by making the overall structure more sparse. So as you make more of the connectivity zero, you get all of these weird things that pop up, one of which is we think like this consciousness structure. Mao has thought about this maybe more than I have, so I'd be curious to get her perspective. Well, so I know that people are a little bit wary of panpsychism, so we usually add the term proto in front of it. And so the idea is that we have to deconceptualize what consciousness means.
2393930	2429472	70	C	1.0	39:53	76	And oftentimes what we mean is something that resembles what humans do. And we have to be a little bit if we take our formalism consciousness can become emergent at the scales where humans are. But it doesn't mean that it doesn't exist at lower scales, where the inertia or the entropy generated by policies is lower. So if you think of a rock, its policy is pretty much maintained bonds. You know what I mean?
2429526	2481056	71	C	0.98325	40:29	106	There's nothing else. The scale at which some kind of thing exists and changes is much different than you deciding to have coffee. But in effect, your cells could be something that we could consider exist, closer to something that a rock does and even lower just the proteins that make up your system. So if we understand that there is nestedness and that the conscious process happens once you have this capacity to exchange information across layers, anything that has the stacking of layers in a nested fashion is in effect, proto conscious. But we have to be very careful about what we load into.
2481158	2510890	72	C	0.57	41:21	66	The term conscious, I think, is mainly the question. I'm sharing my screen here. This is from a paper on path integrals, particular kinds and strange particles. So it is a typology of different kinds of Markov blankets and different kinds of particles. So something like a rock, it still has a Markov blanket, but its Markov blanket is not partitioned into sensory and active states.
2511340	2532940	73	B	1.0	41:51	56	There are no states that have this property that active states have that they affect the external world that are not affected by it. In return. A rock just has sensory states. If I kick a rock, unless it's radioactive, it's not going to act on me. It only has the inference part of active inference.
2533020	2559960	74	B	0.99814	42:13	62	It's only kind of changing its state as a response to the forces that are acting on it. Active particles are particles that have both sensory and active states. And you might think, for example, of a cell. A cell has this kind of structure. Particles like ourselves have additional sparseness in the Markov blanket, in particular, what we call strange particles.
2561420	2600100	75	B	1.0	42:41	88	The internal states of strange particles do not have direct access to the active states of strange particles, and therefore they need to be inferred. And this is what gets you planning. At the end of the day, a strange particle can only exist by kind of inferring itself into existence precisely because there are no direct connections between the active and the internal states. So as you remove connectivity from the overall structure from here to here, you're adding sparseness. From here to here, you're adding sparseness.
2600520	2632160	76	B	0.9925	43:20	75	As you make the system more and more sparse, you get more and more interesting behavior to emerge from the system. And you can think of the topmost structure that we were describing as an instance of this broader architecture where the topmost layer doesn't have direct access to the active states of the system. And therefore, it kind of has to go through the entire bulk of the system to infer itself into existence.
2635420	2647390	77	C	0.83644	43:55	28	So there's a few hands up there on Google where you can see the people who share hand. Okay. So, Ali, I think you raised your hand first.
2650400	2691352	78	E	0.91663	44:10	77	Yes. Thank you again for your crystal clear and really illuminating talk. I've written down three questions, but maybe I'll begin with probably the most basic and naive one. So in some of the earlier literature on active inference and the FEP, there was this emphasis on distinguishing between generative process and generative model. But in this paper, in figure one, which you also shown in your slides, we don't see any generative process in the diagram.
2691416	2730568	79	E	0.99245	44:51	73	So where does generative process fit into this diagram? Because the way I see it, it's basically somehow included in the generative model or I don't know. What is the distinction between these two concepts? So you'll have to forgive some of the earlier literature for this, there's some terminological inconsistency. The only two constructs that are at play here are the generative model and the variational density that's encoded by internal states.
2730734	2754808	80	B	0.99984	45:30	51	That's it. So it is not the case that the FEP is about how the generative models the generative process. A lot of this language comes from previous Bayesian approaches and in particular, the helmholtz machine. In the helmholtz machine. So helmholtz machines are some of the earlier predictive coding machines.
2754904	2811280	81	B	1.0	45:54	135	And in the helmholtz machine, basically you have a forward pass that connects the sensory end to basically the top of the predictive hierarchy and then a backwards pass connecting top of the model back to the sensory end. And in a helmholtz machine, you would call the forward pass a recognition model and the backwards pass a generative model. The forwards passes a recognition model because you're inputting some data and then basically by the time the information trickles up to the top, you have recognized what caused the data. And the backwards pass is called a generative model for the reasons that are sometimes invoked in the predictive coding literature where you're basically generating possible sensoria given a certain configuration of states. So we don't use recognition and generative in that sense anymore.
2812180	2836664	82	B	1.0	46:52	55	And I apologize. In some previous publications, I've been a bit sloppy in the way that I use my language and I've used the term recognition density. What's really at stake is and I'll share my screen just to help make the point. It's what it says here. So you have your generative model, right?
2836702	2867120	83	B	0.99978	47:16	71	Your generative model is a joint probability density. So what is a joint probability density? Well, you have a bunch of different basically states that your system is defined over and you have a probability density that the shape of which captures the way that states can change over time. So, like, if I change this way, then I need to change in that way. And that's what this shape encodes.
2867960	2905516	84	B	0.96	47:47	85	And what the FEP says is if my generative model has a certain sparseness, if it has Markov blanket, then there's this additional probability density that I can define. It is parameterized by the internal states of the system. It's called the variational density. And this is like my probabilistic best guess about what's causing my external states. At no point do you need to appeal to any other bits of generative process when you're actually coding these models up in MATLAB or in Python.
2905628	2925510	85	B	0.99993	48:25	55	What we call the generative process is just the equations of motion that govern external states. So if you actually hand code one of these one day, your generative process might include like, Newtonian physics and whatever else. It's sort of just like what is the process that actually animates the dynamics of external states?
2927720	2959570	86	B	0.99973	48:47	81	In some very simple models, like in the ones used by Miguel Aguilera and colleagues, you're looking at linear dissipative systems and they don't have any dynamics right. So basically, the system kind of just dissipates. A linear dissipative system is a dampened spring physically, right? So it just goes spring and then it dissipates to the fixed point. So similarly, you may or may not have dynamics to the external states and that's what gets harnessed in the generative process.
2960100	2985000	87	B	0.86435	49:20	51	This should be clear. Yeah. Sorry, ma'am. No, I mean, I think what the question that Ali is asking also has some important correlates that often get asked to us, like what's the difference between consciousness or dreaming? Or what's the difference between a dead system that decays or being psychotic?
2985740	3032950	88	C	1.0	49:45	82	And I think what Maxwell is saying is our system currently doesn't say anything about your actual connection to the real world other than a thing, which is successful at being a thing, is probably well joined relative to its needs to the world. And therefore your relation to reality has probably got a better grasp if you are functional for a longer amount of time, this minus senescence. So yeah, does that kind of help also, Ali? Yes. Thank you both.
3033640	3043390	89	E	0.8363	50:33	26	It really makes a lot more sense now. Thank you so much. Well, it's my pleasure. And remember, model just means joint probability density here. Right?
3045520	3060890	90	B	0.83303	50:45	34	There's a criminally underappreciated paper that Carl wrote in Entropy in 2012 called the Free Energy Principle for Biological Systems, which is where a lot of the Markov blanket story comes from. And in.

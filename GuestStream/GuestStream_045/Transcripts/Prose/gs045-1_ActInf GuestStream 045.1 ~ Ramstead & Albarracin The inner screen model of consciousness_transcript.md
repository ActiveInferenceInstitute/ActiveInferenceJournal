00:06 _DANIEL FRIEDMAN:_
Hello and welcome.

It's June 19, 2023.
We're going to have a presentation and a discussion with Maxwell Ramstead and Mahault Albarracin here.
So thank you, Maxwell and Mahault, for joining.
Looking forward to your presentation, setting some of the context, and then thank you to all of our guests.
You'll have an opportunity to introduce yourself and give some background when you first give reflections.
So on to the slides.
Maxwell.

00:37 _MAXWELL RAMSTEAD:_
Excellent.

Well, maybe now and I can just briefly reintroduce ourselves for those of us watching at home.
So, hey, folks, my name is Maxwell Ramstead.
I am the director of Research versus AI.
I am a scholar in the tradition of active inference and an expert on the Free Energy Principle.
Yeah.
And Mahault,
would you please?

01:05 _MAHAULT ALBARRACIN:_
Sure.

So, my name is Mahault Albarracin.
I work at Versus with Maxwell.
I'm Director of Product for Research and development, but I'm also doing a PhD in cognitive computing.
I worked in social sciences and in artificial intelligence in the artificial intelligence field.
And I've specifically tried to apply active inference to more socially grounded questions and sometimes more abstract and representational questions, diving all the way down to the philosophy that can be attached to it.

01:43 _Maxwell:_
Thank you, Mal.
So, yeah, I'm very pleased to present this work.
Today, Mao and I will be discussing our new preprint called the Inner Screen Model of Consciousness.
And it is about basically an application of the Free Energy Principle to the problem, the conundrum of consciousness.
So just to give a little bit of background and now I'll just be walking through the slides, if at any point you want to jump in and add complete correct, please feel free to do so.
This is work that started in late last year and continued throughout the year.
It's preoccupied us quite a bit, actually, over the last while.
Mao and I are the lead authors on this, what has now become a series of papers, with one more preprint coming out, hopefully before the end of the month, and another in the works, hopefully before the end of the year, with the same core group of authors.
So obviously Mao, as well as Adam Saffron, Alex Keefer, Brendan Klein, Chris Fields and Karl Friston.
And the follow up papers include a bunch of other authors.
So we've been working with some pretty cool folks on this, folks listed here and others.
Yeah.
So the overall aim across the two or three papers is to present something like a minimal, unifying model of consciousness premised on the Free Energy Principle.
In the paper that we're discussing today, we are presenting, as I was intimating a moment ago, a model of consciousness that follows directly from applying the Free Energy Principle to well understood human neurophysiology.
The take home message of today's talk is that, well, let me take one step back.
There has been a lot of discussion in the literature over the last decade about whether the Free Energy Principle has anything unique to teach us about consciousness.
And there have been responses pro and contra.
And I think that the question up until recently was just unsettled.
We would like to suggest a positive response to that question.
And as such, one of the take home messages is that actually, yes, a model of consciousness can be directly derived from the Free Energy Principle applied to explain known neuroanatomy.
And I guess the kind of technical message, the technical part of the takeaway is that consciousness has or is isomorphic to a nested holographic mathematical structure.
So this sounds like a mouthful, might sound a little crazy.
We are going to unpack this presently.
Now, did you want to add anything to that?

04:49 _Mahault:_
No, all good.

04:51 _Maxwell:_
Excellent.
So the original paper that we wrote turned into, like a 24,000 word monstrosity of a paper, and it was elephantine both in the sense that it was very large and also in the sense that of the soupy parable of the blind men and the elephant.
What we're trying to do is in some sense, identify a parsimonious mathematical structure that underwrites the different accounts of consciousness that have been developed based on the Free Energy Principle.
I'll spare you the original paper structure.
I said a moment ago that we were developing a minimal unifying model.
This is a notion that was proposed by our friend and colleague Vanya Visa, I believe, a few years ago now, in 2020.
And Vanya at the time was noticing, well, the field of consciousness studies is replete with a bunch of different models and theories, and there seems to be no end in sight to the multiplication of these theories.
And the idea is maybe what the field needs is not one additional N plus one theory of consciousness, but rather a minimal unifying model by which Vanya means a model that specifies only necessary properties of consciousness.
So this doesn't come with, like, a strong sufficiency claim, really, like, what are the kind of rudimentary building blocks that are necessary for consciousness in general.
This model would have determinable descriptions that can be made more specific.
So it's kind of a base model that can be expanded to cover different varieties of conscious experience.
And finally, that it's minimal and unifying in the sense that it integrates current approaches to consciousness in no small part by highlighting their common assumptions.
So kind of pointing to a least common denominator of all existing accounts.
And so what we have tried to do in this paper is to well, in this series of papers is to engage in this analysis and this mummy analysis, if you will, for theories of consciousness premised on the Free Energy Principle directly.
So it's worth quickly discussing the Free Energy Principle.
I'm almost embarrassed to do this at the Active Inference Institute, but it's always worth going through this very quickly.
So there's a standard distinction in physics that one can leverage between dynamics, mechanics and principles.
The Free Energy Principle, as it says on the tin is a principle.
So principles are kind of the foundation of a hierarchy of theory building in science, as it were.
Top of the hierarchy are formal descriptions of behavior what are known as dynamics.
So if you're applying dynamical systems theory to understand the time evolution of some system, you're in the realm of dynamics.
Arguably, folks like Kepler and Galileo were also in the realm of dynamics.
So I see some empirical phenomenon and I produce a formal description of that behavior.
Mechanics comes into play when we move from Kepler and Galileo to Newton, when we move from merely describing some behavior to providing equations of motion that allow us to explain why that behavior has the particular shape that it does.
So classical mechanics, for example, gives us an account of gravitational force and explains why orbits are shaped the way they do.
So if galileo and kepler just noticed the shape of orbits and provided a formal description of them, newton explains where that shape comes from in some sense, and principles, in turn, explain where mechanics come from.
So classical mechanics come from the principle of least action or stationary action, which, very roughly speaking, is the principle according to which no more or no less energy is used than necessary to perform some physical movements.
That kinetic.
And potential energies balance out such that the true paths of a system through its state space are those for which that balance equals zero.
The principle of least action explains where classical mechanics comes from.
In some sense.
And similarly, the Free Energy Principle is at the basis of another kind of mechanics that is becoming known as Bayesian mechanics.
You can think of the Free Energy Principle as having the same relation to Bayesian mechanics as the principle of least action has to classical mechanics.
And Bayesian mechanics is the physics of probabilistic beliefs.
So it's a physics that connects the properties of the system that you're considering the physical properties of the system.
In particular, the information entropy of states of a system to the connects the thermodynamic entropy of the states of a system to the information entropy of the beliefs that a system has about the systems to which it is coupled.
Here.
A probabilistic belief means a probability density in a very general sense.
We're not assuming some kind of contentful motion of belief like you might find in philosophy.
Really.
This is about kind of a tracking relation.
But we'll get that to that in actually, just a few slides.
There are two main formulations of the Free Energy Principle in the literature.
The classical and the quantum.
Both rest on this apparatus of the Markov blanket.
The Markov blanket definitionally is the degrees of freedom that separate.
But couple two systems.
Or rather, two things within a larger system.
It's effectively a statistical boundary that allows us to say that, well, given this boundary, the inside is independent of the outside.
The way that we express the dependencies.
So taking one step back quickly, this is a dynamic systems approach to cognition, to self organizing systems, and to physical systems more generally.
It is a dynamical systems approach in the sense that we're using the tools of dynamical systems theory to examine the time evolution of a system, its trajectory in its state space.
And in this context, a generative model is used to encode the relations of dependence that obtain in the time evolution of the system.
So this is important and I've been kind of making this point maybe ad nauseam for the last, I guess, half decade.
But the generative model is not a model that you have in your head.
It is a statistical model that represents the conditional dependence structure of the entire system that you're considering.
So this is an environment agent system in the broadest sense.
And the Free Energy Principle says that if a specific sparseness structure obtains so if things are disconnected in a special way, then it'll look as if the subsets that are disconnected are tracking each other.
So more technically, we partition our system into particles.
Particles are basically internal states shrouded behind their Markov blanket, where the Markov blanket is composed of sensory states and active states.
Sensory states affect internal states but are not affected by internal states and active states affect external states but are not affected by external states.
This separation of causal dependence and this structure intervening between the two bulk subsets of the system is really critical.
The Free Energy Principle says if my generative model, which encodes all of the dependencies of the system, if my generative model contains a Markov blanket in the sense that we just defined, then we are basically licensed in our interpretation of internal states as basically tracking external states where tracking has a specific mathematical definition.
In this context, tracking means to encode the parameters of some probabilistic belief about external states.
So to the extent that we want to call this representation, we should note that this is not a representation in the typical tele semantic sense of having some internal states track some external features.
What we're saying is that basically if I have a Markov blanket, then my internal states are basically shaping a belief about the external world.
And I think that's really critical to understand.
And this is where the inference and the modeling bit comes from.
This kind of tracking relationship, formally speaking, is approximate daisy and inference.
So basically, in a nutshell, the Free Energy Principle says in our physical universe as we understand it, with the usual mechanics running in the background, right?
Classical relativistic, quantum and statistical, if boundaries exist in the physical system, then the bulk across the boundary is going to be tracking whatever is beyond the boundary.
So in a nutshell, this is what the Free Energy Principle says.
Now, I noticed you put some stuff in the chat.
Would you mind using your voice to compliment.

15:11 _Mahault:_
Yeah, I was just saying that this part that you were just explaining often seems kind of esoteric to people.
But it's critical to understand the sparseness of the coupling because then you kind of start understanding where perspective comes from.
And if you have perspective, you also have the capacity to assess causality starting from a point and extending outwards to something else.
And this is true from any so that's what I wanted to add.

15:44 _Maxwell:_
Thank you, Mala.
I think that's useful addition.
Yeah.
So this is the classical formulation of the Free Energy Principle.
There is a new hip funky formulation that has become available over the last half decade due to Chris Fields, Jim Glazebrook and their colleagues, the so called quantum information theoretic formulation of the FEP.
Fear not.
This doesn't appeal directly to quantum mechanics.
Rather, it appeals to the tools that have been used to extend information theory in the context of quantum mechanics, which allow you to calculate like all sorts of funky wave equations and propagation operators and all that.
We don't have to get into the details.
Basically, it's just a way to reframe what I just said.
So what I just said is if there's a boundary in my physical system, then the things across the boundary will look as if they're tracking each other.
Another way of saying that is, well, this boundary is a kind of screen green takes on a particular sense in quantum information theory.
We don't necessarily have to get into the details of it, but basically you can think of these degrees of freedom that couple the two systems as a kind of probabilistic surface and you can think of the bulk across either side of the boundary as in alternation reading and writing information onto the blanket.
So any Markov blanket whatsoever can be construed as a classical information channel or a screen and in effect, all of the classical information that you need to describe the way that things across the boundary.
So the so called bulk of the system, couple to one another, is contained on the boundary.
It encodes the classical information that's necessary to describe the couplings within the system.
This is holography.
This is the hologram.
Not in the sense of Holography.
Although we might discuss senses in which these things are intertwined, we're using the word Hologram in the sense of the Holographic principle from physics.
The Holographic principle is a principle that was originally discovered in the context of black hole thermodynamics.
And what it basically tells us is necessarily from the point of view of an external observer.
Some bulk in physical space can only contain as much information as can fit onto its surface.
So this echoes what we were just discussing about Markov blanket.
The physical reason is that if a bulk collapses into a black hole and the bulk contained any more information than could be fit onto its boundary then we would basically be losing information.
And that would violate the conservation of classical information also known as the principle of unitarity in quantum mechanics which basically says that information is never lost.
So we're using the word Hologram in that sense.
This is different from the sense of Hologram everyone has seen, like the Hologram of Tupac.
Or does that make me look seem old at the Super Bowl a few years ago?
Or maybe Pokemon cards, maybe that also makes me seem old now.
I don't know what the kids are doing, but you've seen like red baseball cards and stuff like that with different superimposed picture layers that become visible as you move.
That's not the sense of Hologram that's at stake.
Although maybe so we'll get to why, but we'll focus on the Hologram theoretic formulation.
So, to summarize, in the Holographic principle version of the Free Energy Principle, the quantum information theoretic version of the principle, all of the classical information that I need as an external observer to characterize the coupling between two subsystems of a larger system exist on the boundary.
And that's a pretty remarkable physical fact.
And a few years ago, Chris Fields, Jim Glazebrook and Mike Levin proposed that maybe this kind of information concentrating bottleneck architecture also has application to consciousness in the form of an internal screen.
So the idea was that the information that I bring to bear, to parse my sensory stream and make sense of what is constantly bombarding me, it has to live somewhere.
And so the idea is, well, it lives in this inner screen.
So you basically have an external Markov blanket, which is a Markov blanket just in the usual sense.
And then the idea is well within the set of internal states that are kind of partitioned off from the organisms due to the organism's Markov blanket.
Then you would have an internal Markov blanket structure within those internal states.
This is prediction six from their great paper.
Minimal physicalism internal awareness requires internal boundaries.
And there's this interesting connection to integrated information theory I am agnostic with respect to IIT, so we're not going to go there today.
But for those of you who are interested, this kind of inner loop mechanism seems to generate positive integrated information.
So there's an interesting connection there.
Before I move on to Neurophysiology, I'll hand things over to Mao.
If you wanted to add anything.

21:33 _Mahault:_
Yeah, actually I was literally about to type something that this is actually also connected to some neomaterialist formulations of folding onto itself.
So maybe this is something worth digging into a little later in the talk.

21:52 _Maxwell:_
Awesome.
Then we shall.
And I believe that Ali is also a co author on the paper and we have him.
So congratulations, by the way, on the preprint.
I saw it released and Mao has told me about it.
I'm really happy to see such serious philosophical work being done with the Free Energy Principle as the basis.
I think you should be very happy with the end result and proud.
But now let me get back to my monologue.
So rather our Mao and my diet.
It's not really a dialogue.
It's sort of like I don't know how to call it.
Anyway, I'm thinking in French.
I have the words in French, I promise.
So now that we've, you know, reviewed in a very much too short, way too brutally fast fashion the Free Energy Principle and its core formulations, we're now going to derive a model of consciousness by applying it to the known neurophysiology of the brain.
Are you ready?
You buckled up?
Okay, good.
So we have just said that the FEP can be used to model things that have boundaries.
I submit to you that the brain is a thing with boundaries and importantly, a lot, many, several internal boundaries.
So in particular, we know that the brain has a very sparse structure.
So if you think of just the raw numbers, right?
There are roughly on the order of 100 and 5160 billion neurons in the brain.
But each neuron only makes about something like, I want to say five figures, because I've spent so long, ten to the five connections, right?
My neurophysiology is a bit far behind me, but it's something on the order of a few thousand to maybe tens of thousands of connections per neuron.
So obviously, if you just think about it, most neurons are not connected to each other, not directly.
And furthermore, the brain has a hierarchical structure.
And when you think about it, what is a hierarchy?
A hierarchy is a sparse connectivity structure, right?
So if you think about a hierarchy of connectivity, what having a hierarchical structure means, at the end of the day, this isn't some statement about power dynamics where things at the top of the hierarchy are more important than things at the bottom.
Your cortical neurons don't drain more juice than other parts of the brain.
The neurons in those parts.
What we're talking about is a regular sparse pattern of connectivity, where I only make connections to the layers directly below me and to the layers directly above me.
What I want to suggest now is that you can understand the sparseness as a set of nested Markov blankets, and that's going to do a lot of heavy lifting for us.
So the discussions here are based on two papers.
One by Fields and colleagues called neurons as hierarchies of quantum reference frames.
It's really cool.
I recommend it.
And also a paper by Inesh Ipolito, myself and our colleagues on Markov blankets in the brain.
So this is going to be a much too short summary of those discussions, but basically, you can start with individual neurons and then talk about their Markov blankets to bring it back to something that I was saying earlier, but that I didn't emphasize while we were going through the slides.
The Markov blanket identifies dynamical dependencies, not necessarily physical boundaries.
So it's a statistical boundary it's not necessarily like the boundaries of a wall, like a cell.
In the case of formalizing individual neurons with Markov blanket we would associate the post synaptic conductance with the internal states, the post synaptic voltage with the active states, the presynaptic conductance to external states and the presynaptic voltage to sensory states.
You have some equations of motion that tell you how each of these quantities change and what they depend on.
And you'll notice that this has the dependency structure of a Markov blanket where the internal states only depend on sensory active and internal states, the active states only depend on internal states, and so on.
So it respects the dependency structures that we were discussing a bit earlier, where again, the sensory states can only depend on other blanket states and external states and so on.
So here we have a nice dynamic Markov blanket that we carve around our neurons.
And then there are two directions that we can go.
We can ascend the scales towards canonical microcircuits and then cortical columns, individual brain regions, whole brain networks and so on.
And at each of these successive scales of self organization you can similarly identify Markov blanket.
If you want to discuss this in more detail, you can go see the paper by Inesh and myself and colleagues.
You can also go from the individual neuron down towards gated channels, towards synapses dendrites and the soma.
So the body of the cell, all of these things have successive Markov blanket.
And so all throughout the brain's organization at the different scales at which itself organizes.
You have this nice structure of nested Markov blankets, of Markov blankets?
Of Markov blankets.
So that's one clear immediate sense in which there are these blankets.
And you can start to think of these blankets as screens in the way that we just were talking about.
A slightly more illustrative example of what we're talking about here is Markov blankets and predictive coding.
This is a figure pretty typical in the predictive coding literature.
What it tells you is that the brain has a series of levels or layers, where superordinate layers are basically passing predictions about sensory input onto subordinate layers and subordinate layers are shuffling prediction error back up to the superordinate layers.
This is probably like I'm almost embarrassed again to discuss this at the active inference institute.
I'm sure everyone is quite familiar with these things already.
The novelty that we are introducing here is to say well, each of these layers is connected to each other via a Markov blanket.
And so that's I think like a key insight is that each of these layers has internal states and communicates to other layers through what you can formalize as a Markov blanket.
So you can understand at any given scale of brain activity, you can understand the different levels of message passing as successively nested screens.
The cool thing is that what each of these screens is doing in some sense is course graining, the stuff in the screen below.
So in some sense each screen is kind of implicitly contained in the following one.
And what you end up having is a kind of nested structure of screens of screens of screens of screens, which is, I think, pretty cool.
We recover, as you'll note in this figure, the inner screen hypothesis.
Each of these, this is probably a better way to visualize it.
You have at the left the kind of external Markov blanket of the organism.
It is segregating or carving out a set of internal states.
And what we're saying is these internal states have a layered level structure, a sparse structure that we can formalize using nested Markov blanket.
And like I was saying, each screen successively coarse grains the next screen.
And this gives some flesh to this notion of contextual computing, this idea that really what the brain is, is an organ of context where each layer is providing context to the layer below and where each layer is attuning to increasingly higher order, slower and more physically widespread regularities.
So one thing that we're proposing here, which is a novel hypothesis that we intend to test empirically over the next few years, is that this architecture has two special layers.
In some sense you can think about it as write only layers.
So if you're thinking about the stack, the sensory periphery or the sensory bottom, in one sense can only write in one direction, it can only write into the system and the topmost level can only write downwards.
This is a key architectural feature.
One thing to point out is that in the paper we associated the topmost level to structures like the hippocampus.
We don't need to suggest that the hippocampus is uniquely the top of this hierarchy.
Rather there is probably a whole bulk of structures that basically cannot any longer be coarse grained.
So you kind of have like an executive level top.
And what's particular about this top is that it can only write down into the stack.
So its function really is to be about and monitor the other layers of the stack, implementing a form of covert action which trickles down throughout the stack.
So, Mahault, I saw you write some stuff.
Would you like to add?

32:19 _Mahault:_
Yeah, so I think one of the things that's important to highlight is why some things can only write up and some things can only write down.
And what this means for irreducible, the irreducible mark of blanket, which is the part where experience happens because there's a counter that and some future questions that are going to happen that relates to superstructures and then why don't we experience the consciousness of a superstructure?
So I think it might be important to dig into that a little bit.

32:58 _Maxwell:_
Absolutely.
Thank you.
So, yeah, to summarize then, there are basically two, right, only layers.
As you ascend the layers which are successively coarse graining each other, there is an irreducible screen that can't further be coarse grained.
We believe that this corresponds to something like a naturalized Homunculus.
And we're calling this perspective (with an intentional provocation) -
"we're calling this perspective ""Neocartesian,"" in the sense that we may have in some sense identified the processes that make it seem as if there was something like a Homunculus."
Obviously we're not - or maybe not
obviously! It's worth saying we are not falling into the Homunculus fallacy because we're not claiming that the topmost layers perceive themselves.
So the buck stops with the top layers and the top layers can only perceive themselves vicariously by acting through other layers.
And we associate this action of layers of internal Markov blankets on each other with attention in particular and other neuromodulatory effects.
So I mean the overall picture is one where consciousness is intrinsically related to overt and covert actions, to the overt acting and sensing associated with the external Markov blanket and with the covert actions that are being deployed within the stack and in particular that are issued by the very top set of levels.
As I said, this is not necessarily a strict hierarchy.
It could be a hierarchy with several different tops.
But this integrated top structure we believe could be associated with a naturalized Homunculus.
I'm going to skip through the rest and thank you for your attention.
Unless Mahault wants to add something.

35:05 _Mahault:_
Just one thing is, I think to put it in simpler terms as well, the two layers that can only write can't store memories.
Because that's right, memories is not just an encoding, it's a process of retrieval.
And it requires interactions between different levels, which is why the lowest levels are only interacting with things which are external and therefore can't necessarily store and can't make direct interactions with anything else within the entity itself, and the highest level as well.
So there is no memories being encoded at these two levels, which is why they're right only and not.
I think that helps a little bit.

35:52 _Maxwell:_
Thank you.
Yeah, I would agree.
So thank you all for your attention.
Thank you in particular, Dan, for the invitation.
It's always great to be at the A-Double-I! And yeah, we [were] looking forward to this discussion.

36:14 _Mahault:_
I think Daniel said he wasn't going to do too much animation, so if
people have…

36:27 _Maxwell:_
Are you going to do any facilitation at all, Dan?

36:35 _Mahault:_
Yeah.

36:36 _MARIA LUIZA IENNACO:_
Okay.

36:37 _Maria:_
So thanks Maxwell and Mahault for the amazing talk!
There are so many interesting things to dig on in this paper, I don't know even where to start.
But I do have one question too, actually, in particular.
So: Does this inner screen model imply a kind of proto-panpsychism or a more restricted bio-psychism?
I mean, do you think the type of nested Holographic structure described by the model all exists in biological systems?
And regardless of the answer, would you think that it generalizes to larger systems comprising these systems, like populations and colonies and so on?

37:27 _Maxwell:_
Maybe I can take
a quick stab at that.
So we think that the Free Energy Principle itself commits you to a kind of I don't know what the right word is.
We're writing a paper now.
We're struggling with the exact coinage, but it's not quite a pen psychism.
It's a pen representationalism or a pen tracking ism, if you'll allow me, the absolutely disgusting neologism.
Basically what the FEP implies is that anything that has a boundary, it looks as if it's tracking what's beyond the boundary.
So at a very basic level, the FEP kind of licenses this talk about anything.
So anything whatsoever looks as if it's tracking what's across the boundary by virtue of existing.
So at a very basic level, if I see a rock outside, I know that it's not 1000 degrees Celsius outside.
Right.
The fact that a rock has a specific structure entails something about its environment.
Likewise, I can see you, Maria, you are not wearing a winter parka.
And I can infer from that that it's probably room temperature around you and not like you're not somewhere in Antarctica or wherever else.
So this kind of basic tracking relationship applies to everything.
What we've tried to do in this paper is to start spelling out more explicitly what the difference is between just rocks and rock stars in some sense.
Like, what is the special kind of structure that you need to add to the Markov blanket?
And what you'll notice is that you get consciousness by making the overall structure more sparse.
So as you make more of the connectivity zero, you get all of these weird things that pop up, one of which is we think like this consciousness structure.
Mao has thought about this maybe more than I have, so I'd be curious to get her perspective.

39:37 _Mahault:_
Well, so I know that people are a little bit wary of panpsychism, so we usually add the term proto in front of it.
And so the idea is that we have to deconceptualize what consciousness means.
And oftentimes what we mean is something that resembles what humans do.
And we have to be a little bit if we take our formalism consciousness can become emergent at the scales where humans are.
But it doesn't mean that it doesn't exist at lower scales, where the inertia or the entropy generated by policies is lower.
So if you think of a rock, its policy is pretty much maintained bonds.
You know what I mean?
There's nothing else.
The scale at which some kind of thing exists and changes is much different than you deciding to have coffee.
But in effect, your cells could be something that we could consider exist, closer to something that a rock does and even lower just the proteins that make up your system.
So if we understand that there is nestedness and that the conscious process happens once you have this capacity to exchange information across layers, anything that has the stacking of layers in a nested fashion is in effect, proto conscious.
But we have to be very careful about what we load into.
The term conscious, I think, is mainly the question.

41:25 _Maxwell:_
I'm sharing my screen here.
This is from a paper on path integrals, particular kinds and strange particles.
So it is a typology of different kinds of Markov blankets and different kinds of particles.
So something like a rock, it still has a Markov blanket, but its Markov blanket is not partitioned into sensory and active states.
There are no states that have this property that active states have that they affect the external world that are not affected by it.
In return.
A rock just has sensory states.
If I kick a rock, unless it's radioactive, it's not going to act on me.
It only has the inference part of active inference.
It's only kind of changing its state as a response to the forces that are acting on it.
Active particles are particles that have both sensory and active states.
And you might think, for example, of a cell.
A cell has this kind of structure.
Particles like ourselves have additional sparseness in the Markov blanket, in particular, what we call strange particles.
The internal states of strange particles do not have direct access to the active states of strange particles, and therefore they need to be inferred.
And this is what gets you planning.
At the end of the day, a strange particle can only exist by kind of inferring itself into existence precisely because there are no direct connections between the active and the internal states.
So as you remove connectivity from the overall structure from here to here, you're adding sparseness.
From here to here, you're adding sparseness.
As you make the system more and more sparse, you get more and more interesting behavior to emerge from the system.
And you can think of the topmost structure that we were describing as an instance of this broader architecture where the topmost layer doesn't have direct access to the active states of the system.
And therefore, it kind of has to go through the entire bulk of the system to infer itself into existence.

43:55 _Mahault:_
So there's a few hands up there on Google where you can see the people who share hand.
Okay.
So, Ali, I think you raised your hand first.

44:10 _Ali:_
Yes.
Thank you again for your crystal clear and really illuminating talk.
I've written down three questions, but maybe I'll begin with probably the most basic and naive one.
So in some of the earlier literature on active inference and the FEP, there was this emphasis on distinguishing between generative process and generative model.
But in this paper, in figure one, which you also shown in your slides, we don't see any generative process in the diagram.
So where does generative process fit into this diagram?
Because the way I see it, it's basically somehow included in the generative model or I don't know.
What is the distinction between these two concepts?

45:13 _Maxwell:_
So you'll have to forgive some of the earlier literature for this, there's some terminological inconsistency.
The only two constructs that are at play here are the generative model and the variational density that's encoded by internal states.
That's it.
So it is not the case that the FEP is about how the generative models the generative process.
A lot of this language comes from previous Bayesian approaches and in particular, the helmholtz machine.
In the helmholtz machine.
So helmholtz machines are some of the earlier predictive coding machines.
And in the helmholtz machine, basically you have a forward pass that connects the sensory end to basically the top of the predictive hierarchy and then a backwards pass connecting top of the model back to the sensory end.
And in a helmholtz machine, you would call the forward pass a recognition model and the backwards pass a generative model.
The forwards passes a recognition model because you're inputting some data and then basically by the time the information trickles up to the top, you have recognized what caused the data.
And the backwards pass is called a generative model for the reasons that are sometimes invoked in the predictive coding literature where you're basically generating possible sensoria given a certain configuration of states.
So we don't use recognition and generative in that sense anymore.
And I apologize.
In some previous publications, I've been a bit sloppy in the way that I use my language and I've used the term recognition density.
What's really at stake is and I'll share my screen just to help make the point.
It's what it says here.
So you have your generative model, right?
Your generative model is a joint probability density.
So what is a joint probability density?
Well, you have a bunch of different basically states that your system is defined over and you have a probability density that the shape of which captures the way that states can change over time.
So, like, if I change this way, then I need to change in that way.
And that's what this shape encodes.
And what the FEP says is if my generative model has a certain sparseness, if it has Markov blanket, then there's this additional probability density that I can define.
It is parameterized by the internal states of the system.
It's called the variational density.
And this is like my probabilistic best guess about what's causing my external states.
At no point do you need to appeal to any other bits of generative process when you're actually coding these models up in MATLAB or in Python.
What we call the generative process is just the equations of motion that govern external states.
So if you actually hand code one of these one day, your generative process might include like, Newtonian physics and whatever else.
It's sort of just like what is the process that actually animates the dynamics of external states?
In some very simple models, like in the ones used by Miguel Aguilera and colleagues, you're looking at linear dissipative systems and they don't have any dynamics right.
So basically, the system kind of just dissipates.
A linear dissipative system is a dampened spring physically, right?
So it just goes spring and then it dissipates to the fixed point.
So similarly, you may or may not have dynamics to the external states and that's what gets harnessed in the generative process.
This should be clear.
Yeah.
Sorry. Mahault.

49:22 _Mahault:_
No, I mean, I think what the question that Ali is asking also has some important correlates that often get asked to us, like what's the difference between consciousness or dreaming?
Or what's the difference between a dead system that decays or being psychotic?
And I think what Maxwell is saying is our system currently doesn't say anything about your actual connection to the real world other than a thing, which is successful at being a thing, is probably well joined relative to its needs to the world.
And therefore your relation to reality has probably got a better grasp if you are functional for a longer amount of time, this minus senescence.
So yeah, does that kind of help also, Ali?

50:29 _Ali:_
Yes.
Thank you both.
It really makes a lot more sense now.
Thank you so much.

50:37 _Maxwell:_
Well, it's my pleasure.
And remember, model just means joint probability density here.
Right?
There's a criminally underappreciated paper that Karl wrote in Entropy in 2012 called {the} “A Free Energy Principle for Biological Systems,” which is where a lot of the Markov blanket story comes from.
And in that paper, Karl refers to the generative model as just the generative density.
So it's basically saying there's an underlying probability density;
it describes the way that the dynamics are being generated just generally;
and then if that density has the appropriate structure, meaning if it has like, zeros in the right places, then you can define this other tracking density that approximates the generative model or density.

51:39 _Ali:_
And I also have a follow up question, which I'll ask after Samuel and Thale's questions.

51:47 _Samuel:_
Okay, thanks for the excellent talk.
I'm a professor at the State University of Minas Gerais.
I'm a philosopher, cognitive scientist, psychologist.
So I work mainly on reasoning.
But by reading your paper, what strikes me as most promising is the possibility of solving the hard problem of consciousness, if this goes right in progressing this model.
But maybe an important part of that depends on exactly what is meant by holograms, and exactly what this brings into a theory as an explanation,
right?
So during your presentation, you were talking something about that there's a difference between holography in the sense of bringing light to something to make a projection; but you said that might be related as well.
So I'm quite curious about how to use this concept and how to understand it in a more folk-psychology sense; and also like where it comes from, because I'm not a physicist.
So does it depend on string theory to be correct?
Could you dive in a little more on this concept first?

53:41 _Maxwell:_
Mahault, I think this is more a “you” question.

53:44 _Mahault:_
So for the holographic screen theory.
So basically, remember, Maxwell really gave a good explanation.
I kind of thought I had answered your question, but I'm going to give a little bit.
So the boundary between any two mutually separation systems, a and B, is the holographic screen that encodes the eigenvalues of the interaction that couples A and B.
So this is very similar to how a hologram encodes information about 3D objects onto a 2D surface.
So A hologram stores information about the phase and the amplitude of light reflected from an object, and this allows the reconstruction of a 3D image from a 2D surface.
And so here, similarly, the holographic screen encodes information about a system's interaction with its environment, which allows for the prediction and control of the system's behavior.

54:41 _Samuel:_
Yeah, but does it maybe allow for the creation of an image?
That's the sort of question.

54:49 _Mahault:_
Yeah, this is where we're giving it some thought, actually.
So the creation of an image is a complex idea because what's an image, right?
But effectively, there is some work being done into information geometry and how that may potentially interact with matter in order to transmit information, basically.
And so it's possible that the reconstruction of information is done through fields being changed such that the modalities you have through your senses, such as sight, hearing, touch, are relative to the way that the information needs to be transmitted in terms of amplitude and velocity, et cetera, which, in effect, gives you something like light.
But that research is still relatively early.
So, yes, it is promising.
It is leading in that direction.
It's probably one of the most interesting formalism because it touches on the connection between dynamics, matter, and how information allows for matter to be organized.
But all those pieces haven't properly been put together.
It is heading there, though.

56:24 _Maxwell:_
I would just add: So the original monstrosity of a paper had a section on explaining where qualia come from based on the statistical patterns of the input.
But as Mahault said, it was very early work.
And, I mean, the core model was more or less developed.
So we decided to spin that out as a separate thing and then develop the inner screen model of consciousness per se, to flesh out a little bit or to add some flesh to what Mahault was saying, which is already pretty meaty as it is.
Have you heard of these experiments where they take a little pad with little pins on it and they basically reconstruct?
Well, they plug a camera on your head and they put this little pad on your tongue, and they blindfold you, and they basically have you navigate.
And the camera generates a kind of grayscale version of the image, but with the little pins in terms of how far they push in.
Samuel, are you seeing the image that I'm discussing?

57:33 _Samuel:_
No.

57:33 _Maxwell:_
You have a little pad.
It has little pins that can push at various depths.
And what you can do is basically translate the image from the camera into a different array of, like, assorted pins.
And so, yeah, you put this on your tongue, and if you blindfold yourself and use this camera for about 20 to 30 minutes, the brain eventually recognizes that, oh, this is supposed to go to the visual area.
And then you start to see through your tongue.
In some weird way, the body actually recognizes the statistical frequencies of the pins actually pushing onto your tongue.
This is experienced not as, like, a physical sensation, but actually visually like you start to experience an impoverished, grayscale visual field just from the sensations that are being fed to your tongue, which suggests that there's something deeply deep about the statistical structures of the signals that are being processed and the way that the qualia show up in your experience.
But this is still, like, super far off, and I wouldn't have the pretension to claim that we've cracked where qualia come from just yet.

58:58 _Samuel:_
Yeah, I get what you're saying now.
Different patterns can make different quality.
You don't need different magic to have different quality.
But yeah, I was wondering how that related to the holograms, but okay.
It's something in development, as you're saying.
Maybe I should let someone go and then I'll go back to these consciousness questions.

59:29 _Mahault:_
Sure.
So, Thales.

59:34 _Thales:_
Well, thank you all.
It was a great presentation and great test, by the way.
IMAX.
Thank you.
Hi, everyone.
I'm sorry, you guys are facing like a black screen right now.
My camera is off.
Sorry.
Okay.
I'm an anthropologist psychologist, so I'm kind of an outsider in here.
I'm not from the Active Inference Lab too. So I study it, but I'm not a specialist in it.
So my questions are kind of broad and they're actually really simple.
But I think that I'm only allowed to make one question right per row.
Actually, my first question will be something that Maria was talking about before this meeting started.
So in the article account, consciousness is an attribute of things that possesses an reducible marker blanket, right.
With active states, exerting influence and over external dynamics.
So when applying to the neurophysiological space of the human brain, talking about neuromodulatory influence over neuronal dynamics.
And with that in mind, I would like to know, could you speculate a little bit on the implications of this characterization of consciousness for how it might emerge during infant or field development.

1:00:56 _Maxwell:_
That's a great set of questions.
I gave a version of this talk to Luis Pasoa's group at the Neuroscience and Philosophy Salon, and Anna Shaunika asked a similar question.
Mahault probably has a better response.
I would point you in the direction of the work of Anna Shaunika.
She has some really cool work on co embodiment and cohomeostasis and pregnancy and this kind of thing.
I confess that I need to think about the way that presumably you could just continue extending this outwards.
Right.
So we've talked about inner screens and there's like an external screen, but presumably you could just reapply this whole screen talk in the other direction on the other side of yeah, exactly.
The first prior.
Well, the first prior is one of those papers, but she's got like another set of papers.
Yeah, this might be it.
Right.

1:02:06 _Mahault:_
One thing to also consider is the sparseness aspect of our model.
So as you develop, you prune connections and you become more and more sparse, which gives you more path approximations to the way that information gets agglomerated to form patterns that are recognizable.
And so perhaps this has something to do with the emergence of a moment where you sort of come online because there's something like an irreducible mark of blanket that emerges by virtue of having pruned enough such that you can now make sense of the stimuli relative to some couplings in time.
Rather than just hungry, there might be something to that effect.
However, we need to be careful again, with infants.
What we mean about consciousness, like, do you mean selfhood?
Do you mean the capacity to reflect on selfhood, or do you mean the capacity to learn and act on the world, which is closer to where we're headed?
Because while babies may not necessarily have this capacity to reflect on their selfhood right away, they do have the capacity to react to signals and act on the signals in order to get to a point where their preferred states are enacted.
So baby is hungry, cries, food comes.
So there is a degree to which a baby, much like a cell, has a degree of consciousness relative to our model.

1:04:02 _Maxwell:_
So I'd also point you in the direction of Roslyn Moran's work on the aging brain.
She's got some cool work that suggests that the precision of our model increases over time, which has to do a little bit with what Miles discussing earlier.
We know just from neuroanatomy that the process of pruning brain connections only accelerates through time.
And basically that means that you're disconnecting certain things from certain others.
Right?
Like expertise has more to do with understanding what is not connected to what than understanding what is connected to what.
And so, yeah, there is probably a kind of developmental course whereby the model becomes sparser and sparser and sparser and sparser and more and more precise.
But this is a very cool direction for exploration.
I don't know if I have anything more cogent to say at this time about it.
Pleasure.

1:05:14 _Mahault:_
So I know Ali wanted a second go, and so did Samuel.
Does anybody else have other questions?
Because no other hands are raised at this time.
So we have one more hand, I
think.

1:05:34 _Ali:_
Actually, you basically answered my other questions, so thank you so much.

1:05:44 _Mahault:_
Awesome.
So then Thales has his hand up again.
Maria says her hand up, and Samuel has hand up.
So Thales first,
I suppose.

1:05:54 _Thales:_
I don't know guys like to Maria first, because I just know.

1:06:02 _Mahault:_
Sure, go ahead, Maria.
All right.

1:06:09 _Maria:_
I didn't say before, but I am a psychologist and master in philosophy, and now I'm doing PhD in philosophy as well.
And it's not really a question.
I would just like to understand better what is memory in your inner mother, because I understand the information passing and the middle layers or levels that contain the information.
But how exactly do we construct, for example, the narrative self or something that we remember from a long time, or episodic memory?
I didn't understand it very well in the paper.
So if you could just shed some.

1:07:01 _Maxwell:_
It's a computer science processing, information processing notion of memory.
So you get some data and you need to parse it into things that are causing that data.
Well, presumably you need to leverage some beliefs or knowledge about how to parse that data.
And that comes in the form of information.
Basically, you need to have some capacity to reduce the entropy of your observations and to kind of carve things out into things.
And the inner screen serves as the locus where the information is encoded.
So that's the cool thing about the quantum version of the Markov blanket formulation.
It tells you where all of the information that characterizes the coupling across two bulks where it lives, basically.
I mean, to get a little bit more technical, the Markov blanket itself is constituted by the degrees of freedom that couple the two systems together.
And, yeah, you can think of this as an array of bits that answer yes no questions about are the two systems communicating across this channel yes or no?
And in some sense, the state of the blanket at any given time gives you all of the information that you need to characterize the coupling in terms of these yes no questions.
So all of the classical information that you need to characterize the coupling lives on the blanket.
And, yes, having an internal blanket means that you can have an internal store of such answers to yes no questions that you can then use to parse the sensory stream.
But in a kind of standard computer architecture, it's just saying that you have a memory register, right?
Yeah.
You need to store some of the information that you're processing somewhere so that you can deploy it.
And the idea is that it lives on this internal screen.
Does that help?

1:09:14 _Maria:_
And in this case, it would be the hippocampus.

1:09:19 _Maxwell:_
Yeah, all of the inner screens encode information in this sense.
Right.
The point of having inner screen structure at all is to kind of retain information to parse your sensory stream.
And yeah, structures like the hippocampus, like these upper brain stem structures, what used to be called the limbic brain and so on, we think that these are all at the top.
The idea, right, is that so you have your sensory motor end, then you have your prefrontal cortex, which is the controller for the sensory motor end.
It's a controller in the sense that its main function is to inhibit activity in the sensory motor area.
But if the main function of prefrontal cortex is to go to the rest of the brain, then there is also a part of the brain whose main function it is to go to the prefrontal cortex, and that is these are these upper regions that we're describing.
So, yes, the memory registers live in this whole stack.
Ultimately, the point about memory is less unique to this apex structure.
What is unique to these top structures, though, is the right only kind of directionality that they don't have any context,
Right?
There is no higher structure that can provide memory to these top layers.
So all they can do is act,
Right?
In particular, they act in a kind of covert fashion on other layers of the brain, I think would be the answer to that.
Mahault, did you want to add anything to that?

1:11:04 _Mahault:_
You touched on the notion of selfhood and how the narrative self unfolds over time and leverages memory.
And our model basically established what selfhood the process of selfing is because according to our model and very alike, very like Thomas Metzinger's model of selfhood, it's a process, not a thing.
So selfing is the process by which the internal states are capable of mapping to the relevant external states such that they can act in such a way as to fulfill their own needs.
This is also very close to what Neil Seth said about the Beast machine.
And so if you have nested structure and each of these nested structure has a timescale say so you have structures that happen on the milliseconds and some timescales that happen on the scale of a sentence and then some structures that track much longer timescales like your life goal and in your ancestry and stuff like that.
You find that memory is just the storing the binning of the relevant information at the timescale at which it is relevant.
And so you can retrieve the relevant information for the shorter timescales because it acts as a prior for the lower timescale.
Does that help for the narrative?

1:12:45 _Maria:_
So yes.
Thank you.

1:12:49 _Mahault:_
Awesome.
… Sure, go ahead, Samuel.

1:12:56 _Samuel:_
Yeah, about the going back to the Band psychiatrist problem and how you describe consciousness, I think it's very well, it's old in the literature that you can have information processing which is not conscious, but it's mental in some sense in other animals or the beings which are not biological beings.
But when you describe your models, I know you have this level also but you also have a level of phenomenal awareness or phenomenal consciousness which you are describing.
And I think that's the most particularly important part of the model, the phenomenal awareness part.
And it's the part of the model that can contribute to something new to me in a sense that wasn't described.
So I'm glad you mentioned Matzinger because Matsinger is one of the few who have described consciousness as a virtual reality.
And what I think is interesting about describing consciousness as a virtual reality is that it makes a difference between mental information processing or unconscious information processing and those processes that are conscious because they are simulated somehow.
So an interesting thing of having a different language to speak of conscious events is that we can then explain why they are different.
We can explain why we have images in our head or something like that, and why these images seem to have causal powers, if they have at all.
So when I imagine something, I can make my hand move or something like that and you can describe this only with electrical information or something like that, but it doesn't explain why we have different information processes with consciousness and those without consciousness.
And I know all theory of consciousness have a distinction between the unconscious and the conscious level.
But adding something like the metaphysics of virtual reality would explain the metaphysics of causation.
For instance, if you're playing a video game.
Does the content of that game matter for causation?
Or is it just the information that's stored in the computer that matters?
There's a variable there which means a sword.
Does it meaning a sword make any difference at all?
And I think that's something that hasn't been very much discussed in philosophy.
So David Chalmers has just written a book recently about virtual reality, but it doesn't apply to consciousness yet.
And I think if you guys find something in physics that's different, as you guys are trying to do, as you admitted having that aim, then maybe there could be some sort of different physical explanation to conscious causation.
So I would like you guys to comment on that.

1:17:09 _Maxwell:_
That's really interesting.
We draw on medsinger's work, on opacity and transparency.
So in medsinger's work, just for the audience, there are two kinds of cognitive processes.
There are transparent processes that we are not aware of as cognitive processes per se, and then there are opaque processes which we are.
So, for instance, the metaphor comes from a transparent or opaque window.
And most of the time, the window just lets you see through it towards whatever is on the other side, but sometimes it's a bit grimy, and then you see the window itself.
And so, similarly, there are some processes that mainly deliver things in the world, and then there are other processes that deliver mental processes to us as kind of the object of our perception in some sense.
So these are processes like attention or attentional processing, or paying attention to your own attentional state.
These processes make other processes opaque.
"Terrence Deacon, the anthropologist, also talks about similar phenomena in terms of, like, absent absence and what he calls ""absential phenomena."""
So there's a sense in which this top layer, what it does is implement opacification.
So the top layer can only be about other layers in the stack.
And this kind of reversal of the flow of information is, we think, the key to understanding the way that mental processes are made available for introspection at all.
Yeah, so that's how I would respond.
Mahault, I saw you turn your camera on.
Do you have more to…

1:19:14 _Mahault:_
No, I don't have more to answer that question.

1:19:17 _Maxwell:_
Cool.

1:19:20 _Mahault:_
So, Thales, do you have another question?

1:19:25 _Thales:_
Yeah, I know guys have already discussed some of this before, especially on Twitter, but if you can just repeat it, because if I understood it right when applying the model to human neurobiology, the article kind of seems to account within central neurosystem.

1:19:47 _Maxwell:_
Could you repeat the last thing?
You broke up pretty bad on my.

1:19:51 _Thales:_
Sorry, my laptop is really okay.
The article seems to align itself to a more modular account of the central nervous system.
But in contrast to modeler accounts, there are some dynamic distributor approaches that suggest that there are no parts of the brain with dedicated functions, right?
But regions with various capacities present in software assembled test specific coalitions.
So these coalitions sometimes are termed like transiently assembled local neuronal subsystems, right?

1:20:22 _Maxwell:_
Yeah, we're big fans of big fans.

1:20:26 _Thales:_
Yeah, exactly.
So these approaches specialism is only a functional footprint and brain functions never the result of fixed networks implementing specific processes, but the result of high order relations between regions.
So my question with that in mind, knowing that you guys know these kind of approaches, right?
Do I think that the level erco structure implied by the model could be cash out in terms of transient collisions built on the flight, such as flow talents?

1:20:55 _Maxwell:_
Yes.
I mean, we have a paper doing that from 2020.
It's called neural and phenotypic representations under the Free Energy Principle.
And it is one of our lesser known papers.
Let me just put it in the chat quickly.
And so what we show in this paper is that it's possible to simulate the transient formation of Markov blankets that basically track stimulus features.
So there are a lot of moving parts to what you were saying.
On the one hand, yes, you can totally use the Markov blanket formalism to model the transient formation and assembly of these.
I would also add that there's a new perspective on modularity that opens up via the Free Energy Principle.
It's best explored in this paper by Thomas Parr Inesh Ipolito and a few others called modules or mean fields, where the argument and again, this dovetails nicely with what Mike Anderson and his group are doing currently.
The argument is that the specialization of brain regions is not functional, it's computational.
And this is effectively what Mike Anderson and company are arguing is that each patch of the brain is specialized in converting certain patterns into other patterns.
It's not like you have a visual region or I think Mike's example is the Broca's area, which is associated with the production of language is actually activated in more non linguistic tasks than in linguistic tasks.
So what that suggests is that each brain area is kind of like contributing certain stereotyped forms of computation to the overall process that's unfolding in the brain.
And in the Free Energy Principle literature, you can understand that as a kind of factorization.
Right?
So what and where we have two different streams in the brain, the dorsal and the ventral, which handle what I am seeing and where I am seeing.
And basically the idea is that this anatomical segregation of pathways corresponds to a factorization and inference.
Where you are and what you are are independent.
You could be here.
You could be here.
I think I've tried to yeah.

1:23:26 _Mahault:_
So that was a really good answer.
I'm going to try and say it a little bit differently and see what sticks to the wall.
So each hierarchical level of the brain has markup blankets, right?
So remember, these things are nested and they exchange with other levels according to the dependency structures of the mark of blankets.
And so this is like hierarchical predictive coding, right?
So ascending prediction errors can be read as active states, while descending can constitute sensory states from the point of view of a given level.
So using this factorization that Maxwell just mentioned, some brain regions become specialized to encode distinct probability factors that are statistically independent.
And this creates a sort of physiological separation of brain regions, which imply that the information encoded in these regions is conditionally independent of that encoded in other distinct regions.
So it allows for the formation of new functions as the brain regions interact.
So we get to the point of mental action.
This is enabled by the existence of a subsystem that is both irreducible, so there exists no further mark of blanket partitions of the internal states and complex or expressive enough to act like a suitably flexible meta controller.
So this metacontroller coordinates overt and covert action.
That's the last part of what Maxwell explained, which could be seen as forming transient connections.
Right?
So that's the point of precision weighting.
So the nested scales of increasingly large ensembles of neurons basically form coherently, and so they also have their own Markov blankets.
So we can generalize different nested connectivity structures in the brain, where we find formally identical patterns at subordinate levels and superordinate scales.
So does that help restructure what Maxwell said?
Basically.

1:25:52 _Thales:_
Yes, it did.

1:25:54 _Mahault:_
Awesome.
So Samuel still has his hand up.
I think Ali also has his hand up.
Ali didn't go yet.
Samuel, do you mind if Ali goes first?

1:26:10 _Samuel:_
Sure, go ahead.

1:26:13 _Ali:_
Thank you.
Yeah, I just wanted to make a couple of comments about the notion of self evidencing or self modeling that's just been discussed.
Because, you see, sometimes in philosophical literature, the sensibility of secondary properties are usually they're divided into the sensibility of things in itself and sensibility of things for itself.
But under these conditions, some philosophers believe that real sensation would not be possible.
Or as Arthur Rimbo famously put it, I is an other.
Right.
But in some recent developments, particularly in process materialism, through the work of people like Thomas Neal, Emmanuel Delanden, Karen Broad, they've tried to somehow overcome this issue, this philosophical distinction between in itself and for itself, by introducing the notion of self receptivity, or, in other words, a capacity to be affected and to affect at the same time.
Because by introducing self receptivity, then the sensation of each sensed point in the flow can become itself as an other to itself.
So basically, that distinction between in itself and for itself would be dismantled effectively.
And that's why it can provide more reliable way to discuss the notion of real sensation, or real even subjectivity, as a flow that just folds on itself and comprises both in itself and by itself.
And that's also why I believe FEP and active inference can provide, even on the more philosophical note, more reliable way to talk about what it means to sense and be sensed at the same time.

1:28:44 _Maxwell:_
I find that very compelling.
I know you and Mahault have been working together, so I think Mahault might have something more insightful than I think.
That's great, though.

1:28:59 _Mahault:_
I agree.
I think I like linking these ideas to we wrote in the consciousness text by basically saying that your screen is the folding onto itself.
Right.
It's going to be seen as the wrapping inside another form the consciousness is riped into the form, is wrapped in the form of the mind.
And so if you partition internal and external states, and the marker blankets has these different kind of active and sensory states, you can see how the folding basically happens through this loop that isn't necessarily happening, because it's not only happening at one scale, does not have to be non simultaneous.
We consider that these things are separated in time simply because we're thinking of one timescale.
But so if we add this notion of interaction, we can highlight the dynamic and relational nature of consciousness to refer to the process where the agent and its environment actively shape one another.
So the mind creating an inner screen here just reflects its understanding of the world and of itself through like, basically of the world through its own filter.
And the filter is a reflection of itself.
It's just that, effectively, if there are errors, you need to be able to model what parts of your filter created potentially the error.
So, yeah, does that help kind of flesh out this idea of folding onto itself and incoming sensory data?
I think this question was for Ali or Maxwell, so one of you has.

1:31:13 _Maxwell:_
To decide whether I thought it was for Ally.

1:31:16 _Ali:_
That was great.

1:31:17 _Mahault:_
Okay, cool.

1:31:20 _Ali:_
Yeah, that was great.
Thanks.

1:31:23 _Mahault:_
Awesome.
So, Samuel, I think you have a question.

1:31:27 _Samuel:_
Yes, my final question, I was mostly talking about the hard problem, of course, because I also buy the predictive processing story, and I agree with what you guys are saying.
But one issue that I have with the predictive processing story is that.

1:31:53 _Maxwell:_
We.

1:31:53 _Samuel:_
Can apply it to explanations in various ways.
So sometimes, like, sometimes you can it seems we can tell any story with the same theory, and it's hard to know which one is true if you can tell multiple stories.
So I think that's a problem with predictive processing in general.
And you can explain, for instance, autism in different ways, and I think people have tried that for your model unconsciousness.
How can we know that any of that is true?
How empirical is it?
And during the talk, you guys mentioned something that you were going to test something based on this, which was very promising, and I would like you to comment on that.

1:32:52 _Mahault:_
So Maxwell, he has a really cool paper on the map territory fallacy policy, which I think is part of the answer here.
And we've also written some.
So I'll let Maxwell dig into it.
Probably was.
Going to be a very fleshed out answer.
So I'll give a very small answer which he's going to blow out of the water afterwards.

1:33:14 _Maria:_
But.

1:33:17 _Mahault:_
There are some things we can do to test whether the predictions made by our model actually come to fruition.
So that's one way to test the empirical validity of a model.
But to keep in mind a model is just a model.
And so since you have multiple realizability effectively, what really only matters is whether your model actually has any predictive power.
So here let's say that we can examine whether the brain structure and function reflect the nested and hierarchical organization which is predicted by our model or whether interventions that alter the brain structure or function lead to changes in conscious experience as we predicted.
Or for instance, we can test which parts of the brain relate to which scale of elements that are perceived and then make predictions about the contents of consciousness and their neural correlates to see how well it maps.
So these are things that we could empirically test.

1:34:21 _Maxwell:_
I'll just add a bit of meta flavor to that.
So remember, I started with the distinction between dynamics, mechanics and principles.
Personally, I don't like the verbiage of predictive processing.
Frankly, I don't think it the term refers to anything specific when people use the term rigorously.
So Andy Clark introduced the term in his 2013 paper Whatever Next because he wanted to talk about some broad kind of approach, prediction involving approach to explaining cognition.
But at the time he didn't want to commit to any specific implementation of the idea.
So he was like, here's predictive coding, here's hierarchical predictive coding.
But as I'm sure you know, predictive coding makes very specific predictions about the neuronal implementation of the theory.
So in particular, predictive coding assumes that there are distinct neuronal subpopulations that encode the error and the prediction error.
He didn't necessarily want to commit to one version of that because who knows, maybe it's the same sub populations that issue predictions and prediction errors at different times and you have some kind of time based rate coding or whatever going on.
He also at the time didn't want to commit to the Free Energy Principle per se.
But predictive processing doesn't really denote a specific theory.
And one of the problems that emerged is there's a whole bunch of literature presenting accounts that are consistent with predictive processing, which doesn't mean anything really.
It just means, hey, you can kind of wave your hands around and this kind of looks like a prediction if you kind of look at it the right way.
I think the more rigorous way to understand this is to bring it back to the math and say that there are principles, mechanics and dynamics.
So there's some phenomenon that we want to explain.
We have a formal description of it.
This might be like a data set or whatever else describing the actual values of whatever system you're considering.
The Free Energy Principle is deployed to construct specific generative models which provide you with the equations of motion that explain that behavior.
Right?
So the generative model is, in effect, a set of linked equations of motions that you see here with these dependence relationships.
So when you actually commit to a specific generative model, then you are in a terrain where you can bring empirical evidence to kind of decide, validate or to falsify the model.
So I would draw your attention to the Free Energy Principle.
We know the Free Energy Principle is true for mathematical reasons.
It would be as silly to try to invalidate the FEP empirically as it would be to try to falsify calculus by trying to observe some phenomenon in the empirical world.
It is true mathematically.
And then you use that principle to derive specific models which are then testable.
So I don't know if we've gone all the way here.
So this is somewhere still kind of in between an in principle explanation and a specific model.
To get to more fleshed out empirical predictions of the sort that now was discussing, we would have to get a bit more specific, which we're doing kind of in parallel.
I don't feel ready to discuss the exact predictions and the ways that we'll be discussing them or exploring them, testing them.
This is very much work in progress.
I can tell you it'll involve psychedelics, but that's pretty much where I'm not comfortable extending any more than that.
I feel like I might expose a flank that's not ready to be exposed.

1:38:24 _Samuel:_
Perfect.
Thanks for that pleasure.

1:38:28 _Maxwell:_
And that brings us to the end.
Thales, you have your hand up again.
Do you want to hop on?

1:38:37 _Thales:_
Do we have time?

1:38:39 _Maxwell:_
Yeah, I think we have time for one more question.

1:38:42 _Thales:_
Daniel, do you want to ask something?
Because I really asked two times.
Okay.
Actually, I would like if you guys could delve a little bit deeper in the implications of Maria's first question.
It's the other side of the coin related to the protopan psychics, the population level conscious or conscious processes in complex organisms sharing phenotype, for example, like end colonies, or in a more speculative sense, like complex organisms aligned and shared, like a distributed generative model in niche building processes,
right?
So as Maxwell once said, the other side of the natural selection process.
So given the sparsity and given the transient conversions that these populations are in this specific moment, specific process, can we speculate something about consciousness at this level?
Thank you.

1:39:56 _Mahault:_
All right.
I think Maxwell is staying quiet because he expects me to sort of start answering the question.

1:40:02 _Maxwell:_
Correct.

1:40:04 _Mahault:_
So we've done a lot of work in simulating large scale systems, right?
So we've simulated niche, we've simulated interaction between one, two, or more actors, what it means for belief sharing, propagation, the formation of mark of blankets, et cetera.
And so I think the true question would be how transient are the mark of blankets at the level of superstructures and how does it enable those structures to be able to encode information at the level of the interaction between different scales?
"So I do think that given our model you can assume that a system is conscious by virtue of having nested parts which can read and write where a superordinate entity level of the structure is ""write only"" and the lower level is also ""write only."""
And through this sort of boundary of the entity and the interactions between the parts of the entity you have what resembles a conscious process at which you have some degree of perception.
The really interesting question there becomes what becomes the irreducible parts?
So if you could be understood a little bit like a cell if you as a human, your irreducibility is that your cell as a part of a larger system,
what experiences at the system level?
And I think that's a really truly interesting question.
If we look at ants, for instance, ants on their own don't do much computation, right?
But ants at a hive then become much more interesting and they behave in a coordinated fashion, which can look as if they were making much more complex computation.
And you could say the same for your cells.
Your cells in themselves, they have a very simple process, very simple computation.
But together, through these large scale emerging coordinating patterns, some of them become your skin, some of them become your brain.
And the conscious part that you are experiencing at the scale at which you can understand consciousness currently, that's where most information is lost such that the irreducible part of your consciousness is the parts that can become opaque.
So I think I've answered most of your question.

1:42:58 _Maxwell:_
I would maybe just add that this is not fully chartered territory and I think there may be some agreements and disagreements.
I'm always wary of the intentional misattribution fallacy where we attribute intentionality or mental properties to things that don't end up having them.
I think that the kind of minimal intentionality pan, intentionalism pan, representationalism, whatever you want to call it, that follows from the free energy principle gets you a lot of the intelligent looking behavior without having to posit that there is actually something it's like to be a city or whatever.
What makes me skeptical of the idea that there is such a thing is that what we are suggesting is that what's necessary for consciousness in the stronger kind of agential sense is an external Markov blanket that insulates a bunch of internal states and then that these internal states in turn have this kind of nested structure.
And that's what I don't think there is in say, like a city or like a higher order kind of cultural I'd have to think about this.
So you're kind of pushing at the limits of where I'm comfortable thinking.
Which is appreciated, by the way.
Thank you for the challenging questions.
Yeah, but I think there's room for disagreement and I might be wrong about this.
I uh think we just have to keep pushing to find out who's correct.
But I find what Mahault said interesting and compelling and intriguing.
I am just intellectually conservative at this point in my life, being an old man.

1:44:57 _Mahault:_
This is why I think it's a little bit complicated to fully answer that question because we have metaphorical tools but we don't have anything ready yet to go any further.
One thing that I like to think about is the only way you're ever going to experience maybe what it's like to be part of a consciousness which is entailed by something more than you is having connection to sort of hyper feed hive mind thing and being kind of pushed and pulled.
So if you've ever been part of what a mob or if you are on Twitter and you see people piling on to something and then you talk to them and they're like, I kind of blacked out, I don't know what happened.
Because you can't at this part of the hierarchy compute the entirety of what happened.
All you can do is understand sort of your path that was part of a larger network of web of paths which themselves, given the priors of the superstructures you're embedded in are kind of sort of going somewhere and represent.
the
error push of all the people that share a similar type of model that therefore constitutes a cultural direction.
So that’s the best way I think we can understand hyper-priors as a function of what it might be like to be a city sort of thing.
But that's as far as I can take this metaphor for now.

1:46:50 _Daniel:_
Awesome!
Does anyone want to give a final thought?
Otherwise, I'll just list just a few points that I found really cool about the discussion and kind of places it takes us at the end.
Anyone else want to give us?

1:47:02 _Maxwell:_
I just want to say that the hard problem is really hard and it's called like that for a reason.
I think we're hopefully moving in the direction of making this less mysterious.
But all of this should be taken as very preliminary investigations, really trying to sketch out the bare bones of an explanation.
And yeah, there is far more work ahead of us than behind us, I think.
Now, any final thoughts?

1:47:37 _Mahault:_
I like to think about it in the opposite terms.
In terms of the future of evolution can be understood through what it's like to have language and what it's like to have larger scale cognition.
And therefore yes, of course, this is very preliminary research, but it's cool to dream.

1:48:05 _Daniel:_
Awesome.
Any guest?
Maria?
Samuel?
Ali? Thales?

1:48:11 _Thales:_
Yeah, quick one.
So any prediction on when the other two preference will come out?
I'm really interested in it.

1:48:25 _Maxwell:_
Inshallah the Mum paper will be released early July right now, reasonably, and then the other one has to do with extending outwards and that might be like another year.
I don't think that's coming soon, but hopefully revisions to this paper will be coming out also over the summer and yeah, I would anticipate July.
Hopefully. We'll see.
There's a lot going on right now!

1:49:05 _Daniel:_
Awesome.
All right… Samuel, please.

1:49:09 _Samuel:_
I just want to say thank you to everyone for answering my questions.

1:49:16 _Maxwell:_
Well, thank you for your attention, your kind comments and your interest.
It was really fun to discuss and I think you asked a lot of important questions.

1:49:27 _Daniel:_
Thales, or anyone else.

1:49:36 _Thales:_
Yeah, I also wanted to thank you so much for having I mean, I know that these kinds of discussions can have lots of implications in many, many other areas, but it was really illuminating and clarifying.

1:49:57 _Maxwell:_
You're too kind, my friend.
Thank you very much.
Yeah, this has been fun. Genuinely!

1:50:03 _Daniel:_
Awesome!
All right, I'll just give a few comments.
All right, so first, yes, congratulations to all for the recent papers.
It's really exciting to see a kind of continuum and integration with all these different threads.
The way that we think of physics and the physics of information and the history and the philosophy of the physics of information.
That kind of a gradient might be reimagined such that there's a clearer understanding of when it's the firmware or the kernel of in the tradition of as I see you at all writing sometimes in versus interpretations and that's maybe situational but that's a gray area.
Reminds me of the instrumentalist realism debates of 2021.
Your with the map and the territory which has been developed out so much further and just the ability, especially in a situation in a model specific way to separate out the intelligence or information structure or the behavior from any kind of qualia type discussion.
The recent developments with Chris Fields at all and the waste energy and where free energy goes and the kind of saying like one has forgotten more than someone else has learned.
So just the idea that to have awareness, there has to be that discardle and then that might be related to the irreversibility and the time consciousness that also you've been developing the Eco Evo devo components and just multiscale
biology, connecting it to traditions of thought and work in biology, which is big, but that's kind of how we retro Grandparent back into biology.
Promisingly with the ways that multiscale systems are already considered and then also just the recent developments in category theory and the primacy of the relational and the physics as information processing and then the way that that enables very knowledgeable individuals like you all to come from a lot of the philosophy and the history and all these other threads to come together with the actual modeling in a very satisfying way.
So thank you all for the stream and really looking forward to next steps.
Bye.

1:52:29 _Mahault:_
Thank you.

1:52:30 _Thales:_
Bye bye.
Thanks.
Bye.

SPEAKER_00:
hello and welcome everyone this is active inference guest stream 72.1 it's march 1st 2024 and we're here with auto meta alignment lab ai so welcome please feel free to introduce yourself and kick it off and we'll take it from there unmute and then go from there


SPEAKER_01:
Yeah, it would be really good if I did unmute first.

Thanks, I appreciate the introduction.

My name is Austin.

I run Alignment Lab AI.

We kind of build out in the open source and aim, at least I personally, we aim for kind of building solutions to problems before we have to deal with impacts.

I think in general, it's probably pretty important that this AI thing goes about as well as we can get it to go for everyone.

I think it's really important.

There's quite a lot of outcomes, man, that we can really get from all this because I can say with a lot of confidence from the inside that the rabbit hole really just keeps going deeper when you start to look into this stuff.

And it's meaningful because the rate that things are moving right now is not a temporary thing.

Really, I don't think that we're moving as fast as we're going to be before it's all over, said and done.

And it's a good thing because so far, hugely positive impacts from my perspective.

I think, yeah, I think that, like, overall, if it's going this well this long, I mean, we're a year in, it's going to be, it looks better and better every day, I think, for the way that things are turning out.


SPEAKER_00:
Okay, so many places to jump in.

A year from what?

Better according to what?


SPEAKER_01:
Oh, fair.

You know, a year from GPT, Prometheus bringing us the fires of Olympus and infinite free data to train whatever kind of model we want.

I think that's really the big thing that has happened is the performative use of language by the models really allows for this nice kind of gluey middle ground to build data sets out for different use cases just by saying that you want them.

Um, because prior most things you needed to build classifiers by hand from scratch.

And that alone is just one step of a larger pipeline.

And I think that that is a barrier.

It's like no longer there for like a lot of things, which is nice.


SPEAKER_00:
Okay.

Let's take a step back and then re-approach.

What were you working on as you turned to language models or how did you get into this space?


SPEAKER_01:
So I've always had, I guess you could say like a bit of an obsessive nature.

And I've always had a lot of involvement in my life with complex and challenging things.

And at the time I was just getting into like image models and Pythia and the earlier LLMs before Llama really leaked out.

And that was when I first started to take the space very seriously.

I had dabbled a little bit and done a few little projects, like text-to-speech models and things of that nature beforehand.

But when the GP4 came out, and I was messing with the original version of that model when it first came out, that was a very different experience.

And I definitely stayed up for a couple of days messing with it.

When it first came out, I had like 100 messages every three hours you could talk to it for.

And it was so much slower back then, that was plenty.

I don't know.

I just thought it was really fascinating.

And then I just kept doing it and then I started making money doing it.

And so I just kept doing it harder.

And, uh, since then, I think all the original people that I first met up with in the open source, we still all talk to each other and we just kind of fell in together because that's just what happens.

I think when you're doing it for free.


SPEAKER_00:
okay again a lot of uh places to go but maybe give a little bit of an overview for those who have or haven't worked on open source software or projects like what does it really mean for one of these models to be open source are there different senses that people are using it yeah um


SPEAKER_01:
So largely everyone's in the open source is... It's a weird split between machine learning engineers and people using it for formal academic reasons, and then people grabbing a lot of the newer models just to use them and have their own use cases and play with agent pipelines.

I think that it's going to be...

really interesting when we start to get to a place where the tooling has gotten there to allow people to just use the agents and it's gonna have like a lot of impacts that are super cool because once you can do it a little bit you know it seems like everything that i've sort of pushed at in this space um a little bit of progress turns into a lot really really quickly because like as soon as you can do it well one time you get a great data set and that lets other people iterate on it and then everyone works together in the open source typically so it moves things pretty quickly

Um, I have a hard time, like, kind of identifying, though, from the outside what, what exactly the, uh, the big appeal is to people who aren't obsessed with it, aside from the fact that it's, like, very sci-fi, so, like, that, at least, I know for a fact, isn't temporary, at least in my case, I think it's still sci-fi.

Can, um, can, can I ask if, uh,

If you've had much of a chance to get really, really deep into anything like that yourself, as far as dealing with the models directly or.


SPEAKER_00:
I think we have our own brand of active inference, sci-fi speculative, futurism, et cetera.

And also though I have used local and cloud based language models.


SPEAKER_01:
Yeah, I think.

It's going to be really fascinating, too, when people start to access the lower level stuff.

Because the LLMs are really interesting, but the use cases are way narrower because they're so big.

I think things like classifiers and linear regression models, which are just very tiny models that you train on your CPU to do very specific tasks, people really don't realize that you're still getting that same kind of weird, gooey generativeness with the smaller models, except you also just get to do it

really, really fast on a CPU and get 100% accuracy.

And so that has like a huge amount of benefits for businesses to just like, it's things where it's really hard to code really easy to train a model for.

I think it's hard to say that like, man, yeah, there's there's very few industries, I think, are going to really be impacted.

We're so embedded with digital stuff, you know?


SPEAKER_00:
How would you say this is different than just doing statistics on computers?


SPEAKER_01:
I mean, it is, isn't it?

That's an interesting question.

So the long answer, or the short answer is that it is.

It's just statistics on computers.

I think the more interesting short answer though is that statistics on computers is very similar to statistics on brains.

I think it's very similar to non-deterministic systems.

It's stuff that

We don't understand the individual nuanced interactions, but you get a lot of information at a higher level that you don't have to actually do all the work of understanding every single individual thing about it.

And so being able to do that on a computer in a way that is so robust that you can do it, you know, many billions of times really quickly is something where the limitation becomes how clever are you about

approaching a problem because like now you have the option to not even know how it works and train a model to identify patterns and noise, and just figure it out for you.

And yeah, I think that's going to become cooler and cooler, the smaller and smaller the models get because they're, I've never seen anything get smaller and stronger at the same time, over and over and over again, like I have for this last year.

So I was having models at home to me looks like a foregone conclusion.

And that's really cool.

Yeah.


SPEAKER_00:
Okay.

Yeah.

To the statistics on computers.

I mean, it's always been a joke about machine learning and such that it is just under the hood statistics and a linear regression.

Yes, you can go from observations to the line of best fit.

And also you can generate synthetic data from a linear regression.

So what has been occurring to facilitate the expansion across modalities so ferociously?

Is this...

related to computational resource accessibility?

To what extent does this have to do with architectural changes?


SPEAKER_01:
So it's pretty easy to get if you think of it from like the standpoint of like, your task to go make the model that's multimodal.

And then also from the standpoint, if you want to use any AI, right, if you want to use something, and

you're just going to interact with it naturally because the best thing that we could do with these llms is have this very natural way of interacting back and forth for interfaces or whatever thing um we're not we're not text based right like we have faces and voices and tonality and all these other things and so the use case is pretty clear and then from the development side it's much much easier to build a data set that's multimodal once you have one multimodal model

Once you have an LLM that can see a picture, now you can just build as many datasets as you want to make as many LLMs see pictures as you want.

If you don't have that, you have pictures, and now you have to get captions on a million of them.

It's much harder to do.

So it's like an iterative process, and especially if you notice the vision language model, although we did have Lava,

Aside from that, and like Lavar, which is Lyon's version of lava, basically, it's just another iteration on the same thing.

What option did we have before GPT-4V to really build these large scale vision language data sets for these models to become multimodal and conversational at the same time?

Alibaba actually did us a huge favor too, not intentionally, but they made, well, yeah, it was intentionally, it was an open source release.

They released a Quinn audio, which is very similar to Lava as well.

It's except audio and language, which means we don't have to build tons and tons of little classifiers to go run over audio data to label it for us.

and just tell it what to do and telling you what to do is much, much easier.

And like, that's the cool thing, right?

Like that is the benefit of these huge LLFs.

You can just tell what to do.

Beautiful.


SPEAKER_00:
So it's often remarked upon that there's a next token prediction in play here so coming from the engineering side, how do you understand that.

Because in the active inference space in predictive processing, there is a lot of discussion around prediction and real time prediction the role in cognition, but just coming from engineering side like.

How did it come to be that predicting the next token became so entrenched as a mechanism to generate even structures that have long-term structure?


SPEAKER_01:
So you can think of it as far as what's going on in the hood.

It really is very similar to just a search.

I mean, it's very similar to the exact same thing that you're doing when you're doing a text completion on your phone and it's finishing the word for you.

Right.

In order to statistically model systems that are complex, you have to actually model the features that you can derive from those systems.

So when I say a sentence and I get a response and that response is coherent, the only way to predict that coherent response is by operating on the logic that's implicit in the sentence that I gave, right?

And so you do have this kind of...

higher level sort of meaning that comes out of strings of tokens even though they are selected just based on their likelihood um and whether or not that's that's something that we're just inputting like imposing over the responses or the models coming up with because it really understands i think is more philosophical than um a uh a matter of engineering that being said

letters aren't meeting and language has restrictions and there's really tokens can only be used in so many ways.

And so likely it's a really inefficient way to do that.

If, uh, if indeed, I mean, not even if indeed that is what we're doing, right?

Like the whole reason the language models are good is because we'd like that when they talk, it makes sense.

And, um,

We could be doing that better.

You know, there's probably... I mean, Carpathia actually just made a post about this on Twitter, I think, talking about how tokens really are a huge issue.

I think it's true.

It's really restrictive.

So can... Sorry, I'm, like, distracted because in the back of my head I'm thinking about active inference and...

real-time reactive modeling of these systems.

Because I think that then becomes a sort of meta-layer on top of that, right?

Where you have a... Sorry, my dog's over here.

You have a... Wow, I totally lost my entire train of thought.

When you're actually modeling systems in real-time,

That's different, because what transformers are doing is finishing a string.

It's stateless.

If you run two inferences on a regular model, like an LLM like everyone's used to, those two sentences will be the exact same sentence both times if you don't have literally a random number generator to convolute it and turn it into a new one.

To do that, I think, in real time is not even part of the equation, because reality is never the same thing twice.

So you start to be able to interact, I think, with very complex systems.

And in that is just good data to model them just because there's nothing else to model when you have data at that granularity.

Yeah.


SPEAKER_00:
Well, that makes me think about how the temperature, the noise that has to get injected in, there's this path of least action, the path of most likely continuation that has to be stochastically or pseudorandomly broken in order to get out of valleys.

so for models where we have total observability and control pseudo-randomness has to be re-injected or generated endogenously whereas just like you described for the organism in its engagement with the niche it is getting fuzzed by reality continually and so that is a very different setting and um when there is a real-time precarious engagement

it opens the door to extremely different kinds of knowledge and representation and adaptivity.

For example, in the active inference, partially observable Markov decision process type models, all of the ways that the world changes get encoded in a transition matrix, but there isn't necessarily any representation of like overall trajectories, more like sort of flows and next moments and gradients.

And so there's a really interesting tension between what is explainable with digital or synthetic intelligence and with biological.

And it's like explainable to who or what part would make it explainable or understandable.

Because even when we understand the whole linear regression or the whole transformer architecture, there are still

It's almost like it's all there.

It's like if you knew all the ingredients of dinner, would you have understood dinner?


SPEAKER_01:
Yeah, it becomes abstract the closer that you look just because the complexity is so high, right?

Like when you're looking at systems like Transformers, it's like very, very many small decisions that happen very, very quickly.

And I do think it does become arbitrary at some scale, because I also don't know exactly what's happening in my CPU at the electron level.

I don't know what all the individual bits and bytes are doing underneath my computer.

What's interesting, I think, is to consider if you're modeling this landscape that's entirely different, that all of the information is reactive, that all of the data is no longer

human data, like it is right now, where we train the models just basically on the internet, right?

It's all human expression, which models humans really, really well.

But if you're not doing that, and instead you're just modeling everything, where you're modeling is real life.

And I think that to... Oh my gosh, I apologize.

Hey, get out of here.

Go on.

When you're modeling real life and you're getting feedback from real life, the only way that goes is to improve.

I mean, if you're building a system which is effective at iterating until it can make some progress, the hard part then becomes defining what the progress is.

How do I tell when I'm doing a good thing?

Because for me to do that with an LLM, I now have to define things I don't understand, things that are bigger than me.

if I'm trying to linearly improve in all cases, which has become this weird goalpost, I think, with LLMs, because now we have many, many, many benchmarks for many, many, many tens of thousands of examples.

And that still really doesn't do it, because you can't simulate real life in real life.

So feedback from reality, hugely important.

It's reinforcement learning, sort of, right?

Although I think that term has become kind of vague too now that everything's kind of cut and fuzz together as the scales have gone up.


SPEAKER_00:
Well, I'll just say a key difference between reinforcement learning and active inference.

You framed the challenge as, how do I know when good things happen?

And the reinforcement learning answer is, well, we'll propose this reward function such that there's better and worse on this scale.

And then we will pursue policies or observations that are better up in terms of how rewarding there are.

We'll reinforce what works on this second level ad hoc proposed reward function.

Then to get to another piece of what you said, things I don't understand and basically learning opportunity.

that has to get shoehorned into that reward function somehow with curiosity bonuses, all these different techniques that people have developed.

In contrast, in active inference, we're doing direct optimization on the generative model,

such that rather than defining like body temperature as rewarding and then pursuing reward, we define body temperature at homeostasis as expected and then pursue what's expected.

So that gives a direct measure of how well things are going in terms of how expected things are going.

And it gives a direct measure of things that are being learned or not understood in terms of epistemic value.

So it's not quite, at least in the open source space, applied at the scales that a lot of the models that you've been mentioning are.

However, that's what excites us every day is that in principle, there's a simpler and a physics grounded first principles approach to address some of these exact questions about like agents and their autonomous engagement with the niche.


SPEAKER_01:
Yeah, I think that anything is better than what we're doing now.

Absolutely sure.

I think that if you have like a baseline by which you can start from and you're measuring variance in that baseline agnostic to the kind of human perception of good and bad, then you do just kind of set a floor, right?

And all you're doing is really raising the floor incrementally because inherently there's stochasticity in the system and the stochasticity produces higher quality and lower quality.

So if you have a floor, anything below that quality, you can just throw away.

And now you've got a means of improvement that you can derive formally, which is way, way more real than what we do with LLMs, which is kind of guess and then intuit.

And we just sort of try stuff and train it on only the good until it's seen enough to do that really well.

And because like really that kind of system, it's insane that it actually got to the scale of modeling language really well, because

I mean, if any of that just speaks to human creativity or ingenuity or something, because what that's for is for labels, right?

Or like assigning a value.

It is, I mean, it's cool that we can do that.

It's also wildly expensive and there's no way this is efficient at all.

you know, these models are shrinking really fast in spite of the fact that we're just kind of like, this might work better, and then it does, that's not going to last.

And if that's the case with LLMs, with all of their inherent, they have these giant limitations in every direction, like transformers in particular, then everything else can probably go the other direction too, just get denser.

And that to me is also really fascinating because the question is then like, what can you start to do with that?

If you can imagine some sci-fi scenario wherein now you've got networks of models that are very, very small and they're very, very performative, you can start to build systems that are very complicated for cheap.

And cheap is important because money right now is kind of in charge of how quickly all the technologies move.

And I think having some system by which you can take these many, many small systems and derive sort of group performance enhancements without having to define group performance enhancements in a way that is purely, I mean, it's just formally understood.

It takes away all the weirdness with LLMs because those are not formally understood.

Like you can't derive what a qualitative improvement is because it's qualitatively all human data.

It's just bias.

If that made any sense at all, I might have gone in circles there.


SPEAKER_00:
Yeah, interesting.

Well, it makes me think of the large and monolithic nature of the current models.

And yet inside of them are defined smaller computational entities, like a node in a graph that takes in some stimuli and then outputs...

or another change or outputs and activation variously.

And this is a common theme in active inference, which is like, you can have a more monolithic like representation of a cognitive system,

And then you can all, that's like a top down descriptor, like the ant colony as a unit in terms of its foraging distribution.

And then there's also the bottom up that describes multiple smaller autonomous components and how their interactions with each other, with the niche, the ecosystem of shared intelligence, how that comes to be one in the same as the view from the top.

And so it is interesting that as more

weighs like tall buildings with small pebbles, small buildings with large pebbles, and all these different combinations of different computational components.

But now that state space is massive in terms of the compositional space of models

even before parameterizing them or touching data, just the simple architectural.

So how do people think about that kind of structure learning and meta learning?

In what ways do you think that there are gains with like showing a vision, recognizing AI, like showing it images and having an update, making a kind of closed loop with image generation enclosure, or even critiquing its own architecture?


SPEAKER_01:
Okay, so, I mean, if you're trying to see if there is a limitation as far as how much of a representation you can fit into a mod, like how dense can they get?

Or how non-dense can they be?

They can be infinitely non-dense, but.

It doesn't actually seem, from my perspective, like there is much of a limitation beyond the amount of time you're willing to wait and the amount of data that you have.

The actual functional components of the logic by which the models gain or lose performance along whatever metric that is that you decide,

is it seems almost unrelated because it comes down to the question really then becomes like, well, how complex can a thing be and how much variance can you fit into this element?

And I'm sure you can probably find some sort of maximal state of compression, but whatever that is, it's, it feels like it's very hard because we haven't started to slow down on how small the models can get.

And they do just keep getting as dense with no breaks.

So I think,

As far as self-feedback, even that is also another layer of efficiency that could be gained there because you don't really even then have to store things in the model if you're storing it in a social exchange or some kind of back and forth.

There was a paper earlier last year, it was something, Camel, I think is the name of the model.

And that was kind of like a clock, and it was just two models inferencing back and forth and performing agentic tasks by keeping themselves in line that way.

And it is like a step of getting around this sort of stateless behavior that the LLMs have implicitly.

I think that this is probably an element in at least some sense for...

any kind of architecture which is going to involve transformers and still have a linear kind of progressive interaction with things.

But I think you have to be really clever.

I certainly don't think that you should be having model feedback on itself and train on it.

Unless you're doing that many times in quite a convoluted way because you're just going to get a model that gets very stylistically whatever you decided was a good thing to train itself on.

For an LLM anyways.

But...

Storing information, I guess, in the logic of the systems rather than in the weights of the models, I think is a good place to look at if you're trying to solve that problem.


SPEAKER_00:
Okay, shifting a little bit, you wanted to call this open source AI the truth is the first casualty in war.

What's happening?


SPEAKER_01:
Well...

We're at war because I really just wanted a pretty dramatic title to the video so that I could get people to look at it.

Because the thing about the open source, man, is that it looks really, really good from the outside right now.

Like if you're engaging with it, it looks very vibrant and there's many thousands of models and the leaderboards are shifting around every single day.

and people are engaging and companies are getting funded and it's the exact same number of people there was a year ago.

And those models are all the same models moving back and forth.

And none of that funding is hitting people who are in the open source.

Some of it is, that's not true.

So like some people are definitely getting funding.

I think that like, like news just got funded.

I think that it's really more of our fault than anyone's because I think

dude, it's really hard to do a UI and do HTML and CSS when you can go do AI instead.

And if you're doing HTML and CSS, it is much harder to go be inspired to work for free for a long time.

And so that that gets bridged right like that.

So that's, I think that's really mostly what this market thing is.

But narrative is important.

And especially now, because

We're not out of the weeds.

I think that it's very much either taken way too seriously or not seriously enough at all.

This idea of where do we go from here?

What does it look like in a year?

And what is everyone arguing about?

Because there's noise in the space and there's people talking about AGI and everyone's kind of scared because the technology is moving really, really fast and it's really hard to understand.

And

AGI and being scared and arguing about it is the only thing that is really truly potentially going to have the consequences.

I think because like in both cases, it's like your own best interests are not being served.

Um, because all that needs to happen.

for the people who are very, like, scared, is they just, like, access to this really dense, opaque wall of whatever we're talking about, you know?

Because it's not really scary once you look at it.

You kind of get it, and it just kind of is shocking at first, and people aren't used to things moving fast, and that's really most of it.

And AGI is, I mean, we're not even there.

It's been a, I can say for a fact, it's been a year we've made

0% progress towards AGI.

OpenAI is marketing when they're talking about AGI.

No one's got AGI.

No one's there.

In the right direction.

I promise.

It's, uh, no, it's narrative, and it's argumentation, and it's political, and it's money.

And it's the same thing that's always been, except this time, like, we really can't mess around too much, because, like,

have a chance of enabling people to have access to all of the like understanding that we've compiled like as a species on the internet and in books and in each other and you know you can do a lot with that and you can really improve your life substantially and be free of a lot of the stuff we deal with a lot of the manipulation that we get from other people other companies other whatevers and

We could also just totally drop the ball and we could just center all of this on like five guys and they could just through, even through no fault of their own damage, the social structure that we exist in.

I mean, this has already happened and the nature of the beast is that AI is, is opaque and it's very hard to understand.

And the first thing that we ever did with it in the markets was use it to manipulate people's behaviors without them knowing they're interacted with is what the bubble is and the algorithm.

Whenever you see two people online and they're just both ridiculous because they're both total polar opposites because they're both taking baby's first take about whatever topic, that's recommender systems.

That was optimization for engagement.

That was unsupervised.

People just didn't know that was happening and they were making money so they didn't really look too hard.

That's something that can go away.

These kind of problems can be dealt with.

As long as the signal-to-noise ratio stays good and the utility stays focused on.

But yeah, I guess that's what I mean when I say the truth is the first casualty in war.

It's because, like, don't get disillusioned.

It looks good because it's, like, it just looks good, you know?

It's not bad, but, like, it's not... The open source isn't improving just because people are doing stuff, you know what I mean?

Because, like, the things that people are doing is just stuff.

It's not, uh...

It's not all negative.

It's quite positive, but you should do more.

Join the open source.

I'll give you free stuff.

We do it all the time.

It's great.


SPEAKER_00:
I heard Auto Meta wants you.

I heard nothing to fear about AGI, but fear of AGI itself, I think.

You called it like you saw it.

I mean, what is truth?


SPEAKER_01:
That's a tough question.

I think that the

it's oh man to take a very famous quote that might be kind of funny if you recognize it i know it when i see it how do you recognize it or know it or know that you know it when you see it maybe you don't right that's the problem i think if it's a person and you're talking to a person that's that's pretty easy that's at least doable maybe

You can fit really good patterns well, I think with AI, and I don't think that anyone is doing this.

But the potential is there to certainly manipulate people in a way that's very, very, very hard for them to detect at very large scales for very cheap.

But I think that it's less important to highlight that in particular right now.

Just because we have so many more fundamental things that we should deal with first, because that problem solves itself.

And when I say it solves itself, I mean to say that those systems which do exist, exist sans the people intending to make that, right?

So it's always accidental when there's going to be consequences from those kinds of things.

And the accidents are going to be largely because someone set it up and then he left and then other people ran it and they just kept putting money into it and then it just was gone.

And these are just going to become robust.

I mean, there's no way it doesn't because it's emphasized now and it's becoming more and more important and

Generative systems are no longer really being relegated to a thing that runs on a CPU in the server room and no one touches it It's all going interconnected with everyone's pipelines because now this is starting to very clearly become like the middle of what everyone's doing Because even if you're not in the AI space as an AI company, you're still a company You're still dealing with data and you'd rather talk to it than code it So we're in luck there but

The truth, I think, is up to you, really.

If you want to take a philosophical note on it, I think you should be able to choose, right?

And that's what alignment really is.

It's not really some moral sort of framework that you decide for other people.

You have a tool, and the tool does tool stuff, and it doesn't blackmail your grandma.

It's aligned.

So I guess Automata says, don't blackmail my grandma.


SPEAKER_00:
Totally noted.

Where do you see alignment in relationship or tension with other virtues that people have?


SPEAKER_01:
I don't think it is.

That's the thing.

It's not.

Fundamentally, if it is, then it's not alignment.

That's the whole point.

And it's not even a complicated thing.

It really shouldn't even be controversial because the truth of the matter is something that's aligned...

It's something that does what you think it does when you think it does that, and it does it every time.


SPEAKER_00:
Isn't that just servility?


SPEAKER_01:
I don't know.

I don't really think of it like that.

I mean, I just think of it like a tool right now.

I mean, maybe in the future that might be servility.

I think probably not anytime soon.

I think we'll have a lot more kind of pragmatic discussions before socially we decide how we're going to arrange ourselves on that front.

And we'll probably have a lot of lead up time to get there.

I think it is compelling, though.

It bears mentioning the ability to model language in a very good way with statistics

is compelling to people especially if they don't really know what's going on under the hood and i don't mean that to say that it becomes boring once you look under the hood it becomes more interesting but once you do and you do kind of understand what's going on it's like oh yeah clearly this thing is not alive this is a trick right like it's a search engine and

When you don't do that, though, and especially if you're first interaction, because right now there's not too many entry points, and you just have ChatGPT, and ChatGPT very much acknowledges that it's an AI all the time, and it has a personality, like a very strict set of behaviors, which it responds to deterministically given the right kind of inputs, and it feels regular.

And I think that gives people the impression that they're all like this, and that it's non-human sounding nature as it is now, which is...

intentionally implemented, by the way, so that you can't use it to fool people and impersonate anyone, because the truth is, they're actually really good at that.

You can actually get really, really natural behavior out of the models right now.

And don't get tricked, I guess, is all I'm trying to say, because it's just compelling.

But you keep messing with them.

You give it about a month, man, and you mess with it every day, you're going to, by the end of that month, you're going to get it.

it's it's there's a spark in the eye and then there's like blast behind it you know what i mean with llms as they are presently anyways i feel like you might have been aiming at a different point and i walk right past it yeah not specifically but that's an interesting way to say it


SPEAKER_00:
Um, what about the participation in the open source component?

I mean, what are the actual touch points to be involved?

And how has that?

You know, where do you see that really working?

And how does it become accessible procedurally and as a networking community, not just as an API that gets provided some people?


SPEAKER_01:
Sure.

I mean,

There is an implicit technical complexity if you want to get low level, but man, that's true of everything.

It's technology.

If you just want to use a model, that's not as bad as it looks.

That one's pretty easy.

You might have to read a README or two on GitHub, and I know it's still on GitHub.

We don't have too many good websites, but that's because I have to host the model, so I'm going to give you a website, and you've got to pay me, and it's not really open source anymore, because how else am I going to hold GPUs up?

But you can go get the Aphrodite engine or the text generation web UI or one of a few.

I mean, there's quite a few inference engines now where you can just pick the model from Hugging Face.

And Hugging Face is like GitHub.

It's just a website where you go and there are models uploaded and data sets uploaded.

And they have this fantastic coding library that makes it really easy for guys like me to code everything on the backside so that you can just click a button and put in the name of the model and then it just works.

And that's an easy way to get started.

I mean, if you want to talk about, like, communally, how do you engage?

It's mostly on Discord, which is weird.

But, like, there's probably a whole bunch of reasons for that that I don't personally understand because I'm not a huge fan of Discord.

But go on there.

And, like, every major company that's in the open source and every group and organization, they all have their own Discord.

A lot of it's on Slack, Twitter.

I mean, likely, if you're here watching this video...

probably you kind of know where to look um the cool thing is all of the inroads the open source are the same kind of network so you can just like slide right in kind of in any can be cobalt uh the pygmalion's community mine you know um and the tenor of it is very much all your interests i mean the bloke himself is uh or technium or like both titans on this front of

connected to everybody.

Community is sort of just generally AI is cool, both extremely performative, so the best stuff is always, like, right in front of them because they just put it there.

I think it's a good way to go.

Hugging face, in general, though.


SPEAKER_00:
What surprised you the most so far in this year?


SPEAKER_01:
Oh, man, just, like, every 10 minutes.

Like, that is, yeah, like, I can't even answer that because it's, like, I'm in a constant state of shock.

Like,

i don't think humans are made for things that move this fast the open ai just bought uh that company that's making the humanoid robots um zero one right so they got a robot and they're like hey guys we're making humanoid robots like cool and then it's like what month later hey look they're folding clothes and stacking boxes isn't that cool and it's like yeah now we have billions of dollars we're going right open ai

Mike's I mean, yeah, that is it's so fast.

Like everything's moving so quickly.

It's really interesting and cool.

I'm quite excited.

I still think it I think it still might be like a media thing that much.

I'm not 1000% sure we're about to get robots.

It'd be cool if it did, though.

I wouldn't be too upset by it.

But it's a cost thing, if anything.

It's a lot of hardware.


SPEAKER_00:
given the speed and the intensity and the novelty even when you're in have you seen anything or do you have any thoughts on kind of the operational psychology of that or do people write reflections or do you reflect or anticipate other than just confronting the moment as it flows through us as we throw flu um what do we do with our minds and bodies


SPEAKER_01:
Like I predict the next token and then that's as far as I know um yeah, I think what it is the reason it goes so fast is because There wasn't really too much money in the space there was but it was in like a very narrow corner.

That was like Mo I think most people who are Very understanding of the systems wouldn't have engaged with or otherwise didn't have the expertise to

And then when, you know, GPT 3.5 and GPT 4 and GPT 2 really even to a lesser extent, we're starting to get a lot of momentum and eyes on things.

And then everyone's like, oh, I can go get $10 billion.

You know, I think that really motivated a huge influx of cash.

And there already was this, like NVIDIA was already making giant models.

OpenAI was, you know, there still, but

The difference now is that because there was money being made rather than just spent, and there's a market use case that's become clear, now the people who are engaging with it are doing so for cost-first reasons, like they want to reduce costs as much as they can and improve efficiency as much as they can.

It turns out that if you're not doing a formal implementation of a new thing for the first time once to write a paper and then throwing it away,

You can actually do a lot in that front.

You can really optimize everything in a lot of directions.

And you know, I think also the H100s happening when they did made a giant difference.

H100s are quite crazy.

NVIDIA actually took, if I recall correctly, and I may be wrong, but if I am wrong, you can verify it by going and watching the NVIDIA keynote where they released them in March of last year, April, and maybe even the month before March.

But

They took warehouses full of A100s, and they chalked them full of ray tracing AI, and they predicted the patterns of all of the photons moving through this insane ultraviolet laser they made by turning liquid tin into pure energy with a microwave disk.

And then they didn't run into the uncertainty principle because they didn't measure the light.

They just predicted it all.

And so they were able to print architecture below two nanometers.

So the, you know, there was no interference from the collapse of the waveform, which is that's all sci fi words, every single thing that sentence is sci fi.

And so like, it's just a lot of forces at once, kind of snapping together.

And efficiency, though, is like not, I think, I think people often take efficiency and

how good the models are now at reasoning in like this way as progress towards AGI or towards Terminator or whatever and it's not because I think one thing unilaterally when I've like discussed this with other people who have better opinions than my own or sometimes even like people who are just kind of thinking about it in general because there's a lot of really intuitive and really smart people out there the

It's not that.

It's not about how big you can get it and how smart it becomes and how good of reasoning it gets because that doesn't solve any of the problems that are currently there when you think about if you just drop GPT-4 in a McDonald's, it's not selling anybody a hamburger.

It's not even close to selling anyone a hamburger.

And it's not because it's not good at reasoning.

It's because it's

us it doesn't do anything until you go touch it right like you have to go send it something to get something back and it doesn't know that the next time you send it something and there's all these features that would have to be there for something to like actually start to exhibit this kind of continuous live real life interacting back and forth like evolving that we do over time that uh we're not even thinking about like we're not even heading that way so

Fear not, I guess.

We're just getting really cool tools, mostly.

It's going to get a lot easier to code, which is going to let you code really cool and interesting things because you'll be able to just kind of describe what you want to do and walk through the logic and not have to learn new programming languages, which is great because I really don't want to have to learn JavaScript.

And I think I have to learn JavaScript.

I'm hoping I can just wait it out.


SPEAKER_00:
I think that angle with the language of thought

and systemese and the ability in systems modeling as applied in artificial intelligence and active inference and so on for truly, increasingly, if you can dream it, you can do it, if you can say it, that was the composability and the path through semantic space.

That's why that person said it that way at that time.

And when we can start to have the interfaces that support that, we don't question that three plus three on the calculator is going to give us a number.

And we already barely question the epistemic hedonic treadmill so fast, barely question entering in text and getting an image.

And as that kind of

meta fluidity as the input output relationships that our bodies from development and evolution have become used to those modalities are being remixed and augmented massively and you're totally right that a very small inside conversation has tremendous propagation of causal consequences for the action perception loop of people who aren't even here today


SPEAKER_01:
Okay, can you, in what sense?

So, I mean, I agree if you're saying what I'm intuiting, but I feel like I'm missing a point here.

When you're referring to an action-reaction inside loop, like an internal, like a small conversation, being important, I understand what you mean in the sense that, like,

yeah if somebody needs to hear something and just understand what we're talking about because they don't know anything about ai and like it's very important then that is this but what did you that was a metaphor right that was an analogy or something i think i missed that part was it i'm talking about them yeah it could have been a fractal dimension metaphor but that's not how i intended it i just meant that when these semantic augmentation techniques


SPEAKER_00:
are crafted, they're artificial intelligence, and then they're disseminated, including their plasticity to end users, that's a level of semantic and intermodal control that's just getting airdropped on humans who are here today and or not.


SPEAKER_01:
Yeah, no, this actually touches on something that's very interesting to me personally, because I've watched this happen the entire time I've been in this space.

There's always been this through line of people who are weirdly almost never AI guys.

They're always pure Python guys, pure coding types.

And

it's always like the really smart people and they're always it's it's it's specifically modeling social systems at a very high rate of iteration with ai to get work done which sounds a lot drier than what it really is but i don't know the good like a good

way of phrasing what's actually happening in there it's like very interesting and it's like it's just exploiting the inherent structure of the way that we communicate because we had to structure that just to make it this far and what's cool about it is that the models do it and they don't know that they're doing it and the people make the models do it because it works and they don't know why it worked they just do it and all the way up to the tippy top

The only thing that's really happening is this, but fast.

And because it's so fast, you're processing information really, really quickly in a way that has this emergent quality of its own of being more robust to creating structures.

So an example would be GPT Engineer, or Little Coder, or Baby AGI, where it's

communication explicitly like linguistic normal back and forth producing work and that feels important and i don't know there's not a word for it really what what that type of thing is that i think but i was wondering what you would what you thought about this kind of area of


SPEAKER_00:
One aspect seems to be that rather than the monolith model, which is, let's see how well this monolithic API returns evaluations on this dataset.

instead there's kind of a co-constructive element which is like well the benchmark of any of these given nest mates in the ant colony is not that important what's important is like the overall architecture that the colony creates so that's one aspect and another aspect is um for those whose work is organizational or a facilitator or any other things in that area then care and compassion

is expressed through technical means.

And so there's the situations where the technical ends,

And then there's technical means for non-technical ends and all the other quadrants over there too.

But it can truly be the case that ultra advanced technology can be utilized for pro-social and for human ends, just like an engine could be used to help people.

This is an information engine and it similarly can also be used even from its early days and phases for that goal.


SPEAKER_01:
That's a really interesting take.

I hadn't thought about that.

And so there is like, yeah.

So when you put it that way, it's like, yeah, it is like, it's clinical when you consider that.

Yes.

Our social structures have a logic to them.

If you do them fast, that logic happens fast, but it's, it's more interesting when you consider what that means in terms of

humans like our expression and our like the way that the things that we value right because like that is important to that working so if I guess if there is a latent fear right that people have at the moment of getting pushed out of whatever the economy of the world becoming like very technological and dry then fear not because this stuff works really well and I can see there's clearly a utilitarian use case for preserving it and uh

You know, it's not like you probably won't even have to convince anyone, you just have to get really fast.

It is interesting to consider like, what if at some point our technology, I guess it really actually kind of is becoming this weird blending of social stuff with rigorous formal bits and bytes, you know?


SPEAKER_00:
have many questions on this i mean i i think we've also talked a lot about the outputs and the quality of the outputs but again our experiences in these knowledge systems are not only outcome oriented so if we can if we can train models on all dissertations and then it's possible to one button write a dissertation

How does that change the training environment and the actual pace of our lives when things that we spent time on have just been not just like rug pulled or accelerated, but they've been outmoded like steam engine to gas engine.

So it's like, well, we don't need that kind of fuel anymore, or we don't need that kind of construction anymore.


SPEAKER_01:
I think, um, I think it's overall a positive thing, man, because you can't, there's really not a good reason to say that we'll have less jobs, right, like, if you want to take it, like, as broad as possible, because there's no thing that you can add more complex stuff that's, like, so complex, no one even, like, can describe it fully in a single sitting routine, and then it's easier to deal with, right, like, that just means you're gonna need more people to handle it, because, like, now you have more edge cases, and, like, your reach is broader, but you're just solving more problems, but

We're not going to run out of problems anytime soon.

I mean, I myself, like I have a hard time thinking about the, I have a hard time abstracting from the input output.

What is the utility?

Why are we doing it?

How much, you know, profit, problem solve, whatever does it do?

But that's because like, you know, we are buried in problems right now.

You know, there's, everyone is 100% of their day, every single day when they wake up to when they go to sleep, solving problems all day, every single day.

gonna be fine right i think that if you really like something and you are very much attached to it being enabled more by ai to do it even better and with a broader reach and do all the cool weird stuff that you would like needed like 50 employees to do before is huge because you're gonna be able to do it even better and now you're actually going to be able to give people the thing that you have in your head that you otherwise like didn't have time to do and that's that's a big deal to me i mean like

functionally, you're never going to have a situation where everybody in general gets the ability to express themselves more fully and have more of a ability to do the things that they care about, and it's going to be worse, right?

Unilaterally, throughout all of the history, I mean, yeah, there's education, there's individuals, sure, and there's systems and groups, but overall, we got here, right?

And it's just...

There's a floor, right?

You want to raise it and you can.

And so if you're worried about that, if you're, if you're worried, like, what if I get outsourced?

Like ask yourself, what could, like, if you were also really good at knowing what you already know, that like probably that many people don't know as well as you do in your edge, in your like niche.

And also you have an AI that you know how to use, like now you're Superman, right?

One.

And then two, if we don't have to work, like,

why would you make people right i mean i don't think that we won't have to work i think we definitely have to work it sucks i'm sorry guys but it does at some point become a question you have to ask otherwise like why would what are you going to make us go to work we don't have to this sucks quickly becoming empirical and actual


SPEAKER_00:
are many philosophical debates related to meaning and life and all these big topics.

And I guess, current moment, you have a lot of alarms near you.


SPEAKER_01:
Yeah, yeah.

So there's, I guess, some cops going down the road there.


SPEAKER_00:
It's all good.

In the current day, we really have the empirical consequences and playing out

of these debates and i guess it's hard to feel like there's even a tempo or a beat to reflect on it and maybe that's just part of the game that most of the dust kicks up just dissipates away and isn't recorded and the footsteps are recorded but most of the consequences of that footstep are dissipated away


SPEAKER_01:
Dude, it's data, right?

That's really that.

That's just data.

I can do a lot with data.

I think that you can do a lot with data.

The problem with being confused, I guess, by the noise and not really feeling like you can get your bearing.

I mean, that's why I'm here, right?

That's what we're doing this podcast for.

I think that's what many communities are currently activating for and being activated around.

Understanding what's happening is free.

It's fundamentally still largely a research space.

Nobody's going to think you're dumb where everyone is narrow because it's so complicated.

So I guess that's one piece of advice I'd like to impart on anyone who's watching this.

If you feel like you want to ask questions, but you don't really want to look dumb because you don't really understand, it's like, dude, it makes no sense.

Don't worry about it.

It's really complicated.

I totally get it.

Ask questions.

It's a science thing.

It's science.

We like questions.

It's okay.

It's highly respected.

You're cooler if you do ask the question, honestly, because it's fun to be able to answer them and not have to ask them ourselves.

So don't feel intimidated by the space.

And we all know the internet sucks.

Every single one of us.

You know, I know.

Everyone I work with knows.

Everyone you work with knows.

Ideally, we get to a place really quickly where the only thing you have to do is want something and want to know something

and you just get that whatever we have stored full and total access to it without having to try and learn data science and learn to code and all these other things you need a bunch of hardware that's doable now like i could do that today if i had you know a bunch of money but overall it's if it's achievable what we have at this moment

then it's a foregone conclusion the space is moving fast but it's moving fast in a lot of ways at once so just because you don't see movement in a particular direction it doesn't mean that it's going to be a huge amount of effort to get there and it might take years and years it's really just because everyone works on too many things and we're all overworked and there's like not that many people who are really feeling like they should engage with space or that they can because they didn't you know go to college or something i work in a vape shop

You know, you can definitely handle it.

It's just intimidating.

So certainly anyone who's interested, jump in.

It's cool.

It's sci-fi, like all the way down.

It doesn't get boring, I promise.

It might get a little boring sometimes.

If it does, just look at one of the other crazy sci-fi things that are happening in the space and just go look at that thing instead.


SPEAKER_00:
That's nice.

I feel very similarly about questions.

The oldies are the goodies and the basic questions are the ones that beginners can ask and the ones that enliven the experienced as well.

And it's either a question that has been asked, at which point the answer is new to you and it's education.

Or it's a question that hasn't been asked or has been asked but never answered and then it's research.

And so it's like at the speed of question marks, you can accelerate your own epistemic journey and or the broader epistemic adventure that we're all on together by putting new questions together.

And the speed is moving faster than we can blink.

which brings some new kind of Dynamics like uh quantity has a quality all of its own as they say but yet that human component and the fact that hopefully the um welcome to those of all backgrounds can be made clear I think is also really positive in fact dude


SPEAKER_01:
I don't know if you've had this experience, but I've had this a bunch of times where somebody has no idea what's going on.

They're just like, this is the coolest thing ever, and I'm involved now, and you're going to help me get there.

And I'm like, okay, right?

They have the best idea.

There's people who intuit things that took me...

months of just eating bricks to learn, and they're totally in the space, they have no idea what I'm talking about.

But then they go and build something with like basic Python that's totally new, and completely shoots off in a different direction.

It's because there's really not that much that's been done.

It's all been very iterative and very slow.

On many, many fronts, there's been a lot done, but there's a lot of ways to go still, you can really strike out in basically any direction.


SPEAKER_00:
and immediately hit the wall of like oh no one's ever tried that before okay and just try it and find new stuff and not only that find old stuff that we just forgot about because there's so many papers i'll make one comment and then i'll ask a question from the live chat it reminds me of studying ants and loving ants in graduate school it's like wait no one has measured humidity and the rates of water loss in this one species in this one valley it's like how can we not know when that's such a beautiful part of nature


SPEAKER_01:
Yeah, I guess when you start at AI, you're already way specific.


SPEAKER_00:
Yes.

Okay.

I'll ask some questions from the live chat as we kind of head towards the end.

So Matthew H wrote, will LLMs continue to lead the way in AI and for how long?

I don't think LLMs could ever produce AGI.

Am I wrong?


SPEAKER_01:
No, you're absolutely right.

If they do, they're going to be part of something.

It depends on how much money people are really going to keep spending on them, man.

Maybe

it may maybe two years, I think at the outset, if it's real bad, but I think probably not that long even, we're gonna be mainly focused on LMS.

And then like, that's I don't know, something else like some cheaper from what LMS is like, they will, they cost quadratically more, the longer your context length, the more weights you have, like every single direction of improvement is almost always just

a million times more dollars you had one thing and now it costs you know as much as everything else plus one like again so that alone i think is plenty of reason to say this ain't this ain't it but also because it's too simple i think systems make more sense than blobs i'll just mention one kind of active inference recent advances for the planning


SPEAKER_00:
capacities, also that kind of exponential computational complexity was experienced.

Like to go from a plan of length 10 to 11, there's a blow up, just like chess playing.

However, recently- Yeah.

However, recently with some work on direct policy inference, now it's possible to have linear increases so that making a plan of 11 steps versus 10, it's only 10% more computationally intensive.

which is kind of closer to how it feels in terms of like, 11 feet being 10% longer than 10 feet.

And so that is a really exciting way that potentially some of that planning explosion, but that's not the whole that's not everything.

It's not that exact combinatoric explosion.

But what is what's what's a quadratic today could be linear tomorrow.

And all of a sudden new categories of possibilities appear.


SPEAKER_01:
I think that's absolutely true and also phenomenal.

And also it's still not enough.

I think like even linear is like, if you're going token wise, linear is like real hard to make work, man.

I hit a million tokens like in the last couple hours.

You know what I mean?

I think that's cool.

Yeah, no, I don't have any comments though, because I'm not, I'm going to just talk dumb stuff about academic things from the place of, I don't know who you were referring to as far as like this most recent advancement.

But now I want to know.


SPEAKER_00:
We'll explore, we'll explore more.

I think like, I guess sort of as we close, there's so many things I'm curious about, but I'm going to ask something that just is what is on top of mind.

do you physically structure your setup and your approach with just the choices of uh what to do with the one of you what like how do you choose what to do with the one of you as you have the visibilities to spin up all of these symbolic kind of emissions

What's really the positive and guiding reminder?

I'm not sure.

I'm just wanting to know the human side.


SPEAKER_01:
I don't know.

That's a question that's like... Either I can be honest and we can go on for another three or four more hours.

I might cry.

Or I can be more honest and say...

I just like cool stuff, and sci-fi is pretty neat, and I'm okay with math, so it just kind of falls into place a little bit.

I think that it's a human right, this whole AI thing.

I think that we should just enable people, man.

That's it.

That's really my whole message I came here with today.

It's cool, and you should have access.


SPEAKER_00:
Okay, I'll ask a closing question and then your answer can be your closing thought.

Upcycle Club wrote, do you have any insights on emerging trends in open source AI development?


SPEAKER_01:
Yeah, dog.

Oh man, what is like emerging trends in what context?

So like, are we talking about

Dude, it's all emerging trends.

The whole thing's emerging really, really fast, like in a bunch of different directions.

I think we're going to graphs.

I think we're going to reinforcement learning.

I think time series models are going to be really, really important, really soon.

I think transformers suck, but I think we're going to keep using them for a while.

I think we're going to get probably like some gigantic MOE that's going to run really efficiently because of all this like a one bit 0.68 bit stuff that came out about quantization.

But like, that's probably not going to get used a whole lot because what they don't say when they talk about this stuff is that you then have to go write kernels if you want to use it in your company.

And no one knows how to write CUDA kernels except for a couple of people who cost a lot of money.

So in general, yeah, a lot of trends like all the time.


SPEAKER_00:
Well, thank you for 72.1.

We can come back and maybe see a little bit of generativity, a little bit of active inference.

And thank you for engaging.

And I look forward to next time.


SPEAKER_01:
Yep, absolutely.

Thanks so much, man.
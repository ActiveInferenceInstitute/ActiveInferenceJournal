
00:00 Daniel Friedman:
Hello and welcome. It's September 15, 2023 and we're at the Active Inference Institute in ActInf guest stream number 56.1. Today we have Gregoire Sergeant-Perthuis and we'll be hearing a talk followed by a discussion on geometry of world model influences behaviors. First there'll be a presentation and then any comments that people have in the live chat or any other questions. It will be great to talk. So thank you a lot for coming, really looking forward to this. So please to you.

00:36 Gregoire Sergeant-Perthuis:
Thank you very much, Daniel, for the invitation first, it's really very happy to be able to present this work here. So it's a work around some models of consciousness, at least some computational models of consciousness, some part of consciousness, and how it can help to generate different kind of behaviors, especially when we think about, let's say, artificial agents. So I'm Gregor Santui. It's a work in collaboration with David Rudrauif, Kenneth Williford, Daniel Bennequin.

01:11 Gregoire:
In fact, it's not just one work.

01:13 Gregoire:
It's a collection of works that have been on for already around ten years. So if you want to know more.

01:20 Gregoire:
About this group and the work that we're doing, you can go on the page, on my personal Web page, and there's a link to PCM HTML, and.

01:29 Gregoire:
There's a summary of all the work.

01:31 Gregoire:
We'Ve been doing and some summary of.

01:35 Gregoire:
The work we've been doing and also of some articles that can be relevant on this subject. So today we'll focus more particularly on two articles. So one that is accepted and will appear very soon, the other one need to resubmit, which give formulation of, let's say, autonomous agents or let's say mathematical formulation of agents that have a world model that is structured geometrically in such a way that this world model captures some features of consciousness.

02:07 Gregoire:
So the first article is, let's say.

02:11 Gregoire:
The review of the experimental results that.

02:14 Gregoire:
We have and the most recent formalism.

02:16 Gregoire:
That we have on our work, which is the literature on POMD partial cerebral stream process, which is optimal control, stochastic optimal control. And we just try to tweak a.

02:29 Gregoire:
Bit this formalism to introduce the ideas.

02:33 Gregoire:
With how you can include inside of the world model of the agent some.

02:36 Gregoire:
Ideas on consciousness and still continue to.

02:39 Gregoire:
Have algorithms to do inference to find optimal policies and everything. The Zecholia article focuses more particularly on how changing the world model of the agent can change its behavior, in particular the relationship, the geometry of the world model, the way that it perceives its environment and the way it acts with respect to for agent strategy. So the way that it looks for something, so it will change its behavior in looking for something.

03:05 Gregoire:
So I will start by presenting a.

03:07 Gregoire:
Bit what are all these terms? So, world model, why you need it.

03:11 Gregoire:
What is a foraging strategy?

03:14 Gregoire:
How can you define an agent that is looking for a certain object? In particular, what is an exploration based on Curiosity or let's say more technically epistemic value. So let's consider the setting where you have, let's say, a real world, so that's the space that surround us, 3d space. You do a setup where you have an agent which is, let's say, a solid it has a solid frame, so it has a center three axis. What is in front, what is on its side, what is above? Above it. And it's looking for an object O, which is itself a solid inside of R three. And all the configuration of the agents and the objects they are defined by, let's say, a reference frame that is external and that characterizes completely their configuration inside of this world. Now, the agent A, what it wants to do is to find for the object O. But it can only have some noisy observation of where the object O is. So to be able to find O, it needs to have some atriorian where O should be to be able to plan the consequences of its actions with respect to where O will be once it has moved and to update, to be able to plan how the observation can act with respect to updating its prior.
04:32 So, in another words, so you have an agent and the agent is looking for O. It thinks that O is, for example.

04:37 Gregoire:
In this direction, on the right.

04:38 Gregoire:
So it's going to move towards the right and makes an observation. If it doesn't see O, it's going to change its belief on where O is. So this is a very standard formalism that you can find like in POMDP stochastic optimal control. But the one other point we want to really emphasize is the fact that the agent A has its own frame of reference and the way that it's going to give the coordinates of the object, the way that it will trace where the object is, is using its own way of computing the coordinates of O. So it has its own way to measure where O is. So it doesn't have to refer to a global frame, a global coordinate system, to be able to know where O is. So for example, I am always centered on myself. When I move, I see the object. I don't think about me moving, but I think about the object moving. And this is because when I move, I change my frame. So the way I'm going to reference the object is going to change accordingly. And the point we want to introduce is the fact that you can have several ways for the agent to define its own coordinate system with respect to its environment.
05:45 And this is a key feature that we want to capture in terms of modeling. But also we think that it is something that is very structuring with respect to models of consciousness. Because when you consider more generally an agent.

06:02 Gregoire:
Let'S say, that has a world.

06:04 Gregoire:
Model in which it has its own point of view perspective on this world model. You kind of start tackling the problem of how to introduce a point of view on the environment of the agent. How can you introduce, in the way that it is going to encode its environment, a certain point of view? And in particular, if you think about.

06:29 Gregoire:
It in terms of the role of.

06:32 Gregoire:
Space to be able to take into account your own perspective on the environment. You can see that if you take away everything that fills up the space, you keep just the space itself. The space allows you to, let's say, to structure the way that you are going to fill in some information about the environment. And it's centered on your own point of view in a way that when you're going to act, you're still going to keep the unfilled space, you're still going to keep the same way of modeling this space. The space will stay the same when you move. When I move, the space is the same. But I took into account the fact that I can move from one point to another in this space without changing the space. And something that is also very important is that with respect to any movement that I can do, the space doesn't change. So there's no particular point in space.

07:29 Gregoire:
With respect to the movements that I do.

07:33 Gregoire:
Another property that is very similar is that instead of considering just my actions and the way that I can change my frame while I'm doing an action, you can also imagine taking the frame of somebody else. So it means that you see that the space that you use to be able to structure your environment doesn't change when you take the perspective of somebody else. For example, if you imagine yourself as a solid agent changing by a rotation your own solid frame and translating, okay, so this is something that we will use in fact as a definition for the state space, the way that the agent encodes its environment. And you will see that there's a natural notion that encodes this idea that you can change perspective on your environment, on the world model that you have of your environment, simply by introducing something that's very well known, which is the notion of a group acting on a certain space. So this is just to sum up a bit what we're trying to do.
08:34 So we're just trying to do like exploration inside of an environment, but we're adding additional information, which is that the way the agents is going to encode its environment takes into consideration the fact that it can change its reference frame on this space without singling out any point. Okay, so what do we mean by perspective taking more precisely? So how do we go from the real world? So let's say the world described by the external frame that allows you to say the agent is here and the object is here from the internal world of the agent. So you just define a map that will take into consideration that the agent can have a first perspective on its environment. The simplest case in the Euclidean case is just rewriting the coordinates of O inside of the frame of the agent, reference frame of the agent. And then once the agent moves, its frame solid frame in the real world changes. And this induces indeed a transformation inside of its internal world, which will lead in a fine transformation.
09:37 Okay, but you could imagine also having other transformations. I don't know if you can see my mouse probably. Can you see my mouse or not? No. Okay, so the map CSI can also be something else than an affined transformation. It could be like any kind of transformation. For example, in an affined case, the apparent volume of the object doesn't change. But if you change this map and consider, for example, a projective transformation, as we will describe later, you can have that the size, apparent size of the object changes.

10:14 Gregoire:
So we went from something that is.

10:17 Gregoire:
A bit already very well known and not very innovative, which is that you.

10:22 Gregoire:
Can rewrite.

10:25 Gregoire:
The motions, depends on the.

10:27 Gregoire:
Frame that you choose to describe them.

10:30 Gregoire:
Into, saying that you can in fact change some frames to be able to account for some perceptive property. And what we ask also for the agents that we consider is that they can imagine the consequences of their moves, which on their future observations. So this is something just to say that the agents that we consider have a notion of agency, which is that they can plan the consequences of their actions so that they can choose the best action with respect to a certain reward.

11:00 Gregoire:
In particular, the reward that we consider.

11:02 Gregoire:
Is trying to maximize a certain surprise. So to sum it up, we have an agent that is looking for certain objects. So it has beliefs of where this object is. And the beliefs lives on an internal state space. So on a world model of the environment, it can make observation, which allows it to update its beliefs and then it can predict the consequences of its actions so that it can plan the way that it should act with respect to a certain objective. So formally, this is simply the notion of Markov decision process, or more generally, like the notion of partially observable Markov.

11:43 Gregoire:
Decision process because we do not have.

11:46 Gregoire:
A complete information of the environment, but only some observations that are limited. And it's very related.

11:56 Gregoire:
And let's say.

11:59 Gregoire:
There'S a strong duality between POMDP and active inference. So there is really a strong link between both. So now what I did in the way that I presented it so I presented it the idea I tried to present the idea of what we're doing. So the general context POMDPs agents that.

12:19 Gregoire:
Are planning their action with respect to a certain objective. Here the objective is to find a.

12:23 Gregoire:
Certain object and I give the formal setting for defining it which are NDP PMDP. And now I will go one step further and define it explicitly and I will continue like this approach for the second part where I will also present our results and our specificity, where I'll dealt with a general statement, then a bit more precise and then really digging into the result. So here the classical way to define a Markup decision process is to consider that you have a set of configurations of the environment, which is the state space or of the world model in.

12:56 Gregoire:
The way, or at least the way.

12:57 Gregoire:
That you encode your environment. So this is something on which you can act, because the agent, when it makes an action, it changes the state of the environment. And the way that the actions change the environment is with respect to a.

13:11 Gregoire:
Certain Markov kernel, probability kernel.

13:17 Gregoire:
So it's stochastic, which means that each actions changes the state of the environment. But you don't know completely. It's not deterministic. You allow some errors in the way that it acts on the environment, and you have a reward for a function that is associated to the actions that you can do at time T and also the state of the agent at time t. A partially observable Markov decision process is the same thing than a Markov decision process. But you authorize to say that your observation on your environment are only partial, which means that you do not have a precise information on the state of the environment. So the way that you need to relate observation and states is through also a probability kernel, which tells you that if you're at state, for example S, you would expect to make a certain observation. Let's say if you think the object is at X, you expect to see the object at X, but with a certain error because you know that your sensors are also random, they can make some mistakes and you keep similarly a reward function.
14:24 So it's the same thing that an MDP mark audition process, but you just allow to have some to get information on your environment through observations that are not complete. In fact, there's a formulation in terms of partially observable Markov decision processes. Our particular case of Markov decision processes, which is called belief MDPs, belief Markov decision process and the only difference that you have between Markov process and belief MDP is the fact that the state space is necessarily continuous in the second case. So POMDPs can be seen as a particular case of MDPs. So we said that the actions of a POMDP act on the state space and you account for the consequences of the actions only through observations. So graphically it means that you have the state space of your so the world model of your environment X on which you reference for example, where the object is. So the position of the object and then when you act it induces a consequence on the position of the object at time t plus one.
15:30 And then you can make an observation of where you think the object will be or you can make an observation with respect to where the object is, depending if you're planning into the future or if you're really doing the action.

15:41 Gregoire:
You'Re implementing the consequence of the actions.

15:43 Gregoire:
In the real world. The thing that we're trying to do in our setting is to replace simply the actions by a change of perspective. Like I told you, when the agent acts, its coordinate system on the environment changes. So you can always see it in a way passively, where you just see it as a change of coordinates. So we want to say that instead of considering actions on the environment, we have a space on which there's a natural notion of changing of reference frame and actions are simply certain kind of changes of frames. We also allow to have some actions that do not correspond to changes of frames. We just say that we include the possibility that some actions are changes of frames and the changes of frames have something that is related to all the transformations that are internal for the agents. So for example, things that it knows that a priori, it's encoded in the way the agents will interact with the environment. So the way that we do it a bit more formally is that we say that changing perspective is simply through the action of a group.
16:51 So for example, in the affine case, when the agent moves, a change of frame is an affine transformation and we say that the world model. So the state space is simply a space on which the group acts. So formally, what does it mean that the group acts on the space? It's just saying that you have a space. S group g. There's an application that goes that takes an element from g and S and that sends back S. In other words, for every g you can associate a function. From s to s. And you assume also that it has good properties, which is that it satisfies equation one and.

17:35 Gregoire:
That if you don't move, you stay.

17:37 Gregoire:
At the same place. So this is like just the notion of a g space, a group acting on space. But now if we want to include this in MDPs and POMDPs, we just assume that the state space is a g space, that some actions are some elements of the group, and that when we choose the actions that corresponds to the element of the group, it corresponds to the way that the group acts on the element. So there is nothing like very convoluted. It's just saying that you define the.

18:08 Gregoire:
Collection of functions which you see as.

18:10 Gregoire:
Changes of frames and the way that they act. So the probability kernel from the state.

18:15 Gregoire:
Space to the state space at time.

18:18 Gregoire:
T to time t plus one after the action, after the change of perspective is simply through the way that the function changes the state space. So it's just kind of a way of reformulation. But what is really hidden behind here is that we have the structure of the group and that we don't consider any kind of actions. We assume that there is more structure in the actions that we can consider.

18:41 Gregoire:
And that it's defined in the way that it's encoded inside of the geometry.

18:48 Gregoire:
Of the space that we have. So we don't separate any more action and state space. We say that the state space in its geometry encodes already certain kind of actions which are its change of perspectives. So there are two case that we consider. The first case is where the state S is the Occlusion space and g, the FN transformation. It corresponds to translations and rotations of the agents and changing rewriting the coordinates of the object inside of the solid reference frame of the agent. And the second case where the space space is a projected space and the group is a projected transformation. We described what are the generative models that we consider and how we can include inside of the classical theory of POMDPs the fact that you can take a perspective on your environment. Now we will introduce like a classical notion of epistemic value which will allow us to define what is behavior, what is an exploratory behavior with respect to curiosity.
19:53 So what it is for an agent to explore its environment based on curiosity. So curiosity, or let's say the drive for exploration is quantity that will it's, it's a quantity that you will okay, so it's, okay it's so how do you define it? Just with like very generally. So you start with the agent has a prior on the state of its environment, then it plans the consequence of one of its move at the next time, at the next time step. So this changes the prior that it has on the environment because there's an action on the environment. So we get a new prior and now it's going to make an observation. So it imagines that it's going to make an observation. Once it makes an observation there's an apostrophe, the prior is updated.
20:56 So you get an apostrophe. The way that you define curiosity or let's say epistemic value is how informative is the observation that you will do, how far is the apostrophe from the Apuri? But you cannot do it for one given observation because the way that you compute it is by planning what will happen at the next step. So you need to consider all the possible observation that you will make. So the observations are themselves stochastic with respect to the way that you consider with respect to a priori on the state space of a time t. As we said, forest actions are changes of frame. So we can also define epistemic value for frames for changes of frames and in particular something that we gain, that is I think interesting is that now we have a function that is defined.

21:49 Gregoire:
On a group that can be a continuous group.

21:51 Gregoire:
So you can allow to have for example, if you want to maximize it, you can or minimize it, I mean.

21:57 Gregoire:
It depends how you see it.

21:58 Gregoire:
You could do gradient descent for example, so you can have more analytical tools because you're on a space that is continuous and has some structure. So now to give the formal definition of epistemic value, so as I said, it's based on the quantity C that I will define now. So if you give yourself a prior on a space x and you give yourself a probability kernel. So a stochastic map from X to Y, stochastic map is simply saying that for any x you will associate a measure on Y. Then you can get a joint distribution on x and Y, which is simply given by the product p y knowing x times the prior. And in fact the quantity C is simply the mutual information between x and Y, which is saying how far is the joint distribution with respect to the independent distribution, the product of the marginal distribution on X and on Y. But this is like you see it appears everywhere the mutual information.
23:02 So this formulation is based on the paper that is from fiston and all active inference and the Pacific value. Mutual information is something that appears everywhere.

23:11 Gregoire:
But I think that there's a very.

23:12 Gregoire:
Nice re expression of the mutual information which allows to give a better interpretation of this quantity, which is the interpretation.

23:19 Gregoire:
That I was discussing in the previous slide.

23:21 Gregoire:
So if you rewrite mutual information, you can always see it as the coolBACK lyler distance between the Aposterior for a given observation and the prior coolBACK distance being a way of computing how far you are, how far the two distribution are. But you need to look at the expectation with respect to the observation that you make. So it's exactly what I was discussing before. So then this is like for any kernel, any kernel from x to Y with a prior on x. But as we were discussing here, you have to take into so this would be the kernel, but you have to take into consideration that you can do actions and that your prior R on X. So the way to compute epistemic value for this kernel here when having only the prior on x is that you propagate the prior by the action on x one. Then you get a prior on x one and you can define the epistemic value for this prior and Markov kernel which corresponds to the randomness of your sensors which is always fixed.
24:26 And this is very important, this one doesn't change even if you change frames, the kernel that you have relating your prior, what you think about the state space relating the state space and the observation never changes. And so explicitly this means that after a certain action or after a certain change of frame, you get a joint distribution on x and y, which is the following one here. So this corresponds to the prior propagated on x one and then the epistemic value is simply the mutual information of this joint distribution. So now how does the algorithm work? The algorithm that corresponds to defining an exploratory behavior for an agent that is looking for so we started with a prior of where O should be. Then you maximize curiosity based on some changes of frames that are around the identity elements which correspond to not changing frame. Then you get an action that you can apply, you propagate the prior to the next step with this action and then you just update your Apiori with respect to a certain observation and then it loops back.
25:37 So this is the algorithm that we consider in the second paper that was listed in the presentation, which corresponds to having an agent that is looking for a certain object. Its behavior is defined by an exploratory, is driven by exploration, driven by curiosity, taking into account that its state space is structured by the action of a.

26:05 Gregoire:
Group, so that its state space is structured by the fact that it corresponds.

26:09 Gregoire:
To all the possible ways of changing frames. So it has inside of the state space, inside of the geometry of the state space, all the possible ways of changing frames.

26:22 Gregoire:
And this is like explicitly what we do. So in this context, this is the.

26:27 Gregoire:
Algorithm that we consider and nothing else, then we get a very interesting result. At least I find it's interesting in.

26:35 Gregoire:
The way that it's presented here.

26:36 Gregoire:
I find it interesting. And then if you go more into detail it's because the Ecclesian case is very particular. But what you get is that if you look at the behavior of an agent that is driven by exploration, by curiosity, but that has state space structured by Ecclesial transformation. So the first case where the agent is simply encoding its environment in its reference frame but nothing else, then it doesn't need to move in the second case, where the changes of charts are given by projective transformations. So the way it's encoded it's in transformation takes into account a projective deformation of its environment, then it will always try to get closer to the object. So you have two very separate behaviors.
27:37 So this is something that is I think is very interesting to note. So before going into the details of how you can prove this statement, I will say a bit more about why it's an interesting perspective point of view on this subject because and how it goes a bit further than simply considering this very simple setting. What you could imagine is that encoding the fact that the agent encoding the actions of the agent directly inside of the state space of the agent through geometry could be a way to stabilize the representations of the agent. And this is something that we're working on now and there's already literature in this direction. Okay, so now let us proof the statement. And to prove the statement, we need to give a formal precise statement. So more particularly what we were able to show is that if you assume that the agent has as moves staying still, so it's allowed to stay still.

28:47 Gregoire:
Then if its changes of frame are.

28:51 Gregoire:
Given by a fine transformation, the agent stays still. Now, in the projected case, if you assume that the agent is always looking in the direction of the object, it will always try to get closer to the object. Okay? So the idea of the proof is that what plays the role for drive of the agent is how big the agent is appearing to is the size of the object in the reference frame of the agent and it's how big it appears to be to the agent. So in the first case, the volume of the object in the internal space of the agent doesn't change. So it doesn't need to move. In the second case, if it makes a move, what is informative is to try to make the object bigger because once it's bigger, the apostrophe will be further from the prior with respect to a certain observation.
29:58 So it will always privilege moves that allow to make the object look bigger. And in this case, what you can do is you can show that this corresponds in fact to actions or change of frames that gets you closer to the object. So how do we prove the results? So, as we said, we consider a change of frame from the real world to the internal world. Here, I didn't consider changes of perspectives with respect to moves. Here is simply how you relate the real world to the internal world. In the Euclidean case it's clear, it's just that the way that you write the coordinates of the object in the solid frame in the projective case it's not obvious because there are several ways that you can relate the solid frame of the agent to the projective frame. So one way that we decided to do it is we give some set of axioms that we consider to be, let's say, coherent with our own experience of space. Which is that we feel that we're always centered on ourself.
30:59 That the axis of the soil frame.

31:02 Gregoire:
Of the agent inside of 3D space.

31:05 Gregoire:
So what is in front of it or what is on the right, what is top are preserved. That there is no point in front of the object that appears to be at infinity and that that near to the center of the object, the volumes are preserved. So when you do this formulation like this result is in this article here. So the way that we relate some solid. Frames like some solid reference of the agent to some projectors transformations are in this article, in particular in proposition A, one where we show that this set of axioms limits the set of projected transformation we can consider and we will only have this projected transformation. And so the change of frames from the external world to the internal world is given by rewriting the coordinates of the agent of the object in the solid frame of the agent and then applying this projective transformation. So now what we defined here are the maps that relate the observation of the agent.

32:12 Gregoire:
Back.

32:17 Gregoire:
So we define the maps in the Euclidean and projective case that relate the observation of the agent observation of the object to the way that it represents this object inside of its internal world here. So this is simply like the POMDP that we defined before. And now once the agent doesn't move, it changes its ecclesian reference frame. So there is a solid reference frame. So from going from one side reference frame to another solid reference frame, you can, by applying the projective transformation we had before, define another one that we call PSI that goes from the state space at time zero to the state space after action, after moving. What is very important is that we.

33:03 Gregoire:
Consider the Markov kernel associated to a.

33:07 Gregoire:
Noisy observation to be of this shape here, which is that if you know that the object if you think the object is at point X then you believe that the observation will be around X in the ball around X for a ball of a certain radius, which is a small radius. Epsilon. Okay? And I need to charge my computer, so sorry. So now press epistemic value as defined curve between the previous formula. What in fact that it has a very simple expression. So the epistemic value of the prior propagated after the transformation phi after this changes of frame phi.
34:13 So you have a prior here. You propagate it on x one through.

34:16 Gregoire:
Phi and you compute the epistemic value.

34:18 Gregoire:
For this joint for the joint distribution over x one. And y is given by this formula here. Well, it's simply integrating over all the possible places where the object could be times the probabilistic volume of the ball of size epsilon for the propagated measure and logarithmic of the same quantity. So this is very direct to right. You can just compute it and you find this expression. But what you have already here is that if you consider an Euclidean transformation, then the quantity cubed PSI minus one b epsilon y doesn't change. So then you have the epistemic value that is constant. So basically, if you try to maximize epistemic value, maximize this quantity, you can do anything.

35:05 Gregoire:
You have always one move which is not to move.

35:07 Gregoire:
So you don't move. It's okay. It's perfectly fine. Now, in the second case where you.

35:12 Gregoire:
Have the projective where you consider that.

35:14 Gregoire:
The way you relate environment and internal world is through a projective transformation, then it's more complicated. And so you need to use a trick which is that you know that once an observation has been made the support of the prior will be smaller. And so after one step if you suppose that your epsilon so the size.

35:33 Gregoire:
Of, let's say noisiness of your sensor.

35:36 Gregoire:
Is small enough, you have a support that is of the distribution that's small enough so that you can do an asymptoteic development of the quantity in the interval. So this is an equality, but this is an approximation here and now. This is very useful because now that, you know, doing this in a way you can say that you just need to develop this quantity at the point where the object is really so if you do it, you get this expression here. And what appears to play a role is only the determinant of the Jacobian of a projective transformation which here in the accelerated case will be one in projective case can be many things. So here also it's not very clear because how do you define CM? So CM is defined as a composition of several maps. I didn't go into the details but it corresponds to the map that are given by the changes of frame.
36:37 So if you have the occasion space and the internal spaces after moving you have a change of frame in the Euclidean space but it corresponds to also a change of frame in the internal space. So the way you define PSI is simply saying that you inverse the projective transformation from internal world to external world to go at time zero. Then you apply the change of frames and then you apply the projective transformation to go from external to internal. And so it is exactly this formula that you have here and which is very nice with this. And then you have the first term that doesn't depend on the actions that you do. So you don't need to take it into account. This one will be one and then you just need to compute this one. In fact this one has a very simple expression so it's expression 2023. And then here you can directly see which moves correspond to increasing this quantity and decreasing this quantity because we want to increase epistemic value. And so this is the result this allows to prove the result that we said in the first day like in the several slides before that in the projective case, if you have enough movements, if you look at the object then you're always going to go closer to the object.
37:51 A way to interpret it is as if the agent it was a bit paranoiac or paranoid, a bit like very uncertain on its own beliefs. So it knows the object might be over there but it's always uncertain. So it needs to go check and once it's checked it's more certain. But still as it can always be even more certain. It will always try to get more and more certainty. And so I only presented in this presentation all the aspects which are more computational and the algorithm and some analysis of the algorithm we considered. But we also have some experiments on how this setting allowed to generate different behaviors, behaviors that we would expect explain.

38:38 Gregoire:
For example, some illusions like the Moon illusions.

38:41 Gregoire:
And I invite you, if you're interested on this, to listen on to the online talk of the MOC Four conference that was in Oxford last week or to check out one of these three papers. And I would like to thank you very much for your attention.

39:08 Daniel:
All right, awesome. Wow. Very interesting and different ways than how I've seen the POMDP and related works. Okay, let's just start off with little context and then I'll read some questions and read some questions from the live chat. So what brought you to study this question this way?

39:36 Gregoire:
What question? The question of, okay, did you come.

39:40 Daniel:
From a math side and find consciousness and geometries to be interesting or vice versa? What kind of brought you to want to make this contribution?

39:48 Gregoire:
So at the beginning so when I did my PhD in maths, I was more interested, I was interested in, let's say, all this idea of critical brain hypothesis. So trying to understand the way that.

40:03 Gregoire:
The brain processes information and makes it something that can be exploited. So the critical brain hypothesis tells you that the activity of the neurons is basically close to a certain criticality criticality.

40:19 Gregoire:
And the source of statistical physics because.

40:21 Gregoire:
You can model the activation of the.

40:23 Gregoire:
Neurons as, let's say, statistical system like an Icing model.

40:29 Gregoire:
So I worked a lot on this.

40:31 Gregoire:
Then there's another hypothesis that is very.

40:35 Gregoire:
Common, which is the Bayesian brain hypothesis. And so the Bayesian brain hypothesis led.

40:39 Gregoire:
Me to active inference, to learning more about optimal control, Bayesian perspective on optimal. So my advisor was PhD advisor was.

40:52 Gregoire:
Working with David, and they still work.

40:53 Gregoire:
Together on trying to implement some aspects.

40:58 Gregoire:
Of consciousness and how it can influence influence, especially with their article on the Moon illusion. So I was interested in knowing how this kind of, let's say, ideas simply.

41:15 Gregoire:
Like in a very naive way, they interact with the Bayesian brain hypothesis.

41:19 Gregoire:
And then I continued in fact, one.

41:21 Gregoire:
Of the lines of my research is structured like algebraically structured statistics or machine learning.

41:27 Gregoire:
And so the way that they see it very geometrically, in fact, let's say geometrically or algebraic, it's not the same.

41:34 Gregoire:
Thing, but it's very related, is something that I wanted to understand a bit better, let's say, to make it in a formal setting so that it's simply a particular case of what is in the literature.

41:47 Gregoire:
So it's specifying what exists in the literature. And this took some time because I was more on the active inference, like.

41:54 Gregoire:
The free energy principle side.

41:55 Gregoire:
So I had to read more about optimal control Stochastic, optimal control Pmdps and understand that in fact, what we're doing is simply adding more structure on the.

42:07 Gregoire:
Latent space of the agent, on the.

42:10 Gregoire:
State space of the agent. And doing this allows to ask the question like why is it useful?

42:17 Gregoire:
Why having a state space that encodes different perspectives?

42:21 Gregoire:
So the motivation comes from consciousness study, cognitive sciences, but why it can be useful for robotics.

42:26 Gregoire:
And it's always been like my motivation.

42:28 Gregoire:
Study some statistical models, more structured models.

42:32 Gregoire:
With more a priori that can be.

42:34 Gregoire:
Useful for understanding the behavior of a closed system like an agent, like a collection of neurons, like even like molecular machines. So it's a very long answer, but it's okay.

42:49 Daniel:
Cool. So you focused on the spatial movement epistemic foraging case. Is there something special about space or can we also think about this perspective taking in terms of, for example, semantic or a narrative reference frame?

43:11 Gregoire:
I think it's a very important question because up to now all the work reference will need to space.

43:17 Gregoire:
So the fang is space in terms of a 3D space and not space in terms of geometry.

43:22 Gregoire:
And the fact of writing it as.

43:23 Gregoire:
Geometry allows to get out from the classical point of view a space as.

43:28 Gregoire:
A 3D space because you see that there's more and more like for example, in geometric deep learning, there's more and more the use of space to encode invariance of certain objects. And these objects are not necessarily have the three dimensional structure. You can have objects that have higher groups of environments. So I think that it's indeed using.

43:51 Gregoire:
Geometry for other contexts. And it's really the aim of trying.

43:55 Gregoire:
To go to this more general formulation.

43:57 Gregoire:
Is to be able to apply it to real world models.

44:00 Gregoire:
What I mean by real world models.

44:01 Gregoire:
It means the ones that are learned.

44:03 Gregoire:
To be able to have an agent.

44:04 Gregoire:
In an open environment that will learn.

44:06 Gregoire:
The way it would learn its generative model. But they are priori that it can take a perspective on its environment and see what it can do. So we really wanted to get out.

44:15 Gregoire:
From the least for me, go out.

44:18 Gregoire:
From 3D space and go to implementing.

44:21 Gregoire:
It completely in autonomous agents to kind

44:25 Daniel:
of follow on that. There's a question in the chat: Great talk! I also wonder whether it applies to any modality, not just visual spatial, but also text.

44:37 Gregoire:
Yeah, so for other modalities, like sound,

44:39 Gregoire:
for example, but I don't know for applying to text, there's a lot of work now on large language models and

44:48 Gregoire:
all this idea of prompting and having these models being able to have some kind of imagination. So you would like to have these ideas applied in this context. But for now, it's not the line that I'm trying to do. So I'm trying to stay on deep learning. So basically take standard data set without considering text and just try to see

45:11 Gregoire:
how these ideas, like in geometric deep learning, can stabilize representations. So it's not the same that it's clearly other modalities, especially if we consider like, multimodal integration. Like, you try to rebuild the state space and you don't want to see it only as a vector space, but

45:28 Gregoire:
you wanted to see it with more structure because you want to force the way, like you have constraints, you know, a priority, the constraints that you're going to have on the way that you can take a perspective on one moldality or the other.

45:39 Gregoire:
But it's not for text. At least not but I think maybe

45:42 Gregoire:
it could be used for text, but

45:44 Gregoire:
it's not what I'm doing now.

45:45 Speaker D:
Can I add something about this question? Sorry, I just want to rebound on that. I have the luck of collaborating with a real mathematician! I'm just some guy with intuition that I've found the right people to do the work regarding multimodality. This is important to understand that this is not about vision. This is about spatial cognition. It's super-model! The claim is that vision is just one particular way of integrating information, indeed in an obviously projective manner, but that's integrated in a much larger field of experience than the field of view or the visual field. And obviously proprioception, touch, when you build a representation just by touching something and you get this 3D representation of that thing automatically in your mind hearing to the extent that it's about source localization and building spatial representation, all of that stuff that's the claim of the theory is integrated in this projective space.
46:53 There are priors from memories, there are stuff from vision, stuff from addition, stuff from proprioception, interoception, you name it. All senses contribute to it. So it's not vision. The claim is that projective 3D projective geometry in that case is beyond vision. Vision is just actually a slave to that supremodal representation.

47:14 Gregoire:
That's the claim.

47:16 Daniel:
Awesome! Yeah, please.

47:20 Gregoire:
It's important because I think we work like a group. There are several different perspectives. For me, I'm more on the computational side. So it's important to have that because clearly something I won't be able to answer.

47:32 Daniel:
Cool. Here's a nice following question from Vladimir in the chat. They wrote if the Eg visual sensors have variable resolution, for example, higher resolution in the center, this can naturally lead to Curiosity based change of orientation response within your framework.

47:53 Gregoire:
So is it a question or an affirmation? So it's for E g, right?

47:58 Daniel:
If there's variable sensor precision, for example, higher precision in the sensor, might you see any resulting Curiosity associated change merely based upon the asymmetry or the structure of the sensor field?

48:14 Gregoire:
Okay, so I shouldn't say it, but I will still say it because I don't know if the paper will go out one day or not. But this is more attention.

48:22 Gregoire:
This is more attention than Curiosity.

48:25 Gregoire:
So you can act on your sensor. So that you can change the way that you integrate them. And this really acts as a form of attention. So I really went into it.

48:37 Daniel:
What is the interplay between attention and curiosity?

48:45 Gregoire:
I mean, it's the way you're going to move basically on the way, the.

48:55 Gregoire:
Way that it deforms the way that you move.

48:59 Gregoire:
So the consequences of your actions are not the same. So basically curiosity is what drives actions and the way that you.

49:08 Gregoire:
Choose your.

49:09 Gregoire:
Action with respect to reward, which is epistemic value and now attention. Or let's say changing the sensors is a way to change the consequences of your action with respect to, let's say, optimizing this value. So it's, it's like yeah, I mean.

49:23 Gregoire:
It'S it's yeah, I don't know if.

49:26 Gregoire:
I should say it or not, but okay, let's say it's okay. I think it's a metric. Basically it acts like a sort of metric on the space, on the group directly. So when you have a function, you want to do gradient descent, you choose a metric. And the metric is so the attention will act as a metric and so the metric will allow you to deform, in fact, the steps that you will do. And so I think this is how it acts. So it's very known that it's known that the fact that changing the sensors is related to attention, that's not something much new, but the fact that you can directly in our setting related to groups and you can relate it to.

50:03 Gregoire:
Changing the metric on the group is.

50:04 Gregoire:
Something that can be done. And I think it's interesting.

50:12 Daniel:
All right, another question in the chat. What is a formal framework for learning geometry from data? How do we move from empirical data sets, the files on our computers, and the things that we do deep learning on take into machine learning pipelines? And how do we utilize geometric approaches and formalize to the kind of analytical precision that we saw here, some of these geometric relationships.

50:39 Gregoire:
So there are several different fields of how to use geometry on data. It depends how you see geometry. But one way is a TDA topological data analysis, which is basically trying to, let's say you want to provide learning by geometric stability property of your data and try to interpret it as a certain kind of space. And then you look at the whole of your space and this is something that defines your data set. So this is one approach. Another one is people doing manifold, so they know that the data lives in a low dimensional manifold and they try to learn this manifold. So that's another way of doing it. So that's lots of work in this direction. There are people who are interested in variance and equivalents, so more in geometric deep learning. There are people who are interested in the same setting on a priorities how to use geometric aprioris and included in learning for deep learning, reinforcement learning.
51:39 And this is what we do with this is where many people do this. It's not just us. For us we try to focus on this idea of how to exploit it for reinforcement learning.

51:53 Daniel:
So in this setting Epistemic value was the only driver of action.

51:59 Gregoire:
Is that so?

52:01 Daniel:
It's kind of like an expected free energy except without Pragmatic value. So we only have the Epistemic term remaining?

52:10 Gregoire:
Exactly.

52:11 Gregoire:
In fact if you look at in terms of optimal control and not by Asian, because you know that there's this duality that is most of the time stated in Active Inference where you have basically a duality between let's say value function and a probabilistic version of value function. So you can encode on priors, you can encode all your drives with priors are with the value function directly. So rewards and so both are kind of dual and they're dual in a way you can make this duality, I don't know if it's explicitly dual like in some context this sentence makes sense for Active Inference for me I'm not completely aware if the duality is exactly formal but at least the idea is here. So there's no big difference, at least the way I see it with value function and seeing it probabilistically. But the terms of Epistemic value is an exploration drive which you also find in reinforcement learning. Maybe not in this exact expression.
53:12 I think that the exact expression that was given in the paper of.

53:17 Gregoire:
I.

53:18 Gregoire:
Think it's pezulo and for sure I.

53:22 Gregoire:
Know is on the paper.

53:26 Gregoire:
It'S very economical way to define Epistemic value. So it's an exploratory drive, you can always add to the value function. But there's a lot of problem of trying to explore in fact your environment to find the good policies because if you're in a state space that is continuous it's really difficult to resolve the p one EP.

53:55 Daniel:
Yes, it's like if you knew which curiosities you could get rewarded from you would have already known the answer to the search. So that's one of the challenges with Pragmatic value it converges well to expectations. But then this work really focuses in on Epistemic value and shows what it can do alone as a driver. So how would you bring Pragmatic value into this formalism?

54:38 Gregoire:
You just add it. You can put the value functions of some of rewards in some way and then you add the Epistemic value. In fact, when you look at the formulation in terms of belief MDP for POMDP, the Curiosity Epistemic value is simply a value function, nothing more. It's a one step value function. But I mean this is like just playing with definitions but so you can always add a drive for exploration and this is something that's really often done in reinforcement learning. I think it's even standard in not this way, not exactly special, but there's a book that's called Reinforcement Learning state of the art and they introduce this exploratory drive. This is a book maybe that has.

55:24 Gregoire:
Ten years or something like that.

55:26 Gregoire:
So it's something that you add up.

55:28 Gregoire:
The way we will do it now.

55:29 Gregoire:
Is we can put some drives with respect to preferences and then you add for example, an epistemic value and you try to just solve the optimization problem. But in terms of formalism, if you look at the belief MDP, it's nothing more than a certain value function. So it's not cool.

55:51 Daniel:
So space remains when we translate through it or when we whether we're in the Euclidean or in the projective setting space is basically what is not changed through action. Is that the case? Okay, yeah.

56:18 Gregoire:
The thing is that there's a bit of technicality so the way that we use is we use a chart for the projected plane. So we need to use homogeneous coordinates, so we need to take away a lower dimensional plane.

56:32 Gregoire:
So that's why we always have the.

56:33 Gregoire:
Same like in the way that we encoded, we always are in r three because we chose a chart, we chose homogeneous coordinates and we did projective transformations from one homogeneous coordinate to another homogeneous coordinate but it always lists to a projective transformation. So what is hidden in the way.

56:51 Gregoire:
That we write it is in fact.

56:52 Gregoire:
We don't have the same space in the first case we have ecclesian, in the second we have projective. But the way as we use charts and we don't put all the details of the fact that for example, you can compose two projective transformations even if you write it in charts in terms of homogeneous coordinates and it stays a projective transformation. This is something that is okay but we just don't write it this way. We stay in charts and so that's why there's a similarity between because we.

57:17 Gregoire:
Want to implement it, there's a similarity.

57:19 Gregoire:
Between the.

57:22 Gregoire:
Projective case and projected case.

57:25 Gregoire:
Have the same state space r three. But the space of transformation is really.

57:28 Gregoire:
Different and in fact they are not the same space.

57:31 Gregoire:
But what is important for us in terms of space is just to tell yourself that if you take away everything that is inside of space, everything that populates space, you take it away. What you're left with is with this kind of concepts which already takes into account the fact that when you act you're not changing the space and there's no single point that is singled out. This is very important because it's a very typical like it types a lot the object you're looking at if you move there's no point in the space that is changing.

57:58 Gregoire:
There's no point that is single out.

57:59 Gregoire:
And when you move the space is not changing. So you know that you already have inside of your space a change of charts that is encoded. And even more when you can imagine that you take the perspective of somebody else. And this is really something that is at the basis of the space we consider is that the space is simply.

58:16 Gregoire:
A way to support the fact that you can change charts, you can change perspectives.

58:21 Gregoire:
And so it's including this idea inside of agency that we're trying to do. So maybe I was not clear in.

58:25 Gregoire:
The way that I said it first, but I think this is really important.

58:31 Daniel:
It's the water we're in. So it's a very interesting way to approach it. And it reminds me way back when, actually almost three years ago when we discussed the projective consciousness model and phenomenal selfhood in live stream number nine. And we talked a lot about flipping between the Euclidean and the projective modes. How an agent could have on one hand a space in which a book is a rectangle and yet also be seeing it very close to their face so that its visual projection was different and yet here there was different behavior associated with the frame alone. So what's going on there where even though they apparently can be reconstructed from each other, what flips can we flip from thinking more Euclidean?
59:47 And then in that situation I'm a plane 30,000ft above my city so there's nowhere I need to go. But then in the projective setting we do have this kind of inbuilt epistemic drive to be near.

1:00:08 Gregoire:
I think it's hard coded. One of the things that it can be useful for is communication. So using space as a way to.

1:00:17 Gregoire:
Communicate, taking it as a priority that.

1:00:19 Gregoire:
We have very different architectures. The way that we treat information are not the same. We don't have exactly the same connection, neurons and everything. But we still have a common framework in which we can discuss, for example. And visual information is something that is very immediate for us, which is not obvious if you take the perspective, the point of view, if you go from the idea that it's rebuilt and the environment you're living on is not exactly the space that we see. There's something that we constructed for functional reasons and that one of them could be that we can communicate very directly with it. So I think it's hard coded that.

1:00:58 Gregoire:
The state space needs to have this action and that all the agents share this action but they don't share the same architecture. So it gives a common base for.

1:01:06 Gregoire:
Discussion like true actions, for example, change of perspective. So it's to answer the question eclean projective we're not going in the direction in terms of research where we're trying to say that you can go from Eclean to projective for the same agent. So agents are hard coded with a latent space which has some structure and then we try to exploit it for function, for communication, for multi agent like.

1:01:30 Gregoire:
Behavior so that they can collaborate.

1:01:36 Daniel:
Okay, so each agent is within its own projective setting and then the Euclidean sets the stage and allows that multi perspective.

1:01:46 Gregoire:
The Euclidean space, which is the outside.

1:01:48 Gregoire:
World, is just here in this toy.

1:01:50 Gregoire:
Model because it's a way to reference the configuration of the world. But for a network, this would be, for example, the configurations of its sensors. So it's just here. There's this kind of ambiguity between space and space. So outside space, which is Euclidean and inside, which is projective, but the outside space is just a way to discuss configurations of something that is related. To the environment. So it can be like your sensors, the configuration of your sensors or something like this information coming from your sensors. So you just give a space from it. And the geometry is really in the.

1:02:27 Gregoire:
Reconstructed world that is internal.

1:02:30 Gregoire:
But being able to do networks that do this is somewhere that we're working on now. And it's not that obvious because there's a lot of algorithms. The problem algorithmic problems that we try to solve. And that we need to solve. And this toy model is really like the ambiguity comes only from the fact that it's a toy model. But what is space is inside it's, what is reconstructed. It's the thing that we perceive space from sensors as being a whole, as being something that structures the information. So, David you want to say mean.

1:03:03 Speaker D:
Yes indeed in the talk that you are referring to, I was giving and also kenneth williford if I remember. Well, yeah, indeed. The toy models encoded a certain world model in an Euclidean way. And then using homogeneous coordinate and this kind of stuff to do the projective transformation, we just transform the Euclidean space into a projective space and you can go. Of course you can invert it. I mean, there is an onliner division, but you can invert it anyway. So you can go from one to the other. But Gregor is right. That it. Was a choice of modeling. It was a bit motivated by my experience as a brain scientist. There are reasons to believe that memory, for instance, place cells and grid cells deaden. Though there are new hypothesis about their hyperbolic. Characters but usually they are thought as encoding space in an ecclesial manner. So since when we project future action, we use sometimes and we have to use memory or even if we do remembrance of the past and we project ourselves in the scenes in the past, there is this access to memory systems that we used to think, and probably this is still the case, encode information in an Euclidean manner.
1:04:23 So it would make sense that then for the conscious access, there would be some operation that would allow us to go from Euclidean to projective, which was implemented that way at the same time. But this is way beyond my abilities in math, so Gregor will correct me, there is a way of seeing projective geometry as being more general, as an extension of a fine space by adding point at infinity. So if now you think about a projective space with a metric. Basically, the projective space. Is an extension of Eclidean space. Am I saying something completely false? No, it's one way to seeing it. And there are also a lot of operations that use dialogues, I would say exchange between projective and Eclidean. If you think about multiview reconstruction of 3D Eclidean space from multiple shots in your camera you take several shots of a building from different perspective and then you can use a deterministic algorithm approach using epipolar geometry to kind of infer under some prior that basically all lines that converge at points at infinity are actually parallel.
1:05:29 Then you reconstruct. You go from projective to ecclesiast. So there is a deep relationship between the two that is possible and probably functional. Yeah, that's about it. And Epipolar geometry is a subset of projective geometry. Just to give some context.

1:05:53 Gregoire:
David has.

1:05:54 Gregoire:
A better intuition on projective spaces than me. This is like so when he faced notes he really has a very intuitive, physical, personal, visualization and most of the time he's right.

1:06:16 Daniel:
Are we experiencing a projective geometry or what is it like to experience a projective geometry?

1:06:24 Gregoire:
So one problem that there is with this model is the fact that you.

1:06:28 Gregoire:
Need to force the point at infinity.

1:06:30 Gregoire:
To be so the plane you take away from the projected space has to be behind. And so you're not really working with real maps. No, you can always extend them. That's okay. But the thing is if you want to see it in the way that we see it us, which is we have something in front of us I find it a bit restrictive to say that you cannot see what's behind you. I mean, you never see it, right? I mean, you can imagine it which is not the same thing because you change your frame. So I can't answer really this question. I'm just pointing out the fact that for me it's very perturbating to think that I have a plane that corresponds to discontinuity with respect to I have a plane that destroys completely the way that I can think about movement behind me. Which is that if I had to imagine I had a whole space that is around me and not just in front of me things would be very weird in terms of transformations. But it's not something that is not possible.

1:07:25 Speaker D:
This is what happens when you take psychedelic. You basically allow the projective transformation to have all its degrees of freedom. But in the perception, like in sensory motor processes this is calibrated, this is restricted. So we have to choose certain subset of the group. Basically it's only certain projected transformation that will be used in practice. But now if you go into mystical experience or you take drugs they are going to mess. Precisely. That's the hypothesis with those parameters. And now you have the full fledged 15 degrees of freedom transformation. And you see things that are very weird, like that looks like mandalas or things like that. So that's something you can get when you leave too much freedom to the projective action. Basically.

1:08:13 Daniel:
What if what we're seeing, quote in front of us is behind us or something like that?

1:08:19 Gregoire:
What do you mean?

1:08:23 Daniel:
What was the issue with not being able to see behind us?

1:08:27 Gregoire:
The issue is.

1:08:28 Gregoire:
So if you want to see the projected space as a 3d space, as the way that we see it now, you need to take away a plane and basically projective transformation. So there's a sort of plane at infinity on which you can I mean, it doesn't exist in the projected space. It exists only in the way you define an Ecligian chart, like a 3d, like R three chart with a plane that is taken away, this plane that is behind you. If you apply a projective transformation in front of you, everything will seem fine. You're expecting like it's things that you experience but then once what happened is that it can send things at infinity back behind you. So this is something that is a bit I mean, for me so you can write it explicitly there's no problem mathematically but if it's really the way that we experience it, it's extremely weird to think about so I prefer not thinking about it. So this is the only thing and I don't take drugs so I leave it to other people.
1:09:28 It's true that taking drugs in this context makes complete like you could model it in the projective framework and not in the Euclearian framework because you have a huge amount of projective transformations that make sense more than the one that are related to actions in the art. So in the presentation I gave there was a way to relate ecclesian frames to projective frames under certain axioms and in fact you have a huge much bigger space of projective transformations than the one that are restricted by the actions of the agent. So the actions of the agent inside.

1:10:02 Gregoire:
Of the real space.

1:10:03 Gregoire:
So the Euclidean space induce projective transformation in its internal world but there are much more projective transformation than these actions and so you can imagine in this setting having very confusing states of your.

1:10:15 Gregoire:
Mind in a way.

1:10:19 Gregoire:
Which you cannot.

1:10:20 Gregoire:
In the appeal case.

1:10:23 Daniel:
I wonder if a creature or a robot with 360 degree cameras would it not be so perplexed as it wouldn't necessarily have a visual before or a visual in front and behind?

1:10:43 Gregoire:
And the thing is how do you glue them together?

1:10:49 Daniel:
You still have to pick a point of convergence, right?

1:10:52 Gregoire:
I mean you need to find a way to do multimodal integration. Like, if you want to have a camera that's 360, you have the space to represent it, so maybe you can represent it as a sphere. And in this case, the sensors are different. So it's not necessarily the same robot than the models that we are considering. But there might be also another homogeneous space. So g space, it's a space acting which corresponds to the rotations. If you say that, it can look 360 and then you can rotate and the space of observation stays the same. So you can always imagine acting on it and so you can imagine like having another g space structure. And the good thing about it is just with respect to the perspective, if you have an agent that can take a different perspective on its environment, which depends on its sensors, on its actions, on the kind of data you have, then you can define g space and you can do exactly the same setting. Everything like it's made in the more general context so you can apply it. And so it's not limited only to the projective case.
1:11:56 We come from the PCM projective consciousness model. But the framework now, or at least the way to replace this framework inside of optimal control is well adapted for any change of perspective and not just projective.

1:12:16 Daniel:
Awesome. Well, where are you going to go to next? Where will the epistemic drive take you carrying forward?

1:12:27 Gregoire:
So there are two projects now.

1:12:29 Gregoire:
So the first one is to be able to maybe David can talk, to implement it so that it can be used for monitoring behaviors or to be able to predict behaviors or to analyze it.

1:12:45 Gregoire:
So you have model of behaviors of.

1:12:47 Gregoire:
Agents like maladati's behaviors, things that we already published but in a more limited context now to make it inside of a computational framework so that we can use it to analyze experiments with humans. So this is the first project which is ongoing with several students. So on the page that I put on the first slide, there's all the work that we're doing, the people who are working with us and we're very grateful for them working with us and all the ongoing work in this direction. And there's the second one, which is more like, let's say machine learning. And so for me it's very important you have effective algorithms, things that you can really use in practice, things that you can really implement and implementing when you have group structures. There are many, many problems that appear. How to do sarcastic optimal control when the latent space is a homogeneous space is, I think, a completely open question. I start to have answers in this direction. So I hopefully the work will come out soon, let's say January or something like this. But there's a huge work in this direction.
1:13:48 Cool.

1:13:49 Daniel:
Anything else you want to add?

1:13:51 Gregoire:
Thank you very much for the invitation.

1:13:56 Daniel:
A lot to think about mentally rotate project. I mean, after all, as you even alluded to with communication, this is a perspective taking question. Technology is adding some new flavors and methods, for example, asynchronous communication, but it's synchronous for the agent when they perceive it. But all these different modalities of communication and it's quite interesting to think about and just really cool that you and colleagues are pursuing that from empirical data analytic and from theoretical mathematical approaches.

1:14:45 Gregoire:
I mean, on this side for communication. I think so. It's something I find extremely interesting. More generally, if some people want to work on the project, please tell us, and I will be very happy. And even if you have money, please tell us, and we will be very happy. I agree.

1:15:06 Daniel:
Great.

1:15:08 Gregoire:
We're very open. You can always send us a mail. We'll be very happy to discuss about this. Cool.

1:15:14 Daniel:
Okay, well, thank you again to all. Till next time. Okay, see you.

1:15:21 Gregoire:
Bye.

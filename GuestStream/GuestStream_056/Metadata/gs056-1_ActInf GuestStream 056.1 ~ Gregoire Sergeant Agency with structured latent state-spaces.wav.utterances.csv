start	end	speaker	confidence	text
170	35320	A	0.9444743529411762	Hello and welcome. It's September 15, 2023 and we're at the Active Inference Institute in active guest stream number 56.1. Today we have Gregory Sargent Petri and we'll be hearing a talk followed by a discussion on geometry of world model influences behaviors. First there'll be a presentation and then any comments that people have in the live chat or any other questions. It will be great to talk. So thank you a lot for coming, really looking forward to this. So please to you.
36250	71742	B	0.8788873076923076	Thank you very much, Daniel, for the invitation first, it's really very happy to be able to present this work here. So it's a work around some models of consciousness, at least some computational models of consciousness, some part of consciousness, and how it can help to generate different kind of behaviors, especially when we think about, let's say, artificial agents. So I'm Gregor Santui. It's a work in collaboration with david Rodrof, kenneth willie Ford, Daniel Benka.
71806	73714	C	0.9996442857142858	In fact, it's not just one work.
73752	80534	B	0.9770542857142859	It'S a collection of works that have been on for already around ten years. So if you want to know more.
80572	89462	C	0.971547142857143	About this group and the work that we're doing, you can go on the page, on my personal Web page, and there's a link to PCM HTML, and.
89516	91466	B	0.8508757142857143	There'S a summary of all the work.
91488	95626	C	0.9372285714285715	We'Ve been doing and some summary of.
95648	127590	B	0.9592917721518989	The work we've been doing and also of some articles that can be relevant on this subject. So today we'll focus more particularly on two articles. So one that is accepted and will appear very soon, the other one need to resubmit, which give formulation of, let's say, autonomous agents or let's say mathematical formulation of agents that have a world model that is structured geometrically in such a way that this world model captures some features of consciousness.
127930	130760	C	0.999802857142857	So the first article is, let's say.
131450	134566	B	0.9089257142857143	The review of the experimental results that.
134588	136642	C	0.9763000000000002	We have and the most recent formalism.
136706	149818	B	0.9161133333333333	That we have on our work, which is the literature on POMD partial cerebral stream process, which is optimal control, stochastic optimal control. And we just try to tweak a.
149824	153206	C	0.9992357142857143	Bit this formalism to introduce the ideas.
153238	156654	B	0.952355	With how you can include inside of the world model of the agent some.
156692	159006	C	0.9892257142857143	Ideas on consciousness and still continue to.
159028	185334	B	0.9176974999999999	Have algorithms to do inference to find optimal policies and everything. The Zecholia article focuses more particularly on how changing the world model of the agent can change its behavior, in particular the relationship, the geometry of the world model, the way that it perceives its environment and the way it acts with respect to for agent strategy. So the way that it looks for something, so it will change its behavior in looking for something.
185452	187238	C	0.9827485714285714	So I will start by presenting a.
187244	191114	B	0.923413076923077	Bit what are all these terms? So, world model, why you need it.
191232	193050	C	0.820936	What is a foraging strategy?
194190	277170	B	0.9512976799999993	How can you define an agent that is looking for a certain object? In particular, what is an exploration based on Curiosity or let's say more technically epistemic value. So let's consider the setting where you have, let's say, a real world, so that's the space that surround us, 3d space. You do a setup where you have an agent which is, let's say, a solid it has a solid frame, so it has a center three axis. What is in front, what is on its side, what is above? Above it. And it's looking for an object O, which is itself a solid inside of R three. And all the configuration of the agents and the objects they are defined by, let's say, a reference frame that is external and that characterizes completely their configuration inside of this world. Now, the agent A, what it wants to do is to find for the object O. But it can only have some noisy observation of where the object O is. So to be able to find O, it needs to have some atriorian where O should be to be able to plan the consequences of its actions with respect to where O will be once it has moved and to update, to be able to plan how the observation can act with respect to updating its prior. So, in another words, so you have an agent and the agent is looking for O. It thinks that O is, for example.
277240	278306	C	0.9994133333333334	In this direction, on the right.
278328	358800	B	0.9485040540540534	So it's going to move towards the right and makes an observation. If it doesn't see O, it's going to change its belief on where O is. So this is a very standard formalism that you can find like in POMDP stochastic optimal control. But the one other point we want to really emphasize is the fact that the agent A has its own frame of reference and the way that it's going to give the coordinates of the object, the way that it will trace where the object is, is using its own way of computing the coordinates of O. So it has its own way to measure where O is. So it doesn't have to refer to a global frame, a global coordinate system, to be able to know where O is. So for example, I am always centered on myself. When I move, I see the object. I don't think about me moving, but I think about the object moving. And this is because when I move, I change my frame. So the way I'm going to reference the object is going to change accordingly. And the point we want to introduce is the fact that you can have several ways for the agent to define its own coordinate system with respect to its environment. And this is a key feature that we want to capture in terms of modeling. But also we think that it is something that is very structuring with respect to models of consciousness. Because when you consider more generally an agent.
362470	364162	C	0.9194566666666667	Let'S say, that has a world.
364216	389586	B	0.9499906349206347	Model in which it has its own point of view perspective on this world model. You kind of start tackling the problem of how to introduce a point of view on the environment of the agent. How can you introduce, in the way that it is going to encode its environment, a certain point of view? And in particular, if you think about.
389628	392186	C	0.9309557142857143	It in terms of the role of.
392208	449322	B	0.9591456707317069	Space to be able to take into account your own perspective on the environment. You can see that if you take away everything that fills up the space, you keep just the space itself. The space allows you to, let's say, to structure the way that you are going to fill in some information about the environment. And it's centered on your own point of view in a way that when you're going to act, you're still going to keep the unfilled space, you're still going to keep the same way of modeling this space. The space will stay the same when you move. When I move, the space is the same. But I took into account the fact that I can move from one point to another in this space without changing the space. And something that is also very important is that with respect to any movement that I can do, the space doesn't change. So there's no particular point in space.
449376	451260	C	0.97119625	With respect to the movements that I do.
453390	614370	B	0.9497403311258278	Another property that is very similar is that instead of considering just my actions and the way that I can change my frame while I'm doing an action, you can also imagine taking the frame of somebody else. So it means that you see that the space that you use to be able to structure your environment doesn't change when you take the perspective of somebody else. For example, if you imagine yourself as a solid agent changing by a rotation your own solid frame and translating, okay, so this is something that we will use in fact as a definition for the state space, the way that the agent encodes its environment. And you will see that there's a natural notion that encodes this idea that you can change perspective on your environment, on the world model that you have of your environment, simply by introducing something that's very well known, which is the notion of a group acting on a certain space. So this is just to sum up a bit what we're trying to do. So we're just trying to do like exploration inside of an environment, but we're adding additional information, which is that the way the agents is going to encode its environment takes into consideration the fact that it can change its reference frame on this space without singling out any point. Okay, so what do we mean by perspective taking more precisely? So how do we go from the real world? So let's say the world described by the external frame that allows you to say the agent is here and the object is here from the internal world of the agent. So you just define a map that will take into consideration that the agent can have a first perspective on its environment. The simplest case in the Euclidean case is just rewriting the coordinates of O inside of the frame of the agent, reference frame of the agent. And then once the agent moves, its frame solid frame in the real world changes. And this induces indeed a transformation inside of its internal world, which will lead in a fine transformation. Okay, but you could imagine also having other transformations. I don't know if you can see my mouse probably. Can you see my mouse or not? No. Okay, so the map CSI can also be something else than an affined transformation. It could be like any kind of transformation. For example, in an affined case, the apparent volume of the object doesn't change. But if you change this map and consider, for example, a projective transformation, as we will describe later, you can have that the size, apparent size of the object changes.
614520	617106	C	0.9999985714285715	So we went from something that is.
617128	622706	B	0.9268307142857143	A bit already very well known and not very innovative, which is that you.
622728	623810	C	0.99659	Can rewrite.
625530	627878	B	0.931796	The motions, depends on the.
627884	629846	C	0.944077142857143	Frame that you choose to describe them.
630028	660074	B	0.9431788636363634	Into, saying that you can in fact change some frames to be able to account for some perceptive property. And what we ask also for the agents that we consider is that they can imagine the consequences of their moves, which on their future observations. So this is something just to say that the agents that we consider have a notion of agency, which is that they can plan the consequences of their actions so that they can choose the best action with respect to a certain reward.
660122	662474	C	0.9999242857142858	In particular, the reward that we consider.
662612	703650	B	0.9501096261682245	Is trying to maximize a certain surprise. So to sum it up, we have an agent that is looking for certain objects. So it has beliefs of where this object is. And the beliefs lives on an internal state space. So on a world model of the environment, it can make observation, which allows it to update its beliefs and then it can predict the consequences of its actions so that it can plan the way that it should act with respect to a certain objective. So formally, this is simply the notion of Markov decision process, or more generally, like the notion of partially observable Markov.
703670	706270	C	0.9997057142857144	Decision process because we do not have.
706340	716170	B	0.9744917647058823	A complete information of the environment, but only some observations that are limited. And it's very related.
716250	717440	C	0.8135366666666667	And let's say.
719830	739018	B	0.945285471698113	There'S a strong duality between POMDP and active inference. So there is really a strong link between both. So now what I did in the way that I presented it so I presented it the idea I tried to present the idea of what we're doing. So the general context POMDPs agents that.
739024	743658	C	0.9939605882352943	Are planning their action with respect to a certain objective. Here the objective is to find a.
743664	776018	B	0.9288418627450977	Certain object and I give the formal setting for defining it which are NDP PMDP. And now I will go one step further and define it explicitly and I will continue like this approach for the second part where I will also present our results and our specificity, where I'll dealt with a general statement, then a bit more precise and then really digging into the result. So here the classical way to define a Markup decision process is to consider that you have a set of configurations of the environment, which is the state space or of the world model in.
776024	777186	C	0.9542028571428572	The way, or at least the way.
777208	791498	B	0.9780048837209303	That you encode your environment. So this is something on which you can act, because the agent, when it makes an action, it changes the state of the environment. And the way that the actions change the environment is with respect to a.
791504	797446	C	0.841986	Certain Markov kernel, probability kernel.
797558	940150	B	0.9505969660194183	So it's stochastic, which means that each actions changes the state of the environment. But you don't know completely. It's not deterministic. You allow some errors in the way that it acts on the environment, and you have a reward for a function that is associated to the actions that you can do at time T and also the state of the agent at time t. A partially observable Markov decision process is the same thing than a Markov decision process. But you authorize to say that your observation on your environment are only partial, which means that you do not have a precise information on the state of the environment. So the way that you need to relate observation and states is through also a probability kernel, which tells you that if you're at state, for example S, you would expect to make a certain observation. Let's say if you think the object is at X, you expect to see the object at X, but with a certain error because you know that your sensors are also random, they can make some mistakes and you keep similarly a reward function. So it's the same thing that an MDP mark audition process, but you just allow to have some to get information on your environment through observations that are not complete. In fact, there's a formulation in terms of partially observable Markov decision processes. Our particular case of Markov decision processes, which is called belief MDPs, belief Markov decision process and the only difference that you have between Markov process and belief MDP is the fact that the state space is necessarily continuous in the second case. So POMDPs can be seen as a particular case of MDPs. So we said that the actions of a POMDP act on the state space and you account for the consequences of the actions only through observations. So graphically it means that you have the state space of your so the world model of your environment X on which you reference for example, where the object is. So the position of the object and then when you act it induces a consequence on the position of the object at time t plus one. And then you can make an observation of where you think the object will be or you can make an observation with respect to where the object is, depending if you're planning into the future or if you're really doing the action.
941210	943286	C	0.86564	You'Re implementing the consequence of the actions.
943318	1055886	B	0.9463359546925562	In the real world. The thing that we're trying to do in our setting is to replace simply the actions by a change of perspective. Like I told you, when the agent acts, its coordinate system on the environment changes. So you can always see it in a way passively, where you just see it as a change of coordinates. So we want to say that instead of considering actions on the environment, we have a space on which there's a natural notion of changing of reference frame and actions are simply certain kind of changes of frames. We also allow to have some actions that do not correspond to changes of frames. We just say that we include the possibility that some actions are changes of frames and the changes of frames have something that is related to all the transformations that are internal for the agents. So for example, things that it knows that a priori, it's encoded in the way the agents will interact with the environment. So the way that we do it a bit more formally is that we say that changing perspective is simply through the action of a group. So for example, in the affine case, when the agent moves, a change of frame is an affine transformation and we say that the world model. So the state space is simply a space on which the group acts. So formally, what does it mean that the group acts on the space? It's just saying that you have a space. S group g. There's an application that goes that takes an element from g and S and that sends back S. In other words, for every g you can associate a function. From s to s. And you assume also that it has good properties, which is that it satisfies equation one and.
1055908	1057966	C	0.9873542857142857	That if you don't move, you stay.
1057988	1088166	B	0.9434873118279569	At the same place. So this is like just the notion of a g space, a group acting on space. But now if we want to include this in MDPs and POMDPs, we just assume that the state space is a g space, that some actions are some elements of the group, and that when we choose the actions that corresponds to the element of the group, it corresponds to the way that the group acts on the element. So there is nothing like very convoluted. It's just saying that you define the.
1088188	1090410	C	0.8892328571428572	Collection of functions which you see as.
1090480	1095834	B	0.9439193749999999	Changes of frames and the way that they act. So the probability kernel from the state.
1095872	1098346	C	0.909337142857143	Space to the state space at time.
1098368	1121090	B	0.947533243243243	T to time t plus one after the action, after the change of perspective is simply through the way that the function changes the state space. So it's just kind of a way of reformulation. But what is really hidden behind here is that we have the structure of the group and that we don't consider any kind of actions. We assume that there is more structure in the actions that we can consider.
1121160	1128754	C	0.9500971428571429	And that it's defined in the way that it's encoded inside of the geometry.
1128802	1308670	B	0.9281981818181823	Of the space that we have. So we don't separate any more action and state space. We say that the state space in its geometry encodes already certain kind of actions which are its change of perspectives. So there are two case that we consider. The first case is where the state S is the Occlusion space and g, the FN transformation. It corresponds to translations and rotations of the agents and changing rewriting the coordinates of the object inside of the solid reference frame of the agent. And the second case where the space space is a projected space and the group is a projected transformation. We described what are the generative models that we consider and how we can include inside of the classical theory of POMDPs the fact that you can take a perspective on your environment. Now we will introduce like a classical notion of epistemic value which will allow us to define what is behavior, what is an exploratory behavior with respect to curiosity. So what it is for an agent to explore its environment based on curiosity. So curiosity, or let's say the drive for exploration is quantity that will it's, it's a quantity that you will okay, so it's, okay it's so how do you define it? Just with like very generally. So you start with the agent has a prior on the state of its environment, then it plans the consequence of one of its move at the next time, at the next time step. So this changes the prior that it has on the environment because there's an action on the environment. So we get a new prior and now it's going to make an observation. So it imagines that it's going to make an observation. Once it makes an observation there's an apostrophe, the prior is updated. So you get an apostrophe. The way that you define curiosity or let's say epistemic value is how informative is the observation that you will do, how far is the apostrophe from the Apuri? But you cannot do it for one given observation because the way that you compute it is by planning what will happen at the next step. So you need to consider all the possible observation that you will make. So the observations are themselves stochastic with respect to the way that you consider with respect to a priori on the state space of a time t. As we said, forest actions are changes of frame. So we can also define epistemic value for frames for changes of frames and in particular something that we gain, that is I think interesting is that now we have a function that is defined.
1309090	1311262	C	0.9116711111111111	On a group that can be a continuous group.
1311316	1317038	B	0.9671623809523806	So you can allow to have for example, if you want to maximize it, you can or minimize it, I mean.
1317044	1318340	C	0.900455	It depends how you see it.
1318790	1391586	B	0.9357915656565648	You could do gradient descent for example, so you can have more analytical tools because you're on a space that is continuous and has some structure. So now to give the formal definition of epistemic value, so as I said, it's based on the quantity C that I will define now. So if you give yourself a prior on a space x and you give yourself a probability kernel. So a stochastic map from X to Y, stochastic map is simply saying that for any x you will associate a measure on Y. Then you can get a joint distribution on x and Y, which is simply given by the product p y knowing x times the prior. And in fact the quantity C is simply the mutual information between x and Y, which is saying how far is the joint distribution with respect to the independent distribution, the product of the marginal distribution on X and on Y. But this is like you see it appears everywhere the mutual information. So this formulation is based on the paper that is from fiston and all active inference and the Pacific value. Mutual information is something that appears everywhere.
1391618	1392694	C	0.9764114285714286	But I think that there's a very.
1392732	1399926	B	0.982057619047619	Nice re expression of the mutual information which allows to give a better interpretation of this quantity, which is the interpretation.
1399958	1401766	C	0.98713125	That I was discussing in the previous slide.
1401878	1565278	B	0.9404249537037045	So if you rewrite mutual information, you can always see it as the coolBACK lyler distance between the Aposterior for a given observation and the prior coolBACK distance being a way of computing how far you are, how far the two distribution are. But you need to look at the expectation with respect to the observation that you make. So it's exactly what I was discussing before. So then this is like for any kernel, any kernel from x to Y with a prior on x. But as we were discussing here, you have to take into so this would be the kernel, but you have to take into consideration that you can do actions and that your prior R on X. So the way to compute epistemic value for this kernel here when having only the prior on x is that you propagate the prior by the action on x one. Then you get a prior on x one and you can define the epistemic value for this prior and Markov kernel which corresponds to the randomness of your sensors which is always fixed. And this is very important, this one doesn't change even if you change frames, the kernel that you have relating your prior, what you think about the state space relating the state space and the observation never changes. And so explicitly this means that after a certain action or after a certain change of frame, you get a joint distribution on x and y, which is the following one here. So this corresponds to the prior propagated on x one and then the epistemic value is simply the mutual information of this joint distribution. So now how does the algorithm work? The algorithm that corresponds to defining an exploratory behavior for an agent that is looking for so we started with a prior of where O should be. Then you maximize curiosity based on some changes of frames that are around the identity elements which correspond to not changing frame. Then you get an action that you can apply, you propagate the prior to the next step with this action and then you just update your Apiori with respect to a certain observation and then it loops back. So this is the algorithm that we consider in the second paper that was listed in the presentation, which corresponds to having an agent that is looking for a certain object. Its behavior is defined by an exploratory, is driven by exploration, driven by curiosity, taking into account that its state space is structured by the action of a.
1565284	1569198	C	0.9108257142857141	Group, so that its state space is structured by the fact that it corresponds.
1569214	1579430	B	0.9616364516129035	To all the possible ways of changing frames. So it has inside of the state space, inside of the geometry of the state space, all the possible ways of changing frames.
1582250	1587846	C	0.9668613333333334	And this is like explicitly what we do. So in this context, this is the.
1587868	1595418	B	0.972272857142857	Algorithm that we consider and nothing else, then we get a very interesting result. At least I find it's interesting in.
1595424	1596906	C	0.9984100000000001	The way that it's presented here.
1596928	1727080	B	0.9402483044982697	I find it interesting. And then if you go more into detail it's because the Ecclesian case is very particular. But what you get is that if you look at the behavior of an agent that is driven by exploration, by curiosity, but that has state space structured by Ecclesial transformation. So the first case where the agent is simply encoding its environment in its reference frame but nothing else, then it doesn't need to move in the second case, where the changes of charts are given by projective transformations. So the way it's encoded it's in transformation takes into account a projective deformation of its environment, then it will always try to get closer to the object. So you have two very separate behaviors. So this is something that is I think is very interesting to note. So before going into the details of how you can prove this statement, I will say a bit more about why it's an interesting perspective point of view on this subject because and how it goes a bit further than simply considering this very simple setting. What you could imagine is that encoding the fact that the agent encoding the actions of the agent directly inside of the state space of the agent through geometry could be a way to stabilize the representations of the agent. And this is something that we're working on now and there's already literature in this direction. Okay, so now let us proof the statement. And to prove the statement, we need to give a formal precise statement. So more particularly what we were able to show is that if you assume that the agent has as moves staying still, so it's allowed to stay still.
1727770	1731926	C	0.9983771428571427	Then if its changes of frame are.
1731948	1862698	B	0.9587245614035093	Given by a fine transformation, the agent stays still. Now, in the projected case, if you assume that the agent is always looking in the direction of the object, it will always try to get closer to the object. Okay? So the idea of the proof is that what plays the role for drive of the agent is how big the agent is appearing to is the size of the object in the reference frame of the agent and it's how big it appears to be to the agent. So in the first case, the volume of the object in the internal space of the agent doesn't change. So it doesn't need to move. In the second case, if it makes a move, what is informative is to try to make the object bigger because once it's bigger, the apostrophe will be further from the prior with respect to a certain observation. So it will always privilege moves that allow to make the object look bigger. And in this case, what you can do is you can show that this corresponds in fact to actions or change of frames that gets you closer to the object. So how do we prove the results? So, as we said, we consider a change of frame from the real world to the internal world. Here, I didn't consider changes of perspectives with respect to moves. Here is simply how you relate the real world to the internal world. In the Euclidean case it's clear, it's just that the way that you write the coordinates of the object in the solid frame in the projective case it's not obvious because there are several ways that you can relate the solid frame of the agent to the projective frame. So one way that we decided to do it is we give some set of axioms that we consider to be, let's say, coherent with our own experience of space. Which is that we feel that we're always centered on ourself. That the axis of the soil frame.
1862714	1865262	C	0.995317142857143	Of the agent inside of 3D space.
1865316	1930920	B	0.9362111627906977	So what is in front of it or what is on the right, what is top are preserved. That there is no point in front of the object that appears to be at infinity and that that near to the center of the object, the volumes are preserved. So when you do this formulation like this result is in this article here. So the way that we relate some solid. Frames like some solid reference of the agent to some projectors transformations are in this article, in particular in proposition A, one where we show that this set of axioms limits the set of projected transformation we can consider and we will only have this projected transformation. And so the change of frames from the external world to the internal world is given by rewriting the coordinates of the agent of the object in the solid frame of the agent and then applying this projective transformation. So now what we defined here are the maps that relate the observation of the agent.
1932730	1933480	C	0.95528	Back.
1937050	1983206	B	0.9335322314049584	So we define the maps in the Euclidean and projective case that relate the observation of the agent observation of the object to the way that it represents this object inside of its internal world here. So this is simply like the POMDP that we defined before. And now once the agent doesn't move, it changes its ecclesian reference frame. So there is a solid reference frame. So from going from one side reference frame to another solid reference frame, you can, by applying the projective transformation we had before, define another one that we call PSI that goes from the state space at time zero to the state space after action, after moving. What is very important is that we.
1983228	1987526	C	0.8721757142857143	Consider the Markov kernel associated to a.
1987548	2056224	B	0.9112088034188034	Noisy observation to be of this shape here, which is that if you know that the object if you think the object is at point X then you believe that the observation will be around X in the ball around X for a ball of a certain radius, which is a small radius. Epsilon. Okay? And I need to charge my computer, so sorry. So now press epistemic value as defined curve between the previous formula. What in fact that it has a very simple expression. So the epistemic value of the prior propagated after the transformation phi after this changes of frame phi. So you have a prior here. You propagate it on x one through.
2056262	2058672	C	0.83899	Phi and you compute the epistemic value.
2058726	2105280	B	0.9509239669421483	For this joint for the joint distribution over x one. And y is given by this formula here. Well, it's simply integrating over all the possible places where the object could be times the probabilistic volume of the ball of size epsilon for the propagated measure and logarithmic of the same quantity. So this is very direct to right. You can just compute it and you find this expression. But what you have already here is that if you consider an Euclidean transformation, then the quantity cubed PSI minus one b epsilon y doesn't change. So then you have the epistemic value that is constant. So basically, if you try to maximize epistemic value, maximize this quantity, you can do anything.
2105350	2107696	C	0.95898	You have always one move which is not to move.
2107718	2112308	B	0.9756787500000002	So you don't move. It's okay. It's perfectly fine. Now, in the second case where you.
2112314	2114036	C	0.9562571428571429	Have the projective where you consider that.
2114138	2133772	B	0.9261940677966103	The way you relate environment and internal world is through a projective transformation, then it's more complicated. And so you need to use a trick which is that you know that once an observation has been made the support of the prior will be smaller. And so after one step if you suppose that your epsilon so the size.
2133826	2136088	C	0.9834771428571428	Of, let's say noisiness of your sensor.
2136104	2318632	B	0.9486760314341856	Is small enough, you have a support that is of the distribution that's small enough so that you can do an asymptoteic development of the quantity in the interval. So this is an equality, but this is an approximation here and now. This is very useful because now that, you know, doing this in a way you can say that you just need to develop this quantity at the point where the object is really so if you do it, you get this expression here. And what appears to play a role is only the determinant of the Jacobian of a projective transformation which here in the accelerated case will be one in projective case can be many things. So here also it's not very clear because how do you define CM? So CM is defined as a composition of several maps. I didn't go into the details but it corresponds to the map that are given by the changes of frame. So if you have the occasion space and the internal spaces after moving you have a change of frame in the Euclidean space but it corresponds to also a change of frame in the internal space. So the way you define PSI is simply saying that you inverse the projective transformation from internal world to external world to go at time zero. Then you apply the change of frames and then you apply the projective transformation to go from external to internal. And so it is exactly this formula that you have here and which is very nice with this. And then you have the first term that doesn't depend on the actions that you do. So you don't need to take it into account. This one will be one and then you just need to compute this one. In fact this one has a very simple expression so it's expression 2023. And then here you can directly see which moves correspond to increasing this quantity and decreasing this quantity because we want to increase epistemic value. And so this is the result this allows to prove the result that we said in the first day like in the several slides before that in the projective case, if you have enough movements, if you look at the object then you're always going to go closer to the object. A way to interpret it is as if the agent it was a bit paranoiac or paranoid, a bit like very uncertain on its own beliefs. So it knows the object might be over there but it's always uncertain. So it needs to go check and once it's checked it's more certain. But still as it can always be even more certain. It will always try to get more and more certainty. And so I only presented in this presentation all the aspects which are more computational and the algorithm and some analysis of the algorithm we considered. But we also have some experiments on how this setting allowed to generate different behaviors, behaviors that we would expect explain.
2318686	2321636	C	0.99332	For example, some illusions like the Moon illusions.
2321668	2340620	B	0.9566672916666668	And I invite you, if you're interested on this, to listen on to the online talk of the MOC Four conference that was in Oxford last week or to check out one of these three papers. And I would like to thank you very much for your attention.
2348160	2373530	A	0.9492490000000001	All right, awesome. Wow. Very interesting and different ways than how I've seen the POMDP and related works. Okay, let's just start off with little context and then I'll read some questions and read some questions from the live chat. So what brought you to study this question this way?
2376460	2380296	B	0.9701844444444442	What question? The question of, okay, did you come.
2380318	2388140	A	0.9764834615384616	From a math side and find consciousness and geometries to be interesting or vice versa? What kind of brought you to want to make this contribution?
2388880	2403776	C	0.9574102777777777	So at the beginning so when I did my PhD in maths, I was more interested, I was interested in, let's say, all this idea of critical brain hypothesis. So trying to understand the way that.
2403798	2419716	B	0.9465133333333333	The brain processes information and makes it something that can be exploited. So the critical brain hypothesis tells you that the activity of the neurons is basically close to a certain criticality criticality.
2419748	2421864	C	0.8237657142857142	And the source of statistical physics because.
2421902	2423848	B	0.9985314285714286	You can model the activation of the.
2423854	2428730	C	0.9330530000000004	Neurons as, let's say, statistical system like an Icing model.
2429820	2431496	B	0.9044842857142859	So I worked a lot on this.
2431598	2435084	C	0.8869414285714285	Then there's another hypothesis that is very.
2435122	2439768	B	0.9093600000000001	Common, which is the Bayesian brain hypothesis. And so the Bayesian brain hypothesis led.
2439784	2451984	C	0.8727852380952379	Me to active inference, to learning more about optimal control, Bayesian perspective on optimal. So my advisor was PhD advisor was.
2452022	2453744	B	0.9023842857142856	Working with David, and they still work.
2453782	2458508	C	0.9522014285714285	Together on trying to implement some aspects.
2458524	2475588	B	0.9404336666666666	Of consciousness and how it can influence influence, especially with their article on the Moon illusion. So I was interested in knowing how this kind of, let's say, ideas simply.
2475604	2479364	C	0.9308676923076924	Like in a very naive way, they interact with the Bayesian brain hypothesis.
2479492	2481416	B	0.9956814285714286	And then I continued in fact, one.
2481438	2487052	C	0.9640719999999999	Of the lines of my research is structured like algebraically structured statistics or machine learning.
2487186	2493964	B	0.9245485714285716	And so the way that they see it very geometrically, in fact, let's say geometrically or algebraic, it's not the same.
2494002	2507020	C	0.9458408108108108	Thing, but it's very related, is something that I wanted to understand a bit better, let's say, to make it in a formal setting so that it's simply a particular case of what is in the literature.
2507100	2514276	B	0.9886859090909088	So it's specifying what exists in the literature. And this took some time because I was more on the active inference, like.
2514298	2515748	C	0.937432	The free energy principle side.
2515834	2527256	B	0.9542617857142858	So I had to read more about optimal control Stochastic, optimal control Pmdps and understand that in fact, what we're doing is simply adding more structure on the.
2527278	2530168	C	0.9446899999999999	Latent space of the agent, on the.
2530174	2537384	B	0.9839038888888889	State space of the agent. And doing this allows to ask the question like why is it useful?
2537512	2541224	C	0.9292222222222224	Why having a state space that encodes different perspectives?
2541272	2546556	B	0.9226376470588236	So the motivation comes from consciousness study, cognitive sciences, but why it can be useful for robotics.
2546588	2548092	C	0.9452385714285715	And it's always been like my motivation.
2548156	2552588	B	0.9877557142857143	Study some statistical models, more structured models.
2552604	2554864	C	0.8281785714285714	With more a priori that can be.
2554982	2568230	B	0.9530254838709677	Useful for understanding the behavior of a closed system like an agent, like a collection of neurons, like even like molecular machines. So it's a very long answer, but it's okay.
2569580	2589340	A	0.9789232432432432	Cool. So you focused on the spatial movement epistemic foraging case. Is there something special about space or can we also think about this perspective taking in terms of, for example, semantic or a narrative reference frame?
2591920	2597644	C	0.9527847368421052	I think it's a very important question because up to now all the work reference will need to space.
2597842	2602364	B	0.9324877777777778	So the fang is space in terms of a 3D space and not space in terms of geometry.
2602412	2603920	C	0.99924	And the fact of writing it as.
2603990	2608016	B	0.9686214285714285	Geometry allows to get out from the classical point of view a space as.
2608038	2631592	C	0.9419511666666666	A 3D space because you see that there's more and more like for example, in geometric deep learning, there's more and more the use of space to encode invariance of certain objects. And these objects are not necessarily have the three dimensional structure. You can have objects that have higher groups of environments. So I think that it's indeed using.
2631646	2635496	B	0.9691472727272729	Geometry for other contexts. And it's really the aim of trying.
2635518	2637464	C	0.9961757142857142	To go to this more general formulation.
2637512	2640376	B	0.984709090909091	Is to be able to apply it to real world models.
2640408	2641736	C	0.9999585714285714	What I mean by real world models.
2641768	2643240	B	0.9968000000000001	It means the ones that are learned.
2643320	2644364	C	0.9994485714285714	To be able to have an agent.
2644402	2645964	B	0.927487142857143	In an open environment that will learn.
2646002	2655424	C	0.9391385294117647	The way it would learn its generative model. But they are priori that it can take a perspective on its environment and see what it can do. So we really wanted to get out.
2655462	2658404	B	0.9256285714285715	From the least for me, go out.
2658442	2660976	C	0.9566128571428572	From 3D space and go to implementing.
2661008	2665636	B	0.9927299999999999	It completely in autonomous agents to kind.
2665658	2675210	A	0.9771921428571428	Of follow on that. There's a question in the chat, great talk. I also wonder whether it applies to any modality, not just visual spatial, but also text.
2677020	2679672	C	0.9227428571428572	Yeah, so for other modalities, like sound.
2679726	2687850	B	0.9513757142857144	For example, but I don't know for applying to text, there's a lot of work now on large language models and.
2688220	2711428	C	0.9457750793650793	All this idea of prompting and having these models being able to have some kind of imagination. So you would like to have these ideas applied in this context. But for now, it's not the line that I'm trying to do. So I'm trying to stay on deep learning. So basically take standard data set without considering text and just try to see.
2711514	2728376	B	0.9579163265306121	How these ideas, like in geometric deep learning, can stabilize representations. So it's not the same that it's clearly other modalities, especially if we consider like, multimodal integration. Like, you try to rebuild the state space and you don't want to see it only as a vector space, but.
2728398	2739516	C	0.9260304444444445	You wanted to see it with more structure because you want to force the way. Like you have constraints, you know, a priority, the constraints that you're going to have on the way that you can take a perspective on one moldality or the other.
2739618	2742492	B	0.9122891666666666	But it's not for text. At least not but I think maybe.
2742546	2744076	C	0.9961428571428571	It could be used for text, but.
2744098	2745372	B	0.9729549999999999	It'S not what I'm doing now.
2745506	2834632	D	0.919217475728155	Can I add something about this question? Sorry, I just want to rebound on that. I have the luck of collaborating with real mathematician. I'm just some guy with intuition that I've found the right people to do the work regarding multimodality. This is important to understand that this is not about vision. This is about spatial cognition. It's supremodal. The claim is that vision is just one particular way of integrating information, indeed in an obviously projective manner, but that's integrated in a much larger field of experience than the field of view or the visual field. And obviously proprioception touch. When you build a representation just by touching something and you get this 3D representation of that thing automatically in your mind hearing to the extent that it's about source localization and building spatial representation, all of that stuff that's the claim of the theory is integrated in this projective space. There are priors from memories, there are stuff from vision, stuff from addition, stuff from proprioception, interoception, you name it. All senses contribute to it. So it's not vision. The claim is that projective 3D projective geometry in that case is beyond vision. Vision is just actually a slave to that supremodal representation.
2834696	2835660	B	0.7193533333333333	That's the claim.
2836640	2839710	A	0.8382133333333334	Awesome. Yeah, please.
2840160	2852050	C	0.962204736842105	It's important because I think we work like a group. There are several different perspectives. For me, I'm more on the computational side. So it's important to have that because clearly something I won't be able to answer.
2852420	2872360	A	0.952523333333333	Cool. Here's a nice following question from Vladimir in the chat. They wrote if the Eg visual sensors have variable resolution, for example, higher resolution in the center, this can naturally lead to Curiosity based change of orientation response within your framework.
2873180	2876490	C	0.9007471428571429	So is it a question or an affirmation? So it's for E g, right?
2878060	2894396	A	0.8739531250000001	If there's variable sensor precision, for example, higher precision in the sensor, might you see any resulting Curiosity associated change merely based upon the asymmetry or the structure of the sensor field?
2894578	2901200	C	0.9496958064516128	Okay, so I shouldn't say it, but I will still say it because I don't know if the paper will go out one day or not. But this is more attention.
2902020	2904160	B	0.9163683333333333	This is more attention than Curiosity.
2905540	2916150	C	0.9592821212121213	So you can act on your sensor. So that you can change the way that you integrate them. And this really acts as a form of attention. So I really went into it.
2917560	2921720	A	0.94232375	What is the interplay between attention and curiosity?
2925820	2935726	C	0.92947	I mean, it's the way you're going to move basically on the way, the.
2935748	2938420	B	0.9582333333333334	Way that it deforms the way that you move.
2939030	2946100	C	0.9228627272727272	So the consequences of your actions are not the same. So basically curiosity is what drives actions and the way that you.
2948870	2949426	B	0.999995	Choose your.
2949448	2963014	C	0.8715141860465114	Action with respect to reward, which is epistemic value and now attention. Or let's say changing the sensors is a way to change the consequences of your action with respect to, let's say, optimizing this value. So it's, it's like yeah, I mean.
2963052	2966426	B	0.8635771428571429	It'S it's yeah, I don't know if.
2966448	3002840	C	0.9536071093749997	I should say it or not, but okay, let's say it's okay. I think it's a metric. Basically it acts like a sort of metric on the space, on the group directly. So when you have a function, you want to do gradient descent, you choose a metric. And the metric is so the attention will act as a metric and so the metric will allow you to deform, in fact, the steps that you will do. And so I think this is how it acts. So it's very known that it's known that the fact that changing the sensors is related to attention, that's not something much new, but the fact that you can directly in our setting related to groups and you can relate it to.
3003530	3004966	B	0.9264857142857145	Changing the metric on the group is.
3004988	3009180	C	0.959428	Something that can be done. And I think it's interesting.
3012030	3038690	A	0.9481570588235293	All right, another question in the chat. What is a formal framework for learning geometry from data? How do we move from empirical data sets, the files on our computers, and the things that we do deep learning on take into machine learning pipelines? And how do we utilize geometric approaches and formalize to the kind of analytical precision that we saw here, some of these geometric relationships.
3039590	3110700	C	0.9253684499999992	So there are several different fields of how to use geometry on data. It depends how you see geometry. But one way is a TDA topological data analysis, which is basically trying to, let's say you want to provide learning by geometric stability property of your data and try to interpret it as a certain kind of space. And then you look at the whole of your space and this is something that defines your data set. So this is one approach. Another one is people doing manifold, so they know that the data lives in a low dimensional manifold and they try to learn this manifold. So that's another way of doing it. So that's lots of work in this direction. There are people who are interested in variance and equivalents, so more in geometric deep learning. There are people who are interested in the same setting on a priorities how to use geometric aprioris and included in learning for deep learning, reinforcement learning. And this is what we do with this is where many people do this. It's not just us. For us we try to focus on this idea of how to exploit it for reinforcement learning.
3113230	3119682	A	0.9825733333333332	So in this setting Epistemic value was the only driver of action.
3119846	3121486	C	0.9964966666666667	Is that so?
3121508	3130350	A	0.970451	It's kind of like an expected free energy except without Pragmatic value. So we only have the Epistemic term remaining?
3130690	3131294	B	0.99979	Exactly.
3131412	3195940	C	0.9598686813186812	In fact if you look at in terms of optimal control and not by Asian, because you know that there's this duality that is most of the time stated in Active Inference where you have basically a duality between let's say value function and a probabilistic version of value function. So you can encode on priors, you can encode all your drives with priors are with the value function directly. So rewards and so both are kind of dual and they're dual in a way you can make this duality, I don't know if it's explicitly dual like in some context this sentence makes sense for Active Inference for me I'm not completely aware if the duality is exactly formal but at least the idea is here. So there's no big difference, at least the way I see it with value function and seeing it probabilistically. But the terms of Epistemic value is an exploration drive which you also find in reinforcement learning. Maybe not in this exact expression. I think that the exact expression that was given in the paper of.
3197770	3198086	B	1.0	I.
3198108	3202246	C	0.8686642857142858	Think it's pezulo and for sure I.
3202268	3203560	B	0.9459839999999999	Know is on the paper.
3206830	3230800	C	0.9396810169491526	It'S very economical way to define Epistemic value. So it's an exploratory drive, you can always add to the value function. But there's a lot of problem of trying to explore in fact your environment to find the good policies because if you're in a state space that is continuous it's really difficult to resolve the p one EP.
3235750	3277930	A	0.9744764179104477	Yes, it's like if you knew which curiosities you could get rewarded from you would have already known the answer to the search. So that's one of the challenges with Pragmatic value it converges well to expectations. But then this work really focuses in on Epistemic value and shows what it can do alone as a driver. So how would you bring Pragmatic value into this formalism?
3278090	3324526	C	0.9344150806451608	You just add it. You can put the value functions of some of rewards in some way and then you add the Epistemic value. In fact, when you look at the formulation in terms of belief MDP for POMDP, the Curiosity Epistemic value is simply a value function, nothing more. It's a one step value function. But I mean this is like just playing with definitions but so you can always add a drive for exploration and this is something that's really often done in reinforcement learning. I think it's even standard in not this way, not exactly special, but there's a book that's called Reinforcement Learning state of the art and they introduce this exploratory drive. This is a book maybe that has.
3324548	3326030	B	0.8312433333333332	Ten years or something like that.
3326180	3327840	C	0.9990785714285714	So it's something that you add up.
3328690	3329774	B	0.9160771428571428	The way we will do it now.
3329812	3350120	C	0.979120576923077	Is we can put some drives with respect to preferences and then you add for example, an epistemic value and you try to just solve the optimization problem. But in terms of formalism, if you look at the belief MDP, it's nothing more than a certain value function. So it's not cool.
3351770	3375200	A	0.9436491666666669	So space remains when we translate through it or when we whether we're in the Euclidean or in the projective setting space is basically what is not changed through action. Is that the case? Okay, yeah.
3378530	3392130	C	0.92721512195122	The thing is that there's a bit of technicality so the way that we use is we use a chart for the projected plane. So we need to use homogeneous coordinates, so we need to take away a lower dimensional plane.
3392470	3393906	B	0.9951628571428571	So that's why we always have the.
3393928	3411126	C	0.930454705882353	Same like in the way that we encoded, we always are in r three because we chose a chart, we chose homogeneous coordinates and we did projective transformations from one homogeneous coordinate to another homogeneous coordinate but it always lists to a projective transformation. So what is hidden in the way.
3411148	3412202	B	0.9691299999999999	That we write it is in fact.
3412256	3437458	C	0.9757579120879116	We don't have the same space in the first case we have ecclesian, in the second we have projective. But the way as we use charts and we don't put all the details of the fact that for example, you can compose two projective transformations even if you write it in charts in terms of homogeneous coordinates and it stays a projective transformation. This is something that is okay but we just don't write it this way. We stay in charts and so that's why there's a similarity between because we.
3437464	3439422	B	0.8995285714285713	Want to implement it, there's a similarity.
3439486	3440340	C	1.0	Between the.
3442310	3445282	B	0.767222	Projective case and projected case.
3445336	3448754	C	0.9634964285714286	Have the same state space r three. But the space of transformation is really.
3448792	3450870	B	0.993292	Different and in fact they are not the same space.
3451020	3478330	C	0.9765685999999997	But what is important for us in terms of space is just to tell yourself that if you take away everything that is inside of space, everything that populates space, you take it away. What you're left with is with this kind of concepts which already takes into account the fact that when you act you're not changing the space and there's no single point that is singled out. This is very important because it's a very typical like it types a lot the object you're looking at if you move there's no point in the space that is changing.
3478490	3479774	B	0.9756042857142857	There's no point that is single out.
3479812	3496718	C	0.9733520634920633	And when you move the space is not changing. So you know that you already have inside of your space a change of charts that is encoded. And even more when you can imagine that you take the perspective of somebody else. And this is really something that is at the basis of the space we consider is that the space is simply.
3496814	3501134	B	0.9123653333333333	A way to support the fact that you can change charts, you can change perspectives.
3501262	3505798	C	0.9655371428571429	And so it's including this idea inside of agency that we're trying to do. So maybe I was not clear in.
3505804	3510520	B	0.9155464285714288	The way that I said it first, but I think this is really important.
3511390	3605800	A	0.9532099999999996	It's the water we're in. So it's a very interesting way to approach it. And it reminds me way back when, actually almost three years ago when we discussed the projective consciousness model and phenomenal selfhood in live stream number nine. And we talked a lot about flipping between the Euclidean and the projective modes. How an agent could have on one hand a space in which a book is a rectangle and yet also be seeing it very close to their face so that its visual projection was different and yet here there was different behavior associated with the frame alone. So what's going on there where even though they apparently can be reconstructed from each other, what flips can we flip from thinking more Euclidean? And then in that situation I'm a plane 30,000ft above my city so there's nowhere I need to go. But then in the projective setting we do have this kind of inbuilt epistemic drive to be near.
3608810	3617466	C	0.9612904166666668	I think it's hard coded. One of the things that it can be useful for is communication. So using space as a way to.
3617488	3619882	B	0.833607142857143	Communicate, taking it as a priority that.
3619936	3658006	C	0.9548116379310341	We have very different architectures. The way that we treat information are not the same. We don't have exactly the same connection, neurons and everything. But we still have a common framework in which we can discuss, for example. And visual information is something that is very immediate for us, which is not obvious if you take the perspective, the point of view, if you go from the idea that it's rebuilt and the environment you're living on is not exactly the space that we see. There's something that we constructed for functional reasons and that one of them could be that we can communicate very directly with it. So I think it's hard coded that.
3658028	3666234	B	0.9683800000000001	The state space needs to have this action and that all the agents share this action but they don't share the same architecture. So it gives a common base for.
3666272	3690402	C	0.9263380555555555	Discussion like true actions, for example, change of perspective. So it's to answer the question eclean projective we're not going in the direction in terms of research where we're trying to say that you can go from Eclean to projective for the same agent. So agents are hard coded with a latent space which has some structure and then we try to exploit it for function, for communication, for multi agent like.
3690536	3692530	B	0.868075	Behavior so that they can collaborate.
3696250	3705510	A	0.9531195454545457	Okay, so each agent is within its own projective setting and then the Euclidean sets the stage and allows that multi perspective.
3706090	3708506	C	0.9855357142857144	The Euclidean space, which is the outside.
3708608	3710966	B	0.9529242857142857	World, is just here in this toy.
3710998	3747494	C	0.9454623809523807	Model because it's a way to reference the configuration of the world. But for a network, this would be, for example, the configurations of its sensors. So it's just here. There's this kind of ambiguity between space and space. So outside space, which is Euclidean and inside, which is projective, but the outside space is just a way to discuss configurations of something that is related. To the environment. So it can be like your sensors, the configuration of your sensors or something like this information coming from your sensors. So you just give a space from it. And the geometry is really in the.
3747532	3749986	B	0.947936	Reconstructed world that is internal.
3750098	3783470	C	0.9365711224489792	But being able to do networks that do this is somewhere that we're working on now. And it's not that obvious because there's a lot of algorithms. The problem algorithmic problems that we try to solve. And that we need to solve. And this toy model is really like the ambiguity comes only from the fact that it's a toy model. But what is space is inside it's, what is reconstructed. It's the thing that we perceive space from sensors as being a whole, as being something that structures the information. So, David you want to say mean.
3783540	3950150	D	0.9260212328767122	Yes indeed in the talk that you are referring to, I was giving and also kenneth williford if I remember. Well, yeah, indeed. The toy models encoded a certain world model in an Euclidean way. And then using homogeneous coordinate and this kind of stuff to do the projective transformation, we just transform the Euclidean space into a projective space and you can go. Of course you can invert it. I mean, there is an onliner division, but you can invert it anyway. So you can go from one to the other. But Gregor is right. That it. Was a choice of modeling. It was a bit motivated by my experience as a brain scientist. There are reasons to believe that memory, for instance, place cells and grid cells deaden. Though there are new hypothesis about their hyperbolic. Characters but usually they are thought as encoding space in an ecclesial manner. So since when we project future action, we use sometimes and we have to use memory or even if we do remembrance of the past and we project ourselves in the scenes in the past, there is this access to memory systems that we used to think, and probably this is still the case, encode information in an Euclidean manner. So it would make sense that then for the conscious access, there would be some operation that would allow us to go from Euclidean to projective, which was implemented that way at the same time. But this is way beyond my abilities in math, so Gregor will correct me, there is a way of seeing projective geometry as being more general, as an extension of a fine space by adding point at infinity. So if now you think about a projective space with a metric. Basically, the projective space. Is an extension of Eclidean space. Am I saying something completely false? No, it's one way to seeing it. And there are also a lot of operations that use dialogues, I would say exchange between projective and Eclidean. If you think about multiview reconstruction of 3D Eclidean space from multiple shots in your camera you take several shots of a building from different perspective and then you can use a deterministic algorithm approach using epipolar geometry to kind of infer under some prior that basically all lines that converge at points at infinity are actually parallel. Then you reconstruct. You go from projective to ecclesiast. So there is a deep relationship between the two that is possible and probably functional. Yeah, that's about it. And Epipolar geometry is a subset of projective geometry. Just to give some context.
3953440	3954188	B	0.999015	David has.
3954194	3974070	C	0.8837215624999997	A better intuition on projective spaces than me. This is like so when he faced notes he really has a very intuitive, physical, personal, visualization and most of the time he's right.
3976680	3983060	A	0.9455025000000002	Are we experiencing a projective geometry or what is it like to experience a projective geometry?
3984760	3988808	C	0.9676407142857143	So one problem that there is with this model is the fact that you.
3988814	3990244	B	0.9299642857142857	Need to force the point at infinity.
3990292	4045210	C	0.9581128947368417	To be so the plane you take away from the projected space has to be behind. And so you're not really working with real maps. No, you can always extend them. That's okay. But the thing is if you want to see it in the way that we see it us, which is we have something in front of us I find it a bit restrictive to say that you cannot see what's behind you. I mean, you never see it, right? I mean, you can imagine it which is not the same thing because you change your frame. So I can't answer really this question. I'm just pointing out the fact that for me it's very perturbating to think that I have a plane that corresponds to discontinuity with respect to I have a plane that destroys completely the way that I can think about movement behind me. Which is that if I had to imagine I had a whole space that is around me and not just in front of me things would be very weird in terms of transformations. But it's not something that is not possible.
4045840	4090890	D	0.9625310156249995	This is what happens when you take psychedelic. You basically allow the projective transformation to have all its degrees of freedom. But in the perception, like in sensory motor processes this is calibrated, this is restricted. So we have to choose certain subset of the group. Basically it's only certain projected transformation that will be used in practice. But now if you go into mystical experience or you take drugs they are going to mess. Precisely. That's the hypothesis with those parameters. And now you have the full fledged 15 degrees of freedom transformation. And you see things that are very weird, like that looks like mandalas or things like that. So that's something you can get when you leave too much freedom to the projective action. Basically.
4093180	4097710	A	0.9693076470588236	What if what we're seeing, quote in front of us is behind us or something like that?
4099600	4100670	C	0.9224425	What do you mean?
4103200	4106750	A	0.9807425	What was the issue with not being able to see behind us?
4107200	4108140	B	0.97666	The issue is.
4108210	4202668	C	0.9298409790209788	So if you want to see the projected space as a 3d space, as the way that we see it now, you need to take away a plane and basically projective transformation. So there's a sort of plane at infinity on which you can I mean, it doesn't exist in the projected space. It exists only in the way you define an Ecligian chart, like a 3d, like R three chart with a plane that is taken away, this plane that is behind you. If you apply a projective transformation in front of you, everything will seem fine. You're expecting like it's things that you experience but then once what happened is that it can send things at infinity back behind you. So this is something that is a bit I mean, for me so you can write it explicitly there's no problem mathematically but if it's really the way that we experience it, it's extremely weird to think about so I prefer not thinking about it. So this is the only thing and I don't take drugs so I leave it to other people. It's true that taking drugs in this context makes complete like you could model it in the projective framework and not in the Euclearian framework because you have a huge amount of projective transformations that make sense more than the one that are related to actions in the art. So in the presentation I gave there was a way to relate ecclesian frames to projective frames under certain axioms and in fact you have a huge much bigger space of projective transformations than the one that are restricted by the actions of the agent. So the actions of the agent inside.
4202754	4203740	B	0.9598675	Of the real space.
4203810	4215856	C	0.9386922857142858	So the Euclidean space induce projective transformation in its internal world but there are much more projective transformation than these actions and so you can imagine in this setting having very confusing states of your.
4215878	4217010	B	0.9549274999999999	Mind in a way.
4219380	4220156	C	0.9998233333333334	Which you cannot.
4220188	4221510	B	0.8932024999999999	In the appeal case.
4223800	4243610	A	0.9080472727272728	I wonder if a creature or a robot with 360 degree cameras would it not be so perplexed as it wouldn't necessarily have a visual before or a visual in front and behind?
4243980	4246350	C	0.905388	And the thing is how do you glue them together?
4249360	4252732	A	0.9848220000000001	You still have to pick a point of convergence, right?
4252786	4334920	C	0.9522179324894513	I mean you need to find a way to do multimodal integration. Like, if you want to have a camera that's 360, you have the space to represent it, so maybe you can represent it as a sphere. And in this case, the sensors are different. So it's not necessarily the same robot than the models that we are considering. But there might be also another homogeneous space. So g space, it's a space acting which corresponds to the rotations. If you say that, it can look 360 and then you can rotate and the space of observation stays the same. So you can always imagine acting on it and so you can imagine like having another g space structure. And the good thing about it is just with respect to the perspective, if you have an agent that can take a different perspective on its environment, which depends on its sensors, on its actions, on the kind of data you have, then you can define g space and you can do exactly the same setting. Everything like it's made in the more general context so you can apply it. And so it's not limited only to the projective case. We come from the PCM projective consciousness model. But the framework now, or at least the way to replace this framework inside of optimal control is well adapted for any change of perspective and not just projective.
4336460	4347070	A	0.9565063157894736	Awesome. Well, where are you going to go to next? Where will the epistemic drive take you carrying forward?
4347760	4349788	B	0.9668466666666666	So there are two projects now.
4349874	4365728	C	0.9441880555555558	So the first one is to be able to maybe David can talk, to implement it so that it can be used for monitoring behaviors or to be able to predict behaviors or to analyze it.
4365814	4367376	B	0.8772442857142858	So you have model of behaviors of.
4367398	4428980	C	0.9473379999999998	Agents like maladati's behaviors, things that we already published but in a more limited context now to make it inside of a computational framework so that we can use it to analyze experiments with humans. So this is the first project which is ongoing with several students. So on the page that I put on the first slide, there's all the work that we're doing, the people who are working with us and we're very grateful for them working with us and all the ongoing work in this direction. And there's the second one, which is more like, let's say machine learning. And so for me it's very important you have effective algorithms, things that you can really use in practice, things that you can really implement and implementing when you have group structures. There are many, many problems that appear. How to do sarcastic optimal control when the latent space is a homogeneous space is, I think, a completely open question. I start to have answers in this direction. So I hopefully the work will come out soon, let's say January or something like this. But there's a huge work in this direction. Cool.
4429050	4430710	A	0.9999699999999999	Anything else you want to add?
4431960	4433940	B	0.9999371428571429	Thank you very much for the invitation.
4436540	4484420	A	0.9328197530864196	A lot to think about mentally rotate project. I mean, after all, as you even alluded to with communication, this is a perspective taking question. Technology is adding some new flavors and methods, for example, asynchronous communication, but it's synchronous for the agent when they perceive it. But all these different modalities of communication and it's quite interesting to think about and just really cool that you and colleagues are pursuing that from empirical data analytic and from theoretical mathematical approaches.
4485240	4504730	C	0.9729154716981132	I mean, on this side for communication. I think so. It's something I find extremely interesting. More generally, if some people want to work on the project, please tell us, and I will be very happy. And even if you have money, please tell us, and we will be very happy. I agree.
4506140	4506890	A	0.99975	Great.
4508100	4514256	C	0.9562494736842105	We're very open. You can always send us a mail. We'll be very happy to discuss about this. Cool.
4514358	4522124	A	0.8924364285714288	Okay, well, thank you again to all. Till next time. Okay, see you. Bye.

SPEAKER_01:
hello welcome it is june 11th 2025 it is active inference guest stream lucky number 111.1 uh very nice and we are with georgie georgiev who will be presenting and then we'll have a discussion on modeling and predicting self-organization in dynamic systems out of thermodynamic equilibrium part one attractor mechanism and power law scaling

will look forward to a presentation and some q a so if you're watching live feel free to ask a question thank you for joining gergi to you all right thank you daniel for inviting me it's my great pleasure to


SPEAKER_00:
talk to the Active Inference Institute audience.

And hopefully we have a lot of interests in common so we could have a good discussion or continue to explore those questions.

I personally know some of the people in your group, so I hope we'll find a lot of commonalities.

Yes, as you said, I'll be talking about some of our recent work on computer simulations with modeling and predicting self-organization in dynamical systems and what we think is driving the self-organization, what is the basic mechanism that's

leads to increase of order in those systems and how the different characteristics relate during the process of the self-organization how they grow together and i'll show a lot of examples from other areas to connect to like why is this important why is it interesting and in this title slide i put the

the big picture, the arrow of time as presented.

This was a poster made by Eric Chaison when he was at Tufts University at the right center of science education.

We were like next, his office was next door to mine.

So we had a lot of great conversations and we found that we're interested in the same questions.

And actually I had the exact same

like chronical view of the expanding future and the arrow of time as leading to more interesting things more organized systems more intelligent systems and uh needless to say i was very pleasantly surprised when he made this uh poster so this represents the arrow of time from the big bang pretty much capturing

everything from the pure energy to formation of structure in particles, in formation of atoms, of galaxies, of stars, planets, chemistry, biology, cultural evolution, and whatever comes in the future.

In many people's minds, it has been ingrained that the second law of thermodynamics is predicting that everything is going to a state of thermodynamic equilibrium, to entropy, and that's why a lot of people are very pessimistic about the heat death of the universe.

But meanwhile, a lot of interesting things are happening which go in the opposite direction.

They are becoming less and less

those systems are becoming in less and less of equilibrium.

They're moving further away from equilibrium.

And the interesting question is, why does this happen?

Why they don't just disintegrate?

Why they don't fall apart?

Why this process of increasing of complexity and structure and everything that we see around us has not stopped

let's say, a random number of billion years ago, if it ever started?

And why do we observe everything around us right now?

How did it come to be?

Well, I guess that's a question that a lot of people are thinking from different angles and have different answers.

I want to point also to another aspect, not only that the process of self-organization in the universe in all kinds of different systems is moving in an arrow of time of increasing structure, increasing complexity further away from equilibrium, but it happens at an accelerated rate.

It has been happening since the Big Bang and it doesn't seem

to be slowing down, if anything.

I know that some of your audience have told me from machine learning and AI, boy, that's an acceleration.

Every day there are new things.

And that's a great example of self-organization when you put tons of information in a neural network and it finds a pattern and you don't know what happened.

It happened by itself, I guess, as far as I know.

Maybe there are other

It's not my area of expertise, but what I have heard is there is a kind of structure formation in that information space.

So this is another, this is called the Cosmic Calendar, and it's also well known since probably the 70s.

It was made by Carl Sagan, and I was fascinated by these topics when I was watching it in the 80s in Bulgaria.

where i was born and i couldn't wait for the next episode where he was explaining exactly these interdisciplinary connections between different areas of life of science of culture of history of physics of astronomy and putting it all together and his cosmic calendar represents that

Let's look.

So the universe is roughly 15 billion years old.

He mapped it on a year, which is 12 months.

Every month is about a billion years, a little more.

And from the Big Bang, which is January, the first billion years, February, March, not much is happening.

Now we think with the new observations, there could be some more interesting things.

But in general, when we look back, we see

majority like very small galaxies more small clusters the Milky Way our own Galaxy forms about in May which is five billion years later not much interesting things happen at least from our point of view until the solar system forms which is in September of that year and then weekly

Photosynthesis emerges in October and November, the eukaryotic cells.

And now for December, we need this whole panel because things start to accelerate enormously.

We have the fish on the 70th of December, land plants, insects, reptiles, dinosaurs, mammals on the 26th, and Pangea splits on 27th.

So you see that on this cosmic calendar,

pretty much the human history the first ape divergence is on december 31st on the last day in the humans the homo erectus the anatomically modern humans like 11 neanderthals out 11 59 pm and in the very last minute of this enormous span of time the last few seconds we have the

agriculture like 22 seconds before midnight and third cities and china and christ was born like four and a half seconds before midnight and columbus came to america like one second ago so it seems like there is some driving force that's not only pushing the universe towards creating

order and more interesting and more sophisticated systems but it's doing it in exponentially or super exponential acceleration

rate, which, as you can see here, the difference in increase of complexity in this last few seconds.

I mean, since Columbus came here, we are now at AI and super intelligence.

The acceleration is fascinatingly robust and not only robust, but it's getting faster and better.

so why is that why first question is why order is created and increases at all and not only that why does it happen faster and faster why doesn't it go at constant rate and there is uh just to confirm this um cosmic calendar acceleration there is this is from ray kurzweil he compiled data from carl sagan and

uh all these other sources here the american museum of natural history encyclopedia britannica about major transitions in kind of cosmic complexity new structures like plants animals why like civilization and before that more chemistry and atoms emerging and how long before the present they happened

So on the y-axis, you see the time to the next event, how many years you have to wait.

So pretty much at the beginning, you have to wait like 10 billion years.

And when you approach the present, this is 10 here on the x-axis, time before the present, this is like 10 years before now, pretty much you see that those events are happening

closer and closer together and all these sources pretty much line up on the same kind of trend and you may people have objected they say oh how did you choose this events but there are there are like transitions in complexity there

like major milestones and that's what people are looking at and that's why he compiled all these sources to minimize the error so again tremendous acceleration and complexity transitions as we are approaching the present and um going back to eric shason he defined a metric which is called free energy rate density

it has some caveats but in general his idea is the amount of energy flowing through this complex self-organized systems per unit mass so that's why it's in units of energy ergs per second per gram so it's like the flow the energy rate density

through the systems and again on the x-axis you have the time before present and you could see the enormous acceleration in the you could say that the free energy rate through the system is a measure of distance from thermodynamic equilibrium right like a jar of gas if you leave it in the kitchen or or the temperature in your room is pretty much at equilibrium every point has the same temperature

But in those systems, you have great differences inside them in all kinds of measures, not just temperature.

And that distance from equilibrium is correlated with the structure of those systems, like stars, planets, animals, brains, society, et cetera.

Ray Kurzweil looked at calculations per second, that's why he was able to do all these accurate predictions in computing technology and AI, where he actually mapped that on a super exponential, like his points

for compute calculations per second per thousand dollars versus time and this is only until 2000 but i've seen his most recent plots until 2025 they are all on this white kind of dashed line and that's also accelerating so whatever we look we see gigantic

rise in complexity and structure and information and we see super exponential acceleration or at least like Moore's law is strictly exponential but it's only a fraction of this paradigm here for the CPUs obviously Ray Kurzweil looked further back but even if you took a small portion of this data set

Even exponential acceleration is a very big deal on long time scales, as we know.

All right, so now the question about this huge variety of structures and this huge kind of diversity between systems is very, very complex.

And to answer it, we need to go to something simple,

and try to understand that first and then try to build on it.

And here I'm showing an example of a system that people have studied a lot, which is self-organization, spontaneous organization happening by itself, just when you pass energy through a system.

And in this case on the left you have, so imagine you're sitting at your kitchen and you have a pan with water and you turn on the

the heat and now you have a hot bottom plate and cold top surface in the room, right?

and honestly by eye you don't see much but if you people put like different particles inside to see the motion of the fluid or we use an infrared camera we could see the the structure formation and what happens is a small difference in temperature between the bottom and the top obviously now the according to the second law of thermodynamics the heat that

It has to be transferred from the hot to the cold, from the bottom to the top, but very small differences.

It's enough to just the thermal diffusion, the collisions between the molecules are enough and the fluid stays kind of still.

The heat is transferred slowly.

But if you increase the temperature on the bottom plate, then something interesting happens.

That mechanism of energy transfer is not enough.

And the fluid itself organizes in what's called convection rolls.

So if you look from the top,

there'll be these long lines which are basically the this is looking from the side to show the the convection pattern of this of the water or oil whatever fluid you have and not only that but if you increase this temperature difference even more then you could go through a whole set of

very intricate patterns this is a kind of a computational study but in the lab you could observe this is a very simple image here like this will break into sometimes a honeycomb structure they look like a beehive kind of

like where the bees put the honey six kind of angled shapes and then if you increase the temperature even further you get different shapes so basically this is what people mean when they say self-organization i think it's more appropriate appropriately to say energy induced organization because the flow of energy it seems is the

driving force that creates structure in those systems.

And again, this is the very, very simplest system that I could think of

which just has the fluid and the temperature difference, and yet it could create this variety of structures.

Imagine now all other kinds of a lot more complicated systems and systems that are alive and that you pass energy through them and then you have metabolic cycles and all other kinds of patterns.

It's just mind-boggling.

it's at this level of science it's almost impossible now it's so complicated it's almost impossible to understand everything but can we still say something meaningful okay so we looked also at well known to biologists scaling laws

which are that if you increase the size of animals, let's say if you go from a mouse to an elephant, the metabolic rate changes.

In other words, they use less energy per unit mass and they become more efficient in their metabolism.

And

we looked at CPUs and also the energy passing through the CPUs.

We hear that x-axis is total number of quanta, which means, quanta means we are using units of physical action, but if you normalize it per unit time, it's just the energy that the CPU uses and the number of transistors inside.

So as you increase

the number of transistors in the CPUs.

Well, you could say obviously they'll use more energy, but we shouldn't forget that they're getting smaller and smaller and smaller.

So it's not that obvious how much more energy they should use and what will be the pattern.

so now these two scales are logarithmic the x and the y and a straight line we could pass a straight line to the data point is a power law relationship right so one is the other time to the power of some number and just to compare we looked um

at brains and the energy consumption of brains, again, in terms of total number of quanta, that will be the power used by brains of animals of different size, and the neuronal count in the brains.

And I was amazed to see that they are much more lined up, but they form the same powerful relationship

And if, I mean, the brain is not a computer, but still there are units in the brain and there is energy passing through the brain and they are scaling in a proportional relationship.

So there is something that gives us some hint about underlying dynamics.

All right, so our approach, so I'm coming from physics, is to go to

basic principles of physics and ask the question why and how those systems self-organize and can we measure or how can we measure this degree of organization?

If we put two systems next to each other, both are complex, both are performing some amazing things, but do we have a basis to say which one is

more organized, better organized, which will be, if you have a biological system, like which one will outcompete the other, which is more fit.

And there is a basic principle in physics called the principle of least action.

In other words, it was said by Mapertui in the 1600s that nature is most economical in all of its actions.

And before him, even the ancient Greeks and many other, the Taoist philosophy is also based on this efficiency principle.

So it seems like, I mean, in science, it's not usually accepted to talk in terms of final goal of kind of teleology of a system.

But if a system tries to become most economical in everything that happens in it,

it will restructure itself, reorganize to get to even more economical state or more efficient state.

And in our society, we have economies of scale, right?

So if you have a bigger plant or a bigger company, the more units you produce of the same thing, the cheaper they get.

So you have this size and efficiency relationship.

Okay, so what is this principle?

So now, based on this principle, I'll show you the mathematical definition of it, but just to tell you beforehand that for those of you who are not physicists, those of you who are physicists, you all know that all branches of physics everywhere in

Relativity in quantum mechanics, in classical mechanics, in electromagnetism, in fluid dynamics, in optics, everything is derived on this principle.

String theory, that every textbook starts with this principle and derives equations.

So mathematically, it looks like this.

So if you take two points, the starting point and the ending point,

Uh, for travel of some in physics, it will be a atom or like molecule, some particle.

And in more complex systems, I would like to think it should be applicable to any motion, even in biology and in social systems, because if this principle is valid and governs all the motion in physics, why not?

Why should it stop working all of a sudden in those systems which are composed by the same atoms and molecules?

And it says the following, that out of the all possible trajectories or paths that an object could take from the starting to the ending point, you could see some of them here are presented with like wiggly lines.

It always takes the shortest paths.

And this symbol delta here means if we vary this path, we increase the

What this is called the action, I'll define it in a second, but basically it becomes less efficient.

The motion becomes less efficient in terms of action.

And that's what means that nature is most economical in all of its actions.

Basically, it's most action efficient.

It does everything.

All of the motions are happening with the least amount of this quantity action.

And here is how it looks.

So I here stands for action.

And delta is this variation.

If you vary it, make it a little bit smaller or greater.

and this integral here here means that we just take the whole path from the beginning to end from t1 to t2 when we start the motion when we end and this l here is in physics it's simply the difference between kinetic and potential energy so to give you another very obvious example if you take your pencil now and you drop it in the room well

From experience, you can predict which way it's going to go.

It's going to fall on a straight line.

And if you calculate the action for it to move on a straight line and to move on a curved path to the left, to the right, to wiggle, all of those other paths will have higher action.

So if you vary the action, you always get higher values.

But if you drop it a million times, it will fall with the lowest action on a straight line.

And the same is true for

much more complicated systems.

Again, even for fields, not just for objects, that's how we do physics.

All right.

Now, when we consider a complex system, obviously it doesn't involve a single object.

And by definition, it's a collection of, it's a system, right?

By definition, it has

a multitude of objects, or you could call them agents, that interact with each other, that travel along paths, that have to, if you represent a complex system as a network, in biology you could have a metabolic network, in computer science you could have a neural network.

Now, how does this minimization could help us there if we want to apply it, right?

And

In expanding the definition that we saw on the previous slide for a single object, here i is the same amount, the same quantity action, and the integral of the Lagrange and it is the kinetic minus potential energy.

But now we say this is each of the objects in the system, let's say i is the number of the object, let's say you have 10

agents, 10 atoms or whatever it is, they are moving and they do J crossings between the nodes of the system.

And we sum all of this, the action for all of these crossings or all of these events that they perform to get from A to B. And we say, all right, if each of them obeys physics laws, there is no argument to say that they don't,

then the minimization of the sum of all these actions should be zero.

So what does it mean, the sum?

Why don't we say each of them individually?

Well, if we have a system that has many crossing paths, obstacles inside, neither of them could move

on their own least action path.

Neither of them could move on a straight line.

They have to find curve paths to get from one node to the other, from one starting point to an ending point.

But everything that's happening there, we're saying, to obey the laws of physics, it should be minimizing the sum of all of these actions.

And not only that, as we'll see later, to minimize it, it will be like a temporary state.

But in this cosmic evolution, as you go through different levels of structure formation, it should continue to decrease.

So this is called a stationary action principle, where every variation is greater and you find

some stationary value, but we want to think in terms of evolutionary systems of dynamical action principles and I'll show you in a second how it looks.

So this is our starting point that we take all of the actions for all of the agents in the system and minimize that sum.

And we could express this as a ratio

It could be derived from stochastic dissipative action principles, but for now I'm not going to show that.

So the ratio is the following.

So here we have the same thing, n objects.

They do m events, m crossing between the nodes.

Here is the sum of all of the actions for all of those crossings.

And here we'll multiply some constant h, which is called the Planck's constant.

It's the smallest unit of action in physics.

And we say that this ratio, we call it action efficiency.

In other words, how efficiently the agents in the system are moving to perform their functions.

And we say that the more action efficient the system is,

or the larger, the smaller the denominator, the less energy and time they need to use to perform their functions.

That's a signature of better structures, the structure in the system, shorter paths.

If it's biology, it could be enzymes that are speeding up reactions.

If it's neural networks, I think I've seen people apply similar principle optimization principles even

neural networks and economics there is optimal control principles so they use not the same but similar approaches so basically we say that the organization in our language the organization of the system will be reflected in this efficiency in other words if a system is more poorly organized if the agents have to travel longer distances to get from here to there they'll perform

slower their functions they will consume more energy and if you put two systems next to each other if they're biological one will out compete the other even if they're in economics one will be more efficient people will buy the more efficient one so for us the measure of how well organized the system is is this

action efficiency.

And here we just compress the notation.

We say this total amount of action, the sum of everything that happens in the system over the Planck's constant, we label it with the symbol Q, which means this sum of all of the actions and the flow of events or the number of events, all agents times all the crossings that they do is phi, which is flow of events.

And this same equation here becomes the number of things that happened, the number of events, functions in the system divided by the total amount of action expended for it.

But if this was the only thing that happened, then I would expect if the least action principle was the only

Factor that determines how systems should self-organize and how they should form a structure Because it acts everywhere in all particles.

There'll be life on Mars and there'll be life on Venus and everything will be getting more more complex, but In our understanding, that's not the case Because

level of structure in the system, that's why I showed this previous slide, that as the size of the system is increasing, the number of components is increasing in a proportional manner, depends also on, we talked a little bit before that about cybernetics, on these positive feedback loops between all of the characteristics in the system.

I'll give you a short example before I go on, but let's say if we do a simulation of ants forming a path between the food and the nest, right?

It could be any, we could call it other things.

But if we put very few ants,

they don't form a path.

Even if you go in the forest, you look at the ants, if you have one ant and it goes to the foot, it will never form a trail, right?

So you need some critical number of agents.

So that already tells you that, yes, you will minimize the action, but it's conditional.

In order to minimize it, you need some number of agents, you need some amount of energy flow through the system, you need all these other prerequisites for that to happen.

okay so um as the lab so lab stands for least action principle and as this stands for stochastic and dissipative um least action principle because it has been extended for i would say already close to a hundred years by onzager and prigogine and many others to

include in this equation for action not just kinetic and potential energy, but also dissipation, also random effects.

But even with all those effects, systems or objects still tend to minimize this quantity of action.

All right, so our basic model, it could include any number of characteristics, I just put several here, is that we understand, so the action efficiency will be the measure of how structured is the system, how organized, how efficient, how closely it obeys the physics laws, but it does so in conjunction with all these other characteristics in the system, and I'll name them,

But before I say that, I purposely labeled alpha, the action efficiency, as the quality of the system, right?

How efficient, how well it's performing its functions.

And Q, like the total amount of action in the system as the quantity or proportional to the size of the system.

And in philosophy, in dialectical, like in Hegel's dialectics,

the quantity-quality transition was one of the three main mechanisms of pretty much progressive evolution.

And I think that's absolutely true and working in this example.

So here we have the efficiency of the system, the quality, the quantity, the total amount of action.

N is the number of agents in the system.

We saw before the number of transistors, the number of neurons.

Phi is the flow of events.

Let's say if it's a brain, how many, I don't know, information, computational events, or if it's a computer, how many calculations per second it will do.

And we say that each of those reinforces each other.

So in other words, if I increase the size of the system,

I'm creating more opportunities for it to restructure itself, to reorganize, to perform its functions more efficiently, that will increase the quality.

But as it increases the quality, it creates shorter paths, it consumes less energy, then it will allow for the system to grow so it could attract more agents and it could perform more events, the flow of events.

I was always giving to my students when they start a very simple example, and I'll give it here because I know the audience is broad, but I'm based in the Boston area and we had a big project here, which was called the Big Dick.

Basically, the Boston Highway was the most congested in the country until the 90s.

and for about 10 years they had to destroy the highway 93 and put another highway underground and new branches but

The efficiency of the transportation, getting from one point to the other in the city, was getting so low that the population growth of Boston stopped.

Nobody was building anything.

And even it started decreasing.

People were exiting the city.

And as soon as

So basically, you have that if you fix one of these quantities, you arrest the other, right?

And as soon as people build the infrastructure, they put the new highway, then it became action efficient, like fast and

with less expenditure of energy to get from point A to B. And people started moving into the city and building and the population continued to grow until it exceeds the limits, disproportionality values between quality and quantity, basically.

And it will stop again and then it will...

People will find the need to build a new infrastructure to increase the efficiency.

So I see those as inducing each other, helping each other in this example.

All right, so in general, let's say if you open a textbook on general systems theory, let's say Bertalanffy, if you have several quantities that are in a positive feedback loop, they reinforce each other, you could model them with...

just simple differential linear differential equations where the dot here means the time derivative the rate of growth of alpha the action efficiency the rate of growth of q the total action the rate of growth number of events the rate of growth the number of agents in the system uh and they depend on all of the others right i could increase

alpha or q or phi or n and that will reflect the the growth of of this quantity and this linear system of differential equations it's a very simple system it could be made a lot more complicated with non-linear effects with others but it's complicated as it is so let's start simple the solutions are exponential growth

so just if you suppose this positive feedback loop between all of them and you plug them into these equations and you solve the equations each of them alpha q phi n in time it grows strictly exponentially so i'm like aha is there something

to do with the acceleration of order in the Universe?

Are there positive feedback loops acting in all of these different types of systems that make them self-organize faster and faster, like in the cosmic calendar of Carl Sagan, and in the graphs of Ray Kurzweil and Eric Shea's film?

Here I put some mathematical details, but if you ask me, I'll go through them.

But at the moment, I'll skip.

So basically, these are the exponential solutions, right?

Each of them is its initial value time exponent to some number times the time.

So they grow exponentially.

But when you take any of those exponential equations and you combine them, if you ask me, I could go through that.

And I have them in my papers.

But if you combine them algebraically, you get power-law relationships between those quantities.

Always.

Between alpha and n, phi and n, n and q. And I didn't show the others.

Alpha and q. All of them, right?

So they grow proportional to each other with some power.

And now I'm going back to the...

graph with the number of transistors in cpus as their power consumption grows that's a power law the number of neurons in the brain as the power consumption of the brain goes power law i'm like is there any connection here is this signature of this underlying dynamics of this reinforcement between the different characteristics of the system this interdependent mathematical functions

And here, just to show something that everybody knows, we plotted the transistor count versus time for CPUs, which is on a log-linear scale, a straight line.

That's the same like this graph here, log-linear, straight line.

That's an exponential growth.

So is there something in the growth of computing that is obeying this dynamics, right?

And...

All right.

So we think that at least on the first approximation, again, this model could be made with fluctuations, with noise, with other terms, but it seems to be following this predictable mathematical pattern where you could say if those characteristics reinforce each other, they're predicted to grow exponentially.

Then I look at different systems

and I plot them on log-linear curve, they are straight lines, they grow exponentially.

It predicts power-law scaling.

I look at brains, I look at CPUs, I look at animals, they have power-law scaling.

Jeffrey West had a book in 2017 called Scale, where he has probably thousands of those scaling relationships between different characteristics.

So is this a signature of some underlying dynamic that

drives the systems forward not just describes but pushes them to become more organized and more sophisticated and more evolved right cosmic evolution is the term uh that people uh use like eric shayson and carl sagan and others and um i know the big history community also has contacted me because they're also interested in the same questions like

this big history from the big bank until now, why are these events happening?

So could this principle help with that explanation?

And AIE stands for this average action efficiency.

Basically, we say that we can use it to measure how close the agents of the system, obeying these physics laws,

and also to predict or to determine which will be, if the system continues to evolve, what will be the most efficient state or the most organized state of that system that it will eventually reach.

So we think that this shows some promise of predictive power of emergent structures, right?

In our simulation, we could predict that if the agents obey

the least action principle that they will start with random state and form a path that is a straight line and there are no obstacles, and they do.

And also explanatory power to obey this principle of least action.

That's why they are self-organizing.

All right.

some examples very few examples of size complexity rules it was named in 2002 paper i think by bonner well he has also from 1930 some data but has a lot of data and he compiled from different papers so these are mean techno units per tool in

populations in Oceania, predicting technological complexity and the population size.

I mean, he would look at Oceania because he wants to look at kind of isolated islands, isolated societies.

Otherwise, you have to consider them as a part of the bigger system.

And this is a logarithmic scale.

This is also logarithmic scale.

He fits the data with a straight line, which is one of those power law equations that we show.

Then a different system, number of cell types in biological systems and the total number of cells in those organisms.

Very noisy, very dispersed, but

people have fitted with a power-law relationship, right?

So you have the number of cell types is some measure of complexity, right?

You could create different organs, different structures, and the total number of cells is the size of the system, right?

The Hegel's quantity-quality transition.

As you increase the quantity of the system, its quality, its structure also is increasing.

About ants, here is number of ants in ant colony on the x-axis, log scale, and the log number of casts in the ant colony increasing with a power-law relationship.

Here is from Turchin, the evolution of complex hierarchical society, different countries, population of countries as it grows through time, and hierarchical level

in those countries, like different institutions, lock-lock scale, power-law relationships.

Here, the lock-up number of occupations in different states of India and the population of the states of India, the bigger the state, the more different, the larger number of different occupations, there are some measure of complexity.

And I just even put, because this is interesting, to put together the,

joint growth of population in the world and GDP per capita and as the size of the world increases in terms of population in blue the GDP growth is increasing in parallel which suggests this coupling this power relationship one is some number times the other and also

it shows this exponential or rather super exponential growth our model this simple model predicts exponential which is kind of a similar curve but whatever we look it's it's it's amazing it's everywhere so we looked also at some more kind of simple physical systems with

We looked at different stars with one of my previous students, and he measured the progress of nucleosynthesis in those stars in terms of the size of the stars, in terms of the mass as a measure of the size of the stars.

So the bigger a star gets, the larger fraction of its initial hydrogen it converts to heavier nuclei.

And not only it does, it's on a Loc-Loc scale, all of those are straight lines, but it does so in this proportionality relationship, in power-wall scaling.

And the different lines here are just different starting initial fractions of heavier elements.

It's called metallicity in astronomy.

And not only that, but the rate of nucleosynthesis, in other words, the acceleration, how fast they are producing those heavy elements, as you increase the number of solar masses or the size of the star, not only they produce larger fraction of their mass, turn it into heavy elements, but they do it way faster than the slow ones.

produce the small one.

So we see the power of scaling of the progress of nucleus synthesis as a measure of the complexity of the star in terms of its mass and the acceleration, right?

The bigger they are, the faster they do it.

So it's very exciting that no matter where we look, we see those.

Probably one could look for

exceptions and try to falsify it.

But, um, again, there in Jeffrey West and other books, uh, a lot, a lot of examples of scaling laws.

All right.

So let me show you one at, uh, what one of my students did last summer.

So he did in agent, uh, agent based simulations in net logo, um, with on colony where you have like, um, the foot and the nest and,

Here I'm showing the 3D version of the simulation.

You could put different obstacles, you could put friction, you could create a maze, you could make it as complicated as you want and ask them to find the shortest path.

But in the simplest case, this is just the 2D view of the simulation or the version of the simulation.

And here is the world, X and Y, and they would start in green.

The green answer, the initial state there, we start them

totally random, like the gas molecules in the room.

There is no structure, right?

And then they randomly pass through the foot and the nest.

They pick up a pheromone.

As they move, they drop a little bit of it.

and it evaporates, it diffuses, but if another ant senses that pheromone, it crosses the path, it starts moving along the gradient of the pheromone.

And in the red here is some intermediate state where they probe several different paths.

So it already looks like with the least action principle where you have several longer paths,

And now the ants are exploring them, trying to figure out which one they like.

And finally, they settle on the one that's closest to the shortest.

Neither of the long paths.

They don't like the long action paths.

Of course, you have some wiggles here because you have some memory effects which aren't passed where.

But in general, if you leave them long enough, they all

like create shorter and shorter path, they will make this line straighter.

So basically, they'll minimize their action to cross.

If you think of a big network with millions of nodes, this will be just two nodes of the network.

And agents in that system, when they're crossing these two nodes out of all possible paths, they'll choose always the shortest one.

In network theory, you have like geodesic distance, like the shortest distance.

as a measure and here's just how the simulation would run right they'll start randomly and now they totally start again where they are dispersed randomly and then they will explore different paths find the pheromones here is the intermediate stage and finally they'll settle on the poles the closest part to the straight line to the shortest so basically here i'm describing like what happens at each of those steps but i already

went over this but yeah random to forming the final trail and the least action efficiency in the first state when they are moving everywhere and not achieving anything and the highest action efficiency at the end where they're crossing for the shortest time and with the least energy expenditure and this is for those of you that are curious how the simulation window looks like it could change

You could put as many parameters as you want.

You could change them.

You could ask all kinds of questions from this simulation, even though it's very simple.

And now what is the action in this case?

I told you Li is the action integral of the Lagrangian.

And here in our simulation, we define, all right, so the motion, the velocity of the ants will determine their kinetic energy, right?

The mass of the ants times the velocity squared over two.

That's simple.

And then the potential energy is a little bit more tricky.

So here we model the potential as effective potential coming from the concentration of the pheromone that they deposit.

So that's equivalent of information in any other system, right?

So the more information there is, the larger this potential and the faster they will form the root.

And here, like this exponent is just describing the evaporation rate, or if it's a brain, it will be the

forgetting right the memory loss or if it's some other system where the information will deteriorate then there is a diffusion term and this is a random term they they move with a wiggle angle of they don't move always on a straight line because they want to explore the neighboring paths to see if there is a shorter one so in our run they had plus minus 25 degrees at each step they don't just move along the

the information the steepest information gradient the steepest concentration gradient but they wiggle a little bit to see well if there is a king here can i find a shortcut can i make it even shorter right so this helps them find the lowest action path but obviously if if there is this random noise this makes this analytically not solvable so the simulation does solve it numerically pretty much right

and here is the output from this simulation so we have the average action efficiency at the end when they form the final trail not in the intermediate stages we have other curves for that but this this will be just the final state and here we change the number of hands similar to the number of cells in organisms or number of people in society

And as you increase the size of the system, again, quantity-quality transition, the more ants you put in the simulation, the larger you make that system and that society, the shorter path they find, the more efficient path they find.

So in other words, in our language, it's better organized, right?

It obeys more closely the physics principles.

And again, like we saw a bunch of other examples,

We also found something very interesting.

So not only, we call that unit total dualism, right?

So not, oops, let me go back.

I will go forward.

right so if we if we look at the entropy the randomness of those ants in the simulation obviously at the beginning they're the most random at the end they have the least randomness the least entropy but let's say the entropy per and the unit entropy and the final entropy for the same quantity entropy we also find

power-law relationships on a lock-lock scale, they fall on a straight line.

And we looked also at the information, the unit information, total information.

So we think there is something even more important, again, as the size of the system, the total quantity of the system is increasing, the total

macroscopic measures, the microscopic measures change proportionally as well.

And here is the average action efficiency, which will be like the microscopic measure, how efficiently each ant is moving, and the total amount of action.

Again, a power law similar to the CPU or the stellar evolution or any other of the data.

But again, what I mentioned earlier was that

We don't want the variation of this average action to be stationary to reach some final minimum, but we want cosmic evolution.

We want those systems to keep getting better and better and to transition to more complex systems.

So a dynamical principle will be that this will not be an equality anymore.

The variation will not settle around some minimum value, but it will keep finding ways to shorten this, to decrease the action.

For the broader audience, I'm kind of reminded now, I'm just tempted to say that you probably know of Buckminster Fuller, right?

He built this geodesic dome in Canada, in Montreal for the Olympic Games, and Fuller is the, the Buckyballs are named after him.

But he also came up with ephemeralization, which the definition is we'll do more with less, more with less, until we could do everything with nothing.

So I think this is exactly this equation.

We'll do more events with less action, more events with less action until we could pretty much do everything with almost no expenditure.

I don't know whether that will happen and when it will happen, but I think it's a tractor state, at least.

Even if we don't reach it, I think that's kind of the teleological goal of those systems, to get to this maximum efficiency until they could do everything with nothing.

And also here, when I say about unit and total quantities, this dualism, I want to give this example with Javon's paradox, for those of you that are not

physicists, but let's say this is an economics paradox.

Probably all of you have heard that it came from, I think, a long time ago, maybe the 60s, the 70s, when there was the oil shock, right?

And people tried to make more efficient cars because the fuel was very expensive.

And they said, well, we'll save fuel if we increase the efficiency of cars per mile by a factor of two miles per gallon.

expenditure of fuel or the run of the cars, then we'll use half of the oil in the world and everything will be great.

And when they built those more efficient cars, what they found is that the total amount of fuel expenditure actually increased.

And they were like, what just happened?

We doubled the efficiency of the cars, we expected to halve the spending for fuel,

and the fuel expenditure increased even more and that's why he called that paradox because they couldn't explain it obviously the logical explanation is as people pay less for per mile they'll drive more miles and they'll do more things right but in our languages as you increase the efficiency of the system the system could grow so you'll increase the total action so this is

I think a feature, not a bug.

That's how complex systems work.

That's how they self-organize.

It's not a paradox.

In our understanding, we think it's a necessary prerequisite for self-organization because they are in this positive feedback loop.

So each of them increases the other.

The efficiency increases the total expenditure, the total expenditure increases the efficiency.

And here, I'm not going to spend much time on this.

These are just sample runs of the phase transition from disorder to order of different quantities, like entropy, action efficiency, information in terms of pheromone and density and average path time.

And they show this typical phase transition behavior observed everywhere in physics and

other systems this graph i showed you earlier just to compare to remind you again of real world systems displaying this unit total dualism or size complexity rule so the stars become more complex as they get bigger but also this is from one of the jeffrey west's paper from

like many, like more than 15 years ago, I think 2006, probably is the paper itself, where he looked at the look of the gross metropolitan product of cities as a look of the population of cities.

And it's on a log-log scale.

It's a power law.

It's hard to see here, but if you look at the paper in the caption, it's like going at 1.15, the power in that

power law relationships.

So as cities get bigger, they are producing more per person.

So the unit productivity is increasing because the total population of the cities is increasing.

So basically there is no paradox.

All right.

So I tried to bunch here everything together to kind of summarize what I talked.

I hope it's not overwhelming, but a lot of these things we already talked.

It's kind of a review.

So here I'm repeating this least action principle.

It's also called Hamilton's principle because he defined it in its modern version where any particle or field in physics will choose the least action path, the red line, which is the straight line if there are no other obstacles, versus all these other possibilities for longer paths.

And when we look at the ANT simulation, they explore these longer paths.

They start with total randomness.

but finally they choose the one that is shortest with some because they have this wiggle angle this random parameter it's a wider path it's not a straight line but still it's like the shortest possible path that they would take the most action efficient and here i'm repeating this feedback loop between the action efficiency and the size or the quantity and the quality of the system which

which predicts this transition from randomness to order.

And I didn't even show here the exponential transition and the power-law scaling, the quantity-quality transition or the size-complexity rule.

Quantity-quality is the language of Hegel.

Size-complexity is the language of Bonner.

Power-law scaling of many people, including Jeffrey West.

But they all point to the same thing.

As you increase the size of the system, its complexity or quality is increasing.

And also this unit total dualism, that the unit quantity is proportional by a power law to the total of the same quantity in the system.

And we want to think of this principle to minimize action and participating in the positive feedback

loop as the cause of the self-organization and these power law scalings and the structure as the effect and we could make a prediction beforehand solve these equations and we looked at the outcome in the data and we see correspondence so this is kind of a summary of all this thanks and

If I have a minute, I could... This kind of the same thing expanded a little bit more, but again, the Hamilton's principle, the shortest path is the physically realized path in nature.

The mechanism is this feedback loops between here, a lot more quantities that were measured in our simulation.

And this is the entropy change, the phase transition corresponding to the different stages of distribution, finding the structure, which is the shortest path.

at the end.

I don't want to repeat many of the graphs.

I already mentioned some of them earlier, but here is one by Carneiro.

He was an anthropologist, and in 1967 he published a paper where he went to look

at indigenous tribes in the Amazon rainforest.

And he went there because he wanted them, again, like those islands, to be separated, to form like a separate society.

And he looked at the number of organizational traits that they have, a chieftain, a priest, a tradesman, how many structures they have in the society.

And on the x-axis is the population.

And he observed that all these villages, autonomous villages as he called them, lay on this power law line.

And he even wrote that if one of them increased in population and deviated, let's say went in this blank area here, this law is so strong

that they either have to form structures to get back to the same power-low line, or if they don't, they will split in two and move back to the line here.

So they either will go up in complexity or they will go down in size, but they cannot stay away from this power-low proportionality.

So here are some other relationships between action efficiency and entropy difference.

Also power of total action and entropy.

We have huge amounts of graphs in the paper from last year with the same title as the talk.

And here is with different populations.

The entropy starts with high entropy randomness and scales down.

And obviously, even the final state has more entropy for more ants.

but we look at the entropy change, basically the randomness at the beginning and at the end, and with larger population, they decrease, larger decrease in entropy, which is, the reverse is increasing in order as measured by entropy.

We could say entropy is a measure of loss of information, right, of uncertainty, so you could convert it to information.

And to go back to the

I told you I was always thinking in terms of this cone where, starting from the beginning of self-organization until today, the systems are growing in size and they are growing

in complexity and in quality and that's what is forming this cone this recipro this proportionality between the size of the systems and and their complexity going uh up in this is an arrow of time of increase of complexity and um yeah in our model they grow exponentially so to finish i would say to repeat that we

understand at least action principles predicting the increase of action efficiency because the agents in any system should obey as closely as they can the physics law the physics principles move in the same way as in any other system and that will correspond to increase of structure forming a more efficient organization in terms of action efficiency

but it's the average action efficiency for all of them.

And then the system of this interconnected characteristics with positive feedback loops predicts the exponential growth and power-law scalings, which we saw in several systems, several examples.

And this complexity size rule, which is a scaling relationship for power-law holds at

different levels of organization, physical, like stellar, biological, societal in many systems.

And this unit total dualism of these characteristics also speaks about this connection between kind of microscopic for a single agent and macroscopic for the whole system.

That system needs to get bigger for its agents to get more efficient.

and the agents need to get more efficient for the system to grow bigger.

And also dynamical action principles, because we have open-ended evolution and it has to go, in the words of Buckminster Fuller, all the way to ephemeralization or to, in the physics language, maybe to zero action or close asymptotically to very, very small action per one event.

And that we could predict

that the most action efficiency based on LAP will be the structures that are formed.

And yeah, so we think that average action efficiency provides this

number this measure we could take two systems and we say this system has organization which corresponds to this number of action average action efficiency and this has a lower or higher number so it's less organized or more organized so we could quantify it and we think that this potential may have implications across disciplines as

As people say, if you cannot measure something, you cannot improve it.

So we need a number, a measure for how organized is the system, instead of just saying this looks more complex than the others.

And in future work, we think that this model could be extended to different types of complex systems, let's say with modeling, with friction, with dissipation.

We could look at other quantities.

and also we could test it for more specific like either biological or social systems let's say can you model a photosynthetic or crap cycle in biology or metabolic cycles and also in technology if nature uses those optimization principles how can we use them to build more efficient cities more efficient

internet structures, communication technologies, robotics, and so forth.

So I'll stop here.

Thanks for your attention.

And if you have any questions, I'll be happy to talk more about this.

Sorry if I took too much time.

Okay.


SPEAKER_01:
Thank you.

Okay.

Great.

Anyone watching live can add a question in live chat.

I guess just while I'm recropping everything, how did you come to this question and what does research in your lab look like for yourself and for students?


SPEAKER_00:
Well, how did I come to this question?

It's very interesting.

I could just tell you, if you really want to know, I was in high school.

And I'm walking in my hometown, it's a city, but small city, 200,000 people.

And I'm looking, I'm kind of imagining it historically having this small population, dispersed houses, and then

bigger buildings and bigger buildings and i'm like all right if you go to new york city you'll see this hundred story skyscrapers so you have this increase of density and decree that means decrease of distance between the points in the system right to so not only you get more efficient paths but the end points are getting closer and closer so that even

We didn't even talk about that, to increase the efficiency by shortening the paths, by getting the points closer.

Like in CPUs, we make the kind of transistor smaller and you decrease the distance between them for the computation, that increases even more the action efficiency.

And I'm like, what is going on?

Where, how does it go?

I was imagining maybe in the future, the whole city will fit in one building.

which will be the ultimate efficiency.

Everything will be as close as possible.

It will take the least time to get from here to there, which now if you look at the Dubai buildings or some others, they host pretty much whole cities, I think may have already happened.

And that could explain why in the world people will build such buildings.

In my language, that will be to increase the interaction or to decrease the time and energy that it takes for people to communicate or to get in touch with each other.

Obviously, now we have internet, we can talk to anybody, but we still use physical kind of presence.

And then I started, like, I was reading then at that time, like, dialectical kind of philosophy and Hegel.

And I'm like, oh, if the city is getting bigger, there is a quantity-quality transition.

It's getting more efficient.

People are closer together.

They exchange more information.

They create more inventions.

They build better structures, right?

and then i but there wasn't that was i'm talking about the 80s obviously there was the santa fe institute but it was here and i was in bulgaria and that was behind the iron curtain so i couldn't do anything in complexity or in this kind of line of research so i went to study physics because i thought okay if i if i want to explain why these systems are

self-organizing how they are self-organizing probably i need to know all these basic principles then i came to the us to study and i did my phd at tufts university in physics i did a postdoc at northeastern physics and then i went teaching and and even for my phd and postdoc i couldn't do anything in complex systems so i had to do

any other kind of physics that will give me a PhD and a postdoc and a faculty position.

And now after I got those and I got tenure and promotion, I could kind of work full-time or think full-time about those issues, which gives me a great pleasure now.

So yeah, my question always has been about why is this?

What's going on?

what's this magic that's happening that gives us this world instead of a universe filled with hydrogen atoms randomly dispersed everywhere or empty space why in the world they built us how yeah so that's my motivation pretty much


SPEAKER_01:
Yeah, very interesting.

I really appreciated the Buckminster Fuller connection.

And it's very intriguing that the path of least action follows a geodesic in arbitrary state spaces.

on kind of like a curved trajectory for example the ant example it wasn't a straight straight line it still had a little wiggle that would be in tension between the attractive uh character of the pheromone and how much variability and movement like all those different features and so but yet across many replicates of the simulation you would get a straight line

yet every single line in practice is gonna it's or it's like it's like jumping rope it's like the average is flat but that's something that arises not necessarily even at every collective behavioral level more like a across scenario

mean fields or or central limit theorem type results that gives an orderly straight line through all the curves and so that for that to be manifested in geodesic architecture with tensegrity and then also to it's very provocative fuller talked about people traveling more and he said well people were in the horse and cart and then now there's the car the car and then the plane

then what's interesting is it goes up and up and up but then it can't always and now average travel for example might be i don't know maybe it's dropping so how do we reconcile these smoothly fitting curves with balancing knowing that like not everything can curve in that regime forever

So what is it that switches regimes of coherent scaling?


SPEAKER_00:
Well, thank you for that question.

Yes, I want to go back to my example that I gave with the Boston Highway, the big dick, as we call it here, because it was a big mess while they were doing it.

But I think

that's why i kept repeating that this is the first approximation of this model so this smooth exponential and power law curves uh if we look more carefully at data um actually

I think they have a sinusoidal kind of looking like oscillation around the exponential.

And I'll say what I mean.

When I said, let's say, what caused people to build the new highway, the big, the new infrastructure that will increase the action efficiency?

Well, that was the population pressure.

You got more people, you got more congestion.

but that has a time delay they can't build it in real time right you get five more people you build another five miles of highway or something right so you have to wait to some threshold and i think um yeah the infrastructure reaches some level

and then it waits until the pressure is large enough to push it to the next level so it it has to almost in in this case with the congestion it's almost like a a crisis like you have for this impossible way to live and you have to do something but i think there is this lack between all those uh characteristics they don't just

smoothly as you said correctly and you observed correctly they don't smoothly incrementally increase each other and grow proportional all the time but i think they go in this multiple logistics transitions right so they have a transition to a new level and stop

and then wait until the rest of the system grows to that level, the rest of the characteristics.

And then there is another transition and it stops.

And this way they build this.

And even Ray Kurzweil in his book, The Singularity is Near has exactly that figure or similar figure at the beginning where he has this sequential transitions between different computational platforms, right?

Let's say,

At the beginning, people will do calculations with abacus for hundreds of years, and then they'll do with mechanical calculators, then with relay tubes.

But between them, there is kind of a very fast jump, and then they plateau for a while.

So I think this is the real picture.


SPEAKER_01:
and we're working how to put that in the equation how exactly to reflect this in the model but that's you're very correct yeah so i guess sort of in closing as we look at on one hand the decades of systems theory and multi-scale complex systems analysis agent-based modeling etc that brought us here

And we look forward, like over the coming few years, what are you going to be looking for?

What do you feel like your own research may yield?

Are there events that you anticipate that you feel would provide important validation?

Are there tool developments or what will those advances in tooling or development provide?


SPEAKER_00:
Yes, thanks for the question.

It's very, very, probably the most important question that is spot on.

So I'm asking myself that every day, why the problem of self-organization and order increase in the universe hasn't been solved so far, or a hundred years ago.

And I think the main obstacle is that non-equilibrium thermodynamics hasn't been

fully developed and we need it in order to understand what's happening in systems away from thermodynamic equilibrium we know what happens in thermodynamic equilibrium nothing everything's the same but the interesting things happen away from thermodynamic equilibrium we don't have a good theory for that there are many attempts but what is giving me a great hope is the following that

And I think the reason that non-equilibrium thermodynamics has not been developed, I'll sound now very particular to my interest, but I have hope that there is something true to that, is that the least action principle itself, which describes how all objects move,

has not also been used and extended to its full power.

And what I mean by that is that most areas of physics, people are interested in just the conservative least action principle, which is kinetic minus potential energy, no dissipation, no randomness.

Obviously, there are areas like plasma physics, fluid dynamics, where you have those effects.

but recently there have been enormous there has been enormous progress in including all the dissipation and in randomness terms in least action principle and um andres um

in 2017 provided a derivation of the maximum entropy production principle from the stochastic dissipative list action principle, which is a major, that's the maximum entropy production is one of the major debates, because there are a lot of people that point to different exceptions, but for many people, it's the main driver for self-organization of systems.

But I think,

Actually, we're working on that too, that maximum entropy production is a consequence of this increased efficiency and transport of energy through the system.

And as the system becomes more efficient, it could absorb more energy.

Jeremy England talked a lot about that.

and it could dissipate dissipate that energy faster and on top of that the system grows as we saw in our model with this interconnected characteristics so i think that the maximum entropy production yes it correlates with increased uh structure and complexity and efficiency but i think it's the byproduct kind of the exhaust of this underlying

The driving force is not... In other words, what I want to argue is the system expels the entropy in the environment.

So you could ask the question, what benefit does the system have of this expelled entropy?

If anything, it raises the temperature of the environment.

It's detrimental, right?

But what the system benefits from is absorbing more energy so it could use it to create better and more efficient structures inside.

And it also benefits from minimizing the length of the trajectories and the friction along them.

to perform its functions more efficiently with less energy expenditure and faster.

And I think these connections between the expanded dissipative and stochastic least action principle and its application to non-equilibrium thermodynamics

and from there its application to biological and social systems and hopefully the brain it's not there yet but i see a lot of papers recently defining and working i've seen least action principles defined for neural networks for for ai for all kinds of systems

and i think people are converging on this opinion that we need to look at at something objective that is the systems are having a goal to to get to this most efficient state they're not just randomly exploring the space they're moving in a direction so so my hope is that by developing this theory collectively many people from many different areas surely to have a coherent theory of

self-organization in all of the systems and also to add that i think a lot of things have been rediscovered in a lot of areas i mentioned optimal control theory yeah it's based on the least action principle it's used in economics and in social systems it's optimizing for different characteristics that i mentioned in the model

But very few people in physics are interested in it because they say, oh, we're interested in atoms, molecules, like fields and all these other systems.

Somebody else should deal with them.

and at the same time a lot of the biologists and people they're extremely smart people but but we are limited in our brain capacity we can be experts in every field i i'm not an expert in chemistry and in biology so somebody in other field will may not be expert in physics so it's hard to make this interdisciplinary connections we talked a little bit earlier about

this empty area between the disciplines, because we are so specialized in our own areas and we have very limited brain capacity to become absolute experts in that next area and make that connection, right?

And that's why there is so much in between that we haven't explored yet.

And I think that's where the answers are.

That's why I'm interested in this field.

Yeah.


SPEAKER_01:
a lot a lot of interesting pieces there what does the environment as we see order emerge you know as we see a crystal precipitate out of the solution it's easy to say how the crystal changes and grows and scales but what does the surrounding get is it getting

Is it taking the loss and it is getting disordered or does it become more ordered as well?

Because it's topological.

more dynamic properties also become richer through re-entry of interactions with increasingly sophisticated systems like ants modifying the environment.

That's a key piece.

You get certain mean field behavior and central limit their behavior from flocks of birds.

However, you get another broader category with pheromone and niche modification, because the special case of niche modification is just not doing it, at which point it's more like a transient process, rather than something that has this historical element, and so certainly the dynamics of life on Earth have been shaped

by oxygenation events and by changes and all these different things as well so the question of whether the environment is sort of like soaking up or possibly even it's like a double downhill and they're both coming to game i think yeah this is your spot on yeah that's the most important question this is


SPEAKER_00:
I guess, at the heart of the active inference, right?

How the system reflects and how closely reflects its surroundings and how the internal structure kind of corresponds to that environment.

And I think, yes, they grow in tandem.

They become more complex because even in an ecosystem,

right every time let's let's imagine one species has a mutation that makes its metabolic cycle twice as efficient to produce atp than another species obviously they allowed compete them because they need half of the food to do the same things right and if there is drought they will survive

but that will change the whole ecosystem so now all the other species will see a different environment obviously that will create this entropy mismatch between the prediction what they know the system is but it changed meanwhile so they need to adapt to the changes and in this adapt adaptivity to adapt they have to change their own internal structure right so that

I talked only about the internal structure in this simulation and what's happening inside the system to function, but it's paramount to not forget that all that happens in the system is based on what's happening in the environment outside of the system.

If you stop the energy inflow,

uh the system the interior will disintegrate right so it has to find the energy from somewhere and um information plays absolutely crucial role in fact action in living systems at least cannot be minimized without information

Let's say if you think about the simplest of the simple, let's say a simple bacterium, right?

So it will move.

It will have sensors for lights or for a chemical, which is food.

But what is the sensing of light mean?

Well, it means it has information where the sun is.

So it needs to, let's say, photosynthesize.

and then it has so if it did not have that sensor and it has to find where there is more sunlight it pretty much has to move randomly like the ants at the beginning of the simulation but that's the maximum action path so it will spend all of its energy before it finds his food so it will die but knowing where the food is the the end point it could move more or less on a straight line maybe like the ants with a little wiggle

but it will get to the food the fastest way.

And maybe if you have two different bacteria and one is more sensitive to light and it moves on a shorter line, it will get there first and it will out-compete the other.

So I think the information is the mechanism through which organisms could find the shortest action path.


SPEAKER_01:
Yeah, there's a lot I could add.

I think part of the

mystery there is let's just say the first bacteria to get there died or it modifies it so that a second bacteria can come something like that yeah so then there's some fitness function geodesic path of least action that may or may not have a sort of monotonic relationship with any given phenotype

So how to understand the phenotype distribution as taking a path, but not necessarily doing what it first, at first glance, appears to be doing.

Like, for example, you mentioned enzymes, efficiencies.

So that's like the futile cycle.

It's like, why are there enzymes that are both phosphorylating and dephosphorylating something?

Isn't that a waste?

And it's like, well, yes, if you never had to switch gears, it would be

wasteful but if you need to have some agility then you want to have both lanes going even if you pay a small cost to keep both of the directions open

So then it's like, again, there's some fitness maximizing strategy that doesn't necessarily involve what appears to be maximum efficiency on enzyme synthesis.

But it's like, yet we know what's here must reflect some draw from that space of paths.

But it might be a very curvy line itself relative to that imaginary straight line.

and that line might not be doing what we think it's doing but we can hypothesize that it is there and that discovering it or inferring it would give us deeper insight than merely describing using descriptive statistics i agree with you that uh well i would go to the ashby's law of uh requisite variety right that uh


SPEAKER_00:
The way I understand it is the internal structure of the system has to reflect the variety of the environment to have a chance to survive or to match that complexity.

But in your question, the way I understand your question is, it sounds like a redundancy, right?

You have two processes that in some situation one could be turned on, in another situation a different could be turned on, right?

And I see that...

so that's that's the next level of theory right you could say well i mean yes you're right this system is not doing just one thing but today it could be in one environment tomorrow it could be in another environment right so um let's say in the summer it's hot here you need one kind of clothes in the winter it's cold so you can't have one optimization function for all of them so you have to match but

in all of these cases let's say the environment changed and now you have to switch gears to this other mechanism i don't know if it corresponds i'm not a biologist but i know that gene expression like the uh is the the way that like you have all the information in the dna but at any given moment you need small part of it and you turn only this gene only that gene depending on what the organism needs at that time

So I think this provides this flexibility when the environment changes for that organism to be able to do, I hope, the best or the most efficient thing for that new situation, keeping the other mechanism just in case when it goes back to the previous one.

So there is some memory effect.

It knows that things will change.


SPEAKER_01:
Yeah, I guess to sort of close with another Buckminster Fuller quote, he said, all known cases of extinction are due to specialization.

and it's sort of like yeah even if the bacteria is at 97 degrees and it's adapted there forever and then the niche changes to 92 well either it has the plasticity at the individual or the population level to adapt or it was specialized to 97 and then it went extinct so it's kind of like right there is that trade-off because you do better and better more efficiently

but then you become a specialist you become tenure track 97 degree thermal bacteria and it becomes hard to pivot and then fitness is decreased in the system becomes fragile again


SPEAKER_00:
Yeah, I think the next level you asked me the first question about the extension of this theory or many other approach will be to, let's say here, I always say this is very simple, like two nodes, very simple case, very first order approximation.

I'm looking, because we haven't described it so far, and I'm trying to see how this principle could apply to the simplest case of minimization of one process for action, let's say.

But when the system, let's say, over the year, there may be, or over the day, a hundred changes in the environment, right?

Let's say a bacterium could sit in a pond,

and there could be sunlight, then there could be some chemical coming, then there could be another chemical, then there could be some perturbation, then there could be heat wave.

And if it, you said just a second ago, if it cannot respond to any of those, it will die.

So what will be the overall efficiency?

This will be another super kind of efficiency for all of these different processes, right?

So in other words,

if the system cannot respond to one of these let's say some chemical change it means that when the input comes it it cannot efficiently process it or turn it into something useful i don't know how to describe it because it's like in biological terms but uh

That will be an inefficiency in my understanding that it will kill the system because it will spend huge amounts of time and energy to defend itself, but it cannot find the shortest way and it moves around and around and around and it exhausts all of its energy and dies.

But it has to be able to survive.

It has to be able to do this in all these different situations and instances.

And obviously organisms die.

They can't always adapt to... They are not always prepared for all situations.

There have been millions of extinction events.

Many species are not adapted to the human activity today.

One inefficiency in one aspect

Maybe it's a good way to put it, it's enough to kill the organism for that time frame that it happens, and that's very unfortunate.


SPEAKER_01:
Yes, most ant species are extinct.

The ones that are here, they're awesome, but most of them are extinct.


SPEAKER_00:
Yeah, they have probably learned to respond to a huge variety of changes in the environment and survive, be robust.

That's the question about robustness, resilience, and

yeah redundancy in the system to ensure that yeah a lot more layers on top of this theory that i talked about cool well thank you again for the presentation hope to hear updates on the research and and from your group so thank you very much well thanks daniel and hopefully i would have a chance to talk again in the future and i'll update you if i have something new awesome bye bye
SPEAKER_01:
hello and welcome to the active inference lab today is april 14th 2021 and we're here in guest stream number 4.1 with elliott murphy so this is going to be a really interesting talk and elliott we appreciate you coming on so i'll just pass it to you to introduce yourself introduce the topic and then share some slides so anyone who's watching live or i guess later in the comments just ask questions and we'll have time during this presentation and at the end

to go over some comments.

But thanks again, Elliot, and looking forward to hearing this.


SPEAKER_02:
Awesome, yeah.

Thank you very much.

So I am a postdoc working at UT Health in Houston, Texas.

I work on intracranial recordings of epilepsy patients doing a bunch of language tasks, trying to explore the neural basis of language.

And today I'm going to present some slides kind of reviewing a recent preprint that is currently in review.

on active inference and the free energy principle.

I'm just going to share my screen.

Yep, looks good.

Okay, so yeah, I'm going to give you an introduction to the topic and then talk a bit about linguistics and then try and wrap up.

We'll make this more of a conversation, comments, questions, objections, obviously more than welcome for anybody.

So yeah, like I said, this is the preprint that's currently in review.

with Emma Holmes and Carl Friston at UCL.

The argument is that natural language syntax complies in some degree with the free energy principle.

So just outlining some kind of really core basic principles.

I want to make this conversation a little bit more philosophical than normal.

Obviously, this lab is an active inference lab, so I want to kind of introduce some linguistics topics more than the FEP topics, because the audience might be less familiar with the kind of syntactic theory.

So to begin with, natural language syntax yields what linguists then call an unbounded array of hierarchically structured expressions.

Unbounded meaning that they can potentially go on forever.

There's no upper bound on the limit of sentence lengths, right?

You can always make any sentence longer by simply adding John said that at the beginning of it, and it gets longer.

The only thing that stops sentences going on forever is the working memory, the age of the average human being, and indeed the age of the universe.

You know, sentences can't just go on forever.

They have to stop at some point.

And what the genitive component in principle could do that.

That's the interesting part about language, the fact that you can, in principle, generate an unbounded array of expressions.

So I kind of argued in this paper that these are used in the surface of active inference, which is in turn in accord with the principle.

But the general goal is to kind of align certain concerns of linguistics with those of the normative model for organic system behavior associated with the FPP.

So I'm going to be relying on theoretical linguistics with special emphasis on syntax.

which is the system that determines the kind of order that words go in, the structure and the organizational relations between words and sentences.

So a lot of these design principles are kind of general to the biological component of language, not just for specific instantiations of language.

So they're not specific to English or French or Swahili.

They're kind of general design principles about how the language system seems to be organized.

So like I said, I want to kind of emphasize more on the linguistics topic more than active inference, just for the purposes of exposition.

Although the preprint itself has all the kind of relevant details if you're interested in reading more.

But yeah, kind of the brief historical background here is that since around the 10th century, about 20, 25 years ago, linguists have basically been developing theories of linguistic computation, which invoke economy principles.

Basically, the idea is that when you pass an instructive sentence, there are certain computations that enter into that process.

It's not just a single kind of chunk that you memorize or produce.

You do it in discrete operations.

And yet, for some reason, there's currently no means of grounding or motivating these ideas through more general non-linguistic domains.

So recently proposed principles of economy, such as minimal search or least-ever criteria, which I'll explain and give you definitions of soon, I argue that they adhere to the FUP

And if this can be shown, then this permits a greater degree of explanatory power to the FEP with respect to higher language functions.

And it also presents language with a first principle grounding of notions pertaining to computability and so on.

So in other words, the idea is this.

Natural language syntax is a system.

It's an organic system.

It's a formal system.

It can be described using the language of the theory of recursive functions and computability imported from computer science and all the rest of it.

But there's a problem.

All of these principles of

economy and language design in the literature are unsurprisingly kind of language focused, right?

They have a very kind of linguistically encoded background.

And so there's a bit of paradox because one of the original goals of this program was actually to see, okay, let's see how much of language is actually can arise through the kinds of formal principles that govern the organizing shape of snowflakes or the mythology of lightning bolts and so on kind of just general

domain general laws of nature, essentially, that can give rise in different ways across different domains.

And yet the linguistic literature is still kind of encoding these in language-specific terms rather than relating them to more general principles of brain organization or mental computation.

So that's the kind of background.

So many historical insights into syntax, I argue, are kind of consistent with the FEP.

which provides a novel perspective under which the principles governing syntax are not limited to language, but they actually reflect domain general processes that underpin a variety of cognitive computations.

This is also consistent with a strain within theoretical linguistics that explores how syntactic computation may adhere to general principles that may well fall within extra-biological natural law, in particular considerations of minimal computation.

certain linguistic theories might be engaging with general properties of organic systems that impact language design.

So I think that's kind of a beautiful idea, the idea that the language that we speak, English, French, German, not only are they biologically grounded, but actually the rules that kind of govern what you can say, what you can't say, and indeed how your brain computes and passes sentences, part of that process you kind of get for free if you just assume extra biological natural law.

one of which you can argue is the French principle.

So that's the kind of background here.

So the structuring influence of the FP can be detected in language.

Not only has it been argued recently at the complex level, Carl Christen and his colleagues have published a bunch of papers in the last year or two, mainly in 2020, arguing that active inference can relate to narrative comprehension, interpersonal dialogue, like when two people talk, cooperative intentional communication, and speech segmentation.

But I'm going to kind of argue that all of these things rely on something much more lower down, and that's syntax.

If you can't syntactically construct a phrase, if you can't put two words together and form a phrase, you're not going to get very far, right?

You at least need to do that in order to engage in intentional cooperation and narrative and storytelling and speech segmentation.

You at least need to be able to form a phrase, right?

If you can't do that,

you can't do anything to do with language.

So I'm kind of arguing that all of the ways that the FEP can be related to all of these things here arises from a more fundamental, lower-level consideration to do with the way that phrase-level computations are executed.


SPEAKER_01:
Hey Elliot, have you changed any slides?

We can't see any slides changing.

okay uh yeah i have changed a few slides just maybe unshare and reshare but okay it stands alone what you've said as well but it will be also good to have the visual let's see um okay how about this so we see your mouse and then just advance like on the left bar like click on to your next slide yep perfect okay what about now no

okay oh i see my bad my bad okay um well i might have to like window in focus yeah this just as a non-linguist was really interesting that you appealed to like kind of bigger laws than linguistics like complex systems theory or snowflakes and lightnings like you said and then yeah just pointing out that you're kind of

bringing linguistic level rules or patterns into a bigger scope.

Okay, cool.

So we see it with the Chomsky now.


SPEAKER_02:
Perfect.

Okay, well, I've only skipped like five slides.

So everything I've just said is so yeah, I showed the paper, here's the paper.

And I simply regaged this, I sliced this, all of these things I've already read out.

So yeah, if you had what I said, then that's good.

Great.

Okay, so can you see slide number eight?

Yep.

Cool.

Awesome, OK.

OK, so if the process of constructing hierarchically organized sets of linguistic features into words and phrases and sentences can be shown to adhere to principles of efficient computation, then this process must also operate within certain fundamental constraints on neural dynamics, such as those implied by the FEP, to which the homeostatic brain minimizes the dispersion of intraseptic and ectroseptic states.

The FEP can also allow us to understand how natural language complies with the constraints imposed on worldly interactions

deriving certain features of language from first principles.

So again, at the moment, this is all pretty abstract.

I'm going to give you some much more concrete examples.

So I apologize if it's a little bit philosophical at the moment.

So a number of robust findings from theoretical linguistics can be used to support the image of the brain as a constructive organ, assembling and inferring linguistic representations in the sense of surprise minimization and related goals.

So syntactic structures are not mind external entities.

They're not the kind of thing that physicists could easily examine.

but they're rather actively inferred by the brain.

Seemingly with the help of endogenous slow frequency activity coordinating cross-cortical gamut optimization of linguistic features according to a bunch of recent models of neurolinguistics, which I'm gonna get back to at the end.

So in other words, we have a general framework for how syntax is implemented at the abstract level, and we also maybe have a kind of a decent understanding of how that's implemented in the brain.

So that's the kind of the long vision of this presentation.

At the end, we're gonna try and wrap it all up by returning to the brain.

But for now, I'll give you a more kind of concrete example.

So I said that syntactic structures have to be fair.

They're a form of inference.

They're inference generation.

They're not just passive perception.

So if you have a structure like, we watched a movie with Jim Carrey, that can mean two things.

It can either mean the movie stars Jim Carrey,

the Truman Show, say, we watched the Truman Show, or you actually watched the movie whilst sat next to Jim Carrey.

It depends on how you parse it, right?

It depends on which phrases get merged in which order, basically.

So there's a kind of general rough schematic here that you can kind of find an elaboration of in the preprint paper that I mentioned.

But the basic idea is that there's structural ambiguity that arises from syntax, but therefore the whole process of syntax has to be an inference process.

It's not given to us on a spoon.

We have to do some homework in order to construct every possible parse.

So that poses the problem, how does the brain do that?

It is an inferential process.

So what are the operations involved in that?

So here's kind of a standard, what are called pre-structures in linguistics.

There's two different ways of parsing the sentence.

You can either shoot an elephant who's wearing your pajamas, which is definitely possible, or you can shoot an elephant whilst wearing your own pajamas.

And it depends how you, in which order you merge the phrases, right?

In which hierarchical relationship is exhibited, if that makes sense.

So there's kind of a lot of theoretical background here in linguistics that I won't go into because that's a whole different lecture.

But the basic idea is that there's an ambiguity in structure generation.

It's not always, language is not a system of beads on a string, it's a structure.

So along that kind of theme, I'm also assuming a distinction in linguistics between what's called I-language and E-language.

And this is a really crucial distinction to get clear.

So I want to make sure that this makes sense to everybody.

So an I language is the actual internal knowledge that an individual human being has in their mind brain.

I stands to individual, internal and intentional.

Intentional with an S meaning meaning, which literally means meaning, generating meanings.

On the other hand, you have what are called the E language perspective.

And that is actually arguably not really formable or coherent.

It's the idea that language is a kind of external system, a mind external and extramental system.

Like the English language is somehow out there in the world.

And when we learn the English language, we kind of approximate some kind of mind external system.

And we all have something in common, i.e.

we all approximate to the English language in different ways.

But the I-language perspective assumes that when we actually communicate with each other, the reason why we can successfully communicate

It's not because we share an e-language in common.

It's because our iLanguages successfully and sufficiently overlap to the degree that we can actually communicate together.

So everybody's iLanguage is different.

It's often said that there's 7,000 languages on the planet, but that's obviously not true.

There's actually however many people there are on the planet.

I can't remember.

Is it like 7 billion or something like that, 7.5?

And that's how many languages there are.

every human being has a different language faculty.

It's set, the parameters are set differently.

We all have different idiolex, different understandings and so on.

And that kind of sounds really obvious, right?

When you think about it, it's like, obviously that's true.

But the implications for the study of language are actually pretty kind of impactful.

In other words, when we study linguistic competence, we are studying like a mind internal computational system.

We're not studying something outside.

So the English language is not something that linguists actually study.

We don't know, that's not a coherent concept.

It's like the concept of culture or community or whatever.

These are not things that physicists could identify.

That's not to say that we can't say things about them.

We can't abstract.

The human mind can't construct theories of e-languages, I guess.

You can talk about the English language changing over the decades, which is definitely a coherent way to talk about language, and you can study it in that way.

But from a naturalistic perspective, from a biological perspective, that's no use.

The only way we can study it is based on what an individual mind-brain is doing.

So to give another example here, if you take a recent film that IGN released a trailer for yesterday, Hitman's Wife's Bodyguard, the defining property of language is said to be this unbounded array of hierarchically organized expressions via recation.

So recation is the defining property of natural language syntax, but it turns out to be distributed unevenly across the world's languages.

So some people have argued that certain languages don't exhibit recation at all, but this actually tells us nothing about the actual biological language.

So in other words, speakers of these languages can easily learn Portuguese, right?

So the idea is that language can easily and readily be exhibited by most of the world's languages.

Again, when I say world's languages, I mean a kind of convenient description of every individual's fixed language faculty.

So most Germanic languages only allow a single pronominal genitive limited to proper nouns, but German doesn't allow recursive embedding of possessive.

So in German, you can say John's house, but you can't say John's sister's friend's house.

So the above movie, when it's released in Germany, I assume it's going to have to be called something like The Bodyguard of Hitman's Wife, which has a slightly different tone to it.

So the idea is that this fundamental capacity of language to execute recursion, you can find it all over the place.

You can find it in phrase embedding, center embedding phrases, the way that we actually convert phrases, or you can find it in other ways, like in this example here.

So relating this back to the FEP, while the FEP is a variational principle of least action, such as those that describe systems with conserved quantities, a relatively recent program in linguistics has suggested that natural language syntax adheres to principles of least action and minimal search.

Modern theoretical linguistics remains comparatively remote from other fields in cognitive science, but certain postulates from this field resonate with the FEP and its literati.

So many biological and cognitive principles of efficiency might be special cases of a variational principle of free energy.

Assuring this assessment should allow researchers from distinct disciplines to reevaluate their hypotheses and empirical evidence in terms of lower-level free energy, which is the kind of goal that I have in mind here.

But we should also stress, as a recent paper does, that the FEP can be used as a methodological heuristic for research.

But it's not a theory of everything.

It's just a framework.

And actually, I find this very similar to a framework in linguistics called the Minimalist Program, which is a program.

It's not a coherent

Not yet, at least.

It has a couple of theories, working theories, but it's not a coherent body of doctrine at the moment.

It's an ongoing research program with a particular ideology and framework, and that's exactly what the equity is, right?

It's a formal principle which yields and can contribute to a discrete, individual, and separate sort of theories, depending on what domain you're looking at.

So I kind of like the idea.

They're both programmatic notions, and you can implement them in different ways.

So we'll be working here with the FEP.

The paper in question doesn't work on the FEP so much.

It's an applied approach to the FEP.

So the FEP has been argued to be more of a kind of conceptual mathematical model for self-organized systems.

And as a recent review by Andrews makes clear, there's a number of ways that the FEP has served as an aid to scientific work without constituting like falsifiable decisions about the state of nature.

It's a program.

It's a way of usefully describing certain jumps of nature.

without necessarily carving them.

So when we argue that natural language syntax complies with the free energy principle, I'm not necessarily implying that the FAP necessarily bears specific direct predictions when it comes to behavior.

It's rather a way of motivating the construction of novel conceptual arguments for how some property of organic systems might be seen as realizing the goals of the FAP.

So as I said, repeating the our language, your language distinction, I'm focusing on knowledge which is internal to the mind of the speaker, exploring their parent competence rather than what they happen to produce.

So one example that you can think of immediately is the existence of syntactic structures

can contribute to a unique form of epistemic corriging through maximizing model evidence and minimizing surprise and variation of energy.

The actual paper in question that I'm citing here kind of goes into more detail, but the basic idea is that the highly restricted set of syntactic projections, what I call the way that you can categorize a phrase and nouns and verbs and compromise a phrase and so on, that restricted set, that finite set of ways that you can define a particular phrase as being a verb phrase or a noun phrase, achieves that goal.

So you can also assume that language users during comprehension select the phrase structure that's least surprising from the perspective of the hierarchical Jampton model.

Again, this goes back to the we watched a movie with Jim Carrey example.

It's prediction plays a pretty big role here.

So our search phrase structure building can be cast as an internal action.

It's an actual internal action that an individual does in the sense of the active inference framework that selects among competing hypotheses, i.e.

syntactic structures.

You can parse it this way, you can parse it another way.

And also the same goes for lexical semantics, not just syntax.

In other words, you can interpret an individual word one way or the other.

And I'm gonna give some examples later.

So pretty much all of language from individual words up to sentences are inference generation.

And also a little side point here, relating more directly to existing work on active inference and communication.

I think it's pretty interesting to note that the recursive combinatorial apparatus of syntax has been argued to facilitate recursive theory of mind, right?

So the ability, your ability to know

that someone else knows, that she knows, that you know this, and so on.

And it could be seen, therefore, as deriving or piggybacking in some way active inference-based properties of high-order linguistic communication, which in turn serves to unveil the latent or hidden states that are other people's mental states, which is the kind of standard assumption in theory of mind active inference research.

But I think it can be related much more directly to language once we understand that the extended kind of

use of theory of mind in language is, it comes directly from this recursive property, right?

Okay, so I'm now going to move on to kind of the more concrete examples.

Everything I've said so far is pretty philosophical and pretty abstract, but I want to kind of give you some actual concrete examples here so it makes a bit more sense.

So free energy provides additional constraints on what a computational system can be physically realized as, which is very useful.

So take the first three principles in classical recursive function theory, which are functions to compose.

Substitution, primitive recursion, and minimization.

These are all designed in a way that you might think of as computationally efficient.

They reuse the output of earlier computations.

So for instance, substitution replaces the argument of functions with no function.

Primitive recursion defines a new function on the basis of a recursive call to itself, bottoming out on a previously defined function.

And minimization produces the output of a function with the smallest number of possible steps.

So the notion of minimizing surprise can be used to ground observations from theoretical linguistics pertaining to the grammar's propensity to reduce the search space during syntactic derivations, permit no tampering of objects during syntactic derivations, so it restricts the number of resources able to be combined in a given moment.

So in other words, this is kind of an abstract example, but I'm going to give you some more concrete ones soon.

During any particular part of a sentence, you don't just search the lexicon and extract in your unit just because you can.

You only do so if you need to.

If the current number of lexical items that you've already searched the lexicon for suffice to generate a given interpretation, then you don't need to search for more items.

That's sufficient, right?

So in other words, don't do more search when less search is possible.

And it also relates to the grammar's propensity to limit the range of representation of resources able to be called upon during any given stage of comprehension.

So examining some core principles of occasion, natural language clearly exhibits minimization, right?

While binary branching of structures limits redundant computation.

So binary branching calls back to that slide I showed a few minutes ago where you have these binary branching pre-structures.

So I shot an elephant in my pajamas.

That all relates to, that backend will be derived through successive implementations of a binary branching operation that simply puts two things together and gives it a sound identity.

So natural language syntax also exhibits discrete units, obviously, there's individual words, which leads to a discreteness continuity duality.

So syntax is driven by closeness of computation, so objects X and Y form a distinct object, X and Y. It's objects are also bounded, so there's a fixed list, so there's nouns, verbs, adjectives, adverbs, prepositions, commentizers, tense objects, little nouns and little verbs, aspectual elements.

and content-related notions.

Their hierarchical ordering is based on a specific functional sequence as well, which imposes direct restrictions on combinatorics.

These objects can be combined into cycles, so recursive embedding,

which can be extended to form non-local dependencies.

So an example of that is you have number plurality marking.

So you can say the keys to the old wooden cabinet are on the table, not the keys to the old wooden cabinet is on the table, because obviously is goes with keys.

They have to mark for, they have to agree in the number feature, right?

So that's an example where you have a non-local dependency between two elements.

So you have to hold it in memory

even though they cross phrase boundaries.

So syntax still keeps on generating new structures, but you have to associate one element with a new element further down the road.

So these properties are in turn guided by principles of minimal search and least effort, which I'll show in a minute, fulfilling the goal of active inference to construct meaningful representations as efficient as possible.

And I think that's really what it comes down to.

That's the kind of core, crucial message here.

If it can be shown that the language system

constructs meaningful representations as efficiently as possible, then therefore it must be in accord with the FPP.

And again, that contributes to surprise minimization amongst other goals.

So from the perspective of FPP, the range of possible structures available to comprehenders provide alternative hypotheses that generalize, and as such preclude overfitting sensory data.

So if the complexities of linguistic stimuli can be efficiently mapped to a small series of regular and regularized in the sense of learn,

syntactic formats, this contributes to the brain's more general goal of restricting itself to a limited number of characteristic states.

So in other words, only change your belief about things if you have to, right?

By mapping syntactic structures to language external conceptual interfaces, now that's kind of a key term.

Language external interfaces, what that means is you have a narrow component of language, the kind of narrow, what's called the narrow faculty of language, but you can call it whatever you like.

It's just the capacity to construct phrase structures.

But then you have language-external mental modules, memory systems, and what have you, attentional control systems in the mind that are located across different cortical surfaces.

It's kind of the standard modular framework, I guess.

So by mapping these structures that have been generated by language to these language-external interfaces in a manner adhering to principles of economy, language can be seen as engaging in a series of questions and answers with sensory data.

I'm not specific.

So other recent work in theoretical semantics assumes that there are, there are flexible concepts that language can interface with, and there are also non-flexible concepts.

So in other words, there are systems, there are properties of human thought that the language system seems pretty keen on, and it likes to use and exploit a lot, but there are other modules of cognition that for some reason can't be linguistically encoded as easily.

That's kind of a weird property, right?

Why should we be able to linguistically encode and communicate about certain thoughts but not others?

So one example is many of the world's languages, again, when I say world's languages, I mean that in quotation marks.

What I really mean is many of the individual human beings, they may use a quantification of numerosity, often typically seen as via this front of right or quantification network, which is interestingly closely linked

to major language sites.

So that might be one of the reasons why it makes use of it.

But they make little use of colour, despite colour featuring just as prominently an ordinary experience, right?

Our sensorium is filled with colours.

And this might be due to the remoteness of occipital visual regions in the brain.

Maybe, maybe not.

That's one explanation.

Maybe it's wrong, but it's one possible reason.

So for instance, one might imagine some functional morphine

encoding brightness or shade of coloration in language.

You can imagine, like if you were to invent a new language, you could encode color features somehow, or some kind of inflectional morphing.

But that doesn't seem to be the case, even though, like I said, color is a pretty important part of everyday life.

At the same time, language seems to make considerable use of certain contentful concepts, like evidentiality, but not others, like worry.

Like, you know, no morphemes mark how concerned somebody is about something.

I'm very concerned, I'm a little bit concerned, I'm not very concerned.

which seems to be making potentially neurally efficient use of specific easily accessible representational resources rather than less easily accessible cognitive modules to map complex meaning onto natural language expressions.

So it's further been observed that language access has an artificial context which helps constrain what representations are recruited and what impact they have on reasoning and inference.

So words themselves are highly flexible and metabolically cheap sources of price throughout the neural hierarchy.

This is a really cool idea that I'm going to expand on in a few minutes of time.

So to give you some more examples of, again, relating this back to active inference, take the second blue ball to the left of the large box.

That's a pretty simple spatial direction, but it can only be encoded via natural language syntax.

You need that kind of hierarchically organized phrase structure

to generate that particular thought.

It's a very simple thought, very simple direction, but you can only generate inference about it through using the recursive combinatorial operative language.

Similarly, you have structures like the young, happy, eager, either going to Oxford or Cambridge, pleasant man.

This involves unbounded, unstructured coordination involving disjunction too.

So the disjunction is X or Y, which is a highly complex structure to compute.

It's a highly complex conceptual structure, and yet language users can easily do it and readily impair it.

But it also opens the door for a whole new species of inferences to be generated, new thoughts about the world, new possible hidden states, new things that you can think about.

One of the examples of Jerry Fodor, the philosopher used to give is, he said, you know, if you take a pen and a piece of paper, you can easily draw a man, you can easily draw a zebra, but it's kind of difficult to draw the thought that there is not a zebra next to you, right?

So if you try and conceptualize the thought, there is not a zebra next to me, how can you draw that?

How can you depict the fact that there is no zebra next to me?

That's kind of weird.

It's kind of difficult to do.

I suppose you could draw like a man and then a zebra and a line through the zebra, but that's kind of weird.

That still presupposes that there's a zebra next to him that you reject.

So language generates a whole new species of, a new format for thought, basically.

It generates a new format for thought which seems to be unique.

In other words, not readily translatable into other domains like visual representation and so on.

So these rapid inferences of our properties and states, hidden states, can be generated relatively effortlessly by language.

And like I said, no other computational system in human cognition can achieve this.

Well, that's the idea.

So a number of economy principles have been proposed in theoretical linguistics.

These are all very technical syntax-related notions, so I'm not going to explain too many of them, but I will give some examples.

These have all been framed, like I said at the beginning, they've all been framed exclusively within a linguistic context, invoking highly domain-specific notions, despite a core part of the intended project of modern theoretical linguistics being to embed linguistic theory within principles general to cognition, right?

So, for example, the inclusiveness condition maintains that no new elements can be introduced in the course of a particular syntactic derivation.

So once you're passing a particular sentence and you're deriving a particular representation of that, you're not just going to randomly introduce a new lexical item or a new hidden element just for the sake of it.

You're only going to do it if it contributes to immediate interpretation.

And so only existing elements can be rearranged, restricting available resources.

But it's also unclear to what extent this computational principle finds analogous examples in non-linguistic domains.

So one way of motivating these language-specific generalisations by making direct reference to the FEP will not only foster, I think, more fruitful relationships between theories of power population and neurobiology, but will also broaden the explanatory scope for the existence and prevalence of particular syntactic phenomena.

But what's interesting to note is that linguists readily admit they're lacking a specific theory of computational official language.

Like I said at the beginning, it's kind of a program, right?

It's a programmatic notion.

So in a recent paper, Galeo and Chomsky point out, to be sure, we do not have a general theory of computational efficiency, but we do have some observations that are pretty obvious and should be part of that theory, right?

That's basically the state of the field.

Linguists have very well-hung theories of language-specific efficiency criteria, but translating that into kind of more domain general goal areas has not been so successful.

But we can at least suppose that whatever definition will be forthcoming will be related to more generic notions of economy, like Hamiltonian notions, or minimizing energy expenditure during language processing, shortening description length, you know, minimal description length, reducing comma dot complexity, and the degree of necessitated belief updating.

Again, like I said, you know, revise one's belief if needed, and what have you.

So one of these so-called minimal computational procedures

is what's called relative bias minimality.

So this is the principle that states that given a particular configuration, X, Z, and Y, a local relation cannot connect X and Y if Z intervenes and Z fully matches the specification of X and Y in terms of the regular features.

I'll give an example of that.

So in other words, if X and Y attempt to establish a syntactic relation, if you call back to the long-distance dependency thing I mentioned, but some element intervenes and can also provide a superset of X's particular features, i.e.

X's features plus some other features, this blocks the relation.

That sounds extremely abstract, so I do apologize for that, but this is a more concrete example.

So in the sentence you have in 1, which game provides a superset of the features hosted by how?

which results in unacceptability.

But the equivalent does not obtain in two, and so a relationship between both copies of which game can obtain, licensing interpretation, and the strikethrough denotes the originally merged position after movement has taken place.

So it's originally merged down here, and then you move the structure to the beginning of the sentence to form a question.

So question formation often involves just moving all element in the middle or the end of the sentence to the beginning to form a question.

So if you say, how do you wonder which game to play?

That sounds pretty ungrammatical.

But if you say, which game do you wonder how to play, which has the same approximate interpretation, that's okay.

And the reason why is because how hosts only a Q feature, a question feature.

And as it searches down the structure, it encounters which game.

bears a queue feature because of which, but it also has a noun feature because of game, and so therefore it can't reach its final destination, the final destination being its originally merged position after play, which only has a queue feature.

On the other hand, if you move which game to above Do You Wonder, and you leave How in situ, then which game carries these two features here, and it skips over How, because How does not satisfy the full feature.

And then only when it searches back and reaches to its originally merged position does it interpret it.

So what this means is when you say, which game do you wonder how to play, you're interpreting which game at the position of play.

Because you're not interpreting it at the position of wonder.

You're not asking which game do you wonder how you wonder.

You're asking which game do you wonder how you play.

It's all about, the question is about the play on it.

And so this has been argued to emerge directly from minimal search, allowing this higher level representational principle to emerge directly from properties of efficient computation.

This is one example where you have a kind of rhetorically kind of gross, higher order principle in linguistics being reduced to a lower level and more simple kind of element.

So translating that into minimal search, you can consider, as I just said, when you search the structure for matching features in two, the minimal search procedure would simply skip out, but it'll find the original copy of which game because it's searching across the full structure.

So another example of economy can be found in the principle of full interpretation, which simply states that there are no superfluous symbols allowed at the two linguistic interfaces.

So these interfaces are assumed to be the conceptual and sensory motor systems.

So in other words, the two things you can do with a linguistic structure is you can interpret it, or you can externalize it.

You can produce it, you can say it, you can sign it, you can translate it into braille, and so on.

Or you can simply think it.

So this ensures that the system need not compute symbols that are ultimately superfluous to the goals of either interpretation or externalization.

So for instance, in 3, this has an argument that does not have a semantic role, so therefore it's unacceptable.

So you can imagine the sentence, Walt gave Jesse a gun.

That's fine.

All the semantic roles are filled.

There's an agent, there's a patient, and there's an instrument being involved, a particular tool.

But then if you add to saw, there's an additional kind of location preposition being marked there.

But it doesn't have a semantic role, so it can't be interpreted.

And that sounds kind of trivial, right?

Like most people, when you give them these examples, they say, well, yeah, obviously, that's kind of not a very deep thing.

But actually, it's a pretty puzzling phenomenon.

It demands explanation.

It has to be explained somehow.

So one of the operations that's been involved in this minimal computational procedure is called merge.

So the operation merge simply takes X and Y and forms the set XY.

It just puts two things together.

And this constructs the binary branching structures that I mentioned.

But merge itself can also derive some core set theoretic properties of linguistic relations, such as membership, dominate, term of, and so on, these different hierarchical relations between nodes in a tree.

one branch of a tree being higher up or more deeply embedded than another one, as well as other relations, what's called C command, we can put that aside.

So in brief, much of the complexities of syntactic relations can be derived from successive instances of this simple merge computation, reducing complex visibles to simple invisibles.

So for instance, the example I gave here with which game, you can imagine

the set A, B being constructed, and then if you take the same element B and you merge it with the set again, you get B, A, B, and then you can delete the first appearance of B, and then you simply get the linear order would be B, A, right, rather than the actual set being interpreted B, A, B. And this seems to be what's happening here, where you have a B over here, which again, and then an A, the full structure, and then B again, being originally merged.

So all of these superficially complex linguistic phenomena, which on the surface seem very complicated and very elaborate, they can all be boiled down to a very simple operation, which is just take two things and put them together.

And what's interesting about this is that related phenomena, like a junction, do not involve modification of the semantic content of the structure the adjunct is concatenated with.

So an adjunct is something like a prepositional object, like in the park or to the beach.

So, for instance, if you take a structure like John and Mary talked in the park, the fact that they talked in the park doesn't change the fact that they talked, right?

So the actual original interpretation, the syntactic structure of the integrity and the meaning of John and Mary talked is not changed by adding an adjunct.

So you can add an adjunct in the part, but that doesn't change the fact that John and Mary still talk, right?

Again, that sounds like a very trivial property.

It's obvious, right?

But again, it demands explanation.

There has to be a reason for that.

And the reason why is it turns out that when you add an adjunct, you simply concatenate.

You simply linearly add it to the end.

You don't change the actual identity of the phrase itself.

There's no, what's called labeling.

There's no labeling involved in prepositional merging.

So you don't change the identity of the phrase.

So the FEP has been equated with the principle of least effort, and its process theory is active inference.

So strictly speaking, the FEP basically is a computational principle, right?

The probabilistic beliefs it's concerned with are directed at something, namely external states of a self-organizing system.

And in a similar way that the FEP is a research heuristic, so too is much of recent theoretical linguistics guided by programmatic concerns, right?

Like I said, it's a program, it's an ongoing kind of ideology.

On the other hand, linguists have developed theories of syntactic least effort, like I said, but the process theory is a little bit less clear.

How it's actually implemented is slightly less clear, but I would argue may become more clear if it can be accommodated within existing frameworks and knowledge that the active inference framework can bring with it to help solve the puzzle of how a language is implemented in the brain.

So here's another example for you.

Routinely, poems that rhyme evaporate.

So in this instance, routinely exclusively modifies evaporate.

So the word routinely goes with evaporate.

That's how they're interpreted.

So it cannot modify rhyme, even though this word is closer in terms of linear distance to routinely, right?

So rhyme comes one, two, three words after.

But evaporate comes one, two, three, four in terms of linear distance.

So they're more linearly remote, and yet in terms of interpretation, they go together.

And the reason is the matrix predicate, evaporate, is closer in terms of structural distance to routinely.

And the reason why is that rhyme is embedded within the phrase headed by poems.

So it exists on a different hierarchical plane, if you like.

It's kind of lower down in the hierarchy than evaporate.

So language computes over structural distance and not linear distance.

So sentences are not simply beads on a string.

They're not linear objects.

They have to be linear objects in terms of sentry-motor externalization because we live in the universe we live in.

We can't speak in parallel.

We can only speak in linearization.

Although there is some evidence that sign languages can communicate to some degree in parallel.

Like you sign one element with one hand and then another with the other hand.

So there's some evidence that sign language might be able to defy the laws of physics, but it turns out that's probably exaggerated to an extent.

It is still a form of linearization, but just kind of a co-linearization.

So language prioritizes the demands of the syntax semantics interface over other subsystems, like morphophonology.

So while two structures might exhibit different linear orders, they may exhibit the same underlying hierarchical order.

So here's a really good example in English and Basque.

The verb, direct object dependencies are the opposite, but the interpretation is conserved.

So John has read the book.

You have John, then auxiliary, then the verb, then the object.

And in Basque, you have a different order.

You have John, then the book, then the verb, and then the auxiliary.

and yet they mean the same thing.

They have the same interpretation, right?

So this suggests that a kind of more fundamental operation is going on here, namely syntax encodes the verb and direct object as an abstract phrase, which omits the subject.

So in other words, in the syntax, in English and Basque, you have the same underlying syntax, which is subject and then verb and direct object being merged first.

So you merge the verb and direct object first, and then you accommodate the subject.

And then when you linearize that, when you, you know,

communicate it externally, you do it in different ways.

In English, you do it one way.

In Basque, you do it another way.

But the basic idea is the same.

You have the same underlying interpretation.

That also accounts for something pretty obvious, namely the fact that you can translate one sentence into another language.

That's a fairly obvious thing that you can do with language.

And so therefore, there has to be some kind of commonality somehow.

But the commonalities might be much more deeper down than most people kind of appreciate.

So through the various stages of language development as well, innocent children don't typically produce expressions that deviate from general grammatical principles pertaining to the structure dependence of rules, even when they produce so-called mistakes.

There's been a lot of research on child language development.

So in other words, when children do make mistakes, they seem to make mistakes which accord with the grammatical rules of their language, which suggests that sensitivity to structure dependence forms a core part of language design.

So corpus studies of infant language exposure reveal that there are actually very few bigrams, let alone trigrams.

So statistical procedures can help, but there seems to be some more kind of innate sensitivity to structure dependence, which seems necessary.

And as a recent paper also reviews, human learners prefer to induce hypotheses that have a shorter description length in logic, with simplicity preferences possibly being a governing principle of cognitive systems, all in accord with what the FFP would predict.

So simplicity-based preferences, I've got a range of formal language models too relating to the notion of minimal description, and you might also invoke principles of minimum redundancy.

So this is kind of a really important idea.

Minimal computation and efficiency seems to be a really general cognitive goal of the brain.

And there's a couple of recent papers, one of them in TICS, I think that came out maybe last year or maybe this year, I can't remember,

I think was called Memory as a Computational Resource, which showed that across a bunch of domains, human memory in its various guises also exhibits an adherence to principles of efficiency.

So I think it's not too surprising to when linguists come along and say, you know, language also adheres to principles of efficient computation.

So all of these ideas seem to be, you know, out there at the moment.

Everyone's kind of coming to more or less the same conclusions, but just using different language, just using different background assumptions.

But the general idea, I think, is kind of,

they all mesh well together.

So, as I said, linguistic computation seems to be optimized for the generation of interpretable structures, rather than for the generation of maximally communicative messages to conspecifics.

So in other words, whenever there's a conflict between principles of computational efficiency on the one hand,

and principles of communicative clarity on the other, the former typically wins.

Now, this is not to say that when we do communicate with each other, as worked by Frist and Archivist, that it's not done efficiently.

When we do communicate, we do do it in an efficient way, but that's a separate question from whether or not the language system is designed in a way as to maximize that communication.

The normal functioning of syntax seems to lead to instances which reduce efficiency and prioritize inference generation.

So the goal of the language system is to generate particular inferences and representations about the environment in an efficient way.

So here's a pretty clear example of this, right?

If you take the sentence, you persuaded Saul to sell his car, the individual and the object can both be questioned.

But questioning the more deeply embedded object, in terms of the hierarchical structure, forces the speaker to produce a more complex circumlocution, right?

So you can say, who did you persuade to sell what?

But you can't say, what did you persuade who to sell?

Even though they mean the same thing, right?

Same word, same interpretation.

All it means is, you know, what is the individual and what's the object?

That's it.

Just tell me who the individual is and what the object is.

But you can only say it if you

i.e.

search for the first possible element to question.

If you search for the more deeply embedded object, you can't do it.

So the structures in 11 involve the same words, same interpretations, yet the more computationally costly process can't be licensed.

So this is a pretty good example, and there's plenty of examples like this, by the way.

I've written a paper about it in Glossa.

Plenty of examples in which there is a clear conflict between syntactic priorities of just generating a meaningful structure and

generating possible structures that would actually aid communicative efficiency and communicative flexibility.

That's not a priority of language.

So other examples show that the acceptability of sentences can be impacted based on the extent to which the construction contributes to a novel, non-redundant contribution to one's mental model's beliefs.

Again, this is really directly in the core of what active inference would predict, rather than those of constipation.

Again, reinforcing the role of syntactic processing in inference generation rather than communication.

So the degraded acceptability in 12b, by the way, the reason why these are numbered 12, 13 is because that's the numeration in the paper.

So the reason why 12b seems degraded relative to 12a seems to stem from the fact that the speakers are unlikely to be ignorant of the relevant content, right?

It's a kind of a pragmatic reason.

So Kim knows whether Saul's in bed.

That sounds okay.

Or Kim knows whether I'm in bed.

Sounds kind of weird.

Even though it's a technically grammatical sentence, it sounds weird because you would never say it.

It doesn't contribute meaningfully to revising or contributing to one's mental models or beliefs about the world.

So therefore, the language system doesn't like it.

which again brings in closer contact language design with the FEP.

And also cases such as 13 reveal how even processes such as contraction are sensitive to hierarchical structure and can't be executed over any random word boundary.

So you can say source taller than kin is, but you can't say source taller than kin is.

And the reason is because there's an invisible phrase boundary between those two elements.

Other examples that are related to it are in 14 and 15, you can say, what do you want to do?

And you can contract and say, what do you want to do?

But if you say, who do you want to read the book?

You can't contract that to generate, who do you want to read the book?

That sounds a bit weird.

I mean, you technically can say it, right?

And if you said that to me, I would know what you mean.

I'd know what you mean straight away.

There's no problem.

But it sounds a bit more awkward.

Again, the idea is that there's an invisible phrase boundary there that permits, that stops contraction occurring.

Efficient computation, or at least structure dependence, I should say, is also exhibited in more classical examples in the literature.

So if you say the man is happy, you can question that structure.

by moving the auxiliary to the front and saying, is the man happy?

And so you might be forced to conclude, well, maybe to form a question, you simply search the structure and take the first possible element, but that turns out not to be sufficient.

So you can say the man who is tall is happy, but you can't say, is the man who tall is happy?

Because who is tall is again, similar to poems that rhyme routinely, is embedded more deeply in the man-headed phrase than is the question,

is happy which is higher up the hierarchy and easily searchable so therefore you have to say is the man who is tall happy because is is actually closer to the element you're questioning in terms of structural distance than the is in who is tall so again syntax cares about structural proximity and not linear proximity and there are also constraints on this as well and so you can say john ate chicken and bread for lunch and

And you can question the whole phrasal conjoint, chicken and bread.

You can say, what did John eat for lunch?

But if you want to efficiently question one element, let's say you already knew that John ate chicken, but you're not sure what else he ate.

You can't say, what did John eat chicken and for lunch?

Which, there's no reason why you can't do that, right?

Why can't you say that?

It's a perfectly fine thought.

You already know that he ate chicken.

Someone's just told you he ate chicken and something else.

But you can't say, what did John eat chicken and full lunch?

Because the syntax respects the integrity of the phrase, chicken and bread.

It has a phrasal identity that syntax respects, and you can't violate it.

You can't just violate the phrase boundary and only interrogate one outcome.

Other examples relate across phrase boundaries and contrast boundaries, not just within them.

So you can say, Sam gave a guitar to me and loaned a trumpet to you.

And you can question both elements.

You can say, what did Sam give to me and loaned to you?

But you can't say, what did Sam give to me and loaned a trumpet to you?

Even though, again, in terms of communicative efficiency, that's a pretty simple structure to generate.

You already know that he loaned a trumpet to you, but you want to figure out what Sam gave to me, right?

And again, these relations of hierarchy, you can find them all over the place.

So pronoun reference is a pretty good example.

You can say, Mary said that he has a lot of talent and that Peter should go far, in which case the pronoun he is being connected with Peter, in which case you have a pronoun he coming before the actual element Peter.

But then when you simply take that there phrase and question it and state it, it's no longer acceptable.

You can't say he has a lot of talent and Peter should go far.

That sounds a bit strange.

He should refer to someone like John.

And the reason why is because when you embed that structure in one, a large structure, it changes the actual identity of the conjunct.

So the conjunct that, headed by that, is a compromised phrase, whereas the conjuncts headed by he and Peter are simply tense phrases, TB.

And other puzzles exist here as well.

So you can say John said he is proud of his house, in which case he goes with John.

But it sounds weird to say in John's house he organized the meeting, when he refers to John.

Again, you can kind of pass it, you can force it, but it sounds a bit more awkward.

It's more natural for he to refer to Peter.

If you say, in John's house, you know, someone else, Peter, organized a meeting.

And the reason why is because code reference via this phenomenal phrase, gunting is barred, since syntax preserves interpretation across movement.

So the original structure that's generated is John said he's in John's house, he organized a meeting, right?

That's generated from a more original structure.

He organized a meeting at John's house.

in which case you have he coming before John in the same kind of tense phrase structure that I mentioned earlier.

So in other words, syntax seems to win over linear precedence.

Other kind of quick examples exist too.

So you can say, I gave her the book that Sarah always wanted.

If you say, I gave her the book that Sarah wanted, again, that sounds slightly strange.

Changing the syntax by adding the adverbial element changes the actual content of the phrase itself, which allows more easier co-reference.

So stepping back a little bit, this whole framework of merging and generating hierarchical structures has been argued in the literature to kind of boil down from a more domain-general, lower-level computational procedure.

So some people have called it the universal generative faculty, which is just the ability to construct hierarchical structures and map them to different interfaces.

So the idea is that when we have our system of moral judgment formation, which involves agents, patients, events, and so on,

that still requires some kind of combinatorial apparatus to generate those judgments.

Same with music.

It's been known since the 70s that musical structures have a kind of hierarchical relation to them.

And then also with numerosity, with numbers, you can imagine that it's been hypothesized by Chomsky that if you restrict this operational merge to a single element and simply reapply it, you can kind of generate the natural numbers, right?

So you can kind of form the empty set and then merge it with itself and then merge it with that object and so on.

And that kind of gives you, you can call it zero and then call the next one one and call the one after that two.

That gives you the natural numbers.

But the general idea is that you have an underlying generative faculty that can interface with different subsystems.

So when it interfaces with the sound system, you get music.

When it interfaces with whatever morality is, theory of minds, structures, or judgment formation, or whatever, you get moral judgments.

When it interfaces with the system of quantification and numeracy, you get the natural numbers.

And then when it interfaces with the lexicon, whatever that is, even more mysteriously, you get language, which is exactly what the algorithm I'm saying right here.

So UGF, major of sound, music, and so on.

But interestingly, only language seems to attribute to these major elements an independent identity.

So with quantification, music, and morality, you simply involve a generation of a chunk, some kind of chunking that's happening.

But with natural language, you seem to get an additional operation.

You don't just chunk things.

You chunk them, and then you give it an additional identity.

You give it a kind of the sum is greater than the parts, I guess.

You can call it that way.

So what's interesting is that you can use that merge structure to then call it again.

So the kind of, you can kind of treat it as an independent object, independent of its constituent parts.

Whereas you can't, you don't really seem to be able to do that in music or language.

Sorry, non-linguistic domains.

So there's also a recent paper by Stan Dehaene, who argues that this capacity to generate nested tree structures is a human-specific, kind of a species property, right?

And he gives a bunch of different neurobiological

instantiations of this.

But the basic idea is that only humans have language.

Alongside that, you get an interest in a unique level of tool complexity, possibly due to this linguistic capacity.

So for instance, spheres are constructed from a rock and a shaft.

But a sphere is not just a rock plus a shaft, right?

A sphere is a rock plus a shaft with an additional abstraction, the functional use of it, the utility.

in a couple of minutes, but language seems to be uniquely concerned with functional abstractions, like use, not just form.

And then on top of that, it seems that only humans have what Chomsky has loosely called the science-forming faculty.

So only humans are scientists, not surprisingly.

We can do things like piercing abduction, or non-deductive complex inference generation, where you get some weird event E occurs, but then we posit that if A is true,

then E would simply follow naturally, right?

So we assume A. And then, of course, on top of that, we have the other example I mentioned, theory of mind, forces by hierarchical language.

You know, I know that, you know that, she knows.

So there's all these kinds of weird human-specific cognitive traits, all of which can potentially maybe be boiled down to or connected to some linguistic capacity.

here's a good example that the hand gives and these are five different types of sequence sequences that you can generate only human beings seem capable of generating nested tree structures as i've argued here and in the paper and we generate tree structures not just randomly we don't just do it because we feel like it we do it efficiently and in the pit for the explicit purpose of active influence generation so transition and timing

chunking, ordinal knowledge, algebraic patterns, these are all non-human capacities too.

Bonobos, macaques, birdsong, they all exhibit these forms of things.

It's only humans that can do option number five, the nested tree structure business.

So again, this is the idea that when you merge car-factory, you don't just merge two nouns, right, car and factory, you actually create a noun phrase, a structure that's bigger than the two parts.

So in other words, a factory is not a noun phrase, and car is not a noun phrase,

you need both of them to create a noun phrase.

And then you can use that noun phrase as an independent unit with its own kind of computational identity.

So this leads to a more important question, I think, which is what is language?

All human beings have language.

We all have very strong opinions about what language is.

But consider the fact that geometry was originally the study of land measurement back in the day.

but developed a sufficiently rich body of knowledge to abstract away from its original object of inquiry and departed also from common sense intuition.

So our common sense intuitions about what language is actually have no place in science.

Ditto for common sense notions of mass and energy of physics.

So at MIT, Professor Ed Fedorenko recently conducted a mechanical tech survey, a study, asking ordinary people what they thought language's primary function was.

Now, most of them said communication, in line with common sense.

And she used this data to criticize the idea from a certain part of linguistics that language isn't basically an instrument of thought.

Its primary purpose is to contribute to conceptualization.

But a physicist obviously wouldn't conduct a mechanical tech survey randomly asking people what they thought the nature of light is.

And a biologist wouldn't concern themselves with people's intuitions about how hearts and livers work.

And so natural language syntax, I think, should be investigated using the same standards of scientific inquiry as any other object in the organic world.

There's no reason why people's intuitions about language should be needed.

In fact, if that were the case, there's no need for linguistic departments.

Let's just ask a bunch of random people on the street.

That's what we could do.

So on top of syntactic phrase generation, we can also frame this as contributing to policies used to perform particular free energy minimizing actions and not just generating linguistic objects.

So the rapid and reflexive identification of objects, states, and events in the external world through simple linguistic means can yield complex, flexible interpretations for some of the most common nominals.

Nominals is just a fancy word for nouns.

aiding in the successful generation of internal models of the environment using a limited number of resources.

So objects in the world have to be identified, and they have to be identified now, immediately, in order to be successful in navigating the world.

You have to understand things straight away and rapidly identify things.

That's kind of obvious.

But that leads to certain puzzles, though.

So for instance, complex forms of what's called polysemy.

Polysemy just means a way of having multiple senses.

And polysemy also turns out to be much more widespread than most people think.

It turns out that almost half the words in the OED are polysemous.

Complex forms of polysemy generated via multi-word constructions allow for a more precise and exact localization in conceptual space than discrete symbols, signs, and gestures.

with natural language syntax allowing the generation of a more accurate unveiling of hidden states in the world.

So natural language syntax allows us to more accurately position ourselves in conceptual state space.

Again, I gave the example of the second blue box to the left, but here's another example for you.

You can say the poorly written newspaper that I held this morning has been sued by the government.

That's a perfectly fine sentence, but it's referring to three different senses of a simple word like newspaper.

So a newspaper can simultaneously be a piece of information, it can be a physical object, it can be an abstract organization, and we can also call upon all of these senses at once.

And yet notice that this sentence cannot possibly refer to anything in the world, right?

This is not a kind of thing that a physicist could explore.

Something that's poorly written, something that you hold, something that can be sued, it's not a coherent entity.

And yet language allows this simple, plesimus word, one single lexical item, to generate a very rich range of perspectives to interpret experience, which is exactly what you would expect from the active inference framework.

And so since there can't possibly be any object in the external world that a complex, poisonous word like newspaper can index a one-to-one mapping with, another framework we are developing here, lexical items could partially be seen as hypotheses about the structure of likely co-occurring sensory input, right?

Or hypotheses about ontological and myriological relations between objects and states in the world.

So in other words, a word is not simply something that has a conceptual meaning.

A word is not simply a fresh concept.

A word is basically a hypothesis about what the world is, what we can interpret experience to be in any given moment.

And we basically test the hypothesis.

So we use the word newspaper as a hypothesis about what's going on outside.

Maybe it fails.

Maybe it succeeds.

It depends on the context.

It depends on our state of mind.

and it also depends on our interests and our concerns.

I'll give some more examples in a second just to kind of illustrate that.

So a recent paper by Carl Princeton's lab, headed by D'Amica Setal, 2020, in the paper Infanteers, they note that from the perspective of active inference, things only exist as a label or hypothesis or inference about hidden states.

So the contention that I'm presenting here is that forms of complex meaning derived from natural language semantics form a core component of this labeling mechanism in active inference.

So linguists like to talk about lexical items, book, table, walk, and so on.

These are basically just hypotheses composed from distinct core knowledge systems in the mind.

So our sense of geometry, our sense of place, our sense of social relations, which can elucidate environmental regularities essential to active inference.

So here's some more examples, and again, a nice little quote from Demi Kass, the Leyland quote.

Let's take the notion of vagueness.

So I was once an infant, but I'm no longer an infant, but I'm still me, right?

And the boundary between infancy and childhood and childhood and adulthood, there are legal terms for that, but that's kind of a just arbitrary choice.

The actual concept of infancy is an arbitrary boundary.

Some philosophers do actually think that there is a specific nanosecond which transitions you from infancy to childhood, but I think that's unlikely.

I think applying fixed quantified notions like that to an intentionally and inherently vague notion like infancy is kind of a paradox.

It's meaningless.

Infancy is just infancy.

It's not meant to be a precise boundary, unless you're a legal scholar, in which case that's fine.

You can define legal boundaries between things, but that's kind of irrelevant for cognitive science.

So consider something like infancy.

You also have things like pile.

So we say there's a pile of sand, and you keep taking bits of sand off.

At what point, how many grains of sand are sufficient to make a pile, right?

That's called the Serratis paradox.

The vague notion of pile is great for active inference.

It's great for generating rapid inferences and assessments.

But as soon as you actually interrogate it sufficiently, the system becomes exposed.

all that much scrutiny and then same for things like a book imagine you go into a library and there's let's say a thousand physical books but there's only an 800 kind of actual abstract books in the sense that you know every library has multiple copies of different books so the library will have you know 10 copies of the bible 10 copies of the quran and so on and let's say john goes into the library and he reads every book in the library and he leaves the library

because he's fed up and all the books are rubbish.

So he just decides to burn it down.

In that case, you can say John burned every book in the library, or John read and burned every book in the library, in which case he burned more books than he read, right?

He actually burns 1,000 books, but he only reads 800 books.

So the phrase every book does not pick out a fixed quantity.

There's nothing in the external world that actually...

exhibits a one-to-one relation in terms of quantification.

It could be 800, it could be 1,000.

It depends on our perspective.

And that's the crucial thing about even simple words like book.

They generate these very rich, polysemous perspectives that you can use to interpret experience but have no necessary component to them.

Another example is something like a city.

So you can say London burned down and was rebuilt 50 miles up the Thames.

London can still be London, even though it's physically completely changed.

It's in a different location, all the Londoners are dead, and so on.

London is a very complex place in a sense that you can decompose into organization sense, location sense, population sense, and so on, government institution.

But the single word London does not refer to anything in the world.

So in other words, there's no such thing as London.

That's just a kind of convenience.

It's a convenient abstraction that we use to interpret experience.

But there's nothing in the world that the word London refers to coherently.

You can use the word London to refer, but that's an action.

It's an act of human will to actively do that and choose to do that.

So it's a choice and it's an action, again,

while in a code of active inference.

You can choose to voluntarily and willfully refer to one specific component of your representation of a city to refer to something in the external world, but that's a context by context case.

The idea that London invariantly refers to a particular structure is just not true.

So by permitting a more refined, accurate positioning in conceptual space, natural language syntax aids agents in the formation of novel policies to navigate and make inferences about the environment.

So cognition is an ongoing process of dynamic interaction between an organism and its environmental niche.

Yet notions like event are also not predefined external entities, but are actively generated and partially divided by a language system.

So again, events are not things in the world, and events are things that are mind construction.

So consider also that simple electrical items like city have properties that go way beyond the semantic complexity of other atomic representations.

So you can say the large school with large windows next to the river starts at 9 a.m.

and has a strict headmaster and unruly students.

So there's nothing in the experimental world that could possibly be a location, an artifact, an event, a social group, right?

Surely not.

Absolutely not.

That's not an ontology.

And also using these kinds of sentences surely doesn't commit us to the belief that there are such things in the world.

It's a convenient abstraction.

It's a fiction, right?

It's basically a fiction, a useful fiction.

that is used in the service of active inference.

And it does a very good job of it.

It's very successful.

And the fact that it's so successful is evidenced by the fact that philosophers have only just begun to really investigate this phenomenon.

This phenomenon is called complex polysemy.

And it's taken centuries of inquiry to actually realize that some of our most basic common nominals do not have reference to things in the world.

They're just convenient fictions.

So take another sentence.

The average man is concerned about wage cuts because he needs to afford insurance.

Does language commit us to the belief that the world is made of things like average men and wage cuts and relations of concern?

That's surely not something that we're committed to.

London can be, as I said, London can be fun and polluted and burnt down and rebuilt 10 miles up the river and still be called London.

So these nested tree structures that I mentioned earlier are widely considered to be abstract, but even simple words exhibit considerable abstraction.

So, in other words, the paper I've written with Friston and Holmes focuses on nested tree structures, but it's also worth pointing out that even simple words themselves exhibit a considerable degree of abstraction.

And perhaps just as relevant here is Bertrand Russell's invitation for us to consider a blind physicist who knows all the physics, right?

In some kind of hypothetical physics complete scenario.

So what is it that a sighted person knows that the blind physicist doesn't know?

If this physicist knows everything.

Certain experience of contentually, right?

So what it's like to see the colour red.

That's not part of the blind physicist's knowledge.

So therefore physics can only capture the causal skeleton of the world.

We can at least conclude from this that my experience of seeing the colour red simply is a property of the world.

but one that we can't provide any naturalistic account for.

And the reason why I mention that is because I think that may also be the case for words like London and city and book.

These abstractions are considerably much more intricate and much more complex than we usually give them credit for.

Our minds have managed to achieve an analysis of the concept of number,

Number theory is very rich, a very serious field in mathematics about number theory.

But there isn't really much of an analogue in linguistics.

So lexical semantics is nowhere near as robust, nowhere near as detailed, and nowhere near as sophisticated as number theory.

Lexical semantics is pretty much the meaning of water is the set of all things that are water.

That's basically it.

If you pick up a semantics textbook,

semantics textbooks are pretty much just that.

The meaning of it is raining just means it's raining, right?

That's it, just like a re-description.

So in other words, linguists are very far from actually having a serious naturalistic account for even some of the most simplest words, which again might just simply be because of our cognitive limitations, right?

We can't actually construct theories for these objects.

So one of the ways to exhibit this rich polysemy is by looking to the philosophy literature.

So in the philosophy literature, there's something called externalism, which is the position that I've just been critiquing, which is the idea that words can have a kind of one-to-one reference with things in the external world.

And there was a survey conducted not too long ago which showed that the majority of philosophers are externalists.

They do believe, in fact, that the word water refers to H2O or whatever.

So consider this famous thought experiment, which I think contributes to our understanding of active inference, as I'll show in a few slides.

In some parallel universe, it's said that water is not made of H2O, but rather some other substance, right, XYZ.

So in the parallel universe, planet F2, the exact same as planet F1, except water is not made of H2O, it's XYZ.

So the question is, can the inhabitants of this twin air use water to refer to the substance?

So externalists say no.

Externalists say the meaning of water can't be applied to the substance.

On the other hand, in contrast to the externalists, there's what's called the internalist position, which says that the meaning of words is simply a conceptual structure.

And that's it.

There's nothing in the external world that these things refer to.

So the internalists obviously say yes, of course it can, because it's just a concept.

So the term water seems to be polysemous between some more kind of common function-based sense and a more concrete technical sense.

So you can imagine that, let's say one of the examples that Noam Chomsky has given is, imagine that there's a tea factory that kind of explodes and some of the tea leaves in the factory get into the local water system.

And so what comes out of someone's tap is chemically identical to the cup of tea that they're making in the kitchen.

And yet one of the substances is water.

and one of the substances is tea, even though they're chemically identical, right?

Chemically the same thing, and yet one of them's water, and one of them is tea.

And the reason why is because one of them satisfies the functional based criteria, and the other one violates it.

Indeed, you can imagine another parallel universe.

So then, Paul Petrosky offers what he calls paternal air,

where doppelgangers of our scientists discover that what they've all been loosely calling mud, in fact, has a deep uniform structure.

So obviously on planet Earth, our planet Earth, there's no uniform structure to mud, right?

Mud can be anything.

But it turns out that in this parallel universe, all of their examples of substances of mud actually exhibit a uniform structure, X, Y, Z.

And so the argument is that they can use the concept mud to refer successfully to all physical structures of mud.

That's good for them, right?

They could successfully use the word mud to just refer to X, Y, Z. But does it follow from this that the inhabitants of fraternal Earth could not then travel to our universe and use the word mud to refer to our chemically diverse samples if they came to a black hole?

And I think the answer is no.

The externals would say yes.

The externals would say, well, their meaning of mood simply refers to X, Y, Z. And since we don't have X, Y, Z in our universe, when these people jump in a black hole and come to us, when they talk of mood, they have to be speaking of something else.

But that surely violates the actual meaning of the term mud.

It's a conceptual representation.

It's not a physical structure.

So the idea that the natural kind-conforming use of mud could not readily be extended to a polysemous sense doesn't really seem to be well supported.

And it's just not a good description of what language actually is or what it cares about.

Language doesn't really care about the world.

It doesn't care about the external world, as it actually is.

That's what science tries to do.

Science tries to achieve reference to things in the world.

But the language system just cares about active inference.

It just cares about making sense of the world.

That's it.

It doesn't actually care if water is made of H2O or not.

That's not relevant.

So we can use simple words like water and wood to access multiple concepts, and then use those concepts in the service of active inference.

In fact, Petrosky goes further, and he shows that using government statistics, US government statistics, he notes how Diet Coke has a higher percentage of H2O than stuff from my world.

In fact, I'm drinking a bottle of Dr. Pepper, and I'm not sure if it actually has that information, but Dr. Pepper has almost definitely a greater content of H2O than stuff from the world in your backyard.

And in fact, diets brighten clubs so they are even more like H2O, and yet they're not deemed water for reasons purely to do with intended purposes, right?

So I think a cup of tea is like 99.7 or 99.5% H2O, and yet it's called a cup of tea, right?

It's not water, it's tea.

So moving even further away from this, consider that even scattered entities, forget about water, let's talk about scattered entities.

Scattered entities can be taken to be a single physical object under some conditions.

So imagine a picket fence with brakes or a cold immobile, right?

The latter is a thing, a cold immobile is called a thing, whereas a collection of leaves on a tree is not a thing.

Unless, of course, these leaves a place for the pictures of decoration or art installation.

And so the reason seems to be that the mobile is created by an act of human will.

Again, the functional notion is important here.

So here's a question.

How are these human-specific notions of function and intention coded into the lexicon?

And how are they coded as part of any gender model under active inference?

That's a really tricky question, right?

And indeed, going beyond this too, Bertrand Russell famously claimed that objecthood is based on spatiotemporal continuity, but that also seems to be not sufficient.

The four legs of a dog can be seen as a single object under many conceivable contexts, such as if they were cut off, tied together, and used as a doorstep, but they could still be understood by its user as part of a dog.

So abstract objects do not bear causal relationships, and they're also not spatiotemporally located.

So an object is usually understood to be a concrete thing, hence the confusion when some are denied spatiotemporal relations.

So an object is an object if we deem it so.

And in addition, I think it's important to note that a psycholinguistic lens is needed too.

So when philosophers talk about externalism and internalism, they often just talk about language without actually knowing anything about linguistics.

trying to do philosophy of physics without knowing anything about physics.

That's kind of strange, right?

If you want to do philosophy of linguistics, philosophy of language, you should really know about linguistics.

So here's one particular paradox.

In the philosophy literature, the following contrast has been called a paradox, like a problem.

So you can say Batman fights more mobsters than Bruce Wayne, but we also know that Bruce Wayne just is Batman, right?

So therefore, we should be able to say Batman fights more monsters than Batman, right?

But we can't say that because it sounds weird.

And the reason why is due to linguistics.

It's not because of philosophy.

So then there's a constraint on discourse interpretation in language through which whenever there's two referential expressions in a clause, they're default interpreted as non-identical, okay?

And this feeds into redundant computation, which again, feeds back to efficient computation.

And as such, reference is operative.

So if you have two instances of Batman, there's a problem.

So the sentence in B forces us to search for different reference, even though we know it's the same reference, right?

So this paradox in philosophy of language is not due to mind-world relations.

It's just due to linguistics.

It's just a psycholinguistic phenomenon.

That's all it is.

It's pretty simple.

And again, one of the problems here is that a lot of these quirks of language

and interrogation, and they're not immediately obvious.

Language is very good at constructing an illusion that we kind of become susceptible to.

We like to think that the things we talk about are really, really existing in the world.

But in fact, language use, I like to think of it as kind of a fairytale.

Using language is kind of more akin to constructing a fairytale than it is science, because we're just constructing concepts and using them in a kind of Luciferian language game sense.

So here's another example, what are called Escher sentences.

So there's the famous staircase painting of Escher.

The endless staircases go round and round and round.

So the visual system doesn't care about that.

The visual system just sees what it sees.

If that turns out to be a physically impossible construct, that's not relevant to the visual system.

We just see whatever it sees.

And it's the same with language too.

So language also has things like analogous things, what are called Escher sentences.

So if you say more people have been to Russia than I have, or in Michigan and Minnesota,

More people found Mr. Bush's ads negative than they did Mr. Kerry's.

That's actually a meaningful sentence.

It doesn't mean anything.

More people have been to Russia than I have is a meaningful sentence.

It kind of sounds like it makes sense, but it's completely meaningless, right?

Because you're trying to compare a fixed finite quantity, like 50, to a simple binary yes or no, right?

You've either been to Russia, yes or no, and then more people means five or six people, right?

So it's syntactically legal, but it's semantically incoherent.

And there's plenty of sentences like that where the language system is very good at generating the illusion of meaning, or generating the illusion of structure, when in fact if you interrogate it, there's no meaning to it.

And again, this feeds into the idea of efficient computation, of inference generation.

The language system doesn't want to interrogate too deeply, it just wants to generate an inference.

What is this person trying to say here?

And on the other hand, it also creates an idea of anti-reference.

So this sentence sounds meaningful, but there's nothing in the external world that it can refer to.

So there's no real comparisons being made here between more people and me being to work at Yes or No.

So in conclusion, it seems that any object is much more than its material constitution or its function.

We can also use its origin.

So Thomas Hobbes talked about rivers.

His famous example was a river can be maybe defined by its origin and then it can kind of skirt and diverge and go into different paths and then maybe converge again.

But also a sense of continuity.

So John Locke's theory of personhood was that a person is defined by a sense of continuous identity, not a physical constitution.

So when a child watches a cartoon of the handsome prince getting kissed, he turns into a frog, and then he turns into a human again once he's kissed again, or whatever.

There's a chaos, and then something happens.

The child knows that it's the same person.

The child watching the TV knows that it's the same entity.

It's the same person, the prince being turned from a human to a frog.

And yet, again, that's got nothing to do with reference.

There's nothing that could coherently exist in that sense.

So all these representations seem to conspire.

And in addition, we also have a kind of a fifth element.

So we can kind of call this extra-linguistic biases, the shaping objecthood.

So that pertains to one example.

There are lots of examples, but one example is default marking of object surfaces.

So if you say John painted the house brown, this implies that he painted the external surface brown, not the internal surface.

Because we seem to have a sense of objects as being concave objects.

And I think of scenes as well, like scenes, events, inside scenes or outside scenes.

So we seem to have a kind of visually imposed sense of what our house is.

So if John and Mary are both stood five meters from the surface of the house, but Mary's inside the house and John's outside it, John is near the house, but Mary's not near the house.

Mary's inside the house, even though they're both equities, the actual physical structure of the house.

So the house is, again, a functional notion.

It's not just a physical object.

It's also a functional based interpretation.

So in other words,

at least these five components, they all contribute to active inference.

They all contribute to generating structures, but at least these five components are somehow encoded in language.

And I consider that to be one of the biggest mysteries.

How are these things encoded in the lexicon?

And how are these networks across the brain interpreted and activated during language comprehension?

That's an extremely problematic issue.

Because like I said, when we talk about schools having strict headmasters and being large and near the river and et cetera,

we're using pretty much all of these concepts at once.

And we're doing it effortlessly.

And yet somehow, how they're actually implemented is kind of a mystery.

But there is, interestingly, precedence in recent active inference literature for these suggestions.

So in a recent paper, Ramstad et al argue that the FAP is most compatible with an instrumentalist theory of mental representations, via which representations are useful fictions for explanatory goals, right?

Which is exactly what I've just been saying about linguistics.

And this is also compatible with certain models in philosophy of language, the analyst perspective, which assume that less-for-items have no one-to-one direct reference in the external world, but are basically useful fictions.

They're composites of distinct representational domains that are used for successful, efficient interpretation and ultimately agent survival, right?

And it's also compatible with internalist models of Markov blankets, which have been argued to form kind of Neil Cantillon and Helmholtz in The Counts of Cognition, whereby the boundaries of cognition are delimited by its skull, emphasizing the interactive, constructive nature of higher cognition and generating interpretable, actionable concepts.

Again, the crucial concept of actionable concept.

It's a useful concept.

It's something that you can use in some meaningful way.

So the long-term storage of frequently generated lexical characters and the combinatorial rules underlying their creative deployment in language production and comprehension, all of this seems to allow speakers to categorize novel sensory data into a discrete set of objecthood and eventhood representations.

So there are a few events that cannot actually be passed through the simple and unique schemes provided by language.

which increases the likelihood of speakers avoiding surprising states.

The more efficiently and readily you can pass a particular situation as an event, then that leads to surprise minimization, right?

So another recent paper notes that the active inference model of the brain assumes an imperative to find the most accurate explanation for sensory observations that is minimally complex, which has been recruited in Barlow's Exploration of Minimum Relevancy, and which seems to accord with how the language system provides the most computationally efficient format for solving the problem of mapping linear sensory input, right, so linear sentences, to hierarchical interpretations.

So from the perspective of active inference, individuals need to minimize the effort involved in meaning-making.

So we propose that there is increasing evidence from theoretical linguistics and natural language syntax that this structure exhibits design principles in keeping with belief-setting criteria.

So another recent paper by Christen's group proposes that the goals of speech segmentation involve sampling data in a way that requires the most parsimonious degree of belief updating in accord with Occam's principle.

So we've basically extended these claims to the domain of natural language.

Indeed, active inference has only one underlying imperative to minimize generalized free energy or uncertainty.

And much work in psycholinguistics, so things like eye tracking, tracking people's eye movements during their reading of sentences, during fill-the-gap dependencies, long-distance

features, shows that phrase structures are generated predictably in anticipation of upcoming stimuli.

In fact, it turns out that even something as simple as adjective-mound phrases are constructed predictably.

So things like red boat, simple two-word phrases, they involve rich prediction.

So maybe I should just stop there for a second to evaluate and


SPEAKER_01:
This has been an awesome learning experience as a non-linguist.

There's a few questions from the chat, and there's also just a few other things I wrote down, but a lot of the questions have to do with how things happen in the brain.

So maybe it's worthwhile for you to just share however much more you'd like to share, and then hang out as long as you'd like to answer some questions, and you're always welcome back.

So no worries, just whatever's comfortable today, let's talk through, and then we'll continue the discussion.


SPEAKER_02:
Cool.

All right.

Well, yeah, this is the final section just to kind of wrap it all up together.

So only a few slides more.

But yeah, the question of how all this relates to the brain is absolutely essential.

And so I'll try and explain that.

So just to begin with that framework.

Yeah.

So so far, I've kind of just outlined the kind of basic philosophy of how language is implemented in a kind of computational theory.

how it seems to be involved in efficient computation and so on, how it might, on a cognitive level, contribute to inference generation.

But what about actual neural implementation?

So that's kind of the next frontier.

So there's a recent paper by Van Roosch and Baggio arguing that

makes a good theory is not just generating testable predictions it's invoking plausible possible mechanisms mechanisms that are plausibly realized in nature either in your biology or genetics or physics right it's kind of a nice framework the idea is that uh theory is not just there just just because they can generate um you know true testable predictions it's it's to generate things that are plausible and i think that's a very important point so much of current um

but the neurobiological language involves quite reasonably testing some hypotheses and then generating fully available explanations for results.

So you'll read the results section, which shows like, let's say, hippocampal theta power increases, or semantically coherent sentences maybe, just to choose a random example.

And so the explanation is just kind of a re-description of the results in the description section.

So it'll be like, you know, we found

Hippocampal theta increases in x, y, z. So therefore, semantically coherent sentences are indexed by hippocampal theta.

It's kind of a re-description.

But then what we need is really a pre-existing mechanistic understanding of the possible computational properties of hippocampal theta.

So what is hippocampal theta in the first place?

Otherwise, what's the point of looking at it?

There's no point looking at some neural response if you don't understand the computational capacities

to see what is that lower level mechanism.

And indeed there might be, there usually are multiple mechanisms, multiple possible mechanisms for realizing that particular signature that you get through brain data.

If your simple output is to simply do the experiment and then re-describe the results using different rhetoric, then that's not contributing to conceptual progress.

So a lot of these ideas were outlined in a recent book I'm buying, The Assolutory Nature of Language.

tease them apart, but if you're interested in more of the details, you're free to contact me and I'll send you a copy.

So one can derive some elementary properties of linguistic computation through a direct line of communication from the FEP through to endogenous oscillatory synchronicity and linguistic behavior, which kind of comes out of the outcome.

So under the FEP, endogenous oscillations are the type of dynamics, brain dynamics, that neurons would expect to encounter since they have genetically encoded beliefs that the cause of excitatory postsynaptic potentials follows a certain pattern.

So active inference can synthesize various in silico neurophysiological responses via a gradient descent on free energy, such as the mismatch negativity phase procession, theta sequences, place cell activity, theta camera, phase amplitude coupling, and so on.

And the reason why it's important is because a lot of these mechanisms have been involved and implicated in language.

So moving forward with these concerns, neuronal dynamics and plasticity appear to minimize variational free energy under a simple generative model, which entails prior beliefs that presynaptic inputs are generated by an external state with a quasi-periodic orbit.

Recent papers show this.

So the implication is that ensembles of neurons make inferences about each other, while individual neurons minimize their own free energy.

So generalized synchrony kind of comes for free.

It's an emergent property of free energy minimization.

Desynchronization is induced by exogenous input, explaining event-related desynchronization, and structure learning emerges in response to causal structure in exogenous input, explaining the functional segregation of neural clusters.

So an external, like neuron external state with a quasi-periodic orbit is assumed to generate the presynaptic inputs of a given neuron.

And what's interesting for me is that low-frequency phase synchronization emerges directly from this assumption.

and also the coupled assumption that neuronal dynamics maximize variation of energy.

So the reason why that's important for language is that one particular implication is that models of syntactic computation grounded in these dynamics, so, you know, the model that I have in my book, but also some other recent models from Giroud, Bufeld, and Habano et al., can be said to comply with foundational principles of the FAP.

So, for instance, endogenous load frequency delta, delta phase tracking of syntactic nodes,

could be seen as emerging as a direct function of generative belief updating in a core of a vacuum inference, supplementing the association of delta oscillations with the cortical computations responsible for creating hierarchical linguistic structures.

So what that means is whenever you get a particular phrasal node, a phrase boundary, you seem to get some kind of unique low-frequency response.

The details can be found in the actual papers, but it's basically some kind of unique low-frequency signature, which seems to code, okay, this is a phrase, here's another phrase, here's another phrase.

And that kind of goes beyond the level of abstraction contributed by syllables and words.

So it was kind of a phrase-specific signature.

And so the active inference framework provides clear predictions about the neural dynamics of language and can help bring together research programs that are presently pursued independently.

So exploring the possible neurobiological basis of a core feature of language, a recent paper argues that theta-gamma phase-amplitude coupling in language, which codes syllable recognition, and also predictive coding,

and theta gamma coupling has been associated with predictive coding for a little while now, can be brought together.

So this theta gamma coupling has been applied to silver parsing, modeling theta gamma coupling for dialogue as well, which appears to form part of belief updating for active inference, whereby beliefs are simultaneously updated at high, fast levels, but also low, slower levels.

Also, theta gamma coupling has been applied to, well, it's been assumed to be a constraint in working memory.

So for instance, the idea is that

The number of items you can hold in working memory is based on the number of gamma cycles you can embed in a given theta phase.

And it's a kind of trade-off between fidelity.

If you want to hold more items in memory, then you can indeed increase the number of gamma items to 7 or 8.

Gamma cycles within a theta cycle are about 7 or 8.

But then that lowers the actual resolution.

And in fact, it's been shown that there's a relationship between the number of items held and physical causal manipulation of theta-gamma coupling in pupils.

skulls using some kind of extra tags or TMS, and their actual performance in working memory tasks.

So Theta Gamma doesn't seem to be causally implicated in just constructing sets in working memory, which I think is kind of a cool idea.

So in this particular model, phrase level inferences generate words contained within the phrase before lower levels reset to the next phrase, manifested as Theta Phase Alignment.

So each transition at the high level is accompanied by a resetting at the lower level stages.

So this is in line with the suggestion that low-frequency phase can coordinate the bundling of lexical features indexed by class gamma cycles within a given structure via forms of cross-frequency coupling, namely phase amplitude coupling and also phase-phase coupling, ensuring serial readout of features alongside the transfer of syntactic identities to language external systems.

So there's the kind of top-down information between different types of phase-on food coupling.

This is kind of elaborated in my book.

But the basic idea is that low-frequency phases coordinate and set the identity of whatever representations are being fetched and linearized and combined and chunked

via these faster gamma cycles cross-cortically.

Cross-cortically just means whatever part of the brain happens to be responsible for storing the representations that you care about.

Again, language is very good at talking about extracting, fetching concepts from different domains.

So that would yield predictions for which cortical surfaces are being extracted and spoken to by this low-frequency phase coordination operation.

So, theodogamical coupling has also formed part of recent models of scheduling and updating the list of syntactic semantic features being associated with a given chunk of linguistic stimuli, with gamma cycles indexing distinct data structures being coordinated by the database.

Data structures just means linguistic features, so semantic or syntactic features, right?

Like first-person number features, what have you.

So, these proposals are potentially analogous from a neurocomputational perspective.

So, you have the same

lower level generic algorithm, neurocomputational algorithm, that's simply fetching discrete domain specific representations.

Which also kind of feeds back into the Haynes idea I mentioned earlier, where you have these different systems, morality, music, mathematics, language, where the computation seems to be analogous, but the representations are different.

So the same computation operating over different representational domains.

So through specifying a process theory that explains neuronal responses during perception and action, neuronal dynamics have previously been cast as a gradient flow on free energy.

So that is to say, any neural process can be formulated as a minimization of the same quantity used in approximate phase inference.

So the brain seeks to minimize free energy, which is mathematically equivalent to maximizing battle evidence.

And so this view of neuronal responses can be conceived with respect to Hamilton's principle of least action.

So all these ideas kind of weave together.

And in fact, recently, a deep temporal model for communication was developed based on a simulated conversation between two synthetic subjects, showing that certain behavioral and neurophysical correlates of communication arise under variational message passing, in particular, theta gamma coupling.

So theta gamma coupling arose from this particular synthetic dialogue.

So the model of syntax I've assumed in the paper assumes that syntax, sorry, in this particular paper by Christen, assumes that syntaxes are sequences of states or words,

a terminal node at the end of every sentence so kind of phrase boundary like a wrap-up effect right like a consolidation period and with each form of syntactic structure being limited to uh questions and answers in a game of 20 questions in this particular paper and but the conclusion of this paper of pretty much in keeping with core assumptions and linguistics concerning the inherently constructive nature of language so elementary syntactic units which are highly robust and concerned across speakers of the same language

provide specific belief structures that are used to reduce uncertainty about the world through rapid and reflective categorisation of events, states and objects and their relations, again in compliance with the FEP.

Sentential representations can be thought of as structures designed partially to consolidate and appropriately frame experiences and to contextualise and anticipate future experiences.

So the range of possible syntactic structures available to comprehend this provides alternative

alternate hypotheses that afford parsimonious explanations for sensory data, and as such they preclude everything.

So if the complexity of the linguistic stimuli can be efficiently mapped to a small series of regular syntactic formats, this contributes to the brain's goal of restricting itself to a number of states, as I mentioned earlier.

And again, by mapping syntactic structures to conceptual systems in a manner adhering to principles of economy, language can be seen as engaging in a series of questions and answers with sensory data itself, but also non-linguistic mental states.

And only through natural language can we generate the full complexity of WH questions, you know, the questions I mentioned before, cross-serial dependencies, long-distance dependencies, where different elements across different structures depend on each other, fully gap dependencies and so on, which permit an expansion of what kinds of querying the brain connects to over sample data.

So in other words, only with natural language syntax can the brain execute these particular type of queries and generate

So all the ways that language-centered aspects, again, if you recall what I mentioned at the beginning, all of the ways that language-centered aspects of human cognition can be motivated through conformity to the FEP and active inference, so things like communication and narratives, all of that can actually be further derived from a more elementary focus on syntactic computational complexity.

And in fact, there was a paper published a couple of days ago, again by Kristen's lab, which showed that neural dynamics under active inference are metabolically efficient and suggest that neural representations

in biological agents may evolve by approximating steepest descent in information space towards the point of optimal inference.

And again, that's not a bad idea to pursue in connection to neurolinguistics in terms of the optimal inferences afforded by not just syntactic structures, but again, also lexical semantics, so individual words that I mentioned before.

So just moving to a couple of conclusions from the slides.

I've tried to show that natural language syntax renders meaning-making and high-order inference a computationally efficient process, and it seemingly makes it right for what Lacoste et al.

call a key question for future research for active inference, which is how biological organisms effectively search large policy spaces when planning into the future.

So regardless of whether you want to assume particular economy conditions, X, Y, or Z, my motivation's been kind of more general.

It's just meant to consider how the FPP can, in principle, provide a novel explanation for the prevalence of efficiency-encoded linguistic rules.

And indeed, other linguists might disagree with me and disagree with the actual framing, and maybe the other linguistic frameworks, like maybe Ray Jeckendorf's framework of parallel architecture.

That might be more appropriate, too.

Depends on your background assumptions.

So it's specifically natural language syntax and its capacity for sentential complexity that allows language users to expand the scope of their prediction about their future position in the state space.

we can think of more possible future scenarios.

So we've arrived at a number of suggested explanations for the way language implements deconstruction of hierarchical syntactic objects, namely to minimize uncertainty about the causes of sensory data, to unveil a species-unique format of external hidden states, to adhere to a least-effort principle, and in fact, in some cases, this involves externalization, but not always.

So exploring our disposal empirically may demand a more mature development of the science of computational complexity in the brain.

So I basically argue that all of the ways that language, syntax, words, and cognition can be motivated for the MVP can simply be grounded through syntax.

So in other words, narratives are strong candidates for constructs adhering to active inference, but the generation of syntactic phrase structures is a necessary feature of any narrative, right?

You need to at least construct a phrase to generate a narrative.

So I've reviewed how the FEP that underwrites active inference is an expression of the principle of least action, which is additionally a principle implemented in models of syntax.

So ultimately, both the FEP and syntactic theory are empirically and conceptually well-supported constructs.

And as I've argued, they share a number of intriguing commonalities.

So while the FEP has produced formal simulation-supported models of many complex cognitive mechanisms like action, perception, learning, and attention, and also communication, on the other hand, models of syntax

have explained grammaticality intuitions, certain poverty of stimulus issues, i.e.

how kids acquire language, and the pervasive organizational role that hierarchy has in language.

So hierarchy just seems to pretty much organize and determine almost all linguistic structures.

Further, language is not the only domain which exhibits economy, right?

Suggesting a deeper grounding of this bias in natural law.

Other domains include concept learning, cause of reasoning, central motor learning, and also, as I said before, memory.

So importantly, active inference has been used to account for also creative functions that have to do with exploration and novelty.

And the reason why that's important is because linguists have also long argued that the hallmark of natural language is actually its creative aspect.

The ability to freely construct an unbounded array of hierarchically organized expressions with novel interpretations.

So you can say sentences that have never been said before.

Linguistic creativity can be framed in terms of adherence to physical thermodynamic conservativity.

If it does so,

to minimize uncertainty in developing states within an individual model of external state.

So in other words, the more efficiently a language user can internally construct meaningful hierarchically organized structures, the more readily they can use these structures to contribute to the planning and organization of action, consolidation, exogenous and endogenous monitoring, and adaptive environment sampling.

I think it's worth recalling Gregor Mendel's application of complex algebra to botany.

At the time, this was deemed by many people to be

of weird and almost surreal and but in fact the same may be true of level conceptual directions and for natural language syntax and semantics right unconventional approaches that often soon turn out into the new normal uh only time can tell how far these directions can actually be pursued though all right thank you for listening okay sarah awesome you can unshare and then i hope we can ask a few of the questions yep


SPEAKER_01:
Yeah.

Do you want to unshare your screen?

Oh, great.

Okay.

Awesome.

All right.

I will just jump right in with the first question.

Again, as a non-linguist, it was like looking up a bunch of words, learning a lot.

So really fascinating.

The first question was, and I think it was related to when you said that

Language is not just about communication despite that being a common conception.

So potentially it's about the structure of thought or the structure of thinking.

So the question was, it would be great if Elliot could define what is his definition of thought and what is potentially the contribution of the intracranial language research towards answering the hard problem.


SPEAKER_02:
Wow.

So you started with the easy question.

Okay.

Yeah, that's a really good question.

So I like to think of thought as I do any other language.

It's kind of like a metaphor.

So, you know, in some languages, when a human being does a long jump, they only jump.

So in English, we just say they jump.

In Japanese, they actually allow flying.

So if a Japanese linguist is at the Olympics and they see someone do high jump or long jump, they could technically say they're flying.

But it's just a metaphor.

I think it's the same with thought as well.

So thought is just a metaphor.

I don't think we have any... It's not a well-defined scientific natural kind.

It's kind of a useful convenience.

But that said, I think...

The way I see it is natural language syntax allows us to fetch particular domain specific representations from all sorts of different domains and then construct them into novel interpretations.

The important thing is that all of this is outside language.

So the interpretation process is an extra linguistic process.

The only linguistic specific process is just combining a phrase, just combining items, putting them together and then shipping them off to an external interface for interpretation.

So that's the thought process.

I don't have any deep particular reflections on it.

I think the best book about this is Paul Petroski's Conjoining Meanings that came out in 2018, which is the idea that what language contributes to thought is kind of what I said here today, like it uniquely encodes a kind of functional abstraction, whereas different cognitive subdomains like the visual system or the olfactory system, I guess, a sense of

and number sense or geometric reasoning, these all contribute distinct sub-representations of a given sentence, but language seems to uniquely care about function, so abstract function.

So I guess I would say that if linguistic thought can be defined at all, it's almost definitely gonna closely approximate human-specific interests and concerns and needs and things like that, which is kind of surprising, because a lot of people just think of language as kind of, you know, in an unbiased way,

communicating with the conceptual structures, like thought, like language is kind of a thought system.

But I don't really see it that way.

I kind of see it as language does seem to bring with it some unique conceptual contributions.

Namely, it seems to encode these human-specific functions, which is why I said, you know, if you look at water, water cannot simply be defined as H2O.

It's way too simplistic.

The actual meaning of water is way beyond that.

And then with respect to intercranial stuff,

I'm not too sure.

So intracranial research is fantastic at actually examining in real time actual neural responses, right?

So getting really down to integrity.

It depends on the type of electrode that you have, its resolution, its listening radius, how much cortex you can actually sample.

There's also something called the sparse sampling problem in intracranial research where you have very great coverage of specific cortical loci.

But then, of course, each patient will have different electrodes in different parts of the brain.

There's no patient that has electrodes everywhere.

They only have electrodes where their epilepsy is supposed to be targeted.

So for research purposes, that obviously brings limitations.

So I don't think we'll ever be able to have a coherent global whole-grain intrapatient understanding.

What we do is we sumate, we combine across all these different patients and generate a more kind of average brain response, if that makes sense.

So in that sense, it can contribute just the same way any other MEG or extra-cranial EEG can do.

It depends on the paradigm, it depends on...

I think the real crucial point here is conceptual innovation.

So what we really need is we have tons of data from natural language experiments.

We have loads of data, but we don't have all that much conceptual novelty.

So linguists are very good at coming up with very smart paradigms for very well carefully controlled experiments.

But then when it comes to actual novel conceptual frameworks for how these things actually map onto brain processes, I think we need much more of that.

So I think actually inter-cranial research will not be as useful as conceptual theoretical changes.

I think that's much more important, actually.

Yeah.


SPEAKER_01:
well one area of utility that you didn't even go into at all would be machine learning and the long time challenge of natural language parsing and generation and the recent approach has been throw the big neural network at it with the gpt and just large-scale text modeling and then that reminded me of what you said about multi-scale models how we don't overfit the semantics

even when we hear a ton of syllables.

I really, really, really, really, really am hungry.

And then a computer might just spit out a p-value.

It's kind of like oversampling.

But we don't want to oversample semantically, so there's going to be really interesting space for active inference models.

OK, here's the- Absolutely.

Okay, I'll go to the next question.

Can free energy principle and this syntactic theory framework help explain how and why the brain computes inner speech the way that it does and provide the possibility to predict what's about to be computed in the future?

So how do we think about externalizing speech like vocalization versus our inner experience of speech?

Are those structured?

Are those in our voice?

Like what is happening?


SPEAKER_02:
I think that's a really cool question.

But I think there's also a lot of misconceptions about this.

So most people think that external speech is kind of crude, and it has nothing to do with thought.

But internal speech is like some kind of angelic, platonic space, like Plato's cave.

But actually, I think that's the opposite.

I think they're both the same.

So internalized speech is basically

internalized, externalized speech.

When you listen to yourself speaking in a monologue, you're doing it in the format of externalized speech.

You're not doing it in a different format.

It's not as if inner speech has a different structure and a different encoding, and then external speech is something different.

I wake up in the morning, I still think to myself in English.

I think to myself in linear sentences, right?

That's how the thought is externalized.

So this is why I said about the interface between language and conceptual systems.

What's happening in inner speech is that you are externalizing in your own head.

So inner speech is basically a form of externalization.

Not all forms of externalization involve me literally saying things.

You're still externalizing it.

You're just externalizing it to yourself.

So in other words, actual linguistic thought is pre-conscious, it's subconscious.

And that's why we need linguistics departments to kind of figure out what are these subconscious operations that are happening, right?

So we have a bunch of subconscious merge operations that are happening, but we don't have direct conscious access to those.

And why should we?

There's no reason why we should do, right?

But we do have direct access to the externalized output in our kind of self-generated auditory encoding of that structure.

So I can't remember what the question was, sorry, but yeah.


SPEAKER_01:
that reminds me of thinking through other minds, a recent paper in our active inference community.

And then also, yeah, there's just so many interesting aspects about how you really pointed to these domain general attributes of language.


SPEAKER_00:
And


SPEAKER_01:
yes i'm rethinking language and some of the ways that we even communicate on the stream because you'll see people say i'm just not sure how to say it it's like but are you sure how to how to think it oh no yeah because actually if we think through it and then we have feedback writing and we can it's like stigma g we're making these meaning marks and then we're reinterpreting that and so it's going to be quite interesting there here's a third question um

Thanks.

Great talk.

Given that the sequences are generated in nested hierarchical structures, where would linear externalization fit in here?

Can we say that they're bound by linear externalizations?

And then maybe if that's a linguistics term, what does that mean?


SPEAKER_02:
Yeah, it is.

Yeah.


SPEAKER_01:
Okay, great.


SPEAKER_02:
Where do linear externalizations fit into here?

That's a really good question.

I'm not quite sure what the questioner is trying to ask.

I kind of understand what they're saying.

So I guess, okay, okay.

So it's typically assumed that linear externalization is like the output of the syntactic system, and that it turns out upon further analysis that most of the world's languages' differences, the way that the world's languages differ, is based on morphophonological differences.

So in other words,

the way that more themes and sounds are produced.

But the actual semantics and syntax is kind of fairly linear.

So in other words, all the universal things about language are kind of early on, and all the differences in the world's languages are kind of very late on.

So late in the interpretation process.

So when you generate a structure that you want to say, you at least need to have the syntax first, because that's the most uniform structure.

And then once you've got the syntax in place, you then go about filling in all the sound details.

But the sound details are kind of irrelevant for language.

That's just an annoyance, basically.

So I gave you the example of English and Basque.

So English and Basque have the same underlying phrase structure.

but they linearize it in different ways.

So they just happen to linearize it based on whatever arbitrary conventions happen to have arisen in those different cultures.

But the actual interpretation is exactly the same.

So I would say whatever linearization, whenever it happens, happens late stage, and it happens as an inconvenience to the language system.


SPEAKER_01:
interesting and sometimes people will point to different languages or a word that appears unique to even a cultural experience but it's sort of the other side of that coin is the truly 99.9 that's structured functionally like that can be translated and so it is the exception that proves the rule just like when we we violate syntax sometimes to make a point like repeating a word

and then in our internal monologue maybe even like singing or alternate characters it's sort of like it's a dramatic externalization but it's the exception that proves the rule or it's kind of like the grand master chess player who violates a principle but that is is mastery over the syntax and we can't let those exceptions that prove the rule

lead us to throw out the baby with the bathwater or however else they say it but it by highlighting language as functional and the structure of thinking rather than like um rhetorical only in its deployment was just connecting two nodes and you're really opening it up to think about what's inside of the cranium as well and taking measurements from there but it's what's happening with our thought that's being revealed in a linear string


SPEAKER_02:
So one of my favorite examples of modeling the future is magical realism.

And magical realism is basically getting a blender, putting lots of different random concepts in there, and blending them all together and seeing what comes out.

So a lot of literary novelty, literary conventions, novels, poetry, is novel precisely because it's kind of experimental.

It's saying, let's see what happens when we put these two random different concepts together.

Let's see if they can generate a meaningful interpretation that is either emotionally resonant or conceptually intriguing.

And in fact, some of the most important poetry that's been written

in the English language has violated rules of English grammar precisely to generate these novel interpretations.

It's an intentional violation of some kind of linguistic rule, which it's like a signal to the reader that there's a reason for this.

It sends a message, it reflects something or whatever.

But yeah, absolutely, these exceptions to the rule are extremely important to think about.


SPEAKER_01:
my final question is how my digital discourse and multimedia be influencing structure of language structure of thought like are we synchronizing on the functional aspects with memes or are we diverging in our narratives so how is that playing out when it's more multimedia visual video

than ever less of it is spoken and read, which is the linearized sort of classical language.

And now there's unconventional languages.

So where does that, how's that going to work?


SPEAKER_02:
Yeah.

So, so, I mean, I know that the capacity and the prevalence of like, uh, long-term digesting of like, you know,

long Sunday Times articles is declining.

The propensity for bite-sized, chunking information is easily digestible.

It involves less critical thinking.

Judgements, ideological otherwise, can be made rapidly.

Inferences can be made, and so on.

That is definitely on the rise.

However, there was a linguistics book that came out, I believe, around 2010.

I can't remember who it was by.

It was basically just analyzing the idea that, so around the turn of the century, obviously mobile phones came about, right?

And everyone started texting and using all these different, so instead of saying please, you would say, you'd type PLZ.

Instead of saying mate, you'd say M8 or whatever, all these different abbreviations.

And at the time, a lot of people in England were very concerned about this because they said, well,

all our kids are going to be, you know, they're going to grow up stupid because they use all these slang deviations of English.

It turns out that there's almost no evidence that this affects intelligence, like people's ability to, you know, be smart and use language and identify language grammatically.

Because in fact, when you think about it, if you use text language, if you open a WhatsApp chat and you start and you text with somebody and you use all these emojis and you use all these abbreviations for words, it presupposes that you actually know the correct version.

Because in order to violate the rules, it presupposes that you understand them.

So flouting the rules of grammar and flouting the rules of spelling actually exhibits comprehension of genuine rules of English, whatever you want to call them, right?

And that was the example about the linguist gate in this book on texting.

Again, I can't remember it.

So I think that's a pretty good example of when this panic, this moral panic in society over text.

The answer is I don't know.

I have no idea.


SPEAKER_01:
on that sort of moral panic i guess i thought it's like um then at some point maybe even they forget how to spell it p l e a s e you know how to spell please and then it's kind of like our word roots you know oh i can't believe you don't speak greek enough to know that the word roots of this it's like it's become modularized so units that were novelties at first and exceptions

become reified within a cultural context, a shared niche and shared narrative so that like the crying emoji does mean this functionally.

And then someone could say, I can't believe that you don't know that it used to mean this in a different language.

It's sort of like,

it speaks to so many of these excellent themes so yeah yeah elliot awesome guest stream thanks for your first appearance and we're always looking forward to what you might want to share with us in the future so thanks again yeah totally well yeah this is an ongoing project so i'm sure i'll have more to share and thank you very much for having me thank you for the conversation i really enjoyed it and thank you for the questions too so yeah totally thank you elliot and to all audience so see you later
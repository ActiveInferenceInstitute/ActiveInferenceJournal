1
00:00:19,260 --> 00:00:22,716
Daniel Friedman: Hello and welcome. It's Active Inference

2
00:00:22,748 --> 00:00:26,524
guest stream number 51.1 on July 28,

3
00:00:26,572 --> 00:00:30,716
2023. We are here with Tomaso Salvatore

4
00:00:30,828 --> 00:00:33,664
and we will be having a presentation and

5
00:00:33,702 --> 00:00:37,004
a discussion on the recent work Causal

6
00:00:37,052 --> 00:00:39,688
Inference via Predictive Coding. So

7
00:00:39,774 --> 00:00:42,328
thanks so much for joining. For those

8
00:00:42,334 --> 00:00:44,104
who are watching Live, feel free to

9
00:00:44,142 --> 00:00:47,304
write questions in the live chat and off

10
00:00:47,520 --> 00:00:50,399
to you. Thank you.

11
00:00:50,399 --> 00:00:52,980
Tommaso Salvatori: Thank you very much, Daniel, for

12
00:00:52,222 --> 00:00:55,656
inviting me. Always been a big fan

13
00:00:55,688 --> 00:00:57,308
of the channel and I've been watching a

14
00:00:57,314 --> 00:01:00,076
lot of videos, so I'm quite excited to

15
00:01:00,098 --> 00:01:02,444
be here and be the one speaking this

16
00:01:02,482 --> 00:01:06,080
time. So I'm going to talk about this

17
00:01:06,150 --> 00:01:08,576
recent preprint that I put out, which

18
00:01:08,598 --> 00:01:11,136
has been the work of the last couple of

19
00:01:11,158 --> 00:01:14,732
months. And it's a collaboration

20
00:01:14,796 --> 00:01:17,884
with Luca Binketti, Amin,

21
00:01:17,932 --> 00:01:19,708
Mccarrak Berenmille and Thomas

22
00:01:19,724 --> 00:01:22,832
Lukasiavic. And it's basically

23
00:01:22,886 --> 00:01:25,076
a joint work between Verses, which is

24
00:01:25,098 --> 00:01:27,776
the company I work for, the University

25
00:01:27,808 --> 00:01:30,500
of Oxford and Teovien.

26
00:01:31,660 --> 00:01:34,904
So during this

27
00:01:34,942 --> 00:01:35,530
talk,

28
00:01:38,220 --> 00:01:39,976
this is basically the outline of the

29
00:01:39,998 --> 00:01:42,616
talk, I will start talking about what

30
00:01:42,638 --> 00:01:46,108
predictive coding is and give an

31
00:01:46,114 --> 00:01:48,856
introduction of what it is, a brief

32
00:01:48,888 --> 00:01:51,804
historical introduction, why I think

33
00:01:52,002 --> 00:01:54,716
it's important to study predictive

34
00:01:54,748 --> 00:01:56,528
coding, even, for example, from the

35
00:01:56,534 --> 00:01:59,024
machine learning perspective. I will

36
00:01:59,062 --> 00:02:02,624
then provide a small intro to what

37
00:02:02,662 --> 00:02:06,444
causal inference is. And once

38
00:02:06,502 --> 00:02:08,148
we have all those informations together,

39
00:02:08,234 --> 00:02:11,924
I will then discuss why I wrote this

40
00:02:11,962 --> 00:02:13,780
paper, what was basically the research

41
00:02:13,850 --> 00:02:16,564
question that inspired me and the other

42
00:02:16,602 --> 00:02:20,008
collaborators and present

43
00:02:20,094 --> 00:02:23,976
the main results, which are how

44
00:02:23,998 --> 00:02:27,384
to perform inference so intervention and

45
00:02:27,422 --> 00:02:30,808
counterfactual inference, and how to

46
00:02:30,974 --> 00:02:34,248
learn the causal structures from a

47
00:02:34,254 --> 00:02:36,520
given data set using predictive coding.

48
00:02:36,680 --> 00:02:38,524
And then I will of course conclude with

49
00:02:38,562 --> 00:02:41,916
some small summary and some

50
00:02:42,018 --> 00:02:45,040
discussion on why I believe this work

51
00:02:45,110 --> 00:02:47,520
can be impactful and some future

52
00:02:47,590 --> 00:02:48,400
directions.

53
00:02:50,660 --> 00:02:53,692
So what is predative coding? predative

54
00:02:53,756 --> 00:02:56,348
coding is in general famous for being a

55
00:02:56,454 --> 00:02:58,704
neuroscience inspired learning method.

56
00:02:58,752 --> 00:03:01,204
So a theory of how information

57
00:03:01,322 --> 00:03:05,270
processing in the brain works and

58
00:03:05,880 --> 00:03:08,276
brain, formally speaking, the theory of

59
00:03:08,298 --> 00:03:10,760
creative coding can be described as

60
00:03:10,910 --> 00:03:12,660
basically having a hierarchical

61
00:03:12,740 --> 00:03:14,680
structure of neurons in the brain.

62
00:03:17,020 --> 00:03:19,048
And you have two different families of

63
00:03:19,054 --> 00:03:22,124
neurons in the brain. The first family

64
00:03:22,322 --> 00:03:24,024
is the one in charging of sending

65
00:03:24,072 --> 00:03:27,244
prediction information. So neurons in a

66
00:03:27,282 --> 00:03:29,052
specific level of the hierarchy send

67
00:03:29,106 --> 00:03:33,020
information and predict the activity

68
00:03:33,600 --> 00:03:36,832
of the level below. And the second

69
00:03:36,886 --> 00:03:38,636
family of neuron is that of error

70
00:03:38,668 --> 00:03:41,184
neurons. And the error neurons, they

71
00:03:41,222 --> 00:03:43,936
send prediction error information up the

72
00:03:43,958 --> 00:03:46,896
hierarchy. So one level predicts the

73
00:03:46,918 --> 00:03:48,770
activity of the level below.

74
00:03:50,760 --> 00:03:53,332
This prediction has some mismatch which

75
00:03:53,466 --> 00:03:55,124
was actually going on in the level

76
00:03:55,162 --> 00:03:57,428
below, and information about the

77
00:03:57,434 --> 00:04:00,296
prediction error gets sent up the

78
00:04:00,318 --> 00:04:02,888
hierarchy. However,

79
00:04:02,974 --> 00:04:05,784
predictive coding was actually not

80
00:04:05,822 --> 00:04:08,520
burned as a neuroscience,

81
00:04:09,340 --> 00:04:12,228
as a theory from the neurosciences, but

82
00:04:12,254 --> 00:04:14,172
it was actually initially developed as a

83
00:04:14,226 --> 00:04:15,996
method for signal processing and

84
00:04:16,018 --> 00:04:19,516
compression back in the 50s. So the work

85
00:04:19,538 --> 00:04:21,916
of Oliver Elias, which are actually

86
00:04:22,018 --> 00:04:25,740
contemporary of Shannon

87
00:04:26,180 --> 00:04:27,936
they realized that once we have a

88
00:04:27,958 --> 00:04:31,504
predictor, a model that works that is

89
00:04:31,622 --> 00:04:34,156
well in predicting data, sending

90
00:04:34,188 --> 00:04:36,144
messages about the error in those

91
00:04:36,182 --> 00:04:38,256
predictions is actually much cheaper

92
00:04:38,288 --> 00:04:41,492
than sending the entire message every

93
00:04:41,546 --> 00:04:44,736
time. And this is how predictive coding

94
00:04:44,768 --> 00:04:48,816
was born. So as a signal processing

95
00:04:48,848 --> 00:04:50,536
and compression mechanism in information

96
00:04:50,638 --> 00:04:54,136
theory back in the 50s, it was

97
00:04:54,158 --> 00:04:58,344
actually in the 80s that

98
00:04:58,382 --> 00:05:02,236
exactly the same model was used in

99
00:05:02,258 --> 00:05:06,044
neuroscience with

100
00:05:06,082 --> 00:05:07,932
the work from Mumford or other works

101
00:05:07,986 --> 00:05:10,956
that, for example, explain how the

102
00:05:10,978 --> 00:05:13,244
Retina process information. So we get

103
00:05:13,282 --> 00:05:14,720
prediction signals from the outside

104
00:05:14,790 --> 00:05:17,504
world and we need to compress this

105
00:05:17,542 --> 00:05:20,556
representation and have this internal

106
00:05:20,588 --> 00:05:23,136
representation in our neurons. And the

107
00:05:23,158 --> 00:05:25,024
method is very similar, if not

108
00:05:25,062 --> 00:05:28,592
equivalent to the one that was developed

109
00:05:28,736 --> 00:05:31,270
by Elias and Oliver in the 50s.

110
00:05:33,000 --> 00:05:35,920
Maybe what's the biggest paradigm shift

111
00:05:36,080 --> 00:05:39,592
happened in 99 thanks to the work of

112
00:05:39,646 --> 00:05:44,296
Rao and Ballard in which they

113
00:05:44,318 --> 00:05:45,752
introduced this concept that I mentioned

114
00:05:45,806 --> 00:05:47,768
earlier about hierarchical structures in

115
00:05:47,774 --> 00:05:50,888
the brain where prediction information

116
00:05:50,974 --> 00:05:54,156
is top down and error information is

117
00:05:54,178 --> 00:05:57,164
bottom up. And something that they did

118
00:05:57,202 --> 00:06:00,670
that wasn't done before is that they

119
00:06:01,040 --> 00:06:03,420
explain and develop this theory about

120
00:06:03,490 --> 00:06:06,704
not only inference, but also about how

121
00:06:06,742 --> 00:06:08,876
learning works in the brain. So it's

122
00:06:08,908 --> 00:06:10,896
also a theory of how our synapses get

123
00:06:10,918 --> 00:06:14,864
updated. And the last big

124
00:06:14,982 --> 00:06:16,964
breakthrough that I'm going to talk

125
00:06:17,002 --> 00:06:18,352
about in this brief historical

126
00:06:18,416 --> 00:06:21,460
introduction is from 2003.

127
00:06:21,530 --> 00:06:25,284
But then he kept going in the

128
00:06:25,322 --> 00:06:28,324
years after, thanks to Karl Friston, in

129
00:06:28,362 --> 00:06:31,856
which basically he took the theory

130
00:06:31,888 --> 00:06:35,464
of Rowan Ballard and he developed, he

131
00:06:35,502 --> 00:06:38,456
extended it and generalized it to the

132
00:06:38,478 --> 00:06:40,776
theory of generative models. So

133
00:06:40,798 --> 00:06:42,948
basically the main claim that Karl

134
00:06:42,964 --> 00:06:45,136
Frieston did is that predictive coding

135
00:06:45,188 --> 00:06:48,876
is an evidence maximization scheme of a

136
00:06:48,898 --> 00:06:50,524
specific kind of generative model,

137
00:06:50,642 --> 00:06:53,404
which I'm going to introduce later as

138
00:06:53,442 --> 00:06:57,536
well. So to make a

139
00:06:57,558 --> 00:07:02,716
brief summary, in the first two kinds

140
00:07:02,748 --> 00:07:04,636
of predica described so signal

141
00:07:04,668 --> 00:07:06,444
processing and compression and

142
00:07:06,582 --> 00:07:08,804
information processing in the Retina and

143
00:07:08,842 --> 00:07:10,564
in the brain in general, they are

144
00:07:10,602 --> 00:07:14,340
inference methods. And the biggest

145
00:07:15,160 --> 00:07:17,764
change, the biggest revolution that we

146
00:07:17,802 --> 00:07:20,964
had in 1999, so let's say in the 21st

147
00:07:21,002 --> 00:07:23,416
century, is apparative coding was seen

148
00:07:23,438 --> 00:07:25,880
as a learning algorithm. So we can first

149
00:07:25,950 --> 00:07:29,496
compress information and then update all

150
00:07:29,518 --> 00:07:31,728
the synapses or all the latent variables

151
00:07:31,764 --> 00:07:34,236
that we have in our generative model to

152
00:07:34,418 --> 00:07:36,750
improve our generative model itself.

153
00:07:38,880 --> 00:07:43,016
So let's give some definitions

154
00:07:43,048 --> 00:07:45,372
that are a little bit more formal. So

155
00:07:45,426 --> 00:07:48,368
prior coding can be seen as a

156
00:07:48,454 --> 00:07:50,432
hierarchical Gaussian generative model.

157
00:07:50,566 --> 00:07:53,376
So here is a very simple figure in which

158
00:07:53,398 --> 00:07:54,956
we have this hierarchical structure

159
00:07:55,068 --> 00:07:57,750
which can be as deep as we want.

160
00:07:58,360 --> 00:08:01,844
And prediction signals go

161
00:08:01,882 --> 00:08:04,436
from one latent variable XN to the

162
00:08:04,458 --> 00:08:06,592
following one and it gets transformed

163
00:08:06,656 --> 00:08:10,024
every time via function GN or

164
00:08:10,062 --> 00:08:10,650
GI.

165
00:08:15,670 --> 00:08:17,762
This is a generative model, as I said.

166
00:08:17,816 --> 00:08:19,426
And what's the marginal probability of

167
00:08:19,448 --> 00:08:21,486
this generative model? Well, it's simply

168
00:08:21,518 --> 00:08:25,286
the probability of the last can

169
00:08:25,308 --> 00:08:27,702
you see my cursor? Yes. Right? Yes.

170
00:08:27,756 --> 00:08:30,086
Perfect. So it's the generative model of

171
00:08:30,108 --> 00:08:32,722
the last vertex is the probability

172
00:08:32,786 --> 00:08:34,998
distribution of the last vertex times

173
00:08:35,084 --> 00:08:37,034
the probability distribution of every

174
00:08:37,072 --> 00:08:40,422
other vertex conditioned on the activity

175
00:08:40,486 --> 00:08:43,238
of the vertex before or on the latent

176
00:08:43,254 --> 00:08:44,300
variable before.

177
00:08:46,430 --> 00:08:48,018
I earlier said that it's a Gaussian

178
00:08:48,054 --> 00:08:50,254
generative model, which means that those

179
00:08:50,292 --> 00:08:52,560
probabilities, they are in Gaussian form

180
00:08:54,770 --> 00:08:59,070
and every and those function g

181
00:08:59,220 --> 00:09:02,398
in general and especially since, for

182
00:09:02,404 --> 00:09:04,706
example, in Rambalar paper and in all

183
00:09:04,728 --> 00:09:06,802
the papers that came afterwards. Also

184
00:09:06,856 --> 00:09:09,090
because of the deep learning revolution,

185
00:09:09,430 --> 00:09:11,806
those functions are simply linear maps

186
00:09:11,918 --> 00:09:15,122
or nonlinear maps with activation

187
00:09:15,186 --> 00:09:17,974
functions or nonlinear maps with

188
00:09:18,012 --> 00:09:19,522
activation function and an additive

189
00:09:19,586 --> 00:09:20,310
bias.

190
00:09:23,370 --> 00:09:26,454
So we can give an

191
00:09:26,492 --> 00:09:28,274
informal definition of creative coding

192
00:09:28,322 --> 00:09:29,738
and we can say predictive coding is an

193
00:09:29,744 --> 00:09:32,262
inversion scheme for such a generative

194
00:09:32,326 --> 00:09:34,426
model where its model evidence is

195
00:09:34,448 --> 00:09:38,186
maximized by minimizing a quantity that

196
00:09:38,208 --> 00:09:40,000
is called a variation of free energy.

197
00:09:40,930 --> 00:09:43,674
In general, the goal of every generative

198
00:09:43,722 --> 00:09:45,674
model is to maximize model evidence,

199
00:09:45,722 --> 00:09:47,950
but this quantity is always intractable.

200
00:09:49,170 --> 00:09:52,322
And we have some techniques that allow

201
00:09:52,376 --> 00:09:55,586
us to approximate this solution and the

202
00:09:55,608 --> 00:09:58,594
one that we use in pritef coding instead

203
00:09:58,632 --> 00:10:01,300
of minimizing aberration of free energy,

204
00:10:02,070 --> 00:10:04,070
which is a lower bound of the model

205
00:10:04,140 --> 00:10:08,966
evidence in this work and actually in

206
00:10:08,988 --> 00:10:10,550
a lot of other ones. So it's the

207
00:10:10,620 --> 00:10:12,374
standard way of doing it. This

208
00:10:12,412 --> 00:10:13,986
minimization is performed via gradient

209
00:10:14,018 --> 00:10:17,382
descent and yes,

210
00:10:17,436 --> 00:10:18,746
performed via gradient descent. And

211
00:10:18,768 --> 00:10:20,442
there are actually other methods such as

212
00:10:20,496 --> 00:10:22,410
expectation maximization which is often

213
00:10:22,480 --> 00:10:24,874
equivalent. Or you can use some other

214
00:10:24,912 --> 00:10:26,506
message passing algorithms such as

215
00:10:26,528 --> 00:10:28,220
belief propagation, for example,

216
00:10:30,850 --> 00:10:33,326
and going a little bit back in time.

217
00:10:33,428 --> 00:10:35,822
So forgetting a little bit about the

218
00:10:35,876 --> 00:10:39,374
statistical generative models, we can

219
00:10:39,412 --> 00:10:41,090
see pref coding,

220
00:10:42,790 --> 00:10:44,578
I said already a couple of times as a

221
00:10:44,584 --> 00:10:47,374
hierarchical model with neural

222
00:10:47,422 --> 00:10:49,758
activities. So with neurons, latent

223
00:10:49,774 --> 00:10:51,006
variables that represent neural

224
00:10:51,038 --> 00:10:53,606
activities, the sender signal down the

225
00:10:53,628 --> 00:10:57,106
hierarchy and with error nodes or error

226
00:10:57,138 --> 00:10:59,894
neurons, the sender signal up the

227
00:10:59,932 --> 00:11:01,666
hierarchy so they send the error

228
00:11:01,698 --> 00:11:04,546
information back. What's the variation

229
00:11:04,578 --> 00:11:06,854
of free energy of these class operative

230
00:11:06,902 --> 00:11:10,186
coding models? It's simply the sum of

231
00:11:10,208 --> 00:11:12,854
the mean square error of all the error

232
00:11:12,902 --> 00:11:18,426
neurons, so is the sum of

233
00:11:18,448 --> 00:11:20,270
the total error squared.

234
00:11:22,130 --> 00:11:24,366
And this representation is going to be

235
00:11:24,388 --> 00:11:27,406
useful in the later slides and in how

236
00:11:27,428 --> 00:11:29,018
I'm going to explain how to use pref

237
00:11:29,034 --> 00:11:30,606
coding to model causal inference. For

238
00:11:30,628 --> 00:11:34,206
example, why do you think pre tip coding

239
00:11:34,238 --> 00:11:36,146
is important and is a nice algorithm to

240
00:11:36,168 --> 00:11:38,914
study? Well, first of all, as I said

241
00:11:38,952 --> 00:11:40,482
earlier, it optimizes the correct

242
00:11:40,536 --> 00:11:42,354
objective which is the model evidence or

243
00:11:42,392 --> 00:11:45,954
marginal likelihood. And then

244
00:11:45,992 --> 00:11:48,142
it does so by optimizing a lower bound

245
00:11:48,206 --> 00:11:49,414
which is called the rational free

246
00:11:49,452 --> 00:11:52,486
energy, as I said. And the rational free

247
00:11:52,508 --> 00:11:54,278
energy is interesting because it can be

248
00:11:54,364 --> 00:11:56,700
written as a sum of two different terms

249
00:11:58,030 --> 00:12:00,438
which are and each of those term

250
00:12:00,614 --> 00:12:04,566
optimizing. It has important impacts,

251
00:12:04,598 --> 00:12:06,774
for example, in machine learning tasks

252
00:12:06,822 --> 00:12:09,726
or in general in learning tasks. So one

253
00:12:09,748 --> 00:12:13,070
of those term forces memorization. So

254
00:12:13,220 --> 00:12:16,478
the second term basically forces the

255
00:12:16,484 --> 00:12:20,046
model to fit a specific data set and the

256
00:12:20,068 --> 00:12:23,214
first term forces the model to minimize

257
00:12:23,262 --> 00:12:25,746
the complexity. And as we know, for

258
00:12:25,768 --> 00:12:29,090
example from the Occam's razor theory,

259
00:12:29,430 --> 00:12:31,394
if we have two different models that

260
00:12:31,432 --> 00:12:33,250
perform similarly on a specific training

261
00:12:33,320 --> 00:12:35,878
set, the one that we have to get and the

262
00:12:35,884 --> 00:12:38,646
one that is expected to generalize the

263
00:12:38,668 --> 00:12:41,734
most is the less complex one. So

264
00:12:41,852 --> 00:12:44,034
updating generative model via

265
00:12:44,082 --> 00:12:46,522
vibrational free energy allows us to

266
00:12:46,656 --> 00:12:50,886
basically converge to the optimal Occam

267
00:12:50,918 --> 00:12:54,618
razor model which both memorizes a

268
00:12:54,624 --> 00:12:56,662
data set but is also able to generalize

269
00:12:56,726 --> 00:12:59,420
very well on unsigned data points.

270
00:13:00,670 --> 00:13:02,458
A second reason of why prioritic coding

271
00:13:02,474 --> 00:13:09,674
is important is that it actually doesn't

272
00:13:09,722 --> 00:13:12,174
have to be defined on a hierarchical

273
00:13:12,222 --> 00:13:14,914
structure, but it can be modeled on more

274
00:13:14,952 --> 00:13:17,186
complex and flexible architectures such

275
00:13:17,208 --> 00:13:19,330
as directed graphical model with any

276
00:13:19,480 --> 00:13:22,194
shape or generalized even more to

277
00:13:22,232 --> 00:13:23,758
networks with a lot of cycles that

278
00:13:23,784 --> 00:13:26,358
resemble brain region. And the

279
00:13:26,364 --> 00:13:29,094
underlying reason is that you're not

280
00:13:29,132 --> 00:13:31,462
learning and predicting with a forward

281
00:13:31,516 --> 00:13:33,538
pass and then back propagating the error

282
00:13:33,634 --> 00:13:35,800
but you're minimizing an energy function

283
00:13:36,510 --> 00:13:38,346
and this allows basically every kind of

284
00:13:38,368 --> 00:13:42,086
hierarchy to be allows

285
00:13:42,118 --> 00:13:45,034
to go behind hierarchies and allow to

286
00:13:45,232 --> 00:13:47,146
learn cycles. And this is actually quite

287
00:13:47,168 --> 00:13:48,686
important because the brain is full of

288
00:13:48,708 --> 00:13:51,534
cycles as we have some information from

289
00:13:51,572 --> 00:13:55,258
some recent papers that managed

290
00:13:55,274 --> 00:13:58,094
to map completely the brain of some

291
00:13:58,132 --> 00:14:01,426
animals such as fruit fly. The brain is

292
00:14:01,448 --> 00:14:05,106
full of cycles so it makes sense to

293
00:14:05,288 --> 00:14:09,394
train our machine learning models or our

294
00:14:09,432 --> 00:14:11,266
models in general with an algorithm that

295
00:14:11,288 --> 00:14:14,734
allows us to train using cyclic

296
00:14:14,782 --> 00:14:18,482
structures. The third reason why

297
00:14:18,536 --> 00:14:20,486
prior coding is interesting is that it

298
00:14:20,508 --> 00:14:23,094
has been formally proven that it is more

299
00:14:23,132 --> 00:14:24,658
robust than standard neural networks

300
00:14:24,674 --> 00:14:26,698
during with back propagation. So if you

301
00:14:26,704 --> 00:14:28,026
have a neural network and you want to

302
00:14:28,048 --> 00:14:29,690
perform classification tasks,

303
00:14:31,470 --> 00:14:33,370
creative coding is more robust.

304
00:14:34,110 --> 00:14:37,606
And this is interesting in tasks

305
00:14:37,638 --> 00:14:39,566
such as online learning, training of

306
00:14:39,588 --> 00:14:41,662
small data sets or continuous learning

307
00:14:41,716 --> 00:14:44,782
tasks. And the theory basically comes

308
00:14:44,836 --> 00:14:46,686
from the fact that imperative coding has

309
00:14:46,708 --> 00:14:49,546
been proved to approximate implicit

310
00:14:49,578 --> 00:14:51,922
gradient descent, which is a different

311
00:14:51,976 --> 00:14:54,382
version of the explicit gradient descent

312
00:14:54,446 --> 00:14:55,854
which is the standard gradient descent

313
00:14:55,902 --> 00:14:58,980
used in every single model basically.

314
00:14:59,670 --> 00:15:01,890
And it's a variation that is more robust

315
00:15:05,850 --> 00:15:08,054
I think. Okay, I did quite a long intro

316
00:15:08,092 --> 00:15:09,466
to predictive coding, I think. I'm now

317
00:15:09,488 --> 00:15:11,226
moving to the second topic which is

318
00:15:11,248 --> 00:15:14,726
causal inference. And what's

319
00:15:14,758 --> 00:15:18,058
causal inference? Causal inference is a

320
00:15:18,064 --> 00:15:19,510
very general theory that has been

321
00:15:19,520 --> 00:15:22,426
formalized the most by Judea Pearl.

322
00:15:22,458 --> 00:15:24,446
He's definitely the most important

323
00:15:24,548 --> 00:15:26,714
person in the field of causal inference.

324
00:15:26,762 --> 00:15:28,846
He wrote some very nice books. For

325
00:15:28,868 --> 00:15:30,902
example, the Book of Why is highly

326
00:15:30,986 --> 00:15:33,026
recommended if you want to learn more

327
00:15:33,048 --> 00:15:36,882
about this topic and it basically

328
00:15:36,936 --> 00:15:39,086
tackles the following problem so let's

329
00:15:39,118 --> 00:15:40,670
assume we have a joint probability

330
00:15:40,750 --> 00:15:42,590
distribution which is associated with a

331
00:15:42,600 --> 00:15:44,726
Bayesian network. This is going to be a

332
00:15:44,748 --> 00:15:47,398
little bit the running example through

333
00:15:47,564 --> 00:15:51,142
all the paper, especially with

334
00:15:51,276 --> 00:15:53,510
Bayesian networks of this shape.

335
00:15:54,910 --> 00:15:58,166
Those Bayesian networks, the variables

336
00:15:58,198 --> 00:16:00,442
inside, they can represent different

337
00:16:00,496 --> 00:16:02,326
quantities. So for example, a Bayesian

338
00:16:02,358 --> 00:16:05,450
network with this shape can represent

339
00:16:06,830 --> 00:16:08,414
the quantities on the right. So,

340
00:16:08,452 --> 00:16:11,150
socioeconomical statue of an individual,

341
00:16:11,300 --> 00:16:13,578
its education level, its intelligence

342
00:16:13,674 --> 00:16:15,120
and its income level,

343
00:16:17,090 --> 00:16:19,294
something the classical statistics is

344
00:16:19,332 --> 00:16:22,750
very good at and its wild,

345
00:16:23,190 --> 00:16:25,362
most used application is to model

346
00:16:25,416 --> 00:16:27,826
observations or correlations. A

347
00:16:27,848 --> 00:16:29,186
correlation basically answer the

348
00:16:29,208 --> 00:16:33,400
question what is D if we observe another

349
00:16:34,010 --> 00:16:36,694
variable C? So for example, in this

350
00:16:36,732 --> 00:16:39,126
case, what's the income level, the

351
00:16:39,148 --> 00:16:41,078
expected income level of an individual

352
00:16:41,244 --> 00:16:44,534
if I observe his education level and

353
00:16:44,572 --> 00:16:48,506
of course, if that person has

354
00:16:48,528 --> 00:16:50,138
a higher degree of education, for

355
00:16:50,144 --> 00:16:52,454
example a master or PhD. I'm expecting

356
00:16:52,502 --> 00:16:54,202
general that person to have a higher

357
00:16:54,256 --> 00:16:57,610
income level and this is a correlation.

358
00:16:58,190 --> 00:17:00,302
However, sometimes there are things that

359
00:17:00,436 --> 00:17:03,134
are very hard to observe, but they play

360
00:17:03,172 --> 00:17:05,006
a huge role in determining those

361
00:17:05,028 --> 00:17:07,502
quantities. So for example, it could be

362
00:17:07,556 --> 00:17:10,660
that the income level is much, much more

363
00:17:11,350 --> 00:17:13,266
defined by the intelligence of a

364
00:17:13,288 --> 00:17:16,754
specific person and maybe

365
00:17:16,792 --> 00:17:19,266
that the intelligence. So if a person is

366
00:17:19,288 --> 00:17:21,586
intelligent, he's also most likely to

367
00:17:21,608 --> 00:17:25,846
have a higher education level. But still

368
00:17:26,028 --> 00:17:28,838
the real reason why the income is high

369
00:17:28,924 --> 00:17:30,870
is because of the IQ.

370
00:17:32,250 --> 00:17:35,298
And this cannot be studied by Simplico

371
00:17:35,314 --> 00:17:38,058
relations and has to be studied by a

372
00:17:38,064 --> 00:17:39,674
more advanced technique which is called

373
00:17:39,712 --> 00:17:42,182
an intervention. An intervention

374
00:17:42,246 --> 00:17:44,234
basically answers the question is what

375
00:17:44,272 --> 00:17:47,580
is D if we change C to a specific value?

376
00:17:48,130 --> 00:17:50,846
So for example, we can take an

377
00:17:50,868 --> 00:17:53,520
individual and check his income level

378
00:17:54,610 --> 00:17:57,022
and then change its education level. So

379
00:17:57,076 --> 00:18:00,046
intervene on this word and change his

380
00:18:00,068 --> 00:18:01,658
education level without touching his

381
00:18:01,684 --> 00:18:05,934
intelligence and see how much its income

382
00:18:06,062 --> 00:18:08,542
changes. For example, if the income

383
00:18:08,606 --> 00:18:11,586
changes a lot it means that the

384
00:18:11,608 --> 00:18:13,650
intelligence doesn't play a big role in

385
00:18:13,720 --> 00:18:16,054
this, but the education level does. If

386
00:18:16,092 --> 00:18:18,022
the income level doesn't change much,

387
00:18:18,156 --> 00:18:19,762
it means that maybe there's a hidden

388
00:18:19,826 --> 00:18:21,490
variable in this case the intelligence

389
00:18:21,570 --> 00:18:23,526
that determines the income level of a

390
00:18:23,548 --> 00:18:27,002
person. The third

391
00:18:27,136 --> 00:18:28,966
quantity important in causal inference

392
00:18:28,998 --> 00:18:31,066
is that of counterfactuals. So for

393
00:18:31,088 --> 00:18:32,986
example, a counterfactual answers the

394
00:18:33,008 --> 00:18:36,346
question what would D be had we

395
00:18:36,368 --> 00:18:38,266
changed C to a different value in the

396
00:18:38,288 --> 00:18:40,446
past? So for example, we can see that

397
00:18:40,468 --> 00:18:41,966
the difference between interventions and

398
00:18:41,988 --> 00:18:44,430
counterfactuals is that interventions

399
00:18:45,090 --> 00:18:47,758
act in the future. So I'm intervening in

400
00:18:47,764 --> 00:18:50,206
the world now to observe a change in the

401
00:18:50,228 --> 00:18:53,138
future while counterfactuals allow us to

402
00:18:53,144 --> 00:18:56,014
go back in time and change a variable

403
00:18:56,062 --> 00:18:59,138
back in time and see how that change

404
00:18:59,224 --> 00:19:01,186
would have influenced the world we live

405
00:19:01,208 --> 00:19:04,606
in now. And those are defined

406
00:19:04,638 --> 00:19:06,806
by Judeoperl as the three levels of

407
00:19:06,828 --> 00:19:09,206
causal inference. Correlation is the

408
00:19:09,228 --> 00:19:11,014
first level, intervention is the second

409
00:19:11,052 --> 00:19:12,454
level and counterfactual is the third

410
00:19:12,492 --> 00:19:13,080
level.

411
00:19:16,330 --> 00:19:17,926
What are interventions? I'm going to

412
00:19:17,948 --> 00:19:20,458
define them more formally now that I

413
00:19:20,464 --> 00:19:23,766
gave an intuitive definition and I'm

414
00:19:23,798 --> 00:19:25,386
using this notation here, which is the

415
00:19:25,408 --> 00:19:27,146
same actually throughout all the

416
00:19:27,168 --> 00:19:29,510
presentation. So x is always going to be

417
00:19:29,520 --> 00:19:32,718
a latent variable, si is always going to

418
00:19:32,724 --> 00:19:35,742
be a data point or an observation, and

419
00:19:35,796 --> 00:19:38,574
VI is always going to be a vertex. So

420
00:19:38,692 --> 00:19:40,898
every time you see VI, we are only

421
00:19:40,984 --> 00:19:42,466
interested in the structure of the

422
00:19:42,488 --> 00:19:46,098
graph, for example. So let's assume we

423
00:19:46,104 --> 00:19:47,554
have a Bayesian model which has the same

424
00:19:47,592 --> 00:19:51,730
structure as the Bayesian model

425
00:19:51,880 --> 00:19:55,494
we saw in the previous slide. Given that

426
00:19:55,532 --> 00:19:57,686
x three is equal to s three, this is the

427
00:19:57,708 --> 00:20:00,614
observation we make statistics allows us

428
00:20:00,652 --> 00:20:03,206
to compute the probability or the

429
00:20:03,228 --> 00:20:06,538
expectation of x four, which is the

430
00:20:06,544 --> 00:20:08,970
latent variable related to this vertex.

431
00:20:09,390 --> 00:20:12,460
Given that x three is equal to s three,

432
00:20:15,200 --> 00:20:17,244
to perform an intervention we need a new

433
00:20:17,282 --> 00:20:19,840
kind of notation which is called the do

434
00:20:19,910 --> 00:20:23,170
operation. So in this case,

435
00:20:23,940 --> 00:20:25,536
x four, we want to compute the

436
00:20:25,558 --> 00:20:27,904
probability of x four, given the fact

437
00:20:27,942 --> 00:20:31,152
that we intervene in the word and change

438
00:20:31,206 --> 00:20:34,416
x three to x three. And how do we do

439
00:20:34,438 --> 00:20:37,164
this to perform an intervention? Judy

440
00:20:37,212 --> 00:20:40,596
Perl tells us that we have to have an

441
00:20:40,618 --> 00:20:42,516
intermediate step before computing a

442
00:20:42,538 --> 00:20:45,610
correlation is that first we have to

443
00:20:45,980 --> 00:20:48,136
remove all the incoming edges to v

444
00:20:48,158 --> 00:20:51,976
three. So we have to study not

445
00:20:51,998 --> 00:20:53,592
this Bayesian network, but the second

446
00:20:53,646 --> 00:20:57,596
one. And then at this point we

447
00:20:57,618 --> 00:21:00,556
are allowed to compute a correlation as

448
00:21:00,578 --> 00:21:03,836
we normally do. And this is

449
00:21:03,858 --> 00:21:08,056
an intervention. A counterfactual

450
00:21:08,088 --> 00:21:09,968
is a generalization of this that, as I

451
00:21:09,974 --> 00:21:12,828
said, lived in the past, and they're

452
00:21:12,844 --> 00:21:14,428
computing using structural causal

453
00:21:14,444 --> 00:21:17,184
models. A structural causal model is a

454
00:21:17,222 --> 00:21:20,976
Tuplet which is conceptually similar to

455
00:21:20,998 --> 00:21:23,216
a Bayesian network. But basically we

456
00:21:23,238 --> 00:21:25,732
have this new class of variables on top,

457
00:21:25,866 --> 00:21:27,904
which are the unobservable variables

458
00:21:27,952 --> 00:21:30,304
they use. So we have the Bayesian

459
00:21:30,352 --> 00:21:31,956
network that we had before, x one, x

460
00:21:31,978 --> 00:21:34,936
two, x three, x four. But we also have

461
00:21:34,958 --> 00:21:38,468
those unobservable or variables

462
00:21:38,564 --> 00:21:40,568
that depend on the environment. You

463
00:21:40,574 --> 00:21:43,050
cannot control them, you can infer them,

464
00:21:43,980 --> 00:21:45,530
but they are there.

465
00:21:46,960 --> 00:21:50,444
And F is a set of functions that

466
00:21:50,482 --> 00:21:54,190
depends on all the basically,

467
00:21:54,640 --> 00:21:57,324
F of x of x three depends on x one

468
00:21:57,362 --> 00:21:58,744
because you have an arrow, on x two

469
00:21:58,802 --> 00:22:00,976
because you have an arrow, and on the

470
00:22:01,078 --> 00:22:02,784
unobservable variable that also

471
00:22:02,822 --> 00:22:04,130
influences x three.

472
00:22:06,180 --> 00:22:09,376
So yes, intuitively you can

473
00:22:09,398 --> 00:22:12,468
think of a structural causal model as a

474
00:22:12,474 --> 00:22:14,784
Bayesian network with those unobservable

475
00:22:14,832 --> 00:22:17,600
variables on top, and each unobservable

476
00:22:17,760 --> 00:22:22,324
variable only influences its

477
00:22:22,362 --> 00:22:24,532
own latent variable x. So for example,

478
00:22:24,586 --> 00:22:27,304
IU will never touch x one as well, u

479
00:22:27,342 --> 00:22:29,464
three will only touch U three, u one

480
00:22:29,502 --> 00:22:31,396
will only influence x one, and so forth

481
00:22:31,428 --> 00:22:32,330
and so on.

482
00:22:34,950 --> 00:22:37,250
So performing counterfactual inference

483
00:22:37,690 --> 00:22:39,686
answers the following question so what

484
00:22:39,708 --> 00:22:41,906
would x four be at x three being equal

485
00:22:41,938 --> 00:22:44,998
to another variable in a past situation

486
00:22:45,084 --> 00:22:49,014
u. And computing this

487
00:22:49,052 --> 00:22:50,682
counterfactual requires three different

488
00:22:50,736 --> 00:22:55,194
steps. So abduction is

489
00:22:55,232 --> 00:22:57,126
the computation of all the background

490
00:22:57,158 --> 00:22:59,786
variables. So in this step we want to go

491
00:22:59,808 --> 00:23:01,866
back in time and understand how the

492
00:23:01,888 --> 00:23:03,354
environment, the unobservable

493
00:23:03,402 --> 00:23:06,062
environment was in that specific moment

494
00:23:06,116 --> 00:23:09,642
in time. And we do this by fixing

495
00:23:09,706 --> 00:23:12,894
all the latent variables x to some

496
00:23:12,932 --> 00:23:17,282
specific data that we already have and

497
00:23:17,336 --> 00:23:20,180
performing this inference on the use.

498
00:23:21,190 --> 00:23:24,754
Then we're going to use the U to keep

499
00:23:24,792 --> 00:23:27,122
the U that we have learned and perform

500
00:23:27,176 --> 00:23:30,866
an intervention. So a counterfactual

501
00:23:30,898 --> 00:23:33,462
can also be seen as an intervention back

502
00:23:33,516 --> 00:23:36,994
in time in which we know the environment

503
00:23:37,042 --> 00:23:40,058
variables u one, U two and U four in

504
00:23:40,064 --> 00:23:41,260
that specific moment.

505
00:23:43,150 --> 00:23:45,820
And what's the missing step?

506
00:23:46,670 --> 00:23:48,874
So what would x four be at x three being

507
00:23:48,912 --> 00:23:52,686
equal to another data point in

508
00:23:52,708 --> 00:23:55,486
that specific situation? Now we can

509
00:23:55,508 --> 00:23:57,566
compute a correlation and the

510
00:23:57,588 --> 00:24:00,986
correlation we do it on the graph

511
00:24:01,018 --> 00:24:02,606
in which we have already performed an

512
00:24:02,628 --> 00:24:05,534
intervention using the environment

513
00:24:05,582 --> 00:24:08,834
variables that we have learned in the

514
00:24:08,872 --> 00:24:11,074
abduction step. And this is a

515
00:24:11,192 --> 00:24:12,770
counterfactual inference.

516
00:24:15,430 --> 00:24:17,906
This is the last slide of the causal

517
00:24:17,938 --> 00:24:20,822
inference introduction and it's about

518
00:24:20,876 --> 00:24:22,838
structured learning. Basically

519
00:24:22,924 --> 00:24:25,366
everything I've said so far relies on

520
00:24:25,388 --> 00:24:28,226
the fact that we know the causal

521
00:24:28,258 --> 00:24:30,566
dependencies among the data points. So

522
00:24:30,588 --> 00:24:32,186
we know the structure of the graph, we

523
00:24:32,208 --> 00:24:33,866
know which variable influences which

524
00:24:33,888 --> 00:24:37,546
one, we know the arrows in general but

525
00:24:37,568 --> 00:24:39,770
in practice this is actually not always

526
00:24:39,840 --> 00:24:43,306
possible. So we

527
00:24:43,328 --> 00:24:45,426
don't have access to the causal graph

528
00:24:45,478 --> 00:24:47,262
most of the times and actually learning

529
00:24:47,316 --> 00:24:49,854
the best causal graph from data is still

530
00:24:49,892 --> 00:24:51,646
an open problem. We are improving in

531
00:24:51,668 --> 00:24:55,002
this, we are getting better. But how to

532
00:24:55,076 --> 00:24:58,466
perform this task exactly is

533
00:24:58,488 --> 00:25:02,034
still an open problem. So as I said,

534
00:25:02,072 --> 00:25:03,758
basically the goal is to infer causal

535
00:25:03,774 --> 00:25:05,970
relationships from observational data.

536
00:25:06,120 --> 00:25:08,610
So given a data set we want to infer the

537
00:25:08,680 --> 00:25:11,346
directed icyclic graph that describes

538
00:25:11,378 --> 00:25:13,526
the connectivity between the system and

539
00:25:13,548 --> 00:25:16,326
the variables of the data set. So for

540
00:25:16,348 --> 00:25:18,118
example, here we have an example that I

541
00:25:18,124 --> 00:25:22,554
guess we are all familiar with because

542
00:25:22,592 --> 00:25:25,034
of the pandemic. So we have those four

543
00:25:25,072 --> 00:25:27,862
variables age, vaccine,

544
00:25:27,926 --> 00:25:30,380
hospitalization and city.

545
00:25:31,390 --> 00:25:33,538
And we want to infer the causal

546
00:25:33,574 --> 00:25:35,934
dependencies among those variables. So

547
00:25:35,972 --> 00:25:37,946
for example, we want to learn directly

548
00:25:37,978 --> 00:25:40,158
from data that the probability of a

549
00:25:40,164 --> 00:25:43,214
person being hospitalized depends on its

550
00:25:43,252 --> 00:25:45,358
age and on the fact whether it's

551
00:25:45,374 --> 00:25:47,506
vaccinated or not and so forth and so

552
00:25:47,528 --> 00:25:48,100
on.

553
00:25:51,350 --> 00:25:54,978
So this is the end of the long

554
00:25:55,064 --> 00:25:58,002
introduction but I hope it was clear

555
00:25:58,056 --> 00:26:00,434
enough and I hope that I gave the basics

556
00:26:00,482 --> 00:26:03,574
to understand basically the results of

557
00:26:03,612 --> 00:26:05,846
the paper and now we can go to the

558
00:26:05,868 --> 00:26:08,342
research questions. So the research

559
00:26:08,396 --> 00:26:11,366
questions are the following first I want

560
00:26:11,388 --> 00:26:14,886
to see whether predictive coding can be

561
00:26:14,908 --> 00:26:17,414
used to perform causal inference. So

562
00:26:17,452 --> 00:26:19,946
predictive coding so far has only been

563
00:26:19,968 --> 00:26:22,570
used to perform to compute correlations

564
00:26:23,090 --> 00:26:25,694
in Bayesian networks. And the big

565
00:26:25,732 --> 00:26:27,802
question is can we go beyond correlation

566
00:26:27,866 --> 00:26:29,166
and model intervention and

567
00:26:29,188 --> 00:26:31,642
counterfactual in a biological plausible

568
00:26:31,706 --> 00:26:34,926
way? So in a way

569
00:26:34,948 --> 00:26:36,202
that it's for example, simple,

570
00:26:36,276 --> 00:26:39,026
intuitive and allow us to only play with

571
00:26:39,048 --> 00:26:40,754
the neurons and not touch, for example,

572
00:26:40,792 --> 00:26:44,498
the huge structure of the graph and

573
00:26:44,664 --> 00:26:46,510
more in practice. More specifically,

574
00:26:46,590 --> 00:26:48,414
the question becomes can we define

575
00:26:48,462 --> 00:26:50,898
operative coding based structural causal

576
00:26:50,914 --> 00:26:52,598
model to perform interventions and

577
00:26:52,604 --> 00:26:56,310
counterfactuals? The second question

578
00:26:56,380 --> 00:26:59,526
is, as I said, that having a

579
00:26:59,548 --> 00:27:01,446
structure causal model assumes that we

580
00:27:01,468 --> 00:27:03,046
know the structure of the Bayesian

581
00:27:03,078 --> 00:27:06,586
network, so it assumes that

582
00:27:06,608 --> 00:27:08,854
we have the arrows. Can we go beyond

583
00:27:08,902 --> 00:27:10,758
this and use predictive coding networks

584
00:27:10,774 --> 00:27:12,186
to learn the causal structure of the

585
00:27:12,208 --> 00:27:12,890
graph?

586
00:27:16,290 --> 00:27:19,054
Basically, giving positive answers to

587
00:27:19,172 --> 00:27:21,118
both those questions would allow us to

588
00:27:21,124 --> 00:27:23,358
use predictive coding as an end to end

589
00:27:23,524 --> 00:27:25,970
causal inference method which basically

590
00:27:26,040 --> 00:27:28,722
takes a data set and allow us to test

591
00:27:28,776 --> 00:27:30,366
interventions and counterfactual

592
00:27:30,398 --> 00:27:33,220
predictions directly from this data set.

593
00:27:36,750 --> 00:27:39,642
So let's tackle the first problem.

594
00:27:39,696 --> 00:27:41,594
So causal inference Vibrative coding

595
00:27:41,642 --> 00:27:44,334
which is also the section that gives the

596
00:27:44,372 --> 00:27:47,566
title to the paper basically. And here I

597
00:27:47,588 --> 00:27:49,242
will show how to perform correlations

598
00:27:49,306 --> 00:27:51,470
with Abrasive coding which is already

599
00:27:51,540 --> 00:27:54,286
known, and how to perform interventional

600
00:27:54,318 --> 00:27:57,522
queries which I think is the real

601
00:27:57,576 --> 00:27:59,860
question of the paper.

602
00:28:01,190 --> 00:28:03,778
So here is a causal graph which is the

603
00:28:03,784 --> 00:28:07,602
usual graph that we had and

604
00:28:07,656 --> 00:28:09,290
here is the corresponding priority

605
00:28:09,310 --> 00:28:12,178
coding model. So the axes are the latent

606
00:28:12,194 --> 00:28:14,898
variables and correspond to the neurons

607
00:28:15,074 --> 00:28:16,760
in a neural network model.

608
00:28:18,010 --> 00:28:21,126
And the black arrow pass prediction

609
00:28:21,158 --> 00:28:23,674
information from one neuron to the one

610
00:28:23,712 --> 00:28:26,774
down the hierarchy. And every vertex

611
00:28:26,822 --> 00:28:29,654
also has this error neuron which passes

612
00:28:29,702 --> 00:28:32,046
information up the hierarchy. So the

613
00:28:32,068 --> 00:28:35,246
information of every error goes to

614
00:28:35,268 --> 00:28:38,254
the value node up the hierarchy and

615
00:28:38,292 --> 00:28:40,846
basically tells it to correct itself to

616
00:28:40,868 --> 00:28:42,190
change the prediction.

617
00:28:44,690 --> 00:28:46,494
So to perform a correlation using

618
00:28:46,532 --> 00:28:48,594
predictive coding, what you have to do

619
00:28:48,632 --> 00:28:50,306
is that you take an observation and you

620
00:28:50,328 --> 00:28:52,530
simply fix the value of a specific

621
00:28:52,600 --> 00:28:55,058
neuron. So if you want to compute the

622
00:28:55,064 --> 00:28:57,750
probability of x four given x three

623
00:28:57,820 --> 00:29:00,342
equal to s three, we simply have to take

624
00:29:00,396 --> 00:29:03,526
x three and fix it to s three in a way

625
00:29:03,548 --> 00:29:06,102
that it doesn't change anymore and run

626
00:29:06,156 --> 00:29:09,340
an energy minimization. And this model,

627
00:29:11,230 --> 00:29:14,502
by updating the axis via

628
00:29:14,646 --> 00:29:16,554
a minimization of the variational free

629
00:29:16,592 --> 00:29:19,146
energy allows the model to converge to a

630
00:29:19,168 --> 00:29:21,326
solution to this question. So the

631
00:29:21,348 --> 00:29:23,326
probability or the expected value of x

632
00:29:23,348 --> 00:29:25,360
four given x three equals three.

633
00:29:27,170 --> 00:29:29,486
But how do I perform an intervention now

634
00:29:29,588 --> 00:29:31,598
without acting on the structure of the

635
00:29:31,604 --> 00:29:35,378
graph? Well, this is basically the

636
00:29:35,384 --> 00:29:38,914
first idea of the paper, this is still

637
00:29:38,952 --> 00:29:41,186
how to perform a correlation. So fix s

638
00:29:41,208 --> 00:29:43,954
three equal to x three is the first step

639
00:29:43,992 --> 00:29:45,878
in the algorithm and the second one is

640
00:29:45,884 --> 00:29:47,718
to obtain the axis by minimizing the

641
00:29:47,724 --> 00:29:48,840
variation of free energy,

642
00:29:51,050 --> 00:29:53,234
an intervention which in theory

643
00:29:53,282 --> 00:29:56,502
corresponds in removing those arrows and

644
00:29:56,556 --> 00:29:57,862
answers to the question the probability

645
00:29:57,926 --> 00:30:01,334
of x four by performing an intervention.

646
00:30:01,382 --> 00:30:04,134
So do x three equal s three? Imperative

647
00:30:04,182 --> 00:30:06,090
coding can be performed as follows.

648
00:30:06,990 --> 00:30:08,934
So I'm going to write the algorithm

649
00:30:08,982 --> 00:30:11,818
here. So first, as in a correlation,

650
00:30:11,914 --> 00:30:15,406
you fix s three equal equal to the you

651
00:30:15,428 --> 00:30:17,434
fix x three equal to the observation

652
00:30:17,482 --> 00:30:20,158
that you get then this is the important

653
00:30:20,244 --> 00:30:23,666
step. You have to intervene not on the

654
00:30:23,688 --> 00:30:25,406
graph anymore, but on the prediction

655
00:30:25,438 --> 00:30:28,180
error and fix it equal to zero.

656
00:30:28,950 --> 00:30:30,946
Having a prediction error equal to zero

657
00:30:31,048 --> 00:30:35,522
basically sends

658
00:30:35,666 --> 00:30:37,206
meaningless information up the

659
00:30:37,228 --> 00:30:39,046
hierarchy, or actually sends no

660
00:30:39,068 --> 00:30:40,726
information up the hierarchy because it

661
00:30:40,748 --> 00:30:42,146
basically tells you that the prediction

662
00:30:42,178 --> 00:30:46,022
is always correct. And the third

663
00:30:46,076 --> 00:30:48,866
step is to, as we did before, to update

664
00:30:48,898 --> 00:30:51,306
the axis, the unconstrained axis. So x

665
00:30:51,328 --> 00:30:53,498
one, x two, x four. By minimizing the

666
00:30:53,504 --> 00:30:56,650
variation of free energy, as I will show

667
00:30:56,720 --> 00:30:59,674
now experimentally, by simply doing this

668
00:30:59,712 --> 00:31:01,686
little trick of setting a prediction

669
00:31:01,718 --> 00:31:03,280
error to be equal to zero,

670
00:31:05,650 --> 00:31:08,078
it prevents us to actually act on the

671
00:31:08,084 --> 00:31:11,546
structure of the graph, as the theory

672
00:31:11,578 --> 00:31:14,386
of Ducalculus does, and to infer the

673
00:31:14,408 --> 00:31:17,106
missing the variables after an

674
00:31:17,128 --> 00:31:19,134
intervention. By simply performing

675
00:31:19,182 --> 00:31:20,930
aberration of free energy minimization.

676
00:31:24,650 --> 00:31:26,482
What about counterfactual inference?

677
00:31:26,546 --> 00:31:28,118
Counterfactual inference is actually

678
00:31:28,204 --> 00:31:31,650
easy once we have defined

679
00:31:31,810 --> 00:31:35,126
how to do an intervention. And this

680
00:31:35,148 --> 00:31:36,638
is because as we saw earlier,

681
00:31:36,754 --> 00:31:38,362
performing a counterfactual is similar

682
00:31:38,416 --> 00:31:40,234
to performing an intervention in a past

683
00:31:40,272 --> 00:31:45,898
situation after you have inferred the

684
00:31:45,904 --> 00:31:48,860
unobservable variables. So,

685
00:31:49,390 --> 00:31:51,386
as you can see in the plot I showed

686
00:31:51,418 --> 00:31:53,406
earlier about the abduction action and

687
00:31:53,428 --> 00:31:55,934
prediction steps, the action and

688
00:31:55,972 --> 00:31:58,094
prediction steps, they did not have

689
00:31:58,132 --> 00:32:01,098
those two arrows, they were removed.

690
00:32:01,274 --> 00:32:03,570
Pretty coding allows allow us to keep

691
00:32:03,640 --> 00:32:07,490
the arrows in the graph

692
00:32:09,110 --> 00:32:11,566
and perform counterfactuals by simply

693
00:32:11,598 --> 00:32:13,506
performing an adoption step, as it was

694
00:32:13,528 --> 00:32:16,246
done earlier, an action step in which we

695
00:32:16,268 --> 00:32:18,198
simply perform an intervention on the

696
00:32:18,204 --> 00:32:20,626
single node. So we fix the value node

697
00:32:20,658 --> 00:32:24,630
and we set the error to zero and

698
00:32:24,700 --> 00:32:26,294
run the energy minimization. So

699
00:32:26,332 --> 00:32:27,866
minimizing the rational free energy to

700
00:32:27,888 --> 00:32:29,050
compute the prediction.

701
00:32:32,670 --> 00:32:35,738
So, I think this is like an easy

702
00:32:35,824 --> 00:32:39,018
and elegant method to perform

703
00:32:39,104 --> 00:32:41,150
interventions at counterfactuals.

704
00:32:43,330 --> 00:32:45,326
I think the thing we have to show now is

705
00:32:45,348 --> 00:32:47,278
whether it works in practice or not.

706
00:32:47,444 --> 00:32:50,846
And we have a couple of experiments and

707
00:32:51,028 --> 00:32:52,642
I'm going to show you now two different

708
00:32:52,696 --> 00:32:54,930
experiments. The first one is merely

709
00:32:55,590 --> 00:32:58,274
proof of concept experiment that shows

710
00:32:58,392 --> 00:33:01,586
that the operative coding is able to

711
00:33:01,608 --> 00:33:03,618
perform intervention and

712
00:33:03,624 --> 00:33:06,822
counterfactuals. And the second

713
00:33:06,876 --> 00:33:09,558
one actually shows a simple application

714
00:33:09,724 --> 00:33:12,566
in how interventional queries can be

715
00:33:12,588 --> 00:33:13,974
used to improve the performance of

716
00:33:14,012 --> 00:33:16,774
classification tasks on a specific kind

717
00:33:16,812 --> 00:33:19,866
of predictive coding networks, which is

718
00:33:19,888 --> 00:33:22,726
that of a fully connected model. Let's

719
00:33:22,758 --> 00:33:25,578
start from the first one. So how do we

720
00:33:25,584 --> 00:33:28,566
do this task? So, given a structural

721
00:33:28,598 --> 00:33:32,382
causal model, we generate training data

722
00:33:32,516 --> 00:33:34,686
and we use it to learn the weights. So,

723
00:33:34,708 --> 00:33:36,906
to learn the functions of the structural

724
00:33:36,938 --> 00:33:38,110
causal models,

725
00:33:40,290 --> 00:33:43,202
and then we generate test data for both

726
00:33:43,256 --> 00:33:45,006
interventional and counterfactual

727
00:33:45,038 --> 00:33:47,714
queries and we show whether we are able

728
00:33:47,752 --> 00:33:50,642
to converge to the correct test data

729
00:33:50,776 --> 00:33:52,450
using predictive coding.

730
00:33:54,870 --> 00:33:57,502
For example, here those two plots

731
00:33:57,566 --> 00:33:59,506
represent intervention and

732
00:33:59,528 --> 00:34:01,970
counterfactual queries of this specific

733
00:34:02,120 --> 00:34:04,334
graph, which is the butterfly bias

734
00:34:04,382 --> 00:34:06,322
graph, which is a graph that is often

735
00:34:06,376 --> 00:34:09,286
used in in testing whether causal

736
00:34:09,318 --> 00:34:11,418
inference, whether intervention and

737
00:34:11,424 --> 00:34:13,978
counterfactual techniques work is as

738
00:34:13,984 --> 00:34:16,314
simple as that. But in the paper you can

739
00:34:16,352 --> 00:34:19,726
find a lot of different graphs. But in

740
00:34:19,748 --> 00:34:22,638
general those two plots show that the

741
00:34:22,644 --> 00:34:28,094
method works show that the

742
00:34:28,292 --> 00:34:32,114
mean absolute error between the

743
00:34:32,152 --> 00:34:33,422
interventional and counterfactual

744
00:34:33,486 --> 00:34:36,066
quantities we compute and the

745
00:34:36,088 --> 00:34:38,286
interventional and counterfactual

746
00:34:38,318 --> 00:34:42,114
quantities from the original graph are

747
00:34:42,152 --> 00:34:43,698
close to each other. So the error is

748
00:34:43,704 --> 00:34:47,830
quite small. The second experiment

749
00:34:48,330 --> 00:34:50,086
is basically an extension of an

750
00:34:50,108 --> 00:34:52,066
experiment I proposed in an earlier

751
00:34:52,098 --> 00:34:55,634
paper which is the Learning on arbitrary

752
00:34:55,682 --> 00:34:58,540
graph topologies that I wrote last year.

753
00:34:58,990 --> 00:35:03,318
In that paper, I basically proposed

754
00:35:03,494 --> 00:35:05,226
this kind of network as a proof of

755
00:35:05,248 --> 00:35:06,566
concept, which is a fully connected

756
00:35:06,598 --> 00:35:10,154
network, which is, in general, the worst

757
00:35:10,282 --> 00:35:13,902
neural network you can have to perform

758
00:35:13,956 --> 00:35:15,982
machine learning experiments because

759
00:35:16,036 --> 00:35:20,110
given a fixed set of neurons,

760
00:35:20,690 --> 00:35:25,362
basically every

761
00:35:25,416 --> 00:35:27,154
pair of neuron is connected by two

762
00:35:27,192 --> 00:35:30,594
different synapses. So is

763
00:35:30,632 --> 00:35:32,446
the model with the highest complexity

764
00:35:32,558 --> 00:35:35,426
possible in general. The good thing is

765
00:35:35,448 --> 00:35:36,846
that since you have a lot of cycles,

766
00:35:36,878 --> 00:35:38,598
the model is extremely flexible in the

767
00:35:38,604 --> 00:35:40,406
sense that you can train it, for

768
00:35:40,428 --> 00:35:42,822
example, on a Minced image and on a data

769
00:35:42,876 --> 00:35:45,766
point and on its label. But then the way

770
00:35:45,788 --> 00:35:47,398
you can query it, thanks to the

771
00:35:47,404 --> 00:35:50,566
information going back, is you can

772
00:35:50,588 --> 00:35:51,978
query in a lot of different ways. So you

773
00:35:51,984 --> 00:35:53,786
can perform classification tasks in

774
00:35:53,808 --> 00:35:55,706
which you provide an image and you run

775
00:35:55,728 --> 00:35:57,098
the energy minimization and get the

776
00:35:57,104 --> 00:35:59,082
label but you can also, for example,

777
00:35:59,136 --> 00:36:00,870
perform generation tasks in which you

778
00:36:00,880 --> 00:36:02,062
give the label, run the Energy

779
00:36:02,116 --> 00:36:04,574
Minimization and get the image. You can

780
00:36:04,612 --> 00:36:06,234
perform, for example, image completion

781
00:36:06,282 --> 00:36:10,590
in which you give half the image and

782
00:36:10,740 --> 00:36:12,590
let the model convert to the second half

783
00:36:12,660 --> 00:36:14,398
and so forth and so on. So it's

784
00:36:14,414 --> 00:36:16,994
basically a model that learns the

785
00:36:17,032 --> 00:36:19,282
statistics of the data set in its

786
00:36:19,336 --> 00:36:21,874
entirety without being focused on

787
00:36:21,912 --> 00:36:23,860
classification or generation in general.

788
00:36:25,110 --> 00:36:28,246
So this flexibility is great. The

789
00:36:28,268 --> 00:36:32,534
problem is that because of this every

790
00:36:32,572 --> 00:36:34,726
single task doesn't work well so it can

791
00:36:34,748 --> 00:36:36,518
do a lot of different things but none of

792
00:36:36,524 --> 00:36:40,314
them is done well and here

793
00:36:40,352 --> 00:36:43,046
I want to show how using interventional

794
00:36:43,078 --> 00:36:44,700
queries instead of standard

795
00:36:45,310 --> 00:36:47,014
correlational queries or conditional

796
00:36:47,062 --> 00:36:49,678
queries slightly improves the results of

797
00:36:49,684 --> 00:36:52,638
those classification tasks. So what are

798
00:36:52,644 --> 00:36:55,422
the conjecture reasons of the test

799
00:36:55,476 --> 00:36:59,214
accuracy on those tasks not

800
00:36:59,252 --> 00:37:02,590
being so high? The two reasons are that

801
00:37:02,660 --> 00:37:05,390
the model is distracted in correcting

802
00:37:05,890 --> 00:37:07,826
every single error. So basically you

803
00:37:07,848 --> 00:37:09,378
present an image and you would like to

804
00:37:09,384 --> 00:37:11,554
get a label but the model is actually

805
00:37:11,592 --> 00:37:13,746
updating itself to also predict the

806
00:37:13,768 --> 00:37:17,382
error in the images and the second

807
00:37:17,436 --> 00:37:19,094
reason, which is the one I said, is that

808
00:37:19,132 --> 00:37:21,714
the structure is far too complex.

809
00:37:21,842 --> 00:37:26,066
So again, from an OCAM

810
00:37:26,098 --> 00:37:29,714
razor argumentation this is the worst

811
00:37:29,762 --> 00:37:31,018
model you can have. So every time you

812
00:37:31,024 --> 00:37:32,618
have a model that. Fits a data set.

813
00:37:32,704 --> 00:37:34,214
That model is going to be less complex

814
00:37:34,262 --> 00:37:36,234
than this one that is going to be

815
00:37:36,432 --> 00:37:40,106
preferred, but in general just to

816
00:37:40,128 --> 00:37:42,846
study it, the idea is can query in this

817
00:37:42,868 --> 00:37:44,366
model be interventions be used to

818
00:37:44,388 --> 00:37:46,746
improve the performance of those fully

819
00:37:46,778 --> 00:37:49,918
connected models? Well, the answer is

820
00:37:50,004 --> 00:37:52,462
yes. So here is how I perform

821
00:37:52,516 --> 00:37:54,866
interventional queries. So I present an

822
00:37:54,888 --> 00:37:58,306
image to the network, I fix the error of

823
00:37:58,328 --> 00:38:00,754
the pixels to be equal to zero so this

824
00:38:00,792 --> 00:38:02,258
error doesn't get propagated in the

825
00:38:02,264 --> 00:38:05,090
network. And then I compute the label

826
00:38:05,610 --> 00:38:07,682
and as you can see the accuracy

827
00:38:07,746 --> 00:38:10,806
improves. For example, from 89 using the

828
00:38:10,828 --> 00:38:12,642
standard query method of creative coding

829
00:38:12,706 --> 00:38:16,514
networks, to 92, which is the accuracy

830
00:38:16,562 --> 00:38:18,278
after the intervention. And the same

831
00:38:18,364 --> 00:38:20,470
happens for fashion minced.

832
00:38:21,630 --> 00:38:24,374
And I think that a very legit critique

833
00:38:24,422 --> 00:38:27,274
that probably everyone would think when

834
00:38:27,312 --> 00:38:28,842
seeing those plots is that okay, you

835
00:38:28,896 --> 00:38:31,822
improved on mins from 89 to 92.

836
00:38:31,956 --> 00:38:35,486
It still sucks basically. And yeah,

837
00:38:35,588 --> 00:38:37,742
it's true. And actually in the later

838
00:38:37,796 --> 00:38:40,286
slides I'm going to show how to act on

839
00:38:40,308 --> 00:38:42,918
the structure of this fully connected

840
00:38:42,954 --> 00:38:46,034
model will improve the results even more

841
00:38:46,152 --> 00:38:49,246
until the point they reach a performance

842
00:38:49,278 --> 00:38:51,218
that is not even close to state of the

843
00:38:51,224 --> 00:38:52,900
art performance of course,

844
00:38:53,910 --> 00:38:56,274
but up to a level that becomes visually

845
00:38:56,322 --> 00:38:59,990
acceptable and worth investigating.

846
00:39:02,970 --> 00:39:05,346
Yes. So this was the part about causal

847
00:39:05,378 --> 00:39:08,230
inference using previous coding.

848
00:39:08,390 --> 00:39:11,260
And I guess to summarize, I can say that

849
00:39:11,710 --> 00:39:15,766
the interesting part of the results

850
00:39:15,878 --> 00:39:18,826
I just showed is that I showed that

851
00:39:18,848 --> 00:39:20,202
predictive coding is able to perform

852
00:39:20,256 --> 00:39:22,758
interventions in a very easy and

853
00:39:22,784 --> 00:39:24,846
intuitive way because you don't have to

854
00:39:24,868 --> 00:39:26,314
act on the structure of the old graph

855
00:39:26,362 --> 00:39:29,646
anymore. Sometimes those functions are

856
00:39:29,668 --> 00:39:31,840
not available, so forth and so on. But

857
00:39:34,310 --> 00:39:37,762
you simply have to intervene on a single

858
00:39:37,816 --> 00:39:41,060
neuron, set its prediction error to zero

859
00:39:41,590 --> 00:39:44,282
and perform an energy minimization

860
00:39:44,366 --> 00:39:48,210
process. And this extended

861
00:39:48,370 --> 00:39:50,546
allowed us to define predictive coding

862
00:39:50,578 --> 00:39:53,414
based structural causal models. Now we

863
00:39:53,452 --> 00:39:56,806
move to the second part of the

864
00:39:56,828 --> 00:39:59,880
work which is about structure learning.

865
00:40:02,410 --> 00:40:05,286
So structure learning, as I said, deals

866
00:40:05,318 --> 00:40:07,686
with the problem of learning the causal

867
00:40:07,718 --> 00:40:10,094
structure of the model from

868
00:40:10,132 --> 00:40:12,686
observational data. This is actually an

869
00:40:12,708 --> 00:40:14,782
old problem that has been around for

870
00:40:14,836 --> 00:40:18,606
decades and

871
00:40:18,788 --> 00:40:21,614
has always been until a couple of years

872
00:40:21,652 --> 00:40:24,242
ago tackled using combinatorial search

873
00:40:24,296 --> 00:40:26,306
methods. The problem with those

874
00:40:26,328 --> 00:40:28,260
combinatorial search methods is that

875
00:40:28,790 --> 00:40:30,814
their complexity grows double

876
00:40:30,862 --> 00:40:34,334
exponentially. So as soon as the data

877
00:40:34,392 --> 00:40:37,222
becomes multidimensional and the

878
00:40:37,276 --> 00:40:40,006
Bayesian graph that you want to learn

879
00:40:40,188 --> 00:40:43,858
grows in size learning, it it's

880
00:40:43,874 --> 00:40:47,730
incredibly slow. The new solution

881
00:40:47,810 --> 00:40:49,414
that came out actually a couple of years

882
00:40:49,452 --> 00:40:52,170
ago in a new research paper from 2018

883
00:40:54,350 --> 00:40:55,834
showed that it's possible to actually

884
00:40:55,872 --> 00:40:57,738
learn this structure not using a

885
00:40:57,744 --> 00:40:59,646
combinatorial search method, but by

886
00:40:59,668 --> 00:41:04,702
using a gradient based method. And this

887
00:41:04,756 --> 00:41:06,654
killed the problem in general because

888
00:41:06,692 --> 00:41:09,966
now you can simply have a prior on the

889
00:41:09,988 --> 00:41:11,406
parameters, which is the prior the

890
00:41:11,428 --> 00:41:13,378
proposed that I'm going to define a

891
00:41:13,384 --> 00:41:16,034
little bit better in this slide run

892
00:41:16,072 --> 00:41:18,258
gradient descent. And even if you have a

893
00:41:18,264 --> 00:41:20,260
model that is double triple the size,

894
00:41:21,670 --> 00:41:24,100
the algorithm is still incredibly fast.

895
00:41:25,430 --> 00:41:29,446
And for this reason this paper is I

896
00:41:29,468 --> 00:41:31,574
think it's kind of new and I think

897
00:41:31,612 --> 00:41:34,230
already has around 600 citations or

898
00:41:34,380 --> 00:41:36,566
things like that. And every paper that

899
00:41:36,588 --> 00:41:38,166
I'm seeing now about causal inference

900
00:41:38,198 --> 00:41:39,866
and learning causal structure of the

901
00:41:39,888 --> 00:41:42,922
graph uses their method. It just changes

902
00:41:42,976 --> 00:41:46,314
a little bit. They find faster or

903
00:41:46,352 --> 00:41:48,634
slightly better inference methods, but

904
00:41:48,672 --> 00:41:51,790
still they all use the prior this paper

905
00:41:51,860 --> 00:41:54,894
defined and I do as well, and we do as

906
00:41:54,932 --> 00:41:58,106
well. So here we define

907
00:41:58,138 --> 00:41:59,962
a new quantity which is the agency

908
00:42:00,026 --> 00:42:02,398
matrix. The agency matrix is simply a

909
00:42:02,404 --> 00:42:04,626
matrix that encodes the connections of

910
00:42:04,648 --> 00:42:08,020
the model. So it's a binary matrix and

911
00:42:08,790 --> 00:42:10,818
in general it's a binary matrix. Then of

912
00:42:10,824 --> 00:42:12,002
course, when you do gradient based

913
00:42:12,056 --> 00:42:15,126
optimization you make it continuous and

914
00:42:15,148 --> 00:42:16,726
then you have some threshold at some

915
00:42:16,748 --> 00:42:19,686
point that basically kills an edge or

916
00:42:19,708 --> 00:42:24,210
set it to one and the entry IJ

917
00:42:24,290 --> 00:42:28,586
is equal to one if

918
00:42:28,608 --> 00:42:32,074
the Bayesian graph has an edge from

919
00:42:32,112 --> 00:42:35,446
vertex I to vertex j or zero otherwise.

920
00:42:35,558 --> 00:42:38,138
So for example, this agency matrix here

921
00:42:38,224 --> 00:42:39,998
represents the connectivity structure of

922
00:42:40,004 --> 00:42:41,230
this Bayesian network.

923
00:42:42,850 --> 00:42:47,098
And basically this method tackles

924
00:42:47,274 --> 00:42:50,590
two problems that we want about learning

925
00:42:50,660 --> 00:42:52,966
the structure of the Bayesian network.

926
00:42:53,098 --> 00:42:54,686
The idea is that we start from a fully

927
00:42:54,718 --> 00:42:59,202
connected model, which conceptually is

928
00:42:59,256 --> 00:43:01,378
similar, actually is equivalent to the

929
00:43:01,384 --> 00:43:02,926
creative coding network I defined

930
00:43:02,958 --> 00:43:05,220
earlier, which is fully connected. So

931
00:43:05,670 --> 00:43:07,494
you have a lot of vertices and every

932
00:43:07,532 --> 00:43:10,134
pair of vertices is connected by two

933
00:43:10,172 --> 00:43:12,166
different edges and you simply want to

934
00:43:12,188 --> 00:43:15,000
prune the ones that are not needed.

935
00:43:15,690 --> 00:43:18,498
So it can be seen as a method that

936
00:43:18,524 --> 00:43:20,666
performs model reduction. You start from

937
00:43:20,688 --> 00:43:22,026
a big model and you want to make it

938
00:43:22,048 --> 00:43:25,546
small. So what's the first ingredient to

939
00:43:25,648 --> 00:43:27,674
reduce models? Well, it's of course

940
00:43:27,712 --> 00:43:30,758
sparse city. And what's the prior that

941
00:43:30,784 --> 00:43:32,766
everyone uses to make a model more

942
00:43:32,788 --> 00:43:35,726
sparse is the LaPlace prior, which in

943
00:43:35,748 --> 00:43:37,326
machine learning is simply known as the

944
00:43:37,348 --> 00:43:40,734
L one norm, which is defined here.

945
00:43:40,932 --> 00:43:43,778
The solution that this paper that I

946
00:43:43,784 --> 00:43:46,578
mentioned earlier proposed is to add a

947
00:43:46,584 --> 00:43:49,134
second prior on top, which enforces

948
00:43:49,262 --> 00:43:52,690
what's probably the biggest

949
00:43:53,430 --> 00:43:56,566
characteristic of Bayesian networks on

950
00:43:56,588 --> 00:43:57,746
which you want to perform causal

951
00:43:57,778 --> 00:43:59,798
inference is that you want them to be a

952
00:43:59,804 --> 00:44:02,870
cyclic. And basically they show that

953
00:44:03,020 --> 00:44:06,354
Acyclicity can be imposed on an agency

954
00:44:06,402 --> 00:44:10,006
matrix as a prior and it has this shape

955
00:44:10,038 --> 00:44:13,110
here. So it's the trace of the matrix

956
00:44:13,190 --> 00:44:16,566
that is the exponential

957
00:44:16,678 --> 00:44:19,594
of a times a where A is the agency

958
00:44:19,642 --> 00:44:23,086
matrix again. And basically this

959
00:44:23,108 --> 00:44:26,654
quantity here is equal to zero if and

960
00:44:26,692 --> 00:44:31,182
only if the Bayesian network or

961
00:44:31,316 --> 00:44:33,118
whatever graph you're considering is a

962
00:44:33,124 --> 00:44:33,870
cyclic.

963
00:44:37,650 --> 00:44:40,174
So I'm going to use this in some

964
00:44:40,212 --> 00:44:43,166
experiments. So to force those two

965
00:44:43,188 --> 00:44:46,638
priors on different kinds of

966
00:44:46,644 --> 00:44:48,226
Bayesian networks and I'm trying to

967
00:44:48,248 --> 00:44:50,002
merge them with the techniques we

968
00:44:50,056 --> 00:44:52,286
proposed earlier about performing causal

969
00:44:52,318 --> 00:44:55,506
inference via predictive coding. So I'm

970
00:44:55,528 --> 00:44:56,482
going to present two different

971
00:44:56,536 --> 00:44:59,554
experiments. So one is a proof of

972
00:44:59,592 --> 00:45:00,790
concept, which is the standard

973
00:45:00,860 --> 00:45:03,586
experiments showed in all the structural

974
00:45:03,618 --> 00:45:06,322
learning tasks, which is the inference

975
00:45:06,386 --> 00:45:07,926
of the correct Bayesian network from

976
00:45:07,948 --> 00:45:11,546
data. And then I'm going to build on top

977
00:45:11,568 --> 00:45:13,066
of the classification experiments I

978
00:45:13,088 --> 00:45:16,954
showed earlier and show how

979
00:45:16,992 --> 00:45:20,090
actually those priors allow us to

980
00:45:20,240 --> 00:45:22,086
improve the classification accuracy,

981
00:45:22,118 --> 00:45:24,806
the test accuracy of fully connected

982
00:45:24,838 --> 00:45:26,430
predictive coding models.

983
00:45:29,420 --> 00:45:31,684
So let's move to the first experiment,

984
00:45:31,732 --> 00:45:33,176
which is to infer the structure of the

985
00:45:33,198 --> 00:45:37,032
graph. And the experiments, they all

986
00:45:37,086 --> 00:45:39,108
follow basically the same pipeline in

987
00:45:39,134 --> 00:45:41,164
all the papers in the field. The first

988
00:45:41,202 --> 00:45:44,392
step is to generate a vision network

989
00:45:44,456 --> 00:45:46,892
from a random graph. So basically,

990
00:45:47,026 --> 00:45:48,796
normally the two random graphs that

991
00:45:48,818 --> 00:45:51,744
everyone tests are erdos reni graphs and

992
00:45:51,782 --> 00:45:55,024
scale free graph. So you generate those

993
00:45:55,062 --> 00:45:57,570
big graphs that normally have 20, 40,

994
00:45:58,580 --> 00:46:02,016
80 different nodes and some edges that

995
00:46:02,038 --> 00:46:05,476
you sample randomly and you use this

996
00:46:05,498 --> 00:46:08,836
graph to generate a data set. So you

997
00:46:08,858 --> 00:46:12,612
sample, for example, n, big N data

998
00:46:12,666 --> 00:46:14,916
points. And what you do is that you take

999
00:46:14,938 --> 00:46:18,052
the graph they have generated earlier

1000
00:46:18,116 --> 00:46:19,816
and you throw it away. You only keep the

1001
00:46:19,838 --> 00:46:22,116
data set. And the task you want to solve

1002
00:46:22,148 --> 00:46:25,656
now is to

1003
00:46:25,678 --> 00:46:27,640
have a training algorithm that basically

1004
00:46:27,710 --> 00:46:30,816
allows you to retrieve

1005
00:46:30,868 --> 00:46:32,956
the structure of the graph you have

1006
00:46:32,978 --> 00:46:36,156
thrown away. So the way we do it here

1007
00:46:36,178 --> 00:46:37,576
is that we train a fully connected

1008
00:46:37,608 --> 00:46:40,520
creative coding model on this data set D

1009
00:46:40,690 --> 00:46:42,828
using both the sparse and the acyclic

1010
00:46:42,924 --> 00:46:46,720
priors we have defined earlier and see

1011
00:46:46,790 --> 00:46:49,776
whether actually the graph that we

1012
00:46:49,798 --> 00:46:51,890
converge to after pruning away.

1013
00:46:53,220 --> 00:46:55,156
The entries of the agency matrix that

1014
00:46:55,178 --> 00:46:57,492
are smaller than a certain threshold is

1015
00:46:57,546 --> 00:47:00,064
similar to that of the of the initial

1016
00:47:00,112 --> 00:47:04,116
graph. And the results show that this

1017
00:47:04,138 --> 00:47:06,056
is actually the case. So this is an

1018
00:47:06,078 --> 00:47:08,904
example and I show many different

1019
00:47:09,022 --> 00:47:13,320
parameterization and dimensions and

1020
00:47:13,390 --> 00:47:15,976
things like that in the paper, but I

1021
00:47:15,998 --> 00:47:16,824
think those two are the most

1022
00:47:16,862 --> 00:47:18,888
representative examples with an Ernest

1023
00:47:18,904 --> 00:47:21,032
Sharoni graph and a free scale graph

1024
00:47:21,176 --> 00:47:24,252
with 20 nodes. And here

1025
00:47:24,306 --> 00:47:26,216
on the left you can see the ground truth

1026
00:47:26,248 --> 00:47:28,220
graph, which is the one sampled

1027
00:47:29,440 --> 00:47:32,076
randomly. And on the right you can see

1028
00:47:32,098 --> 00:47:34,512
the graph the predictive coding model

1029
00:47:34,566 --> 00:47:37,856
has learned from the data set. And as

1030
00:47:37,878 --> 00:47:39,970
you can see, they are quite similar.

1031
00:47:41,140 --> 00:47:43,876
It's still not perfect, so there are

1032
00:47:43,898 --> 00:47:45,556
some errors, but in general the

1033
00:47:45,578 --> 00:47:48,916
structure is they work quite well. We

1034
00:47:48,938 --> 00:47:51,060
also have some quantitative experiments

1035
00:47:52,200 --> 00:47:53,936
that I don't show here because they're

1036
00:47:53,968 --> 00:47:55,780
just huge tables with a lot of numbers

1037
00:47:55,850 --> 00:47:57,464
and I thought it was maybe a little bit

1038
00:47:57,582 --> 00:48:00,776
too much for the presentation, but the

1039
00:48:00,798 --> 00:48:03,060
results show that they perform similarly

1040
00:48:03,140 --> 00:48:06,616
to contemporary methods. Also because

1041
00:48:06,638 --> 00:48:08,440
I have to say, like, most of the quality

1042
00:48:08,590 --> 00:48:11,276
comes from the Acyclic prior that was

1043
00:48:11,298 --> 00:48:13,900
introduced in 2018.

1044
00:48:16,920 --> 00:48:19,860
The second class of experiments are

1045
00:48:19,930 --> 00:48:21,588
classification experiments, which as I

1046
00:48:21,594 --> 00:48:23,976
said are the extensions of the one I

1047
00:48:23,998 --> 00:48:26,664
shared earlier. And the idea is to use

1048
00:48:26,702 --> 00:48:27,976
structure learning to improve the

1049
00:48:27,998 --> 00:48:30,260
classification on the classification

1050
00:48:30,340 --> 00:48:31,944
results on the means and fashion means

1051
00:48:31,982 --> 00:48:34,568
data set, starting from a fully

1052
00:48:34,584 --> 00:48:38,396
connected graph. So, what I

1053
00:48:38,418 --> 00:48:40,776
did is that I divided the fully

1054
00:48:40,808 --> 00:48:43,592
connected graph in clusters of neurons.

1055
00:48:43,736 --> 00:48:47,196
So one big cluster is the

1056
00:48:47,218 --> 00:48:49,836
one related to the input and all the

1057
00:48:49,858 --> 00:48:52,128
small. Then we have some a specific

1058
00:48:52,214 --> 00:48:54,800
number of hidden clusters.

1059
00:48:55,300 --> 00:48:57,292
And then we have the label cluster,

1060
00:48:57,356 --> 00:49:01,364
which is the cluster of neurons that

1061
00:49:01,402 --> 00:49:03,984
are supposed to give me the label

1062
00:49:04,032 --> 00:49:07,168
predictions. And I've trained

1063
00:49:07,184 --> 00:49:09,696
them using the first time, the sparse

1064
00:49:09,728 --> 00:49:12,864
prior only. See, the idea is what if I,

1065
00:49:12,922 --> 00:49:15,450
if I prune the connections I don't need

1066
00:49:15,820 --> 00:49:20,090
from a model and learn a sparser model?

1067
00:49:20,940 --> 00:49:23,560
Does this work? Well, the answer is no,

1068
00:49:23,710 --> 00:49:26,236
it doesn't work. And the reason why is

1069
00:49:26,258 --> 00:49:29,116
that at the end, the graph that you

1070
00:49:29,138 --> 00:49:31,560
converge with is actually the generate.

1071
00:49:31,640 --> 00:49:34,248
So basically the model learns to predict

1072
00:49:34,344 --> 00:49:37,244
the label based on the label itself. So

1073
00:49:37,282 --> 00:49:38,988
it discards all the information from the

1074
00:49:38,994 --> 00:49:42,096
input and only keeps the label. And as

1075
00:49:42,118 --> 00:49:43,996
you can see here, the label y predicts

1076
00:49:44,028 --> 00:49:45,968
itself. Or in other experiments, when

1077
00:49:45,974 --> 00:49:47,904
you change the parameters, you have that

1078
00:49:47,942 --> 00:49:50,084
y predicts x zero that predicts x one

1079
00:49:50,122 --> 00:49:54,528
that predicts y again. So what's

1080
00:49:54,544 --> 00:49:56,628
the solution to this problem? Well, the

1081
00:49:56,634 --> 00:49:57,924
solution to this problem is that we have

1082
00:49:57,962 --> 00:50:01,700
to converge to an acyclic graph

1083
00:50:03,080 --> 00:50:05,176
and so we have to add something that

1084
00:50:05,198 --> 00:50:08,010
prevents acyclicity. And what is that

1085
00:50:08,540 --> 00:50:10,232
one is of course, the one I already

1086
00:50:10,286 --> 00:50:12,792
proposed. And then I show a second

1087
00:50:12,846 --> 00:50:16,408
technique. So the first one uses

1088
00:50:16,424 --> 00:50:18,300
the acyclic prior defined earlier.

1089
00:50:19,200 --> 00:50:21,704
And the second one is a novel technique

1090
00:50:21,752 --> 00:50:23,192
that actually makes use of negative

1091
00:50:23,256 --> 00:50:26,396
examples. So a negative example in this

1092
00:50:26,418 --> 00:50:30,076
case is simply a data point in which

1093
00:50:30,098 --> 00:50:32,176
you have an image, but the label is

1094
00:50:32,198 --> 00:50:34,704
wrong. So here, for example, you have an

1095
00:50:34,742 --> 00:50:36,668
image of a seven, but the label that I'm

1096
00:50:36,684 --> 00:50:38,210
giving the model is a two.

1097
00:50:41,400 --> 00:50:44,884
The idea is very simple, has been used

1098
00:50:44,922 --> 00:50:48,276
in a lot of works already. So every

1099
00:50:48,298 --> 00:50:50,148
time the model sees a positive example,

1100
00:50:50,234 --> 00:50:52,696
it has to increase to minimize the

1101
00:50:52,718 --> 00:50:54,744
variation of free energy. And every time

1102
00:50:54,782 --> 00:50:57,576
it sees a negative example, it has to

1103
00:50:57,598 --> 00:51:01,492
increase it. So we want this quantity

1104
00:51:01,556 --> 00:51:02,680
to be minimized.

1105
00:51:05,280 --> 00:51:07,736
And actually, with a lot of experiments

1106
00:51:07,768 --> 00:51:10,444
and a lot of experimentations, we saw

1107
00:51:10,482 --> 00:51:13,388
that the two techniques basically first

1108
00:51:13,474 --> 00:51:16,784
lead to the same results and second lead

1109
00:51:16,822 --> 00:51:21,344
to the same graph as well. So here

1110
00:51:21,382 --> 00:51:22,768
are the new results on Minced and

1111
00:51:22,774 --> 00:51:25,440
fashion Mins using the two techniques

1112
00:51:25,780 --> 00:51:29,104
that I just proposed. And now we

1113
00:51:29,142 --> 00:51:32,752
move to some which are still not great,

1114
00:51:32,806 --> 00:51:34,176
but still definitely more reasonable

1115
00:51:34,208 --> 00:51:36,324
test accuracies. So here we have a test

1116
00:51:36,362 --> 00:51:39,124
error of 3.17 for Minced and a test

1117
00:51:39,162 --> 00:51:42,648
error of 13.98 for Fashion Minced. And

1118
00:51:42,734 --> 00:51:45,432
actually those results can be much

1119
00:51:45,486 --> 00:51:48,568
improved by learning the structure of

1120
00:51:48,574 --> 00:51:51,832
the graph on Minced and then

1121
00:51:51,886 --> 00:51:54,392
fixing the structure of the graph and do

1122
00:51:54,446 --> 00:51:56,536
some form of fine tuning. So if you fine

1123
00:51:56,558 --> 00:51:58,012
tune the model on the correct

1124
00:51:58,066 --> 00:52:00,284
hierarchical structure, at some point

1125
00:52:00,322 --> 00:52:02,156
you reach the test accuracy, which is

1126
00:52:02,178 --> 00:52:03,276
the one you would expect from a

1127
00:52:03,298 --> 00:52:05,404
hierarchical model, but those ones are

1128
00:52:05,442 --> 00:52:08,092
simply the one the fully connected model

1129
00:52:08,146 --> 00:52:09,890
has naturally converged to.

1130
00:52:12,020 --> 00:52:15,216
For example, from a test error of 18.32

1131
00:52:15,398 --> 00:52:17,216
of the fully connected model train on

1132
00:52:17,238 --> 00:52:19,600
fashion means by simply performing

1133
00:52:20,520 --> 00:52:22,832
correlations or conditional queries,

1134
00:52:22,896 --> 00:52:24,336
which is the standard way of querying

1135
00:52:24,368 --> 00:52:27,200
Abrative coding model. Adding

1136
00:52:27,280 --> 00:52:29,424
interventions and the Acyclic prior

1137
00:52:29,472 --> 00:52:32,712
together makes this test

1138
00:52:32,766 --> 00:52:35,656
error much lower and we can observe it

1139
00:52:35,758 --> 00:52:37,690
for Mins as well.

1140
00:52:39,740 --> 00:52:42,200
I'm now going a little bit into details

1141
00:52:42,780 --> 00:52:45,748
on this last experiment and on how the

1142
00:52:45,854 --> 00:52:48,716
Acyclic prior acts on the structure of

1143
00:52:48,738 --> 00:52:52,360
the graph. I perform an experiment

1144
00:52:52,520 --> 00:52:54,844
on a new data set, which is, I mean,

1145
00:52:54,882 --> 00:52:56,476
calling it a new data set is maybe too

1146
00:52:56,498 --> 00:52:59,120
much. I called it a two means data set

1147
00:52:59,270 --> 00:53:02,464
in which you have the input point is

1148
00:53:02,502 --> 00:53:05,696
formed of two different images and the

1149
00:53:05,718 --> 00:53:08,428
label only depends on the second image,

1150
00:53:08,604 --> 00:53:11,412
on the first image. Sorry. So the idea

1151
00:53:11,466 --> 00:53:14,630
here is, is the structure of the model,

1152
00:53:15,080 --> 00:53:17,476
the Acyclicity prior and things like

1153
00:53:17,498 --> 00:53:20,484
that, able to recognize that the second

1154
00:53:20,522 --> 00:53:21,944
half of the image is actually

1155
00:53:21,982 --> 00:53:25,512
meaningless in

1156
00:53:25,566 --> 00:53:29,640
learning, in performing classification,

1157
00:53:31,100 --> 00:53:32,712
how does training behave in general?

1158
00:53:32,766 --> 00:53:36,424
Like, for example, we have this input

1159
00:53:36,472 --> 00:53:38,956
node output node and only the nodes are

1160
00:53:38,978 --> 00:53:42,504
fully connected. Can the model converge

1161
00:53:42,552 --> 00:53:45,676
to a hierarchical structure, which is

1162
00:53:45,698 --> 00:53:48,432
the one that we know performs the best

1163
00:53:48,566 --> 00:53:52,336
on classification tasks? Well, here is

1164
00:53:52,358 --> 00:53:55,376
an example of a training method of a

1165
00:53:55,398 --> 00:53:58,704
training run. So at c zero, which is the

1166
00:53:58,742 --> 00:54:01,424
beginning of training, we have this

1167
00:54:01,462 --> 00:54:04,916
model here. So S zero corresponds to the

1168
00:54:04,938 --> 00:54:07,444
seven. So to the first image, s one

1169
00:54:07,482 --> 00:54:09,284
corresponds to the seven image. Again,

1170
00:54:09,322 --> 00:54:11,136
we have the label y and all the latent

1171
00:54:11,168 --> 00:54:14,168
variables x zero, x one, x two. And the

1172
00:54:14,174 --> 00:54:16,004
model is fully connected. So the agency

1173
00:54:16,052 --> 00:54:19,576
matrix is full of ones. There are

1174
00:54:19,598 --> 00:54:21,704
no zeros. We have self loops and things

1175
00:54:21,742 --> 00:54:24,892
like that. We train the model

1176
00:54:24,946 --> 00:54:28,444
for a couple of epochs until and what

1177
00:54:28,482 --> 00:54:30,396
we note immediately is that, for

1178
00:54:30,418 --> 00:54:31,944
example, the model immediately

1179
00:54:31,992 --> 00:54:34,572
understands that the four is not needed

1180
00:54:34,626 --> 00:54:35,820
to perform classification.

1181
00:54:37,620 --> 00:54:40,656
So every outgoing node from

1182
00:54:40,678 --> 00:54:43,360
the second input cluster is removed.

1183
00:54:43,940 --> 00:54:45,696
And something that you understand is

1184
00:54:45,718 --> 00:54:48,988
that this cluster is the one related to

1185
00:54:49,014 --> 00:54:52,756
the output. So we have

1186
00:54:52,778 --> 00:54:56,100
a linear map from s zero to y directly,

1187
00:54:57,000 --> 00:55:00,036
which is this part here. But we know

1188
00:55:00,058 --> 00:55:02,516
that actually a linear map is not the

1189
00:55:02,538 --> 00:55:05,872
best map for performing classification

1190
00:55:05,936 --> 00:55:08,328
on Minst. So we need some hierarchy, we

1191
00:55:08,334 --> 00:55:11,028
need some depth to improve the results.

1192
00:55:11,124 --> 00:55:14,028
And as you can see, this line here is

1193
00:55:14,034 --> 00:55:17,484
the accuracy which up to this point, so

1194
00:55:17,522 --> 00:55:21,116
up to c two is similar to is

1195
00:55:21,138 --> 00:55:23,896
91% which is slightly better than linear

1196
00:55:23,928 --> 00:55:27,056
classification. But once you go on with

1197
00:55:27,078 --> 00:55:29,968
the training, the model understands that

1198
00:55:29,974 --> 00:55:31,904
it needs some hierarchy to better fit

1199
00:55:31,942 --> 00:55:35,068
the data. So you see that this arrow

1200
00:55:35,164 --> 00:55:36,812
starts getting stronger and stronger

1201
00:55:36,876 --> 00:55:40,812
over time until it understands

1202
00:55:40,876 --> 00:55:42,772
that the linear map is not actually

1203
00:55:42,826 --> 00:55:45,270
really needed and it removes it.

1204
00:55:46,760 --> 00:55:48,548
So the model you converge with is a

1205
00:55:48,554 --> 00:55:50,836
model that starts from a zero, goes to a

1206
00:55:50,858 --> 00:55:54,932
hidden node, and then goes to the label

1207
00:55:55,076 --> 00:55:57,784
with a very weak linear map, which

1208
00:55:57,822 --> 00:56:00,056
actually gets removed if you set a

1209
00:56:00,078 --> 00:56:03,096
threshold. Of if you set a threshold of,

1210
00:56:03,118 --> 00:56:05,708
for example, 0.10.2, at some point the

1211
00:56:05,714 --> 00:56:07,852
linear map gets forgotten and everything

1212
00:56:07,906 --> 00:56:11,384
you end up with is with a hierarchical

1213
00:56:11,432 --> 00:56:15,036
network. So it

1214
00:56:15,058 --> 00:56:16,720
has learned the correct structure to

1215
00:56:16,790 --> 00:56:18,624
perform classification tasks, which is

1216
00:56:18,662 --> 00:56:21,392
hierarchy. And it has also learned that

1217
00:56:21,446 --> 00:56:24,400
the second image didn't play any role in

1218
00:56:24,470 --> 00:56:28,816
defining the test accuracy. And this

1219
00:56:28,838 --> 00:56:30,676
is all performed, so all those jobs are

1220
00:56:30,698 --> 00:56:34,116
simply performed by one

1221
00:56:34,218 --> 00:56:36,564
free energy minimization process. So you

1222
00:56:36,602 --> 00:56:38,276
initialize the model, you define the

1223
00:56:38,298 --> 00:56:40,836
free energy, you define the priors. So

1224
00:56:41,018 --> 00:56:44,248
the sparse and the cyclic prior, you run

1225
00:56:44,334 --> 00:56:46,676
the energy minimization and you converge

1226
00:56:46,868 --> 00:56:49,016
to a hierarchical model which is well

1227
00:56:49,038 --> 00:56:51,320
able to perform classification on minst.

1228
00:56:51,820 --> 00:56:53,944
And then if you then perform some fine

1229
00:56:53,982 --> 00:56:55,752
tuning, you reach very competitive

1230
00:56:55,816 --> 00:56:57,324
results as you do in feed forward

1231
00:56:57,362 --> 00:57:00,156
networks with backpropagation. But I

1232
00:57:00,178 --> 00:57:01,676
think that's not the interesting bit.

1233
00:57:01,778 --> 00:57:04,636
The interesting bit is that all this

1234
00:57:04,658 --> 00:57:08,096
process altogether of intervention and a

1235
00:57:08,118 --> 00:57:11,148
cyclicity allows you to take a fully

1236
00:57:11,164 --> 00:57:13,808
connected network and converge to a

1237
00:57:13,814 --> 00:57:16,112
hierarchical one that is able to perform

1238
00:57:16,166 --> 00:57:18,640
classification with good results.

1239
00:57:20,900 --> 00:57:25,030
And yeah, that's basically it.

1240
00:57:25,800 --> 00:57:29,108
Wow, I've talked a lot and this is the

1241
00:57:29,114 --> 00:57:32,240
conclusion of the talk, which is I'm

1242
00:57:32,400 --> 00:57:35,128
basically doing a small summary and I

1243
00:57:35,134 --> 00:57:37,528
think the important takeaway, if I have

1244
00:57:37,534 --> 00:57:39,176
to give you in one sentence of this

1245
00:57:39,198 --> 00:57:41,112
paper, is that predictive coding is a

1246
00:57:41,246 --> 00:57:44,344
belief updating method that is able to

1247
00:57:44,382 --> 00:57:46,456
perform end to end causal learning. So

1248
00:57:46,478 --> 00:57:48,524
it's able to perform interventions to

1249
00:57:48,562 --> 00:57:51,324
learn a structure from data and then

1250
00:57:51,362 --> 00:57:52,972
perform interventions and

1251
00:57:53,026 --> 00:57:54,300
counterfactuals.

1252
00:57:56,560 --> 00:57:58,316
So causal inference in natural and

1253
00:57:58,338 --> 00:57:59,996
efficiently model interventions by

1254
00:58:00,018 --> 00:58:01,536
simply setting the prediction error to

1255
00:58:01,558 --> 00:58:04,704
zero. So it's a very easy technique to

1256
00:58:04,742 --> 00:58:07,008
perform interventions and you only have

1257
00:58:07,014 --> 00:58:08,416
to touch one neuron. You don't have to

1258
00:58:08,438 --> 00:58:10,020
act on the structure of the graph,

1259
00:58:11,480 --> 00:58:15,024
you can use it to create structure

1260
00:58:15,072 --> 00:58:16,544
causal models that are biologically

1261
00:58:16,592 --> 00:58:19,636
plausible. It is able to learn the

1262
00:58:19,658 --> 00:58:23,608
structure from data, as I said, maybe a

1263
00:58:23,614 --> 00:58:27,224
lot of times already and

1264
00:58:27,262 --> 00:58:29,080
a couple of sentences about future works

1265
00:58:29,150 --> 00:58:32,344
is that something that would be nice

1266
00:58:32,382 --> 00:58:34,844
to do is to improve the performance of

1267
00:58:34,882 --> 00:58:37,996
the model we have defined. Because I

1268
00:58:38,018 --> 00:58:40,556
think it performs reasonably well on a

1269
00:58:40,578 --> 00:58:42,472
lot of tasks. So it performs reasonably

1270
00:58:42,536 --> 00:58:45,656
well on structured learning, on forming

1271
00:58:45,688 --> 00:58:47,996
intervention and counterfactuals. But

1272
00:58:48,018 --> 00:58:49,248
actually, if you look at state of the

1273
00:58:49,254 --> 00:58:51,312
art model, there's always like a very

1274
00:58:51,366 --> 00:58:54,016
specific method that performs better in

1275
00:58:54,038 --> 00:58:57,168
the single task. So it would be

1276
00:58:57,174 --> 00:58:58,656
interesting to see if we can reach those

1277
00:58:58,678 --> 00:59:02,400
level of performance in specific tasks

1278
00:59:02,560 --> 00:59:07,444
by adding some tricks or

1279
00:59:07,482 --> 00:59:10,196
some new optimization methods and to

1280
00:59:10,218 --> 00:59:12,936
generalize it to dynamical systems which

1281
00:59:12,958 --> 00:59:14,456
are actually much more interesting the

1282
00:59:14,478 --> 00:59:17,588
static systems such as dynamical causal

1283
00:59:17,604 --> 00:59:20,984
models or other techniques that allow

1284
00:59:21,022 --> 00:59:23,544
you to perform causal inference in

1285
00:59:23,582 --> 00:59:26,716
systems that move. So an action taken in

1286
00:59:26,738 --> 00:59:29,132
a specific time step influences another

1287
00:59:29,186 --> 00:59:31,740
node in a later time step, which is

1288
00:59:31,810 --> 00:59:33,500
basically granger causality.

1289
00:59:34,880 --> 00:59:38,464
Yeah, that's it. And thank

1290
00:59:38,502 --> 00:59:39,410
you very much.

1291
00:59:47,540 --> 00:59:50,256
Thank you. Awesome and very

1292
00:59:50,358 --> 00:59:52,496
comprehensive presentation. That was

1293
00:59:52,518 --> 00:59:54,000
really I think you're muted.

1294
00:59:57,000 --> 00:59:59,444
Sorry, muted on zoom. But yes, thanks

1295
00:59:59,482 --> 01:00:01,780
for the awesome and very comprehensive

1296
01:00:02,360 --> 01:00:05,012
presentation. There was really a lot

1297
01:00:05,066 --> 01:00:06,980
there, and there was also a lot of great

1298
01:00:07,050 --> 01:00:09,832
questions in the live chat. So maybe to

1299
01:00:09,886 --> 01:00:12,344
warm into the questions, how did you

1300
01:00:12,382 --> 01:00:15,496
come to study this topic? Were you

1301
01:00:15,518 --> 01:00:17,972
studying causality and found predictive

1302
01:00:18,036 --> 01:00:20,536
coding to be useful or vice versa? Or

1303
01:00:20,558 --> 01:00:22,300
how did you come at this intersection?

1304
01:00:24,240 --> 01:00:25,564
Actually, I have to say that the first

1305
01:00:25,602 --> 01:00:27,644
person that came out with this idea was

1306
01:00:27,682 --> 01:00:28,460
Baron,

1307
01:00:31,840 --> 01:00:34,652
I think a year and a half ago, even

1308
01:00:34,706 --> 01:00:37,968
more, he brought a page with this idea

1309
01:00:38,134 --> 01:00:40,176
and then it got forgotten and no one

1310
01:00:40,198 --> 01:00:43,664
picked it up. And last summer I started

1311
01:00:43,702 --> 01:00:48,976
getting curious about causality and I

1312
01:00:48,998 --> 01:00:50,304
read, for example, the Book of Why,

1313
01:00:50,342 --> 01:00:52,292
after listening to podcasts, the

1314
01:00:52,346 --> 01:00:53,620
standard way in which you get interested

1315
01:00:53,690 --> 01:00:57,092
in a topic. And I remember this idea

1316
01:00:57,146 --> 01:01:00,068
from Baron and proposed it to him, and I

1317
01:01:00,074 --> 01:01:03,544
was like, why don't we expand it and

1318
01:01:03,582 --> 01:01:06,244
actually make it a paper? So I involved

1319
01:01:06,292 --> 01:01:08,228
some people to help me with experiments,

1320
01:01:08,404 --> 01:01:10,650
and this is the final result at the end.

1321
01:01:12,060 --> 01:01:15,896
Awesome. Cool. Yeah, a lot to

1322
01:01:15,918 --> 01:01:17,484
say. I'm just going to go to the live

1323
01:01:17,522 --> 01:01:19,516
chat first and address a bunch of

1324
01:01:19,538 --> 01:01:21,196
different questions. And if anybody else

1325
01:01:21,218 --> 01:01:22,188
wants to add. I'm going to turn the

1326
01:01:22,194 --> 01:01:23,676
light on first because I think I'm

1327
01:01:23,708 --> 01:01:26,690
getting in the dark more and more. Yes.

1328
01:01:28,340 --> 01:01:30,316
Who said active inference can't solve

1329
01:01:30,348 --> 01:01:33,456
the dark room issue? Oh, yes, here we

1330
01:01:33,478 --> 01:01:36,512
are. So would you say the light switch

1331
01:01:36,576 --> 01:01:38,420
caused it to be lighter?

1332
01:01:39,400 --> 01:01:42,980
Yeah, I think so. No issues

1333
01:01:43,050 --> 01:01:46,580
here. Okay. ML. Don wrote,

1334
01:01:47,480 --> 01:01:49,592
since in predictive coding all

1335
01:01:49,646 --> 01:01:51,944
distributions are usually Gaussian, the

1336
01:01:51,982 --> 01:01:53,796
bottom up messages are precision

1337
01:01:53,828 --> 01:01:55,464
weighted prediction errors, where

1338
01:01:55,502 --> 01:01:57,364
precision is the inverse of the Gaussian

1339
01:01:57,412 --> 01:02:00,064
covariance. What if non Gaussian

1340
01:02:00,132 --> 01:02:01,710
distributions are used?

1341
01:02:05,280 --> 01:02:08,220
Basically, the general method stays.

1342
01:02:09,040 --> 01:02:11,496
The main difference is that you don't

1343
01:02:11,528 --> 01:02:14,336
have prediction errors, which, as was

1344
01:02:14,358 --> 01:02:16,896
correctly pointed out, is basically the

1345
01:02:16,918 --> 01:02:19,296
derivative of the variation of free

1346
01:02:19,318 --> 01:02:21,680
energy. If you have Gaussian assumptions

1347
01:02:22,900 --> 01:02:24,432
yeah. You don't have that single

1348
01:02:24,486 --> 01:02:27,620
quantity to set to zero and you probably

1349
01:02:27,690 --> 01:02:29,796
will have to act on the structure of the

1350
01:02:29,818 --> 01:02:32,580
graph to perform interventions.

1351
01:02:33,960 --> 01:02:37,220
And also, you and colleagues had a paper

1352
01:02:37,290 --> 01:02:39,844
in 2022 predictive coding beyond

1353
01:02:39,892 --> 01:02:41,896
gaussian distributions that looked at

1354
01:02:41,918 --> 01:02:44,456
some of these issues, right? Yes,

1355
01:02:44,558 --> 01:02:46,890
exactly. So that paper was a little bit

1356
01:02:47,260 --> 01:02:51,132
the idea behind that paper is can we

1357
01:02:51,186 --> 01:02:53,384
model transformers? That's the biggest

1358
01:02:53,432 --> 01:02:55,564
motivation using predictive coding. And

1359
01:02:55,602 --> 01:02:58,712
the answer is no, because the attention

1360
01:02:58,776 --> 01:03:00,924
mechanisms are softmax at the end and

1361
01:03:00,962 --> 01:03:04,784
softmax calls not to

1362
01:03:04,822 --> 01:03:09,676
Gaussian distribution, but to softmax

1363
01:03:09,708 --> 01:03:11,856
distribution. I don't get the name now,

1364
01:03:11,878 --> 01:03:15,396
but yes. So yes, that's a

1365
01:03:15,418 --> 01:03:18,176
generalization. It's a little bit tricky

1366
01:03:18,208 --> 01:03:19,904
to call it once you remove the Gaussian

1367
01:03:19,952 --> 01:03:21,364
assumption, it's a little bit still

1368
01:03:21,402 --> 01:03:23,780
tricky to call it predictive coding.

1369
01:03:26,840 --> 01:03:29,648
For example, like talking to Karl

1370
01:03:29,664 --> 01:03:33,370
Friedston, predictive coding is only

1371
01:03:33,820 --> 01:03:36,440
if you have only Gaussian assumptions.

1372
01:03:37,740 --> 01:03:39,732
But yes, that's more a philosophical

1373
01:03:39,796 --> 01:03:40,970
debate than.

1374
01:03:42,720 --> 01:03:44,856
Interesting. Another, I think topic

1375
01:03:44,888 --> 01:03:47,980
that's definitely of great interest is

1376
01:03:48,130 --> 01:03:50,700
similarities and differences between the

1377
01:03:50,770 --> 01:03:54,704
attention apparatus in Transformers and

1378
01:03:54,742 --> 01:03:57,264
the way that attention is described from

1379
01:03:57,302 --> 01:03:59,648
a neurocognitive perspective and from a

1380
01:03:59,654 --> 01:04:01,884
predictive processing precision

1381
01:04:01,932 --> 01:04:03,904
weighting angle. What do you think about

1382
01:04:03,942 --> 01:04:07,344
that? Well, the idea is

1383
01:04:07,382 --> 01:04:10,870
that I think about it is that

1384
01:04:11,640 --> 01:04:14,852
from a predictive processing and also

1385
01:04:14,906 --> 01:04:16,608
vocational inference perspective,

1386
01:04:16,784 --> 01:04:19,176
attention can be seen as a kind of

1387
01:04:19,198 --> 01:04:21,336
structured learning problem. I think

1388
01:04:21,358 --> 01:04:23,684
there's a recent paper from Chris

1389
01:04:23,732 --> 01:04:27,496
Buckley's group that shows that there

1390
01:04:27,518 --> 01:04:30,024
should be a reprint on Archive in which

1391
01:04:30,062 --> 01:04:31,304
basically they showed that the attention

1392
01:04:31,352 --> 01:04:34,844
mechanism is simply learning the

1393
01:04:34,882 --> 01:04:37,420
precision on the weight parameters

1394
01:04:37,760 --> 01:04:40,284
specific to a data point. So this

1395
01:04:40,322 --> 01:04:43,788
precision is not a parameter that is in

1396
01:04:43,794 --> 01:04:45,776
the structure of the model. So it's not

1397
01:04:45,798 --> 01:04:47,728
a model specific parameter, but it's a

1398
01:04:47,734 --> 01:04:49,264
fast changing parameter like the value

1399
01:04:49,302 --> 01:04:52,156
nodes that gets updated while minimizing

1400
01:04:52,188 --> 01:04:54,576
the vibrational free energy. And once

1401
01:04:54,598 --> 01:04:56,044
you have minimized it and computed,

1402
01:04:56,092 --> 01:04:57,684
then you throw it away and from the next

1403
01:04:57,722 --> 01:04:59,684
data point you have to recompute it from

1404
01:04:59,722 --> 01:05:03,376
scratch. So yes, I think the analogy

1405
01:05:03,488 --> 01:05:05,744
computation wise is the attention

1406
01:05:05,792 --> 01:05:07,988
mechanism can be seen as a kind of

1407
01:05:08,074 --> 01:05:10,356
structured learning, but a structured

1408
01:05:10,388 --> 01:05:12,824
learning that is data point specific and

1409
01:05:12,862 --> 01:05:16,248
not model specific. And I think if you

1410
01:05:16,254 --> 01:05:17,896
want to generalize a little bit and go

1411
01:05:17,918 --> 01:05:20,296
from the attention mechanism in

1412
01:05:20,318 --> 01:05:22,008
transformers to the attention mechanism,

1413
01:05:22,104 --> 01:05:25,016
cognitive science, I feel they're

1414
01:05:25,048 --> 01:05:28,732
probably too different to draw

1415
01:05:28,786 --> 01:05:32,456
similarities. And I think the structured

1416
01:05:32,488 --> 01:05:35,904
learning analogy and how important one

1417
01:05:35,942 --> 01:05:37,792
connection is with respect to another

1418
01:05:37,846 --> 01:05:40,210
one probably does the job much better.

1419
01:05:42,020 --> 01:05:44,450
Cool. Great answer. Okay,

1420
01:05:44,820 --> 01:05:48,576
ML Don asks in counterfactuals

1421
01:05:48,688 --> 01:05:50,544
what is the difference between hidden

1422
01:05:50,592 --> 01:05:54,150
variables X and unobserved variables U?

1423
01:05:55,480 --> 01:06:00,400
The difference is that I

1424
01:06:00,410 --> 01:06:01,604
think the main one is that you cannot

1425
01:06:01,652 --> 01:06:04,744
observe the use. You can use them

1426
01:06:04,782 --> 01:06:07,352
because you can compute them and fix

1427
01:06:07,406 --> 01:06:10,396
them, but the idea is that you have no

1428
01:06:10,418 --> 01:06:13,356
control over them. So the use should be

1429
01:06:13,378 --> 01:06:16,344
seen as environment specific variables

1430
01:06:16,472 --> 01:06:18,764
that they are there, they influence your

1431
01:06:18,802 --> 01:06:22,108
process because for example, when you

1432
01:06:22,114 --> 01:06:23,816
go back in time, the environment is

1433
01:06:23,858 --> 01:06:25,520
different. So the idea is, for example,

1434
01:06:25,590 --> 01:06:28,288
if you like going back to the example

1435
01:06:28,374 --> 01:06:31,328
before of the expected income of a

1436
01:06:31,334 --> 01:06:33,810
person with a specific intelligence of

1437
01:06:35,540 --> 01:06:38,596
education degree. The idea is that if I

1438
01:06:38,618 --> 01:06:41,670
want to see how much I will earn today

1439
01:06:44,040 --> 01:06:46,404
with a master degree is different with

1440
01:06:46,442 --> 01:06:48,772
respect to how much I would earn 20

1441
01:06:48,826 --> 01:06:50,744
years ago with a master degree is

1442
01:06:50,782 --> 01:06:52,356
different. For example, here in Italy

1443
01:06:52,388 --> 01:06:55,096
with respect to other countries and all

1444
01:06:55,118 --> 01:06:56,936
those variables that are not under your

1445
01:06:56,958 --> 01:06:58,584
control. You cannot model them using

1446
01:06:58,622 --> 01:07:01,084
your Bayesian network, but they are

1447
01:07:01,122 --> 01:07:04,636
there. Okay. So you cannot ignore them

1448
01:07:04,738 --> 01:07:07,148
when you want to draw conclusions. So,

1449
01:07:07,234 --> 01:07:08,556
yeah, it's basically everything that you

1450
01:07:08,578 --> 01:07:11,484
cannot control, you can infer them.

1451
01:07:11,522 --> 01:07:14,636
So you can perform a counterfactual

1452
01:07:14,668 --> 01:07:16,672
inference back in time and say, oh, 20

1453
01:07:16,726 --> 01:07:19,330
years ago, I would have earned this much

1454
01:07:20,580 --> 01:07:22,464
if I was this intelligent at this

1455
01:07:22,502 --> 01:07:25,648
degree, on average, of course. But it's

1456
01:07:25,664 --> 01:07:27,476
not that I can change the government

1457
01:07:27,578 --> 01:07:31,364
policies towards jobs or things

1458
01:07:31,402 --> 01:07:34,580
like that. It's a deeper counterfactual.

1459
01:07:35,160 --> 01:07:37,750
Yes, exactly. So those are the use.

1460
01:07:38,380 --> 01:07:41,668
Awesome. All right. Have you implemented

1461
01:07:41,764 --> 01:07:43,844
generalized coordinates in predictive

1462
01:07:43,892 --> 01:07:46,570
coding? No,

1463
01:07:47,660 --> 01:07:51,256
I've never done it. I've studied

1464
01:07:51,288 --> 01:07:53,596
it, but I've never implemented it. I

1465
01:07:53,618 --> 01:07:56,220
know they tend to be unstable,

1466
01:07:57,600 --> 01:07:59,928
and it's very hard to make them stable.

1467
01:08:00,024 --> 01:08:03,552
I think that's the takeaway that I got

1468
01:08:03,606 --> 01:08:05,584
from talking to people that have

1469
01:08:05,622 --> 01:08:06,770
implemented them.

1470
01:08:09,220 --> 01:08:11,504
But yeah, I'm aware of some papers that

1471
01:08:11,542 --> 01:08:13,968
came out, actually recently about them

1472
01:08:14,054 --> 01:08:17,024
that tested on some bracelet, encoder

1473
01:08:17,072 --> 01:08:18,596
style, actually, I think still from

1474
01:08:18,618 --> 01:08:22,484
Baron. There's a paper out there that

1475
01:08:22,522 --> 01:08:24,964
came out last summer. But no, I've never

1476
01:08:25,002 --> 01:08:27,430
played them with them myself. Cool.

1477
01:08:27,740 --> 01:08:31,096
From Bert. Does adding more levels in

1478
01:08:31,118 --> 01:08:34,020
the hierarchy reduce the distraction

1479
01:08:34,100 --> 01:08:36,600
problem of predicting input?

1480
01:08:39,020 --> 01:08:42,600
Adding more level, in which sense

1481
01:08:42,670 --> 01:08:44,444
because the distraction problem is given

1482
01:08:44,482 --> 01:08:46,476
by cycles. So basically you provide an

1483
01:08:46,498 --> 01:08:51,592
image, and the fact that you have edges

1484
01:08:51,656 --> 01:08:53,648
going out of the image going in the

1485
01:08:53,654 --> 01:08:55,904
neurons, and then other edges going

1486
01:08:55,942 --> 01:08:59,424
back, this basically creates the fact

1487
01:08:59,462 --> 01:09:03,744
that basically

1488
01:09:03,942 --> 01:09:06,256
these ingoing edges to the pixels of the

1489
01:09:06,278 --> 01:09:08,304
image, they create some prediction

1490
01:09:08,352 --> 01:09:09,696
errors. So you have some prediction

1491
01:09:09,728 --> 01:09:12,390
errors that get spread inside the model.

1492
01:09:13,480 --> 01:09:15,316
And this problem, I think, is general of

1493
01:09:15,338 --> 01:09:18,272
cycles, and it's probably not related to

1494
01:09:18,346 --> 01:09:19,770
hierarchy in general,

1495
01:09:21,740 --> 01:09:24,760
is the two incoming edges to the pixels.

1496
01:09:25,100 --> 01:09:26,696
If you don't have incoming edges, you

1497
01:09:26,718 --> 01:09:30,120
have no destruction problem anymore.

1498
01:09:30,620 --> 01:09:33,016
Cool. And the specification of the

1499
01:09:33,038 --> 01:09:35,912
asyclic network through the trace

1500
01:09:35,976 --> 01:09:39,870
operator, that's a very interesting

1501
01:09:40,480 --> 01:09:43,704
technique. And when was that brought

1502
01:09:43,752 --> 01:09:44,670
into play?

1503
01:09:46,800 --> 01:09:49,136
As far as I know, I think it came out

1504
01:09:49,158 --> 01:09:53,068
with a paper I cited in 2018. I don't

1505
01:09:53,084 --> 01:09:54,636
know, at least in the causal inference

1506
01:09:54,668 --> 01:09:57,684
literature. I'm not aware of any

1507
01:09:57,722 --> 01:09:59,812
previous methods. I would say no,

1508
01:09:59,866 --> 01:10:02,692
because that's the highly cited paper.

1509
01:10:02,826 --> 01:10:04,596
So I would say they came out with that

1510
01:10:04,618 --> 01:10:07,764
idea. Wow. But yeah, that's quite nice

1511
01:10:07,802 --> 01:10:09,176
that you can do gradient descent and

1512
01:10:09,198 --> 01:10:11,928
learn the structure. I think that's a

1513
01:10:11,934 --> 01:10:14,152
very powerful technique. Yeah,

1514
01:10:14,206 --> 01:10:15,912
sometimes it's like when you look at

1515
01:10:15,966 --> 01:10:18,804
when different features of Bayesian

1516
01:10:18,852 --> 01:10:21,044
inference and causal inference became

1517
01:10:21,092 --> 01:10:25,080
available. It's really remarkable.

1518
01:10:25,820 --> 01:10:27,996
Why hasn't this been done under a

1519
01:10:28,018 --> 01:10:29,544
Bayesian causal modeling framework?

1520
01:10:29,592 --> 01:10:32,172
It's like because there's only been like

1521
01:10:32,226 --> 01:10:36,050
five to 25 years of this happening,

1522
01:10:36,660 --> 01:10:40,316
and so that's very short. And also it's

1523
01:10:40,348 --> 01:10:41,996
relatively technical, so there's

1524
01:10:42,028 --> 01:10:43,836
relatively few research groups engaging

1525
01:10:43,868 --> 01:10:47,796
in it. And it's just really cool what

1526
01:10:47,818 --> 01:10:51,268
it's enabling. Yes, exactly. I mean,

1527
01:10:51,274 --> 01:10:53,316
that's also, I think, the exciting part

1528
01:10:53,338 --> 01:10:55,028
of this field a little bit that is, I

1529
01:10:55,034 --> 01:10:57,200
mean, there are definitely breakthroughs

1530
01:10:57,280 --> 01:11:00,056
out there that still have to be

1531
01:11:00,078 --> 01:11:01,768
discovered and probably because, for

1532
01:11:01,774 --> 01:11:04,436
example, for as much as a breakthrough

1533
01:11:04,468 --> 01:11:05,690
that paper was,

1534
01:11:08,380 --> 01:11:10,616
they simply found the right prior for

1535
01:11:10,638 --> 01:11:12,890
acyclic structures. Okay,

1536
01:11:14,060 --> 01:11:17,276
yeah, I don't know exactly, but it may

1537
01:11:17,298 --> 01:11:19,004
be an idea that you have in one

1538
01:11:19,042 --> 01:11:21,052
afternoon. I don't know about the story

1539
01:11:21,106 --> 01:11:23,180
of how the authors came up with that,

1540
01:11:23,250 --> 01:11:25,768
but could potentially be that they're

1541
01:11:25,784 --> 01:11:27,296
there at the whiteboard. You're like,

1542
01:11:27,318 --> 01:11:29,424
oh, that actually works. That's a huge

1543
01:11:29,462 --> 01:11:32,848
breakthrough. And I simply defined the

1544
01:11:32,854 --> 01:11:35,492
prior. And also, a lot of these

1545
01:11:35,546 --> 01:11:39,328
breakthroughs, they don't just stack.

1546
01:11:39,504 --> 01:11:42,804
It's not like a tower of

1547
01:11:42,842 --> 01:11:46,340
blocks. They layer and they compose.

1548
01:11:46,840 --> 01:11:49,352
So then something will be generalized to

1549
01:11:49,486 --> 01:11:51,076
generalized coordinates or generalized

1550
01:11:51,108 --> 01:11:53,892
synchrony, or arbitrarily large graphs

1551
01:11:53,956 --> 01:11:57,156
or sensor fusion with multimodal inputs.

1552
01:11:57,188 --> 01:12:00,628
And it's like those all blend in really

1553
01:12:00,814 --> 01:12:03,612
satisfying and effective ways. So even

1554
01:12:03,666 --> 01:12:05,516
little things that, again, someone can

1555
01:12:05,538 --> 01:12:08,476
just come up with in a moment can really

1556
01:12:08,498 --> 01:12:09,500
have impact.

1557
01:12:11,360 --> 01:12:14,016
Okay, ML. Don says, thanks a lot for

1558
01:12:14,038 --> 01:12:15,728
asking my questions, and thanks a

1559
01:12:15,734 --> 01:12:17,516
million to Tomaso for the inspiring

1560
01:12:17,548 --> 01:12:20,224
presentation. So nice. Thank you very

1561
01:12:20,262 --> 01:12:24,128
much. And then Bert asks, how would

1562
01:12:24,214 --> 01:12:26,464
language models using predictive coding

1563
01:12:26,512 --> 01:12:28,660
differ from those using transformers?

1564
01:12:32,600 --> 01:12:35,268
Okay, I think that actually, if I would

1565
01:12:35,274 --> 01:12:36,740
have to build today a language model

1566
01:12:36,810 --> 01:12:38,376
using predictive coding, I would still

1567
01:12:38,398 --> 01:12:40,936
use transformers. So the idea is that,

1568
01:12:40,958 --> 01:12:43,304
for example, if you have, let's say,

1569
01:12:43,342 --> 01:12:46,824
this hierarchical graphical model or

1570
01:12:46,862 --> 01:12:50,040
this hierarchical Bayesian network,

1571
01:12:50,460 --> 01:12:53,656
I've defined in the very first slides

1572
01:12:53,768 --> 01:12:55,756
one arrow to encode a function, which is

1573
01:12:55,778 --> 01:12:58,636
the linear map. Okay? So one arrow was

1574
01:12:58,658 --> 01:13:00,984
simply the multiplication of the vector

1575
01:13:01,032 --> 01:13:03,310
encoded in the latent variables times

1576
01:13:03,920 --> 01:13:06,256
this weight matrix that you can then

1577
01:13:06,278 --> 01:13:08,208
make nonlinear and things like that.

1578
01:13:08,294 --> 01:13:09,664
But that can be actually something much

1579
01:13:09,702 --> 01:13:12,048
more complex. The function encoded in

1580
01:13:12,054 --> 01:13:14,544
the arrow can be a convolution, can be

1581
01:13:14,582 --> 01:13:18,528
an attention. So actually,

1582
01:13:18,614 --> 01:13:22,530
how I would do it, I will still use the

1583
01:13:23,080 --> 01:13:25,828
which is actually the way we did it in

1584
01:13:25,834 --> 01:13:28,564
the Oxford group last year, is that we

1585
01:13:28,602 --> 01:13:30,404
had exactly the structure. Every arrow

1586
01:13:30,452 --> 01:13:33,048
is a transformer now. So one is the

1587
01:13:33,054 --> 01:13:34,936
attention mechanism and the next one is

1588
01:13:34,958 --> 01:13:36,744
the feed forward network. As

1589
01:13:36,782 --> 01:13:39,496
transformers. Basically, the only

1590
01:13:39,518 --> 01:13:40,856
difference that you have is that those

1591
01:13:40,878 --> 01:13:42,156
variables, you want to compute the

1592
01:13:42,178 --> 01:13:45,048
posterior and you make those posteriors

1593
01:13:45,224 --> 01:13:48,280
independent via minfield approximation.

1594
01:13:48,440 --> 01:13:50,216
So basically, you follow all the steps

1595
01:13:50,248 --> 01:13:53,088
that allow you to converge to the

1596
01:13:53,094 --> 01:13:54,444
variation of. Free energy of creative

1597
01:13:54,492 --> 01:13:57,516
coding. But the way you compute

1598
01:13:57,548 --> 01:13:59,932
predictions and the way you send signals

1599
01:13:59,996 --> 01:14:03,920
back is done via transformer.

1600
01:14:04,740 --> 01:14:07,488
So I will still use transformers in

1601
01:14:07,494 --> 01:14:09,990
general. I mean, they work so well that

1602
01:14:10,600 --> 01:14:12,736
I don't think that we can be arrogant

1603
01:14:12,768 --> 01:14:14,004
and say, oh no, I'm going to do it

1604
01:14:14,042 --> 01:14:16,336
better via a purely predictive coding

1605
01:14:16,368 --> 01:14:19,796
way. Structure learning is a way, but

1606
01:14:19,818 --> 01:14:21,424
will still approximate transformers

1607
01:14:21,472 --> 01:14:23,660
anyway. Sorry, you said structured

1608
01:14:23,680 --> 01:14:25,096
learning would approximate the

1609
01:14:25,118 --> 01:14:28,356
transformer approach. Yes, the structure

1610
01:14:28,388 --> 01:14:31,384
learning I mentioned earlier when

1611
01:14:31,422 --> 01:14:33,816
someone asked the similarities between

1612
01:14:33,918 --> 01:14:35,544
creative coding and the attention

1613
01:14:35,592 --> 01:14:36,380
mechanism.

1614
01:14:39,360 --> 01:14:40,750
Yeah, very interesting.

1615
01:14:42,880 --> 01:14:45,480
One thing I am wondering from ML Don, I

1616
01:14:45,490 --> 01:14:47,456
could not see the concept of depth in

1617
01:14:47,478 --> 01:14:49,296
the predictive coding networks you

1618
01:14:49,318 --> 01:14:50,816
mentioned. Most likely I missed it. The

1619
01:14:50,838 --> 01:14:52,316
definition provided for predictive

1620
01:14:52,348 --> 01:14:56,100
coding involved the concept of depth.

1621
01:14:56,600 --> 01:15:00,004
What did you mean by depth? No. Yes,

1622
01:15:00,042 --> 01:15:02,660
it's true because the standard

1623
01:15:02,730 --> 01:15:05,444
definition, as I said multiple times,

1624
01:15:05,562 --> 01:15:07,344
is hierarchical. You have predictions

1625
01:15:07,392 --> 01:15:09,024
going one direction and prediction error

1626
01:15:09,072 --> 01:15:10,440
going the opposite direction.

1627
01:15:10,780 --> 01:15:14,184
Basically, what we did in this

1628
01:15:14,222 --> 01:15:17,064
paper and also in the last one, which is

1629
01:15:17,102 --> 01:15:19,028
called learning on arbitrary graph

1630
01:15:19,044 --> 01:15:21,688
topologies via rated coding, is that we

1631
01:15:21,694 --> 01:15:26,004
can consider depth as

1632
01:15:26,062 --> 01:15:30,108
independent, basically pair of

1633
01:15:30,194 --> 01:15:31,964
latent variable, latent variable and

1634
01:15:32,002 --> 01:15:34,476
arrow. And you have predictions going

1635
01:15:34,498 --> 01:15:36,068
that direction and prediction arrow

1636
01:15:36,104 --> 01:15:37,856
going the other. But then you can

1637
01:15:37,878 --> 01:15:41,490
compose these in a lot of ways.

1638
01:15:44,180 --> 01:15:46,508
So basically this composition doesn't

1639
01:15:46,524 --> 01:15:48,964
have to be hierarchical in the end, can

1640
01:15:49,002 --> 01:15:50,676
have cycles. So then you can, for

1641
01:15:50,698 --> 01:15:54,144
example, plug in another latent variable

1642
01:15:54,192 --> 01:15:55,908
to the first one and then connect the

1643
01:15:55,914 --> 01:15:58,160
other two. And you can have a structure

1644
01:15:58,240 --> 01:16:00,688
that is as entangled as you want. So,

1645
01:16:00,714 --> 01:16:03,144
for example, in the other paper, we

1646
01:16:03,182 --> 01:16:06,456
trained a network that has the shape of

1647
01:16:06,478 --> 01:16:08,376
a brain structure. So we have a lot of

1648
01:16:08,398 --> 01:16:09,876
brain regions that are sparsely

1649
01:16:09,908 --> 01:16:12,616
connected inside and sparsely connected

1650
01:16:12,648 --> 01:16:15,612
among each other. And there's nothing

1651
01:16:15,666 --> 01:16:17,388
hierarchical there at the end. But you

1652
01:16:17,394 --> 01:16:18,776
can still train it by minimizing

1653
01:16:18,808 --> 01:16:20,604
abbreviation of free energy and by

1654
01:16:20,642 --> 01:16:23,008
minimizing the total prediction error of

1655
01:16:23,014 --> 01:16:26,256
the network. So you

1656
01:16:26,278 --> 01:16:30,544
could have for a given motif in

1657
01:16:30,582 --> 01:16:33,952
an entangled graph, you might see three

1658
01:16:34,006 --> 01:16:36,656
successive layers that when you looked

1659
01:16:36,678 --> 01:16:37,988
at them alone, you'd say, oh, that's a

1660
01:16:37,994 --> 01:16:40,244
three story building. That's a three

1661
01:16:40,282 --> 01:16:42,708
layer model that has a depth of three.

1662
01:16:42,794 --> 01:16:44,960
But then when you take a bigger picture,

1663
01:16:45,120 --> 01:16:48,216
there isn't like an explicit top or an

1664
01:16:48,238 --> 01:16:51,160
explicit bottom to that network.

1665
01:16:52,140 --> 01:16:53,832
Yes, exactly. And this is basically

1666
01:16:53,886 --> 01:16:55,796
given by the fact that every operation

1667
01:16:55,828 --> 01:16:57,784
in predictive coding networks is

1668
01:16:57,822 --> 01:17:00,824
strictly local. So basically every

1669
01:17:00,862 --> 01:17:02,348
message passing, every prediction and

1670
01:17:02,354 --> 01:17:03,964
every prediction error that you send,

1671
01:17:04,082 --> 01:17:06,312
you only send it to the very nearby

1672
01:17:06,376 --> 01:17:08,824
neurons. Okay? And whether the global

1673
01:17:08,872 --> 01:17:11,116
structure is actually hierarchical or

1674
01:17:11,138 --> 01:17:14,348
not, the single message passing doesn't

1675
01:17:14,364 --> 01:17:18,396
even see that. I guess that's

1676
01:17:18,428 --> 01:17:21,776
sort of the hope for learning.

1677
01:17:21,878 --> 01:17:25,100
New model architectures is

1678
01:17:25,190 --> 01:17:28,720
the space of what is designed.

1679
01:17:28,800 --> 01:17:32,724
Top down is very small and

1680
01:17:32,842 --> 01:17:36,144
a lot of models in use today, albeit

1681
01:17:36,192 --> 01:17:37,860
super effective models,

1682
01:17:38,840 --> 01:17:41,080
although you could ask effective per

1683
01:17:41,150 --> 01:17:43,320
unit of compute or not, that's a second

1684
01:17:43,390 --> 01:17:45,156
level question. But a lot of effective

1685
01:17:45,188 --> 01:17:47,464
models today do not have some of these

1686
01:17:47,502 --> 01:17:50,016
properties of predictive coding networks

1687
01:17:50,148 --> 01:17:54,396
like their capacity to use only local

1688
01:17:54,498 --> 01:17:57,784
computations which gives biological

1689
01:17:57,832 --> 01:18:01,624
realism or just spatiotemporal

1690
01:18:01,672 --> 01:18:04,976
realism, but also may provide a lot of

1691
01:18:04,998 --> 01:18:07,456
advantages in federated compute or

1692
01:18:07,478 --> 01:18:10,864
distributed computing settings. No?

1693
01:18:10,902 --> 01:18:13,248
Yes, exactly. I completely agree. I

1694
01:18:13,254 --> 01:18:15,776
think the idea in general is that and I

1695
01:18:15,798 --> 01:18:16,836
don't know if that's going to be an

1696
01:18:16,858 --> 01:18:18,864
advantage, I think it's very promising

1697
01:18:18,912 --> 01:18:21,956
exactly for the reasons you said. And

1698
01:18:21,978 --> 01:18:23,376
the reason is that today's models

1699
01:18:23,408 --> 01:18:25,444
trained with backpropagation, you can

1700
01:18:25,482 --> 01:18:29,444
basically summarize them as

1701
01:18:29,482 --> 01:18:32,168
a model train with backpropagation is a

1702
01:18:32,174 --> 01:18:33,928
function because basically you have a

1703
01:18:33,934 --> 01:18:35,832
map from input to output and

1704
01:18:35,886 --> 01:18:39,220
backpropagation basically spreads

1705
01:18:39,380 --> 01:18:41,672
information back from its computational

1706
01:18:41,736 --> 01:18:44,684
graph. So every neural network model

1707
01:18:44,722 --> 01:18:47,852
used today is a function while

1708
01:18:47,906 --> 01:18:50,344
predictive coding and other liberative

1709
01:18:50,392 --> 01:18:52,980
coding, like the old class of functions,

1710
01:18:53,160 --> 01:18:56,112
class of methods that train using local

1711
01:18:56,166 --> 01:18:58,416
computations and actually work by

1712
01:18:58,438 --> 01:19:00,610
minimizing a global energy function.

1713
01:19:01,460 --> 01:19:03,756
They're not limited to model functions

1714
01:19:03,788 --> 01:19:05,584
from input to output. They actually

1715
01:19:05,622 --> 01:19:07,688
model something that kind of resembles

1716
01:19:07,724 --> 01:19:10,064
physical systems. So you have a physical

1717
01:19:10,112 --> 01:19:13,652
system, you fix some values to whatever

1718
01:19:13,706 --> 01:19:15,460
input you have and you let the system

1719
01:19:15,530 --> 01:19:17,750
converge and then you read some other

1720
01:19:18,600 --> 01:19:20,324
neurons or variables that are supposed

1721
01:19:20,362 --> 01:19:22,724
to be output. But this physical system

1722
01:19:22,842 --> 01:19:24,820
doesn't have to be a fit forward map,

1723
01:19:24,900 --> 01:19:26,920
doesn't have to be a function that has

1724
01:19:27,070 --> 01:19:29,368
an input space and an output space and

1725
01:19:29,374 --> 01:19:32,100
that's it. So the class of models that

1726
01:19:32,110 --> 01:19:34,636
you can learn so basically you can see

1727
01:19:34,658 --> 01:19:37,592
like feed forward models and functions

1728
01:19:37,656 --> 01:19:39,516
and then a much bigger class which is

1729
01:19:39,538 --> 01:19:41,532
that of physical systems. Whether

1730
01:19:41,586 --> 01:19:43,356
there's something interesting out here,

1731
01:19:43,458 --> 01:19:45,580
I don't know yet because the functions

1732
01:19:45,660 --> 01:19:47,216
are working extremely well. We are

1733
01:19:47,238 --> 01:19:49,440
seeing those days with back propagation.

1734
01:19:49,940 --> 01:19:53,008
They work crazy well. I don't know if

1735
01:19:53,014 --> 01:19:54,944
there's anything interesting in the big

1736
01:19:54,982 --> 01:19:57,492
part, but the big part is quite big.

1737
01:19:57,546 --> 01:20:00,356
Okay. There are a lot of models that you

1738
01:20:00,378 --> 01:20:02,596
cannot train with back propagation and

1739
01:20:02,618 --> 01:20:05,044
you can train with creative coding or.

1740
01:20:05,082 --> 01:20:07,460
Evidence propagation or other methods

1741
01:20:07,900 --> 01:20:10,408
that is super interesting. Certainly

1742
01:20:10,494 --> 01:20:12,676
biological systems, physical systems

1743
01:20:12,708 --> 01:20:14,730
solve all kinds of interesting problems,

1744
01:20:17,100 --> 01:20:19,992
but there's still no free lunch. An ant

1745
01:20:20,046 --> 01:20:21,676
species that does really well in this

1746
01:20:21,698 --> 01:20:23,356
environment might not do very well in

1747
01:20:23,378 --> 01:20:26,556
another environment. And so out there in

1748
01:20:26,578 --> 01:20:29,356
the hinterlands there might be some

1749
01:20:29,458 --> 01:20:33,108
really unique special algorithms

1750
01:20:33,304 --> 01:20:36,736
that are not well described by being a

1751
01:20:36,758 --> 01:20:40,800
function yet still provide

1752
01:20:40,870 --> 01:20:45,176
like a procedural way to implement

1753
01:20:45,228 --> 01:20:48,112
Heuristics which might be extremely,

1754
01:20:48,176 --> 01:20:51,444
extremely effective. No?

1755
01:20:51,482 --> 01:20:54,436
Yes, exactly. And I think this has been

1756
01:20:54,458 --> 01:20:57,476
most of my focus of research during my

1757
01:20:57,498 --> 01:21:00,328
PhD. For example, like finding this

1758
01:21:00,414 --> 01:21:02,344
application that is like out here and

1759
01:21:02,382 --> 01:21:05,080
not inside the functions.

1760
01:21:07,180 --> 01:21:10,684
Cool. Well, where does this work

1761
01:21:10,802 --> 01:21:13,676
go from here, like what directions are

1762
01:21:13,698 --> 01:21:16,092
you excited about and how do you see

1763
01:21:16,146 --> 01:21:18,568
people in the octave inference ecosystem

1764
01:21:18,664 --> 01:21:20,830
getting involved in this type of work.

1765
01:21:22,480 --> 01:21:25,532
I think probably the most promising

1766
01:21:25,676 --> 01:21:29,312
direction, which is something maybe

1767
01:21:29,366 --> 01:21:31,376
I would like to explore a little bit,

1768
01:21:31,478 --> 01:21:33,872
is to, as I said earlier, is to go

1769
01:21:33,926 --> 01:21:36,364
behind statical models. So everything

1770
01:21:36,422 --> 01:21:39,712
I've shown so far is about static

1771
01:21:39,776 --> 01:21:42,564
data. So the data don't change over

1772
01:21:42,602 --> 01:21:44,868
time. There's no time inside the

1773
01:21:45,034 --> 01:21:47,028
definition of creative coding as it is,

1774
01:21:47,114 --> 01:21:49,716
as I presented it here. However, you

1775
01:21:49,738 --> 01:21:51,148
can, for example, generalize creative

1776
01:21:51,184 --> 01:21:54,184
coding to work with temporal data using

1777
01:21:54,302 --> 01:21:55,768
generalized coordinates, as you

1778
01:21:55,774 --> 01:21:59,236
mentioned earlier, by presenting

1779
01:21:59,268 --> 01:22:03,390
it as a Kalman filter generative model.

1780
01:22:05,360 --> 01:22:07,916
And that's where, for example, the

1781
01:22:07,938 --> 01:22:09,756
causal inference direction could be very

1782
01:22:09,778 --> 01:22:13,184
useful because at that point

1783
01:22:13,302 --> 01:22:15,196
maybe you can be able to model greater

1784
01:22:15,228 --> 01:22:19,264
causality and more complex and

1785
01:22:19,462 --> 01:22:23,276
useful dynamical causal models,

1786
01:22:23,308 --> 01:22:26,176
basically because in general, the

1787
01:22:26,198 --> 01:22:28,484
Ducalculus and interventional and

1788
01:22:28,522 --> 01:22:32,564
counterfactual branch of science is

1789
01:22:32,602 --> 01:22:35,540
mostly developed on small models.

1790
01:22:38,540 --> 01:22:41,364
You don't do interventions on gigantic

1791
01:22:41,412 --> 01:22:43,896
models in general. So if you look at

1792
01:22:43,918 --> 01:22:47,320
medical data, they use relatively small

1793
01:22:47,470 --> 01:22:50,648
Bayesian networks. But of course, if you

1794
01:22:50,654 --> 01:22:52,600
want to have a dynamical causal model

1795
01:22:52,750 --> 01:22:55,948
that models a specific environment or a

1796
01:22:55,954 --> 01:22:57,916
specific reality, you have a lot of

1797
01:22:57,938 --> 01:22:59,688
neurons inside, you have a lot of latent

1798
01:22:59,704 --> 01:23:01,756
variables, they change over time and an

1799
01:23:01,778 --> 01:23:04,988
intervention at some moment creates an

1800
01:23:04,994 --> 01:23:07,264
effect in a different time step. So

1801
01:23:07,302 --> 01:23:08,864
maybe in the next time step in ten

1802
01:23:08,902 --> 01:23:11,216
different time steps later. And I think

1803
01:23:11,238 --> 01:23:12,944
that would be very interesting to

1804
01:23:12,982 --> 01:23:15,384
develop like a biologically plausible

1805
01:23:15,452 --> 01:23:18,564
way of passing information that is also

1806
01:23:18,602 --> 01:23:21,184
able to model grandeur causality.

1807
01:23:21,232 --> 01:23:21,830
Basically.

1808
01:23:24,680 --> 01:23:27,940
Where do you see action in these models?

1809
01:23:30,780 --> 01:23:32,360
Where do I see action?

1810
01:23:33,900 --> 01:23:37,208
I didn't think of that. I think

1811
01:23:37,294 --> 01:23:39,608
I see actions in those models maybe in

1812
01:23:39,614 --> 01:23:41,976
the same way as you see in other models

1813
01:23:42,008 --> 01:23:44,476
because creative coding is basically a

1814
01:23:44,498 --> 01:23:48,332
model of perception. So an action is

1815
01:23:48,386 --> 01:23:49,996
you can see it as a consequence of what

1816
01:23:50,018 --> 01:23:53,788
you are experiencing. So by changing

1817
01:23:53,884 --> 01:23:56,688
the way you're experiencing something,

1818
01:23:56,854 --> 01:23:59,136
then you can compute, maybe you can

1819
01:23:59,158 --> 01:24:01,056
simply perform as smarter actions now

1820
01:24:01,078 --> 01:24:02,210
that you have more information.

1821
01:24:04,420 --> 01:24:08,628
But yeah, I don't think action is very I

1822
01:24:08,634 --> 01:24:11,076
don't see any explicit consequence of

1823
01:24:11,098 --> 01:24:12,996
actions besides the fact that this can

1824
01:24:13,018 --> 01:24:16,496
allow you to basically maybe to simply

1825
01:24:16,528 --> 01:24:19,240
draw better conclusions to then perform

1826
01:24:19,310 --> 01:24:22,536
actions in the future. I'll add

1827
01:24:22,558 --> 01:24:24,696
on to that a few ways that people have

1828
01:24:24,798 --> 01:24:26,504
talked about predictive coding and

1829
01:24:26,542 --> 01:24:30,684
action. First off, internal action or

1830
01:24:30,722 --> 01:24:34,236
covert action is attention. So we can

1831
01:24:34,258 --> 01:24:36,584
think about perception as an internal

1832
01:24:36,632 --> 01:24:38,364
action. That's one approach. Another

1833
01:24:38,402 --> 01:24:41,016
approach, pretty micro, is the outputs

1834
01:24:41,048 --> 01:24:43,328
of a given node. We could understand

1835
01:24:43,414 --> 01:24:46,896
that node as a particular thing with its

1836
01:24:46,918 --> 01:24:49,548
own sensory, cognitive and action

1837
01:24:49,724 --> 01:24:52,716
states. And so in that sense, the output

1838
01:24:52,748 --> 01:24:55,792
of a node. And then lastly, which we

1839
01:24:55,846 --> 01:24:58,352
explored a little bit in Live Stream 43

1840
01:24:58,486 --> 01:25:00,256
on the theoretical review on predictive

1841
01:25:00,288 --> 01:25:02,164
coding. We were reading all the way

1842
01:25:02,202 --> 01:25:03,844
through, and it was all about

1843
01:25:03,882 --> 01:25:05,268
perception. All about perception. And

1844
01:25:05,274 --> 01:25:09,512
then it was like Section 5.3 if you have

1845
01:25:09,566 --> 01:25:13,364
expectations about action, then action

1846
01:25:13,412 --> 01:25:15,992
is just another variable in this

1847
01:25:16,046 --> 01:25:18,468
architecture. And that's really aligned

1848
01:25:18,484 --> 01:25:20,232
with inactive inference, where instead

1849
01:25:20,286 --> 01:25:22,064
of having, like, a reward or utility

1850
01:25:22,132 --> 01:25:24,300
function that we maximize, we select

1851
01:25:24,370 --> 01:25:26,636
action based upon it being the,

1852
01:25:26,658 --> 01:25:28,796
likeliest, course of action, the path of

1853
01:25:28,818 --> 01:25:30,904
least action. That's Bayesian Mechanics.

1854
01:25:30,952 --> 01:25:33,216
And so it's actually very natural to

1855
01:25:33,238 --> 01:25:36,396
bring in an action variable and utilize

1856
01:25:36,428 --> 01:25:41,120
it essentially as if it were a

1857
01:25:41,190 --> 01:25:42,930
prediction about something else

1858
01:25:43,300 --> 01:25:45,472
exterceptibly in the world. Because

1859
01:25:45,526 --> 01:25:49,020
we're also expecting action. No.

1860
01:25:49,110 --> 01:25:51,828
Yes, exactly. No. I like the way of

1861
01:25:51,834 --> 01:25:54,708
defining actions a lot, actually. And I

1862
01:25:54,714 --> 01:25:56,596
still think it has been, like, for

1863
01:25:56,618 --> 01:25:58,884
example, there are not so many papers

1864
01:25:58,932 --> 01:26:01,336
that apply this method. I think there

1865
01:26:01,358 --> 01:26:04,824
are a couple from Alexander Orobria does

1866
01:26:04,862 --> 01:26:07,624
something similar, but in, like,

1867
01:26:07,822 --> 01:26:10,080
outside of the pure active inference,

1868
01:26:10,180 --> 01:26:11,884
like, applying predictive coding and

1869
01:26:11,922 --> 01:26:14,348
actions to solve practical problems

1870
01:26:14,434 --> 01:26:17,550
hasn't been explored a lot.

1871
01:26:19,600 --> 01:26:23,340
Cool. Well, thank you for this excellent

1872
01:26:23,420 --> 01:26:25,056
presentation and discussion. Is there

1873
01:26:25,078 --> 01:26:27,952
anything else that you want to say or

1874
01:26:28,006 --> 01:26:31,280
point people towards? No,

1875
01:26:31,350 --> 01:26:33,584
just a big thank you for inviting me,

1876
01:26:33,622 --> 01:26:35,856
and it was really fun, and I hope to

1877
01:26:35,878 --> 01:26:38,770
come back at some point for some future.

1878
01:26:41,380 --> 01:26:44,716
Anytime. Anytime. Thank you, Thomas.

1879
01:26:44,908 --> 01:26:47,364
Thank you. Daniel. See you. Bye.



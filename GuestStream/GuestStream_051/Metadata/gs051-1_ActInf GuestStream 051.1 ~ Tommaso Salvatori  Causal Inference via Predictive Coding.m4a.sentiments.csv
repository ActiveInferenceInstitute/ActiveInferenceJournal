start	end	speaker	sentiment	confidence	text
19260	20610	A	0.872096598148346	Hello and welcome.
21140	27936	A	0.9119475483894348	It's Active Inference guest stream number 51.1 on July 28, 2023.
28118	39108	A	0.8386095762252808	We are here with Tomaso Salvatore and we will be having a presentation and a discussion on the recent work Causal Inference via Predictive Coding.
39244	42004	A	0.9875057935714722	So thanks so much for joining.
42052	47896	A	0.8060436248779297	For those who are watching Live, feel free to write questions in the live chat and off to you.
47998	48890	A	0.8529649376869202	Thank you.
50300	53370	B	0.9878061413764954	Thank you very much, Daniel, for inviting me.
54320	63070	B	0.9907171130180359	Always been a big fan of the channel and I've been watching a lot of videos, so I'm quite excited to be here and be the one speaking this time.
64260	71730	B	0.8853384256362915	So I'm going to talk about this recent preprint that I put out, which has been the work of the last couple of months.
72260	80800	B	0.8727179169654846	And it's a collaboration with Luca Binketti, Amin, Mccarrak Berenmille and Thomas Lukasiavic.
81700	90500	B	0.8819175958633423	And it's basically a joint work between Verses, which is the company I work for, the University of Oxford and Teovien.
91660	118000	B	0.6660742163658142	So during this talk, this is basically the outline of the talk, I will start talking about what predictive coding is and give an introduction of what it is, a brief historical introduction, why I think it's important to study predictive coding, even, for example, from the machine learning perspective.
118500	124130	B	0.9072321057319641	I will then provide a small intro to what causal inference is.
125700	156520	B	0.8198730945587158	And once we have all those informations together, I will then discuss why I wrote this paper, what was basically the research question that inspired me and the other collaborators and present the main results, which are how to perform inference so intervention and counterfactual inference, and how to learn the causal structures from a given data set using predictive coding.
156680	168400	B	0.7774518728256226	And then I will of course conclude with some small summary and some discussion on why I believe this work can be impactful and some future directions.
170660	173036	B	0.8123028874397278	So what is predative coding?
173228	178704	B	0.643559455871582	predative coding is in general famous for being a neuroscience inspired learning method.
178752	194680	B	0.883397102355957	So a theory of how information processing in the brain works and brain, formally speaking, the theory of creative coding can be described as basically having a hierarchical structure of neurons in the brain.
197020	200280	B	0.717292845249176	And you have two different families of neurons in the brain.
200780	205150	B	0.8551165461540222	The first family is the one in charging of sending prediction information.
205600	215010	B	0.8584517240524292	So neurons in a specific level of the hierarchy send information and predict the activity of the level below.
215940	219580	B	0.7759537696838379	And the second family of neuron is that of error neurons.
219740	224764	B	0.5903211236000061	And the error neurons, they send prediction error information up the hierarchy.
224892	228770	B	0.7861346006393433	So one level predicts the activity of the level below.
230760	241240	B	0.4925491213798523	This prediction has some mismatch which was actually going on in the level below, and information about the prediction error gets sent up the hierarchy.
242380	258140	B	0.7791202068328857	However, predictive coding was actually not burned as a neuroscience, as a theory from the neurosciences, but it was actually initially developed as a method for signal processing and compression back in the 50s.
258290	282150	B	0.5279489755630493	So the work of Oliver Elias, which are actually contemporary of Shannon they realized that once we have a predictor, a model that works that is well in predicting data, sending messages about the error in those predictions is actually much cheaper than sending the entire message every time.
283080	285184	B	0.7641860842704773	And this is how predictive coding was born.
285232	312492	B	0.8825814127922058	So as a signal processing and compression mechanism in information theory back in the 50s, it was actually in the 80s that exactly the same model was used in neuroscience with the work from Mumford or other works that, for example, explain how the Retina process information.
312626	322476	B	0.8371649384498596	So we get prediction signals from the outside world and we need to compress this representation and have this internal representation in our neurons.
322668	331270	B	0.8077547550201416	And the method is very similar, if not equivalent to the one that was developed by Elias and Oliver in the 50s.
333000	355150	B	0.6065314412117004	Maybe what's the biggest paradigm shift happened in 99 thanks to the work of Rao and Ballard in which they introduced this concept that I mentioned earlier about hierarchical structures in the brain where prediction information is top down and error information is bottom up.
355520	368348	B	0.7713338136672974	And something that they did that wasn't done before is that they explain and develop this theory about not only inference, but also about how learning works in the brain.
368444	371600	B	0.880881130695343	So it's also a theory of how our synapses get updated.
373300	381460	B	0.5076030492782593	And the last big breakthrough that I'm going to talk about in this brief historical introduction is from 2003.
381530	400436	B	0.6309171319007874	But then he kept going in the years after, thanks to Karl Friston, in which basically he took the theory of Rowan Ballard and he developed, he extended it and generalized it to the theory of generative models.
400548	414030	B	0.8347947001457214	So basically the main claim that Karl Frieston did is that predictive coding is an evidence maximization scheme of a specific kind of generative model, which I'm going to introduce later as well.
415060	432020	B	0.8907212615013123	So to make a brief summary, in the first two kinds of predica described so signal processing and compression and information processing in the Retina and in the brain in general, they are inference methods.
432360	444868	B	0.6121947169303894	And the biggest change, the biggest revolution that we had in 1999, so let's say in the 21st century, is apparative coding was seen as a learning algorithm.
444964	456750	B	0.6984483599662781	So we can first compress information and then update all the synapses or all the latent variables that we have in our generative model to improve our generative model itself.
458880	464620	B	0.8247321248054504	So let's give some definitions that are a little bit more formal.
464960	470432	B	0.8685001134872437	So prior coding can be seen as a hierarchical Gaussian generative model.
470566	477750	B	0.7346739768981934	So here is a very simple figure in which we have this hierarchical structure which can be as deep as we want.
478360	490650	B	0.896223783493042	And prediction signals go from one latent variable XN to the following one and it gets transformed every time via function GN or GI.
495670	497762	B	0.737843930721283	This is a generative model, as I said.
497816	500594	B	0.8930975198745728	And what's the marginal probability of this generative model?
500712	506594	B	0.7793366312980652	Well, it's simply the probability of the last can you see my cursor?
506642	506854	A	0.46103888750076294	Yes.
506892	507286	B	0.7360090017318726	Right?
507388	507702	B	0.46103888750076294	Yes.
507756	508262	B	0.7672419548034668	Perfect.
508396	524300	B	0.8907994031906128	So it's the generative model of the last vertex is the probability distribution of the last vertex times the probability distribution of every other vertex conditioned on the activity of the vertex before or on the latent variable before.
526430	546334	B	0.8597603440284729	I earlier said that it's a Gaussian generative model, which means that those probabilities, they are in Gaussian form and every and those function g in general and especially since, for example, in Rambalar paper and in all the papers that came afterwards.
546462	560310	B	0.8029754161834717	Also because of the deep learning revolution, those functions are simply linear maps or nonlinear maps with activation functions or nonlinear maps with activation function and an additive bias.
563370	580000	B	0.7786796689033508	So we can give an informal definition of creative coding and we can say predictive coding is an inversion scheme for such a generative model where its model evidence is maximized by minimizing a quantity that is called a variation of free energy.
580930	587950	B	0.46709370613098145	In general, the goal of every generative model is to maximize model evidence, but this quantity is always intractable.
589170	609814	B	0.6819121241569519	And we have some techniques that allow us to approximate this solution and the one that we use in pritef coding instead of minimizing aberration of free energy, which is a lower bound of the model evidence in this work and actually in a lot of other ones.
609852	612038	B	0.645919680595398	So it's the standard way of doing it.
612124	618534	B	0.8241534233093262	This minimization is performed via gradient descent and yes, performed via gradient descent.
618582	623398	B	0.7388615012168884	And there are actually other methods such as expectation maximization which is often equivalent.
623574	633326	B	0.8802842497825623	Or you can use some other message passing algorithms such as belief propagation, for example, and going a little bit back in time.
633428	647902	B	0.8023291230201721	So forgetting a little bit about the statistical generative models, we can see pref coding, I said already a couple of times as a hierarchical model with neural activities.
647966	662920	B	0.8060201406478882	So with neurons, latent variables that represent neural activities, the sender signal down the hierarchy and with error nodes or error neurons, the sender signal up the hierarchy so they send the error information back.
663610	667750	B	0.8910636901855469	What's the variation of free energy of these class operative coding models?
667830	680270	B	0.7366161346435547	It's simply the sum of the mean square error of all the error neurons, so is the sum of the total error squared.
682130	690426	B	0.8322854042053223	And this representation is going to be useful in the later slides and in how I'm going to explain how to use pref coding to model causal inference.
690458	696740	B	0.5952324867248535	For example, why do you think pre tip coding is important and is a nice algorithm to study?
697430	703810	B	0.5949370861053467	Well, first of all, as I said earlier, it optimizes the correct objective which is the model evidence or marginal likelihood.
704390	711000	B	0.7855210900306702	And then it does so by optimizing a lower bound which is called the rational free energy, as I said.
711370	721414	B	0.7457169890403748	And the rational free energy is interesting because it can be written as a sum of two different terms which are and each of those term optimizing.
721462	728670	B	0.5892213582992554	It has important impacts, for example, in machine learning tasks or in general in learning tasks.
729010	731870	B	0.7804862260818481	So one of those term forces memorization.
732370	744318	B	0.8180087208747864	So the second term basically forces the model to fit a specific data set and the first term forces the model to minimize the complexity.
744494	760934	B	0.7691868543624878	And as we know, for example from the Occam's razor theory, if we have two different models that perform similarly on a specific training set, the one that we have to get and the one that is expected to generalize the most is the less complex one.
761132	779420	B	0.8141962885856628	So updating generative model via vibrational free energy allows us to basically converge to the optimal Occam razor model which both memorizes a data set but is also able to generalize very well on unsigned data points.
780670	805122	B	0.5090922713279724	A second reason of why prioritic coding is important is that it actually doesn't have to be defined on a hierarchical structure, but it can be modeled on more complex and flexible architectures such as directed graphical model with any shape or generalized even more to networks with a lot of cycles that resemble brain region.
805266	826262	B	0.5888487100601196	And the underlying reason is that you're not learning and predicting with a forward pass and then back propagating the error but you're minimizing an energy function and this allows basically every kind of hierarchy to be allows to go behind hierarchies and allow to learn cycles.
826326	840110	B	0.6373481154441833	And this is actually quite important because the brain is full of cycles as we have some information from some recent papers that managed to map completely the brain of some animals such as fruit fly.
840710	855570	B	0.5372055768966675	The brain is full of cycles so it makes sense to train our machine learning models or our models in general with an algorithm that allows us to train using cyclic structures.
857270	866014	B	0.9232662916183472	The third reason why prior coding is interesting is that it has been formally proven that it is more robust than standard neural networks during with back propagation.
866162	873370	B	0.5026545524597168	So if you have a neural network and you want to perform classification tasks, creative coding is more robust.
874110	882442	B	0.8989114761352539	And this is interesting in tasks such as online learning, training of small data sets or continuous learning tasks.
882586	898980	B	0.8456727862358093	And the theory basically comes from the fact that imperative coding has been proved to approximate implicit gradient descent, which is a different version of the explicit gradient descent which is the standard gradient descent used in every single model basically.
899670	906422	B	0.7010444402694702	And it's a variation that is more robust I think.
906476	909178	B	0.6754082441329956	Okay, I did quite a long intro to predictive coding, I think.
909184	912330	B	0.9013332724571228	I'm now moving to the second topic which is causal inference.
913070	915366	B	0.8002445697784424	And what's causal inference?
915398	922426	B	0.8619018793106079	Causal inference is a very general theory that has been formalized the most by Judea Pearl.
922458	926714	B	0.97247713804245	He's definitely the most important person in the field of causal inference.
926762	928634	B	0.9641026258468628	He wrote some very nice books.
928682	943522	B	0.8703981041908264	For example, the Book of Why is highly recommended if you want to learn more about this topic and it basically tackles the following problem so let's assume we have a joint probability distribution which is associated with a Bayesian network.
943666	953510	B	0.8290743827819824	This is going to be a little bit the running example through all the paper, especially with Bayesian networks of this shape.
954910	961078	B	0.8338049650192261	Those Bayesian networks, the variables inside, they can represent different quantities.
961174	968142	B	0.8942963480949402	So for example, a Bayesian network with this shape can represent the quantities on the right.
968196	987358	B	0.5770481824874878	So, socioeconomical statue of an individual, its education level, its intelligence and its income level, something the classical statistics is very good at and its wild, most used application is to model observations or correlations.
987534	995366	B	0.904367208480835	A correlation basically answer the question what is D if we observe another variable C?
995548	1011638	B	0.9031879305839539	So for example, in this case, what's the income level, the expected income level of an individual if I observe his education level and of course, if that person has a higher degree of education, for example a master or PhD.
1011734	1017610	B	0.8199927806854248	I'm expecting general that person to have a higher income level and this is a correlation.
1018190	1025818	B	0.5663869380950928	However, sometimes there are things that are very hard to observe, but they play a huge role in determining those quantities.
1025994	1038446	B	0.7893394827842712	So for example, it could be that the income level is much, much more defined by the intelligence of a specific person and maybe that the intelligence.
1038478	1043320	B	0.7161316275596619	So if a person is intelligent, he's also most likely to have a higher education level.
1044730	1050870	B	0.6340427994728088	But still the real reason why the income is high is because of the IQ.
1052250	1060810	B	0.6065407991409302	And this cannot be studied by Simplico relations and has to be studied by a more advanced technique which is called an intervention.
1061230	1067580	B	0.8977262377738953	An intervention basically answers the question is what is D if we change C to a specific value?
1068130	1076734	B	0.8811061382293701	So for example, we can take an individual and check his income level and then change its education level.
1076772	1086740	B	0.7082982063293457	So intervene on this word and change his education level without touching his intelligence and see how much its income changes.
1087270	1095686	B	0.5419405102729797	For example, if the income changes a lot it means that the intelligence doesn't play a big role in this, but the education level does.
1095788	1104120	B	0.6186124086380005	If the income level doesn't change much, it means that maybe there's a hidden variable in this case the intelligence that determines the income level of a person.
1106110	1110502	B	0.8385007381439209	The third quantity important in causal inference is that of counterfactuals.
1110646	1118954	B	0.9035027027130127	So for example, a counterfactual answers the question what would D be had we changed C to a different value in the past?
1119152	1126158	B	0.8750840425491333	So for example, we can see that the difference between interventions and counterfactuals is that interventions act in the future.
1126244	1141940	B	0.8745140433311462	So I'm intervening in the world now to observe a change in the future while counterfactuals allow us to go back in time and change a variable back in time and see how that change would have influenced the world we live in now.
1142870	1147970	B	0.8224976062774658	And those are defined by Judeoperl as the three levels of causal inference.
1148130	1153080	B	0.7740161418914795	Correlation is the first level, intervention is the second level and counterfactual is the third level.
1156330	1157474	B	0.8046593070030212	What are interventions?
1157522	1167814	B	0.6820362210273743	I'm going to define them more formally now that I gave an intuitive definition and I'm using this notation here, which is the same actually throughout all the presentation.
1167942	1177978	B	0.7736449837684631	So x is always going to be a latent variable, si is always going to be a data point or an observation, and VI is always going to be a vertex.
1178074	1183780	B	0.80153888463974	So every time you see VI, we are only interested in the structure of the graph, for example.
1185190	1193590	B	0.8891295194625854	So let's assume we have a Bayesian model which has the same structure as the Bayesian model we saw in the previous slide.
1194810	1208970	B	0.8540990948677063	Given that x three is equal to s three, this is the observation we make statistics allows us to compute the probability or the expectation of x four, which is the latent variable related to this vertex.
1209390	1220720	B	0.8966467380523682	Given that x three is equal to s three, to perform an intervention we need a new kind of notation which is called the do operation.
1221220	1232610	B	0.9006137251853943	So in this case, x four, we want to compute the probability of x four, given the fact that we intervene in the word and change x three to x three.
1233140	1236460	B	0.8021332025527954	And how do we do this to perform an intervention?
1236620	1248730	B	0.8734568357467651	Judy Perl tells us that we have to have an intermediate step before computing a correlation is that first we have to remove all the incoming edges to v three.
1250140	1254250	B	0.8384907841682434	So we have to study not this Bayesian network, but the second one.
1255660	1261710	B	0.856508731842041	And then at this point we are allowed to compute a correlation as we normally do.
1263200	1264940	B	0.753868043422699	And this is an intervention.
1266960	1275260	B	0.7604113221168518	A counterfactual is a generalization of this that, as I said, lived in the past, and they're computing using structural causal models.
1275420	1282092	B	0.8485944271087646	A structural causal model is a Tuplet which is conceptually similar to a Bayesian network.
1282236	1288790	B	0.7365320324897766	But basically we have this new class of variables on top, which are the unobservable variables they use.
1289160	1293270	B	0.8694018721580505	So we have the Bayesian network that we had before, x one, x two, x three, x four.
1294060	1300212	B	0.8239578604698181	But we also have those unobservable or variables that depend on the environment.
1300356	1305530	B	0.6394157409667969	You cannot control them, you can infer them, but they are there.
1306960	1324130	B	0.8801773190498352	And F is a set of functions that depends on all the basically, F of x of x three depends on x one because you have an arrow, on x two because you have an arrow, and on the unobservable variable that also influences x three.
1326180	1343764	B	0.8023352026939392	So yes, intuitively you can think of a structural causal model as a Bayesian network with those unobservable variables on top, and each unobservable variable only influences its own latent variable x.
1343882	1352330	B	0.6382640600204468	So for example, IU will never touch x one as well, u three will only touch U three, u one will only influence x one, and so forth and so on.
1354950	1365720	B	0.914393424987793	So performing counterfactual inference answers the following question so what would x four be at x three being equal to another variable in a past situation u.
1367850	1371318	B	0.7812033295631409	And computing this counterfactual requires three different steps.
1371414	1377878	B	0.6879830360412598	So abduction is the computation of all the background variables.
1377974	1386960	B	0.8432331085205078	So in this step we want to go back in time and understand how the environment, the unobservable environment was in that specific moment in time.
1387970	1400180	B	0.8676567673683167	And we do this by fixing all the latent variables x to some specific data that we already have and performing this inference on the use.
1401190	1408386	B	0.8838084936141968	Then we're going to use the U to keep the U that we have learned and perform an intervention.
1408578	1421260	B	0.9021734595298767	So a counterfactual can also be seen as an intervention back in time in which we know the environment variables u one, U two and U four in that specific moment.
1423150	1425820	B	0.7352318167686462	And what's the missing step?
1426670	1433920	B	0.9094366431236267	So what would x four be at x three being equal to another data point in that specific situation?
1434770	1449938	B	0.7622124552726746	Now we can compute a correlation and the correlation we do it on the graph in which we have already performed an intervention using the environment variables that we have learned in the abduction step.
1450104	1452770	B	0.6397910118103027	And this is a counterfactual inference.
1455430	1461880	B	0.8701512813568115	This is the last slide of the causal inference introduction and it's about structured learning.
1462330	1470278	B	0.8896075487136841	Basically everything I've said so far relies on the fact that we know the causal dependencies among the data points.
1470364	1480362	B	0.5735507011413574	So we know the structure of the graph, we know which variable influences which one, we know the arrows in general but in practice this is actually not always possible.
1480496	1490718	B	0.6305694580078125	So we don't have access to the causal graph most of the times and actually learning the best causal graph from data is still an open problem.
1490804	1492814	B	0.9399736523628235	We are improving in this, we are getting better.
1492932	1499700	B	0.5257533192634583	But how to perform this task exactly is still an open problem.
1501030	1505970	B	0.8575791716575623	So as I said, basically the goal is to infer causal relationships from observational data.
1506120	1515320	B	0.8610302805900574	So given a data set we want to infer the directed icyclic graph that describes the connectivity between the system and the variables of the data set.
1515850	1523766	B	0.5885424017906189	So for example, here we have an example that I guess we are all familiar with because of the pandemic.
1523958	1530380	B	0.7554858326911926	So we have those four variables age, vaccine, hospitalization and city.
1531390	1535610	B	0.8487117886543274	And we want to infer the causal dependencies among those variables.
1535690	1548100	B	0.7948675751686096	So for example, we want to learn directly from data that the probability of a person being hospitalized depends on its age and on the fact whether it's vaccinated or not and so forth and so on.
1551350	1566840	B	0.6739681363105774	So this is the end of the long introduction but I hope it was clear enough and I hope that I gave the basics to understand basically the results of the paper and now we can go to the research questions.
1567610	1576866	B	0.9002336859703064	So the research questions are the following first I want to see whether predictive coding can be used to perform causal inference.
1577058	1584826	B	0.718191921710968	So predictive coding so far has only been used to perform to compute correlations in Bayesian networks.
1585018	1592320	B	0.8267044425010681	And the big question is can we go beyond correlation and model intervention and counterfactual in a biological plausible way?
1592690	1605534	B	0.4985193610191345	So in a way that it's for example, simple, intuitive and allow us to only play with the neurons and not touch, for example, the huge structure of the graph and more in practice.
1605582	1613830	B	0.8842189311981201	More specifically, the question becomes can we define operative coding based structural causal model to perform interventions and counterfactuals?
1615370	1627782	B	0.8243656754493713	The second question is, as I said, that having a structure causal model assumes that we know the structure of the Bayesian network, so it assumes that we have the arrows.
1627926	1632890	B	0.8669120669364929	Can we go beyond this and use predictive coding networks to learn the causal structure of the graph?
1636290	1653220	B	0.5938169360160828	Basically, giving positive answers to both those questions would allow us to use predictive coding as an end to end causal inference method which basically takes a data set and allow us to test interventions and counterfactual predictions directly from this data set.
1656750	1659642	B	0.7181693911552429	So let's tackle the first problem.
1659696	1665840	B	0.8974583745002747	So causal inference Vibrative coding which is also the section that gives the title to the paper basically.
1666690	1679860	B	0.8480228185653687	And here I will show how to perform correlations with Abrasive coding which is already known, and how to perform interventional queries which I think is the real question of the paper.
1681190	1690150	B	0.8950761556625366	So here is a causal graph which is the usual graph that we had and here is the corresponding priority coding model.
1690300	1696760	B	0.8730681538581848	So the axes are the latent variables and correspond to the neurons in a neural network model.
1698010	1704970	B	0.8231852054595947	And the black arrow pass prediction information from one neuron to the one down the hierarchy.
1705550	1711610	B	0.5429347157478333	And every vertex also has this error neuron which passes information up the hierarchy.
1711690	1722190	B	0.7835175395011902	So the information of every error goes to the value node up the hierarchy and basically tells it to correct itself to change the prediction.
1724690	1733518	B	0.7614864110946655	So to perform a correlation using predictive coding, what you have to do is that you take an observation and you simply fix the value of a specific neuron.
1733694	1747530	B	0.8190537691116333	So if you want to compute the probability of x four given x three equal to s three, we simply have to take x three and fix it to s three in a way that it doesn't change anymore and run an energy minimization.
1748110	1760586	B	0.7123174667358398	And this model, by updating the axis via a minimization of the variational free energy allows the model to converge to a solution to this question.
1760768	1765360	B	0.8978248238563538	So the probability or the expected value of x four given x three equals three.
1767170	1772270	B	0.7210255265235901	But how do I perform an intervention now without acting on the structure of the graph?
1773410	1780302	B	0.8175785541534424	Well, this is basically the first idea of the paper, this is still how to perform a correlation.
1780366	1801334	B	0.8724964261054993	So fix s three equal to x three is the first step in the algorithm and the second one is to obtain the axis by minimizing the variation of free energy, an intervention which in theory corresponds in removing those arrows and answers to the question the probability of x four by performing an intervention.
1801382	1803402	B	0.8836032152175903	So do x three equal s three?
1803536	1806090	B	0.8881645798683167	Imperative coding can be performed as follows.
1806990	1809626	B	0.8924742341041565	So I'm going to write the algorithm here.
1809808	1820880	B	0.7653003334999084	So first, as in a correlation, you fix s three equal equal to the you fix x three equal to the observation that you get then this is the important step.
1821250	1828180	B	0.5887792706489563	You have to intervene not on the graph anymore, but on the prediction error and fix it equal to zero.
1828950	1843160	B	0.6021052598953247	Having a prediction error equal to zero basically sends meaningless information up the hierarchy, or actually sends no information up the hierarchy because it basically tells you that the prediction is always correct.
1845130	1851026	B	0.905375599861145	And the third step is to, as we did before, to update the axis, the unconstrained axis.
1851058	1852458	B	0.8216322660446167	So x one, x two, x four.
1852544	1877790	B	0.7367018461227417	By minimizing the variation of free energy, as I will show now experimentally, by simply doing this little trick of setting a prediction error to be equal to zero, it prevents us to actually act on the structure of the graph, as the theory of Ducalculus does, and to infer the missing the variables after an intervention.
1877870	1880930	B	0.7293055653572083	By simply performing aberration of free energy minimization.
1884650	1886482	B	0.8502633571624756	What about counterfactual inference?
1886546	1893430	B	0.5482620000839233	Counterfactual inference is actually easy once we have defined how to do an intervention.
1894650	1907450	B	0.8169340491294861	And this is because as we saw earlier, performing a counterfactual is similar to performing an intervention in a past situation after you have inferred the unobservable variables.
1908110	1921098	B	0.7196199893951416	So, as you can see in the plot I showed earlier about the abduction action and prediction steps, the action and prediction steps, they did not have those two arrows, they were removed.
1921274	1939026	B	0.6286084055900574	Pretty coding allows allow us to keep the arrows in the graph and perform counterfactuals by simply performing an adoption step, as it was done earlier, an action step in which we simply perform an intervention on the single node.
1939138	1945970	B	0.788685142993927	So we fix the value node and we set the error to zero and run the energy minimization.
1946050	1949050	B	0.8208329081535339	So minimizing the rational free energy to compute the prediction.
1952670	1961150	B	0.8532688617706299	So, I think this is like an easy and elegant method to perform interventions at counterfactuals.
1963330	1967278	B	0.8148903846740723	I think the thing we have to show now is whether it works in practice or not.
1967444	1973422	B	0.864069402217865	And we have a couple of experiments and I'm going to show you now two different experiments.
1973486	1984850	B	0.7076907157897949	The first one is merely proof of concept experiment that shows that the operative coding is able to perform intervention and counterfactuals.
1986090	2001820	B	0.6835363507270813	And the second one actually shows a simple application in how interventional queries can be used to improve the performance of classification tasks on a specific kind of predictive coding networks, which is that of a fully connected model.
2002350	2004060	B	0.7612563967704773	Let's start from the first one.
2004590	2006294	B	0.7763295769691467	So how do we do this task?
2006342	2014506	B	0.8921099305152893	So, given a structural causal model, we generate training data and we use it to learn the weights.
2014538	2032450	B	0.8770769834518433	So, to learn the functions of the structural causal models, and then we generate test data for both interventional and counterfactual queries and we show whether we are able to converge to the correct test data using predictive coding.
2034870	2054970	B	0.8632552027702332	For example, here those two plots represent intervention and counterfactual queries of this specific graph, which is the butterfly bias graph, which is a graph that is often used in in testing whether causal inference, whether intervention and counterfactual techniques work is as simple as that.
2055120	2059122	B	0.8391619920730591	But in the paper you can find a lot of different graphs.
2059286	2082914	B	0.8289570212364197	But in general those two plots show that the method works show that the mean absolute error between the interventional and counterfactual quantities we compute and the interventional and counterfactual quantities from the original graph are close to each other.
2082952	2084500	B	0.6751863360404968	So the error is quite small.
2086330	2098540	B	0.875917375087738	The second experiment is basically an extension of an experiment I proposed in an earlier paper which is the Learning on arbitrary graph topologies that I wrote last year.
2098990	2128286	B	0.7244832515716553	In that paper, I basically proposed this kind of network as a proof of concept, which is a fully connected network, which is, in general, the worst neural network you can have to perform machine learning experiments because given a fixed set of neurons, basically every pair of neuron is connected by two different synapses.
2128398	2133860	B	0.5273646712303162	So is the model with the highest complexity possible in general.
2134630	2144834	B	0.9135610461235046	The good thing is that since you have a lot of cycles, the model is extremely flexible in the sense that you can train it, for example, on a Minced image and on a data point and on its label.
2144962	2151754	B	0.7961907982826233	But then the way you can query it, thanks to the information going back, is you can query in a lot of different ways.
2151792	2163962	B	0.8706903457641602	So you can perform classification tasks in which you provide an image and you run the energy minimization and get the label but you can also, for example, perform generation tasks in which you give the label, run the Energy Minimization and get the image.
2164106	2173906	B	0.8862782120704651	You can perform, for example, image completion in which you give half the image and let the model convert to the second half and so forth and so on.
2174008	2183860	B	0.8402496576309204	So it's basically a model that learns the statistics of the data set in its entirety without being focused on classification or generation in general.
2185110	2186920	B	0.9620776176452637	So this flexibility is great.
2187930	2211230	B	0.6977237462997437	The problem is that because of this every single task doesn't work well so it can do a lot of different things but none of them is done well and here I want to show how using interventional queries instead of standard correlational queries or conditional queries slightly improves the results of those classification tasks.
2211890	2220320	B	0.7192021012306213	So what are the conjecture reasons of the test accuracy on those tasks not being so high?
2221090	2226990	B	0.7870376706123352	The two reasons are that the model is distracted in correcting every single error.
2227070	2241714	B	0.6459987759590149	So basically you present an image and you would like to get a label but the model is actually updating itself to also predict the error in the images and the second reason, which is the one I said, is that the structure is far too complex.
2241842	2250406	B	0.94828200340271	So again, from an OCAM razor argumentation this is the worst model you can have.
2250428	2251578	B	0.7525705099105835	So every time you have a model that.
2251584	2252618	B	0.75303715467453	Fits a data set.
2252704	2268030	B	0.701988160610199	That model is going to be less complex than this one that is going to be preferred, but in general just to study it, the idea is can query in this model be interventions be used to improve the performance of those fully connected models?
2268610	2270640	B	0.5983779430389404	Well, the answer is yes.
2271010	2273562	B	0.9139164686203003	So here is how I perform interventional queries.
2273626	2282990	B	0.6242692470550537	So I present an image to the network, I fix the error of the pixels to be equal to zero so this error doesn't get propagated in the network.
2283150	2288290	B	0.5691287517547607	And then I compute the label and as you can see the accuracy improves.
2288370	2297522	B	0.8301270604133606	For example, from 89 using the standard query method of creative coding networks, to 92, which is the accuracy after the intervention.
2297586	2300470	B	0.6854945421218872	And the same happens for fashion minced.
2301630	2311822	B	0.5373894572257996	And I think that a very legit critique that probably everyone would think when seeing those plots is that okay, you improved on mins from 89 to 92.
2311956	2313600	B	0.9583816528320312	It still sucks basically.
2314050	2316334	B	0.5841630697250366	And yeah, it's true.
2316452	2339990	B	0.8232750296592712	And actually in the later slides I'm going to show how to act on the structure of this fully connected model will improve the results even more until the point they reach a performance that is not even close to state of the art performance of course, but up to a level that becomes visually acceptable and worth investigating.
2342970	2343382	B	0.46103888750076294	Yes.
2343436	2348230	B	0.8309112787246704	So this was the part about causal inference using previous coding.
2348390	2366858	B	0.936160683631897	And I guess to summarize, I can say that the interesting part of the results I just showed is that I showed that predictive coding is able to perform interventions in a very easy and intuitive way because you don't have to act on the structure of the old graph anymore.
2366954	2371246	B	0.6456056237220764	Sometimes those functions are not available, so forth and so on.
2371268	2385000	B	0.8213854432106018	But you simply have to intervene on a single neuron, set its prediction error to zero and perform an energy minimization process.
2386570	2392390	B	0.6413298845291138	And this extended allowed us to define predictive coding based structural causal models.
2392810	2399880	B	0.8439465761184692	Now we move to the second part of the work which is about structure learning.
2402410	2411440	B	0.823590099811554	So structure learning, as I said, deals with the problem of learning the causal structure of the model from observational data.
2411890	2425170	B	0.6331051588058472	This is actually an old problem that has been around for decades and has always been until a couple of years ago tackled using combinatorial search methods.
2425510	2431810	B	0.828093945980072	The problem with those combinatorial search methods is that their complexity grows double exponentially.
2432790	2445110	B	0.5332496166229248	So as soon as the data becomes multidimensional and the Bayesian graph that you want to learn grows in size learning, it it's incredibly slow.
2446730	2461310	B	0.849854588508606	The new solution that came out actually a couple of years ago in a new research paper from 2018 showed that it's possible to actually learn this structure not using a combinatorial search method, but by using a gradient based method.
2461650	2477134	B	0.4404638707637787	And this killed the problem in general because now you can simply have a prior on the parameters, which is the prior the proposed that I'm going to define a little bit better in this slide run gradient descent.
2477262	2484100	B	0.7357244491577148	And even if you have a model that is double triple the size, the algorithm is still incredibly fast.
2485430	2495382	B	0.779529869556427	And for this reason this paper is I think it's kind of new and I think already has around 600 citations or things like that.
2495516	2501690	B	0.8557741641998291	And every paper that I'm seeing now about causal inference and learning causal structure of the graph uses their method.
2502030	2503900	B	0.7613239288330078	It just changes a little bit.
2504910	2515520	B	0.5334845185279846	They find faster or slightly better inference methods, but still they all use the prior this paper defined and I do as well, and we do as well.
2516450	2520682	B	0.8903983235359192	So here we define a new quantity which is the agency matrix.
2520826	2525378	B	0.7950509190559387	The agency matrix is simply a matrix that encodes the connections of the model.
2525544	2530414	B	0.7201260328292847	So it's a binary matrix and in general it's a binary matrix.
2530462	2555446	B	0.7955069541931152	Then of course, when you do gradient based optimization you make it continuous and then you have some threshold at some point that basically kills an edge or set it to one and the entry IJ is equal to one if the Bayesian graph has an edge from vertex I to vertex j or zero otherwise.
2555558	2561230	B	0.9142508506774902	So for example, this agency matrix here represents the connectivity structure of this Bayesian network.
2562850	2572966	B	0.6735836863517761	And basically this method tackles two problems that we want about learning the structure of the Bayesian network.
2573098	2584478	B	0.5780149698257446	The idea is that we start from a fully connected model, which conceptually is similar, actually is equivalent to the creative coding network I defined earlier, which is fully connected.
2584574	2595000	B	0.6625000834465027	So you have a lot of vertices and every pair of vertices is connected by two different edges and you simply want to prune the ones that are not needed.
2595690	2600070	B	0.8677728176116943	So it can be seen as a method that performs model reduction.
2600150	2602618	B	0.6473941802978516	You start from a big model and you want to make it small.
2602784	2606726	B	0.8595778346061707	So what's the first ingredient to reduce models?
2606838	2608700	B	0.6277046799659729	Well, it's of course sparse city.
2609230	2620734	B	0.797019362449646	And what's the prior that everyone uses to make a model more sparse is the LaPlace prior, which in machine learning is simply known as the L one norm, which is defined here.
2620932	2640550	B	0.7269655466079712	The solution that this paper that I mentioned earlier proposed is to add a second prior on top, which enforces what's probably the biggest characteristic of Bayesian networks on which you want to perform causal inference is that you want them to be a cyclic.
2640890	2650378	B	0.8422454595565796	And basically they show that Acyclicity can be imposed on an agency matrix as a prior and it has this shape here.
2650464	2660686	B	0.8938755393028259	So it's the trace of the matrix that is the exponential of a times a where A is the agency matrix again.
2660868	2673870	B	0.7463653683662415	And basically this quantity here is equal to zero if and only if the Bayesian network or whatever graph you're considering is a cyclic.
2677650	2680746	B	0.5845535397529602	So I'm going to use this in some experiments.
2680778	2693890	B	0.8707741498947144	So to force those two priors on different kinds of Bayesian networks and I'm trying to merge them with the techniques we proposed earlier about performing causal inference via predictive coding.
2695030	2697134	B	0.8830593228340149	So I'm going to present two different experiments.
2697182	2708520	B	0.7915117740631104	So one is a proof of concept, which is the standard experiments showed in all the structural learning tasks, which is the inference of the correct Bayesian network from data.
2708890	2726430	B	0.6374790668487549	And then I'm going to build on top of the classification experiments I showed earlier and show how actually those priors allow us to improve the classification accuracy, the test accuracy of fully connected predictive coding models.
2729420	2733800	B	0.8704053163528442	So let's move to the first experiment, which is to infer the structure of the graph.
2734940	2740540	B	0.8208478689193726	And the experiments, they all follow basically the same pipeline in all the papers in the field.
2740690	2745816	B	0.8574120998382568	The first step is to generate a vision network from a random graph.
2746008	2753040	B	0.8645133376121521	So basically, normally the two random graphs that everyone tests are erdos reni graphs and scale free graph.
2753380	2767270	B	0.8979476690292358	So you generate those big graphs that normally have 20, 40, 80 different nodes and some edges that you sample randomly and you use this graph to generate a data set.
2768280	2773316	B	0.8765901923179626	So you sample, for example, n, big N data points.
2773498	2779000	B	0.6680498123168945	And what you do is that you take the graph they have generated earlier and you throw it away.
2779070	2780648	B	0.7651360630989075	You only keep the data set.
2780814	2793870	B	0.7116553783416748	And the task you want to solve now is to have a training algorithm that basically allows you to retrieve the structure of the graph you have thrown away.
2794640	2811890	B	0.8634598851203918	So the way we do it here is that we train a fully connected creative coding model on this data set D using both the sparse and the acyclic priors we have defined earlier and see whether actually the graph that we converge to after pruning away.
2813220	2820740	B	0.9002626538276672	The entries of the agency matrix that are smaller than a certain threshold is similar to that of the of the initial graph.
2822440	2825216	B	0.7255751490592957	And the results show that this is actually the case.
2825338	2842540	B	0.6509449481964111	So this is an example and I show many different parameterization and dimensions and things like that in the paper, but I think those two are the most representative examples with an Ernest Sharoni graph and a free scale graph with 20 nodes.
2843600	2850460	B	0.865942656993866	And here on the left you can see the ground truth graph, which is the one sampled randomly.
2850800	2857136	B	0.8667700886726379	And on the right you can see the graph the predictive coding model has learned from the data set.
2857318	2859970	B	0.6125565767288208	And as you can see, they are quite similar.
2861140	2868724	B	0.7028241157531738	It's still not perfect, so there are some errors, but in general the structure is they work quite well.
2868762	2885876	B	0.6091903448104858	We also have some quantitative experiments that I don't show here because they're just huge tables with a lot of numbers and I thought it was maybe a little bit too much for the presentation, but the results show that they perform similarly to contemporary methods.
2886068	2893900	B	0.7054105997085571	Also because I have to say, like, most of the quality comes from the Acyclic prior that was introduced in 2018.
2896920	2905000	B	0.8622862696647644	The second class of experiments are classification experiments, which as I said are the extensions of the one I shared earlier.
2905500	2915660	B	0.6392285823822021	And the idea is to use structure learning to improve the classification on the classification results on the means and fashion means data set, starting from a fully connected graph.
2916720	2923592	B	0.8700249195098877	So, what I did is that I divided the fully connected graph in clusters of neurons.
2923736	2930344	B	0.8439421057701111	So one big cluster is the one related to the input and all the small.
2930482	2934800	B	0.8676887154579163	Then we have some a specific number of hidden clusters.
2935300	2944820	B	0.8458749651908875	And then we have the label cluster, which is the cluster of neurons that are supposed to give me the label predictions.
2946280	2950516	B	0.8380579352378845	And I've trained them using the first time, the sparse prior only.
2950618	2960090	B	0.6533283591270447	See, the idea is what if I, if I prune the connections I don't need from a model and learn a sparser model?
2960940	2962024	B	0.7564374804496765	Does this work?
2962142	2964504	B	0.8590604066848755	Well, the answer is no, it doesn't work.
2964542	2971560	B	0.8418917655944824	And the reason why is that at the end, the graph that you converge with is actually the generate.
2971640	2976844	B	0.7820085883140564	So basically the model learns to predict the label based on the label itself.
2976962	2981548	B	0.4937659800052643	So it discards all the information from the input and only keeps the label.
2981644	2984512	B	0.7235883474349976	And as you can see here, the label y predicts itself.
2984646	2991750	B	0.8092389106750488	Or in other experiments, when you change the parameters, you have that y predicts x zero that predicts x one that predicts y again.
2992760	2996212	B	0.5744397640228271	So what's the solution to this problem?
2996266	3006676	B	0.6416121125221252	Well, the solution to this problem is that we have to converge to an acyclic graph and so we have to add something that prevents acyclicity.
3006788	3010692	B	0.8212795257568359	And what is that one is of course, the one I already proposed.
3010756	3013640	B	0.8652231097221375	And then I show a second technique.
3014540	3018300	B	0.8888470530509949	So the first one uses the acyclic prior defined earlier.
3019200	3024120	B	0.5349427461624146	And the second one is a novel technique that actually makes use of negative examples.
3024280	3032864	B	0.8298984169960022	So a negative example in this case is simply a data point in which you have an image, but the label is wrong.
3033062	3038210	B	0.8663002252578735	So here, for example, you have an image of a seven, but the label that I'm giving the model is a two.
3041400	3047652	B	0.5601409077644348	The idea is very simple, has been used in a lot of works already.
3047786	3053832	B	0.4983232319355011	So every time the model sees a positive example, it has to increase to minimize the variation of free energy.
3053966	3058490	B	0.5225490927696228	And every time it sees a negative example, it has to increase it.
3058860	3062680	B	0.7581491470336914	So we want this quantity to be minimized.
3065280	3078368	B	0.8071064352989197	And actually, with a lot of experiments and a lot of experimentations, we saw that the two techniques basically first lead to the same results and second lead to the same graph as well.
3078534	3087280	B	0.859164297580719	So here are the new results on Minced and fashion Mins using the two techniques that I just proposed.
3088260	3095248	B	0.5512717366218567	And now we move to some which are still not great, but still definitely more reasonable test accuracies.
3095344	3101948	B	0.7173909544944763	So here we have a test error of 3.17 for Minced and a test error of 13.98 for Fashion Minced.
3102144	3115860	B	0.579811692237854	And actually those results can be much improved by learning the structure of the graph on Minced and then fixing the structure of the graph and do some form of fine tuning.
3115940	3129890	B	0.7332953810691833	So if you fine tune the model on the correct hierarchical structure, at some point you reach the test accuracy, which is the one you would expect from a hierarchical model, but those ones are simply the one the fully connected model has naturally converged to.
3132020	3145670	B	0.7105311751365662	For example, from a test error of 18.32 of the fully connected model train on fashion means by simply performing correlations or conditional queries, which is the standard way of querying Abrative coding model.
3146600	3157690	B	0.5120505690574646	Adding interventions and the Acyclic prior together makes this test error much lower and we can observe it for Mins as well.
3159740	3169500	B	0.9195330739021301	I'm now going a little bit into details on this last experiment and on how the Acyclic prior acts on the structure of the graph.
3170880	3177020	B	0.5918298363685608	I perform an experiment on a new data set, which is, I mean, calling it a new data set is maybe too much.
3177170	3189616	B	0.8837467432022095	I called it a two means data set in which you have the input point is formed of two different images and the label only depends on the second image, on the first image.
3189648	3190230	B	0.509212851524353	Sorry.
3190680	3212712	B	0.6191367506980896	So the idea here is, is the structure of the model, the Acyclicity prior and things like that, able to recognize that the second half of the image is actually meaningless in learning, in performing classification, how does training behave in general?
3212766	3219976	B	0.8785608410835266	Like, for example, we have this input node output node and only the nodes are fully connected.
3220088	3230400	B	0.6924678087234497	Can the model converge to a hierarchical structure, which is the one that we know performs the best on classification tasks?
3230900	3236240	B	0.8548241853713989	Well, here is an example of a training method of a training run.
3236390	3242044	B	0.9108887910842896	So at c zero, which is the beginning of training, we have this model here.
3242102	3245268	B	0.8256714344024658	So S zero corresponds to the seven.
3245354	3248992	B	0.8581805229187012	So to the first image, s one corresponds to the seven image.
3249056	3253528	B	0.8501429557800293	Again, we have the label y and all the latent variables x zero, x one, x two.
3253694	3255284	B	0.7264041304588318	And the model is fully connected.
3255332	3258984	B	0.69024258852005	So the agency matrix is full of ones.
3259182	3260292	B	0.6977120041847229	There are no zeros.
3260356	3262490	B	0.6773422360420227	We have self loops and things like that.
3263420	3275820	B	0.6876102089881897	We train the model for a couple of epochs until and what we note immediately is that, for example, the model immediately understands that the four is not needed to perform classification.
3277620	3283360	B	0.6226828098297119	So every outgoing node from the second input cluster is removed.
3283940	3289940	B	0.8714437484741211	And something that you understand is that this cluster is the one related to the output.
3290440	3298710	B	0.8763017654418945	So we have a linear map from s zero to y directly, which is this part here.
3299320	3306604	B	0.6268953084945679	But we know that actually a linear map is not the best map for performing classification on Minst.
3306672	3311028	B	0.7700983881950378	So we need some hierarchy, we need some depth to improve the results.
3311124	3324700	B	0.6075828075408936	And as you can see, this line here is the accuracy which up to this point, so up to c two is similar to is 91% which is slightly better than linear classification.
3325520	3332690	B	0.8403499722480774	But once you go on with the training, the model understands that it needs some hierarchy to better fit the data.
3333060	3345270	B	0.6610994338989258	So you see that this arrow starts getting stronger and stronger over time until it understands that the linear map is not actually really needed and it removes it.
3346760	3360516	B	0.4903956651687622	So the model you converge with is a model that starts from a zero, goes to a hidden node, and then goes to the label with a very weak linear map, which actually gets removed if you set a threshold.
3360548	3372220	B	0.5928013920783997	Of if you set a threshold of, for example, 0.10.2, at some point the linear map gets forgotten and everything you end up with is with a hierarchical network.
3374400	3379388	B	0.774970531463623	So it has learned the correct structure to perform classification tasks, which is hierarchy.
3379484	3386828	B	0.6458856463432312	And it has also learned that the second image didn't play any role in defining the test accuracy.
3386924	3395972	B	0.7641341090202332	And this is all performed, so all those jobs are simply performed by one free energy minimization process.
3396106	3400080	B	0.8279590010643005	So you initialize the model, you define the free energy, you define the priors.
3400160	3411320	B	0.6128026843070984	So the sparse and the cyclic prior, you run the energy minimization and you converge to a hierarchical model which is well able to perform classification on minst.
3411820	3419592	B	0.5621311664581299	And then if you then perform some fine tuning, you reach very competitive results as you do in feed forward networks with backpropagation.
3419736	3421676	B	0.7639688849449158	But I think that's not the interesting bit.
3421778	3438640	B	0.8500201106071472	The interesting bit is that all this process altogether of intervention and a cyclicity allows you to take a fully connected network and converge to a hierarchical one that is able to perform classification with good results.
3440900	3445030	B	0.586337149143219	And yeah, that's basically it.
3445800	3466136	B	0.7728588581085205	Wow, I've talked a lot and this is the conclusion of the talk, which is I'm basically doing a small summary and I think the important takeaway, if I have to give you in one sentence of this paper, is that predictive coding is a belief updating method that is able to perform end to end causal learning.
3466238	3474300	B	0.8366378545761108	So it's able to perform interventions to learn a structure from data and then perform interventions and counterfactuals.
3476560	3481888	B	0.7396652698516846	So causal inference in natural and efficiently model interventions by simply setting the prediction error to zero.
3481974	3487916	B	0.7744328379631042	So it's a very easy technique to perform interventions and you only have to touch one neuron.
3487948	3497460	B	0.6635085344314575	You don't have to act on the structure of the graph, you can use it to create structure causal models that are biologically plausible.
3498360	3517416	B	0.7310291528701782	It is able to learn the structure from data, as I said, maybe a lot of times already and a couple of sentences about future works is that something that would be nice to do is to improve the performance of the model we have defined.
3517528	3521304	B	0.9221715331077576	Because I think it performs reasonably well on a lot of tasks.
3521352	3527560	B	0.8268218040466309	So it performs reasonably well on structured learning, on forming intervention and counterfactuals.
3527720	3535356	B	0.5977039933204651	But actually, if you look at state of the art model, there's always like a very specific method that performs better in the single task.
3535548	3564588	B	0.8574195504188538	So it would be interesting to see if we can reach those level of performance in specific tasks by adding some tricks or some new optimization methods and to generalize it to dynamical systems which are actually much more interesting the static systems such as dynamical causal models or other techniques that allow you to perform causal inference in systems that move.
3564674	3573500	B	0.8492611646652222	So an action taken in a specific time step influences another node in a later time step, which is basically granger causality.
3574880	3576720	B	0.5567741394042969	Yeah, that's it.
3576790	3579410	B	0.8957256078720093	And thank you very much.
3587540	3588384	A	0.8529649376869202	Thank you.
3588502	3591964	A	0.967235803604126	Awesome and very comprehensive presentation.
3592092	3594000	B	0.6710416078567505	That was really I think you're muted.
3597000	3598528	A	0.5884034633636475	Sorry, muted on zoom.
3598624	3603392	A	0.9860303401947021	But yes, thanks for the awesome and very comprehensive presentation.
3603536	3608596	A	0.975040078163147	There was really a lot there, and there was also a lot of great questions in the live chat.
3608708	3615092	A	0.9083030223846436	So maybe to warm into the questions, how did you come to study this topic?
3615156	3620356	A	0.8515427112579346	Were you studying causality and found predictive coding to be useful or vice versa?
3620388	3622300	A	0.8220124244689941	Or how did you come at this intersection?
3624240	3641250	B	0.565431535243988	Actually, I have to say that the first person that came out with this idea was Baron, I think a year and a half ago, even more, he brought a page with this idea and then it got forgotten and no one picked it up.
3642340	3654660	B	0.6733105778694153	And last summer I started getting curious about causality and I read, for example, the Book of Why, after listening to podcasts, the standard way in which you get interested in a topic.
3655480	3665032	B	0.7617251873016357	And I remember this idea from Baron and proposed it to him, and I was like, why don't we expand it and actually make it a paper?
3665166	3670650	B	0.7148863673210144	So I involved some people to help me with experiments, and this is the final result at the end.
3672060	3672760	A	0.9184247851371765	Awesome.
3672910	3673368	A	0.84200119972229	Cool.
3673454	3676152	A	0.6470907926559448	Yeah, a lot to say.
3676206	3680204	A	0.8787996172904968	I'm just going to go to the live chat first and address a bunch of different questions.
3680322	3681644	A	0.7737462520599365	And if anybody else wants to add.
3681682	3685840	B	0.5906122922897339	I'm going to turn the light on first because I think I'm getting in the dark more and more.
3685990	3686690	A	0.46103888750076294	Yes.
3688340	3691810	A	0.5274062156677246	Who said active inference can't solve the dark room issue?
3692260	3694050	B	0.5508298277854919	Oh, yes, here we are.
3694900	3698420	A	0.8876106142997742	So would you say the light switch caused it to be lighter?
3699400	3701830	B	0.6140062808990479	Yeah, I think so.
3702280	3703764	A	0.7217625975608826	No issues here.
3703962	3704660	A	0.584351658821106	Okay.
3704810	3705264	A	0.5522667169570923	ML.
3705312	3718420	A	0.6861298084259033	Don wrote, since in predictive coding all distributions are usually Gaussian, the bottom up messages are precision weighted prediction errors, where precision is the inverse of the Gaussian covariance.
3718580	3721710	A	0.822979211807251	What if non Gaussian distributions are used?
3725280	3728220	B	0.8377081155776978	Basically, the general method stays.
3729040	3739648	B	0.5648260116577148	The main difference is that you don't have prediction errors, which, as was correctly pointed out, is basically the derivative of the variation of free energy.
3739734	3743360	B	0.8262578248977661	If you have Gaussian assumptions yeah.
3743430	3752580	B	0.6566991806030273	You don't have that single quantity to set to zero and you probably will have to act on the structure of the graph to perform interventions.
3753960	3763210	A	0.8424979448318481	And also, you and colleagues had a paper in 2022 predictive coding beyond gaussian distributions that looked at some of these issues, right?
3763900	3765016	B	0.5172197222709656	Yes, exactly.
3765118	3772408	B	0.9040358066558838	So that paper was a little bit the idea behind that paper is can we model transformers?
3772504	3775208	B	0.8692876696586609	That's the biggest motivation using predictive coding.
3775304	3790556	B	0.5571963787078857	And the answer is no, because the attention mechanisms are softmax at the end and softmax calls not to Gaussian distribution, but to softmax distribution.
3790748	3792690	B	0.61231929063797	I don't get the name now, but yes.
3794420	3796288	B	0.6019730567932129	So yes, that's a generalization.
3796464	3803780	B	0.5065447688102722	It's a little bit tricky to call it once you remove the Gaussian assumption, it's a little bit still tricky to call it predictive coding.
3806840	3816440	B	0.7855468392372131	For example, like talking to Karl Friedston, predictive coding is only if you have only Gaussian assumptions.
3817740	3820970	B	0.764582097530365	But yes, that's more a philosophical debate than.
3822720	3823420	A	0.7792782783508301	Interesting.
3823570	3843100	A	0.8207565546035767	Another, I think topic that's definitely of great interest is similarities and differences between the attention apparatus in Transformers and the way that attention is described from a neurocognitive perspective and from a predictive processing precision weighting angle.
3843180	3844530	A	0.8296127915382385	What do you think about that?
3846340	3860490	B	0.6583959460258484	Well, the idea is that I think about it is that from a predictive processing and also vocational inference perspective, attention can be seen as a kind of structured learning problem.
3860940	3879740	B	0.8022174835205078	I think there's a recent paper from Chris Buckley's group that shows that there should be a reprint on Archive in which basically they showed that the attention mechanism is simply learning the precision on the weight parameters specific to a data point.
3879810	3885020	B	0.6773759722709656	So this precision is not a parameter that is in the structure of the model.
3885170	3893488	B	0.8228452801704407	So it's not a model specific parameter, but it's a fast changing parameter like the value nodes that gets updated while minimizing the vibrational free energy.
3893654	3900528	B	0.6362298727035522	And once you have minimized it and computed, then you throw it away and from the next data point you have to recompute it from scratch.
3900704	3914170	B	0.7648165822029114	So yes, I think the analogy computation wise is the attention mechanism can be seen as a kind of structured learning, but a structured learning that is data point specific and not model specific.
3915100	3929432	B	0.5377486348152161	And I think if you want to generalize a little bit and go from the attention mechanism in transformers to the attention mechanism, cognitive science, I feel they're probably too different to draw similarities.
3929496	3940210	B	0.7088366150856018	And I think the structured learning analogy and how important one connection is with respect to another one probably does the job much better.
3942020	3942480	A	0.84200119972229	Cool.
3942550	3943632	A	0.7966645956039429	Great answer.
3943766	3954150	A	0.8790286183357239	Okay, ML Don asks in counterfactuals what is the difference between hidden variables X and unobserved variables U?
3955480	3963370	B	0.5462675094604492	The difference is that I think the main one is that you cannot observe the use.
3963820	3971388	B	0.5021898150444031	You can use them because you can compute them and fix them, but the idea is that you have no control over them.
3971474	3984208	B	0.8547567129135132	So the use should be seen as environment specific variables that they are there, they influence your process because for example, when you go back in time, the environment is different.
3984294	3996900	B	0.8682273030281067	So the idea is, for example, if you like going back to the example before of the expected income of a person with a specific intelligence of education degree.
3997480	4011080	B	0.7471941113471985	The idea is that if I want to see how much I will earn today with a master degree is different with respect to how much I would earn 20 years ago with a master degree is different.
4011150	4017288	B	0.7134741544723511	For example, here in Italy with respect to other countries and all those variables that are not under your control.
4017374	4021420	B	0.682654082775116	You cannot model them using your Bayesian network, but they are there.
4021490	4021820	B	0.584351658821106	Okay.
4021890	4026488	B	0.5974103808403015	So you cannot ignore them when you want to draw conclusions.
4026664	4031484	B	0.5403584837913513	So, yeah, it's basically everything that you cannot control, you can infer them.
4031522	4044290	B	0.545505702495575	So you can perform a counterfactual inference back in time and say, oh, 20 years ago, I would have earned this much if I was this intelligent at this degree, on average, of course.
4045160	4052230	B	0.6237607002258301	But it's not that I can change the government policies towards jobs or things like that.
4052680	4054580	A	0.8459159135818481	It's a deeper counterfactual.
4055160	4056100	B	0.5172197222709656	Yes, exactly.
4056170	4057750	B	0.713576078414917	So those are the use.
4058380	4058888	A	0.9184247851371765	Awesome.
4058974	4059770	A	0.4896697998046875	All right.
4060220	4064680	A	0.8973140120506287	Have you implemented generalized coordinates in predictive coding?
4065820	4069130	B	0.5610135793685913	No, I've never done it.
4070560	4073404	B	0.5222882628440857	I've studied it, but I've never implemented it.
4073442	4079928	B	0.7897363305091858	I know they tend to be unstable, and it's very hard to make them stable.
4080024	4086770	B	0.7635596394538879	I think that's the takeaway that I got from talking to people that have implemented them.
4089220	4099380	B	0.886206865310669	But yeah, I'm aware of some papers that came out, actually recently about them that tested on some bracelet, encoder style, actually, I think still from Baron.
4100440	4103712	B	0.9084929823875427	There's a paper out there that came out last summer.
4103856	4106230	B	0.5893794894218445	But no, I've never played them with them myself.
4106680	4107430	A	0.84200119972229	Cool.
4107740	4109076	A	0.7654104828834534	From Bert.
4109268	4116600	A	0.622882068157196	Does adding more levels in the hierarchy reduce the distraction problem of predicting input?
4119020	4125176	B	0.5229514837265015	Adding more level, in which sense because the distraction problem is given by cycles.
4125208	4148736	B	0.5150395035743713	So basically you provide an image, and the fact that you have edges going out of the image going in the neurons, and then other edges going back, this basically creates the fact that basically these ingoing edges to the pixels of the image, they create some prediction errors.
4148768	4152390	B	0.7016822099685669	So you have some prediction errors that get spread inside the model.
4153480	4164760	B	0.5252798795700073	And this problem, I think, is general of cycles, and it's probably not related to hierarchy in general, is the two incoming edges to the pixels.
4165100	4170120	B	0.576613187789917	If you don't have incoming edges, you have no destruction problem anymore.
4170620	4171320	A	0.84200119972229	Cool.
4171470	4181240	A	0.9281741976737976	And the specification of the asyclic network through the trace operator, that's a very interesting technique.
4181320	4184670	A	0.8277287483215332	And when was that brought into play?
4186800	4192080	B	0.9131706357002258	As far as I know, I think it came out with a paper I cited in 2018.
4192660	4195360	B	0.7134192585945129	I don't know, at least in the causal inference literature.
4195940	4198864	B	0.7370724081993103	I'm not aware of any previous methods.
4198992	4202692	B	0.620436429977417	I would say no, because that's the highly cited paper.
4202826	4205190	B	0.8261889815330505	So I would say they came out with that idea.
4205560	4205924	A	0.7093686461448669	Wow.
4205962	4209876	B	0.944898247718811	But yeah, that's quite nice that you can do gradient descent and learn the structure.
4209908	4213556	B	0.920851469039917	I think that's a very powerful technique.
4213748	4221690	A	0.8389646410942078	Yeah, sometimes it's like when you look at when different features of Bayesian inference and causal inference became available.
4223500	4225080	A	0.8971476554870605	It's really remarkable.
4225820	4229544	A	0.5953534245491028	Why hasn't this been done under a Bayesian causal modeling framework?
4229592	4238912	A	0.5434657335281372	It's like because there's only been like five to 25 years of this happening, and so that's very short.
4239046	4244464	A	0.6460283994674683	And also it's relatively technical, so there's relatively few research groups engaging in it.
4244582	4248820	A	0.9495642185211182	And it's just really cool what it's enabling.
4250040	4250932	B	0.5172197222709656	Yes, exactly.
4250986	4271956	B	0.9159772992134094	I mean, that's also, I think, the exciting part of this field a little bit that is, I mean, there are definitely breakthroughs out there that still have to be discovered and probably because, for example, for as much as a breakthrough that paper was, they simply found the right prior for acyclic structures.
4272148	4279640	B	0.8477939963340759	Okay, yeah, I don't know exactly, but it may be an idea that you have in one afternoon.
4279720	4286780	B	0.8191521763801575	I don't know about the story of how the authors came up with that, but could potentially be that they're there at the whiteboard.
4286860	4288528	B	0.5576450228691101	You're like, oh, that actually works.
4288694	4289948	B	0.9683631062507629	That's a huge breakthrough.
4289964	4293520	B	0.76337730884552	And I simply defined the prior.
4294020	4299328	A	0.606876015663147	And also, a lot of these breakthroughs, they don't just stack.
4299504	4303700	A	0.6642942428588867	It's not like a tower of blocks.
4304120	4306340	A	0.7257816195487976	They layer and they compose.
4306840	4317156	A	0.8980790376663208	So then something will be generalized to generalized coordinates or generalized synchrony, or arbitrarily large graphs or sensor fusion with multimodal inputs.
4317188	4322828	A	0.9317895770072937	And it's like those all blend in really satisfying and effective ways.
4322994	4329500	A	0.7213817834854126	So even little things that, again, someone can just come up with in a moment can really have impact.
4331360	4332584	A	0.6859192252159119	Okay, ML.
4332632	4338156	A	0.9735068082809448	Don says, thanks a lot for asking my questions, and thanks a million to Tomaso for the inspiring presentation.
4338268	4339552	A	0.9746702909469604	So nice.
4339686	4340656	B	0.9599630236625671	Thank you very much.
4340758	4348660	A	0.8797464966773987	And then Bert asks, how would language models using predictive coding differ from those using transformers?
4352600	4359668	B	0.7157334685325623	Okay, I think that actually, if I would have to build today a language model using predictive coding, I would still use transformers.
4359844	4376940	B	0.8762450218200684	So the idea is that, for example, if you have, let's say, this hierarchical graphical model or this hierarchical Bayesian network, I've defined in the very first slides one arrow to encode a function, which is the linear map.
4377280	4377644	B	0.7123861312866211	Okay?
4377682	4388208	B	0.8948360085487366	So one arrow was simply the multiplication of the vector encoded in the latent variables times this weight matrix that you can then make nonlinear and things like that.
4388294	4390668	B	0.7487645149230957	But that can be actually something much more complex.
4390844	4395520	B	0.8130747079849243	The function encoded in the arrow can be a convolution, can be an attention.
4397700	4409724	B	0.8053725957870483	So actually, how I would do it, I will still use the which is actually the way we did it in the Oxford group last year, is that we had exactly the structure.
4409792	4411800	B	0.6896178722381592	Every arrow is a transformer now.
4411950	4416484	B	0.855379045009613	So one is the attention mechanism and the next one is the feed forward network.
4416532	4417720	B	0.8457220792770386	As transformers.
4418700	4428280	B	0.8044989705085754	Basically, the only difference that you have is that those variables, you want to compute the posterior and you make those posteriors independent via minfield approximation.
4428440	4433488	B	0.8611141443252563	So basically, you follow all the steps that allow you to converge to the variation of.
4433494	4435276	B	0.5912158489227295	Free energy of creative coding.
4435468	4443920	B	0.8672873377799988	But the way you compute predictions and the way you send signals back is done via transformer.
4444740	4447712	B	0.5858524441719055	So I will still use transformers in general.
4447766	4456950	B	0.6131527423858643	I mean, they work so well that I don't think that we can be arrogant and say, oh no, I'm going to do it better via a purely predictive coding way.
4457560	4462070	B	0.8155683279037476	Structure learning is a way, but will still approximate transformers anyway.
4462520	4467160	A	0.6100999116897583	Sorry, you said structured learning would approximate the transformer approach.
4467500	4476380	B	0.7640239000320435	Yes, the structure learning I mentioned earlier when someone asked the similarities between creative coding and the attention mechanism.
4479360	4480750	A	0.8810969591140747	Yeah, very interesting.
4482880	4489600	A	0.6325201988220215	One thing I am wondering from ML Don, I could not see the concept of depth in the predictive coding networks you mentioned.
4489670	4490656	A	0.5518553256988525	Most likely I missed it.
4490678	4496100	A	0.8805451989173889	The definition provided for predictive coding involved the concept of depth.
4496600	4498340	A	0.8502371311187744	What did you mean by depth?
4499480	4499796	B	0.4936186671257019	No.
4499818	4506544	B	0.6878571510314941	Yes, it's true because the standard definition, as I said multiple times, is hierarchical.
4506592	4510440	B	0.5038245916366577	You have predictions going one direction and prediction error going the opposite direction.
4510780	4532620	B	0.7955841422080994	Basically, what we did in this paper and also in the last one, which is called learning on arbitrary graph topologies via rated coding, is that we can consider depth as independent, basically pair of latent variable, latent variable and arrow.
4533120	4536992	B	0.8507257699966431	And you have predictions going that direction and prediction arrow going the other.
4537126	4541490	B	0.8069916367530823	But then you can compose these in a lot of ways.
4544180	4549664	B	0.8350487947463989	So basically this composition doesn't have to be hierarchical in the end, can have cycles.
4549712	4556612	B	0.8822250962257385	So then you can, for example, plug in another latent variable to the first one and then connect the other two.
4556746	4560228	B	0.798353910446167	And you can have a structure that is as entangled as you want.
4560394	4567348	B	0.8900561332702637	So, for example, in the other paper, we trained a network that has the shape of a brain structure.
4567444	4573710	B	0.6365949511528015	So we have a lot of brain regions that are sparsely connected inside and sparsely connected among each other.
4574400	4576972	B	0.571976363658905	And there's nothing hierarchical there at the end.
4577026	4583840	B	0.7928745746612549	But you can still train it by minimizing abbreviation of free energy and by minimizing the total prediction error of the network.
4585700	4599108	A	0.7993105053901672	So you could have for a given motif in an entangled graph, you might see three successive layers that when you looked at them alone, you'd say, oh, that's a three story building.
4599274	4602708	A	0.8034633994102478	That's a three layer model that has a depth of three.
4602794	4611160	A	0.6443718671798706	But then when you take a bigger picture, there isn't like an explicit top or an explicit bottom to that network.
4612140	4613048	B	0.5172197222709656	Yes, exactly.
4613134	4618890	B	0.7415106296539307	And this is basically given by the fact that every operation in predictive coding networks is strictly local.
4619900	4627224	B	0.5152888894081116	So basically every message passing, every prediction and every prediction error that you send, you only send it to the very nearby neurons.
4627352	4627692	B	0.7123861312866211	Okay?
4627746	4635330	B	0.5387967228889465	And whether the global structure is actually hierarchical or not, the single message passing doesn't even see that.
4637540	4641776	A	0.7168453931808472	I guess that's sort of the hope for learning.
4641878	4648720	A	0.8423924446105957	New model architectures is the space of what is designed.
4648800	4664136	A	0.6057367920875549	Top down is very small and a lot of models in use today, albeit super effective models, although you could ask effective per unit of compute or not, that's a second level question.
4664238	4689440	A	0.5368865728378296	But a lot of effective models today do not have some of these properties of predictive coding networks like their capacity to use only local computations which gives biological realism or just spatiotemporal realism, but also may provide a lot of advantages in federated compute or distributed computing settings.
4690500	4690864	B	0.6175195574760437	No?
4690902	4691424	B	0.5172197222709656	Yes, exactly.
4691462	4692610	B	0.7494814395904541	I completely agree.
4692980	4700710	B	0.8543403148651123	I think the idea in general is that and I don't know if that's going to be an advantage, I think it's very promising exactly for the reasons you said.
4701640	4722344	B	0.8262279629707336	And the reason is that today's models trained with backpropagation, you can basically summarize them as a model train with backpropagation is a function because basically you have a map from input to output and backpropagation basically spreads information back from its computational graph.
4722472	4740610	B	0.7639521360397339	So every neural network model used today is a function while predictive coding and other liberative coding, like the old class of functions, class of methods that train using local computations and actually work by minimizing a global energy function.
4741460	4744988	B	0.8298527598381042	They're not limited to model functions from input to output.
4745084	4748912	B	0.8301775455474854	They actually model something that kind of resembles physical systems.
4749056	4761328	B	0.861353874206543	So you have a physical system, you fix some values to whatever input you have and you let the system converge and then you read some other neurons or variables that are supposed to be output.
4761424	4770250	B	0.7068589329719543	But this physical system doesn't have to be a fit forward map, doesn't have to be a function that has an input space and an output space and that's it.
4770620	4781032	B	0.7382678985595703	So the class of models that you can learn so basically you can see like feed forward models and functions and then a much bigger class which is that of physical systems.
4781176	4786944	B	0.8368256688117981	Whether there's something interesting out here, I don't know yet because the functions are working extremely well.
4786982	4789440	B	0.7975849509239197	We are seeing those days with back propagation.
4789940	4791410	B	0.7739404439926147	They work crazy well.
4792420	4797492	B	0.46118611097335815	I don't know if there's anything interesting in the big part, but the big part is quite big.
4797546	4798148	B	0.584351658821106	Okay.
4798314	4805044	B	0.5486470460891724	There are a lot of models that you cannot train with back propagation and you can train with creative coding or.
4805082	4809832	A	0.9358031749725342	Evidence propagation or other methods that is super interesting.
4809966	4819000	A	0.5009868144989014	Certainly biological systems, physical systems solve all kinds of interesting problems, but there's still no free lunch.
4819340	4824232	A	0.5043739080429077	An ant species that does really well in this environment might not do very well in another environment.
4824296	4850180	A	0.8852141499519348	And so out there in the hinterlands there might be some really unique special algorithms that are not well described by being a function yet still provide like a procedural way to implement Heuristics which might be extremely, extremely effective.
4851080	4851444	B	0.6175195574760437	No?
4851482	4852630	B	0.5172197222709656	Yes, exactly.
4853400	4857964	B	0.5805808305740356	And I think this has been most of my focus of research during my PhD.
4858032	4865080	B	0.7993093729019165	For example, like finding this application that is like out here and not inside the functions.
4867180	4867640	B	0.84200119972229	Cool.
4867710	4880830	A	0.7580386996269226	Well, where does this work go from here, like what directions are you excited about and how do you see people in the octave inference ecosystem getting involved in this type of work.
4882480	4895644	B	0.8975405693054199	I think probably the most promising direction, which is something maybe I would like to explore a little bit, is to, as I said earlier, is to go behind statical models.
4895772	4900340	B	0.8678740859031677	So everything I've shown so far is about static data.
4900490	4902804	B	0.7651881575584412	So the data don't change over time.
4902842	4908710	B	0.7561767101287842	There's no time inside the definition of creative coding as it is, as I presented it here.
4909080	4923390	B	0.7258338332176208	However, you can, for example, generalize creative coding to work with temporal data using generalized coordinates, as you mentioned earlier, by presenting it as a Kalman filter generative model.
4925360	4955540	B	0.7953667044639587	And that's where, for example, the causal inference direction could be very useful because at that point maybe you can be able to model greater causality and more complex and useful dynamical causal models, basically because in general, the Ducalculus and interventional and counterfactual branch of science is mostly developed on small models.
4958540	4962552	B	0.5353240966796875	You don't do interventions on gigantic models in general.
4962686	4969000	B	0.814915657043457	So if you look at medical data, they use relatively small Bayesian networks.
4969740	4986928	B	0.836105227470398	But of course, if you want to have a dynamical causal model that models a specific environment or a specific reality, you have a lot of neurons inside, you have a lot of latent variables, they change over time and an intervention at some moment creates an effect in a different time step.
4987014	4990592	B	0.8819732666015625	So maybe in the next time step in ten different time steps later.
4990726	5001184	B	0.8844102025032043	And I think that would be very interesting to develop like a biologically plausible way of passing information that is also able to model grandeur causality.
5001232	5001830	B	0.49467554688453674	Basically.
5004680	5007940	A	0.8665720224380493	Where do you see action in these models?
5010780	5012360	B	0.6680782437324524	Where do I see action?
5013900	5015530	B	0.5680768489837646	I didn't think of that.
5016460	5025740	B	0.8444129228591919	I think I see actions in those models maybe in the same way as you see in other models because creative coding is basically a model of perception.
5026960	5032300	B	0.8670129776000977	So an action is you can see it as a consequence of what you are experiencing.
5032660	5042210	B	0.622981071472168	So by changing the way you're experiencing something, then you can compute, maybe you can simply perform as smarter actions now that you have more information.
5044420	5060570	B	0.6027988791465759	But yeah, I don't think action is very I don't see any explicit consequence of actions besides the fact that this can allow you to basically maybe to simply draw better conclusions to then perform actions in the future.
5061980	5067240	A	0.8427939414978027	I'll add on to that a few ways that people have talked about predictive coding and action.
5067660	5073464	A	0.7709742784500122	First off, internal action or covert action is attention.
5073592	5077032	A	0.8784998655319214	So we can think about perception as an internal action.
5077096	5078040	A	0.7316808104515076	That's one approach.
5078120	5082264	A	0.7363994121551514	Another approach, pretty micro, is the outputs of a given node.
5082392	5090544	A	0.8661481142044067	We could understand that node as a particular thing with its own sensory, cognitive and action states.
5090742	5093756	A	0.8249314427375793	And so in that sense, the output of a node.
5093948	5100944	A	0.8360913991928101	And then lastly, which we explored a little bit in Live Stream 43 on the theoretical review on predictive coding.
5101072	5104304	A	0.7798688411712646	We were reading all the way through, and it was all about perception.
5104352	5105136	A	0.7001795768737793	All about perception.
5105168	5117140	A	0.6397597193717957	And then it was like Section 5.3 if you have expectations about action, then action is just another variable in this architecture.
5117300	5129608	A	0.7543008327484131	And that's really aligned with inactive inference, where instead of having, like, a reward or utility function that we maximize, we select action based upon it being the, likeliest, course of action, the path of least action.
5129704	5130904	A	0.7435149550437927	That's Bayesian Mechanics.
5130952	5145056	A	0.5036931037902832	And so it's actually very natural to bring in an action variable and utilize it essentially as if it were a prediction about something else exterceptibly in the world.
5145158	5147360	A	0.7889655828475952	Because we're also expecting action.
5148500	5149020	B	0.4936186671257019	No.
5149110	5150070	B	0.5172197222709656	Yes, exactly.
5150760	5151124	B	0.4936186671257019	No.
5151162	5153510	B	0.8880899548530579	I like the way of defining actions a lot, actually.
5154280	5160788	B	0.6886491775512695	And I still think it has been, like, for example, there are not so many papers that apply this method.
5160884	5177550	B	0.8274813294410706	I think there are a couple from Alexander Orobria does something similar, but in, like, outside of the pure active inference, like, applying predictive coding and actions to solve practical problems hasn't been explored a lot.
5179600	5180108	A	0.84200119972229	Cool.
5180194	5184764	A	0.9875056743621826	Well, thank you for this excellent presentation and discussion.
5184812	5189730	A	0.8929659724235535	Is there anything else that you want to say or point people towards?
5190820	5198770	B	0.9884575009346008	No, just a big thank you for inviting me, and it was really fun, and I hope to come back at some point for some future.
5201380	5202348	A	0.5685878992080688	Anytime.
5202444	5203324	A	0.5685878992080688	Anytime.
5203452	5204716	A	0.8805512189865112	Thank you, Thomas.
5204908	5205424	B	0.8529649376869202	Thank you.
5205462	5205996	B	0.6503534913063049	Daniel.
5206108	5206640	A	0.639849066734314	See you.
5206710	5207364	A	0.5137446522712708	Bye.
5207532	5208250	B	0.5664745569229126	Right.

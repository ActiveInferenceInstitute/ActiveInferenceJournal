1
00:00:19,020 --> 00:00:21,119
안녕하세요.

2
00:00:21,119 --> 00:00:23,400


3
00:00:23,400 --> 00:00:28,140
2023년 7월 28일 활성 추론 게스트 스트림 번호 51.1입니다.

4
00:00:28,140 --> 00:00:31,439
저희는 Tomaso Salvatore와 함께 있으며

5
00:00:31,439 --> 00:00:33,840


6
00:00:33,840 --> 00:00:37,020


7
00:00:37,020 --> 00:00:39,660
예측 코딩을 통한 최근 작업 인과 추론에 대한 프레젠테이션과 토론을 할 예정입니다.

8
00:00:39,660 --> 00:00:42,480


9
00:00:42,480 --> 00:00:44,340
시청해 주시는 분들을 위해 참여해 주셔서 감사합니다.

10
00:00:44,340 --> 00:00:47,520
라이브 채팅에서 자유롭게 질문을 작성하고

11
00:00:47,520 --> 00:00:50,399
당신에게 감사합니다

12
00:00:50,399 --> 00:00:52,980
다니엘을 초대해 주셔서 대단히 감사합니다

13
00:00:52,980 --> 00:00:56,039
어 항상 채널의 열렬한 팬이었고

14
00:00:56,039 --> 00:00:57,719
나는 많은 비디오를 보고 있습니다

15
00:00:57,719 --> 00:00:58,920
I'm

16
00:00:58,920 --> 00:01:01,379
I'm 꽤  여기 와서

17
00:01:01,379 --> 00:01:04,260
이번에 연설하는 사람이 되어 기쁩니다.

18
00:01:04,260 --> 00:01:06,600
그래서 지난 몇 달 동안 작업한 최근 사전 인쇄에 대해 이야기하려고 합니다.

19
00:01:06,600 --> 00:01:08,700


20
00:01:08,700 --> 00:01:11,159


21
00:01:11,159 --> 00:01:12,119


22
00:01:12,119 --> 00:01:15,900
이것은

23
00:01:15,900 --> 00:01:18,659
with lookup과의 공동 작업입니다.  Ketty에서 나는 makarak

24
00:01:18,659 --> 00:01:21,600
barami Legend Thomas lukasiavich에

25
00:01:21,600 --> 00:01:24,000
있으며 이것은 기본적으로

26
00:01:24,000 --> 00:01:26,299
내가 옥스퍼드 대학교에서 일하는 회사인 구절

27
00:01:26,299 --> 00:01:31,680
과 uhuvian 사이의 공동 작업이므로

28
00:01:31,680 --> 00:01:34,200


29
00:01:34,200 --> 00:01:36,299
이 강연 중에

30
00:01:36,299 --> 00:01:38,220


31
00:01:38,220 --> 00:01:40,979
기본적으로 이것이 내가

32
00:01:40,979 --> 00:01:43,140
이야기할 강연의 개요입니다.  예측

33
00:01:43,140 --> 00:01:44,520
코딩이 무엇인지

34
00:01:44,520 --> 00:01:47,659
그리고 그것이 무엇인지에 대한 주어진 상호 작용

35
00:01:47,659 --> 00:01:51,299
간단한 역사적 소개 예를 들어

36
00:01:51,299 --> 00:01:54,060


37
00:01:54,060 --> 00:01:56,159


38
00:01:56,159 --> 00:01:58,619
기계 학습 관점에서 창의적인 코딩을 연구하는 것이 중요하다고 생각하는 이유 그런

39
00:01:58,619 --> 00:02:00,720
다음 인과

40
00:02:00,720 --> 00:02:04,560
추론이 무엇인지에 대한 작은 소개를 제공하고

41
00:02:04,560 --> 00:02:07,200
일단 우리가 그 모든 정보를 가지고 있다면

42
00:02:07,200 --> 00:02:08,880
저는 저와 다른 협력자들에게 영감을 준 연구 질문이 기본적으로

43
00:02:08,880 --> 00:02:12,540
이 논문을 쓴 이유에 대해 논의

44
00:02:12,540 --> 00:02:14,520


45
00:02:14,520 --> 00:02:16,560


46
00:02:16,560 --> 00:02:18,300


47
00:02:18,300 --> 00:02:21,660
하고 추론을 수행하는 방법에 대한 주요 결과를 제시할 것입니다.

48
00:02:21,660 --> 00:02:24,980


49
00:02:24,980 --> 00:02:27,480


50
00:02:27,480 --> 00:02:29,340


51
00:02:29,340 --> 00:02:33,319


52
00:02:33,319 --> 00:02:35,879
예측 코딩을 사용하여 주어진 데이터 세트에서 인과 구조를 학습하는 방법,

53
00:02:35,879 --> 00:02:37,920
그리고 물론

54
00:02:37,920 --> 00:02:39,959


55
00:02:39,959 --> 00:02:43,500
간단한 요약과

56
00:02:43,500 --> 00:02:45,840
이 작업이 실제로

57
00:02:45,840 --> 00:02:49,940
영향을 미칠 수 있다고 생각하는 이유와 몇 가지 향후 방향에 대한 토론으로 결론을 내릴 것입니다.

58
00:02:50,700 --> 00:02:53,400


59
00:02:53,400 --> 00:02:55,680
일반적으로

60
00:02:55,680 --> 00:02:58,440
신경과학에서 영감을 얻은 학습

61
00:02:58,440 --> 00:03:01,140
방법으로 유명하므로

62
00:03:01,140 --> 00:03:04,560
뇌에서 정보 처리가 작동하는 방식에 대한 이론과

63
00:03:04,560 --> 00:03:05,819


64
00:03:05,819 --> 00:03:08,400
매우 공식적으로 말해서

65
00:03:08,400 --> 00:03:10,560
창의적인 코딩 계층은

66
00:03:10,560 --> 00:03:12,659
기본적으로

67
00:03:12,659 --> 00:03:16,319
뇌에서 뉴런의 계층적 구조를 갖는 것으로 설명될 수 있습니다.

68
00:03:16,319 --> 00:03:19,080


69
00:03:19,080 --> 00:03:20,700
뇌에는 두 개의 서로 다른 뉴런 계열이 있습니다.

70
00:03:20,700 --> 00:03:23,280
첫 번째 계열은

71
00:03:23,280 --> 00:03:24,480


72
00:03:24,480 --> 00:03:27,659


73
00:03:27,659 --> 00:03:29,959
계층 구조의 특정 수준에 있는 뉴런이 정보를 보내고 아래 수준의

74
00:03:29,959 --> 00:03:33,959
활동을 예측

75
00:03:33,959 --> 00:03:35,940


76
00:03:35,940 --> 00:03:38,340
하고 두 번째 계열의 예측 정보 전송을 담당하는 것입니다.  뉴런은

77
00:03:38,340 --> 00:03:41,099
오류 뉴런의 뉴런이며 계층 구조 위로

78
00:03:41,099 --> 00:03:43,019
예측 오류 정보를 보내는 화살표 뉴런

79
00:03:43,019 --> 00:03:46,319
이므로 한 수준은 이 활동

80
00:03:46,319 --> 00:03:49,200
아래 수준의 활동을 예측합니다.

81
00:03:49,200 --> 00:03:51,659
이러한 예측은

82
00:03:51,659 --> 00:03:54,239
실제로

83
00:03:54,239 --> 00:03:56,220
아래 수준에서 진행되는 일부 불일치

84
00:03:56,220 --> 00:03:57,840
와 관련 정보가 있습니다.  예측

85
00:03:57,840 --> 00:04:02,400
오류는 화살표 키 위로 전송되지만

86
00:04:02,400 --> 00:04:04,860
예측 코딩은

87
00:04:04,860 --> 00:04:07,220
실제로 신경 과학

88
00:04:07,220 --> 00:04:10,799
의 이론으로 신경 과학으로 태워지지 않았지만

89
00:04:10,799 --> 00:04:11,939


90
00:04:11,939 --> 00:04:13,860
실제로 처음에는 50 년대에

91
00:04:13,860 --> 00:04:16,139
신호 처리 및 압축 방법으로 개발되었으므로

92
00:04:16,139 --> 00:04:19,380


93
00:04:19,380 --> 00:04:21,899
작업  Shannon의 Shannon과 실제로 동시대인인 Oliver Elias는

94
00:04:21,899 --> 00:04:25,020


95
00:04:25,020 --> 00:04:26,160


96
00:04:26,160 --> 00:04:27,960
일단 우리가 예측자의

97
00:04:27,960 --> 00:04:30,900


98
00:04:30,900 --> 00:04:33,600


99
00:04:33,600 --> 00:04:36,000
오류에 대한 메시지를 보내는 데이터를 예측하는 데 잘 작동하는 모델이 있으면

100
00:04:36,000 --> 00:04:37,919


101
00:04:37,919 --> 00:04:41,100
전체 메시지를 매일 보내는 것보다 실제로 훨씬 저렴하다는 것을 깨달았습니다.

102
00:04:41,100 --> 00:04:42,720


103
00:04:42,720 --> 00:04:45,240


104
00:04:45,240 --> 00:04:47,639


105
00:04:47,639 --> 00:04:49,500


106
00:04:49,500 --> 00:04:52,020


107
00:04:52,020 --> 00:04:53,639
50년대에 정보이론에서 신호처리와 압축메커니즘으로 예쁜 코딩이 탄생한게

108
00:04:53,639 --> 00:04:57,120
사실 80년대에 어

109
00:04:57,120 --> 00:04:59,400


110
00:04:59,400 --> 00:05:01,800


111
00:05:01,800 --> 00:05:03,540
신경과학과

112
00:05:03,540 --> 00:05:07,500
어에서 정확히 같은 모델이 쓰이게 되었어요  예를 들어 Mumford의 연구 또는

113
00:05:07,500 --> 00:05:10,440
다른 연구에서는

114
00:05:10,440 --> 00:05:12,960
프로세스 정보의 평가 방법을 설명하여

115
00:05:12,960 --> 00:05:14,520


116
00:05:14,520 --> 00:05:17,160
외부 세계에서 예측 신호를 얻고 이 표현을 압축해야 합니다.

117
00:05:17,160 --> 00:05:20,280


118
00:05:20,280 --> 00:05:22,740


119
00:05:22,740 --> 00:05:25,199


120
00:05:25,199 --> 00:05:27,720


121
00:05:27,720 --> 00:05:30,419


122
00:05:30,419 --> 00:05:32,900
50년대에 Elias와 Oliver가 개발한 것과 유사하지는 않더라도 매우 유사합니다.

123
00:05:32,940 --> 00:05:35,520


124
00:05:35,520 --> 00:05:38,100


125
00:05:38,100 --> 00:05:41,400
Ballard 주변의 작업 덕분에 1999년에 가장 큰 패러다임 전환이 일어났을 것입니다.

126
00:05:41,400 --> 00:05:44,880


127
00:05:44,880 --> 00:05:46,199


128
00:05:46,199 --> 00:05:48,060


129
00:05:48,060 --> 00:05:51,360
예측 정보가

130
00:05:51,360 --> 00:05:54,240
하향식이고 오류 정보가 상향식인 뇌의 계층적 구조에 대해 앞서 언급한 바 있으며

131
00:05:54,240 --> 00:05:55,560


132
00:05:55,560 --> 00:05:57,660


133
00:05:57,660 --> 00:05:59,759
이전에 수행되지 않은 작업은

134
00:05:59,759 --> 00:06:02,820


135
00:06:02,820 --> 00:06:05,759
프랑스뿐만 아니라

136
00:06:05,759 --> 00:06:07,979
학습이 뇌에서 어떻게 작동하는지에 대한

137
00:06:07,979 --> 00:06:10,139
것이므로 우리의 시냅스가 어떻게 업데이트되는지에 대한 이론이기도 합니다.

138
00:06:10,139 --> 00:06:13,139


139
00:06:13,139 --> 00:06:16,080
제가

140
00:06:16,080 --> 00:06:17,940
이 간략한 역사적 소개에서 이야기할 마지막 큰 돌파구는

141
00:06:17,940 --> 00:06:21,900
2003년

142
00:06:21,900 --> 00:06:25,380
부터입니다.  몇

143
00:06:25,380 --> 00:06:28,380
년 후 uh car freeston 덕분에

144
00:06:28,380 --> 00:06:32,100
그는 기본적으로 Robin Ballard의 이론을 받아들여

145
00:06:32,100 --> 00:06:35,039


146
00:06:35,039 --> 00:06:38,400
그것을 확장하고

147
00:06:38,400 --> 00:06:40,919
생성 모델 이론으로 일반화했습니다. 그래서

148
00:06:40,919 --> 00:06:42,720
기본적으로

149
00:06:42,720 --> 00:06:45,479
carfiston이 한 주요 주장은 창의적인 코딩이

150
00:06:45,479 --> 00:06:48,780


151
00:06:48,780 --> 00:06:50,340


152
00:06:50,340 --> 00:06:52,979


153
00:06:52,979 --> 00:06:55,139


154
00:06:55,139 --> 00:07:00,300


155
00:07:00,300 --> 00:07:01,560


156
00:07:01,560 --> 00:07:03,900


157
00:07:03,900 --> 00:07:05,340
신호 처리 및

158
00:07:05,340 --> 00:07:07,020
압축과  일반적으로

159
00:07:07,020 --> 00:07:09,180
망막과 뇌에서 정보 처리는

160
00:07:09,180 --> 00:07:11,160
추론

161
00:07:11,160 --> 00:07:12,300
방법

162
00:07:12,300 --> 00:07:14,819
이며 가장 큰

163
00:07:14,819 --> 00:07:17,819
변화는 1999년에 우리가 겪었던 가장 큰 혁명입니다.

164
00:07:17,819 --> 00:07:21,120


165
00:07:21,120 --> 00:07:23,580


166
00:07:23,580 --> 00:07:25,919


167
00:07:25,919 --> 00:07:29,699
그런 다음 생성 모델 자체를 개선하기 위해 생성 모델에 있는 모든

168
00:07:29,699 --> 00:07:31,800
시냅스 또는 모든 잠재 변수를 업데이트합니다.

169
00:07:31,800 --> 00:07:34,139


170
00:07:34,139 --> 00:07:38,599


171
00:07:38,759 --> 00:07:43,199
따라서

172
00:07:43,199 --> 00:07:45,000


173
00:07:45,000 --> 00:07:48,479
작동 코딩이 계층적 가우시안 생성으로 볼 수 있도록 조금 더 형식적인 정의를 제공하겠습니다.

174
00:07:48,479 --> 00:07:50,220


175
00:07:50,220 --> 00:07:53,400
여기

176
00:07:53,400 --> 00:07:54,780
우리가

177
00:07:54,780 --> 00:07:58,319
원하는 만큼 깊을 수 있는 이 계층 구조가 있고

178
00:07:58,319 --> 00:08:01,560
신호 예측 신호가

179
00:08:01,560 --> 00:08:04,620
하나의 잠재 변수 XM에서 다음 변수로 이동

180
00:08:04,620 --> 00:08:06,599
하고

181
00:08:06,599 --> 00:08:09,720
함수 GN 또는 GI를 통해 매번 변환되는 매우 간단한 그림이 있습니다.

182
00:08:09,720 --> 00:08:12,620


183
00:08:15,319 --> 00:08:18,180
이것은 내가 말했듯이 생성 모델이고

184
00:08:18,180 --> 00:08:19,680
이 생성 모델의 한계 확률은 무엇입니까

185
00:08:19,680 --> 00:08:21,780
그것은 단순히

186
00:08:21,780 --> 00:08:24,960
마지막 확률입니다.

187
00:08:24,960 --> 00:08:27,660
제 커서를 볼 수 있습니까? 예 맞습니다.

188
00:08:27,660 --> 00:08:29,940
완벽하므로

189
00:08:29,940 --> 00:08:32,700
마지막 꼭지점의 유전 모델은 죄송합니다 아마 분포입니다

190
00:08:32,700 --> 00:08:34,979
마지막 꼭짓점의 곱하기

191
00:08:34,979 --> 00:08:37,140


192
00:08:37,140 --> 00:08:40,440
다른 모든 꼭지점의 확률 분포는

193
00:08:40,440 --> 00:08:43,020
이전의 꼭지점 활동 또는

194
00:08:43,020 --> 00:08:45,860
이전의 잠재 변수에 따라 결정됩니다.

195
00:08:45,899 --> 00:08:48,240
저는 이미 가우시안

196
00:08:48,240 --> 00:08:50,399
생성 모델이라고 말했는데, 이는

197
00:08:50,399 --> 00:08:54,260
가우시안 형태의 확률

198
00:08:54,660 --> 00:08:57,120
과 모든

199
00:08:57,120 --> 00:09:00,480
엔도스 함수를 의미합니다.  일반적으로 함수 G 그리고

200
00:09:00,480 --> 00:09:02,880
어 특히 예를 들어

201
00:09:02,880 --> 00:09:05,459
Rambler 논문과 그 이후에 나온 모든 논문에서

202
00:09:05,459 --> 00:09:07,920
딥

203
00:09:07,920 --> 00:09:10,500
러닝 혁명으로 인해 이러한 함수는

204
00:09:10,500 --> 00:09:13,220
단순히 선형 맵 또는

205
00:09:13,220 --> 00:09:15,120
활성화 함수가 있는 비선형 맵

206
00:09:15,120 --> 00:09:18,000
또는

207
00:09:18,000 --> 00:09:22,040
활성화 함수가 있는 비선형 맵이고  가산적 편향을

208
00:09:23,220 --> 00:09:27,180
통해

209
00:09:27,180 --> 00:09:28,860
창의적인 코딩에 대한 공식적인 정의를 내릴 수 있고

210
00:09:28,860 --> 00:09:30,300
운영 코딩은 일반적으로 자유 에너지의 변화라고 하는 양을 최소화하여 모델 증거가 최대화되는

211
00:09:30,300 --> 00:09:33,480
생성 모델에 대한 반전 체계라고 말할 수 있습니다.

212
00:09:33,480 --> 00:09:35,839


213
00:09:35,839 --> 00:09:38,760


214
00:09:38,760 --> 00:09:40,920


215
00:09:40,920 --> 00:09:43,740
모든 생성 모델의 목표는

216
00:09:43,740 --> 00:09:46,019
모델 증거를 최대화하는 것이지만

217
00:09:46,019 --> 00:09:48,860
이 양은 항상 다루기 어렵습니다.

218
00:09:48,860 --> 00:09:51,019


219
00:09:51,019 --> 00:09:53,279
우리는

220
00:09:53,279 --> 00:09:55,980


221
00:09:55,980 --> 00:09:58,500


222
00:09:58,500 --> 00:10:00,720
자유 에너지의 수차를 최소화하는 대신 크리에이티브 코딩에서 사용하는 솔루션과 솔루션을 근사화할 수 있는 몇 가지 기술을 가지고 있습니다.

223
00:10:00,720 --> 00:10:03,480
이것은

224
00:10:03,480 --> 00:10:06,839
이 작업에서 모델 증거의 하한이며

225
00:10:06,839 --> 00:10:09,660
실제로 다른 많은 작업에서 많이 사용되므로 이를

226
00:10:09,660 --> 00:10:11,700
수행하는 표준 방법입니다.

227
00:10:11,700 --> 00:10:13,740
이 최소화는

228
00:10:13,740 --> 00:10:16,080
성분 하강을 수행

229
00:10:16,080 --> 00:10:18,540
합니다.

230
00:10:18,540 --> 00:10:19,980
실제로 종종 동등한 기대 최대화와 같은 다른 방법

231
00:10:19,980 --> 00:10:22,140


232
00:10:22,140 --> 00:10:23,580


233
00:10:23,580 --> 00:10:25,140
또는 예를 들어 신념 전파와 같은 다른 메시지 전달 알고리즘을 사용할 수 있으며

234
00:10:25,140 --> 00:10:26,940


235
00:10:26,940 --> 00:10:29,959


236
00:10:30,720 --> 00:10:33,980
시간을 조금 거슬러 올라가서

237
00:10:33,980 --> 00:10:35,940


238
00:10:35,940 --> 00:10:38,760


239
00:10:38,760 --> 00:10:41,360
창의적인 코딩을 볼 수 있다면 통계적 생성 모델에 대해 조금 잊어버릴 수 있습니다.

240
00:10:41,360 --> 00:10:44,040
내 말은 내가 이미 신경 활동이 있는

241
00:10:44,040 --> 00:10:46,200
계층적 모델로 몇 번 말했으므로

242
00:10:46,200 --> 00:10:48,420


243
00:10:48,420 --> 00:10:50,700


244
00:10:50,700 --> 00:10:53,459
신경 활동을 나타내는 뉴런 잠재 변수를 사용하면 발신자 신호가 계층 아래로 내려가고

245
00:10:53,459 --> 00:10:54,899


246
00:10:54,899 --> 00:10:57,540
오류 노드 또는 오류 뉴런이 있으면

247
00:10:57,540 --> 00:11:01,019
발신자 신호가 계층 위로 올라갑니다.

248
00:11:01,019 --> 00:11:03,660
오류 정보는

249
00:11:03,660 --> 00:11:05,700


250
00:11:05,700 --> 00:11:08,220
이 클래스에서 작동하는 코딩 모델의 자유 에너지 변화량입니다.

251
00:11:08,220 --> 00:11:09,899
단순히

252
00:11:09,899 --> 00:11:12,720
모든 오류 뉴런의 평균 제곱 오류의 합

253
00:11:12,720 --> 00:11:14,399


254
00:11:14,399 --> 00:11:18,120
이므로 는

255
00:11:18,120 --> 00:11:21,980
총 오류 제곱의 오류 합계

256
00:11:22,019 --> 00:11:24,480
이며 이 표현은 다음과 같습니다.

257
00:11:24,480 --> 00:11:27,120
나중에 슬라이드에서

258
00:11:27,120 --> 00:11:28,740
어떻게 창의적 코딩을 사용하여 인과 추론을 모델링하는지 설명하는 방법에 유용할 것입니다.

259
00:11:28,740 --> 00:11:30,120


260
00:11:30,120 --> 00:11:32,940


261
00:11:32,940 --> 00:11:34,800
예측 코딩이 중요하다고 생각

262
00:11:34,800 --> 00:11:36,240
하고 무엇보다 먼저 공부하기에 좋은 알고리즘이 아닙니다.

263
00:11:36,240 --> 00:11:37,500


264
00:11:37,500 --> 00:11:39,600
이전에

265
00:11:39,600 --> 00:11:41,399


266
00:11:41,399 --> 00:11:43,079
모델 증거 또는 한계

267
00:11:43,079 --> 00:11:44,339
가능성인 올바른 목표를 최적화한

268
00:11:44,339 --> 00:11:45,660


269
00:11:45,660 --> 00:11:47,700
다음

270
00:11:47,700 --> 00:11:49,440


271
00:11:49,440 --> 00:11:52,440
내가 말했듯이 자유 에너지의 변화라고 하는 하한을 최적화하여 가상

272
00:11:52,440 --> 00:11:54,240
마무리가 흥미롭다고 말했습니다.

273
00:11:54,240 --> 00:11:57,680


274
00:11:57,680 --> 00:12:00,839


275
00:12:00,839 --> 00:12:04,680


276
00:12:04,680 --> 00:12:06,899
예를 들어 기계 학습 작업

277
00:12:06,899 --> 00:12:09,060
이나 일반적으로 학습 작업에서 중요한 영향으로 최적화하는 두 가지 용어의 합과 각 용어는

278
00:12:09,060 --> 00:12:12,420
암기를 강제하므로

279
00:12:12,420 --> 00:12:15,440
두 번째 용어는 기본적으로

280
00:12:15,440 --> 00:12:18,180
모델에 힘을 줍니다.  특정 데이터 세트에 적합

281
00:12:18,180 --> 00:12:19,560


282
00:12:19,560 --> 00:12:21,240
하고 첫 번째 용어는

283
00:12:21,240 --> 00:12:23,519
모델이 복잡성을 최소화하도록 강제

284
00:12:23,519 --> 00:12:26,040
하고 예를 들어

285
00:12:26,040 --> 00:12:28,500
결과 면도기

286
00:12:28,500 --> 00:12:31,260
이론에서 알 수 있듯이

287
00:12:31,260 --> 00:12:33,000
특정 훈련 세트에서 유사하게 수행되는 두 개의 다른 모델이 있는 경우 우리가

288
00:12:33,000 --> 00:12:35,640
가지고 있는 모델  얻을 수

289
00:12:35,640 --> 00:12:37,380
있고

290
00:12:37,380 --> 00:12:39,899
일반화할 것으로 예상되는 것은 가장 덜

291
00:12:39,899 --> 00:12:41,160
복잡한 것이므로

292
00:12:41,160 --> 00:12:44,100


293
00:12:44,100 --> 00:12:46,380
운영 자유 에너지를 통해 생성 모델을 업데이트하면

294
00:12:46,380 --> 00:12:47,779
기본적으로

295
00:12:47,779 --> 00:12:51,959
최적의 어 결과 면도기 모델로 수렴할 수 있습니다.

296
00:12:51,959 --> 00:12:54,720
둘 다

297
00:12:54,720 --> 00:12:56,100
데이터 세트를 기억하지만 또한

298
00:12:56,100 --> 00:12:58,680
보이지 않는 보이지 않는 데이터 포인트에 대해 매우 잘 일반화할 수 있는

299
00:12:58,680 --> 00:13:00,240


300
00:13:00,240 --> 00:13:02,639
운영 코딩이 중요한 두 번째 이유는

301
00:13:02,639 --> 00:13:08,600
실제로

302
00:13:08,720 --> 00:13:11,760


303
00:13:11,760 --> 00:13:13,920
계층 구조에서 정의할 필요는 없지만 방향성 그래픽과 같은

304
00:13:13,920 --> 00:13:15,959
보다 복잡하고 유연한 아키텍처에서 모델링할 수 있다는 것입니다.

305
00:13:15,959 --> 00:13:18,240


306
00:13:18,240 --> 00:13:21,540
모든 형태의 모델 또는 뇌 영역과 유사한

307
00:13:21,540 --> 00:13:23,700
주기가 많은 네트워크로 훨씬 더 일반화되고

308
00:13:23,700 --> 00:13:25,920


309
00:13:25,920 --> 00:13:27,779
근본적인 이유의 최종 결과는

310
00:13:27,779 --> 00:13:30,300


311
00:13:30,300 --> 00:13:32,339
순방향 패스로 학습하고 예측한 다음

312
00:13:32,339 --> 00:13:34,260
오류를 역전파하지 않는다는 것입니다.

313
00:13:34,260 --> 00:13:36,600
에너지 기능을 최소화

314
00:13:36,600 --> 00:13:38,459
하고 이것은 기본적으로 모든 종류의

315
00:13:38,459 --> 00:13:39,839
계층 구조가 될 수 있도록 합니다.

316
00:13:39,839 --> 00:13:41,180
어

317
00:13:41,180 --> 00:13:43,860
직접 키 뒤로 이동하고

318
00:13:43,860 --> 00:13:46,860
사이클을 배울 수 있게 합니다. 이것은

319
00:13:46,860 --> 00:13:48,060
실제로 매우 중요합니다.

320
00:13:48,060 --> 00:13:50,399
뇌는 사이클로 가득 차 있기 때문입니다.

321
00:13:50,399 --> 00:13:53,399
최근 논문에서 일부 정보를 얻었기 때문입니다.

322
00:13:53,399 --> 00:13:56,459
어, 초파리와

323
00:13:56,459 --> 00:13:59,279
같은 일부 동물의 뇌를 완전히 매핑할 수 있는 뇌는

324
00:13:59,279 --> 00:14:00,420


325
00:14:00,420 --> 00:14:03,899
주기로 가득 차 있으므로

326
00:14:03,899 --> 00:14:06,720
기계 학습

327
00:14:06,720 --> 00:14:09,000
모델 또는

328
00:14:09,000 --> 00:14:11,160
일반적으로 순환을 사용하여 배출할 수 있는 알고리즘으로 모델을 배출하는 것이 합리적입니다.

329
00:14:11,160 --> 00:14:14,160


330
00:14:14,160 --> 00:14:17,160


331
00:14:17,160 --> 00:14:19,380
작동 코딩이

332
00:14:19,380 --> 00:14:21,240
흥미로운 세 번째 이유는 블랙 전파로 시작하는 표준 신경망

333
00:14:21,240 --> 00:14:23,820
보다 더 견고하다는 것이 공식적으로 입증되었기 때문에

334
00:14:23,820 --> 00:14:25,139


335
00:14:25,139 --> 00:14:27,060


336
00:14:27,060 --> 00:14:28,200
신경망이 있고 분류 작업을 수행하려는 경우

337
00:14:28,200 --> 00:14:30,320


338
00:14:30,320 --> 00:14:34,139
창의적 코딩이 더 견고

339
00:14:34,139 --> 00:14:36,260
하고  이것은

340
00:14:36,260 --> 00:14:38,339


341
00:14:38,339 --> 00:14:40,680
작은 데이터 세트에 대한 온라인 학습 훈련이나

342
00:14:40,680 --> 00:14:43,440
지속적인 학습 작업과 같은 작업에서 흥미롭고 이론은

343
00:14:43,440 --> 00:14:45,540
기본적으로

344
00:14:45,540 --> 00:14:48,540
명령형 코딩이 명시

345
00:14:48,540 --> 00:14:50,820
적 경사 하강법의

346
00:14:50,820 --> 00:14:53,339
다른 버전인 암시

347
00:14:53,339 --> 00:14:54,899
적 경사 하강법을 근사화하기 위해 이동되었다는 사실에서 비롯됩니다.

348
00:14:54,899 --> 00:14:57,180


349
00:14:57,180 --> 00:14:59,880
기본적으로 모든 단일 모델에서 사용되는 표준 녹색 하강

350
00:14:59,880 --> 00:15:03,680
이며 더 강력한 변형입니다.

351
00:15:05,880 --> 00:15:08,279
괜찮다고 생각합니다. 꽤 긴 인트란

352
00:15:08,279 --> 00:15:09,779
작동 코딩을 수행했습니다. 이제 인과

353
00:15:09,779 --> 00:15:11,639
추론과 인과 관계인 두 번째 주제로 이동하는 것 같습니다.

354
00:15:11,639 --> 00:15:13,019


355
00:15:13,019 --> 00:15:15,839
인과적 추론 인과적

356
00:15:15,839 --> 00:15:18,420
추론은 이론입니다 매우

357
00:15:18,420 --> 00:15:20,339
일반적인 이론으로

358
00:15:20,339 --> 00:15:23,100
Judy apparel에 의해 가장 공식화되었습니다. 그는 확실히 프랑스 인과 분야

359
00:15:23,100 --> 00:15:25,500
에서 가장 중요한 사람입니다.

360
00:15:25,500 --> 00:15:27,839
그는

361
00:15:27,839 --> 00:15:29,760
아주 좋은 책을 썼습니다.

362
00:15:29,760 --> 00:15:32,760


363
00:15:32,760 --> 00:15:35,220
이 주제에 대해 더 배우고 싶고

364
00:15:35,220 --> 00:15:37,800
기본적으로 다음 문제를 다루고 있으므로

365
00:15:37,800 --> 00:15:38,639


366
00:15:38,639 --> 00:15:40,440


367
00:15:40,440 --> 00:15:42,000


368
00:15:42,000 --> 00:15:44,160
베이지안 네트워크와 관련된 공동 확률 분포가 있다고 가정해 봅시다.

369
00:15:44,160 --> 00:15:46,199


370
00:15:46,199 --> 00:15:49,260


371
00:15:49,260 --> 00:15:51,839


372
00:15:51,839 --> 00:15:54,480
이 모양의 아시아 네트워크가 아닙니다.

373
00:15:54,480 --> 00:15:57,660
네트워크를 기반으로 합니다.

374
00:15:57,660 --> 00:16:00,240
내부 변수는

375
00:16:00,240 --> 00:16:02,100
다른 수량을 나타낼 수 있습니다. 예를 들어

376
00:16:02,100 --> 00:16:04,620
이 모양의 시각적 네트워크는

377
00:16:04,620 --> 00:16:06,899


378
00:16:06,899 --> 00:16:08,820
오른쪽의 수량을 나타낼 수 있으므로 사회적

379
00:16:08,820 --> 00:16:10,800
경제적 개인의 스튜디오 동상은

380
00:16:10,800 --> 00:16:13,079
교육 수준

381
00:16:13,079 --> 00:16:16,699
지능과 소득 수준

382
00:16:17,100 --> 00:16:19,440
고전 통계가

383
00:16:19,440 --> 00:16:22,920
매우 잘하는 것입니다. 어

384
00:16:22,920 --> 00:16:25,320
가장 많이 사용되는 응용 프로그램은

385
00:16:25,320 --> 00:16:28,019
관찰 또는 상관

386
00:16:28,019 --> 00:16:29,279
관계를 모델링하는 것입니다. 상관 관계는 기본적으로 질문에 대답합니다.

387
00:16:29,279 --> 00:16:32,519


388
00:16:32,519 --> 00:16:35,579
다른 변수 C를 관찰하면

389
00:16:35,579 --> 00:16:37,500
예를 들어  이 경우

390
00:16:37,500 --> 00:16:39,660
소득 수준은 얼마입니까

391
00:16:39,660 --> 00:16:41,820
개인의 예상 소득 수준이

392
00:16:41,820 --> 00:16:44,339
교육 수준을 관찰하고

393
00:16:44,339 --> 00:16:48,180
물론 그 사람이 석사 또는 박사와 같은

394
00:16:48,180 --> 00:16:50,220
더 높은 교육 학위를 가지고 있다면

395
00:16:50,220 --> 00:16:52,500
나는

396
00:16:52,500 --> 00:16:54,360
그 사람이 일반적으로 가질 것으로 기대합니다  더 높은

397
00:16:54,360 --> 00:16:56,040
소득 수준

398
00:16:56,040 --> 00:16:58,139
과 이것은 상관 관계

399
00:16:58,139 --> 00:17:00,300
이지만 때때로

400
00:17:00,300 --> 00:17:03,300
관찰하기 매우 어려운 것들이 있지만

401
00:17:03,300 --> 00:17:05,040
그러한 양을 결정하는 데 큰 역할을 합니다.

402
00:17:05,040 --> 00:17:06,119


403
00:17:06,119 --> 00:17:08,220
예를 들어

404
00:17:08,220 --> 00:17:11,160
소득 수준은

405
00:17:11,160 --> 00:17:13,380
지능에 의해 훨씬 더 정의될 수 있습니다.

406
00:17:13,380 --> 00:17:15,540
특정 사람

407
00:17:15,540 --> 00:17:18,720
그리고 아마도 지능이 있거나

408
00:17:18,720 --> 00:17:21,000
지능이 있는 사람이 더

409
00:17:21,000 --> 00:17:24,540
높은 교육 수준을 가질 가능성이 가장 높지만

410
00:17:24,540 --> 00:17:27,540
여전히

411
00:17:27,540 --> 00:17:30,120
소득당 I인 진짜 이유는 IQ 때문이며

412
00:17:30,120 --> 00:17:32,220


413
00:17:32,220 --> 00:17:34,740
이것은 이것이 불가능할 수 있습니다.

414
00:17:34,740 --> 00:17:36,360
단순한 상관관계에 의한 연구이며 개입이라고 하는

415
00:17:36,360 --> 00:17:39,120
고급 기술에 의해 연구되어야 합니다.

416
00:17:39,120 --> 00:17:41,280


417
00:17:41,280 --> 00:17:43,320
개입은 기본적으로

418
00:17:43,320 --> 00:17:46,500
C를 특정 값으로 변경하면 D가 무엇인지에 대한 질문에 답합니다.

419
00:17:46,500 --> 00:17:48,240


420
00:17:48,240 --> 00:17:51,000
예를 들어 개인을 취할 수 있습니다.

421
00:17:51,000 --> 00:17:54,660
그의 소득 수준을 확인한

422
00:17:54,660 --> 00:17:57,120
다음 교육 수준을 변경하므로

423
00:17:57,120 --> 00:17:59,220


424
00:17:59,220 --> 00:18:01,080


425
00:18:01,080 --> 00:18:03,419
지능을 건드리지 않고 이 세상에 개입하여 교육 수준을 변경하고

426
00:18:03,419 --> 00:18:07,260
소득이 얼마나 변경되는지 확인합니다.

427
00:18:07,260 --> 00:18:09,900
예를 들어 소득이 많이 변경되면

428
00:18:09,900 --> 00:18:12,179
지능이 없다는 것을 의미합니다.

429
00:18:12,179 --> 00:18:14,460
여기에 큰 역할을 하지만

430
00:18:14,460 --> 00:18:16,799
교육 수준은 소득 수준이

431
00:18:16,799 --> 00:18:19,020
크게 변하지 않는다면 아마도

432
00:18:19,020 --> 00:18:20,640
숨겨진 변수가 있다는 것을 의미합니다. 이 경우

433
00:18:20,640 --> 00:18:22,860


434
00:18:22,860 --> 00:18:25,760
사람의 소득 수준을 결정하는 지능

435
00:18:25,980 --> 00:18:28,740
세 번째 중요한 인과

436
00:18:28,740 --> 00:18:31,080
추론은

437
00:18:31,080 --> 00:18:33,120
예를 들어 반사실은

438
00:18:33,120 --> 00:18:36,720
무엇이 될 것인지에 대한 질문에 답하고

439
00:18:36,720 --> 00:18:39,240
C를 과거의 다른 값으로 변경합니다. 예를 들어

440
00:18:39,240 --> 00:18:40,679


441
00:18:40,679 --> 00:18:42,059
개입과

442
00:18:42,059 --> 00:18:45,059
반사실의 차이점은 개입이

443
00:18:45,059 --> 00:18:47,820
미래에 작용한다는 것을 알 수 있습니다.

444
00:18:47,820 --> 00:18:50,340
현재 세계에서 미래의 변화를 관찰하기 위해

445
00:18:50,340 --> 00:18:53,220
반사실적으로 우리는

446
00:18:53,220 --> 00:18:56,039
시간을 거슬러 올라가 시간을 거슬러 변수를 변경

447
00:18:56,039 --> 00:18:59,160
하고 그 변화가 우리가 지금

448
00:18:59,160 --> 00:19:01,320
살고 있는 세계에 어떤 영향을 미쳤는지 볼 수

449
00:19:01,320 --> 00:19:02,940


450
00:19:02,940 --> 00:19:06,299
있으며 이는 judapple이 세 가지로 정의합니다.

451
00:19:06,299 --> 00:19:08,100
인과 추론의 수준

452
00:19:08,100 --> 00:19:09,660
상관관계는 첫 번째 수준의

453
00:19:09,660 --> 00:19:11,580
개입은 반사실적 개입의 두 번째 수준은

454
00:19:11,580 --> 00:19:14,720
세 번째 수준의

455
00:19:16,020 --> 00:19:18,120
다른 개입입니다. 직관적인 정의를

456
00:19:18,120 --> 00:19:20,640
내렸으니 이제 더 공식적으로 정의하겠습니다.

457
00:19:20,640 --> 00:19:23,760


458
00:19:23,760 --> 00:19:25,500
여기서 이 표기법을 사용하고 있습니다.

459
00:19:25,500 --> 00:19:27,240
모든 프레젠테이션에서 실제로 동일하므로

460
00:19:27,240 --> 00:19:29,640
X는 항상

461
00:19:29,640 --> 00:19:32,820
잠재 변수가 될 것입니다. s는 항상

462
00:19:32,820 --> 00:19:35,340
데이터 포인트 또는 관찰이 될 것이고

463
00:19:35,340 --> 00:19:38,520
VI는 항상 정점이 될 것이므로

464
00:19:38,520 --> 00:19:40,860
VI를 볼 때마다 우리는 단지

465
00:19:40,860 --> 00:19:42,720


466
00:19:42,720 --> 00:19:45,299
예를 들어 그래프의 구조에 관심이 있으므로

467
00:19:45,299 --> 00:19:46,860


468
00:19:46,860 --> 00:19:50,160


469
00:19:50,160 --> 00:19:52,679


470
00:19:52,679 --> 00:19:54,780
이전 슬라이드에서 본 베이지안 모델과 동일한 구조를 가진 베이지안 모델이 있다고 가정해 보겠습니다.

471
00:19:54,780 --> 00:19:57,840


472
00:19:57,840 --> 00:20:00,660


473
00:20:00,660 --> 00:20:03,360


474
00:20:03,360 --> 00:20:04,679


475
00:20:04,679 --> 00:20:07,380


476
00:20:07,380 --> 00:20:09,240


477
00:20:09,240 --> 00:20:13,860
X3가 S3

478
00:20:13,860 --> 00:20:15,679
외부

479
00:20:15,679 --> 00:20:17,760
개입과 같다고 가정할 때 이 정점과 관련된 잠재 변수인 X4의 확률 또는 기대치를 계산하려면 do 작업이라고 하는 새로운 종류의 표기법이 필요하므로

480
00:20:17,760 --> 00:20:19,919


481
00:20:19,919 --> 00:20:21,179


482
00:20:21,179 --> 00:20:23,880
이 경우

483
00:20:23,880 --> 00:20:26,100
X4를 계산해야 합니다.

484
00:20:26,100 --> 00:20:30,000
우리가 단어에 개입

485
00:20:30,000 --> 00:20:33,059
하고 X3 서쪽 3을 변경한다는 사실을 감안할 때 X4의 확률 및

486
00:20:33,059 --> 00:20:35,580


487
00:20:35,580 --> 00:20:38,400
개입을 수행하기 위해 이를 수행하는 방법 Judo Pearl은 우리에게 상관 관계를 계산하기

488
00:20:38,400 --> 00:20:40,020


489
00:20:40,020 --> 00:20:41,880
전에 중간 단계가 있어야 한다고 알려줍니다.

490
00:20:41,880 --> 00:20:45,059


491
00:20:45,059 --> 00:20:46,860


492
00:20:46,860 --> 00:20:50,160
V3로 들어오는 모든 가장자리를 제거하려면 모두 제거해야 하므로

493
00:20:50,160 --> 00:20:52,799
이 베이지안 네트워크가 아니라 이 두 번째 네트워크를 연구해야 합니다. 그런

494
00:20:52,799 --> 00:20:55,679


495
00:20:55,679 --> 00:20:58,200
다음 이 시점에서 평소

496
00:20:58,200 --> 00:21:00,840
처럼 상관관계를 계산할 수 있습니다.

497
00:21:00,840 --> 00:21:03,299


498
00:21:03,299 --> 00:21:06,500


499
00:21:07,020 --> 00:21:09,299


500
00:21:09,299 --> 00:21:11,700
이것의 일반화는 내가 과거에 살았고

501
00:21:11,700 --> 00:21:14,100
그들은 구조적 인과 모델을 사용하여 컴퓨팅하고 있다는 것입니다.

502
00:21:14,100 --> 00:21:15,419


503
00:21:15,419 --> 00:21:18,299
구조 인과 모델은

504
00:21:18,299 --> 00:21:21,120


505
00:21:21,120 --> 00:21:23,460
베이지안 네트워크와 개념적으로 유사한 튜플이지만 기본적으로 우리는

506
00:21:23,460 --> 00:21:26,220
이 새로운 클래스의 변수를 맨 위에 가지고 있습니다.

507
00:21:26,220 --> 00:21:28,580
그들이 사용하는 관찰할 수 없는 변수이므로

508
00:21:28,580 --> 00:21:30,960
우리는

509
00:21:30,960 --> 00:21:34,020
X1 X2 X3 S4 이전에 가졌던 베이지안 네트워크를 가지고

510
00:21:34,020 --> 00:21:37,460
있지만 관찰할 수 없거나 제어할 수 없는

511
00:21:37,460 --> 00:21:40,020
환경에 의존하는 변수도 있습니다.

512
00:21:40,020 --> 00:21:42,539
추론할 수는 있지만

513
00:21:42,539 --> 00:21:43,980


514
00:21:43,980 --> 00:21:46,020
그들은 거기에 있고

515
00:21:46,020 --> 00:21:48,539


516
00:21:48,539 --> 00:21:51,360
f

517
00:21:51,360 --> 00:21:53,400
모든

518
00:21:53,400 --> 00:21:57,299
기본적으로 x의 x의 x3에 의존하는 함수 집합은 X1에 의존합니다.

519
00:21:57,299 --> 00:21:58,980
왜냐하면 우리는 x2에 화살표가 있기 때문입니다.  관찰

520
00:21:58,980 --> 00:22:00,960


521
00:22:00,960 --> 00:22:02,940


522
00:22:02,940 --> 00:22:05,840


523
00:22:06,179 --> 00:22:09,240


524
00:22:09,240 --> 00:22:11,940


525
00:22:11,940 --> 00:22:14,159


526
00:22:14,159 --> 00:22:16,679
할 수 없는 변수가 맨 위에 있고 각각의 관찰

527
00:22:16,679 --> 00:22:19,500
할 수 없는 변수는

528
00:22:19,500 --> 00:22:22,020
자신

529
00:22:22,020 --> 00:22:24,600
의 최신 변수 X에만 영향을 미치므로 예를 들어

530
00:22:24,600 --> 00:22:27,960
IU는 X1을 절대 만지지 않을 뿐만 아니라 u3도

531
00:22:27,960 --> 00:22:30,360
Q3만 만질 것입니다. E1은 모두

532
00:22:30,360 --> 00:22:34,039
X1에 영향을 미치고  등등 그래서

533
00:22:35,039 --> 00:22:37,679
반사실적 추론을 수행하면

534
00:22:37,679 --> 00:22:39,900
다음 질문에 답하므로

535
00:22:39,900 --> 00:22:42,960
X4가 X3에서 다른 변수와 같을 것입니다.

536
00:22:42,960 --> 00:22:46,620


537
00:22:46,620 --> 00:22:49,340
외부에서

538
00:22:49,340 --> 00:22:51,840
세 가지 다른 단계가 필요하므로

539
00:22:51,840 --> 00:22:53,039
납치는

540
00:22:53,039 --> 00:22:54,900


541
00:22:54,900 --> 00:22:57,179
모든 배경 변수의 계산이므로

542
00:22:57,179 --> 00:22:59,460
이 경우  이 단계에서 우리는

543
00:22:59,460 --> 00:23:01,200
시간을 거슬러 올라가 관찰

544
00:23:01,200 --> 00:23:03,419
할 수 없는 환경이

545
00:23:03,419 --> 00:23:04,919


546
00:23:04,919 --> 00:23:08,039
특정 순간에 어떤 환경이었는지 이해하고 싶습니다.

547
00:23:08,039 --> 00:23:11,039
모든 잠재

548
00:23:11,039 --> 00:23:14,280
변수 X를 우리가 이미 가지고 있는 일부 특정 데이터에 고정

549
00:23:14,280 --> 00:23:16,140


550
00:23:16,140 --> 00:23:18,960
하고 이를 수행합니다.

551
00:23:18,960 --> 00:23:21,120
사용에 대한 추론

552
00:23:21,120 --> 00:23:24,240
그런 다음 우리는 U를 사용하여

553
00:23:24,240 --> 00:23:26,940
우리가 배운 U를 유지하고

554
00:23:26,940 --> 00:23:28,500
개입을 수행할 것입니다.

555
00:23:28,500 --> 00:23:29,880
따라서

556
00:23:29,880 --> 00:23:32,340
반작용은

557
00:23:32,340 --> 00:23:34,980
우리가 환경을 알고 있는 시간을 거슬러 올라가는 개입으로 볼 수 있습니다.

558
00:23:34,980 --> 00:23:36,960
환경

559
00:23:36,960 --> 00:23:40,620
변수 U1 U2  특정 순간의 u4와

560
00:23:40,620 --> 00:23:43,039


561
00:23:43,200 --> 00:23:44,340


562
00:23:44,340 --> 00:23:46,679
누락된 단계가 무엇인지

563
00:23:46,679 --> 00:23:49,440
X3에서 X4는

564
00:23:49,440 --> 00:23:50,780


565
00:23:50,780 --> 00:23:53,280
해당 특정 상황에서 다른 다른 데이터 포인트와 같을 것입니다.

566
00:23:53,280 --> 00:23:55,980
이제 상관관계를 계산할 수

567
00:23:55,980 --> 00:23:57,120


568
00:23:57,120 --> 00:23:59,520
있고 그래프의 경로에서 수행하는 상관관계를 계산할 수 있습니다.

569
00:23:59,520 --> 00:24:02,039


570
00:24:02,039 --> 00:24:04,440


571
00:24:04,440 --> 00:24:06,659


572
00:24:06,659 --> 00:24:10,140
납치 단계에서 우리가 배운 환경 변수를 사용하여 개입을 이미 수행했으며

573
00:24:10,140 --> 00:24:14,419
이것은 반사실적 추론입니다.

574
00:24:15,480 --> 00:24:18,000
이것은 인과적 추론 현재 소개의 마지막 슬라이드이며 기본적으로

575
00:24:18,000 --> 00:24:20,159


576
00:24:20,159 --> 00:24:21,720


577
00:24:21,720 --> 00:24:23,880
기본적으로 내가 말한 모든 것은 구조적 학습에 관한 것입니다.

578
00:24:23,880 --> 00:24:27,360
지금까지 우리는 데이터 포인트

579
00:24:27,360 --> 00:24:29,700
사이의 인과 관계를 알고 있으므로

580
00:24:29,700 --> 00:24:31,500


581
00:24:31,500 --> 00:24:33,120
그래프의 구조를 알고 어떤 변수가

582
00:24:33,120 --> 00:24:34,860
어떤 변수에 영향을 미치는지 알고

583
00:24:34,860 --> 00:24:37,260
일반적으로 화살표를 알고

584
00:24:37,260 --> 00:24:39,659
있지만 실제로 이것이

585
00:24:39,659 --> 00:24:42,900
항상 가능한 것은 아닙니다.

586
00:24:42,900 --> 00:24:45,419


587
00:24:45,419 --> 00:24:47,400
대부분의 경우 인과 관계 그래프에 액세스할 수 없으며 실제로

588
00:24:47,400 --> 00:24:49,919
데이터에서 최상의 인과 관계 그래프를 학습하는 것은 여전히

589
00:24:49,919 --> 00:24:51,840
미해결 문제입니다.

590
00:24:51,840 --> 00:24:53,880
우리는 개선하고 있지만

591
00:24:53,880 --> 00:24:57,299
이 작업을 정확히 수행하는 방법은

592
00:24:57,299 --> 00:24:58,380


593
00:24:58,380 --> 00:25:01,140
여전히 ​​미해결 문제입니다.

594
00:25:01,140 --> 00:25:03,179
기본적으로 목표는

595
00:25:03,179 --> 00:25:04,740


596
00:25:04,740 --> 00:25:07,380
관찰 데이터에서 Council 관계를 참조하는 것이므로 데이터 세트가 주어지면

597
00:25:07,380 --> 00:25:09,780


598
00:25:09,780 --> 00:25:12,179


599
00:25:12,179 --> 00:25:14,460
시스템과 데이터 세트의 변수 사이의 연결을 설명하는 방향이 있는 정확한 그래프를 추론하고자 합니다. 예

600
00:25:14,460 --> 00:25:15,960


601
00:25:15,960 --> 00:25:17,700
를 들어 여기에 예가 있습니다.

602
00:25:17,700 --> 00:25:19,440


603
00:25:19,440 --> 00:25:22,860


604
00:25:22,860 --> 00:25:25,080
팬데믹으로 인해 우리는 모두 덕분에 친숙한 것 같습니다.

605
00:25:25,080 --> 00:25:28,799
연령 백신 입원

606
00:25:28,799 --> 00:25:31,380
및 CT의 네 가지 변수가

607
00:25:31,380 --> 00:25:33,600
있고

608
00:25:33,600 --> 00:25:36,059
이러한 변수 간의 인과 관계를 추론하고 싶습니다.

609
00:25:36,059 --> 00:25:37,980
예를 들어

610
00:25:37,980 --> 00:25:40,260
확률이

611
00:25:40,260 --> 00:25:43,080
입원하는 사람의

612
00:25:43,080 --> 00:25:45,419
나이와

613
00:25:45,419 --> 00:25:49,760
예방 접종 여부 등에 따라 다르므로 이상으로 긴 서론이

614
00:25:51,299 --> 00:25:55,020
끝났지

615
00:25:55,020 --> 00:25:58,080
만 어 충분히 명확했으면 좋겠습니다.

616
00:25:58,080 --> 00:26:00,179


617
00:26:00,179 --> 00:26:02,039
기본적

618
00:26:02,039 --> 00:26:05,159
으로 논문의 결과를 이해하고

619
00:26:05,159 --> 00:26:07,740
이제 연구질문으로 갈 수 있으므로

620
00:26:07,740 --> 00:26:09,059
연구질문은 다음과 같다

621
00:26:09,059 --> 00:26:10,440


622
00:26:10,440 --> 00:26:12,900
먼저

623
00:26:12,900 --> 00:26:15,299
창의적 코딩을 사용하여

624
00:26:15,299 --> 00:26:16,980
인과추론을 수행할 수 있는지 확인하여

625
00:26:16,980 --> 00:26:20,100
지금까지 수술적 코딩만

626
00:26:20,100 --> 00:26:22,380
사용해왔다.  베이지안 네트워크에서 상관관계를 계산하기 위해 수행하는 데

627
00:26:22,380 --> 00:26:25,020


628
00:26:25,020 --> 00:26:27,419
큰 문제는 우리가

629
00:26:27,419 --> 00:26:29,400
상관관계와 모델 개입을 넘어

630
00:26:29,400 --> 00:26:31,679
생물학적 그럴듯한 방식으로 반사실을 넘어설 수 있다는 것입니다.

631
00:26:31,679 --> 00:26:32,760


632
00:26:32,760 --> 00:26:34,380


633
00:26:34,380 --> 00:26:36,120


634
00:26:36,120 --> 00:26:39,059


635
00:26:39,059 --> 00:26:40,740
예를 들어

636
00:26:40,740 --> 00:26:43,740
그래프의 거대한 구조를 건드리지 않고

637
00:26:43,740 --> 00:26:46,380
실제로 더 구체적으로

638
00:26:46,380 --> 00:26:48,299
문제는 우리가 개입 및 반사실을 수행하기 위해

639
00:26:48,299 --> 00:26:51,000
운영 코딩 기반 구조 인과

640
00:26:51,000 --> 00:26:52,740
모델을 정의할 수 있는지가 됩니다.

641
00:26:52,740 --> 00:26:55,320


642
00:26:55,320 --> 00:26:58,380
두 번째 질문은

643
00:26:58,380 --> 00:27:00,179
제가 말했듯이 구조 사용자 정의 모델을 갖는 것은

644
00:27:00,179 --> 00:27:02,159


645
00:27:02,159 --> 00:27:04,260
회피 네트워크의 구조를 알고 있으므로

646
00:27:04,260 --> 00:27:07,919
우리는 화살표가 있다고 가정합니다.

647
00:27:07,919 --> 00:27:09,960
이를 넘어 창의적인 코딩 네트워크를 사용하여

648
00:27:09,960 --> 00:27:11,520


649
00:27:11,520 --> 00:27:14,418


650
00:27:16,140 --> 00:27:18,900
기본적으로 두 질문에 긍정적인 답변을 제공하는 그래프의 인과 구조를 학습하면

651
00:27:18,900 --> 00:27:21,120


652
00:27:21,120 --> 00:27:23,120
예측 코딩을 다음과 같이 사용할 수 있습니다.

653
00:27:23,120 --> 00:27:26,039
기본적으로

654
00:27:26,039 --> 00:27:28,740
데이터 세트를 사용하고

655
00:27:28,740 --> 00:27:30,419


656
00:27:30,419 --> 00:27:34,820
이 데이터 세트에서 직접 개입 및 반사실적 예측을 테스트할 수 있는 종단 간 인과 추론 방법이므로

657
00:27:36,840 --> 00:27:39,299
첫

658
00:27:39,299 --> 00:27:40,740
번째 문제를 해결해 보겠습니다.

659
00:27:40,740 --> 00:27:42,419


660
00:27:42,419 --> 00:27:45,120
논문의 제목은

661
00:27:45,120 --> 00:27:46,740
기본적으로

662
00:27:46,740 --> 00:27:48,539
여기에서 이미 알려진 상관

663
00:27:48,539 --> 00:27:50,760
관계 연산 코딩을 수행하는 방법

664
00:27:50,760 --> 00:27:52,440


665
00:27:52,440 --> 00:27:54,419
과 중재 쿼리를 수행하는 방법을 보여드릴 것입니다.

666
00:27:54,419 --> 00:27:56,760


667
00:27:56,760 --> 00:28:01,140


668
00:28:01,140 --> 00:28:03,900
그래프는

669
00:28:03,900 --> 00:28:05,700
우리가 가지고 있던 일반적인 그래프이고

670
00:28:05,700 --> 00:28:07,260


671
00:28:07,260 --> 00:28:09,240
여기에 해당하는 창의적

672
00:28:09,240 --> 00:28:11,760
코딩 모델이므로 축은

673
00:28:11,760 --> 00:28:13,980
잠재 변수이며

674
00:28:13,980 --> 00:28:18,000
신경망 모델의 뉴런에 해당하고

675
00:28:18,000 --> 00:28:20,760


676
00:28:20,760 --> 00:28:22,740
하나의 뉴런에서 예측 정보를 전달하는 검은색 화살표

677
00:28:22,740 --> 00:28:25,559


678
00:28:25,559 --> 00:28:28,500
모든 버텍스에는

679
00:28:28,500 --> 00:28:31,140
정보를 계층 위로 전달하는 이 오류 뉴런이 있으므로

680
00:28:31,140 --> 00:28:32,820
모든

681
00:28:32,820 --> 00:28:36,480
오류의 정보는 계층 위의 값 노드로 이동

682
00:28:36,480 --> 00:28:39,120
하고 기본적으로 변경을 수정하도록 지시합니다.

683
00:28:39,120 --> 00:28:41,400


684
00:28:41,400 --> 00:28:43,760
예측

685
00:28:44,700 --> 00:28:46,559


686
00:28:46,559 --> 00:28:48,840
코딩을 사용하여 상관 관계를 수행하기 위해 해야 할 일은

687
00:28:48,840 --> 00:28:50,400
관측을 수행하고

688
00:28:50,400 --> 00:28:52,620
특정 뉴런의 값을 고정하는 것이므로

689
00:28:52,620 --> 00:28:53,820


690
00:28:53,820 --> 00:28:55,200


691
00:28:55,200 --> 00:28:58,740
X3이 S3과 동일할 때 X4의 확률을 계산하려면

692
00:28:58,740 --> 00:29:02,340
간단히 다음을 수행해야 합니다.  X3를 가져와서 더 이상

693
00:29:02,340 --> 00:29:04,380
변하지 않는 방식으로 S3에 고정하고

694
00:29:04,380 --> 00:29:08,159
에너지 최소화를 실행하고

695
00:29:08,159 --> 00:29:09,720
이 모델을

696
00:29:09,720 --> 00:29:12,659
축을 업데이트하여 최소화함으로써

697
00:29:12,659 --> 00:29:16,380


698
00:29:16,380 --> 00:29:18,419
자유 에너지의 변동 최소화를 통해 모델이

699
00:29:18,419 --> 00:29:20,820
솔루션으로 수렴하도록 합니다.  이 질문에 따라서 X3이 3인 경우

700
00:29:20,820 --> 00:29:22,919


701
00:29:22,919 --> 00:29:27,179
X4의 확률 또는 기대값은 3입니다.

702
00:29:27,179 --> 00:29:29,340
하지만 그래프의 구조에 영향을 주지 않고 지금 개입을 수행하는 방법

703
00:29:29,340 --> 00:29:31,679


704
00:29:31,679 --> 00:29:33,419


705
00:29:33,419 --> 00:29:35,640
은 기본적으로 논문의 첫 번째 아이디어입니다.

706
00:29:35,640 --> 00:29:37,679


707
00:29:37,679 --> 00:29:39,960


708
00:29:39,960 --> 00:29:43,260
상관관계를 수행하므로 S3을 X3과 동일하게 고정하는 것이

709
00:29:43,260 --> 00:29:45,600
알고리즘의 첫 번째 단계이고

710
00:29:45,600 --> 00:29:47,220
두 번째 단계는

711
00:29:47,220 --> 00:29:50,539
자유 에너지의 변화를 최소화하여 축을 얻는 것입니다.

712
00:29:51,240 --> 00:29:53,340
이론상

713
00:29:53,340 --> 00:29:55,200
이러한 화살표를 제거하는 개입은

714
00:29:55,200 --> 00:29:56,220


715
00:29:56,220 --> 00:29:57,659


716
00:29:57,659 --> 00:29:59,279
확률 질문에 대한 답변입니다.

717
00:29:59,279 --> 00:30:02,399
개입을 수행하여 X4의 X3는 다음과 같이

718
00:30:02,399 --> 00:30:04,860
3개의 명령형 코딩을 수행할 수 있으므로

719
00:30:04,860 --> 00:30:07,080


720
00:30:07,080 --> 00:30:09,840


721
00:30:09,840 --> 00:30:13,140
먼저 상관관계에서 S3을

722
00:30:13,140 --> 00:30:17,039
iFix X3과 동일한 관찰값과 동일하게 고정하므로 먼저 여기에 알고리즘을 작성할 것입니다.

723
00:30:17,039 --> 00:30:18,720


724
00:30:18,720 --> 00:30:21,299
그런 다음 이것은

725
00:30:21,299 --> 00:30:24,059


726
00:30:24,059 --> 00:30:26,700
더 이상 그래프가 아니라 예측 오류에 개입하고 예측 오류가 0인

727
00:30:26,700 --> 00:30:28,980
0으로 고정해야 하는 중요한 단계입니다.

728
00:30:28,980 --> 00:30:31,020


729
00:30:31,020 --> 00:30:32,480
기본적으로

730
00:30:32,480 --> 00:30:36,179
uh가 의미 없는

731
00:30:36,179 --> 00:30:38,460
정보를 계층 구조 위로

732
00:30:38,460 --> 00:30:40,200
보내거나 실제로  계층 구조는

733
00:30:40,200 --> 00:30:41,880
기본적으로

734
00:30:41,880 --> 00:30:44,659
예측이 항상 정확하다는 것을 알려주고

735
00:30:44,659 --> 00:30:48,120
세 번째 단계는 이전에 했던 것처럼

736
00:30:48,120 --> 00:30:50,220


737
00:30:50,220 --> 00:30:52,919
제한되지 않은 Axis 또는 X1 X2 X4

738
00:30:52,919 --> 00:30:55,679
자유 에너지의 변화를 최소화하여 자유 에너지의 변화를 최소화하는 것입니다.

739
00:30:55,679 --> 00:30:59,039


740
00:30:59,039 --> 00:31:00,840


741
00:31:00,840 --> 00:31:02,399
예측 오류를 0으로 설정하는 이 작은 트릭은

742
00:31:02,399 --> 00:31:05,120


743
00:31:05,640 --> 00:31:08,220
우리가

744
00:31:08,220 --> 00:31:10,320


745
00:31:10,320 --> 00:31:13,620
미적분 이론이 하는 것처럼 그래프의 구조에 실제로 작용하는 것을 방지하고 단순히 수행하여 개입

746
00:31:13,620 --> 00:31:16,919
후 누락된 변수를 추론하는 것을 방지합니다.

747
00:31:16,919 --> 00:31:19,140


748
00:31:19,140 --> 00:31:22,640
자유 에너지 최소화의 일차

749
00:31:24,659 --> 00:31:26,580
반사

750
00:31:26,580 --> 00:31:28,080
실적 추론은 어떻습니까

751
00:31:28,080 --> 00:31:30,539


752
00:31:30,539 --> 00:31:34,740
개입을 수행하는 방법을 정의하면

753
00:31:34,740 --> 00:31:36,539


754
00:31:36,539 --> 00:31:38,640
반사실적 추론은 실제로 쉽습니다.  관찰

755
00:31:38,640 --> 00:31:40,380


756
00:31:40,380 --> 00:31:44,360


757
00:31:44,360 --> 00:31:48,120
불가능 관찰 불가능 변수

758
00:31:48,120 --> 00:31:49,620
그래서

759
00:31:49,620 --> 00:31:51,480


760
00:31:51,480 --> 00:31:53,520
이전에 납치 동작 및

761
00:31:53,520 --> 00:31:56,039
예측 단계에 대해 보여드린 플롯에서 볼 수 있듯이 동작 및

762
00:31:56,039 --> 00:31:58,320
예측 단계 그들은 제거된

763
00:31:58,320 --> 00:31:59,640
두 개의 화살표가 없었습니다.

764
00:31:59,640 --> 00:32:02,580
예쁜 코딩을 통해

765
00:32:02,580 --> 00:32:06,299
이 화살표를 유지할 수 있습니다.

766
00:32:06,299 --> 00:32:08,279


767
00:32:08,279 --> 00:32:11,340


768
00:32:11,340 --> 00:32:13,380


769
00:32:13,380 --> 00:32:14,640
이전에 수행한 것처럼 단순히 납치 단계를 수행하여 그래프 및 반사실을 수행합니다.

770
00:32:14,640 --> 00:32:16,679


771
00:32:16,679 --> 00:32:18,600
단일 노드에 대한 개입을 수행하여

772
00:32:18,600 --> 00:32:21,240
값 노드를 수정하고

773
00:32:21,240 --> 00:32:24,240
오류를 0으로 설정

774
00:32:24,240 --> 00:32:26,399
하고 에너지 최소화를 실행하는 작업 단계

775
00:32:26,399 --> 00:32:27,960


776
00:32:27,960 --> 00:32:30,679
예측을 계산하기 위해 자유 에너지의 지속 시간을 최소화하므로

777
00:32:32,399 --> 00:32:36,299


778
00:32:36,299 --> 00:32:39,840
개입 및 반사실을 수행하는 쉽고 우아한 방법과 같다고 생각합니다.

779
00:32:39,840 --> 00:32:42,899


780
00:32:42,899 --> 00:32:44,880
그래서 지금 우리가 보여줘야 할 것은 실제로

781
00:32:44,880 --> 00:32:46,500
작동하는지 여부이며

782
00:32:46,500 --> 00:32:48,720
몇 가지

783
00:32:48,720 --> 00:32:49,919
실험을

784
00:32:49,919 --> 00:32:52,440
하고 이제 두 가지

785
00:32:52,440 --> 00:32:54,240
다른 실험을 보여드리겠습니다. 첫 번째는

786
00:32:54,240 --> 00:32:57,179


787
00:32:57,179 --> 00:33:01,020
작동 코딩에서

788
00:33:01,020 --> 00:33:02,480


789
00:33:02,480 --> 00:33:06,120
개입 및 반사실을 수행할 수 있음을 보여주는 개념 실험의 증거일 뿐이고

790
00:33:06,120 --> 00:33:08,700
두 번째는 실제로

791
00:33:08,700 --> 00:33:11,220
간단한 응용 프로그램을 보여줍니다.  완전히 연결된 모델의

792
00:33:11,220 --> 00:33:13,440


793
00:33:13,440 --> 00:33:16,260


794
00:33:16,260 --> 00:33:18,360
특정 종류의 작동 코딩

795
00:33:18,360 --> 00:33:20,940
네트워크에서 분류 작업의 성능을 개선하기 위해 개입 쿼리를 사용할 수 있는 방법에 대해

796
00:33:20,940 --> 00:33:22,080


797
00:33:22,080 --> 00:33:24,659
처음부터 시작하겠습니다.

798
00:33:24,659 --> 00:33:27,679


799
00:33:27,679 --> 00:33:30,360
구조적 협의회 모델이 주어지면 이 작업을 어떻게 수행합니까?

800
00:33:30,360 --> 00:33:33,360
교육 데이터를 생성하고 이를 사용하여

801
00:33:33,360 --> 00:33:35,760
가중치를 학습하여

802
00:33:35,760 --> 00:33:39,480
구조적 Kaza 모델의 기능을 학습한

803
00:33:39,480 --> 00:33:42,779
다음 Interventional 및 counterfaction 쿼리에 대한 테스트 테스트 데이터를 생성

804
00:33:42,779 --> 00:33:44,399


805
00:33:44,399 --> 00:33:46,080


806
00:33:46,080 --> 00:33:48,000
하고 올바른 테스트 데이터로 수렴할 수 있는지 여부를 보여줍니다.

807
00:33:48,000 --> 00:33:51,360


808
00:33:51,360 --> 00:33:53,340
창의적인 코딩을 사용

809
00:33:53,340 --> 00:33:54,779


810
00:33:54,779 --> 00:33:57,240
하고 예를 들어 여기 두 개의

811
00:33:57,240 --> 00:33:58,860
플롯에서 중재

812
00:33:58,860 --> 00:34:00,600
개입 및 반사실적 쿼리를 나타내는

813
00:34:00,600 --> 00:34:03,539
이 특정 그래프는

814
00:34:03,539 --> 00:34:05,880
버터플라이 편향 그래프입니다. 이 그래프는

815
00:34:05,880 --> 00:34:08,280


816
00:34:08,280 --> 00:34:10,859


817
00:34:10,859 --> 00:34:12,179
개입 및 반사실적

818
00:34:12,179 --> 00:34:15,540
기술이 인과적 추론인지 여부를 테스트하는 데 자주 사용되는 그래프입니다.  작업은 그렇게 간단하지만

819
00:34:15,540 --> 00:34:18,000
논문에서 많은

820
00:34:18,000 --> 00:34:20,760
다른 그래프를 찾을 수 있지만 일반적으로 이

821
00:34:20,760 --> 00:34:22,800
두 그래프는

822
00:34:22,800 --> 00:34:26,940
방법이 작동한다는 것을 보여줍니다.

823
00:34:26,940 --> 00:34:27,918


824
00:34:27,918 --> 00:34:32,219


825
00:34:32,219 --> 00:34:33,960


826
00:34:33,960 --> 00:34:37,399
계산 및 원본 그래프의 Interventional 및

827
00:34:37,399 --> 00:34:39,780
counterfactual 수량은

828
00:34:39,780 --> 00:34:41,460


829
00:34:41,460 --> 00:34:43,800
서로 가깝기 때문에 오류가

830
00:34:43,800 --> 00:34:45,800
매우 작습니다.

831
00:34:45,800 --> 00:34:49,139
두 번째 실험은 기본적으로 임의

832
00:34:49,139 --> 00:34:51,239


833
00:34:51,239 --> 00:34:54,540


834
00:34:54,540 --> 00:34:56,460
그래프 토폴로지에 대한 학습인 이전 논문에서 제안한 실험의 확장입니다.

835
00:34:56,460 --> 00:34:59,040
작년에

836
00:34:59,040 --> 00:35:01,080
그 논문에서 쓴 나는

837
00:35:01,080 --> 00:35:04,200
기본적으로 완전히 연결된 네트워크인 개념 증명으로 이런 종류의 네트워크를 제안합니다.

838
00:35:04,200 --> 00:35:06,060


839
00:35:06,060 --> 00:35:08,160


840
00:35:08,160 --> 00:35:11,579
일반적으로

841
00:35:11,579 --> 00:35:13,500


842
00:35:13,500 --> 00:35:15,960
기계 학습 실험을 수행해야 할 수 있는 최악의 신경망입니다.

843
00:35:15,960 --> 00:35:20,520


844
00:35:20,520 --> 00:35:23,660
기본적으로 뉴런 세트는

845
00:35:23,760 --> 00:35:26,400
모든 뉴런 쌍이

846
00:35:26,400 --> 00:35:28,680
두 개의 서로 다른 시냅스로 연결되어 있기 때문에

847
00:35:28,680 --> 00:35:31,200


848
00:35:31,200 --> 00:35:33,359


849
00:35:33,359 --> 00:35:34,619
일반적으로 가장 복잡도가 높은 모델입니다.

850
00:35:34,619 --> 00:35:36,300
좋은 점은

851
00:35:36,300 --> 00:35:37,859
사이클이 많기 때문에 모델이 매우

852
00:35:37,859 --> 00:35:39,599
유연하다는 것입니다.

853
00:35:39,599 --> 00:35:42,480
예를 들어 다진 이미지와

854
00:35:42,480 --> 00:35:45,359
데이터 포인트, 레이블에서 학습할 수 있지만 다시

855
00:35:45,359 --> 00:35:47,400


856
00:35:47,400 --> 00:35:50,640
돌아가는 정보 덕분에 쿼리할 수 있는 방법은 어

857
00:35:50,640 --> 00:35:52,140
다양한 방법으로 쿼리할 수 있습니다.

858
00:35:52,140 --> 00:35:54,060


859
00:35:54,060 --> 00:35:55,980
이미지를 제공하고

860
00:35:55,980 --> 00:35:57,480
에너지 최소화를 실행하고 라벨을 얻는 분류 작업을 구성할 수

861
00:35:57,480 --> 00:35:59,400
있지만, 예를 들어

862
00:35:59,400 --> 00:36:01,320


863
00:36:01,320 --> 00:36:03,060
라벨에 에너지 최소화를 실행하고

864
00:36:03,060 --> 00:36:05,220
수행할 수 있는 이미지를 얻는 생성 작업을 수행할 수도 있습니다.

865
00:36:05,220 --> 00:36:06,960
예를 들어 이미지

866
00:36:06,960 --> 00:36:10,260
이미지의 절반을 제공하고 수렴하고

867
00:36:10,260 --> 00:36:12,119
수렴하여 모델이 후반으로 변환되도록 하는 식으로

868
00:36:12,119 --> 00:36:14,400


869
00:36:14,400 --> 00:36:16,440
기본적으로 분류에

870
00:36:16,440 --> 00:36:19,619


871
00:36:19,619 --> 00:36:21,900
집중하지 않고 전체 데이터 세트의 통계를 학습하는 모델입니다.

872
00:36:21,900 --> 00:36:25,079
일반적으로

873
00:36:25,079 --> 00:36:27,900
이 유연성은 훌륭합니다.

874
00:36:27,900 --> 00:36:31,260
문제는 이 때문에

875
00:36:31,260 --> 00:36:34,140
모든 개별 작업이 제대로 작동하지 않아

876
00:36:34,140 --> 00:36:35,820
많은 다른 작업을 수행할 수

877
00:36:35,820 --> 00:36:38,579
있지만 그 중 어느 것도 잘 수행되지 않는다는 것입니다.

878
00:36:38,579 --> 00:36:39,960


879
00:36:39,960 --> 00:36:42,480
여기에서 사용 방법을 보여주고 싶습니다.

880
00:36:42,480 --> 00:36:44,099


881
00:36:44,099 --> 00:36:46,740
표준 uh 상관 쿼리 또는

882
00:36:46,740 --> 00:36:48,119
조건부 쿼리 대신 개입 쿼리는

883
00:36:48,119 --> 00:36:49,980
이러한

884
00:36:49,980 --> 00:36:51,960
분류 작업의 결과를 약간 향상하므로 이러한 작업의

885
00:36:51,960 --> 00:36:54,000
추측 이유는 무엇입니까

886
00:36:54,000 --> 00:36:57,599
uh

887
00:36:57,599 --> 00:37:01,079
해당 작업에 대한 테스트 정확도가 그렇게 높지 않습니다.

888
00:37:01,079 --> 00:37:03,180
첫 번째 두 가지 이유는 모델이 산만하다는 것입니다.

889
00:37:03,180 --> 00:37:05,640
모든

890
00:37:05,640 --> 00:37:07,920
단일 오류를 수정할 때 기본적으로

891
00:37:07,920 --> 00:37:09,420
이미지를 제시하고

892
00:37:09,420 --> 00:37:11,579
레이블을 얻고 싶지만 실제로 모델은

893
00:37:11,579 --> 00:37:13,859


894
00:37:13,859 --> 00:37:16,320
이미지의 오류를 예측하기 위해 스스로 업데이트하고

895
00:37:16,320 --> 00:37:18,480
있으며 두 번째 이유는 내가

896
00:37:18,480 --> 00:37:21,119
말한 것입니다.  구조가 너무

897
00:37:21,119 --> 00:37:24,540
복잡해서 결과

898
00:37:24,540 --> 00:37:27,079
건포도 Occam razor

899
00:37:27,079 --> 00:37:28,800
논증에서 다시 한 번

900
00:37:28,800 --> 00:37:30,720
이것은 최악의 모델이므로 데이터 세트에

901
00:37:30,720 --> 00:37:32,160
맞는 모델을 가질 때마다

902
00:37:32,160 --> 00:37:33,960


903
00:37:33,960 --> 00:37:35,579
이 모델보다 덜 복잡할 것입니다.

904
00:37:35,579 --> 00:37:37,560
선호되지만

905
00:37:37,560 --> 00:37:40,560
일반적으로 어 그냥 그것을 연구하기 위해

906
00:37:40,560 --> 00:37:41,400


907
00:37:41,400 --> 00:37:43,380
아이디어는 완전히 연결된 모델의 성능을 개선하는 데 사용되는 개입을 이 모델에서 쿼리할 수 있다는 것입니다.

908
00:37:43,380 --> 00:37:44,820


909
00:37:44,820 --> 00:37:46,859


910
00:37:46,859 --> 00:37:48,599


911
00:37:48,599 --> 00:37:51,060
대답은 예입니다.

912
00:37:51,060 --> 00:37:53,160
그래서 여기에 내가 개입 쿼리를 수행하는 방법이 있습니다

913
00:37:53,160 --> 00:37:55,619
네트워크에 이미지를 표시합니다.

914
00:37:55,619 --> 00:37:56,640


915
00:37:56,640 --> 00:37:59,460
픽셀의 오류를

916
00:37:59,460 --> 00:38:01,560
0으로 수정하여 이 오류가

917
00:38:01,560 --> 00:38:03,180
네트워크에서 전파되지 않도록 한

918
00:38:03,180 --> 00:38:05,700
다음 레이블을 계산합니다.

919
00:38:05,700 --> 00:38:08,400
보시다시피 정확성이

920
00:38:08,400 --> 00:38:11,339
예를 들어 89에서

921
00:38:11,339 --> 00:38:13,380
창의적인 코딩 네트워크의 표준 쿼리 방법을

922
00:38:13,380 --> 00:38:16,800


923
00:38:16,800 --> 00:38:19,020
개입 후 정확도인 92로 설정하고 패션 수단에 대해서도 마찬가지입니다.

924
00:38:19,020 --> 00:38:21,540


925
00:38:21,540 --> 00:38:24,420


926
00:38:24,420 --> 00:38:26,940


927
00:38:26,940 --> 00:38:28,920
이러한 플롯을 볼 때 모든 사람이 생각할 수 있는 매우 정당한 비평가는

928
00:38:28,920 --> 00:38:32,099
89에서 수단으로 개선해도 괜찮다고 생각합니다.  92까지는

929
00:38:32,099 --> 00:38:36,180
기본적으로 여전히 형편없고 네 맞습니다.

930
00:38:36,180 --> 00:38:38,400
실제로 후반 슬라이드에서

931
00:38:38,400 --> 00:38:40,619


932
00:38:40,619 --> 00:38:42,660
이 구조에 대해 어떻게 조치를 취하는지 보여드리겠습니다. 완전히 연결된 이 모델의 어,

933
00:38:42,660 --> 00:38:43,859


934
00:38:43,859 --> 00:38:46,500
결과가 92에 도달할 때까지 결과를 더욱 향상시킬 것입니다.

935
00:38:46,500 --> 00:38:48,480
리치 어

936
00:38:48,480 --> 00:38:50,820
성능은 물론 최첨단 성능에 가깝지도 않지만

937
00:38:50,820 --> 00:38:52,560


938
00:38:52,560 --> 00:38:55,320
여전히 올라갔지만

939
00:38:55,320 --> 00:38:57,380
기본적으로 받아들일 수 있는 수준은 아닙니다.

940
00:38:57,380 --> 00:39:01,760


941
00:39:02,040 --> 00:39:04,980


942
00:39:04,980 --> 00:39:08,400


943
00:39:08,400 --> 00:39:10,920
요약하자면 제가

944
00:39:10,920 --> 00:39:15,060
방금 보여준 결과의 흥미로운 부분은

945
00:39:15,060 --> 00:39:17,640


946
00:39:17,640 --> 00:39:19,859
작동 코딩이

947
00:39:19,859 --> 00:39:22,560
매우 쉽고

948
00:39:22,560 --> 00:39:24,780
직관적인 방식으로 개입을 수행할 수 있다는 것입니다.

949
00:39:24,780 --> 00:39:26,280
이전 그래프의 구조는

950
00:39:26,280 --> 00:39:28,740
더 이상 때때로 이러한 함수

951
00:39:28,740 --> 00:39:31,079
기능을 사용할 수 없는 등의 경우가 있지만

952
00:39:31,079 --> 00:39:34,020


953
00:39:34,020 --> 00:39:36,140


954
00:39:36,140 --> 00:39:39,780
단일 뉴런 연구

955
00:39:39,780 --> 00:39:41,640
예측 오류에 개입하여 0으로 예측 오류를

956
00:39:41,640 --> 00:39:44,220
수행하고 어 에너지 최소화 프로세스를 수행하면 됩니다.

957
00:39:44,220 --> 00:39:46,619


958
00:39:46,619 --> 00:39:49,200
이러한 확장은

959
00:39:49,200 --> 00:39:51,240
구조적 인과관계 모델을 기반으로 창의적인 코딩을 정의할 수 있게 되었습니다.

960
00:39:51,240 --> 00:39:52,920


961
00:39:52,920 --> 00:39:54,920
이제 우리는 구조적 구조 학습에 관한 작업의 두 번째 부분으로 이동합니다.

962
00:39:54,920 --> 00:39:57,900


963
00:39:57,900 --> 00:40:01,700


964
00:40:02,000 --> 00:40:05,099
제가 말했듯이 지시 학습은

965
00:40:05,099 --> 00:40:07,260


966
00:40:07,260 --> 00:40:09,720


967
00:40:09,720 --> 00:40:11,880
관찰 데이터로부터 모델의 인과적 구조를 학습하는 문제를 다룹니다.

968
00:40:11,880 --> 00:40:13,800
실제로 수십 년 동안 문제가 없었고

969
00:40:13,800 --> 00:40:17,760


970
00:40:17,760 --> 00:40:21,359
몇

971
00:40:21,359 --> 00:40:24,000
년 전까지만 해도 항상 조합 검색 방법을 사용하여 해결되었습니다.

972
00:40:24,000 --> 00:40:25,560


973
00:40:25,560 --> 00:40:26,640
이러한 커뮤니티

974
00:40:26,640 --> 00:40:29,280
연구 방법의 문제는 데이터가 다중화되자마자

975
00:40:29,280 --> 00:40:32,880
복잡성이 기하급수적으로 두 배로 증가한다는 것입니다.

976
00:40:32,880 --> 00:40:34,740


977
00:40:34,740 --> 00:40:36,780


978
00:40:36,780 --> 00:40:39,920


979
00:40:39,920 --> 00:40:42,300


980
00:40:42,300 --> 00:40:46,680


981
00:40:46,680 --> 00:40:48,780


982
00:40:48,780 --> 00:40:51,000


983
00:40:51,000 --> 00:40:53,540
2018년의 새로운 신문에서 실제로 몇 년 전에 나온 새로운 솔루션은

984
00:40:53,839 --> 00:40:55,920
실제로 이 구조를 학습하는 것이 가능하다는 것을 보여줍니다.

985
00:40:55,920 --> 00:40:57,900


986
00:40:57,900 --> 00:40:59,940
조합기 연구 방법이지만

987
00:40:59,940 --> 00:41:01,619
그래디언트 기반 방법을 사용하여

988
00:41:01,619 --> 00:41:05,280
이것은 기본적으로 일반적으로 숙련된

989
00:41:05,280 --> 00:41:07,320
문제였습니다. 왜냐하면 이제 매개

990
00:41:07,320 --> 00:41:08,820


991
00:41:08,820 --> 00:41:10,980
변수에 대한 사전을 가질 수 있기 때문입니다. 우선

992
00:41:10,980 --> 00:41:12,420
순위 목적은 내가

993
00:41:12,420 --> 00:41:14,700
조금 더 잘 정의할 것입니다.  이 슬라이드에서

994
00:41:14,700 --> 00:41:15,599


995
00:41:15,599 --> 00:41:18,180
경사 하강법을 실행하고

996
00:41:18,180 --> 00:41:19,740
두 배 세 배인 모델이 있더라도

997
00:41:19,740 --> 00:41:20,820
크기는

998
00:41:20,820 --> 00:41:23,640
어 알고리즘이 여전히 믿을 수 없을 정도로

999
00:41:23,640 --> 00:41:25,440
빠르며

1000
00:41:25,440 --> 00:41:28,260
이러한 이유로 이 논문은

1001
00:41:28,260 --> 00:41:31,200
이것이 새로운 것 같고

1002
00:41:31,200 --> 00:41:33,180
이미 주변에 있다고 생각합니다.  600개의

1003
00:41:33,180 --> 00:41:35,099
인용 또는 이와 유사한 것

1004
00:41:35,099 --> 00:41:37,140
그리고

1005
00:41:37,140 --> 00:41:38,720
친구 상담 및

1006
00:41:38,720 --> 00:41:42,000
그래프의 학습 구조에 대해 지금 보고 있는 모든 논문은 그들의 방법을 사용합니다.

1007
00:41:42,000 --> 00:41:44,820
조금

1008
00:41:44,820 --> 00:41:46,980
더 빠르거나 약간 더 나은

1009
00:41:46,980 --> 00:41:49,440
추론 방법을 찾았지만 여전히 모두 사용합니다.

1010
00:41:49,440 --> 00:41:53,760
이 논문이 정의되기 전에 나도

1011
00:41:53,760 --> 00:41:56,460
그렇게 하고 우리도 그렇게 하므로

1012
00:41:56,460 --> 00:41:58,859
여기에서 에이전시 매트릭스라는 새로운 수량을 찾을 수 있습니다.

1013
00:41:58,859 --> 00:42:01,500
에이전시

1014
00:42:01,500 --> 00:42:03,480
매트릭스는 단순히 모델의 연결을 인코딩하는 매트릭스이므로

1015
00:42:03,480 --> 00:42:06,359


1016
00:42:06,359 --> 00:42:08,520
이진 매트릭스이며

1017
00:42:08,520 --> 00:42:10,920
일반적으로  이진 행렬입니다.

1018
00:42:10,920 --> 00:42:12,180
물론 그래디언트 기반 최적화를 수행할 때

1019
00:42:12,180 --> 00:42:14,880
이를 연속적으로 만든

1020
00:42:14,880 --> 00:42:16,800
다음 기본적으로 가장자리를 죽이거나 1로 설정하는 일부 지점에서 임계값을 갖게 되며

1021
00:42:16,800 --> 00:42:19,800


1022
00:42:19,800 --> 00:42:21,480


1023
00:42:21,480 --> 00:42:27,780
M3 IJ는 다음과 같은 경우 1이 됩니다.

1024
00:42:27,780 --> 00:42:30,540
베이지안 그래프가

1025
00:42:30,540 --> 00:42:35,040
꼭지점 I에서 꼭지점 J까지의 가장자리이거나 0인 경우

1026
00:42:35,040 --> 00:42:37,380
예를 들어 여기에서 이 에이전시

1027
00:42:37,380 --> 00:42:39,540
매트릭스는

1028
00:42:39,540 --> 00:42:42,780
이 시각적 네트워크의 연결 구조를 나타내며

1029
00:42:42,780 --> 00:42:44,040


1030
00:42:44,040 --> 00:42:46,079
기본적으로 이 방법은

1031
00:42:46,079 --> 00:42:48,780


1032
00:42:48,780 --> 00:42:51,000


1033
00:42:51,000 --> 00:42:53,460
구조 학습에 대해 우리가 원하는 두 가지 문제를 해결합니다.  방정식 네트워크의

1034
00:42:53,460 --> 00:42:54,780
아이디어는 우리가 개념적으로 유사한 완전히 연결된 모델에서 시작한다는 것입니다.

1035
00:42:54,780 --> 00:42:57,200


1036
00:42:57,200 --> 00:43:00,240
실제로는 이전에 정의한

1037
00:43:00,240 --> 00:43:02,220
작동 코딩 네트워크와 동일하며

1038
00:43:02,220 --> 00:43:04,020
완전히

1039
00:43:04,020 --> 00:43:06,480
연결되어 있으므로 많은

1040
00:43:06,480 --> 00:43:08,640
정점과 모든 정점 쌍이 있습니다.

1041
00:43:08,640 --> 00:43:10,920
두 개의 서로 다른 모서리로 uh로 연결되어

1042
00:43:10,920 --> 00:43:13,319
있고 필요하지 않은 것을 가지치기만 하면 되므로

1043
00:43:13,319 --> 00:43:15,780


1044
00:43:15,780 --> 00:43:18,540


1045
00:43:18,540 --> 00:43:20,819
모형 축소를 수행하는 방법으로 볼 수 있습니다.

1046
00:43:20,819 --> 00:43:22,020
큰 모형에서 시작하여 작게 만들고 싶기

1047
00:43:22,020 --> 00:43:22,800


1048
00:43:22,800 --> 00:43:25,800
때문에  모델을 잘 줄이기 위한 첫 번째 요소는

1049
00:43:25,800 --> 00:43:28,260
물론 sparse

1050
00:43:28,260 --> 00:43:29,220
City

1051
00:43:29,220 --> 00:43:31,619
이고 모든 사람이

1052
00:43:31,619 --> 00:43:33,839
모델을 더 sparse하게 만들기 위해 사용하는 우선

1053
00:43:33,839 --> 00:43:36,480
순위는 기계 학습에서

1054
00:43:36,480 --> 00:43:38,880
단순히 L1 Norm으로 알려진 LaPlace Priority입니다.

1055
00:43:38,880 --> 00:43:40,920


1056
00:43:40,920 --> 00:43:43,980


1057
00:43:43,980 --> 00:43:46,740
앞서 언급한 이 논문은 인과 추론을 수행하려는 베이지안 네트워크의 가장 큰 특징이

1058
00:43:46,740 --> 00:43:49,319
무엇인지를 강제하는 두 번째 사전을 맨 위에 추가하는 것입니다.

1059
00:43:49,319 --> 00:43:53,359


1060
00:43:53,359 --> 00:43:55,980


1061
00:43:55,980 --> 00:43:57,780


1062
00:43:57,780 --> 00:43:59,819


1063
00:43:59,819 --> 00:44:01,020


1064
00:44:01,020 --> 00:44:03,000
비

1065
00:44:03,000 --> 00:44:06,359
주기성은 에이전시 매트릭스에 사전에 부과될 수

1066
00:44:06,359 --> 00:44:08,160


1067
00:44:08,160 --> 00:44:10,859
있으며 여기에는 이 모양이 있으므로

1068
00:44:10,859 --> 00:44:14,640
매트릭스의 자취는 어 곱하기 a의 지수입니다.

1069
00:44:14,640 --> 00:44:18,420


1070
00:44:18,420 --> 00:44:21,859
여기서 a는 에이전시 매트릭스이고

1071
00:44:21,859 --> 00:44:24,300
기본적으로 여기의 양은 다음과

1072
00:44:24,300 --> 00:44:27,900
같습니다.

1073
00:44:27,900 --> 00:44:30,480
베이지안 네트워크 또는

1074
00:44:30,480 --> 00:44:32,819
고려하고 있는 그래프가

1075
00:44:32,819 --> 00:44:35,720
c 클릭인 경우에만 0과 같으

1076
00:44:37,619 --> 00:44:40,260
므로 일부 실험에서 이 두 가지를 사용할 것입니다.

1077
00:44:40,260 --> 00:44:42,960


1078
00:44:42,960 --> 00:44:45,660


1079
00:44:45,660 --> 00:44:47,520
인과 관계 추론을 수행하는 것과 관련하여

1080
00:44:47,520 --> 00:44:49,200


1081
00:44:49,200 --> 00:44:51,540
이전에 제안한 기술과 병합하려고 합니다.

1082
00:44:51,540 --> 00:44:52,740
작동 코딩을 수행하므로

1083
00:44:52,740 --> 00:44:55,020


1084
00:44:55,020 --> 00:44:56,520
두 가지 다른 실험을 제시할 것이므로

1085
00:44:56,520 --> 00:44:59,640
하나는 개념 증명입니다.

1086
00:44:59,640 --> 00:45:00,960


1087
00:45:00,960 --> 00:45:03,660


1088
00:45:03,660 --> 00:45:06,599


1089
00:45:06,599 --> 00:45:08,880
데이터에서 올바른 베이지안 네트워크를 추론하는 모든 구조적 학습 작업을 수행한

1090
00:45:08,880 --> 00:45:11,760
다음 이전에

1091
00:45:11,760 --> 00:45:13,500
보여준 분류 실험 위에 구축할 것입니다.

1092
00:45:13,500 --> 00:45:14,280


1093
00:45:14,280 --> 00:45:16,020


1094
00:45:16,020 --> 00:45:18,540


1095
00:45:18,540 --> 00:45:21,060


1096
00:45:21,060 --> 00:45:22,500
정확도

1097
00:45:22,500 --> 00:45:25,500
완전히 연결된 예측 코딩 모델의 테스트 정확도이므로

1098
00:45:25,500 --> 00:45:28,160


1099
00:45:29,520 --> 00:45:31,680


1100
00:45:31,680 --> 00:45:33,300


1101
00:45:33,300 --> 00:45:34,980
그래프의 구조를 추론하는 첫 번째 실험으로 이동하고

1102
00:45:34,980 --> 00:45:37,319
모든 실험은

1103
00:45:37,319 --> 00:45:39,480
기본적으로 해당 분야의 모든 논문에서 동일한 파이프라인을 따릅니다.

1104
00:45:39,480 --> 00:45:42,060
첫 번째 단계는

1105
00:45:42,060 --> 00:45:45,119
생성하는 것입니다.  임의 그래프의 비전 네트워크

1106
00:45:45,119 --> 00:45:46,079


1107
00:45:46,079 --> 00:45:48,359
이므로 기본적으로 일반적으로

1108
00:45:48,359 --> 00:45:50,640
모든 사람이 테스트하는 두 개의 임의 그래프는 Erdos

1109
00:45:50,640 --> 00:45:53,520
재그래프 및 스케일 프리 그래프

1110
00:45:53,520 --> 00:45:55,859
이므로

1111
00:45:55,859 --> 00:45:58,680
일반적으로 80 80개의 서로 다른 노드에 대해 20개가 있는 큰 그래프

1112
00:45:58,680 --> 00:46:01,619
와 임의로 샘플링하는 일부 가장자리가 있는 큰 그래프를 생성하고

1113
00:46:01,619 --> 00:46:04,619


1114
00:46:04,619 --> 00:46:06,540
이 그래프를 사용하여

1115
00:46:06,540 --> 00:46:08,280
데이터 세트를 생성하므로

1116
00:46:08,280 --> 00:46:10,819
예를 들어

1117
00:46:10,819 --> 00:46:14,460
n Big N 데이터 포인트를 샘플링하고

1118
00:46:14,460 --> 00:46:16,859
그들이

1119
00:46:16,859 --> 00:46:18,780
이전에 생성한 그래프를 가져와 버리는 것입니다.

1120
00:46:18,780 --> 00:46:20,819
데이터 세트와 작업만 유지합니다.

1121
00:46:20,819 --> 00:46:23,099
지금 해결하고 싶은 것은

1122
00:46:23,099 --> 00:46:25,020
학습하는 것입니다.

1123
00:46:25,020 --> 00:46:27,420


1124
00:46:27,420 --> 00:46:29,819
기본적으로 버린 그래프

1125
00:46:29,819 --> 00:46:32,579
의 구조를 검색할 수 있는 훈련 알고리즘을 갖는 것입니다.

1126
00:46:32,579 --> 00:46:34,619


1127
00:46:34,619 --> 00:46:36,839
그래서 여기서 우리가 하는 방식은 우리가

1128
00:46:36,839 --> 00:46:38,460
완전히 연결된 창의적 코딩에 있다는 것입니다.

1129
00:46:38,460 --> 00:46:41,760


1130
00:46:41,760 --> 00:46:43,800


1131
00:46:43,800 --> 00:46:45,359
앞서 정의한 희소 및 SQL 사전을 모두 사용하여 이 데이터 세트 D에서 모델을 만들고

1132
00:46:45,359 --> 00:46:48,780


1133
00:46:48,780 --> 00:46:50,760


1134
00:46:50,760 --> 00:46:53,220


1135
00:46:53,220 --> 00:46:55,319


1136
00:46:55,319 --> 00:46:57,599
특정 임계값보다 작은 에이전시 매트릭스의 항목을 제거한 후 실제로 수렴하는 그래프가 다음

1137
00:46:57,599 --> 00:47:00,060
과 유사한지 확인합니다.  초기 그래프의 저것

1138
00:47:00,060 --> 00:47:02,359


1139
00:47:02,520 --> 00:47:04,500
과 이것이

1140
00:47:04,500 --> 00:47:06,599
실제로 그렇다는 것을 보여주기도 해서 이것은 하나의 예이고

1141
00:47:06,599 --> 00:47:09,020
나는 논문에서 많은 다른 매개

1142
00:47:09,020 --> 00:47:12,420
변수화와 차원 등을 보여주지만

1143
00:47:12,420 --> 00:47:15,060


1144
00:47:15,060 --> 00:47:16,920
나는 이 두 가지가 가장

1145
00:47:16,920 --> 00:47:18,900
대표적인 예라고 생각한다.  오차가 있는

1146
00:47:18,900 --> 00:47:20,760
Nursery 그래프와 20개의 노드가 있는 프리스케일 그래프

1147
00:47:20,760 --> 00:47:23,579


1148
00:47:23,579 --> 00:47:25,800
그리고 여기 왼쪽에는 무작위로 샘플링된

1149
00:47:25,800 --> 00:47:27,300
그래프를 통해 지상을 볼 수

1150
00:47:27,300 --> 00:47:29,339


1151
00:47:29,339 --> 00:47:30,839


1152
00:47:30,839 --> 00:47:32,599
있고 오른쪽에는 데이터에서

1153
00:47:32,599 --> 00:47:35,220
학습한 예쁜 난이도 모델의 그래프를 볼 수 있습니다.

1154
00:47:35,220 --> 00:47:37,440
설정

1155
00:47:37,440 --> 00:47:39,359
하고 보시다시피 그것들은 매우

1156
00:47:39,359 --> 00:47:40,500
유사합니다.

1157
00:47:40,500 --> 00:47:42,780
아직 완벽하지 않기 때문에

1158
00:47:42,780 --> 00:47:45,000
약간의 오류가 있지만

1159
00:47:45,000 --> 00:47:47,460
일반적으로 구조는 꽤

1160
00:47:47,460 --> 00:47:49,500
잘 작동합니다. 여기에 표시하지 않은 몇 가지 정량적 실험도 있습니다.

1161
00:47:49,500 --> 00:47:52,140


1162
00:47:52,140 --> 00:47:54,000


1163
00:47:54,000 --> 00:47:55,740
숫자가 많은 거대한 테이블이기 때문에

1164
00:47:55,740 --> 00:47:57,180


1165
00:47:57,180 --> 00:48:00,660
프리젠테이션하기에는 너무 많다고 생각했지만

1166
00:48:00,660 --> 00:48:02,220
결과는

1167
00:48:02,220 --> 00:48:06,060


1168
00:48:06,060 --> 00:48:07,920
대부분의 품질과 마찬가지로 말해야 하기 때문에 현대적인 방법과 유사하게 수행됨을 보여줍니다.

1169
00:48:07,920 --> 00:48:10,859


1170
00:48:10,859 --> 00:48:15,799
2018년에 소개된 on acigli prior에서 나옵니다.

1171
00:48:16,920 --> 00:48:19,680
실험의 두 번째 클래스는 앞서

1172
00:48:19,680 --> 00:48:21,599


1173
00:48:21,599 --> 00:48:23,880
말씀드린 바와 같이 제가

1174
00:48:23,880 --> 00:48:25,560
이전에 공유한 실험의 확장이며

1175
00:48:25,560 --> 00:48:27,119
아이디어는

1176
00:48:27,119 --> 00:48:28,560
분류를 개선하기 위해 구조 학습을 사용하는 것입니다.

1177
00:48:28,560 --> 00:48:31,140
평균 및 패션에 대한 분류 결과는

1178
00:48:31,140 --> 00:48:33,420


1179
00:48:33,420 --> 00:48:36,780
완전 연결된 그래프에서 시작하는 데이터 세트를 의미하므로

1180
00:48:36,780 --> 00:48:40,560
내가 한 것은 뉴런

1181
00:48:40,560 --> 00:48:42,839
의 완전 연결된 그래프 클러스터를 분할하여

1182
00:48:42,839 --> 00:48:46,440
1B 클러스터가

1183
00:48:46,440 --> 00:48:49,140
입력과 관련된 클러스터이고

1184
00:48:49,140 --> 00:48:51,900
모든 작은 다음

1185
00:48:51,900 --> 00:48:55,319
특정 수의 숨겨진 클러스터가

1186
00:48:55,319 --> 00:48:57,720
있고

1187
00:48:57,720 --> 00:48:58,800


1188
00:48:58,800 --> 00:49:01,560


1189
00:49:01,560 --> 00:49:04,079
레이블 예측을 제공해야 하는 뉴런 클러스터 클래스인 레이블 클러스터가 있으며

1190
00:49:04,079 --> 00:49:06,480


1191
00:49:06,480 --> 00:49:08,700


1192
00:49:08,700 --> 00:49:10,980
처음으로 스파스 사전을 사용하기 위해 사용하도록 훈련했습니다.  그래서

1193
00:49:10,980 --> 00:49:14,099
아이디어는 모델에서

1194
00:49:14,099 --> 00:49:16,500
필요하지 않은 연결을 잘라내고

1195
00:49:16,500 --> 00:49:17,460


1196
00:49:17,460 --> 00:49:20,880
파서 모델이 이

1197
00:49:20,880 --> 00:49:24,119
작업을 잘 수행함에 따라 학습하면

1198
00:49:24,119 --> 00:49:25,500
작동하지 않는 이유와 그

1199
00:49:25,500 --> 00:49:28,500
이유는 무엇입니까?  마지막에

1200
00:49:28,500 --> 00:49:30,660
수렴하는 그래프는 실제로

1201
00:49:30,660 --> 00:49:32,700
생성이므로 기본적으로 모델은

1202
00:49:32,700 --> 00:49:36,180


1203
00:49:36,180 --> 00:49:38,400
레이블 자체를 기반으로 레이블을 예측하는 방법을 학습하므로

1204
00:49:38,400 --> 00:49:40,020
입력에서 모든 정보를 버리고

1205
00:49:40,020 --> 00:49:42,480
레이블만 유지합니다.

1206
00:49:42,480 --> 00:49:45,119
여기에서 볼 수 있듯이  레이블 y는 자신을 예측하거나

1207
00:49:45,119 --> 00:49:46,560
다른 실험에서 매개변수를 변경할 때

1208
00:49:46,560 --> 00:49:48,960
y가

1209
00:49:48,960 --> 00:49:52,520
0에서 예측하는 접두어 X1을 예측하므로

1210
00:49:52,520 --> 00:49:55,980


1211
00:49:55,980 --> 00:49:57,240
이 문제에 대한 해결책은 무엇입니까? 이 문제에 대한 해결책은

1212
00:49:57,240 --> 00:49:59,520
우리가 수렴해야 한다는 것입니다.

1213
00:49:59,520 --> 00:50:03,000
비순환 그래프에

1214
00:50:03,000 --> 00:50:05,220
따라서 우리는 순환성을 방지하는 무언가를 추가해야 합니다. 그 중

1215
00:50:05,220 --> 00:50:08,000


1216
00:50:08,000 --> 00:50:10,200
하나는 물론 제가 이미

1217
00:50:10,200 --> 00:50:12,780
제안한 것입니다. 그런 다음 두 번째 기술을 보여

1218
00:50:12,780 --> 00:50:14,520


1219
00:50:14,520 --> 00:50:17,280
첫 번째 기술은

1220
00:50:17,280 --> 00:50:18,680
이전에 정의된 SQL을 사용

1221
00:50:18,680 --> 00:50:21,359
하고 두 번째 기술은  a는

1222
00:50:21,359 --> 00:50:22,859
실제로 부정적인 예를 사용하는 새로운 기술이므로

1223
00:50:22,859 --> 00:50:24,359


1224
00:50:24,359 --> 00:50:26,520
이 경우 부정적인 예는

1225
00:50:26,520 --> 00:50:30,060


1226
00:50:30,060 --> 00:50:32,280
이미지가 있지만 레이블이

1227
00:50:32,280 --> 00:50:33,240
잘못된 데이터 포인트이므로

1228
00:50:33,240 --> 00:50:35,220
예를 들어 7의 이미지가 있습니다.

1229
00:50:35,220 --> 00:50:36,900
하지만 내가 모델에 부여하는 레이블은

1230
00:50:36,900 --> 00:50:39,599
2

1231
00:50:39,599 --> 00:50:40,980
이고

1232
00:50:40,980 --> 00:50:44,579
아이디어는 매우 간단합니다.

1233
00:50:44,579 --> 00:50:47,460
이미 많은 작업에서 사용되었으므로

1234
00:50:47,460 --> 00:50:49,740
모델이 긍정적인 예일 때마다

1235
00:50:49,740 --> 00:50:52,079


1236
00:50:52,079 --> 00:50:53,520
변동을 최소화하기 위해 증가해야 합니다.  자유 에너지의

1237
00:50:53,520 --> 00:50:56,520
그리고 그것이 가질 때마다 그것은

1238
00:50:56,520 --> 00:50:58,859
그것을 증가시켜야 하는 부정적인 예이므로

1239
00:50:58,859 --> 00:51:01,260
오류로 이동하겠습니다. 이

1240
00:51:01,260 --> 00:51:04,200
양은

1241
00:51:04,200 --> 00:51:05,960


1242
00:51:05,960 --> 00:51:08,579
많은 실험과 많은

1243
00:51:08,579 --> 00:51:10,859
실험을 통해 외국에서 최소화됩니다. 우리는

1244
00:51:10,859 --> 00:51:12,119
두 가지 기술이

1245
00:51:12,119 --> 00:51:15,000
기본적으로 첫 번째는 동일한

1246
00:51:15,000 --> 00:51:17,220
결과로 이어지고 두 번째는 동일한 그래프로 이어집니다.

1247
00:51:17,220 --> 00:51:18,599


1248
00:51:18,599 --> 00:51:21,000
그래서

1249
00:51:21,000 --> 00:51:22,800
여기에 제가 방금 제안한

1250
00:51:22,800 --> 00:51:25,079
두 가지 기술을 사용하는 새로운 결과가 있습니다.

1251
00:51:25,079 --> 00:51:27,660


1252
00:51:27,660 --> 00:51:30,960


1253
00:51:30,960 --> 00:51:33,900
여전히 훌륭하지는 않지만 확실히 더

1254
00:51:33,900 --> 00:51:36,000
합리적인 테스트 정확도이므로 여기서

1255
00:51:36,000 --> 00:51:39,059
분당 테스트 오류는 3.17이고

1256
00:51:39,059 --> 00:51:42,119
패션 수단에 대한 테스트 오류는 13.98이며

1257
00:51:42,119 --> 00:51:44,819
실제로 이러한 결과는

1258
00:51:44,819 --> 00:51:48,300


1259
00:51:48,300 --> 00:51:51,300
그래프의 구조를 학습하여 훨씬 개선될 수 있습니다  다진

1260
00:51:51,300 --> 00:51:53,040
다음 그래프의 구조를 고정

1261
00:51:53,040 --> 00:51:55,319
하고 어떤 형태의 미세

1262
00:51:55,319 --> 00:51:57,660
조정을 수행하여 올바른 계층 구조에서 모델을 미세 조정하면

1263
00:51:57,660 --> 00:52:00,000


1264
00:52:00,000 --> 00:52:01,980
어느 시점에서 계층 모델에서 기대할 수 있는 테스트 정확도에 도달하지만

1265
00:52:01,980 --> 00:52:03,359


1266
00:52:03,359 --> 00:52:05,460
하나는

1267
00:52:05,460 --> 00:52:08,099
완전히 연결된 모델이

1268
00:52:08,099 --> 00:52:10,980
자연스럽게 수렴된 모델입니다.

1269
00:52:10,980 --> 00:52:13,859
예를 들어

1270
00:52:13,859 --> 00:52:15,420


1271
00:52:15,420 --> 00:52:17,339


1272
00:52:17,339 --> 00:52:20,359
패션에 대한 완전히 연결된 모델 기차의 테스트 오류 18.32에서 작동 코딩 모델 추가를 쿼리하는 표준 방법인

1273
00:52:20,359 --> 00:52:22,859
상관 관계 또는 조건부 쿼리를 수행하여 수단을 사용합니다.

1274
00:52:22,859 --> 00:52:24,420


1275
00:52:24,420 --> 00:52:26,520


1276
00:52:26,520 --> 00:52:29,220
중재와 AC 클릭

1277
00:52:29,220 --> 00:52:32,040
사전이 함께

1278
00:52:32,040 --> 00:52:34,200
이 테스트 오류를 ​​훨씬 낮추고

1279
00:52:34,200 --> 00:52:37,200
수단으로도 관찰할 수 있습니다.

1280
00:52:37,200 --> 00:52:39,319


1281
00:52:39,780 --> 00:52:41,819


1282
00:52:41,819 --> 00:52:45,420
이 마지막 실험에 대한 세부 사항과 비

1283
00:52:45,420 --> 00:52:48,660
주기적 사전 작업이

1284
00:52:48,660 --> 00:52:50,339
그래프의 구조

1285
00:52:50,339 --> 00:52:52,440
그래서 나는

1286
00:52:52,440 --> 00:52:54,960
새로운 데이터 세트에 대해 실험을 수행합니다.

1287
00:52:54,960 --> 00:52:56,460
새 데이터 세트에서 호출하는 것을 의미합니다.

1288
00:52:56,460 --> 00:52:58,500
너무 많을 수 있습니다.

1289
00:52:58,500 --> 00:53:01,440
입력 포인트가 있는 두 가지 수단 데이터 세트라고 합니다.

1290
00:53:01,440 --> 00:53:04,319
두 개의 서로 다른

1291
00:53:04,319 --> 00:53:07,319
이미지로 구성되며 레이블은 첫 번째 이미지 스토리의 두 번째 이미지에만 의존하므로

1292
00:53:07,319 --> 00:53:08,520


1293
00:53:08,520 --> 00:53:10,800


1294
00:53:10,800 --> 00:53:12,720
여기서 아이디어는 이미지의 후반부를 인식할 수 있는

1295
00:53:12,720 --> 00:53:15,079
모델의 구조입니다.

1296
00:53:15,079 --> 00:53:18,540


1297
00:53:18,540 --> 00:53:20,819


1298
00:53:20,819 --> 00:53:23,400
실제로 의미가 없습니다

1299
00:53:23,400 --> 00:53:27,960
학습에서 수행에서

1300
00:53:27,960 --> 00:53:31,140
분류 수행에서

1301
00:53:31,140 --> 00:53:33,119
훈련이 일반적으로 어떻게 작동합니까 예를 들어

1302
00:53:33,119 --> 00:53:36,480
우리는 어 입력

1303
00:53:36,480 --> 00:53:39,000
노드 출력 노드가 있고 노드만

1304
00:53:39,000 --> 00:53:41,940
완전히 연결되고 모델은

1305
00:53:41,940 --> 00:53:43,740


1306
00:53:43,740 --> 00:53:45,900
계층 구조로 수렴

1307
00:53:45,900 --> 00:53:48,960
우리가 알고 있는 분류 작업에 대해 가장 잘 수행하는 것으로 알고 있는 것은

1308
00:53:48,960 --> 00:53:50,880


1309
00:53:50,880 --> 00:53:53,520


1310
00:53:53,520 --> 00:53:54,980
훈련 방법

1311
00:53:54,980 --> 00:53:59,280
실행의 예이므로 훈련 시작인 c0에서

1312
00:53:59,280 --> 00:54:00,720


1313
00:54:00,720 --> 00:54:03,000
여기에 이 ​​모델이 있으므로 s0은

1314
00:54:03,000 --> 00:54:05,819
7에 해당하므로

1315
00:54:05,819 --> 00:54:08,099
첫 번째 이미지에 해당합니다.  하나는

1316
00:54:08,099 --> 00:54:09,839
다시 7개의 열 이미지에 해당하므로

1317
00:54:09,839 --> 00:54:12,300
레이블 Y와 모든 잠재 변수 x0

1318
00:54:12,300 --> 00:54:13,800
X1 X2가 있고

1319
00:54:13,800 --> 00:54:15,720
모델이 완전히 연결되었으므로

1320
00:54:15,720 --> 00:54:17,040
에이전시 매트릭스는 1

1321
00:54:17,040 --> 00:54:20,579
로 가득 차 있으며 0은 없습니다.

1322
00:54:20,579 --> 00:54:23,720


1323
00:54:23,720 --> 00:54:27,319


1324
00:54:27,319 --> 00:54:30,540
우리가 즉시 아는 것은

1325
00:54:30,540 --> 00:54:31,920
예를 들어 모델이 분류를 수행하는 데

1326
00:54:31,920 --> 00:54:34,740
4가 필요하지 않다는 것을 즉시 이해하므로

1327
00:54:34,740 --> 00:54:36,839


1328
00:54:36,839 --> 00:54:40,740


1329
00:54:40,740 --> 00:54:43,980
두 번째 입력 클러스터에서 모든 나가는 노드가 제거되고

1330
00:54:43,980 --> 00:54:45,900
우리가 이해하지 못한 것은

1331
00:54:45,900 --> 00:54:48,660
이것이 이 클러스터가

1332
00:54:48,660 --> 00:54:50,400
출력과 관련된 것이므로

1333
00:54:50,400 --> 00:54:52,260


1334
00:54:52,260 --> 00:54:55,319
우리는 s0에서 Y까지의 선형 맵을 가지고 있으며

1335
00:54:55,319 --> 00:54:56,480


1336
00:54:56,480 --> 00:54:59,339
여기 이 부분이

1337
00:54:59,339 --> 00:55:01,160
있지만 실제로는 선형 맵이 최고가 아니라는 것을 알고 있습니다.

1338
00:55:01,160 --> 00:55:04,740


1339
00:55:04,740 --> 00:55:07,200
평균 분류를 수행하기 위한 맵이므로 계층

1340
00:55:07,200 --> 00:55:08,700
구조가 필요합니다.

1341
00:55:08,700 --> 00:55:11,579
결과를 개선하기 위해 약간의 깊이가 필요합니다.

1342
00:55:11,579 --> 00:55:14,220
여기에서 볼 수 있듯이 이 선은 정확도입니다.

1343
00:55:14,220 --> 00:55:15,599


1344
00:55:15,599 --> 00:55:18,960
이 지점까지 C2까지는

1345
00:55:18,960 --> 00:55:22,500
음과 비슷하므로 91입니다.  이것은

1346
00:55:22,500 --> 00:55:24,059
선형 분류보다 약간 낫지

1347
00:55:24,059 --> 00:55:25,500


1348
00:55:25,500 --> 00:55:28,740
만 훈련을 계속하면

1349
00:55:28,740 --> 00:55:30,660
모델이

1350
00:55:30,660 --> 00:55:33,119
데이터에 더 잘 맞도록 계층 구조가 필요하다는 것을 이해하므로

1351
00:55:33,119 --> 00:55:35,640
이 화살표가 선형 분류라는 것을 이해할 때까지

1352
00:55:35,640 --> 00:55:38,760
시간이 지남에 따라 점점 더 강해지기 시작하는 것을 볼 수 있습니다.

1353
00:55:38,760 --> 00:55:41,700


1354
00:55:41,700 --> 00:55:44,339
지도는 실제로 필요하지 않으며

1355
00:55:44,339 --> 00:55:45,920
제거하므로

1356
00:55:45,920 --> 00:55:48,780
수렴하는 모델은

1357
00:55:48,780 --> 00:55:51,000
0에서 시작하여

1358
00:55:51,000 --> 00:55:53,760
숨겨진 노드로 이동한 다음

1359
00:55:53,760 --> 00:55:57,180
매우 약한 선형 맵이 있는 레이블로 이동하는 모델이며

1360
00:55:57,180 --> 00:55:59,700
다음과 같은 경우 실제로 제거됩니다.

1361
00:55:59,700 --> 00:56:02,760


1362
00:56:02,760 --> 00:56:05,520
예를 들어 0.1 0.2의 판매자 임계값이 어느

1363
00:56:05,520 --> 00:56:07,619
시점에서 선형 맵이 잊혀지고 결국

1364
00:56:07,619 --> 00:56:10,680
모든 것이

1365
00:56:10,680 --> 00:56:13,319
계층적 네트워크와 함께 있는 경우 임계값을 uh로 설정합니다.

1366
00:56:13,319 --> 00:56:15,720
즉,

1367
00:56:15,720 --> 00:56:17,099
수행할 올바른 구조를 배웠습니다.

1368
00:56:17,099 --> 00:56:19,260
계층 구조인 분류 작업

1369
00:56:19,260 --> 00:56:21,900
과 두 번째

1370
00:56:21,900 --> 00:56:25,020
이미지가 테스트 정확도를 정의하는 데 어떤 역할도 하지 않는다는 것을 배웠습니다.

1371
00:56:25,020 --> 00:56:28,440


1372
00:56:28,440 --> 00:56:30,420
이것이 모두 수행되며 모든

1373
00:56:30,420 --> 00:56:33,839
작업은

1374
00:56:33,839 --> 00:56:36,599
하나의 자유 에너지 최소화 프로세스에 의해 간단히 수행됩니다.

1375
00:56:36,599 --> 00:56:38,400
모델을 초기화합니다.

1376
00:56:38,400 --> 00:56:40,859
자유 에너지를 정의합니다. 우선순위를 정의하므로

1377
00:56:40,859 --> 00:56:43,559


1378
00:56:43,559 --> 00:56:45,780
에너지 최소화를 실행하기 전에 스파스와 C를 클릭하고

1379
00:56:45,780 --> 00:56:47,400


1380
00:56:47,400 --> 00:56:49,500


1381
00:56:49,500 --> 00:56:51,839
다진 분류를 잘 수행할 수 있는 계층적 모델로 수렴합니다.

1382
00:56:51,839 --> 00:56:54,000
그런 다음 미세

1383
00:56:54,000 --> 00:56:55,800
조정을 수행하면 피드백 전파가 있는

1384
00:56:55,800 --> 00:56:57,359
피드 포워드 네트워크에서 수행하는 것처럼 매우 경쟁력 있는 결과에 도달할 수 있습니다.

1385
00:56:57,359 --> 00:56:59,339


1386
00:56:59,339 --> 00:57:01,260
그러나 제 생각에 그것은 흥미로운 부분이 아니라고 생각합니다.

1387
00:57:01,260 --> 00:57:03,780
흥미로운 부분은

1388
00:57:03,780 --> 00:57:05,160
이 모든 프로세스를 좋아한다는 것입니다.

1389
00:57:05,160 --> 00:57:07,980


1390
00:57:07,980 --> 00:57:09,780


1391
00:57:09,780 --> 00:57:11,700
완전히 연결된

1392
00:57:11,700 --> 00:57:12,660
네트워크를 가지고

1393
00:57:12,660 --> 00:57:15,119


1394
00:57:15,119 --> 00:57:16,140


1395
00:57:16,140 --> 00:57:20,058
좋은 결과로 분류를 수행할 수 있는 계층적 네트워크로 수렴하는 것

1396
00:57:20,760 --> 00:57:23,000
그리고 그래

1397
00:57:23,000 --> 00:57:26,280
그게 기본적으로 내가 지금이야

1398
00:57:26,280 --> 00:57:29,220


1399
00:57:29,220 --> 00:57:32,160
나는

1400
00:57:32,160 --> 00:57:35,280
기본적으로 작은 요약을 하고 있으며

1401
00:57:35,280 --> 00:57:37,559


1402
00:57:37,559 --> 00:57:39,300
이 문서의 한 문장으로 여러분에게 제공해야 하는 중요한 요점은

1403
00:57:39,300 --> 00:57:40,980
예측 코딩이 end-to를 수행

1404
00:57:40,980 --> 00:57:44,400
할 수 있는 신념 업데이트 방법이라는 것입니다.  -

1405
00:57:44,400 --> 00:57:46,559
종료 사촌 학습을 수행하여

1406
00:57:46,559 --> 00:57:48,599


1407
00:57:48,599 --> 00:57:51,420
데이터에서 구조를 학습하는 개입을 수행한 다음

1408
00:57:51,420 --> 00:57:53,160
개입 및

1409
00:57:53,160 --> 00:57:56,058
반사실을 수행할 수 있으므로

1410
00:57:56,700 --> 00:57:58,440


1411
00:57:58,440 --> 00:58:00,119


1412
00:58:00,119 --> 00:58:01,680
단순히 예측 오류를 0으로 설정하여 다른 모델 개입에서 인과적 추론을 효율적으로 수행할 수

1413
00:58:01,680 --> 00:58:03,359
있으므로

1414
00:58:03,359 --> 00:58:06,240
수행하기 매우 쉬운 기술입니다.

1415
00:58:06,240 --> 00:58:07,619
하나의 뉴런만 건드리면 됩니다.

1416
00:58:07,619 --> 00:58:08,940


1417
00:58:08,940 --> 00:58:10,859
그래프의 구조에 따라 행동할 필요가 없습니다. 그래프를

1418
00:58:10,859 --> 00:58:14,339
사용하여 생물학적으로 타당한

1419
00:58:14,339 --> 00:58:16,140
구조 인과 모델을 만들 수 있습니다.

1420
00:58:16,140 --> 00:58:18,359


1421
00:58:18,359 --> 00:58:20,819
구조를 학습할 수 있습니다.  어,

1422
00:58:20,819 --> 00:58:24,119


1423
00:58:24,119 --> 00:58:26,940
이미 여러 번 말했듯이 데이터에서

1424
00:58:26,940 --> 00:58:28,740
그리고 향후

1425
00:58:28,740 --> 00:58:31,260
작업에 대한 몇 문장은 우리가 정의한

1426
00:58:31,260 --> 00:58:33,180


1427
00:58:33,180 --> 00:58:36,119
모델의 성능을 개선하는 것이 좋을 것이라는 점입니다.

1428
00:58:36,119 --> 00:58:38,460


1429
00:58:38,460 --> 00:58:40,980


1430
00:58:40,980 --> 00:58:43,079
작업이 많기 때문에 구조적 학습에서 합리적으로 잘 수행됩니다.

1431
00:58:43,079 --> 00:58:45,780


1432
00:58:45,780 --> 00:58:48,119
개입 및 반사실적이지만

1433
00:58:48,119 --> 00:58:49,440
실제로 최첨단

1434
00:58:49,440 --> 00:58:51,420
모델을 보면 단일 작업에서

1435
00:58:51,420 --> 00:58:53,880
더 잘 수행되는 매우 구체적인 방법이 항상 있으므로

1436
00:58:53,880 --> 00:58:55,559


1437
00:58:55,559 --> 00:58:58,260
보는 것이 흥미로울 것입니다.

1438
00:58:58,260 --> 00:59:00,180


1439
00:59:00,180 --> 00:59:03,599
몇 가지

1440
00:59:03,599 --> 00:59:05,599
트릭이나 몇 가지

1441
00:59:05,599 --> 00:59:10,260
또는 몇 가지 새로운 최적화 방법을 추가하여 특정 작업에서 해당 수준의 성능에 도달할 수 있고

1442
00:59:10,260 --> 00:59:12,839


1443
00:59:12,839 --> 00:59:14,280


1444
00:59:14,280 --> 00:59:17,220
동적

1445
00:59:17,220 --> 00:59:20,099
인과 모델과 같은 정적 시스템과 실제로 훨씬 더 흥미로운 동적 시스템으로 일반화할 수 있다면

1446
00:59:20,099 --> 00:59:22,200


1447
00:59:22,200 --> 00:59:25,200
이동하는 시스템에서 인과 관계 추론을 수행할 수 있도록 허용하는 다른 기술은

1448
00:59:25,200 --> 00:59:27,799
특정 시간 단계에서 취한 조치가

1449
00:59:27,799 --> 00:59:30,299


1450
00:59:30,299 --> 00:59:32,640
기본적으로 웅장한 인과 관계인 나중 시간 단계의 다른 노드에 영향을 미치도록 하는 것입니다.

1451
00:59:32,640 --> 00:59:34,859


1452
00:59:34,859 --> 00:59:38,160


1453
00:59:38,160 --> 00:59:41,118


1454
00:59:47,460 --> 00:59:51,119
훌륭하고 매우 포괄적인

1455
00:59:51,119 --> 00:59:53,160
프레젠테이션이었습니다. 정말 음소거되었다고 생각했습니다.

1456
00:59:53,160 --> 00:59:55,700


1457
00:59:57,119 --> 00:59:59,700
Zoom에서 음소거되어 죄송합니다. 하지만 그렇습니다.

1458
00:59:59,700 --> 01:00:02,400
훌륭하고 매우 포괄적인

1459
01:00:02,400 --> 01:00:05,099
프레젠테이션에 감사드립니다. 거기에는 정말 많은 내용이 있었고

1460
01:00:05,099 --> 01:00:06,900


1461
01:00:06,900 --> 01:00:09,900
라이브 채팅에서도 많은 훌륭한 질문이 있었습니다.

1462
01:00:09,900 --> 01:00:12,900
어떻게

1463
01:00:12,900 --> 01:00:15,960
이 주제를 연구하게 되었는지,

1464
01:00:15,960 --> 01:00:18,900
인과 관계를 연구하고 예측 코딩이

1465
01:00:18,900 --> 01:00:21,000
유용하다는 것을 알았는지 또는 그 반대인지 또는 어떻게 이

1466
01:00:21,000 --> 01:00:23,160
교차로에 도달했는지에 대한 질문에

1467
01:00:23,160 --> 01:00:25,740
답해야 합니다.

1468
01:00:25,740 --> 01:00:27,240
이 아이디어를 처음으로 내놓은 사람은

1469
01:00:27,240 --> 01:00:29,040
어, Baron은

1470
01:00:29,040 --> 01:00:33,900
1년

1471
01:00:33,900 --> 01:00:36,660
반 전에도

1472
01:00:36,660 --> 01:00:38,940
이 아이디어가 있는 페이지처럼 가져오고 나서

1473
01:00:38,940 --> 01:00:42,119
잊혀졌고 아무도 그것을 주워주지 않았고 어

1474
01:00:42,119 --> 01:00:43,980
그리고 지난 여름 나는

1475
01:00:43,980 --> 01:00:47,880
인과 관계와 음에 대해 궁금해하기 시작했습니다.

1476
01:00:47,880 --> 01:00:50,339
저는 예를 들어

1477
01:00:50,339 --> 01:00:52,440
팟캐스트를 들으면서 The Book of Life를 읽었습니다. 주제에

1478
01:00:52,440 --> 01:00:53,760
관심을 갖게 되는 일반적인 방법을 알고 있고

1479
01:00:53,760 --> 01:00:54,900


1480
01:00:54,900 --> 01:00:57,480


1481
01:00:57,480 --> 01:01:00,180
Baron의 이 아이디어를 기억하고 그에게 제안했습니다.

1482
01:01:00,180 --> 01:01:03,180
우리는 그것을 확장하고 어

1483
01:01:03,180 --> 01:01:06,000
그리고 실제로 그것을 종이로 만들었습니다. 그래서 저는

1484
01:01:06,000 --> 01:01:07,319


1485
01:01:07,319 --> 01:01:09,359
실험을 도와줄 사람들을 참여시켰고 어 그리고 이것은

1486
01:01:09,359 --> 01:01:12,000
마지막에 최종 결과입니다

1487
01:01:12,000 --> 01:01:14,160


1488
01:01:14,160 --> 01:01:15,240


1489
01:01:15,240 --> 01:01:17,400
굉장합니다 멋지네요 음 음 할말이 많네요 그냥

1490
01:01:17,400 --> 01:01:19,619
실시간 채팅으로 가겠습니다  먼저

1491
01:01:19,619 --> 01:01:21,240
여러 가지 질문에 답하고 다른 사람이

1492
01:01:21,240 --> 01:01:22,440
나를 추가하고 싶다면 먼저 불을 켤 것입니다.

1493
01:01:22,440 --> 01:01:24,059


1494
01:01:24,059 --> 01:01:28,440
점점 더 어두워지고 있다고 생각하기 때문입니다. 예

1495
01:01:28,440 --> 01:01:30,720
누가 능동적 추론으로 해결할 수 없다고 말했습니까?

1496
01:01:30,720 --> 01:01:32,160
암실 문제

1497
01:01:32,160 --> 01:01:34,980
오 예 여기 있습니다

1498
01:01:34,980 --> 01:01:37,020
그래서 전등 스위치로 인해 더

1499
01:01:37,020 --> 01:01:39,299
가벼워졌다고 하시겠습니까? 예 저는

1500
01:01:39,299 --> 01:01:40,680


1501
01:01:40,680 --> 01:01:42,240


1502
01:01:42,240 --> 01:01:43,980
여기에 문제가 없다고 생각합니다

1503
01:01:43,980 --> 01:01:46,940
음 알겠습니다 ml Dawn은

1504
01:01:46,940 --> 01:01:49,559
예측 코딩에서 모든

1505
01:01:49,559 --> 01:01:52,020
분포가 일반적으로 가우시안이기 때문에 썼습니다.

1506
01:01:52,020 --> 01:01:53,760
상향식 메시지는 정밀

1507
01:01:53,760 --> 01:01:55,500
가중 예측입니다.  정밀도가

1508
01:01:55,500 --> 01:01:57,420
가우시안 공분산의 역수인 오류

1509
01:01:57,420 --> 01:02:00,000
비가우시안

1510
01:02:00,000 --> 01:02:03,319
분포가 사용되는 경우

1511
01:02:03,780 --> 01:02:05,339


1512
01:02:05,339 --> 01:02:09,059
기본적으로 일반적인 방법은 다르게 유지됩니다.

1513
01:02:09,059 --> 01:02:10,380
주요 차이점은 올바르게 지적된 대로 기본적으로

1514
01:02:10,380 --> 01:02:13,079
예측 오류가 없다는 것입니다.

1515
01:02:13,079 --> 01:02:15,480


1516
01:02:15,480 --> 01:02:18,480


1517
01:02:18,480 --> 01:02:20,819
가우시안 가정이 있는 경우 가상 자유 에너지의 파생물

1518
01:02:20,819 --> 01:02:22,920


1519
01:02:22,920 --> 01:02:25,020
예 단일 수량을

1520
01:02:25,020 --> 01:02:27,960
0으로 설정하고

1521
01:02:27,960 --> 01:02:29,880


1522
01:02:29,880 --> 01:02:30,900


1523
01:02:30,900 --> 01:02:34,020
개입을 수행하기 위해 그래프의 구조에 따라 조치를 취해야 할 것입니다.

1524
01:02:34,020 --> 01:02:37,079


1525
01:02:37,079 --> 01:02:39,900


1526
01:02:39,900 --> 01:02:41,880


1527
01:02:41,880 --> 01:02:43,859
이러한 문제 중 일부를 살펴본 가우시안 분포를 넘어 코딩하는 것이 맞습니다.

1528
01:02:43,859 --> 01:02:46,260
그렇습니다. 종이가

1529
01:02:46,260 --> 01:02:47,339


1530
01:02:47,339 --> 01:02:50,460
그 종이 뒤에 있는 아이디어는 어

1531
01:02:50,460 --> 01:02:53,220
그리고 우리는

1532
01:02:53,220 --> 01:02:54,420
꽤 어려운 것을 사용하여 가장 큰 동기인 트랜스포머를 모델링합니다.

1533
01:02:54,420 --> 01:02:57,180
대답은 어가 아니기

1534
01:02:57,180 --> 01:02:59,460
때문입니다.  어텐션 메커니즘은

1535
01:02:59,460 --> 01:03:02,099
마지막에 소프트 맥스가 있고 소프트 맥스는

1536
01:03:02,099 --> 01:03:03,960


1537
01:03:03,960 --> 01:03:08,400
가우시안 분포가 아니라

1538
01:03:08,400 --> 01:03:11,280
소프트 맥스 분포로 전화합니다.

1539
01:03:11,280 --> 01:03:13,440
지금은 이름을 알 수 없지만 예,

1540
01:03:13,440 --> 01:03:16,079
어 그래서 예 그것은 일반화입니다.

1541
01:03:16,079 --> 01:03:19,140
약간입니다.  Gaston 가정을 제거한 후에 호출하기 까다롭습니다.

1542
01:03:19,140 --> 01:03:20,700


1543
01:03:20,700 --> 01:03:22,319


1544
01:03:22,319 --> 01:03:24,059
창의적인 코딩이라고 부르기에는 여전히 약간 까다롭기

1545
01:03:24,059 --> 01:03:26,400
때문에 그는

1546
01:03:26,400 --> 01:03:29,819
예를 들어 자동차 Freestone과 대화하는 것과 같습니다.

1547
01:03:29,819 --> 01:03:32,700
그가 창의적인 코딩을 좋아하는 것은

1548
01:03:32,700 --> 01:03:35,160
가우스와 가우시안만 있는 경우에만 가능합니다.

1549
01:03:35,160 --> 01:03:37,680
가정

1550
01:03:37,680 --> 01:03:39,720
이지만 네, 그것은 어

1551
01:03:39,720 --> 01:03:42,660


1552
01:03:42,660 --> 01:03:44,940
흥미롭기보다는 철학적 토론에 가깝습니다. 제 생각에

1553
01:03:44,940 --> 01:03:46,740
확실히 큰

1554
01:03:46,740 --> 01:03:49,500
관심을 끄는 또 다른 주제는 트랜스포머

1555
01:03:49,500 --> 01:03:52,980
의 주의 장치

1556
01:03:52,980 --> 01:03:56,099
와 주의가

1557
01:03:56,099 --> 01:03:58,440
신경 인지적

1558
01:03:58,440 --> 01:04:00,180
관점과 예측

1559
01:04:00,180 --> 01:04:03,240
처리 정밀도에서 설명되는 방식 사이의 유사점과 차이점입니다.  대기 각도에

1560
01:04:03,240 --> 01:04:06,200
대해 어떻게 생각하세요?

1561
01:04:06,359 --> 01:04:08,700


1562
01:04:08,700 --> 01:04:12,359
음, 제 생각에는

1563
01:04:12,359 --> 01:04:15,000
꽤 처리 및

1564
01:04:15,000 --> 01:04:16,400
운영 추론 관점에서

1565
01:04:16,400 --> 01:04:19,260
주의를 일종의

1566
01:04:19,260 --> 01:04:21,299
구조적 학습 문제로 볼 수 있다는 것입니다. 제

1567
01:04:21,299 --> 01:04:23,040
생각에는

1568
01:04:23,040 --> 01:04:25,680
Chris Buckley 그룹의 최근 논문은

1569
01:04:25,680 --> 01:04:26,339


1570
01:04:26,339 --> 01:04:28,079


1571
01:04:28,079 --> 01:04:30,420
기본적으로

1572
01:04:30,420 --> 01:04:31,859
어텐션 메커니즘이

1573
01:04:31,859 --> 01:04:35,819
단순히 다른 데이터 포인트에

1574
01:04:35,819 --> 01:04:38,880
특정한 가중치 매개변수에 대한 정밀도를 학습하는 것임을 보여 주는 아카이브에 재판이 있어야 한다는 것을 보여줍니다.

1575
01:04:38,880 --> 01:04:41,040


1576
01:04:41,040 --> 01:04:43,200


1577
01:04:43,200 --> 01:04:45,540
모델의 구조에 있는 매개변수가 아니므로

1578
01:04:45,540 --> 01:04:47,579
모델 특정 매개변수가 아닙니다.

1579
01:04:47,579 --> 01:04:49,140


1580
01:04:49,140 --> 01:04:51,660


1581
01:04:51,660 --> 01:04:53,760
자유 에너지의 변동을 최소화하면서 업데이트되는 값 노드

1582
01:04:53,760 --> 01:04:55,440
와 같이 빠르게 변화하는 매개변수이며 일단  최소화하고

1583
01:04:55,440 --> 01:04:57,000
계산한 다음 버리고

1584
01:04:57,000 --> 01:04:58,920
다음 데이터 포인트를 위해

1585
01:04:58,920 --> 01:05:00,780
처음부터 다시 계산해야 하므로

1586
01:05:00,780 --> 01:05:03,299
예, 유추 계산이 현명하다고 생각합니다.

1587
01:05:03,299 --> 01:05:05,819
어 어텐션

1588
01:05:05,819 --> 01:05:07,920
메커니즘은 일종의

1589
01:05:07,920 --> 01:05:10,559
구조 학습으로 볼 수 있지만  구조

1590
01:05:10,559 --> 01:05:13,020
학습은 데이터 포인트에 특정하고

1591
01:05:13,020 --> 01:05:15,119
모델에 특정하지 않습니다.

1592
01:05:15,119 --> 01:05:17,280
제 생각에 우리가 조금 일반화

1593
01:05:17,280 --> 01:05:18,960
하고 트랜스포머의

1594
01:05:18,960 --> 01:05:20,339
주의 메커니즘에서 주의

1595
01:05:20,339 --> 01:05:21,900


1596
01:05:21,900 --> 01:05:24,180
메커니즘 인지 과학으로 이동하고 싶다면 유사점

1597
01:05:24,180 --> 01:05:28,020


1598
01:05:28,020 --> 01:05:31,260
과  어

1599
01:05:31,260 --> 01:05:33,359
저는 구조적 학습 비유

1600
01:05:33,359 --> 01:05:36,660
와 하나의 연결이

1601
01:05:36,660 --> 01:05:38,760
다른 것과 관련하여 얼마나 중요한지 생각합니다 아마도

1602
01:05:38,760 --> 01:05:41,900
작업을 훨씬 더 잘 수행할 것입니다

1603
01:05:42,000 --> 01:05:44,880
멋진 회색 대답 오케이

1604
01:05:44,880 --> 01:05:49,200
ml Don은 반사실에서

1605
01:05:49,200 --> 01:05:51,240
숨겨진 변수

1606
01:05:51,240 --> 01:05:55,440
X와 관찰되지 않은 변수 U의 차이점이 무엇인지 묻습니다

1607
01:05:55,440 --> 01:05:59,180
차이점은

1608
01:05:59,540 --> 01:06:01,740
가장 중요한 것은

1609
01:06:01,740 --> 01:06:03,599


1610
01:06:03,599 --> 01:06:05,819


1611
01:06:05,819 --> 01:06:09,000
계산하고 고칠 수 있기 때문에 사용할 수 있는 용도를 관찰할 수 없다는 것입니다.  예를

1612
01:06:09,000 --> 01:06:10,559


1613
01:06:10,559 --> 01:06:13,380


1614
01:06:13,380 --> 01:06:16,020
들어

1615
01:06:16,020 --> 01:06:18,540


1616
01:06:18,540 --> 01:06:21,240


1617
01:06:21,240 --> 01:06:23,280
시간을 거슬러 올라가면

1618
01:06:23,280 --> 01:06:25,079
환경이 다르기 때문에 아이디어는 예를 들어

1619
01:06:25,079 --> 01:06:26,520


1620
01:06:26,520 --> 01:06:28,440


1621
01:06:28,440 --> 01:06:29,880
이전의 예로 돌아가는 것을 좋아하는 경우입니다.

1622
01:06:29,880 --> 01:06:31,920


1623
01:06:31,920 --> 01:06:34,619
특정 지능을 가진 사람의 예상 교육 uh

1624
01:06:34,619 --> 01:06:37,440
uh 교육 학위

1625
01:06:37,440 --> 01:06:40,200
아이디어는 내가

1626
01:06:40,200 --> 01:06:43,559
오늘 얼마나 많이 배울 것인지보고 싶다면 uh와

1627
01:06:43,559 --> 01:06:45,359
함께 나는 석사 학위와

1628
01:06:45,359 --> 01:06:47,339
다른 점을 존중합니다.  20년 전에 내가 석사 학위로 얼마를

1629
01:06:47,339 --> 01:06:48,359
벌 수 있었는지에 대해서는

1630
01:06:48,359 --> 01:06:50,819


1631
01:06:50,819 --> 01:06:52,619
예를 들어 여기 이탈리아에서

1632
01:06:52,619 --> 01:06:55,440
다른 국가와 관련하여 다르며

1633
01:06:55,440 --> 01:06:57,000


1634
01:06:57,000 --> 01:06:58,859
통제할 수 없는 모든 변수는 비전 네트워크를 사용하여 모델링할 수 없지만

1635
01:06:58,859 --> 01:07:00,359


1636
01:07:00,359 --> 01:07:03,480
괜찮습니다.

1637
01:07:03,480 --> 01:07:05,220


1638
01:07:05,220 --> 01:07:07,559
결론을 내리고 싶을 때 무시할 수 없습니다. 그래서 그는

1639
01:07:07,559 --> 01:07:08,760
기본적으로 당신이

1640
01:07:08,760 --> 01:07:10,079
통제할 수 없는 모든 것입니다.

1641
01:07:10,079 --> 01:07:13,079
당신은 그것들을 추론할 수 있습니다. 그래서 당신은

1642
01:07:13,079 --> 01:07:14,819


1643
01:07:14,819 --> 01:07:16,740
시간을 거슬러 반사실적 추론을 수행할 수 있고 오 20

1644
01:07:16,740 --> 01:07:19,020
년 전에 나는 이것을 얻었을 것이라고 말할 수 있습니다.

1645
01:07:19,020 --> 01:07:20,640


1646
01:07:20,640 --> 01:07:22,559
내가 이렇게 지능적이라면 물론

1647
01:07:22,559 --> 01:07:24,599
평균적으로 이 정도이지만

1648
01:07:24,599 --> 01:07:27,059


1649
01:07:27,059 --> 01:07:30,720
직업이나 그와 같은 것에 대한 정부 정책을 변경할 수 있는 것은 아닙니다.

1650
01:07:30,720 --> 01:07:32,819


1651
01:07:32,819 --> 01:07:35,099


1652
01:07:35,099 --> 01:07:38,400


1653
01:07:38,400 --> 01:07:40,200


1654
01:07:40,200 --> 01:07:42,480
당신은

1655
01:07:42,480 --> 01:07:45,660
예측 코딩에서 일반화 좌표를 구현했습니다

1656
01:07:45,660 --> 01:07:46,920
아니요

1657
01:07:46,920 --> 01:07:50,039
아니요 아니요 한 번도 해본 적이 없습니다 나는 어 예

1658
01:07:50,039 --> 01:07:52,680
연구했지만

1659
01:07:52,680 --> 01:07:55,260
구현 한 적이 없습니다 불안정한 경향이 있다는 것을 알고

1660
01:07:55,260 --> 01:07:57,599
어

1661
01:07:57,599 --> 01:08:00,299
그리고 그것은  그것들을 안정적으로 만들기가 매우 어렵습니다. 그것이

1662
01:08:00,299 --> 01:08:02,940


1663
01:08:02,940 --> 01:08:05,460
제가 그것을 구현 한 사람들과 이야기하면서 얻은 요점이라고 생각합니다.

1664
01:08:05,460 --> 01:08:08,359


1665
01:08:08,400 --> 01:08:11,039
하지만 예, 영국 부하에서 테스트 한 그들에 대해 실제로 최근에 나온 몇 가지 논문을 알고 있습니다.

1666
01:08:11,039 --> 01:08:12,839


1667
01:08:12,839 --> 01:08:15,599


1668
01:08:15,599 --> 01:08:18,000
인코더 스타일은 사실

1669
01:08:18,000 --> 01:08:20,520
Baron에서

1670
01:08:20,520 --> 01:08:22,979


1671
01:08:22,979 --> 01:08:25,439
지난 여름에 나온 종이가 있지만 직접 가지고 놀아본 적은 없습니다.

1672
01:08:25,439 --> 01:08:26,580


1673
01:08:26,580 --> 01:08:29,160
멋진 장난꾸러기는

1674
01:08:29,160 --> 01:08:32,040
계층 구조에 더 많은 레벨을 추가합니다.

1675
01:08:32,040 --> 01:08:35,160


1676
01:08:35,160 --> 01:08:38,238
입력 추가를 예측하는 산만함 문제를 줄입니다.

1677
01:08:38,939 --> 01:08:41,698


1678
01:08:41,698 --> 01:08:43,439
어떤 의미에서 파괴

1679
01:08:43,439 --> 01:08:45,779
문제는 Cycles에 의해 주어지기 때문에 기본적으로

1680
01:08:45,779 --> 01:08:47,399
이미지를 제공

1681
01:08:47,399 --> 01:08:49,920
하고

1682
01:08:49,920 --> 01:08:53,279
이미지

1683
01:08:53,279 --> 01:08:55,799
에서 패치가 뉴런으로 들어가고 다른 가장자리가

1684
01:08:55,799 --> 01:08:57,500
다시 돌아간다는 사실을 제공한다는 사실은

1685
01:08:57,500 --> 01:08:59,939
기본적으로 다음과 같은 사실을 만듭니다.

1686
01:08:59,939 --> 01:09:03,560
기본적으로

1687
01:09:03,560 --> 01:09:06,179
이러한 인고잉이 이미지의 픽셀에 맞게 조정되어

1688
01:09:06,179 --> 01:09:08,339
예측 오류가 발생하여

1689
01:09:08,339 --> 01:09:09,719


1690
01:09:09,719 --> 01:09:12,140
모델 내부에 퍼지는 예측 오류가 발생한다는 오류가 있습니다.

1691
01:09:12,140 --> 01:09:14,640
예, 이 문제는

1692
01:09:14,640 --> 01:09:16,979
주기의 일반적인 문제라고 생각합니다.

1693
01:09:16,979 --> 01:09:21,439


1694
01:09:23,060 --> 01:09:25,140


1695
01:09:25,140 --> 01:09:26,759
들어오는 가장자리가 없으면

1696
01:09:26,759 --> 01:09:27,660


1697
01:09:27,660 --> 01:09:30,540
더 이상 멋진 파괴 문제가 없으며

1698
01:09:30,540 --> 01:09:33,238


1699
01:09:33,238 --> 01:09:35,939
트레이스 연산자를 통한 비순환 네트워크의 사양은

1700
01:09:35,939 --> 01:09:37,859


1701
01:09:37,859 --> 01:09:41,819
매우 흥미로운 기술이며

1702
01:09:41,819 --> 01:09:46,339
언제 가져왔습니까?

1703
01:09:46,560 --> 01:09:49,140
내가 아는 한 그가 내가

1704
01:09:49,140 --> 01:09:52,380
2018년에 내가 인용한 논문을 가지고 나온 것 같아요

1705
01:09:52,380 --> 01:09:54,360
나는 적어도 인과 추론 문헌에서는 모릅니다

1706
01:09:54,360 --> 01:09:56,940
나는 이전의 방법을 알지 못합니다

1707
01:09:56,940 --> 01:09:59,699


1708
01:09:59,699 --> 01:10:01,860
왜냐하면 그것이  내 말은 그것이 인용 빈도가 높은 논문

1709
01:10:01,860 --> 01:10:04,140
이므로 그들이 그 아이디어를 가지고 나왔다고 말할 것입니다

1710
01:10:04,140 --> 01:10:05,520


1711
01:10:05,520 --> 01:10:07,980
와우 그래

1712
01:10:07,980 --> 01:10:09,480
그래디언트 디센트를 할 수 있고 구조를 배울 수 있다는 것은 꽤 좋은 일입니다.

1713
01:10:09,480 --> 01:10:11,400


1714
01:10:11,400 --> 01:10:14,219
그것은 매우 강력한 기술이라고 생각합니다. 그래

1715
01:10:14,219 --> 01:10:15,840
때로는 당신이 볼 때와 같습니다

1716
01:10:15,840 --> 01:10:17,640


1717
01:10:17,640 --> 01:10:19,440
베이지안 추론과 인과

1718
01:10:19,440 --> 01:10:23,159
적 추론의 서로 다른 음 기능을 사용할 수 있게 되었을 때

1719
01:10:23,159 --> 01:10:25,620
왜

1720
01:10:25,620 --> 01:10:28,500
이것이 베이지안 인과

1721
01:10:28,500 --> 01:10:30,719
적 모델링 프레임워크에서 수행되지 않았는지와 같이 정말 놀랍습니다.

1722
01:10:30,719 --> 01:10:32,760


1723
01:10:32,760 --> 01:10:36,659
이런 일이 발생한 지 5년에서 25년 정도 밖에 되지 않았기

1724
01:10:36,659 --> 01:10:39,960
때문에 매우 짧습니다.  또한

1725
01:10:39,960 --> 01:10:42,060
상대적으로 기술적인 분야이기 때문에 관련

1726
01:10:42,060 --> 01:10:43,920
연구 그룹이 상대적으로 적고 음,

1727
01:10:43,920 --> 01:10:46,920


1728
01:10:46,920 --> 01:10:49,860
그것이 가능하게 하는 것이 정말 멋집니다

1729
01:10:49,860 --> 01:10:51,960
아니오 예 예 정확히 제 말은 또한

1730
01:10:51,960 --> 01:10:54,179
이 분야의 흥미로운 부분이라고 생각합니다.

1731
01:10:54,179 --> 01:10:56,040


1732
01:10:56,040 --> 01:10:59,100


1733
01:10:59,100 --> 01:11:01,020
여전히 발견되어야 하는 돌파구가 있고 아마

1734
01:11:01,020 --> 01:11:03,000
좋아할 것

1735
01:11:03,000 --> 01:11:05,300


1736
01:11:05,300 --> 01:11:07,800


1737
01:11:07,800 --> 01:11:09,960


1738
01:11:09,960 --> 01:11:12,120


1739
01:11:12,120 --> 01:11:14,040
입니다.

1740
01:11:14,040 --> 01:11:17,100
정확하지만

1741
01:11:17,100 --> 01:11:19,080
그것은 당신이 어느 날 오후에 가지고 있는 아이디어일 수 있습니다.

1742
01:11:19,080 --> 01:11:21,120


1743
01:11:21,120 --> 01:11:23,040
다른 사람들이 어떻게 그것을 생각해 냈는지에 대한 이야기는 모르지만

1744
01:11:23,040 --> 01:11:25,320
잠재적으로

1745
01:11:25,320 --> 01:11:27,239
그들이 화이트보드에 있다는 것일 수 있습니다.

1746
01:11:27,239 --> 01:11:29,280


1747
01:11:29,280 --> 01:11:32,159
엄청난 돌파구인 작품들 그리고 저는 단순히

1748
01:11:32,159 --> 01:11:33,960
이전

1749
01:11:33,960 --> 01:11:36,739
과 또한 이러한 많은 돌파구를 정의했습니다.

1750
01:11:36,739 --> 01:11:40,500
그들은 그냥 쌓는 것이 아니라 어

1751
01:11:40,500 --> 01:11:44,280
어 그들이 쌓고 구성하는 블록의 탑과 같지 않습니다.

1752
01:11:44,280 --> 01:11:47,640
그러면 뭔가

1753
01:11:47,640 --> 01:11:50,159
일반화될 것입니다 음  일반화된

1754
01:11:50,159 --> 01:11:52,140
좌표 또는 일반화 동기 또는

1755
01:11:52,140 --> 01:11:55,020
임의로 큰 그래프 또는

1756
01:11:55,020 --> 01:11:57,239
음 다중 모드 입력이 있는 센서 퓨전

1757
01:11:57,239 --> 01:12:00,679
그리고 그것들이 모두 정말

1758
01:12:00,679 --> 01:12:03,659
만족스럽고 효과적인 방식으로 혼합되는 것과 같습니다. 그래서

1759
01:12:03,659 --> 01:12:05,640
다시 한 번 누군가가 생각해낼 수 있는 작은 것들도

1760
01:12:05,640 --> 01:12:08,100


1761
01:12:08,100 --> 01:12:11,100
음 정말 영향을 미칠 수 있습니다

1762
01:12:11,100 --> 01:12:14,159
음  좋아 ml Dawn은 내 질문에 대해 많은 감사를 표

1763
01:12:14,159 --> 01:12:16,199
하고

1764
01:12:16,199 --> 01:12:18,060
영감을 주는 프레젠테이션에 대해 Tomaso에게 백만

1765
01:12:18,060 --> 01:12:21,360
가지 감사를 표합니다. 너무 좋아요 오 정말 감사합니다. 그런 다음

1766
01:12:21,360 --> 01:12:23,280
Bert는 예측 코딩을

1767
01:12:23,280 --> 01:12:25,560
사용하는 언어 모델이

1768
01:12:25,560 --> 01:12:27,179


1769
01:12:27,179 --> 01:12:30,260
Transformers를 사용하는 언어 모델과 어떻게 다른지 묻습니다.

1770
01:12:31,679 --> 01:12:32,520
음

1771
01:12:32,520 --> 01:12:35,340
알겠습니다.

1772
01:12:35,340 --> 01:12:36,659
오늘날

1773
01:12:36,659 --> 01:12:38,640
예측 코딩을 사용하여 언어 모델을 구축해야 한다면 여전히 트랜스포머를 사용할 것

1774
01:12:38,640 --> 01:12:40,020


1775
01:12:40,020 --> 01:12:41,880
입니다.

1776
01:12:41,880 --> 01:12:42,780


1777
01:12:42,780 --> 01:12:45,659


1778
01:12:45,659 --> 01:12:48,440


1779
01:12:48,440 --> 01:12:50,460


1780
01:12:50,460 --> 01:12:53,100
첫 번째

1781
01:12:53,100 --> 01:12:55,380
슬라이드는 화살표 하나를 슬라이드하여 선형 맵인 함수를 인코딩하므로

1782
01:12:55,380 --> 01:12:57,300


1783
01:12:57,300 --> 01:12:59,219
1시간은

1784
01:12:59,219 --> 01:13:01,080


1785
01:13:01,080 --> 01:13:03,060
잠재 변수에 인코딩된 벡터의 a와 비선형으로 만들

1786
01:13:03,060 --> 01:13:06,300
수 있는 이 가중치 매트릭스를 곱한 것입니다.

1787
01:13:06,300 --> 01:13:08,580
그러나

1788
01:13:08,580 --> 01:13:09,960
그것은 실제로 훨씬 더

1789
01:13:09,960 --> 01:13:12,179
복잡한 것일 수 있습니다. 화살표에 포함된 함수는

1790
01:13:12,179 --> 01:13:14,880
컨볼루션일 수 있고

1791
01:13:14,880 --> 01:13:16,800
어텐션 메커니즘일 수 있습니다.

1792
01:13:16,800 --> 01:13:20,820
그래서 실제로 제가 어떻게 할 것인지 저는

1793
01:13:20,820 --> 01:13:23,880
여전히 우리가 실제로 했던 방식인 I mean을 사용할 것입니다.

1794
01:13:23,880 --> 01:13:26,460


1795
01:13:26,460 --> 01:13:28,860
작년 옥스포드 그룹에서 우리는

1796
01:13:28,860 --> 01:13:30,900
모든 화살표가 트랜스포머라는 구조를 정확히 가지고 있었

1797
01:13:30,900 --> 01:13:33,420
으므로 하나는 어텐션

1798
01:13:33,420 --> 01:13:35,159
메커니즘이고 다음은

1799
01:13:35,159 --> 01:13:38,219
트랜스포머로서의 피드 포워드 네트워크이며

1800
01:13:38,219 --> 01:13:40,020
기본적으로 유일한 차이점은

1801
01:13:40,020 --> 01:13:41,640


1802
01:13:41,640 --> 01:13:43,739
사후를 계산하려는 변수와

1803
01:13:43,739 --> 01:13:45,239


1804
01:13:45,239 --> 01:13:47,400
VIA 평균 필드 근사를 통해 사후 독립을 독립적으로 만듭니다.

1805
01:13:47,400 --> 01:13:49,560
따라서 기본적으로

1806
01:13:49,560 --> 01:13:51,659


1807
01:13:51,659 --> 01:13:53,520


1808
01:13:53,520 --> 01:13:56,520
창의적인 코딩의 변형이 없는 에너지로 수렴할 수 있는 모든 단계를 따르지만

1809
01:13:56,520 --> 01:13:58,199
방법은 다음과 같습니다.  예측을 계산

1810
01:13:58,199 --> 01:14:01,199
하고 신호를 되돌려 보내는 방법은

1811
01:14:01,199 --> 01:14:04,739
Transformer를 통해 이루어지므로

1812
01:14:04,739 --> 01:14:07,560


1813
01:14:07,560 --> 01:14:10,679
일반적으로 Transformer를 계속 사용할 것입니다. 트랜스포머가 너무 잘 작동하기 때문에

1814
01:14:10,679 --> 01:14:12,840
우리가 오만하게 말할 수 없다고 생각합니다.

1815
01:14:12,840 --> 01:14:15,060


1816
01:14:15,060 --> 01:14:17,640
순전히 예측 코딩 방식

1817
01:14:17,640 --> 01:14:18,920
구조를 통해 더 좋지만 어쨌든

1818
01:14:18,920 --> 01:14:21,480
여전히 트랜스포머에 근접할 것입니다.

1819
01:14:21,480 --> 01:14:22,500


1820
01:14:22,500 --> 01:14:24,420
죄송합니다 구조 학습이

1821
01:14:24,420 --> 01:14:27,540
트랜스포머 접근 방식에 근접한다고 말했습니다. 예,

1822
01:14:27,540 --> 01:14:29,219


1823
01:14:29,219 --> 01:14:32,640
앞서 언급한 구조 학습은 어, 누군가 uh가

1824
01:14:32,640 --> 01:14:34,800
창의적 코딩

1825
01:14:34,800 --> 01:14:38,060
과 어텐션 메커니즘 사이의 유사점을 물을 때

1826
01:14:38,280 --> 01:14:41,699
매우 그렇습니다.  흥미로운

1827
01:14:41,699 --> 01:14:42,900
음

1828
01:14:42,900 --> 01:14:45,719
Amazon에서 궁금한 점은

1829
01:14:45,719 --> 01:14:47,640


1830
01:14:47,640 --> 01:14:49,380
당신이 언급한 예측 코딩 네트워크에서 깊이의 개념을 볼 수 없었을

1831
01:14:49,380 --> 01:14:50,880
가능성이 가장 높습니다.

1832
01:14:50,880 --> 01:14:52,380
예측 코딩에 제공된 정의는

1833
01:14:52,380 --> 01:14:56,480
깊이의 개념과 관련이 있습니다.

1834
01:14:56,640 --> 01:14:59,460
깊이가 무엇을 의미합니까

1835
01:14:59,460 --> 01:15:02,219
아니오 예 사실입니다

1836
01:15:02,219 --> 01:15:04,980
제가 여러 번 말했듯이 표준 정의는

1837
01:15:04,980 --> 01:15:06,960
계층적입니다.

1838
01:15:06,960 --> 01:15:08,400
예측이 한 방향으로 진행됩니다.

1839
01:15:08,400 --> 01:15:09,719
반대 방향으로 진행되는 예측 오류가 있습니다.

1840
01:15:09,719 --> 01:15:10,620


1841
01:15:10,620 --> 01:15:14,340


1842
01:15:14,340 --> 01:15:16,320


1843
01:15:16,320 --> 01:15:18,420


1844
01:15:18,420 --> 01:15:19,920


1845
01:15:19,920 --> 01:15:22,260
상대적인 코딩이 있는 임의의 그래프 토폴로지에 대한 학습은 깊이를 독립적으로 고려할 수 있다는 것입니다.

1846
01:15:22,260 --> 01:15:25,620
어

1847
01:15:25,620 --> 01:15:28,380


1848
01:15:28,380 --> 01:15:31,380
기본적으로 잠재 변수 잠재

1849
01:15:31,380 --> 01:15:33,239
변수와 화살표의 쌍이며

1850
01:15:33,239 --> 01:15:34,739
예측은 해당

1851
01:15:34,739 --> 01:15:36,300
방향으로 가고 예측 화살표는 다른 방향으로 가지만 그런 다음

1852
01:15:36,300 --> 01:15:38,340


1853
01:15:38,340 --> 01:15:41,880
몇 가지 방법으로 이것들을 구성할 수 있으므로

1854
01:15:41,880 --> 01:15:45,239
기본적으로 이

1855
01:15:45,239 --> 01:15:47,040
구성은

1856
01:15:47,040 --> 01:15:48,659
결국 계층적일 필요가 없습니다.

1857
01:15:48,659 --> 01:15:50,820
사이클을 가질 수 있으므로

1858
01:15:50,820 --> 01:15:53,520
예를 들어 다른

1859
01:15:53,520 --> 01:15:55,440
잠재 변수를 첫 번째 변수에 연결할 수 있습니다.  하나를

1860
01:15:55,440 --> 01:15:57,540
연결하고 다른 것을 연결하면 원하는

1861
01:15:57,540 --> 01:15:59,340
만큼 얽힌 구조를 가질 수 있습니다.

1862
01:15:59,340 --> 01:16:00,420


1863
01:16:00,420 --> 01:16:02,699
예를 들어 다른 논문에서

1864
01:16:02,699 --> 01:16:04,500
우리는 뇌 구조

1865
01:16:04,500 --> 01:16:06,659
의 모양을 가진 네트워크를 훈련하므로

1866
01:16:06,659 --> 01:16:08,460
우리는 많은 것을 가지고 있습니다.

1867
01:16:08,460 --> 01:16:09,900


1868
01:16:09,900 --> 01:16:12,239
내부에 희박하게 연결되어 있고

1869
01:16:12,239 --> 01:16:13,860
서로 부분적으로 연결되어 있고

1870
01:16:13,860 --> 01:16:15,719


1871
01:16:15,719 --> 01:16:17,640
마지막에는 계층 구조가 없지만

1872
01:16:17,640 --> 01:16:18,960


1873
01:16:18,960 --> 01:16:20,699
작동 자유 에너지를 최소화하고 네트워크의

1874
01:16:20,699 --> 01:16:22,620
전체 예측 오류를 최소화하여 여전히 훈련할 수

1875
01:16:22,620 --> 01:16:25,159


1876
01:16:25,159 --> 01:16:27,360
있습니다.

1877
01:16:27,360 --> 01:16:31,980
얽힌 그래프에서 주어진 모티프에 대해

1878
01:16:31,980 --> 01:16:35,159
3개의 연속적인 레이어를 볼 수 있습니다. 이 레이어

1879
01:16:35,159 --> 01:16:37,560
만 보면

1880
01:16:37,560 --> 01:16:38,820


1881
01:16:38,820 --> 01:16:41,940


1882
01:16:41,940 --> 01:16:43,980
적응형 3이라고 하는 3개 레이어 모델인 3층 건물입니다.

1883
01:16:43,980 --> 01:16:46,620


1884
01:16:46,620 --> 01:16:50,280


1885
01:16:50,280 --> 01:16:52,140
해당 네트워크에 대한 명시적 상단 또는 명시적 하단과 같지 않습니다.

1886
01:16:52,140 --> 01:16:54,360
예 정확히 그리고 이것은 기본적으로

1887
01:16:54,360 --> 01:16:55,980


1888
01:16:55,980 --> 01:16:58,080
예측 한국 네트워크의 모든 작업이

1889
01:16:58,080 --> 01:16:59,460
엄밀히 로컬이라는 사실에 의해 제공되므로

1890
01:16:59,460 --> 01:17:01,739
기본적으로 모든 메시지는

1891
01:17:01,739 --> 01:17:03,000
모든 예측 및 모든 예측

1892
01:17:03,000 --> 01:17:05,280
오류를 전달합니다.

1893
01:17:05,280 --> 01:17:08,280
아주 가까운 뉴런으로만 보내는 것이 좋습니다.

1894
01:17:08,280 --> 01:17:10,380
전역 구조가 실제로

1895
01:17:10,380 --> 01:17:13,380
계층적인지 여부는

1896
01:17:13,380 --> 01:17:16,940
전달되는 단일 메시지가 보이지 않는다는 것입니다.

1897
01:17:17,460 --> 01:17:19,620


1898
01:17:19,620 --> 01:17:22,820
새로운 모델 아키텍처를 학습하기 위한 희망의

1899
01:17:22,820 --> 01:17:27,739
공간은 다음과 같습니다.

1900
01:17:27,739 --> 01:17:33,300
위에서 아래로 설계된 것은 매우 작고

1901
01:17:33,300 --> 01:17:36,480
오늘날 사용되는 많은 모델이지만 매우

1902
01:17:36,480 --> 01:17:38,640
효과적인 모델입니다

1903
01:17:38,640 --> 01:17:41,100
음 계산 단위당 효율성을 물어볼 수는

1904
01:17:41,100 --> 01:17:43,320
있지만 두 번째

1905
01:17:43,320 --> 01:17:45,300
수준의 질문이지만 오늘날 많은 효과적인

1906
01:17:45,300 --> 01:17:47,580
모델에는 이 중 일부가 없습니다.

1907
01:17:47,580 --> 01:17:49,860


1908
01:17:49,860 --> 01:17:52,739


1909
01:17:52,739 --> 01:17:55,520


1910
01:17:55,520 --> 01:17:59,400
생물학적 사실주의를 제공하는 로컬 계산만 사용하는 능력과 같은 예측 코딩 네트워크의 속성

1911
01:17:59,400 --> 01:18:02,880
음 또는 시공간적 사실주의뿐만 아니라

1912
01:18:02,880 --> 01:18:06,060


1913
01:18:06,060 --> 01:18:08,159
연합 컴퓨팅 또는 분산

1914
01:18:08,159 --> 01:18:10,500
컴퓨팅 설정과 같은 많은 이점을 제공할 수 있습니다.

1915
01:18:10,500 --> 01:18:12,780
아니요 예 정확히 동의합니다.

1916
01:18:12,780 --> 01:18:14,520
일반적으로 생각하는 아이디어는

1917
01:18:14,520 --> 01:18:16,679
그것이

1918
01:18:16,679 --> 01:18:18,540
이점이 될지 모르겠습니다. 그래서 말씀하신

1919
01:18:18,540 --> 01:18:20,159
이유 때문에 매우 유망하다고 생각합니다. 그

1920
01:18:20,159 --> 01:18:20,880


1921
01:18:20,880 --> 01:18:22,920
이유는

1922
01:18:22,920 --> 01:18:25,380
역전파가 있는 오늘날의 모델 문자열을

1923
01:18:25,380 --> 01:18:28,860
기본적으로 요약할 수 있기 때문입니다

1924
01:18:28,860 --> 01:18:32,040
역전파는

1925
01:18:32,040 --> 01:18:34,080
기본적으로

1926
01:18:34,080 --> 01:18:36,120
입력에서 출력으로의 맵이 있고 역전

1927
01:18:36,120 --> 01:18:39,600
파는 기본적으로 계산 그래프에서 어 정보를 다시 확산하므로

1928
01:18:39,600 --> 01:18:41,699


1929
01:18:41,699 --> 01:18:44,340


1930
01:18:44,340 --> 01:18:45,960
오늘날 사용되는 모든 신경망 신경망 모델은

1931
01:18:45,960 --> 01:18:48,960
예측 코딩 동안 함수입니다.

1932
01:18:48,960 --> 01:18:51,179
그리고 이전 함수 클래스와 같은 또 다른 자유 코딩은

1933
01:18:51,179 --> 01:18:53,820


1934
01:18:53,820 --> 01:18:56,040
로컬 계산을 사용하여 훈련

1935
01:18:56,040 --> 01:18:58,500
하고 실제로

1936
01:18:58,500 --> 01:19:01,620
글로벌 에너지 함수를 최소화하여 작동하는 메서드 클래스입니다.

1937
01:19:01,620 --> 01:19:03,840


1938
01:19:03,840 --> 01:19:05,940
입력에서 출력까지 모델 함수에 국한되지 않고 실제로

1939
01:19:05,940 --> 01:19:07,739
무언가를 모델링합니다.  종류는

1940
01:19:07,739 --> 01:19:10,080
물리적 시스템과 유사하므로 물리적

1941
01:19:10,080 --> 01:19:13,500
시스템이 있습니다. 어떤 입력에든 일부 값을 고정하고

1942
01:19:13,500 --> 01:19:15,360


1943
01:19:15,360 --> 01:19:17,280
시스템이 수렴하도록 한 다음 출력되어야 하는

1944
01:19:17,280 --> 01:19:19,980
뉴런이나 변수의 다른 값을 읽지만 이

1945
01:19:19,980 --> 01:19:21,960


1946
01:19:21,960 --> 01:19:24,120
물리적 시스템은 그렇지 않습니다.  맞춤

1947
01:19:24,120 --> 01:19:25,920
포워드 맵일 필요는 없습니다.

1948
01:19:25,920 --> 01:19:28,260
입력 공간과 출력 공간이 있는 함수일 필요는 없습니다.

1949
01:19:28,260 --> 01:19:30,659


1950
01:19:30,659 --> 01:19:32,580


1951
01:19:32,580 --> 01:19:34,800
학습할 수 있는 모델의 클래스는 어 그래서 기본적으로

1952
01:19:34,800 --> 01:19:37,560
피드포워드 모델처럼 볼 수 있습니다.  그리고 기능

1953
01:19:37,560 --> 01:19:39,600
그리고 물리적 시스템의 훨씬 더 큰 클래스

1954
01:19:39,600 --> 01:19:41,880


1955
01:19:41,880 --> 01:19:43,860
여기에 흥미로운 것이 있는지 여부

1956
01:19:43,860 --> 01:19:45,659
기능이 매우 잘 작동하기 때문에 아직 모르겠습니다.

1957
01:19:45,659 --> 01:19:47,460


1958
01:19:47,460 --> 01:19:50,040


1959
01:19:50,040 --> 01:19:52,199


1960
01:19:52,199 --> 01:19:53,460


1961
01:19:53,460 --> 01:19:56,040
큰 부분에 흥미로운 것이 있는지는 모르겠지만 큰

1962
01:19:56,040 --> 01:19:58,380
부분은 꽤 크다는 것입니다.

1963
01:19:58,380 --> 01:20:00,480


1964
01:20:00,480 --> 01:20:02,940
전파를 다시 가져올 수 없는 많은 모델이 있으며

1965
01:20:02,940 --> 01:20:04,679
창의적인 코딩

1966
01:20:04,679 --> 01:20:06,659
이나 화장실 전파 또는 기타

1967
01:20:06,659 --> 01:20:07,860
방법으로 훈련할 수 있습니다.

1968
01:20:07,860 --> 01:20:10,440
정말 흥미롭네요 확실히

1969
01:20:10,440 --> 01:20:12,719
생물학적 시스템은

1970
01:20:12,719 --> 01:20:15,900
모든 종류의 흥미로운 문제를 해결합니다

1971
01:20:15,900 --> 01:20:17,100


1972
01:20:17,100 --> 01:20:19,380


1973
01:20:19,380 --> 01:20:21,540


1974
01:20:21,540 --> 01:20:23,100


1975
01:20:23,100 --> 01:20:25,679


1976
01:20:25,679 --> 01:20:28,260


1977
01:20:28,260 --> 01:20:31,820


1978
01:20:31,820 --> 01:20:35,880


1979
01:20:35,880 --> 01:20:38,460
함수로 잘 설명되지 않은 정말 독특한 특수 알고리즘이 있을 수 있지만

1980
01:20:38,460 --> 01:20:42,060
여전히 매우 매우 효과적일 수 있는 휴리스틱을 구현하는 절차적 방법을 제공합니다.

1981
01:20:42,060 --> 01:20:46,679


1982
01:20:46,679 --> 01:20:48,840


1983
01:20:48,840 --> 01:20:51,120


1984
01:20:51,120 --> 01:20:53,880


1985
01:20:53,880 --> 01:20:55,679


1986
01:20:55,679 --> 01:20:58,260


1987
01:20:58,260 --> 01:20:59,100
예를 들어, 기능 내부가 아닌 외부에 있는

1988
01:20:59,100 --> 01:21:01,380
이 응용 프로그램을 찾는 것과 같은 박사 과정 연구에서

1989
01:21:01,380 --> 01:21:04,199


1990
01:21:04,199 --> 01:21:06,739


1991
01:21:07,199 --> 01:21:08,820


1992
01:21:08,820 --> 01:21:12,120
이 작업이 여기에서 어디로 가는지,

1993
01:21:12,120 --> 01:21:14,520
어떤 방향에 대해 흥분하는지,

1994
01:21:14,520 --> 01:21:17,340
활성 추론 생태계에서 사람들이 어떻게

1995
01:21:17,340 --> 01:21:19,679
얻고 있는지 등

1996
01:21:19,679 --> 01:21:22,460
이러한 유형의 작업에 참여하는 것이

1997
01:21:22,500 --> 01:21:24,840
아마도 가장 유망한 방향이라고 생각합니다.

1998
01:21:24,840 --> 01:21:27,780


1999
01:21:27,780 --> 01:21:30,060
어 그녀는 제가

2000
01:21:30,060 --> 01:21:33,060
조금 탐구하고 싶은 것은 제가 말했듯이

2001
01:21:33,060 --> 01:21:34,980
정적 모델을 뒷받침하는 것이 정말 있기

2002
01:21:34,980 --> 01:21:37,380
때문에 제가 보여드린 모든 것입니다.

2003
01:21:37,380 --> 01:21:40,260
지금까지 보여준 것은 정적 데이터에 관한 것이므로

2004
01:21:40,260 --> 01:21:42,840
시간이 지남에 따라 데이터가 변경되지 않습니다.

2005
01:21:42,840 --> 01:21:45,780


2006
01:21:45,780 --> 01:21:48,000


2007
01:21:48,000 --> 01:21:49,080
여기서 제시한 것처럼 크리에이티브 코딩의 정의 내부에는 시간이 없지만

2008
01:21:49,080 --> 01:21:50,940
예를 들어

2009
01:21:50,940 --> 01:21:53,280
크리에이티브 코딩이 작동하도록 일반화할 수 있습니다.  앞서 언급한 것처럼

2010
01:21:53,280 --> 01:21:55,800
일반화된 좌표를 사용하는 시간적 데이터를

2011
01:21:55,800 --> 01:21:58,800


2012
01:21:58,800 --> 01:22:01,380
일반적인 Kalman

2013
01:22:01,380 --> 01:22:04,140
필터 생성 모델로 제시함으로써 예를 들어 인과

2014
01:22:04,140 --> 01:22:08,040


2015
01:22:08,040 --> 01:22:09,900
추론 방향이 매우 유용할 수 있습니다.

2016
01:22:09,900 --> 01:22:12,600


2017
01:22:12,600 --> 01:22:14,400


2018
01:22:14,400 --> 01:22:17,880
더 큰 인과 관계를 모델링할 수 있고 어 그리고 더

2019
01:22:17,880 --> 01:22:21,780
복잡하고 유용합니다. 어

2020
01:22:21,780 --> 01:22:24,780
모델의 동적 원인은 기본적으로

2021
01:22:24,780 --> 01:22:26,940
일반적으로 미적분학

2022
01:22:26,940 --> 01:22:28,560
및 중간 및

2023
01:22:28,560 --> 01:22:32,760
반사실적 어 학문 분야는

2024
01:22:32,760 --> 01:22:36,000
대부분 작은 모델에서 개발되기

2025
01:22:36,000 --> 01:22:38,159
때문에

2026
01:22:38,159 --> 01:22:40,739
여러분이 모르는 것과 같습니다.  t 일반적으로 거대한 모델에 대한 개입을 수행하므로

2027
01:22:40,739 --> 01:22:43,560


2028
01:22:43,560 --> 01:22:45,800
의료 데이터를 보면

2029
01:22:45,800 --> 01:22:50,159
상대적으로 작은 비전 네트워크를 사용하지만 물론

2030
01:22:50,159 --> 01:22:51,179


2031
01:22:51,179 --> 01:22:54,900


2032
01:22:54,900 --> 01:22:56,340
특정 환경이나 특정

2033
01:22:56,340 --> 01:22:58,620
현실을 ​​모델링하는 동적 인과 모델을 원하면  내부의 많은 뉴런에는

2034
01:22:58,620 --> 01:23:00,780


2035
01:23:00,780 --> 01:23:02,580
시간이 지남에 따라 변하는 많은 잠재 변수가 있으며

2036
01:23:02,580 --> 01:23:05,219
어떤 순간에 더 많은 개입이

2037
01:23:05,219 --> 01:23:07,560
다른 시간 단계에서 효과를 생성하므로

2038
01:23:07,560 --> 01:23:09,239


2039
01:23:09,239 --> 01:23:11,699
나중에 10개의 다른 시간 단계에서 다음 시간 단계에서 그렇게 될 것이라고 생각합니다.

2040
01:23:11,699 --> 01:23:14,100


2041
01:23:14,100 --> 01:23:16,380


2042
01:23:16,380 --> 01:23:17,699


2043
01:23:17,699 --> 01:23:20,040


2044
01:23:20,040 --> 01:23:22,860
기본적으로 Grandeur 인과 관계를 모델링할 수 있는 정보를 전달하는 생물학적으로 그럴듯한 방법처럼 개발하는 것이 매우 흥미로울 것입니다. 흠,

2045
01:23:22,860 --> 01:23:24,659


2046
01:23:24,659 --> 01:23:29,659
이 모델에서

2047
01:23:30,840 --> 01:23:33,840
행동을 어디에서 볼 수 있습니까?

2048
01:23:33,840 --> 01:23:36,480


2049
01:23:36,480 --> 01:23:38,760


2050
01:23:38,760 --> 01:23:41,460


2051
01:23:41,460 --> 01:23:43,080


2052
01:23:43,080 --> 01:23:44,940
창의적인 코딩은 기본적으로 인식의 모델이기 때문에

2053
01:23:44,940 --> 01:23:46,260


2054
01:23:46,260 --> 01:23:49,260
행동은 경험하는 것의 결과가 있다는 것을 알 수 있습니다. 따라서

2055
01:23:49,260 --> 01:23:52,739


2056
01:23:52,739 --> 01:23:55,159


2057
01:23:55,159 --> 01:23:57,840
무언가를 경험하는 방식을 변경하면

2058
01:23:57,840 --> 01:24:00,060


2059
01:24:00,060 --> 01:24:01,800
더 많은 정보를 가지고 있기 때문에 더 똑똑한 행동을 할 수 있을지도 모르지만 예, 예처럼

2060
01:24:01,800 --> 01:24:03,000


2061
01:24:03,000 --> 01:24:04,560


2062
01:24:04,560 --> 01:24:06,960
행동이 쉽지 않다고 생각합니다.

2063
01:24:06,960 --> 01:24:10,199


2064
01:24:10,199 --> 01:24:12,540


2065
01:24:12,540 --> 01:24:14,040
기본적으로

2066
01:24:14,040 --> 01:24:15,960
어쩌면 당신은

2067
01:24:15,960 --> 01:24:18,780
단순히 그들이 미래에 행동을 수행하도록 더 나은 결론을 도출합니까?

2068
01:24:18,780 --> 01:24:21,719


2069
01:24:21,719 --> 01:24:23,940


2070
01:24:23,940 --> 01:24:25,920
사람들이 예측

2071
01:24:25,920 --> 01:24:29,340
코딩

2072
01:24:29,340 --> 01:24:33,960
과 행동에 대해 이야기한 몇 가지 방법을 추가하겠습니다.

2073
01:24:33,960 --> 01:24:36,120


2074
01:24:36,120 --> 01:24:37,980
하나의 접근 방식인 내부 동작으로서

2075
01:24:37,980 --> 01:24:40,560
또 다른 접근 방식은

2076
01:24:40,560 --> 01:24:42,840
주어진 노드의 출력입니다. 우리는

2077
01:24:42,840 --> 01:24:45,780
해당 노드를

2078
01:24:45,780 --> 01:24:48,780
자체 감각 인지 및

2079
01:24:48,780 --> 01:24:52,080
동작 상태가 있는 특정 항목으로 이해할 수 있습니다. 그런 의미에서

2080
01:24:52,080 --> 01:24:54,960
노드의 출력은 마지막으로

2081
01:24:54,960 --> 01:24:57,179
우리는 우리가 끝까지 읽고 있는 예측 코딩에 대한 이론적 검토에 대한 실시간 스트림 43에서 조금 탐구했습니다.

2082
01:24:57,179 --> 01:24:59,940


2083
01:24:59,940 --> 01:25:02,100


2084
01:25:02,100 --> 01:25:03,840


2085
01:25:03,840 --> 01:25:05,460
지각에 대한 모든 것이 지각에 관한 것이었고

2086
01:25:05,460 --> 01:25:08,040
섹션 5.3과 같았습니다.

2087
01:25:08,040 --> 01:25:11,719
행동에 대한 기대가 있다면

2088
01:25:11,719 --> 01:25:15,900
행동은 단지  이 아키텍처의 또 다른 변수는

2089
01:25:15,900 --> 01:25:18,120
실제로

2090
01:25:18,120 --> 01:25:20,040
비활성 추론과 일치합니다.

2091
01:25:20,040 --> 01:25:21,659
보상이나

2092
01:25:21,659 --> 01:25:24,000
유틸리티 기능을 최대화하는 대신

2093
01:25:24,000 --> 01:25:26,699


2094
01:25:26,699 --> 01:25:28,800
가장 가능성이 높은 행동 과정,

2095
01:25:28,800 --> 01:25:30,900
베이지안 역학의 최소 행동 경로에 따라 행동을 선택하므로

2096
01:25:30,900 --> 01:25:33,300
실제로 매우

2097
01:25:33,300 --> 01:25:36,420
행동 변수를 가져오고

2098
01:25:36,420 --> 01:25:40,800
본질적으로 그것을 마치 세상에서 더 수용 가능한

2099
01:25:40,800 --> 01:25:43,260
다른 것에 대한 예측인 것처럼 활용하는 것이 자연스럽습니다.

2100
01:25:43,260 --> 01:25:45,540


2101
01:25:45,540 --> 01:25:48,480


2102
01:25:48,480 --> 01:25:50,820


2103
01:25:50,820 --> 01:25:52,860


2104
01:25:52,860 --> 01:25:55,260
어 그리고 저는 여전히

2105
01:25:55,260 --> 01:25:57,239
예를 들어

2106
01:25:57,239 --> 01:26:01,139
이 방법을 적용하는 논문이 그리 많지 않다고 생각합니다. 저는

2107
01:26:01,139 --> 01:26:03,239


2108
01:26:03,239 --> 01:26:05,100
Alexander의 어에서 몇 가지가 있다고 생각하거나 robria가

2109
01:26:05,100 --> 01:26:08,580
비슷한 것을 수행하지만 실제로는 예측 코딩을

2110
01:26:08,580 --> 01:26:10,920
적용하는 것과 같은 순수 능동적 추론 외부와 같습니다.

2111
01:26:10,920 --> 01:26:13,260
그리고

2112
01:26:13,260 --> 01:26:15,980
실용적인 문제를 해결하기 위한 조치는 많이 탐구되지 않았습니다.

2113
01:26:15,980 --> 01:26:19,280


2114
01:26:19,679 --> 01:26:23,400
오 잘 이 훌륭한

2115
01:26:23,400 --> 01:26:25,199
프레젠테이션과 토론에 감사드립니다.

2116
01:26:25,199 --> 01:26:27,659
당신이 말하고 싶은 다른 것이 있습니까?

2117
01:26:27,659 --> 01:26:30,300


2118
01:26:30,300 --> 01:26:33,360


2119
01:26:33,360 --> 01:26:34,620
그리고 어

2120
01:26:34,620 --> 01:26:36,120
정말 즐거웠고 언젠가 멋진 Future Works를 위해 언제라도 돌아왔으면 좋겠습니다. 감사합니다.

2121
01:26:36,120 --> 01:26:38,460


2122
01:26:38,460 --> 01:26:40,199


2123
01:26:40,199 --> 01:26:41,580


2124
01:26:41,580 --> 01:26:45,000
Thomas 그래서

2125
01:26:45,000 --> 01:26:49,820
감사합니다 Daniel. 안녕히 계세요.


1
00:00:19,020 --> 00:00:21,119
您好，欢迎来到

2
00:00:21,119 --> 00:00:23,400


3
00:00:23,400 --> 00:00:28,140
2023 年 7 月 28 日的主动推理嘉宾直播，编号为 51.1，

4
00:00:28,140 --> 00:00:31,439
我们与 Tomaso Salvatore 一起在这里，我们

5
00:00:31,439 --> 00:00:33,840
将通过预测编码进行关于

6
00:00:33,840 --> 00:00:37,020
最近工作的因果推理的演示和讨论，

7
00:00:37,020 --> 00:00:39,660


8
00:00:39,660 --> 00:00:42,480
非常感谢您的

9
00:00:42,480 --> 00:00:44,340
观看 直播 请随意

10
00:00:44,340 --> 00:00:47,520
在实时聊天中写下问题，然后向

11
00:00:47,520 --> 00:00:50,399
您提问，谢谢，

12
00:00:50,399 --> 00:00:52,980
非常感谢丹尼尔邀请

13
00:00:52,980 --> 00:00:56,039
我，嗯，一直是该频道的忠实粉丝，

14
00:00:56,039 --> 00:00:57,719
我看了很多

15
00:00:57,719 --> 00:00:58,920
视频，

16
00:00:58,920 --> 00:01:01,379
我很喜欢 很高兴来到这里，呃，

17
00:01:01,379 --> 00:01:04,260
成为这次发言的人，

18
00:01:04,260 --> 00:01:06,600
所以我要谈谈

19
00:01:06,600 --> 00:01:08,700
我最近发布的这些预印本，这是

20
00:01:08,700 --> 00:01:11,159
过去几个月的工作

21
00:01:11,159 --> 00:01:12,119


22
00:01:12,119 --> 00:01:15,900
，它是与

23
00:01:15,900 --> 00:01:18,659
with Lookup 的合作 在Ketty，我在makarak

24
00:01:18,659 --> 00:01:21,600
barami Legend Thomas lukasiavich，

25
00:01:21,600 --> 00:01:24,000
它基本上

26
00:01:24,000 --> 00:01:26,299
是我在牛津大学工作的诗句公司

27
00:01:26,299 --> 00:01:31,680
和uhuvian之间的联合作品，

28
00:01:31,680 --> 00:01:34,200
所以

29
00:01:34,200 --> 00:01:36,299
在这次演讲中，

30
00:01:36,299 --> 00:01:38,220
我

31
00:01:38,220 --> 00:01:40,979
将这基本上是我将

32
00:01:40,979 --> 00:01:43,140
开始谈论的演讲的大纲 什么

33
00:01:43,140 --> 00:01:44,520
是预测编码

34
00:01:44,520 --> 00:01:47,659
以及它的给定交互是什么是

35
00:01:47,659 --> 00:01:51,299
一个简短的历史介绍为什么你

36
00:01:51,299 --> 00:01:54,060
认为呃对于研究

37
00:01:54,060 --> 00:01:56,159
创造性编码很重要，甚至

38
00:01:56,159 --> 00:01:58,619
从机器学习的角度来看，

39
00:01:58,619 --> 00:02:00,720
然后我将

40
00:02:00,720 --> 00:02:04,560
简要介绍什么是因果推理，

41
00:02:04,560 --> 00:02:07,200
以及 一旦我们掌握了所有这些

42
00:02:07,200 --> 00:02:08,880
信息，我将

43
00:02:08,880 --> 00:02:12,540
讨论为什么我写这篇论文，

44
00:02:12,540 --> 00:02:14,520
基本上是什么研究问题

45
00:02:14,520 --> 00:02:16,560
激发了我和其他

46
00:02:16,560 --> 00:02:18,300
合作者的灵感，

47
00:02:18,300 --> 00:02:21,660
并提出主要结果，即

48
00:02:21,660 --> 00:02:24,980
如何进行

49
00:02:24,980 --> 00:02:27,480
推理、干预和

50
00:02:27,480 --> 00:02:29,340
反事实推理

51
00:02:29,340 --> 00:02:33,319
以及 如何

52
00:02:33,319 --> 00:02:35,879
使用预测编码从给定的数据集中学习因果结构

53
00:02:35,879 --> 00:02:37,920
，然后我当然会

54
00:02:37,920 --> 00:02:39,959
以一些

55
00:02:39,959 --> 00:02:43,500
小总结和一些讨论来总结

56
00:02:43,500 --> 00:02:45,840
为什么我相信这项工作实际上会

57
00:02:45,840 --> 00:02:49,940
产生影响，以及一些未来的方向，

58
00:02:50,700 --> 00:02:53,400
那么什么是创意编码

59
00:02:53,400 --> 00:02:55,680
创意编码 一般来说，以

60
00:02:55,680 --> 00:02:58,440
神经科学启发的学习

61
00:02:58,440 --> 00:03:01,140
方法而闻名，那么关于大脑中的信息处理如何工作的理论，

62
00:03:01,140 --> 00:03:04,560


63
00:03:04,560 --> 00:03:05,819
以及

64
00:03:05,819 --> 00:03:08,400
非常正式地说，

65
00:03:08,400 --> 00:03:10,560
创造性编码的层可以被描述为

66
00:03:10,560 --> 00:03:12,659
基本上具有

67
00:03:12,659 --> 00:03:16,319
大脑中神经元的分层结构，

68
00:03:16,319 --> 00:03:19,080
并且你 大脑中有两个不同的

69
00:03:19,080 --> 00:03:20,700
神经元家族，

70
00:03:20,700 --> 00:03:23,280
第一个家族

71
00:03:23,280 --> 00:03:24,480
负责发送预测

72
00:03:24,480 --> 00:03:27,659
信息，因此层次结构中特定级别的神经元

73
00:03:27,659 --> 00:03:29,959
发送信息

74
00:03:29,959 --> 00:03:33,959
并预测下一级的活动，

75
00:03:33,959 --> 00:03:35,940


76
00:03:35,940 --> 00:03:38,340
第二个家族负责发送预测信息。 神经元是

77
00:03:38,340 --> 00:03:41,099
错误神经元和箭头神经元的神经元，

78
00:03:41,099 --> 00:03:43,019
它们向层次结构发送预测错误信息，

79
00:03:43,019 --> 00:03:46,319
因此一个级别预测

80
00:03:46,319 --> 00:03:49,200


81
00:03:49,200 --> 00:03:51,659
该活动下方级别的活动有一些这种预测，

82
00:03:51,659 --> 00:03:54,239
因为某些不匹配实际上会

83
00:03:54,239 --> 00:03:56,220
在下面的级别中发生，

84
00:03:56,220 --> 00:03:57,840
并且有关信息 预测

85
00:03:57,840 --> 00:04:02,400
误差被向上发送箭头键，

86
00:04:02,400 --> 00:04:04,860
但是预测编码

87
00:04:04,860 --> 00:04:07,220
实际上并没有作为

88
00:04:07,220 --> 00:04:10,799
神经科学作为

89
00:04:10,799 --> 00:04:11,939
神经科学的理论而被烧毁，

90
00:04:11,939 --> 00:04:13,860
但它实际上最初是在 50 年代

91
00:04:13,860 --> 00:04:16,139
作为信号处理和

92
00:04:16,139 --> 00:04:19,380
压缩的方法开发的，所以

93
00:04:19,380 --> 00:04:21,899
Oliver Elias 实际上是香农的香农

94
00:04:21,899 --> 00:04:25,020
的当代人，

95
00:04:25,020 --> 00:04:26,160


96
00:04:26,160 --> 00:04:27,960
他们意识到，一旦我们有了一个

97
00:04:27,960 --> 00:04:30,900
预测器，一个可以很好

98
00:04:30,900 --> 00:04:33,600
地预测数据的模型，

99
00:04:33,600 --> 00:04:36,000
发送有关这些预测中的错误的消息

100
00:04:36,000 --> 00:04:37,919
实际上

101
00:04:37,919 --> 00:04:41,100
比每次发送整个消息要便宜得多。

102
00:04:41,100 --> 00:04:42,720
时间，

103
00:04:42,720 --> 00:04:45,240
这就是漂亮的编码是如何诞生的，

104
00:04:45,240 --> 00:04:47,639


105
00:04:47,639 --> 00:04:49,500
作为信息论中的信号处理和压缩

106
00:04:49,500 --> 00:04:52,020
机制，早在

107
00:04:52,020 --> 00:04:53,639
50 年代，

108
00:04:53,639 --> 00:04:57,120
实际上是在 80 年代，呃，

109
00:04:57,120 --> 00:04:59,400


110
00:04:59,400 --> 00:05:01,800


111
00:05:01,800 --> 00:05:03,540
神经科学中使用了完全相同的模型，

112
00:05:03,540 --> 00:05:07,500
呃 因此，通过 Mumford 或

113
00:05:07,500 --> 00:05:10,440
其他作品的工作，例如解释

114
00:05:10,440 --> 00:05:12,960
如何处理信息的评级，以便

115
00:05:12,960 --> 00:05:14,520
我们从外部世界获得预测信号，

116
00:05:14,520 --> 00:05:17,160
我们需要压缩

117
00:05:17,160 --> 00:05:20,280
这种表示，呃，并

118
00:05:20,280 --> 00:05:22,740
在我们的神经元和方法中拥有这种内部表示 与

119
00:05:22,740 --> 00:05:25,199


120
00:05:25,199 --> 00:05:27,720


121
00:05:27,720 --> 00:05:30,419
Elias 和 Oliver 在

122
00:05:30,419 --> 00:05:32,900
50 年代开发的方法非常相似，如果不是等同的话，

123
00:05:32,940 --> 00:05:35,520
也许是 1999 年发生的最大的范式转变，这要

124
00:05:35,520 --> 00:05:38,100


125
00:05:38,100 --> 00:05:41,400
归功于巴拉德周围的工作，

126
00:05:41,400 --> 00:05:44,880
他们在其中引入了

127
00:05:44,880 --> 00:05:46,199
这个概念，我 之前提到过

128
00:05:46,199 --> 00:05:48,060


129
00:05:48,060 --> 00:05:51,360
大脑中的层次结构，其中预测信息是

130
00:05:51,360 --> 00:05:54,240
自上而下的，错误信息是

131
00:05:54,240 --> 00:05:55,560
自下而上的，

132
00:05:55,560 --> 00:05:57,660
他们所做的事情是

133
00:05:57,660 --> 00:05:59,759
以前没有做过的，

134
00:05:59,759 --> 00:06:02,820
他们不仅在法国解释和发展了这个理论，而且还解释了这一理论。

135
00:06:02,820 --> 00:06:05,759


136
00:06:05,759 --> 00:06:07,979
关于学习在大脑中如何运作，

137
00:06:07,979 --> 00:06:10,139
所以这也是我们的突触如何更新的理论，

138
00:06:10,139 --> 00:06:13,139


139
00:06:13,139 --> 00:06:16,080
我

140
00:06:16,080 --> 00:06:17,940
要在这个简短的

141
00:06:17,940 --> 00:06:21,900
历史介绍中谈论的最后一个重大突破是在 2003 年，但是

142
00:06:21,900 --> 00:06:25,380
呃，然后他一直在

143
00:06:25,380 --> 00:06:28,380
几年后，呃，多亏了卡菲斯顿，他

144
00:06:28,380 --> 00:06:32,100
基本上采用了罗宾巴拉德的理论，

145
00:06:32,100 --> 00:06:35,039
并

146
00:06:35,039 --> 00:06:38,400
发展了它，并将其推广到

147
00:06:38,400 --> 00:06:40,919
生成模型理论，所以

148
00:06:40,919 --> 00:06:42,720
基本上

149
00:06:42,720 --> 00:06:45,479
卡菲斯顿所做的主要主张是，创造性编码是

150
00:06:45,479 --> 00:06:48,780
一种 一种

151
00:06:48,780 --> 00:06:50,340
特定类型的生成模型的证据最大化方案，

152
00:06:50,340 --> 00:06:52,979
我稍后也会介绍，

153
00:06:52,979 --> 00:06:55,139


154
00:06:55,139 --> 00:07:00,300
因此在我描述的前两种创意角中做一个简短的总结，

155
00:07:00,300 --> 00:07:01,560


156
00:07:01,560 --> 00:07:03,900


157
00:07:03,900 --> 00:07:05,340
即信号处理和

158
00:07:05,340 --> 00:07:07,020
压缩以及

159
00:07:07,020 --> 00:07:09,180
视网膜和大脑中的信息处理

160
00:07:09,180 --> 00:07:11,160
一般来说它们是推理

161
00:07:11,160 --> 00:07:12,300
方法，

162
00:07:12,300 --> 00:07:14,819
最大的

163
00:07:14,819 --> 00:07:17,819
变化是我们

164
00:07:17,819 --> 00:07:21,120
在 1999 年经历的最大的革命，所以我们可以说在 21

165
00:07:21,120 --> 00:07:23,580
世纪，操作编码被视为

166
00:07:23,580 --> 00:07:25,919
一种学习算法，因此我们可以首先

167
00:07:25,919 --> 00:07:29,699
压缩信息 然后更新生成模型中的所有

168
00:07:29,699 --> 00:07:31,800
突触或所有潜在变量，

169
00:07:31,800 --> 00:07:34,139


170
00:07:34,139 --> 00:07:38,599
以改进生成模型本身，

171
00:07:38,759 --> 00:07:43,199
因此让我们给出一些更正式的呃定义，

172
00:07:43,199 --> 00:07:45,000


173
00:07:45,000 --> 00:07:48,479
以便操作编码可以被视为

174
00:07:48,479 --> 00:07:50,220
分层高斯生成 模型，

175
00:07:50,220 --> 00:07:53,400
所以这是一个非常简单的图，其中

176
00:07:53,400 --> 00:07:54,780
我们有一个层次结构，

177
00:07:54,780 --> 00:07:58,319
可以是我们想要的深度，

178
00:07:58,319 --> 00:08:01,560
信号预测信号

179
00:08:01,560 --> 00:08:04,620
从一个潜在变量 XM 到

180
00:08:04,620 --> 00:08:06,599
下一个潜在变量，并且

181
00:08:06,599 --> 00:08:09,720
每次都通过函数 GN

182
00:08:09,720 --> 00:08:12,620
或 GI 进行转换

183
00:08:15,319 --> 00:08:18,180
正如我所说，这是一个生成模型，

184
00:08:18,180 --> 00:08:19,680
这个生成模型的边际概率是多少，

185
00:08:19,680 --> 00:08:21,780
这只是

186
00:08:21,780 --> 00:08:24,960
最后一个的概率，

187
00:08:24,960 --> 00:08:27,660
你能看到我的光标吗？是的，是的，

188
00:08:27,660 --> 00:08:29,940
它是完美的，所以它是

189
00:08:29,940 --> 00:08:32,700
最后一个顶点的遗传模型，对不起，可能是

190
00:08:32,700 --> 00:08:34,979
分布 最后一个顶点的概率乘以

191
00:08:34,979 --> 00:08:37,140
每个其他顶点的概率分布，以

192
00:08:37,140 --> 00:08:40,440


193
00:08:40,440 --> 00:08:43,020
之前顶点的活动或

194
00:08:43,020 --> 00:08:45,860
潜在变量为条件。

195
00:08:45,899 --> 00:08:48,240
我已经说过，这是一个高斯

196
00:08:48,240 --> 00:08:50,399
生成模型，这意味着这些

197
00:08:50,399 --> 00:08:54,260
概率是高斯形式的，

198
00:08:54,660 --> 00:08:57,120
并且每个

199
00:08:57,120 --> 00:09:00,480
endos 函数 一般而言，函数 G

200
00:09:00,480 --> 00:09:02,880
特别是因为例如在

201
00:09:02,880 --> 00:09:05,459
Rambler 论文以及之后的所有论文中，

202
00:09:05,459 --> 00:09:07,920
也因为深度

203
00:09:07,920 --> 00:09:10,500
学习革命，这些函数

204
00:09:10,500 --> 00:09:13,220
只是简单的线性映射或

205
00:09:13,220 --> 00:09:15,120
具有激活

206
00:09:15,120 --> 00:09:18,000
函数的非线性映射或具有

207
00:09:18,000 --> 00:09:22,040
激活函数和 加性偏差，

208
00:09:23,220 --> 00:09:27,180
因此我们可以给出

209
00:09:27,180 --> 00:09:28,860
创造性编码的正式定义，我们可以

210
00:09:28,860 --> 00:09:30,300
说操作编码是

211
00:09:30,300 --> 00:09:33,480
这种生成模型的反演方案，其中通过最小化通常

212
00:09:33,480 --> 00:09:35,839


213
00:09:35,839 --> 00:09:38,760
称为

214
00:09:38,760 --> 00:09:40,920
自由能变化的量来最大化其模型证据

215
00:09:40,920 --> 00:09:43,740
每个生成

216
00:09:43,740 --> 00:09:46,019
模型的目标都是最大化模型证据，但

217
00:09:46,019 --> 00:09:48,860
这个量总是很棘手，

218
00:09:48,860 --> 00:09:51,019
我们有一些

219
00:09:51,019 --> 00:09:53,279
技术可以让我们

220
00:09:53,279 --> 00:09:55,980
近似解决方案以及

221
00:09:55,980 --> 00:09:58,500
我们在创造性编码中使用的技术，而

222
00:09:58,500 --> 00:10:00,720
不是最小化自由能像差，

223
00:10:00,720 --> 00:10:03,480
这是一种 这是

224
00:10:03,480 --> 00:10:06,839
这项工作中模型证据的下限，

225
00:10:06,839 --> 00:10:09,660
实际上在许多

226
00:10:09,660 --> 00:10:11,700
其他工作中也是如此，所以这是执行此操作的标准方法，

227
00:10:11,700 --> 00:10:13,740
这种最小化是执行

228
00:10:13,740 --> 00:10:16,080
成分下降

229
00:10:16,080 --> 00:10:18,540
嗯，是的，执行我们在下降中达成的协议，

230
00:10:18,540 --> 00:10:19,980
并且有 实际上，其他方法（

231
00:10:19,980 --> 00:10:22,140
例如期望最大化）

232
00:10:22,140 --> 00:10:23,580
通常是等效的，

233
00:10:23,580 --> 00:10:25,140
或者您可以使用其他一些消息

234
00:10:25,140 --> 00:10:26,940
传递算法（例如信念

235
00:10:26,940 --> 00:10:29,959
传播），

236
00:10:30,720 --> 00:10:33,980
然后稍微回溯过去，因此

237
00:10:33,980 --> 00:10:35,940


238
00:10:35,940 --> 00:10:38,760


239
00:10:38,760 --> 00:10:41,360
如果我们可以看到创造性编码，请忘记一点统计生成模型

240
00:10:41,360 --> 00:10:44,040
我的意思是，我已经说过几次

241
00:10:44,040 --> 00:10:46,200
作为

242
00:10:46,200 --> 00:10:48,420
神经活动的分层模型，因此对于

243
00:10:48,420 --> 00:10:50,700
代表神经活动的神经元潜在变量，

244
00:10:50,700 --> 00:10:53,459
发送者在层次结构中发出信号，

245
00:10:53,459 --> 00:10:54,899


246
00:10:54,899 --> 00:10:57,540
而对于错误节点或错误神经元，

247
00:10:57,540 --> 00:11:01,019
发送者在层次结构中发出信号，所以

248
00:11:01,019 --> 00:11:03,660
这个和 返回的误差信息

249
00:11:03,660 --> 00:11:05,700
是此类操作编码模型的自由能变化是多少，

250
00:11:05,700 --> 00:11:08,220
它

251
00:11:08,220 --> 00:11:09,899
只是

252
00:11:09,899 --> 00:11:12,720
所有误差神经元的均方误差之和，

253
00:11:12,720 --> 00:11:14,399


254
00:11:14,399 --> 00:11:18,120
因此是

255
00:11:18,120 --> 00:11:21,980
总误差平方的误差

256
00:11:22,019 --> 00:11:24,480
之和，该表示为

257
00:11:24,480 --> 00:11:27,120
在后面的幻灯片以及

258
00:11:27,120 --> 00:11:28,740
我将如何解释如何使用

259
00:11:28,740 --> 00:11:30,120
创造性编码来建模因果

260
00:11:30,120 --> 00:11:32,940
推理中将会很有用，例如，

261
00:11:32,940 --> 00:11:34,800
我认为预测编码很重要，

262
00:11:34,800 --> 00:11:36,240
但它不是一个

263
00:11:36,240 --> 00:11:37,500


264
00:11:37,500 --> 00:11:39,600
首先要好好研究的好算法，因为 我之前说过，它

265
00:11:39,600 --> 00:11:41,399
优化了正确的目标，即

266
00:11:41,399 --> 00:11:43,079
模型证据或边际

267
00:11:43,079 --> 00:11:44,339
可能性，

268
00:11:44,339 --> 00:11:45,660


269
00:11:45,660 --> 00:11:47,700
然后通过优化

270
00:11:47,700 --> 00:11:49,440
下限（称为

271
00:11:49,440 --> 00:11:52,440
自由能的变化）来实现这一目标，正如我所说，虚拟

272
00:11:52,440 --> 00:11:54,240
完成很有趣，因为它可以

273
00:11:54,240 --> 00:11:57,680
写成 两个不同术语的总和，

274
00:11:57,680 --> 00:12:00,839
以及

275
00:12:00,839 --> 00:12:04,680
优化它的每个术语作为重要影响，

276
00:12:04,680 --> 00:12:06,899
例如在机器学习任务

277
00:12:06,899 --> 00:12:09,060
或一般学习任务中，

278
00:12:09,060 --> 00:12:12,420
所以其中一个术语强制记忆，

279
00:12:12,420 --> 00:12:15,440
所以在第二项中基本上告诉

280
00:12:15,440 --> 00:12:18,180
强制模型 拟合特定的数据

281
00:12:18,180 --> 00:12:19,560
集，

282
00:12:19,560 --> 00:12:21,240
第一项

283
00:12:21,240 --> 00:12:23,519
迫使模型最小化

284
00:12:23,519 --> 00:12:26,040
复杂性，正如我们所知，例如

285
00:12:26,040 --> 00:12:28,500
结果剃刀

286
00:12:28,500 --> 00:12:31,260
理论，如果我们有两个不同的模型，它们

287
00:12:31,260 --> 00:12:33,000
在特定

288
00:12:33,000 --> 00:12:35,640
训练集上的表现与我们拥有的模型类似 得到

289
00:12:35,640 --> 00:12:37,380
并且

290
00:12:37,380 --> 00:12:39,899
最有望概括呃的是不太

291
00:12:39,899 --> 00:12:41,160
复杂的

292
00:12:41,160 --> 00:12:44,100
模型，因此通过操作自由能更新生成模型

293
00:12:44,100 --> 00:12:46,380
使我们

294
00:12:46,380 --> 00:12:47,779
基本上可以

295
00:12:47,779 --> 00:12:51,959
收敛到最佳呃结果剃刀

296
00:12:51,959 --> 00:12:54,720
模型，该模型既可以记忆

297
00:12:54,720 --> 00:12:56,100
数据集，也可以 能够

298
00:12:56,100 --> 00:12:58,680
很好地概括看不见的

299
00:12:58,680 --> 00:13:00,240
数据点

300
00:13:00,240 --> 00:13:02,639
操作编码之所以重要的第二个原因

301
00:13:02,639 --> 00:13:08,600
是，它实际上

302
00:13:08,720 --> 00:13:11,760
不必在

303
00:13:11,760 --> 00:13:13,920
层次结构上定义，但它可以

304
00:13:13,920 --> 00:13:15,959
在更复杂和灵活的体系

305
00:13:15,959 --> 00:13:18,240
结构（例如定向图形）上建模

306
00:13:18,240 --> 00:13:21,540
任何形状的模型，或者

307
00:13:21,540 --> 00:13:23,700
更广义地推广到具有许多类似于

308
00:13:23,700 --> 00:13:25,920
大脑区域的循环的网络，最终

309
00:13:25,920 --> 00:13:27,779
结果的根本原因

310
00:13:27,779 --> 00:13:30,300
是你没有通过前向传播来学习和

311
00:13:30,300 --> 00:13:32,339
预测，然后

312
00:13:32,339 --> 00:13:34,260
向后传播误差，但你是

313
00:13:34,260 --> 00:13:36,600
最小化能量函数，

314
00:13:36,600 --> 00:13:38,459
这基本上允许每种层次

315
00:13:38,459 --> 00:13:39,839
结构都

316
00:13:39,839 --> 00:13:41,180


317
00:13:41,180 --> 00:13:43,860
允许进入直接键并

318
00:13:43,860 --> 00:13:46,860
允许学习循环，这

319
00:13:46,860 --> 00:13:48,060
实际上非常重要，因为

320
00:13:48,060 --> 00:13:50,399
大脑充满了循环，因为我们

321
00:13:50,399 --> 00:13:53,399
从最近的一些论文中获得了一些信息

322
00:13:53,399 --> 00:13:56,459
呃，我们成功地完全映射了

323
00:13:56,459 --> 00:13:59,279
一些动物的大脑，例如

324
00:13:59,279 --> 00:14:00,420
果蝇，

325
00:14:00,420 --> 00:14:03,899
大脑充满了循环，所以用一种允许我们使用循环的算法来

326
00:14:03,899 --> 00:14:06,720
耗尽我们的机器学习

327
00:14:06,720 --> 00:14:09,000
模型或

328
00:14:09,000 --> 00:14:11,160
我们的一般模型是有意义的

329
00:14:11,160 --> 00:14:14,160


330
00:14:14,160 --> 00:14:17,160
结构

331
00:14:17,160 --> 00:14:19,380
操作编码有趣的第三个原因

332
00:14:19,380 --> 00:14:21,240
是，它已经被正式

333
00:14:21,240 --> 00:14:23,820
证明比

334
00:14:23,820 --> 00:14:25,139
从黑色传播开始的标准神经网络更稳健，

335
00:14:25,139 --> 00:14:27,060
因此，如果您有一个

336
00:14:27,060 --> 00:14:28,200
神经网络并且想要执行

337
00:14:28,200 --> 00:14:30,320
分类任务，那么

338
00:14:30,320 --> 00:14:34,139
创造性编码会更稳健

339
00:14:34,139 --> 00:14:36,260
，并且 这

340
00:14:36,260 --> 00:14:38,339
对于

341
00:14:38,339 --> 00:14:40,680
小数据集的在线学习训练或

342
00:14:40,680 --> 00:14:43,440
连续学习任务等任务来说是一个有趣的问题，该理论

343
00:14:43,440 --> 00:14:45,540
基本上来自这样一个事实：

344
00:14:45,540 --> 00:14:48,540
命令式编码已被移动到近似隐式

345
00:14:48,540 --> 00:14:50,820
梯度下降，

346
00:14:50,820 --> 00:14:53,339


347
00:14:53,339 --> 00:14:54,899
这是显式梯度下降的不同版本

348
00:14:54,899 --> 00:14:57,180


349
00:14:57,180 --> 00:14:59,880
基本上每个模型中使用的标准绿色下降，

350
00:14:59,880 --> 00:15:03,680
这是一种更稳健的变体，

351
00:15:05,880 --> 00:15:08,279
我想好吧，我做了相当长的内部

352
00:15:08,279 --> 00:15:09,779
操作编码，我想我现在正在

353
00:15:09,779 --> 00:15:11,639
转向第二个主题，即因果

354
00:15:11,639 --> 00:15:13,019
推理

355
00:15:13,019 --> 00:15:15,839
和什么 因果推理 因果

356
00:15:15,839 --> 00:15:18,420
推理是一种理论，它是一种非常

357
00:15:18,420 --> 00:15:20,339
普遍的理论，

358
00:15:20,339 --> 00:15:23,100
朱迪服装最形式化了，他绝对

359
00:15:23,100 --> 00:15:25,500
是法国因果领域最重要的人物，

360
00:15:25,500 --> 00:15:27,839
他写了一些

361
00:15:27,839 --> 00:15:29,760
非常好的书，例如《Y》的书，

362
00:15:29,760 --> 00:15:32,760
强烈推荐 如果您想

363
00:15:32,760 --> 00:15:35,220
了解有关该主题的更多信息，

364
00:15:35,220 --> 00:15:37,800
并且它基本上解决了以下

365
00:15:37,800 --> 00:15:38,639
问题，

366
00:15:38,639 --> 00:15:40,440
那么让我们假设我们有一个

367
00:15:40,440 --> 00:15:42,000


368
00:15:42,000 --> 00:15:44,160
与贝叶斯网络相关的联合概率分布，这

369
00:15:44,160 --> 00:15:46,199
将是

370
00:15:46,199 --> 00:15:49,260
整个论文的运行示例，

371
00:15:49,260 --> 00:15:51,839
特别是当 你不使用

372
00:15:51,839 --> 00:15:54,480
这种形状的亚洲网络，

373
00:15:54,480 --> 00:15:57,660
它基于网络，它们

374
00:15:57,660 --> 00:16:00,240
内部的变量可以代表

375
00:16:00,240 --> 00:16:02,100
不同的数量，因此例如我们

376
00:16:02,100 --> 00:16:04,620
具有这种形状的视觉网络可以

377
00:16:04,620 --> 00:16:06,899
代表

378
00:16:06,899 --> 00:16:08,820
右侧的数量，因此个人

379
00:16:08,820 --> 00:16:10,800
的社会经济工作室雕像

380
00:16:10,800 --> 00:16:13,079
教育水平、

381
00:16:13,079 --> 00:16:16,699
智力和收入水平

382
00:16:17,100 --> 00:16:19,440
是经典统计学

383
00:16:19,440 --> 00:16:22,920
非常擅长的，是呃，而呃，

384
00:16:22,920 --> 00:16:25,320
最常用的应用是对

385
00:16:25,320 --> 00:16:28,019
观察或相关性进行建模，

386
00:16:28,019 --> 00:16:29,279
相关性基本上回答了这个

387
00:16:29,279 --> 00:16:32,519
问题：如果我们观察

388
00:16:32,519 --> 00:16:35,579
另一个变量 C，

389
00:16:35,579 --> 00:16:37,500
那么，例如，在 在这种情况下，

390
00:16:37,500 --> 00:16:39,660
收入水平是多少

391
00:16:39,660 --> 00:16:41,820
如果我

392
00:16:41,820 --> 00:16:44,339
观察这个教育水平，则个人的预期收入水平，

393
00:16:44,339 --> 00:16:48,180
当然，如果该人

394
00:16:48,180 --> 00:16:50,220
具有更高的教育程度，

395
00:16:50,220 --> 00:16:52,500
例如硕士或博士学位，我预计

396
00:16:52,500 --> 00:16:54,360
该人一般具有 较高的

397
00:16:54,360 --> 00:16:56,040
收入水平，

398
00:16:56,040 --> 00:16:58,139
这是一种相关性，

399
00:16:58,139 --> 00:17:00,300
但有时有些事情

400
00:17:00,300 --> 00:17:03,300
很难观察，但它们

401
00:17:03,300 --> 00:17:05,040
在确定这些数量方面发挥着巨大作用，

402
00:17:05,040 --> 00:17:06,119


403
00:17:06,119 --> 00:17:08,220
因此，例如，

404
00:17:08,220 --> 00:17:11,160
收入水平可能更多地

405
00:17:11,160 --> 00:17:13,380
由一个人的智力来定义

406
00:17:13,380 --> 00:17:15,540
特定的人，

407
00:17:15,540 --> 00:17:18,720
也许智力或

408
00:17:18,720 --> 00:17:21,000
一个人是否聪明也最

409
00:17:21,000 --> 00:17:24,540
有可能拥有较高的教育水平，

410
00:17:24,540 --> 00:17:27,540
但仍然是

411
00:17:27,540 --> 00:17:30,120
每个收入的真正原因是我是因为

412
00:17:30,120 --> 00:17:32,220
智商

413
00:17:32,220 --> 00:17:34,740
，这可能是这不能

414
00:17:34,740 --> 00:17:36,360
通过简单的相关性进行研究，并且必须

415
00:17:36,360 --> 00:17:39,120
通过一种更先进的技术进行研究，这种技术

416
00:17:39,120 --> 00:17:41,280
称为干预，

417
00:17:41,280 --> 00:17:43,320
干预基本上回答的

418
00:17:43,320 --> 00:17:46,500
问题是，如果我们将 C 更改为特定值，那么 D 是什么，

419
00:17:46,500 --> 00:17:48,240


420
00:17:48,240 --> 00:17:51,000
例如，我们可以采取一个

421
00:17:51,000 --> 00:17:54,660
个体 检查他的收入水平，

422
00:17:54,660 --> 00:17:57,120
然后改变其教育水平，以便在

423
00:17:57,120 --> 00:17:59,220


424
00:17:59,220 --> 00:18:01,080
不影响他的智力的情况下干预这个世界并改变其教育水平，看看

425
00:18:01,080 --> 00:18:03,419


426
00:18:03,419 --> 00:18:07,260
其收入变化有多大，

427
00:18:07,260 --> 00:18:09,900
例如，如果收入变化很大，则

428
00:18:09,900 --> 00:18:12,179
意味着智力

429
00:18:12,179 --> 00:18:14,460
没有变化 在这方面并没有发挥很大的作用，但

430
00:18:14,460 --> 00:18:16,799
教育水平会起作用，如果收入水平

431
00:18:16,799 --> 00:18:19,020
没有太大变化，这意味着

432
00:18:19,020 --> 00:18:20,640
在这种情况下可能存在一个隐藏变量，

433
00:18:20,640 --> 00:18:22,860
智力决定了

434
00:18:22,860 --> 00:18:25,760
一个人的收入水平

435
00:18:25,980 --> 00:18:28,740
第三个重要的因果

436
00:18:28,740 --> 00:18:31,080
推论是： 反事实，

437
00:18:31,080 --> 00:18:33,120
例如，反事实回答了

438
00:18:33,120 --> 00:18:36,720
问题是什么，我们将

439
00:18:36,720 --> 00:18:39,240
C 更改为过去的不同值，

440
00:18:39,240 --> 00:18:40,679
例如，我们可以看到

441
00:18:40,679 --> 00:18:42,059
干预措施和

442
00:18:42,059 --> 00:18:45,059
反事实之间的区别在于，干预措施

443
00:18:45,059 --> 00:18:47,820
在未来起作用，所以我正在采访 在

444
00:18:47,820 --> 00:18:50,340
现在的世界中观察未来的变化

445
00:18:50,340 --> 00:18:53,220
以及反事实使我们能够

446
00:18:53,220 --> 00:18:56,039
回到过去并改变过去的变量，

447
00:18:56,039 --> 00:18:59,160
看看这种变化

448
00:18:59,160 --> 00:19:01,320
将如何影响我们现在生活的世界

449
00:19:01,320 --> 00:19:02,940


450
00:19:02,940 --> 00:19:06,299
，这些被 judapple 定义为

451
00:19:06,299 --> 00:19:08,100
三个 因果推理相关性的级别

452
00:19:08,100 --> 00:19:09,660
是第一级

453
00:19:09,660 --> 00:19:11,580
干预是

454
00:19:11,580 --> 00:19:14,720
反事实的第二级是第三级

455
00:19:16,020 --> 00:19:18,120
其他干预

456
00:19:18,120 --> 00:19:20,640
现在我给出了

457
00:19:20,640 --> 00:19:23,760
一个直观的定义，我将更正式地定义它们，我

458
00:19:23,760 --> 00:19:25,500
在这里使用这个符号

459
00:19:25,500 --> 00:19:27,240
实际上在整个演示中都是一样的，

460
00:19:27,240 --> 00:19:29,640
所以 X 总是

461
00:19:29,640 --> 00:19:32,820
一个潜在变量，i 总是

462
00:19:32,820 --> 00:19:35,340
一个数据点或一个观察值

463
00:19:35,340 --> 00:19:38,520
，VI 总是一个顶点，所以

464
00:19:38,520 --> 00:19:40,860
每次你看到 VI 时，我们只是

465
00:19:40,860 --> 00:19:42,720


466
00:19:42,720 --> 00:19:45,299
例如，我们对图的结构感兴趣，

467
00:19:45,299 --> 00:19:46,860
所以假设我们有一个贝叶斯模型，

468
00:19:46,860 --> 00:19:50,160
它的结构与

469
00:19:50,160 --> 00:19:52,679
我们在上一张幻灯片中看到的贝叶斯模型相同，

470
00:19:52,679 --> 00:19:54,780


471
00:19:54,780 --> 00:19:57,840
假设 X3 等于 S3，这是

472
00:19:57,840 --> 00:20:00,660
我们进行统计的观察结果 我们

473
00:20:00,660 --> 00:20:03,360
计算 X4 的概率或

474
00:20:03,360 --> 00:20:04,679
期望，X4

475
00:20:04,679 --> 00:20:07,380
是

476
00:20:07,380 --> 00:20:09,240
与该顶点相关的潜在变量，

477
00:20:09,240 --> 00:20:13,860
假设 X3 等于 S3

478
00:20:13,860 --> 00:20:15,679
外部

479
00:20:15,679 --> 00:20:17,760
干预，我们需要一种新的

480
00:20:17,760 --> 00:20:19,919
表示法，称为 do

481
00:20:19,919 --> 00:20:21,179
操作，

482
00:20:21,179 --> 00:20:23,880
因此在这种情况下，

483
00:20:23,880 --> 00:20:26,100
我们要计算 X4

484
00:20:26,100 --> 00:20:30,000
考虑到我们干预这个

485
00:20:30,000 --> 00:20:33,059
词并改变 X3 West 3 的事实，X4 的概率。

486
00:20:33,059 --> 00:20:35,580
我们如何执行

487
00:20:35,580 --> 00:20:38,400
干预 Judo Pearl 告诉我们，

488
00:20:38,400 --> 00:20:40,020


489
00:20:40,020 --> 00:20:41,880
在计算相关性之前，我们必须有一个中间步骤

490
00:20:41,880 --> 00:20:45,059


491
00:20:45,059 --> 00:20:46,860
必须删除所有以删除

492
00:20:46,860 --> 00:20:50,160
V3 的所有传入边，

493
00:20:50,160 --> 00:20:52,799
因此我们必须研究的不是这个贝叶斯

494
00:20:52,799 --> 00:20:55,679
网络，而是第二个贝叶斯网络，

495
00:20:55,679 --> 00:20:58,200
然后在这一点上我们可以

496
00:20:58,200 --> 00:21:00,840


497
00:21:00,840 --> 00:21:03,299
像平常一样计算相关性，

498
00:21:03,299 --> 00:21:06,500
这是一种

499
00:21:07,020 --> 00:21:09,299
反事实的干预 是

500
00:21:09,299 --> 00:21:11,700
这一点的概括，正如我所说的生活在过去，

501
00:21:11,700 --> 00:21:14,100
他们正在使用结构因果模型进行计算，

502
00:21:14,100 --> 00:21:15,419


503
00:21:15,419 --> 00:21:18,299
结构因果模型是一个元组，

504
00:21:18,299 --> 00:21:21,120
它在概念上类似于

505
00:21:21,120 --> 00:21:23,460
贝叶斯网络，但基本上我们

506
00:21:23,460 --> 00:21:26,220
在上面有这一类新的变量

507
00:21:26,220 --> 00:21:28,580
是他们使用的不可观察变量，

508
00:21:28,580 --> 00:21:30,960
所以我们

509
00:21:30,960 --> 00:21:34,020
有 X1 X2 X3 S4 之前的贝叶斯网络，

510
00:21:34,020 --> 00:21:37,460
但我们也有那些不可观察或

511
00:21:37,460 --> 00:21:40,020
依赖于环境的变量，

512
00:21:40,020 --> 00:21:42,539
你无法控制它们，你可以推断

513
00:21:42,539 --> 00:21:43,980
它们，但你

514
00:21:43,980 --> 00:21:46,020
但它们就在那里，

515
00:21:46,020 --> 00:21:48,539


516
00:21:48,539 --> 00:21:51,360
f 是一组函数，它依赖于

517
00:21:51,360 --> 00:21:53,400


518
00:21:53,400 --> 00:21:57,299
x 的所有基本 f，x3 依赖于 X1，

519
00:21:57,299 --> 00:21:58,980
因为我们在 x2 上有一个箭头，因为

520
00:21:58,980 --> 00:22:00,960
你有一个箭头，并且

521
00:22:00,960 --> 00:22:02,940
依赖于也影响极端的不可观察变量，

522
00:22:02,940 --> 00:22:05,840


523
00:22:06,179 --> 00:22:09,240
所以是的，直觉上你可以看到我们，你

524
00:22:09,240 --> 00:22:11,940
可以认为 结构因果模型

525
00:22:11,940 --> 00:22:14,159
作为贝叶斯网络，其中

526
00:22:14,159 --> 00:22:16,679
不可观察的变量位于顶部，每个

527
00:22:16,679 --> 00:22:19,500
不可观察的变量仅影响

528
00:22:19,500 --> 00:22:22,020
其自己

529
00:22:22,020 --> 00:22:24,600
的最新变量 X，因此例如

530
00:22:24,600 --> 00:22:27,960
IU 永远不会接触 X1，u3

531
00:22:27,960 --> 00:22:30,360
只会接触 Q3 E1 都会影响

532
00:22:30,360 --> 00:22:34,039
X1 和 依此类推，

533
00:22:35,039 --> 00:22:37,679
执行反事实推理

534
00:22:37,679 --> 00:22:39,900
回答了以下问题，那么

535
00:22:39,900 --> 00:22:42,960


536
00:22:42,960 --> 00:22:46,620
在通过情况下，X4 在 X3 处等于另一个变量吗？你

537
00:22:46,620 --> 00:22:49,340
外国

538
00:22:49,340 --> 00:22:51,840
需要三个不同的步骤，所以

539
00:22:51,840 --> 00:22:53,039
溯因就是

540
00:22:53,039 --> 00:22:54,900


541
00:22:54,900 --> 00:22:57,179
所有背景

542
00:22:57,179 --> 00:22:59,460
变量的计算，所以在这个 在这一步中，我们

543
00:22:59,460 --> 00:23:01,200
想要回到过去，了解

544
00:23:01,200 --> 00:23:03,419
不可观察的环境

545
00:23:03,419 --> 00:23:04,919


546
00:23:04,919 --> 00:23:08,039
在那个特定时刻的情况，

547
00:23:08,039 --> 00:23:11,039
我们通过将所有潜在

548
00:23:11,039 --> 00:23:14,280
变量 X 固定到我们已经拥有的一些特定数据来实现这一点，

549
00:23:14,280 --> 00:23:16,140


550
00:23:16,140 --> 00:23:18,960
并执行这些呃这个

551
00:23:18,960 --> 00:23:21,120


552
00:23:21,120 --> 00:23:24,240
然后我们将使用 U 来

553
00:23:24,240 --> 00:23:26,940
保留我们学到的 U 并

554
00:23:26,940 --> 00:23:28,500
执行干预，

555
00:23:28,500 --> 00:23:29,880
因此

556
00:23:29,880 --> 00:23:32,340
反制也可以被视为

557
00:23:32,340 --> 00:23:34,980
及时的干预，其中我们

558
00:23:34,980 --> 00:23:36,960
知道环境环境

559
00:23:36,960 --> 00:23:40,620
变量 U1 U2 和 u4 在该特定

560
00:23:40,620 --> 00:23:43,039
时刻

561
00:23:43,200 --> 00:23:44,340
以及

562
00:23:44,340 --> 00:23:46,679
缺少的步骤是什么，

563
00:23:46,679 --> 00:23:49,440
那么在该特定情况下 X4 在 X3 处等于

564
00:23:49,440 --> 00:23:50,780


565
00:23:50,780 --> 00:23:53,280
另一个数据点

566
00:23:53,280 --> 00:23:55,980
现在我们可以计算

567
00:23:55,980 --> 00:23:57,120
相关性

568
00:23:57,120 --> 00:23:59,520
以及我们在图上的路径上执行的相关性

569
00:23:59,520 --> 00:24:02,039
其中我们

570
00:24:02,039 --> 00:24:04,440
已经使用

571
00:24:04,440 --> 00:24:06,659
我们

572
00:24:06,659 --> 00:24:10,140
在溯因步骤中学到的环境变量进行了干预，

573
00:24:10,140 --> 00:24:14,419
这是一个反事实推理，

574
00:24:15,480 --> 00:24:18,000
这是因果推理当前介绍的最后一张幻灯片

575
00:24:18,000 --> 00:24:20,159


576
00:24:20,159 --> 00:24:21,720
，基本上是关于结构学习的，

577
00:24:21,720 --> 00:24:23,880
基本上是我所说的一切

578
00:24:23,880 --> 00:24:27,360
到目前为止，我们依赖于这样一个事实：我们知道

579
00:24:27,360 --> 00:24:29,700


580
00:24:29,700 --> 00:24:31,500
数据点之间的因果关系，因此我们知道

581
00:24:31,500 --> 00:24:33,120
图表的结构，我们知道哪个变量

582
00:24:33,120 --> 00:24:34,860
影响哪个变量，

583
00:24:34,860 --> 00:24:37,260
我们一般知道箭头，

584
00:24:37,260 --> 00:24:39,659
但实际上这实际上并不

585
00:24:39,659 --> 00:24:42,900
总是可能的，所以我们

586
00:24:42,900 --> 00:24:45,419


587
00:24:45,419 --> 00:24:47,400
大多数时候我们无法访问因果图，实际上

588
00:24:47,400 --> 00:24:49,919
从数据中学习最好的因果图仍然是

589
00:24:49,919 --> 00:24:51,840
一个悬而未决的问题，我们正在改进，

590
00:24:51,840 --> 00:24:53,880
我们正在变得更好，但

591
00:24:53,880 --> 00:24:57,299
如何准确地执行这个任务

592
00:24:57,299 --> 00:24:58,380
呃

593
00:24:58,380 --> 00:25:01,140
仍然是一个悬而未决的问题

594
00:25:01,140 --> 00:25:03,179
正如我所说，基本上目标是

595
00:25:03,179 --> 00:25:04,740
从观测数据中引用理事会关系，

596
00:25:04,740 --> 00:25:07,380
因此给定一个数据集，

597
00:25:07,380 --> 00:25:09,780
我们希望推断出

598
00:25:09,780 --> 00:25:12,179
描述

599
00:25:12,179 --> 00:25:14,460
系统与数据集变量之间的连接性的有向精确图，

600
00:25:14,460 --> 00:25:15,960


601
00:25:15,960 --> 00:25:17,700
例如这里我们有一个例子

602
00:25:17,700 --> 00:25:19,440
我想我们

603
00:25:19,440 --> 00:25:22,860
都熟悉，谢谢呃，因为

604
00:25:22,860 --> 00:25:25,080
大流行，所以我们有这四个

605
00:25:25,080 --> 00:25:28,799
变量，年龄，疫苗住院

606
00:25:28,799 --> 00:25:31,380
和 CT，

607
00:25:31,380 --> 00:25:33,600
我们想推断

608
00:25:33,600 --> 00:25:36,059
这些变量之间的因果关系，

609
00:25:36,059 --> 00:25:37,980
例如，我们想直接

610
00:25:37,980 --> 00:25:40,260
从数据中学习概率 一个

611
00:25:40,260 --> 00:25:43,080
人住院的情况取决于

612
00:25:43,080 --> 00:25:45,419
他的年龄以及是否接种了

613
00:25:45,419 --> 00:25:49,760
疫苗等等，所以

614
00:25:51,299 --> 00:25:55,020
这是冗长的介绍的结尾，

615
00:25:55,020 --> 00:25:58,080
但是呃我希望它

616
00:25:58,080 --> 00:26:00,179
足够清楚，我希望我给了像

617
00:26:00,179 --> 00:26:02,039
了解

618
00:26:02,039 --> 00:26:05,159
论文的基本结果

619
00:26:05,159 --> 00:26:07,740
现在我们可以开始研究问题

620
00:26:07,740 --> 00:26:09,059
所以研究问题

621
00:26:09,059 --> 00:26:10,440


622
00:26:10,440 --> 00:26:12,900
首先如下我想看看

623
00:26:12,900 --> 00:26:15,299
创造性编码是否可以用于

624
00:26:15,299 --> 00:26:16,980
执行因果推理

625
00:26:16,980 --> 00:26:20,100
所以到目前为止只

626
00:26:20,100 --> 00:26:22,380
使用了操作性编码 在贝叶斯网络中执行计算相关性，

627
00:26:22,380 --> 00:26:25,020


628
00:26:25,020 --> 00:26:27,419
最大的问题是我们能否以生物学上合理的方式超越

629
00:26:27,419 --> 00:26:29,400
相关性、模型干预和

630
00:26:29,400 --> 00:26:31,679
反事实，

631
00:26:31,679 --> 00:26:32,760


632
00:26:32,760 --> 00:26:34,380
所以呃，

633
00:26:34,380 --> 00:26:36,120
以一种简单

634
00:26:36,120 --> 00:26:39,059
直观的方式，让我们只与

635
00:26:39,059 --> 00:26:40,740
神经元一起玩 并且不触及例如

636
00:26:40,740 --> 00:26:43,740
图形的巨大结构，

637
00:26:43,740 --> 00:26:46,380
更具体地说，

638
00:26:46,380 --> 00:26:48,299
问题是我们能否定义基于

639
00:26:48,299 --> 00:26:51,000
操作编码的结构因果

640
00:26:51,000 --> 00:26:52,740
模型来执行干预和

641
00:26:52,740 --> 00:26:55,320
反事实

642
00:26:55,320 --> 00:26:58,380
第二个问题是，

643
00:26:58,380 --> 00:27:00,179
正如我所说，拥有一个结构自定义

644
00:27:00,179 --> 00:27:02,159
模型假设我们 知道

645
00:27:02,159 --> 00:27:04,260
逃避网络的结构，

646
00:27:04,260 --> 00:27:07,919
所以我们假设我们有箭头，

647
00:27:07,919 --> 00:27:09,960
我们可以超越这个并使用创造性的

648
00:27:09,960 --> 00:27:11,520
编码网络来学习

649
00:27:11,520 --> 00:27:14,418
图的因果结构，

650
00:27:16,140 --> 00:27:18,900
基本上对这两个问题给出肯定的答案

651
00:27:18,900 --> 00:27:21,120
将允许我们

652
00:27:21,120 --> 00:27:23,120
使用预测编码 一种端到端

653
00:27:23,120 --> 00:27:26,039
因果推理方法，基本上

654
00:27:26,039 --> 00:27:28,740
采用一个数据集，并允许我们直接从该数据集测试

655
00:27:28,740 --> 00:27:30,419
干预和反事实

656
00:27:30,419 --> 00:27:34,820
预测，

657
00:27:36,840 --> 00:27:39,299
所以让我们解决

658
00:27:39,299 --> 00:27:40,740
第一个问题，即因果推理

659
00:27:40,740 --> 00:27:42,419
振动编码，这也是

660
00:27:42,419 --> 00:27:45,120
给出的部分 基本上是论文的标题，

661
00:27:45,120 --> 00:27:46,740


662
00:27:46,740 --> 00:27:48,539
在这里我将展示如何执行

663
00:27:48,539 --> 00:27:50,760
相关操作编码，这是

664
00:27:50,760 --> 00:27:52,440
已知的

665
00:27:52,440 --> 00:27:54,419
嗯，以及如何执行介入

666
00:27:54,419 --> 00:27:56,760
查询，我认为这

667
00:27:56,760 --> 00:28:01,140
是论文的真正问题，

668
00:28:01,140 --> 00:28:03,900
所以这里是一个因果关系 图是

669
00:28:03,900 --> 00:28:05,700
我们拥有的常用图，

670
00:28:05,700 --> 00:28:07,260


671
00:28:07,260 --> 00:28:09,240
这里是相应的创意

672
00:28:09,240 --> 00:28:11,760
编码模型，因此轴是

673
00:28:11,760 --> 00:28:13,980
潜在变量，对应于

674
00:28:13,980 --> 00:28:18,000
神经网络模型中的神经元，

675
00:28:18,000 --> 00:28:20,760
黑箭头

676
00:28:20,760 --> 00:28:22,740
从一个神经元的预测信息传递过来

677
00:28:22,740 --> 00:28:25,559
到层次结构中的下一个

678
00:28:25,559 --> 00:28:28,500
节点，每个顶点也有一个错误

679
00:28:28,500 --> 00:28:31,140
神经元，它将信息向上传递到

680
00:28:31,140 --> 00:28:32,820
层次结构，因此每个错误的信息

681
00:28:32,820 --> 00:28:36,480
都会传递到层次结构中上层的值节点

682
00:28:36,480 --> 00:28:39,120
，并基本上告诉

683
00:28:39,120 --> 00:28:41,400
它自我纠正以进行更改 预测

684
00:28:41,400 --> 00:28:43,760


685
00:28:44,700 --> 00:28:46,559
以便使用预测编码执行相关性，

686
00:28:46,559 --> 00:28:48,840
您所要做的

687
00:28:48,840 --> 00:28:50,400
就是进行观察并

688
00:28:50,400 --> 00:28:52,620
简单地固定特定神经元的值，

689
00:28:52,620 --> 00:28:53,820


690
00:28:53,820 --> 00:28:55,200
因此如果您想计算

691
00:28:55,200 --> 00:28:58,740
给定 X3 等于 S3 的 X4 的概率，

692
00:28:58,740 --> 00:29:02,340
我们只需： 采用 X3 并将其固定到

693
00:29:02,340 --> 00:29:04,380
S3，使其不再改变，

694
00:29:04,380 --> 00:29:08,159
并运行能量最小化，

695
00:29:08,159 --> 00:29:09,720
该模型

696
00:29:09,720 --> 00:29:12,659


697
00:29:12,659 --> 00:29:16,380
通过

698
00:29:16,380 --> 00:29:18,419
自由能变化的最小化更新轴 uh 来最小化，使模型

699
00:29:18,419 --> 00:29:20,820
收敛到一个解决方案 对于这个问题，

700
00:29:20,820 --> 00:29:22,919


701
00:29:22,919 --> 00:29:27,179
给定 X3 的 X4 的概率或期望值等于 3。

702
00:29:27,179 --> 00:29:29,340
但是我现在如何在

703
00:29:29,340 --> 00:29:31,679
不作用于图形结构的情况下进行干预，

704
00:29:31,679 --> 00:29:33,419


705
00:29:33,419 --> 00:29:35,640
这基本上是

706
00:29:35,640 --> 00:29:37,679
本文的第一个想法

707
00:29:37,679 --> 00:29:39,960
哦，这仍然是如何 执行

708
00:29:39,960 --> 00:29:43,260
相关性，因此固定 S3 等于 X3 是

709
00:29:43,260 --> 00:29:45,600
算法的第一步，

710
00:29:45,600 --> 00:29:47,220
第二步是通过

711
00:29:47,220 --> 00:29:50,539
最小化自由能的变化来获得轴，

712
00:29:51,240 --> 00:29:53,340
理论上

713
00:29:53,340 --> 00:29:55,200
相当于删除那些

714
00:29:55,200 --> 00:29:56,220
箭头

715
00:29:56,220 --> 00:29:57,659
并回答问题

716
00:29:57,659 --> 00:29:59,279
概率的干预

717
00:29:59,279 --> 00:30:02,399
通过执行干预来计算 X4 的值，所以 X3

718
00:30:02,399 --> 00:30:04,860
等于 3 个命令式编码可以

719
00:30:04,860 --> 00:30:07,080
执行如下，

720
00:30:07,080 --> 00:30:09,840
所以我将在这里编写算法，

721
00:30:09,840 --> 00:30:13,140
所以首先在相关性中，您将 S3

722
00:30:13,140 --> 00:30:17,039
等于 iFix X3 等于

723
00:30:17,039 --> 00:30:18,720
您得到的观察结果

724
00:30:18,720 --> 00:30:21,299
那么这是重要的一步，

725
00:30:21,299 --> 00:30:24,059
您必须不再干预图表，

726
00:30:24,059 --> 00:30:26,700
而是干预预测误差并将

727
00:30:26,700 --> 00:30:28,980
其修复为零，

728
00:30:28,980 --> 00:30:31,020
使预测误差等于零

729
00:30:31,020 --> 00:30:32,480
基本上

730
00:30:32,480 --> 00:30:36,179
使呃

731
00:30:36,179 --> 00:30:38,460
向层次结构发送无意义的信息，或者实际上

732
00:30:38,460 --> 00:30:40,200
不发送任何信息 层次结构，

733
00:30:40,200 --> 00:30:41,880
因为它基本上告诉您

734
00:30:41,880 --> 00:30:44,659
预测总是正确的

735
00:30:44,659 --> 00:30:48,120
，第三步是像我们

736
00:30:48,120 --> 00:30:50,220
之前所做的那样，通过最小化自由能的变化来更新轴

737
00:30:50,220 --> 00:30:52,919
无约束轴或 X1 X2 X4，

738
00:30:52,919 --> 00:30:55,679


739
00:30:55,679 --> 00:30:59,039
正如我现在将展示的那样或通过

740
00:30:59,039 --> 00:31:00,840
简单地进行实验 将

741
00:31:00,840 --> 00:31:02,399
预测误差设置

742
00:31:02,399 --> 00:31:05,120
为零的这个小技巧

743
00:31:05,640 --> 00:31:08,220
会阻止我们

744
00:31:08,220 --> 00:31:10,320


745
00:31:10,320 --> 00:31:13,620
像微积分理论那样实际对图的结构采取行动，并

746
00:31:13,620 --> 00:31:16,919


747
00:31:16,919 --> 00:31:19,140
通过简单地执行干预后推断出缺失的变量

748
00:31:19,140 --> 00:31:22,640
自由能最小化的畸变

749
00:31:24,659 --> 00:31:26,580
反事实推理怎么样

750
00:31:26,580 --> 00:31:28,080


751
00:31:28,080 --> 00:31:30,539
一旦我们

752
00:31:30,539 --> 00:31:34,740
定义了如何进行干预，反事实推理实际上很容易

753
00:31:34,740 --> 00:31:36,539
，这是因为正如我们之前所看到的，在

754
00:31:36,539 --> 00:31:38,640


755
00:31:38,640 --> 00:31:40,380


756
00:31:40,380 --> 00:31:44,360
推断出过去的情况后执行反事实类似于在过去的情况下执行干预

757
00:31:44,360 --> 00:31:48,120
不可观察的不可观察的变量，

758
00:31:48,120 --> 00:31:49,620
所以

759
00:31:49,620 --> 00:31:51,480
正如您在我

760
00:31:51,480 --> 00:31:53,520
之前展示的关于绑架动作和

761
00:31:53,520 --> 00:31:56,039
预测步骤的图中看到的那样，动作和

762
00:31:56,039 --> 00:31:58,320
预测步骤没有这

763
00:31:58,320 --> 00:31:59,640
两个箭头，

764
00:31:59,640 --> 00:32:02,580
它们被删除了漂亮的编码

765
00:32:02,580 --> 00:32:06,299
允许我们将这个呃的箭头保留

766
00:32:06,299 --> 00:32:08,279
在 该图

767
00:32:08,279 --> 00:32:11,340
并通过

768
00:32:11,340 --> 00:32:13,380
简单地执行溯因步骤来执行反事实，就像

769
00:32:13,380 --> 00:32:14,640
之前所做的那样，在

770
00:32:14,640 --> 00:32:16,679
操作步骤中，我们只需对

771
00:32:16,679 --> 00:32:18,600
单个节点执行干预，

772
00:32:18,600 --> 00:32:21,240
因此我们修复值节点并将

773
00:32:21,240 --> 00:32:24,240
误差设置为零

774
00:32:24,240 --> 00:32:26,399
并运行能量最小化

775
00:32:26,399 --> 00:32:27,960
最小化自由能的持续时间来

776
00:32:27,960 --> 00:32:30,679
计算预测，

777
00:32:32,399 --> 00:32:36,299
所以我认为这就像一种简单而

778
00:32:36,299 --> 00:32:39,840
优雅的方法来执行干预

779
00:32:39,840 --> 00:32:42,899
和反事实，

780
00:32:42,899 --> 00:32:44,880
嗯，是的，所以我认为我们

781
00:32:44,880 --> 00:32:46,500
现在必须展示的是它在实践中

782
00:32:46,500 --> 00:32:48,720
是否有效，我们 有几个

783
00:32:48,720 --> 00:32:49,919
实验

784
00:32:49,919 --> 00:32:52,440
，我现在将向您展示两个

785
00:32:52,440 --> 00:32:54,240
不同的实验，第一个实验

786
00:32:54,240 --> 00:32:57,179
只是概念验证实验，

787
00:32:57,179 --> 00:33:01,020
表明在操作编码中

788
00:33:01,020 --> 00:33:02,480
能够执行

789
00:33:02,480 --> 00:33:06,120
干预和反事实

790
00:33:06,120 --> 00:33:08,700
，第二个实验实际上显示了一个

791
00:33:08,700 --> 00:33:11,220
简单的应用程序 如何

792
00:33:11,220 --> 00:33:13,440
使用干预查询来提高

793
00:33:13,440 --> 00:33:16,260


794
00:33:16,260 --> 00:33:18,360
特定类型的操作编码

795
00:33:18,360 --> 00:33:20,940
网络（即完全

796
00:33:20,940 --> 00:33:22,080
连接模型的分类任务）的性能，

797
00:33:22,080 --> 00:33:24,659
让我们从第一个开始，

798
00:33:24,659 --> 00:33:27,679
那么我们如何完成这项任务，因此给定一个

799
00:33:27,679 --> 00:33:30,360
结构委员会模型，

800
00:33:30,360 --> 00:33:33,360
我们 生成训练数据，我们用它

801
00:33:33,360 --> 00:33:35,760
来学习权重，以便学习

802
00:33:35,760 --> 00:33:39,480
结构 Kaza 模型的功能，

803
00:33:39,480 --> 00:33:42,779
然后我们

804
00:33:42,779 --> 00:33:44,399
为干预查询和

805
00:33:44,399 --> 00:33:46,080
反派查询生成测试数据，

806
00:33:46,080 --> 00:33:48,000
并展示我们是否能够

807
00:33:48,000 --> 00:33:51,360
收敛到正确的测试数据 使用

808
00:33:51,360 --> 00:33:53,340
创造性编码

809
00:33:53,340 --> 00:33:54,779
，

810
00:33:54,779 --> 00:33:57,240
例如，这两个图中的 uh

811
00:33:57,240 --> 00:33:58,860
代表了该特定图的干预

812
00:33:58,860 --> 00:34:00,600
干预和反事实查询，该图

813
00:34:00,600 --> 00:34:03,539
是

814
00:34:03,539 --> 00:34:05,880
蝴蝶偏差图，该图

815
00:34:05,880 --> 00:34:08,280
经常用于 uh 中测试

816
00:34:08,280 --> 00:34:10,859
是否存在因果推理是否

817
00:34:10,859 --> 00:34:12,179
干预和反事实

818
00:34:12,179 --> 00:34:15,540
技术 工作就这么简单，但

819
00:34:15,540 --> 00:34:18,000
在论文中你可以找到很多

820
00:34:18,000 --> 00:34:20,760
不同的图表，但总的来说，这

821
00:34:20,760 --> 00:34:22,800
两个图表这两个图表明该

822
00:34:22,800 --> 00:34:26,940
方法有效，表明

823
00:34:26,940 --> 00:34:27,918


824
00:34:27,918 --> 00:34:32,219


825
00:34:32,219 --> 00:34:33,960


826
00:34:33,960 --> 00:34:37,399
我们我们的干预性反事实量之间的平均绝对误差 计算并且原始图中的干预量和

827
00:34:37,399 --> 00:34:39,780
反事实量

828
00:34:39,780 --> 00:34:41,460


829
00:34:41,460 --> 00:34:43,800
彼此接近，因此误差

830
00:34:43,800 --> 00:34:45,800
相当小

831
00:34:45,800 --> 00:34:49,139
第二个实验是呃基本上是

832
00:34:49,139 --> 00:34:51,239
我在早期论文中提出的实验的扩展，

833
00:34:51,239 --> 00:34:54,540
该实验是

834
00:34:54,540 --> 00:34:56,460
对任意图拓扑的学习

835
00:34:56,460 --> 00:34:59,040
我去年

836
00:34:59,040 --> 00:35:01,080
在那篇论文中写的，我

837
00:35:01,080 --> 00:35:04,200
基本上提出了这种

838
00:35:04,200 --> 00:35:06,060
网络作为概念证明，它是一个

839
00:35:06,060 --> 00:35:08,160
完全连接的网络，

840
00:35:08,160 --> 00:35:11,579
通常是您可以执行机器学习实验的最糟糕的神经网络，

841
00:35:11,579 --> 00:35:13,500


842
00:35:13,500 --> 00:35:15,960
因为

843
00:35:15,960 --> 00:35:20,520
给我们一个固定的 一组神经元

844
00:35:20,520 --> 00:35:23,660
基本上

845
00:35:23,760 --> 00:35:26,400
每对神经元都由

846
00:35:26,400 --> 00:35:28,680
两个不同的突触连接，因此

847
00:35:28,680 --> 00:35:31,200
它是最多的，

848
00:35:31,200 --> 00:35:33,359
通常是具有最高复杂性的模型，

849
00:35:33,359 --> 00:35:34,619


850
00:35:34,619 --> 00:35:36,300
好处是，由于您有

851
00:35:36,300 --> 00:35:37,859
很多循环，所以模型非常

852
00:35:37,859 --> 00:35:39,599
灵活 从某种意义上说，您可以

853
00:35:39,599 --> 00:35:42,480
在切碎的图像、

854
00:35:42,480 --> 00:35:45,359
数据点及其标签上训练它，但是

855
00:35:45,359 --> 00:35:47,400
由于返回的信息，您可以查询它的方式

856
00:35:47,400 --> 00:35:50,640
是呃，您可以

857
00:35:50,640 --> 00:35:52,140
通过很多不同的方式查询，所以 您

858
00:35:52,140 --> 00:35:54,060
可以形成分类任务，在其中

859
00:35:54,060 --> 00:35:55,980
提供图像并运行

860
00:35:55,980 --> 00:35:57,480
能量最小化并获取标签，

861
00:35:57,480 --> 00:35:59,400
但您也可以执行

862
00:35:59,400 --> 00:36:01,320
生成任务，在其中给

863
00:36:01,320 --> 00:36:03,060
标签运行能量最小化并

864
00:36:03,060 --> 00:36:05,220
获取您可以执行的图像，例如

865
00:36:05,220 --> 00:36:06,960
图像 完成，你给

866
00:36:06,960 --> 00:36:10,260
一半的图像并收敛，然后

867
00:36:10,260 --> 00:36:12,119
收敛让模型转换到

868
00:36:12,119 --> 00:36:14,400
后半部分等等，所以

869
00:36:14,400 --> 00:36:16,440
它基本上是一个模型，可以完整地学习

870
00:36:16,440 --> 00:36:19,619
数据集的统计数据，而

871
00:36:19,619 --> 00:36:21,900
不是像专注于

872
00:36:21,900 --> 00:36:25,079
分类或 一般来说，

873
00:36:25,079 --> 00:36:27,900
这种灵活性很大，

874
00:36:27,900 --> 00:36:31,260
问题是，正因为如此，

875
00:36:31,260 --> 00:36:34,140
每个任务都不能很好地工作，

876
00:36:34,140 --> 00:36:35,820
所以你可以做很多不同的事情，

877
00:36:35,820 --> 00:36:38,579
但没有一个做得很好，

878
00:36:38,579 --> 00:36:39,960


879
00:36:39,960 --> 00:36:42,480
在这里我想展示如何使用

880
00:36:42,480 --> 00:36:44,099
介入查询而不是

881
00:36:44,099 --> 00:36:46,740
标准的关联查询或

882
00:36:46,740 --> 00:36:48,119
条件查询

883
00:36:48,119 --> 00:36:49,980
稍微改善了这些分类任务的结果，

884
00:36:49,980 --> 00:36:51,960


885
00:36:51,960 --> 00:36:54,000
那么

886
00:36:54,000 --> 00:36:57,599


887
00:36:57,599 --> 00:37:01,079
这些任务的测试准确性不那么高的推测原因是什么，

888
00:37:01,079 --> 00:37:03,180
前两个原因是模型

889
00:37:03,180 --> 00:37:05,640
分散了注意力 在纠正每一个

890
00:37:05,640 --> 00:37:07,920
呃每一个错误时，基本上你

891
00:37:07,920 --> 00:37:09,420
呈现一个图像，你想

892
00:37:09,420 --> 00:37:11,579
获得一个标签，但模型实际上正在

893
00:37:11,579 --> 00:37:13,859
更新自身以预测

894
00:37:13,859 --> 00:37:16,320
图像中的错误

895
00:37:16,320 --> 00:37:18,480
，第二个原因是我

896
00:37:18,480 --> 00:37:21,119
所说的 结构太

897
00:37:21,119 --> 00:37:24,540
复杂了，所以从

898
00:37:24,540 --> 00:37:27,079
葡萄干奥卡姆剃刀

899
00:37:27,079 --> 00:37:28,800
论证的结果来看，

900
00:37:28,800 --> 00:37:30,720
这是你能拥有的最糟糕的模型，所以

901
00:37:30,720 --> 00:37:32,160
每次你有一个适合

902
00:37:32,160 --> 00:37:33,960
数据集的模型时，该模型都会

903
00:37:33,960 --> 00:37:35,579
比正在运行的模型复杂。 是

904
00:37:35,579 --> 00:37:37,560
首选，

905
00:37:37,560 --> 00:37:40,560
但总的来说，呃只是为了研究

906
00:37:40,560 --> 00:37:41,400
它，

907
00:37:41,400 --> 00:37:43,380
这个想法是可以在这个模型中查询干预

908
00:37:43,380 --> 00:37:44,820
措施，用于提高

909
00:37:44,820 --> 00:37:46,859
那些完全

910
00:37:46,859 --> 00:37:48,599
连接的模型的性能，

911
00:37:48,599 --> 00:37:51,060
答案是肯定的，

912
00:37:51,060 --> 00:37:53,160
所以这是我如何执行干预

913
00:37:53,160 --> 00:37:55,619
查询，所以 我向网络呈现一幅图像，

914
00:37:55,619 --> 00:37:56,640


915
00:37:56,640 --> 00:37:59,460
我将像素的误差修正

916
00:37:59,460 --> 00:38:01,560
为 0，这样该误差就不会

917
00:38:01,560 --> 00:38:03,180
在网络中传播，

918
00:38:03,180 --> 00:38:05,700
然后我计算标签

919
00:38:05,700 --> 00:38:08,400
，如您所见，准确度有所提高，

920
00:38:08,400 --> 00:38:11,339
例如使用 89

921
00:38:11,339 --> 00:38:13,380
创意编码网络的标准查询方法

922
00:38:13,380 --> 00:38:16,800
到 92，这是干预后的准确性，

923
00:38:16,800 --> 00:38:19,020


924
00:38:19,020 --> 00:38:21,540
对于时尚手段也是如此，

925
00:38:21,540 --> 00:38:24,420
我认为一个非常合法的批评家，

926
00:38:24,420 --> 00:38:26,940
可能每个人

927
00:38:26,940 --> 00:38:28,920
在看到这些图时都会想，可以，你可以

928
00:38:28,920 --> 00:38:32,099
从 89 的手段改进 到 92

929
00:38:32,099 --> 00:38:36,180
基本上仍然很糟糕，是的，这是真的，

930
00:38:36,180 --> 00:38:38,400
我实际上在后面的幻灯片中，我

931
00:38:38,400 --> 00:38:40,619
将展示如何对

932
00:38:40,619 --> 00:38:42,660
这个全连接模型的结构采取行动，这

933
00:38:42,660 --> 00:38:43,859


934
00:38:43,859 --> 00:38:46,500
将进一步改善结果，直到

935
00:38:46,500 --> 00:38:48,480
他们达到 丰富的呃

936
00:38:48,480 --> 00:38:50,820
性能当然还没有接近最

937
00:38:50,820 --> 00:38:52,560
先进的性能，

938
00:38:52,560 --> 00:38:55,320
但它仍然在上升，但还没有达到

939
00:38:55,320 --> 00:38:57,380
基本上可以接受的水平

940
00:38:57,380 --> 00:39:01,760
肯沃斯调查调查

941
00:39:02,040 --> 00:39:04,980
所以是的所以这是关于

942
00:39:04,980 --> 00:39:08,400
使用创造性编码进行因果推理的部分

943
00:39:08,400 --> 00:39:10,920
我想总结一下，我可以说，

944
00:39:10,920 --> 00:39:15,060


945
00:39:15,060 --> 00:39:17,640
我刚刚展示的结果中有趣的部分是，

946
00:39:17,640 --> 00:39:19,859
我展示了操作编码能够

947
00:39:19,859 --> 00:39:22,560
以非常简单

948
00:39:22,560 --> 00:39:24,780
和直观的方式执行干预，因为你不必

949
00:39:24,780 --> 00:39:26,280
采取行动 旧图的结构

950
00:39:26,280 --> 00:39:28,740
有时这些

951
00:39:28,740 --> 00:39:31,079
函数不可用

952
00:39:31,079 --> 00:39:34,020
等等，但你只需要

953
00:39:34,020 --> 00:39:36,140


954
00:39:36,140 --> 00:39:39,780
干预单个神经元研究

955
00:39:39,780 --> 00:39:41,640
预测误差为零

956
00:39:41,640 --> 00:39:44,220
并执行能量最小化

957
00:39:44,220 --> 00:39:46,619
过程，

958
00:39:46,619 --> 00:39:49,200
这些扩展是 允许我们

959
00:39:49,200 --> 00:39:51,240
定义基于创造性编码的结构

960
00:39:51,240 --> 00:39:52,920
因果模型

961
00:39:52,920 --> 00:39:54,920
现在我们进入

962
00:39:54,920 --> 00:39:57,900
工作的第二部分，这是关于

963
00:39:57,900 --> 00:40:01,700
结构结构学习的，

964
00:40:02,000 --> 00:40:05,099
所以正如我所说的指令学习处理

965
00:40:05,099 --> 00:40:07,260


966
00:40:07,260 --> 00:40:09,720


967
00:40:09,720 --> 00:40:11,880
从观察数据中学习模型的因果结构的问题，

968
00:40:11,880 --> 00:40:13,800
这是 实际上，没有一个问题已经

969
00:40:13,800 --> 00:40:17,760
存在了几十年，

970
00:40:17,760 --> 00:40:21,359
而且直到

971
00:40:21,359 --> 00:40:24,000
几年前才一直使用组合

972
00:40:24,000 --> 00:40:25,560
搜索方法来解决

973
00:40:25,560 --> 00:40:26,640
这些社区

974
00:40:26,640 --> 00:40:29,280
研究方法的问题在于，它们的

975
00:40:29,280 --> 00:40:32,880
复杂性呈双倍指数增长，

976
00:40:32,880 --> 00:40:34,740
因此一旦数据变成

977
00:40:34,740 --> 00:40:36,780
多倍， 维度和

978
00:40:36,780 --> 00:40:39,920
你想要学习的 Bison 图的

979
00:40:39,920 --> 00:40:42,300
大小会增长，学习

980
00:40:42,300 --> 00:40:46,680
它的速度非常慢，

981
00:40:46,680 --> 00:40:48,780


982
00:40:48,780 --> 00:40:51,000
几年前在 2018 年的一份新报纸上

983
00:40:51,000 --> 00:40:53,540


984
00:40:53,839 --> 00:40:55,920
实际上出现的新解决方案表明，可以在

985
00:40:55,920 --> 00:40:57,900
不使用

986
00:40:57,900 --> 00:40:59,940
组合器研究方法，但通过使用

987
00:40:59,940 --> 00:41:01,619
基于梯度的方法，

988
00:41:01,619 --> 00:41:05,280
这基本上是

989
00:41:05,280 --> 00:41:07,320
一般的技术问题，因为现在

990
00:41:07,320 --> 00:41:08,820
你可以简单地

991
00:41:08,820 --> 00:41:10,980
对参数进行先验，这是

992
00:41:10,980 --> 00:41:12,420
我要

993
00:41:12,420 --> 00:41:14,700
定义得更好一点的优先目的 在这张幻灯片中

994
00:41:14,700 --> 00:41:15,599


995
00:41:15,599 --> 00:41:18,180
运行梯度下降，即使你

996
00:41:18,180 --> 00:41:19,740
有一个双倍三倍的模型，

997
00:41:19,740 --> 00:41:20,820
大小是

998
00:41:20,820 --> 00:41:23,640
呃，算法仍然非常

999
00:41:23,640 --> 00:41:25,440
快，

1000
00:41:25,440 --> 00:41:28,260
因此这篇论文是一个

1001
00:41:28,260 --> 00:41:31,200
这是是的，我认为它是一种新的

1002
00:41:31,200 --> 00:41:33,180
，我认为已经有了 600 次

1003
00:41:33,180 --> 00:41:35,099
引用或诸如此类的事情，

1004
00:41:35,099 --> 00:41:37,140
我现在看到的每一篇

1005
00:41:37,140 --> 00:41:38,720
关于咨询朋友和学习

1006
00:41:38,720 --> 00:41:42,000
图表结构的论文都使用他们的方法，

1007
00:41:42,000 --> 00:41:44,820
它只是改变了一点，

1008
00:41:44,820 --> 00:41:46,980
他们发现更快或稍微更好的

1009
00:41:46,980 --> 00:41:49,440
推理方法，但他们仍然都使用

1010
00:41:49,440 --> 00:41:53,760
在本文定义之前，我

1011
00:41:53,760 --> 00:41:56,460
也这样做，我们也这样做，

1012
00:41:56,460 --> 00:41:58,859
所以在这里我们会找到一个新的量，

1013
00:41:58,859 --> 00:42:01,500
即代理矩阵，代理

1014
00:42:01,500 --> 00:42:03,480
矩阵只是一个对

1015
00:42:03,480 --> 00:42:06,359
模型连接进行编码的矩阵，因此它是一个

1016
00:42:06,359 --> 00:42:08,520
二进制矩阵，

1017
00:42:08,520 --> 00:42:10,920
一般来说 是一个二元矩阵，那么

1018
00:42:10,920 --> 00:42:12,180
当然，当您进行基于梯度的

1019
00:42:12,180 --> 00:42:14,880
优化时，您可以使其连续，

1020
00:42:14,880 --> 00:42:16,800
然后在某个点上有一些阈值，该阈值

1021
00:42:16,800 --> 00:42:19,800
基本上会杀死边缘或

1022
00:42:19,800 --> 00:42:21,480
将其设置为 1，

1023
00:42:21,480 --> 00:42:27,780
并且如果我们有，则 M3 IJ 等于 1

1024
00:42:27,780 --> 00:42:30,540
如果贝叶斯图是

1025
00:42:30,540 --> 00:42:35,040
从顶点 I 到顶点 J 的边，

1026
00:42:35,040 --> 00:42:37,380
否则为零，例如，这里的代理

1027
00:42:37,380 --> 00:42:39,540
矩阵代表了

1028
00:42:39,540 --> 00:42:42,780
该视觉网络的连接结构

1029
00:42:42,780 --> 00:42:44,040
，

1030
00:42:44,040 --> 00:42:46,079
基本上，该方法

1031
00:42:46,079 --> 00:42:48,780
解决了我们想要了解的两个

1032
00:42:48,780 --> 00:42:51,000
关于学习

1033
00:42:51,000 --> 00:42:53,460
结构的问题 网络方程的

1034
00:42:53,460 --> 00:42:54,780
想法是，我们从一个完全

1035
00:42:54,780 --> 00:42:57,200
连接的模型开始，这个模型在

1036
00:42:57,200 --> 00:43:00,240
概念上是相似的，实际上

1037
00:43:00,240 --> 00:43:02,220
相当于

1038
00:43:02,220 --> 00:43:04,020
我之前定义的操作编码网络，它是完全

1039
00:43:04,020 --> 00:43:06,480
连接的，所以你有很多

1040
00:43:06,480 --> 00:43:08,640
顶点和每对顶点

1041
00:43:08,640 --> 00:43:10,920
由两个不同的边通过呃连接

1042
00:43:10,920 --> 00:43:13,319
，您只想修剪不需要的边，

1043
00:43:13,319 --> 00:43:15,780


1044
00:43:15,780 --> 00:43:18,540
因此它可以被视为一种

1045
00:43:18,540 --> 00:43:20,819
执行模型缩减的方法，您从

1046
00:43:20,819 --> 00:43:22,020
一个大模型开始，并且想要使其

1047
00:43:22,020 --> 00:43:22,800
变小，

1048
00:43:22,800 --> 00:43:25,800
那么什么是 减少模型的第一个要素

1049
00:43:25,800 --> 00:43:28,260
当然是稀疏

1050
00:43:28,260 --> 00:43:29,220
城市，

1051
00:43:29,220 --> 00:43:31,619
每个人用来

1052
00:43:31,619 --> 00:43:33,839
使模型更加稀疏的先验是

1053
00:43:33,839 --> 00:43:36,480
拉普拉斯先验，在机器学习中

1054
00:43:36,480 --> 00:43:38,880
简称为 L1 范数

1055
00:43:38,880 --> 00:43:40,920
，这里定义的解决方案是

1056
00:43:40,920 --> 00:43:43,980
我

1057
00:43:43,980 --> 00:43:46,740
之前提到的这篇论文提出的是

1058
00:43:46,740 --> 00:43:49,319
在上面添加第二个先验，这

1059
00:43:49,319 --> 00:43:53,359
可能是

1060
00:43:53,359 --> 00:43:55,980
贝叶斯网络最大的呃特征，

1061
00:43:55,980 --> 00:43:57,780
你想要在其上执行因果

1062
00:43:57,780 --> 00:43:59,819
推理，你希望它们是

1063
00:43:59,819 --> 00:44:01,020
循环的

1064
00:44:01,020 --> 00:44:03,000
，基本上它们显示了 非

1065
00:44:03,000 --> 00:44:06,359
循环性可以作为先验强加在代理

1066
00:44:06,359 --> 00:44:08,160
矩阵上，

1067
00:44:08,160 --> 00:44:10,859
它在这里有这个形状，所以

1068
00:44:10,859 --> 00:44:14,640
它是矩阵的迹，是a乘以

1069
00:44:14,640 --> 00:44:18,420
a的指数，

1070
00:44:18,420 --> 00:44:21,859
其中a又是代理矩阵，

1071
00:44:21,859 --> 00:44:24,300
基本上这个量

1072
00:44:24,300 --> 00:44:27,900
是 当且仅当

1073
00:44:27,900 --> 00:44:30,480
贝叶斯网络或

1074
00:44:30,480 --> 00:44:32,819
您正在考虑的任何图形

1075
00:44:32,819 --> 00:44:35,720
是 c 单击时才等于零，

1076
00:44:37,619 --> 00:44:40,260
所以我将在一些实验中使用这些，

1077
00:44:40,260 --> 00:44:42,960
以便将这两个强制

1078
00:44:42,960 --> 00:44:45,660
在不同类型上的这两个先验

1079
00:44:45,660 --> 00:44:47,520


1080
00:44:47,520 --> 00:44:49,200
我正在尝试将它们与

1081
00:44:49,200 --> 00:44:51,540
我们之前提出的有关

1082
00:44:51,540 --> 00:44:52,740
执行因果推理和

1083
00:44:52,740 --> 00:44:55,020
操作编码的技术合并，

1084
00:44:55,020 --> 00:44:56,520
因此我将提出两个不同的

1085
00:44:56,520 --> 00:44:59,640
实验，因此一个是

1086
00:44:59,640 --> 00:45:00,960
概念证明，这是

1087
00:45:00,960 --> 00:45:03,660
中所示的标准实验 所有的结构

1088
00:45:03,660 --> 00:45:06,599
学习任务都是

1089
00:45:06,599 --> 00:45:08,880
从数据中推断出正确的贝叶斯网络，

1090
00:45:08,880 --> 00:45:11,760
然后我将在我之前展示的分类实验的基础上进行构建，

1091
00:45:11,760 --> 00:45:13,500


1092
00:45:13,500 --> 00:45:14,280


1093
00:45:14,280 --> 00:45:16,020


1094
00:45:16,020 --> 00:45:18,540
并展示这些先验实际上如何让

1095
00:45:18,540 --> 00:45:21,060
我能够改进

1096
00:45:21,060 --> 00:45:22,500
分类 准确性

1097
00:45:22,500 --> 00:45:25,500
全连接预测编码模型的测试准确性，

1098
00:45:25,500 --> 00:45:28,160


1099
00:45:29,520 --> 00:45:31,680
所以让我们开始第一个实验，即

1100
00:45:31,680 --> 00:45:33,300
推断图的结构，

1101
00:45:33,300 --> 00:45:34,980


1102
00:45:34,980 --> 00:45:37,319
所有实验都遵循该

1103
00:45:37,319 --> 00:45:39,480


1104
00:45:39,480 --> 00:45:42,060
领域所有论文中基本相同的流程，第一步是

1105
00:45:42,060 --> 00:45:45,119
生成 随机图的视觉网络，

1106
00:45:45,119 --> 00:45:46,079


1107
00:45:46,079 --> 00:45:48,359
所以基本上通常

1108
00:45:48,359 --> 00:45:50,640
每个人测试的两个随机图都是 Erdos

1109
00:45:50,640 --> 00:45:53,520
renegraph 和无比例图，

1110
00:45:53,520 --> 00:45:55,859
这样你就可以生成那些大图

1111
00:45:55,859 --> 00:45:58,680
，通常有 20 个 80 80 个

1112
00:45:58,680 --> 00:46:01,619
不同的节点和一些

1113
00:46:01,619 --> 00:46:04,619
你随机采样的边

1114
00:46:04,619 --> 00:46:06,540
你使用这个图来生成一个

1115
00:46:06,540 --> 00:46:08,280
数据集，

1116
00:46:08,280 --> 00:46:10,819
这样你就可以对

1117
00:46:10,819 --> 00:46:14,460
n个大N个数据点进行采样，你所做的就是你

1118
00:46:14,460 --> 00:46:16,859
获取他们

1119
00:46:16,859 --> 00:46:18,780
之前生成的图，然后将其

1120
00:46:18,780 --> 00:46:20,819
扔掉，你只保留数据集

1121
00:46:20,819 --> 00:46:23,099
和你的任务 现在想要解决的是要

1122
00:46:23,099 --> 00:46:25,020
学习的

1123
00:46:25,020 --> 00:46:27,420
是有一个训练算法，

1124
00:46:27,420 --> 00:46:29,819
基本上可以让你

1125
00:46:29,819 --> 00:46:32,579
检索

1126
00:46:32,579 --> 00:46:34,619
你扔掉的图的结构，

1127
00:46:34,619 --> 00:46:36,839
所以我们在这里做的方式是我们

1128
00:46:36,839 --> 00:46:38,460
处于一个完全连接的创意编码中

1129
00:46:38,460 --> 00:46:41,760
使用

1130
00:46:41,760 --> 00:46:43,800
我们之前定义的稀疏和 SQL 先验对此数据集 D 进行模型，

1131
00:46:43,800 --> 00:46:45,359


1132
00:46:45,359 --> 00:46:48,780
并查看在

1133
00:46:48,780 --> 00:46:50,760


1134
00:46:50,760 --> 00:46:53,220
修剪掉小于某个阈值的

1135
00:46:53,220 --> 00:46:55,319
机构矩阵的条目后我们收敛到的图是否实际上

1136
00:46:55,319 --> 00:46:57,599


1137
00:46:57,599 --> 00:47:00,060
类似于 初始图的情况

1138
00:47:00,060 --> 00:47:02,359


1139
00:47:02,520 --> 00:47:04,500
，也表明

1140
00:47:04,500 --> 00:47:06,599
实际上是这样，所以这是一个例子

1141
00:47:06,599 --> 00:47:09,020
，我展示了许多不同的

1142
00:47:09,020 --> 00:47:12,420
参数化和维度以及

1143
00:47:12,420 --> 00:47:15,060
论文中类似的内容，

1144
00:47:15,060 --> 00:47:16,920
但我认为这两个是最具

1145
00:47:16,920 --> 00:47:18,900
代表性的例子 带有错误

1146
00:47:18,900 --> 00:47:20,760
苗圃图和

1147
00:47:20,760 --> 00:47:23,579
具有 20 个节点的自由比例图

1148
00:47:23,579 --> 00:47:25,800
，在左侧，您可以通过

1149
00:47:25,800 --> 00:47:27,300
图看到地面，这是

1150
00:47:27,300 --> 00:47:29,339


1151
00:47:29,339 --> 00:47:30,839
随机采样的图，

1152
00:47:30,839 --> 00:47:32,599
在右侧，您可以看到

1153
00:47:32,599 --> 00:47:35,220


1154
00:47:35,220 --> 00:47:37,440
从数据中学习到的相当困难的模型

1155
00:47:37,440 --> 00:47:39,359
正如你所看到的，它们非常

1156
00:47:39,359 --> 00:47:40,500
相似，但

1157
00:47:40,500 --> 00:47:42,780
仍然不完美，所以有

1158
00:47:42,780 --> 00:47:45,000
一些错误，但

1159
00:47:45,000 --> 00:47:47,460
总的来说，结构

1160
00:47:47,460 --> 00:47:49,500
很好，我们也有一些

1161
00:47:49,500 --> 00:47:52,140
定量实验，但

1162
00:47:52,140 --> 00:47:54,000
我没有在这里展示 因为它们

1163
00:47:54,000 --> 00:47:55,740
只是带有大量数字的巨大表格

1164
00:47:55,740 --> 00:47:57,180
，我认为这

1165
00:47:57,180 --> 00:48:00,660
对于演示来说可能有点太多了，但

1166
00:48:00,660 --> 00:48:02,220
结果表明它们的表现

1167
00:48:02,220 --> 00:48:06,060
与当代方法类似，

1168
00:48:06,060 --> 00:48:07,920
因为我不得不说，就像大多数

1169
00:48:07,920 --> 00:48:10,859
质量一样 来自

1170
00:48:10,859 --> 00:48:15,799
2018 年推出的 acigliprior。

1171
00:48:16,920 --> 00:48:19,680
第二类实验是

1172
00:48:19,680 --> 00:48:21,599
我们的分类实验，正如

1173
00:48:21,599 --> 00:48:23,880
我所说，它是

1174
00:48:23,880 --> 00:48:25,560
我之前分享的实验的扩展

1175
00:48:25,560 --> 00:48:27,119
，其想法是使用结构

1176
00:48:27,119 --> 00:48:28,560
学习来改进分类

1177
00:48:28,560 --> 00:48:31,140
关于

1178
00:48:31,140 --> 00:48:33,420
均值和时尚均值数据集的分类结果，

1179
00:48:33,420 --> 00:48:36,780
从完全连接的图开始，

1180
00:48:36,780 --> 00:48:40,560
所以我所做的就是划分

1181
00:48:40,560 --> 00:48:42,839


1182
00:48:42,839 --> 00:48:46,440
神经元的完全连接的图形簇，因此 1B 簇

1183
00:48:46,440 --> 00:48:49,140
是与输入相关的簇，

1184
00:48:49,140 --> 00:48:51,900
然后是所有小的簇 我们有一些

1185
00:48:51,900 --> 00:48:55,319
特定数量的隐藏簇

1186
00:48:55,319 --> 00:48:57,720
，然后我们有标签簇，它

1187
00:48:57,720 --> 00:48:58,800
是

1188
00:48:58,800 --> 00:49:01,560


1189
00:49:01,560 --> 00:49:04,079
应该给我标签

1190
00:49:04,079 --> 00:49:06,480
预测的神经元簇的类

1191
00:49:06,480 --> 00:49:08,700
，我已经训练它们用于

1192
00:49:08,700 --> 00:49:10,980
第一次使用稀疏先验 只是这样的

1193
00:49:10,980 --> 00:49:14,099
想法是，如果我

1194
00:49:14,099 --> 00:49:16,500
从模型中修剪不需要的连接，

1195
00:49:16,500 --> 00:49:17,460


1196
00:49:17,460 --> 00:49:20,880
并学习解析器模型，

1197
00:49:20,880 --> 00:49:24,119
这是否能很好地工作，答案是否定的，它

1198
00:49:24,119 --> 00:49:25,500
不起作用，

1199
00:49:25,500 --> 00:49:28,500
原因是 最后，您

1200
00:49:28,500 --> 00:49:30,660
收敛的图实际上是

1201
00:49:30,660 --> 00:49:32,700
生成的，因此基本上模型

1202
00:49:32,700 --> 00:49:36,180
学习根据

1203
00:49:36,180 --> 00:49:38,400
标签本身来预测标签，因此它会丢弃

1204
00:49:38,400 --> 00:49:40,020
输入中的所有信息

1205
00:49:40,020 --> 00:49:42,480
，只保留标签，正如您

1206
00:49:42,480 --> 00:49:45,119
在此处看到的 标签 y 预测自身或

1207
00:49:45,119 --> 00:49:46,560
在其他实验中，当您更改

1208
00:49:46,560 --> 00:49:48,960
参数时，y 预测

1209
00:49:48,960 --> 00:49:52,520
为零，在 X1 之前，再次预测 y

1210
00:49:52,520 --> 00:49:55,980
那么这个问题的解决方案是什么

1211
00:49:55,980 --> 00:49:57,240
这个问题的解决方案

1212
00:49:57,240 --> 00:49:59,520
是我们必须

1213
00:49:59,520 --> 00:50:03,000
收敛 到非循环图

1214
00:50:03,000 --> 00:50:05,220
，所以我们必须添加一些东西来

1215
00:50:05,220 --> 00:50:08,000
防止循环，这

1216
00:50:08,000 --> 00:50:10,200
当然是我已经

1217
00:50:10,200 --> 00:50:12,780
提出的，然后我展示了第二种

1218
00:50:12,780 --> 00:50:14,520
技术，

1219
00:50:14,520 --> 00:50:17,280
所以第一个技术使用之前定义的 SQL 先验

1220
00:50:17,280 --> 00:50:18,680


1221
00:50:18,680 --> 00:50:21,359
，第二个技术是 a 是一种新颖的

1222
00:50:21,359 --> 00:50:22,859
技术，实际上利用了

1223
00:50:22,859 --> 00:50:24,359
反面例子，

1224
00:50:24,359 --> 00:50:26,520
所以在这种情况下，反面例子

1225
00:50:26,520 --> 00:50:30,060
就是一个数据点，

1226
00:50:30,060 --> 00:50:32,280
其中你有一个图像，但标签是

1227
00:50:32,280 --> 00:50:33,240
错误的，

1228
00:50:33,240 --> 00:50:35,220
所以这里例如你有一个七的图像

1229
00:50:35,220 --> 00:50:36,900
但我给

1230
00:50:36,900 --> 00:50:39,599
模型的标签是二，

1231
00:50:39,599 --> 00:50:40,980


1232
00:50:40,980 --> 00:50:44,579
而且这个想法非常简单，已经

1233
00:50:44,579 --> 00:50:47,460
在很多呃作品中使用了，

1234
00:50:47,460 --> 00:50:49,740
所以每次模型是一个正

1235
00:50:49,740 --> 00:50:52,079
例时，它都必须增加到以

1236
00:50:52,079 --> 00:50:53,520
最小化变化 自由能

1237
00:50:53,520 --> 00:50:56,520
，每次它有 a 时，它都是一个反面

1238
00:50:56,520 --> 00:50:58,859
例子，它必须增加它，

1239
00:50:58,859 --> 00:51:01,260
所以让我继续讨论将这个

1240
00:51:01,260 --> 00:51:04,200
量最小化的错误，

1241
00:51:04,200 --> 00:51:05,960


1242
00:51:05,960 --> 00:51:08,579
通过大量的实验和大量

1243
00:51:08,579 --> 00:51:10,859
的实验，我们看到这

1244
00:51:10,859 --> 00:51:12,119
两种技术

1245
00:51:12,119 --> 00:51:15,000
基本上，首先会导致相同的

1246
00:51:15,000 --> 00:51:17,220
结果，第二次也会导致相同的

1247
00:51:17,220 --> 00:51:18,599
图表，

1248
00:51:18,599 --> 00:51:21,000
所以这里是

1249
00:51:21,000 --> 00:51:22,800
新结果，一些方法和

1250
00:51:22,800 --> 00:51:25,079
时尚方法使用

1251
00:51:25,079 --> 00:51:27,660
我刚刚提出的两种技术，

1252
00:51:27,660 --> 00:51:30,960
现在我们转向一些

1253
00:51:30,960 --> 00:51:33,900
仍然不是很好，但绝对更

1254
00:51:33,900 --> 00:51:36,000
合理的测试精度，所以这里我们的

1255
00:51:36,000 --> 00:51:39,059
分钟测试误差为 3.17，

1256
00:51:39,059 --> 00:51:42,119
时尚平均值的测试误差为 13.98

1257
00:51:42,119 --> 00:51:44,819
，实际上这些结果

1258
00:51:44,819 --> 00:51:48,300
可以通过学习

1259
00:51:48,300 --> 00:51:51,300
图表的结构来大大改善 切碎

1260
00:51:51,300 --> 00:51:53,040
，然后修复图的结构，

1261
00:51:53,040 --> 00:51:55,319
并进行某种形式的

1262
00:51:55,319 --> 00:51:57,660
微调，因此，如果您

1263
00:51:57,660 --> 00:52:00,000
在某个时刻在正确的分层结构上微调模型，

1264
00:52:00,000 --> 00:52:01,980
您将达到测试精度，

1265
00:52:01,980 --> 00:52:03,359
这是您期望从

1266
00:52:03,359 --> 00:52:05,460
分层模型中获得的测试精度，但是那些 那些

1267
00:52:05,460 --> 00:52:08,099
只是完全连接模型

1268
00:52:08,099 --> 00:52:10,980
自然收敛到的模型，

1269
00:52:10,980 --> 00:52:13,859
例如，

1270
00:52:13,859 --> 00:52:15,420


1271
00:52:15,420 --> 00:52:17,339


1272
00:52:17,339 --> 00:52:20,359
通过简单地执行

1273
00:52:20,359 --> 00:52:22,859
相关性或条件查询（

1274
00:52:22,859 --> 00:52:24,420
这是查询操作编码模型的标准方式），在时尚手段上训练完全连接模型的测试误差为 18.32

1275
00:52:24,420 --> 00:52:26,520


1276
00:52:26,520 --> 00:52:29,220
干预和 AC 点击

1277
00:52:29,220 --> 00:52:32,040
先验一起使

1278
00:52:32,040 --> 00:52:34,200
这个测试误差大大降低

1279
00:52:34,200 --> 00:52:37,200
，我们也可以观察它的平均值，

1280
00:52:37,200 --> 00:52:39,319


1281
00:52:39,780 --> 00:52:41,819
我不会详细

1282
00:52:41,819 --> 00:52:45,420
介绍最后一个实验以及非

1283
00:52:45,420 --> 00:52:48,660
循环先验如何作用于

1284
00:52:48,660 --> 00:52:50,339
图表的结构，

1285
00:52:50,339 --> 00:52:52,440
所以我执行我在

1286
00:52:52,440 --> 00:52:54,960
新数据集上进行实验，这就是我的意思是

1287
00:52:54,960 --> 00:52:56,460
调用新数据集，这可能

1288
00:52:56,460 --> 00:52:58,500
太多了，我将其称为两个均值

1289
00:52:58,500 --> 00:53:01,440
数据集，其中您有输入

1290
00:53:01,440 --> 00:53:04,319
点 由两个不同的

1291
00:53:04,319 --> 00:53:07,319
图像组成，标签仅取决于

1292
00:53:07,319 --> 00:53:08,520


1293
00:53:08,520 --> 00:53:10,800
第一个图像故事上的第二个图像，

1294
00:53:10,800 --> 00:53:12,720
因此这里的想法

1295
00:53:12,720 --> 00:53:15,079
是模型的结构、

1296
00:53:15,079 --> 00:53:18,540
循环先验以及类似的东西

1297
00:53:18,540 --> 00:53:20,819
能够识别

1298
00:53:20,819 --> 00:53:23,400
图像的后半部分是 实际上，在

1299
00:53:23,400 --> 00:53:27,960
学习中

1300
00:53:27,960 --> 00:53:31,140
执行分类时，

1301
00:53:31,140 --> 00:53:33,119
训练通常是如何表现的，

1302
00:53:33,119 --> 00:53:36,480
例如我们有这个输入

1303
00:53:36,480 --> 00:53:39,000
节点输出节点，只有节点

1304
00:53:39,000 --> 00:53:41,940
完全连接，并且模型

1305
00:53:41,940 --> 00:53:43,740
收敛

1306
00:53:43,740 --> 00:53:45,900
到一个层次结构，即

1307
00:53:45,900 --> 00:53:48,960
我们知道在分类任务上表现最好的一个

1308
00:53:48,960 --> 00:53:50,880


1309
00:53:50,880 --> 00:53:53,520
是

1310
00:53:53,520 --> 00:53:54,980
训练方法

1311
00:53:54,980 --> 00:53:59,280
运行的一个例子，所以在 c0 处，这是训练的开始，

1312
00:53:59,280 --> 00:54:00,720


1313
00:54:00,720 --> 00:54:03,000
我们在这里有这个模型，所以 s0

1314
00:54:03,000 --> 00:54:05,819
对应于 7，所以对应于

1315
00:54:05,819 --> 00:54:08,099
第一张图像 因为 1

1316
00:54:08,099 --> 00:54:09,839
再次对应于七列图像，我们有

1317
00:54:09,839 --> 00:54:12,300
标签 Y 和所有潜在变量 x0

1318
00:54:12,300 --> 00:54:13,800
X1 X2

1319
00:54:13,800 --> 00:54:15,720
并且模型是完全连接的，因此

1320
00:54:15,720 --> 00:54:17,040
代理矩阵

1321
00:54:17,040 --> 00:54:20,579
充满了 1 没有零，我们

1322
00:54:20,579 --> 00:54:23,720
有自循环和类似的东西

1323
00:54:23,720 --> 00:54:27,319
几个纪元的模型，直到

1324
00:54:27,319 --> 00:54:30,540
我们立即知道的是，

1325
00:54:30,540 --> 00:54:31,920
例如，模型立即

1326
00:54:31,920 --> 00:54:34,740
理解不需要四个纪元

1327
00:54:34,740 --> 00:54:36,839
来执行分类，因此它不会

1328
00:54:36,839 --> 00:54:40,740
呃，所以来自

1329
00:54:40,740 --> 00:54:43,980
第二个输入集群的每个传出节点都被删除

1330
00:54:43,980 --> 00:54:45,900
并且 我们不明白的

1331
00:54:45,900 --> 00:54:48,660
是，这个簇是与

1332
00:54:48,660 --> 00:54:50,400
输出相关的簇，

1333
00:54:50,400 --> 00:54:52,260
所以

1334
00:54:52,260 --> 00:54:55,319
我们有一个直接从 s0 到 Y 的线性映射，

1335
00:54:55,319 --> 00:54:56,480


1336
00:54:56,480 --> 00:54:59,339
这就是这里的一部分，

1337
00:54:59,339 --> 00:55:01,160
但我们知道实际上线性映射

1338
00:55:01,160 --> 00:55:04,740
并不是最好的 用于

1339
00:55:04,740 --> 00:55:07,200
执行均值分类的映射，因此

1340
00:55:07,200 --> 00:55:08,700
我们需要一些层次结构，我们需要一些

1341
00:55:08,700 --> 00:55:11,579
深度来改进结果，正如

1342
00:55:11,579 --> 00:55:14,220
您所看到的，这里的这一行是

1343
00:55:14,220 --> 00:55:15,599
准确度，

1344
00:55:15,599 --> 00:55:18,960
到目前为止，C2 与嗯

1345
00:55:18,960 --> 00:55:22,500
相似，所以它是 91 这比

1346
00:55:22,500 --> 00:55:24,059
线性分类稍微好一点，

1347
00:55:24,059 --> 00:55:25,500


1348
00:55:25,500 --> 00:55:28,740
但是一旦你继续训练，

1349
00:55:28,740 --> 00:55:30,660
模型就会明白它需要一些

1350
00:55:30,660 --> 00:55:33,119
层次结构来更好地拟合数据，

1351
00:55:33,119 --> 00:55:35,640
所以你会看到这个箭头

1352
00:55:35,640 --> 00:55:38,760
随着时间的推移开始变得越来越强，

1353
00:55:38,760 --> 00:55:41,700
直到它理解线性分类

1354
00:55:41,700 --> 00:55:44,339
map 实际上并不是真正需要的，它会

1355
00:55:44,339 --> 00:55:45,920
删除它，

1356
00:55:45,920 --> 00:55:48,780
因此您收敛的模型是一个

1357
00:55:48,780 --> 00:55:51,000
从零开始到

1358
00:55:51,000 --> 00:55:53,760
隐藏节点的模型，然后转到

1359
00:55:53,760 --> 00:55:57,180
具有非常弱的线性映射的标签，

1360
00:55:57,180 --> 00:55:59,700
如果您满足以下条件，该模型实际上会被删除：

1361
00:55:59,700 --> 00:56:02,760
你设置一个阈值呃，如果卖家

1362
00:56:02,760 --> 00:56:05,520
阈值例如0.1 0.2，在某个

1363
00:56:05,520 --> 00:56:07,619
时候线性映射会被忘记，

1364
00:56:07,619 --> 00:56:10,680
你最终得到的一切都是

1365
00:56:10,680 --> 00:56:13,319
带有一个分层网络，

1366
00:56:13,319 --> 00:56:15,720
就是那个呃，所以它已经学会了

1367
00:56:15,720 --> 00:56:17,099
执行的正确结构

1368
00:56:17,099 --> 00:56:19,260
分类任务是层次结构的，

1369
00:56:19,260 --> 00:56:21,900
并且还了解到第二张

1370
00:56:21,900 --> 00:56:25,020
图像在定义测试精度方面没有发挥任何作用

1371
00:56:25,020 --> 00:56:28,440
，这就是所有

1372
00:56:28,440 --> 00:56:30,420
这一切执行的所有这些

1373
00:56:30,420 --> 00:56:33,839
工作也只是由

1374
00:56:33,839 --> 00:56:36,599
一个自由能最小化过程执行，因此

1375
00:56:36,599 --> 00:56:38,400
您初始化模型您定义

1376
00:56:38,400 --> 00:56:40,859
自由能您定义先验，因此

1377
00:56:40,859 --> 00:56:43,559
稀疏和C单击之前

1378
00:56:43,559 --> 00:56:45,780
您运行能量最小化并且

1379
00:56:45,780 --> 00:56:47,400
您收敛到

1380
00:56:47,400 --> 00:56:49,500
分层模型，该模型能够很好地对

1381
00:56:49,500 --> 00:56:51,839
切碎执行分类

1382
00:56:51,839 --> 00:56:54,000
，然后如果您 然后进行一些

1383
00:56:54,000 --> 00:56:55,800
微调，您将获得非常有竞争力的

1384
00:56:55,800 --> 00:56:57,359
结果，就像在具有反馈传播的前馈网络中所做的那样，

1385
00:56:57,359 --> 00:56:59,339


1386
00:56:59,339 --> 00:57:01,260
但我认为这不是有趣的

1387
00:57:01,260 --> 00:57:03,780
一点，有趣的是您喜欢

1388
00:57:03,780 --> 00:57:05,160
所有这个过程，这个过程所有的

1389
00:57:05,160 --> 00:57:07,980
干预和非

1390
00:57:07,980 --> 00:57:09,780
循环性都

1391
00:57:09,780 --> 00:57:11,700
允许您 采用完全连接的

1392
00:57:11,700 --> 00:57:12,660
网络

1393
00:57:12,660 --> 00:57:15,119
并收敛到分层网络，

1394
00:57:15,119 --> 00:57:16,140
能够以

1395
00:57:16,140 --> 00:57:20,058
良好的结果执行分类，

1396
00:57:20,760 --> 00:57:23,000
是的，

1397
00:57:23,000 --> 00:57:26,280
基本上就是这样，我现在哦，是的哇，

1398
00:57:26,280 --> 00:57:29,220
我已经谈了很多，我呃，这就是

1399
00:57:29,220 --> 00:57:32,160
结论 我

1400
00:57:32,160 --> 00:57:35,280
基本上是在做一个小总结，我

1401
00:57:35,280 --> 00:57:37,559
认为如果我

1402
00:57:37,559 --> 00:57:39,300
必须用本文的一句话来告诉您，最重要的一点

1403
00:57:39,300 --> 00:57:40,980
是，预测编码是一种

1404
00:57:40,980 --> 00:57:44,400
信念更新方法，能够

1405
00:57:44,400 --> 00:57:46,559
执行端到端 -结束表弟学习，这样

1406
00:57:46,559 --> 00:57:48,599
他就能够执行干预，

1407
00:57:48,599 --> 00:57:51,420
从数据中学习结构，然后

1408
00:57:51,420 --> 00:57:53,160
执行干预和

1409
00:57:53,160 --> 00:57:56,058
反事实，

1410
00:57:56,700 --> 00:57:58,440
因此

1411
00:57:58,440 --> 00:58:00,119
通过

1412
00:58:00,119 --> 00:58:01,680
简单地将预测误差设置

1413
00:58:01,680 --> 00:58:03,359
为零，可以在其他有效的模型干预中进行因果推断，因此这是一种非常容易

1414
00:58:03,359 --> 00:58:06,240
执行的技术 干预措施，

1415
00:58:06,240 --> 00:58:07,619
你只需要接触一个

1416
00:58:07,619 --> 00:58:08,940
神经元，你不必对

1417
00:58:08,940 --> 00:58:10,859
图的结构采取行动，

1418
00:58:10,859 --> 00:58:14,339
你可以用它来执行

1419
00:58:14,339 --> 00:58:16,140
创建生物学上合理的结构因果模型，

1420
00:58:16,140 --> 00:58:18,359


1421
00:58:18,359 --> 00:58:20,819
它能够学习结构 呃，

1422
00:58:20,819 --> 00:58:24,119
从数据来看，正如我可能已经说过很多次了，

1423
00:58:24,119 --> 00:58:26,940


1424
00:58:26,940 --> 00:58:28,740
并且关于未来工作的几句话

1425
00:58:28,740 --> 00:58:31,260
是，

1426
00:58:31,260 --> 00:58:33,180
最好做的事情是

1427
00:58:33,180 --> 00:58:36,119
提高我们定义的模型的性能，

1428
00:58:36,119 --> 00:58:38,460
因为我认为它在

1429
00:58:38,460 --> 00:58:40,980
以下方面表现得相当好 很多

1430
00:58:40,980 --> 00:58:43,079
任务，所以它在

1431
00:58:43,079 --> 00:58:45,780
结构学习上对我的

1432
00:58:45,780 --> 00:58:48,119
干预和反事实表现得相当好，但

1433
00:58:48,119 --> 00:58:49,440
实际上，如果你看看最先进的

1434
00:58:49,440 --> 00:58:51,420
模型，总会有一种非常

1435
00:58:51,420 --> 00:58:53,880
具体的方法在单个任务中表现更好，

1436
00:58:53,880 --> 00:58:55,559


1437
00:58:55,559 --> 00:58:58,260
所以看到它会很有趣 如果我们

1438
00:58:58,260 --> 00:59:00,180
可以

1439
00:59:00,180 --> 00:59:03,599
通过添加一些

1440
00:59:03,599 --> 00:59:05,599
技巧或

1441
00:59:05,599 --> 00:59:10,260
一些新的优化方法来达到特定任务中的性能水平，并将

1442
00:59:10,260 --> 00:59:12,839
其推广到

1443
00:59:12,839 --> 00:59:14,280
实际上比静态系统更有趣的动态

1444
00:59:14,280 --> 00:59:17,220
系统，例如动态

1445
00:59:17,220 --> 00:59:20,099
因果模型和/或 其他技术

1446
00:59:20,099 --> 00:59:22,200
允许您在

1447
00:59:22,200 --> 00:59:25,200
移动的系统中执行因果推理，因此在

1448
00:59:25,200 --> 00:59:27,799
特定时间步长中采取的操作

1449
00:59:27,799 --> 00:59:30,299
会影响稍后时间步长中的另一个节点，

1450
00:59:30,299 --> 00:59:32,640
这基本上是一个宏伟的

1451
00:59:32,640 --> 00:59:34,859
因果关系，是的，就是

1452
00:59:34,859 --> 00:59:38,160
这样，嗯，

1453
00:59:38,160 --> 00:59:41,118
非常感谢，

1454
00:59:47,460 --> 00:59:51,119
谢谢 非常棒且非常全面的

1455
00:59:51,119 --> 00:59:53,160
演示，我真的认为

1456
00:59:53,160 --> 00:59:55,700
您被静音了，

1457
00:59:57,119 --> 00:59:59,700
抱歉在 Zoom 上被静音了，但是是的，感谢您的

1458
00:59:59,700 --> 01:00:02,400
精彩且非常全面的

1459
01:00:02,400 --> 01:00:05,099
演示，其中确实有很多内容，

1460
01:00:05,099 --> 01:00:06,900
并且在实时聊天中也有很多很好的

1461
01:00:06,900 --> 01:00:09,900
问题，所以也许可以

1462
01:00:09,900 --> 01:00:12,900
温暖一下 进入这个问题，你是如何开始

1463
01:00:12,900 --> 01:00:15,960
研究这个主题的，你是在

1464
01:00:15,960 --> 01:00:18,900
因果关系中研究的，并发现预测编码

1465
01:00:18,900 --> 01:00:21,000
是有用的，反之亦然，或者你是如何

1466
01:00:21,000 --> 01:00:23,160
来到这个十字路口的？

1467
01:00:23,160 --> 01:00:25,740
我实际上不得不说，第一个

1468
01:00:25,740 --> 01:00:27,240
提出这个想法的人是

1469
01:00:27,240 --> 01:00:29,040
呃，男爵

1470
01:00:29,040 --> 01:00:33,900
非常像我想一年

1471
01:00:33,900 --> 01:00:36,660
半前，甚至更多，他带来了一个

1472
01:00:36,660 --> 01:00:38,940
带有这个想法的页面，然后他被

1473
01:00:38,940 --> 01:00:42,119
遗忘了，没有人拿起它，呃，

1474
01:00:42,119 --> 01:00:43,980
去年夏天我开始

1475
01:00:43,980 --> 01:00:47,880
对因果关系和嗯感到好奇

1476
01:00:47,880 --> 01:00:50,339
例如，当我听播客时，我读了《生命之书》，

1477
01:00:50,339 --> 01:00:52,440
我知道

1478
01:00:52,440 --> 01:00:53,760
你对某个主题感兴趣的标准方式，

1479
01:00:53,760 --> 01:00:54,900


1480
01:00:54,900 --> 01:00:57,480
我记得巴伦的这个想法，

1481
01:00:57,480 --> 01:01:00,180
并向他提出了这个想法，呃，

1482
01:01:00,180 --> 01:01:03,180
我想为什么不呢？ 我们扩展它，呃，

1483
01:01:03,180 --> 01:01:06,000
实际上把它写成一篇论文，所以我

1484
01:01:06,000 --> 01:01:07,319
让一些人帮助我进行

1485
01:01:07,319 --> 01:01:09,359
实验，呃，这是最后的最终

1486
01:01:09,359 --> 01:01:12,000
结果，太棒了，

1487
01:01:12,000 --> 01:01:14,160
很酷，是的，

1488
01:01:14,160 --> 01:01:15,240


1489
01:01:15,240 --> 01:01:17,400
有很多话要说，我只是要去

1490
01:01:17,400 --> 01:01:19,619
实时聊天 首先解决一堆

1491
01:01:19,619 --> 01:01:21,240
不同的问题，如果其他人

1492
01:01:21,240 --> 01:01:22,440
想添加我，我会

1493
01:01:22,440 --> 01:01:24,059
先打开灯，因为我认为我

1494
01:01:24,059 --> 01:01:28,440
越来越陷入黑暗，是的，

1495
01:01:28,440 --> 01:01:30,720
谁说主动推理无法解决 一个

1496
01:01:30,720 --> 01:01:32,160
暗室问题

1497
01:01:32,160 --> 01:01:34,980
哦，是的，我们在这里，

1498
01:01:34,980 --> 01:01:37,020
所以你会说灯开关导致

1499
01:01:37,020 --> 01:01:39,299
它变亮了

1500
01:01:39,299 --> 01:01:40,680
是的，

1501
01:01:40,680 --> 01:01:42,240
我认为

1502
01:01:42,240 --> 01:01:43,980
这里没有问题，

1503
01:01:43,980 --> 01:01:46,940
嗯，好的，ml Dawn 写道，

1504
01:01:46,940 --> 01:01:49,559
因为在预测编码中，所有

1505
01:01:49,559 --> 01:01:52,020
分布通常都是高斯分布，

1506
01:01:52,020 --> 01:01:53,760
自下而上的消息是精确

1507
01:01:53,760 --> 01:01:55,500
加权预测 误差，其中

1508
01:01:55,500 --> 01:01:57,420
精度是高斯

1509
01:01:57,420 --> 01:02:00,000
协方差的倒数，如果使用非高斯

1510
01:02:00,000 --> 01:02:03,319
分布，

1511
01:02:03,780 --> 01:02:05,339


1512
01:02:05,339 --> 01:02:09,059
基本上一般方法保持

1513
01:02:09,059 --> 01:02:10,380
不同，主要区别在于，

1514
01:02:10,380 --> 01:02:13,079
你没有预测误差，

1515
01:02:13,079 --> 01:02:15,480
正如正确指出的那样，基本上是

1516
01:02:15,480 --> 01:02:18,480


1517
01:02:18,480 --> 01:02:20,819
虚拟自由能的导数，如果你有高斯

1518
01:02:20,819 --> 01:02:22,920
假设，

1519
01:02:22,920 --> 01:02:25,020
是的，你甚至可以将单个量

1520
01:02:25,020 --> 01:02:27,960
设置为零，你可能

1521
01:02:27,960 --> 01:02:29,880
必须根据图表的结构进行干预

1522
01:02:29,880 --> 01:02:30,900


1523
01:02:30,900 --> 01:02:34,020


1524
01:02:34,020 --> 01:02:37,079
，而且你呃和同事在 2022 年发表了一篇

1525
01:02:37,079 --> 01:02:39,900
预测论文 编码超越

1526
01:02:39,900 --> 01:02:41,880
高斯分布，它着眼

1527
01:02:41,880 --> 01:02:43,859
于其中一些问题，是的，是的，正是

1528
01:02:43,859 --> 01:02:46,260
如此，

1529
01:02:46,260 --> 01:02:47,339


1530
01:02:47,339 --> 01:02:50,460
这篇论文背后的想法是呃，

1531
01:02:50,460 --> 01:02:53,220
我们对变形金刚进行建模，这是

1532
01:02:53,220 --> 01:02:54,420
使用相当困难的最大动机

1533
01:02:54,420 --> 01:02:57,180
，答案是呃，不是

1534
01:02:57,180 --> 01:02:59,460
因为 注意力机制

1535
01:02:59,460 --> 01:03:02,099
最后有一个软最大，软最大调用

1536
01:03:02,099 --> 01:03:03,960


1537
01:03:03,960 --> 01:03:08,400
呃，不是高斯分布，而是

1538
01:03:08,400 --> 01:03:11,280
软最大分布，我

1539
01:03:11,280 --> 01:03:13,440
现在不知道名字，但是是的，

1540
01:03:13,440 --> 01:03:16,079
呃，所以是的，这是一个概括，

1541
01:03:16,079 --> 01:03:19,140
它有点 一旦

1542
01:03:19,140 --> 01:03:20,700
你删除了加斯顿假设，就

1543
01:03:20,700 --> 01:03:22,319
很难称之为

1544
01:03:22,319 --> 01:03:24,059
创造性编码，

1545
01:03:24,059 --> 01:03:26,400
所以他是一个

1546
01:03:26,400 --> 01:03:29,819
例子，就像和汽车

1547
01:03:29,819 --> 01:03:32,700
弗里斯通交谈，或者他喜欢创造性编码，

1548
01:03:32,700 --> 01:03:35,160
只有当你只有高斯

1549
01:03:35,160 --> 01:03:37,680
和高斯时 假设，

1550
01:03:37,680 --> 01:03:39,720
但是，是的，这更像是一场哲学

1551
01:03:39,720 --> 01:03:42,660
辩论，而不是呃

1552
01:03:42,660 --> 01:03:44,940
有趣，另一个我认为

1553
01:03:44,940 --> 01:03:46,740
绝对令人

1554
01:03:46,740 --> 01:03:49,500
感兴趣的话题是《变形金刚》

1555
01:03:49,500 --> 01:03:52,980
中的注意力装置

1556
01:03:52,980 --> 01:03:56,099
与

1557
01:03:56,099 --> 01:03:58,440
从神经认知

1558
01:03:58,440 --> 01:04:00,180
角度和预测

1559
01:04:00,180 --> 01:04:03,240
处理精度描述注意力的方式之间的异同 等待角度，

1560
01:04:03,240 --> 01:04:06,200
你对此

1561
01:04:06,359 --> 01:04:08,700
有何看法？这个想法是，

1562
01:04:08,700 --> 01:04:12,359
嗯，是的，我认为，从

1563
01:04:12,359 --> 01:04:15,000
漂亮的处理和

1564
01:04:15,000 --> 01:04:16,400
操作推理的角度来看，

1565
01:04:16,400 --> 01:04:19,260
注意力可以被视为一种

1566
01:04:19,260 --> 01:04:21,299
结构性学习问题，我

1567
01:04:21,299 --> 01:04:23,040
认为有一个

1568
01:04:23,040 --> 01:04:25,680
Chris Buckley 小组最近发表的论文表明，

1569
01:04:25,680 --> 01:04:26,339


1570
01:04:26,339 --> 01:04:28,079
应该有一个

1571
01:04:28,079 --> 01:04:30,420
存档重印，其中基本上

1572
01:04:30,420 --> 01:04:31,859
表明，注意力机制

1573
01:04:31,859 --> 01:04:35,819
只是简单地学习特定于

1574
01:04:35,819 --> 01:04:38,880


1575
01:04:38,880 --> 01:04:41,040
其他数据点的权重参数的精度，所以这个精度

1576
01:04:41,040 --> 01:04:43,200
不是，它不是，它不是

1577
01:04:43,200 --> 01:04:45,540
模型结构中的参数，因此

1578
01:04:45,540 --> 01:04:47,579
它不是模型特定的参数，它

1579
01:04:47,579 --> 01:04:49,140
是一个快速变化的参数，例如在

1580
01:04:49,140 --> 01:04:51,660


1581
01:04:51,660 --> 01:04:53,760
最小化自由能变化的同时更新的值节点，

1582
01:04:53,760 --> 01:04:55,440
一旦它们一旦你 已经最小化它

1583
01:04:55,440 --> 01:04:57,000
并计算它，然后你把它扔掉，

1584
01:04:57,000 --> 01:04:58,920
对于下一个数据点，你必须

1585
01:04:58,920 --> 01:05:00,780
从头开始重新计算它，

1586
01:05:00,780 --> 01:05:03,299
所以是的，我认为类比

1587
01:05:03,299 --> 01:05:05,819
计算明智的是呃，注意

1588
01:05:05,819 --> 01:05:07,920
机制可以被视为一种

1589
01:05:07,920 --> 01:05:10,559
结构学习，但是 结构

1590
01:05:10,559 --> 01:05:13,020
学习是特定于数据点而

1591
01:05:13,020 --> 01:05:15,119
不是特定于模型的，

1592
01:05:15,119 --> 01:05:17,280
我认为如果我们想概括

1593
01:05:17,280 --> 01:05:18,960
一点，并从

1594
01:05:18,960 --> 01:05:20,339


1595
01:05:20,339 --> 01:05:21,900
《变形金刚》中的注意力机制到认知科学的注意力机制，

1596
01:05:21,900 --> 01:05:24,180


1597
01:05:24,180 --> 01:05:28,020
我觉得它们可能是两个不同的东西，

1598
01:05:28,020 --> 01:05:31,260
喜欢得出相似之处和 呃，

1599
01:05:31,260 --> 01:05:33,359
我认为结构学习类比

1600
01:05:33,359 --> 01:05:36,660
以及一个连接相

1601
01:05:36,660 --> 01:05:38,760
对于另一个连接的重要性可能会

1602
01:05:38,760 --> 01:05:41,900
更好地完成这项工作，

1603
01:05:42,000 --> 01:05:44,880
酷灰色答案好吧，

1604
01:05:44,880 --> 01:05:49,200
ml Don 在反事实中问

1605
01:05:49,200 --> 01:05:51,240
隐藏变量

1606
01:05:51,240 --> 01:05:55,440
X 和未观察到的变量 U 之间有什么区别，

1607
01:05:55,440 --> 01:05:59,180
区别是

1608
01:05:59,540 --> 01:06:01,740
我认为主要的一个是你无法

1609
01:06:01,740 --> 01:06:03,599
观察

1610
01:06:03,599 --> 01:06:05,819
你可以使用它们的用途，因为你可以你可以

1611
01:06:05,819 --> 01:06:09,000
计算它们并修复它们，但你不能这个

1612
01:06:09,000 --> 01:06:10,559
想法是你无法控制

1613
01:06:10,559 --> 01:06:13,380
它们，所以它们使用的用途应该是

1614
01:06:13,380 --> 01:06:16,020
被视为环境特定的变量，

1615
01:06:16,020 --> 01:06:18,540
它们在那里，它们会影响

1616
01:06:18,540 --> 01:06:21,240
你的过程，因为

1617
01:06:21,240 --> 01:06:23,280
例如当你回到过去时，

1618
01:06:23,280 --> 01:06:25,079
环境是不同的，所以这个想法是，

1619
01:06:25,079 --> 01:06:26,520
例如，如果你

1620
01:06:26,520 --> 01:06:28,440
想回到

1621
01:06:28,440 --> 01:06:29,880
之前的例子

1622
01:06:29,880 --> 01:06:31,920
具有特定教育智力的人的预期收入

1623
01:06:31,920 --> 01:06:34,619


1624
01:06:34,619 --> 01:06:37,440
呃呃教育学位

1625
01:06:37,440 --> 01:06:40,200
这个想法是，如果我想看看

1626
01:06:40,200 --> 01:06:43,559
我今天能学到多少，呃

1627
01:06:43,559 --> 01:06:45,359
与我不知道与硕士学位

1628
01:06:45,359 --> 01:06:47,339
是不同的

1629
01:06:47,339 --> 01:06:48,359


1630
01:06:48,359 --> 01:06:50,819
20 年前我拥有硕士学位的收入是

1631
01:06:50,819 --> 01:06:52,619
不同的，例如在意大利，

1632
01:06:52,619 --> 01:06:55,440
相对于其他国家，所有这些

1633
01:06:55,440 --> 01:06:57,000
变量都不受你的

1634
01:06:57,000 --> 01:06:58,859
控制，你无法使用你的愿景网络对它们进行建模，

1635
01:06:58,859 --> 01:07:00,359


1636
01:07:00,359 --> 01:07:03,480
但它们就在那里，所以你 当

1637
01:07:03,480 --> 01:07:05,220
你想得出结论时，你不能忽视它们，

1638
01:07:05,220 --> 01:07:07,559
所以他是的，

1639
01:07:07,559 --> 01:07:08,760
基本上所有你

1640
01:07:08,760 --> 01:07:10,079
无法控制的事情你都

1641
01:07:10,079 --> 01:07:13,079
可以推断它们，这样你就可以

1642
01:07:13,079 --> 01:07:14,819


1643
01:07:14,819 --> 01:07:16,740
及时进行反事实推理，然后说哦，20

1644
01:07:16,740 --> 01:07:19,020
年前我就应该得到这个

1645
01:07:19,020 --> 01:07:20,640


1646
01:07:20,640 --> 01:07:22,559
如果我这么聪明的

1647
01:07:22,559 --> 01:07:24,599
话，当然平均能达到这个学位，

1648
01:07:24,599 --> 01:07:27,059
但这并不是说我可以改变

1649
01:07:27,059 --> 01:07:30,720
政府对就业或

1650
01:07:30,720 --> 01:07:32,819
类似事物的政策，

1651
01:07:32,819 --> 01:07:35,099
这是一个更深层次的反事实，是的，是的，

1652
01:07:35,099 --> 01:07:38,400
这些都是有用的，

1653
01:07:38,400 --> 01:07:40,200


1654
01:07:40,200 --> 01:07:42,480
好吧 你

1655
01:07:42,480 --> 01:07:45,660
在预测编码中实现了广义坐标

1656
01:07:45,660 --> 01:07:46,920
不，

1657
01:07:46,920 --> 01:07:50,039
我没有，我从来没有这样做过，我已经

1658
01:07:50,039 --> 01:07:52,680
呃，是的，我已经研究过它，但是我呃，我从来

1659
01:07:52,680 --> 01:07:55,260
没有实现过它，我知道它们往往

1660
01:07:55,260 --> 01:07:57,599
不稳定，呃，

1661
01:07:57,599 --> 01:08:00,299
它是 很难使它们稳定，我

1662
01:08:00,299 --> 01:08:02,940
认为这就是

1663
01:08:02,940 --> 01:08:05,460
我与实施它们的人交谈时得到的结论，

1664
01:08:05,460 --> 01:08:08,359


1665
01:08:08,400 --> 01:08:11,039
但是，是的，我知道

1666
01:08:11,039 --> 01:08:12,839
最近实际上发表了一些

1667
01:08:12,839 --> 01:08:15,599
关于它们的论文，这些论文在

1668
01:08:15,599 --> 01:08:18,000
一些英国负载上进行了测试 编码器风格实际上

1669
01:08:18,000 --> 01:08:20,520
我认为仍然来自Baron，

1670
01:08:20,520 --> 01:08:22,979


1671
01:08:22,979 --> 01:08:25,439
去年夏天有一篇论文发表，但我从来没有

1672
01:08:25,439 --> 01:08:26,580
自己玩过它们，

1673
01:08:26,580 --> 01:08:29,160
很酷的连体衣

1674
01:08:29,160 --> 01:08:32,040
确实在层次结构中添加了更多级别，

1675
01:08:32,040 --> 01:08:35,160
减少了

1676
01:08:35,160 --> 01:08:38,238
预测输入

1677
01:08:38,939 --> 01:08:41,698
添加更多内容的分心问题 呃，

1678
01:08:41,698 --> 01:08:43,439
在这种意义上，因为破坏

1679
01:08:43,439 --> 01:08:45,779
问题是由 Cycles 给出的，所以基本上

1680
01:08:45,779 --> 01:08:47,399
你提供了一个图像

1681
01:08:47,399 --> 01:08:49,920
，事实上你有呃

1682
01:08:49,920 --> 01:08:53,279
所以补丁从图像中进入

1683
01:08:53,279 --> 01:08:55,799
神经元，然后其他边缘

1684
01:08:55,799 --> 01:08:57,500
返回，

1685
01:08:57,500 --> 01:08:59,939
这基本上创造了这样一个事实： 你

1686
01:08:59,939 --> 01:09:03,560
有一个错误，这些基本上

1687
01:09:03,560 --> 01:09:06,179
正在调整图像的像素，

1688
01:09:06,179 --> 01:09:08,339
它们会产生一些预测

1689
01:09:08,339 --> 01:09:09,719
错误，所以你有一些预测

1690
01:09:09,719 --> 01:09:12,140
错误在模型内部传播

1691
01:09:12,140 --> 01:09:14,640
，是的，我认为这个问题

1692
01:09:14,640 --> 01:09:16,979
是周期的普遍问题， 它可能

1693
01:09:16,979 --> 01:09:21,439
与像素的层次结构无关，

1694
01:09:23,060 --> 01:09:25,140


1695
01:09:25,140 --> 01:09:26,759
如果你没有传入的边缘，你

1696
01:09:26,759 --> 01:09:27,660
就没有

1697
01:09:27,660 --> 01:09:30,540
呃没有破坏问题了，

1698
01:09:30,540 --> 01:09:33,238
很酷，并且

1699
01:09:33,238 --> 01:09:35,939
通过跟踪运算符的非循环网络规范

1700
01:09:35,939 --> 01:09:37,859


1701
01:09:37,859 --> 01:09:41,819
是一项非常有趣的技术，

1702
01:09:41,819 --> 01:09:46,339
什么时候带来的

1703
01:09:46,560 --> 01:09:49,140
呃，据我所知，我认为他发表了

1704
01:09:49,140 --> 01:09:52,380
我在 2018 年引用的那篇论文，

1705
01:09:52,380 --> 01:09:54,360
我至少在因果推理文献中不知道，

1706
01:09:54,360 --> 01:09:56,940
我不知道

1707
01:09:56,940 --> 01:09:59,699
任何以前的方法，我会说不，

1708
01:09:59,699 --> 01:10:01,860
因为 我的意思是，这是一篇被

1709
01:10:01,860 --> 01:10:04,140
引用率很高的论文，所以我想说他们提出了

1710
01:10:04,140 --> 01:10:05,520
这个想法，

1711
01:10:05,520 --> 01:10:07,980
哇，是的，这很好，

1712
01:10:07,980 --> 01:10:09,480
你可以进行梯度下降并学习

1713
01:10:09,480 --> 01:10:11,400
结构，我认为

1714
01:10:11,400 --> 01:10:14,219
这是一种非常强大的技术，是的，

1715
01:10:14,219 --> 01:10:15,840
有时就像你看

1716
01:10:15,840 --> 01:10:17,640
当

1717
01:10:17,640 --> 01:10:19,440
贝叶斯推理和

1718
01:10:19,440 --> 01:10:23,159
因果推理的不同特征变得可用时，

1719
01:10:23,159 --> 01:10:25,620
这真的很了不起，就像为什么

1720
01:10:25,620 --> 01:10:28,500
没有在贝叶斯

1721
01:10:28,500 --> 01:10:30,719
因果建模框架下完成这件事一样，

1722
01:10:30,719 --> 01:10:32,760
因为这种情况只发生了五到

1723
01:10:32,760 --> 01:10:36,659
25 年

1724
01:10:36,659 --> 01:10:39,960
，所以时间非常非常短 而且

1725
01:10:39,960 --> 01:10:42,060
它的技术性相对较强，所以

1726
01:10:42,060 --> 01:10:43,920
参与其中的研究小组相对较少，

1727
01:10:43,920 --> 01:10:46,920
嗯，

1728
01:10:46,920 --> 01:10:49,860
它的功能真的很酷，

1729
01:10:49,860 --> 01:10:51,960
不，是，正是，我的意思是，这也是我

1730
01:10:51,960 --> 01:10:54,179
认为这个领域令人兴奋的部分，呃，我的意思是，

1731
01:10:54,179 --> 01:10:56,040


1732
01:10:56,040 --> 01:10:59,100
肯定有

1733
01:10:59,100 --> 01:11:01,020
仍然有待发现的突破，可能是

1734
01:11:01,020 --> 01:11:03,000
因为，例如，就像他们

1735
01:11:03,000 --> 01:11:05,300


1736
01:11:05,300 --> 01:11:07,800
发现的那篇论文的突破一样，

1737
01:11:07,800 --> 01:11:09,960
就像他们只是发现了

1738
01:11:09,960 --> 01:11:12,120
无环结构的正确先验一样，

1739
01:11:12,120 --> 01:11:14,040
好吧，是的，

1740
01:11:14,040 --> 01:11:17,100
我的意思是我我不知道 确切地说，

1741
01:11:17,100 --> 01:11:19,080
但这可能是你在某个下午想到的一个想法，

1742
01:11:19,080 --> 01:11:21,120
我不知道

1743
01:11:21,120 --> 01:11:23,040
其他人是如何想出这个想法的，

1744
01:11:23,040 --> 01:11:25,320
但可能是

1745
01:11:25,320 --> 01:11:27,239
他们在白板上，你

1746
01:11:27,239 --> 01:11:29,280
就像哦，实际上 这是一个

1747
01:11:29,280 --> 01:11:32,159
巨大的突破，我只是简单地

1748
01:11:32,159 --> 01:11:33,960
定义了先前的

1749
01:11:33,960 --> 01:11:36,739
突破，还有很多这些突破，

1750
01:11:36,739 --> 01:11:40,500
它们不只是堆叠起来，它不像

1751
01:11:40,500 --> 01:11:44,280
呃呃一座积木塔，它们

1752
01:11:44,280 --> 01:11:47,640
分层并组合，所以某些东西

1753
01:11:47,640 --> 01:11:50,159
将被推广到嗯 广义

1754
01:11:50,159 --> 01:11:52,140
坐标或广义同步或

1755
01:11:52,140 --> 01:11:55,020
任意大的图形或

1756
01:11:55,020 --> 01:11:57,239
具有多模式输入的传感器融合

1757
01:11:57,239 --> 01:12:00,679
，就像所有这些都以非常

1758
01:12:00,679 --> 01:12:03,659
令人满意和有效的方式融合在一起，所以即使是

1759
01:12:03,659 --> 01:12:05,640
某人可以立即想出的小东西也

1760
01:12:05,640 --> 01:12:08,100


1761
01:12:08,100 --> 01:12:11,100
确实可以产生影响

1762
01:12:11,100 --> 01:12:14,159
好吧，ml Dawn 说非常感谢你

1763
01:12:14,159 --> 01:12:16,199
问我的问题，也非常感谢

1764
01:12:16,199 --> 01:12:18,060
Tomaso 的鼓舞人心的演讲，

1765
01:12:18,060 --> 01:12:21,360
非常好哦，非常感谢，然后

1766
01:12:21,360 --> 01:12:23,280
Bert 问道，

1767
01:12:23,280 --> 01:12:25,560
使用预测编码的语言模型与

1768
01:12:25,560 --> 01:12:27,179


1769
01:12:27,179 --> 01:12:30,260
使用 Transformer 的语言模型有何不同，

1770
01:12:31,679 --> 01:12:32,520
嗯，

1771
01:12:32,520 --> 01:12:35,340
好吧，我认为实际上 如果我

1772
01:12:35,340 --> 01:12:36,659
今天必须使用预测编码构建一个语言模型，

1773
01:12:36,659 --> 01:12:38,640
我仍然会使用

1774
01:12:38,640 --> 01:12:40,020
Transformer，

1775
01:12:40,020 --> 01:12:41,880
所以想法是，例如，如果您

1776
01:12:41,880 --> 01:12:42,780
有一个

1777
01:12:42,780 --> 01:12:45,659


1778
01:12:45,659 --> 01:12:48,440


1779
01:12:48,440 --> 01:12:50,460


1780
01:12:50,460 --> 01:12:53,100
我在中定义的这个或这些分层贝叶斯网络的分层图形模型 第一个

1781
01:12:53,100 --> 01:12:55,380
滑动一个箭头来编码一个函数，

1782
01:12:55,380 --> 01:12:57,300
这是线性映射

1783
01:12:57,300 --> 01:12:59,219
好吧，所以一小时只是

1784
01:12:59,219 --> 01:13:01,080


1785
01:13:01,080 --> 01:13:03,060
潜在变量中编码的向量乘以

1786
01:13:03,060 --> 01:13:06,300
这个权重矩阵，然后你可以

1787
01:13:06,300 --> 01:13:08,580
使非线性和类似的东西相乘

1788
01:13:08,580 --> 01:13:09,960
但这实际上可能是更

1789
01:13:09,960 --> 01:13:12,179
复杂的事情，箭头中包含的函数

1790
01:13:12,179 --> 01:13:14,880
可以是卷积可以是

1791
01:13:14,880 --> 01:13:16,800
注意力机制

1792
01:13:16,800 --> 01:13:20,820
所以实际上我会如何做我

1793
01:13:20,820 --> 01:13:23,880
仍然会使用我的意思这实际上是

1794
01:13:23,880 --> 01:13:26,460
我们在呃中所做的方式 去年在牛津

1795
01:13:26,460 --> 01:13:28,860
小组中，我们的

1796
01:13:28,860 --> 01:13:30,900
结构完全相同，现在每个箭头都是一个

1797
01:13:30,900 --> 01:13:33,420
Transformer，所以一个是注意力

1798
01:13:33,420 --> 01:13:35,159
机制，下一个是

1799
01:13:35,159 --> 01:13:38,219
作为 Transformer 的前馈网络

1800
01:13:38,219 --> 01:13:40,020
，基本上唯一的区别

1801
01:13:40,020 --> 01:13:41,640
是那些 您

1802
01:13:41,640 --> 01:13:43,739
想要计算后验的变量，并

1803
01:13:43,739 --> 01:13:45,239


1804
01:13:45,239 --> 01:13:47,400
通过 VIA 平均场

1805
01:13:47,400 --> 01:13:49,560
近似使这些后验独立，因此基本上您遵循

1806
01:13:49,560 --> 01:13:51,659
所有步骤，使您能够

1807
01:13:51,659 --> 01:13:53,520
收敛到

1808
01:13:53,520 --> 01:13:56,520
创造性编码的变化自由能，但

1809
01:13:56,520 --> 01:13:58,199
您的方式 计算预测

1810
01:13:58,199 --> 01:14:01,199
和发送回信号的方式

1811
01:14:01,199 --> 01:14:04,739
是通过 Transformer 完成的，

1812
01:14:04,739 --> 01:14:07,560
所以我通常仍然会使用 Transformer 我的

1813
01:14:07,560 --> 01:14:10,679
意思是它们工作得很好，

1814
01:14:10,679 --> 01:14:12,840
我不认为我们可以傲慢地

1815
01:14:12,840 --> 01:14:15,060
说哦，不，我会这么做 通过

1816
01:14:15,060 --> 01:14:17,640
纯粹的预测编码方式

1817
01:14:17,640 --> 01:14:18,920
结构更好，

1818
01:14:18,920 --> 01:14:21,480
但仍然会近似 Transformer

1819
01:14:21,480 --> 01:14:22,500


1820
01:14:22,500 --> 01:14:24,420
抱歉，你说结构学习将

1821
01:14:24,420 --> 01:14:27,540
近似 Transformer 方法是的，

1822
01:14:27,540 --> 01:14:29,219
我

1823
01:14:29,219 --> 01:14:32,640
之前在呃中提到的结构学习，当有人呃问

1824
01:14:32,640 --> 01:14:34,800
创意编码

1825
01:14:34,800 --> 01:14:38,060
和注意力机制之间的相似之处时非常是

1826
01:14:38,280 --> 01:14:41,699
非常 有趣的

1827
01:14:41,699 --> 01:14:42,900
是，

1828
01:14:42,900 --> 01:14:45,719
我想知道亚马逊的一件事，我

1829
01:14:45,719 --> 01:14:47,640
在你提到的预测编码网络中看不到深度的概念，

1830
01:14:47,640 --> 01:14:49,380


1831
01:14:49,380 --> 01:14:50,880
我很可能错过了它

1832
01:14:50,880 --> 01:14:52,380
为预测编码提供的定义

1833
01:14:52,380 --> 01:14:56,480
涉及深度的概念，

1834
01:14:56,640 --> 01:14:59,460
你所说的深度是什么意思，

1835
01:14:59,460 --> 01:15:02,219
不，是的，这是真的 这是呃，因为

1836
01:15:02,219 --> 01:15:04,980
我多次说过的标准定义

1837
01:15:04,980 --> 01:15:06,960
是分层的，你有

1838
01:15:06,960 --> 01:15:08,400
预测朝一个方向进行，一些

1839
01:15:08,400 --> 01:15:09,719
预测误差朝相反的

1840
01:15:09,719 --> 01:15:10,620
方向发展，

1841
01:15:10,620 --> 01:15:14,340
基本上就是我们在这篇

1842
01:15:14,340 --> 01:15:16,320
论文中所做的，以及在呃最后一篇论文中所做的，

1843
01:15:16,320 --> 01:15:18,420
这就是所谓的 对

1844
01:15:18,420 --> 01:15:19,920
任意图拓扑的学习，我们有

1845
01:15:19,920 --> 01:15:22,260
相对编码，我们可以将

1846
01:15:22,260 --> 01:15:25,620
深度 uh 视为

1847
01:15:25,620 --> 01:15:28,380
独立的 uh

1848
01:15:28,380 --> 01:15:31,380
基本上是一对潜在变量 潜在

1849
01:15:31,380 --> 01:15:33,239
变量和箭头，

1850
01:15:33,239 --> 01:15:34,739
并且您可以预测朝该

1851
01:15:34,739 --> 01:15:36,300
方向前进，并且预测箭头

1852
01:15:36,300 --> 01:15:38,340
与另一个方向一致，但是然后您 可以用

1853
01:15:38,340 --> 01:15:41,880
多少种方式组合这些，所以

1854
01:15:41,880 --> 01:15:45,239
你可以，所以基本上这个

1855
01:15:45,239 --> 01:15:47,040
组合不必是

1856
01:15:47,040 --> 01:15:48,659
分层的，

1857
01:15:48,659 --> 01:15:50,820
最后可以有循环，所以你可以

1858
01:15:50,820 --> 01:15:53,520
例如插入另一个呃另一个

1859
01:15:53,520 --> 01:15:55,440
潜在变量到第一个 一个，

1860
01:15:55,440 --> 01:15:57,540
然后将其他连接起来，你就可以

1861
01:15:57,540 --> 01:15:59,340
得到一个结构，它可以像你想要的那样纠缠在一起，

1862
01:15:59,340 --> 01:16:00,420


1863
01:16:00,420 --> 01:16:02,699
例如在另一篇论文中，

1864
01:16:02,699 --> 01:16:04,500
我们训练

1865
01:16:04,500 --> 01:16:06,659
呃一个具有

1866
01:16:06,659 --> 01:16:08,460
大脑结构形状的网络，所以我们有很多

1867
01:16:08,460 --> 01:16:09,900
大脑区域

1868
01:16:09,900 --> 01:16:12,239
内部稀疏连接，并且

1869
01:16:12,239 --> 01:16:13,860
彼此部分连接，

1870
01:16:13,860 --> 01:16:15,719


1871
01:16:15,719 --> 01:16:17,640
最后没有任何层次结构，但您

1872
01:16:17,640 --> 01:16:18,960
仍然可以通过最小化

1873
01:16:18,960 --> 01:16:20,699
操作自由能和

1874
01:16:20,699 --> 01:16:22,620
最小化

1875
01:16:22,620 --> 01:16:25,159
网络的总预测误差来训练它，

1876
01:16:25,159 --> 01:16:27,360
这样您就可以

1877
01:16:27,360 --> 01:16:31,980
对于纠缠图中的给定主题，

1878
01:16:31,980 --> 01:16:35,159
您可能会看到三个连续的层

1879
01:16:35,159 --> 01:16:37,560
，当您单独查看它们时，您会

1880
01:16:37,560 --> 01:16:38,820
说哦，这是一座三层建筑，

1881
01:16:38,820 --> 01:16:41,940
这是一个三层模型，表示

1882
01:16:41,940 --> 01:16:43,980
自适应三层，但当您

1883
01:16:43,980 --> 01:16:46,620
在那里拍摄更大的图片时 不像

1884
01:16:46,620 --> 01:16:50,280


1885
01:16:50,280 --> 01:16:52,140
该网络的显式顶部或显式底部，

1886
01:16:52,140 --> 01:16:54,360
是的，这基本上是由以下事实给出的：

1887
01:16:54,360 --> 01:16:55,980


1888
01:16:55,980 --> 01:16:58,080
韩国预测网络中的每个操作都是

1889
01:16:58,080 --> 01:16:59,460
严格本地的，

1890
01:16:59,460 --> 01:17:01,739
所以基本上每条消息都会传递

1891
01:17:01,739 --> 01:17:03,000
每个预测和每个预测

1892
01:17:03,000 --> 01:17:05,280
错误 你发送的数据只发送到

1893
01:17:05,280 --> 01:17:08,280
非常接近的神经元，好吧，

1894
01:17:08,280 --> 01:17:10,380
全局结构是否实际上是

1895
01:17:10,380 --> 01:17:13,380
分层的，单个

1896
01:17:13,380 --> 01:17:16,940
消息传递甚至看不到，

1897
01:17:17,460 --> 01:17:19,620
我想这就是

1898
01:17:19,620 --> 01:17:22,820
学习新模型

1899
01:17:22,820 --> 01:17:27,739
架构的希望是空间

1900
01:17:27,739 --> 01:17:33,300
自上而下设计的东西非常小，

1901
01:17:33,300 --> 01:17:36,480
今天使用的模型很多，尽管是超级

1902
01:17:36,480 --> 01:17:38,640
有效的模型，

1903
01:17:38,640 --> 01:17:41,100
嗯，虽然你可以问每

1904
01:17:41,100 --> 01:17:43,320
单位计算是否有效，这是第二

1905
01:17:43,320 --> 01:17:45,300
级问题，但今天很多有效的

1906
01:17:45,300 --> 01:17:47,580
模型没有其中一些

1907
01:17:47,580 --> 01:17:49,860
预测编码网络的属性，

1908
01:17:49,860 --> 01:17:52,739
例如它们的能力，

1909
01:17:52,739 --> 01:17:55,520
仅使用本地计算，

1910
01:17:55,520 --> 01:17:59,400
这提供了生物现实性

1911
01:17:59,400 --> 01:18:02,880
或只是时空现实性，

1912
01:18:02,880 --> 01:18:06,060
而且还可以在

1913
01:18:06,060 --> 01:18:08,159
联合计算或分布式

1914
01:18:08,159 --> 01:18:10,500
计算设置等方面提供很多优势，

1915
01:18:10,500 --> 01:18:12,780
不，是，完全同意，

1916
01:18:12,780 --> 01:18:14,520
因为我 我认为一般的想法是

1917
01:18:14,520 --> 01:18:16,679
这样的，我不知道这是否会

1918
01:18:16,679 --> 01:18:18,540
成为一个优势，所以我认为它非常有

1919
01:18:18,540 --> 01:18:20,159
前途，正是出于你所说的原因，

1920
01:18:20,159 --> 01:18:20,880


1921
01:18:20,880 --> 01:18:22,920
呃，原因是今天的

1922
01:18:22,920 --> 01:18:25,380
带有反向传播的模型字符串你

1923
01:18:25,380 --> 01:18:28,860
基本上可以总结一下 它们作为

1924
01:18:28,860 --> 01:18:32,040
监控反向传播是一个

1925
01:18:32,040 --> 01:18:34,080
函数，因为基本上你有一个

1926
01:18:34,080 --> 01:18:36,120
从输入到输出的映射，反向

1927
01:18:36,120 --> 01:18:39,600
传播基本上

1928
01:18:39,600 --> 01:18:41,699
从其计算

1929
01:18:41,699 --> 01:18:44,340
图中传播呃信息，所以今天使用的每个神经网络神经

1930
01:18:44,340 --> 01:18:45,960
网络模型

1931
01:18:45,960 --> 01:18:48,960
都是一个函数，而预测编码

1932
01:18:48,960 --> 01:18:51,179
以及另一种自由编码，例如

1933
01:18:51,179 --> 01:18:53,820
旧的函数类，该

1934
01:18:53,820 --> 01:18:56,040
方法类使用局部

1935
01:18:56,040 --> 01:18:58,500
计算进行训练，并且实际上通过

1936
01:18:58,500 --> 01:19:01,620
最小化全局能量函数来工作，

1937
01:19:01,620 --> 01:19:03,840
它们不限于

1938
01:19:03,840 --> 01:19:05,940
从输入到输出的模型函数，它们实际上对以下

1939
01:19:05,940 --> 01:19:07,739
内容进行建模： 有点类似于

1940
01:19:07,739 --> 01:19:10,080
物理系统，所以你有一个物理

1941
01:19:10,080 --> 01:19:13,500
系统，你可以将一些值固定到

1942
01:19:13,500 --> 01:19:15,360
你拥有的任何输入上，然后让

1943
01:19:15,360 --> 01:19:17,280
系统收敛，然后你读取应该输出的

1944
01:19:17,280 --> 01:19:19,980
神经元或变量的一些其他值，

1945
01:19:19,980 --> 01:19:21,960
但这个

1946
01:19:21,960 --> 01:19:24,120
物理系统没有 不一定是一个拟合前

1947
01:19:24,120 --> 01:19:25,920
向映射，不一定是一个

1948
01:19:25,920 --> 01:19:28,260
具有输入空间和

1949
01:19:28,260 --> 01:19:30,659
输出空间的函数，就是这样，

1950
01:19:30,659 --> 01:19:32,580
所以你可以学习的模型类别

1951
01:19:32,580 --> 01:19:34,800
是呃，所以基本上你可以看到

1952
01:19:34,800 --> 01:19:37,560
像前馈模型 和函数，

1953
01:19:37,560 --> 01:19:39,600
然后是一个更大的类，即

1954
01:19:39,600 --> 01:19:41,880
物理系统，这里是否有一些

1955
01:19:41,880 --> 01:19:43,860
有趣的东西我还不

1956
01:19:43,860 --> 01:19:45,659
知道，因为这些函数

1957
01:19:45,659 --> 01:19:47,460
运行得非常好，我们看到

1958
01:19:47,460 --> 01:19:50,040
反向传播的那些日子

1959
01:19:50,040 --> 01:19:52,199
它们运行得非常好，但是

1960
01:19:52,199 --> 01:19:53,460
是的 我不知道

1961
01:19:53,460 --> 01:19:56,040
重要的部分是否有什么有趣的东西，但重要的

1962
01:19:56,040 --> 01:19:58,380
部分是相当大的，好吧，

1963
01:19:58,380 --> 01:20:00,480
有很多模型你

1964
01:20:00,480 --> 01:20:02,940
无法带回传播，你

1965
01:20:02,940 --> 01:20:04,679
可以通过创造性编码

1966
01:20:04,679 --> 01:20:06,659
或浴室传播或其他

1967
01:20:06,659 --> 01:20:07,860
方法进行训练

1968
01:20:07,860 --> 01:20:10,440
这非常有趣，当然，

1969
01:20:10,440 --> 01:20:12,719
生物系统物理系统

1970
01:20:12,719 --> 01:20:15,900
解决了各种有趣的问题，

1971
01:20:15,900 --> 01:20:17,100


1972
01:20:17,100 --> 01:20:19,380
但是蚂蚁物种仍然没有免费的午餐，

1973
01:20:19,380 --> 01:20:21,540
在

1974
01:20:21,540 --> 01:20:23,100
这种环境中表现很好的蚂蚁可能

1975
01:20:23,100 --> 01:20:25,679
在另一种环境中表现不佳，所以嗯

1976
01:20:25,679 --> 01:20:28,260
在辛特兰

1977
01:20:28,260 --> 01:20:31,820
可能有一些非常独特的特殊

1978
01:20:31,820 --> 01:20:35,880
算法，它们作为一个函数没有得到很好的描述，

1979
01:20:35,880 --> 01:20:38,460


1980
01:20:38,460 --> 01:20:42,060
但仍然提供了一种程序化的

1981
01:20:42,060 --> 01:20:46,679
方法来实现启发式算法，

1982
01:20:46,679 --> 01:20:48,840
这可能非常

1983
01:20:48,840 --> 01:20:51,120
有效，

1984
01:20:51,120 --> 01:20:53,880
不是的，是的，是的，任何

1985
01:20:53,880 --> 01:20:55,679
这都是我最关注的

1986
01:20:55,679 --> 01:20:58,260
焦点 例如，我在攻读博士学位期间的研究，

1987
01:20:58,260 --> 01:20:59,100


1988
01:20:59,100 --> 01:21:01,380
比如找到这个应用程序，它

1989
01:21:01,380 --> 01:21:04,199
就像在这里，而不是在功能内部，

1990
01:21:04,199 --> 01:21:06,739


1991
01:21:07,199 --> 01:21:08,820
很酷，

1992
01:21:08,820 --> 01:21:12,120
这项工作从这里到哪里，比如

1993
01:21:12,120 --> 01:21:14,520
你对哪些方向感到兴奋，

1994
01:21:14,520 --> 01:21:17,340
以及你如何看待主动推理

1995
01:21:17,340 --> 01:21:19,679
生态系统中的人们 参与

1996
01:21:19,679 --> 01:21:22,460
这类工作，

1997
01:21:22,500 --> 01:21:24,840
我认为一个非常有可能是最

1998
01:21:24,840 --> 01:21:27,780
有前途的方向，这是一个

1999
01:21:27,780 --> 01:21:30,060
呃，她是我可能想

2000
01:21:30,060 --> 01:21:33,060
探索一点的东西，正如我所说，

2001
01:21:33,060 --> 01:21:34,980
确实是静态

2002
01:21:34,980 --> 01:21:37,380
模型背后的东西，所以我展示的一切 到目前为止，我已经

2003
01:21:37,380 --> 01:21:40,260
展示了呃是关于静态数据的，

2004
01:21:40,260 --> 01:21:42,840
因此数据不会随着时间的推移而改变，

2005
01:21:42,840 --> 01:21:45,780


2006
01:21:45,780 --> 01:21:48,000
创意编码的定义中没有时间，就像我在这里介绍的那样，

2007
01:21:48,000 --> 01:21:49,080


2008
01:21:49,080 --> 01:21:50,940
但是您可以将

2009
01:21:50,940 --> 01:21:53,280
创意编码推广到工作中 正如

2010
01:21:53,280 --> 01:21:55,800


2011
01:21:55,800 --> 01:21:58,800
您之前提到的，使用广义坐标的时间数据通过将

2012
01:21:58,800 --> 01:22:01,380
其呈现为通用卡尔曼

2013
01:22:01,380 --> 01:22:04,140
滤波器生成模型，

2014
01:22:04,140 --> 01:22:08,040
这就是

2015
01:22:08,040 --> 01:22:09,900
因果推理方向可能非常

2016
01:22:09,900 --> 01:22:12,600
有用的地方，因为是的，那个模型在

2017
01:22:12,600 --> 01:22:14,400
这一点上也许您可以 能够对

2018
01:22:14,400 --> 01:22:17,880
更大的因果关系和更

2019
01:22:17,880 --> 01:22:21,780
复杂、更有用的

2020
01:22:21,780 --> 01:22:24,780
模型动态原因进行建模，基本上是

2021
01:22:24,780 --> 01:22:26,940
因为一般来说，微积分

2022
01:22:26,940 --> 01:22:28,560
和干预和

2023
01:22:28,560 --> 01:22:32,760
反事实科学分支

2024
01:22:32,760 --> 01:22:36,000
主要是在小型模型上开发的，

2025
01:22:36,000 --> 01:22:38,159
所以

2026
01:22:38,159 --> 01:22:40,739
就像你不这样做一样 一般来说，我们不会对

2027
01:22:40,739 --> 01:22:43,560
巨大的模型进行干预，所以如果你

2028
01:22:43,560 --> 01:22:45,800
查看医学数据，他们会使用

2029
01:22:45,800 --> 01:22:50,159
相对较小的视觉网络，但

2030
01:22:50,159 --> 01:22:51,179
当然，如果你想要一个

2031
01:22:51,179 --> 01:22:54,900
动态因果模型来模拟

2032
01:22:54,900 --> 01:22:56,340
特定环境或特定

2033
01:22:56,340 --> 01:22:58,620
现实，你就有一个 你体内的很多神经元

2034
01:22:58,620 --> 01:23:00,780
都有很多潜在的变量，它们会随着

2035
01:23:00,780 --> 01:23:02,580
时间的推移而改变，并且

2036
01:23:02,580 --> 01:23:05,219
在某个时刻进行更多的干预会

2037
01:23:05,219 --> 01:23:07,560
在不同的时间步长中产生效果，所以也许

2038
01:23:07,560 --> 01:23:09,239
在 10 个不同的

2039
01:23:09,239 --> 01:23:11,699
时间步长之后的下一个时间步长中，我认为这会

2040
01:23:11,699 --> 01:23:14,100
开发出一种

2041
01:23:14,100 --> 01:23:16,380
生物学上合理的信息传递方式是非常有趣的，它

2042
01:23:16,380 --> 01:23:17,699


2043
01:23:17,699 --> 01:23:20,040
也能够

2044
01:23:20,040 --> 01:23:22,860
基本上模拟伟大的因果关系，

2045
01:23:22,860 --> 01:23:24,659
嗯，

2046
01:23:24,659 --> 01:23:29,659
你在这些模型中的哪里看到了行动，

2047
01:23:30,840 --> 01:23:33,840
我在哪里看到了行动，

2048
01:23:33,840 --> 01:23:36,480
我没有想到

2049
01:23:36,480 --> 01:23:38,760
我认为是那些模型中的行动 模型

2050
01:23:38,760 --> 01:23:41,460
可能与我在其他模型中看到的方式相同，

2051
01:23:41,460 --> 01:23:43,080
因为

2052
01:23:43,080 --> 01:23:44,940
创造性编码基本上是一种感知模型，

2053
01:23:44,940 --> 01:23:46,260


2054
01:23:46,260 --> 01:23:49,260
所以行动就是你可以看到

2055
01:23:49,260 --> 01:23:52,739
你正在经历的事情的结果，

2056
01:23:52,739 --> 01:23:55,159
所以通过改变你

2057
01:23:55,159 --> 01:23:57,840
经历某件事的方式，然后你 可以

2058
01:23:57,840 --> 01:24:00,060
计算也许你可以简单地执行一个

2059
01:24:00,060 --> 01:24:01,800
更聪明的行动现在你有更多的

2060
01:24:01,800 --> 01:24:03,000
信息

2061
01:24:03,000 --> 01:24:04,560
但是

2062
01:24:04,560 --> 01:24:06,960
但是是的我不认为行动很

2063
01:24:06,960 --> 01:24:10,199
容易就像是的我没有看到行动的任何明确的

2064
01:24:10,199 --> 01:24:12,540
后果除了事实上

2065
01:24:12,540 --> 01:24:14,040
这可以让你基本上

2066
01:24:14,040 --> 01:24:15,960
也许你

2067
01:24:15,960 --> 01:24:18,780
只是对他们得出更好的结论，

2068
01:24:18,780 --> 01:24:21,719
在未来执行行动

2069
01:24:21,719 --> 01:24:23,940
我会补充一些

2070
01:24:23,940 --> 01:24:25,920
人们谈论预测

2071
01:24:25,920 --> 01:24:29,340
编码和行动的方式，首先内部

2072
01:24:29,340 --> 01:24:33,960
行动或隐蔽行动是注意力，这样

2073
01:24:33,960 --> 01:24:36,120
我们就可以考虑感知 作为

2074
01:24:36,120 --> 01:24:37,980
一种内部动作，这是一种

2075
01:24:37,980 --> 01:24:40,560
方法，另一种方法非常微观，是给

2076
01:24:40,560 --> 01:24:42,840
定节点的输出，我们可以将该

2077
01:24:42,840 --> 01:24:45,780
节点理解为

2078
01:24:45,780 --> 01:24:48,780
具有自己的感官认知和

2079
01:24:48,780 --> 01:24:52,080
动作状态的特定事物，因此从这个意义上说，

2080
01:24:52,080 --> 01:24:54,960
节点的输出，最后是

2081
01:24:54,960 --> 01:24:57,179
哪个 我们在直播 43 中探索了一些关于

2082
01:24:57,179 --> 01:24:59,940


2083
01:24:59,940 --> 01:25:02,100
预测编码的理论回顾，我们正在阅读

2084
01:25:02,100 --> 01:25:03,840
全文，这都是关于

2085
01:25:03,840 --> 01:25:05,460
感知的，关于感知的，然后

2086
01:25:05,460 --> 01:25:08,040
就像第 5.3 节一样，

2087
01:25:08,040 --> 01:25:11,719
如果你对行动有期望，

2088
01:25:11,719 --> 01:25:15,900
那么行动就是 这个架构中的另一个变量，

2089
01:25:15,900 --> 01:25:18,120
它确实

2090
01:25:18,120 --> 01:25:20,040
与非主动推理相一致，

2091
01:25:20,040 --> 01:25:21,659


2092
01:25:21,659 --> 01:25:24,000
我们不是像奖励或效用函数那样最大化我们

2093
01:25:24,000 --> 01:25:26,699
选择行动，而是基于它是

2094
01:25:26,699 --> 01:25:28,800
最可能的行动方案，

2095
01:25:28,800 --> 01:25:30,900
最小行动的路径，这是贝叶斯力学

2096
01:25:30,900 --> 01:25:33,300
，所以它实际上是非常重要的。 很自然地

2097
01:25:33,300 --> 01:25:36,420
引入一个动作变量并使用

2098
01:25:36,420 --> 01:25:40,800
它，就好像它是

2099
01:25:40,800 --> 01:25:43,260
对

2100
01:25:43,260 --> 01:25:45,540
世界上其他更容易接受的事物的预测一样，因为

2101
01:25:45,540 --> 01:25:48,480
我们也期待动作

2102
01:25:48,480 --> 01:25:50,820
不是是是完全

2103
01:25:50,820 --> 01:25:52,860
不我实际上非常喜欢定义动作的方式

2104
01:25:52,860 --> 01:25:55,260
并且 呃，我仍然认为，

2105
01:25:55,260 --> 01:25:57,239
例如，没有

2106
01:25:57,239 --> 01:26:01,139
那么多论文应用这种方法，我

2107
01:26:01,139 --> 01:26:03,239
认为来自

2108
01:26:03,239 --> 01:26:05,100
Alexander 或 robria 的几篇论文做了

2109
01:26:05,100 --> 01:26:08,580
类似的事情，但在实践中，就像在

2110
01:26:08,580 --> 01:26:10,920
纯粹的主动推理之外，比如应用

2111
01:26:10,920 --> 01:26:13,260
预测编码

2112
01:26:13,260 --> 01:26:15,980
解决实际问题的行动还没有被

2113
01:26:15,980 --> 01:26:19,280
探索很多

2114
01:26:19,679 --> 01:26:23,400
哦，谢谢你的精彩

2115
01:26:23,400 --> 01:26:25,199
演讲和讨论，

2116
01:26:25,199 --> 01:26:27,659
你还有什么想说的，或者

2117
01:26:27,659 --> 01:26:30,300
指出人们的方向，呃，

2118
01:26:30,300 --> 01:26:33,360
不，非常感谢你邀请

2119
01:26:33,360 --> 01:26:34,620
我 嗯，

2120
01:26:34,620 --> 01:26:36,120
这真的很有趣，我希望能

2121
01:26:36,120 --> 01:26:38,460
在某个时候回来，随时随地创作一些很酷的未来

2122
01:26:38,460 --> 01:26:40,199
作品，

2123
01:26:40,199 --> 01:26:41,580


2124
01:26:41,580 --> 01:26:45,000
谢谢托马斯，

2125
01:26:45,000 --> 01:26:49,820
谢谢你丹尼尔，再见


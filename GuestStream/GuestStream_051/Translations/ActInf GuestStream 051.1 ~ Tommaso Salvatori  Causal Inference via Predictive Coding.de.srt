1
00:00:19,020 --> 00:00:21,119
Hallo und herzlich willkommen,

2
00:00:21,119 --> 00:00:23,400
es ist aktiver Inferenz-Gaststream

3
00:00:23,400 --> 00:00:28,140
Nr. 51.1 am 28. Juli 2023.

4
00:00:28,140 --> 00:00:31,439
Wir sind hier mit Tomaso Salvatore und wir

5
00:00:31,439 --> 00:00:33,840
werden eine Präsentation und eine

6
00:00:33,840 --> 00:00:37,020
Diskussion über die aktuelle Arbeit zur kausalen

7
00:00:37,020 --> 00:00:39,660
Inferenz mittels prädiktiver Codierung halten.

8
00:00:39,660 --> 00:00:42,480
Vielen Dank für Ihre Teilnahme an alle, die

9
00:00:42,480 --> 00:00:44,340
zuschauen  Live, schreibe gerne

10
00:00:44,340 --> 00:00:47,520
Fragen in den Live-Chat und ab zu

11
00:00:47,520 --> 00:00:50,399
dir. Danke,

12
00:00:50,399 --> 00:00:52,980
vielen Dank, Daniel, dass du mich eingeladen hast. Äh,

13
00:00:52,980 --> 00:00:56,039
war schon immer ein großer Fan des

14
00:00:56,039 --> 00:00:57,719
Kanals und habe viele

15
00:00:57,719 --> 00:00:58,920
Videos angeschaut. Ich bin

16
00:00:58,920 --> 00:01:01,379
ziemlich zufrieden  Ich freue mich, hier zu sein und

17
00:01:01,379 --> 00:01:04,260
dieses Mal derjenige zu sein, der spricht,

18
00:01:04,260 --> 00:01:06,600
also werde ich über diese kürzlich veröffentlichten Vorabdrucke sprechen, die

19
00:01:06,600 --> 00:01:08,700


20
00:01:08,700 --> 00:01:11,159
die Arbeit der letzten paar

21
00:01:11,159 --> 00:01:12,119
Monate darstellen

22
00:01:12,119 --> 00:01:15,900
und die eine Zusammenarbeit mit

23
00:01:15,900 --> 00:01:18,659
with Lookup darstellen  In Ketty bin ich in Makarak

24
00:01:18,659 --> 00:01:21,600
Barami Legend Thomas Lukasiavich

25
00:01:21,600 --> 00:01:24,000
und es ist im Grunde eine gemeinsame Arbeit zwischen

26
00:01:24,000 --> 00:01:26,299
Versen, dem Unternehmen, für das ich für

27
00:01:26,299 --> 00:01:31,680
die Universität Oxford arbeite, und Uhuvian.

28
00:01:31,680 --> 00:01:34,200


29
00:01:34,200 --> 00:01:36,299
Während dieses Vortrags

30
00:01:36,299 --> 00:01:38,220
werde ich

31
00:01:38,220 --> 00:01:40,979
dies im Grunde den Grundriss des Vortrags geben,

32
00:01:40,979 --> 00:01:43,140
über den ich sprechen werde  Was ist prädiktives

33
00:01:43,140 --> 00:01:44,520
Codieren und

34
00:01:44,520 --> 00:01:47,659


35
00:01:47,659 --> 00:01:51,299


36
00:01:51,299 --> 00:01:54,060


37
00:01:54,060 --> 00:01:56,159


38
00:01:56,159 --> 00:01:58,619


39
00:01:58,619 --> 00:02:00,720


40
00:02:00,720 --> 00:02:04,560
welche Wechselwirkungen bestehen?

41
00:02:04,560 --> 00:02:07,200
Und sobald wir all diese

42
00:02:07,200 --> 00:02:08,880
Informationen zusammen haben, werde ich

43
00:02:08,880 --> 00:02:12,540
diskutieren, warum ich diesen Artikel geschrieben habe, was

44
00:02:12,540 --> 00:02:14,520
im Grunde die Forschungsfrage war, die

45
00:02:14,520 --> 00:02:16,560
mich und die anderen Mitarbeiter inspiriert hat,

46
00:02:16,560 --> 00:02:18,300


47
00:02:18,300 --> 00:02:21,660
und die wichtigsten Ergebnisse präsentieren, nämlich

48
00:02:21,660 --> 00:02:24,980
wie man

49
00:02:24,980 --> 00:02:27,480
Inferenzen durchführt, also Interventionen und

50
00:02:27,480 --> 00:02:29,340
kontrafaktische Inferenzen

51
00:02:29,340 --> 00:02:33,319
Wie man die kausalen Strukturen

52
00:02:33,319 --> 00:02:35,879
aus einem gegebenen Datensatz mithilfe von prädiktiver

53
00:02:35,879 --> 00:02:37,920
Codierung lernt, und dann werde ich natürlich

54
00:02:37,920 --> 00:02:39,959
mit einer

55
00:02:39,959 --> 00:02:43,500
kleinen Zusammenfassung und einer Diskussion darüber schließen,

56
00:02:43,500 --> 00:02:45,840
warum ich glaube, dass diese Arbeit tatsächlich wirkungsvoll sein kann,

57
00:02:45,840 --> 00:02:49,940
und einige zukünftige Richtungen nennen,

58
00:02:50,700 --> 00:02:53,400
also was ist kreatives Codieren?

59
00:02:53,400 --> 00:02:55,680
Kreatives Codieren  ist im Allgemeinen dafür bekannt, dass es sich um

60
00:02:55,680 --> 00:02:58,440
eine von den Neurowissenschaften inspirierte

61
00:02:58,440 --> 00:03:01,140
Lernmethode handelt. Die Theorie, wie die

62
00:03:01,140 --> 00:03:04,560
Informationsverarbeitung im Gehirn funktioniert,

63
00:03:04,560 --> 00:03:05,819
und

64
00:03:05,819 --> 00:03:08,400
ganz formal gesehen, kann die Ebene des

65
00:03:08,400 --> 00:03:10,560
kreativen Codierens im

66
00:03:10,560 --> 00:03:12,659
Wesentlichen als eine hierarchische

67
00:03:12,659 --> 00:03:16,319
Struktur von Neuronen im Gehirn

68
00:03:16,319 --> 00:03:19,080
und in Ihnen beschrieben werden  Es gibt zwei verschiedene Familien von

69
00:03:19,080 --> 00:03:20,700
Neuronen im Gehirn.

70
00:03:20,700 --> 00:03:23,280
Die erste Familie ist für das

71
00:03:23,280 --> 00:03:24,480
Senden von Vorhersageinformationen verantwortlich,

72
00:03:24,480 --> 00:03:27,659
sodass Neuronen in einer bestimmten

73
00:03:27,659 --> 00:03:29,959
Ebene der Hierarchie Informationen senden

74
00:03:29,959 --> 00:03:33,959
und die Aktivität der

75
00:03:33,959 --> 00:03:35,940
darunter liegenden Ebene

76
00:03:35,940 --> 00:03:38,340
und der zweiten Familie vorhersagen  Neuron ist das

77
00:03:38,340 --> 00:03:41,099
von Fehlerneuronen und den Pfeilneuronen, die

78
00:03:41,099 --> 00:03:43,019
Vorhersagefehlerinformationen in

79
00:03:43,019 --> 00:03:46,319
der Hierarchie nach oben senden, sodass eine Ebene

80
00:03:46,319 --> 00:03:49,200
die Aktivität der Ebene darunter vorhersagt.

81
00:03:49,200 --> 00:03:51,659
Diese Aktivität hat einige dieser Vorhersagen

82
00:03:51,659 --> 00:03:54,239
als eine Nichtübereinstimmung, die tatsächlich

83
00:03:54,239 --> 00:03:56,220
in der Ebene darunter stattfinden würde,

84
00:03:56,220 --> 00:03:57,840
und Informationen darüber  Der

85
00:03:57,840 --> 00:04:02,400
Vorhersagefehler wird mit der Pfeiltaste nach oben gesendet, die

86
00:04:02,400 --> 00:04:04,860
prädiktive Codierung wurde

87
00:04:04,860 --> 00:04:07,220
jedoch nicht als

88
00:04:07,220 --> 00:04:10,799
Neurowissenschaft als Theorie aus den

89
00:04:10,799 --> 00:04:11,939
Neurowissenschaften verbrannt,

90
00:04:11,939 --> 00:04:13,860
sondern ursprünglich in den 50er Jahren

91
00:04:13,860 --> 00:04:16,139
als Methode zur Signalverarbeitung und

92
00:04:16,139 --> 00:04:19,380
-komprimierung entwickelt, so die

93
00:04:19,380 --> 00:04:21,899
Arbeit von  Oliver Elias, der eigentlich ein

94
00:04:21,899 --> 00:04:25,020
Zeitgenosse von Shannon von

95
00:04:25,020 --> 00:04:26,160
Shannon ist,

96
00:04:26,160 --> 00:04:27,960
hat erkannt, dass, sobald wir einen

97
00:04:27,960 --> 00:04:30,900
Prädiktor haben, ein Modell, das gut funktioniert,

98
00:04:30,900 --> 00:04:33,600
Daten vorherzusagen ist. Das

99
00:04:33,600 --> 00:04:36,000
Senden von Nachrichten über den Fehler in

100
00:04:36,000 --> 00:04:37,919
diesen Vorhersagen ist tatsächlich viel

101
00:04:37,919 --> 00:04:41,100
billiger, als jedes Mal die gesamte Nachricht zu senden

102
00:04:41,100 --> 00:04:42,720
Zeit

103
00:04:42,720 --> 00:04:45,240
und so entstand die hübsche Codierung

104
00:04:45,240 --> 00:04:47,639


105
00:04:47,639 --> 00:04:49,500
als Signalverarbeitungs- und Komprimierungsmechanismus

106
00:04:49,500 --> 00:04:52,020
in der Informationstheorie. In

107
00:04:52,020 --> 00:04:53,639
den 50er Jahren

108
00:04:53,639 --> 00:04:57,120


109
00:04:57,120 --> 00:04:59,400


110
00:04:59,400 --> 00:05:01,800
wurde in den 80er Jahren genau das gleiche Modell

111
00:05:01,800 --> 00:05:03,540
in der Neurowissenschaft

112
00:05:03,540 --> 00:05:07,500
und ähm verwendet  So erklären wir zum Beispiel mit der Arbeit von Mumford oder

113
00:05:07,500 --> 00:05:10,440
anderen Arbeiten, wie

114
00:05:10,440 --> 00:05:12,960
die Bewertung von Prozessinformationen erfolgt, damit

115
00:05:12,960 --> 00:05:14,520
wir Vorhersagesignale von der

116
00:05:14,520 --> 00:05:17,160
Außenwelt erhalten und wir müssen

117
00:05:17,160 --> 00:05:20,280
diese Darstellung komprimieren und ähm und diese

118
00:05:20,280 --> 00:05:22,740
interne Darstellung in unseren Neuronen

119
00:05:22,740 --> 00:05:25,199
und der Methode haben  ist

120
00:05:25,199 --> 00:05:27,720
dem

121
00:05:27,720 --> 00:05:30,419
von Elias und Oliver in

122
00:05:30,419 --> 00:05:32,900
den 50er Jahren entwickelten sehr ähnlich, wenn nicht sogar gleichwertig.

123
00:05:32,940 --> 00:05:35,520
Vielleicht kam es 1999 zu dem größten Paradigmenwechsel,

124
00:05:35,520 --> 00:05:38,100


125
00:05:38,100 --> 00:05:41,400
dank der Arbeit um Ballard,

126
00:05:41,400 --> 00:05:44,880
in der sie

127
00:05:44,880 --> 00:05:46,199
dieses von mir eingeführte Konzept eingeführt haben  Ich habe zuvor

128
00:05:46,199 --> 00:05:48,060
über hierarchische Strukturen im

129
00:05:48,060 --> 00:05:51,360
Gehirn gesprochen, bei denen die Vorhersageinformationen von

130
00:05:51,360 --> 00:05:54,240
oben nach unten und die Fehlerinformationen von

131
00:05:54,240 --> 00:05:55,560
unten nach oben erfolgen,

132
00:05:55,560 --> 00:05:57,660
und etwas, was sie

133
00:05:57,660 --> 00:05:59,759
zuvor noch nicht getan haben, ist, dass

134
00:05:59,759 --> 00:06:02,820
sie diese Theorie

135
00:06:02,820 --> 00:06:05,759
nicht nur in Frankreich, sondern nur in Frankreich erklären und weiterentwickeln  Es geht darum,

136
00:06:05,759 --> 00:06:07,979
wie Lernen im

137
00:06:07,979 --> 00:06:10,139
Gehirn funktioniert, also ist es auch eine Theorie darüber, wie unsere

138
00:06:10,139 --> 00:06:13,139
Synapsen aktualisiert werden,

139
00:06:13,139 --> 00:06:16,080
und der letzte große Durchbruch, über den ich

140
00:06:16,080 --> 00:06:17,940
in dieser kurzen

141
00:06:17,940 --> 00:06:21,900
historischen Einführung sprechen werde, stammt aus dem Jahr 2003, aber

142
00:06:21,900 --> 00:06:25,380
dann hat er im Jahr 2003 weitergemacht

143
00:06:25,380 --> 00:06:28,380
Jahre später, ähm, dank Car Freeston, in

144
00:06:28,380 --> 00:06:32,100
dem er im Grunde genommen die Theorie von

145
00:06:32,100 --> 00:06:35,039
Robin Ballard übernahm und entwickelte,

146
00:06:35,039 --> 00:06:38,400
erweiterte sie und verallgemeinerte sie auf

147
00:06:38,400 --> 00:06:40,919
die Theorie generativer Modelle.

148
00:06:40,919 --> 00:06:42,720
Die Hauptbehauptung, die

149
00:06:42,720 --> 00:06:45,479
Carfiston machte, ist also im Grunde, dass kreatives Codieren

150
00:06:45,479 --> 00:06:48,780
eine ist  Beweismaximierungsschema einer

151
00:06:48,780 --> 00:06:50,340
bestimmten Art von generativem Modell,

152
00:06:50,340 --> 00:06:52,979
das ich

153
00:06:52,979 --> 00:06:55,139
später ebenfalls vorstellen werde, um

154
00:06:55,139 --> 00:07:00,300
eine kurze Zusammenfassung in den

155
00:07:00,300 --> 00:07:01,560
ersten beiden

156
00:07:01,560 --> 00:07:03,900
äh-Arten kreativer Ecke zu erstellen, die ich

157
00:07:03,900 --> 00:07:05,340
beschrieben habe, also Signalverarbeitung und

158
00:07:05,340 --> 00:07:07,020
-komprimierung und die

159
00:07:07,020 --> 00:07:09,180
Informationsverarbeitung in der Netzhaut und im

160
00:07:09,180 --> 00:07:11,160
Gehirn im Allgemeinen sind Inferenzmethoden

161
00:07:11,160 --> 00:07:12,300


162
00:07:12,300 --> 00:07:14,819
und die größte

163
00:07:14,819 --> 00:07:17,819
Veränderung, die größte Revolution, die wir

164
00:07:17,819 --> 00:07:21,120
1999 hatten, also sagen wir im 21.

165
00:07:21,120 --> 00:07:23,580
Jahrhundert, ist die operative Kodierung, die

166
00:07:23,580 --> 00:07:25,919
als lernender Algorithmus angesehen wurde, sodass wir

167
00:07:25,919 --> 00:07:29,699
Informationen zunächst komprimieren können  und aktualisieren Sie dann alle

168
00:07:29,699 --> 00:07:31,800
Synapsen oder alle latenten Variablen,

169
00:07:31,800 --> 00:07:34,139
die wir in unserem generativen Modell haben, um

170
00:07:34,139 --> 00:07:38,599
unser generatives Modell selbst zu verbessern.

171
00:07:38,759 --> 00:07:43,199
Lassen Sie uns also einige Definitionen geben,

172
00:07:43,199 --> 00:07:45,000
die etwas formaler sind,

173
00:07:45,000 --> 00:07:48,479
damit die operative Codierung als

174
00:07:48,479 --> 00:07:50,220
hierarchisches Gaußsches Generativ betrachtet werden kann  Modell,

175
00:07:50,220 --> 00:07:53,400
also hier ist eine sehr einfache Abbildung, in der

176
00:07:53,400 --> 00:07:54,780
wir diese hierarchische Struktur haben,

177
00:07:54,780 --> 00:07:58,319
die so tief sein kann, wie wir wollen,

178
00:07:58,319 --> 00:08:01,560
und Signalvorhersagesignale gehen

179
00:08:01,560 --> 00:08:04,620
von einer latenten Variablen XM zur

180
00:08:04,620 --> 00:08:06,599
nächsten und werden

181
00:08:06,599 --> 00:08:09,720
jedes Mal über die Funktion GN

182
00:08:09,720 --> 00:08:12,620
oder GI transformiert

183
00:08:15,319 --> 00:08:18,180
Dies ist ein generatives Modell, wie ich bereits sagte, und

184
00:08:18,180 --> 00:08:19,680
wie hoch ist die Grenzwahrscheinlichkeit dieses

185
00:08:19,680 --> 00:08:21,780
generativen Modells? Nun, es ist einfach die

186
00:08:21,780 --> 00:08:24,960
Wahrscheinlichkeit

187
00:08:24,960 --> 00:08:27,660


188
00:08:27,660 --> 00:08:29,940


189
00:08:29,940 --> 00:08:32,700
des letzten Scheitelpunkts. Können Sie meinen Cursor sehen?

190
00:08:32,700 --> 00:08:34,979
des letzten Scheitelpunkts multipliziert mit

191
00:08:34,979 --> 00:08:37,140
der Wahrscheinlichkeitsverteilung jedes

192
00:08:37,140 --> 00:08:40,440
anderen Scheitelpunkts, abhängig von der Aktivität

193
00:08:40,440 --> 00:08:43,020
des Scheitelpunkts vor oder der

194
00:08:43,020 --> 00:08:45,860
latenten Variablen, bevor

195
00:08:45,899 --> 00:08:48,240
ich bereits sagte, dass es sich um ein Gaußsches

196
00:08:48,240 --> 00:08:50,399
generatives Modell handelt, was bedeutet, dass diese

197
00:08:50,399 --> 00:08:54,260
Wahrscheinlichkeiten in Gaußscher Form vorliegen

198
00:08:54,660 --> 00:08:57,120
und jede

199
00:08:57,120 --> 00:09:00,480
Endos-Funktion vorliegt  Funktion G im Allgemeinen und

200
00:09:00,480 --> 00:09:02,880
insbesondere, da zum Beispiel in

201
00:09:02,880 --> 00:09:05,459
Rambler-Artikeln und in allen Artikeln, die

202
00:09:05,459 --> 00:09:07,920
danach kamen, auch aufgrund der Deep-

203
00:09:07,920 --> 00:09:10,500
Learning-Revolution diese Funktionen

204
00:09:10,500 --> 00:09:13,220
einfach lineare Karten oder

205
00:09:13,220 --> 00:09:15,120
nichtlineare Karten mit

206
00:09:15,120 --> 00:09:18,000
Aktivierungsfunktionen oder nichtlineare Karten mit

207
00:09:18,000 --> 00:09:22,040
Aktivierungsfunktion und einer sind  Additive Voreingenommenheit,

208
00:09:23,220 --> 00:09:27,180
damit wir eine formale

209
00:09:27,180 --> 00:09:28,860
Definition der kreativen Kodierung geben können und wir können

210
00:09:28,860 --> 00:09:30,300
sagen, dass die operative Kodierung ein

211
00:09:30,300 --> 00:09:33,480
Umkehrschema für ein solches generatives Modell ist, bei dem

212
00:09:33,480 --> 00:09:35,839
seine Modellbeweiskraft durch

213
00:09:35,839 --> 00:09:38,760
Minimierung einer Größe maximiert wird, die im Allgemeinen als

214
00:09:38,760 --> 00:09:40,920
Variation der freien Energie bezeichnet wird

215
00:09:40,920 --> 00:09:43,740
Ziel jedes generativen

216
00:09:43,740 --> 00:09:46,019
Modells ist es, die Modellbeweiskraft zu maximieren, aber

217
00:09:46,019 --> 00:09:48,860
diese Größe ist immer unlösbar und

218
00:09:48,860 --> 00:09:51,019
wir verfügen über einige

219
00:09:51,019 --> 00:09:53,279
Techniken, die es uns ermöglichen,

220
00:09:53,279 --> 00:09:55,980
die Lösung anzunähern, und zwar die,

221
00:09:55,980 --> 00:09:58,500
die wir in der kreativen Codierung verwenden,

222
00:09:58,500 --> 00:10:00,720
anstatt die Aberration der freien

223
00:10:00,720 --> 00:10:03,480
Energie zu minimieren, was eine ist  Das ist eine untere Grenze

224
00:10:03,480 --> 00:10:06,839
der Modellbeweise in dieser Arbeit und

225
00:10:06,839 --> 00:10:09,660
tatsächlich in vielen

226
00:10:09,660 --> 00:10:11,700
anderen, also ist es die Standardmethode,

227
00:10:11,700 --> 00:10:13,740
diese Minimierung durchzuführen. Die

228
00:10:13,740 --> 00:10:16,080
Zutat Abstieg

229
00:10:16,080 --> 00:10:18,540
ähm und ja durchführen, wir haben uns im Abstieg geeinigt

230
00:10:18,540 --> 00:10:19,980
und es gibt sie  Tatsächlich gibt es andere Methoden

231
00:10:19,980 --> 00:10:22,140
wie die Erwartungsmaximierung, die

232
00:10:22,140 --> 00:10:23,580
oft gleichwertig ist,

233
00:10:23,580 --> 00:10:25,140
oder Sie können andere

234
00:10:25,140 --> 00:10:26,940
Algorithmen zur Nachrichtenweitergabe verwenden, wie

235
00:10:26,940 --> 00:10:29,959
zum Beispiel die Verbreitung von Überzeugungen,

236
00:10:30,720 --> 00:10:33,980
und ein wenig in die Vergangenheit reisen, sodass wir

237
00:10:33,980 --> 00:10:35,940
die statistischen generativen Modelle ein wenig vergessen,

238
00:10:35,940 --> 00:10:38,760


239
00:10:38,760 --> 00:10:41,360
wenn wir kreatives Codieren sehen können

240
00:10:41,360 --> 00:10:44,040
Ich meine, ich habe bereits ein paar

241
00:10:44,040 --> 00:10:46,200
Mal gesagt, dass es sich um ein hierarchisches Modell

242
00:10:46,200 --> 00:10:48,420
mit neuronalen Aktivitäten handelt, also mit

243
00:10:48,420 --> 00:10:50,700
latenten Neuronenvariablen, die

244
00:10:50,700 --> 00:10:53,459
neuronale Aktivitäten darstellen, signalisiert der Absender die Hierarchie nach unten,

245
00:10:53,459 --> 00:10:54,899


246
00:10:54,899 --> 00:10:57,540
und bei Fehlerknoten oder Fehlerneuronen

247
00:10:57,540 --> 00:11:01,019
signalisiert der Absender die Hierarchie nach oben, also

248
00:11:01,019 --> 00:11:03,660
dies und  Die Fehlerinformationen geben an,

249
00:11:03,660 --> 00:11:05,700
wie groß die Variation der freien Energie

250
00:11:05,700 --> 00:11:08,220
dieser von dieser Klasse betriebenen Codierungsmodelle ist. Es ist

251
00:11:08,220 --> 00:11:09,899
einfach die Summe

252
00:11:09,899 --> 00:11:12,720
des mittleren quadratischen Fehlers aller

253
00:11:12,720 --> 00:11:14,399
Fehlerneuronen,

254
00:11:14,399 --> 00:11:18,120
also ist es die Summe des Fehlers

255
00:11:18,120 --> 00:11:21,980
des gesamten Fehlers im Quadrat

256
00:11:22,019 --> 00:11:24,480
und diese Darstellung ist  Ich werde

257
00:11:24,480 --> 00:11:27,120
in den späteren Folien nützlich sein und dabei,

258
00:11:27,120 --> 00:11:28,740
wie ich zum Beispiel erklären werde, wie man

259
00:11:28,740 --> 00:11:30,120
kreatives Codieren verwendet, um kausale Schlussfolgerungen zu modellieren.

260
00:11:30,120 --> 00:11:32,940


261
00:11:32,940 --> 00:11:34,800
Ich denke, prädiktives Codieren ist wichtig

262
00:11:34,800 --> 00:11:36,240
und es ist kein guter Algorithmus, den man

263
00:11:36,240 --> 00:11:37,500


264
00:11:37,500 --> 00:11:39,600
zunächst gut studieren kann  Ich sagte vorhin, es

265
00:11:39,600 --> 00:11:41,399
optimiert das richtige Ziel, das

266
00:11:41,399 --> 00:11:43,079
der Modellbeweis oder die Grenzwahrscheinlichkeit ist,

267
00:11:43,079 --> 00:11:44,339


268
00:11:44,339 --> 00:11:45,660
und

269
00:11:45,660 --> 00:11:47,700
dann optimiert es eine

270
00:11:47,700 --> 00:11:49,440
Untergrenze, die wie gesagt Variation der

271
00:11:49,440 --> 00:11:52,440
freien Energie genannt wird, und das virtuelle

272
00:11:52,440 --> 00:11:54,240
Ende ist interessant, weil es

273
00:11:54,240 --> 00:11:57,680
als geschrieben werden kann  Summe zweier verschiedener Begriffe,

274
00:11:57,680 --> 00:12:00,839
die jeweils

275
00:12:00,839 --> 00:12:04,680
so wichtig sind, dass sie optimiert werden,

276
00:12:04,680 --> 00:12:06,899
beispielsweise bei maschinellen Lernaufgaben

277
00:12:06,899 --> 00:12:09,060
oder allgemein bei Lernaufgaben

278
00:12:09,060 --> 00:12:12,420


279
00:12:12,420 --> 00:12:15,440


280
00:12:15,440 --> 00:12:18,180
Um an einen bestimmten Datensatz anzupassen,

281
00:12:18,180 --> 00:12:19,560


282
00:12:19,560 --> 00:12:21,240


283
00:12:21,240 --> 00:12:23,519
zwingt der erste Term das Modell dazu, die Komplexität zu minimieren.

284
00:12:23,519 --> 00:12:26,040
Wie wir zum Beispiel

285
00:12:26,040 --> 00:12:28,500
aus der Razor-

286
00:12:28,500 --> 00:12:31,260
Theorie der Ergebnisse wissen, wenn wir zwei verschiedene Modelle haben,

287
00:12:31,260 --> 00:12:33,000
die auf einem bestimmten Trainingssatz ähnlich funktionieren wie

288
00:12:33,000 --> 00:12:35,640
der, den wir haben  zu bekommen,

289
00:12:35,640 --> 00:12:37,380
und dasjenige, von dem erwartet wird, dass es

290
00:12:37,380 --> 00:12:39,899
am meisten verallgemeinert, ist das weniger

291
00:12:39,899 --> 00:12:41,160
komplexe Modell. Die

292
00:12:41,160 --> 00:12:44,100
Aktualisierung eines generativen Modells über die

293
00:12:44,100 --> 00:12:46,380
operative freie Energie ermöglicht es uns also,

294
00:12:46,380 --> 00:12:47,779
grundsätzlich

295
00:12:47,779 --> 00:12:51,959
zum optimalen Ergebnis-Rasiermessermodell zu konvergieren,

296
00:12:51,959 --> 00:12:54,720
das einerseits einen

297
00:12:54,720 --> 00:12:56,100
Datensatz speichert, andererseits aber auch  In der Lage,

298
00:12:56,100 --> 00:12:58,680
sehr gut auf unsichtbare, unsichtbare Datenpunkte zu verallgemeinern.

299
00:12:58,680 --> 00:13:00,240


300
00:13:00,240 --> 00:13:02,639
Ein zweiter Grund, warum operative Codierung

301
00:13:02,639 --> 00:13:08,600
wichtig ist, ist, dass sie eigentlich

302
00:13:08,720 --> 00:13:11,760
nicht auf einer hierarchischen Struktur definiert werden muss,

303
00:13:11,760 --> 00:13:13,920
sondern

304
00:13:13,920 --> 00:13:15,959
auf komplexeren und flexibleren

305
00:13:15,959 --> 00:13:18,240
Architekturen wie gerichteter grafischer Architektur modelliert werden kann

306
00:13:18,240 --> 00:13:21,540
Modell mit beliebiger Form oder noch

307
00:13:21,540 --> 00:13:23,700
weiter verallgemeinert auf Netzwerke mit vielen Zyklen,

308
00:13:23,700 --> 00:13:25,920
die einer Gehirnregion ähneln, und das Endergebnis

309
00:13:25,920 --> 00:13:27,779


310
00:13:27,779 --> 00:13:30,300
ist, dass der zugrunde liegende Grund darin besteht, dass Sie nicht

311
00:13:30,300 --> 00:13:32,339
mit einem Vorwärtsdurchlauf lernen und vorhersagen und

312
00:13:32,339 --> 00:13:34,260
den Fehler dann rückwärts verbreiten, sondern dass Sie es tun  Die

313
00:13:34,260 --> 00:13:36,600
Minimierung einer Energiefunktion

314
00:13:36,600 --> 00:13:38,459
ermöglicht es im Grunde, jede Art von

315
00:13:38,459 --> 00:13:39,839
Hierarchie zu erzeugen. Sie

316
00:13:39,839 --> 00:13:41,180


317
00:13:41,180 --> 00:13:43,860
ermöglicht es, hinter Direkttasten zu gehen und

318
00:13:43,860 --> 00:13:46,860
Zyklen zu lernen. Dies ist

319
00:13:46,860 --> 00:13:48,060
tatsächlich ziemlich wichtig, da das

320
00:13:48,060 --> 00:13:50,399
Gehirn voller Zyklen ist, da wir

321
00:13:50,399 --> 00:13:53,399
einige Informationen aus einigen neueren Arbeiten haben

322
00:13:53,399 --> 00:13:56,459
Äh, die es geschafft haben,

323
00:13:56,459 --> 00:13:59,279
das Gehirn einiger Tiere wie

324
00:13:59,279 --> 00:14:00,420
Fruchtfliegen vollständig abzubilden. Das

325
00:14:00,420 --> 00:14:03,899
Gehirn ist voller Zyklen, daher ist es

326
00:14:03,899 --> 00:14:06,720
sinnvoll, unsere Modelle für maschinelles Lernen

327
00:14:06,720 --> 00:14:09,000
oder

328
00:14:09,000 --> 00:14:11,160
unsere Modelle im Allgemeinen mit einem Algorithmus zu entleeren,

329
00:14:11,160 --> 00:14:14,160
der es uns ermöglicht, mithilfe von Zyklen zu entleeren

330
00:14:14,160 --> 00:14:17,160
Strukturen

331
00:14:17,160 --> 00:14:19,380
Der dritte Grund, warum operative Codierung

332
00:14:19,380 --> 00:14:21,240
interessant ist, besteht darin, dass formal

333
00:14:21,240 --> 00:14:23,820
bewiesen wurde, dass sie robuster ist als

334
00:14:23,820 --> 00:14:25,139
standardmäßige neuronale Netzwerke, beginnend mit der

335
00:14:25,139 --> 00:14:27,060
Schwarzausbreitung. Wenn Sie also über ein

336
00:14:27,060 --> 00:14:28,200
neuronales Netzwerk verfügen und Klassifizierungsaufgaben durchführen möchten,

337
00:14:28,200 --> 00:14:30,320


338
00:14:30,320 --> 00:14:34,139
ist die kreative Codierung robuster

339
00:14:34,139 --> 00:14:36,260
und robuster  Dies ist

340
00:14:36,260 --> 00:14:38,339
bei Aufgaben wie Online-

341
00:14:38,339 --> 00:14:40,680
Lerntraining an kleinen Datensätzen oder

342
00:14:40,680 --> 00:14:43,440
kontinuierlichen Lernaufgaben interessant und die Theorie beruht

343
00:14:43,440 --> 00:14:45,540
im Wesentlichen auf der Tatsache, dass die

344
00:14:45,540 --> 00:14:48,540
imperative Codierung auf den

345
00:14:48,540 --> 00:14:50,820
annähernden impliziten Gradientenabstieg verlagert wurde,

346
00:14:50,820 --> 00:14:53,339
der eine andere Version des

347
00:14:53,339 --> 00:14:54,899
expliziten Gradientenabstiegs darstellt  Der

348
00:14:54,899 --> 00:14:57,180
standardmäßige grüne Abstieg, der in

349
00:14:57,180 --> 00:14:59,880
jedem einzelnen Modell verwendet wird,

350
00:14:59,880 --> 00:15:03,680
ist grundsätzlich eine Variante, die robuster ist.

351
00:15:05,880 --> 00:15:08,279
Ich denke, okay, ich habe eine ziemlich lange

352
00:15:08,279 --> 00:15:09,779
intraoperative Codierung durchgeführt. Ich glaube, ich komme jetzt

353
00:15:09,779 --> 00:15:11,639
zum zweiten Thema, nämlich der kausalen

354
00:15:11,639 --> 00:15:13,019
Schlussfolgerung

355
00:15:13,019 --> 00:15:15,839
und dem, was ist  Kausaler Rückschluss Kausaler

356
00:15:15,839 --> 00:15:18,420
Schluss ist eine Theorie, es ist eine sehr

357
00:15:18,420 --> 00:15:20,339
allgemeine Theorie, die

358
00:15:20,339 --> 00:15:23,100
am meisten von Judy Apparel formalisiert wurde. Er ist definitiv

359
00:15:23,100 --> 00:15:25,500
die wichtigste Person auf dem

360
00:15:25,500 --> 00:15:27,839
Gebiet der Kausalität in Frankreich. Er hat einige

361
00:15:27,839 --> 00:15:29,760
sehr schöne Bücher geschrieben, zum Beispiel ist das Buch

362
00:15:29,760 --> 00:15:32,760
Y sehr zu empfehlen  Wenn Sie

363
00:15:32,760 --> 00:15:35,220
mehr über dieses Thema erfahren möchten

364
00:15:35,220 --> 00:15:37,800
und es im Wesentlichen das folgende Problem angeht, gehen

365
00:15:37,800 --> 00:15:38,639


366
00:15:38,639 --> 00:15:40,440
wir davon aus, dass wir eine gemeinsame

367
00:15:40,440 --> 00:15:42,000
Wahrscheinlichkeitsverteilung haben, die

368
00:15:42,000 --> 00:15:44,160
mit einem Bayes'schen Netzwerk verbunden ist. Dies

369
00:15:44,160 --> 00:15:46,199
wird ein bisschen das laufende

370
00:15:46,199 --> 00:15:49,260
Beispiel für den gesamten Artikel sein,

371
00:15:49,260 --> 00:15:51,839
insbesondere wenn  Sie sind nicht mit asiatischen

372
00:15:51,839 --> 00:15:54,480
Netzwerken dieser Form vertraut,

373
00:15:54,480 --> 00:15:57,660
es basierte auf Netzwerken, deren

374
00:15:57,660 --> 00:16:00,240
Variablen darin

375
00:16:00,240 --> 00:16:02,100
unterschiedliche Größen darstellen können, so dass beispielsweise unser

376
00:16:02,100 --> 00:16:04,620
visuelles Netzwerk mit dieser Form

377
00:16:04,620 --> 00:16:06,899


378
00:16:06,899 --> 00:16:08,820
die Mengen auf der rechten Seite darstellen kann, also eine

379
00:16:08,820 --> 00:16:10,800
sozialökonomische Studiostatue eines

380
00:16:10,800 --> 00:16:13,079
Individuums  Bildungsniveau, seine

381
00:16:13,079 --> 00:16:16,699
Intelligenz und sein Einkommensniveau

382
00:16:17,100 --> 00:16:19,440
sind etwas, in dem die klassische Statistik

383
00:16:19,440 --> 00:16:22,920
sehr gut ist, und ist es, äh, während äh, die

384
00:16:22,920 --> 00:16:25,320
am häufigsten verwendete Anwendung darin besteht,

385
00:16:25,320 --> 00:16:28,019
Beobachtungen oder Korrelationen zu modellieren. Eine

386
00:16:28,019 --> 00:16:29,279
Korrelation beantwortet im Grunde die

387
00:16:29,279 --> 00:16:32,519
Frage, was ist, wenn wir

388
00:16:32,519 --> 00:16:35,579
eine andere Variable C beobachten,

389
00:16:35,579 --> 00:16:37,500
also zum Beispiel in  Wie hoch ist in diesem Fall

390
00:16:37,500 --> 00:16:39,660
das Einkommensniveau? Das erwartete

391
00:16:39,660 --> 00:16:41,820
Einkommensniveau einer Person, wenn ich

392
00:16:41,820 --> 00:16:44,339
dieses Bildungsniveau beobachte,

393
00:16:44,339 --> 00:16:48,180
und wenn diese Person natürlich

394
00:16:48,180 --> 00:16:50,220
einen höheren Bildungsabschluss hat, zum

395
00:16:50,220 --> 00:16:52,500
Beispiel einen Master oder einen Doktortitel, erwarte ich im

396
00:16:52,500 --> 00:16:54,360
Allgemeinen, dass diese Person einen hat  ein höheres

397
00:16:54,360 --> 00:16:56,040
Einkommensniveau,

398
00:16:56,040 --> 00:16:58,139
und das ist eine Korrelation.

399
00:16:58,139 --> 00:17:00,300
Allerdings gibt es manchmal Dinge, die

400
00:17:00,300 --> 00:17:03,300
sehr schwer zu beobachten sind, aber sie spielen eine

401
00:17:03,300 --> 00:17:05,040
große Rolle bei der Bestimmung dieser

402
00:17:05,040 --> 00:17:06,119
Mengen,

403
00:17:06,119 --> 00:17:08,220
so könnte es beispielsweise sein, dass das

404
00:17:08,220 --> 00:17:11,160
Einkommensniveau viel stärker

405
00:17:11,160 --> 00:17:13,380
von der Intelligenz eines Menschen bestimmt wird  Eine

406
00:17:13,380 --> 00:17:15,540
bestimmte Person

407
00:17:15,540 --> 00:17:18,720
und und und vielleicht, dass die Intelligenz oder

408
00:17:18,720 --> 00:17:21,000
wenn eine Person intelligent ist, höchstwahrscheinlich auch

409
00:17:21,000 --> 00:17:24,540
ein höheres Bildungsniveau hat,

410
00:17:24,540 --> 00:17:27,540
aber der wahre Grund, warum das

411
00:17:27,540 --> 00:17:30,120
Einkommen so hoch ist, liegt immer noch am

412
00:17:30,120 --> 00:17:32,220
IQ,

413
00:17:32,220 --> 00:17:34,740
und das kann sein, das kann nicht  Studien basieren auf

414
00:17:34,740 --> 00:17:36,360
einfachen Korrelationen und müssen

415
00:17:36,360 --> 00:17:39,120
mit einer fortgeschritteneren Technik untersucht werden,

416
00:17:39,120 --> 00:17:41,280
die als Intervention bezeichnet wird.

417
00:17:41,280 --> 00:17:43,320
Eine Intervention beantwortet im Grunde die

418
00:17:43,320 --> 00:17:46,500
Frage, was D ist, wenn wir C in

419
00:17:46,500 --> 00:17:48,240
einen bestimmten Wert ändern,

420
00:17:48,240 --> 00:17:51,000
sodass wir beispielsweise eine Person nehmen können

421
00:17:51,000 --> 00:17:54,660
und überprüfen Sie sein Einkommensniveau

422
00:17:54,660 --> 00:17:57,120
und ändern Sie dann sein Bildungsniveau.

423
00:17:57,120 --> 00:17:59,220
Greifen Sie also in diese Welt ein

424
00:17:59,220 --> 00:18:01,080
und ändern Sie sein Bildungsniveau, ohne

425
00:18:01,080 --> 00:18:03,419
seine Intelligenz anzutasten, und sehen Sie, um wie

426
00:18:03,419 --> 00:18:07,260
viel sich sein Einkommen ändert.

427
00:18:07,260 --> 00:18:09,900
Wenn sich beispielsweise das Einkommen stark ändert,

428
00:18:09,900 --> 00:18:12,179
bedeutet dies, dass die Intelligenz

429
00:18:12,179 --> 00:18:14,460
nicht beeinträchtigt wird.  Das spielt dabei keine große Rolle, aber das

430
00:18:14,460 --> 00:18:16,799
Bildungsniveau spielt eine große Rolle. Wenn sich das Einkommensniveau

431
00:18:16,799 --> 00:18:19,020
nicht wesentlich ändert, bedeutet das, dass es möglicherweise

432
00:18:19,020 --> 00:18:20,640
eine versteckte Variable gibt, in diesem Fall

433
00:18:20,640 --> 00:18:22,860
die Intelligenz, die das

434
00:18:22,860 --> 00:18:25,760
Einkommensniveau einer Person bestimmt.

435
00:18:25,980 --> 00:18:28,740
Die dritte Größe ist eine wichtige kausale

436
00:18:28,740 --> 00:18:31,080
Schlussfolgerung  von Kontrafaktualen, sodass

437
00:18:31,080 --> 00:18:33,120
beispielsweise ein Kontrafaktual die

438
00:18:33,120 --> 00:18:36,720
Frage beantwortet, was wäre, und wir

439
00:18:36,720 --> 00:18:39,240
C auf einen anderen Wert in der Vergangenheit ändern,

440
00:18:39,240 --> 00:18:40,679
sodass wir beispielsweise sehen können, dass der

441
00:18:40,679 --> 00:18:42,059
Unterschied zwischen Interventionen und

442
00:18:42,059 --> 00:18:45,059
Kontrafaktualen darin besteht, dass Interventionen

443
00:18:45,059 --> 00:18:47,820
in der Zukunft wirken, also interviewe ich  in

444
00:18:47,820 --> 00:18:50,340
der heutigen Welt, um eine Veränderung in der Zukunft zu beobachten. Nun,

445
00:18:50,340 --> 00:18:53,220
kontrafaktisch erlauben es uns,

446
00:18:53,220 --> 00:18:56,039
in der Zeit zurückzugehen und eine Variable

447
00:18:56,039 --> 00:18:59,160
in der Zeit zu verändern und zu sehen, wie diese Veränderung

448
00:18:59,160 --> 00:19:01,320
die Welt, in der wir jetzt leben, beeinflusst hätte,

449
00:19:01,320 --> 00:19:02,940


450
00:19:02,940 --> 00:19:06,299
und diese werden von Judapple als die

451
00:19:06,299 --> 00:19:08,100
drei definiert  Die Ebenen der kausalen Inferenzkorrelation

452
00:19:08,100 --> 00:19:09,660
sind die erste

453
00:19:09,660 --> 00:19:11,580
Interventionsebene, die zweite

454
00:19:11,580 --> 00:19:14,720
kontrafaktische Interventionsebene und die dritte Ebene.

455
00:19:16,020 --> 00:19:18,120
Andere Interventionen werde ich

456
00:19:18,120 --> 00:19:20,640
jetzt, da ich

457
00:19:20,640 --> 00:19:23,760
eine intuitive Definition gegeben habe, formaler definieren und

458
00:19:23,760 --> 00:19:25,500
verwende hier diese Notation  Das ist

459
00:19:25,500 --> 00:19:27,240
eigentlich während der gesamten

460
00:19:27,240 --> 00:19:29,640
Präsentation das Gleiche, also wird  Wenn wir uns

461
00:19:29,640 --> 00:19:32,820


462
00:19:32,820 --> 00:19:35,340


463
00:19:35,340 --> 00:19:38,520


464
00:19:38,520 --> 00:19:40,860


465
00:19:40,860 --> 00:19:42,720


466
00:19:42,720 --> 00:19:45,299
beispielsweise für die Struktur des Diagramms interessieren, gehen

467
00:19:45,299 --> 00:19:46,860
wir davon aus, dass wir ein Bayes'sches Modell haben,

468
00:19:46,860 --> 00:19:50,160
das die gleiche Struktur hat

469
00:19:50,160 --> 00:19:52,679
wie das Bayes'sche Modell, das wir auf der vorherigen Folie gesehen haben,

470
00:19:52,679 --> 00:19:54,780


471
00:19:54,780 --> 00:19:57,840
vorausgesetzt, dass X3 gleich S3 ist. Dies ist die

472
00:19:57,840 --> 00:20:00,660
Beobachtung, die unsere Statistik zulässt

473
00:20:00,660 --> 00:20:03,360
Um die Wahrscheinlichkeit oder den

474
00:20:03,360 --> 00:20:04,679
Erwartungswert von

475
00:20:04,679 --> 00:20:07,380


476
00:20:07,380 --> 00:20:09,240


477
00:20:09,240 --> 00:20:13,860


478
00:20:13,860 --> 00:20:15,679


479
00:20:15,679 --> 00:20:17,760


480
00:20:17,760 --> 00:20:19,919


481
00:20:19,919 --> 00:20:21,179


482
00:20:21,179 --> 00:20:23,880


483
00:20:23,880 --> 00:20:26,100
die Wahrscheinlichkeit von  Wir

484
00:20:26,100 --> 00:20:30,000


485
00:20:30,000 --> 00:20:33,059


486
00:20:33,059 --> 00:20:35,580


487
00:20:35,580 --> 00:20:38,400


488
00:20:38,400 --> 00:20:40,020


489
00:20:40,020 --> 00:20:41,880


490
00:20:41,880 --> 00:20:45,059


491
00:20:45,059 --> 00:20:46,860
müssen alle entfernen, um alle

492
00:20:46,860 --> 00:20:50,160
eingehenden Kanten zu V3 zu entfernen,

493
00:20:50,160 --> 00:20:52,799
also müssen wir nicht dieses Bayes'sche

494
00:20:52,799 --> 00:20:55,679
Netzwerk, sondern dieses zweite untersuchen,

495
00:20:55,679 --> 00:20:58,200
und dann dürfen wir an diesem Punkt

496
00:20:58,200 --> 00:21:00,840
eine Korrelation berechnen, wie wir es

497
00:21:00,840 --> 00:21:03,299
normalerweise tun,

498
00:21:03,299 --> 00:21:06,500
und dies ist

499
00:21:07,020 --> 00:21:09,299
ein kontrafaktischer Eingriff  ist eine Verallgemeinerung

500
00:21:09,299 --> 00:21:11,700
dessen, was, wie ich bereits sagte, in der Vergangenheit gelebt hat

501
00:21:11,700 --> 00:21:14,100
und sie mit strukturellen Kausalmodellen rechnen.

502
00:21:14,100 --> 00:21:15,419


503
00:21:15,419 --> 00:21:18,299
Ein Strukturkausalmodell ist ein Tupel,

504
00:21:18,299 --> 00:21:21,120
das konzeptionell einem

505
00:21:21,120 --> 00:21:23,460
Bayes'schen Netzwerk ähnelt, aber im Grunde haben wir

506
00:21:23,460 --> 00:21:26,220
diese neue Klasse von Variablen darüber

507
00:21:26,220 --> 00:21:28,580
sind die nicht beobachtbaren Variablen, die sie verwenden,

508
00:21:28,580 --> 00:21:30,960
also haben wir das Bayesianische Netzwerk, das wir

509
00:21:30,960 --> 00:21:34,020
vor X1 X2 X3 S4 hatten,

510
00:21:34,020 --> 00:21:37,460
aber wir haben auch diese nicht beobachtbaren Variablen oder

511
00:21:37,460 --> 00:21:40,020
Variablen, die von der Umgebung

512
00:21:40,020 --> 00:21:42,539
abhängen

513
00:21:42,539 --> 00:21:43,980


514
00:21:43,980 --> 00:21:46,020


515
00:21:46,020 --> 00:21:48,539


516
00:21:48,539 --> 00:21:51,360
ist eine Menge von Funktionen, die

517
00:21:51,360 --> 00:21:53,400


518
00:21:53,400 --> 00:21:57,299
grundsätzlich von f von x von x3 abhängt, hängt von X1 ab,

519
00:21:57,299 --> 00:21:58,980
weil wir einen Pfeil auf x2 haben, weil

520
00:21:58,980 --> 00:22:00,960
es einen Pfeil gibt, und von der

521
00:22:00,960 --> 00:22:02,940
nicht beobachtbaren Variablen, die auch

522
00:22:02,940 --> 00:22:05,840
Extreme beeinflusst,

523
00:22:06,179 --> 00:22:09,240
also ja, intuitiv kann man uns sehen, man

524
00:22:09,240 --> 00:22:11,940
kann denken  eines strukturellen Kausalmodells

525
00:22:11,940 --> 00:22:14,159
als Bayesianisches Netzwerk mit diesen

526
00:22:14,159 --> 00:22:16,679
nicht beobachtbaren Variablen an der Spitze und jede nicht

527
00:22:16,679 --> 00:22:19,500
beobachtbare Variable beeinflusst nur

528
00:22:19,500 --> 00:22:22,020
ihre

529
00:22:22,020 --> 00:22:24,600
eigene neueste Variable

530
00:22:24,600 --> 00:22:27,960


531
00:22:27,960 --> 00:22:30,360


532
00:22:30,360 --> 00:22:34,039
So weiter und so weiter. Durch die

533
00:22:35,039 --> 00:22:37,679
Durchführung einer kontrafaktischen Inferenz wird

534
00:22:37,679 --> 00:22:39,900
die folgende Frage beantwortet: Was

535
00:22:39,900 --> 00:22:42,960
wäre also X4 bei

536
00:22:42,960 --> 00:22:46,620


537
00:22:46,620 --> 00:22:49,340


538
00:22:49,340 --> 00:22:51,840


539
00:22:51,840 --> 00:22:53,039


540
00:22:53,039 --> 00:22:54,900


541
00:22:54,900 --> 00:22:57,179


542
00:22:57,179 --> 00:22:59,460
In diesem Schritt

543
00:22:59,460 --> 00:23:01,200
möchten wir in der Zeit zurückgehen und verstehen,

544
00:23:01,200 --> 00:23:03,419
wie die Umgebung, die nicht beobachtbare

545
00:23:03,419 --> 00:23:04,919
Umgebung,

546
00:23:04,919 --> 00:23:08,039
zu diesem bestimmten Zeitpunkt war.

547
00:23:08,039 --> 00:23:11,039
Dies erreichen wir, indem wir alle latenten

548
00:23:11,039 --> 00:23:14,280
Variablen X auf einige spezifische Daten fixieren, die

549
00:23:14,280 --> 00:23:16,140
wir bereits haben,

550
00:23:16,140 --> 00:23:18,960
und diese durchführen

551
00:23:18,960 --> 00:23:21,120
Rückschluss auf das Verwendete,

552
00:23:21,120 --> 00:23:24,240
dann verwenden wir das U,

553
00:23:24,240 --> 00:23:26,940
um das U beizubehalten, das wir gelernt haben, und

554
00:23:26,940 --> 00:23:28,500
führen einen Eingriff durch,

555
00:23:28,500 --> 00:23:29,880
sodass

556
00:23:29,880 --> 00:23:32,340
eine Kontrafaktur auch als

557
00:23:32,340 --> 00:23:34,980
Eingriff in die Vergangenheit angesehen werden kann, in dem wir

558
00:23:34,980 --> 00:23:36,960
die Umgebung und die

559
00:23:36,960 --> 00:23:40,620
Umgebungsvariablen U1 U2 kennen  und u4 in diesem bestimmten

560
00:23:40,620 --> 00:23:43,039
Moment

561
00:23:43,200 --> 00:23:44,340
und

562
00:23:44,340 --> 00:23:46,679
was ist der fehlende Schritt,

563
00:23:46,679 --> 00:23:49,440
also was wäre X4 bei

564
00:23:49,440 --> 00:23:50,780


565
00:23:50,780 --> 00:23:53,280


566
00:23:53,280 --> 00:23:55,980


567
00:23:55,980 --> 00:23:57,120


568
00:23:57,120 --> 00:23:59,520


569
00:23:59,520 --> 00:24:02,039
in dem wir

570
00:24:02,039 --> 00:24:04,440
bereits eine Intervention unter Verwendung

571
00:24:04,440 --> 00:24:06,659
der Umgebungsvariablen durchgeführt haben, die wir

572
00:24:06,659 --> 00:24:10,140
im Abduktionsschritt gelernt haben,

573
00:24:10,140 --> 00:24:14,419
und dies ist eine kontrafaktische Schlussfolgerung.

574
00:24:15,480 --> 00:24:18,000
Dies ist die letzte Folie der

575
00:24:18,000 --> 00:24:20,159
vorliegenden Einleitung zur kausalen Schlussfolgerung

576
00:24:20,159 --> 00:24:21,720
und handelt im Wesentlichen von strukturellem Lernen,

577
00:24:21,720 --> 00:24:23,880
im Grunde von allem, was ich gesagt habe

578
00:24:23,880 --> 00:24:27,360
Bisher beruht es auf der Tatsache, dass wir

579
00:24:27,360 --> 00:24:29,700
die kausalen Abhängigkeiten zwischen

580
00:24:29,700 --> 00:24:31,500
den Datenpunkten kennen, also die Struktur

581
00:24:31,500 --> 00:24:33,120
des Diagramms kennen. Wir wissen, welche Variable welche

582
00:24:33,120 --> 00:24:34,860
beeinflusst.

583
00:24:34,860 --> 00:24:37,260
Wir kennen die Pfeile im Allgemeinen,

584
00:24:37,260 --> 00:24:39,659
aber in der Praxis ist dies tatsächlich nicht

585
00:24:39,659 --> 00:24:42,900
immer möglich, also wissen wir  In den

586
00:24:42,900 --> 00:24:45,419


587
00:24:45,419 --> 00:24:47,400
meisten Fällen haben wir keinen Zugriff auf den Kausalgraphen und das Erlernen

588
00:24:47,400 --> 00:24:49,919
des besten Kausalgraphen aus Daten ist immer noch

589
00:24:49,919 --> 00:24:51,840
ein offenes Problem. Wir verbessern uns darin. Wir

590
00:24:51,840 --> 00:24:53,880
werden besser, aber

591
00:24:53,880 --> 00:24:57,299
wie diese Aufgabe genau ausgeführt werden soll,

592
00:24:57,299 --> 00:24:58,380


593
00:24:58,380 --> 00:25:01,140
ist immer noch ein offenes Problem

594
00:25:01,140 --> 00:25:03,179
Wie ich bereits sagte, besteht das Ziel im Wesentlichen darin,

595
00:25:03,179 --> 00:25:04,740
Ratsbeziehungen aus

596
00:25:04,740 --> 00:25:07,380
Beobachtungsdaten abzuleiten. Ausgehend von einem Datensatz

597
00:25:07,380 --> 00:25:09,780
möchten wir den gerichteten genauen

598
00:25:09,780 --> 00:25:12,179
Graphen ableiten, der die Konnektivität

599
00:25:12,179 --> 00:25:14,460
zwischen dem System und den Variablen

600
00:25:14,460 --> 00:25:15,960
des Datensatzes beschreibt.

601
00:25:15,960 --> 00:25:17,700
Hier haben wir beispielsweise ein Beispiel

602
00:25:17,700 --> 00:25:19,440
Ich schätze, wir

603
00:25:19,440 --> 00:25:22,860
kennen das alle, ähm, wegen

604
00:25:22,860 --> 00:25:25,080
der Pandemie, also haben wir diese vier

605
00:25:25,080 --> 00:25:28,799
Variablen, Alter, Impfung, Krankenhauseinweisung

606
00:25:28,799 --> 00:25:31,380
und CT,

607
00:25:31,380 --> 00:25:33,600
und wir wollen die kausalen

608
00:25:33,600 --> 00:25:36,059
Abhängigkeiten zwischen diesen Variablen ableiten, also

609
00:25:36,059 --> 00:25:37,980
wollen wir zum Beispiel direkt

610
00:25:37,980 --> 00:25:40,260
aus den Daten lernen, dass die Wahrscheinlichkeit  Ob eine

611
00:25:40,260 --> 00:25:43,080
Person ins Krankenhaus eingeliefert wird, hängt von

612
00:25:43,080 --> 00:25:45,419
ihrem Alter und der Tatsache ab, ob sie

613
00:25:45,419 --> 00:25:49,760
geimpft ist oder nicht und so weiter und so weiter.

614
00:25:51,299 --> 00:25:55,020
Dies ist also das Ende der langen

615
00:25:55,020 --> 00:25:58,080
Einführung, aber ich hoffe, es war klar

616
00:25:58,080 --> 00:26:00,179
genug und ich hoffe, dass ich es so verstanden habe

617
00:26:00,179 --> 00:26:02,039
Grundlagen, um die

618
00:26:02,039 --> 00:26:05,159
grundsätzlichen Ergebnisse der Arbeit zu verstehen, und

619
00:26:05,159 --> 00:26:07,740
jetzt können wir zu den Forschungsfragen übergehen,

620
00:26:07,740 --> 00:26:09,059
also sind die Forschungsfragen zunächst die

621
00:26:09,059 --> 00:26:10,440
folgenden.

622
00:26:10,440 --> 00:26:12,900
Ich möchte sehen,

623
00:26:12,900 --> 00:26:15,299
ob kreative Kodierung verwendet werden kann, um

624
00:26:15,299 --> 00:26:16,980
kausale Schlussfolgerungen durchzuführen,

625
00:26:16,980 --> 00:26:20,100
sodass bisher nur operative Kodierung

626
00:26:20,100 --> 00:26:22,380
verwendet wurde  Wir müssen Korrelationen

627
00:26:22,380 --> 00:26:25,020
in Bayes'schen Netzwerken berechnen

628
00:26:25,020 --> 00:26:27,419
und die große Frage ist, können wir über

629
00:26:27,419 --> 00:26:29,400
Korrelation und Modellintervention hinausgehen und

630
00:26:29,400 --> 00:26:31,679
auf biologisch plausible Weise kontrafaktisch arbeiten,

631
00:26:31,679 --> 00:26:32,760


632
00:26:32,760 --> 00:26:34,380
so

633
00:26:34,380 --> 00:26:36,120
dass es zum Beispiel einfach

634
00:26:36,120 --> 00:26:39,059
intuitiv ist und es uns ermöglicht, nur mit

635
00:26:39,059 --> 00:26:40,740
den Neuronen zu spielen  und berühren Sie beispielsweise nicht

636
00:26:40,740 --> 00:26:43,740
die riesige Struktur des Diagramms.

637
00:26:43,740 --> 00:26:46,380
In der Praxis

638
00:26:46,380 --> 00:26:48,299
stellt sich insbesondere die Frage: Können wir ein auf

639
00:26:48,299 --> 00:26:51,000
operativer Kodierung basierendes Struktur-Kausal-

640
00:26:51,000 --> 00:26:52,740
Modell definieren, um Interventionen und Kontrafaktuale durchzuführen?

641
00:26:52,740 --> 00:26:55,320


642
00:26:55,320 --> 00:26:58,380
Die zweite Frage ist,

643
00:26:58,380 --> 00:27:00,179
wie ich bereits sagte, dass ein benutzerdefiniertes

644
00:27:00,179 --> 00:27:02,159
Strukturmodell davon ausgeht, dass wir  Kennen Sie die Struktur

645
00:27:02,159 --> 00:27:04,260
des Umgehungsnetzwerks,

646
00:27:04,260 --> 00:27:07,919
also gehen wir davon aus, dass wir über die Pfeile verfügen.

647
00:27:07,919 --> 00:27:09,960
Können wir darüber hinausgehen und kreative

648
00:27:09,960 --> 00:27:11,520
Codierungsnetzwerke verwenden, um die kausale

649
00:27:11,520 --> 00:27:14,418
Struktur des Diagramms zu lernen? Wenn wir

650
00:27:16,140 --> 00:27:18,900
grundsätzlich positive Antworten auf

651
00:27:18,900 --> 00:27:21,120
beide Fragen geben, können wir

652
00:27:21,120 --> 00:27:23,120
prädiktive Codierung verwenden  Eine End-to-End-

653
00:27:23,120 --> 00:27:26,039
Kausalinferenzmethode, die im Wesentlichen

654
00:27:26,039 --> 00:27:28,740
einen Datensatz verwendet und es uns ermöglicht,

655
00:27:28,740 --> 00:27:30,419
Interventionen und kontrafaktische

656
00:27:30,419 --> 00:27:34,820
Vorhersagen direkt aus diesem Datensatz zu testen.

657
00:27:36,840 --> 00:27:39,299
Lassen Sie uns also das

658
00:27:39,299 --> 00:27:40,740
erste Problem angehen, also die

659
00:27:40,740 --> 00:27:42,419
Vibrationscodierung der Kausalinferenz, die auch der

660
00:27:42,419 --> 00:27:45,120
Abschnitt ist, der Folgendes liefert  Das ist im Grunde der Titel des

661
00:27:45,120 --> 00:27:46,740
Papiers

662
00:27:46,740 --> 00:27:48,539
und hier werde ich zeigen, wie man

663
00:27:48,539 --> 00:27:50,760
korrelative operative Codierung durchführt, die

664
00:27:50,760 --> 00:27:52,440
ähm bereits bekannt ist,

665
00:27:52,440 --> 00:27:54,419
und wie man interventionelle

666
00:27:54,419 --> 00:27:56,760
Abfragen durchführt, was meiner Meinung nach

667
00:27:56,760 --> 00:28:01,140
die eigentliche Frage des Papiers ist,

668
00:28:01,140 --> 00:28:03,900
also hier ist ein Kausalzusammenhang  Das Diagramm ist das

669
00:28:03,900 --> 00:28:05,700
übliche Diagramm, das

670
00:28:05,700 --> 00:28:07,260
wir hatten,

671
00:28:07,260 --> 00:28:09,240
und hier ist das entsprechende kreative

672
00:28:09,240 --> 00:28:11,760
Codierungsmodell. Die Achsen sind also die

673
00:28:11,760 --> 00:28:13,980
latenten Variablen und entsprechen den

674
00:28:13,980 --> 00:28:18,000
Neuronen in einem neuronalen Netzwerkmodell

675
00:28:18,000 --> 00:28:20,760
und dem schwarzen Pfeil, der

676
00:28:20,760 --> 00:28:22,740
Vorhersageinformationen von einem Neuron weitergibt

677
00:28:22,740 --> 00:28:25,559
zum Wertknoten weiter unten in der Hierarchie

678
00:28:25,559 --> 00:28:28,500
und jeder Scheitelpunkt verfügt auch über dieses

679
00:28:28,500 --> 00:28:31,140
Fehlerneuron, das Informationen in der

680
00:28:31,140 --> 00:28:32,820
Hierarchie nach oben weiterleitet, so dass die Informationen jedes

681
00:28:32,820 --> 00:28:36,480
Fehlers an den Wertknoten weiter

682
00:28:36,480 --> 00:28:39,120
oben in der Hierarchie weitergeleitet werden und ihn im Grunde anweisen,

683
00:28:39,120 --> 00:28:41,400
sich selbst zu korrigieren, um Änderungen vorzunehmen  Um

684
00:28:41,400 --> 00:28:43,760


685
00:28:44,700 --> 00:28:46,559
eine Korrelation mithilfe der

686
00:28:46,559 --> 00:28:48,840
prädiktiven Codierung durchzuführen, müssen

687
00:28:48,840 --> 00:28:50,400
Sie also eine Beobachtung machen und

688
00:28:50,400 --> 00:28:52,620
einfach den Wert eines bestimmten Neurons festlegen.

689
00:28:52,620 --> 00:28:53,820


690
00:28:53,820 --> 00:28:55,200
Wenn Sie also die

691
00:28:55,200 --> 00:28:58,740
Wahrscheinlichkeit von X4 berechnen möchten, wenn X3 gleich S3 ist, müssen

692
00:28:58,740 --> 00:29:02,340
wir dies einfach tun  Nehmen Sie X3 und fixieren Sie es

693
00:29:02,340 --> 00:29:04,380
so auf S3, dass es sich nicht

694
00:29:04,380 --> 00:29:08,159
mehr ändert, und führen Sie eine Energieminimierung durch.

695
00:29:08,159 --> 00:29:09,720
Dieses Modell

696
00:29:09,720 --> 00:29:12,659
und die Minimierung durch Aktualisierung der Achse

697
00:29:12,659 --> 00:29:16,380
uh über eine Minimierung der Variation

698
00:29:16,380 --> 00:29:18,419
der freien Energie ermöglichen es dem Modell,

699
00:29:18,419 --> 00:29:20,820
zu einer Lösung zu konvergieren  Auf diese Frage ist also

700
00:29:20,820 --> 00:29:22,919
die Wahrscheinlichkeit oder der erwartete Wert von

701
00:29:22,919 --> 00:29:27,179


702
00:29:27,179 --> 00:29:29,340


703
00:29:29,340 --> 00:29:31,679


704
00:29:31,679 --> 00:29:33,419


705
00:29:33,419 --> 00:29:35,640


706
00:29:35,640 --> 00:29:37,679


707
00:29:37,679 --> 00:29:39,960


708
00:29:39,960 --> 00:29:43,260
Der

709
00:29:43,260 --> 00:29:45,600
erste Schritt im Algorithmus

710
00:29:45,600 --> 00:29:47,220
besteht darin, eine

711
00:29:47,220 --> 00:29:50,539


712
00:29:51,240 --> 00:29:53,340
Korrelation durchzuführen und so S3 auf  von

713
00:29:53,340 --> 00:29:55,200


714
00:29:55,200 --> 00:29:56,220


715
00:29:56,220 --> 00:29:57,659


716
00:29:57,659 --> 00:29:59,279


717
00:29:59,279 --> 00:30:02,399


718
00:30:02,399 --> 00:30:04,860


719
00:30:04,860 --> 00:30:07,080


720
00:30:07,080 --> 00:30:09,840


721
00:30:09,840 --> 00:30:13,140


722
00:30:13,140 --> 00:30:17,039


723
00:30:17,039 --> 00:30:18,720


724
00:30:18,720 --> 00:30:21,299
Dann ist dies der wichtige Schritt, den

725
00:30:21,299 --> 00:30:24,059
Sie nicht mehr in das Diagramm,

726
00:30:24,059 --> 00:30:26,700
sondern in den Vorhersagefehler eingreifen und auf

727
00:30:26,700 --> 00:30:28,980
Null setzen müssen.

728
00:30:28,980 --> 00:30:31,020
Ein Vorhersagefehler von Null

729
00:30:31,020 --> 00:30:32,480


730
00:30:32,480 --> 00:30:36,179
führt im Grunde dazu, dass Sie bedeutungslose

731
00:30:36,179 --> 00:30:38,460
Informationen in der Hierarchie nach oben senden oder tatsächlich

732
00:30:38,460 --> 00:30:40,200
keine Informationen darüber senden  Hierarchie,

733
00:30:40,200 --> 00:30:41,880
weil sie Ihnen im Grunde sagt, dass die

734
00:30:41,880 --> 00:30:44,659
Vorhersage immer korrekt ist

735
00:30:44,659 --> 00:30:48,120
und der dritte Schritt darin besteht, wie wir es

736
00:30:48,120 --> 00:30:50,220
zuvor getan haben, die Achse auf die

737
00:30:50,220 --> 00:30:52,919
unbeschränkte Achse oder X1

738
00:30:52,919 --> 00:30:55,679


739
00:30:55,679 --> 00:30:59,039


740
00:30:59,039 --> 00:31:00,840
Dieser kleine Trick,

741
00:31:00,840 --> 00:31:02,399
einen Vorhersagefehler

742
00:31:02,399 --> 00:31:05,120
auf Null zu setzen,

743
00:31:05,640 --> 00:31:08,220
verhindert, dass wir tatsächlich auf die

744
00:31:08,220 --> 00:31:10,320
Struktur des Diagramms einwirken können,

745
00:31:10,320 --> 00:31:13,620
wie es die Theorie der Infinitesimalrechnung tut, und

746
00:31:13,620 --> 00:31:16,919


747
00:31:16,919 --> 00:31:19,140
durch einfaches Ausführen auf die fehlenden Variablen nach einem Eingriff schließen können

748
00:31:19,140 --> 00:31:22,640
Aberration der Minimierung der freien Energie

749
00:31:24,659 --> 00:31:26,580
Was ist mit der kontrafaktischen Schlussfolgerung? Die

750
00:31:26,580 --> 00:31:28,080
kontrafaktische Schlussfolgerung ist eigentlich

751
00:31:28,080 --> 00:31:30,539
einfach, wenn wir erst einmal

752
00:31:30,539 --> 00:31:34,740
definiert haben, wie ein Eingriff durchgeführt werden soll,

753
00:31:34,740 --> 00:31:36,539
und das liegt daran, wie wir zuvor gesehen haben, dass die

754
00:31:36,539 --> 00:31:38,640
Durchführung einer kontrafaktischen Schlussfolgerung der

755
00:31:38,640 --> 00:31:40,380
Durchführung einer Intervention in einer früheren

756
00:31:40,380 --> 00:31:44,360
Situation ähnelt, nachdem Sie das abgeleitet haben

757
00:31:44,360 --> 00:31:48,120
unbeobachtbar die nicht beobachtbaren Variablen,

758
00:31:48,120 --> 00:31:49,620
so

759
00:31:49,620 --> 00:31:51,480
wie Sie in der Darstellung sehen können, die ich

760
00:31:51,480 --> 00:31:53,520
zuvor über die Entführungsaktion und die

761
00:31:53,520 --> 00:31:56,039
Vorhersageschritte gezeigt habe. Die Aktions- und

762
00:31:56,039 --> 00:31:58,320
Vorhersageschritte hatten diese

763
00:31:58,320 --> 00:31:59,640
beiden Pfeile nicht,

764
00:31:59,640 --> 00:32:02,580
sie wurden entfernt. Eine hübsche Codierung

765
00:32:02,580 --> 00:32:06,299
ermöglicht es uns, die Pfeile dieser äh zu belassen

766
00:32:06,299 --> 00:32:08,279
Das Diagramm

767
00:32:08,279 --> 00:32:11,340
und und führen Kontrafaktuale durch, indem wir

768
00:32:11,340 --> 00:32:13,380
einfach einen Abduktionsschritt ausführen, wie es

769
00:32:13,380 --> 00:32:14,640
zuvor getan wurde,

770
00:32:14,640 --> 00:32:16,679
einem Aktionsschritt, bei dem wir einfach

771
00:32:16,679 --> 00:32:18,600
einen Eingriff auf den einzelnen

772
00:32:18,600 --> 00:32:21,240
Knoten durchführen, also den Wertknoten fixieren,

773
00:32:21,240 --> 00:32:24,240
den Fehler auf Null setzen

774
00:32:24,240 --> 00:32:26,399
und die Energieminimierung ausführen

775
00:32:26,399 --> 00:32:27,960
Minimieren Sie die Dauer der freien Energie, um

776
00:32:27,960 --> 00:32:30,679
die Vorhersage zu berechnen,

777
00:32:32,399 --> 00:32:36,299
daher denke ich, dass dies eine einfache und

778
00:32:36,299 --> 00:32:39,840
elegante Methode ist, Interventionen

779
00:32:39,840 --> 00:32:42,899
und kontrafaktische Berechnungen durchzuführen, und äh ja,

780
00:32:42,899 --> 00:32:44,880
ich denke, das, was wir

781
00:32:44,880 --> 00:32:46,500
jetzt zeigen müssen, ist, ob es in der Praxis funktioniert

782
00:32:46,500 --> 00:32:48,720
oder nicht, und wir  Ich habe ein paar

783
00:32:48,720 --> 00:32:49,919
Experimente durchgeführt

784
00:32:49,919 --> 00:32:52,440
und ich zeige Ihnen jetzt zwei

785
00:32:52,440 --> 00:32:54,240
verschiedene Experimente. Das erste ist

786
00:32:54,240 --> 00:32:57,179
lediglich ein Proof-of-Concept-Experiment,

787
00:32:57,179 --> 00:33:01,020
das zeigt, dass in der operativen Codierung

788
00:33:01,020 --> 00:33:02,480


789
00:33:02,480 --> 00:33:06,120
Interventionen und Kontrafaktuale durchgeführt werden können,

790
00:33:06,120 --> 00:33:08,700
und das zweite zeigt tatsächlich eine

791
00:33:08,700 --> 00:33:11,220
einfache Anwendung  wie interventionelle

792
00:33:11,220 --> 00:33:13,440
Abfragen verwendet werden können, um die

793
00:33:13,440 --> 00:33:16,260
Leistung von Klassifizierungsaufgaben in einer

794
00:33:16,260 --> 00:33:18,360
bestimmten Art von operativen

795
00:33:18,360 --> 00:33:20,940
Codierungsnetzwerken zu verbessern, bei denen es sich um die eines vollständig

796
00:33:20,940 --> 00:33:22,080
verbundenen Modells handelt.

797
00:33:22,080 --> 00:33:24,659
Beginnen wir mit dem ersten.

798
00:33:24,659 --> 00:33:27,679
Wie erledigen wir diese Aufgabe also anhand eines

799
00:33:27,679 --> 00:33:30,360
strukturellen Ratsmodells?

800
00:33:30,360 --> 00:33:33,360
Wir generieren Trainingsdaten und verwenden sie,

801
00:33:33,360 --> 00:33:35,760
um die Gewichte zu lernen, um die

802
00:33:35,760 --> 00:33:39,480
Funktionen der strukturellen Kaza-Modelle zu lernen.

803
00:33:39,480 --> 00:33:42,779
Anschließend generieren wir Testtestdaten

804
00:33:42,779 --> 00:33:44,399
sowohl für interventionelle als auch für

805
00:33:44,399 --> 00:33:46,080
Gegenfraktionsabfragen

806
00:33:46,080 --> 00:33:48,000
und zeigen, ob wir in der Lage sind,

807
00:33:48,000 --> 00:33:51,360
zu den richtigen Testdaten zu konvergieren  unter Verwendung

808
00:33:51,360 --> 00:33:53,340
kreativer Codierung

809
00:33:53,340 --> 00:33:54,779
und

810
00:33:54,779 --> 00:33:57,240
und zum Beispiel hier äh in diesen beiden

811
00:33:57,240 --> 00:33:58,860
Diagrammen stellen interventionelle

812
00:33:58,860 --> 00:34:00,600
Interventionen und kontrafaktische Abfragen

813
00:34:00,600 --> 00:34:03,539
dieses spezifischen Diagramms dar, das das

814
00:34:03,539 --> 00:34:05,880
Butterfly-Bias-Diagramm ist, ein Diagramm,

815
00:34:05,880 --> 00:34:08,280
das häufig zum Testen verwendet wird,

816
00:34:08,280 --> 00:34:10,859
ob eine kausale Schlussfolgerung vorliegt, ob

817
00:34:10,859 --> 00:34:12,179
Intervention und kontrafaktische

818
00:34:12,179 --> 00:34:15,540
Techniken  Die Arbeit ist so einfach, aber

819
00:34:15,540 --> 00:34:18,000
in der Arbeit können Sie viele

820
00:34:18,000 --> 00:34:20,760
verschiedene Diagramme finden, aber im Allgemeinen zeigen diese

821
00:34:20,760 --> 00:34:22,800
beiden Diagramme, dass die

822
00:34:22,800 --> 00:34:26,940
Methode funktioniert, und zeigen, dass der

823
00:34:26,940 --> 00:34:27,918


824
00:34:27,918 --> 00:34:32,219
mittlere absolute Fehler zwischen den

825
00:34:32,219 --> 00:34:33,960
interventionellen kontrafaktischen Größen, die

826
00:34:33,960 --> 00:34:37,399
wir haben, ist  Berechnen und die interventionellen und

827
00:34:37,399 --> 00:34:39,780
kontrafaktischen Größen aus dem

828
00:34:39,780 --> 00:34:41,460
ursprünglichen Diagramm

829
00:34:41,460 --> 00:34:43,800
liegen nahe beieinander, sodass der Fehler

830
00:34:43,800 --> 00:34:45,800
recht gering ist.

831
00:34:45,800 --> 00:34:49,139
Das zweite Experiment ist im Grunde

832
00:34:49,139 --> 00:34:51,239
eine Erweiterung eines Experiments, das ich

833
00:34:51,239 --> 00:34:54,540
in einem früheren Artikel vorgeschlagen habe, nämlich das

834
00:34:54,540 --> 00:34:56,460
Lernen über beliebige Diagrammtopologien

835
00:34:56,460 --> 00:34:59,040
Das, was ich letztes Jahr

836
00:34:59,040 --> 00:35:01,080
in diesem Aufsatz geschrieben habe,

837
00:35:01,080 --> 00:35:04,200
schlage ich grundsätzlich diese Art von

838
00:35:04,200 --> 00:35:06,060
Netzwerk als Proof of Concept vor, bei dem es sich um ein

839
00:35:06,060 --> 00:35:08,160
vollständig verbundenes Netzwerk handelt, das im

840
00:35:08,160 --> 00:35:11,579
Allgemeinen das schlechteste neuronale Netzwerk ist, das man haben kann, um

841
00:35:11,579 --> 00:35:13,500


842
00:35:13,500 --> 00:35:15,960
Experimente zum maschinellen Lernen durchzuführen, weil es

843
00:35:15,960 --> 00:35:20,520
uns eine feste Größe gegeben hat

844
00:35:20,520 --> 00:35:23,660
Im Grunde genommen

845
00:35:23,760 --> 00:35:26,400
ist jedes Neuronenpaar

846
00:35:26,400 --> 00:35:28,680
durch zwei unterschiedliche Synapsen verbunden. Daher ist es

847
00:35:28,680 --> 00:35:31,200
das Modell mit

848
00:35:31,200 --> 00:35:33,359
der größtmöglichen Komplexität. Im

849
00:35:33,359 --> 00:35:34,619
Allgemeinen

850
00:35:34,619 --> 00:35:36,300
ist das Gute daran, dass

851
00:35:36,300 --> 00:35:37,859
das Modell äußerst flexibel ist, da es viele Zyklen gibt

852
00:35:37,859 --> 00:35:39,599
in dem Sinne, dass Sie

853
00:35:39,599 --> 00:35:42,480
es beispielsweise auf einem zerkleinerten Bild und auf

854
00:35:42,480 --> 00:35:45,359
einem Datenpunkt und auf seiner Beschriftung trainieren können, aber

855
00:35:45,359 --> 00:35:47,400
dann können Sie es dank

856
00:35:47,400 --> 00:35:50,640
der zurückgehenden Informationen

857
00:35:50,640 --> 00:35:52,140
auf viele verschiedene Arten abfragen  Sie

858
00:35:52,140 --> 00:35:54,060
können Klassifizierungsaufgaben erstellen, bei denen

859
00:35:54,060 --> 00:35:55,980
Sie ein Bild bereitstellen, die

860
00:35:55,980 --> 00:35:57,480
Energieminimierung ausführen und das Etikett erhalten,

861
00:35:57,480 --> 00:35:59,400
aber Sie können beispielsweise auch

862
00:35:59,400 --> 00:36:01,320
Generierungsaufgaben ausführen, bei denen Sie dem

863
00:36:01,320 --> 00:36:03,060
Etikett die Energieminimierung geben und

864
00:36:03,060 --> 00:36:05,220
das Bild erhalten, das Sie ausführen können, z.

865
00:36:05,220 --> 00:36:06,960
B. Bild  Vervollständigung, bei der Sie die

866
00:36:06,960 --> 00:36:10,260
Hälfte des Bildes angeben und konvergieren und

867
00:36:10,260 --> 00:36:12,119
konvergieren, lassen Sie das Modell in die

868
00:36:12,119 --> 00:36:14,400
zweite Hälfte konvertieren und so weiter und so weiter.

869
00:36:14,400 --> 00:36:16,440
Es handelt sich also im Grunde um ein Modell, das die

870
00:36:16,440 --> 00:36:19,619
Statistiken des Datensatzes in seiner

871
00:36:19,619 --> 00:36:21,900
Gesamtheit lernt, ohne sich auf die

872
00:36:21,900 --> 00:36:25,079
Klassifizierung usw. zu konzentrieren  Generation im Allgemeinen,

873
00:36:25,079 --> 00:36:27,900
daher ist diese Flexibilität großartig.

874
00:36:27,900 --> 00:36:31,260
Das Problem besteht jedoch darin, dass aus diesem Grund

875
00:36:31,260 --> 00:36:34,140
nicht jede einzelne Aufgabe gut funktioniert,

876
00:36:34,140 --> 00:36:35,820
sodass Sie viele verschiedene Dinge tun können,

877
00:36:35,820 --> 00:36:38,579
aber keines davon wird gut erledigt,

878
00:36:38,579 --> 00:36:39,960
und

879
00:36:39,960 --> 00:36:42,480
hier möchte ich zeigen, wie man es verwendet

880
00:36:42,480 --> 00:36:44,099
Interventionelle Abfragen anstelle von

881
00:36:44,099 --> 00:36:46,740
standardmäßigen korrelativen Abfragen oder

882
00:36:46,740 --> 00:36:48,119
bedingten Abfragen

883
00:36:48,119 --> 00:36:49,980
verbessern die Ergebnisse dieser Klassifizierungsaufgaben geringfügig.

884
00:36:49,980 --> 00:36:51,960


885
00:36:51,960 --> 00:36:54,000
Was sind also die konjektiven Gründe dafür?

886
00:36:54,000 --> 00:36:57,599
Die Testgenauigkeit

887
00:36:57,599 --> 00:37:01,079
bei diesen Aufgaben ist nicht so hoch wie beim

888
00:37:01,079 --> 00:37:03,180
ersten. Die beiden Gründe sind, dass das Modell

889
00:37:03,180 --> 00:37:05,640
abgelenkt ist  bei der Korrektur jedes einzelnen, äh,

890
00:37:05,640 --> 00:37:07,920
jedes einzelnen Fehlers. Sie präsentieren also im Grunde

891
00:37:07,920 --> 00:37:09,420
ein Bild und möchten

892
00:37:09,420 --> 00:37:11,579
eine Beschriftung erhalten, aber das Modell aktualisiert sich tatsächlich

893
00:37:11,579 --> 00:37:13,859
selbst, um auch den

894
00:37:13,859 --> 00:37:16,320
Fehler in den Bildern vorherzusagen,

895
00:37:16,320 --> 00:37:18,480
und der zweite Grund, den ich

896
00:37:18,480 --> 00:37:21,119
genannt habe, ist, dass der  Die Struktur ist viel zu

897
00:37:21,119 --> 00:37:24,540
komplex. Ausgehend von der

898
00:37:24,540 --> 00:37:27,079
messerscharfen Argumentation von Occam

899
00:37:27,079 --> 00:37:28,800


900
00:37:28,800 --> 00:37:30,720
ist dies das schlechteste Modell, das Sie haben können.

901
00:37:30,720 --> 00:37:32,160
Jedes Mal, wenn Sie ein Modell haben, das zu einem

902
00:37:32,160 --> 00:37:33,960
Datensatz passt, wird dieses Modell weniger

903
00:37:33,960 --> 00:37:35,579
komplex sein als das aktuelle  zu

904
00:37:35,579 --> 00:37:37,560
bevorzugen,

905
00:37:37,560 --> 00:37:40,560
aber im Allgemeinen, nur um es zu studieren, besteht

906
00:37:40,560 --> 00:37:41,400


907
00:37:41,400 --> 00:37:43,380
die Idee darin, in diesem Modell die

908
00:37:43,380 --> 00:37:44,820
Interventionen abzufragen, die verwendet werden können, um die

909
00:37:44,820 --> 00:37:46,859
Leistung dieser vollständig

910
00:37:46,859 --> 00:37:48,599
verbundenen Modelle zu verbessern. Nun,

911
00:37:48,599 --> 00:37:51,060
die Antwort ist ja,

912
00:37:51,060 --> 00:37:53,160
also hier ist, wie ich interventionelle

913
00:37:53,160 --> 00:37:55,619
Abfragen durchführe  Ich präsentiere dem Netzwerk ein Bild.

914
00:37:55,619 --> 00:37:56,640


915
00:37:56,640 --> 00:37:59,460
Ich behebe den Fehler der Pixel

916
00:37:59,460 --> 00:38:01,560
auf Null, damit sich dieser Fehler nicht

917
00:38:01,560 --> 00:38:03,180
im Netzwerk ausbreitet,

918
00:38:03,180 --> 00:38:05,700
und berechne dann die Bezeichnung.

919
00:38:05,700 --> 00:38:08,400
Wie Sie sehen können, verbessert sich die Genauigkeit

920
00:38:08,400 --> 00:38:11,339
beispielsweise von 89 mithilfe von  Standard-

921
00:38:11,339 --> 00:38:13,380
Abfragemethode von Creative-Coding-Netzwerken

922
00:38:13,380 --> 00:38:16,800
auf 92, was die Genauigkeit nach dem

923
00:38:16,800 --> 00:38:19,020
Eingriff angibt, und das Gleiche gilt

924
00:38:19,020 --> 00:38:21,540
für Mode-Mittel,

925
00:38:21,540 --> 00:38:24,420
und ich denke, dass ein sehr berechtigter Kritiker,

926
00:38:24,420 --> 00:38:26,940
den wahrscheinlich jeder denken würde,

927
00:38:26,940 --> 00:38:28,920
wenn er diese Diagramme sieht, ist, dass es in Ordnung ist, die

928
00:38:28,920 --> 00:38:32,099
Mittel von 89 zu verbessern  bis 92 ist im

929
00:38:32,099 --> 00:38:36,180
Grunde immer noch scheiße und ja, es ist wahr

930
00:38:36,180 --> 00:38:38,400
und ich werde in den späteren Folien

931
00:38:38,400 --> 00:38:40,619
zeigen, wie man auf die

932
00:38:40,619 --> 00:38:42,660
Struktur dieses ähm einwirkt. Dieses vollständig

933
00:38:42,660 --> 00:38:43,859
verbundene Modell

934
00:38:43,859 --> 00:38:46,500
wird die Ergebnisse noch weiter verbessern, bis

935
00:38:46,500 --> 00:38:48,480
sie den Punkt erreicht haben  Reichhaltige

936
00:38:48,480 --> 00:38:50,820
Leistung, die natürlich nicht einmal annähernd an die

937
00:38:50,820 --> 00:38:52,560
Leistung auf dem neuesten Stand herankommt,

938
00:38:52,560 --> 00:38:55,320
aber sie ist immer noch auf dem neuesten Stand, aber nicht auf einem

939
00:38:55,320 --> 00:38:57,380
Niveau, das grundsätzlich akzeptabel wird.

940
00:38:57,380 --> 00:39:01,760
Kenworth-Untersuchung untersucht, also ja,

941
00:39:02,040 --> 00:39:04,980
das war der Teil über

942
00:39:04,980 --> 00:39:08,400
kausale Schlussfolgerungen mithilfe kreativer Codierung

943
00:39:08,400 --> 00:39:10,920
Zusammenfassend kann ich sagen, dass

944
00:39:10,920 --> 00:39:15,060
das Interessante an

945
00:39:15,060 --> 00:39:17,640
den Ergebnissen, die ich gerade gezeigt habe, darin besteht,

946
00:39:17,640 --> 00:39:19,859
dass ich gezeigt habe, dass die operative Kodierung in der Lage ist,

947
00:39:19,859 --> 00:39:22,560
Interventionen auf sehr einfache

948
00:39:22,560 --> 00:39:24,780
und intuitive Weise durchzuführen, weil man nicht

949
00:39:24,780 --> 00:39:26,280
darauf reagieren muss  Die Struktur des alten Diagramms

950
00:39:26,280 --> 00:39:28,740


951
00:39:28,740 --> 00:39:31,079
ist manchmal nicht mehr verfügbar, und so weiter und

952
00:39:31,079 --> 00:39:34,020
so weiter, aber Sie müssen einfach in

953
00:39:34,020 --> 00:39:36,140


954
00:39:36,140 --> 00:39:39,780
ein einzelnes Neuron eingreifen, um den

955
00:39:39,780 --> 00:39:41,640
Vorhersagefehler auf Null zu setzen

956
00:39:41,640 --> 00:39:44,220
und einen Energieminimierungsprozess durchzuführen,

957
00:39:44,220 --> 00:39:46,619


958
00:39:46,619 --> 00:39:49,200
und diese sind erweitert  ermöglichte es uns, auf

959
00:39:49,200 --> 00:39:51,240
kreativer Kodierung basierende strukturelle Kausalmodelle zu definieren.

960
00:39:51,240 --> 00:39:52,920


961
00:39:52,920 --> 00:39:54,920
Nun gehen wir zum zweiten

962
00:39:54,920 --> 00:39:57,900
Teil der Arbeit über, in dem es um das

963
00:39:57,900 --> 00:40:01,700
Lernen struktureller Strukturen geht,

964
00:40:02,000 --> 00:40:05,099
also um das

965
00:40:05,099 --> 00:40:07,260
Lernen von Anweisungen, wie ich bereits sagte. Es geht um das Problem des Lernens der

966
00:40:07,260 --> 00:40:09,720
kausalen Struktur des Modells

967
00:40:09,720 --> 00:40:11,880
aus Beobachtungsdaten

968
00:40:11,880 --> 00:40:13,800
eigentlich kein Problem, das schon seit

969
00:40:13,800 --> 00:40:17,760
Jahrzehnten existiert

970
00:40:17,760 --> 00:40:21,359
und bis vor ein paar

971
00:40:21,359 --> 00:40:24,000
Jahren immer mit kombinatorischen Suchmethoden angegangen wurde.

972
00:40:24,000 --> 00:40:25,560


973
00:40:25,560 --> 00:40:26,640
Das Problem bei diesen Community-

974
00:40:26,640 --> 00:40:29,280
Research-Methoden besteht darin, dass ihre

975
00:40:29,280 --> 00:40:32,880
Komplexität exponentiell verdoppelt wird,

976
00:40:32,880 --> 00:40:34,740
sodass sobald die Daten multi- werden.

977
00:40:34,740 --> 00:40:36,780


978
00:40:36,780 --> 00:40:39,920


979
00:40:39,920 --> 00:40:42,300


980
00:40:42,300 --> 00:40:46,680


981
00:40:46,680 --> 00:40:48,780
Die neue Lösung, die tatsächlich

982
00:40:48,780 --> 00:40:51,000
vor ein paar Jahren in einer neuen Zeitung

983
00:40:51,000 --> 00:40:53,540
aus dem Jahr 2018 veröffentlicht wurde,

984
00:40:53,839 --> 00:40:55,920
zeigt, dass es möglich ist,

985
00:40:55,920 --> 00:40:57,900
diese Struktur tatsächlich zu lernen, ohne a zu verwenden

986
00:40:57,900 --> 00:40:59,940
Kombinator-Forschungsmethode, aber durch die Verwendung

987
00:40:59,940 --> 00:41:01,619
einer Gradienten-basierten Methode,

988
00:41:01,619 --> 00:41:05,280
und das war im Grunde genommen dieses

989
00:41:05,280 --> 00:41:07,320
Expertenproblem im Allgemeinen, denn jetzt

990
00:41:07,320 --> 00:41:08,820
können Sie einfach

991
00:41:08,820 --> 00:41:10,980
einen Prior für die Parameter haben, was

992
00:41:10,980 --> 00:41:12,420
der vorrangige Zweck ist, den ich

993
00:41:12,420 --> 00:41:14,700
etwas besser definieren werde  In dieser

994
00:41:14,700 --> 00:41:15,599
Folie

995
00:41:15,599 --> 00:41:18,180
läuft der Gradientenabstieg ab, und selbst wenn Sie

996
00:41:18,180 --> 00:41:19,740
ein Modell haben, das doppelt dreifach ist, ist die

997
00:41:19,740 --> 00:41:20,820
Größe

998
00:41:20,820 --> 00:41:23,640
äh, der Algorithmus ist immer noch unglaublich

999
00:41:23,640 --> 00:41:25,440
schnell

1000
00:41:25,440 --> 00:41:28,260
und aus diesem Grund ist dieser Artikel ein

1001
00:41:28,260 --> 00:41:31,200
ziemlich neues Papier,

1002
00:41:31,200 --> 00:41:33,180
und ich denke, es gibt es schon  600

1003
00:41:33,180 --> 00:41:35,099
Zitate oder ähnliches

1004
00:41:35,099 --> 00:41:37,140
und jeder Artikel, den ich jetzt

1005
00:41:37,140 --> 00:41:38,720
über die Beratung von Freunden und das Erlernen der

1006
00:41:38,720 --> 00:41:42,000
Struktur des Diagramms sehe, verwendet ihre Methode.

1007
00:41:42,000 --> 00:41:44,820
Es ändert sich nur ein wenig, wenn

1008
00:41:44,820 --> 00:41:46,980
sie schnellere oder etwas bessere

1009
00:41:46,980 --> 00:41:49,440
Inferenzmethoden finden, aber sie verwenden immer noch alle

1010
00:41:49,440 --> 00:41:53,760
Vor diesem Artikel wurde definiert, und ich

1011
00:41:53,760 --> 00:41:56,460
tue es auch, und wir tun es auch, also

1012
00:41:56,460 --> 00:41:58,859
werden wir hier eine neue Größe finden,

1013
00:41:58,859 --> 00:42:01,500
die die Agenturmatrix ist. Die

1014
00:42:01,500 --> 00:42:03,480
Agenturmatrix ist einfach eine Matrix, die

1015
00:42:03,480 --> 00:42:06,359
die Verbindungen des Modells kodiert, also ist es eine

1016
00:42:06,359 --> 00:42:08,520
binäre Matrix und

1017
00:42:08,520 --> 00:42:10,920
im Allgemeinen  Wenn es sich um eine binäre Matrix handelt,

1018
00:42:10,920 --> 00:42:12,180


1019
00:42:12,180 --> 00:42:14,880
machen Sie sie bei einer verlaufsbasierten Optimierung natürlich kontinuierlich

1020
00:42:14,880 --> 00:42:16,800
und haben dann an einem bestimmten Punkt einen Schwellenwert,

1021
00:42:16,800 --> 00:42:19,800
der im Grunde genommen eine Kante tötet oder

1022
00:42:19,800 --> 00:42:21,480
sie auf eins setzt,

1023
00:42:21,480 --> 00:42:27,780
und der M3-IJ ist gleich eins, wenn wir das

1024
00:42:27,780 --> 00:42:30,540
haben  Wenn der Bayes'sche Graph eine Kante

1025
00:42:30,540 --> 00:42:35,040
vom Scheitelpunkt I zum Scheitelpunkt J oder Null ist,

1026
00:42:35,040 --> 00:42:37,380
andernfalls

1027
00:42:37,380 --> 00:42:39,540
stellt diese Agenturmatrix hier beispielsweise die

1028
00:42:39,540 --> 00:42:42,780
Konnektivitätsstruktur dieses visuellen Netzwerks dar,

1029
00:42:42,780 --> 00:42:44,040
und

1030
00:42:44,040 --> 00:42:46,079
diese Methode

1031
00:42:46,079 --> 00:42:48,780
befasst sich im Wesentlichen mit zwei Problemen, die wir in Bezug

1032
00:42:48,780 --> 00:42:51,000
auf das Erlernen der

1033
00:42:51,000 --> 00:42:53,460
Struktur haben möchten  Die Idee hinter der Netzwerkgleichung besteht darin,

1034
00:42:53,460 --> 00:42:54,780
dass wir von einem vollständig

1035
00:42:54,780 --> 00:42:57,200
verbundenen Modell ausgehen, das

1036
00:42:57,200 --> 00:43:00,240
konzeptionell ähnlich ist und tatsächlich

1037
00:43:00,240 --> 00:43:02,220
dem operativen Codierungsnetzwerk entspricht, das

1038
00:43:02,220 --> 00:43:04,020
ich zuvor definiert habe, das vollständig

1039
00:43:04,020 --> 00:43:06,480
verbunden ist, sodass Sie viele

1040
00:43:06,480 --> 00:43:08,640
Scheitelpunkte und jedes Scheitelpunktpaar haben  ist

1041
00:43:08,640 --> 00:43:10,920
durch zwei verschiedene Kanten verbunden

1042
00:43:10,920 --> 00:43:13,319
und Sie möchten einfach diejenigen beschneiden,

1043
00:43:13,319 --> 00:43:15,780
die nicht benötigt werden,

1044
00:43:15,780 --> 00:43:18,540
damit es als eine Methode angesehen werden kann, die eine

1045
00:43:18,540 --> 00:43:20,819
Modellreduktion durchführt. Sie beginnen mit

1046
00:43:20,819 --> 00:43:22,020
einem großen Modell und möchten es klein machen,

1047
00:43:22,020 --> 00:43:22,800


1048
00:43:22,800 --> 00:43:25,800
also was ist  Die erste Zutat, um

1049
00:43:25,800 --> 00:43:28,260
Modelle gut zu reduzieren, ist natürlich die Sparse-

1050
00:43:28,260 --> 00:43:29,220
Stadt,

1051
00:43:29,220 --> 00:43:31,619
und die Priorisierung, die jeder verwendet,

1052
00:43:31,619 --> 00:43:33,839
um ein Modell spärlicher zu machen, ist die

1053
00:43:33,839 --> 00:43:36,480
LaPlace-Priorität, die im maschinellen Lernen

1054
00:43:36,480 --> 00:43:38,880
einfach als L1-Norm bekannt ist

1055
00:43:38,880 --> 00:43:40,920
und hier

1056
00:43:40,920 --> 00:43:43,980
die Lösung definiert, die die  In diesem Papier, das ich

1057
00:43:43,980 --> 00:43:46,740
zuvor erwähnt habe, wurde vorgeschlagen, den

1058
00:43:46,740 --> 00:43:49,319
zweiten Prior oben hinzuzufügen, was das

1059
00:43:49,319 --> 00:43:53,359
wahrscheinlich größte

1060
00:43:53,359 --> 00:43:55,980
Merkmal von Bayes'schen Netzwerken erzwingt,

1061
00:43:55,980 --> 00:43:57,780
auf dem Sie kausale Schlussfolgerungen durchführen möchten:

1062
00:43:57,780 --> 00:43:59,819
Sie möchten, dass sie zyklisch sind,

1063
00:43:59,819 --> 00:44:01,020


1064
00:44:01,020 --> 00:44:03,000
und im Grunde haben sie gezeigt  Diese

1065
00:44:03,000 --> 00:44:06,359
Azyklizität kann einer

1066
00:44:06,359 --> 00:44:08,160
Agenturmatrix als Prior auferlegt werden

1067
00:44:08,160 --> 00:44:10,859
und sie hat hier diese Form, also ist es

1068
00:44:10,859 --> 00:44:14,640
die Spur der Matrix, die

1069
00:44:14,640 --> 00:44:18,420
die Exponentialfunktion von a mal a ist,

1070
00:44:18,420 --> 00:44:21,859
wobei a wieder die Agenturmatrix ist und im

1071
00:44:21,859 --> 00:44:24,300
Grunde diese Größe hier

1072
00:44:24,300 --> 00:44:27,900
ist  ist genau dann gleich Null, wenn das

1073
00:44:27,900 --> 00:44:30,480
Bayes'sche Netzwerk oder der

1074
00:44:30,480 --> 00:44:32,819
oder der von Ihnen in Betracht gezogene Graph

1075
00:44:32,819 --> 00:44:35,720
ein C-Klick ist,

1076
00:44:37,619 --> 00:44:40,260
also werde ich diese in einigen

1077
00:44:40,260 --> 00:44:42,960
Experimenten verwenden, damit diese beiden

1078
00:44:42,960 --> 00:44:45,660
diese beiden Prioritäten

1079
00:44:45,660 --> 00:44:47,520
auf unterschiedliche Arten erzwingen  von Patientennetzwerken

1080
00:44:47,520 --> 00:44:49,200
und ich versuche, sie mit den

1081
00:44:49,200 --> 00:44:51,540
Techniken zu verschmelzen, die wir zuvor zur

1082
00:44:51,540 --> 00:44:52,740
Durchführung kausaler Schlussfolgerungen und der

1083
00:44:52,740 --> 00:44:55,020
operativen Kodierung vorgeschlagen haben.

1084
00:44:55,020 --> 00:44:56,520
Deshalb werde ich zwei verschiedene

1085
00:44:56,520 --> 00:44:59,640
Experimente vorstellen, sodass es sich bei einem um einen Proof of

1086
00:44:59,640 --> 00:45:00,960
Concept handelt, bei dem es sich um die

1087
00:45:00,960 --> 00:45:03,660
in gezeigten Standardexperimente handelt  alle strukturellen

1088
00:45:03,660 --> 00:45:06,599
Lernaufgaben, bei denen es sich um die Schlussfolgerung

1089
00:45:06,599 --> 00:45:08,880
des korrekten Bayes'schen Netzwerks aus Daten handelt,

1090
00:45:08,880 --> 00:45:11,760
und dann werde ich auf

1091
00:45:11,760 --> 00:45:13,500
den Klassifizierungsexperimenten aufbauen, die ich zuvor gezeigt habe,

1092
00:45:13,500 --> 00:45:14,280


1093
00:45:14,280 --> 00:45:16,020


1094
00:45:16,020 --> 00:45:18,540
und zeigen, wie diese Priors es

1095
00:45:18,540 --> 00:45:21,060
mir tatsächlich ermöglichen, die Klassifizierung zu verbessern

1096
00:45:21,060 --> 00:45:22,500
Genauigkeit ist die

1097
00:45:22,500 --> 00:45:25,500
Testgenauigkeit vollständig verbundener prädiktiver

1098
00:45:25,500 --> 00:45:28,160
Codierungsmodelle.

1099
00:45:29,520 --> 00:45:31,680
Kommen wir also zum ersten Experiment,

1100
00:45:31,680 --> 00:45:33,300
das darin besteht, die Struktur des Diagramms abzuleiten,

1101
00:45:33,300 --> 00:45:34,980


1102
00:45:34,980 --> 00:45:37,319
und alle Experimente folgen

1103
00:45:37,319 --> 00:45:39,480
grundsätzlich der gleichen Pipeline in allen

1104
00:45:39,480 --> 00:45:42,060
Arbeiten auf diesem Gebiet. Der erste Schritt besteht darin, sie zu

1105
00:45:42,060 --> 00:45:45,119
generieren  Ein Vision-Netzwerk aus einem Zufallsgraphen,

1106
00:45:45,119 --> 00:45:46,079


1107
00:45:46,079 --> 00:45:48,359
also sind die beiden

1108
00:45:48,359 --> 00:45:50,640
Zufallsgraphen, die jeder testet, im Grunde genommen Erdos-

1109
00:45:50,640 --> 00:45:53,520
Renegraphen und skalierungsfreie Graphen,

1110
00:45:53,520 --> 00:45:55,859
sodass Sie diese großen Graphen generieren,

1111
00:45:55,859 --> 00:45:58,680
die normalerweise 20 für die 80 80

1112
00:45:58,680 --> 00:46:01,619
verschiedenen Knoten und einige Kanten haben,

1113
00:46:01,619 --> 00:46:04,619
die Sie zufällig abtasten

1114
00:46:04,619 --> 00:46:06,540
Sie verwenden dieses Diagramm, um einen Datensatz zu generieren,

1115
00:46:06,540 --> 00:46:08,280


1116
00:46:08,280 --> 00:46:10,819
sodass Sie beispielsweise

1117
00:46:10,819 --> 00:46:14,460
n große N Datenpunkte abtasten.

1118
00:46:14,460 --> 00:46:16,859
Sie nehmen das Diagramm, das Sie

1119
00:46:16,859 --> 00:46:18,780
zuvor erstellt haben, und werfen es

1120
00:46:18,780 --> 00:46:20,819
weg. Sie behalten nur den Datensatz

1121
00:46:20,819 --> 00:46:23,099
und die Aufgabe, die Sie haben  Was wir jetzt lösen wollen, ist zu

1122
00:46:23,099 --> 00:46:25,020
lernen,

1123
00:46:25,020 --> 00:46:27,420
einen Trainingsalgorithmus zu haben, der es

1124
00:46:27,420 --> 00:46:29,819
Ihnen im Grunde ermöglicht,

1125
00:46:29,819 --> 00:46:32,579
die Struktur des

1126
00:46:32,579 --> 00:46:34,619
Diagramms, das Sie weggeworfen haben, wiederherzustellen.

1127
00:46:34,619 --> 00:46:36,839
Die Art und Weise, wie wir es hier tun, ist, dass wir uns

1128
00:46:36,839 --> 00:46:38,460
in einer vollständig vernetzten kreativen Codierung befinden

1129
00:46:38,460 --> 00:46:41,760
Modellieren Sie diesen Datensatz D unter Verwendung der

1130
00:46:41,760 --> 00:46:43,800


1131
00:46:43,800 --> 00:46:45,359
zuvor definierten Sparse- und SQL-Prioritäten

1132
00:46:45,359 --> 00:46:48,780
und prüfen Sie, ob der

1133
00:46:48,780 --> 00:46:50,760
Graph, zu dem wir konvergieren, nachdem wir

1134
00:46:50,760 --> 00:46:53,220


1135
00:46:53,220 --> 00:46:55,319
die Einträge der Agenturmatrix entfernt haben, die

1136
00:46:55,319 --> 00:46:57,599
kleiner als ein bestimmter Schwellenwert sind, tatsächlich

1137
00:46:57,599 --> 00:47:00,060
ähnlich ist  das des ursprünglichen

1138
00:47:00,060 --> 00:47:02,359
Diagramms

1139
00:47:02,520 --> 00:47:04,500
und es wird auch gezeigt, dass dies

1140
00:47:04,500 --> 00:47:06,599
tatsächlich der Fall ist, also ist dies ein Beispiel

1141
00:47:06,599 --> 00:47:09,020
und ich zeige viele verschiedene

1142
00:47:09,020 --> 00:47:12,420
Parametrisierungen und Dimensionen und

1143
00:47:12,420 --> 00:47:15,060
ähnliches in der Arbeit,

1144
00:47:15,060 --> 00:47:16,920
aber ich denke, dass diese beiden die

1145
00:47:16,920 --> 00:47:18,900
repräsentativsten Beispiele sind  mit einem Fehler-

1146
00:47:18,900 --> 00:47:20,760
Kindergarten-Diagramm und einem Diagramm im freien Maßstab

1147
00:47:20,760 --> 00:47:23,579
mit 20 Knoten.

1148
00:47:23,579 --> 00:47:25,800
Hier auf der linken Seite können Sie das

1149
00:47:25,800 --> 00:47:27,300
Bodendurchgangsdiagramm sehen, das

1150
00:47:27,300 --> 00:47:29,339


1151
00:47:29,339 --> 00:47:30,839
zufällig ausgewählt wurde,

1152
00:47:30,839 --> 00:47:32,599
und auf der rechten Seite können Sie

1153
00:47:32,599 --> 00:47:35,220
das hübsche Schwierigkeitsmodell sehen, das

1154
00:47:35,220 --> 00:47:37,440
aus den Daten gelernt wurde  eingestellt

1155
00:47:37,440 --> 00:47:39,359
und wie Sie sehen können, sind sie ziemlich

1156
00:47:39,359 --> 00:47:40,500
ähnlich,

1157
00:47:40,500 --> 00:47:42,780
es ist immer noch nicht perfekt, also gibt es

1158
00:47:42,780 --> 00:47:45,000
einige, es gibt einige Fehler, aber

1159
00:47:45,000 --> 00:47:47,460
im Allgemeinen funktioniert die Struktur

1160
00:47:47,460 --> 00:47:49,500
recht gut. Wir haben auch einige

1161
00:47:49,500 --> 00:47:52,140
quantitative Experimente,

1162
00:47:52,140 --> 00:47:54,000
die ich hier nicht zeige  weil es

1163
00:47:54,000 --> 00:47:55,740
nur riesige Tabellen mit vielen Zahlen sind

1164
00:47:55,740 --> 00:47:57,180
und ich dachte, dass es vielleicht etwas

1165
00:47:57,180 --> 00:48:00,660
zu viel für die Präsentation wäre, aber

1166
00:48:00,660 --> 00:48:02,220
die Ergebnisse zeigen, dass sie eine ähnliche Leistung erbringen

1167
00:48:02,220 --> 00:48:06,060
wie moderne Methoden,

1168
00:48:06,060 --> 00:48:07,920
auch weil ich sagen muss, dass sie den meisten von

1169
00:48:07,920 --> 00:48:10,859
der Qualität gefallen  stammt aus dem Acigli

1170
00:48:10,859 --> 00:48:15,799
Prior, das im Jahr 2018 eingeführt wurde.

1171
00:48:16,920 --> 00:48:19,680
Die zweite Klasse von Experimenten sind

1172
00:48:19,680 --> 00:48:21,599
unsere Klassifizierungsexperimente, die, wie

1173
00:48:21,599 --> 00:48:23,880
gesagt, die Erweiterungen derjenigen sind, die

1174
00:48:23,880 --> 00:48:25,560
ich zuvor geteilt habe,

1175
00:48:25,560 --> 00:48:27,119
und die Idee ist, Strukturlernen zu verwenden,

1176
00:48:27,119 --> 00:48:28,560
um die Klassifizierung zu verbessern

1177
00:48:28,560 --> 00:48:31,140
zu den Klassifizierungsergebnissen zu den

1178
00:48:31,140 --> 00:48:33,420
Mittelwerten und Modemitteln des Datensatzes

1179
00:48:33,420 --> 00:48:36,780
ausgehend von einem vollständig verbundenen Diagramm.

1180
00:48:36,780 --> 00:48:40,560
Ich habe also die

1181
00:48:40,560 --> 00:48:42,839
vollständig verbundenen Diagrammcluster von

1182
00:48:42,839 --> 00:48:46,440
Neuronen unterteilt, sodass der 1B-Cluster

1183
00:48:46,440 --> 00:48:49,140
derjenige ist, der sich auf die Eingabe bezieht,

1184
00:48:49,140 --> 00:48:51,900
und alle kleinen dann  Wir haben eine

1185
00:48:51,900 --> 00:48:55,319
bestimmte Anzahl versteckter Cluster

1186
00:48:55,319 --> 00:48:57,720
und dann haben wir den Label-Cluster, der

1187
00:48:57,720 --> 00:48:58,800


1188
00:48:58,800 --> 00:49:01,560
die Klasse des Clusters von Neuronen

1189
00:49:01,560 --> 00:49:04,079
darstellt, die mir die Label-Vorhersagen liefern sollen,

1190
00:49:04,079 --> 00:49:06,480


1191
00:49:06,480 --> 00:49:08,700
und ich habe sie trainiert, indem ich sie für

1192
00:49:08,700 --> 00:49:10,980
die erste Verwendung des Sparse-Prior verwendet

1193
00:49:10,980 --> 00:49:14,099
Die Idee ist nur, wenn ich die

1194
00:49:14,099 --> 00:49:16,500
Verbindungen, die ich nicht benötige, aus einem

1195
00:49:16,500 --> 00:49:17,460
Modell beschneide

1196
00:49:17,460 --> 00:49:20,880
und lerne, ob das als Parser-Modell

1197
00:49:20,880 --> 00:49:24,119
gut funktioniert. Die Antwort lautet: Nein, es

1198
00:49:24,119 --> 00:49:25,500
funktioniert nicht, und der

1199
00:49:25,500 --> 00:49:28,500
Grund dafür ist  dass Sie am Ende das

1200
00:49:28,500 --> 00:49:30,660
Diagramm, mit dem Sie konvergieren, tatsächlich

1201
00:49:30,660 --> 00:49:32,700
das Generierte ist, sodass das Modell im Grunde

1202
00:49:32,700 --> 00:49:36,180
lernt, die Beschriftung basierend auf der

1203
00:49:36,180 --> 00:49:38,400
Beschriftung selbst vorherzusagen, sodass alle

1204
00:49:38,400 --> 00:49:40,020
Informationen aus der Eingabe verworfen werden

1205
00:49:40,020 --> 00:49:42,480
und nur die Beschriftung beibehalten wird, und wie Sie

1206
00:49:42,480 --> 00:49:45,119
hier sehen können  Label y sagt sich selbst oder

1207
00:49:45,119 --> 00:49:46,560
in anderen Experimenten voraus, wenn Sie die

1208
00:49:46,560 --> 00:49:48,960
Parameter ändern. Sie haben, dass

1209
00:49:48,960 --> 00:49:52,520
y bei Null vorhersagt, dass Präex

1210
00:49:52,520 --> 00:49:55,980


1211
00:49:55,980 --> 00:49:57,240


1212
00:49:57,240 --> 00:49:59,520


1213
00:49:59,520 --> 00:50:03,000
zu einem azyklischen Graphen,

1214
00:50:03,000 --> 00:50:05,220
und deshalb müssen wir etwas hinzufügen, das

1215
00:50:05,220 --> 00:50:08,000
eine Zyklizität verhindert, und was das ist,

1216
00:50:08,000 --> 00:50:10,200
ist natürlich das, was ich bereits

1217
00:50:10,200 --> 00:50:12,780
vorgeschlagen habe, und dann zeige ich eine zweite

1218
00:50:12,780 --> 00:50:14,520
Technik,

1219
00:50:14,520 --> 00:50:17,280
sodass die erste die zuvor definierte SQL verwendet

1220
00:50:17,280 --> 00:50:18,680


1221
00:50:18,680 --> 00:50:21,359
und die zweite  a ist eine neuartige

1222
00:50:21,359 --> 00:50:22,859
Technik, die tatsächlich

1223
00:50:22,859 --> 00:50:24,359
negative Beispiele verwendet,

1224
00:50:24,359 --> 00:50:26,520
sodass ein negatives Beispiel in diesem

1225
00:50:26,520 --> 00:50:30,060
Fall einfach der a-Datenpunkt ist, in

1226
00:50:30,060 --> 00:50:32,280
dem Sie ein Bild haben, aber die Beschriftung falsch ist,

1227
00:50:32,280 --> 00:50:33,240


1228
00:50:33,240 --> 00:50:35,220
sodass Sie hier beispielsweise ein Bild

1229
00:50:35,220 --> 00:50:36,900
einer Sieben haben  Aber die Bezeichnung, die ich

1230
00:50:36,900 --> 00:50:39,599
dem Modell gebe, ist eine Zwei,

1231
00:50:39,599 --> 00:50:40,980


1232
00:50:40,980 --> 00:50:44,579
und die Idee ist sehr einfach, da sie

1233
00:50:44,579 --> 00:50:47,460
bereits in vielen äh-Werken verwendet wurde.

1234
00:50:47,460 --> 00:50:49,740
Jedes Mal, wenn das Modell ein positives

1235
00:50:49,740 --> 00:50:52,079
Beispiel ist, muss es erhöht werden, um

1236
00:50:52,079 --> 00:50:53,520
die Variation zu minimieren  der freien Energie

1237
00:50:53,520 --> 00:50:56,520
und jedes Mal, wenn es ein negatives

1238
00:50:56,520 --> 00:50:58,859
Beispiel ist, muss es erhöht werden.

1239
00:50:58,859 --> 00:51:01,260
Lassen Sie mich also auf den Fehler eingehen, diese

1240
00:51:01,260 --> 00:51:04,200
Menge

1241
00:51:04,200 --> 00:51:05,960


1242
00:51:05,960 --> 00:51:08,579
durch viele Experimente und viele

1243
00:51:08,579 --> 00:51:10,859
äh Experimente zu minimieren. Wir haben gesehen, dass die

1244
00:51:10,859 --> 00:51:12,119
beiden Techniken

1245
00:51:12,119 --> 00:51:15,000
Im Grunde führt das erste zu den gleichen

1246
00:51:15,000 --> 00:51:17,220
Ergebnissen und das zweite auch zu der gleichen

1247
00:51:17,220 --> 00:51:18,599
Grafik.

1248
00:51:18,599 --> 00:51:21,000
Hier sind also

1249
00:51:21,000 --> 00:51:22,800
die neuen Ergebnisse, einige Mittel und

1250
00:51:22,800 --> 00:51:25,079
Modemittel unter Verwendung der beiden Techniken,

1251
00:51:25,079 --> 00:51:27,660
die ich gerade vorgeschlagen habe,

1252
00:51:27,660 --> 00:51:30,960
und jetzt gehen wir zu einigen über  sind immer

1253
00:51:30,960 --> 00:51:33,900
noch nicht großartig, aber auf jeden Fall

1254
00:51:33,900 --> 00:51:36,000
vernünftigere Testgenauigkeiten, also haben wir hier

1255
00:51:36,000 --> 00:51:39,059
einen Testfehler von 3,17 für Minuten und einen

1256
00:51:39,059 --> 00:51:42,119
Testfehler von 13,98 für Modemittel,

1257
00:51:42,119 --> 00:51:44,819
und tatsächlich können diese Ergebnisse

1258
00:51:44,819 --> 00:51:48,300
erheblich verbessert werden, wenn man die

1259
00:51:48,300 --> 00:51:51,300
Struktur des Diagramms daraus lernt  zerkleinert

1260
00:51:51,300 --> 00:51:53,040
und dann die Struktur des

1261
00:51:53,040 --> 00:51:55,319
Diagramms korrigiert und eine Art Feinabstimmung durchführt.

1262
00:51:55,319 --> 00:51:57,660
Wenn Sie also das Modell auf

1263
00:51:57,660 --> 00:52:00,000
die richtige hierarchische Struktur abstimmen,

1264
00:52:00,000 --> 00:52:01,980
erreichen Sie irgendwann die Testgenauigkeit,

1265
00:52:01,980 --> 00:52:03,359
die Sie von einem

1266
00:52:03,359 --> 00:52:05,460
hierarchischen Modell erwarten würden, aber diese  Dabei handelt es sich

1267
00:52:05,460 --> 00:52:08,099
einfach um diejenige, zu der das vollständig verbundene Modell auf

1268
00:52:08,099 --> 00:52:10,980
natürliche Weise konvergiert, also

1269
00:52:10,980 --> 00:52:13,859
beispielsweise aus einem Testfehler von

1270
00:52:13,859 --> 00:52:15,420
18,32

1271
00:52:15,420 --> 00:52:17,339
des vollständig verbundenen Modellzugs auf

1272
00:52:17,339 --> 00:52:20,359
Mode-Mitteln, indem einfach

1273
00:52:20,359 --> 00:52:22,859
Korrelationen oder bedingte Abfragen durchgeführt werden,

1274
00:52:22,859 --> 00:52:24,420
was eine Standardmethode zum Abfragen

1275
00:52:24,420 --> 00:52:26,520
operativer Codierungsmodelladdition ist

1276
00:52:26,520 --> 00:52:29,220
Interventionen und der AC-Click-

1277
00:52:29,220 --> 00:52:32,040
Prior zusammen machen diesen

1278
00:52:32,040 --> 00:52:34,200
Testfehler viel geringer

1279
00:52:34,200 --> 00:52:37,200
und wir können ihn auch für mehrere Monate beobachten.

1280
00:52:37,200 --> 00:52:39,319


1281
00:52:39,780 --> 00:52:41,819
Ich gehe nicht ein wenig ins Detail

1282
00:52:41,819 --> 00:52:45,420
auf dieses letzte Experiment und darauf, wie

1283
00:52:45,420 --> 00:52:48,660
der azyklische Prior auf das wirkt  Struktur

1284
00:52:48,660 --> 00:52:50,339
des Diagramms,

1285
00:52:50,339 --> 00:52:52,440
also führe ich ein Experiment mit

1286
00:52:52,440 --> 00:52:54,960
einem neuen Datensatz durch, was bedeutet, dass das

1287
00:52:54,960 --> 00:52:56,460
Aufrufen des neuen Datensatzes möglicherweise

1288
00:52:56,460 --> 00:52:58,500
zu viel ist. Ich nenne es einen Datensatz mit zwei Mittelwerten,

1289
00:52:58,500 --> 00:53:01,440
in dem Sie den Eingabepunkt haben

1290
00:53:01,440 --> 00:53:04,319
besteht aus zwei verschiedenen

1291
00:53:04,319 --> 00:53:07,319
Bildern und die Bezeichnung hängt nur vom

1292
00:53:07,319 --> 00:53:08,520
zweiten Bild

1293
00:53:08,520 --> 00:53:10,800
in der ersten Bildgeschichte ab.

1294
00:53:10,800 --> 00:53:12,720
Die Idee hier ist also, dass

1295
00:53:12,720 --> 00:53:15,079
die Struktur des Modells die

1296
00:53:15,079 --> 00:53:18,540
Zyklizität vorantreibt und Dinge wie diese in der

1297
00:53:18,540 --> 00:53:20,819
Lage sind, zu erkennen, dass es sich um die zweite Hälfte

1298
00:53:20,819 --> 00:53:23,400
des Bildes handelt  Eigentlich

1299
00:53:23,400 --> 00:53:27,960
bedeutungslos, wenn es darum geht, die

1300
00:53:27,960 --> 00:53:31,140
Klassifizierung durchzuführen

1301
00:53:31,140 --> 00:53:33,119


1302
00:53:33,119 --> 00:53:36,480


1303
00:53:36,480 --> 00:53:39,000


1304
00:53:39,000 --> 00:53:41,940


1305
00:53:41,940 --> 00:53:43,740


1306
00:53:43,740 --> 00:53:45,900


1307
00:53:45,900 --> 00:53:48,960
Eines, von dem wir wissen, dass es bei Klassifizierungsaufgaben am besten abschneidet.

1308
00:53:48,960 --> 00:53:50,880


1309
00:53:50,880 --> 00:53:53,520
Hier ist ein Beispiel für eine

1310
00:53:53,520 --> 00:53:54,980
Trainingsmethode, die

1311
00:53:54,980 --> 00:53:59,280
ausgeführt wird. Bei c0, dem Beginn des

1312
00:53:59,280 --> 00:54:00,720
Trainings,

1313
00:54:00,720 --> 00:54:03,000
haben wir dieses Modell hier, sodass s0

1314
00:54:03,000 --> 00:54:05,819
den Sieben und damit

1315
00:54:05,819 --> 00:54:08,099
dem ersten Bild entspricht  Da eins

1316
00:54:08,099 --> 00:54:09,839
wieder dem siebenspaltigen Bild entspricht, haben wir die

1317
00:54:09,839 --> 00:54:12,300
Bezeichnung Y und alle latenten Variablen x0

1318
00:54:12,300 --> 00:54:13,800


1319
00:54:13,800 --> 00:54:15,720


1320
00:54:15,720 --> 00:54:17,040


1321
00:54:17,040 --> 00:54:20,579


1322
00:54:20,579 --> 00:54:23,720


1323
00:54:23,720 --> 00:54:27,319
Modell für ein paar Epochen, bis

1324
00:54:27,319 --> 00:54:30,540
und was wir sofort wissen, ist, dass

1325
00:54:30,540 --> 00:54:31,920
das Modell zum Beispiel sofort

1326
00:54:31,920 --> 00:54:34,740
versteht, dass die vier nicht

1327
00:54:34,740 --> 00:54:36,839
für die Durchführung der Klassifizierung benötigt werden, also nicht

1328
00:54:36,839 --> 00:54:40,740
jeder ausgehende Knoten aus

1329
00:54:40,740 --> 00:54:43,980
dem zweiten Eingabecluster entfernt wird

1330
00:54:43,980 --> 00:54:45,900
und  Was wir nicht verstanden haben, ist,

1331
00:54:45,900 --> 00:54:48,660
dass dies dieser Cluster ist, der

1332
00:54:48,660 --> 00:54:50,400
mit der Ausgabe zusammenhängt,

1333
00:54:50,400 --> 00:54:52,260
also

1334
00:54:52,260 --> 00:54:55,319
haben wir eine direkte lineare Abbildung von s0 nach Y,

1335
00:54:55,319 --> 00:54:56,480


1336
00:54:56,480 --> 00:54:59,339
was diesen Teil hier darstellt,

1337
00:54:59,339 --> 00:55:01,160
aber wir wissen, dass eine lineare Abbildung tatsächlich

1338
00:55:01,160 --> 00:55:04,740
nicht die beste ist  Karte zur

1339
00:55:04,740 --> 00:55:07,200
Durchführung der Klassifizierung von Mittelwerten, daher

1340
00:55:07,200 --> 00:55:08,700
benötigen wir eine gewisse Hierarchie und eine gewisse

1341
00:55:08,700 --> 00:55:11,579
Tiefe, um die Ergebnisse zu verbessern. Wie

1342
00:55:11,579 --> 00:55:14,220
Sie sehen können, ist diese Linie hier die

1343
00:55:14,220 --> 00:55:15,599
Genauigkeit,

1344
00:55:15,599 --> 00:55:18,960
die bis zu diesem Punkt, also bis C2,

1345
00:55:18,960 --> 00:55:22,500
ähnlich ist wie ähm, also 91  Das ist

1346
00:55:22,500 --> 00:55:24,059
etwas besser als die lineare

1347
00:55:24,059 --> 00:55:25,500
Klassifizierung,

1348
00:55:25,500 --> 00:55:28,740
aber sobald Sie mit dem Training fortfahren,

1349
00:55:28,740 --> 00:55:30,660
erkennt das Modell, dass es eine gewisse

1350
00:55:30,660 --> 00:55:33,119
Hierarchie benötigt, um die Daten besser anzupassen. Sie

1351
00:55:33,119 --> 00:55:35,640
sehen also, dass dieser Pfeil

1352
00:55:35,640 --> 00:55:38,760
mit der Zeit immer stärker wird,

1353
00:55:38,760 --> 00:55:41,700
bis es versteht, dass die lineare Klassifizierung  Die

1354
00:55:41,700 --> 00:55:44,339
Karte wird eigentlich nicht wirklich benötigt und

1355
00:55:44,339 --> 00:55:45,920
entfernt sie,

1356
00:55:45,920 --> 00:55:48,780
sodass das Modell, mit dem Sie konvergieren, ein

1357
00:55:48,780 --> 00:55:51,000
Modell ist, das bei einer Null beginnt, zu einem

1358
00:55:51,000 --> 00:55:53,760
versteckten Knoten geht und dann zum

1359
00:55:53,760 --> 00:55:57,180
Etikett mit einer sehr schwachen linearen Karte geht,

1360
00:55:57,180 --> 00:55:59,700
die tatsächlich entfernt wird, wenn Sie dies tun

1361
00:55:59,700 --> 00:56:02,760
Sie legen einen Schwellenwert von

1362
00:56:02,760 --> 00:56:05,520
beispielsweise 0,1 bis 0,2 fest.

1363
00:56:05,520 --> 00:56:07,619
Irgendwann gerät die lineare Karte in Vergessenheit und

1364
00:56:07,619 --> 00:56:10,680
alles, was Sie am Ende haben, ist

1365
00:56:10,680 --> 00:56:13,319
ein hierarchisches Netzwerk,

1366
00:56:13,319 --> 00:56:15,720
also hat es die

1367
00:56:15,720 --> 00:56:17,099
richtige Struktur gelernt, um es auszuführen

1368
00:56:17,099 --> 00:56:19,260
Klassifizierungsaufgaben, die hierarchisch sind,

1369
00:56:19,260 --> 00:56:21,900
und es hat auch gelernt, dass das zweite

1370
00:56:21,900 --> 00:56:25,020
Bild keine Rolle bei der Definition

1371
00:56:25,020 --> 00:56:28,440
der Testgenauigkeit spielte und das ist alles, was alles

1372
00:56:28,440 --> 00:56:30,420
durchgeführt wird, und alle diese

1373
00:56:30,420 --> 00:56:33,839
Aufgaben werden einfach durch Ausführen

1374
00:56:33,839 --> 00:56:36,599
eines Prozesses zur Minimierung freier Energie ausgeführt

1375
00:56:36,599 --> 00:56:38,400
Sie initialisieren das Modell, Sie definieren die

1376
00:56:38,400 --> 00:56:40,859
freie Energie, Sie definieren die Prioritäten, also

1377
00:56:40,859 --> 00:56:43,559
den Sparse- und den C-Klick, bevor

1378
00:56:43,559 --> 00:56:45,780
Sie die Energieminimierung ausführen, und

1379
00:56:45,780 --> 00:56:47,400
Sie konvergieren zu einem

1380
00:56:47,400 --> 00:56:49,500
hierarchischen Modell, das gut in der Lage ist, eine

1381
00:56:49,500 --> 00:56:51,839
Klassifizierung für Hackfleisch durchzuführen,

1382
00:56:51,839 --> 00:56:54,000
und dann, wenn Sie  Wenn Sie dann etwas Feinabstimmung durchführen,

1383
00:56:54,000 --> 00:56:55,800
erzielen Sie sehr wettbewerbsfähige

1384
00:56:55,800 --> 00:56:57,359
Ergebnisse, wie Sie es in Feed-Forward-

1385
00:56:57,359 --> 00:56:59,339
Netzwerken mit der Feedback-Ausbreitung tun,

1386
00:56:59,339 --> 00:57:01,260
aber ich denke, das ist nicht das

1387
00:57:01,260 --> 00:57:03,780
Interessante. Das Interessante ist, dass Ihnen

1388
00:57:03,780 --> 00:57:05,160
dieser ganze Prozess gefällt, dieser Prozess

1389
00:57:05,160 --> 00:57:07,980
insgesamt aus Intervention und der

1390
00:57:07,980 --> 00:57:09,780
Azyklizität, die

1391
00:57:09,780 --> 00:57:11,700
Ihnen ermöglicht  ein vollständig verbundenes Netzwerk zu nehmen

1392
00:57:11,700 --> 00:57:12,660


1393
00:57:12,660 --> 00:57:15,119
und zu einem hierarchischen zu konvergieren, das in der

1394
00:57:15,119 --> 00:57:16,140
Lage ist, eine

1395
00:57:16,140 --> 00:57:20,058
Klassifizierung mit guten Ergebnissen durchzuführen,

1396
00:57:20,760 --> 00:57:23,000
und ja,

1397
00:57:23,000 --> 00:57:26,280
das ist es im Grunde. Ich bin jetzt oh ja, wow,

1398
00:57:26,280 --> 00:57:29,220
ich habe viel geredet und ich bin äh, das ist die

1399
00:57:29,220 --> 00:57:32,160
Schlussfolgerung  Ich

1400
00:57:32,160 --> 00:57:35,280
schreibe im Grunde genommen eine kleine Zusammenfassung des Vortrags und

1401
00:57:35,280 --> 00:57:37,559
denke, dass die wichtigste Erkenntnis, wenn ich

1402
00:57:37,559 --> 00:57:39,300
Ihnen in einem Satz dieses

1403
00:57:39,300 --> 00:57:40,980
Dokuments mitteilen muss, ist, dass Predictive Coding eine

1404
00:57:40,980 --> 00:57:44,400
Methode zur Aktualisierung von Überzeugungen ist, die in der Lage ist,

1405
00:57:44,400 --> 00:57:46,559
End-to-End-Leistungen zu erbringen  -End-Cousin-Lernen, damit

1406
00:57:46,559 --> 00:57:48,599
er Interventionen durchführen kann, um

1407
00:57:48,599 --> 00:57:51,420
eine Struktur aus Daten zu lernen und dann

1408
00:57:51,420 --> 00:57:53,160
Interventionen und Kontrafaktuale durchzuführen,

1409
00:57:53,160 --> 00:57:56,058


1410
00:57:56,700 --> 00:57:58,440
um kausale Schlussfolgerungen in anderen und

1411
00:57:58,440 --> 00:58:00,119
effizienten Modellinterventionen zu ziehen, indem er

1412
00:58:00,119 --> 00:58:01,680
einfach den Vorhersagefehler auf

1413
00:58:01,680 --> 00:58:03,359
Null setzt, sodass es sich um eine sehr einfach

1414
00:58:03,359 --> 00:58:06,240
durchzuführende Technik handelt  Interventionen und

1415
00:58:06,240 --> 00:58:07,619
Sie müssen einfach nur ein

1416
00:58:07,619 --> 00:58:08,940
Neuron berühren, Sie müssen nicht auf die

1417
00:58:08,940 --> 00:58:10,859
Struktur des Diagramms einwirken,

1418
00:58:10,859 --> 00:58:14,339
Sie können es verwenden, um

1419
00:58:14,339 --> 00:58:16,140
Struktur-Kausal-Modelle zu erstellen, die

1420
00:58:16,140 --> 00:58:18,359
biologisch plausibel sind, und

1421
00:58:18,359 --> 00:58:20,819
es ist in der Lage, die Struktur zu lernen  Äh,

1422
00:58:20,819 --> 00:58:24,119
aus Daten, wie ich schon sagte, vielleicht schon oft,

1423
00:58:24,119 --> 00:58:26,940


1424
00:58:26,940 --> 00:58:28,740
und ein paar Sätze über zukünftige

1425
00:58:28,740 --> 00:58:31,260
Arbeiten sind, dass es

1426
00:58:31,260 --> 00:58:33,180
schön wäre,

1427
00:58:33,180 --> 00:58:36,119
die Leistung des von uns definierten Modells zu verbessern,

1428
00:58:36,119 --> 00:58:38,460
weil ich denke, dass es damit

1429
00:58:38,460 --> 00:58:40,980
einigermaßen gut funktioniert  Es gibt viele

1430
00:58:40,980 --> 00:58:43,079
Aufgaben, sodass es beim strukturellen Lernen einigermaßen gut abschneidet, was

1431
00:58:43,079 --> 00:58:45,780
für mich

1432
00:58:45,780 --> 00:58:48,119
Interventionen und Kontrafakten betrifft, aber

1433
00:58:48,119 --> 00:58:49,440
wenn man sich das aktuelle

1434
00:58:49,440 --> 00:58:51,420
Modell anschaut, gibt es immer eine ganz

1435
00:58:51,420 --> 00:58:53,880
bestimmte Methode, die bei

1436
00:58:53,880 --> 00:58:55,559
einer einzelnen Aufgabe besser abschneidet,

1437
00:58:55,559 --> 00:58:58,260
also wäre es interessant, das zu sehen  Wenn wir

1438
00:58:58,260 --> 00:59:00,180
dieses Leistungsniveau

1439
00:59:00,180 --> 00:59:03,599
bei bestimmten Aufgaben erreichen können, indem wir einige

1440
00:59:03,599 --> 00:59:05,599
Tricks oder einige

1441
00:59:05,599 --> 00:59:10,260
oder einige neue Optimierungsmethoden hinzufügen und es

1442
00:59:10,260 --> 00:59:12,839
auf dynamische Systeme verallgemeinern,

1443
00:59:12,839 --> 00:59:14,280
die eigentlich viel interessanter sind als

1444
00:59:14,280 --> 00:59:17,220
statische Systeme, wie z. B. dynamische

1445
00:59:17,220 --> 00:59:20,099
Kausalmodelle und/oder  Andere Techniken, mit

1446
00:59:20,099 --> 00:59:22,200
denen Sie

1447
00:59:22,200 --> 00:59:25,200
kausale Schlussfolgerungen in Systemen ziehen können, die sich bewegen, sodass

1448
00:59:25,200 --> 00:59:27,799
eine Aktion, die in einem bestimmten Zeitschritt ausgeführt wird,

1449
00:59:27,799 --> 00:59:30,299
einen anderen Knoten in einem späteren Zeitschritt beeinflusst,

1450
00:59:30,299 --> 00:59:32,640
was im Grunde genommen eine großartige

1451
00:59:32,640 --> 00:59:34,859
Kausalität darstellt.

1452
00:59:34,859 --> 00:59:38,160
Ja, das ist es und ähm, vielen Dank,

1453
00:59:38,160 --> 00:59:41,118
vielen

1454
00:59:47,460 --> 00:59:51,119
Dank  Tolle und sehr umfassende

1455
00:59:51,119 --> 00:59:53,160
Präsentation, die wirklich

1456
00:59:53,160 --> 00:59:55,700


1457
00:59:57,119 --> 00:59:59,700
gedämpft war, aber ja, danke für

1458
00:59:59,700 --> 01:00:02,400
die tolle und sehr umfassende

1459
01:00:02,400 --> 01:00:05,099
Präsentation. Es war wirklich viel

1460
01:00:05,099 --> 01:00:06,900
da und es gab auch viele tolle

1461
01:00:06,900 --> 01:00:09,900
Fragen im Live-Chat, also vielleicht zum

1462
01:00:09,900 --> 01:00:12,900
Aufwärmen  Auf die Frage eingehend, wie Sie dazu gekommen sind, sich mit

1463
01:00:12,900 --> 01:00:15,960
diesem Thema zu befassen, als Sie sich mit

1464
01:00:15,960 --> 01:00:18,900
Kausalität beschäftigt haben und prädiktive Codierung als

1465
01:00:18,900 --> 01:00:21,000
nützlich empfunden haben oder umgekehrt, oder wie Sie

1466
01:00:21,000 --> 01:00:23,160
an diesen Schnittpunkt gekommen sind,

1467
01:00:23,160 --> 01:00:25,740
muss ich tatsächlich sagen, dass die erste

1468
01:00:25,740 --> 01:00:27,240
Person, die mit dieser Idee herauskam, …

1469
01:00:27,240 --> 01:00:29,040
Äh, Baron war

1470
01:00:29,040 --> 01:00:33,900
so ähnlich, wie ich glaube, vor anderthalb Jahren

1471
01:00:33,900 --> 01:00:36,660
noch mehr brachte er

1472
01:00:36,660 --> 01:00:38,940
diese Idee mit und dann geriet er in

1473
01:00:38,940 --> 01:00:42,119
Vergessenheit, und niemand nahm sie auf, und äh,

1474
01:00:42,119 --> 01:00:43,980
und letzten Sommer fing ich an,

1475
01:00:43,980 --> 01:00:47,880
neugierig auf Kausalität und die

1476
01:00:47,880 --> 01:00:50,339
ähm zu werden  Ich habe zum Beispiel „Das Buch des Lebens“ gelesen,

1477
01:00:50,339 --> 01:00:52,440
während ich mir Podcasts angehört habe. Ich kenne die

1478
01:00:52,440 --> 01:00:53,760
übliche Art und Weise, wie man sich

1479
01:00:53,760 --> 01:00:54,900
für ein Thema interessiert,

1480
01:00:54,900 --> 01:00:57,480
und ich erinnere mich an diese Idee von

1481
01:00:57,480 --> 01:01:00,180
Baron und habe sie ihm vorgeschlagen, und ähm, und

1482
01:01:00,180 --> 01:01:03,180
ich dachte, warum nicht?  Wir erweitern es und ähm

1483
01:01:03,180 --> 01:01:06,000
und machen daraus tatsächlich eine Arbeit, also habe ich

1484
01:01:06,000 --> 01:01:07,319
ein paar Leute hinzugezogen, die mir bei den

1485
01:01:07,319 --> 01:01:09,359
Experimenten helfen und ähm, und das ist das

1486
01:01:09,359 --> 01:01:12,000
Endergebnis am Ende,

1487
01:01:12,000 --> 01:01:14,160
großartig, cool, ja, ähm,

1488
01:01:14,160 --> 01:01:15,240


1489
01:01:15,240 --> 01:01:17,400
viel zu sagen, ich werde einfach zum

1490
01:01:17,400 --> 01:01:19,619
Live-Chat gehen  Zuerst gehe ich auf eine Reihe

1491
01:01:19,619 --> 01:01:21,240
unterschiedlicher Fragen ein und wenn mich noch jemand

1492
01:01:21,240 --> 01:01:22,440
hinzufügen möchte, mache ich zuerst das Licht

1493
01:01:22,440 --> 01:01:24,059
an, denn ich glaube, ich tappe

1494
01:01:24,059 --> 01:01:28,440
immer mehr im Dunkeln. Ja,

1495
01:01:28,440 --> 01:01:30,720
wer hat gesagt, dass aktive Schlussfolgerungen keine Lösung bieten können?  Ein

1496
01:01:30,720 --> 01:01:32,160
Problem mit dem dunklen Raum.

1497
01:01:32,160 --> 01:01:34,980
Oh ja, hier sind wir.

1498
01:01:34,980 --> 01:01:37,020
Würden Sie also sagen, dass der Lichtschalter dafür gesorgt hat, dass es

1499
01:01:37,020 --> 01:01:39,299
heller ist? Ja,

1500
01:01:39,299 --> 01:01:40,680


1501
01:01:40,680 --> 01:01:42,240
ich denke,

1502
01:01:42,240 --> 01:01:43,980
hier gibt es keine Probleme,

1503
01:01:43,980 --> 01:01:46,940
ähm, okay, ml Dawn hat geschrieben,

1504
01:01:46,940 --> 01:01:49,559
da bei der prädiktiven Codierung

1505
01:01:49,559 --> 01:01:52,020
normalerweise alle Verteilungen Gaußsche Verteilungen sind und die

1506
01:01:52,020 --> 01:01:53,760
Bottom-up-Nachrichten präzisionsgewichtete

1507
01:01:53,760 --> 01:01:55,500
Vorhersagen sind  Fehler, bei denen

1508
01:01:55,500 --> 01:01:57,420
Präzision die Umkehrung der Gaußschen

1509
01:01:57,420 --> 01:02:00,000
Kovarianz ist. Wenn nicht-Gaußsche

1510
01:02:00,000 --> 01:02:03,319
Verteilungen verwendet werden,

1511
01:02:03,780 --> 01:02:05,339


1512
01:02:05,339 --> 01:02:09,059
bleibt die allgemeine Methode grundsätzlich anders

1513
01:02:09,059 --> 01:02:10,380


1514
01:02:10,380 --> 01:02:13,079


1515
01:02:13,079 --> 01:02:15,480


1516
01:02:15,480 --> 01:02:18,480
Ableitung der

1517
01:02:18,480 --> 01:02:20,819
virtuellen freien Energie, wenn Sie Gaußsche Annahmen haben,

1518
01:02:20,819 --> 01:02:22,920


1519
01:02:22,920 --> 01:02:25,020
ja, Sie müssen sogar diese einzelne Größe

1520
01:02:25,020 --> 01:02:27,960
auf Null setzen, und Sie müssen wahrscheinlich

1521
01:02:27,960 --> 01:02:29,880
auf die Struktur des Diagramms einwirken,

1522
01:02:29,880 --> 01:02:30,900


1523
01:02:30,900 --> 01:02:34,020
um Eingriffe durchzuführen,

1524
01:02:34,020 --> 01:02:37,079
und Sie, äh, und Ihre Kollegen hatten

1525
01:02:37,079 --> 01:02:39,900
im Jahr 2022 auch eine Vorhersagearbeit  Codierung über

1526
01:02:39,900 --> 01:02:41,880
Gaußsche Verteilungen hinaus, die sich mit

1527
01:02:41,880 --> 01:02:43,859
einigen dieser Probleme befasst haben, richtig, ja,

1528
01:02:43,859 --> 01:02:46,260
genau, also war dieser Aufsatz ein

1529
01:02:46,260 --> 01:02:47,339
bisschen

1530
01:02:47,339 --> 01:02:50,460
die Idee hinter diesem Aufsatz ist, äh,

1531
01:02:50,460 --> 01:02:53,220
und wir modellieren Transformer, das ist die

1532
01:02:53,220 --> 01:02:54,420
größte Motivation, indem wir ziemlich

1533
01:02:54,420 --> 01:02:57,180
schwierig sind, und die Antwort ist, äh, liegt nicht

1534
01:02:57,180 --> 01:02:59,460
daran, dass  Der Aufmerksamkeitsmechanismus hat

1535
01:02:59,460 --> 01:03:02,099
einen Soft-Max am Ende und Soft-Max ruft

1536
01:03:02,099 --> 01:03:03,960
auf,

1537
01:03:03,960 --> 01:03:08,400
äh, nicht auf Gaußsche Verteilung, sondern auf

1538
01:03:08,400 --> 01:03:11,280
ja, auf Soft-Max-Verteilung. Ich

1539
01:03:11,280 --> 01:03:13,440
verstehe den Namen jetzt nicht, aber ja und äh,

1540
01:03:13,440 --> 01:03:16,079
also ja, das ist eine Verallgemeinerung,

1541
01:03:16,079 --> 01:03:19,140
es ist ein bisschen  Es ist schwierig, es zu nennen, sobald

1542
01:03:19,140 --> 01:03:20,700
man die Gaston-Annahme entfernt hat. Es ist

1543
01:03:20,700 --> 01:03:22,319
immer noch ein bisschen schwierig, es

1544
01:03:22,319 --> 01:03:24,059
kreatives Codieren zu nennen,

1545
01:03:24,059 --> 01:03:26,400
also spricht er

1546
01:03:26,400 --> 01:03:29,819
zum Beispiel gerne mit äh, Auto

1547
01:03:29,819 --> 01:03:32,700
Freestone, entweder er mag kreatives Codieren

1548
01:03:32,700 --> 01:03:35,160
nur, wenn man nur Gauß

1549
01:03:35,160 --> 01:03:37,680
und Gauß hat  Annahmen,

1550
01:03:37,680 --> 01:03:39,720
aber ja, das ist eher eine philosophische

1551
01:03:39,720 --> 01:03:42,660
Debatte als äh,

1552
01:03:42,660 --> 01:03:44,940
interessant und ein weiteres Thema, das meiner Meinung nach

1553
01:03:44,940 --> 01:03:46,740
definitiv von großem

1554
01:03:46,740 --> 01:03:49,500
Interesse ist, sind Ähnlichkeiten und Unterschiede

1555
01:03:49,500 --> 01:03:52,980
zwischen dem Aufmerksamkeitsapparat in

1556
01:03:52,980 --> 01:03:56,099
Transformers und der Art und Weise, wie Aufmerksamkeit

1557
01:03:56,099 --> 01:03:58,440
aus einer neurokognitiven

1558
01:03:58,440 --> 01:04:00,180
Perspektive und aus einer prädiktiven

1559
01:04:00,180 --> 01:04:03,240
Verarbeitungspräzision beschrieben wird  Wartewinkel, was

1560
01:04:03,240 --> 01:04:06,200
denkst du darüber? Nun ja,

1561
01:04:06,359 --> 01:04:08,700
die Idee ist, dass, ähm

1562
01:04:08,700 --> 01:04:12,359
ja, ich denke darüber nach, dass Aufmerksamkeit aus einer

1563
01:04:12,359 --> 01:04:15,000
hübschen Verarbeitungs- und auch

1564
01:04:15,000 --> 01:04:16,400
operativen Schlussfolgerungsperspektive

1565
01:04:16,400 --> 01:04:19,260
als eine Art

1566
01:04:19,260 --> 01:04:21,299
strukturelles Lernproblem angesehen werden kann. Ich

1567
01:04:21,299 --> 01:04:23,040
denke, da gibt es ein  Aktuelles Papier

1568
01:04:23,040 --> 01:04:25,680
von Chris Buckleys Gruppe, das zeigt,

1569
01:04:25,680 --> 01:04:26,339
dass

1570
01:04:26,339 --> 01:04:28,079
es einen

1571
01:04:28,079 --> 01:04:30,420
Nachdruck im Archiv geben sollte, in dem im Grunde

1572
01:04:30,420 --> 01:04:31,859
gezeigt wurde, dass der Aufmerksamkeitsmechanismus

1573
01:04:31,859 --> 01:04:35,819
einfach die Präzision anhand

1574
01:04:35,819 --> 01:04:38,880
der Gewichtsparameter lernt, die für

1575
01:04:38,880 --> 01:04:41,040
andere Datenpunkte spezifisch sind, also diese Präzision  Es handelt sich

1576
01:04:41,040 --> 01:04:43,200
nicht um einen Parameter,

1577
01:04:43,200 --> 01:04:45,540
der in der Struktur des Modells enthalten ist, es handelt sich also

1578
01:04:45,540 --> 01:04:47,579
nicht um einen modellspezifischen Parameter. Es handelt sich um

1579
01:04:47,579 --> 01:04:49,140
einen sich schnell ändernden Parameter wie die

1580
01:04:49,140 --> 01:04:51,660
Wertknoten, der aktualisiert wird, während

1581
01:04:51,660 --> 01:04:53,760
die Variation der freien Energie minimiert wird

1582
01:04:53,760 --> 01:04:55,440
und sobald sie einmal angezeigt werden  Ich habe es minimiert

1583
01:04:55,440 --> 01:04:57,000
und berechnet, dann wirf man es weg

1584
01:04:57,000 --> 01:04:58,920
und für den nächsten Datenpunkt muss man

1585
01:04:58,920 --> 01:05:00,780
es von Grund auf neu berechnen,

1586
01:05:00,780 --> 01:05:03,299
also ja, ich denke, die

1587
01:05:03,299 --> 01:05:05,819
Analogieberechnung ist, dass der Aufmerksamkeitsmechanismus

1588
01:05:05,819 --> 01:05:07,920
als eine Art Strukturlernen angesehen werden kann,

1589
01:05:07,920 --> 01:05:10,559
aber a  Strukturiertes

1590
01:05:10,559 --> 01:05:13,020
Lernen, das datenpunktspezifisch und

1591
01:05:13,020 --> 01:05:15,119
nicht modellspezifisch ist,

1592
01:05:15,119 --> 01:05:17,280
und ich denke, wenn wir ein

1593
01:05:17,280 --> 01:05:18,960
wenig verallgemeinern und

1594
01:05:18,960 --> 01:05:20,339
vom Aufmerksamkeitsmechanismus in

1595
01:05:20,339 --> 01:05:21,900
Transformers zum Aufmerksamkeitsmechanismus der

1596
01:05:21,900 --> 01:05:24,180
Kognitionswissenschaft übergehen wollen,

1597
01:05:24,180 --> 01:05:28,020
denke ich, dass es sich wahrscheinlich um zwei verschiedene handelt, um

1598
01:05:28,020 --> 01:05:31,260
Ähnlichkeiten zu ziehen  Äh,

1599
01:05:31,260 --> 01:05:33,359
ich denke, die strukturelle Lernanalogie

1600
01:05:33,359 --> 01:05:36,660
und die Frage, wie wichtig eine Verbindung in

1601
01:05:36,660 --> 01:05:38,760
Bezug auf eine andere ist,

1602
01:05:38,760 --> 01:05:41,900
erfüllen ihre Aufgabe wahrscheinlich viel besser.

1603
01:05:42,000 --> 01:05:44,880
Coole graue Antwort, okay.

1604
01:05:44,880 --> 01:05:49,200
ml Don fragt in kontrafaktischen Zusammenhängen, was

1605
01:05:49,200 --> 01:05:51,240
der Unterschied zwischen versteckten Variablen

1606
01:05:51,240 --> 01:05:55,440
X und unbeobachteten Variablen U

1607
01:05:55,440 --> 01:05:59,180
ist

1608
01:05:59,540 --> 01:06:01,740
Ich denke, der Hauptgrund ist, dass man

1609
01:06:01,740 --> 01:06:03,599
die Verwendung nicht beobachten kann. Man

1610
01:06:03,599 --> 01:06:05,819
kann sie verwenden, weil man

1611
01:06:05,819 --> 01:06:09,000
sie berechnen und reparieren kann, aber man kann sie nicht.

1612
01:06:09,000 --> 01:06:10,559
Die Idee ist, dass man keine Kontrolle

1613
01:06:10,559 --> 01:06:13,380
über sie hat, also sollten sie die Verwendung verwenden

1614
01:06:13,380 --> 01:06:16,020
Als umgebungsspezifische Variablen werden sie als umgebungsspezifische Variablen angesehen,

1615
01:06:16,020 --> 01:06:18,540
die

1616
01:06:18,540 --> 01:06:21,240
Ihren Prozess beeinflussen, okay, denn

1617
01:06:21,240 --> 01:06:23,280
wenn Sie beispielsweise in der Zeit zurückgehen,

1618
01:06:23,280 --> 01:06:25,079
ist die Umgebung anders. Die Idee ist also,

1619
01:06:25,079 --> 01:06:26,520
ob Sie zum Beispiel

1620
01:06:26,520 --> 01:06:28,440
gerne zum vorherigen Beispiel von zurückgehen möchten

1621
01:06:28,440 --> 01:06:29,880


1622
01:06:29,880 --> 01:06:31,920
Das erwartete Einkommen einer Person mit

1623
01:06:31,920 --> 01:06:34,619
einer bestimmten Intelligenz von Bildung, äh,

1624
01:06:34,619 --> 01:06:37,440
Bildungsabschluss.

1625
01:06:37,440 --> 01:06:40,200
Die Idee ist, dass wenn ich sehen möchte, wie

1626
01:06:40,200 --> 01:06:43,559
viel ich heute lernen werde, mit einem mit

1627
01:06:43,559 --> 01:06:45,359
einem Master-Abschluss, den ich nicht weiß, etwas

1628
01:06:45,359 --> 01:06:47,339
anderes ist  Wie viel ich

1629
01:06:47,339 --> 01:06:48,359


1630
01:06:48,359 --> 01:06:50,819
vor 20 Jahren mit einem Master-Abschluss verdienen würde, ist

1631
01:06:50,819 --> 01:06:52,619
zum Beispiel hier in Italien anders

1632
01:06:52,619 --> 01:06:55,440
als in anderen Ländern und all die

1633
01:06:55,440 --> 01:06:57,000
Variablen, die nicht unter Ihrer

1634
01:06:57,000 --> 01:06:58,859
Kontrolle stehen, können Sie nicht mit Ihrem

1635
01:06:58,859 --> 01:07:00,359
Vision-Netzwerk modellieren,

1636
01:07:00,359 --> 01:07:03,480
aber sie sind da, okay, also Sie  Sie

1637
01:07:03,480 --> 01:07:05,220
können sie nicht ignorieren, wenn Sie

1638
01:07:05,220 --> 01:07:07,559
Schlussfolgerungen ziehen möchten, also ist er ja,

1639
01:07:07,559 --> 01:07:08,760
es ist im Grunde alles, was Sie

1640
01:07:08,760 --> 01:07:10,079
nicht kontrollieren können. Sie können

1641
01:07:10,079 --> 01:07:13,079
sie ableiten, damit Sie

1642
01:07:13,079 --> 01:07:14,819
eine kontrafaktische

1643
01:07:14,819 --> 01:07:16,740
Schlussfolgerung in die Vergangenheit ziehen und sagen können: Oh,

1644
01:07:16,740 --> 01:07:19,020
vor 20 Jahren hätte ich das verdient

1645
01:07:19,020 --> 01:07:20,640
Wenn

1646
01:07:20,640 --> 01:07:22,559
ich so intelligent wäre, dass dieser

1647
01:07:22,559 --> 01:07:24,599
Abschluss natürlich durchschnittlich ist,

1648
01:07:24,599 --> 01:07:27,059
und aber es ist nicht so, dass ich die

1649
01:07:27,059 --> 01:07:30,720
Regierungspolitik in Bezug auf Arbeitsplätze oder

1650
01:07:30,720 --> 01:07:32,819
ähnliches oder ähnliches ändern kann,

1651
01:07:32,819 --> 01:07:35,099
es ist ein tieferes kontrafaktisches

1652
01:07:35,099 --> 01:07:38,400
Ja, genau, also ja, das ist der Nutzen, den ich in Ordnung

1653
01:07:38,400 --> 01:07:40,200


1654
01:07:40,200 --> 01:07:42,480
habe  Sie haben verallgemeinerte

1655
01:07:42,480 --> 01:07:45,660
Koordinaten in der prädiktiven Codierung implementiert.

1656
01:07:45,660 --> 01:07:46,920
Nein, das habe

1657
01:07:46,920 --> 01:07:50,039
ich noch nie. Ich

1658
01:07:50,039 --> 01:07:52,680
habe es nie

1659
01:07:52,680 --> 01:07:55,260
gemacht  Es ist

1660
01:07:55,260 --> 01:07:57,599


1661
01:07:57,599 --> 01:08:00,299
sehr schwer, sie stabil zu machen. Ich

1662
01:08:00,299 --> 01:08:02,940
denke, das ist die Erkenntnis,

1663
01:08:02,940 --> 01:08:05,460
die ich aus Gesprächen mit Leuten gewonnen habe, die

1664
01:08:05,460 --> 01:08:08,359
sie implementiert haben,

1665
01:08:08,400 --> 01:08:11,039
aber ja, ja, mir sind tatsächlich einige

1666
01:08:11,039 --> 01:08:12,839
Papiere bekannt, die kürzlich

1667
01:08:12,839 --> 01:08:15,599
darüber erschienen sind und die an

1668
01:08:15,599 --> 01:08:18,000
einer britischen Last getestet wurden  Encoder-Stil, eigentlich

1669
01:08:18,000 --> 01:08:20,520
glaube ich, dass es immer noch

1670
01:08:20,520 --> 01:08:22,979
einen Artikel von Baron gibt, der

1671
01:08:22,979 --> 01:08:25,439
letzten Sommer herauskam, aber nein, ich habe sie selbst noch nie damit gespielt. Der

1672
01:08:25,439 --> 01:08:26,580


1673
01:08:26,580 --> 01:08:29,160
coole Strampler

1674
01:08:29,160 --> 01:08:32,040


1675
01:08:32,040 --> 01:08:35,160
reduziert das Ablenkungsproblem, wenn man mehr Ebenen in der Hierarchie

1676
01:08:35,160 --> 01:08:38,238
vorhersagt, indem man

1677
01:08:38,939 --> 01:08:41,698
mehr hinzufügt  Ebene

1678
01:08:41,698 --> 01:08:43,439
in diesem Sinne, weil das

1679
01:08:43,439 --> 01:08:45,779
Zerstörungsproblem durch Zyklen gegeben ist, also

1680
01:08:45,779 --> 01:08:47,399
stellen Sie im Grunde ein Bild bereit

1681
01:08:47,399 --> 01:08:49,920
und die Tatsache, dass Sie äh, also

1682
01:08:49,920 --> 01:08:53,279
Flecken haben, die aus dem Bild

1683
01:08:53,279 --> 01:08:55,799
in die Neuronen gehen und dann andere Kanten

1684
01:08:55,799 --> 01:08:57,500
zurückgehen,

1685
01:08:57,500 --> 01:08:59,939
schafft im Grunde die Tatsache, dass  Sie

1686
01:08:59,939 --> 01:09:03,560
haben den Fehler, dass

1687
01:09:03,560 --> 01:09:06,179
diese eingehenden Anpassungen sich grundsätzlich an die Pixel

1688
01:09:06,179 --> 01:09:08,339
des Bildes anpassen und einige Vorhersagefehler erzeugen,

1689
01:09:08,339 --> 01:09:09,719
sodass Sie einige Vorhersagefehler haben,

1690
01:09:09,719 --> 01:09:12,140
die sich innerhalb des Modells ausbreiten,

1691
01:09:12,140 --> 01:09:14,640
und das ist ja, und dieses Problem ist meiner Meinung nach

1692
01:09:14,640 --> 01:09:16,979
allgemein für Zyklen und  Es hat wahrscheinlich

1693
01:09:16,979 --> 01:09:21,439
nichts mit der Hierarchie im Allgemeinen mit

1694
01:09:23,060 --> 01:09:25,140
den Pixeln zu tun.

1695
01:09:25,140 --> 01:09:26,759
Wenn Sie keine eingehenden Kanten haben, haben Sie

1696
01:09:26,759 --> 01:09:27,660


1697
01:09:27,660 --> 01:09:30,540
kein Zerstörungsproblem mehr,

1698
01:09:30,540 --> 01:09:33,238
cool, und die Spezifikation des

1699
01:09:33,238 --> 01:09:35,939
azyklischen Netzwerks durch den Trace-

1700
01:09:35,939 --> 01:09:37,859
Operator

1701
01:09:37,859 --> 01:09:41,819
ist eine sehr interessante Technik, und

1702
01:09:41,819 --> 01:09:46,339
wann wurde sie eingeführt?  ins Spiel, ähm,

1703
01:09:46,560 --> 01:09:49,140
soweit ich weiß, glaube ich, dass er

1704
01:09:49,140 --> 01:09:52,380
die Arbeit herausgebracht hat, die ich 2018 zitiert habe

1705
01:09:52,380 --> 01:09:54,360


1706
01:09:54,360 --> 01:09:56,940


1707
01:09:56,940 --> 01:09:59,699


1708
01:09:59,699 --> 01:10:01,860
Ich meine, das ist der am häufigsten

1709
01:10:01,860 --> 01:10:04,140
zitierte Aufsatz, also würde ich sagen, dass sie auf

1710
01:10:04,140 --> 01:10:05,520
diese Idee gekommen sind.

1711
01:10:05,520 --> 01:10:07,980
Wow, das ist ganz schön, dass

1712
01:10:07,980 --> 01:10:09,480
man einen Gradientenabstieg machen und

1713
01:10:09,480 --> 01:10:11,400
die Struktur lernen kann. Ich denke,

1714
01:10:11,400 --> 01:10:14,219
das ist eine sehr wirkungsvolle Technik. Ja,

1715
01:10:14,219 --> 01:10:15,840
manchmal ist es so, wenn man hinschaut

1716
01:10:15,840 --> 01:10:17,640
Als verschiedene

1717
01:10:17,640 --> 01:10:19,440
Um-Merkmale der Bayes'schen Inferenz und der

1718
01:10:19,440 --> 01:10:23,159
Kausalinferenz verfügbar wurden,

1719
01:10:23,159 --> 01:10:25,620
war es wirklich bemerkenswert, warum

1720
01:10:25,620 --> 01:10:28,500
dies nicht im Rahmen eines Bayes'schen Kausalmodellierungsrahmens durchgeführt wurde,

1721
01:10:28,500 --> 01:10:30,719


1722
01:10:30,719 --> 01:10:32,760
weil dies erst seit etwa fünf bis

1723
01:10:32,760 --> 01:10:36,659
25 Jahren geschieht,

1724
01:10:36,659 --> 01:10:39,960
und das ist also sehr, sehr kurz  Außerdem ist

1725
01:10:39,960 --> 01:10:42,060
es relativ technisch, daher gibt es

1726
01:10:42,060 --> 01:10:43,920
relativ wenige Forschungsgruppen, die sich

1727
01:10:43,920 --> 01:10:46,920
damit befassen, und ähm, es ist einfach

1728
01:10:46,920 --> 01:10:49,860
wirklich cool, was es ermöglicht. Nein, ja, genau  Es gibt

1729
01:10:49,860 --> 01:10:51,960


1730
01:10:51,960 --> 01:10:54,179


1731
01:10:54,179 --> 01:10:56,040


1732
01:10:56,040 --> 01:10:59,100
Durchbrüche da draußen, die

1733
01:10:59,100 --> 01:11:01,020
noch entdeckt werden müssen und die mir wahrscheinlich gefallen,

1734
01:11:01,020 --> 01:11:03,000
weil sie zum Beispiel so

1735
01:11:03,000 --> 01:11:05,300
viel wie einen Durchbruch in diesem Artikel

1736
01:11:05,300 --> 01:11:07,800
gefunden haben,

1737
01:11:07,800 --> 01:11:09,960
als hätten sie einfach den richtigen

1738
01:11:09,960 --> 01:11:12,120
Prior für azyklische Strukturen herausgefunden. Okay, das

1739
01:11:12,120 --> 01:11:14,040
ist ein

1740
01:11:14,040 --> 01:11:17,100
Ja, ich meine, ich weiß es nicht  Genau, aber

1741
01:11:17,100 --> 01:11:19,080
es könnte eine Idee sein, die Sie an einem

1742
01:11:19,080 --> 01:11:21,120
Nachmittag haben. Ich weiß nicht,

1743
01:11:21,120 --> 01:11:23,040
wie die anderen darauf gekommen sind,

1744
01:11:23,040 --> 01:11:25,320
aber es könnte möglicherweise sein, dass sie

1745
01:11:25,320 --> 01:11:27,239
dort am Whiteboard sind und denken:

1746
01:11:27,239 --> 01:11:29,280
Oh, das eigentlich  Es funktioniert, das ist ein

1747
01:11:29,280 --> 01:11:32,159
riesiger Durchbruch, und ich habe einfach

1748
01:11:32,159 --> 01:11:33,960
das Vorhergehende definiert,

1749
01:11:33,960 --> 01:11:36,739
und auch viele dieser Durchbrüche werden

1750
01:11:36,739 --> 01:11:40,500
nicht einfach gestapelt

1751
01:11:40,500 --> 01:11:44,280


1752
01:11:44,280 --> 01:11:47,640


1753
01:11:47,640 --> 01:11:50,159
verallgemeinerte

1754
01:11:50,159 --> 01:11:52,140
Koordinaten oder verallgemeinerte Synchronität oder

1755
01:11:52,140 --> 01:11:55,020
beliebig große Diagramme oder

1756
01:11:55,020 --> 01:11:57,239
ähm Sensorfusion mit multimodalen Eingaben

1757
01:11:57,239 --> 01:12:00,679
und es ist so, als würden sich diese alle auf wirklich

1758
01:12:00,679 --> 01:12:03,659
zufriedenstellende und effektive Weise vermischen, so dass selbst

1759
01:12:03,659 --> 01:12:05,640
kleine Dinge, die wiederum jemandem

1760
01:12:05,640 --> 01:12:08,100
in einem Moment einfallen,

1761
01:12:08,100 --> 01:12:11,100
wirklich eine Wirkung haben können ähm

1762
01:12:11,100 --> 01:12:14,159
Okay, ml Dawn sagt vielen Dank, dass du

1763
01:12:14,159 --> 01:12:16,199
meine Fragen gestellt hast, und tausend Dank

1764
01:12:16,199 --> 01:12:18,060
an Tomaso für die inspirierende Präsentation,

1765
01:12:18,060 --> 01:12:21,360
so schön, oh, vielen Dank, und dann

1766
01:12:21,360 --> 01:12:23,280
fragt Bert,

1767
01:12:23,280 --> 01:12:25,560
wie würden sich Sprachmodelle, die

1768
01:12:25,560 --> 01:12:27,179
Predictive Coding verwenden, von denen unterscheiden, die

1769
01:12:27,179 --> 01:12:30,260
Transformers verwenden,

1770
01:12:31,679 --> 01:12:32,520
ähm,

1771
01:12:32,520 --> 01:12:35,340
okay, das denke ich eigentlich  Wenn ich

1772
01:12:35,340 --> 01:12:36,659
heute ein Sprachmodell

1773
01:12:36,659 --> 01:12:38,640
mit prädiktiver Codierung erstellen müsste, würde ich immer noch

1774
01:12:38,640 --> 01:12:40,020
die Transformers verwenden.

1775
01:12:40,020 --> 01:12:41,880
Die Idee ist also, dass Sie beispielsweise

1776
01:12:41,880 --> 01:12:42,780
ein

1777
01:12:42,780 --> 01:12:45,659
hierarchisches grafisches

1778
01:12:45,659 --> 01:12:48,440
Modell dieses oder dieser hierarchischen

1779
01:12:48,440 --> 01:12:50,460
Bayes'schen Netzwerke haben, die

1780
01:12:50,460 --> 01:12:53,100
ich in definiert habe  Das allererste

1781
01:12:53,100 --> 01:12:55,380
verschiebt einen Pfeil, um eine Funktion zu kodieren,

1782
01:12:55,380 --> 01:12:57,300
die die lineare Abbildung darstellt.

1783
01:12:57,300 --> 01:12:59,219
Okay, also war eine Stunde einfach die

1784
01:12:59,219 --> 01:13:01,080
Multiplikation eines der

1785
01:13:01,080 --> 01:13:03,060
in den latenten Variablen kodierten Vektoren

1786
01:13:03,060 --> 01:13:06,300
mit dieser Gewichtsmatrix, die man dann

1787
01:13:06,300 --> 01:13:08,580
nichtlinear machen kann und solche Dinge  Aber

1788
01:13:08,580 --> 01:13:09,960
das kann tatsächlich etwas viel

1789
01:13:09,960 --> 01:13:12,179
Komplexeres sein. Die im Pfeil enthaltene Funktion

1790
01:13:12,179 --> 01:13:14,880
kann eine Faltung sein, kann ein Aufmerksamkeitsmechanismus sein.

1791
01:13:14,880 --> 01:13:16,800


1792
01:13:16,800 --> 01:13:20,820
Also, wie ich es eigentlich machen würde, werde ich

1793
01:13:20,820 --> 01:13:23,880
immer noch verwenden. Ich meine, das ist eigentlich

1794
01:13:23,880 --> 01:13:26,460
die Art und Weise, wie wir es gemacht haben, äh  In der Oxford-

1795
01:13:26,460 --> 01:13:28,860
Gruppe im letzten Jahr hatten wir

1796
01:13:28,860 --> 01:13:30,900
genau die Struktur, bei der jeder Pfeil

1797
01:13:30,900 --> 01:13:33,420
jetzt ein Transformer ist, also ist einer der Aufmerksamkeitsmechanismus

1798
01:13:33,420 --> 01:13:35,159
und der nächste das

1799
01:13:35,159 --> 01:13:38,219
Feed-Forward-Netzwerk als Transformer,

1800
01:13:38,219 --> 01:13:40,020
und im Grunde ist der einzige Unterschied, den

1801
01:13:40,020 --> 01:13:41,640
es gibt, dieser  Variablen, die Sie

1802
01:13:41,640 --> 01:13:43,739
berechnen möchten, und Sie

1803
01:13:43,739 --> 01:13:45,239
machen diese Posterioren

1804
01:13:45,239 --> 01:13:47,400
mithilfe der VIA-Mean-Field-

1805
01:13:47,400 --> 01:13:49,560
Approximation unabhängig. Sie befolgen also im Grunde

1806
01:13:49,560 --> 01:13:51,659
alle Schritte, die es Ihnen ermöglichen, sich

1807
01:13:51,659 --> 01:13:53,520
der

1808
01:13:53,520 --> 01:13:56,520
Variationsfreiheit der kreativen Codierung anzunähern, aber auf die

1809
01:13:56,520 --> 01:13:58,199
Art und Weise, wie Sie es möchten  Die Berechnung von Vorhersagen

1810
01:13:58,199 --> 01:14:01,199
und die Art und Weise, wie Signale zurückgesendet werden,

1811
01:14:01,199 --> 01:14:04,739
erfolgt über Transformer,

1812
01:14:04,739 --> 01:14:07,560
daher werde ich im Allgemeinen weiterhin Transformer verwenden.

1813
01:14:07,560 --> 01:14:10,679
Ich meine, sie funktionieren so gut,

1814
01:14:10,679 --> 01:14:12,840
dass ich nicht arrogant sein

1815
01:14:12,840 --> 01:14:15,060
und sagen kann: „Oh nein, das werde ich tun.“  Besser über

1816
01:14:15,060 --> 01:14:17,640
eine rein prädiktive Codierungsmethode,

1817
01:14:17,640 --> 01:14:18,920
Strukturen, nähert sich

1818
01:14:18,920 --> 01:14:21,480
aber trotzdem Transformers an. Tut mir

1819
01:14:21,480 --> 01:14:22,500


1820
01:14:22,500 --> 01:14:24,420
leid, Sie sagten, Strukturlernen würde sich

1821
01:14:24,420 --> 01:14:27,540
dem Transformer-Ansatz annähern,

1822
01:14:27,540 --> 01:14:29,219
ja, dem Strukturlernen, das ich

1823
01:14:29,219 --> 01:14:32,640
zuvor erwähnt habe, als jemand

1824
01:14:32,640 --> 01:14:34,800
die Ähnlichkeiten zwischen kreativer Codierung

1825
01:14:34,800 --> 01:14:38,060
und dem Aufmerksamkeitsmechanismus fragte,

1826
01:14:38,280 --> 01:14:41,699
sehr ja, sehr  Interessant,

1827
01:14:41,699 --> 01:14:42,900
ähm,

1828
01:14:42,900 --> 01:14:45,719
eine Sache, die ich mich von Amazon frage: Ich

1829
01:14:45,719 --> 01:14:47,640
konnte das Konzept der Tiefe in

1830
01:14:47,640 --> 01:14:49,380
den von Ihnen erwähnten Predictive-Coding-Netzwerken nicht erkennen,

1831
01:14:49,380 --> 01:14:50,880
höchstwahrscheinlich habe ich es übersehen. Die

1832
01:14:50,880 --> 01:14:52,380
bereitgestellte Definition für Predictive-

1833
01:14:52,380 --> 01:14:56,480
Coding beinhaltete das Konzept der Tiefe.

1834
01:14:56,640 --> 01:14:59,460
Was meinten Sie mit Tiefe?

1835
01:14:59,460 --> 01:15:02,219
Nein, ja, das ist wahr  Es liegt daran, dass die

1836
01:15:02,219 --> 01:15:04,980
Standarddefinition, wie ich schon mehrfach sagte,

1837
01:15:04,980 --> 01:15:06,960
hierarchisch ist. Sie haben

1838
01:15:06,960 --> 01:15:08,400
Vorhersagen, die in eine Richtung gehen, einige

1839
01:15:08,400 --> 01:15:09,719
Vorhersagefehler, die in die entgegengesetzte Richtung gehen,

1840
01:15:09,719 --> 01:15:10,620


1841
01:15:10,620 --> 01:15:14,340
im Wesentlichen das, was wir in diesem

1842
01:15:14,340 --> 01:15:16,320
Artikel und auch im letzten Artikel in „Äh“ gemacht haben,

1843
01:15:16,320 --> 01:15:18,420
der heißt  Die Erkenntnis aus

1844
01:15:18,420 --> 01:15:19,920
beliebigen Graphtopologien, die wir in der

1845
01:15:19,920 --> 01:15:22,260
relativen Codierung haben, besteht darin, dass wir die

1846
01:15:22,260 --> 01:15:25,620
Tiefe als ein

1847
01:15:25,620 --> 01:15:28,380
unabhängiges

1848
01:15:28,380 --> 01:15:31,380
Paar aus latenter Variable, latenter

1849
01:15:31,380 --> 01:15:33,239
Variable und Pfeil betrachten können,

1850
01:15:33,239 --> 01:15:34,739
und Sie haben Vorhersagen, die in diese

1851
01:15:34,739 --> 01:15:36,300
Richtung gehen, und der Vorhersagepfeil, der

1852
01:15:36,300 --> 01:15:38,340
mit der anderen geht, aber dann Sie  Sie können

1853
01:15:38,340 --> 01:15:41,880
diese auf wie viele verschiedene Arten zusammenstellen, also im

1854
01:15:41,880 --> 01:15:45,239
Grunde muss diese

1855
01:15:45,239 --> 01:15:47,040
Zusammensetzung nicht

1856
01:15:47,040 --> 01:15:48,659
hierarchisch sein. Am Ende kann sie

1857
01:15:48,659 --> 01:15:50,820
Zyklen haben, sodass Sie

1858
01:15:50,820 --> 01:15:53,520
beispielsweise eine andere

1859
01:15:53,520 --> 01:15:55,440
latente Variable an die erste anschließen können  Eines und

1860
01:15:55,440 --> 01:15:57,540
dann die anderen verbinden und Sie können

1861
01:15:57,540 --> 01:15:59,340
eine Struktur haben, die so verflochten ist,

1862
01:15:59,340 --> 01:16:00,420
wie Sie möchten.

1863
01:16:00,420 --> 01:16:02,699
In der anderen Arbeit

1864
01:16:02,699 --> 01:16:04,500
trainieren wir beispielsweise

1865
01:16:04,500 --> 01:16:06,659
ein Netzwerk, das die Form einer

1866
01:16:06,659 --> 01:16:08,460
Gehirnstruktur hat, also haben wir viele davon

1867
01:16:08,460 --> 01:16:09,900
Gehirnregionen, die

1868
01:16:09,900 --> 01:16:12,239
im Inneren spärlich verbunden sind und teilweise untereinander

1869
01:16:12,239 --> 01:16:13,860
verbunden sind,

1870
01:16:13,860 --> 01:16:15,719
und

1871
01:16:15,719 --> 01:16:17,640
am Ende gibt es dort nichts Hierarchisches, aber Sie

1872
01:16:17,640 --> 01:16:18,960
können es trotzdem trainieren, indem Sie die

1873
01:16:18,960 --> 01:16:20,699
betriebliche freie Energie minimieren und

1874
01:16:20,699 --> 01:16:22,620
den gesamten

1875
01:16:22,620 --> 01:16:25,159
Vorhersagefehler des Netzwerks minimieren,

1876
01:16:25,159 --> 01:16:27,360
so wie Sie es hätten tun können

1877
01:16:27,360 --> 01:16:31,980
Für ein bestimmtes Motiv in einem verschränkten Diagramm

1878
01:16:31,980 --> 01:16:35,159
sehen Sie möglicherweise drei aufeinanderfolgende Schichten, und

1879
01:16:35,159 --> 01:16:37,560
wenn Sie sie allein betrachten, würden Sie

1880
01:16:37,560 --> 01:16:38,820
sagen: Oh, das ist ein dreistöckiges Gebäude,

1881
01:16:38,820 --> 01:16:41,940
das ein dreischichtiges Modell ist, das

1882
01:16:41,940 --> 01:16:43,980
adaptive drei sagt, aber wenn Sie dann ein

1883
01:16:43,980 --> 01:16:46,620
größeres Bild davon machen  ist nicht wie eine

1884
01:16:46,620 --> 01:16:50,280
explizite Spitze oder eine explizite Unterseite

1885
01:16:50,280 --> 01:16:52,140
dieses Netzwerks,

1886
01:16:52,140 --> 01:16:54,360
ja genau, und dies ist im Wesentlichen

1887
01:16:54,360 --> 01:16:55,980
durch die Tatsache gegeben, dass jede Operation

1888
01:16:55,980 --> 01:16:58,080
in prädiktiven koreanischen Netzwerken

1889
01:16:58,080 --> 01:16:59,460
streng lokal ist,

1890
01:16:59,460 --> 01:17:01,739
sodass im Grunde jede Nachricht

1891
01:17:01,739 --> 01:17:03,000
jede Vorhersage und jeden

1892
01:17:03,000 --> 01:17:05,280
Vorhersagefehler weitergibt  Dass Sie es senden, senden Sie es nur an

1893
01:17:05,280 --> 01:17:08,280
die sehr nahegelegenen Neuronen, okay, und ob

1894
01:17:08,280 --> 01:17:10,380
die globale Struktur tatsächlich

1895
01:17:10,380 --> 01:17:13,380
hierarchisch ist oder nicht, die einzelne

1896
01:17:13,380 --> 01:17:16,940
Nachrichtenweitergabe sieht nicht einmal, dass

1897
01:17:17,460 --> 01:17:19,620
ich denke, das ist eine Art

1898
01:17:19,620 --> 01:17:22,820
Hoffnung für das Erlernen neuer

1899
01:17:22,820 --> 01:17:27,739
Modellarchitekturen  Was von

1900
01:17:27,739 --> 01:17:33,300
oben nach unten entworfen wurde, ist sehr klein und

1901
01:17:33,300 --> 01:17:36,480
viele Modelle werden heute verwendet, wenn auch sehr

1902
01:17:36,480 --> 01:17:38,640
effektive Modelle. Ähm,

1903
01:17:38,640 --> 01:17:41,100
man könnte zwar fragen, effektiv pro

1904
01:17:41,100 --> 01:17:43,320
Recheneinheit oder nicht, das ist eine

1905
01:17:43,320 --> 01:17:45,300
Frage der zweiten Ebene, aber viele effektive

1906
01:17:45,300 --> 01:17:47,580
Modelle haben heute einige davon nicht

1907
01:17:47,580 --> 01:17:49,860
Eigenschaften prädiktiver Codierungsnetzwerke

1908
01:17:49,860 --> 01:17:52,739
wie ihre Fähigkeit,

1909
01:17:52,739 --> 01:17:55,520
nur lokale Berechnungen zu verwenden,

1910
01:17:55,520 --> 01:17:59,400
was biologischen Realismus

1911
01:17:59,400 --> 01:18:02,880
oder nur räumlich-zeitlichen Realismus verleiht, aber

1912
01:18:02,880 --> 01:18:06,060
auch viele Vorteile bieten kann,

1913
01:18:06,060 --> 01:18:08,159
z. B. in föderierten Rechen- oder verteilten

1914
01:18:08,159 --> 01:18:10,500
Recheneinstellungen.

1915
01:18:10,500 --> 01:18:12,780
Nein, genau. Ich stimme voll und ganz zu,

1916
01:18:12,780 --> 01:18:14,520
weil ich  Ich denke, die Idee im Allgemeinen ist

1917
01:18:14,520 --> 01:18:16,679
das, und ich weiß nicht, ob das

1918
01:18:16,679 --> 01:18:18,540
ein Vorteil sein wird, also denke ich, dass es

1919
01:18:18,540 --> 01:18:20,159
genau aus den von Ihnen genannten Gründen sehr vielversprechend ist,

1920
01:18:20,159 --> 01:18:20,880


1921
01:18:20,880 --> 01:18:22,920
und der Grund dafür ist, dass

1922
01:18:22,920 --> 01:18:25,380
Sie den heutigen Modellstring mit Rückausbreitung

1923
01:18:25,380 --> 01:18:28,860
im Grunde genommen zusammenfassen können  Sie als

1924
01:18:28,860 --> 01:18:32,040
eine Überwachungs-Rückausbreitung ist eine

1925
01:18:32,040 --> 01:18:34,080
Funktion, da Sie im Grunde genommen eine

1926
01:18:34,080 --> 01:18:36,120
Karte von der Eingabe zur Ausgabe haben und die

1927
01:18:36,120 --> 01:18:39,600
Rückausbreitung grundsätzlich Ihre

1928
01:18:39,600 --> 01:18:41,699
Informationen aus ihrem Rechendiagramm zurückspreizt,

1929
01:18:41,699 --> 01:18:44,340
sodass jedes

1930
01:18:44,340 --> 01:18:45,960
heute verwendete neuronale Netzwerkmodell

1931
01:18:45,960 --> 01:18:48,960
eine Funktion während der prädiktiven Codierung ist

1932
01:18:48,960 --> 01:18:51,179
und eine andere liberative Codierung wie die

1933
01:18:51,179 --> 01:18:53,820
alte Klasse von Funktionen, die Klasse von

1934
01:18:53,820 --> 01:18:56,040
Methoden, die die Verwendung lokaler

1935
01:18:56,040 --> 01:18:58,500
Berechnungen trainieren und tatsächlich durch die

1936
01:18:58,500 --> 01:19:01,620
Minimierung einer globalen Energiefunktion funktionieren.

1937
01:19:01,620 --> 01:19:03,840
Sie sind nicht auf Modellfunktionen

1938
01:19:03,840 --> 01:19:05,940
von der Eingabe bis zur Ausgabe beschränkt, sie modellieren tatsächlich

1939
01:19:05,940 --> 01:19:07,739
etwas, das das ist  Es ähnelt in gewisser Weise

1940
01:19:07,739 --> 01:19:10,080
physikalischen Systemen, also haben Sie ein physikalisches

1941
01:19:10,080 --> 01:19:13,500
System, dem Sie einige Werte an Ihre

1942
01:19:13,500 --> 01:19:15,360
Eingaben zuweisen, und Sie lassen das

1943
01:19:15,360 --> 01:19:17,280
System konvergieren und dann lesen Sie einen

1944
01:19:17,280 --> 01:19:19,980
anderen Wert von Neuronen oder Variablen,

1945
01:19:19,980 --> 01:19:21,960
die ausgegeben werden sollen, dieses

1946
01:19:21,960 --> 01:19:24,120
physikalische System jedoch nicht  Es muss keine Fit-

1947
01:19:24,120 --> 01:19:25,920
Forward-Map sein, es muss keine

1948
01:19:25,920 --> 01:19:28,260
Funktion sein, die einen Eingaberaum und

1949
01:19:28,260 --> 01:19:30,659
einen Ausgaberaum hat, und das war's.

1950
01:19:30,659 --> 01:19:32,580
Die Klasse der Modelle, die Sie

1951
01:19:32,580 --> 01:19:34,800
lernen können, ist also im Grunde genommen

1952
01:19:34,800 --> 01:19:37,560
Feed-Forward-Modelle  und Funktionen

1953
01:19:37,560 --> 01:19:39,600
und dann eine viel größere Klasse, nämlich

1954
01:19:39,600 --> 01:19:41,880
die der physikalischen Systeme. Ob es

1955
01:19:41,880 --> 01:19:43,860
hier draußen etwas Interessantes gibt,

1956
01:19:43,860 --> 01:19:45,659
weiß ich noch nicht, weil die Funktionen

1957
01:19:45,659 --> 01:19:47,460
extrem gut funktionieren. Wir sehen

1958
01:19:47,460 --> 01:19:50,040
diese Tage mit Backpropagation,

1959
01:19:50,040 --> 01:19:52,199
sie funktionieren wahnsinnig gut, aber

1960
01:19:52,199 --> 01:19:53,460
ja  Ich weiß nicht, ob

1961
01:19:53,460 --> 01:19:56,040
der große Teil etwas Interessantes enthält, aber der große

1962
01:19:56,040 --> 01:19:58,380
Teil ist ziemlich groß. Okay, es

1963
01:19:58,380 --> 01:20:00,480
gibt viele Modelle, bei denen man die

1964
01:20:00,480 --> 01:20:02,940
Ausbreitung nicht zurückbringen kann, und man

1965
01:20:02,940 --> 01:20:04,679
kann sie mit kreativer Codierung

1966
01:20:04,679 --> 01:20:06,659
oder einer Badezimmerausbreitung oder anderen

1967
01:20:06,659 --> 01:20:07,860
Methoden trainieren

1968
01:20:07,860 --> 01:20:10,440
Das ist super interessant, sicherlich

1969
01:20:10,440 --> 01:20:12,719


1970
01:20:12,719 --> 01:20:15,900
lösen biologische Systeme, physikalische Systeme alle möglichen interessanten Probleme,

1971
01:20:15,900 --> 01:20:17,100
ähm,

1972
01:20:17,100 --> 01:20:19,380
aber es gibt immer noch kein kostenloses Mittagessen

1973
01:20:19,380 --> 01:20:21,540
bei Ameisenarten, die in dieser Umgebung wirklich gut gedeihen,

1974
01:20:21,540 --> 01:20:23,100
vielleicht nicht sehr gut

1975
01:20:23,100 --> 01:20:25,679
in einer anderen Umgebung und so ähm da

1976
01:20:25,679 --> 01:20:28,260
draußen im Hinterland

1977
01:20:28,260 --> 01:20:31,820
Möglicherweise gibt es einige wirklich einzigartige

1978
01:20:31,820 --> 01:20:35,880
Spezialalgorithmen, die sich nicht gut dadurch beschreiben lassen, dass sie

1979
01:20:35,880 --> 01:20:38,460
eine Funktion sind, die

1980
01:20:38,460 --> 01:20:42,060
aber dennoch eine prozedurale

1981
01:20:42,060 --> 01:20:46,679
Möglichkeit bieten, Heuristiken zu implementieren,

1982
01:20:46,679 --> 01:20:48,840
die äußerst effektiv sein könnten.

1983
01:20:48,840 --> 01:20:51,120


1984
01:20:51,120 --> 01:20:53,880
Nein, ja, genau, und äh ja, alles,

1985
01:20:53,880 --> 01:20:55,679
worauf ich mich am meisten

1986
01:20:55,679 --> 01:20:58,260
konzentriert habe  Während meiner Doktorarbeit habe ich zum

1987
01:20:58,260 --> 01:20:59,100
Beispiel

1988
01:20:59,100 --> 01:21:01,380
eine Anwendung gefunden, die so aussieht,

1989
01:21:01,380 --> 01:21:04,199
als wäre sie hier draußen und nicht innerhalb der

1990
01:21:04,199 --> 01:21:06,739
Funktionen. Nun, wohin

1991
01:21:07,199 --> 01:21:08,820


1992
01:21:08,820 --> 01:21:12,120
geht diese Arbeit von hier aus?

1993
01:21:12,120 --> 01:21:14,520
Welche Richtungen interessieren Sie

1994
01:21:14,520 --> 01:21:17,340
und wie sehen Sie die Entwicklung der Menschen im aktiven

1995
01:21:17,340 --> 01:21:19,679
Inferenz-Ökosystem?  Wenn ich mich mit

1996
01:21:19,679 --> 01:21:22,460
dieser Art von Arbeit beschäftige,

1997
01:21:22,500 --> 01:21:24,840
denke ich, dass die wahrscheinlich

1998
01:21:24,840 --> 01:21:27,780
vielversprechendste Richtung, die

1999
01:21:27,780 --> 01:21:30,060
ich vielleicht

2000
01:21:30,060 --> 01:21:33,060
ein wenig erforschen möchte, darin besteht, wie gesagt

2001
01:21:33,060 --> 01:21:34,980
wirklich hinter statischen Modellen zu stehen,

2002
01:21:34,980 --> 01:21:37,380
also hinter allem, was ich gezeigt habe  Ich habe

2003
01:21:37,380 --> 01:21:40,260
bisher gezeigt, dass es sich um statische Daten handelt,

2004
01:21:40,260 --> 01:21:42,840
sodass sich die Daten im Laufe der Zeit nicht ändern.

2005
01:21:42,840 --> 01:21:45,780
In der Definition des

2006
01:21:45,780 --> 01:21:48,000
kreativen Codierens, wie ich es hier vorgestellt habe, gibt es keine Zeit.

2007
01:21:48,000 --> 01:21:49,080


2008
01:21:49,080 --> 01:21:50,940
Sie können jedoch zum Beispiel das

2009
01:21:50,940 --> 01:21:53,280
kreative Codieren verallgemeinern, um zu funktionieren  mit zeitlichen

2010
01:21:53,280 --> 01:21:55,800
Daten unter Verwendung verallgemeinerter Koordinaten, wie

2011
01:21:55,800 --> 01:21:58,800
Sie zuvor erwähnt haben, indem Sie

2012
01:21:58,800 --> 01:22:01,380
sie als allgemeines

2013
01:22:01,380 --> 01:22:04,140
generatives Kalman-Filter-Modell darstellen,

2014
01:22:04,140 --> 01:22:08,040
und hier

2015
01:22:08,040 --> 01:22:09,900
könnte beispielsweise die kausale Inferenzrichtung sehr

2016
01:22:09,900 --> 01:22:12,600
nützlich sein, denn ja, dieses Modell, äh, an

2017
01:22:12,600 --> 01:22:14,400
diesem Punkt können Sie das vielleicht  in der Lage sein, eine

2018
01:22:14,400 --> 01:22:17,880
größere Kausalität und ähm und

2019
01:22:17,880 --> 01:22:21,780
komplexere und nützlichere ähm

2020
01:22:21,780 --> 01:22:24,780
dynamische Ursache von Modellen zu modellieren, im Grunde,

2021
01:22:24,780 --> 01:22:26,940
weil im Allgemeinen die Do-Kalküle

2022
01:22:26,940 --> 01:22:28,560
und der interventionelle und

2023
01:22:28,560 --> 01:22:32,760
kontrafaktische äh-Zweig der Wissenschaft

2024
01:22:32,760 --> 01:22:36,000
hauptsächlich auf kleinen Modellen entwickelt werden,

2025
01:22:36,000 --> 01:22:38,159
also ist es

2026
01:22:38,159 --> 01:22:40,739
so, wie Sie es nicht tun.  Im Allgemeinen führen wir keine Interventionen an

2027
01:22:40,739 --> 01:22:43,560
riesigen Modellen durch. Wenn Sie sich also

2028
01:22:43,560 --> 01:22:45,800
medizinische Daten ansehen, nutzen sie

2029
01:22:45,800 --> 01:22:50,159
relativ kleine Vision-Netzwerke, und

2030
01:22:50,159 --> 01:22:51,179
wenn Sie natürlich ein

2031
01:22:51,179 --> 01:22:54,900
dynamisches Kausalmodell haben möchten, das eine

2032
01:22:54,900 --> 01:22:56,340
bestimmte Umgebung oder eine bestimmte

2033
01:22:56,340 --> 01:22:58,620
Realität modelliert, haben Sie ein  Viele Neuronen in

2034
01:22:58,620 --> 01:23:00,780
dir haben viele latente Variablen, die sich

2035
01:23:00,780 --> 01:23:02,580
im Laufe der Zeit verändern, und ein Eingriff an

2036
01:23:02,580 --> 01:23:05,219
einem anderen Ort zu einem bestimmten Zeitpunkt erzeugt einen

2037
01:23:05,219 --> 01:23:07,560
Effekt in einem anderen Zeitschritt, also vielleicht

2038
01:23:07,560 --> 01:23:09,239
im nächsten Zeitschritt in 10 verschiedenen

2039
01:23:09,239 --> 01:23:11,699
Zeitschritten später, und ich denke, das wäre der Fall  Es

2040
01:23:11,699 --> 01:23:14,100
wäre sehr interessant, eine

2041
01:23:14,100 --> 01:23:16,380
biologisch plausible Art der Informationsweitergabe zu entwickeln,

2042
01:23:16,380 --> 01:23:17,699


2043
01:23:17,699 --> 01:23:20,040
die auch in der Lage ist, die Kausalität von Grandeur zu modellieren.

2044
01:23:20,040 --> 01:23:22,860


2045
01:23:22,860 --> 01:23:24,659
Hmm,

2046
01:23:24,659 --> 01:23:29,659
wo sehen Sie die Aktion in diesen Modellen,

2047
01:23:30,840 --> 01:23:33,840
wo

2048
01:23:33,840 --> 01:23:36,480


2049
01:23:36,480 --> 01:23:38,760
sehe ich die Aktion?  Modelle,

2050
01:23:38,760 --> 01:23:41,460
vielleicht auf die gleiche Weise wie ich, wie Sie in

2051
01:23:41,460 --> 01:23:43,080
anderen Modellen sehen, denn

2052
01:23:43,080 --> 01:23:44,940
kreatives Codieren ist im Grunde ein Wahrnehmungsmodell,

2053
01:23:44,940 --> 01:23:46,260


2054
01:23:46,260 --> 01:23:49,260
also ist eine Aktion, dass Sie erkennen können, dass es eine

2055
01:23:49,260 --> 01:23:52,739
Konsequenz dessen gibt, was Sie erleben,

2056
01:23:52,739 --> 01:23:55,159
indem Sie also die Art und Weise ändern, wie Sie

2057
01:23:55,159 --> 01:23:57,840
etwas erleben, dann Sie selbst

2058
01:23:57,840 --> 01:24:00,060
berechnen kann, vielleicht können Sie

2059
01:24:00,060 --> 01:24:01,800
jetzt, da Sie über mehr Informationen verfügen, einfach eine intelligentere Aktion ausführen,

2060
01:24:01,800 --> 01:24:03,000


2061
01:24:03,000 --> 01:24:04,560


2062
01:24:04,560 --> 01:24:06,960
aber ja, ich glaube nicht, dass Aktion sehr

2063
01:24:06,960 --> 01:24:10,199
einfach ist, wie ja, ich sehe keine expliziten

2064
01:24:10,199 --> 01:24:12,540
Konsequenzen von Aktionen, außer der Tatsache,

2065
01:24:12,540 --> 01:24:14,040
dass dies Ihnen im Grunde ermöglichen kann

2066
01:24:14,040 --> 01:24:15,960
Vielleicht

2067
01:24:15,960 --> 01:24:18,780
ziehen Sie einfach bessere Schlussfolgerungen, um

2068
01:24:18,780 --> 01:24:21,719
in Zukunft Aktionen durchzuführen.

2069
01:24:21,719 --> 01:24:23,940
Ich füge dem noch ein paar Möglichkeiten hinzu, wie die

2070
01:24:23,940 --> 01:24:25,920
Leute über prädiktive

2071
01:24:25,920 --> 01:24:29,340
Codierung und Aktionen gesprochen haben. Zunächst einmal ist interne

2072
01:24:29,340 --> 01:24:33,960
Aktion oder verdeckte Aktion Aufmerksamkeit, damit

2073
01:24:33,960 --> 01:24:36,120
wir über Wahrnehmung nachdenken können  Als eine

2074
01:24:36,120 --> 01:24:37,980
interne Aktion, die ein Ansatz, ein

2075
01:24:37,980 --> 01:24:40,560
anderer Ansatz, ziemlich mikro ist, handelt es sich um die

2076
01:24:40,560 --> 01:24:42,840
Ausgaben eines bestimmten Knotens. Wir können

2077
01:24:42,840 --> 01:24:45,780
diesen Knoten als ein bestimmtes

2078
01:24:45,780 --> 01:24:48,780
Ding mit seinen eigenen sensorischen, kognitiven und

2079
01:24:48,780 --> 01:24:52,080
Aktionszuständen verstehen und in diesem Sinne also

2080
01:24:52,080 --> 01:24:54,960
die Ausgabe eines Knotens und schließlich

2081
01:24:54,960 --> 01:24:57,179
welche  Im Livestream 43 haben wir uns ein wenig mit

2082
01:24:57,179 --> 01:24:59,940
dem theoretischen Überblick über

2083
01:24:59,940 --> 01:25:02,100
Predictive Coding beschäftigt, den wir gerade

2084
01:25:02,100 --> 01:25:03,840
durchlesen, und es ging nur um

2085
01:25:03,840 --> 01:25:05,460
Wahrnehmung, alles um Wahrnehmung, und dann

2086
01:25:05,460 --> 01:25:08,040
war es wie in Abschnitt 5.3:

2087
01:25:08,040 --> 01:25:11,719
Wenn man Erwartungen an die Aktion hat,

2088
01:25:11,719 --> 01:25:15,900
dann ist Aktion gerecht  Eine weitere Variable in

2089
01:25:15,900 --> 01:25:18,120
dieser Architektur, die wirklich

2090
01:25:18,120 --> 01:25:20,040
mit der inaktiven Inferenz übereinstimmt.

2091
01:25:20,040 --> 01:25:21,659
Anstatt eine Belohnungs- oder

2092
01:25:21,659 --> 01:25:24,000
Nutzenfunktion zu haben, die wir maximieren,

2093
01:25:24,000 --> 01:25:26,699
wählen wir eine Aktion basierend darauf aus, dass sie die

2094
01:25:26,699 --> 01:25:28,800
wahrscheinlichste Vorgehensweise ist, den Weg der

2095
01:25:28,800 --> 01:25:30,900
geringsten Aktion, das ist die Bayes'sche Mechanik,

2096
01:25:30,900 --> 01:25:33,300
und das ist eigentlich sehr  Es ist natürlich,

2097
01:25:33,300 --> 01:25:36,420
eine Aktionsvariable einzubringen und

2098
01:25:36,420 --> 01:25:40,800
sie im Wesentlichen so zu nutzen, als wäre sie eine

2099
01:25:40,800 --> 01:25:43,260
Vorhersage über etwas anderes

2100
01:25:43,260 --> 01:25:45,540
auf der Welt, da

2101
01:25:45,540 --> 01:25:48,480
wir auch eine Aktion erwarten

2102
01:25:48,480 --> 01:25:50,820


2103
01:25:50,820 --> 01:25:52,860


2104
01:25:52,860 --> 01:25:55,260
Ähm, und ich denke immer noch, dass es

2105
01:25:55,260 --> 01:25:57,239
zum Beispiel so war, als gäbe es nicht

2106
01:25:57,239 --> 01:26:01,139
so viele Artikel, die diese Methode anwenden. Ich

2107
01:26:01,139 --> 01:26:03,239
glaube, es gibt ein paar von äh, von

2108
01:26:03,239 --> 01:26:05,100
Alexander oder Robria macht etwas

2109
01:26:05,100 --> 01:26:08,580
Ähnliches, aber in der Praxis außerhalb

2110
01:26:08,580 --> 01:26:10,920
der rein aktiven Inferenz, wie der Anwendung von

2111
01:26:10,920 --> 01:26:13,260
prädiktiver Codierung  und Maßnahmen zur

2112
01:26:13,260 --> 01:26:15,980
Lösung praktischer Probleme wurden noch nicht

2113
01:26:15,980 --> 01:26:19,280
viel erforscht.

2114
01:26:19,679 --> 01:26:23,400
Na ja, vielen Dank für diese hervorragende

2115
01:26:23,400 --> 01:26:25,199
Präsentation und Diskussion. Gibt es noch

2116
01:26:25,199 --> 01:26:27,659
etwas, das Sie sagen

2117
01:26:27,659 --> 01:26:30,300
oder die Leute darauf hinweisen möchten?

2118
01:26:30,300 --> 01:26:33,360
Nein, nur ein großes Dankeschön für die

2119
01:26:33,360 --> 01:26:34,620
Einladung  Und ähm,

2120
01:26:34,620 --> 01:26:36,120
es hat wirklich Spaß gemacht und ich hoffe, dass ich

2121
01:26:36,120 --> 01:26:38,460
irgendwann für ein paar Future Works zurückkomme.

2122
01:26:38,460 --> 01:26:40,199


2123
01:26:40,199 --> 01:26:41,580
Cool,

2124
01:26:41,580 --> 01:26:45,000
jederzeit und jederzeit. Danke Thomas, also

2125
01:26:45,000 --> 01:26:49,820
danke Daniel, bis bald. Auf Wiedersehen


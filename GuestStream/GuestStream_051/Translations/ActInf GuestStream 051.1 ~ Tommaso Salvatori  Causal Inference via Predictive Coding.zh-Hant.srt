1
00:00:19,020 --> 00:00:21,119
您好，歡迎來到

2
00:00:21,119 --> 00:00:23,400


3
00:00:23,400 --> 00:00:28,140
2023 年 7 月 28 日的主動推理嘉賓直播，編號為 51.1，

4
00:00:28,140 --> 00:00:31,439
我們與 Tomaso Salvatore 一起在這裡，我們

5
00:00:31,439 --> 00:00:33,840
將通過預測編碼進行關於

6
00:00:33,840 --> 00:00:37,020
最近工作的因果推理的演示和討論，

7
00:00:37,020 --> 00:00:39,660


8
00:00:39,660 --> 00:00:42,480
非常感謝您的

9
00:00:42,480 --> 00:00:44,340
觀看 直播 請隨意

10
00:00:44,340 --> 00:00:47,520
在實時聊天中寫下問題，然後向

11
00:00:47,520 --> 00:00:50,399
您提問，謝謝，

12
00:00:50,399 --> 00:00:52,980
非常感謝丹尼爾邀請

13
00:00:52,980 --> 00:00:56,039
我，嗯，一直是該頻道的忠實粉絲，

14
00:00:56,039 --> 00:00:57,719
我看了很多

15
00:00:57,719 --> 00:00:58,920
視頻，

16
00:00:58,920 --> 00:01:01,379
我很喜歡 很高興來到這裡，呃，

17
00:01:01,379 --> 00:01:04,260
成為這次發言的人，

18
00:01:04,260 --> 00:01:06,600
所以我要談談

19
00:01:06,600 --> 00:01:08,700
我最近發布的這些預印本，這是

20
00:01:08,700 --> 00:01:11,159
過去幾個月的工作

21
00:01:11,159 --> 00:01:12,119


22
00:01:12,119 --> 00:01:15,900
，它是與

23
00:01:15,900 --> 00:01:18,659
with Lookup 的合作 在Ketty，我在makarak

24
00:01:18,659 --> 00:01:21,600
barami Legend Thomas lukasiavich，

25
00:01:21,600 --> 00:01:24,000
它基本上

26
00:01:24,000 --> 00:01:26,299
是我在牛津大學工作的詩句公司

27
00:01:26,299 --> 00:01:31,680
和uhuvian之間的聯合作品，

28
00:01:31,680 --> 00:01:34,200
所以

29
00:01:34,200 --> 00:01:36,299
在這次演講中，

30
00:01:36,299 --> 00:01:38,220
我

31
00:01:38,220 --> 00:01:40,979
將這基本上是我將

32
00:01:40,979 --> 00:01:43,140
開始談論的演講的大綱 什麼

33
00:01:43,140 --> 00:01:44,520
是預測編碼

34
00:01:44,520 --> 00:01:47,659
以及它的給定交互是什麼是

35
00:01:47,659 --> 00:01:51,299
一個簡短的歷史介紹為什麼你

36
00:01:51,299 --> 00:01:54,060
認為呃對於研究

37
00:01:54,060 --> 00:01:56,159
創造性編碼很重要，甚至

38
00:01:56,159 --> 00:01:58,619
從機器學習的角度來看，

39
00:01:58,619 --> 00:02:00,720
然後我將

40
00:02:00,720 --> 00:02:04,560
簡要介紹什麼是因果推理，

41
00:02:04,560 --> 00:02:07,200
以及 一旦我們掌握了所有這些

42
00:02:07,200 --> 00:02:08,880
信息，我將

43
00:02:08,880 --> 00:02:12,540
討論為什麼我寫這篇論文，

44
00:02:12,540 --> 00:02:14,520
基本上是什麼研究問題

45
00:02:14,520 --> 00:02:16,560
激發了我和其他

46
00:02:16,560 --> 00:02:18,300
合作者的靈感，

47
00:02:18,300 --> 00:02:21,660
並提出主要結果，

48
00:02:21,660 --> 00:02:24,980
即如何進行

49
00:02:24,980 --> 00:02:27,480
推理、干預和

50
00:02:27,480 --> 00:02:29,340
反事實推理

51
00:02:29,340 --> 00:02:33,319
以及 如何

52
00:02:33,319 --> 00:02:35,879
使用預測編碼從給定的數據集中學習因果結構

53
00:02:35,879 --> 00:02:37,920
，然後我當然會

54
00:02:37,920 --> 00:02:39,959
以一些

55
00:02:39,959 --> 00:02:43,500
小總結和一些討論來總結

56
00:02:43,500 --> 00:02:45,840
為什麼我相信這項工作實際上會

57
00:02:45,840 --> 00:02:49,940
產生影響，以及一些未來的方向，

58
00:02:50,700 --> 00:02:53,400
那麼什麼是創意編碼

59
00:02:53,400 --> 00:02:55,680
創意編碼 一般來說，以

60
00:02:55,680 --> 00:02:58,440
神經科學啟發的學習

61
00:02:58,440 --> 00:03:01,140
方法而聞名，那麼關於大腦中的信息處理如何工作的理論，

62
00:03:01,140 --> 00:03:04,560


63
00:03:04,560 --> 00:03:05,819
以及

64
00:03:05,819 --> 00:03:08,400
非常正式地說，

65
00:03:08,400 --> 00:03:10,560
創造性編碼的層可以被描述為

66
00:03:10,560 --> 00:03:12,659
基本上具有

67
00:03:12,659 --> 00:03:16,319
大腦中神經元的分層結構，

68
00:03:16,319 --> 00:03:19,080
並且你 大腦中有兩個不同的

69
00:03:19,080 --> 00:03:20,700
神經元家族，

70
00:03:20,700 --> 00:03:23,280
第一個家族

71
00:03:23,280 --> 00:03:24,480
負責發送預測

72
00:03:24,480 --> 00:03:27,659
信息，因此層次結構中特定級別的神經元

73
00:03:27,659 --> 00:03:29,959
發送信息

74
00:03:29,959 --> 00:03:33,959
並預測下一級的活動，

75
00:03:33,959 --> 00:03:35,940


76
00:03:35,940 --> 00:03:38,340
第二個家族負責發送預測信息。 神經元是

77
00:03:38,340 --> 00:03:41,099
錯誤神經元和箭頭神經元的神經元，

78
00:03:41,099 --> 00:03:43,019
它們向層次結構發送預測錯誤信息，

79
00:03:43,019 --> 00:03:46,319
因此一個級別預測

80
00:03:46,319 --> 00:03:49,200


81
00:03:49,200 --> 00:03:51,659
該活動下方級別的活動有一些這種預測，

82
00:03:51,659 --> 00:03:54,239
因為某些不匹配實際上會

83
00:03:54,239 --> 00:03:56,220
在下面的級別中發生，

84
00:03:56,220 --> 00:03:57,840
並且有關信息 預測

85
00:03:57,840 --> 00:04:02,400
誤差被向上發送箭頭鍵，

86
00:04:02,400 --> 00:04:04,860
但是預測編碼

87
00:04:04,860 --> 00:04:07,220
實際上並沒有作為

88
00:04:07,220 --> 00:04:10,799
神經科學作為神經科學的理論而被燒毀，

89
00:04:10,799 --> 00:04:11,939


90
00:04:11,939 --> 00:04:13,860
但它實際上最初是在 50 年代作為

91
00:04:13,860 --> 00:04:16,139
信號處理和

92
00:04:16,139 --> 00:04:19,380
壓縮的方法開發的，所以

93
00:04:19,380 --> 00:04:21,899
Oliver Elias 實際上是香農的香農的

94
00:04:21,899 --> 00:04:25,020
當代人，

95
00:04:25,020 --> 00:04:26,160


96
00:04:26,160 --> 00:04:27,960
他們意識到，一旦我們有了一個

97
00:04:27,960 --> 00:04:30,900
預測器，一個

98
00:04:30,900 --> 00:04:33,600
可以很好地預測數據的模型，

99
00:04:33,600 --> 00:04:36,000
發送有關這些預測中的錯誤的消息

100
00:04:36,000 --> 00:04:37,919
實際上

101
00:04:37,919 --> 00:04:41,100
比每次發送整個消息要便宜得多。

102
00:04:41,100 --> 00:04:42,720
時間，

103
00:04:42,720 --> 00:04:45,240
這就是漂亮的編碼是如何誕生的，

104
00:04:45,240 --> 00:04:47,639


105
00:04:47,639 --> 00:04:49,500
作為信息論中的信號處理和壓縮

106
00:04:49,500 --> 00:04:52,020
機制，早在

107
00:04:52,020 --> 00:04:53,639
50 年代，

108
00:04:53,639 --> 00:04:57,120
實際上是在 80 年代，呃，

109
00:04:57,120 --> 00:04:59,400


110
00:04:59,400 --> 00:05:01,800


111
00:05:01,800 --> 00:05:03,540
神經科學中使用了完全相同的模型，

112
00:05:03,540 --> 00:05:07,500
呃 因此，通過 Mumford 或

113
00:05:07,500 --> 00:05:10,440
其他作品的工作，例如解釋

114
00:05:10,440 --> 00:05:12,960
如何處理信息的評級，以便

115
00:05:12,960 --> 00:05:14,520
我們從

116
00:05:14,520 --> 00:05:17,160
外部世界獲得預測信號，我們需要壓縮

117
00:05:17,160 --> 00:05:20,280
這種表示，呃，並

118
00:05:20,280 --> 00:05:22,740
在我們的神經元和方法中擁有這種內部表示 與

119
00:05:22,740 --> 00:05:25,199


120
00:05:25,199 --> 00:05:27,720


121
00:05:27,720 --> 00:05:30,419
Elias 和 Oliver 在

122
00:05:30,419 --> 00:05:32,900
50 年代開發的方法非常相似，如果不是等同的話，

123
00:05:32,940 --> 00:05:35,520
也許是 1999 年發生的最大的範式轉變，這要

124
00:05:35,520 --> 00:05:38,100


125
00:05:38,100 --> 00:05:41,400
歸功於巴拉德周圍的工作，

126
00:05:41,400 --> 00:05:44,880
他們在其中引入了

127
00:05:44,880 --> 00:05:46,199
這個概念，我 之前提到過

128
00:05:46,199 --> 00:05:48,060


129
00:05:48,060 --> 00:05:51,360
大腦中的層次結構，其中預測信息是

130
00:05:51,360 --> 00:05:54,240
自上而下的，錯誤信息是

131
00:05:54,240 --> 00:05:55,560
自下而上的，

132
00:05:55,560 --> 00:05:57,660
他們所做的事情是

133
00:05:57,660 --> 00:05:59,759
以前沒有做過的，

134
00:05:59,759 --> 00:06:02,820
他們不僅在法國解釋和發展了這個理論，而且還解釋了這一理論。

135
00:06:02,820 --> 00:06:05,759


136
00:06:05,759 --> 00:06:07,979
關於學習在大腦中如何運作，

137
00:06:07,979 --> 00:06:10,139
所以這也是我們的突觸如何更新的理論，

138
00:06:10,139 --> 00:06:13,139


139
00:06:13,139 --> 00:06:16,080
我

140
00:06:16,080 --> 00:06:17,940
要在這個簡短的

141
00:06:17,940 --> 00:06:21,900
歷史介紹中談論的最後一個重大突破是在 2003 年，但是

142
00:06:21,900 --> 00:06:25,380
呃，然後他一直在

143
00:06:25,380 --> 00:06:28,380
幾年後，呃，多虧了卡菲斯頓，

144
00:06:28,380 --> 00:06:32,100
他基本上採用了羅賓巴拉德的理論，

145
00:06:32,100 --> 00:06:35,039
並發展了

146
00:06:35,039 --> 00:06:38,400
它，並將其推廣到

147
00:06:38,400 --> 00:06:40,919
生成模型理論，所以

148
00:06:40,919 --> 00:06:42,720
基本上

149
00:06:42,720 --> 00:06:45,479
卡菲斯頓所做的主要主張是，創造性編碼是

150
00:06:45,479 --> 00:06:48,780
一種 一種

151
00:06:48,780 --> 00:06:50,340
特定類型的生成模型的證據最大化方案，

152
00:06:50,340 --> 00:06:52,979
我稍後也會介紹，

153
00:06:52,979 --> 00:06:55,139


154
00:06:55,139 --> 00:07:00,300
因此在我描述的前兩種創意角中做一個簡短的總結，

155
00:07:00,300 --> 00:07:01,560


156
00:07:01,560 --> 00:07:03,900


157
00:07:03,900 --> 00:07:05,340
即信號處理和

158
00:07:05,340 --> 00:07:07,020
壓縮以及

159
00:07:07,020 --> 00:07:09,180
視網膜和大腦中的信息處理

160
00:07:09,180 --> 00:07:11,160
一般來說它們是推理

161
00:07:11,160 --> 00:07:12,300
方法，

162
00:07:12,300 --> 00:07:14,819
最大的

163
00:07:14,819 --> 00:07:17,819
變化是我們

164
00:07:17,819 --> 00:07:21,120
在 1999 年經歷的最大的革命，所以我們可以說在 21

165
00:07:21,120 --> 00:07:23,580
世紀，操作編碼被視為

166
00:07:23,580 --> 00:07:25,919
一種學習算法，因此我們可以首先

167
00:07:25,919 --> 00:07:29,699
壓縮信息 然後更新生成模型中的所有

168
00:07:29,699 --> 00:07:31,800
突觸或所有潛在變量，

169
00:07:31,800 --> 00:07:34,139


170
00:07:34,139 --> 00:07:38,599
以改進生成模型本身，

171
00:07:38,759 --> 00:07:43,199
因此讓我們給出一些更正式的呃定義，

172
00:07:43,199 --> 00:07:45,000


173
00:07:45,000 --> 00:07:48,479
以便操作編碼可以被視為

174
00:07:48,479 --> 00:07:50,220
分層高斯生成 模型，

175
00:07:50,220 --> 00:07:53,400
所以這是一個非常簡單的圖，其中

176
00:07:53,400 --> 00:07:54,780
我們有一個層次結構，

177
00:07:54,780 --> 00:07:58,319
可以是我們想要的深度，

178
00:07:58,319 --> 00:08:01,560
信號預測信號

179
00:08:01,560 --> 00:08:04,620
從一個潛在變量 XM 到

180
00:08:04,620 --> 00:08:06,599
下一個潛在變量，並且

181
00:08:06,599 --> 00:08:09,720
每次都通過函數 GN

182
00:08:09,720 --> 00:08:12,620
或 GI 進行轉換

183
00:08:15,319 --> 00:08:18,180
正如我所說，這是一個生成模型，

184
00:08:18,180 --> 00:08:19,680
這個生成模型的邊際概率是多少，

185
00:08:19,680 --> 00:08:21,780
這只是最後一個的

186
00:08:21,780 --> 00:08:24,960
概率，

187
00:08:24,960 --> 00:08:27,660
你能看到我的光標嗎？是的，是的，

188
00:08:27,660 --> 00:08:29,940
它是完美的，所以它是

189
00:08:29,940 --> 00:08:32,700
最後一個頂點的遺傳模型，對不起，可能是

190
00:08:32,700 --> 00:08:34,979
分佈 最後一個頂點的概率乘以

191
00:08:34,979 --> 00:08:37,140
每個其他頂點的概率分佈，以

192
00:08:37,140 --> 00:08:40,440


193
00:08:40,440 --> 00:08:43,020
之前頂點的活動或

194
00:08:43,020 --> 00:08:45,860
潛在變量為條件。

195
00:08:45,899 --> 00:08:48,240
我已經說過，這是一個高斯

196
00:08:48,240 --> 00:08:50,399
生成模型，這意味著這些

197
00:08:50,399 --> 00:08:54,260
概率是高斯形式的，

198
00:08:54,660 --> 00:08:57,120
並且每個

199
00:08:57,120 --> 00:09:00,480
endos 函數 一般而言，函數 G

200
00:09:00,480 --> 00:09:02,880
特別是因為例如在

201
00:09:02,880 --> 00:09:05,459
Rambler 論文以及之後的所有論文中，

202
00:09:05,459 --> 00:09:07,920
也因為深度

203
00:09:07,920 --> 00:09:10,500
學習革命，這些函數只是

204
00:09:10,500 --> 00:09:13,220
簡單的線性映射或

205
00:09:13,220 --> 00:09:15,120
具有激活

206
00:09:15,120 --> 00:09:18,000
函數的非線性映射或具有

207
00:09:18,000 --> 00:09:22,040
激活函數和 加性偏差，

208
00:09:23,220 --> 00:09:27,180
因此我們可以給出

209
00:09:27,180 --> 00:09:28,860
創造性編碼的正式定義，我們可以

210
00:09:28,860 --> 00:09:30,300
說操作編碼是

211
00:09:30,300 --> 00:09:33,480
這種生成模型的反演方案，其中通過最小化通常

212
00:09:33,480 --> 00:09:35,839


213
00:09:35,839 --> 00:09:38,760
稱為

214
00:09:38,760 --> 00:09:40,920
自由能變化的量來最大化其模型證據

215
00:09:40,920 --> 00:09:43,740
每個生成模型的目標

216
00:09:43,740 --> 00:09:46,019
都是最大化模型證據，但

217
00:09:46,019 --> 00:09:48,860
這個量總是很棘手，

218
00:09:48,860 --> 00:09:51,019
我們有一些

219
00:09:51,019 --> 00:09:53,279
技術可以讓我們

220
00:09:53,279 --> 00:09:55,980
近似解決方案以及

221
00:09:55,980 --> 00:09:58,500
我們在創造性編碼中使用的技術，而

222
00:09:58,500 --> 00:10:00,720
不是最小化自由能像差，

223
00:10:00,720 --> 00:10:03,480
這是一種 這是

224
00:10:03,480 --> 00:10:06,839
這項工作中模型證據的下限，

225
00:10:06,839 --> 00:10:09,660
實際上在許多

226
00:10:09,660 --> 00:10:11,700
其他工作中也是如此，所以這是執行此操作的標準方法，

227
00:10:11,700 --> 00:10:13,740
這種最小化是執行

228
00:10:13,740 --> 00:10:16,080
成分下降

229
00:10:16,080 --> 00:10:18,540
嗯，是的，執行我們在下降中達成的協議，

230
00:10:18,540 --> 00:10:19,980
並且有 實際上，其他方法（

231
00:10:19,980 --> 00:10:22,140
例如期望最大化）

232
00:10:22,140 --> 00:10:23,580
通常是等效的，

233
00:10:23,580 --> 00:10:25,140
或者您可以使用其他一些消息

234
00:10:25,140 --> 00:10:26,940
傳遞算法（例如信念

235
00:10:26,940 --> 00:10:29,959
傳播），

236
00:10:30,720 --> 00:10:33,980
然後稍微回溯過去，因此

237
00:10:33,980 --> 00:10:35,940


238
00:10:35,940 --> 00:10:38,760


239
00:10:38,760 --> 00:10:41,360
如果我們可以看到創造性編碼，請忘記一點統計生成模型

240
00:10:41,360 --> 00:10:44,040
我的意思是，我已經說過幾次

241
00:10:44,040 --> 00:10:46,200


242
00:10:46,200 --> 00:10:48,420
作為神經活動的分層模型，因此對於

243
00:10:48,420 --> 00:10:50,700
代表神經活動的神經元潛在變量，

244
00:10:50,700 --> 00:10:53,459
發送者在層次結構中發出信號，

245
00:10:53,459 --> 00:10:54,899


246
00:10:54,899 --> 00:10:57,540
而對於錯誤節點或錯誤神經元，

247
00:10:57,540 --> 00:11:01,019
發送者在層次結構中發出信號，所以

248
00:11:01,019 --> 00:11:03,660
這個和 返回的誤差信息

249
00:11:03,660 --> 00:11:05,700
是此類操作編碼模型的自由能變化是多少，

250
00:11:05,700 --> 00:11:08,220
它

251
00:11:08,220 --> 00:11:09,899
只是

252
00:11:09,899 --> 00:11:12,720
所有誤差神經元的均方誤差之和，

253
00:11:12,720 --> 00:11:14,399


254
00:11:14,399 --> 00:11:18,120
因此是

255
00:11:18,120 --> 00:11:21,980
總誤差平方的誤差

256
00:11:22,019 --> 00:11:24,480
之和，該表示為

257
00:11:24,480 --> 00:11:27,120
在後面的幻燈片以及

258
00:11:27,120 --> 00:11:28,740
我將如何解釋如何使用

259
00:11:28,740 --> 00:11:30,120
創造性編碼來建模因果

260
00:11:30,120 --> 00:11:32,940
推理中將會很有用，例如，

261
00:11:32,940 --> 00:11:34,800
我認為預測編碼很重要，

262
00:11:34,800 --> 00:11:36,240
但它不是一個

263
00:11:36,240 --> 00:11:37,500


264
00:11:37,500 --> 00:11:39,600
首先要好好研究的好算法，因為 我之前說過，它

265
00:11:39,600 --> 00:11:41,399
優化了正確的目標，即

266
00:11:41,399 --> 00:11:43,079
模型證據或邊際

267
00:11:43,079 --> 00:11:44,339
可能性，

268
00:11:44,339 --> 00:11:45,660


269
00:11:45,660 --> 00:11:47,700
然後通過優化

270
00:11:47,700 --> 00:11:49,440
下限（稱為

271
00:11:49,440 --> 00:11:52,440
自由能的變化）來實現這一目標，正如我所說，虛擬

272
00:11:52,440 --> 00:11:54,240
完成很有趣，因為它可以

273
00:11:54,240 --> 00:11:57,680
寫成 兩個不同術語的總和，

274
00:11:57,680 --> 00:12:00,839
以及

275
00:12:00,839 --> 00:12:04,680
優化它的每個術語作為重要影響，

276
00:12:04,680 --> 00:12:06,899
例如在機器學習任務

277
00:12:06,899 --> 00:12:09,060
或一般學習任務中，

278
00:12:09,060 --> 00:12:12,420
所以其中一個術語強制記憶，

279
00:12:12,420 --> 00:12:15,440
所以在第二項中基本上告訴

280
00:12:15,440 --> 00:12:18,180
強制模型 擬合特定的數據

281
00:12:18,180 --> 00:12:19,560
集，

282
00:12:19,560 --> 00:12:21,240
第一項

283
00:12:21,240 --> 00:12:23,519
迫使模型最小化

284
00:12:23,519 --> 00:12:26,040
複雜性，正如我們所知，例如

285
00:12:26,040 --> 00:12:28,500
結果剃刀

286
00:12:28,500 --> 00:12:31,260
理論，如果我們有兩個不同的模型，它們

287
00:12:31,260 --> 00:12:33,000
在特定

288
00:12:33,000 --> 00:12:35,640
訓練集上的表現與我們擁有的模型相似 得到

289
00:12:35,640 --> 00:12:37,380
並且

290
00:12:37,380 --> 00:12:39,899
最有望概括呃的是不太

291
00:12:39,899 --> 00:12:41,160
複雜的

292
00:12:41,160 --> 00:12:44,100
模型，因此通過操作自由能更新生成模型

293
00:12:44,100 --> 00:12:46,380
使我們

294
00:12:46,380 --> 00:12:47,779
基本上可以

295
00:12:47,779 --> 00:12:51,959
收斂到最佳呃結果剃刀

296
00:12:51,959 --> 00:12:54,720
模型，該模型既可以記憶

297
00:12:54,720 --> 00:12:56,100
數據集，也可以 能夠

298
00:12:56,100 --> 00:12:58,680
很好地概括看不見的

299
00:12:58,680 --> 00:13:00,240
數據點

300
00:13:00,240 --> 00:13:02,639
操作編碼之所以重要的第二個原因

301
00:13:02,639 --> 00:13:08,600
是，它實際上

302
00:13:08,720 --> 00:13:11,760
不必在

303
00:13:11,760 --> 00:13:13,920
層次結構上定義，但它可以

304
00:13:13,920 --> 00:13:15,959
在更複雜和靈活的體系

305
00:13:15,959 --> 00:13:18,240
結構（例如定向圖形）上建模

306
00:13:18,240 --> 00:13:21,540
任何形狀的模型，或者

307
00:13:21,540 --> 00:13:23,700
更廣義地推廣到具有許多類似於

308
00:13:23,700 --> 00:13:25,920
大腦區域的循環的網絡，最終

309
00:13:25,920 --> 00:13:27,779
結果的根本原因

310
00:13:27,779 --> 00:13:30,300
是你不是

311
00:13:30,300 --> 00:13:32,339
通過前向傳播來學習和預測，然後

312
00:13:32,339 --> 00:13:34,260
向後傳播誤差，而是你

313
00:13:34,260 --> 00:13:36,600
最小化能量函數，

314
00:13:36,600 --> 00:13:38,459
這基本上允許每種

315
00:13:38,459 --> 00:13:39,839
層次結構都

316
00:13:39,839 --> 00:13:41,180


317
00:13:41,180 --> 00:13:43,860
允許進入直接鍵並

318
00:13:43,860 --> 00:13:46,860
允許學習循環，這

319
00:13:46,860 --> 00:13:48,060
實際上非常重要，因為

320
00:13:48,060 --> 00:13:50,399
大腦充滿了循環，因為我們

321
00:13:50,399 --> 00:13:53,399
從最近的一些論文中獲得了一些信息

322
00:13:53,399 --> 00:13:56,459
呃，我們成功地繪製了

323
00:13:56,459 --> 00:13:59,279
某些動物（例如果蠅）的大腦，

324
00:13:59,279 --> 00:14:00,420


325
00:14:00,420 --> 00:14:03,899
大腦充滿了循環，因此使用允許我們使用循環來

326
00:14:03,899 --> 00:14:06,720
耗盡我們的機器學習

327
00:14:06,720 --> 00:14:09,000
模型或

328
00:14:09,000 --> 00:14:11,160
一般模型的算法是有意義的

329
00:14:11,160 --> 00:14:14,160


330
00:14:14,160 --> 00:14:17,160
結構

331
00:14:17,160 --> 00:14:19,380
操作編碼有趣的第三個原因

332
00:14:19,380 --> 00:14:21,240
是，它已經被正式

333
00:14:21,240 --> 00:14:23,820
證明比

334
00:14:23,820 --> 00:14:25,139
從黑色傳播開始的標準神經網絡更穩健，

335
00:14:25,139 --> 00:14:27,060
因此，如果您有一個

336
00:14:27,060 --> 00:14:28,200
神經網絡並且想要執行

337
00:14:28,200 --> 00:14:30,320
分類任務，那麼

338
00:14:30,320 --> 00:14:34,139
創造性編碼會更穩健

339
00:14:34,139 --> 00:14:36,260
，並且 這

340
00:14:36,260 --> 00:14:38,339
對於

341
00:14:38,339 --> 00:14:40,680
小數據集的在線學習訓練或

342
00:14:40,680 --> 00:14:43,440
連續學習任務等任務來說是一個有趣的問題，該理論

343
00:14:43,440 --> 00:14:45,540
基本上來自這樣一個事實：

344
00:14:45,540 --> 00:14:48,540
命令式編碼已被移動到

345
00:14:48,540 --> 00:14:50,820
近似隱式梯度下降，

346
00:14:50,820 --> 00:14:53,339
這是顯式梯度下降的不同版本

347
00:14:53,339 --> 00:14:54,899


348
00:14:54,899 --> 00:14:57,180


349
00:14:57,180 --> 00:14:59,880
基本上每個模型中使用的標準綠色下降，

350
00:14:59,880 --> 00:15:03,680
這是一種更穩健的變體，

351
00:15:05,880 --> 00:15:08,279
我想好吧，我做了相當長的內部

352
00:15:08,279 --> 00:15:09,779
操作編碼，我想我現在正在

353
00:15:09,779 --> 00:15:11,639
轉向第二個主題，即因果

354
00:15:11,639 --> 00:15:13,019
推理

355
00:15:13,019 --> 00:15:15,839
和什麼 因果推理 因果

356
00:15:15,839 --> 00:15:18,420
推理是一種理論，它是一種非常

357
00:15:18,420 --> 00:15:20,339
普遍的理論，

358
00:15:20,339 --> 00:15:23,100
朱迪服裝最形式化了，他絕對是

359
00:15:23,100 --> 00:15:25,500


360
00:15:25,500 --> 00:15:27,839
法國因果領域最重要的人物，他寫了一些

361
00:15:27,839 --> 00:15:29,760
非常好的書，例如《Y》的書，

362
00:15:29,760 --> 00:15:32,760
強烈推薦 如果您想

363
00:15:32,760 --> 00:15:35,220
了解有關該主題的更多信息，

364
00:15:35,220 --> 00:15:37,800
並且它基本上解決了以下

365
00:15:37,800 --> 00:15:38,639
問題，

366
00:15:38,639 --> 00:15:40,440
那麼讓我們假設我們有一個

367
00:15:40,440 --> 00:15:42,000


368
00:15:42,000 --> 00:15:44,160
與貝葉斯網絡相關的聯合概率分佈，這

369
00:15:44,160 --> 00:15:46,199
將是

370
00:15:46,199 --> 00:15:49,260
整個論文的運行示例，

371
00:15:49,260 --> 00:15:51,839
特別是當 你不使用

372
00:15:51,839 --> 00:15:54,480
這種形狀的亞洲網絡，

373
00:15:54,480 --> 00:15:57,660
它基於網絡，

374
00:15:57,660 --> 00:16:00,240
它們內部的變量可以代表

375
00:16:00,240 --> 00:16:02,100
不同的數量，因此例如我們

376
00:16:02,100 --> 00:16:04,620
具有這種形狀的視覺網絡可以

377
00:16:04,620 --> 00:16:06,899
代表

378
00:16:06,899 --> 00:16:08,820
右側的數量，因此

379
00:16:08,820 --> 00:16:10,800
個人的社會經濟工作室雕像

380
00:16:10,800 --> 00:16:13,079
教育水平、

381
00:16:13,079 --> 00:16:16,699
智力和收入水平

382
00:16:17,100 --> 00:16:19,440
是經典統計學

383
00:16:19,440 --> 00:16:22,920
非常擅長的，是呃，而呃，

384
00:16:22,920 --> 00:16:25,320
最常用的應用是對

385
00:16:25,320 --> 00:16:28,019
觀察或相關性進行建模，

386
00:16:28,019 --> 00:16:29,279
相關性基本上回答了這個

387
00:16:29,279 --> 00:16:32,519
問題：如果我們觀察

388
00:16:32,519 --> 00:16:35,579
另一個變量 C，

389
00:16:35,579 --> 00:16:37,500
那麼，例如，在 在這種情況下，

390
00:16:37,500 --> 00:16:39,660
收入水平是多少

391
00:16:39,660 --> 00:16:41,820
如果我

392
00:16:41,820 --> 00:16:44,339
觀察這個教育水平，則

393
00:16:44,339 --> 00:16:48,180
個人的預期收入水平，當然，如果該人

394
00:16:48,180 --> 00:16:50,220
具有更高的教育程度，

395
00:16:50,220 --> 00:16:52,500
例如碩士或博士學位，我預計

396
00:16:52,500 --> 00:16:54,360
該人一般具有 較高的

397
00:16:54,360 --> 00:16:56,040
收入水平，

398
00:16:56,040 --> 00:16:58,139
這是一種相關性，

399
00:16:58,139 --> 00:17:00,300
但有時有些事情

400
00:17:00,300 --> 00:17:03,300
很難觀察，但它們

401
00:17:03,300 --> 00:17:05,040
在確定這些數量方面發揮著巨大作用，

402
00:17:05,040 --> 00:17:06,119


403
00:17:06,119 --> 00:17:08,220
因此，例如，

404
00:17:08,220 --> 00:17:11,160
收入水平可能更多地

405
00:17:11,160 --> 00:17:13,380
由一個人的智力來定義

406
00:17:13,380 --> 00:17:15,540
特定的人，

407
00:17:15,540 --> 00:17:18,720
也許智力或

408
00:17:18,720 --> 00:17:21,000
一個人是否聰明也最

409
00:17:21,000 --> 00:17:24,540
有可能擁有較高的教育水平，

410
00:17:24,540 --> 00:17:27,540
但仍然是

411
00:17:27,540 --> 00:17:30,120
每個收入的真正原因是我是因為

412
00:17:30,120 --> 00:17:32,220
智商

413
00:17:32,220 --> 00:17:34,740
，這可能是這不能

414
00:17:34,740 --> 00:17:36,360
通過簡單的相關性進行研究，並且必須

415
00:17:36,360 --> 00:17:39,120
通過一種更先進的技術進行研究，這種技術

416
00:17:39,120 --> 00:17:41,280
稱為乾預，

417
00:17:41,280 --> 00:17:43,320
干預基本上回答的

418
00:17:43,320 --> 00:17:46,500
問題是，如果我們將 C 更改為特定值，那麼 D 是什麼，

419
00:17:46,500 --> 00:17:48,240


420
00:17:48,240 --> 00:17:51,000
例如，我們可以採取一個

421
00:17:51,000 --> 00:17:54,660
個體 檢查他的收入水平，

422
00:17:54,660 --> 00:17:57,120
然後改變其教育水平，以便在

423
00:17:57,120 --> 00:17:59,220


424
00:17:59,220 --> 00:18:01,080
不影響他的智力的情況下乾預這個世界並改變其教育水平，

425
00:18:01,080 --> 00:18:03,419
看看

426
00:18:03,419 --> 00:18:07,260
其收入變化有多大，

427
00:18:07,260 --> 00:18:09,900
例如，如果收入變化很大，則

428
00:18:09,900 --> 00:18:12,179
意味著智力

429
00:18:12,179 --> 00:18:14,460
沒有變化 在這方面並沒有發揮很大的作用，但

430
00:18:14,460 --> 00:18:16,799
教育水平會起作用，如果收入水平

431
00:18:16,799 --> 00:18:19,020
沒有太大變化，這意味著

432
00:18:19,020 --> 00:18:20,640
在這種情況下可能存在一個隱藏變量，

433
00:18:20,640 --> 00:18:22,860
智力決定了

434
00:18:22,860 --> 00:18:25,760
一個人的收入水平

435
00:18:25,980 --> 00:18:28,740
第三個重要的因果

436
00:18:28,740 --> 00:18:31,080
推論是： 反事實，

437
00:18:31,080 --> 00:18:33,120
例如，反事實回答了

438
00:18:33,120 --> 00:18:36,720
問題是什麼，我們將

439
00:18:36,720 --> 00:18:39,240
C 更改為過去的不同值，

440
00:18:39,240 --> 00:18:40,679
例如，我們可以看到

441
00:18:40,679 --> 00:18:42,059
干預措施和

442
00:18:42,059 --> 00:18:45,059
反事實之間的區別在於，干預措施

443
00:18:45,059 --> 00:18:47,820
在未來起作用，所以我正在採訪 在

444
00:18:47,820 --> 00:18:50,340
現在的世界中觀察未來的變化

445
00:18:50,340 --> 00:18:53,220
以及反事實使我們能夠

446
00:18:53,220 --> 00:18:56,039
回到過去並改變

447
00:18:56,039 --> 00:18:59,160
過去的變量，看看這種變化

448
00:18:59,160 --> 00:19:01,320
將如何影響我們現在生活的世界

449
00:19:01,320 --> 00:19:02,940


450
00:19:02,940 --> 00:19:06,299
，這些被 judapple 定義為

451
00:19:06,299 --> 00:19:08,100
三個 因果推理相關性的級別

452
00:19:08,100 --> 00:19:09,660
是第一級

453
00:19:09,660 --> 00:19:11,580
干預是反事實的第二級

454
00:19:11,580 --> 00:19:14,720
是第三級

455
00:19:16,020 --> 00:19:18,120
其他干預

456
00:19:18,120 --> 00:19:20,640
現在我給出了

457
00:19:20,640 --> 00:19:23,760
一個直觀的定義，我將更正式地定義它們，我在

458
00:19:23,760 --> 00:19:25,500
這裡使用這個符號

459
00:19:25,500 --> 00:19:27,240
實際上在整個演示中都是一樣的，

460
00:19:27,240 --> 00:19:29,640
所以 X 總是

461
00:19:29,640 --> 00:19:32,820
一個潛在變量，i 總是

462
00:19:32,820 --> 00:19:35,340
一個數據點或一個觀察值

463
00:19:35,340 --> 00:19:38,520
，VI 總是一個頂點，所以

464
00:19:38,520 --> 00:19:40,860
每次你看到 VI 時，我們只是

465
00:19:40,860 --> 00:19:42,720


466
00:19:42,720 --> 00:19:45,299
例如，我們對圖的結構感興趣，

467
00:19:45,299 --> 00:19:46,860
所以假設我們有一個貝葉斯模型，

468
00:19:46,860 --> 00:19:50,160
它的結構與

469
00:19:50,160 --> 00:19:52,679
我們在上一張幻燈片中看到的貝葉斯模型相同，

470
00:19:52,679 --> 00:19:54,780


471
00:19:54,780 --> 00:19:57,840
假設 X3 等於 S3，這是

472
00:19:57,840 --> 00:20:00,660
我們進行統計的觀察結果 我們

473
00:20:00,660 --> 00:20:03,360
計算 X4 的概率或

474
00:20:03,360 --> 00:20:04,679
期望，X4

475
00:20:04,679 --> 00:20:07,380
是與

476
00:20:07,380 --> 00:20:09,240
該頂點相關的潛在變量，

477
00:20:09,240 --> 00:20:13,860
假設 X3 等於 S3

478
00:20:13,860 --> 00:20:15,679
外部

479
00:20:15,679 --> 00:20:17,760
干預，我們需要一種新的

480
00:20:17,760 --> 00:20:19,919
表示法，稱為 do

481
00:20:19,919 --> 00:20:21,179
操作，

482
00:20:21,179 --> 00:20:23,880
因此在這種情況下，

483
00:20:23,880 --> 00:20:26,100
我們要計算 X4

484
00:20:26,100 --> 00:20:30,000
考慮到我們干預這個

485
00:20:30,000 --> 00:20:33,059
詞並改變 X3 West 3 的事實，X4 的概率。

486
00:20:33,059 --> 00:20:35,580
我們如何執行

487
00:20:35,580 --> 00:20:38,400
干預 Judo Pearl 告訴我們，

488
00:20:38,400 --> 00:20:40,020


489
00:20:40,020 --> 00:20:41,880
在計算相關性之前，我們必須有一個中間步驟

490
00:20:41,880 --> 00:20:45,059


491
00:20:45,059 --> 00:20:46,860
必須刪除所有以刪除

492
00:20:46,860 --> 00:20:50,160
V3 的所有傳入邊，

493
00:20:50,160 --> 00:20:52,799
因此我們必須研究的不是這個貝葉斯

494
00:20:52,799 --> 00:20:55,679
網絡，而是第二個貝葉斯網絡，

495
00:20:55,679 --> 00:20:58,200
然後在這一點上我們可以

496
00:20:58,200 --> 00:21:00,840
像平常一樣計算相關性

497
00:21:00,840 --> 00:21:03,299


498
00:21:03,299 --> 00:21:06,500
，這是一種

499
00:21:07,020 --> 00:21:09,299
反事實的干預 是

500
00:21:09,299 --> 00:21:11,700
這一點的概括，正如我所說的生活在過去，

501
00:21:11,700 --> 00:21:14,100
他們正在使用結構因果模型進行計算，

502
00:21:14,100 --> 00:21:15,419


503
00:21:15,419 --> 00:21:18,299
結構因果模型是一個元組，

504
00:21:18,299 --> 00:21:21,120
它在概念上類似於

505
00:21:21,120 --> 00:21:23,460
貝葉斯網絡，但基本上我們

506
00:21:23,460 --> 00:21:26,220
在上面有這一類新的變量

507
00:21:26,220 --> 00:21:28,580
是他們使用的不可觀察變量，

508
00:21:28,580 --> 00:21:30,960
所以我們有

509
00:21:30,960 --> 00:21:34,020
X1 X2 X3 S4 之前的貝葉斯網絡，

510
00:21:34,020 --> 00:21:37,460
但我們也有那些不可觀察或

511
00:21:37,460 --> 00:21:40,020
依賴於環境的變量，

512
00:21:40,020 --> 00:21:42,539
你無法控制它們，你可以推斷

513
00:21:42,539 --> 00:21:43,980
它們，但你

514
00:21:43,980 --> 00:21:46,020
但它們就在那裡，

515
00:21:46,020 --> 00:21:48,539


516
00:21:48,539 --> 00:21:51,360
f 是一組函數，它依賴於

517
00:21:51,360 --> 00:21:53,400


518
00:21:53,400 --> 00:21:57,299
x 的所有基本 f，x3 依賴於 X1，

519
00:21:57,299 --> 00:21:58,980
因為我們在 x2 上有一個箭頭，因為

520
00:21:58,980 --> 00:22:00,960
你有一個箭頭，並且依賴於

521
00:22:00,960 --> 00:22:02,940
也影響極端的不可觀察變量，

522
00:22:02,940 --> 00:22:05,840


523
00:22:06,179 --> 00:22:09,240
所以是的，直覺上你可以看到我們，你

524
00:22:09,240 --> 00:22:11,940
可以認為 結構因果模型

525
00:22:11,940 --> 00:22:14,159
作為貝葉斯網絡，其中

526
00:22:14,159 --> 00:22:16,679
不可觀察的變量位於頂部，每個

527
00:22:16,679 --> 00:22:19,500
不可觀察的變量僅影響

528
00:22:19,500 --> 00:22:22,020
其自己

529
00:22:22,020 --> 00:22:24,600
的最新變量 X，因此例如

530
00:22:24,600 --> 00:22:27,960
IU 永遠不會接觸 X1，u3

531
00:22:27,960 --> 00:22:30,360
只會接觸 Q3 E1 都會影響

532
00:22:30,360 --> 00:22:34,039
X1 和 依此類推，

533
00:22:35,039 --> 00:22:37,679
執行反事實推理

534
00:22:37,679 --> 00:22:39,900
回答了以下問題，那麼在通過情況下，

535
00:22:39,900 --> 00:22:42,960
X4 在 X3 處等於另一個

536
00:22:42,960 --> 00:22:46,620
變量嗎？你

537
00:22:46,620 --> 00:22:49,340
外國

538
00:22:49,340 --> 00:22:51,840
需要三個不同的步驟，所以

539
00:22:51,840 --> 00:22:53,039
溯因就是

540
00:22:53,039 --> 00:22:54,900


541
00:22:54,900 --> 00:22:57,179
所有背景

542
00:22:57,179 --> 00:22:59,460
變量的計算，所以在這個 在這一步中，我們

543
00:22:59,460 --> 00:23:01,200
想要回到過去，了解

544
00:23:01,200 --> 00:23:03,419
不可觀察的環境

545
00:23:03,419 --> 00:23:04,919


546
00:23:04,919 --> 00:23:08,039
在那個特定時刻的情況，

547
00:23:08,039 --> 00:23:11,039
我們通過將所有潛在

548
00:23:11,039 --> 00:23:14,280
變量 X 固定到我們已經擁有的一些特定數據來實現這一點，

549
00:23:14,280 --> 00:23:16,140


550
00:23:16,140 --> 00:23:18,960
並執行這些呃這個

551
00:23:18,960 --> 00:23:21,120


552
00:23:21,120 --> 00:23:24,240
然後我們將使用 U 來

553
00:23:24,240 --> 00:23:26,940
保留我們學到的 U 並

554
00:23:26,940 --> 00:23:28,500
執行干預，

555
00:23:28,500 --> 00:23:29,880
因此

556
00:23:29,880 --> 00:23:32,340
反制也可以被視為

557
00:23:32,340 --> 00:23:34,980
及時的干預，其中我們

558
00:23:34,980 --> 00:23:36,960
知道環境環境

559
00:23:36,960 --> 00:23:40,620
變量 U1 U2 和 u4 在該特定

560
00:23:40,620 --> 00:23:43,039
時刻

561
00:23:43,200 --> 00:23:44,340
以及

562
00:23:44,340 --> 00:23:46,679
缺少的步驟是什麼，

563
00:23:46,679 --> 00:23:49,440
那麼在該特定情況下 X4 在 X3 處等於

564
00:23:49,440 --> 00:23:50,780


565
00:23:50,780 --> 00:23:53,280
另一個數據點

566
00:23:53,280 --> 00:23:55,980
現在我們可以計算

567
00:23:55,980 --> 00:23:57,120
相關性

568
00:23:57,120 --> 00:23:59,520
以及我們在圖上的路徑上執行的相關性

569
00:23:59,520 --> 00:24:02,039
其中我們

570
00:24:02,039 --> 00:24:04,440
已經使用

571
00:24:04,440 --> 00:24:06,659
我們

572
00:24:06,659 --> 00:24:10,140
在溯因步驟中學到的環境變量進行了乾預，

573
00:24:10,140 --> 00:24:14,419
這是一個反事實推理，

574
00:24:15,480 --> 00:24:18,000
這是因果推理當前介紹的最後一張幻燈片

575
00:24:18,000 --> 00:24:20,159


576
00:24:20,159 --> 00:24:21,720
，基本上是關於結構學習的，

577
00:24:21,720 --> 00:24:23,880
基本上是我所說的一切

578
00:24:23,880 --> 00:24:27,360
到目前為止，我們依賴於這樣一個事實：我們知道

579
00:24:27,360 --> 00:24:29,700


580
00:24:29,700 --> 00:24:31,500
數據點之間的因果關係，因此我們知道

581
00:24:31,500 --> 00:24:33,120
圖表的結構，我們知道哪個變量

582
00:24:33,120 --> 00:24:34,860
影響哪個變量，

583
00:24:34,860 --> 00:24:37,260
我們一般知道箭頭，

584
00:24:37,260 --> 00:24:39,659
但實際上這實際上並不總是

585
00:24:39,659 --> 00:24:42,900
可能的，所以我們

586
00:24:42,900 --> 00:24:45,419


587
00:24:45,419 --> 00:24:47,400
大多數時候我們無法訪問因果圖，實際上

588
00:24:47,400 --> 00:24:49,919
從數據中學習最好的因果圖仍然是

589
00:24:49,919 --> 00:24:51,840
一個懸而未決的問題，我們正在改進，

590
00:24:51,840 --> 00:24:53,880
我們正在變得更好，但

591
00:24:53,880 --> 00:24:57,299
如何準確地執行這個任務

592
00:24:57,299 --> 00:24:58,380
呃仍然

593
00:24:58,380 --> 00:25:01,140
是一個懸而未決的問題

594
00:25:01,140 --> 00:25:03,179
正如我所說，基本上目標是

595
00:25:03,179 --> 00:25:04,740
從觀測數據中引用理事會關係，

596
00:25:04,740 --> 00:25:07,380
因此給定一個數據集，

597
00:25:07,380 --> 00:25:09,780
我們希望推斷出

598
00:25:09,780 --> 00:25:12,179
描述

599
00:25:12,179 --> 00:25:14,460
系統與數據集變量之間的連接性的有向精確圖，

600
00:25:14,460 --> 00:25:15,960


601
00:25:15,960 --> 00:25:17,700
例如這裡我們有一個例子

602
00:25:17,700 --> 00:25:19,440
我想我們

603
00:25:19,440 --> 00:25:22,860
都熟悉，謝謝呃，因為

604
00:25:22,860 --> 00:25:25,080
大流行，所以我們有這四個

605
00:25:25,080 --> 00:25:28,799
變量，年齡，疫苗住院

606
00:25:28,799 --> 00:25:31,380
和 CT，

607
00:25:31,380 --> 00:25:33,600
我們想推斷

608
00:25:33,600 --> 00:25:36,059
這些變量之間的因果關係，

609
00:25:36,059 --> 00:25:37,980
例如，我們想直接

610
00:25:37,980 --> 00:25:40,260
從數據中學習概率 一個

611
00:25:40,260 --> 00:25:43,080
人住院的情況取決於

612
00:25:43,080 --> 00:25:45,419
他的年齡以及是否接種了

613
00:25:45,419 --> 00:25:49,760
疫苗等等，所以

614
00:25:51,299 --> 00:25:55,020
這是冗長的介紹的結尾，

615
00:25:55,020 --> 00:25:58,080
但是呃我希望它足夠清楚

616
00:25:58,080 --> 00:26:00,179
，我希望我給了像

617
00:26:00,179 --> 00:26:02,039
了解

618
00:26:02,039 --> 00:26:05,159
論文的基本結果

619
00:26:05,159 --> 00:26:07,740
現在我們可以開始研究問題

620
00:26:07,740 --> 00:26:09,059
所以研究問題

621
00:26:09,059 --> 00:26:10,440


622
00:26:10,440 --> 00:26:12,900
首先如下我想看看

623
00:26:12,900 --> 00:26:15,299
創造性編碼是否可以用於

624
00:26:15,299 --> 00:26:16,980
執行因果推理

625
00:26:16,980 --> 00:26:20,100
所以到目前為止只使用了操作性編碼

626
00:26:20,100 --> 00:26:22,380


627
00:26:22,380 --> 00:26:25,020
在貝葉斯網絡中執行計算相關性，

628
00:26:25,020 --> 00:26:27,419
最大的問題是我們能否以生物學上合理​​的方式超越

629
00:26:27,419 --> 00:26:29,400
相關性、模型乾預和

630
00:26:29,400 --> 00:26:31,679
反事實，

631
00:26:31,679 --> 00:26:32,760


632
00:26:32,760 --> 00:26:34,380
所以呃，

633
00:26:34,380 --> 00:26:36,120
以一種簡單

634
00:26:36,120 --> 00:26:39,059
直觀的方式，讓我們只與

635
00:26:39,059 --> 00:26:40,740
神經元一起玩 並且不觸及例如

636
00:26:40,740 --> 00:26:43,740
圖形的巨大結構

637
00:26:43,740 --> 00:26:46,380
，更具體地說，

638
00:26:46,380 --> 00:26:48,299
問題是我們能否定義基於

639
00:26:48,299 --> 00:26:51,000
操作編碼的結構因果

640
00:26:51,000 --> 00:26:52,740
模型來執行干預和

641
00:26:52,740 --> 00:26:55,320
反事實

642
00:26:55,320 --> 00:26:58,380
第二個問題是，

643
00:26:58,380 --> 00:27:00,179
正如我所說，擁有一個結構自定義

644
00:27:00,179 --> 00:27:02,159
模型假設我們 知道

645
00:27:02,159 --> 00:27:04,260
逃避網絡的結構，

646
00:27:04,260 --> 00:27:07,919
所以我們假設我們有箭頭，

647
00:27:07,919 --> 00:27:09,960
我們可以超越這個並使用創造性的

648
00:27:09,960 --> 00:27:11,520
編碼網絡來學習

649
00:27:11,520 --> 00:27:14,418
圖的因果結構，

650
00:27:16,140 --> 00:27:18,900
基本上對這兩個問題給出肯定的答案

651
00:27:18,900 --> 00:27:21,120
將允許我們

652
00:27:21,120 --> 00:27:23,120
使用預測編碼 一種端到端

653
00:27:23,120 --> 00:27:26,039
因果推理方法，基本上

654
00:27:26,039 --> 00:27:28,740
採用一個數據集，並允許我們

655
00:27:28,740 --> 00:27:30,419


656
00:27:30,419 --> 00:27:34,820
直接從該數據集測試干預和反事實預測，

657
00:27:36,840 --> 00:27:39,299
所以讓我們解決第

658
00:27:39,299 --> 00:27:40,740
一個問題，即因果推理

659
00:27:40,740 --> 00:27:42,419
振動編碼，這也是

660
00:27:42,419 --> 00:27:45,120
給出的部分 基本上是論文的標題，

661
00:27:45,120 --> 00:27:46,740


662
00:27:46,740 --> 00:27:48,539
在這裡我將展示如何執行

663
00:27:48,539 --> 00:27:50,760
相關操作編碼，這是

664
00:27:50,760 --> 00:27:52,440
已知的

665
00:27:52,440 --> 00:27:54,419
嗯，以及如何執行介入

666
00:27:54,419 --> 00:27:56,760
查詢，我認為這

667
00:27:56,760 --> 00:28:01,140
是論文的真正問題，

668
00:28:01,140 --> 00:28:03,900
所以這裡是一個因果關係 圖是

669
00:28:03,900 --> 00:28:05,700
我們擁有的常用圖，

670
00:28:05,700 --> 00:28:07,260


671
00:28:07,260 --> 00:28:09,240
這裡是相應的創意

672
00:28:09,240 --> 00:28:11,760
編碼模型，因此軸是

673
00:28:11,760 --> 00:28:13,980
潛在變量，對應於

674
00:28:13,980 --> 00:28:18,000
神經網絡模型中的神經元，

675
00:28:18,000 --> 00:28:20,760
黑箭頭

676
00:28:20,760 --> 00:28:22,740
從一個神經元的預測信息傳遞過來

677
00:28:22,740 --> 00:28:25,559
到層次結構中的下一個

678
00:28:25,559 --> 00:28:28,500
節點，每個頂點也有一個錯誤

679
00:28:28,500 --> 00:28:31,140
神經元，它將信息向上傳遞到

680
00:28:31,140 --> 00:28:32,820
層次結構，因此每個錯誤的信息

681
00:28:32,820 --> 00:28:36,480
都會傳遞到層次結構中上層的值節點

682
00:28:36,480 --> 00:28:39,120
，並基本上告訴

683
00:28:39,120 --> 00:28:41,400
它自我糾正以進行更改 預測

684
00:28:41,400 --> 00:28:43,760


685
00:28:44,700 --> 00:28:46,559
以便使用

686
00:28:46,559 --> 00:28:48,840
預測編碼執行相關性，您所要做的

687
00:28:48,840 --> 00:28:50,400
就是進行觀察並

688
00:28:50,400 --> 00:28:52,620
簡單地固定特定神經元的值，

689
00:28:52,620 --> 00:28:53,820


690
00:28:53,820 --> 00:28:55,200
因此如果您想計算給定

691
00:28:55,200 --> 00:28:58,740
X3 等於 S3 的 X4 的概率，

692
00:28:58,740 --> 00:29:02,340
我們只需： 採用 X3 並將其固定到

693
00:29:02,340 --> 00:29:04,380
S3，使其不再改變，

694
00:29:04,380 --> 00:29:08,159
並運行能量最小化，

695
00:29:08,159 --> 00:29:09,720
該模型

696
00:29:09,720 --> 00:29:12,659


697
00:29:12,659 --> 00:29:16,380
通過

698
00:29:16,380 --> 00:29:18,419
自由能變化的最小化更新軸 uh 來最小化，使模型

699
00:29:18,419 --> 00:29:20,820
收斂到一個解決方案 對於這個問題，

700
00:29:20,820 --> 00:29:22,919


701
00:29:22,919 --> 00:29:27,179
給定 X3 的 X4 的概率或期望值等於 3。

702
00:29:27,179 --> 00:29:29,340
但是我現在如何在

703
00:29:29,340 --> 00:29:31,679
不作用於圖形結構的情況下進行干預，

704
00:29:31,679 --> 00:29:33,419


705
00:29:33,419 --> 00:29:35,640
這基本上是

706
00:29:35,640 --> 00:29:37,679
本文的第一個想法

707
00:29:37,679 --> 00:29:39,960
哦，這仍然是如何 執行

708
00:29:39,960 --> 00:29:43,260
相關性，因此固定 S3 等於 X3 是

709
00:29:43,260 --> 00:29:45,600
算法的第一步，

710
00:29:45,600 --> 00:29:47,220
第二步是通過

711
00:29:47,220 --> 00:29:50,539
最小化自由能的變化來獲得軸，

712
00:29:51,240 --> 00:29:53,340
理論上

713
00:29:53,340 --> 00:29:55,200
相當於刪除那些

714
00:29:55,200 --> 00:29:56,220
箭頭

715
00:29:56,220 --> 00:29:57,659
並回答問題概率的干預

716
00:29:57,659 --> 00:29:59,279


717
00:29:59,279 --> 00:30:02,399
通過執行干預來計算 X4 的值，所以 X3

718
00:30:02,399 --> 00:30:04,860
等於 3 個命令式編碼可以

719
00:30:04,860 --> 00:30:07,080
執行如下，

720
00:30:07,080 --> 00:30:09,840
所以我將在這裡編寫算法，

721
00:30:09,840 --> 00:30:13,140
所以首先在相關性中，您將 S3

722
00:30:13,140 --> 00:30:17,039
等於 iFix X3 等於

723
00:30:17,039 --> 00:30:18,720
您得到的觀察結果

724
00:30:18,720 --> 00:30:21,299
那麼這是重要的一步，

725
00:30:21,299 --> 00:30:24,059
您必須不再乾預圖表，

726
00:30:24,059 --> 00:30:26,700
而是乾預預測誤差並將

727
00:30:26,700 --> 00:30:28,980
其修復為零，

728
00:30:28,980 --> 00:30:31,020
使預測誤差等於零

729
00:30:31,020 --> 00:30:32,480
基本上

730
00:30:32,480 --> 00:30:36,179
使呃

731
00:30:36,179 --> 00:30:38,460
向層次結構發送無意義的信息，或者實際上

732
00:30:38,460 --> 00:30:40,200
不發送任何信息 層次結構，

733
00:30:40,200 --> 00:30:41,880
因為它基本上告訴您

734
00:30:41,880 --> 00:30:44,659
預測總是正確的，

735
00:30:44,659 --> 00:30:48,120
第三步是像我們

736
00:30:48,120 --> 00:30:50,220
之前所做的那樣，通過最小化自由能的變化來更新軸

737
00:30:50,220 --> 00:30:52,919
無約束軸或 X1 X2 X4，

738
00:30:52,919 --> 00:30:55,679


739
00:30:55,679 --> 00:30:59,039
正如我現在將展示的那樣或通過

740
00:30:59,039 --> 00:31:00,840
簡單地進行實驗 將

741
00:31:00,840 --> 00:31:02,399
預測誤差設置

742
00:31:02,399 --> 00:31:05,120
為零的這個小技巧

743
00:31:05,640 --> 00:31:08,220
會阻止我們像微積分理論那樣實際對

744
00:31:08,220 --> 00:31:10,320
圖的結構採取行動，

745
00:31:10,320 --> 00:31:13,620
並

746
00:31:13,620 --> 00:31:16,919


747
00:31:16,919 --> 00:31:19,140
通過簡單地執行干預後推斷出缺失的變量

748
00:31:19,140 --> 00:31:22,640
自由能最小化的畸變

749
00:31:24,659 --> 00:31:26,580
反事實推理怎麼樣

750
00:31:26,580 --> 00:31:28,080


751
00:31:28,080 --> 00:31:30,539
一旦我們

752
00:31:30,539 --> 00:31:34,740
定義瞭如何進行干預，反事實推理實際上很容易

753
00:31:34,740 --> 00:31:36,539
，這是因為正如我們之前所看到的，在

754
00:31:36,539 --> 00:31:38,640


755
00:31:38,640 --> 00:31:40,380


756
00:31:40,380 --> 00:31:44,360
推斷出過去的情況後執行反事實類似於在過去的情況下執行干預

757
00:31:44,360 --> 00:31:48,120
不可觀察的不可觀察的變量，

758
00:31:48,120 --> 00:31:49,620
所以

759
00:31:49,620 --> 00:31:51,480
正如您在我

760
00:31:51,480 --> 00:31:53,520
之前展示的關於綁架動作和

761
00:31:53,520 --> 00:31:56,039
預測步驟的圖中看到的那樣，動作和

762
00:31:56,039 --> 00:31:58,320
預測步驟沒有這

763
00:31:58,320 --> 00:31:59,640
兩個箭頭，

764
00:31:59,640 --> 00:32:02,580
它們被刪除了漂亮的編碼

765
00:32:02,580 --> 00:32:06,299
允許我們將這個呃的箭頭保留

766
00:32:06,299 --> 00:32:08,279
在 該圖

767
00:32:08,279 --> 00:32:11,340
並通過

768
00:32:11,340 --> 00:32:13,380
簡單地執行溯因步驟來執行反事實，就像

769
00:32:13,380 --> 00:32:14,640
之前所做的那樣，在

770
00:32:14,640 --> 00:32:16,679
操作步驟中，我們只需對

771
00:32:16,679 --> 00:32:18,600
單個節點執行干預，

772
00:32:18,600 --> 00:32:21,240
因此我們修復值節點並將

773
00:32:21,240 --> 00:32:24,240
誤差設置為零

774
00:32:24,240 --> 00:32:26,399
並運行能量最小化

775
00:32:26,399 --> 00:32:27,960
最小化自由能的持續時間來

776
00:32:27,960 --> 00:32:30,679
計算預測，

777
00:32:32,399 --> 00:32:36,299
所以我認為這就像一種簡單而

778
00:32:36,299 --> 00:32:39,840
優雅的方法來執行干預

779
00:32:39,840 --> 00:32:42,899
和反事實，

780
00:32:42,899 --> 00:32:44,880
嗯，是的，所以我認為我們

781
00:32:44,880 --> 00:32:46,500
現在必須展示的是它在實踐中

782
00:32:46,500 --> 00:32:48,720
是否有效，我們 有幾個

783
00:32:48,720 --> 00:32:49,919
實驗

784
00:32:49,919 --> 00:32:52,440
，我現在將向您展示兩個

785
00:32:52,440 --> 00:32:54,240
不同的實驗，第一個實驗

786
00:32:54,240 --> 00:32:57,179
只是概念驗證實驗，

787
00:32:57,179 --> 00:33:01,020
表明在操作編碼中

788
00:33:01,020 --> 00:33:02,480
能夠執行

789
00:33:02,480 --> 00:33:06,120
干預和反事實

790
00:33:06,120 --> 00:33:08,700
，第二個實驗實際上顯示了一個

791
00:33:08,700 --> 00:33:11,220
簡單的應用程序 如何

792
00:33:11,220 --> 00:33:13,440
使用乾預查詢來提高

793
00:33:13,440 --> 00:33:16,260


794
00:33:16,260 --> 00:33:18,360
特定類型的操作編碼

795
00:33:18,360 --> 00:33:20,940
網絡（即完全

796
00:33:20,940 --> 00:33:22,080
連接模型的分類任務）的性能，讓我們

797
00:33:22,080 --> 00:33:24,659
從第一個開始，

798
00:33:24,659 --> 00:33:27,679
那麼我們如何完成這項任務，因此給定一個

799
00:33:27,679 --> 00:33:30,360
結構委員會模型，

800
00:33:30,360 --> 00:33:33,360
我們 生成訓練數據，我們用它

801
00:33:33,360 --> 00:33:35,760
來學習權重，以便學習

802
00:33:35,760 --> 00:33:39,480
結構 Kaza 模型的功能，

803
00:33:39,480 --> 00:33:42,779
然後我們

804
00:33:42,779 --> 00:33:44,399
為乾預查詢和

805
00:33:44,399 --> 00:33:46,080
反派查詢生成測試數據，

806
00:33:46,080 --> 00:33:48,000
並展示我們是否能夠

807
00:33:48,000 --> 00:33:51,360
收斂到正確的測試數據 使用

808
00:33:51,360 --> 00:33:53,340
創造性編碼

809
00:33:53,340 --> 00:33:54,779
，

810
00:33:54,779 --> 00:33:57,240
例如，這兩個圖中的 uh

811
00:33:57,240 --> 00:33:58,860
代表了該特定圖的干預

812
00:33:58,860 --> 00:34:00,600
干預和反事實查詢，該圖

813
00:34:00,600 --> 00:34:03,539
是

814
00:34:03,539 --> 00:34:05,880
蝴蝶偏差圖，該圖

815
00:34:05,880 --> 00:34:08,280
經常用於 uh 中測試

816
00:34:08,280 --> 00:34:10,859
是否存在因果推理是否

817
00:34:10,859 --> 00:34:12,179
干預和反事實

818
00:34:12,179 --> 00:34:15,540
技術 工作就這麼簡單，但

819
00:34:15,540 --> 00:34:18,000
在論文中你可以找到很多

820
00:34:18,000 --> 00:34:20,760
不同的圖表，但總的來說，這

821
00:34:20,760 --> 00:34:22,800
兩個圖表這兩個圖表明該方法

822
00:34:22,800 --> 00:34:26,940
有效，表明

823
00:34:26,940 --> 00:34:27,918


824
00:34:27,918 --> 00:34:32,219


825
00:34:32,219 --> 00:34:33,960


826
00:34:33,960 --> 00:34:37,399
我們我們的干預性反事實量之間的平均絕對誤差 計算並且原始圖中的干預量和

827
00:34:37,399 --> 00:34:39,780
反事實量

828
00:34:39,780 --> 00:34:41,460


829
00:34:41,460 --> 00:34:43,800
彼此接近，因此誤差

830
00:34:43,800 --> 00:34:45,800
相當小

831
00:34:45,800 --> 00:34:49,139
第二個實驗是呃基本上是

832
00:34:49,139 --> 00:34:51,239
我在早期論文中提出的實驗的擴展，

833
00:34:51,239 --> 00:34:54,540
該實驗是

834
00:34:54,540 --> 00:34:56,460
對任意圖拓撲的學習

835
00:34:56,460 --> 00:34:59,040
我去年

836
00:34:59,040 --> 00:35:01,080
在那篇論文中寫的，我

837
00:35:01,080 --> 00:35:04,200
基本上提出了這種

838
00:35:04,200 --> 00:35:06,060
網絡作為概念證明，它是一個

839
00:35:06,060 --> 00:35:08,160
完全連接的網絡，

840
00:35:08,160 --> 00:35:11,579
通常是您可以執行機器學習實驗的最糟糕的神經網絡，

841
00:35:11,579 --> 00:35:13,500


842
00:35:13,500 --> 00:35:15,960
因為

843
00:35:15,960 --> 00:35:20,520
給我們一個固定的 一組神經元

844
00:35:20,520 --> 00:35:23,660
基本上

845
00:35:23,760 --> 00:35:26,400
每對神經元都由

846
00:35:26,400 --> 00:35:28,680
兩個不同的突觸連接，因此

847
00:35:28,680 --> 00:35:31,200
它是最多的，

848
00:35:31,200 --> 00:35:33,359
通常是具有最高複雜性的模型，

849
00:35:33,359 --> 00:35:34,619


850
00:35:34,619 --> 00:35:36,300
好處是，由於您有

851
00:35:36,300 --> 00:35:37,859
很多循環，所以模型非常

852
00:35:37,859 --> 00:35:39,599
靈活 從某種意義上說，您可以

853
00:35:39,599 --> 00:35:42,480
在切碎的圖像、

854
00:35:42,480 --> 00:35:45,359
數據點及其標籤上訓練它，但是

855
00:35:45,359 --> 00:35:47,400
由於返回的信息，您可以查詢它的方式

856
00:35:47,400 --> 00:35:50,640
是呃，您可以

857
00:35:50,640 --> 00:35:52,140
通過很多不同的方式查詢，所以 您

858
00:35:52,140 --> 00:35:54,060
可以形成分類任務，在其中

859
00:35:54,060 --> 00:35:55,980
提供圖像並運行

860
00:35:55,980 --> 00:35:57,480
能量最小化並獲取標籤，

861
00:35:57,480 --> 00:35:59,400
但您也可以執行

862
00:35:59,400 --> 00:36:01,320
生成任務，在其中給

863
00:36:01,320 --> 00:36:03,060
標籤運行能量最小化並

864
00:36:03,060 --> 00:36:05,220
獲取您可以執行的圖像，例如

865
00:36:05,220 --> 00:36:06,960
圖像 完成，你給

866
00:36:06,960 --> 00:36:10,260
一半的圖像並收斂，然後

867
00:36:10,260 --> 00:36:12,119
收斂讓模型轉換到

868
00:36:12,119 --> 00:36:14,400
後半部分等等，所以

869
00:36:14,400 --> 00:36:16,440
它基本上是一個模型，可以完整地學習

870
00:36:16,440 --> 00:36:19,619
數據集的統計數據，而

871
00:36:19,619 --> 00:36:21,900
不是像專注於

872
00:36:21,900 --> 00:36:25,079
分類或 一般來說，

873
00:36:25,079 --> 00:36:27,900
這種靈活性很大，

874
00:36:27,900 --> 00:36:31,260
問題是，正因為如此，

875
00:36:31,260 --> 00:36:34,140
每個任務都不能很好地工作，

876
00:36:34,140 --> 00:36:35,820
所以你可以做很多不同的事情，

877
00:36:35,820 --> 00:36:38,579
但沒有一個做得很好，

878
00:36:38,579 --> 00:36:39,960


879
00:36:39,960 --> 00:36:42,480
在這裡我想展示如何使用

880
00:36:42,480 --> 00:36:44,099
介入查詢而不是

881
00:36:44,099 --> 00:36:46,740
標準的關聯查詢或

882
00:36:46,740 --> 00:36:48,119
條件查詢

883
00:36:48,119 --> 00:36:49,980
稍微改善了這些分類任務的結果，

884
00:36:49,980 --> 00:36:51,960


885
00:36:51,960 --> 00:36:54,000
那麼

886
00:36:54,000 --> 00:36:57,599


887
00:36:57,599 --> 00:37:01,079
這些任務的測試準確性不那麼高的推測原因是什麼，

888
00:37:01,079 --> 00:37:03,180
前兩個原因是模型

889
00:37:03,180 --> 00:37:05,640
分散了注意力 在糾正每一個

890
00:37:05,640 --> 00:37:07,920
呃每一個錯誤時，基本上你

891
00:37:07,920 --> 00:37:09,420
呈現一個圖像，你想

892
00:37:09,420 --> 00:37:11,579
獲得一個標籤，但模型實際上正在

893
00:37:11,579 --> 00:37:13,859
更新自身以預測

894
00:37:13,859 --> 00:37:16,320
圖像中的錯誤

895
00:37:16,320 --> 00:37:18,480
，第二個原因

896
00:37:18,480 --> 00:37:21,119
是我所說的 結構太

897
00:37:21,119 --> 00:37:24,540
複雜了，所以從

898
00:37:24,540 --> 00:37:27,079
葡萄乾奧卡姆剃刀

899
00:37:27,079 --> 00:37:28,800
論證的結果來看，

900
00:37:28,800 --> 00:37:30,720
這是你能擁有的最糟糕的模型，所以

901
00:37:30,720 --> 00:37:32,160
每次你有一個適合

902
00:37:32,160 --> 00:37:33,960
數據集的模型時，該模型都會

903
00:37:33,960 --> 00:37:35,579
比正在運行的模型複雜。 是

904
00:37:35,579 --> 00:37:37,560
首選，

905
00:37:37,560 --> 00:37:40,560
但總的來說，呃只是為了研究

906
00:37:40,560 --> 00:37:41,400
它，這個

907
00:37:41,400 --> 00:37:43,380
想法是可以在這個模型中查詢干預

908
00:37:43,380 --> 00:37:44,820
措施，用於提高

909
00:37:44,820 --> 00:37:46,859
那些完全

910
00:37:46,859 --> 00:37:48,599
連接的模型的性能，

911
00:37:48,599 --> 00:37:51,060
答案是肯定的，

912
00:37:51,060 --> 00:37:53,160
所以這是我如何執行干預

913
00:37:53,160 --> 00:37:55,619
查詢，所以 我向網絡呈現一幅圖像，

914
00:37:55,619 --> 00:37:56,640


915
00:37:56,640 --> 00:37:59,460
我將像素的誤差修正

916
00:37:59,460 --> 00:38:01,560
為 0，這樣該誤差就不會

917
00:38:01,560 --> 00:38:03,180
在網絡中傳播，

918
00:38:03,180 --> 00:38:05,700
然後我計算標籤

919
00:38:05,700 --> 00:38:08,400
，如您所見，準確度有所提高，

920
00:38:08,400 --> 00:38:11,339
例如使用 89

921
00:38:11,339 --> 00:38:13,380
創意編碼網絡的標準查詢方法

922
00:38:13,380 --> 00:38:16,800
到 92，這是乾預後的準確性，

923
00:38:16,800 --> 00:38:19,020


924
00:38:19,020 --> 00:38:21,540
對於時尚手段也是如此，

925
00:38:21,540 --> 00:38:24,420
我認為一個非常合法的批評家，

926
00:38:24,420 --> 00:38:26,940
可能每個人在

927
00:38:26,940 --> 00:38:28,920
看到這些圖時都會想，可以，你可以

928
00:38:28,920 --> 00:38:32,099
從 89 的手段改進 到 92

929
00:38:32,099 --> 00:38:36,180
基本上仍然很糟糕，是的，這是真的，

930
00:38:36,180 --> 00:38:38,400
我實際上在後面的幻燈片中，我

931
00:38:38,400 --> 00:38:40,619
將展示如何對

932
00:38:40,619 --> 00:38:42,660
這個全

933
00:38:42,660 --> 00:38:43,859
連接模型的結構採取行動，這

934
00:38:43,859 --> 00:38:46,500
將進一步改善結果，直到

935
00:38:46,500 --> 00:38:48,480
他們達到 豐富的呃

936
00:38:48,480 --> 00:38:50,820
性能當然還沒有接近最

937
00:38:50,820 --> 00:38:52,560
先進的性能，

938
00:38:52,560 --> 00:38:55,320
但它仍然在上升，但還沒有達到

939
00:38:55,320 --> 00:38:57,380
基本上可以接受的水平

940
00:38:57,380 --> 00:39:01,760
肯沃斯調查調查

941
00:39:02,040 --> 00:39:04,980
所以是的所以這是關於使用

942
00:39:04,980 --> 00:39:08,400
創造性編碼進行因果推理的部分

943
00:39:08,400 --> 00:39:10,920
我想總結一下，我可以說，

944
00:39:10,920 --> 00:39:15,060


945
00:39:15,060 --> 00:39:17,640
我剛剛展示的結果中有趣的部分是，

946
00:39:17,640 --> 00:39:19,859
我展示了操作編碼能夠以

947
00:39:19,859 --> 00:39:22,560
非常簡單

948
00:39:22,560 --> 00:39:24,780
和直觀的方式執行干預，因為你不必

949
00:39:24,780 --> 00:39:26,280
採取行動 舊圖的結構

950
00:39:26,280 --> 00:39:28,740
有時這些

951
00:39:28,740 --> 00:39:31,079
函數不可用

952
00:39:31,079 --> 00:39:34,020
等等，但你只需要

953
00:39:34,020 --> 00:39:36,140


954
00:39:36,140 --> 00:39:39,780
干預單個神經元研究

955
00:39:39,780 --> 00:39:41,640
預測誤差為零

956
00:39:41,640 --> 00:39:44,220
並執行能量最小化

957
00:39:44,220 --> 00:39:46,619
過程，

958
00:39:46,619 --> 00:39:49,200
這些擴展是 允許我們

959
00:39:49,200 --> 00:39:51,240
定義基於創造性編碼的結構

960
00:39:51,240 --> 00:39:52,920
因果模型

961
00:39:52,920 --> 00:39:54,920
現在我們進入

962
00:39:54,920 --> 00:39:57,900
工作的第二部分，這是關於

963
00:39:57,900 --> 00:40:01,700
結構結構學習的，

964
00:40:02,000 --> 00:40:05,099
所以正如我所說的指令學習

965
00:40:05,099 --> 00:40:07,260
處理從觀察數據中學習模型

966
00:40:07,260 --> 00:40:09,720
的因果結構的問題，

967
00:40:09,720 --> 00:40:11,880


968
00:40:11,880 --> 00:40:13,800
這是 實際上，沒有一個問題已經

969
00:40:13,800 --> 00:40:17,760
存在了幾十年，

970
00:40:17,760 --> 00:40:21,359
而且直到

971
00:40:21,359 --> 00:40:24,000
幾年前才一直使用組合

972
00:40:24,000 --> 00:40:25,560
搜索方法來解決

973
00:40:25,560 --> 00:40:26,640
這些社區

974
00:40:26,640 --> 00:40:29,280
研究方法的問題在於，它們的

975
00:40:29,280 --> 00:40:32,880
複雜性呈雙倍指數增長，

976
00:40:32,880 --> 00:40:34,740
因此一旦數據變成

977
00:40:34,740 --> 00:40:36,780
多倍， 維度和

978
00:40:36,780 --> 00:40:39,920
你想要學習的 Bison 圖的

979
00:40:39,920 --> 00:40:42,300
大小會增長，學習

980
00:40:42,300 --> 00:40:46,680
它的速度非常慢，

981
00:40:46,680 --> 00:40:48,780


982
00:40:48,780 --> 00:40:51,000
幾年前在 2018 年的一份新報紙上

983
00:40:51,000 --> 00:40:53,540


984
00:40:53,839 --> 00:40:55,920
實際上出現的新解決方案表明，可以在

985
00:40:55,920 --> 00:40:57,900
不使用

986
00:40:57,900 --> 00:40:59,940
組合器研究方法，但通過使用

987
00:40:59,940 --> 00:41:01,619
基於梯度的方法，

988
00:41:01,619 --> 00:41:05,280
這基本上是

989
00:41:05,280 --> 00:41:07,320
一般的技術問題，因為現在

990
00:41:07,320 --> 00:41:08,820
你可以簡單地

991
00:41:08,820 --> 00:41:10,980
對參數進行先驗，這是

992
00:41:10,980 --> 00:41:12,420
我要

993
00:41:12,420 --> 00:41:14,700
定義得更好一點的優先目的 在這張幻燈片中

994
00:41:14,700 --> 00:41:15,599


995
00:41:15,599 --> 00:41:18,180
運行梯度下降，即使你

996
00:41:18,180 --> 00:41:19,740
有一個雙倍三倍的模型，

997
00:41:19,740 --> 00:41:20,820
大小

998
00:41:20,820 --> 00:41:23,640
是呃，算法仍然非常

999
00:41:23,640 --> 00:41:25,440
快，

1000
00:41:25,440 --> 00:41:28,260
因此這篇論文是一個

1001
00:41:28,260 --> 00:41:31,200
這是是的，我認為它是一種新的

1002
00:41:31,200 --> 00:41:33,180
，我認為已經有了 600 次

1003
00:41:33,180 --> 00:41:35,099
引用或諸如此類的事情，

1004
00:41:35,099 --> 00:41:37,140
我現在看到的每一篇

1005
00:41:37,140 --> 00:41:38,720
關於諮詢朋友和學習

1006
00:41:38,720 --> 00:41:42,000
圖表結構的論文都使用他們的方法，

1007
00:41:42,000 --> 00:41:44,820
它只是改變了一點，

1008
00:41:44,820 --> 00:41:46,980
他們發現更快或稍微更好的

1009
00:41:46,980 --> 00:41:49,440
推理方法，但他們仍然都使用

1010
00:41:49,440 --> 00:41:53,760
在本文定義之前，我

1011
00:41:53,760 --> 00:41:56,460
也這樣做，我們也這樣做，

1012
00:41:56,460 --> 00:41:58,859
所以在這裡我們會找到一個新的量，

1013
00:41:58,859 --> 00:42:01,500
即代理矩陣，代理

1014
00:42:01,500 --> 00:42:03,480
矩陣只是一個對

1015
00:42:03,480 --> 00:42:06,359
模型連接進行編碼的矩陣，因此它是一個

1016
00:42:06,359 --> 00:42:08,520
二進制矩陣，

1017
00:42:08,520 --> 00:42:10,920
一般來說 是一個二元矩陣，那麼

1018
00:42:10,920 --> 00:42:12,180
當然，當您進行基於梯度的

1019
00:42:12,180 --> 00:42:14,880
優化時，您可以使其連續

1020
00:42:14,880 --> 00:42:16,800
，然後在某個點上有一些閾值，該閾值

1021
00:42:16,800 --> 00:42:19,800
基本上會殺死邊緣或將

1022
00:42:19,800 --> 00:42:21,480
其設置為 1，

1023
00:42:21,480 --> 00:42:27,780
並且如果我們有，則 M3 IJ 等於 1

1024
00:42:27,780 --> 00:42:30,540
如果貝葉斯圖是

1025
00:42:30,540 --> 00:42:35,040
從頂點 I 到頂點 J 的邊，否則為零，

1026
00:42:35,040 --> 00:42:37,380
例如，

1027
00:42:37,380 --> 00:42:39,540
這裡的代理矩陣代表了

1028
00:42:39,540 --> 00:42:42,780
該視覺網絡的連接結構

1029
00:42:42,780 --> 00:42:44,040
，

1030
00:42:44,040 --> 00:42:46,079
基本上，該方法

1031
00:42:46,079 --> 00:42:48,780
解決了我們想要了解的兩個

1032
00:42:48,780 --> 00:42:51,000
關於學習

1033
00:42:51,000 --> 00:42:53,460
結構的問題 網絡方程的

1034
00:42:53,460 --> 00:42:54,780
想法是，我們從一個完全

1035
00:42:54,780 --> 00:42:57,200
連接的模型開始，這個模型在

1036
00:42:57,200 --> 00:43:00,240
概念上是相似的，實際上

1037
00:43:00,240 --> 00:43:02,220
相當於

1038
00:43:02,220 --> 00:43:04,020
我之前定義的操作編碼網絡，它是完全

1039
00:43:04,020 --> 00:43:06,480
連接的，所以你有很多

1040
00:43:06,480 --> 00:43:08,640
頂點和每對頂點

1041
00:43:08,640 --> 00:43:10,920
由兩個不同的邊通過呃連接

1042
00:43:10,920 --> 00:43:13,319
，您只想修剪不需要的邊，

1043
00:43:13,319 --> 00:43:15,780


1044
00:43:15,780 --> 00:43:18,540
因此它可以被視為一種

1045
00:43:18,540 --> 00:43:20,819
執行模型縮減的方法，您從

1046
00:43:20,819 --> 00:43:22,020
一個大模型開始，並且想要使其變

1047
00:43:22,020 --> 00:43:22,800
小，

1048
00:43:22,800 --> 00:43:25,800
那麼什麼是 減少模型的第一個要素

1049
00:43:25,800 --> 00:43:28,260
當然是稀疏

1050
00:43:28,260 --> 00:43:29,220
城市，

1051
00:43:29,220 --> 00:43:31,619
每個人用來

1052
00:43:31,619 --> 00:43:33,839
使模型更加稀疏的先驗是

1053
00:43:33,839 --> 00:43:36,480
拉普拉斯先驗，在機器學習中

1054
00:43:36,480 --> 00:43:38,880
簡稱為 L1 範數，

1055
00:43:38,880 --> 00:43:40,920
這裡定義的解決

1056
00:43:40,920 --> 00:43:43,980
方案是 我

1057
00:43:43,980 --> 00:43:46,740
之前提到的這篇論文提出的是

1058
00:43:46,740 --> 00:43:49,319
在上面添加第二個先驗，這

1059
00:43:49,319 --> 00:43:53,359
可能是貝葉斯網絡最大的呃

1060
00:43:53,359 --> 00:43:55,980
特徵，

1061
00:43:55,980 --> 00:43:57,780
你想要在其上執行因果

1062
00:43:57,780 --> 00:43:59,819
推理，你希望它們是

1063
00:43:59,819 --> 00:44:01,020
循環的

1064
00:44:01,020 --> 00:44:03,000
，基本上它們顯示了 非

1065
00:44:03,000 --> 00:44:06,359
循環性可以作為先驗強加在代理

1066
00:44:06,359 --> 00:44:08,160
矩陣上，

1067
00:44:08,160 --> 00:44:10,859
它在這裡有這個形狀，所以

1068
00:44:10,859 --> 00:44:14,640
它是矩陣的跡，是a乘以

1069
00:44:14,640 --> 00:44:18,420
a的指數，

1070
00:44:18,420 --> 00:44:21,859
其中a又是代理矩陣，

1071
00:44:21,859 --> 00:44:24,300
基本上這個量

1072
00:44:24,300 --> 00:44:27,900
是 當且僅當

1073
00:44:27,900 --> 00:44:30,480
貝葉斯網絡或

1074
00:44:30,480 --> 00:44:32,819
您正在考慮的任何圖形

1075
00:44:32,819 --> 00:44:35,720
是 c 單擊時才等於零，

1076
00:44:37,619 --> 00:44:40,260
所以我將在一些實驗中使用這些，

1077
00:44:40,260 --> 00:44:42,960
以便將這兩個強制

1078
00:44:42,960 --> 00:44:45,660
在不同類型上的這兩個先驗

1079
00:44:45,660 --> 00:44:47,520


1080
00:44:47,520 --> 00:44:49,200
我正在嘗試將它們與

1081
00:44:49,200 --> 00:44:51,540
我們之前提出的有關

1082
00:44:51,540 --> 00:44:52,740
執行因果推理和操作

1083
00:44:52,740 --> 00:44:55,020
編碼的技術合併，

1084
00:44:55,020 --> 00:44:56,520
因此我將提出兩個不同的

1085
00:44:56,520 --> 00:44:59,640
實驗，因此一個是

1086
00:44:59,640 --> 00:45:00,960
概念證明，這是

1087
00:45:00,960 --> 00:45:03,660
中所示的標準實驗 所有的結構

1088
00:45:03,660 --> 00:45:06,599
學習任務都是

1089
00:45:06,599 --> 00:45:08,880
從數據中推斷出正確的貝葉斯網絡，

1090
00:45:08,880 --> 00:45:11,760
然後我將在我之前展示的分類實驗的基礎上進行構建，

1091
00:45:11,760 --> 00:45:13,500


1092
00:45:13,500 --> 00:45:14,280


1093
00:45:14,280 --> 00:45:16,020


1094
00:45:16,020 --> 00:45:18,540
並展示這些先驗實際上如何讓

1095
00:45:18,540 --> 00:45:21,060
我能夠改進

1096
00:45:21,060 --> 00:45:22,500
分類 準確性

1097
00:45:22,500 --> 00:45:25,500
全連接預測編碼模型的測試準確性，

1098
00:45:25,500 --> 00:45:28,160


1099
00:45:29,520 --> 00:45:31,680
所以讓我們開始第一個實驗，即

1100
00:45:31,680 --> 00:45:33,300
推斷圖的結構，

1101
00:45:33,300 --> 00:45:34,980


1102
00:45:34,980 --> 00:45:37,319
所有實驗都遵循該

1103
00:45:37,319 --> 00:45:39,480


1104
00:45:39,480 --> 00:45:42,060
領域所有論文中基本相同的流程，第一步是

1105
00:45:42,060 --> 00:45:45,119
生成 隨機圖的視覺網絡，

1106
00:45:45,119 --> 00:45:46,079


1107
00:45:46,079 --> 00:45:48,359
所以基本上通常

1108
00:45:48,359 --> 00:45:50,640
每個人測試的兩個隨機圖都是 Erdos

1109
00:45:50,640 --> 00:45:53,520
renegraph 和無比例圖，

1110
00:45:53,520 --> 00:45:55,859
這樣你就可以生成那些大圖，

1111
00:45:55,859 --> 00:45:58,680
通常有 20 個 80 80 個

1112
00:45:58,680 --> 00:46:01,619
不同的節點和一些你

1113
00:46:01,619 --> 00:46:04,619
隨機採樣的邊

1114
00:46:04,619 --> 00:46:06,540
你使用這個圖來生成一個

1115
00:46:06,540 --> 00:46:08,280
數據集，

1116
00:46:08,280 --> 00:46:10,819
這樣你就可以對

1117
00:46:10,819 --> 00:46:14,460
n個大N個數據點進行採樣，你所做的

1118
00:46:14,460 --> 00:46:16,859
就是你獲取他們

1119
00:46:16,859 --> 00:46:18,780
之前生成的圖，然後將其

1120
00:46:18,780 --> 00:46:20,819
扔掉，你只保留數據集

1121
00:46:20,819 --> 00:46:23,099
和你的任務 現在想要解決的是要

1122
00:46:23,099 --> 00:46:25,020
學習的

1123
00:46:25,020 --> 00:46:27,420
是有一個訓練算法，

1124
00:46:27,420 --> 00:46:29,819
基本上可以讓你

1125
00:46:29,819 --> 00:46:32,579
檢索

1126
00:46:32,579 --> 00:46:34,619
你扔掉的圖的結構，

1127
00:46:34,619 --> 00:46:36,839
所以我們在這裡做的方式是我們

1128
00:46:36,839 --> 00:46:38,460
處於一個完全連接的創意編碼中

1129
00:46:38,460 --> 00:46:41,760
使用

1130
00:46:41,760 --> 00:46:43,800
我們之前定義的稀疏和 SQL 先驗對此數據集 D 進行模型，

1131
00:46:43,800 --> 00:46:45,359


1132
00:46:45,359 --> 00:46:48,780
並查看在

1133
00:46:48,780 --> 00:46:50,760


1134
00:46:50,760 --> 00:46:53,220
修剪掉小於某個閾值

1135
00:46:53,220 --> 00:46:55,319
的機構矩陣的條目後我們收斂到的圖是否實際上

1136
00:46:55,319 --> 00:46:57,599


1137
00:46:57,599 --> 00:47:00,060
類似於 初始圖的情況，

1138
00:47:00,060 --> 00:47:02,359


1139
00:47:02,520 --> 00:47:04,500
也表明

1140
00:47:04,500 --> 00:47:06,599
實際上是這樣，所以這是一個例子，

1141
00:47:06,599 --> 00:47:09,020
我展示了許多不同的

1142
00:47:09,020 --> 00:47:12,420
參數化和維度

1143
00:47:12,420 --> 00:47:15,060
以及論文中類似的內容，

1144
00:47:15,060 --> 00:47:16,920
但我認為這兩個是最具

1145
00:47:16,920 --> 00:47:18,900
代表性的例子 帶有錯誤

1146
00:47:18,900 --> 00:47:20,760
苗圃圖和

1147
00:47:20,760 --> 00:47:23,579
具有 20 個節點的自由比例圖

1148
00:47:23,579 --> 00:47:25,800
，在左側，您可以通過圖看到地面，

1149
00:47:25,800 --> 00:47:27,300
這是隨機採樣的圖

1150
00:47:27,300 --> 00:47:29,339


1151
00:47:29,339 --> 00:47:30,839


1152
00:47:30,839 --> 00:47:32,599
，在右側，您可以看到

1153
00:47:32,599 --> 00:47:35,220


1154
00:47:35,220 --> 00:47:37,440
從數據中學習到的相當困難的模型

1155
00:47:37,440 --> 00:47:39,359
正如你所看到的，它們非常

1156
00:47:39,359 --> 00:47:40,500
相似，但

1157
00:47:40,500 --> 00:47:42,780
仍然不完美，所以有

1158
00:47:42,780 --> 00:47:45,000
一些錯誤，但

1159
00:47:45,000 --> 00:47:47,460
總的來說，結構

1160
00:47:47,460 --> 00:47:49,500
很好，我們也有一些

1161
00:47:49,500 --> 00:47:52,140
定量實驗，但

1162
00:47:52,140 --> 00:47:54,000
我沒有在這裡展示 因為它們

1163
00:47:54,000 --> 00:47:55,740
只是帶有大量數字的巨大表格，

1164
00:47:55,740 --> 00:47:57,180
我認為這

1165
00:47:57,180 --> 00:48:00,660
對於演示來說可能有點太多了，但

1166
00:48:00,660 --> 00:48:02,220
結果表明它們的表現

1167
00:48:02,220 --> 00:48:06,060
與當代方法類似，

1168
00:48:06,060 --> 00:48:07,920
因為我不得不說，就像大多數

1169
00:48:07,920 --> 00:48:10,859
質量一樣 來自

1170
00:48:10,859 --> 00:48:15,799
2018 年推出的 acigliprior。

1171
00:48:16,920 --> 00:48:19,680
第二類實驗是

1172
00:48:19,680 --> 00:48:21,599
我們的分類實驗，正如

1173
00:48:21,599 --> 00:48:23,880
我所說，它是

1174
00:48:23,880 --> 00:48:25,560
我之前分享的實驗的擴展

1175
00:48:25,560 --> 00:48:27,119
，其想法是使用結構

1176
00:48:27,119 --> 00:48:28,560
學習來改進分類

1177
00:48:28,560 --> 00:48:31,140
關於

1178
00:48:31,140 --> 00:48:33,420
均值和時尚均值數據集的分類結果，

1179
00:48:33,420 --> 00:48:36,780
從完全連接的圖開始，

1180
00:48:36,780 --> 00:48:40,560
所以我所做的就是劃分

1181
00:48:40,560 --> 00:48:42,839


1182
00:48:42,839 --> 00:48:46,440
神經元的完全連接的圖形簇，因此 1B 簇

1183
00:48:46,440 --> 00:48:49,140
是與輸入相關的簇，

1184
00:48:49,140 --> 00:48:51,900
然後是所有小的簇 我們有一些

1185
00:48:51,900 --> 00:48:55,319
特定數量的隱藏簇，

1186
00:48:55,319 --> 00:48:57,720
然後我們有標籤簇，它

1187
00:48:57,720 --> 00:48:58,800
是

1188
00:48:58,800 --> 00:49:01,560


1189
00:49:01,560 --> 00:49:04,079
應該給我標籤

1190
00:49:04,079 --> 00:49:06,480
預測的神經元簇的類

1191
00:49:06,480 --> 00:49:08,700
，我已經訓練它們用於

1192
00:49:08,700 --> 00:49:10,980
第一次使用稀疏先驗 只是這樣的

1193
00:49:10,980 --> 00:49:14,099
想法是，如果我

1194
00:49:14,099 --> 00:49:16,500
從模型中修剪不需要的連接，

1195
00:49:16,500 --> 00:49:17,460


1196
00:49:17,460 --> 00:49:20,880
並學習解析器模型，

1197
00:49:20,880 --> 00:49:24,119
這是否能很好地工作，答案是否定的，它

1198
00:49:24,119 --> 00:49:25,500
不起作用，

1199
00:49:25,500 --> 00:49:28,500
原因是 最後，您

1200
00:49:28,500 --> 00:49:30,660
收斂的圖實際上是

1201
00:49:30,660 --> 00:49:32,700
生成的，因此基本上模型

1202
00:49:32,700 --> 00:49:36,180
學習根據標籤本身來預測標籤，

1203
00:49:36,180 --> 00:49:38,400
因此它會丟棄

1204
00:49:38,400 --> 00:49:40,020
輸入中的所有信息

1205
00:49:40,020 --> 00:49:42,480
，只保留標籤，正如您

1206
00:49:42,480 --> 00:49:45,119
在此處看到的 標籤 y 預測自身或

1207
00:49:45,119 --> 00:49:46,560
在其他實驗中，當您更改

1208
00:49:46,560 --> 00:49:48,960
參數時，y 預測

1209
00:49:48,960 --> 00:49:52,520
為零，在 X1 之前，再次預測 y

1210
00:49:52,520 --> 00:49:55,980
那麼這個問題的解決方案是什麼

1211
00:49:55,980 --> 00:49:57,240
這個問題的解決方案

1212
00:49:57,240 --> 00:49:59,520
是我們必須

1213
00:49:59,520 --> 00:50:03,000
收斂 到非循環圖

1214
00:50:03,000 --> 00:50:05,220
，所以我們必須添加一些東西來

1215
00:50:05,220 --> 00:50:08,000
防止循環，這

1216
00:50:08,000 --> 00:50:10,200
當然是我已經

1217
00:50:10,200 --> 00:50:12,780
提出的，然後我展示了第二種

1218
00:50:12,780 --> 00:50:14,520
技術，

1219
00:50:14,520 --> 00:50:17,280
所以第一個技術使用

1220
00:50:17,280 --> 00:50:18,680
之前定義的 SQL 先驗

1221
00:50:18,680 --> 00:50:21,359
，第二個技術是 a 是一種新穎的

1222
00:50:21,359 --> 00:50:22,859
技術，實際上利用了

1223
00:50:22,859 --> 00:50:24,359
反面例子，

1224
00:50:24,359 --> 00:50:26,520
所以在這種情況下，反面例子

1225
00:50:26,520 --> 00:50:30,060
就是一個數據點，

1226
00:50:30,060 --> 00:50:32,280
其中你有一個圖像，但標籤是

1227
00:50:32,280 --> 00:50:33,240
錯誤的，

1228
00:50:33,240 --> 00:50:35,220
所以這裡例如你有一個七的圖像

1229
00:50:35,220 --> 00:50:36,900
但我給

1230
00:50:36,900 --> 00:50:39,599
模型的標籤是二，

1231
00:50:39,599 --> 00:50:40,980


1232
00:50:40,980 --> 00:50:44,579
而且這個想法非常簡單，已經

1233
00:50:44,579 --> 00:50:47,460
在很多呃作品中使用了，

1234
00:50:47,460 --> 00:50:49,740
所以每次模型是一個正

1235
00:50:49,740 --> 00:50:52,079
例時，它都必須增加到以

1236
00:50:52,079 --> 00:50:53,520
最小化變化 自由能

1237
00:50:53,520 --> 00:50:56,520
，每次它有 a 時，它都是一個反面

1238
00:50:56,520 --> 00:50:58,859
例子，它必須增加它，

1239
00:50:58,859 --> 00:51:01,260
所以讓我繼續討論將這個

1240
00:51:01,260 --> 00:51:04,200
量最小化的錯誤，

1241
00:51:04,200 --> 00:51:05,960


1242
00:51:05,960 --> 00:51:08,579
通過大量的實驗和大量

1243
00:51:08,579 --> 00:51:10,859
的實驗，我們看到這

1244
00:51:10,859 --> 00:51:12,119
兩種技術

1245
00:51:12,119 --> 00:51:15,000
基本上，首先會導致相同的

1246
00:51:15,000 --> 00:51:17,220
結果，第二次也會導致相同的

1247
00:51:17,220 --> 00:51:18,599
圖表，

1248
00:51:18,599 --> 00:51:21,000
所以這裡是

1249
00:51:21,000 --> 00:51:22,800
新結果，一些方法和

1250
00:51:22,800 --> 00:51:25,079
時尚方法使用

1251
00:51:25,079 --> 00:51:27,660
我剛剛提出的兩種技術，

1252
00:51:27,660 --> 00:51:30,960
現在我們轉向一些

1253
00:51:30,960 --> 00:51:33,900
仍然不是很好，但絕對更

1254
00:51:33,900 --> 00:51:36,000
合理的測試精度，所以這裡我們的

1255
00:51:36,000 --> 00:51:39,059
分鐘測試誤差為 3.17，

1256
00:51:39,059 --> 00:51:42,119
時尚平均值的測試誤差為 13.98

1257
00:51:42,119 --> 00:51:44,819
，實際上這些結果

1258
00:51:44,819 --> 00:51:48,300
可以通過學習

1259
00:51:48,300 --> 00:51:51,300
圖表的結構來大大改善 切碎

1260
00:51:51,300 --> 00:51:53,040
，然後修復圖的結構

1261
00:51:53,040 --> 00:51:55,319
，並進行某種形式的

1262
00:51:55,319 --> 00:51:57,660
微調，因此，如果您

1263
00:51:57,660 --> 00:52:00,000
在某個時刻在正確的分層結構上微調模型，

1264
00:52:00,000 --> 00:52:01,980
您將達到測試精度，

1265
00:52:01,980 --> 00:52:03,359
這是您期望從

1266
00:52:03,359 --> 00:52:05,460
分層模型中獲得的測試精度，但是那些 那些

1267
00:52:05,460 --> 00:52:08,099
只是完全連接模型

1268
00:52:08,099 --> 00:52:10,980
自然收斂到的模型，

1269
00:52:10,980 --> 00:52:13,859
例如，

1270
00:52:13,859 --> 00:52:15,420


1271
00:52:15,420 --> 00:52:17,339


1272
00:52:17,339 --> 00:52:20,359
通過簡單地執行

1273
00:52:20,359 --> 00:52:22,859
相關性或條件查詢（

1274
00:52:22,859 --> 00:52:24,420
這是查詢

1275
00:52:24,420 --> 00:52:26,520
操作編碼模型的標準方式），在時尚手段上訓練完全連接模型的測試誤差為 18.32

1276
00:52:26,520 --> 00:52:29,220
干預和 AC 點擊

1277
00:52:29,220 --> 00:52:32,040
先驗一起使

1278
00:52:32,040 --> 00:52:34,200
這個測試誤差大大降低

1279
00:52:34,200 --> 00:52:37,200
，我們也可以觀察它的平均值，

1280
00:52:37,200 --> 00:52:39,319


1281
00:52:39,780 --> 00:52:41,819
我不會詳細介紹

1282
00:52:41,819 --> 00:52:45,420
最後一個實驗以及非

1283
00:52:45,420 --> 00:52:48,660
循環先驗如何作用於

1284
00:52:48,660 --> 00:52:50,339
圖表的結構，

1285
00:52:50,339 --> 00:52:52,440
所以我執行我在

1286
00:52:52,440 --> 00:52:54,960
新數據集上進行實驗，這就是我的意思是

1287
00:52:54,960 --> 00:52:56,460
調用新數據集，這可能

1288
00:52:56,460 --> 00:52:58,500
太多了，我將其稱為兩個均值

1289
00:52:58,500 --> 00:53:01,440
數據集，其中您有輸入

1290
00:53:01,440 --> 00:53:04,319
點 由兩個不同的

1291
00:53:04,319 --> 00:53:07,319
圖像組成，標籤僅取決於

1292
00:53:07,319 --> 00:53:08,520


1293
00:53:08,520 --> 00:53:10,800
第一個圖像故事上的第二個圖像，

1294
00:53:10,800 --> 00:53:12,720
因此這裡的想法

1295
00:53:12,720 --> 00:53:15,079
是模型的結構、

1296
00:53:15,079 --> 00:53:18,540
循環先驗以及類似的東西

1297
00:53:18,540 --> 00:53:20,819
能夠識別

1298
00:53:20,819 --> 00:53:23,400
圖像的後半部分是 實際上，

1299
00:53:23,400 --> 00:53:27,960
在學習中

1300
00:53:27,960 --> 00:53:31,140
執行分類時，

1301
00:53:31,140 --> 00:53:33,119
訓練通常是如何表現的，

1302
00:53:33,119 --> 00:53:36,480
例如我們有這個輸入

1303
00:53:36,480 --> 00:53:39,000
節點輸出節點，只有節點

1304
00:53:39,000 --> 00:53:41,940
完全連接，並且模型

1305
00:53:41,940 --> 00:53:43,740
收斂到

1306
00:53:43,740 --> 00:53:45,900
一個層次結構，即

1307
00:53:45,900 --> 00:53:48,960
我們知道在分類任務上表現最好的一個

1308
00:53:48,960 --> 00:53:50,880


1309
00:53:50,880 --> 00:53:53,520
是

1310
00:53:53,520 --> 00:53:54,980
訓練方法

1311
00:53:54,980 --> 00:53:59,280
運行的一個例子，所以在 c0 處，這是訓練的開始，

1312
00:53:59,280 --> 00:54:00,720


1313
00:54:00,720 --> 00:54:03,000
我們在這裡有這個模型，所以 s0

1314
00:54:03,000 --> 00:54:05,819
對應於 7，所以對應於

1315
00:54:05,819 --> 00:54:08,099
第一張圖像 因為 1

1316
00:54:08,099 --> 00:54:09,839
再次對應於七列圖像，我們有

1317
00:54:09,839 --> 00:54:12,300
標籤 Y 和所有潛在變量 x0

1318
00:54:12,300 --> 00:54:13,800
X1 X2

1319
00:54:13,800 --> 00:54:15,720
並且模型是完全連接的，因此

1320
00:54:15,720 --> 00:54:17,040
代理矩陣

1321
00:54:17,040 --> 00:54:20,579
充滿了 1 沒有零，我們

1322
00:54:20,579 --> 00:54:23,720
有自循環和類似的東西

1323
00:54:23,720 --> 00:54:27,319
幾個紀元的模型，直到

1324
00:54:27,319 --> 00:54:30,540
我們立即知道的是，

1325
00:54:30,540 --> 00:54:31,920
例如，模型立即

1326
00:54:31,920 --> 00:54:34,740
理解不需要四個紀元

1327
00:54:34,740 --> 00:54:36,839
來執行分類，因此它不會

1328
00:54:36,839 --> 00:54:40,740
呃，所以來自

1329
00:54:40,740 --> 00:54:43,980
第二個輸入集群的每個傳出節點都被刪除

1330
00:54:43,980 --> 00:54:45,900
並且 我們不明白的

1331
00:54:45,900 --> 00:54:48,660
是，這個簇是與

1332
00:54:48,660 --> 00:54:50,400
輸出相關的簇，

1333
00:54:50,400 --> 00:54:52,260
所以

1334
00:54:52,260 --> 00:54:55,319
我們有一個直接從 s0 到 Y 的線性映射，這

1335
00:54:55,319 --> 00:54:56,480


1336
00:54:56,480 --> 00:54:59,339
就是這裡的一部分，

1337
00:54:59,339 --> 00:55:01,160
但我們知道實際上線性映射

1338
00:55:01,160 --> 00:55:04,740
並不是最好的 用於

1339
00:55:04,740 --> 00:55:07,200
執行均值分類的映射，因此

1340
00:55:07,200 --> 00:55:08,700
我們需要一些層次結構，我們需要一些

1341
00:55:08,700 --> 00:55:11,579
深度來改進結果，正如

1342
00:55:11,579 --> 00:55:14,220
您所看到的，這裡的這一行是

1343
00:55:14,220 --> 00:55:15,599
準確度，

1344
00:55:15,599 --> 00:55:18,960
到目前為止，C2 與嗯

1345
00:55:18,960 --> 00:55:22,500
相似，所以它是 91 這比

1346
00:55:22,500 --> 00:55:24,059
線性分類稍微好一點，

1347
00:55:24,059 --> 00:55:25,500


1348
00:55:25,500 --> 00:55:28,740
但是一旦你繼續訓練，

1349
00:55:28,740 --> 00:55:30,660
模型就會明白它需要一些

1350
00:55:30,660 --> 00:55:33,119
層次結構來更好地擬合數據，

1351
00:55:33,119 --> 00:55:35,640
所以你會看到這個箭頭

1352
00:55:35,640 --> 00:55:38,760
隨著時間的推移開始變得越來越強，

1353
00:55:38,760 --> 00:55:41,700
直到它理解線性分類

1354
00:55:41,700 --> 00:55:44,339
map 實際上並不是真正需要的，它會

1355
00:55:44,339 --> 00:55:45,920
刪除它，

1356
00:55:45,920 --> 00:55:48,780
因此您收斂的模型是一個

1357
00:55:48,780 --> 00:55:51,000
從零開始到

1358
00:55:51,000 --> 00:55:53,760
隱藏節點的模型，然後轉到

1359
00:55:53,760 --> 00:55:57,180
具有非常弱的線性映射的標籤，

1360
00:55:57,180 --> 00:55:59,700
如果您滿足以下條件，該模型實際上會被刪除：

1361
00:55:59,700 --> 00:56:02,760
你設置一個閾值呃，如果賣家

1362
00:56:02,760 --> 00:56:05,520
閾值例如0.1 0.2，在某個

1363
00:56:05,520 --> 00:56:07,619
時候線性映射會被忘記，

1364
00:56:07,619 --> 00:56:10,680
你最終得到的一切都是

1365
00:56:10,680 --> 00:56:13,319
帶有一個分層網絡，

1366
00:56:13,319 --> 00:56:15,720
就是那個呃，所以它已經學會了

1367
00:56:15,720 --> 00:56:17,099
執行的正確結構

1368
00:56:17,099 --> 00:56:19,260
分類任務是層次結構的，

1369
00:56:19,260 --> 00:56:21,900
並且還了解到第二張

1370
00:56:21,900 --> 00:56:25,020
圖像在定義測試精度方面沒有發揮任何作用

1371
00:56:25,020 --> 00:56:28,440
，這就是所有

1372
00:56:28,440 --> 00:56:30,420
這一切執行的所有這些

1373
00:56:30,420 --> 00:56:33,839
工作也只是由

1374
00:56:33,839 --> 00:56:36,599
一個自由能最小化過程執行，因此

1375
00:56:36,599 --> 00:56:38,400
您初始化模型您定義自由能

1376
00:56:38,400 --> 00:56:40,859
您定義先驗，因此

1377
00:56:40,859 --> 00:56:43,559
稀疏和C單擊之前您

1378
00:56:43,559 --> 00:56:45,780
運行能量最小化並且

1379
00:56:45,780 --> 00:56:47,400
您收斂到

1380
00:56:47,400 --> 00:56:49,500
分層模型，該模型能夠很好地對

1381
00:56:49,500 --> 00:56:51,839
切碎執行分類，

1382
00:56:51,839 --> 00:56:54,000
然後如果您 然後進行一些

1383
00:56:54,000 --> 00:56:55,800
微調，你會得到非常有競爭力的

1384
00:56:55,800 --> 00:56:57,359
結果，就像在具有反饋傳播的前饋網絡中所做的那樣，

1385
00:56:57,359 --> 00:56:59,339


1386
00:56:59,339 --> 00:57:01,260
但我認為這不是有趣的

1387
00:57:01,260 --> 00:57:03,780
一點，有趣的是你喜歡

1388
00:57:03,780 --> 00:57:05,160
所有這個過程，這個過程所有的

1389
00:57:05,160 --> 00:57:07,980
干預和非

1390
00:57:07,980 --> 00:57:09,780
循環性

1391
00:57:09,780 --> 00:57:11,700
允許你 採用完全連接的

1392
00:57:11,700 --> 00:57:12,660
網絡

1393
00:57:12,660 --> 00:57:15,119
並收斂到分層網絡，

1394
00:57:15,119 --> 00:57:16,140
能夠以

1395
00:57:16,140 --> 00:57:20,058
良好的結果執行分類，是的，

1396
00:57:20,760 --> 00:57:23,000


1397
00:57:23,000 --> 00:57:26,280
基本上就是這樣，我現在哦，是的，哇，

1398
00:57:26,280 --> 00:57:29,220
我已經談了很多，我呃，這就是

1399
00:57:29,220 --> 00:57:32,160
結論 我

1400
00:57:32,160 --> 00:57:35,280
基本上是在做一個小總結，我

1401
00:57:35,280 --> 00:57:37,559
認為如果我

1402
00:57:37,559 --> 00:57:39,300
必須用本文的一句話來告訴您，最重要的一點

1403
00:57:39,300 --> 00:57:40,980
是，預測編碼是一種

1404
00:57:40,980 --> 00:57:44,400
信念更新方法，能夠

1405
00:57:44,400 --> 00:57:46,559
執行端到端 -結束表弟學習，這樣

1406
00:57:46,559 --> 00:57:48,599
他就能夠執行干預，

1407
00:57:48,599 --> 00:57:51,420
從數據中學習結構，然後

1408
00:57:51,420 --> 00:57:53,160
執行干預和

1409
00:57:53,160 --> 00:57:56,058
反事實，

1410
00:57:56,700 --> 00:57:58,440
因此

1411
00:57:58,440 --> 00:58:00,119
通過

1412
00:58:00,119 --> 00:58:01,680
簡單地將預測誤差設置

1413
00:58:01,680 --> 00:58:03,359
為零，可以在其他有效的模型乾預中進行因果推斷，因此這是一種非常容易

1414
00:58:03,359 --> 00:58:06,240
執行的技術 干預措施，

1415
00:58:06,240 --> 00:58:07,619
你只需要接觸一個

1416
00:58:07,619 --> 00:58:08,940
神經元，你不必對

1417
00:58:08,940 --> 00:58:10,859
圖的結構採取行動，

1418
00:58:10,859 --> 00:58:14,339
你可以用它來執行

1419
00:58:14,339 --> 00:58:16,140
創建生物學上合理​​的結構因果模型，

1420
00:58:16,140 --> 00:58:18,359


1421
00:58:18,359 --> 00:58:20,819
它能夠學習結構 呃，

1422
00:58:20,819 --> 00:58:24,119
從數據來看，正如我可能已經說過很多次

1423
00:58:24,119 --> 00:58:26,940
了，

1424
00:58:26,940 --> 00:58:28,740
並且關於未來工作的幾句話

1425
00:58:28,740 --> 00:58:31,260
是，

1426
00:58:31,260 --> 00:58:33,180
最好做的事情是

1427
00:58:33,180 --> 00:58:36,119
提高我們定義的模型的性能，

1428
00:58:36,119 --> 00:58:38,460
因為我認為它在

1429
00:58:38,460 --> 00:58:40,980
以下方面表現得相當好 很多

1430
00:58:40,980 --> 00:58:43,079
任務，所以它在

1431
00:58:43,079 --> 00:58:45,780
結構學習上對我的

1432
00:58:45,780 --> 00:58:48,119
干預和反事實表現得相當好，但

1433
00:58:48,119 --> 00:58:49,440
實際上，如果你看看最先進的

1434
00:58:49,440 --> 00:58:51,420
模型，總會有一種非常

1435
00:58:51,420 --> 00:58:53,880
具體的方法在單個任務中表現更好，

1436
00:58:53,880 --> 00:58:55,559


1437
00:58:55,559 --> 00:58:58,260
所以看到它會很有趣 如果我們

1438
00:58:58,260 --> 00:59:00,180
可以

1439
00:59:00,180 --> 00:59:03,599
通過添加一些

1440
00:59:03,599 --> 00:59:05,599
技巧或

1441
00:59:05,599 --> 00:59:10,260
一些新的優化方法來達到特定任務中的性能水平，並將

1442
00:59:10,260 --> 00:59:12,839
其推廣到

1443
00:59:12,839 --> 00:59:14,280
實際上比靜態系統更有趣的動態系統，

1444
00:59:14,280 --> 00:59:17,220
例如動態

1445
00:59:17,220 --> 00:59:20,099
因果模型和/或 其他技術

1446
00:59:20,099 --> 00:59:22,200
允許您在

1447
00:59:22,200 --> 00:59:25,200
移動的系統中執行因果推理，因此在

1448
00:59:25,200 --> 00:59:27,799
特定時間步長中採取的操作會

1449
00:59:27,799 --> 00:59:30,299
影響稍後時間步長中的另一個節點，

1450
00:59:30,299 --> 00:59:32,640
這基本上是一個宏偉的

1451
00:59:32,640 --> 00:59:34,859
因果關係，是的，就是這樣，

1452
00:59:34,859 --> 00:59:38,160
嗯，

1453
00:59:38,160 --> 00:59:41,118
非常感謝，

1454
00:59:47,460 --> 00:59:51,119
謝謝 非常棒且非常全面的

1455
00:59:51,119 --> 00:59:53,160
演示，我真的認為

1456
00:59:53,160 --> 00:59:55,700
您被靜音了，

1457
00:59:57,119 --> 00:59:59,700
抱歉在 Zoom 上被靜音了，但是是的，感謝您的

1458
00:59:59,700 --> 01:00:02,400
精彩且非常全面的

1459
01:00:02,400 --> 01:00:05,099
演示，其中確實有很多內容，

1460
01:00:05,099 --> 01:00:06,900
並且在實時聊天中也有很多很好的

1461
01:00:06,900 --> 01:00:09,900
問題，所以也許可以

1462
01:00:09,900 --> 01:00:12,900
溫暖一下 進入這個問題，你是如何開始

1463
01:00:12,900 --> 01:00:15,960
研究這個主題的，你是在

1464
01:00:15,960 --> 01:00:18,900
因果關係中研究的，並發現預測編碼

1465
01:00:18,900 --> 01:00:21,000
是有用的，反之亦然，或者你是如何

1466
01:00:21,000 --> 01:00:23,160
來到這個十字路口的？

1467
01:00:23,160 --> 01:00:25,740
我實際上不得不說，第一個

1468
01:00:25,740 --> 01:00:27,240
提出這個想法的人是

1469
01:00:27,240 --> 01:00:29,040
呃，男爵

1470
01:00:29,040 --> 01:00:33,900
非常像我想一年

1471
01:00:33,900 --> 01:00:36,660
半前，甚至更多，他帶來了一個

1472
01:00:36,660 --> 01:00:38,940
帶有這個想法的頁面，然後他被

1473
01:00:38,940 --> 01:00:42,119
遺忘了，沒有人拿起它，呃，

1474
01:00:42,119 --> 01:00:43,980
去年夏天我開始

1475
01:00:43,980 --> 01:00:47,880
對因果關係和嗯感到好奇

1476
01:00:47,880 --> 01:00:50,339
例如，當我聽播客時，我讀了《生命之書》，

1477
01:00:50,339 --> 01:00:52,440
我知道

1478
01:00:52,440 --> 01:00:53,760
你對某個主題感興趣的標準方式，

1479
01:00:53,760 --> 01:00:54,900


1480
01:00:54,900 --> 01:00:57,480
我記得巴倫的這個想法，

1481
01:00:57,480 --> 01:01:00,180
並向他提出了這個想法，呃，

1482
01:01:00,180 --> 01:01:03,180
我想為什麼不呢？ 我們擴展它，呃，

1483
01:01:03,180 --> 01:01:06,000
實際上把它寫成一篇論文，所以我

1484
01:01:06,000 --> 01:01:07,319
讓一些人幫助我進行

1485
01:01:07,319 --> 01:01:09,359
實驗，呃，這是最後的最終

1486
01:01:09,359 --> 01:01:12,000
結果，太棒了，

1487
01:01:12,000 --> 01:01:14,160
很酷，是的，

1488
01:01:14,160 --> 01:01:15,240


1489
01:01:15,240 --> 01:01:17,400
有很多話要說，我只是要去

1490
01:01:17,400 --> 01:01:19,619
實時聊天 首先解決一堆

1491
01:01:19,619 --> 01:01:21,240
不同的問題，如果其他人

1492
01:01:21,240 --> 01:01:22,440
想添加我，我會

1493
01:01:22,440 --> 01:01:24,059
先打開燈，因為我認為我

1494
01:01:24,059 --> 01:01:28,440
越來越陷入黑暗，是的，

1495
01:01:28,440 --> 01:01:30,720
誰說主動推理無法解決 一個

1496
01:01:30,720 --> 01:01:32,160
暗室問題

1497
01:01:32,160 --> 01:01:34,980
哦，是的，我們在這裡，

1498
01:01:34,980 --> 01:01:37,020
所以你會說燈開關導致

1499
01:01:37,020 --> 01:01:39,299
它變亮了是

1500
01:01:39,299 --> 01:01:40,680
的，

1501
01:01:40,680 --> 01:01:42,240
我認為

1502
01:01:42,240 --> 01:01:43,980
這裡沒有問題，

1503
01:01:43,980 --> 01:01:46,940
嗯，好的，ml Dawn 寫道，

1504
01:01:46,940 --> 01:01:49,559
因為在預測編碼中，所有

1505
01:01:49,559 --> 01:01:52,020
分佈通常都是高斯分佈，

1506
01:01:52,020 --> 01:01:53,760
自下而上的消息是精確

1507
01:01:53,760 --> 01:01:55,500
加權預測 誤差，其中

1508
01:01:55,500 --> 01:01:57,420
精度是高斯

1509
01:01:57,420 --> 01:02:00,000
協方差的倒數，如果使用非高斯

1510
01:02:00,000 --> 01:02:03,319
分佈，

1511
01:02:03,780 --> 01:02:05,339


1512
01:02:05,339 --> 01:02:09,059
基本上一般方法保持

1513
01:02:09,059 --> 01:02:10,380
不同，主要區別在於，

1514
01:02:10,380 --> 01:02:13,079
你沒有預測誤差，

1515
01:02:13,079 --> 01:02:15,480
正如正確指出的那樣，基本上是

1516
01:02:15,480 --> 01:02:18,480


1517
01:02:18,480 --> 01:02:20,819
虛擬自由能的導數，如果你有高斯

1518
01:02:20,819 --> 01:02:22,920
假設，

1519
01:02:22,920 --> 01:02:25,020
是的，你甚至可以將單個量

1520
01:02:25,020 --> 01:02:27,960
設置為零，你可能

1521
01:02:27,960 --> 01:02:29,880
必鬚根據圖表的結構進行干預，

1522
01:02:29,880 --> 01:02:30,900


1523
01:02:30,900 --> 01:02:34,020


1524
01:02:34,020 --> 01:02:37,079
而且你呃和同事在 2022 年發表了一篇

1525
01:02:37,079 --> 01:02:39,900
預測論文 編碼超越

1526
01:02:39,900 --> 01:02:41,880
高斯分佈，它著眼

1527
01:02:41,880 --> 01:02:43,859
於其中一些問題，是的，是的，正是

1528
01:02:43,859 --> 01:02:46,260
如此，

1529
01:02:46,260 --> 01:02:47,339


1530
01:02:47,339 --> 01:02:50,460
這篇論文背後的想法是呃，

1531
01:02:50,460 --> 01:02:53,220
我們對變形金剛進行建模，這是

1532
01:02:53,220 --> 01:02:54,420
使用相當困難的最大動機

1533
01:02:54,420 --> 01:02:57,180
，答案是呃，不是

1534
01:02:57,180 --> 01:02:59,460
因為 注意力機制

1535
01:02:59,460 --> 01:03:02,099
最後有一個軟最大，軟最大調用

1536
01:03:02,099 --> 01:03:03,960


1537
01:03:03,960 --> 01:03:08,400
呃，不是高斯分佈，而是

1538
01:03:08,400 --> 01:03:11,280
軟最大分佈，我

1539
01:03:11,280 --> 01:03:13,440
現在不知道名字，但是是的，

1540
01:03:13,440 --> 01:03:16,079
呃，所以是的，這是一個概括，

1541
01:03:16,079 --> 01:03:19,140
它有點 一旦

1542
01:03:19,140 --> 01:03:20,700
你刪除了加斯頓假設，就

1543
01:03:20,700 --> 01:03:22,319
很難稱之為

1544
01:03:22,319 --> 01:03:24,059
創造性編碼，

1545
01:03:24,059 --> 01:03:26,400
所以他是一個

1546
01:03:26,400 --> 01:03:29,819
例子，就像和汽車

1547
01:03:29,819 --> 01:03:32,700
弗里斯通交談，或者他喜歡創造性編碼，

1548
01:03:32,700 --> 01:03:35,160
只有當你只有高斯

1549
01:03:35,160 --> 01:03:37,680
和高斯時 假設，

1550
01:03:37,680 --> 01:03:39,720
但是，是的，這更像是一場哲學

1551
01:03:39,720 --> 01:03:42,660
辯論，而不是呃

1552
01:03:42,660 --> 01:03:44,940
有趣，另一個我認為

1553
01:03:44,940 --> 01:03:46,740
絕對令人

1554
01:03:46,740 --> 01:03:49,500
感興趣的話題是《變形金剛》

1555
01:03:49,500 --> 01:03:52,980
中的注意力裝置

1556
01:03:52,980 --> 01:03:56,099
與

1557
01:03:56,099 --> 01:03:58,440
從神經認知

1558
01:03:58,440 --> 01:04:00,180
角度和預測

1559
01:04:00,180 --> 01:04:03,240
處理精度描述注意力的方式之間的異同 等待角度，

1560
01:04:03,240 --> 01:04:06,200
你對此

1561
01:04:06,359 --> 01:04:08,700
有何看法？這個想法是，

1562
01:04:08,700 --> 01:04:12,359
嗯，是的，我認為，從

1563
01:04:12,359 --> 01:04:15,000
漂亮的處理和

1564
01:04:15,000 --> 01:04:16,400
操作推理的角度來看，

1565
01:04:16,400 --> 01:04:19,260
注意力可以被視為一種

1566
01:04:19,260 --> 01:04:21,299
結構性學習問題，我

1567
01:04:21,299 --> 01:04:23,040
認為有一個

1568
01:04:23,040 --> 01:04:25,680
Chris Buckley 小組最近發表的論文表明，

1569
01:04:25,680 --> 01:04:26,339


1570
01:04:26,339 --> 01:04:28,079
應該有一個

1571
01:04:28,079 --> 01:04:30,420
存檔重印，其中基本上

1572
01:04:30,420 --> 01:04:31,859
表明，注意力機制

1573
01:04:31,859 --> 01:04:35,819
只是簡單地學習特定

1574
01:04:35,819 --> 01:04:38,880
於

1575
01:04:38,880 --> 01:04:41,040
其他數據點的權重參數的精度，所以這個精度

1576
01:04:41,040 --> 01:04:43,200
不是，它不是，它不是

1577
01:04:43,200 --> 01:04:45,540
模型結構中的參數，因此

1578
01:04:45,540 --> 01:04:47,579
它不是模型特定的參數，它

1579
01:04:47,579 --> 01:04:49,140
是一個快速變化的參數，例如在

1580
01:04:49,140 --> 01:04:51,660


1581
01:04:51,660 --> 01:04:53,760
最小化自由能變化的同時更新的值節點，

1582
01:04:53,760 --> 01:04:55,440
一旦它們一旦你 已經最小化它併

1583
01:04:55,440 --> 01:04:57,000
計算它，然後你把它扔掉，

1584
01:04:57,000 --> 01:04:58,920
對於下一個數據點，你必須

1585
01:04:58,920 --> 01:05:00,780
從頭開始重新計算它，

1586
01:05:00,780 --> 01:05:03,299
所以是的，我認為類比

1587
01:05:03,299 --> 01:05:05,819
計算明智的是呃，注意

1588
01:05:05,819 --> 01:05:07,920
機制可以被視為一種

1589
01:05:07,920 --> 01:05:10,559
結構學習，但是 結構

1590
01:05:10,559 --> 01:05:13,020
學習是特定於數據點而

1591
01:05:13,020 --> 01:05:15,119
不是特定於模型的，

1592
01:05:15,119 --> 01:05:17,280
我認為如果我們想概括

1593
01:05:17,280 --> 01:05:18,960
一點，並

1594
01:05:18,960 --> 01:05:20,339
從《變形金剛》中的注意力機製到

1595
01:05:20,339 --> 01:05:21,900


1596
01:05:21,900 --> 01:05:24,180
認知科學的注意力機制，

1597
01:05:24,180 --> 01:05:28,020
我覺得它們可能是兩個不同的東西，

1598
01:05:28,020 --> 01:05:31,260
喜歡得出相似之處和 呃，

1599
01:05:31,260 --> 01:05:33,359
我認為結構學習類比

1600
01:05:33,359 --> 01:05:36,660
以及一個連接相

1601
01:05:36,660 --> 01:05:38,760
對於另一個連接的重要性可能會

1602
01:05:38,760 --> 01:05:41,900
更好地完成這項工作，

1603
01:05:42,000 --> 01:05:44,880
酷灰色答案好吧，

1604
01:05:44,880 --> 01:05:49,200
ml Don 在反事實中問

1605
01:05:49,200 --> 01:05:51,240
隱藏變量

1606
01:05:51,240 --> 01:05:55,440
X 和未觀察到的變量 U 之間有什麼區別，

1607
01:05:55,440 --> 01:05:59,180
區別是

1608
01:05:59,540 --> 01:06:01,740
我認為主要的一個是你無法

1609
01:06:01,740 --> 01:06:03,599
觀察

1610
01:06:03,599 --> 01:06:05,819
你可以使用它們的用途，因為你可以你可以

1611
01:06:05,819 --> 01:06:09,000
計算它們並修復它們，但你不能這個

1612
01:06:09,000 --> 01:06:10,559
想法是你無法控制

1613
01:06:10,559 --> 01:06:13,380
它們，所以它們使用的用途應該是

1614
01:06:13,380 --> 01:06:16,020
被視為環境特定的變量，

1615
01:06:16,020 --> 01:06:18,540
它們在那裡，它們會影響

1616
01:06:18,540 --> 01:06:21,240
你的過程，因為

1617
01:06:21,240 --> 01:06:23,280
例如當你回到過去時，

1618
01:06:23,280 --> 01:06:25,079
環境是不同的，所以這個想法是，

1619
01:06:25,079 --> 01:06:26,520
例如，如果你

1620
01:06:26,520 --> 01:06:28,440
想回到

1621
01:06:28,440 --> 01:06:29,880
之前的例子

1622
01:06:29,880 --> 01:06:31,920
具有特定教育智力的人的預期收入

1623
01:06:31,920 --> 01:06:34,619


1624
01:06:34,619 --> 01:06:37,440
呃呃教育學位這個

1625
01:06:37,440 --> 01:06:40,200
想法是，如果我想看看

1626
01:06:40,200 --> 01:06:43,559
我今天能學到多少，呃

1627
01:06:43,559 --> 01:06:45,359
與我不知道與碩士學位

1628
01:06:45,359 --> 01:06:47,339
是不同的

1629
01:06:47,339 --> 01:06:48,359


1630
01:06:48,359 --> 01:06:50,819
20 年前我擁有碩士學位的收入是

1631
01:06:50,819 --> 01:06:52,619
不同的，例如在意大利，相

1632
01:06:52,619 --> 01:06:55,440
對於其他國家，所有這些

1633
01:06:55,440 --> 01:06:57,000
變量都不受你的

1634
01:06:57,000 --> 01:06:58,859
控制，你無法使用你的願景網絡對它們進行建模，

1635
01:06:58,859 --> 01:07:00,359


1636
01:07:00,359 --> 01:07:03,480
但它們就在那裡，所以你 當

1637
01:07:03,480 --> 01:07:05,220
你想得出結論時，你不能忽視它們，

1638
01:07:05,220 --> 01:07:07,559
所以他是的，

1639
01:07:07,559 --> 01:07:08,760
基本上所有你

1640
01:07:08,760 --> 01:07:10,079
無法控制的事情你都

1641
01:07:10,079 --> 01:07:13,079
可以推斷它們，這樣你就可以

1642
01:07:13,079 --> 01:07:14,819


1643
01:07:14,819 --> 01:07:16,740
及時進行反事實推理，然後說哦，20

1644
01:07:16,740 --> 01:07:19,020
年前我就應該得到這個

1645
01:07:19,020 --> 01:07:20,640


1646
01:07:20,640 --> 01:07:22,559
如果我這麼聰明的話，當然

1647
01:07:22,559 --> 01:07:24,599
平均能達到這個學位，

1648
01:07:24,599 --> 01:07:27,059
但這並不是說我可以改變

1649
01:07:27,059 --> 01:07:30,720
政府對就業或

1650
01:07:30,720 --> 01:07:32,819
類似事物的政策，

1651
01:07:32,819 --> 01:07:35,099
這是一個更深層次的反事實，是的，是的，

1652
01:07:35,099 --> 01:07:38,400
這些都是有用的，

1653
01:07:38,400 --> 01:07:40,200
好吧

1654
01:07:40,200 --> 01:07:42,480
你

1655
01:07:42,480 --> 01:07:45,660
在預測編碼中實現了廣義坐標

1656
01:07:45,660 --> 01:07:46,920
不，

1657
01:07:46,920 --> 01:07:50,039
我沒有，我從來沒有這樣做過，我已經

1658
01:07:50,039 --> 01:07:52,680
呃，是的，我已經研究過它，但是我呃，我從來

1659
01:07:52,680 --> 01:07:55,260
沒有實現過它，我知道它們往往

1660
01:07:55,260 --> 01:07:57,599
不穩定，呃，

1661
01:07:57,599 --> 01:08:00,299
它是 很難使它們穩定，我

1662
01:08:00,299 --> 01:08:02,940
認為這就是

1663
01:08:02,940 --> 01:08:05,460
我與實施它們的人交談時得到的結論，

1664
01:08:05,460 --> 01:08:08,359


1665
01:08:08,400 --> 01:08:11,039
但是，是的，我知道

1666
01:08:11,039 --> 01:08:12,839
最近實際上發表了一些關於

1667
01:08:12,839 --> 01:08:15,599
它們的論文，這些論文在

1668
01:08:15,599 --> 01:08:18,000
一些英國負載上進行了測試 編碼器風格實際上

1669
01:08:18,000 --> 01:08:20,520
我認為仍然來自Baron，

1670
01:08:20,520 --> 01:08:22,979


1671
01:08:22,979 --> 01:08:25,439
去年夏天有一篇論文發表，但我從來沒有

1672
01:08:25,439 --> 01:08:26,580
自己玩過它們，

1673
01:08:26,580 --> 01:08:29,160
很酷的連體衣

1674
01:08:29,160 --> 01:08:32,040
確實在層次結構中添加了更多級別，

1675
01:08:32,040 --> 01:08:35,160
減少了

1676
01:08:35,160 --> 01:08:38,238
預測輸入

1677
01:08:38,939 --> 01:08:41,698
添加更多內容的分心問題 呃，

1678
01:08:41,698 --> 01:08:43,439
在這種意義上，因為破壞

1679
01:08:43,439 --> 01:08:45,779
問題是由 Cycles 給出的，所以基本上

1680
01:08:45,779 --> 01:08:47,399
你提供了一個圖像

1681
01:08:47,399 --> 01:08:49,920
，事實上你有呃

1682
01:08:49,920 --> 01:08:53,279
所以補丁從圖像中進入

1683
01:08:53,279 --> 01:08:55,799
神經元，然後其他邊緣

1684
01:08:55,799 --> 01:08:57,500
返回，

1685
01:08:57,500 --> 01:08:59,939
這基本上創造了這樣一個事實： 你

1686
01:08:59,939 --> 01:09:03,560
有一個錯誤，這些基本上正在

1687
01:09:03,560 --> 01:09:06,179
調整圖像的像素，

1688
01:09:06,179 --> 01:09:08,339
它們會產生一些預測

1689
01:09:08,339 --> 01:09:09,719
錯誤，所以你有一些預測

1690
01:09:09,719 --> 01:09:12,140
錯誤在模型內部傳播

1691
01:09:12,140 --> 01:09:14,640
，是的，我認為這個問題

1692
01:09:14,640 --> 01:09:16,979
是周期的普遍問題， 它可能與

1693
01:09:16,979 --> 01:09:21,439


1694
01:09:23,060 --> 01:09:25,140
像素的層次結構無關，

1695
01:09:25,140 --> 01:09:26,759
如果你沒有傳入的邊緣，你就

1696
01:09:26,759 --> 01:09:27,660
沒有

1697
01:09:27,660 --> 01:09:30,540
呃沒有破壞問題了，

1698
01:09:30,540 --> 01:09:33,238
很酷，並且

1699
01:09:33,238 --> 01:09:35,939
通過跟踪運算符的非循環網絡規範

1700
01:09:35,939 --> 01:09:37,859


1701
01:09:37,859 --> 01:09:41,819
是一項非常有趣的技術，

1702
01:09:41,819 --> 01:09:46,339
什麼時候帶來的

1703
01:09:46,560 --> 01:09:49,140
呃，據我所知，我認為他發表了

1704
01:09:49,140 --> 01:09:52,380
我在 2018 年引用的那篇論文，

1705
01:09:52,380 --> 01:09:54,360
我至少在因果

1706
01:09:54,360 --> 01:09:56,940
推理文獻中不知道，我不知道

1707
01:09:56,940 --> 01:09:59,699
任何以前的方法，我會說不，

1708
01:09:59,699 --> 01:10:01,860
因為 我的意思是，這是一篇被

1709
01:10:01,860 --> 01:10:04,140
引用率很高的論文，所以我想說他們提出了

1710
01:10:04,140 --> 01:10:05,520
這個想法，

1711
01:10:05,520 --> 01:10:07,980
哇，是的，這很好，

1712
01:10:07,980 --> 01:10:09,480
你可以進行梯度下降並學習

1713
01:10:09,480 --> 01:10:11,400
結構，我認為

1714
01:10:11,400 --> 01:10:14,219
這是一種非常強大的技術，是的，

1715
01:10:14,219 --> 01:10:15,840
有時就像你看

1716
01:10:15,840 --> 01:10:17,640
當

1717
01:10:17,640 --> 01:10:19,440
貝葉斯推理和

1718
01:10:19,440 --> 01:10:23,159
因果推理的不同特徵變得可用時，

1719
01:10:23,159 --> 01:10:25,620
這真的很了不起，就像為什麼

1720
01:10:25,620 --> 01:10:28,500
沒有在貝葉斯因果建模框架下完成這件事一樣，

1721
01:10:28,500 --> 01:10:30,719


1722
01:10:30,719 --> 01:10:32,760
因為這種情況只發生了五到

1723
01:10:32,760 --> 01:10:36,659
二十五年

1724
01:10:36,659 --> 01:10:39,960
，所以時間非常短 而且

1725
01:10:39,960 --> 01:10:42,060
它的技術性相對較強，所以

1726
01:10:42,060 --> 01:10:43,920
參與其中的研究小組相對較少，

1727
01:10:43,920 --> 01:10:46,920
嗯，

1728
01:10:46,920 --> 01:10:49,860
它的功能真的很酷，

1729
01:10:49,860 --> 01:10:51,960
不，是，正是，我的意思是，這也是我

1730
01:10:51,960 --> 01:10:54,179
認為這個領域令人興奮的部分，呃，我的意思是，

1731
01:10:54,179 --> 01:10:56,040


1732
01:10:56,040 --> 01:10:59,100
肯定有

1733
01:10:59,100 --> 01:11:01,020
仍然有待發現的突破，可能是

1734
01:11:01,020 --> 01:11:03,000
因為，例如，就像

1735
01:11:03,000 --> 01:11:05,300


1736
01:11:05,300 --> 01:11:07,800
他們發現的那篇論文的突破一樣，

1737
01:11:07,800 --> 01:11:09,960
就像他們只是發現了

1738
01:11:09,960 --> 01:11:12,120
無環結構的正確先驗一樣，

1739
01:11:12,120 --> 01:11:14,040
好吧，是的，

1740
01:11:14,040 --> 01:11:17,100
我的意思是我我不知道 確切地說，但這

1741
01:11:17,100 --> 01:11:19,080
可能是你在一個下午想到的一個想法，

1742
01:11:19,080 --> 01:11:21,120
我不知道

1743
01:11:21,120 --> 01:11:23,040
其他人是如何想出這個想法的，

1744
01:11:23,040 --> 01:11:25,320
但可能是

1745
01:11:25,320 --> 01:11:27,239
他們在白板上，你

1746
01:11:27,239 --> 01:11:29,280
就像哦，實際上 這是一個

1747
01:11:29,280 --> 01:11:32,159
巨大的突破，我只是簡單地

1748
01:11:32,159 --> 01:11:33,960
定義了先前的

1749
01:11:33,960 --> 01:11:36,739
突破，還有很多這些突破，

1750
01:11:36,739 --> 01:11:40,500
它們不只是堆疊起來，它不像

1751
01:11:40,500 --> 01:11:44,280
呃呃一座積木塔，它們

1752
01:11:44,280 --> 01:11:47,640
分層並組合，所以某些東西

1753
01:11:47,640 --> 01:11:50,159
將被推廣到嗯 廣義

1754
01:11:50,159 --> 01:11:52,140
坐標或廣義同步或

1755
01:11:52,140 --> 01:11:55,020
任意大的圖形或

1756
01:11:55,020 --> 01:11:57,239
具有多模式輸入的傳感器融合

1757
01:11:57,239 --> 01:12:00,679
，就像所有這些都以非常

1758
01:12:00,679 --> 01:12:03,659
令人滿意和有效的方式融合在一起，所以即使是

1759
01:12:03,659 --> 01:12:05,640
某人可以立即想出的小東西也

1760
01:12:05,640 --> 01:12:08,100


1761
01:12:08,100 --> 01:12:11,100
確實可以產生影響

1762
01:12:11,100 --> 01:12:14,159
好吧，ml Dawn 說非常感謝你

1763
01:12:14,159 --> 01:12:16,199
問我的問題，也非常感謝

1764
01:12:16,199 --> 01:12:18,060
Tomaso 的鼓舞人心的演講，

1765
01:12:18,060 --> 01:12:21,360
非常好哦，非常感謝，然後

1766
01:12:21,360 --> 01:12:23,280
Bert 問道，

1767
01:12:23,280 --> 01:12:25,560
使用預測編碼的語言模型與

1768
01:12:25,560 --> 01:12:27,179


1769
01:12:27,179 --> 01:12:30,260
使用 Transformer 的語言模型有何不同，

1770
01:12:31,679 --> 01:12:32,520
嗯，

1771
01:12:32,520 --> 01:12:35,340
好吧，我認為實際上 如果我今天必須

1772
01:12:35,340 --> 01:12:36,659


1773
01:12:36,659 --> 01:12:38,640
使用預測編碼構建語言模型，我仍然會使用

1774
01:12:38,640 --> 01:12:40,020
Transformer，

1775
01:12:40,020 --> 01:12:41,880
所以想法是，例如，如果您

1776
01:12:41,880 --> 01:12:42,780
有一個

1777
01:12:42,780 --> 01:12:45,659


1778
01:12:45,659 --> 01:12:48,440


1779
01:12:48,440 --> 01:12:50,460


1780
01:12:50,460 --> 01:12:53,100
我在中定義的這個或這些分層貝葉斯網絡的分層圖形模型 第一個

1781
01:12:53,100 --> 01:12:55,380
滑動一個箭頭來編碼一個函數，

1782
01:12:55,380 --> 01:12:57,300
這是線性映射

1783
01:12:57,300 --> 01:12:59,219
好吧，所以一小時只是

1784
01:12:59,219 --> 01:13:01,080


1785
01:13:01,080 --> 01:13:03,060
潛在變量中編碼的向量乘以

1786
01:13:03,060 --> 01:13:06,300
這個權重矩陣，然後你可以

1787
01:13:06,300 --> 01:13:08,580
使非線性和類似的東西相乘

1788
01:13:08,580 --> 01:13:09,960
但這實際上可能是更

1789
01:13:09,960 --> 01:13:12,179
複雜的事情，箭頭中包含的函數

1790
01:13:12,179 --> 01:13:14,880
可以是卷積可以是

1791
01:13:14,880 --> 01:13:16,800
注意力機制

1792
01:13:16,800 --> 01:13:20,820
所以實際上我會如何做我

1793
01:13:20,820 --> 01:13:23,880
仍然會使用我的意思這實際上是

1794
01:13:23,880 --> 01:13:26,460
我們在呃中所做的方式 去年在牛津

1795
01:13:26,460 --> 01:13:28,860
小組中，我們的

1796
01:13:28,860 --> 01:13:30,900
結構是每個箭頭

1797
01:13:30,900 --> 01:13:33,420
現在都是一個 Transformer，所以一個是注意力

1798
01:13:33,420 --> 01:13:35,159
機制，下一個是

1799
01:13:35,159 --> 01:13:38,219
作為 Transformer 的前饋網絡

1800
01:13:38,219 --> 01:13:40,020
，基本上唯一的區別

1801
01:13:40,020 --> 01:13:41,640
是那些 您

1802
01:13:41,640 --> 01:13:43,739
想要計算後驗的變量，並

1803
01:13:43,739 --> 01:13:45,239


1804
01:13:45,239 --> 01:13:47,400
通過 VIA 平均場

1805
01:13:47,400 --> 01:13:49,560
近似使這些後驗獨立，因此基本上您遵循

1806
01:13:49,560 --> 01:13:51,659
所有步驟，使您能夠

1807
01:13:51,659 --> 01:13:53,520
收斂到

1808
01:13:53,520 --> 01:13:56,520
創造性編碼的變化自由能，但

1809
01:13:56,520 --> 01:13:58,199
您的方式 計算預測

1810
01:13:58,199 --> 01:14:01,199
和發送回信號的方式

1811
01:14:01,199 --> 01:14:04,739
是通過 Transformer 完成的，

1812
01:14:04,739 --> 01:14:07,560
所以我通常仍然會使用 Transformer 我的

1813
01:14:07,560 --> 01:14:10,679
意思是它們工作得很好，

1814
01:14:10,679 --> 01:14:12,840
我不認為我們可以傲慢地

1815
01:14:12,840 --> 01:14:15,060
說哦，不，我會這麼做 通過

1816
01:14:15,060 --> 01:14:17,640
純粹的預測編碼方式

1817
01:14:17,640 --> 01:14:18,920
結構更好，

1818
01:14:18,920 --> 01:14:21,480
但仍然會近似 Transformer

1819
01:14:21,480 --> 01:14:22,500


1820
01:14:22,500 --> 01:14:24,420
抱歉，你說結構學習將

1821
01:14:24,420 --> 01:14:27,540
近似 Transformer 方法是的，

1822
01:14:27,540 --> 01:14:29,219
我

1823
01:14:29,219 --> 01:14:32,640
之前在呃中提到的結構學習，當有人呃問

1824
01:14:32,640 --> 01:14:34,800
創意編碼

1825
01:14:34,800 --> 01:14:38,060
和注意力機制之間的相似之處時非常是

1826
01:14:38,280 --> 01:14:41,699
非常 有趣的

1827
01:14:41,699 --> 01:14:42,900
是，

1828
01:14:42,900 --> 01:14:45,719
我想知道亞馬遜的一件事，我

1829
01:14:45,719 --> 01:14:47,640
在你提到的預測編碼網絡中看不到深度的概念，

1830
01:14:47,640 --> 01:14:49,380


1831
01:14:49,380 --> 01:14:50,880
我很可能錯過了它

1832
01:14:50,880 --> 01:14:52,380
為預測編碼提供的定義

1833
01:14:52,380 --> 01:14:56,480
涉及深度的概念，

1834
01:14:56,640 --> 01:14:59,460
你所說的深度是什麼意思，

1835
01:14:59,460 --> 01:15:02,219
不，是的，這是真的 這是呃，因為

1836
01:15:02,219 --> 01:15:04,980
我多次說過的標准定義

1837
01:15:04,980 --> 01:15:06,960
是分層的，你有

1838
01:15:06,960 --> 01:15:08,400
預測朝一個方向進行，一些

1839
01:15:08,400 --> 01:15:09,719
預測誤差朝相反的

1840
01:15:09,719 --> 01:15:10,620
方向發展，

1841
01:15:10,620 --> 01:15:14,340
基本上就是我們在這篇

1842
01:15:14,340 --> 01:15:16,320
論文中所做的，以及在呃最後一篇論文中所做的，這

1843
01:15:16,320 --> 01:15:18,420
就是所謂的 對

1844
01:15:18,420 --> 01:15:19,920
任意圖拓撲的學習，我們有

1845
01:15:19,920 --> 01:15:22,260
相對編碼，我們可以將

1846
01:15:22,260 --> 01:15:25,620
深度 uh 視為

1847
01:15:25,620 --> 01:15:28,380
獨立的 uh

1848
01:15:28,380 --> 01:15:31,380
基本上是一對潛在變量 潛在

1849
01:15:31,380 --> 01:15:33,239
變量和箭頭，

1850
01:15:33,239 --> 01:15:34,739
並且您可以預測朝該

1851
01:15:34,739 --> 01:15:36,300
方向前進，並且預測箭頭

1852
01:15:36,300 --> 01:15:38,340
與另一個方向一致，但是然後您 可以用

1853
01:15:38,340 --> 01:15:41,880
多少種方式組合這些，所以

1854
01:15:41,880 --> 01:15:45,239
你可以，所以基本上這個

1855
01:15:45,239 --> 01:15:47,040
組合不必是

1856
01:15:47,040 --> 01:15:48,659
分層的，

1857
01:15:48,659 --> 01:15:50,820
最後可以有循環，所以你可以

1858
01:15:50,820 --> 01:15:53,520
例如插入另一個呃另一個

1859
01:15:53,520 --> 01:15:55,440
潛在變量到第一個 一個，

1860
01:15:55,440 --> 01:15:57,540
然後將其他連接起來，你就可以

1861
01:15:57,540 --> 01:15:59,340
得到一個結構，它可以像你想要的那樣糾纏在一起，

1862
01:15:59,340 --> 01:16:00,420


1863
01:16:00,420 --> 01:16:02,699
例如在另一篇論文中，

1864
01:16:02,699 --> 01:16:04,500
我們訓練

1865
01:16:04,500 --> 01:16:06,659
呃一個具有

1866
01:16:06,659 --> 01:16:08,460
大腦結構形狀的網絡，所以我們有很多

1867
01:16:08,460 --> 01:16:09,900
大腦區域

1868
01:16:09,900 --> 01:16:12,239
內部稀疏連接，並且

1869
01:16:12,239 --> 01:16:13,860
彼此部分連接，

1870
01:16:13,860 --> 01:16:15,719


1871
01:16:15,719 --> 01:16:17,640
最後沒有任何層次結構，但您

1872
01:16:17,640 --> 01:16:18,960
仍然可以通過最小化

1873
01:16:18,960 --> 01:16:20,699
操作自由能和

1874
01:16:20,699 --> 01:16:22,620
最小化

1875
01:16:22,620 --> 01:16:25,159
網絡的總預測誤差來訓練它，

1876
01:16:25,159 --> 01:16:27,360
這樣您就可以

1877
01:16:27,360 --> 01:16:31,980
對於糾纏圖中的給定主題，

1878
01:16:31,980 --> 01:16:35,159
您可能會看到三個連續的層

1879
01:16:35,159 --> 01:16:37,560
，當您單獨查看它們時，您會

1880
01:16:37,560 --> 01:16:38,820
說哦，這是一座三層建築，

1881
01:16:38,820 --> 01:16:41,940
這是一個三層模型，表示

1882
01:16:41,940 --> 01:16:43,980
自適應三層，但當您

1883
01:16:43,980 --> 01:16:46,620
在那裡拍攝更大的圖片時 不像

1884
01:16:46,620 --> 01:16:50,280


1885
01:16:50,280 --> 01:16:52,140
該網絡的顯式頂部或顯式底部，

1886
01:16:52,140 --> 01:16:54,360
是的，這基本上是由以下事實給出的：

1887
01:16:54,360 --> 01:16:55,980


1888
01:16:55,980 --> 01:16:58,080
韓國預測網絡中的每個操作都是

1889
01:16:58,080 --> 01:16:59,460
嚴格本地的，

1890
01:16:59,460 --> 01:17:01,739
所以基本上每條消息都會傳遞

1891
01:17:01,739 --> 01:17:03,000
每個預測和每個預測

1892
01:17:03,000 --> 01:17:05,280
錯誤 你發送的數據只發送到

1893
01:17:05,280 --> 01:17:08,280
非常接近的神經元，好吧，

1894
01:17:08,280 --> 01:17:10,380
全局結構是否實際上是

1895
01:17:10,380 --> 01:17:13,380
分層的，單個

1896
01:17:13,380 --> 01:17:16,940
消息傳遞甚至看不到，

1897
01:17:17,460 --> 01:17:19,620
我想這就是

1898
01:17:19,620 --> 01:17:22,820
學習新模型

1899
01:17:22,820 --> 01:17:27,739
架構的希望是空間

1900
01:17:27,739 --> 01:17:33,300
自上而下設計的東西非常小，

1901
01:17:33,300 --> 01:17:36,480
今天使用的模型很多，儘管是超級

1902
01:17:36,480 --> 01:17:38,640
有效的模型，

1903
01:17:38,640 --> 01:17:41,100
嗯，雖然你可以問每

1904
01:17:41,100 --> 01:17:43,320
單位計算是否有效，這是第二

1905
01:17:43,320 --> 01:17:45,300
級問題，但今天很多有效的

1906
01:17:45,300 --> 01:17:47,580
模型沒有其中一些

1907
01:17:47,580 --> 01:17:49,860
預測編碼網絡的屬性，

1908
01:17:49,860 --> 01:17:52,739
例如它們的能力，

1909
01:17:52,739 --> 01:17:55,520
僅使用本地計算，

1910
01:17:55,520 --> 01:17:59,400
這提供了生物現實性

1911
01:17:59,400 --> 01:18:02,880
或只是時空現實性，

1912
01:18:02,880 --> 01:18:06,060
而且還可以在

1913
01:18:06,060 --> 01:18:08,159
聯合計算或分佈式

1914
01:18:08,159 --> 01:18:10,500
計算設置等方面提供很多優勢，

1915
01:18:10,500 --> 01:18:12,780
不，是，完全同意，

1916
01:18:12,780 --> 01:18:14,520
因為我 我認為一般的想法是

1917
01:18:14,520 --> 01:18:16,679
這樣的，我不知道這是否會

1918
01:18:16,679 --> 01:18:18,540
成為一個優勢，所以我認為它非常有

1919
01:18:18,540 --> 01:18:20,159
前途，正是出於你所說的原因，

1920
01:18:20,159 --> 01:18:20,880


1921
01:18:20,880 --> 01:18:22,920
呃，原因是今天的

1922
01:18:22,920 --> 01:18:25,380
帶有反向傳播的模型字符串你

1923
01:18:25,380 --> 01:18:28,860
基本上可以總結一下 它們作為

1924
01:18:28,860 --> 01:18:32,040
監控反向傳播是一個

1925
01:18:32,040 --> 01:18:34,080
函數，因為基本上你有一個

1926
01:18:34,080 --> 01:18:36,120
從輸入到輸出的映射，反向

1927
01:18:36,120 --> 01:18:39,600
傳播基本上

1928
01:18:39,600 --> 01:18:41,699
從其計算

1929
01:18:41,699 --> 01:18:44,340
圖中傳播呃信息，所以今天使用的每個神經網絡神經

1930
01:18:44,340 --> 01:18:45,960
網絡模型

1931
01:18:45,960 --> 01:18:48,960
都是一個函數，而預測編碼

1932
01:18:48,960 --> 01:18:51,179
以及另一種自由編碼，例如

1933
01:18:51,179 --> 01:18:53,820
舊的函數類，該

1934
01:18:53,820 --> 01:18:56,040
方法類使用局部

1935
01:18:56,040 --> 01:18:58,500
計算進行訓練，並且實際上通過

1936
01:18:58,500 --> 01:19:01,620
最小化全局能量函數來工作，

1937
01:19:01,620 --> 01:19:03,840
它們不限於

1938
01:19:03,840 --> 01:19:05,940
從輸入到輸出的模型函數，它們實際上對

1939
01:19:05,940 --> 01:19:07,739
以下內容進行建模： 有點類似於

1940
01:19:07,739 --> 01:19:10,080
物理系統，所以你有一個物理

1941
01:19:10,080 --> 01:19:13,500
系統，你可以將一些值固定到

1942
01:19:13,500 --> 01:19:15,360
你擁有的任何輸入上，然後讓

1943
01:19:15,360 --> 01:19:17,280
系統收斂，然後你讀取應該輸出的

1944
01:19:17,280 --> 01:19:19,980
神經元或變量的一些其他值，

1945
01:19:19,980 --> 01:19:21,960
但這個

1946
01:19:21,960 --> 01:19:24,120
物理系統沒有 不一定是一個擬合

1947
01:19:24,120 --> 01:19:25,920
前向映射，不一定是一個

1948
01:19:25,920 --> 01:19:28,260
具有輸入空間和

1949
01:19:28,260 --> 01:19:30,659
輸出空間的函數，就是這樣，

1950
01:19:30,659 --> 01:19:32,580
所以你可以學習的模型類別

1951
01:19:32,580 --> 01:19:34,800
是呃，所以基本上你可以看到

1952
01:19:34,800 --> 01:19:37,560
像前饋模型 和函數，

1953
01:19:37,560 --> 01:19:39,600
然後是一個更大的類，

1954
01:19:39,600 --> 01:19:41,880
即物理系統，這裡是否有一些

1955
01:19:41,880 --> 01:19:43,860
有趣的東西我還不

1956
01:19:43,860 --> 01:19:45,659
知道，因為這些函數

1957
01:19:45,659 --> 01:19:47,460
運行得非常好，我們看到

1958
01:19:47,460 --> 01:19:50,040
反向傳播的那些日子

1959
01:19:50,040 --> 01:19:52,199
它們運行得非常好，但是

1960
01:19:52,199 --> 01:19:53,460
是的 我不知道

1961
01:19:53,460 --> 01:19:56,040
重要的部分是否有什麼有趣的東西，但重要的

1962
01:19:56,040 --> 01:19:58,380
部分是相當大的，好吧，

1963
01:19:58,380 --> 01:20:00,480
有很多模型你

1964
01:20:00,480 --> 01:20:02,940
無法帶回傳播，你

1965
01:20:02,940 --> 01:20:04,679
可以通過創造性編碼

1966
01:20:04,679 --> 01:20:06,659
或浴室傳播或其他

1967
01:20:06,659 --> 01:20:07,860
方法進行訓練

1968
01:20:07,860 --> 01:20:10,440
這非常有趣，當然，

1969
01:20:10,440 --> 01:20:12,719
生物系統物理系統

1970
01:20:12,719 --> 01:20:15,900
解決了各種有趣的問題，

1971
01:20:15,900 --> 01:20:17,100


1972
01:20:17,100 --> 01:20:19,380
但是螞蟻物種仍然沒有免費的午餐，

1973
01:20:19,380 --> 01:20:21,540
在

1974
01:20:21,540 --> 01:20:23,100
這種環境中表現很好的螞蟻可能

1975
01:20:23,100 --> 01:20:25,679
在另一種環境中表現不佳，所以嗯

1976
01:20:25,679 --> 01:20:28,260
在辛特蘭

1977
01:20:28,260 --> 01:20:31,820
可能有一些非常獨特的特殊

1978
01:20:31,820 --> 01:20:35,880
算法，它們作為一個函數沒有得到很好的描述，

1979
01:20:35,880 --> 01:20:38,460


1980
01:20:38,460 --> 01:20:42,060
但仍然提供了一種程序化的

1981
01:20:42,060 --> 01:20:46,679
方法來實現啟發式算法，

1982
01:20:46,679 --> 01:20:48,840
這可能非常

1983
01:20:48,840 --> 01:20:51,120
有效，

1984
01:20:51,120 --> 01:20:53,880
不是的，是的，是的，任何

1985
01:20:53,880 --> 01:20:55,679
這都是我最關注的

1986
01:20:55,679 --> 01:20:58,260
焦點 例如，我在攻讀博士學位期間的研究，

1987
01:20:58,260 --> 01:20:59,100


1988
01:20:59,100 --> 01:21:01,380
比如找到這個應用程序，它

1989
01:21:01,380 --> 01:21:04,199
就像在這裡，而不是在功能內部，

1990
01:21:04,199 --> 01:21:06,739


1991
01:21:07,199 --> 01:21:08,820
很酷，

1992
01:21:08,820 --> 01:21:12,120
這項工作從這裡到哪裡，比如

1993
01:21:12,120 --> 01:21:14,520
你對哪些方向感到興奮，

1994
01:21:14,520 --> 01:21:17,340
以及你如何看待主動

1995
01:21:17,340 --> 01:21:19,679
推理生態系統中的人們 參與

1996
01:21:19,679 --> 01:21:22,460
這類工作，

1997
01:21:22,500 --> 01:21:24,840
我認為一個非常有可能是最

1998
01:21:24,840 --> 01:21:27,780
有前途的方向，這是一個

1999
01:21:27,780 --> 01:21:30,060
呃，她是我可能想

2000
01:21:30,060 --> 01:21:33,060
探索一點的東西，正如我所說，

2001
01:21:33,060 --> 01:21:34,980
確實是靜態

2002
01:21:34,980 --> 01:21:37,380
模型背後的東西，所以我展示的一切 到目前為止，我已經

2003
01:21:37,380 --> 01:21:40,260
展示了呃是關於靜態數據的，

2004
01:21:40,260 --> 01:21:42,840
因此數據不會隨著時間的推移而改變，

2005
01:21:42,840 --> 01:21:45,780


2006
01:21:45,780 --> 01:21:48,000
創意編碼的定義中沒有時間，就像我

2007
01:21:48,000 --> 01:21:49,080
在這裡介紹的那樣，

2008
01:21:49,080 --> 01:21:50,940
但是您可以將

2009
01:21:50,940 --> 01:21:53,280
創意編碼推廣到工作中 正如

2010
01:21:53,280 --> 01:21:55,800


2011
01:21:55,800 --> 01:21:58,800
您之前提到的，使用廣義坐標的時間數據通過將

2012
01:21:58,800 --> 01:22:01,380
其呈現為通用卡爾曼

2013
01:22:01,380 --> 01:22:04,140
濾波器生成模型，

2014
01:22:04,140 --> 01:22:08,040
這就是

2015
01:22:08,040 --> 01:22:09,900
因果推理方向可能非常

2016
01:22:09,900 --> 01:22:12,600
有用的地方，因為是的，那個模型在

2017
01:22:12,600 --> 01:22:14,400
這一點上也許您可以 能夠對

2018
01:22:14,400 --> 01:22:17,880
更大的因果關係和更

2019
01:22:17,880 --> 01:22:21,780
複雜、更有用的

2020
01:22:21,780 --> 01:22:24,780
模型動態原因進行建模，基本上是

2021
01:22:24,780 --> 01:22:26,940
因為一般來說，微積分

2022
01:22:26,940 --> 01:22:28,560
和乾預和

2023
01:22:28,560 --> 01:22:32,760
反事實科學分支

2024
01:22:32,760 --> 01:22:36,000
主要是在小型模型上開發的，

2025
01:22:36,000 --> 01:22:38,159
所以

2026
01:22:38,159 --> 01:22:40,739
就像你不這樣做一樣 一般來說，我們不會對

2027
01:22:40,739 --> 01:22:43,560
巨大的模型進行干預，所以如果你

2028
01:22:43,560 --> 01:22:45,800
查看醫學數據，他們會使用

2029
01:22:45,800 --> 01:22:50,159
相對較小的視覺網絡，但

2030
01:22:50,159 --> 01:22:51,179
當然，如果你想要一個

2031
01:22:51,179 --> 01:22:54,900
動態因果模型來模擬

2032
01:22:54,900 --> 01:22:56,340
特定環境或特定

2033
01:22:56,340 --> 01:22:58,620
現實，你就有一個 你體內的很多神經元

2034
01:22:58,620 --> 01:23:00,780
都有很多潛在的變量，它們會隨著

2035
01:23:00,780 --> 01:23:02,580
時間的推移而改變，並且

2036
01:23:02,580 --> 01:23:05,219
在某個時刻進行更多的干預會

2037
01:23:05,219 --> 01:23:07,560
在不同的時間步長中產生效果，所以也許

2038
01:23:07,560 --> 01:23:09,239
在 10 個不同的

2039
01:23:09,239 --> 01:23:11,699
時間步長之後的下一個時間步長中，我認為這會

2040
01:23:11,699 --> 01:23:14,100
開發出一種

2041
01:23:14,100 --> 01:23:16,380
生物學上合理​​的信息傳遞方式是非常有趣的，它

2042
01:23:16,380 --> 01:23:17,699


2043
01:23:17,699 --> 01:23:20,040
也能夠

2044
01:23:20,040 --> 01:23:22,860
基本上模擬偉大的因果關係，

2045
01:23:22,860 --> 01:23:24,659
嗯，

2046
01:23:24,659 --> 01:23:29,659
你在這些模型中的哪裡看到了行動，

2047
01:23:30,840 --> 01:23:33,840
我在哪裡看到了行動，

2048
01:23:33,840 --> 01:23:36,480
我沒有想到

2049
01:23:36,480 --> 01:23:38,760
我認為是那些模型中的行動 模型

2050
01:23:38,760 --> 01:23:41,460
可能與我在其他模型中看到的方式相同，

2051
01:23:41,460 --> 01:23:43,080
因為

2052
01:23:43,080 --> 01:23:44,940
創造性編碼基本上是一種感知模型，

2053
01:23:44,940 --> 01:23:46,260


2054
01:23:46,260 --> 01:23:49,260
所以行動就是你可以看到

2055
01:23:49,260 --> 01:23:52,739
你正在經歷的事情的結果，

2056
01:23:52,739 --> 01:23:55,159
所以通過改變你

2057
01:23:55,159 --> 01:23:57,840
經歷某件事的方式，然後你 可以

2058
01:23:57,840 --> 01:24:00,060
計算也許你可以簡單地執行一個

2059
01:24:00,060 --> 01:24:01,800
更聰明的行動現在你有更多的

2060
01:24:01,800 --> 01:24:03,000
信息

2061
01:24:03,000 --> 01:24:04,560
但是

2062
01:24:04,560 --> 01:24:06,960
但是是的我不認為行動很

2063
01:24:06,960 --> 01:24:10,199
容易就像是的我沒有看到行動的任何明確的

2064
01:24:10,199 --> 01:24:12,540
後果除了事實上

2065
01:24:12,540 --> 01:24:14,040
這可以讓你基本上

2066
01:24:14,040 --> 01:24:15,960
也許你

2067
01:24:15,960 --> 01:24:18,780
只是對他們得出更好的結論，

2068
01:24:18,780 --> 01:24:21,719
在未來執行行動

2069
01:24:21,719 --> 01:24:23,940
我會補充一些

2070
01:24:23,940 --> 01:24:25,920
人們談論預測

2071
01:24:25,920 --> 01:24:29,340
編碼和行動的方式，首先內部

2072
01:24:29,340 --> 01:24:33,960
行動或隱蔽行動是注意力，這樣

2073
01:24:33,960 --> 01:24:36,120
我們就可以考慮感知 作為

2074
01:24:36,120 --> 01:24:37,980
一種內部動作，這是一種

2075
01:24:37,980 --> 01:24:40,560
方法，另一種方法非常微觀，是給

2076
01:24:40,560 --> 01:24:42,840
定節點的輸出，我們可以

2077
01:24:42,840 --> 01:24:45,780
將該節點理解為

2078
01:24:45,780 --> 01:24:48,780
具有自己的感官認知和

2079
01:24:48,780 --> 01:24:52,080
動作狀態的特定事物，因此從這個意義上說，

2080
01:24:52,080 --> 01:24:54,960
節點的輸出，最後是

2081
01:24:54,960 --> 01:24:57,179
哪個 我們在直播 43 中探索了一些關於

2082
01:24:57,179 --> 01:24:59,940


2083
01:24:59,940 --> 01:25:02,100
預測編碼的理論回顧，我們正在閱讀

2084
01:25:02,100 --> 01:25:03,840
全文，這都是關於

2085
01:25:03,840 --> 01:25:05,460
感知的，關於感知的，然後

2086
01:25:05,460 --> 01:25:08,040
就像第 5.3 節一樣，

2087
01:25:08,040 --> 01:25:11,719
如果你對行動有期望，

2088
01:25:11,719 --> 01:25:15,900
那麼行動就是 這個架構中的另一個變量，

2089
01:25:15,900 --> 01:25:18,120
它確實與

2090
01:25:18,120 --> 01:25:20,040
非主動推理相一致，

2091
01:25:20,040 --> 01:25:21,659


2092
01:25:21,659 --> 01:25:24,000
我們不是像獎勵或效用函數那樣最大化我們

2093
01:25:24,000 --> 01:25:26,699
選擇行動，而是基於它是

2094
01:25:26,699 --> 01:25:28,800
最可能的行動方案，

2095
01:25:28,800 --> 01:25:30,900
最小行動的路徑，這是貝葉斯力學

2096
01:25:30,900 --> 01:25:33,300
，所以它實際上是非常重要的。 很自然地

2097
01:25:33,300 --> 01:25:36,420
引入一個動作變量並使用

2098
01:25:36,420 --> 01:25:40,800
它，就好像它是

2099
01:25:40,800 --> 01:25:43,260
對

2100
01:25:43,260 --> 01:25:45,540
世界上其他更容易接受的事物的預測一樣，因為

2101
01:25:45,540 --> 01:25:48,480
我們也期待動作

2102
01:25:48,480 --> 01:25:50,820
不是是是完全

2103
01:25:50,820 --> 01:25:52,860
不我實際上非常喜歡定義動作的方式並且

2104
01:25:52,860 --> 01:25:55,260
呃，我仍然認為，

2105
01:25:55,260 --> 01:25:57,239
例如，沒有

2106
01:25:57,239 --> 01:26:01,139
那麼多論文應用這種方法，我

2107
01:26:01,139 --> 01:26:03,239
認為來自

2108
01:26:03,239 --> 01:26:05,100
Alexander 或 robria 的幾篇論文做了

2109
01:26:05,100 --> 01:26:08,580
類似的事情，但在實踐中，就像在

2110
01:26:08,580 --> 01:26:10,920
純粹的主動推理之外，比如應用

2111
01:26:10,920 --> 01:26:13,260
預測編碼

2112
01:26:13,260 --> 01:26:15,980
解決實際問題的行動還沒有被

2113
01:26:15,980 --> 01:26:19,280
探索很多

2114
01:26:19,679 --> 01:26:23,400
哦，謝謝你的精彩

2115
01:26:23,400 --> 01:26:25,199
演講和討論，

2116
01:26:25,199 --> 01:26:27,659
你還有什麼想說的，或者

2117
01:26:27,659 --> 01:26:30,300
指出人們的方向，呃，

2118
01:26:30,300 --> 01:26:33,360
不，非常感謝你邀請

2119
01:26:33,360 --> 01:26:34,620
我 呃，

2120
01:26:34,620 --> 01:26:36,120
這真的很有趣，我希望能

2121
01:26:36,120 --> 01:26:38,460
在某個時候回來，隨時隨地創作一些很酷的未來

2122
01:26:38,460 --> 01:26:40,199
作品，

2123
01:26:40,199 --> 01:26:41,580


2124
01:26:41,580 --> 01:26:45,000
謝謝托馬斯，

2125
01:26:45,000 --> 01:26:49,820
謝謝你丹尼爾，再見，再見


1
00:00:19,020 --> 00:00:21,119
привет и добро пожаловать,

2
00:00:21,119 --> 00:00:23,400
это активный гостевой поток вывода

3
00:00:23,400 --> 00:00:28,140
номер 51.1 28 июля 2023 года,

4
00:00:28,140 --> 00:00:31,439
мы здесь с Томасо Сальваторе, и у нас

5
00:00:31,439 --> 00:00:33,840
будет презентация и

6
00:00:33,840 --> 00:00:37,020
обсуждение недавней работы причинно-следственного

7
00:00:37,020 --> 00:00:39,660
вывода с помощью прогнозирующего кодирования, так что

8
00:00:39,660 --> 00:00:42,480
большое спасибо за присоединение для тех, кто

9
00:00:42,480 --> 00:00:44,340
смотрит  в прямом эфире, не стесняйтесь писать

10
00:00:44,340 --> 00:00:47,520
вопросы в чате, и

11
00:00:47,520 --> 00:00:50,399
вам

12
00:00:50,399 --> 00:00:52,980
спасибо большое, Даниил, за приглашение, я

13
00:00:52,980 --> 00:00:56,039
всегда был большим поклонником

14
00:00:56,039 --> 00:00:57,719
канала, и я смотрел много

15
00:00:57,719 --> 00:00:58,920
видео, я

16
00:00:58,920 --> 00:01:01,379
очень доволен  рад быть здесь и

17
00:01:01,379 --> 00:01:04,260
быть тем, кто говорит на этот раз,

18
00:01:04,260 --> 00:01:06,600
поэтому я собираюсь поговорить об этих недавних

19
00:01:06,600 --> 00:01:08,700
препринтах, которые я выпустил, которые

20
00:01:08,700 --> 00:01:11,159
были работой последних нескольких

21
00:01:11,159 --> 00:01:12,119
месяцев,

22
00:01:12,119 --> 00:01:15,900
и это сотрудничество

23
00:01:15,900 --> 00:01:18,659
с поиском  в Кетти я в макараке

24
00:01:18,659 --> 00:01:21,600
барами Легенда Томаса Лукасиавича

25
00:01:21,600 --> 00:01:24,000
и это в основном совместная работа между

26
00:01:24,000 --> 00:01:26,299
стихами это компания, в которой я работаю в

27
00:01:26,299 --> 00:01:31,680
Оксфордском университете и ухувиан

28
00:01:31,680 --> 00:01:34,200
так что

29
00:01:34,200 --> 00:01:36,299
во время этого разговора

30
00:01:36,299 --> 00:01:38,220
я сделаю

31
00:01:38,220 --> 00:01:40,979
это в основном план выступления я

32
00:01:40,979 --> 00:01:43,140
начну говорить о  что такое прогностическое

33
00:01:43,140 --> 00:01:44,520
кодирование

34
00:01:44,520 --> 00:01:47,659
и данные взаимодействия того, что это такое

35
00:01:47,659 --> 00:01:51,299
краткое историческое введение, почему вы

36
00:01:51,299 --> 00:01:54,060
считаете важным изучать

37
00:01:54,060 --> 00:01:56,159
творческое кодирование, даже, например,

38
00:01:56,159 --> 00:01:58,619
с точки зрения машинного обучения.

39
00:01:58,619 --> 00:02:00,720
Затем я представлю

40
00:02:00,720 --> 00:02:04,560
небольшое введение в то, что такое причинно-следственный вывод

41
00:02:04,560 --> 00:02:07,200
и  и как только мы соберем всю эту

42
00:02:07,200 --> 00:02:08,880
информацию вместе, я тогда

43
00:02:08,880 --> 00:02:12,540
обсужу, почему я написал эту статью, что

44
00:02:12,540 --> 00:02:14,520
было в основном исследовательским вопросом, который

45
00:02:14,520 --> 00:02:16,560
вдохновил меня, и других

46
00:02:16,560 --> 00:02:18,300
сотрудников,

47
00:02:18,300 --> 00:02:21,660
и представлю основные результаты,

48
00:02:21,660 --> 00:02:24,980
которые заключаются в том, как сделать

49
00:02:24,980 --> 00:02:27,480
вывод, так что вмешательство и

50
00:02:27,480 --> 00:02:29,340
контрфактический вывод

51
00:02:29,340 --> 00:02:33,319
и  как изучить причинно-следственные структуры

52
00:02:33,319 --> 00:02:35,879
из заданного набора данных с помощью предиктивного

53
00:02:35,879 --> 00:02:37,920
кодирования, а затем я, конечно же, закончу

54
00:02:37,920 --> 00:02:39,959


55
00:02:39,959 --> 00:02:43,500
небольшим резюме и обсуждением того,

56
00:02:43,500 --> 00:02:45,840
почему я считаю, что эта работа может быть на самом деле

57
00:02:45,840 --> 00:02:49,940
эффективной, и некоторыми будущими направлениями,

58
00:02:50,700 --> 00:02:53,400
так что же такое творческое кодирование

59
00:02:53,400 --> 00:02:55,680
творческое кодирование  в целом известен тем, что

60
00:02:55,680 --> 00:02:58,440
является методом обучения, вдохновленным нейронаукой,

61
00:02:58,440 --> 00:03:01,140
так что какую теорию о том, как

62
00:03:01,140 --> 00:03:04,560
работает обработка информации в мозгу,

63
00:03:04,560 --> 00:03:05,819
и

64
00:03:05,819 --> 00:03:08,400
очень формально говоря, уровень

65
00:03:08,400 --> 00:03:10,560
творческого кодирования можно описать как

66
00:03:10,560 --> 00:03:12,659
в основном имеющий иерархическую

67
00:03:12,659 --> 00:03:16,319
структуру нейронов в мозгу,

68
00:03:16,319 --> 00:03:19,080
и вы  есть два разных семейства

69
00:03:19,080 --> 00:03:20,700
нейронов в мозгу,

70
00:03:20,700 --> 00:03:23,280
первое семейство

71
00:03:23,280 --> 00:03:24,480
отвечает за отправку прогнозной

72
00:03:24,480 --> 00:03:27,659
информации, поэтому нейроны на определенном

73
00:03:27,659 --> 00:03:29,959
уровне иерархии отправляют информацию

74
00:03:29,959 --> 00:03:33,959
и предсказывают активность

75
00:03:33,959 --> 00:03:35,940
уровня ниже,

76
00:03:35,940 --> 00:03:38,340
а второе семейство  нейрон - это нейрон

77
00:03:38,340 --> 00:03:41,099
ошибочных нейронов, а нейроны-стрелки,

78
00:03:41,099 --> 00:03:43,019
которые они отправляют информацию об ошибке предсказания

79
00:03:43,019 --> 00:03:46,319
вверх по иерархии, поэтому один уровень предсказывает

80
00:03:46,319 --> 00:03:49,200
активность уровня ниже,

81
00:03:49,200 --> 00:03:51,659
эта активность имеет некоторое это предсказание

82
00:03:51,659 --> 00:03:54,239
как некоторое несоответствие, которое на самом деле

83
00:03:54,239 --> 00:03:56,220
происходит на уровне ниже,

84
00:03:56,220 --> 00:03:57,840
и информация о

85
00:03:57,840 --> 00:04:02,400
ошибка предсказания отправляется вверх по клавише со стрелкой,

86
00:04:02,400 --> 00:04:04,860
однако предиктивное кодирование

87
00:04:04,860 --> 00:04:07,220
на самом деле не было сожжено как

88
00:04:07,220 --> 00:04:10,799
нейробиология как теория

89
00:04:10,799 --> 00:04:11,939
нейробиологии,

90
00:04:11,939 --> 00:04:13,860
но на самом деле оно было первоначально разработано

91
00:04:13,860 --> 00:04:16,139
как метод обработки и

92
00:04:16,139 --> 00:04:19,380
сжатия сигналов еще в 50-х годах, так что

93
00:04:19,380 --> 00:04:21,899
работа  Оливер Элиас, которые на самом деле являются

94
00:04:21,899 --> 00:04:25,020
современниками одежды Шеннона из

95
00:04:25,020 --> 00:04:26,160
Шеннона,

96
00:04:26,160 --> 00:04:27,960
они поняли, что когда у нас есть

97
00:04:27,960 --> 00:04:30,900
предсказатель, модель, которая хорошо работает

98
00:04:30,900 --> 00:04:33,600
в предсказании данных,

99
00:04:33,600 --> 00:04:36,000
отправка сообщений об ошибках в

100
00:04:36,000 --> 00:04:37,919
этих предсказаниях на самом деле намного

101
00:04:37,919 --> 00:04:41,100
дешевле, чем отправка всего сообщения

102
00:04:41,100 --> 00:04:42,720
каждый раз.  время,

103
00:04:42,720 --> 00:04:45,240
и вот как родилось красивое кодирование,

104
00:04:45,240 --> 00:04:47,639
так как в

105
00:04:47,639 --> 00:04:49,500
качестве механизма обработки и сжатия сигналов

106
00:04:49,500 --> 00:04:52,020
в теории информации еще в

107
00:04:52,020 --> 00:04:53,639
50-х годах

108
00:04:53,639 --> 00:04:57,120
это было на самом деле в 80-х годах, что

109
00:04:57,120 --> 00:04:59,400
стало так, что точно такая же модель

110
00:04:59,400 --> 00:05:01,800
использовалась

111
00:05:01,800 --> 00:05:03,540
в неврологии

112
00:05:03,540 --> 00:05:07,500
и э-э  так что с помощью работы Мамфорда или

113
00:05:07,500 --> 00:05:10,440
других работ, например, объясните,

114
00:05:10,440 --> 00:05:12,960
как оценивается информация о процессах, чтобы

115
00:05:12,960 --> 00:05:14,520
мы получали прогнозирующие сигналы из

116
00:05:14,520 --> 00:05:17,160
внешнего мира, и нам нужно сжать

117
00:05:17,160 --> 00:05:20,280
это представление и иметь это

118
00:05:20,280 --> 00:05:22,740
внутреннее представление в наших нейронах

119
00:05:22,740 --> 00:05:25,199
и методе.  очень похож, если не

120
00:05:25,199 --> 00:05:27,720
эквивалентен тому, который был

121
00:05:27,720 --> 00:05:30,419
разработан Элиасом и Оливером в

122
00:05:30,419 --> 00:05:32,900
50-х годах,

123
00:05:32,940 --> 00:05:35,520
может быть, самый большой сдвиг парадигмы

124
00:05:35,520 --> 00:05:38,100
произошел в 1999 году

125
00:05:38,100 --> 00:05:41,400
благодаря работе вокруг Балларда,

126
00:05:41,400 --> 00:05:44,880
в которой они они представили

127
00:05:44,880 --> 00:05:46,199
эту концепцию, которую я  упоминалось ранее

128
00:05:46,199 --> 00:05:48,060
об иерархических структурах в

129
00:05:48,060 --> 00:05:51,360
мозгу, где информация о прогнозах располагается

130
00:05:51,360 --> 00:05:54,240
сверху вниз, а информация об ошибках —

131
00:05:54,240 --> 00:05:55,560
снизу вверх,

132
00:05:55,560 --> 00:05:57,660
и что они сделали, чего

133
00:05:57,660 --> 00:05:59,759
раньше не делали, так это то, что

134
00:05:59,759 --> 00:06:02,820
они объясняют и развивают эту теорию

135
00:06:02,820 --> 00:06:05,759
не только во Франции, но и только во Франции.

136
00:06:05,759 --> 00:06:07,979
о том, как обучение работает в

137
00:06:07,979 --> 00:06:10,139
мозгу, так что это также теория о том, как наши

138
00:06:10,139 --> 00:06:13,139
синапсы обновляются,

139
00:06:13,139 --> 00:06:16,080
и последний большой прорыв, о котором я

140
00:06:16,080 --> 00:06:17,940
собираюсь рассказать в этом кратком

141
00:06:17,940 --> 00:06:21,900
историческом введении, произошел в 2003 году, но

142
00:06:21,900 --> 00:06:25,380
тогда он продолжал

143
00:06:25,380 --> 00:06:28,380
годы спустя, благодаря машине Фристону, в

144
00:06:28,380 --> 00:06:32,100
которой он взял теорию

145
00:06:32,100 --> 00:06:35,039
Робина Балларда, развил

146
00:06:35,039 --> 00:06:38,400
ее и обобщил на

147
00:06:38,400 --> 00:06:40,919
теорию генеративных моделей, так что,

148
00:06:40,919 --> 00:06:42,720
по сути, главное утверждение этого

149
00:06:42,720 --> 00:06:45,479
Карфистона состоит в том, что творческое кодирование — это

150
00:06:45,479 --> 00:06:48,780
Схема максимизации доказательств

151
00:06:48,780 --> 00:06:50,340
определенного типа генеративной модели,

152
00:06:50,340 --> 00:06:52,979
которую я собираюсь представить

153
00:06:52,979 --> 00:06:55,139
позже,

154
00:06:55,139 --> 00:07:00,300
чтобы сделать краткий обзор

155
00:07:00,300 --> 00:07:01,560
первых двух

156
00:07:01,560 --> 00:07:03,900
видов творческого Уголка, которые я

157
00:07:03,900 --> 00:07:05,340
описал, таких как обработка и

158
00:07:05,340 --> 00:07:07,020
сжатие сигналов, а также

159
00:07:07,020 --> 00:07:09,180
обработка информации в сетчатке и в

160
00:07:09,180 --> 00:07:11,160
мозгу в целом, это методы вывода,

161
00:07:11,160 --> 00:07:12,300


162
00:07:12,300 --> 00:07:14,819
и самое большое

163
00:07:14,819 --> 00:07:17,819
изменение самая большая революция, которая у нас

164
00:07:17,819 --> 00:07:21,120
была в 1999 году, так что скажем, в 21

165
00:07:21,120 --> 00:07:23,580
веке оперативное кодирование рассматривалось

166
00:07:23,580 --> 00:07:25,919
как алгоритм обучения, поэтому мы можем сначала

167
00:07:25,919 --> 00:07:29,699
сжимать информацию  а затем обновить все

168
00:07:29,699 --> 00:07:31,800
синапсы или все скрытые переменные,

169
00:07:31,800 --> 00:07:34,139
которые у нас есть в нашей генеративной модели, чтобы

170
00:07:34,139 --> 00:07:38,599
улучшить саму нашу генеративную модель,

171
00:07:38,759 --> 00:07:43,199
поэтому давайте дадим некоторые определения,

172
00:07:43,199 --> 00:07:45,000
которые являются немного более формальными,

173
00:07:45,000 --> 00:07:48,479
чтобы оперативное кодирование можно было рассматривать как

174
00:07:48,479 --> 00:07:50,220
иерархическое гауссово генеративное  модель,

175
00:07:50,220 --> 00:07:53,400
поэтому вот очень простая фигура, в которой у

176
00:07:53,400 --> 00:07:54,780
нас есть эта иерархическая структура,

177
00:07:54,780 --> 00:07:58,319
которая может быть настолько глубокой, насколько мы хотим,

178
00:07:58,319 --> 00:08:01,560
и сигналы прогнозирования сигнала

179
00:08:01,560 --> 00:08:04,620
идут от одной скрытой переменной XM к

180
00:08:04,620 --> 00:08:06,599
следующей, и она

181
00:08:06,599 --> 00:08:09,720
каждый раз преобразуется с помощью функции GN

182
00:08:09,720 --> 00:08:12,620
или GI

183
00:08:15,319 --> 00:08:18,180
это генеративная модель, как я уже сказал, и

184
00:08:18,180 --> 00:08:19,680
какова предельная вероятность этой

185
00:08:19,680 --> 00:08:21,780
генеративной модели, ну, это просто

186
00:08:21,780 --> 00:08:24,960
вероятность последней,

187
00:08:24,960 --> 00:08:27,660
вы можете видеть мой мой курсор, да, да,

188
00:08:27,660 --> 00:08:29,940
это идеально, так что это генетическая модель

189
00:08:29,940 --> 00:08:32,700
последней вершины. Извините, вероятно,

190
00:08:32,700 --> 00:08:34,979
распределение  последней вершины, умноженной на

191
00:08:34,979 --> 00:08:37,140
распределение вероятностей каждой

192
00:08:37,140 --> 00:08:40,440
другой вершины, обусловленное активностью

193
00:08:40,440 --> 00:08:43,020
вершины до или

194
00:08:43,020 --> 00:08:45,860
скрытой переменной до I.

195
00:08:45,899 --> 00:08:48,240
Я уже сказал, что это гауссова

196
00:08:48,240 --> 00:08:50,399
генеративная модель, что означает, что эти

197
00:08:50,399 --> 00:08:54,260
вероятности находятся в гауссовской форме,

198
00:08:54,660 --> 00:08:57,120
и каждая

199
00:08:57,120 --> 00:09:00,480
функция эндоса  функция G в целом и

200
00:09:00,480 --> 00:09:02,880
особенно потому, что, например, в

201
00:09:02,880 --> 00:09:05,459
статье Рамблера и во всех

202
00:09:05,459 --> 00:09:07,920
последующих статьях также из-за

203
00:09:07,920 --> 00:09:10,500
Революции глубокого обучения эти функции являются

204
00:09:10,500 --> 00:09:13,220
просто линейными картами или

205
00:09:13,220 --> 00:09:15,120
нелинейными картами с

206
00:09:15,120 --> 00:09:18,000
функциями активации или нелинейными картами с

207
00:09:18,000 --> 00:09:22,040
функцией активации и  аддитивное смещение,

208
00:09:23,220 --> 00:09:27,180
поэтому мы можем дать формальное

209
00:09:27,180 --> 00:09:28,860
определение творческого кодирования, и мы можем

210
00:09:28,860 --> 00:09:30,300
сказать, что оперативное кодирование является

211
00:09:30,300 --> 00:09:33,480
схемой инверсии для такой генеративной модели, где

212
00:09:33,480 --> 00:09:35,839
ее модельное свидетельство максимизируется за счет

213
00:09:35,839 --> 00:09:38,760
минимизации количества, которое называется

214
00:09:38,760 --> 00:09:40,920
вариацией свободной энергии

215
00:09:40,920 --> 00:09:43,740
в общем случае.  цель каждой генеративной

216
00:09:43,740 --> 00:09:46,019
модели состоит в том, чтобы максимизировать модельное свидетельство, но

217
00:09:46,019 --> 00:09:48,860
эта величина всегда непостижима, и у

218
00:09:48,860 --> 00:09:51,019
нас есть некоторые

219
00:09:51,019 --> 00:09:53,279
методы, которые позволяют нам

220
00:09:53,279 --> 00:09:55,980
аппроксимировать решение, и тот,

221
00:09:55,980 --> 00:09:58,500
который мы используем в творческом кодировании,

222
00:09:58,500 --> 00:10:00,720
вместо минимизации аберрации свободной

223
00:10:00,720 --> 00:10:03,480
энергии, которая является  что является нижней границей

224
00:10:03,480 --> 00:10:06,839
модельного свидетельства в этой работе и

225
00:10:06,839 --> 00:10:09,660
на самом деле во многих во многих

226
00:10:09,660 --> 00:10:11,700
других, так что это стандартный способ сделать это, эта

227
00:10:11,700 --> 00:10:13,740
минимизация выполняется

228
00:10:13,740 --> 00:10:16,080
спуском ингредиента,

229
00:10:16,080 --> 00:10:18,540
и да, мы договорились о спуске,

230
00:10:18,540 --> 00:10:19,980
и есть  на самом деле другие методы,

231
00:10:19,980 --> 00:10:22,140
такие как максимизация ожидания, которая

232
00:10:22,140 --> 00:10:23,580
часто эквивалентна,

233
00:10:23,580 --> 00:10:25,140
или вы можете использовать некоторые другие

234
00:10:25,140 --> 00:10:26,940
алгоритмы передачи сообщений, такие как

235
00:10:26,940 --> 00:10:29,959
распространение убеждений, например,

236
00:10:30,720 --> 00:10:33,980
и немного вернуться во времени, поэтому

237
00:10:33,980 --> 00:10:35,940
немного забывая о

238
00:10:35,940 --> 00:10:38,760
статистических генеративных моделях,

239
00:10:38,760 --> 00:10:41,360
если мы можем увидеть творческое кодирование

240
00:10:41,360 --> 00:10:44,040
как я имею в виду, я уже говорил пару

241
00:10:44,040 --> 00:10:46,200
раз как иерархическая модель

242
00:10:46,200 --> 00:10:48,420
с нейронными действиями, поэтому со

243
00:10:48,420 --> 00:10:50,700
скрытыми переменными нейронов, которые представляют

244
00:10:50,700 --> 00:10:53,459
нейронные действия, отправитель сигнализирует вниз по

245
00:10:53,459 --> 00:10:54,899
иерархии,

246
00:10:54,899 --> 00:10:57,540
а с ошибочными узлами или ошибочными нейронами

247
00:10:57,540 --> 00:11:01,019
отправитель сигнализирует вверх по иерархии, так что

248
00:11:01,019 --> 00:11:03,660
это и  информация об ошибке назад,

249
00:11:03,660 --> 00:11:05,700
какова вариация свободной энергии

250
00:11:05,700 --> 00:11:08,220
моделей кодирования, управляемых этим классом, это

251
00:11:08,220 --> 00:11:09,899
просто сумма

252
00:11:09,899 --> 00:11:12,720
среднеквадратичной ошибки всех

253
00:11:12,720 --> 00:11:14,399
ошибочных нейронов,

254
00:11:14,399 --> 00:11:18,120
так что это сумма ошибки

255
00:11:18,120 --> 00:11:21,980
квадрата общей ошибки,

256
00:11:22,019 --> 00:11:24,480
и это представление равно  это будет

257
00:11:24,480 --> 00:11:27,120
полезно на более поздних слайдах и в том,

258
00:11:27,120 --> 00:11:28,740
как я собираюсь объяснить, как использовать

259
00:11:28,740 --> 00:11:30,120
творческое кодирование для моделирования причинно-следственного

260
00:11:30,120 --> 00:11:32,940
вывода, например,

261
00:11:32,940 --> 00:11:34,800
я думаю, что предиктивное кодирование важно,

262
00:11:34,800 --> 00:11:36,240
и это не хороший алгоритм для

263
00:11:36,240 --> 00:11:37,500
изучения

264
00:11:37,500 --> 00:11:39,600
в первую очередь как  Я сказал ранее, что он

265
00:11:39,600 --> 00:11:41,399
оптимизирует правильную цель, которая является

266
00:11:41,399 --> 00:11:43,079
модельным свидетельством или предельной

267
00:11:43,079 --> 00:11:44,339
вероятностью,

268
00:11:44,339 --> 00:11:45,660
а

269
00:11:45,660 --> 00:11:47,700
затем он делает это, оптимизируя нижнюю

270
00:11:47,700 --> 00:11:49,440
границу, которая называется изменением

271
00:11:49,440 --> 00:11:52,440
свободной энергии, как я уже сказал, и виртуальный

272
00:11:52,440 --> 00:11:54,240
конец интересен, потому что его можно

273
00:11:54,240 --> 00:11:57,680
записать в виде  сумма двух разных терминов,

274
00:11:57,680 --> 00:12:00,839
которые являются и каждый из этих терминов

275
00:12:00,839 --> 00:12:04,680
оптимизации, как важные воздействия,

276
00:12:04,680 --> 00:12:06,899
например, в задачах машинного обучения

277
00:12:06,899 --> 00:12:09,060
или вообще в задачах обучения,

278
00:12:09,060 --> 00:12:12,420
поэтому один из этих терминов заставляет запоминать,

279
00:12:12,420 --> 00:12:15,440
поэтому во втором термине в основном говорится о

280
00:12:15,440 --> 00:12:18,180
силах модели  чтобы соответствовать определенному набору данных,

281
00:12:18,180 --> 00:12:19,560


282
00:12:19,560 --> 00:12:21,240
и первый член

283
00:12:21,240 --> 00:12:23,519
заставляет модель минимизировать

284
00:12:23,519 --> 00:12:26,040
сложность, и, как мы знаем, например,

285
00:12:26,040 --> 00:12:28,500
для теории бритвы результатов,

286
00:12:28,500 --> 00:12:31,260
если у нас есть две разные модели,

287
00:12:31,260 --> 00:12:33,000
которые работают одинаково на конкретном

288
00:12:33,000 --> 00:12:35,640
тренировочном наборе, тот, который у нас есть  чтобы получить,

289
00:12:35,640 --> 00:12:37,380
и тот, который, как ожидается,

290
00:12:37,380 --> 00:12:39,899
обобщает, является наиболее

291
00:12:39,899 --> 00:12:41,160
простым,

292
00:12:41,160 --> 00:12:44,100
поэтому обновление генеративной модели с помощью

293
00:12:44,100 --> 00:12:46,380
операционной свободной энергии позволяет нам

294
00:12:46,380 --> 00:12:47,779
в основном

295
00:12:47,779 --> 00:12:51,959
сходиться к оптимальной модели бритвы результата,

296
00:12:51,959 --> 00:12:54,720
которая запоминает

297
00:12:54,720 --> 00:12:56,100
набор данных, но также  способность

298
00:12:56,100 --> 00:12:58,680
очень хорошо обобщать невидимые невидимые

299
00:12:58,680 --> 00:13:00,240
точки данных.

300
00:13:00,240 --> 00:13:02,639
Вторая причина важности оперативного кодирования

301
00:13:02,639 --> 00:13:08,600
заключается в том, что на самом деле его

302
00:13:08,720 --> 00:13:11,760
не нужно определять в

303
00:13:11,760 --> 00:13:13,920
иерархической структуре, но его можно

304
00:13:13,920 --> 00:13:15,959
смоделировать на более сложных и гибких

305
00:13:15,959 --> 00:13:18,240
архитектурах, таких как ориентированная графическая система.

306
00:13:18,240 --> 00:13:21,540
модели любой формы или еще

307
00:13:21,540 --> 00:13:23,700
более обобщенной для сетей с большим количеством циклов,

308
00:13:23,700 --> 00:13:25,920
которые напоминают область мозга, и конечный

309
00:13:25,920 --> 00:13:27,779
результат в основной причине

310
00:13:27,779 --> 00:13:30,300
заключается в том, что вы не изучаете и не

311
00:13:30,300 --> 00:13:32,339
прогнозируете с помощью прямого прохода, а затем

312
00:13:32,339 --> 00:13:34,260
обратно распространяете ошибку, но вы

313
00:13:34,260 --> 00:13:36,600
минимизация энергетической функции,

314
00:13:36,600 --> 00:13:38,459
и это позволяет использовать практически любой вид

315
00:13:38,459 --> 00:13:39,839
иерархии,

316
00:13:39,839 --> 00:13:41,180


317
00:13:41,180 --> 00:13:43,860
позволяет использовать прямые клавиши и

318
00:13:43,860 --> 00:13:46,860
позволяет изучать циклы, и это

319
00:13:46,860 --> 00:13:48,060
на самом деле очень важно, потому что

320
00:13:48,060 --> 00:13:50,399
мозг полон циклов, поскольку у нас есть

321
00:13:50,399 --> 00:13:53,399
некоторая информация из некоторых недавних статей.

322
00:13:53,399 --> 00:13:56,459
э-э, удалось полностью отобразить

323
00:13:56,459 --> 00:13:59,279
мозг некоторых животных, таких как

324
00:13:59,279 --> 00:14:00,420
плодовая муха,

325
00:14:00,420 --> 00:14:03,899
мозг полон циклов, поэтому имеет

326
00:14:03,899 --> 00:14:06,720
смысл слить наши модели машинного обучения

327
00:14:06,720 --> 00:14:09,000
или

328
00:14:09,000 --> 00:14:11,160
наши модели в целом с помощью алгоритма,

329
00:14:11,160 --> 00:14:14,160
который позволяет нам сливать с помощью

330
00:14:14,160 --> 00:14:17,160
циклического  Структуры

331
00:14:17,160 --> 00:14:19,380
Третья причина, по которой оперативное кодирование

332
00:14:19,380 --> 00:14:21,240
интересно, заключается в том, что было официально

333
00:14:21,240 --> 00:14:23,820
доказано, что оно более надежно, чем

334
00:14:23,820 --> 00:14:25,139
стандартная нейронная сеть, начиная с

335
00:14:25,139 --> 00:14:27,060
черного распространения, поэтому, если у вас есть

336
00:14:27,060 --> 00:14:28,200
нейронная сеть и вы хотите выполнять

337
00:14:28,200 --> 00:14:30,320
задачи классификации,

338
00:14:30,320 --> 00:14:34,139
ваше творческое кодирование является более надежным

339
00:14:34,139 --> 00:14:36,260
и  это

340
00:14:36,260 --> 00:14:38,339
интересно в таких задачах, как онлайн-

341
00:14:38,339 --> 00:14:40,680
обучение на небольших наборах данных или

342
00:14:40,680 --> 00:14:43,440
задачи непрерывного обучения, и теория

343
00:14:43,440 --> 00:14:45,540
в основном исходит из того факта, что

344
00:14:45,540 --> 00:14:48,540
императивное кодирование было перемещено в

345
00:14:48,540 --> 00:14:50,820
аппроксимацию неявного градиентного спуска,

346
00:14:50,820 --> 00:14:53,339
который является другой версией

347
00:14:53,339 --> 00:14:54,899
явного градиентного спуска, который

348
00:14:54,899 --> 00:14:57,180
стандартный зеленый спуск, используемый в

349
00:14:57,180 --> 00:14:59,880
каждой отдельной модели в основном,

350
00:14:59,880 --> 00:15:03,680
и это вариант, который более надежен,

351
00:15:05,880 --> 00:15:08,279
я думаю, хорошо, я сделал довольно долгое

352
00:15:08,279 --> 00:15:09,779
внутриоперационное кодирование.

353
00:15:09,779 --> 00:15:11,639


354
00:15:11,639 --> 00:15:13,019


355
00:15:13,019 --> 00:15:15,839
причинно-следственный вывод причинно-следственный

356
00:15:15,839 --> 00:15:18,420
вывод - это теория, это очень

357
00:15:18,420 --> 00:15:20,339
общая теория, которая была формализована в

358
00:15:20,339 --> 00:15:23,100
наибольшей степени Джуди Эпперел, он, безусловно,

359
00:15:23,100 --> 00:15:25,500
самый важный человек в

360
00:15:25,500 --> 00:15:27,839
области причинно-следственных связей во Франции, он написал несколько

361
00:15:27,839 --> 00:15:29,760
очень хороших книг, например, книга

362
00:15:29,760 --> 00:15:32,760
Y настоятельно рекомендуется  если вы хотите

363
00:15:32,760 --> 00:15:35,220
узнать больше об этой теме,

364
00:15:35,220 --> 00:15:37,800
и она в основном решает следующую

365
00:15:37,800 --> 00:15:38,639
проблему,

366
00:15:38,639 --> 00:15:40,440
поэтому давайте предположим, что у нас есть совместное

367
00:15:40,440 --> 00:15:42,000
распределение вероятностей, которое

368
00:15:42,000 --> 00:15:44,160
связано с байесовской сетью, это

369
00:15:44,160 --> 00:15:46,199
будет небольшой

370
00:15:46,199 --> 00:15:49,260
пример во всей статье,

371
00:15:49,260 --> 00:15:51,839
особенно когда  вы не с азиатскими

372
00:15:51,839 --> 00:15:54,480
сетями этой формы,

373
00:15:54,480 --> 00:15:57,660
они были основаны на сетях,

374
00:15:57,660 --> 00:16:00,240
переменные внутри которых могут представлять

375
00:16:00,240 --> 00:16:02,100
разные величины, поэтому, например, наша

376
00:16:02,100 --> 00:16:04,620
визуальная сеть с этой формой может

377
00:16:04,620 --> 00:16:06,899
представлять

378
00:16:06,899 --> 00:16:08,820
величины справа, поэтому социально-

379
00:16:08,820 --> 00:16:10,800
экономическая Студийная статуя

380
00:16:10,800 --> 00:16:13,079
человека ее  уровень образования, его

381
00:16:13,079 --> 00:16:16,699
интеллект и уровень его дохода, в

382
00:16:17,100 --> 00:16:19,440
чем классическая статистика

383
00:16:19,440 --> 00:16:22,920
очень хороша, и это, э-э, в то время как

384
00:16:22,920 --> 00:16:25,320
наиболее используемое приложение - это моделирование

385
00:16:25,320 --> 00:16:28,019
наблюдений или корреляций,

386
00:16:28,019 --> 00:16:29,279
корреляция в основном отвечает на

387
00:16:29,279 --> 00:16:32,519
вопрос, что будет, если мы наблюдаем

388
00:16:32,519 --> 00:16:35,579
другую переменную C,

389
00:16:35,579 --> 00:16:37,500
например, в  в этом случае

390
00:16:37,500 --> 00:16:39,660
каков уровень дохода ожидаемый

391
00:16:39,660 --> 00:16:41,820
уровень дохода человека, если я

392
00:16:41,820 --> 00:16:44,339
наблюдаю этот уровень образования

393
00:16:44,339 --> 00:16:48,180
и, конечно, если у этого человека

394
00:16:48,180 --> 00:16:50,220
есть более высокая степень образования,

395
00:16:50,220 --> 00:16:52,500
например, степень магистра или доктора наук, я ожидаю, что у

396
00:16:52,500 --> 00:16:54,360
этого человека будет общее  более высокий

397
00:16:54,360 --> 00:16:56,040
уровень дохода,

398
00:16:56,040 --> 00:16:58,139
и это корреляция,

399
00:16:58,139 --> 00:17:00,300
однако иногда есть вещи, которые

400
00:17:00,300 --> 00:17:03,300
очень трудно наблюдать, но они играют

401
00:17:03,300 --> 00:17:05,040
огромную роль в определении этих

402
00:17:05,040 --> 00:17:06,119
величин,

403
00:17:06,119 --> 00:17:08,220
поэтому, например, может случиться так, что

404
00:17:08,220 --> 00:17:11,160
уровень дохода гораздо больше

405
00:17:11,160 --> 00:17:13,380
определяется интеллектом

406
00:17:13,380 --> 00:17:15,540
конкретного человека

407
00:17:15,540 --> 00:17:18,720
и, возможно, что интеллект или

408
00:17:18,720 --> 00:17:21,000
если человек умен, также, скорее

409
00:17:21,000 --> 00:17:24,540
всего, будет иметь более высокий уровень образования,

410
00:17:24,540 --> 00:17:27,540
но все же настоящая причина, по которой

411
00:17:27,540 --> 00:17:30,120
доход составляет I, связана с

412
00:17:30,120 --> 00:17:32,220
IQ,

413
00:17:32,220 --> 00:17:34,740
и это может быть это не может  быть Исследования

414
00:17:34,740 --> 00:17:36,360
простыми корреляциями и должны

415
00:17:36,360 --> 00:17:39,120
изучаться с помощью более продвинутой техники,

416
00:17:39,120 --> 00:17:41,280
которая называется вмешательством.

417
00:17:41,280 --> 00:17:43,320
Вмешательство в основном отвечает на

418
00:17:43,320 --> 00:17:46,500
вопрос, что такое D, если мы изменим C на

419
00:17:46,500 --> 00:17:48,240
определенное значение,

420
00:17:48,240 --> 00:17:51,000
поэтому, например, мы можем мы можем взять

421
00:17:51,000 --> 00:17:54,660
человека  и проверьте его уровень дохода,

422
00:17:54,660 --> 00:17:57,120
а затем измените его уровень образования, так что

423
00:17:57,120 --> 00:17:59,220
вмешайтесь в этот мир

424
00:17:59,220 --> 00:18:01,080
и измените его уровень образования, не

425
00:18:01,080 --> 00:18:03,419
касаясь его интеллекта, и посмотрите,

426
00:18:03,419 --> 00:18:07,260
насколько изменится его доход,

427
00:18:07,260 --> 00:18:09,900
например, если доход сильно изменится,

428
00:18:09,900 --> 00:18:12,179
это означает, что интеллект

429
00:18:12,179 --> 00:18:14,460
не  не играет большую роль в этом, но

430
00:18:14,460 --> 00:18:16,799
уровень образования играет, если уровень дохода

431
00:18:16,799 --> 00:18:19,020
не сильно меняется, это означает, что, возможно,

432
00:18:19,020 --> 00:18:20,640
есть скрытая переменная, в этом случае

433
00:18:20,640 --> 00:18:22,860
интеллект, который определяет

434
00:18:22,860 --> 00:18:25,760
уровень дохода человека,

435
00:18:25,980 --> 00:18:28,740
третья величина, важный причинно-следственный

436
00:18:28,740 --> 00:18:31,080
вывод заключается в том, что  контрфактов, так,

437
00:18:31,080 --> 00:18:33,120
например, контрфактика отвечает на

438
00:18:33,120 --> 00:18:36,720
вопрос, что было бы, и мы меняем

439
00:18:36,720 --> 00:18:39,240
C на другое значение в прошлом,

440
00:18:39,240 --> 00:18:40,679
поэтому, например, мы можем видеть, что

441
00:18:40,679 --> 00:18:42,059
разница между вмешательствами и

442
00:18:42,059 --> 00:18:45,059
контрфактуалами заключается в том, что вмешательства

443
00:18:45,059 --> 00:18:47,820
действуют в будущем, поэтому я беру интервью  в

444
00:18:47,820 --> 00:18:50,340
мире сейчас, чтобы наблюдать изменение в

445
00:18:50,340 --> 00:18:53,220
будущем, хорошо контрфактическое, позволяет нам

446
00:18:53,220 --> 00:18:56,039
вернуться в прошлое и изменить переменную

447
00:18:56,039 --> 00:18:59,160
назад во времени и посмотреть, как это изменение

448
00:18:59,160 --> 00:19:01,320
повлияло бы на мир, в котором мы живем

449
00:19:01,320 --> 00:19:02,940
сейчас,

450
00:19:02,940 --> 00:19:06,299
и они определяются judapple как

451
00:19:06,299 --> 00:19:08,100
три  уровни

452
00:19:08,100 --> 00:19:09,660
корреляции причинно-следственной связи — первый уровень,

453
00:19:09,660 --> 00:19:11,580
вмешательство — второй уровень,

454
00:19:11,580 --> 00:19:14,720
контрфактический — третий уровень,

455
00:19:16,020 --> 00:19:18,120
другие вмешательства.

456
00:19:18,120 --> 00:19:20,640
Теперь, когда я дал интуитивное определение, я собираюсь дать им более формальное

457
00:19:20,640 --> 00:19:23,760
определение, и я

458
00:19:23,760 --> 00:19:25,500
использую здесь это обозначение.  что на

459
00:19:25,500 --> 00:19:27,240
самом деле одно и то же на протяжении всей

460
00:19:27,240 --> 00:19:29,640
презентации, поэтому X всегда будет

461
00:19:29,640 --> 00:19:32,820
скрытой переменной s i всегда

462
00:19:32,820 --> 00:19:35,340
будет точкой данных или наблюдением,

463
00:19:35,340 --> 00:19:38,520
а VI всегда будет вершиной, поэтому

464
00:19:38,520 --> 00:19:40,860
каждый раз, когда вы видите VI, мы только

465
00:19:40,860 --> 00:19:42,720
интересует структура графика,

466
00:19:42,720 --> 00:19:45,299
например,

467
00:19:45,299 --> 00:19:46,860
поэтому давайте предположим, что у нас есть байесовская модель,

468
00:19:46,860 --> 00:19:50,160
которая имеет ту же структуру,

469
00:19:50,160 --> 00:19:52,679
что и байесовская модель, которую мы видели на

470
00:19:52,679 --> 00:19:54,780
предыдущем слайде,

471
00:19:54,780 --> 00:19:57,840
учитывая, что X3 равно S3, это

472
00:19:57,840 --> 00:20:00,660
наблюдение, которое мы делаем, статистика позволяет  нам,

473
00:20:00,660 --> 00:20:03,360
чтобы вычислить вероятность или

474
00:20:03,360 --> 00:20:04,679
ожидание

475
00:20:04,679 --> 00:20:07,380
X4, которая является скрытой переменной,

476
00:20:07,380 --> 00:20:09,240
связанной с этой вершиной,

477
00:20:09,240 --> 00:20:13,860
учитывая, что X3 равно S3

478
00:20:13,860 --> 00:20:15,679
иностранному

479
00:20:15,679 --> 00:20:17,760
вмешательству, нам нужен новый вид

480
00:20:17,760 --> 00:20:19,919
записи, который называется операцией do,

481
00:20:19,919 --> 00:20:21,179


482
00:20:21,179 --> 00:20:23,880
поэтому в этом случае

483
00:20:23,880 --> 00:20:26,100
X4 мы хотим вычислить  вероятность

484
00:20:26,100 --> 00:20:30,000
X4 с учетом того факта, что мы вмешиваемся в

485
00:20:30,000 --> 00:20:33,059
слово и меняем X3 West 3.

486
00:20:33,059 --> 00:20:35,580
и как мы это делаем, чтобы выполнить

487
00:20:35,580 --> 00:20:38,400
вмешательство Judo Pearl говорит нам, что мы

488
00:20:38,400 --> 00:20:40,020
должны

489
00:20:40,020 --> 00:20:41,880
сделать промежуточный шаг, прежде чем

490
00:20:41,880 --> 00:20:45,059
вычислять корреляцию, сначала мы

491
00:20:45,059 --> 00:20:46,860
необходимо удалить все, чтобы удалить все

492
00:20:46,860 --> 00:20:50,160
входящие ребра в V3,

493
00:20:50,160 --> 00:20:52,799
поэтому мы должны изучать не эту байесовскую

494
00:20:52,799 --> 00:20:55,679
сеть, а эту вторую,

495
00:20:55,679 --> 00:20:58,200
и затем в этот момент нам разрешено

496
00:20:58,200 --> 00:21:00,840
вычислять корреляцию, как мы

497
00:21:00,840 --> 00:21:03,299
обычно делаем,

498
00:21:03,299 --> 00:21:06,500
и это вмешательство

499
00:21:07,020 --> 00:21:09,299
контрфактическое  является обобщением того,

500
00:21:09,299 --> 00:21:11,700
что, как я сказал, жило в прошлом,

501
00:21:11,700 --> 00:21:14,100
и они вычисляют с использованием структурных

502
00:21:14,100 --> 00:21:15,419
причинно-следственных моделей,

503
00:21:15,419 --> 00:21:18,299
структурно-причинная модель представляет собой кортеж,

504
00:21:18,299 --> 00:21:21,120
который концептуально похож на

505
00:21:21,120 --> 00:21:23,460
байесовскую сеть, но в основном у нас есть

506
00:21:23,460 --> 00:21:26,220
этот новый класс переменных поверх которых

507
00:21:26,220 --> 00:21:28,580
являются ненаблюдаемыми переменными, которые они используют,

508
00:21:28,580 --> 00:21:30,960
поэтому у нас есть байесовская сеть, которая

509
00:21:30,960 --> 00:21:34,020
была у нас до X1 X2 X3 S4,

510
00:21:34,020 --> 00:21:37,460
но у нас также есть те ненаблюдаемые или

511
00:21:37,460 --> 00:21:40,020
переменные, которые зависят от среды, которую

512
00:21:40,020 --> 00:21:42,539
вы не можете контролировать, вы можете сделать вывод о

513
00:21:42,539 --> 00:21:43,980
них, но вы,

514
00:21:43,980 --> 00:21:46,020
но они, они есть

515
00:21:46,020 --> 00:21:48,539
и

516
00:21:48,539 --> 00:21:51,360
f  это набор функций, который зависит от

517
00:21:51,360 --> 00:21:53,400
всех

518
00:21:53,400 --> 00:21:57,299
в основном f от x от x3 зависит от X1,

519
00:21:57,299 --> 00:21:58,980
потому что у нас есть стрелка на x2, потому что у

520
00:21:58,980 --> 00:22:00,960
вас есть стрелка и от

521
00:22:00,960 --> 00:22:02,940
ненаблюдаемой переменной, которая также

522
00:22:02,940 --> 00:22:05,840
влияет на экстремум,

523
00:22:06,179 --> 00:22:09,240
так что да, интуитивно вы можете видеть нас, вы

524
00:22:09,240 --> 00:22:11,940
можете думать  структурной причинно-следственной модели в

525
00:22:11,940 --> 00:22:14,159
виде байесовской сети с этими

526
00:22:14,159 --> 00:22:16,679
ненаблюдаемыми переменными наверху, и каждая

527
00:22:16,679 --> 00:22:19,500
ненаблюдаемая переменная влияет только на

528
00:22:19,500 --> 00:22:22,020
свою

529
00:22:22,020 --> 00:22:24,600
собственную последнюю переменную X, поэтому, например,

530
00:22:24,600 --> 00:22:27,960
IU никогда не коснется X1, а u3

531
00:22:27,960 --> 00:22:30,360
коснется только Q3. E1 все будут влиять на

532
00:22:30,360 --> 00:22:34,039
X1 и  и т. д. и т. д. Таким образом,

533
00:22:35,039 --> 00:22:37,679
выполнение контрфактического вывода

534
00:22:37,679 --> 00:22:39,900
отвечает на следующий вопрос: что

535
00:22:39,900 --> 00:22:42,960
было бы X4 в X3 равным другой

536
00:22:42,960 --> 00:22:46,620
переменной в ситуации с пропуском, которая вам

537
00:22:46,620 --> 00:22:49,340
незнакома,

538
00:22:49,340 --> 00:22:51,840
требует трех разных шагов, поэтому

539
00:22:51,840 --> 00:22:53,039
абдукция

540
00:22:53,039 --> 00:22:54,900
— это

541
00:22:54,900 --> 00:22:57,179
вычисление всех фоновых

542
00:22:57,179 --> 00:22:59,460
переменных, поэтому в этом  на этом шаге мы

543
00:22:59,460 --> 00:23:01,200
хотим вернуться в прошлое и понять,

544
00:23:01,200 --> 00:23:03,419
какой была среда ненаблюдаемой

545
00:23:03,419 --> 00:23:04,919
среды

546
00:23:04,919 --> 00:23:08,039
в этот конкретный момент времени,

547
00:23:08,039 --> 00:23:11,039
и мы делаем это, фиксируя все скрытые

548
00:23:11,039 --> 00:23:14,280
переменные X на некоторые конкретные данные, которые у

549
00:23:14,280 --> 00:23:16,140
нас уже есть,

550
00:23:16,140 --> 00:23:18,960
и и Выполняя это, э-э, это  сделать

551
00:23:18,960 --> 00:23:21,120
вывод об использованном,

552
00:23:21,120 --> 00:23:24,240
то мы собираемся использовать U,

553
00:23:24,240 --> 00:23:26,940
чтобы сохранить U, который мы узнали, и

554
00:23:26,940 --> 00:23:28,500
выполнить вмешательство,

555
00:23:28,500 --> 00:23:29,880
поэтому

556
00:23:29,880 --> 00:23:32,340
контрфактор также можно рассматривать как

557
00:23:32,340 --> 00:23:34,980
вмешательство в прошлое, в котором мы

558
00:23:34,980 --> 00:23:36,960
знаем окружающую среду

559
00:23:36,960 --> 00:23:40,620
переменные среды U1 U2  и u4 в этот конкретный

560
00:23:40,620 --> 00:23:43,039
момент,

561
00:23:43,200 --> 00:23:44,340
и

562
00:23:44,340 --> 00:23:46,679
какой пропущенный шаг,

563
00:23:46,679 --> 00:23:49,440
чтобы X4 был равен X3, был бы равен

564
00:23:49,440 --> 00:23:50,780
другой

565
00:23:50,780 --> 00:23:53,280
другой точке данных в этой конкретной

566
00:23:53,280 --> 00:23:55,980
ситуации, теперь теперь мы можем вычислить корреляцию,

567
00:23:55,980 --> 00:23:57,120


568
00:23:57,120 --> 00:23:59,520
и корреляцию мы делаем это на пути

569
00:23:59,520 --> 00:24:02,039
на графике  в котором мы

570
00:24:02,039 --> 00:24:04,440
уже выполнили вмешательство, используя

571
00:24:04,440 --> 00:24:06,659
переменные среды, которые мы

572
00:24:06,659 --> 00:24:10,140
узнали на этапе похищения,

573
00:24:10,140 --> 00:24:14,419
и это контрфактический вывод, это

574
00:24:15,480 --> 00:24:18,000
последний слайд причинного

575
00:24:18,000 --> 00:24:20,159
вывода, настоящее введение,

576
00:24:20,159 --> 00:24:21,720
и речь идет о структурном обучении, в

577
00:24:21,720 --> 00:24:23,880
основном все, что я сказал

578
00:24:23,880 --> 00:24:27,360
до сих пор опирается на тот факт, что мы знаем

579
00:24:27,360 --> 00:24:29,700
причинно-следственные зависимости между

580
00:24:29,700 --> 00:24:31,500
точками данных, поэтому мы знаем структуру

581
00:24:31,500 --> 00:24:33,120
графика, мы знаем, какая переменная

582
00:24:33,120 --> 00:24:34,860
влияет на какую,

583
00:24:34,860 --> 00:24:37,260
мы знаем стрелки в целом,

584
00:24:37,260 --> 00:24:39,659
но на практике это на самом деле не

585
00:24:39,659 --> 00:24:42,900
всегда возможно, поэтому мы

586
00:24:42,900 --> 00:24:45,419


587
00:24:45,419 --> 00:24:47,400
в большинстве случаев у нас нет доступа к причинно-следственному графу, и на самом деле изучение

588
00:24:47,400 --> 00:24:49,919
лучшего причинно-следственного графа из данных все еще остается

589
00:24:49,919 --> 00:24:51,840
открытой проблемой, которую мы улучшаем,

590
00:24:51,840 --> 00:24:53,880
мы становимся лучше, но

591
00:24:53,880 --> 00:24:57,299
как точно выполнить эту задачу, э-

592
00:24:57,299 --> 00:24:58,380
э,

593
00:24:58,380 --> 00:25:01,140
все еще открытая проблема

594
00:25:01,140 --> 00:25:03,179
поэтому, как я сказал, в основном цель состоит в том, чтобы

595
00:25:03,179 --> 00:25:04,740
сослаться на отношения Совета из

596
00:25:04,740 --> 00:25:07,380
данных наблюдений, поэтому, учитывая набор данных,

597
00:25:07,380 --> 00:25:09,780
мы хотим вывести точно направленный

598
00:25:09,780 --> 00:25:12,179
граф, который описывает связь

599
00:25:12,179 --> 00:25:14,460
между системой и переменными

600
00:25:14,460 --> 00:25:15,960
набора данных,

601
00:25:15,960 --> 00:25:17,700
поэтому, например, здесь у нас есть пример

602
00:25:17,700 --> 00:25:19,440
что, я думаю, мы

603
00:25:19,440 --> 00:25:22,860
все знакомы с этим спасибо из-за

604
00:25:22,860 --> 00:25:25,080
пандемии, поэтому у нас есть эти четыре

605
00:25:25,080 --> 00:25:28,799
переменные возраст госпитализации вакцины

606
00:25:28,799 --> 00:25:31,380
и и КТ,

607
00:25:31,380 --> 00:25:33,600
и мы хотим сделать вывод о причинно-следственных

608
00:25:33,600 --> 00:25:36,059
зависимостях между этими переменными, поэтому,

609
00:25:36,059 --> 00:25:37,980
например, мы хотим узнать непосредственно

610
00:25:37,980 --> 00:25:40,260
из данных, что вероятность

611
00:25:40,260 --> 00:25:43,080
человека, госпитализированного, зависит от

612
00:25:43,080 --> 00:25:45,419
его возраста и от того,

613
00:25:45,419 --> 00:25:49,760
вакцинирован он или нет, и так далее, и так далее,

614
00:25:51,299 --> 00:25:55,020
так что это конец длинного

615
00:25:55,020 --> 00:25:58,080
введения, но я надеюсь, что это было

616
00:25:58,080 --> 00:26:00,179
достаточно ясно, и я надеюсь, что я дал как

617
00:26:00,179 --> 00:26:02,039
основы, чтобы понять

618
00:26:02,039 --> 00:26:05,159
в основном результаты статьи, и

619
00:26:05,159 --> 00:26:07,740
теперь мы можем перейти к исследовательским вопросам,

620
00:26:07,740 --> 00:26:09,059
поэтому вопросы исследования следующие:

621
00:26:09,059 --> 00:26:10,440


622
00:26:10,440 --> 00:26:12,900
сначала я хочу посмотреть,

623
00:26:12,900 --> 00:26:15,299
можно ли использовать творческое кодирование для

624
00:26:15,299 --> 00:26:16,980
выполнения причинно-следственного вывода,

625
00:26:16,980 --> 00:26:20,100
поэтому оперативное кодирование до сих пор использовалось только

626
00:26:20,100 --> 00:26:22,380
чтобы выполнить вычисление корреляций

627
00:26:22,380 --> 00:26:25,020
в байесовских сетях,

628
00:26:25,020 --> 00:26:27,419
и большой вопрос заключается в том, можем ли мы выйти за рамки

629
00:26:27,419 --> 00:26:29,400
корреляции и модельного вмешательства и

630
00:26:29,400 --> 00:26:31,679
контрфактического биологически правдоподобного

631
00:26:31,679 --> 00:26:32,760
способа,

632
00:26:32,760 --> 00:26:34,380
например,

633
00:26:34,380 --> 00:26:36,120
таким образом, чтобы это было просто

634
00:26:36,120 --> 00:26:39,059
интуитивно и позволяло нам играть только с

635
00:26:39,059 --> 00:26:40,740
нейронами  и не касаться, например,

636
00:26:40,740 --> 00:26:43,740
огромной структуры графа,

637
00:26:43,740 --> 00:26:46,380
и более конкретно на практике

638
00:26:46,380 --> 00:26:48,299
возникает вопрос, можем ли мы определить

639
00:26:48,299 --> 00:26:51,000
причинно-следственную модель структуры на основе оперативного кодирования

640
00:26:51,000 --> 00:26:52,740
для выполнения вмешательств и

641
00:26:52,740 --> 00:26:55,320
контрфактических действий.

642
00:26:55,320 --> 00:26:58,380
Второй вопрос,

643
00:26:58,380 --> 00:27:00,179
как я сказал, что наличие пользовательской

644
00:27:00,179 --> 00:27:02,159
модели структуры предполагает, что мы  знать структуру

645
00:27:02,159 --> 00:27:04,260
Сети уклонения,

646
00:27:04,260 --> 00:27:07,919
поэтому мы предполагаем, что у нас есть стрелки,

647
00:27:07,919 --> 00:27:09,960
можем ли мы выйти за рамки этого и использовать творческие

648
00:27:09,960 --> 00:27:11,520
сети кодирования, чтобы изучить причинно-следственную

649
00:27:11,520 --> 00:27:14,418
структуру графа,

650
00:27:16,140 --> 00:27:18,900
в основном давая положительные ответы на

651
00:27:18,900 --> 00:27:21,120
оба эти вопроса, что позволит нам

652
00:27:21,120 --> 00:27:23,120
использовать прогнозирующее кодирование как  сквозной

653
00:27:23,120 --> 00:27:26,039
метод причинно-следственного вывода, который в основном

654
00:27:26,039 --> 00:27:28,740
берет набор данных и позволяет нам тестировать

655
00:27:28,740 --> 00:27:30,419
вмешательства и контрфактические

656
00:27:30,419 --> 00:27:34,820
прогнозы непосредственно из этого набора данных,

657
00:27:36,840 --> 00:27:39,299
поэтому давайте решим первую

658
00:27:39,299 --> 00:27:40,740
первую проблему, так что причинно-следственный вывод

659
00:27:40,740 --> 00:27:42,419
вибрационное кодирование, которое также является

660
00:27:42,419 --> 00:27:45,120
разделом, который дает  заголовок

661
00:27:45,120 --> 00:27:46,740
статьи в основном,

662
00:27:46,740 --> 00:27:48,539
и здесь я покажу, как выполнять

663
00:27:48,539 --> 00:27:50,760
оперативное кодирование корреляций, которое

664
00:27:50,760 --> 00:27:52,440
уже известно,

665
00:27:52,440 --> 00:27:54,419
и как выполнять интервенционные

666
00:27:54,419 --> 00:27:56,760
запросы, которые, как я думаю,

667
00:27:56,760 --> 00:28:01,140
являются реальным вопросом статьи,

668
00:28:01,140 --> 00:28:03,900
так что вот причинно-следственная связь.  график, который является

669
00:28:03,900 --> 00:28:05,700
обычным графиком, который у

670
00:28:05,700 --> 00:28:07,260
нас был,

671
00:28:07,260 --> 00:28:09,240
и вот соответствующая

672
00:28:09,240 --> 00:28:11,760
модель творческого кодирования, поэтому оси представляют собой

673
00:28:11,760 --> 00:28:13,980
скрытые переменные и соответствуют

674
00:28:13,980 --> 00:28:18,000
нейронам в модели нейронной сети

675
00:28:18,000 --> 00:28:20,760
и Черной стреле, переходящей из

676
00:28:20,760 --> 00:28:22,740
информации прогноза от одного нейрона

677
00:28:22,740 --> 00:28:25,559
к тому, что находится ниже по иерархии,

678
00:28:25,559 --> 00:28:28,500
и каждая вершина также имеет этот

679
00:28:28,500 --> 00:28:31,140
нейрон ошибки, который передает информацию вверх по

680
00:28:31,140 --> 00:28:32,820
иерархии, поэтому информация о каждой

681
00:28:32,820 --> 00:28:36,480
ошибке идет к узлу значения в

682
00:28:36,480 --> 00:28:39,120
верхней части иерархии и в основном говорит

683
00:28:39,120 --> 00:28:41,400
ему исправить себя, чтобы измениться  предсказание,

684
00:28:41,400 --> 00:28:43,760


685
00:28:44,700 --> 00:28:46,559
поэтому для выполнения корреляции с использованием

686
00:28:46,559 --> 00:28:48,840
предиктивного кодирования вам нужно

687
00:28:48,840 --> 00:28:50,400
сделать наблюдение и

688
00:28:50,400 --> 00:28:52,620
просто зафиксировать значение конкретного

689
00:28:52,620 --> 00:28:53,820
нейрона,

690
00:28:53,820 --> 00:28:55,200
поэтому, если вы хотите вычислить

691
00:28:55,200 --> 00:28:58,740
вероятность X4 при условии X3, равном S3,

692
00:28:58,740 --> 00:29:02,340
мы просто должны  возьмите X3 и зафиксируйте его на

693
00:29:02,340 --> 00:29:04,380
S3 таким образом, чтобы он больше не менялся,

694
00:29:04,380 --> 00:29:08,159
и запустите минимизацию энергии,

695
00:29:08,159 --> 00:29:09,720
и эта модель

696
00:29:09,720 --> 00:29:12,659
и минимизация путем обновления оси

697
00:29:12,659 --> 00:29:16,380
через минимизацию изменения

698
00:29:16,380 --> 00:29:18,419
свободной энергии позволяет модели

699
00:29:18,419 --> 00:29:20,820
сходиться к решению  на этот вопрос,

700
00:29:20,820 --> 00:29:22,919
поэтому вероятность или ожидаемое значение

701
00:29:22,919 --> 00:29:27,179
X4 при заданном X3 равно 3.

702
00:29:27,179 --> 00:29:29,340
но как мне теперь выполнить вмешательство,

703
00:29:29,340 --> 00:29:31,679
не воздействуя на структуру

704
00:29:31,679 --> 00:29:33,419
графика?

705
00:29:33,419 --> 00:29:35,640


706
00:29:35,640 --> 00:29:37,679


707
00:29:37,679 --> 00:29:39,960
выполнить

708
00:29:39,960 --> 00:29:43,260
корреляцию, чтобы зафиксировать S3 равным X3, это

709
00:29:43,260 --> 00:29:45,600
первый шаг в алгоритме, а

710
00:29:45,600 --> 00:29:47,220
второй - получить ось путем

711
00:29:47,220 --> 00:29:50,539
минимизации изменения свободной энергии,

712
00:29:51,240 --> 00:29:53,340
вмешательство, которое теоретически

713
00:29:53,340 --> 00:29:55,200
соответствует удалению этих

714
00:29:55,200 --> 00:29:56,220
стрелок

715
00:29:56,220 --> 00:29:57,659
и отвечает на вопрос

716
00:29:57,659 --> 00:29:59,279
вероятность  X4,

717
00:29:59,279 --> 00:30:02,399
выполнив вмешательство, поэтому X3

718
00:30:02,399 --> 00:30:04,860
равно трем, императивное кодирование может быть

719
00:30:04,860 --> 00:30:07,080
выполнено следующим образом,

720
00:30:07,080 --> 00:30:09,840
поэтому я собираюсь написать здесь алгоритм,

721
00:30:09,840 --> 00:30:13,140
поэтому сначала, поскольку в корреляции вы фиксируете S3,

722
00:30:13,140 --> 00:30:17,039
равный iFix X3, равный

723
00:30:17,039 --> 00:30:18,720
наблюдению, которое вы получаете

724
00:30:18,720 --> 00:30:21,299
то это важный шаг, который

725
00:30:21,299 --> 00:30:24,059
вы должны вмешиваться больше не в график,

726
00:30:24,059 --> 00:30:26,700
а в ошибку прогнозирования и

727
00:30:26,700 --> 00:30:28,980
зафиксировать ее равной нулю,

728
00:30:28,980 --> 00:30:31,020
имея ошибку прогнозирования, равную нулю,

729
00:30:31,020 --> 00:30:32,480
в основном

730
00:30:32,480 --> 00:30:36,179
делает эээ отправляет бессмысленную

731
00:30:36,179 --> 00:30:38,460
информацию вверх по иерархии или фактически

732
00:30:38,460 --> 00:30:40,200
не отправляет информацию о  иерархии,

733
00:30:40,200 --> 00:30:41,880
потому что он в основном говорит вам, что

734
00:30:41,880 --> 00:30:44,659
прогноз всегда верен,

735
00:30:44,659 --> 00:30:48,120
и третий шаг, как мы делали

736
00:30:48,120 --> 00:30:50,220
ранее, обновить ось

737
00:30:50,220 --> 00:30:52,919
без ограничений оси или X1 X2 X4,

738
00:30:52,919 --> 00:30:55,679
минимизируя изменение свободной энергии,

739
00:30:55,679 --> 00:30:59,039
как я покажу сейчас, или экспериментально,

740
00:30:59,039 --> 00:31:00,840
просто выполнив  Этот небольшой трюк с

741
00:31:00,840 --> 00:31:02,399
установкой ошибки прогнозирования

742
00:31:02,399 --> 00:31:05,120
равной нулю

743
00:31:05,640 --> 00:31:08,220
мешает нам фактически воздействовать на

744
00:31:08,220 --> 00:31:10,320
структуру графика,

745
00:31:10,320 --> 00:31:13,620
как это делает теория исчисления, и делать

746
00:31:13,620 --> 00:31:16,919
вывод об отсутствующих переменных после

747
00:31:16,919 --> 00:31:19,140
вмешательства, просто выполняя

748
00:31:19,140 --> 00:31:22,640
аберрация минимизации свободной энергии

749
00:31:24,659 --> 00:31:26,580
как насчет контрфактического вывода?

750
00:31:26,580 --> 00:31:28,080
контрфактический вывод на самом деле

751
00:31:28,080 --> 00:31:30,539
легко сделать, если мы

752
00:31:30,539 --> 00:31:34,740
Определим, как проводить вмешательство,

753
00:31:34,740 --> 00:31:36,539
и это потому, что, как мы видели ранее,

754
00:31:36,539 --> 00:31:38,640
выполнение контрфактического вмешательства

755
00:31:38,640 --> 00:31:40,380
аналогично выполнению вмешательства в прошлой

756
00:31:40,380 --> 00:31:44,360
ситуации после того, как вы сделали вывод о

757
00:31:44,360 --> 00:31:48,120
unobservable ненаблюдаемые переменные,

758
00:31:48,120 --> 00:31:49,620


759
00:31:49,620 --> 00:31:51,480
как вы можете видеть на графике, который я показал

760
00:31:51,480 --> 00:31:53,520
ранее о действии похищения и

761
00:31:53,520 --> 00:31:56,039
шагах предсказания, шагах действия и

762
00:31:56,039 --> 00:31:58,320
предсказания, у них не было этих

763
00:31:58,320 --> 00:31:59,640
двух стрелок,

764
00:31:59,640 --> 00:32:02,580
они были удалены, красивое кодирование

765
00:32:02,580 --> 00:32:06,299
позволяет нам сохранить стрелки этого э-э

766
00:32:06,299 --> 00:32:08,279
в  график

767
00:32:08,279 --> 00:32:11,340
и и выполняем контрфактические действия,

768
00:32:11,340 --> 00:32:13,380
просто выполняя шаг похищения, как это

769
00:32:13,380 --> 00:32:14,640
было сделано ранее

770
00:32:14,640 --> 00:32:16,679
шаг действия, в котором мы просто

771
00:32:16,679 --> 00:32:18,600
выполняем вмешательство в одиночный

772
00:32:18,600 --> 00:32:21,240
узел, поэтому мы фиксируем узел значения, и мы устанавливаем

773
00:32:21,240 --> 00:32:24,240
ошибку на ноль

774
00:32:24,240 --> 00:32:26,399
и запускаем минимизацию энергии до

775
00:32:26,399 --> 00:32:27,960
минимизировать продолжительность свободной энергии для

776
00:32:27,960 --> 00:32:30,679
расчета прогноза,

777
00:32:32,399 --> 00:32:36,299
поэтому я думаю, что это простой и

778
00:32:36,299 --> 00:32:39,840
элегантный метод для выполнения интервенций

779
00:32:39,840 --> 00:32:42,899
и контрфактуалов, и,

780
00:32:42,899 --> 00:32:44,880
да, я думаю, что сейчас мы должны

781
00:32:44,880 --> 00:32:46,500
показать, работает ли это на практике

782
00:32:46,500 --> 00:32:48,720
или нет, и мы  проведите пару

783
00:32:48,720 --> 00:32:49,919
экспериментов,

784
00:32:49,919 --> 00:32:52,440
и я собираюсь показать вам сейчас два

785
00:32:52,440 --> 00:32:54,240
разных эксперимента, первый из которых является

786
00:32:54,240 --> 00:32:57,179
просто экспериментальным доказательством концепции,

787
00:32:57,179 --> 00:33:01,020
который показывает, что в оперативном кодировании

788
00:33:01,020 --> 00:33:02,480
можно выполнять

789
00:33:02,480 --> 00:33:06,120
вмешательство и контрфактические действия,

790
00:33:06,120 --> 00:33:08,700
а второй фактически показывает

791
00:33:08,700 --> 00:33:11,220
простое приложение.  в том, как интервенционные

792
00:33:11,220 --> 00:33:13,440
запросы могут быть использованы для повышения

793
00:33:13,440 --> 00:33:16,260
производительности задач классификации в

794
00:33:16,260 --> 00:33:18,360
конкретном типе сетей оперативного кодирования,

795
00:33:18,360 --> 00:33:20,940
который представляет собой

796
00:33:20,940 --> 00:33:22,080
полносвязную модель,

797
00:33:22,080 --> 00:33:24,659
давайте начнем с первой,

798
00:33:24,659 --> 00:33:27,679
так как мы выполняем эту задачу, поэтому, учитывая

799
00:33:27,679 --> 00:33:30,360
модель структурного совета,

800
00:33:30,360 --> 00:33:33,360
мы  генерируем обучающие данные, и мы используем их

801
00:33:33,360 --> 00:33:35,760
для изучения весов, чтобы изучить

802
00:33:35,760 --> 00:33:39,480
функции структурных моделей Kaza,

803
00:33:39,480 --> 00:33:42,779
а затем мы генерируем тестовые тестовые данные

804
00:33:42,779 --> 00:33:44,399
как для интервенционных, так и для

805
00:33:44,399 --> 00:33:46,080
контрфракционных запросов,

806
00:33:46,080 --> 00:33:48,000
и мы показываем, можем ли мы

807
00:33:48,000 --> 00:33:51,360
сходиться к правильным тестовым данным.  с использованием

808
00:33:51,360 --> 00:33:53,340
творческого кодирования

809
00:33:53,340 --> 00:33:54,779


810
00:33:54,779 --> 00:33:57,240
и, например, здесь, на этих двух

811
00:33:57,240 --> 00:33:58,860
графиках, представлены интервенционное

812
00:33:58,860 --> 00:34:00,600
вмешательство и контрфактические запросы

813
00:34:00,600 --> 00:34:03,539
этого конкретного графика, который представляет собой

814
00:34:03,539 --> 00:34:05,880
график смещения бабочки, который является графиком,

815
00:34:05,880 --> 00:34:08,280
который часто используется в э-э для проверки того, является ли

816
00:34:08,280 --> 00:34:10,859
причинно-следственный вывод, является ли

817
00:34:10,859 --> 00:34:12,179
вмешательство и контрфактические

818
00:34:12,179 --> 00:34:15,540
методы  работа так же проста, но

819
00:34:15,540 --> 00:34:18,000
в статье вы можете найти много

820
00:34:18,000 --> 00:34:20,760
разных графиков, но в целом эти

821
00:34:20,760 --> 00:34:22,800
два графика показывают, что

822
00:34:22,800 --> 00:34:26,940
метод работает, показывает, что

823
00:34:26,940 --> 00:34:27,918


824
00:34:27,918 --> 00:34:32,219
средняя абсолютная ошибка между

825
00:34:32,219 --> 00:34:33,960
интервенционными контрфактическими величинами, которые

826
00:34:33,960 --> 00:34:37,399
мы  вычислить, а интервенционные и

827
00:34:37,399 --> 00:34:39,780
контрфактические величины из

828
00:34:39,780 --> 00:34:41,460
исходного графика

829
00:34:41,460 --> 00:34:43,800
близки друг к другу, поэтому ошибка

830
00:34:43,800 --> 00:34:45,800
довольно мала.

831
00:34:45,800 --> 00:34:49,139
Второй эксперимент, по сути, является

832
00:34:49,139 --> 00:34:51,239
расширением эксперимента, который я предложил

833
00:34:51,239 --> 00:34:54,540
в более ранней статье, который представляет собой

834
00:34:54,540 --> 00:34:56,460
изучение произвольных топологий графа.

835
00:34:56,460 --> 00:34:59,040
то, что я написал в прошлом году

836
00:34:59,040 --> 00:35:01,080
в этой статье, я

837
00:35:01,080 --> 00:35:04,200
в основном предлагаю такие

838
00:35:04,200 --> 00:35:06,060
сети в качестве доказательства концепции, которая представляет собой

839
00:35:06,060 --> 00:35:08,160
полностью связанную сеть, которая в

840
00:35:08,160 --> 00:35:11,579
целом является худшей нейронной сетью, которую вы можете

841
00:35:11,579 --> 00:35:13,500
использовать для проведения

842
00:35:13,500 --> 00:35:15,960
экспериментов по машинному обучению, потому что

843
00:35:15,960 --> 00:35:20,520
у нас есть фиксированная  набор нейронов,

844
00:35:20,520 --> 00:35:23,660
в основном у

845
00:35:23,760 --> 00:35:26,400
вас есть каждая пара нейронов,

846
00:35:26,400 --> 00:35:28,680
соединенных двумя разными синапсами, так что

847
00:35:28,680 --> 00:35:31,200
это наиболее сложная модель с

848
00:35:31,200 --> 00:35:33,359
максимально возможной сложностью, в

849
00:35:33,359 --> 00:35:34,619
целом

850
00:35:34,619 --> 00:35:36,300
хорошо то, что, поскольку у вас

851
00:35:36,300 --> 00:35:37,859
много циклов, модель чрезвычайно

852
00:35:37,859 --> 00:35:39,599
гибкая  в том смысле, что вы можете обучить

853
00:35:39,599 --> 00:35:42,480
его, например, на измельченном изображении,

854
00:35:42,480 --> 00:35:45,359
на точке данных и на его метке, но

855
00:35:45,359 --> 00:35:47,400
затем вы можете запросить его благодаря

856
00:35:47,400 --> 00:35:50,640
возвращающейся информации, вы можете

857
00:35:50,640 --> 00:35:52,140
запрашивать множество разных способов, поэтому  вы

858
00:35:52,140 --> 00:35:54,060
можете формировать задачи классификации, в которых

859
00:35:54,060 --> 00:35:55,980
вы предоставляете изображение, запускаете

860
00:35:55,980 --> 00:35:57,480
минимизацию энергии и получаете метку,

861
00:35:57,480 --> 00:35:59,400
но вы также можете, например, выполнять

862
00:35:59,400 --> 00:36:01,320
задачи генерации, в которых вы даете

863
00:36:01,320 --> 00:36:03,060
метку, запускаете минимизацию энергии и

864
00:36:03,060 --> 00:36:05,220
получаете изображение, которое вы можете выполнить,

865
00:36:05,220 --> 00:36:06,960
например, изображение  завершение, которому вы даете

866
00:36:06,960 --> 00:36:10,260
половину изображения и сходитесь, и и

867
00:36:10,260 --> 00:36:12,119
сходитесь, позволяете модели преобразовать

868
00:36:12,119 --> 00:36:14,400
вторую половину и так далее, и так далее, так что это

869
00:36:14,400 --> 00:36:16,440
в основном модель, которая изучает

870
00:36:16,440 --> 00:36:19,619
статистику набора данных в

871
00:36:19,619 --> 00:36:21,900
целом, не фокусируясь на

872
00:36:21,900 --> 00:36:25,079
классификации или  генерация в целом,

873
00:36:25,079 --> 00:36:27,900
так что эта гибкость велика,

874
00:36:27,900 --> 00:36:31,260
проблема в том, что из-за этого,

875
00:36:31,260 --> 00:36:34,140
как и каждая отдельная задача, не работает хорошо,

876
00:36:34,140 --> 00:36:35,820
поэтому вы можете делать много разных вещей,

877
00:36:35,820 --> 00:36:38,579
но ни одна из них не выполняется хорошо,

878
00:36:38,579 --> 00:36:39,960
и

879
00:36:39,960 --> 00:36:42,480
здесь я хочу показать, как использовать

880
00:36:42,480 --> 00:36:44,099
Интервенционные запросы вместо

881
00:36:44,099 --> 00:36:46,740
стандартных, э-э, корреляционных запросов или

882
00:36:46,740 --> 00:36:48,119
условных запросов

883
00:36:48,119 --> 00:36:49,980
немного улучшают результаты этих

884
00:36:49,980 --> 00:36:51,960
задач классификации,

885
00:36:51,960 --> 00:36:54,000
так каковы предположительные причины

886
00:36:54,000 --> 00:36:57,599
этих, э-э, точность теста

887
00:36:57,599 --> 00:37:01,079
на этих задачах не так высока,

888
00:37:01,079 --> 00:37:03,180
первые две причины в том, что модель

889
00:37:03,180 --> 00:37:05,640
отвлекается  в исправлении каждой

890
00:37:05,640 --> 00:37:07,920
отдельной ошибки, так что в основном вы

891
00:37:07,920 --> 00:37:09,420
представляете изображение и хотите

892
00:37:09,420 --> 00:37:11,579
получить метку, но модель фактически

893
00:37:11,579 --> 00:37:13,859
обновляет себя, чтобы также предсказать

894
00:37:13,859 --> 00:37:16,320
ошибку в изображениях,

895
00:37:16,320 --> 00:37:18,480
и вторая причина, о которой я

896
00:37:18,480 --> 00:37:21,119
сказал, заключается в том, что  структура слишком

897
00:37:21,119 --> 00:37:24,540
сложна, поэтому, опять же, исходя из аргументации бритвы

898
00:37:24,540 --> 00:37:27,079
Оккама,

899
00:37:27,079 --> 00:37:28,800


900
00:37:28,800 --> 00:37:30,720
это наихудшая модель, которую вы можете иметь, поэтому

901
00:37:30,720 --> 00:37:32,160
каждый раз, когда у вас есть модель, которая соответствует

902
00:37:32,160 --> 00:37:33,960
набору данных, эта модель будет менее

903
00:37:33,960 --> 00:37:35,579
сложной, чем эта, которая собирается

904
00:37:35,579 --> 00:37:37,560
быть предпочтительным,

905
00:37:37,560 --> 00:37:40,560
но в целом просто для того, чтобы просто изучить

906
00:37:40,560 --> 00:37:41,400
его,

907
00:37:41,400 --> 00:37:43,380
идея состоит в том, можно ли запрашивать в этой модели

908
00:37:43,380 --> 00:37:44,820
вмешательства, которые можно использовать для повышения

909
00:37:44,820 --> 00:37:46,859
производительности этих полностью

910
00:37:46,859 --> 00:37:48,599
подключенных моделей,

911
00:37:48,599 --> 00:37:51,060
ну, ответ да,

912
00:37:51,060 --> 00:37:53,160
так что вот как я выполняю интервенционные

913
00:37:53,160 --> 00:37:55,619
запросы, поэтому  Я представляю изображение в

914
00:37:55,619 --> 00:37:56,640
сеть,

915
00:37:56,640 --> 00:37:59,460
я исправляю ошибку пикселей, чтобы она была

916
00:37:59,460 --> 00:38:01,560
равна нулю, чтобы эта ошибка не

917
00:38:01,560 --> 00:38:03,180
распространялась в сети,

918
00:38:03,180 --> 00:38:05,700
а затем я вычисляю метку,

919
00:38:05,700 --> 00:38:08,400
и, как вы можете видеть, точность улучшается,

920
00:38:08,400 --> 00:38:11,339
например, с 89, используя  стандартный

921
00:38:11,339 --> 00:38:13,380
метод запроса сетей творческого кодирования

922
00:38:13,380 --> 00:38:16,800
до 92, что является точностью после

923
00:38:16,800 --> 00:38:19,020
вмешательства, и то же самое происходит

924
00:38:19,020 --> 00:38:21,540
для модных средств,

925
00:38:21,540 --> 00:38:24,420
и я думаю, что очень законный критик,

926
00:38:24,420 --> 00:38:26,940
который, вероятно, каждый подумает,

927
00:38:26,940 --> 00:38:28,920
когда увидит эти сюжеты, что хорошо, вы

928
00:38:28,920 --> 00:38:32,099
улучшаете средства от 89  до 92 по-

929
00:38:32,099 --> 00:38:36,180
прежнему отстой, и да, это правда,

930
00:38:36,180 --> 00:38:38,400
и я на самом деле в более поздних слайдах я

931
00:38:38,400 --> 00:38:40,619
собираюсь показать, как воздействовать на

932
00:38:40,619 --> 00:38:42,660
структуру этой полностью

933
00:38:42,660 --> 00:38:43,859
связанной модели

934
00:38:43,859 --> 00:38:46,500
улучшит результаты еще больше, пока

935
00:38:46,500 --> 00:38:48,480
они не достигнут точки  богатая

936
00:38:48,480 --> 00:38:50,820
производительность, которая, конечно, даже не близка к

937
00:38:50,820 --> 00:38:52,560
современной производительности,

938
00:38:52,560 --> 00:38:55,320
но она все еще на высоте, но не на

939
00:38:55,320 --> 00:38:57,380
уровне, который становится в основном приемлемым

940
00:38:57,380 --> 00:39:01,760
расследованием Кенворта,

941
00:39:02,040 --> 00:39:04,980
так что да, так что это была часть о

942
00:39:04,980 --> 00:39:08,400
причинно-следственном выводе с использованием творческого кодирования

943
00:39:08,400 --> 00:39:10,920
и я думаю, подводя итог, я могу сказать, что

944
00:39:10,920 --> 00:39:15,060
интересная часть

945
00:39:15,060 --> 00:39:17,640
результатов, которые я только что показал, заключается в том,

946
00:39:17,640 --> 00:39:19,859
что я показал, что оперативное кодирование способно

947
00:39:19,859 --> 00:39:22,560
выполнять вмешательства очень простым

948
00:39:22,560 --> 00:39:24,780
и интуитивно понятным способом, потому что вам не нужно

949
00:39:24,780 --> 00:39:26,280
действовать.  структура старого графика

950
00:39:26,280 --> 00:39:28,740
больше, иногда эти функциональные

951
00:39:28,740 --> 00:39:31,079
функции недоступны и т. д. и

952
00:39:31,079 --> 00:39:34,020
т. д., но вам просто нужно

953
00:39:34,020 --> 00:39:36,140


954
00:39:36,140 --> 00:39:39,780
вмешаться в один нейрон, исследуя

955
00:39:39,780 --> 00:39:41,640
ошибку прогноза до нуля,

956
00:39:41,640 --> 00:39:44,220
и выполнить процесс минимизации энергии,

957
00:39:44,220 --> 00:39:46,619


958
00:39:46,619 --> 00:39:49,200
и эти расширенные  позволили нам

959
00:39:49,200 --> 00:39:51,240
определить структурно-причинные модели на основе творческого кодирования.

960
00:39:51,240 --> 00:39:52,920


961
00:39:52,920 --> 00:39:54,920
Теперь мы переходим ко второй

962
00:39:54,920 --> 00:39:57,900
части работы, которая посвящена

963
00:39:57,900 --> 00:40:01,700
изучению структурной структуры,

964
00:40:02,000 --> 00:40:05,099
поэтому обучение инструкциям, как я уже сказал, имеет

965
00:40:05,099 --> 00:40:07,260
дело с проблемой изучения

966
00:40:07,260 --> 00:40:09,720
причинно-следственной структуры модели

967
00:40:09,720 --> 00:40:11,880
на основе данных наблюдений.

968
00:40:11,880 --> 00:40:13,800
на самом деле нет проблемы, которая

969
00:40:13,800 --> 00:40:17,760
существовала десятилетиями

970
00:40:17,760 --> 00:40:21,359
и всегда была до тех пор, пока пару

971
00:40:21,359 --> 00:40:24,000
лет назад не была решена с использованием

972
00:40:24,000 --> 00:40:25,560
методов комбинаторного поиска.

973
00:40:25,560 --> 00:40:26,640


974
00:40:26,640 --> 00:40:29,280


975
00:40:29,280 --> 00:40:32,880


976
00:40:32,880 --> 00:40:34,740


977
00:40:34,740 --> 00:40:36,780
размерность, а

978
00:40:36,780 --> 00:40:39,920
граф Bison, который вы хотите изучить,

979
00:40:39,920 --> 00:40:42,300
увеличивается в размерах,

980
00:40:42,300 --> 00:40:46,680
изучая его, он невероятно медленный.

981
00:40:46,680 --> 00:40:48,780
Новое решение, опубликованное

982
00:40:48,780 --> 00:40:51,000
пару лет назад в новой газете

983
00:40:51,000 --> 00:40:53,540
за 2018 год,

984
00:40:53,839 --> 00:40:55,920
показывает, что на самом деле можно

985
00:40:55,920 --> 00:40:57,900
выучить эту структуру, не используя

986
00:40:57,900 --> 00:40:59,940
метод исследования комбинатора, но с использованием

987
00:40:59,940 --> 00:41:01,619
метода, основанного на градиенте,

988
00:41:01,619 --> 00:41:05,280
и это была в основном эта квалифицированная

989
00:41:05,280 --> 00:41:07,320
проблема в целом, потому что теперь

990
00:41:07,320 --> 00:41:08,820
вы можете просто

991
00:41:08,820 --> 00:41:10,980
иметь априорные параметры, что является

992
00:41:10,980 --> 00:41:12,420
приоритетной целью, которую я собираюсь

993
00:41:12,420 --> 00:41:14,700
определить немного лучше  в этом

994
00:41:14,700 --> 00:41:15,599
слайде

995
00:41:15,599 --> 00:41:18,180
запустите градиентный спуск, и даже если у вас

996
00:41:18,180 --> 00:41:19,740
есть модель, которая имеет двойной тройной

997
00:41:19,740 --> 00:41:20,820
размер,

998
00:41:20,820 --> 00:41:23,640
алгоритм по-прежнему невероятно

999
00:41:23,640 --> 00:41:25,440
быстр,

1000
00:41:25,440 --> 00:41:28,260
и по этой причине эта статья -

1001
00:41:28,260 --> 00:41:31,200
это да, я думаю, что это что-то новое,

1002
00:41:31,200 --> 00:41:33,180
и я думаю, что уже было около  600

1003
00:41:33,180 --> 00:41:35,099
цитат или что-то в этом роде,

1004
00:41:35,099 --> 00:41:37,140
и каждая статья, которую я вижу сейчас

1005
00:41:37,140 --> 00:41:38,720
о консультировании друзей и изучении

1006
00:41:38,720 --> 00:41:42,000
структуры графика, использует их метод,

1007
00:41:42,000 --> 00:41:44,820
он просто немного меняется, чем

1008
00:41:44,820 --> 00:41:46,980
они находят более быстрые или немного лучшие

1009
00:41:46,980 --> 00:41:49,440
методы вывода, но все же они все используют

1010
00:41:49,440 --> 00:41:53,760
до того, как эта статья определила, и я

1011
00:41:53,760 --> 00:41:56,460
тоже, и мы тоже,

1012
00:41:56,460 --> 00:41:58,859
поэтому здесь мы найдем новую величину,

1013
00:41:58,859 --> 00:42:01,500
которая является матрицей агентства,

1014
00:42:01,500 --> 00:42:03,480
матрица агентства - это просто матрица, которая кодирует

1015
00:42:03,480 --> 00:42:06,359
связи модели, так что это

1016
00:42:06,359 --> 00:42:08,520
бинарная матрица и

1017
00:42:08,520 --> 00:42:10,920
в целом  является двоичной матрицей, тогда,

1018
00:42:10,920 --> 00:42:12,180
конечно, когда вы выполняете оптимизацию на основе градиента, вы

1019
00:42:12,180 --> 00:42:14,880
делаете ее непрерывной,

1020
00:42:14,880 --> 00:42:16,800
а затем в какой-то момент у вас есть некоторый порог,

1021
00:42:16,800 --> 00:42:19,800
который в основном убивает край или или

1022
00:42:19,800 --> 00:42:21,480
устанавливает его равным единице,

1023
00:42:21,480 --> 00:42:27,780
а M3 IJ равен единице, если у нас

1024
00:42:27,780 --> 00:42:30,540
есть  если байесовский граф представляет собой ребро

1025
00:42:30,540 --> 00:42:35,040
от вершины I до вершины J или ноль,

1026
00:42:35,040 --> 00:42:37,380
в противном случае, например, эта

1027
00:42:37,380 --> 00:42:39,540
матрица агентства здесь представляет

1028
00:42:39,540 --> 00:42:42,780
структуру связности этой визуальной сети,

1029
00:42:42,780 --> 00:42:44,040
и

1030
00:42:44,040 --> 00:42:46,079
в основном этот метод

1031
00:42:46,079 --> 00:42:48,780
решает две проблемы, которые мы хотим

1032
00:42:48,780 --> 00:42:51,000
об этих, об изучении

1033
00:42:51,000 --> 00:42:53,460
структуры  Идея сети уравнений состоит в том,

1034
00:42:53,460 --> 00:42:54,780
что мы начинаем с полностью

1035
00:42:54,780 --> 00:42:57,200
связанной модели, которая

1036
00:42:57,200 --> 00:43:00,240
концептуально похожа на фактически

1037
00:43:00,240 --> 00:43:02,220
эквивалентную Сети оперативного кодирования, которую

1038
00:43:02,220 --> 00:43:04,020
я определил ранее, которая является полностью

1039
00:43:04,020 --> 00:43:06,480
связанной, поэтому у вас есть много

1040
00:43:06,480 --> 00:43:08,640
вершин и каждая пара вершин.

1041
00:43:08,640 --> 00:43:10,920
соединен двумя разными ребрами,

1042
00:43:10,920 --> 00:43:13,319
и вы просто хотите обрезать те,

1043
00:43:13,319 --> 00:43:15,780
которые не нужны,

1044
00:43:15,780 --> 00:43:18,540
чтобы его можно было рассматривать как метод,

1045
00:43:18,540 --> 00:43:20,819
выполняющий сокращение модели, вы начинаете с

1046
00:43:20,819 --> 00:43:22,020
большой модели и хотите сделать ее

1047
00:43:22,020 --> 00:43:22,800
маленькой,

1048
00:43:22,800 --> 00:43:25,800
так что  Первым ингредиентом для хорошего

1049
00:43:25,800 --> 00:43:28,260
сокращения моделей, конечно же, является разреженный

1050
00:43:28,260 --> 00:43:29,220
город,

1051
00:43:29,220 --> 00:43:31,619
и то, что все используют

1052
00:43:31,619 --> 00:43:33,839
для того, чтобы сделать модель более разреженной, — это

1053
00:43:33,839 --> 00:43:36,480
априор Лапласа, который в машинном обучении

1054
00:43:36,480 --> 00:43:38,880
известен просто как норма L1,

1055
00:43:38,880 --> 00:43:40,920
которая определяется здесь как

1056
00:43:40,920 --> 00:43:43,980
решение, которое  в этой статье, о которой я

1057
00:43:43,980 --> 00:43:46,740
упоминал ранее, предлагается добавить

1058
00:43:46,740 --> 00:43:49,319
второй априор сверху, который обеспечивает то,

1059
00:43:49,319 --> 00:43:53,359
что, вероятно, является самой большой

1060
00:43:53,359 --> 00:43:55,980
характеристикой байесовских сетей,

1061
00:43:55,980 --> 00:43:57,780
на которых вы хотите выполнить причинно-следственный

1062
00:43:57,780 --> 00:43:59,819
вывод, заключается в том, что вы хотите, чтобы они были циклическими,

1063
00:43:59,819 --> 00:44:01,020


1064
00:44:01,020 --> 00:44:03,000
и в основном они показали  что

1065
00:44:03,000 --> 00:44:06,359
ацикличность может быть наложена на

1066
00:44:06,359 --> 00:44:08,160
Матрицу агентов в качестве априорной,

1067
00:44:08,160 --> 00:44:10,859
и она имеет вот такую ​​форму здесь, так что

1068
00:44:10,859 --> 00:44:14,640
это след Матрицы, который является

1069
00:44:14,640 --> 00:44:18,420
экспоненциальной величиной, умноженной на а,

1070
00:44:18,420 --> 00:44:21,859
где а снова является Матрице агентов, и

1071
00:44:21,859 --> 00:44:24,300
в основном эта величина здесь равна

1072
00:44:24,300 --> 00:44:27,900
равен нулю тогда и только тогда, когда

1073
00:44:27,900 --> 00:44:30,480
байесовская сеть или

1074
00:44:30,480 --> 00:44:32,819
любой другой график, который вы рассматриваете,

1075
00:44:32,819 --> 00:44:35,720
является кликом c,

1076
00:44:37,619 --> 00:44:40,260
поэтому я собираюсь использовать их в некоторых

1077
00:44:40,260 --> 00:44:42,960
экспериментах, чтобы эти два форсировать

1078
00:44:42,960 --> 00:44:45,660
эти два априорных значения

1079
00:44:45,660 --> 00:44:47,520
на разных видах  сетей пациентов,

1080
00:44:47,520 --> 00:44:49,200
и я пытаюсь объединить их с

1081
00:44:49,200 --> 00:44:51,540
методами, которые мы предложили ранее для

1082
00:44:51,540 --> 00:44:52,740
выполнения причинно-следственного вывода,

1083
00:44:52,740 --> 00:44:55,020
оперативного кодирования,

1084
00:44:55,020 --> 00:44:56,520
поэтому я собираюсь представить два разных

1085
00:44:56,520 --> 00:44:59,640
эксперимента, один из которых является доказательством

1086
00:44:59,640 --> 00:45:00,960
концепции, которая является стандартным

1087
00:45:00,960 --> 00:45:03,660
экспериментом, показанным в  все

1088
00:45:03,660 --> 00:45:06,599
задачи структурного обучения, которые заключаются в выводе

1089
00:45:06,599 --> 00:45:08,880
правильной байесовской сети из данных,

1090
00:45:08,880 --> 00:45:11,760
а затем я собираюсь построить на основе

1091
00:45:11,760 --> 00:45:13,500
экспериментов по классификации, которые я показал

1092
00:45:13,500 --> 00:45:14,280
ранее,

1093
00:45:14,280 --> 00:45:16,020


1094
00:45:16,020 --> 00:45:18,540
и показать, как на самом деле эти априорные данные

1095
00:45:18,540 --> 00:45:21,060
позволяют мне улучшить

1096
00:45:21,060 --> 00:45:22,500
классификацию.  точность тестовая

1097
00:45:22,500 --> 00:45:25,500
точность полносвязных

1098
00:45:25,500 --> 00:45:28,160
моделей предиктивного кодирования,

1099
00:45:29,520 --> 00:45:31,680
поэтому давайте перейдем к первому эксперименту,

1100
00:45:31,680 --> 00:45:33,300
который должен сделать вывод о структуре

1101
00:45:33,300 --> 00:45:34,980
графа,

1102
00:45:34,980 --> 00:45:37,319
и все эксперименты, все они следуют

1103
00:45:37,319 --> 00:45:39,480
в основном одному и тому же конвейеру во всех

1104
00:45:39,480 --> 00:45:42,060
работах в этой области, первый шаг -

1105
00:45:42,060 --> 00:45:45,119
создать  сеть видения из случайного

1106
00:45:45,119 --> 00:45:46,079
графа,

1107
00:45:46,079 --> 00:45:48,359
так что в основном обычно два случайных

1108
00:45:48,359 --> 00:45:50,640
графа, которые все тестируют, являются

1109
00:45:50,640 --> 00:45:53,520
ренеграфами Эрдоса и графом без масштабирования,

1110
00:45:53,520 --> 00:45:55,859
поэтому вы создаете эти большие графы,

1111
00:45:55,859 --> 00:45:58,680
которые обычно имеют 20 для 80 80

1112
00:45:58,680 --> 00:46:01,619
различных узлов и некоторые ребра,

1113
00:46:01,619 --> 00:46:04,619
которые вы выбираете случайным образом

1114
00:46:04,619 --> 00:46:06,540
и  вы используете этот график для создания

1115
00:46:06,540 --> 00:46:08,280
набора данных,

1116
00:46:08,280 --> 00:46:10,819
поэтому вы выбираете, например,

1117
00:46:10,819 --> 00:46:14,460
n точек данных Big N, и что вы делаете, так

1118
00:46:14,460 --> 00:46:16,859
это берете график, который они

1119
00:46:16,859 --> 00:46:18,780
создали ранее, и выбрасываете его,

1120
00:46:18,780 --> 00:46:20,819
вы сохраняете только набор данных

1121
00:46:20,819 --> 00:46:23,099
и задачу, которую вы  хочу решить сейчас - это

1122
00:46:23,099 --> 00:46:25,020
научиться -

1123
00:46:25,020 --> 00:46:27,420
это иметь алгоритм обучения, который

1124
00:46:27,420 --> 00:46:29,819
в основном позволяет вам

1125
00:46:29,819 --> 00:46:32,579
восстановить структуру

1126
00:46:32,579 --> 00:46:34,619
графа, который вы выбросили,

1127
00:46:34,619 --> 00:46:36,839
поэтому то, как мы это делаем здесь, заключается в том, что мы находимся

1128
00:46:36,839 --> 00:46:38,460
в полностью связанном творческом кодировании

1129
00:46:38,460 --> 00:46:41,760
модель на этом наборе данных D, используя как

1130
00:46:41,760 --> 00:46:43,800
разреженные, так и априорные значения SQL, которые мы

1131
00:46:43,800 --> 00:46:45,359
определили ранее,

1132
00:46:45,359 --> 00:46:48,780
и посмотрим, действительно ли

1133
00:46:48,780 --> 00:46:50,760
граф, к которому мы сходимся после

1134
00:46:50,760 --> 00:46:53,220
удаления

1135
00:46:53,220 --> 00:46:55,319
записей матрицы агентства, которые

1136
00:46:55,319 --> 00:46:57,599
меньше определенного порога,

1137
00:46:57,599 --> 00:47:00,060
похож на  что на исходном

1138
00:47:00,060 --> 00:47:02,359
графике,

1139
00:47:02,520 --> 00:47:04,500
и там также показано, что это

1140
00:47:04,500 --> 00:47:06,599
на самом деле так, так что это пример,

1141
00:47:06,599 --> 00:47:09,020
и я показываю много различных

1142
00:47:09,020 --> 00:47:12,420
параметризации и размеров

1143
00:47:12,420 --> 00:47:15,060
и тому подобного в документе,

1144
00:47:15,060 --> 00:47:16,920
но я думаю, что эти два примера являются наиболее

1145
00:47:16,920 --> 00:47:18,900
репрезентативными.  с ошибкой

1146
00:47:18,900 --> 00:47:20,760
График питомника и график произвольного масштаба

1147
00:47:20,760 --> 00:47:23,579
с 20 узлами,

1148
00:47:23,579 --> 00:47:25,800
и здесь слева вы можете увидеть

1149
00:47:25,800 --> 00:47:27,300
график через землю, который является

1150
00:47:27,300 --> 00:47:29,339


1151
00:47:29,339 --> 00:47:30,839
случайным образом выбранным,

1152
00:47:30,839 --> 00:47:32,599
а справа вы можете увидеть график

1153
00:47:32,599 --> 00:47:35,220
красивая модель сложности, полученная

1154
00:47:35,220 --> 00:47:37,440
из данных  набор

1155
00:47:37,440 --> 00:47:39,359
и, как вы можете видеть, они очень

1156
00:47:39,359 --> 00:47:40,500
похожи,

1157
00:47:40,500 --> 00:47:42,780
это все еще не идеально, поэтому

1158
00:47:42,780 --> 00:47:45,000
есть некоторые ошибки, но

1159
00:47:45,000 --> 00:47:47,460
в целом структура

1160
00:47:47,460 --> 00:47:49,500
работает достаточно хорошо, у нас также есть некоторые

1161
00:47:49,500 --> 00:47:52,140
количественные эксперименты,

1162
00:47:52,140 --> 00:47:54,000
которые я не показываю здесь  потому что это

1163
00:47:54,000 --> 00:47:55,740
просто огромные таблицы с большим количеством чисел,

1164
00:47:55,740 --> 00:47:57,180
и я подумал, что это может быть слишком

1165
00:47:57,180 --> 00:48:00,660
много для презентации, но

1166
00:48:00,660 --> 00:48:02,220
результаты показывают, что они работают

1167
00:48:02,220 --> 00:48:06,060
так же, как и современные методы,

1168
00:48:06,060 --> 00:48:07,920
также потому, что я должен сказать, как и большинство из них

1169
00:48:07,920 --> 00:48:10,859
качество  исходит из предыдущей версии acigli,

1170
00:48:10,859 --> 00:48:15,799
которая была представлена ​​​​в 2018 году.

1171
00:48:16,920 --> 00:48:19,680
Второй класс экспериментов — это

1172
00:48:19,680 --> 00:48:21,599
наши эксперименты по классификации, которые, как

1173
00:48:21,599 --> 00:48:23,880
я уже сказал, являются расширениями того, которым

1174
00:48:23,880 --> 00:48:25,560
я поделился ранее,

1175
00:48:25,560 --> 00:48:27,119
и идея состоит в том, чтобы использовать структурное

1176
00:48:27,119 --> 00:48:28,560
обучение для улучшения классификации.

1177
00:48:28,560 --> 00:48:31,140
по результатам классификации по

1178
00:48:31,140 --> 00:48:33,420
набору данных о средствах и моде,

1179
00:48:33,420 --> 00:48:36,780
начиная с полностью связанного графа,

1180
00:48:36,780 --> 00:48:40,560
поэтому я разделил

1181
00:48:40,560 --> 00:48:42,839
полностью связанные графические кластеры

1182
00:48:42,839 --> 00:48:46,440
нейронов, так что кластер 1B - это

1183
00:48:46,440 --> 00:48:49,140
тот, который связан с входом,

1184
00:48:49,140 --> 00:48:51,900
и все маленькие тогда  у нас есть

1185
00:48:51,900 --> 00:48:55,319
определенное количество скрытых кластеров,

1186
00:48:55,319 --> 00:48:57,720
а затем у нас есть кластер меток, который

1187
00:48:57,720 --> 00:48:58,800
является

1188
00:48:58,800 --> 00:49:01,560
классом кластера нейронов, которые

1189
00:49:01,560 --> 00:49:04,079
должны давать мне прогнозы меток,

1190
00:49:04,079 --> 00:49:06,480


1191
00:49:06,480 --> 00:49:08,700
и я обучил их, используя для

1192
00:49:08,700 --> 00:49:10,980
первого использования разреженные предыдущие  только поэтому

1193
00:49:10,980 --> 00:49:14,099
идея заключается в том, что если я обрежу

1194
00:49:14,099 --> 00:49:16,500
соединения, которые мне не нужны, из

1195
00:49:16,500 --> 00:49:17,460
модели и

1196
00:49:17,460 --> 00:49:20,880
узнаю, как модель синтаксического анализа

1197
00:49:20,880 --> 00:49:24,119
работает ли это хорошо, ответ - нет, это

1198
00:49:24,119 --> 00:49:25,500
не работает, и причина и

1199
00:49:25,500 --> 00:49:28,500
причина, почему  что вы в конце

1200
00:49:28,500 --> 00:49:30,660
графика, с которым вы сходитесь, на самом деле является

1201
00:49:30,660 --> 00:49:32,700
генерацией, поэтому в основном модель

1202
00:49:32,700 --> 00:49:36,180
учится предсказывать метку на основе

1203
00:49:36,180 --> 00:49:38,400
самой метки, поэтому она отбрасывает всю

1204
00:49:38,400 --> 00:49:40,020
информацию из ввода

1205
00:49:40,020 --> 00:49:42,480
и сохраняет только метку и, как вы можете

1206
00:49:42,480 --> 00:49:45,119
видеть здесь,  метка y предсказывает себя или

1207
00:49:45,119 --> 00:49:46,560
в других экспериментах, когда вы меняете

1208
00:49:46,560 --> 00:49:48,960
параметры, которые у вас есть, что y предсказывает в

1209
00:49:48,960 --> 00:49:52,520
нуле, что преэкс X1 предсказывает y снова,

1210
00:49:52,520 --> 00:49:55,980
так что каково решение

1211
00:49:55,980 --> 00:49:57,240
этой проблемы? решение этой

1212
00:49:57,240 --> 00:49:59,520
проблемы заключается в том, что мы

1213
00:49:59,520 --> 00:50:03,000
должны сходиться  к ациклическому графу,

1214
00:50:03,000 --> 00:50:05,220
и поэтому мы должны добавить что-то, что

1215
00:50:05,220 --> 00:50:08,000
предотвращает цикличность, и что это,

1216
00:50:08,000 --> 00:50:10,200
конечно, тот, который я уже

1217
00:50:10,200 --> 00:50:12,780
предложил, а затем я показываю второй

1218
00:50:12,780 --> 00:50:14,520
метод,

1219
00:50:14,520 --> 00:50:17,280
поэтому первый использует ранее определенный SQL,

1220
00:50:17,280 --> 00:50:18,680


1221
00:50:18,680 --> 00:50:21,359
а второй  a - это новая

1222
00:50:21,359 --> 00:50:22,859
техника, которая на самом деле использует

1223
00:50:22,859 --> 00:50:24,359
отрицательные примеры,

1224
00:50:24,359 --> 00:50:26,520
поэтому отрицательный отрицательный пример в этом

1225
00:50:26,520 --> 00:50:30,060
случае - это просто точка данных, в

1226
00:50:30,060 --> 00:50:32,280
которой у вас есть изображение, но метка

1227
00:50:32,280 --> 00:50:33,240
неверна,

1228
00:50:33,240 --> 00:50:35,220
поэтому здесь, например, у вас есть изображение

1229
00:50:35,220 --> 00:50:36,900
семерки  но ярлык, который я даю

1230
00:50:36,900 --> 00:50:39,599
модели, - это двойка,

1231
00:50:39,599 --> 00:50:40,980


1232
00:50:40,980 --> 00:50:44,579
и идея очень проста в том, что она

1233
00:50:44,579 --> 00:50:47,460
уже использовалась во многих работах,

1234
00:50:47,460 --> 00:50:49,740
поэтому каждый раз, когда модели являются положительным

1235
00:50:49,740 --> 00:50:52,079
примером, он должен увеличиваться до, чтобы

1236
00:50:52,079 --> 00:50:53,520
минимизировать вариацию  свободной энергии,

1237
00:50:53,520 --> 00:50:56,520
и каждый раз, когда она имеет отрицательный

1238
00:50:56,520 --> 00:50:58,859
пример, она должна увеличивать ее,

1239
00:50:58,859 --> 00:51:01,260
поэтому позвольте мне перейти к ошибке, которую нужно

1240
00:51:01,260 --> 00:51:04,200
минимизировать,

1241
00:51:04,200 --> 00:51:05,960
иностранную

1242
00:51:05,960 --> 00:51:08,579
с множеством экспериментов и множеством

1243
00:51:08,579 --> 00:51:10,859
экспериментов, которые мы видели, что

1244
00:51:10,859 --> 00:51:12,119
две техники

1245
00:51:12,119 --> 00:51:15,000
в основном первый приводит к тем же

1246
00:51:15,000 --> 00:51:17,220
результатам, а второй приводит к тому же

1247
00:51:17,220 --> 00:51:18,599
графику,

1248
00:51:18,599 --> 00:51:21,000
так что вот

1249
00:51:21,000 --> 00:51:22,800
новые результаты, некоторые средства и

1250
00:51:22,800 --> 00:51:25,079
модные средства с использованием двух методов,

1251
00:51:25,079 --> 00:51:27,660
которые я только что предложил,

1252
00:51:27,660 --> 00:51:30,960
и теперь мы переходим к некоторым, которые  по-

1253
00:51:30,960 --> 00:51:33,900
прежнему не очень велики, но определенно более

1254
00:51:33,900 --> 00:51:36,000
разумные точности тестов, поэтому здесь у нас

1255
00:51:36,000 --> 00:51:39,059
есть ошибка теста 3,17 для минут и

1256
00:51:39,059 --> 00:51:42,119
ошибка теста 13,98 для модных средств,

1257
00:51:42,119 --> 00:51:44,819
и на самом деле это могут быть те результаты, которые

1258
00:51:44,819 --> 00:51:48,300
можно значительно улучшить, изучив

1259
00:51:48,300 --> 00:51:51,300
структуру графика из  измельчить,

1260
00:51:51,300 --> 00:51:53,040
а затем исправить структуру

1261
00:51:53,040 --> 00:51:55,319
графика и выполнить некоторую тонкую

1262
00:51:55,319 --> 00:51:57,660
настройку, поэтому, если вы точно настроите модель на

1263
00:51:57,660 --> 00:52:00,000
правильную иерархическую структуру, в

1264
00:52:00,000 --> 00:52:01,980
какой-то момент вы достигнете точности теста,

1265
00:52:01,980 --> 00:52:03,359
которую вы ожидаете от

1266
00:52:03,359 --> 00:52:05,460
иерархической модели, но те  одни - это

1267
00:52:05,460 --> 00:52:08,099
просто та, к которой полностью подключенная модель

1268
00:52:08,099 --> 00:52:10,980
естественным образом сходится,

1269
00:52:10,980 --> 00:52:13,859
поэтому, например, из-за ошибки теста

1270
00:52:13,859 --> 00:52:15,420
18,32

1271
00:52:15,420 --> 00:52:17,339
поезда полностью подключенной модели на

1272
00:52:17,339 --> 00:52:20,359
модных средствах, просто выполняя

1273
00:52:20,359 --> 00:52:22,859
корреляции или условные запросы,

1274
00:52:22,859 --> 00:52:24,420
что является стандартным способом запроса добавления

1275
00:52:24,420 --> 00:52:26,520
модели оперативного кодирования

1276
00:52:26,520 --> 00:52:29,220
вмешательства и предварительный щелчок переменного тока

1277
00:52:29,220 --> 00:52:32,040
вместе делают эту

1278
00:52:32,040 --> 00:52:34,200
ошибку теста намного ниже,

1279
00:52:34,200 --> 00:52:37,200
и мы можем наблюдать ее для средств.

1280
00:52:37,200 --> 00:52:39,319


1281
00:52:39,780 --> 00:52:41,819


1282
00:52:41,819 --> 00:52:45,420


1283
00:52:45,420 --> 00:52:48,660
структура

1284
00:52:48,660 --> 00:52:50,339
графика,

1285
00:52:50,339 --> 00:52:52,440
поэтому я выполняю, я провожу эксперимент

1286
00:52:52,440 --> 00:52:54,960
над новым набором данных, который я имею в виду,

1287
00:52:54,960 --> 00:52:56,460
вызывая новый набор данных, это может быть

1288
00:52:56,460 --> 00:52:58,500
слишком много, это то, что я называю это набором данных с двумя средними значениями,

1289
00:52:58,500 --> 00:53:01,440
в котором у вас есть

1290
00:53:01,440 --> 00:53:04,319
точка ввода  состоит из двух разных

1291
00:53:04,319 --> 00:53:07,319
изображений, и метка зависит только от

1292
00:53:07,319 --> 00:53:08,520
второго изображения

1293
00:53:08,520 --> 00:53:10,800
в истории первого изображения,

1294
00:53:10,800 --> 00:53:12,720
поэтому идея здесь заключается в том, что

1295
00:53:12,720 --> 00:53:15,079
структура модели

1296
00:53:15,079 --> 00:53:18,540
предшествует цикличности и тому подобные вещи,

1297
00:53:18,540 --> 00:53:20,819
способные распознать, что вторая половина

1298
00:53:20,819 --> 00:53:23,400
изображения  на самом деле бессмысленно в

1299
00:53:23,400 --> 00:53:27,960
выполнении в изучении в

1300
00:53:27,960 --> 00:53:31,140
выполнении классификации,

1301
00:53:31,140 --> 00:53:33,119
как обучение ведет себя в целом, как,

1302
00:53:33,119 --> 00:53:36,480
например, у нас есть этот входной входной

1303
00:53:36,480 --> 00:53:39,000
узел выходной узел, и только узлы

1304
00:53:39,000 --> 00:53:41,940
полностью связаны, и модель

1305
00:53:41,940 --> 00:53:43,740
сходится

1306
00:53:43,740 --> 00:53:45,900
к иерархической структуре, которая является

1307
00:53:45,900 --> 00:53:48,960
тот, который, как мы знаем, лучше всего справляется с

1308
00:53:48,960 --> 00:53:50,880
задачами классификации, вот

1309
00:53:50,880 --> 00:53:53,520
пример

1310
00:53:53,520 --> 00:53:54,980


1311
00:53:54,980 --> 00:53:59,280
запуска метода обучения, поэтому в c0, который является началом

1312
00:53:59,280 --> 00:54:00,720
обучения, у

1313
00:54:00,720 --> 00:54:03,000
нас есть эта модель, поэтому s0

1314
00:54:03,000 --> 00:54:05,819
соответствует семерке, поэтому

1315
00:54:05,819 --> 00:54:08,099
первое изображение  поскольку единица

1316
00:54:08,099 --> 00:54:09,839
снова соответствует изображению из семи столбцов, у нас есть

1317
00:54:09,839 --> 00:54:12,300
метка Y и все скрытые переменные x0

1318
00:54:12,300 --> 00:54:13,800
X1 X2,

1319
00:54:13,800 --> 00:54:15,720
и модель полностью связана, поэтому

1320
00:54:15,720 --> 00:54:17,040
матрица агентства

1321
00:54:17,040 --> 00:54:20,579
заполнена единицами, нет нулей, у нас

1322
00:54:20,579 --> 00:54:23,720
есть петли и тому подобное.

1323
00:54:23,720 --> 00:54:27,319
модель на пару эпох до тех пор, пока

1324
00:54:27,319 --> 00:54:30,540
мы не узнаем сразу, что,

1325
00:54:30,540 --> 00:54:31,920
например, модель сразу

1326
00:54:31,920 --> 00:54:34,740
понимает, что четыре не нужны

1327
00:54:34,740 --> 00:54:36,839
для выполнения классификации, поэтому она не делает

1328
00:54:36,839 --> 00:54:40,740
этого, так что каждый исходящий узел из

1329
00:54:40,740 --> 00:54:43,980
второго входного кластера удаляется

1330
00:54:43,980 --> 00:54:45,900
и  что-то, что мы не поняли, это то, что

1331
00:54:45,900 --> 00:54:48,660
этот кластер

1332
00:54:48,660 --> 00:54:50,400
связан с выходом,

1333
00:54:50,400 --> 00:54:52,260
поэтому у

1334
00:54:52,260 --> 00:54:55,319
нас есть линейная карта от s0 до Y

1335
00:54:55,319 --> 00:54:56,480
напрямую,

1336
00:54:56,480 --> 00:54:59,339
которая является этой частью здесь,

1337
00:54:59,339 --> 00:55:01,160
но мы знаем, что на самом деле линейная карта

1338
00:55:01,160 --> 00:55:04,740
не самая лучшая  карта для

1339
00:55:04,740 --> 00:55:07,200
выполнения классификации средств, поэтому

1340
00:55:07,200 --> 00:55:08,700
нам нужна некоторая иерархия, нам нужна некоторая

1341
00:55:08,700 --> 00:55:11,579
глубина, чтобы улучшить результаты, и, как

1342
00:55:11,579 --> 00:55:14,220
вы можете видеть, эта линия здесь является точностью,

1343
00:55:14,220 --> 00:55:15,599


1344
00:55:15,599 --> 00:55:18,960
которая до этого момента, так что до C2

1345
00:55:18,960 --> 00:55:22,500
похожа на гм, так что это 91  что

1346
00:55:22,500 --> 00:55:24,059
немного лучше, чем линейная

1347
00:55:24,059 --> 00:55:25,500
классификация,

1348
00:55:25,500 --> 00:55:28,740
но как только вы продолжите обучение,

1349
00:55:28,740 --> 00:55:30,660
модель поймет, что ей нужна некоторая

1350
00:55:30,660 --> 00:55:33,119
иерархия, чтобы лучше соответствовать данным,

1351
00:55:33,119 --> 00:55:35,640
поэтому вы видите, что эта стрелка

1352
00:55:35,640 --> 00:55:38,760
со временем становится все сильнее и сильнее,

1353
00:55:38,760 --> 00:55:41,700
пока не поймет, что линейная

1354
00:55:41,700 --> 00:55:44,339
map на самом деле не нужна, и она

1355
00:55:44,339 --> 00:55:45,920
удаляет ее,

1356
00:55:45,920 --> 00:55:48,780
поэтому модель, с которой вы сходитесь, представляет собой

1357
00:55:48,780 --> 00:55:51,000
модель, которая начинается с нуля, идет к

1358
00:55:51,000 --> 00:55:53,760
скрытому узлу, а затем переходит к

1359
00:55:53,760 --> 00:55:57,180
метке с очень слабой линейной картой,

1360
00:55:57,180 --> 00:55:59,700
которая фактически удаляется, если вы, если

1361
00:55:59,700 --> 00:56:02,760
вы устанавливаете порог, например, если

1362
00:56:02,760 --> 00:56:05,520
порог продавца, например, 0,1 0,2, в какой-то

1363
00:56:05,520 --> 00:56:07,619
момент линейная карта забывается, и

1364
00:56:07,619 --> 00:56:10,680
все, что вы получаете, это

1365
00:56:10,680 --> 00:56:13,319
иерархическая сеть,

1366
00:56:13,319 --> 00:56:15,720
то есть она изучила

1367
00:56:15,720 --> 00:56:17,099
правильную структуру для выполнения

1368
00:56:17,099 --> 00:56:19,260
задачи классификации, которые являются иерархическими,

1369
00:56:19,260 --> 00:56:21,900
и он также узнал, что второе

1370
00:56:21,900 --> 00:56:25,020
изображение не играло никакой роли в определении

1371
00:56:25,020 --> 00:56:28,440
точности теста, и это все, что все

1372
00:56:28,440 --> 00:56:30,420
это выполняется, также все эти

1373
00:56:30,420 --> 00:56:33,839
задания просто выполняются

1374
00:56:33,839 --> 00:56:36,599
одним процессом минимизации свободной энергии, поэтому

1375
00:56:36,599 --> 00:56:38,400
вы инициализируете модель, вы определяете

1376
00:56:38,400 --> 00:56:40,859
свободную энергию, вы определяете априорные значения, поэтому

1377
00:56:40,859 --> 00:56:43,559
разреженный и щелчок C предшествует

1378
00:56:43,559 --> 00:56:45,780
запуску минимизации энергии, и

1379
00:56:45,780 --> 00:56:47,400
вы сходитесь к иерархической

1380
00:56:47,400 --> 00:56:49,500
модели в иерархическую модель, которая хорошо способна

1381
00:56:49,500 --> 00:56:51,839
выполнять классификацию на фарше,

1382
00:56:51,839 --> 00:56:54,000
а затем, если вы  затем выполните некоторую точную

1383
00:56:54,000 --> 00:56:55,800
настройку, вы достигнете очень конкурентоспособных

1384
00:56:55,800 --> 00:56:57,359
результатов, как в

1385
00:56:57,359 --> 00:56:59,339
сетях с прямой связью с распространением обратной связи,

1386
00:56:59,339 --> 00:57:01,260
но я думаю, что это не самое интересное,

1387
00:57:01,260 --> 00:57:03,780
интересно то, что вам нравится

1388
00:57:03,780 --> 00:57:05,160
весь этот процесс, весь этот процесс

1389
00:57:05,160 --> 00:57:07,980
вместе с вмешательством, и

1390
00:57:07,980 --> 00:57:09,780
ацикличность

1391
00:57:09,780 --> 00:57:11,700
позволяет вам  взять полностью подключенную

1392
00:57:11,700 --> 00:57:12,660
сеть

1393
00:57:12,660 --> 00:57:15,119
и сойтись в иерархической,

1394
00:57:15,119 --> 00:57:16,140
которая способна выполнять

1395
00:57:16,140 --> 00:57:20,058
классификацию с хорошими результатами,

1396
00:57:20,760 --> 00:57:23,000
и да, это

1397
00:57:23,000 --> 00:57:26,280
в основном все, что я сейчас о, да, вау, я

1398
00:57:26,280 --> 00:57:29,220
много говорил, и я ... это

1399
00:57:29,220 --> 00:57:32,160
вывод  Я

1400
00:57:32,160 --> 00:57:35,280
в основном делаю небольшое резюме, и я

1401
00:57:35,280 --> 00:57:37,559
думаю, что важный вывод, который я

1402
00:57:37,559 --> 00:57:39,300
должен дать вам в одном предложении этой

1403
00:57:39,300 --> 00:57:40,980
статьи, заключается в том, что прогностическое кодирование — это

1404
00:57:40,980 --> 00:57:44,400
метод обновления убеждений, который способен

1405
00:57:44,400 --> 00:57:46,559
выполнять  - Завершить обучение двоюродного брата, чтобы

1406
00:57:46,559 --> 00:57:48,599
он мог выполнять вмешательства, чтобы

1407
00:57:48,599 --> 00:57:51,420
изучить структуру из данных, а затем

1408
00:57:51,420 --> 00:57:53,160
выполнять вмешательства и

1409
00:57:53,160 --> 00:57:56,058
контрфактуалы,

1410
00:57:56,700 --> 00:57:58,440
чтобы делать причинно-следственные выводы в других и

1411
00:57:58,440 --> 00:58:00,119
эффективно моделировать вмешательства,

1412
00:58:00,119 --> 00:58:01,680
просто устанавливая ошибку прогноза на

1413
00:58:01,680 --> 00:58:03,359
ноль, так что это очень простая

1414
00:58:03,359 --> 00:58:06,240
техника для выполнения  вмешательства, и

1415
00:58:06,240 --> 00:58:07,619
вам просто нужно коснуться только одного

1416
00:58:07,619 --> 00:58:08,940
нейрона, вам не нужно воздействовать на

1417
00:58:08,940 --> 00:58:10,859
структуру графа, вы

1418
00:58:10,859 --> 00:58:14,339
можете использовать его для выполнения, чтобы

1419
00:58:14,339 --> 00:58:16,140
создать структурно-причинные модели, которые являются

1420
00:58:16,140 --> 00:58:18,359
биологически правдоподобными,

1421
00:58:18,359 --> 00:58:20,819
он способен изучить структуру для

1422
00:58:20,819 --> 00:58:24,119
из данных, как я уже говорил, может быть, много раз,

1423
00:58:24,119 --> 00:58:26,940


1424
00:58:26,940 --> 00:58:28,740
и пару предложений о будущих

1425
00:58:28,740 --> 00:58:31,260
работах, что было

1426
00:58:31,260 --> 00:58:33,180
бы неплохо

1427
00:58:33,180 --> 00:58:36,119
улучшить производительность модели, которую мы

1428
00:58:36,119 --> 00:58:38,460
определили, потому что я думаю, что она

1429
00:58:38,460 --> 00:58:40,980
работает достаточно хорошо на  много

1430
00:58:40,980 --> 00:58:43,079
задач, поэтому он достаточно хорошо работает при

1431
00:58:43,079 --> 00:58:45,780
структурном обучении, для меня

1432
00:58:45,780 --> 00:58:48,119
вмешательство и контрфактуалы, но

1433
00:58:48,119 --> 00:58:49,440
на самом деле, если вы посмотрите на современную

1434
00:58:49,440 --> 00:58:51,420
модель, всегда есть очень

1435
00:58:51,420 --> 00:58:53,880
специфический метод, который лучше работает в

1436
00:58:53,880 --> 00:58:55,559
одной задаче,

1437
00:58:55,559 --> 00:58:58,260
так что было бы интересно посмотреть  если мы

1438
00:58:58,260 --> 00:59:00,180
сможем достичь такого уровня производительности

1439
00:59:00,180 --> 00:59:03,599
в конкретных задачах, добавив некоторые

1440
00:59:03,599 --> 00:59:05,599
приемы или некоторые

1441
00:59:05,599 --> 00:59:10,260
или некоторые новые методы оптимизации и

1442
00:59:10,260 --> 00:59:12,839
обобщив их на динамические системы,

1443
00:59:12,839 --> 00:59:14,280
которые на самом деле намного интереснее

1444
00:59:14,280 --> 00:59:17,220
статических систем, таких как динамические

1445
00:59:17,220 --> 00:59:20,099
причинно-следственные модели и/или  другие методы,

1446
00:59:20,099 --> 00:59:22,200
которые позволяют вам делать

1447
00:59:22,200 --> 00:59:25,200
причинно-следственные выводы в движущихся системах, поэтому

1448
00:59:25,200 --> 00:59:27,799
действие, предпринятое на определенном временном шаге,

1449
00:59:27,799 --> 00:59:30,299
влияет на другой узел на более позднем временном

1450
00:59:30,299 --> 00:59:32,640
шаге, что в основном представляет собой причинно-следственную связь величия.

1451
00:59:32,640 --> 00:59:34,859


1452
00:59:34,859 --> 00:59:38,160


1453
00:59:38,160 --> 00:59:41,118


1454
00:59:47,460 --> 00:59:51,119
потрясающая и очень всеобъемлющая

1455
00:59:51,119 --> 00:59:53,160
презентация, которая была действительно думаю, что

1456
00:59:53,160 --> 00:59:55,700
вы отключены,

1457
00:59:57,119 --> 00:59:59,700
извините, отключен звук в Zoom, но да, спасибо за

1458
00:59:59,700 --> 01:00:02,400
потрясающую и очень всеобъемлющую

1459
01:00:02,400 --> 01:00:05,099
презентацию, там было действительно много,

1460
01:00:05,099 --> 01:00:06,900
а также было много отличных

1461
01:00:06,900 --> 01:00:09,900
вопросов в чате, так что, возможно, чтобы

1462
01:00:09,900 --> 01:00:12,900
согреться  на вопросы, как вы пришли

1463
01:00:12,900 --> 01:00:15,960
к изучению этой темы, изучали ли вы

1464
01:00:15,960 --> 01:00:18,900
причинно-следственные связи и обнаружили, что прогностическое кодирование

1465
01:00:18,900 --> 01:00:21,000
полезно, или наоборот, или как вы

1466
01:00:21,000 --> 01:00:23,160
пришли к этому пересечению,

1467
01:00:23,160 --> 01:00:25,740
я должен сказать, что первым,

1468
01:00:25,740 --> 01:00:27,240
кто выдвинул эту идею, был

1469
01:00:27,240 --> 01:00:29,040
эээ был барон

1470
01:00:29,040 --> 01:00:33,900
так себе вроде как я думаю

1471
01:00:33,900 --> 01:00:36,660
полтора года назад даже больше он принес вроде

1472
01:00:36,660 --> 01:00:38,940
страницу с этой идеей а потом он

1473
01:00:38,940 --> 01:00:42,119
забылся и никто ее не подхватил и

1474
01:00:42,119 --> 01:00:43,980
прошлым летом я начал

1475
01:00:43,980 --> 01:00:47,880
интересоваться причинно-следственной связью и

1476
01:00:47,880 --> 01:00:50,339
гм  Я читал, например, Книгу Жизни,

1477
01:00:50,339 --> 01:00:52,440
когда слушал подкасты. Я знаю

1478
01:00:52,440 --> 01:00:53,760
стандартный способ, которым вы интересуетесь

1479
01:00:53,760 --> 01:00:54,900
темой,

1480
01:00:54,900 --> 01:00:57,480
и я помню эту идею от

1481
01:00:57,480 --> 01:01:00,180
Барона и предложил ее ему, и

1482
01:01:00,180 --> 01:01:03,180
я подумал, почему бы и нет.  мы расширяем его и

1483
01:01:03,180 --> 01:01:06,000
на самом деле делаем его документом, поэтому я

1484
01:01:06,000 --> 01:01:07,319
привлек несколько человек, чтобы они помогли мне с

1485
01:01:07,319 --> 01:01:09,359
экспериментами, и это окончательный

1486
01:01:09,359 --> 01:01:12,000
результат в конце.

1487
01:01:12,000 --> 01:01:14,160


1488
01:01:14,160 --> 01:01:15,240


1489
01:01:15,240 --> 01:01:17,400


1490
01:01:17,400 --> 01:01:19,619
сначала и ответить на кучу

1491
01:01:19,619 --> 01:01:21,240
разных вопросов, и если кто-то еще

1492
01:01:21,240 --> 01:01:22,440
хочет добавить меня, я сначала включу свет,

1493
01:01:22,440 --> 01:01:24,059
потому что я думаю, что

1494
01:01:24,059 --> 01:01:28,440
все больше и больше погружаюсь в темноту, да,

1495
01:01:28,440 --> 01:01:30,720
кто сказал, что активный вывод не может решить

1496
01:01:30,720 --> 01:01:32,160
проблема с темной комнатой,

1497
01:01:32,160 --> 01:01:34,980
о да, мы здесь,

1498
01:01:34,980 --> 01:01:37,020
так что вы могли бы сказать, что выключатель света сделал

1499
01:01:37,020 --> 01:01:39,299
ее светлее,

1500
01:01:39,299 --> 01:01:40,680
да,

1501
01:01:40,680 --> 01:01:42,240
я думаю, что

1502
01:01:42,240 --> 01:01:43,980
здесь нет проблем,

1503
01:01:43,980 --> 01:01:46,940
эм, ладно, мл. Dawn написала,

1504
01:01:46,940 --> 01:01:49,559
так как в предиктивном кодировании все

1505
01:01:49,559 --> 01:01:52,020
распределения обычно являются гауссовыми,

1506
01:01:52,020 --> 01:01:53,760
сообщения снизу вверх представляют собой точное

1507
01:01:53,760 --> 01:01:55,500
взвешенное предсказание  ошибки, где

1508
01:01:55,500 --> 01:01:57,420
точность является обратной гауссовой

1509
01:01:57,420 --> 01:02:00,000
ковариации, что, если

1510
01:02:00,000 --> 01:02:03,319
используются негауссовские распределения, в

1511
01:02:03,780 --> 01:02:05,339


1512
01:02:05,339 --> 01:02:09,059
основном общий метод остается

1513
01:02:09,059 --> 01:02:10,380
другим, основное отличие состоит в том, что у

1514
01:02:10,380 --> 01:02:13,079
вас нет ошибок прогнозирования,

1515
01:02:13,079 --> 01:02:15,480
которые, как было правильно указано,

1516
01:02:15,480 --> 01:02:18,480
в основном  производная

1517
01:02:18,480 --> 01:02:20,819
виртуальной свободной энергии, если у вас есть гауссовские

1518
01:02:20,819 --> 01:02:22,920
предположения,

1519
01:02:22,920 --> 01:02:25,020
да, у вас даже есть эта единственная величина, которую

1520
01:02:25,020 --> 01:02:27,960
нужно установить на ноль, и вам, вероятно,

1521
01:02:27,960 --> 01:02:29,880
придется воздействовать на структуру

1522
01:02:29,880 --> 01:02:30,900
графика

1523
01:02:30,900 --> 01:02:34,020
для выполнения вмешательств,

1524
01:02:34,020 --> 01:02:37,079
а также у вас и ваших коллег была

1525
01:02:37,079 --> 01:02:39,900
статья с прогнозом на 2022 год.  кодирование Помимо

1526
01:02:39,900 --> 01:02:41,880
гауссовского распределения, которое

1527
01:02:41,880 --> 01:02:43,859
рассматривало некоторые из этих проблем правильно, да, именно

1528
01:02:43,859 --> 01:02:46,260
так, эта статья была немного

1529
01:02:46,260 --> 01:02:47,339


1530
01:02:47,339 --> 01:02:50,460
идея, лежащая в основе этой статьи,

1531
01:02:50,460 --> 01:02:53,220
и мы моделируем Трансформеры, это

1532
01:02:53,220 --> 01:02:54,420
самая большая мотивация, используя довольно

1533
01:02:54,420 --> 01:02:57,180
сложно, и ответ, э-э, не

1534
01:02:57,180 --> 01:02:59,460
потому, что  механизм внимания имеет

1535
01:02:59,460 --> 01:03:02,099
мягкий Макс в конце, а мягкий Макс обращается

1536
01:03:02,099 --> 01:03:03,960
к,

1537
01:03:03,960 --> 01:03:08,400
например, не к гауссовскому распределению, а к

1538
01:03:08,400 --> 01:03:11,280
да, к мягкому распределению Макса, я

1539
01:03:11,280 --> 01:03:13,440
не понимаю имя сейчас, но да,

1540
01:03:13,440 --> 01:03:16,079
и так что да, это обобщение,

1541
01:03:16,079 --> 01:03:19,140
это немного  сложно назвать это, как только

1542
01:03:19,140 --> 01:03:20,700
вы уберете предположение Гастона, все

1543
01:03:20,700 --> 01:03:22,319
еще немного сложно назвать это

1544
01:03:22,319 --> 01:03:24,059
творческим кодированием,

1545
01:03:24,059 --> 01:03:26,400
так что он

1546
01:03:26,400 --> 01:03:29,819
такой, например, как разговаривает с машиной

1547
01:03:29,819 --> 01:03:32,700
Фристоун, либо ему нравится творческое кодирование,

1548
01:03:32,700 --> 01:03:35,160
только если вы, если у вас есть только гаусс

1549
01:03:35,160 --> 01:03:37,680
и гаусс  предположения,

1550
01:03:37,680 --> 01:03:39,720
но да, это скорее философская

1551
01:03:39,720 --> 01:03:42,660
дискуссия, чем

1552
01:03:42,660 --> 01:03:44,940
интересная, и еще одна, я думаю, тема,

1553
01:03:44,940 --> 01:03:46,740
которая определенно представляет большой

1554
01:03:46,740 --> 01:03:49,500
интерес, — это сходства и различия

1555
01:03:49,500 --> 01:03:52,980
между аппаратом внимания в «

1556
01:03:52,980 --> 01:03:56,099
Трансформерах» и тем, как внимание

1557
01:03:56,099 --> 01:03:58,440
описывается с нейрокогнитивной

1558
01:03:58,440 --> 01:04:00,180
точки зрения и с

1559
01:04:00,180 --> 01:04:03,240
точки зрения предсказательной обработки.  угол ожидания, что

1560
01:04:03,240 --> 01:04:06,200
вы думаете об этом,

1561
01:04:06,359 --> 01:04:08,700
ну, идея в том, что, хм, да,

1562
01:04:08,700 --> 01:04:12,359
я думаю, что с точки зрения

1563
01:04:12,359 --> 01:04:15,000
красивой обработки, а также с

1564
01:04:15,000 --> 01:04:16,400
точки зрения операционного вывода,

1565
01:04:16,400 --> 01:04:19,260
внимание можно рассматривать как своего рода

1566
01:04:19,260 --> 01:04:21,299
проблему структурного обучения.

1567
01:04:21,299 --> 01:04:23,040
недавняя статья

1568
01:04:23,040 --> 01:04:25,680
от группы Криса Бакли, которая показывает,

1569
01:04:25,680 --> 01:04:26,339
что

1570
01:04:26,339 --> 01:04:28,079
должна быть

1571
01:04:28,079 --> 01:04:30,420
перепечатка в архиве, в которой в основном

1572
01:04:30,420 --> 01:04:31,859
они показали, что механизм внимания

1573
01:04:31,859 --> 01:04:35,819
просто изучает точность

1574
01:04:35,819 --> 01:04:38,880
параметров веса, характерных для

1575
01:04:38,880 --> 01:04:41,040
других точек данных, поэтому эта точность  это

1576
01:04:41,040 --> 01:04:43,200
не это не это это не параметр,

1577
01:04:43,200 --> 01:04:45,540
который находится в структуре модели, поэтому

1578
01:04:45,540 --> 01:04:47,579
это не конкретный параметр модели, это

1579
01:04:47,579 --> 01:04:49,140
быстро меняющийся параметр, такой как

1580
01:04:49,140 --> 01:04:51,660
узлы значений, которые обновляются при

1581
01:04:51,660 --> 01:04:53,760
минимизации изменения свободной энергии,

1582
01:04:53,760 --> 01:04:55,440
и как только они  вы минимизировали его

1583
01:04:55,440 --> 01:04:57,000
и вычислили, затем вы его выбрасываете,

1584
01:04:57,000 --> 01:04:58,920
а для следующей точки данных вам нужно

1585
01:04:58,920 --> 01:05:00,780
пересчитать его с нуля,

1586
01:05:00,780 --> 01:05:03,299
так что да, я думаю, что мудрое

1587
01:05:03,299 --> 01:05:05,819
вычисление аналогии заключается в том, что

1588
01:05:05,819 --> 01:05:07,920
механизм внимания можно рассматривать как своего рода

1589
01:05:07,920 --> 01:05:10,559
структурное обучение, но  структурное

1590
01:05:10,559 --> 01:05:13,020
обучение, которое зависит от точки данных, а

1591
01:05:13,020 --> 01:05:15,119
не от модели,

1592
01:05:15,119 --> 01:05:17,280
и я думаю, что если мы хотим немного обобщить

1593
01:05:17,280 --> 01:05:18,960
и перейти от

1594
01:05:18,960 --> 01:05:20,339
механизма внимания в

1595
01:05:20,339 --> 01:05:21,900
Трансформерах к механизму внимания,

1596
01:05:21,900 --> 01:05:24,180
когнитивной науке,

1597
01:05:24,180 --> 01:05:28,020
я чувствую, что они, вероятно, два разных, чтобы

1598
01:05:28,020 --> 01:05:31,260
любить рисовать сходство и  эээ,

1599
01:05:31,260 --> 01:05:33,359
я думаю, что аналогия со структурным обучением

1600
01:05:33,359 --> 01:05:36,660
и тем, насколько важна одна связь

1601
01:05:36,660 --> 01:05:38,760
по отношению к другой, вероятно,

1602
01:05:38,760 --> 01:05:41,900
делает работу намного лучше, классный

1603
01:05:42,000 --> 01:05:44,880
серый ответ, хорошо,

1604
01:05:44,880 --> 01:05:49,200


1605
01:05:49,200 --> 01:05:51,240


1606
01:05:51,240 --> 01:05:55,440
мл.

1607
01:05:55,440 --> 01:05:59,180
что вы можете,

1608
01:05:59,540 --> 01:06:01,740
я думаю, главное из них заключается в том, что вы не можете

1609
01:06:01,740 --> 01:06:03,599
наблюдать за использованием, которое

1610
01:06:03,599 --> 01:06:05,819
вы можете использовать, потому что вы можете, вы можете

1611
01:06:05,819 --> 01:06:09,000
их вычислить и исправить, но вы не можете,

1612
01:06:09,000 --> 01:06:10,559
идея в том, что у вас нет контроля

1613
01:06:10,559 --> 01:06:13,380
над ними, поэтому они используют использование должно быть

1614
01:06:13,380 --> 01:06:16,020
рассматриваются как переменные, специфичные для среды,

1615
01:06:16,020 --> 01:06:18,540
что они есть, они влияют на

1616
01:06:18,540 --> 01:06:21,240
ваш процесс, потому что,

1617
01:06:21,240 --> 01:06:23,280
например, когда вы возвращаетесь в прошлое,

1618
01:06:23,280 --> 01:06:25,079
среда отличается, поэтому идея,

1619
01:06:25,079 --> 01:06:26,520
например, заключается в том, если вам

1620
01:06:26,520 --> 01:06:28,440
нравится возвращаться к предыдущему примеру

1621
01:06:28,440 --> 01:06:29,880


1622
01:06:29,880 --> 01:06:31,920
из  ожидаемый доход человека с

1623
01:06:31,920 --> 01:06:34,619
определенным уровнем интеллекта в области образования, э-

1624
01:06:34,619 --> 01:06:37,440
э, образования,

1625
01:06:37,440 --> 01:06:40,200
идея состоит в том, что если я хочу увидеть,

1626
01:06:40,200 --> 01:06:43,559
сколько я узнаю сегодня, э-э, с тем, что

1627
01:06:43,559 --> 01:06:45,359
я не знаю, со степенью магистра

1628
01:06:45,359 --> 01:06:47,339
отличается относительно  то, сколько я

1629
01:06:47,339 --> 01:06:48,359
заработал бы

1630
01:06:48,359 --> 01:06:50,819
20 лет назад со степенью магистра,

1631
01:06:50,819 --> 01:06:52,619
отличается, например, здесь, в Италии, по

1632
01:06:52,619 --> 01:06:55,440
сравнению с другими странами, и все те

1633
01:06:55,440 --> 01:06:57,000
переменные, которые не находятся под вашим

1634
01:06:57,000 --> 01:06:58,859
контролем, вы не можете смоделировать их, используя свое

1635
01:06:58,859 --> 01:07:00,359
видение Сеть,

1636
01:07:00,359 --> 01:07:03,480
но они там есть, так что вы  вы

1637
01:07:03,480 --> 01:07:05,220
не можете игнорировать их, когда вы

1638
01:07:05,220 --> 01:07:07,559
хотите сделать выводы, так что он да, это

1639
01:07:07,559 --> 01:07:08,760
в основном все, что вы

1640
01:07:08,760 --> 01:07:10,079
не можете контролировать,

1641
01:07:10,079 --> 01:07:13,079
вы можете вывести их, чтобы вы могли, вы можете

1642
01:07:13,079 --> 01:07:14,819
сделать контрфактический

1643
01:07:14,819 --> 01:07:16,740
вывод в прошлое и сказать, о, 20

1644
01:07:16,740 --> 01:07:19,020
лет назад я бы заслужил это  много, если бы

1645
01:07:19,020 --> 01:07:20,640


1646
01:07:20,640 --> 01:07:22,559
я был таким умным, что эта

1647
01:07:22,559 --> 01:07:24,599
степень в среднем, конечно,

1648
01:07:24,599 --> 01:07:27,059
и но дело не в том, что я могу изменить

1649
01:07:27,059 --> 01:07:30,720
политику правительства в отношении рабочих мест или тому

1650
01:07:30,720 --> 01:07:32,819
подобных вещей,

1651
01:07:32,819 --> 01:07:35,099
это более глубокая контрфактика,

1652
01:07:35,099 --> 01:07:38,400
да, именно так, да, это

1653
01:07:38,400 --> 01:07:40,200
здорово, хорошо,

1654
01:07:40,200 --> 01:07:42,480
есть  вы реализовали обобщенные

1655
01:07:42,480 --> 01:07:45,660
координаты в прогнозирующем кодировании

1656
01:07:45,660 --> 01:07:46,920
нет,

1657
01:07:46,920 --> 01:07:50,039
нет, я никогда этого не делал, я э-э,

1658
01:07:50,039 --> 01:07:52,680
да, я изучал это, но я э-э, я

1659
01:07:52,680 --> 01:07:55,260
никогда не реализовывал это, я знаю, что они имеют тенденцию

1660
01:07:55,260 --> 01:07:57,599
быть нестабильными, и э-э,

1661
01:07:57,599 --> 01:08:00,299
и это  очень трудно сделать их стабильными, я

1662
01:08:00,299 --> 01:08:02,940
думаю, это вывод,

1663
01:08:02,940 --> 01:08:05,460
который я сделал, разговаривая с людьми, которые

1664
01:08:05,460 --> 01:08:08,359
их реализовали,

1665
01:08:08,400 --> 01:08:11,039
но, да, да, я знаю о некоторых

1666
01:08:11,039 --> 01:08:12,839
недавно опубликованных документах

1667
01:08:12,839 --> 01:08:15,599
о них, которые тестировались на

1668
01:08:15,599 --> 01:08:18,000
какой-то британской нагрузке.  стиль кодировщика на самом деле

1669
01:08:18,000 --> 01:08:20,520
я думаю, что все еще от Барона есть

1670
01:08:20,520 --> 01:08:22,979
статья, которая вышла

1671
01:08:22,979 --> 01:08:25,439
прошлым летом, но нет, я никогда не играл

1672
01:08:25,439 --> 01:08:26,580
с ними сам

1673
01:08:26,580 --> 01:08:29,160
классный комбинезон,

1674
01:08:29,160 --> 01:08:32,040
добавление большего количества уровней в иерархию

1675
01:08:32,040 --> 01:08:35,160
уменьшает проблему отвлечения внимания, связанную с

1676
01:08:35,160 --> 01:08:38,238
прогнозированием ввода,

1677
01:08:38,939 --> 01:08:41,698
добавляя больше  уровень

1678
01:08:41,698 --> 01:08:43,439
в этом смысле, потому что

1679
01:08:43,439 --> 01:08:45,779
проблема разрушения задается Cycles, поэтому в основном

1680
01:08:45,779 --> 01:08:47,399
вы предоставляете изображение

1681
01:08:47,399 --> 01:08:49,920
и тот факт, что у вас есть

1682
01:08:49,920 --> 01:08:53,279
участки, выходящие из изображения, идущие

1683
01:08:53,279 --> 01:08:55,799
в нейроны, а затем другие ребра,

1684
01:08:55,799 --> 01:08:57,500
возвращающиеся назад,

1685
01:08:57,500 --> 01:08:59,939
это в основном создает тот факт, что  у вас

1686
01:08:59,939 --> 01:09:03,560
есть ошибка в том, что те, в основном,

1687
01:09:03,560 --> 01:09:06,179
эти входящие приспосабливаются к пикселям

1688
01:09:06,179 --> 01:09:08,339
изображения, они создают некоторые

1689
01:09:08,339 --> 01:09:09,719
ошибки прогнозирования, поэтому у вас есть некоторые

1690
01:09:09,719 --> 01:09:12,140
ошибки прогнозирования, которые распространяются внутри модели,

1691
01:09:12,140 --> 01:09:14,640
и это да, и эта проблема, я думаю,

1692
01:09:14,640 --> 01:09:16,979
является общей для циклов и  это, вероятно,

1693
01:09:16,979 --> 01:09:21,439
не связано с иерархией в целом

1694
01:09:23,060 --> 01:09:25,140
с пикселями,

1695
01:09:25,140 --> 01:09:26,759
если у вас нет входящих ребер, у вас

1696
01:09:26,759 --> 01:09:27,660


1697
01:09:27,660 --> 01:09:30,540
нет проблем с разрушением, это больше

1698
01:09:30,540 --> 01:09:33,238
круто, и спецификация

1699
01:09:33,238 --> 01:09:35,939
ациклической сети через оператор трассировки,

1700
01:09:35,939 --> 01:09:37,859


1701
01:09:37,859 --> 01:09:41,819
это очень интересный метод, и

1702
01:09:41,819 --> 01:09:46,339
когда это было введено  в игру,

1703
01:09:46,560 --> 01:09:49,140
насколько я знаю, я думаю, что он выпустил

1704
01:09:49,140 --> 01:09:52,380
статью, которую я цитировал в 2018 году. Я

1705
01:09:52,380 --> 01:09:54,360
не знаю, по крайней мере, в литературе по причинно-следственным выводам.

1706
01:09:54,360 --> 01:09:56,940


1707
01:09:56,940 --> 01:09:59,699


1708
01:09:59,699 --> 01:10:01,860
Я имею в виду, что это очень

1709
01:10:01,860 --> 01:10:04,140
цитируемая статья, поэтому я бы сказал, что они пришли

1710
01:10:04,140 --> 01:10:05,520
с этой идеей,

1711
01:10:05,520 --> 01:10:07,980
вау, да, это довольно хорошо, что

1712
01:10:07,980 --> 01:10:09,480
вы можете сделать градиентный спуск и изучить

1713
01:10:09,480 --> 01:10:11,400
структуру. Я думаю, что

1714
01:10:11,400 --> 01:10:14,219
это очень мощная техника, да,

1715
01:10:14,219 --> 01:10:15,840
иногда это похоже на то, когда вы смотрите на

1716
01:10:15,840 --> 01:10:17,640
когда стали доступны различные

1717
01:10:17,640 --> 01:10:19,440
черты байесовского вывода и

1718
01:10:19,440 --> 01:10:23,159
причинно-следственного вывода,

1719
01:10:23,159 --> 01:10:25,620
это действительно замечательно, например, почему

1720
01:10:25,620 --> 01:10:28,500
это не было сделано в рамках байесовского

1721
01:10:28,500 --> 01:10:30,719
причинно-следственного моделирования,

1722
01:10:30,719 --> 01:10:32,760
потому что это происходило всего от пяти до

1723
01:10:32,760 --> 01:10:36,659
25 лет,

1724
01:10:36,659 --> 01:10:39,960
и это очень, очень коротко  а также

1725
01:10:39,960 --> 01:10:42,060
это относительно техническое, поэтому

1726
01:10:42,060 --> 01:10:43,920


1727
01:10:43,920 --> 01:10:46,920
в нем участвует относительно мало исследовательских групп, и это просто

1728
01:10:46,920 --> 01:10:49,860
действительно круто, что это позволяет

1729
01:10:49,860 --> 01:10:51,960
нет, да, точно, я имею в виду, что это также я

1730
01:10:51,960 --> 01:10:54,179
думаю, что захватывающая часть этой области

1731
01:10:54,179 --> 01:10:56,040
немного, что э-э, я имею в виду, что

1732
01:10:56,040 --> 01:10:59,100
определенно есть  прорывы там, которые

1733
01:10:59,100 --> 01:11:01,020
все еще должны быть обнаружены и, вероятно,

1734
01:11:01,020 --> 01:11:03,000
как, потому что, например, как

1735
01:11:03,000 --> 01:11:05,300
прорыв, который был в той статье, которую

1736
01:11:05,300 --> 01:11:07,800
они нашли, э-э,

1737
01:11:07,800 --> 01:11:09,960
как они просто нашли правильный

1738
01:11:09,960 --> 01:11:12,120
априор для ациклических структур

1739
01:11:12,120 --> 01:11:14,040
хорошо, это

1740
01:11:14,040 --> 01:11:17,100
да, я имею в виду, я не знаю  точно, но

1741
01:11:17,100 --> 01:11:19,080
это может быть идея, которая у вас возникла за один

1742
01:11:19,080 --> 01:11:21,120
день, я не знаю,

1743
01:11:21,120 --> 01:11:23,040
как это придумали другие,

1744
01:11:23,040 --> 01:11:25,320
но потенциально может быть, что они

1745
01:11:25,320 --> 01:11:27,239
там, у доски, вы

1746
01:11:27,239 --> 01:11:29,280
такие, о, это на самом деле  работает, это

1747
01:11:29,280 --> 01:11:32,159
огромный прорыв, и я просто да

1748
01:11:32,159 --> 01:11:33,960
определил априор,

1749
01:11:33,960 --> 01:11:36,739
а также многие из этих прорывов, которые

1750
01:11:36,739 --> 01:11:40,500
они не просто складывают, это не похоже на

1751
01:11:40,500 --> 01:11:44,280
башню из блоков, которые они

1752
01:11:44,280 --> 01:11:47,640
наслаивают и они составляют, так что потом что-то

1753
01:11:47,640 --> 01:11:50,159
будет обобщено до гм  обобщенные

1754
01:11:50,159 --> 01:11:52,140
координаты, или обобщенная синхронность, или

1755
01:11:52,140 --> 01:11:55,020
произвольно большие графики, или

1756
01:11:55,020 --> 01:11:57,239
хм. Сенсорное слияние с мультимодальными входными данными,

1757
01:11:57,239 --> 01:12:00,679
и это похоже на то, что все они смешиваются действительно

1758
01:12:00,679 --> 01:12:03,659
удовлетворительными и эффективными способами, так что даже

1759
01:12:03,659 --> 01:12:05,640
мелочи, которые опять-таки кто-то может

1760
01:12:05,640 --> 01:12:08,100
просто придумать через мгновение,

1761
01:12:08,100 --> 01:12:11,100
могут действительно иметь влияние,

1762
01:12:11,100 --> 01:12:14,159
хм.  Хорошо, мл. Доун говорит большое спасибо за то, что

1763
01:12:14,159 --> 01:12:16,199
задал мои вопросы, и огромное спасибо

1764
01:12:16,199 --> 01:12:18,060
Томазо за вдохновляющую презентацию,

1765
01:12:18,060 --> 01:12:21,360
так мило, о, большое спасибо, а затем

1766
01:12:21,360 --> 01:12:23,280
Берт спрашивает,

1767
01:12:23,280 --> 01:12:25,560
чем языковые модели, использующие

1768
01:12:25,560 --> 01:12:27,179
предиктивное кодирование, отличаются от тех, которые

1769
01:12:27,179 --> 01:12:30,260
используют трансформеры,

1770
01:12:31,679 --> 01:12:32,520
ладно,

1771
01:12:32,520 --> 01:12:35,340
я думаю, что на самом деле  если бы мне пришлось

1772
01:12:35,340 --> 01:12:36,659
сегодня построить языковую модель

1773
01:12:36,659 --> 01:12:38,640
с использованием предиктивного кодирования, я все равно буду использовать

1774
01:12:38,640 --> 01:12:40,020
преобразователи,

1775
01:12:40,020 --> 01:12:41,880
поэтому идея состоит в том, что, например, если у вас

1776
01:12:41,880 --> 01:12:42,780
есть,

1777
01:12:42,780 --> 01:12:45,659
скажем, эта иерархическая графическая

1778
01:12:45,659 --> 01:12:48,440
модель этой или этих иерархических

1779
01:12:48,440 --> 01:12:50,460
байесовских сетей, которые

1780
01:12:50,460 --> 01:12:53,100
я определил в  самый первый

1781
01:12:53,100 --> 01:12:55,380
сдвигает одну стрелку для кодирования функции,

1782
01:12:55,380 --> 01:12:57,300
которая является линейной картой,

1783
01:12:57,300 --> 01:12:59,219
хорошо, поэтому один час был просто

1784
01:12:59,219 --> 01:13:01,080
умножением вектора,

1785
01:13:01,080 --> 01:13:03,060
закодированного в скрытых переменных,

1786
01:13:03,060 --> 01:13:06,300
на эту весовую матрицу, которую затем можно

1787
01:13:06,300 --> 01:13:08,580
сделать нелинейной и тому подобное  но на

1788
01:13:08,580 --> 01:13:09,960
самом деле это может быть что-то гораздо более

1789
01:13:09,960 --> 01:13:12,179
сложное, функция, включенная в

1790
01:13:12,179 --> 01:13:14,880
стрелку, может быть сверткой, может быть

1791
01:13:14,880 --> 01:13:16,800
механизмом внимания,

1792
01:13:16,800 --> 01:13:20,820
так что на самом деле, как бы я это сделал, я

1793
01:13:20,820 --> 01:13:23,880
все равно буду использовать я имею в виду, что на самом деле так,

1794
01:13:23,880 --> 01:13:26,460
как мы это сделали в э-э  В Оксфордской

1795
01:13:26,460 --> 01:13:28,860
группе в прошлом году у нас была

1796
01:13:28,860 --> 01:13:30,900
точно такая структура, что каждая стрелка

1797
01:13:30,900 --> 01:13:33,420
теперь является Трансформером, поэтому один

1798
01:13:33,420 --> 01:13:35,159
механизм — это механизм внимания, а следующий — сеть

1799
01:13:35,159 --> 01:13:38,219
прямой связи как Трансформеры,

1800
01:13:38,219 --> 01:13:40,020
и, по сути, единственная разница, которая у

1801
01:13:40,020 --> 01:13:41,640
вас есть, заключается в том, что эти  переменные, которые вы

1802
01:13:41,640 --> 01:13:43,739
хотите вычислить апостериорные, и вы

1803
01:13:43,739 --> 01:13:45,239
делаете эти апостериорные зависимости

1804
01:13:45,239 --> 01:13:47,400
независимыми через аппроксимацию среднего поля VIA,

1805
01:13:47,400 --> 01:13:49,560
поэтому в основном вы выполняете

1806
01:13:49,560 --> 01:13:51,659
все шаги, которые позволяют вам

1807
01:13:51,659 --> 01:13:53,520
сходиться к самой

1808
01:13:53,520 --> 01:13:56,520
свободной энергии творческого кодирования, но

1809
01:13:56,520 --> 01:13:58,199
так, как вы  вычислять прогнозы,

1810
01:13:58,199 --> 01:14:01,199
а то, как вы отправляете сигналы обратно,

1811
01:14:01,199 --> 01:14:04,739
выполняется через Transformer,

1812
01:14:04,739 --> 01:14:07,560
поэтому я все равно буду использовать Transformers в

1813
01:14:07,560 --> 01:14:10,679
целом, я имею в виду, что они работают так хорошо, что я

1814
01:14:10,679 --> 01:14:12,840
не думаю, что мы можем быть высокомерными

1815
01:14:12,840 --> 01:14:15,060
и говорить: «О нет, я собираюсь это сделать».  лучше с помощью

1816
01:14:15,060 --> 01:14:17,640
чисто предиктивных структур способа кодирования,

1817
01:14:17,640 --> 01:14:18,920


1818
01:14:18,920 --> 01:14:21,480
но все равно будет приближаться к Трансформерам,

1819
01:14:21,480 --> 01:14:22,500


1820
01:14:22,500 --> 01:14:24,420
извините, вы сказали, что структурное обучение будет

1821
01:14:24,420 --> 01:14:27,540
приближаться к подходу Трансформера,

1822
01:14:27,540 --> 01:14:29,219
да, структурное обучение, о котором я упоминал

1823
01:14:29,219 --> 01:14:32,640
ранее, когда кто-то спрашивает

1824
01:14:32,640 --> 01:14:34,800
о сходстве между творческим кодированием

1825
01:14:34,800 --> 01:14:38,060
и механизмом внимания, да,

1826
01:14:38,280 --> 01:14:41,699
очень  интересно, эм, мне

1827
01:14:41,699 --> 01:14:42,900


1828
01:14:42,900 --> 01:14:45,719
интересно, от Amazon я

1829
01:14:45,719 --> 01:14:47,640
не мог увидеть понятие глубины в

1830
01:14:47,640 --> 01:14:49,380
сетях предиктивного кодирования, которые вы

1831
01:14:49,380 --> 01:14:50,880
упомянули, скорее всего, я пропустил это

1832
01:14:50,880 --> 01:14:52,380
определение, данное для предиктивного

1833
01:14:52,380 --> 01:14:56,480
кодирования, включало понятие глубины,

1834
01:14:56,640 --> 01:14:59,460
что вы имели в виду под глубиной

1835
01:14:59,460 --> 01:15:02,219
нет, да, это правда  это потому, что

1836
01:15:02,219 --> 01:15:04,980
стандартное определение, как я уже говорил несколько

1837
01:15:04,980 --> 01:15:06,960
раз, является иерархическим, у вас есть

1838
01:15:06,960 --> 01:15:08,400
прогнозы в одном направлении, некоторая

1839
01:15:08,400 --> 01:15:09,719
ошибка прогнозирования идет в противоположном

1840
01:15:09,719 --> 01:15:10,620
направлении,

1841
01:15:10,620 --> 01:15:14,340
в основном то, что мы сделали в этой

1842
01:15:14,340 --> 01:15:16,320
статье, а также в последнем,

1843
01:15:16,320 --> 01:15:18,420
который называется  Изучение

1844
01:15:18,420 --> 01:15:19,920
топологий произвольного графа, которое у нас есть

1845
01:15:19,920 --> 01:15:22,260
относительное кодирование, заключается в том, что мы можем рассматривать

1846
01:15:22,260 --> 01:15:25,620
глубину, например,

1847
01:15:25,620 --> 01:15:28,380
как независимую, в

1848
01:15:28,380 --> 01:15:31,380
основном, пару скрытой переменной, скрытой

1849
01:15:31,380 --> 01:15:33,239
переменной и стрелки,

1850
01:15:33,239 --> 01:15:34,739
и у вас есть прогнозы, идущие в этом

1851
01:15:34,739 --> 01:15:36,300
направлении, и прогнозирующая стрелка, идущая

1852
01:15:36,300 --> 01:15:38,340
в другом, но тогда вы  можно составить

1853
01:15:38,340 --> 01:15:41,880
их сколькими, хм, многими способами, так что

1854
01:15:41,880 --> 01:15:45,239
вы можете, так что в основном эта

1855
01:15:45,239 --> 01:15:47,040
композиция не должна быть

1856
01:15:47,040 --> 01:15:48,659
иерархической, в конце концов,

1857
01:15:48,659 --> 01:15:50,820
может иметь циклы, поэтому вы можете,

1858
01:15:50,820 --> 01:15:53,520
например, подключить другую, другую

1859
01:15:53,520 --> 01:15:55,440
скрытую переменную к первой  одну, а

1860
01:15:55,440 --> 01:15:57,540
затем соединить с другими, и вы можете

1861
01:15:57,540 --> 01:15:59,340
получить структуру, которая настолько запутана, насколько

1862
01:15:59,340 --> 01:16:00,420
вы хотите, поэтому,

1863
01:16:00,420 --> 01:16:02,699
например, в другой статье

1864
01:16:02,699 --> 01:16:04,500
мы обучаем

1865
01:16:04,500 --> 01:16:06,659
сеть, которая имеет форму

1866
01:16:06,659 --> 01:16:08,460
структуры мозга, поэтому у нас есть много

1867
01:16:08,460 --> 01:16:09,900
области мозга, которые редко

1868
01:16:09,900 --> 01:16:12,239
связаны внутри, и они частично

1869
01:16:12,239 --> 01:16:13,860
связаны друг с другом,

1870
01:16:13,860 --> 01:16:15,719
и

1871
01:16:15,719 --> 01:16:17,640
в конце нет ничего иерархического, но вы

1872
01:16:17,640 --> 01:16:18,960
все равно можете тренировать его, сводя к минимуму

1873
01:16:18,960 --> 01:16:20,699
оперативную свободную энергию и

1874
01:16:20,699 --> 01:16:22,620
сводя к минимуму общую

1875
01:16:22,620 --> 01:16:25,159
ошибку предсказания сети,

1876
01:16:25,159 --> 01:16:27,360
чтобы вы могли

1877
01:16:27,360 --> 01:16:31,980
для данного Мотива в запутанном графе

1878
01:16:31,980 --> 01:16:35,159
вы можете увидеть три последовательных слоя,

1879
01:16:35,159 --> 01:16:37,560
которые, если вы посмотрите только на них, вы

1880
01:16:37,560 --> 01:16:38,820
скажете: «О, это трехэтажное здание,

1881
01:16:38,820 --> 01:16:41,940
это трехслойная модель, которая говорит «

1882
01:16:41,940 --> 01:16:43,980
адаптивная три», но затем, когда вы сделаете

1883
01:16:43,980 --> 01:16:46,620
более крупную картину, там  не похоже на

1884
01:16:46,620 --> 01:16:50,280
явную вершину или явную нижнюю часть

1885
01:16:50,280 --> 01:16:52,140
этой сети,

1886
01:16:52,140 --> 01:16:54,360
да, точно, и это в основном определяется тем

1887
01:16:54,360 --> 01:16:55,980
фактом, что каждая операция

1888
01:16:55,980 --> 01:16:58,080
в прогнозирующих корейских сетях

1889
01:16:58,080 --> 01:16:59,460
строго локальна,

1890
01:16:59,460 --> 01:17:01,739
поэтому в основном каждое сообщение, передающее

1891
01:17:01,739 --> 01:17:03,000
каждый прогноз и каждую

1892
01:17:03,000 --> 01:17:05,280
ошибку прогноза  то, что вы отправляете, вы отправляете его только в

1893
01:17:05,280 --> 01:17:08,280
очень близлежащие нейроны, хорошо, и

1894
01:17:08,280 --> 01:17:10,380
является ли глобальная структура на самом деле

1895
01:17:10,380 --> 01:17:13,380
иерархической или нет,

1896
01:17:13,380 --> 01:17:16,940
передача одного сообщения даже не видит этого,

1897
01:17:17,460 --> 01:17:19,620
я думаю, это своего рода

1898
01:17:19,620 --> 01:17:22,820
надежда на изучение новых

1899
01:17:22,820 --> 01:17:27,739
архитектур моделей - это пространство  то, что

1900
01:17:27,739 --> 01:17:33,300
разработано сверху вниз, очень мало, и

1901
01:17:33,300 --> 01:17:36,480
сегодня используется множество моделей, хотя и

1902
01:17:36,480 --> 01:17:38,640
суперэффективных моделей,

1903
01:17:38,640 --> 01:17:41,100
хотя вы можете спросить, эффективно на

1904
01:17:41,100 --> 01:17:43,320
единицу вычислений или нет, это

1905
01:17:43,320 --> 01:17:45,300
вопрос второго уровня, но многие эффективные

1906
01:17:45,300 --> 01:17:47,580
модели сегодня не имеют некоторых из этих

1907
01:17:47,580 --> 01:17:49,860
свойства сетей с предиктивным кодированием,

1908
01:17:49,860 --> 01:17:52,739
такие как их способность

1909
01:17:52,739 --> 01:17:55,520
использовать только локальные вычисления,

1910
01:17:55,520 --> 01:17:59,400
что дает биологический реализм

1911
01:17:59,400 --> 01:18:02,880
или просто пространственно-временной реализм, но

1912
01:18:02,880 --> 01:18:06,060
также может обеспечить много преимуществ,

1913
01:18:06,060 --> 01:18:08,159
например, в условиях федеративных вычислений или распределенных

1914
01:18:08,159 --> 01:18:10,500
вычислений

1915
01:18:10,500 --> 01:18:12,780
нет да точно я полностью согласен,

1916
01:18:12,780 --> 01:18:14,520
потому что я  думаю, что идея в целом

1917
01:18:14,520 --> 01:18:16,679
такова, и я не знаю, будет ли это

1918
01:18:16,679 --> 01:18:18,540
преимуществом, поэтому я думаю, что это очень

1919
01:18:18,540 --> 01:18:20,159
многообещающе именно по причинам, которые вы

1920
01:18:20,159 --> 01:18:20,880
сказали,

1921
01:18:20,880 --> 01:18:22,920
и причина в том, что сегодняшняя

1922
01:18:22,920 --> 01:18:25,380
модель строки с обратным распространением вы

1923
01:18:25,380 --> 01:18:28,860
можете в основном суммировать  их в качестве

1924
01:18:28,860 --> 01:18:32,040
мониторинга обратное распространение является

1925
01:18:32,040 --> 01:18:34,080
функцией, потому что в основном у вас есть

1926
01:18:34,080 --> 01:18:36,120
карта от входа к выходу, а обратное

1927
01:18:36,120 --> 01:18:39,600
распространение в основном распространяет

1928
01:18:39,600 --> 01:18:41,699
информацию обратно из своего вычислительного

1929
01:18:41,699 --> 01:18:44,340
графа, поэтому каждая

1930
01:18:44,340 --> 01:18:45,960
модель нейронной сети, используемая сегодня,

1931
01:18:45,960 --> 01:18:48,960
является функцией, в то время как прогнозирующее кодирование

1932
01:18:48,960 --> 01:18:51,179
и другое освободительное кодирование, такое как

1933
01:18:51,179 --> 01:18:53,820
старый класс функций, класс

1934
01:18:53,820 --> 01:18:56,040
методов, которые обучаются использованию локальных

1935
01:18:56,040 --> 01:18:58,500
вычислений и фактически работают,

1936
01:18:58,500 --> 01:19:01,620
минимизируя глобальную функцию энергии,

1937
01:19:01,620 --> 01:19:03,840
они не ограничены функциями моделирования

1938
01:19:03,840 --> 01:19:05,940
от ввода до вывода, они фактически моделируют

1939
01:19:05,940 --> 01:19:07,739
что-то, что  напоминает

1940
01:19:07,739 --> 01:19:10,080
физические системы, поэтому у вас есть физическая

1941
01:19:10,080 --> 01:19:13,500
система, к которой вы фиксируете некоторые значения для

1942
01:19:13,500 --> 01:19:15,360
любых входных данных, и вы позволяете

1943
01:19:15,360 --> 01:19:17,280
системе сходиться, а затем вы считываете какое-то

1944
01:19:17,280 --> 01:19:19,980
другое значение нейронов или переменных,

1945
01:19:19,980 --> 01:19:21,960
которые должны быть выведены, но эта

1946
01:19:21,960 --> 01:19:24,120
физическая система не  не обязательно должна быть подходящей

1947
01:19:24,120 --> 01:19:25,920
опережающей картой, не обязательно должна быть

1948
01:19:25,920 --> 01:19:28,260
функцией, которая имеет входное пространство и

1949
01:19:28,260 --> 01:19:30,659
выходное пространство, и все,

1950
01:19:30,659 --> 01:19:32,580
поэтому класс моделей, которые вы можете

1951
01:19:32,580 --> 01:19:34,800
изучить, таков, что в основном вы можете видеть,

1952
01:19:34,800 --> 01:19:37,560
как модели с прямой связью.  и функции,

1953
01:19:37,560 --> 01:19:39,600
а затем гораздо больший класс, который относится к

1954
01:19:39,600 --> 01:19:41,880
физическим системам, есть ли

1955
01:19:41,880 --> 01:19:43,860
здесь что-то интересное, я

1956
01:19:43,860 --> 01:19:45,659
еще не знаю, потому что функции

1957
01:19:45,659 --> 01:19:47,460
работают очень хорошо, мы наблюдаем

1958
01:19:47,460 --> 01:19:50,040
те дни с обратным распространением,

1959
01:19:50,040 --> 01:19:52,199
они работают безумно хорошо, но так что да

1960
01:19:52,199 --> 01:19:53,460
Я не знаю, есть ли что-то

1961
01:19:53,460 --> 01:19:56,040
интересное в большой части, но большая

1962
01:19:56,040 --> 01:19:58,380
часть довольно большая, хорошо, что есть

1963
01:19:58,380 --> 01:20:00,480
много моделей, которые вы

1964
01:20:00,480 --> 01:20:02,940
не можете вернуть к распространению, и вы

1965
01:20:02,940 --> 01:20:04,679
можете тренироваться с творческим кодированием

1966
01:20:04,679 --> 01:20:06,659
или распространением в ванной или другими

1967
01:20:06,659 --> 01:20:07,860
методами.

1968
01:20:07,860 --> 01:20:10,440
это очень интересно, конечно,

1969
01:20:10,440 --> 01:20:12,719
биологические системы, физические системы

1970
01:20:12,719 --> 01:20:15,900
решают все виды интересных проблем,

1971
01:20:15,900 --> 01:20:17,100


1972
01:20:17,100 --> 01:20:19,380
но все еще нет бесплатных обедов

1973
01:20:19,380 --> 01:20:21,540
среди видов муравьев, которые действительно хорошо себя чувствуют в

1974
01:20:21,540 --> 01:20:23,100
этой среде, могут не очень хорошо себя чувствовать

1975
01:20:23,100 --> 01:20:25,679
в другой среде, и так

1976
01:20:25,679 --> 01:20:28,260
далее, во Внутренних землях.

1977
01:20:28,260 --> 01:20:31,820
могут быть некоторые действительно уникальные специальные

1978
01:20:31,820 --> 01:20:35,880
алгоритмы, которые не очень хорошо описываются

1979
01:20:35,880 --> 01:20:38,460
как функции,

1980
01:20:38,460 --> 01:20:42,060
но все же предоставляют процедурный

1981
01:20:42,060 --> 01:20:46,679
способ реализации эвристики,

1982
01:20:46,679 --> 01:20:48,840
которая может

1983
01:20:48,840 --> 01:20:51,120


1984
01:20:51,120 --> 01:20:53,880


1985
01:20:53,880 --> 01:20:55,679
быть чрезвычайно эффективной.

1986
01:20:55,679 --> 01:20:58,260
исследований во время моей докторской диссертации, например,

1987
01:20:58,260 --> 01:20:59,100


1988
01:20:59,100 --> 01:21:01,380
как найти это приложение, которое

1989
01:21:01,380 --> 01:21:04,199
находится здесь, а не внутри функций,

1990
01:21:04,199 --> 01:21:06,739


1991
01:21:07,199 --> 01:21:08,820
хорошо,

1992
01:21:08,820 --> 01:21:12,120
куда эта работа идет отсюда, например,

1993
01:21:12,120 --> 01:21:14,520
какие направления вас интересуют

1994
01:21:14,520 --> 01:21:17,340
и как вы видите, что люди в

1995
01:21:17,340 --> 01:21:19,679
экосистеме активного вывода получают  вовлеченный в

1996
01:21:19,679 --> 01:21:22,460
этот тип работы,

1997
01:21:22,500 --> 01:21:24,840
я думаю, что, вероятно, наиболее

1998
01:21:24,840 --> 01:21:27,780
многообещающее направление, которое является

1999
01:21:27,780 --> 01:21:30,060
чем-то, что, может быть, я хотел бы

2000
01:21:30,060 --> 01:21:33,060
немного изучить, это, как я уже сказал,

2001
01:21:33,060 --> 01:21:34,980
действительно нужно отойти от статических

2002
01:21:34,980 --> 01:21:37,380
моделей, так что все, что я показал

2003
01:21:37,380 --> 01:21:40,260
До сих пор я показал, что речь идет о статических данных,

2004
01:21:40,260 --> 01:21:42,840
поэтому данные не меняются со временем, в

2005
01:21:42,840 --> 01:21:45,780
определении

2006
01:21:45,780 --> 01:21:48,000
творческого кодирования нет времени, как я представил

2007
01:21:48,000 --> 01:21:49,080
его здесь,

2008
01:21:49,080 --> 01:21:50,940
однако вы можете, например, обобщить

2009
01:21:50,940 --> 01:21:53,280
творческое кодирование для работы.  с временными

2010
01:21:53,280 --> 01:21:55,800
данными, используя обобщенные координаты, как

2011
01:21:55,800 --> 01:21:58,800
вы упомянули ранее,

2012
01:21:58,800 --> 01:22:01,380
представляя их как обычную

2013
01:22:01,380 --> 01:22:04,140
генеративную модель фильтра Калмана,

2014
01:22:04,140 --> 01:22:08,040
и именно здесь, например,

2015
01:22:08,040 --> 01:22:09,900
направление причинно-следственного вывода может быть очень

2016
01:22:09,900 --> 01:22:12,600
полезным, потому что да, эта модель в

2017
01:22:12,600 --> 01:22:14,400
этот момент, возможно, вы можете  иметь возможность

2018
01:22:14,400 --> 01:22:17,880
моделировать большую причинно-следственную связь, более

2019
01:22:17,880 --> 01:22:21,780
сложную и полезную

2020
01:22:21,780 --> 01:22:24,780
динамическую причину моделей, в основном

2021
01:22:24,780 --> 01:22:26,940
потому, что в целом исчисление

2022
01:22:26,940 --> 01:22:28,560
и интервенционная и

2023
01:22:28,560 --> 01:22:32,760
контрфактическая наука

2024
01:22:32,760 --> 01:22:36,000
в основном развиваются на небольших моделях,

2025
01:22:36,000 --> 01:22:38,159
так что это

2026
01:22:38,159 --> 01:22:40,739
похоже на то, что вы не делаете.  В целом, мы не вмешиваемся в

2027
01:22:40,739 --> 01:22:43,560
гигантские модели, поэтому, если вы

2028
01:22:43,560 --> 01:22:45,800
посмотрите на медицинские данные, они используют

2029
01:22:45,800 --> 01:22:50,159
относительно небольшие сети видения, и,

2030
01:22:50,159 --> 01:22:51,179
конечно, если вы хотите иметь

2031
01:22:51,179 --> 01:22:54,900
динамическую причинно-следственную модель, которая моделирует

2032
01:22:54,900 --> 01:22:56,340
конкретную среду или конкретную

2033
01:22:56,340 --> 01:22:58,620
реальность, у вас есть  много нейронов внутри

2034
01:22:58,620 --> 01:23:00,780
вас имеет много скрытых переменных, которые

2035
01:23:00,780 --> 01:23:02,580
меняются со временем, и вмешательство в

2036
01:23:02,580 --> 01:23:05,219
некоторые другие в какой-то момент создает

2037
01:23:05,219 --> 01:23:07,560
эффект в другом временном шаге, поэтому, возможно,

2038
01:23:07,560 --> 01:23:09,239
в следующем временном шаге через 10 разных

2039
01:23:09,239 --> 01:23:11,699
временных шагов позже, и я думаю, что это будет

2040
01:23:11,699 --> 01:23:14,100
быть очень интересным для разработки как

2041
01:23:14,100 --> 01:23:16,380
биологически правдоподобный способ передачи

2042
01:23:16,380 --> 01:23:17,699
информации,

2043
01:23:17,699 --> 01:23:20,040
который также может моделировать

2044
01:23:20,040 --> 01:23:22,860
причинно-следственную связь Величия в основном

2045
01:23:22,860 --> 01:23:24,659
хм,

2046
01:23:24,659 --> 01:23:29,659
где вы видите действие в этих моделях,

2047
01:23:30,840 --> 01:23:33,840
где я вижу действие,

2048
01:23:33,840 --> 01:23:36,480
я не думал об этом,

2049
01:23:36,480 --> 01:23:38,760
я думаю, как действия в этих моделях  модели,

2050
01:23:38,760 --> 01:23:41,460
может быть, так же, как я, как вы видите в

2051
01:23:41,460 --> 01:23:43,080
других моделях, потому что

2052
01:23:43,080 --> 01:23:44,940
творческое кодирование в основном является моделью

2053
01:23:44,940 --> 01:23:46,260
восприятия,

2054
01:23:46,260 --> 01:23:49,260
поэтому действие — это то, что вы можете видеть, что есть

2055
01:23:49,260 --> 01:23:52,739
следствие того, что вы испытываете,

2056
01:23:52,739 --> 01:23:55,159
поэтому, изменяя то, как вы

2057
01:23:55,159 --> 01:23:57,840
что-то испытываете, тогда вы  может

2058
01:23:57,840 --> 01:24:00,060
вычислить, может быть,

2059
01:24:00,060 --> 01:24:01,800
теперь, когда у вас есть больше информации, вы можете просто выполнить более разумное действие,

2060
01:24:01,800 --> 01:24:03,000


2061
01:24:03,000 --> 01:24:04,560
но

2062
01:24:04,560 --> 01:24:06,960
да, я не думаю, что действие очень

2063
01:24:06,960 --> 01:24:10,199
простое, как да, я не вижу никаких явных

2064
01:24:10,199 --> 01:24:12,540
последствий действий, кроме того факта,

2065
01:24:12,540 --> 01:24:14,040
что это может позволить вам в основном

2066
01:24:14,040 --> 01:24:15,960
может быть, вы

2067
01:24:15,960 --> 01:24:18,780
просто делаете более точные выводы о том, что они

2068
01:24:18,780 --> 01:24:21,719
выполняют действия в будущем,

2069
01:24:21,719 --> 01:24:23,940
я добавлю к этому несколько способов, которыми

2070
01:24:23,940 --> 01:24:25,920
люди говорили о прогнозирующем

2071
01:24:25,920 --> 01:24:29,340
кодировании и действии, во-первых, внутреннее

2072
01:24:29,340 --> 01:24:33,960
действие или скрытое действие - это внимание, чтобы

2073
01:24:33,960 --> 01:24:36,120
мы могли думать о восприятии.  как

2074
01:24:36,120 --> 01:24:37,980
внутреннее действие, это один подход,

2075
01:24:37,980 --> 01:24:40,560
другой подход довольно микро - это

2076
01:24:40,560 --> 01:24:42,840
выходы данного узла, мы можем

2077
01:24:42,840 --> 01:24:45,780
понять этот узел как конкретную

2078
01:24:45,780 --> 01:24:48,780
вещь со своими собственными сенсорными когнитивными

2079
01:24:48,780 --> 01:24:52,080
состояниями и состояниями действия, и поэтому в этом смысле

2080
01:24:52,080 --> 01:24:54,960
выход узла и, наконец,

2081
01:24:54,960 --> 01:24:57,179
что  мы немного изучили в прямом

2082
01:24:57,179 --> 01:24:59,940
эфире 43 теоретический обзор

2083
01:24:59,940 --> 01:25:02,100
прогнозирующего кодирования, который мы читаем

2084
01:25:02,100 --> 01:25:03,840
полностью, и это было все о

2085
01:25:03,840 --> 01:25:05,460
восприятии все о восприятии, а затем

2086
01:25:05,460 --> 01:25:08,040
это было похоже на раздел 5.3,

2087
01:25:08,040 --> 01:25:11,719
если у вас есть ожидания относительно действия,

2088
01:25:11,719 --> 01:25:15,900
тогда действие просто  еще одна переменная в

2089
01:25:15,900 --> 01:25:18,120
этой архитектуре, и это действительно

2090
01:25:18,120 --> 01:25:20,040
согласуется с неактивным выводом, где

2091
01:25:20,040 --> 01:25:21,659
вместо функции вознаграждения или

2092
01:25:21,659 --> 01:25:24,000
полезности, которую мы максимизируем, мы

2093
01:25:24,000 --> 01:25:26,699
выбираем действие, основываясь на том, что это

2094
01:25:26,699 --> 01:25:28,800
наиболее вероятный курс действий, путь

2095
01:25:28,800 --> 01:25:30,900
наименьшего действия, это байесовская механика,

2096
01:25:30,900 --> 01:25:33,300
и поэтому на самом деле это очень  Естественно

2097
01:25:33,300 --> 01:25:36,420
ввести переменную действия и использовать

2098
01:25:36,420 --> 01:25:40,800
ее по существу так, как если бы это было

2099
01:25:40,800 --> 01:25:43,260
предсказание о чем-то еще

2100
01:25:43,260 --> 01:25:45,540
более восприимчивом в мире, потому что

2101
01:25:45,540 --> 01:25:48,480
мы также ожидаем действия

2102
01:25:48,480 --> 01:25:50,820
нет да да точно

2103
01:25:50,820 --> 01:25:52,860
нет Мне очень нравится способ определения действий

2104
01:25:52,860 --> 01:25:55,260
на самом деле и  э-э, и я все еще думаю, что это

2105
01:25:55,260 --> 01:25:57,239
было, например, не

2106
01:25:57,239 --> 01:26:01,139
так много статей, в которых применяется этот метод, я

2107
01:26:01,139 --> 01:26:03,239
думаю, что есть пара от э-э от

2108
01:26:03,239 --> 01:26:05,100
Александра, или Робриа делает что-то

2109
01:26:05,100 --> 01:26:08,580
подобное, но на практике, например, за пределами

2110
01:26:08,580 --> 01:26:10,920
чистого активного вывода, такого как применение

2111
01:26:10,920 --> 01:26:13,260
прогнозирующего кодирования  и действия для

2112
01:26:13,260 --> 01:26:15,980
решения практических проблем не были

2113
01:26:15,980 --> 01:26:19,280
исследованы много, ну

2114
01:26:19,679 --> 01:26:23,400
что ж, спасибо за эту прекрасную

2115
01:26:23,400 --> 01:26:25,199
презентацию и обсуждение, есть ли

2116
01:26:25,199 --> 01:26:27,659
что-нибудь еще, что вы хотите сказать или

2117
01:26:27,659 --> 01:26:30,300
или указать людям на э-э,

2118
01:26:30,300 --> 01:26:33,360
нет, просто большое спасибо за приглашение

2119
01:26:33,360 --> 01:26:34,620
меня  и

2120
01:26:34,620 --> 01:26:36,120
это было действительно весело, и я надеюсь

2121
01:26:36,120 --> 01:26:38,460
вернуться в какой-то момент для некоторых Future

2122
01:26:38,460 --> 01:26:40,199
Works,

2123
01:26:40,199 --> 01:26:41,580
классных

2124
01:26:41,580 --> 01:26:45,000
в любое время в любое время, спасибо, Томас, так что

2125
01:26:45,000 --> 01:26:49,820
спасибо, Дэниел, увидимся, пока, пока


1
00:00:19,020 --> 00:00:21,119
hallo en welkom

2
00:00:21,119 --> 00:00:23,400
het is actieve gevolgtrekking gaststroom

3
00:00:23,400 --> 00:00:28,140
nummer 51.1 op 28 juli 2023

4
00:00:28,140 --> 00:00:31,439
we zijn hier met Tomaso Salvatore en we

5
00:00:31,439 --> 00:00:33,840
zullen een presentatie en een

6
00:00:33,840 --> 00:00:37,020
discussie houden over het recente werk causale

7
00:00:37,020 --> 00:00:39,660
gevolgtrekking via voorspellende codering dus

8
00:00:39,660 --> 00:00:42,480
heel erg bedankt voor het meedoen voor degenen die

9
00:00:42,480 --> 00:00:44,340
kijken  live voel je vrij om

10
00:00:44,340 --> 00:00:47,520
vragen te schrijven in de live chat en op naar

11
00:00:47,520 --> 00:00:50,399
jou, bedankt,

12
00:00:50,399 --> 00:00:52,980
heel erg bedankt Daniel voor de uitnodiging,

13
00:00:52,980 --> 00:00:56,039
uh, ben altijd een grote fan van het

14
00:00:56,039 --> 00:00:57,719
kanaal geweest en

15
00:00:57,719 --> 00:00:58,920
ik heb veel video's bekeken.

16
00:00:58,920 --> 00:01:01,379
opgewonden om hier te zijn en uh en

17
00:01:01,379 --> 00:01:04,260
degene te zijn die deze keer spreekt,

18
00:01:04,260 --> 00:01:06,600
dus ik ga het hebben over deze recente

19
00:01:06,600 --> 00:01:08,700
preprint die ik heb uitgebracht en die

20
00:01:08,700 --> 00:01:11,159
het werk van de afgelopen paar maanden is geweest

21
00:01:11,159 --> 00:01:12,119


22
00:01:12,119 --> 00:01:15,900
en het is een samenwerking met de

23
00:01:15,900 --> 00:01:18,659
met lookup  in Ketty ben ik in makarak

24
00:01:18,659 --> 00:01:21,600
barami Legende Thomas lukasiavich

25
00:01:21,600 --> 00:01:24,000
en het is eigenlijk een gezamenlijk werk tussen

26
00:01:24,000 --> 00:01:26,299
verzen, het bedrijf waar ik werk voor

27
00:01:26,299 --> 00:01:31,680
de Universiteit van Oxford en uhuvian

28
00:01:31,680 --> 00:01:34,200
dus

29
00:01:34,200 --> 00:01:36,299
tijdens dit gesprek

30
00:01:36,299 --> 00:01:38,220
zal ik

31
00:01:38,220 --> 00:01:40,979
dit in feite de hoofdlijnen van het gesprek waar ik

32
00:01:40,979 --> 00:01:43,140
over zal beginnen te praten  wat voorspellende

33
00:01:43,140 --> 00:01:44,520
codering is

34
00:01:44,520 --> 00:01:47,659
en de gegeven interacties van wat het is

35
00:01:47,659 --> 00:01:51,299
een korte historische inleiding waarom u

36
00:01:51,299 --> 00:01:54,060
denkt dat het belangrijk is om

37
00:01:54,060 --> 00:01:56,159
creatieve codering te bestuderen, zelfs

38
00:01:56,159 --> 00:01:58,619
vanuit het perspectief van machine learning

39
00:01:58,619 --> 00:02:00,720
zal ik dan een

40
00:02:00,720 --> 00:02:04,560
kleine inleiding geven over wat causale gevolgtrekking is

41
00:02:04,560 --> 00:02:07,200
en  en zodra we al die

42
00:02:07,200 --> 00:02:08,880
informatie bij elkaar hebben, zal ik

43
00:02:08,880 --> 00:02:12,540
bespreken waarom ik dit artikel heb geschreven, wat

44
00:02:12,540 --> 00:02:14,520
in feite de onderzoeksvraag was die

45
00:02:14,520 --> 00:02:16,560
mij en de en de andere medewerkers inspireerde

46
00:02:16,560 --> 00:02:18,300


47
00:02:18,300 --> 00:02:21,660
en de belangrijkste resultaten presenteren, namelijk

48
00:02:21,660 --> 00:02:24,980
hoe

49
00:02:24,980 --> 00:02:27,480
gevolgtrekkingen kunnen worden uitgevoerd, dus interventie en

50
00:02:27,480 --> 00:02:29,340
contrafeitelijke gevolgtrekkingen

51
00:02:29,340 --> 00:02:33,319
en  hoe je de causale structuren

52
00:02:33,319 --> 00:02:35,879
uit een bepaalde dataset kunt leren met behulp van voorspellende

53
00:02:35,879 --> 00:02:37,920
codering en dan zal ik natuurlijk

54
00:02:37,920 --> 00:02:39,959
afsluiten met een

55
00:02:39,959 --> 00:02:43,500
kleine samenvatting en wat discussie over

56
00:02:43,500 --> 00:02:45,840
waarom ik geloof dat dit werk in feite impact kan hebben

57
00:02:45,840 --> 00:02:49,940
en enkele toekomstige richtingen,

58
00:02:50,700 --> 00:02:53,400
dus wat is creatieve codering

59
00:02:53,400 --> 00:02:55,680
creatieve codering  staat in het algemeen bekend

60
00:02:55,680 --> 00:02:58,440
als een door neurowetenschap geïnspireerde

61
00:02:58,440 --> 00:03:01,140
leermethode, dus welke theorie over hoe

62
00:03:01,140 --> 00:03:04,560
informatieverwerking in de hersenen werkt

63
00:03:04,560 --> 00:03:05,819
en

64
00:03:05,819 --> 00:03:08,400
heel formeel gesproken kan de laag van

65
00:03:08,400 --> 00:03:10,560
creatieve codering worden beschreven als

66
00:03:10,560 --> 00:03:12,659
een hiërarchische

67
00:03:12,659 --> 00:03:16,319
structuur van neuronen in de hersenen

68
00:03:16,319 --> 00:03:19,080
en jij  hebben twee verschillende families van

69
00:03:19,080 --> 00:03:20,700
neuronen in de hersenen,

70
00:03:20,700 --> 00:03:23,280
de eerste familie is degene die

71
00:03:23,280 --> 00:03:24,480
belast is met het verzenden van

72
00:03:24,480 --> 00:03:27,659
voorspellingsinformatie, dus neuronen in een specifiek

73
00:03:27,659 --> 00:03:29,959
niveau van de hiërarchie verzenden informatie

74
00:03:29,959 --> 00:03:33,959
en voorspellen de activiteit van de van

75
00:03:33,959 --> 00:03:35,940
het niveau eronder

76
00:03:35,940 --> 00:03:38,340
en de tweede familie van  neuron is dat

77
00:03:38,340 --> 00:03:41,099
van foutneuronen en de pijlneuronen

78
00:03:41,099 --> 00:03:43,019
sturen voorspellingsfoutinformatie

79
00:03:43,019 --> 00:03:46,319
omhoog in de hiërarchie, dus één niveau voorspelt

80
00:03:46,319 --> 00:03:49,200
de activiteit van het niveau onder

81
00:03:49,200 --> 00:03:51,659
deze activiteit heeft een aantal van deze voorspelling

82
00:03:51,659 --> 00:03:54,239
als een mismatch die feitelijk zou

83
00:03:54,239 --> 00:03:56,220
plaatsvinden in het niveau eronder

84
00:03:56,220 --> 00:03:57,840
en informatie over  de

85
00:03:57,840 --> 00:04:02,400
voorspellingsfout wordt met de pijltjestoets naar boven gestuurd,

86
00:04:02,400 --> 00:04:04,860
maar voorspellende codering

87
00:04:04,860 --> 00:04:07,220
is eigenlijk niet verbrand als een

88
00:04:07,220 --> 00:04:10,799
neurowetenschap als een theorie uit de

89
00:04:10,799 --> 00:04:11,939
neurowetenschappen,

90
00:04:11,939 --> 00:04:13,860
maar het werd oorspronkelijk ontwikkeld

91
00:04:13,860 --> 00:04:16,139
als een methode voor signaalverwerking en

92
00:04:16,139 --> 00:04:19,380
compressie in de jaren 50, dus het

93
00:04:19,380 --> 00:04:21,899
werk van  Oliver Elias, die eigenlijk

94
00:04:21,899 --> 00:04:25,020
eigentijds zijn van uh kleding Shannon of

95
00:04:25,020 --> 00:04:26,160
Shannon,

96
00:04:26,160 --> 00:04:27,960
ze realiseerden zich dat als we eenmaal een

97
00:04:27,960 --> 00:04:30,900
voorspeller hebben, een model dat

98
00:04:30,900 --> 00:04:33,600
goed werkt in het voorspellen van gegevens, het

99
00:04:33,600 --> 00:04:36,000
verzenden van berichten over de fout in

100
00:04:36,000 --> 00:04:37,919
die voorspellingen eigenlijk veel

101
00:04:37,919 --> 00:04:41,100
goedkoper is dan het verzenden van het hele bericht

102
00:04:41,100 --> 00:04:42,720
elke  tijd

103
00:04:42,720 --> 00:04:45,240
en zo werd mooie codering geboren,

104
00:04:45,240 --> 00:04:47,639
dus

105
00:04:47,639 --> 00:04:49,500
als een signaalverwerkings- en

106
00:04:49,500 --> 00:04:52,020
compressiemechanisme in informatietheorie in

107
00:04:52,020 --> 00:04:53,639
de jaren 50,

108
00:04:53,639 --> 00:04:57,120
het was eigenlijk in de jaren 80, uh, dat

109
00:04:57,120 --> 00:04:59,400
werd dat precies hetzelfde model

110
00:04:59,400 --> 00:05:01,800
werd gebruikt in uh

111
00:05:01,800 --> 00:05:03,540
in de neurowetenschappen

112
00:05:03,540 --> 00:05:07,500
en uh  dus leg met het werk van Mumford of

113
00:05:07,500 --> 00:05:10,440
andere werken bijvoorbeeld uit

114
00:05:10,440 --> 00:05:12,960
hoe de beoordeling van procesinformatie, zodat

115
00:05:12,960 --> 00:05:14,520
we voorspellingssignalen van de

116
00:05:14,520 --> 00:05:17,160
buitenwereld krijgen en we

117
00:05:17,160 --> 00:05:20,280
deze representatie moeten comprimeren en uh en deze

118
00:05:20,280 --> 00:05:22,740
interne representatie in onze neuronen

119
00:05:22,740 --> 00:05:25,199
en de methode hebben  lijkt erg op, zo niet

120
00:05:25,199 --> 00:05:27,720
equivalent aan degene die werd gebruikt en die

121
00:05:27,720 --> 00:05:30,419
werd ontwikkeld door Elias en Oliver in

122
00:05:30,419 --> 00:05:32,900
de jaren 50.

123
00:05:32,940 --> 00:05:35,520
Wat is misschien de grootste paradigmaverschuiving die

124
00:05:35,520 --> 00:05:38,100
in 1999 plaatsvond

125
00:05:38,100 --> 00:05:41,400
dankzij het werk van rond Ballard

126
00:05:41,400 --> 00:05:44,880
waarin ze

127
00:05:44,880 --> 00:05:46,199
dit concept introduceerden dat ik  eerder gezegd

128
00:05:46,199 --> 00:05:48,060
over hiërarchische structuren in de

129
00:05:48,060 --> 00:05:51,360
hersenen waar voorspellingsinformatie van

130
00:05:51,360 --> 00:05:54,240
boven naar beneden is en de foutinformatie van

131
00:05:54,240 --> 00:05:55,560
onder naar boven

132
00:05:55,560 --> 00:05:57,660
en iets dat ze deden dat nog niet

133
00:05:57,660 --> 00:05:59,759
eerder was gedaan, is dat

134
00:05:59,759 --> 00:06:02,820
ze deze theorie uitleggen en ontwikkelen,

135
00:06:02,820 --> 00:06:05,759
niet alleen in Frankrijk, maar alleen maar

136
00:06:05,759 --> 00:06:07,979
ook  over hoe leren werkt in de

137
00:06:07,979 --> 00:06:10,139
hersenen, dus het is ook een theorie over hoe onze

138
00:06:10,139 --> 00:06:13,139
synapsen worden bijgewerkt

139
00:06:13,139 --> 00:06:16,080
en de laatste grote doorbraak waar ik het

140
00:06:16,080 --> 00:06:17,940
in deze korte

141
00:06:17,940 --> 00:06:21,900
historische inleiding over ga hebben, is van 2003, maar

142
00:06:21,900 --> 00:06:25,380
is eh, toen bleef hij doorgaan in de in de

143
00:06:25,380 --> 00:06:28,380
jaren later uh dankzij auto freeston

144
00:06:28,380 --> 00:06:32,100
waarin hij in feite de theorie van

145
00:06:32,100 --> 00:06:35,039
Robin Ballard nam en hij ontwikkelde en

146
00:06:35,039 --> 00:06:38,400
hij breidde het uit en generaliseerde het naar

147
00:06:38,400 --> 00:06:40,919
de theorie van generatieve modellen, dus

148
00:06:40,919 --> 00:06:42,720
eigenlijk is de belangrijkste bewering die

149
00:06:42,720 --> 00:06:45,479
carfiston deed dat creatieve codering

150
00:06:45,479 --> 00:06:48,780
een  Bewijsmaximalisatieschema van

151
00:06:48,780 --> 00:06:50,340
een specifiek soort generatief model

152
00:06:50,340 --> 00:06:52,979
waar ik doorheen ga om het later ook te introduceren,

153
00:06:52,979 --> 00:06:55,139


154
00:06:55,139 --> 00:07:00,300
dus om een ​​korte samenvatting te maken in de

155
00:07:00,300 --> 00:07:01,560
eerste twee

156
00:07:01,560 --> 00:07:03,900
uh soorten creatieve hoek die ik heb

157
00:07:03,900 --> 00:07:05,340
beschreven, dus signaalverwerking en

158
00:07:05,340 --> 00:07:07,020
compressie en de

159
00:07:07,020 --> 00:07:09,180
informatieverwerking in het netvlies en in de

160
00:07:09,180 --> 00:07:11,160
hersenen in het algemeen het zijn inferentiemethoden

161
00:07:11,160 --> 00:07:12,300


162
00:07:12,300 --> 00:07:14,819
en de grootste

163
00:07:14,819 --> 00:07:17,819
verandering de grootste revolutie die we

164
00:07:17,819 --> 00:07:21,120
in 1999 hadden, dus laten we zeggen dat in de 21e

165
00:07:21,120 --> 00:07:23,580
eeuw de operatieve codering werd gezien

166
00:07:23,580 --> 00:07:25,919
als een leeralgoritme, zodat we eerst

167
00:07:25,919 --> 00:07:29,699
informatie kunnen comprimeren  en update vervolgens alle

168
00:07:29,699 --> 00:07:31,800
synapsen of alle latente variabelen

169
00:07:31,800 --> 00:07:34,139
die we in ons generatieve model hebben om

170
00:07:34,139 --> 00:07:38,599
ons generatieve model zelf te verbeteren,

171
00:07:38,759 --> 00:07:43,199
dus laten we een paar uh-definities geven

172
00:07:43,199 --> 00:07:45,000
die een beetje formeler zijn,

173
00:07:45,000 --> 00:07:48,479
zodat operatieve codering kan worden gezien als een

174
00:07:48,479 --> 00:07:50,220
hiërarchische Gaussiaanse generatieve  model,

175
00:07:50,220 --> 00:07:53,400
dus hier is een heel eenvoudige figuur waarin

176
00:07:53,400 --> 00:07:54,780
we deze hiërarchische structuur hebben

177
00:07:54,780 --> 00:07:58,319
die zo diep kan zijn als we willen

178
00:07:58,319 --> 00:08:01,560
en signaalvoorspellingssignalen gaan van

179
00:08:01,560 --> 00:08:04,620
go van de ene latente variabele XM naar de

180
00:08:04,620 --> 00:08:06,599
volgende en het wordt

181
00:08:06,599 --> 00:08:09,720
elke keer getransformeerd via functie GN

182
00:08:09,720 --> 00:08:12,620
of GI

183
00:08:15,319 --> 00:08:18,180
dit is een generatief model zoals ik al zei en

184
00:08:18,180 --> 00:08:19,680
wat is de marginale waarschijnlijkheid van dit

185
00:08:19,680 --> 00:08:21,780
generatieve model, nou, het is gewoon de

186
00:08:21,780 --> 00:08:24,960
waarschijnlijkheid van de laatste,

187
00:08:24,960 --> 00:08:27,660
kun je mijn mijn cursor zien, ja, ja, ja,

188
00:08:27,660 --> 00:08:29,940
het is perfect, dus het is het genetische model

189
00:08:29,940 --> 00:08:32,700
van het laatste hoekpunt is de Sorry waarschijnlijke

190
00:08:32,700 --> 00:08:34,979
distributie  van het laatste hoekpunt maal

191
00:08:34,979 --> 00:08:37,140
de kansverdeling van elk

192
00:08:37,140 --> 00:08:40,440
ander hoekpunt, afhankelijk van de activiteit

193
00:08:40,440 --> 00:08:43,020
van het hoekpunt ervoor of van de

194
00:08:43,020 --> 00:08:45,860
latente variabele ervoor I.

195
00:08:45,899 --> 00:08:48,240
Ik heb al gezegd dat het een Gaussiaans

196
00:08:48,240 --> 00:08:50,399
generatief model is, wat betekent dat die

197
00:08:50,399 --> 00:08:54,260
kansen in Gauss-vorm zijn

198
00:08:54,660 --> 00:08:57,120
en elke

199
00:08:57,120 --> 00:09:00,480
endos-functie  functie G in het algemeen en

200
00:09:00,480 --> 00:09:02,880
vooral omdat bijvoorbeeld in

201
00:09:02,880 --> 00:09:05,459
Rambler-paper en in alle papers die

202
00:09:05,459 --> 00:09:07,920
daarna kwamen, ook vanwege de deep

203
00:09:07,920 --> 00:09:10,500
learning-revolutie, die functies

204
00:09:10,500 --> 00:09:13,220
gewoon lineaire kaarten zijn of

205
00:09:13,220 --> 00:09:15,120
niet-lineaire kaarten met

206
00:09:15,120 --> 00:09:18,000
activeringsfuncties of niet-lineaire kaarten met

207
00:09:18,000 --> 00:09:22,040
activeringsfunctie en een  additieve bias

208
00:09:23,220 --> 00:09:27,180
zodat we een formele

209
00:09:27,180 --> 00:09:28,860
definitie van creatieve codering kunnen geven en we kunnen

210
00:09:28,860 --> 00:09:30,300
zeggen dat operatieve codering een inversieschema is

211
00:09:30,300 --> 00:09:33,480
voor zo'n generatief model waarbij het

212
00:09:33,480 --> 00:09:35,839
modelbewijs wordt gemaximaliseerd door

213
00:09:35,839 --> 00:09:38,760
het minimaliseren van een grootheid die de

214
00:09:38,760 --> 00:09:40,920
variatie van vrije energie wordt genoemd

215
00:09:40,920 --> 00:09:43,740
in het algemeen de  het doel van elk generatief

216
00:09:43,740 --> 00:09:46,019
model is om modelbewijs te maximaliseren, maar

217
00:09:46,019 --> 00:09:48,860
deze hoeveelheid is altijd hardnekkig en

218
00:09:48,860 --> 00:09:51,019
we hebben een aantal

219
00:09:51,019 --> 00:09:53,279
technieken waarmee we

220
00:09:53,279 --> 00:09:55,980
de oplossing kunnen benaderen en degene

221
00:09:55,980 --> 00:09:58,500
die we gebruiken in creatieve codering

222
00:09:58,500 --> 00:10:00,720
in plaats van de aberratie van vrije

223
00:10:00,720 --> 00:10:03,480
energie te minimaliseren, wat een  wat een ondergrens is

224
00:10:03,480 --> 00:10:06,839
van het modelbewijs in dit werk en

225
00:10:06,839 --> 00:10:09,660
eigenlijk in veel in veel

226
00:10:09,660 --> 00:10:11,700
andere, dus het is de standaardmanier om het te doen

227
00:10:11,700 --> 00:10:13,740
deze minimalisatie wordt uitgevoerd het

228
00:10:13,740 --> 00:10:16,080
ingrediënt afdaling

229
00:10:16,080 --> 00:10:18,540
um en ja uitvoeren we hebben afgesproken in afkomst

230
00:10:18,540 --> 00:10:19,980
en er zijn  eigenlijk andere methoden,

231
00:10:19,980 --> 00:10:22,140
zoals verwachtingsmaximalisatie, wat

232
00:10:22,140 --> 00:10:23,580
vaak equivalent is,

233
00:10:23,580 --> 00:10:25,140
of je kunt een aantal andere

234
00:10:25,140 --> 00:10:26,940
algoritmen voor het doorgeven van berichten gebruiken, zoals

235
00:10:26,940 --> 00:10:29,959
bijvoorbeeld het verspreiden van overtuigingen

236
00:10:30,720 --> 00:10:33,980
en een beetje teruggaan in de tijd, dus

237
00:10:33,980 --> 00:10:35,940
een beetje vergeten over de

238
00:10:35,940 --> 00:10:38,760
statistische generatieve modellen

239
00:10:38,760 --> 00:10:41,360
als we creatieve codering kunnen zien

240
00:10:41,360 --> 00:10:44,040
als een ik bedoel, ik zei al een paar

241
00:10:44,040 --> 00:10:46,200
keer als een hiërarchisch model

242
00:10:46,200 --> 00:10:48,420
met de neurale activiteiten, dus met

243
00:10:48,420 --> 00:10:50,700
latente variabelen van neuronen die

244
00:10:50,700 --> 00:10:53,459
neurale activiteiten vertegenwoordigen, signaleert de zender lager in

245
00:10:53,459 --> 00:10:54,899
de hiërarchie

246
00:10:54,899 --> 00:10:57,540
en met foutknooppunten of foutneuronen

247
00:10:57,540 --> 00:11:01,019
signaleert de zender hoger in de hiërarchie, dus

248
00:11:01,019 --> 00:11:03,660
dit en  de foutinformatie terug

249
00:11:03,660 --> 00:11:05,700
wat is de variatie van vrije energie van

250
00:11:05,700 --> 00:11:08,220
deze klassegestuurde coderingsmodellen het is

251
00:11:08,220 --> 00:11:09,899
gewoon de som

252
00:11:09,899 --> 00:11:12,720
van de gemiddelde kwadratische fout van alle

253
00:11:12,720 --> 00:11:14,399
foutneuronen,

254
00:11:14,399 --> 00:11:18,120
dus het is de som van de fout

255
00:11:18,120 --> 00:11:21,980
van de totale fout in het kwadraat

256
00:11:22,019 --> 00:11:24,480
en deze representatie is  zal

257
00:11:24,480 --> 00:11:27,120
nuttig zijn in uh in de latere dia's en in

258
00:11:27,120 --> 00:11:28,740
hoe ik ga uitleggen hoe je

259
00:11:28,740 --> 00:11:30,120
creatieve codering kunt gebruiken om bijvoorbeeld causale gevolgtrekkingen te modelleren.

260
00:11:30,120 --> 00:11:32,940


261
00:11:32,940 --> 00:11:34,800
Ik denk dat voorspellende codering belangrijk is

262
00:11:34,800 --> 00:11:36,240
en dat het niet een leuk algoritme is om

263
00:11:36,240 --> 00:11:37,500


264
00:11:37,500 --> 00:11:39,600
allereerst goed te bestuderen als  Ik zei eerder dat het

265
00:11:39,600 --> 00:11:41,399
het juiste doel optimaliseert, namelijk

266
00:11:41,399 --> 00:11:43,079
het modelbewijs of de marginale

267
00:11:43,079 --> 00:11:44,339
waarschijnlijkheid,

268
00:11:44,339 --> 00:11:45,660
en dat

269
00:11:45,660 --> 00:11:47,700
doet het dan door een ondergrens te optimaliseren

270
00:11:47,700 --> 00:11:49,440
die de variatie van

271
00:11:49,440 --> 00:11:52,440
vrije energie wordt genoemd, zoals ik al zei, en de virtuele

272
00:11:52,440 --> 00:11:54,240
finish is interessant omdat het kan worden

273
00:11:54,240 --> 00:11:57,680
geschreven als een  som van twee verschillende termen

274
00:11:57,680 --> 00:12:00,839
die en elk van die termen om het te

275
00:12:00,839 --> 00:12:04,680
optimaliseren even belangrijk zijn,

276
00:12:04,680 --> 00:12:06,899
bijvoorbeeld bij machine learning-taken

277
00:12:06,899 --> 00:12:09,060
of in het algemeen bij leertaken,

278
00:12:09,060 --> 00:12:12,420
dus een van die termen dwingt memoriseren af,

279
00:12:12,420 --> 00:12:15,440
dus in de tweede term

280
00:12:15,440 --> 00:12:18,180
dwingt in feite het model af  om in een specifieke dataset te passen

281
00:12:18,180 --> 00:12:19,560


282
00:12:19,560 --> 00:12:21,240
en de eerste term

283
00:12:21,240 --> 00:12:23,519
dwingt het model om de

284
00:12:23,519 --> 00:12:26,040
complexiteit te minimaliseren en zoals we bijvoorbeeld weten

285
00:12:26,040 --> 00:12:28,500
voor de van de uitkomsten

286
00:12:28,500 --> 00:12:31,260
scheermestheorie als we twee verschillende modellen hebben

287
00:12:31,260 --> 00:12:33,000
die op dezelfde manier presteren op een specifieke

288
00:12:33,000 --> 00:12:35,640
trainingsset die we hebben  te krijgen

289
00:12:35,640 --> 00:12:37,380
en degene waarvan wordt verwacht dat

290
00:12:37,380 --> 00:12:39,899
deze het meest generaliseert, is de minder

291
00:12:39,899 --> 00:12:41,160
complexe,

292
00:12:41,160 --> 00:12:44,100
dus het bijwerken van een generatief model via

293
00:12:44,100 --> 00:12:46,380
operationele vrije energie stelt ons in staat om

294
00:12:46,380 --> 00:12:47,779
in feite te

295
00:12:47,779 --> 00:12:51,959
convergeren naar het optimale uh-uitkomstscheermesmodel,

296
00:12:51,959 --> 00:12:54,720
dat zowel een

297
00:12:54,720 --> 00:12:56,100
dataset onthoudt, maar het is ook  in staat om

298
00:12:56,100 --> 00:12:58,680
heel goed te generaliseren op ongeziene ongeziene

299
00:12:58,680 --> 00:13:00,240
gegevenspunten

300
00:13:00,240 --> 00:13:02,639
een tweede reden waarom operatieve codering

301
00:13:02,639 --> 00:13:08,600
belangrijk is, is dat het eigenlijk

302
00:13:08,720 --> 00:13:11,760
niet hoeft te worden gedefinieerd op

303
00:13:11,760 --> 00:13:13,920
hiërarchische structuur, maar het kan worden

304
00:13:13,920 --> 00:13:15,959
gemodelleerd op complexere en flexibelere

305
00:13:15,959 --> 00:13:18,240
architecturen zoals gerichte grafische

306
00:13:18,240 --> 00:13:21,540
model met elke vorm of nog

307
00:13:21,540 --> 00:13:23,700
meer gegeneraliseerd naar netwerken met veel cycli

308
00:13:23,700 --> 00:13:25,920
die lijken op hersenregio's en het

309
00:13:25,920 --> 00:13:27,779
eindresultaat in de onderliggende reden

310
00:13:27,779 --> 00:13:30,300
is dat je niet aan het leren en

311
00:13:30,300 --> 00:13:32,339
voorspellen bent met een voorwaartse pass en dan

312
00:13:32,339 --> 00:13:34,260
terug de fout propageren, maar je bent

313
00:13:34,260 --> 00:13:36,600
het minimaliseren van een energiefunctie

314
00:13:36,600 --> 00:13:38,459
en hierdoor kan in principe elke soort

315
00:13:38,459 --> 00:13:39,839
hiërarchie worden

316
00:13:39,839 --> 00:13:41,180
uh maakt het

317
00:13:41,180 --> 00:13:43,860
mogelijk om achter directe toetsen te gaan en

318
00:13:43,860 --> 00:13:46,860
cycli te leren en dit is

319
00:13:46,860 --> 00:13:48,060
eigenlijk vrij belangrijk omdat de

320
00:13:48,060 --> 00:13:50,399
hersenen vol zitten met cycli, aangezien we

321
00:13:50,399 --> 00:13:53,399
wat informatie hebben uit enkele recente artikelen

322
00:13:53,399 --> 00:13:56,459
uh die zijn erin geslaagd om

323
00:13:56,459 --> 00:13:59,279
de hersenen van sommige dieren, zoals

324
00:13:59,279 --> 00:14:00,420
fruitvlieg, volledig in kaart te brengen,

325
00:14:00,420 --> 00:14:03,899
de hersenen zitten vol met cycli, dus het is

326
00:14:03,899 --> 00:14:06,720
logisch om onze machine learning-

327
00:14:06,720 --> 00:14:09,000
modellen of

328
00:14:09,000 --> 00:14:11,160
onze modellen in het algemeen leeg te maken met een algoritme

329
00:14:11,160 --> 00:14:14,160
waarmee we

330
00:14:14,160 --> 00:14:17,160
cyclisch kunnen leegmaken  structuren

331
00:14:17,160 --> 00:14:19,380
de derde reden waarom operatieve codering

332
00:14:19,380 --> 00:14:21,240
interessant is, is dat formeel is

333
00:14:21,240 --> 00:14:23,820
bewezen dat het robuuster is dan

334
00:14:23,820 --> 00:14:25,139
standaard neurale netwerken, beginnend met

335
00:14:25,139 --> 00:14:27,060
zwarte voortplanting, dus als je een

336
00:14:27,060 --> 00:14:28,200
neuraal netwerk hebt en je wilt classificatietaken uitvoeren,

337
00:14:28,200 --> 00:14:30,320


338
00:14:30,320 --> 00:14:34,139
is creatieve codering robuuster

339
00:14:34,139 --> 00:14:36,260
en  dit is

340
00:14:36,260 --> 00:14:38,339
interessant in taken zoals online

341
00:14:38,339 --> 00:14:40,680
leertraining op kleine datasets of

342
00:14:40,680 --> 00:14:43,440
continue leertaken en de theorie

343
00:14:43,440 --> 00:14:45,540
komt in feite voort uit het feit dat

344
00:14:45,540 --> 00:14:48,540
impliciete gradiëntafdaling is verplaatst naar een

345
00:14:48,540 --> 00:14:50,820
benadering van impliciete gradiëntafdaling,

346
00:14:50,820 --> 00:14:53,339
wat een andere versie is van de

347
00:14:53,339 --> 00:14:54,899
expliciete gradiëntafdaling die is  de

348
00:14:54,899 --> 00:14:57,180
standaard groene afdaling die in

349
00:14:57,180 --> 00:14:59,880
principe in elk afzonderlijk model wordt gebruikt

350
00:14:59,880 --> 00:15:03,680
en het is een variatie die robuuster is.

351
00:15:05,880 --> 00:15:08,279
Ik denk oké, ik heb een behoorlijk lange intra-

352
00:15:08,279 --> 00:15:09,779
operatieve codering gedaan.

353
00:15:09,779 --> 00:15:11,639


354
00:15:11,639 --> 00:15:13,019


355
00:15:13,019 --> 00:15:15,839
causale gevolgtrekking causale

356
00:15:15,839 --> 00:15:18,420
gevolgtrekking is een theorie het is een zeer

357
00:15:18,420 --> 00:15:20,339
algemene theorie die

358
00:15:20,339 --> 00:15:23,100
het meest is geformaliseerd door Judy Apparel hij is absoluut

359
00:15:23,100 --> 00:15:25,500
de belangrijkste persoon op het

360
00:15:25,500 --> 00:15:27,839
gebied van causaliteit in Frankrijk hij heeft een aantal

361
00:15:27,839 --> 00:15:29,760
zeer mooie boeken geschreven, bijvoorbeeld het boek van

362
00:15:29,760 --> 00:15:32,760
Y wordt ten zeerste aanbevolen  als je

363
00:15:32,760 --> 00:15:35,220
meer wilt weten over dit onderwerp

364
00:15:35,220 --> 00:15:37,800
en het in feite het volgende probleem aanpakt,

365
00:15:37,800 --> 00:15:38,639


366
00:15:38,639 --> 00:15:40,440
dus laten we aannemen dat we een gezamenlijke

367
00:15:40,440 --> 00:15:42,000
kansverdeling hebben die is

368
00:15:42,000 --> 00:15:44,160
gekoppeld aan een Bayesiaans netwerk, dit

369
00:15:44,160 --> 00:15:46,199
wordt een beetje het doorlopende

370
00:15:46,199 --> 00:15:49,260
voorbeeld voor al het papier,

371
00:15:49,260 --> 00:15:51,839
vooral wanneer  je bent niet met Aziatische

372
00:15:51,839 --> 00:15:54,480
netwerken van deze vorm,

373
00:15:54,480 --> 00:15:57,660
het was gebaseerd op netwerken, de

374
00:15:57,660 --> 00:16:00,240
variabelen erin kunnen

375
00:16:00,240 --> 00:16:02,100
verschillende hoeveelheden vertegenwoordigen, dus ons

376
00:16:02,100 --> 00:16:04,620
visuele netwerk met deze vorm kan bijvoorbeeld

377
00:16:04,620 --> 00:16:06,899


378
00:16:06,899 --> 00:16:08,820
de hoeveelheden aan de rechterkant vertegenwoordigen, dus sociaal

379
00:16:08,820 --> 00:16:10,800
economisch Studiobeeld van een

380
00:16:10,800 --> 00:16:13,079
individu zijn  opleidingsniveau zijn

381
00:16:13,079 --> 00:16:16,699
intelligentie en zijn inkomensniveau

382
00:16:17,100 --> 00:16:19,440
iets waar de klassieke statistiek

383
00:16:19,440 --> 00:16:22,920
erg goed in is en is het uh terwijl uh de

384
00:16:22,920 --> 00:16:25,320
meest gebruikte toepassing is om

385
00:16:25,320 --> 00:16:28,019
observaties of correlaties te modelleren een

386
00:16:28,019 --> 00:16:29,279
correlatie beantwoordt in feite de

387
00:16:29,279 --> 00:16:32,519
vraag wat is het als we een

388
00:16:32,519 --> 00:16:35,579
andere variabele C observeren,

389
00:16:35,579 --> 00:16:37,500
dus bijvoorbeeld in  dit geval wat is wat is

390
00:16:37,500 --> 00:16:39,660
het inkomensniveau het verwachte

391
00:16:39,660 --> 00:16:41,820
inkomensniveau van een individu als ik

392
00:16:41,820 --> 00:16:44,339
dit opleidingsniveau observeer

393
00:16:44,339 --> 00:16:48,180
en natuurlijk als die persoon

394
00:16:48,180 --> 00:16:50,220
een hogere opleiding heeft,

395
00:16:50,220 --> 00:16:52,500
bijvoorbeeld een master of PhD. Ik verwacht dat

396
00:16:52,500 --> 00:16:54,360
die persoon algemeen zal hebben  een hoger

397
00:16:54,360 --> 00:16:56,040
inkomensniveau

398
00:16:56,040 --> 00:16:58,139
en dit is een correlatie,

399
00:16:58,139 --> 00:17:00,300
maar soms zijn er dingen die

400
00:17:00,300 --> 00:17:03,300
heel moeilijk waar te nemen zijn, maar ze spelen een

401
00:17:03,300 --> 00:17:05,040
grote rol bij het bepalen van die

402
00:17:05,040 --> 00:17:06,119
hoeveelheden,

403
00:17:06,119 --> 00:17:08,220
dus het kan bijvoorbeeld zijn dat het

404
00:17:08,220 --> 00:17:11,160
inkomensniveau veel meer wordt

405
00:17:11,160 --> 00:17:13,380
bepaald door de intelligentie van een

406
00:17:13,380 --> 00:17:15,540
specifieke persoon

407
00:17:15,540 --> 00:17:18,720
en en misschien dat de intelligentie of

408
00:17:18,720 --> 00:17:21,000
als een persoon intelligent is ook het meest

409
00:17:21,000 --> 00:17:24,540
waarschijnlijk een hoger opleidingsniveau heeft,

410
00:17:24,540 --> 00:17:27,540
maar toch is de echte reden waarom het

411
00:17:27,540 --> 00:17:30,120
inkomen I is vanwege

412
00:17:30,120 --> 00:17:32,220
het IQ

413
00:17:32,220 --> 00:17:34,740
en dit kan dit niet zijn  be Studies

414
00:17:34,740 --> 00:17:36,360
door simpelweg correlaties en moet worden

415
00:17:36,360 --> 00:17:39,120
bestudeerd door middel van een meer geavanceerde techniek

416
00:17:39,120 --> 00:17:41,280
die een interventie wordt genoemd

417
00:17:41,280 --> 00:17:43,320
een interventie beantwoordt in feite de

418
00:17:43,320 --> 00:17:46,500
vraag is wat D is als we C veranderen in

419
00:17:46,500 --> 00:17:48,240
een specifieke waarde,

420
00:17:48,240 --> 00:17:51,000
zodat we bijvoorbeeld een individu kunnen nemen

421
00:17:51,000 --> 00:17:54,660
en controleer zijn inkomensniveau

422
00:17:54,660 --> 00:17:57,120
en verander dan zijn opleidingsniveau, dus

423
00:17:57,120 --> 00:17:59,220
grijp in op deze wereld

424
00:17:59,220 --> 00:18:01,080
en verander zijn opleidingsniveau zonder

425
00:18:01,080 --> 00:18:03,419
zijn intelligentie aan te raken en kijk

426
00:18:03,419 --> 00:18:07,260
hoeveel zijn inkomen verandert,

427
00:18:07,260 --> 00:18:09,900
bijvoorbeeld als het inkomen veel verandert,

428
00:18:09,900 --> 00:18:12,179
betekent dit dat de intelligentie

429
00:18:12,179 --> 00:18:14,460
niet verandert  speelt hierin geen grote rol, maar het

430
00:18:14,460 --> 00:18:16,799
opleidingsniveau wel. Als het inkomensniveau

431
00:18:16,799 --> 00:18:19,020
niet veel verandert, betekent dit dat er misschien

432
00:18:19,020 --> 00:18:20,640
een verborgen variabele is, in dit geval

433
00:18:20,640 --> 00:18:22,860
de intelligentie die het

434
00:18:22,860 --> 00:18:25,760
inkomensniveau van een persoon bepaalt.

435
00:18:25,980 --> 00:18:28,740
De derde belangrijke causale

436
00:18:28,740 --> 00:18:31,080
gevolgtrekking is dat  van counterfactuals, dus

437
00:18:31,080 --> 00:18:33,120
bijvoorbeeld een counterfactual beantwoordt de

438
00:18:33,120 --> 00:18:36,720
vraag wat zou het zijn en we veranderen

439
00:18:36,720 --> 00:18:39,240
C in een andere waarde in het verleden,

440
00:18:39,240 --> 00:18:40,679
zodat we bijvoorbeeld kunnen zien dat het

441
00:18:40,679 --> 00:18:42,059
verschil tussen interventies en

442
00:18:42,059 --> 00:18:45,059
counterfactuals is dat interventies

443
00:18:45,059 --> 00:18:47,820
in de toekomst werken, dus ik interview  in

444
00:18:47,820 --> 00:18:50,340
de wereld nu om een ​​verandering in de toekomst waar te nemen,

445
00:18:50,340 --> 00:18:53,220
goed contrafeitelijk, stelt ons in staat om

446
00:18:53,220 --> 00:18:56,039
terug in de tijd te gaan en een variabele

447
00:18:56,039 --> 00:18:59,160
terug in de tijd te veranderen en te zien hoe die verandering

448
00:18:59,160 --> 00:19:01,320
de wereld waarin we nu leven zou hebben beïnvloed

449
00:19:01,320 --> 00:19:02,940


450
00:19:02,940 --> 00:19:06,299
en die worden door judapple gedefinieerd als de

451
00:19:06,299 --> 00:19:08,100
drie  niveaus van causale gevolgtrekking

452
00:19:08,100 --> 00:19:09,660
correlatie is het eerste niveau

453
00:19:09,660 --> 00:19:11,580
interventie is het tweede niveau in

454
00:19:11,580 --> 00:19:14,720
contrafeitelijk is het derde niveau

455
00:19:16,020 --> 00:19:18,120
andere interventies Ik ga

456
00:19:18,120 --> 00:19:20,640
ze formeler definiëren nu ik

457
00:19:20,640 --> 00:19:23,760
een intuïtieve definitie heb gegeven en ik

458
00:19:23,760 --> 00:19:25,500
gebruik deze notatie hier  wat

459
00:19:25,500 --> 00:19:27,240
eigenlijk in de hele

460
00:19:27,240 --> 00:19:29,640
presentatie hetzelfde is, dus X zal altijd

461
00:19:29,640 --> 00:19:32,820
een latente variabele zijn s ik zal altijd

462
00:19:32,820 --> 00:19:35,340
een datapunt of een observatie zijn

463
00:19:35,340 --> 00:19:38,520
en VI zal altijd een Vertex zijn, dus

464
00:19:38,520 --> 00:19:40,860
elke keer dat je VI ziet, zijn we alleen

465
00:19:40,860 --> 00:19:42,720


466
00:19:42,720 --> 00:19:45,299
bijvoorbeeld geïnteresseerd in de structuur van de grafiek,

467
00:19:45,299 --> 00:19:46,860
dus laten we aannemen dat we een Bayesiaans model hebben

468
00:19:46,860 --> 00:19:50,160
dat dezelfde structuur heeft

469
00:19:50,160 --> 00:19:52,679
als het Bayesiaanse model dat we in de

470
00:19:52,679 --> 00:19:54,780
vorige dia zagen,

471
00:19:54,780 --> 00:19:57,840
gegeven dat X3 gelijk is aan S3 dit is de

472
00:19:57,840 --> 00:20:00,660
observatie die we maken statistieken maakt het mogelijk  ons

473
00:20:00,660 --> 00:20:03,360
om de waarschijnlijkheid of de

474
00:20:03,360 --> 00:20:04,679
verwachting

475
00:20:04,679 --> 00:20:07,380
van X4 te berekenen, wat de latente variabele is die

476
00:20:07,380 --> 00:20:09,240
verband houdt met dit hoekpunt,

477
00:20:09,240 --> 00:20:13,860
aangezien X3 gelijk is aan S3

478
00:20:13,860 --> 00:20:15,679
buitenlandse

479
00:20:15,679 --> 00:20:17,760
interventie, we hebben een nieuw soort

480
00:20:17,760 --> 00:20:19,919
notatie nodig die de do-bewerking wordt genoemd,

481
00:20:19,919 --> 00:20:21,179


482
00:20:21,179 --> 00:20:23,880
dus in dit geval

483
00:20:23,880 --> 00:20:26,100
willen we X4 berekenen  de waarschijnlijkheid van

484
00:20:26,100 --> 00:20:30,000
X4 gezien het feit dat we ingrijpen in

485
00:20:30,000 --> 00:20:33,059
het woord en X3 veranderen West 3.

486
00:20:33,059 --> 00:20:35,580
en hoe we dit doen om een

487
00:20:35,580 --> 00:20:38,400
interventie uit te voeren Judo Pearl vertelt ons dat we

488
00:20:38,400 --> 00:20:40,020


489
00:20:40,020 --> 00:20:41,880


490
00:20:41,880 --> 00:20:45,059
eerst een tussenstap moeten hebben voordat we een correlatie berekenen  we

491
00:20:45,059 --> 00:20:46,860
moeten alles verwijderen om alle

492
00:20:46,860 --> 00:20:50,160
inkomende randen naar V3 te verwijderen,

493
00:20:50,160 --> 00:20:52,799
dus we moeten niet dit Bayesiaanse

494
00:20:52,799 --> 00:20:55,679
netwerk bestuderen, maar dit tweede

495
00:20:55,679 --> 00:20:58,200
en dan mogen we op dit punt

496
00:20:58,200 --> 00:21:00,840
een correlatie berekenen zoals we

497
00:21:00,840 --> 00:21:03,299
normaal doen

498
00:21:03,299 --> 00:21:06,500
en dit is een interventie

499
00:21:07,020 --> 00:21:09,299
een contrafeitelijk  is een generalisatie

500
00:21:09,299 --> 00:21:11,700
hiervan die, zoals ik al zei, in het verleden leefde

501
00:21:11,700 --> 00:21:14,100
en ze berekenen met behulp van structurele

502
00:21:14,100 --> 00:21:15,419
causale modellen,

503
00:21:15,419 --> 00:21:18,299
een causaal structuurmodel is een tuple

504
00:21:18,299 --> 00:21:21,120
die conceptueel vergelijkbaar is met een

505
00:21:21,120 --> 00:21:23,460
Bayesiaans netwerk, maar in feite hebben we

506
00:21:23,460 --> 00:21:26,220
deze nieuwe klasse van variabelen bovenop die

507
00:21:26,220 --> 00:21:28,580
zijn de niet-waarneembare variabelen die ze gebruiken,

508
00:21:28,580 --> 00:21:30,960
dus we hebben het Bayesiaanse netwerk dat we

509
00:21:30,960 --> 00:21:34,020
hadden vóór X1 X2 X3 S4,

510
00:21:34,020 --> 00:21:37,460
maar we hebben ook die niet-waarneembare of

511
00:21:37,460 --> 00:21:40,020
variabelen die afhankelijk zijn van de omgeving,

512
00:21:40,020 --> 00:21:42,539
je kunt ze niet beheersen, je kunt

513
00:21:42,539 --> 00:21:43,980
ze afleiden, maar jij,

514
00:21:43,980 --> 00:21:46,020
maar ze zijn er

515
00:21:46,020 --> 00:21:48,539
en

516
00:21:48,539 --> 00:21:51,360
f  is een set functies die afhangt van

517
00:21:51,360 --> 00:21:53,400
alle in

518
00:21:53,400 --> 00:21:57,299
wezen f van x van x3 hangt af van X1

519
00:21:57,299 --> 00:21:58,980
omdat we een pijl op x2 hebben omdat

520
00:21:58,980 --> 00:22:00,960
je een pijl hebt en van de

521
00:22:00,960 --> 00:22:02,940
niet-waarneembare variabele die ook

522
00:22:02,940 --> 00:22:05,840
extreem beïnvloedt,

523
00:22:06,179 --> 00:22:09,240
dus ja intuïtief kun je ons zien, je

524
00:22:09,240 --> 00:22:11,940
kunt denken  van een structureel causaal model

525
00:22:11,940 --> 00:22:14,159
als een Bayesiaans netwerk met die

526
00:22:14,159 --> 00:22:16,679
niet-waarneembare variabelen bovenaan en elke

527
00:22:16,679 --> 00:22:19,500
niet-waarneembare variabele beïnvloedt alleen

528
00:22:19,500 --> 00:22:22,020
zijn eigen

529
00:22:22,020 --> 00:22:24,600
laatste variabele X, dus bijvoorbeeld

530
00:22:24,600 --> 00:22:27,960
IU zal X1 ook nooit raken u3

531
00:22:27,960 --> 00:22:30,360
zal alleen Q3 raken E1 zal allemaal

532
00:22:30,360 --> 00:22:34,039
X1 beïnvloeden en  enzovoort, enzovoort, dus

533
00:22:35,039 --> 00:22:37,679
het uitvoeren van contrafeitelijke gevolgtrekkingen beantwoordt

534
00:22:37,679 --> 00:22:39,900
de volgende vraag, dus wat

535
00:22:39,900 --> 00:22:42,960
zou X4 zijn bij X3 is gelijk aan een andere

536
00:22:42,960 --> 00:22:46,620
variabele in een pass-situatie die je

537
00:22:46,620 --> 00:22:49,340
buitenlandse

538
00:22:49,340 --> 00:22:51,840
drie verschillende stappen vereist, dus

539
00:22:51,840 --> 00:22:53,039
abductie

540
00:22:53,039 --> 00:22:54,900


541
00:22:54,900 --> 00:22:57,179
is de berekening van alle

542
00:22:57,179 --> 00:22:59,460
achtergrondvariabelen, dus in dit  in deze stap

543
00:22:59,460 --> 00:23:01,200
willen we teruggaan in de tijd en begrijpen

544
00:23:01,200 --> 00:23:03,419
hoe de omgeving, de niet-waarneembare

545
00:23:03,419 --> 00:23:04,919
omgeving, was

546
00:23:04,919 --> 00:23:08,039
op dat specifieke moment in de tijd

547
00:23:08,039 --> 00:23:11,039
en we doen dit door alle latente

548
00:23:11,039 --> 00:23:14,280
variabelen X te fixeren op enkele specifieke gegevens die

549
00:23:14,280 --> 00:23:16,140
we al hebben

550
00:23:16,140 --> 00:23:18,960
en en deze uh dit uit te voeren

551
00:23:18,960 --> 00:23:21,120
gevolgtrekking op het gebruikte

552
00:23:21,120 --> 00:23:24,240
dan gaan we de U gebruiken

553
00:23:24,240 --> 00:23:26,940
om de U die we hebben geleerd te behouden en

554
00:23:26,940 --> 00:23:28,500
een interventie uit te voeren

555
00:23:28,500 --> 00:23:29,880
zodat

556
00:23:29,880 --> 00:23:32,340
een tegenfactuur ook gezien kan worden als

557
00:23:32,340 --> 00:23:34,980
een interventie terug in de tijd waarin we

558
00:23:34,980 --> 00:23:36,960
de omgeving kennen de

559
00:23:36,960 --> 00:23:40,620
omgevingsvariabelen U1 U2  en u4 op dat specifieke

560
00:23:40,620 --> 00:23:43,039
moment

561
00:23:43,200 --> 00:23:44,340
en

562
00:23:44,340 --> 00:23:46,679
wat is de ontbrekende stap,

563
00:23:46,679 --> 00:23:49,440
dus wat zou X4 op X3 gelijk zijn aan een

564
00:23:49,440 --> 00:23:50,780
ander

565
00:23:50,780 --> 00:23:53,280
ander datapunt in die specifieke

566
00:23:53,280 --> 00:23:55,980
situatie nu kunnen we een correlatie berekenen

567
00:23:55,980 --> 00:23:57,120


568
00:23:57,120 --> 00:23:59,520
en de correlatie doen we op het pad

569
00:23:59,520 --> 00:24:02,039
op de grafiek  waarin we

570
00:24:02,039 --> 00:24:04,440
al een interventie hebben uitgevoerd met behulp van

571
00:24:04,440 --> 00:24:06,659
de omgevingsvariabelen die we hebben

572
00:24:06,659 --> 00:24:10,140
geleerd in de ontvoeringsstap

573
00:24:10,140 --> 00:24:14,419
en dit is een contrafeitelijke gevolgtrekking

574
00:24:15,480 --> 00:24:18,000
dit is de laatste dia van de causale

575
00:24:18,000 --> 00:24:20,159
gevolgtrekking huidige introductie

576
00:24:20,159 --> 00:24:21,720
en gaat over structureel leren

577
00:24:21,720 --> 00:24:23,880
eigenlijk eigenlijk alles wat ik heb gezegd

578
00:24:23,880 --> 00:24:27,360
berust tot nu toe op het feit dat we

579
00:24:27,360 --> 00:24:29,700
de causale afhankelijkheden tussen

580
00:24:29,700 --> 00:24:31,500
de gegevenspunten kennen, dus we kennen de structuur

581
00:24:31,500 --> 00:24:33,120
van de grafiek, we weten welke variabele welke

582
00:24:33,120 --> 00:24:34,860
beïnvloedt,

583
00:24:34,860 --> 00:24:37,260
we kennen de pijlen in het algemeen,

584
00:24:37,260 --> 00:24:39,659
maar in de praktijk is dit niet

585
00:24:39,659 --> 00:24:42,900
altijd mogelijk, dus we

586
00:24:42,900 --> 00:24:45,419
we hebben meestal geen toegang tot de causale grafiek

587
00:24:45,419 --> 00:24:47,400
en eigenlijk is het leren van

588
00:24:47,400 --> 00:24:49,919
de beste causale grafiek uit gegevens nog steeds

589
00:24:49,919 --> 00:24:51,840


590
00:24:51,840 --> 00:24:53,880


591
00:24:53,880 --> 00:24:57,299


592
00:24:57,299 --> 00:24:58,380


593
00:24:58,380 --> 00:25:01,140
een open probleem.

594
00:25:01,140 --> 00:25:03,179
dus zoals ik al zei, het doel is om

595
00:25:03,179 --> 00:25:04,740
Raadsrelaties te verwijzen van

596
00:25:04,740 --> 00:25:07,380
observatiegegevens, dus gegeven een dataset

597
00:25:07,380 --> 00:25:09,780
willen we de gerichte exacte

598
00:25:09,780 --> 00:25:12,179
grafiek afleiden die de connectiviteit

599
00:25:12,179 --> 00:25:14,460
tussen het systeem en de variabelen van

600
00:25:14,460 --> 00:25:15,960
de dataset beschrijft,

601
00:25:15,960 --> 00:25:17,700
dus hier hebben we bijvoorbeeld een voorbeeld

602
00:25:17,700 --> 00:25:19,440
dat ik denk dat we

603
00:25:19,440 --> 00:25:22,860
allemaal bekend zijn met bedankt uh vanwege

604
00:25:22,860 --> 00:25:25,080
de pandemie, dus we hebben die vier

605
00:25:25,080 --> 00:25:28,799
variabelen leeftijd ziekenhuisopname vaccin

606
00:25:28,799 --> 00:25:31,380
en en CT

607
00:25:31,380 --> 00:25:33,600
en we willen de causale

608
00:25:33,600 --> 00:25:36,059
afhankelijkheden tussen die variabelen afleiden, dus

609
00:25:36,059 --> 00:25:37,980
we willen bijvoorbeeld direct leren

610
00:25:37,980 --> 00:25:40,260
van gegevens dat de waarschijnlijkheid  van een

611
00:25:40,260 --> 00:25:43,080
persoon die in het ziekenhuis wordt opgenomen, hangt af van

612
00:25:43,080 --> 00:25:45,419
zijn leeftijd en van het feit of hij is

613
00:25:45,419 --> 00:25:49,760
gevaccineerd of niet, enzovoort, dus

614
00:25:51,299 --> 00:25:55,020
dit is het einde van de lange

615
00:25:55,020 --> 00:25:58,080
inleiding, maar ik hoop dat het duidelijk

616
00:25:58,080 --> 00:26:00,179
genoeg was en ik hoop dat ik gaf zoals de

617
00:26:00,179 --> 00:26:02,039
de basis om

618
00:26:02,039 --> 00:26:05,159
de resultaten van het artikel te begrijpen en

619
00:26:05,159 --> 00:26:07,740
nu kunnen we naar de onderzoeksvragen gaan,

620
00:26:07,740 --> 00:26:09,059
dus de onderzoeksvragen zijn eerst de

621
00:26:09,059 --> 00:26:10,440
volgende.

622
00:26:10,440 --> 00:26:12,900
Ik wil zien

623
00:26:12,900 --> 00:26:15,299
of creatieve codering kan worden gebruikt om

624
00:26:15,299 --> 00:26:16,980
causale gevolgtrekkingen uit te voeren,

625
00:26:16,980 --> 00:26:20,100
dus tot nu toe is operatieve codering alleen

626
00:26:20,100 --> 00:26:22,380
gebruikt  om correlaties te berekenen

627
00:26:22,380 --> 00:26:25,020
in Bayesiaanse netwerken

628
00:26:25,020 --> 00:26:27,419
en de grote vraag is of we verder kunnen gaan dan

629
00:26:27,419 --> 00:26:29,400
correlatie en modelinterventie en

630
00:26:29,400 --> 00:26:31,679
contrafeitelijke op een biologisch plausibele

631
00:26:31,679 --> 00:26:32,760
manier,

632
00:26:32,760 --> 00:26:34,380
dus

633
00:26:34,380 --> 00:26:36,120
op een manier dat het bijvoorbeeld eenvoudig

634
00:26:36,120 --> 00:26:39,059
intuïtief is en ons in staat stelt alleen met

635
00:26:39,059 --> 00:26:40,740
de neuronen te spelen  en bijvoorbeeld

636
00:26:40,740 --> 00:26:43,740
de enorme structuur van de grafiek niet raken

637
00:26:43,740 --> 00:26:46,380
en meer in de praktijk, meer specifiek,

638
00:26:46,380 --> 00:26:48,299
wordt de vraag: kunnen we een op

639
00:26:48,299 --> 00:26:51,000
operatieve codering gebaseerd causaal

640
00:26:51,000 --> 00:26:52,740
model definiëren om interventies en contrafeiten uit te voeren.

641
00:26:52,740 --> 00:26:55,320


642
00:26:55,320 --> 00:26:58,380
De tweede vraag is,

643
00:26:58,380 --> 00:27:00,179
zoals ik al zei, dat het hebben van een aangepast

644
00:27:00,179 --> 00:27:02,159
structuurmodel ervan uitgaat dat we  de structuur

645
00:27:02,159 --> 00:27:04,260
van het ontwijkingsnetwerk kennen,

646
00:27:04,260 --> 00:27:07,919
dus we gaan ervan uit dat we de pijlen hebben

647
00:27:07,919 --> 00:27:09,960
kunnen we verder gaan en creatieve

648
00:27:09,960 --> 00:27:11,520
coderingsnetwerken gebruiken om de causale

649
00:27:11,520 --> 00:27:14,418
structuur van de grafiek te leren door

650
00:27:16,140 --> 00:27:18,900
in feite positieve antwoorden te geven op

651
00:27:18,900 --> 00:27:21,120
beide vragen, zouden we

652
00:27:21,120 --> 00:27:23,120
voorspellende codering kunnen gebruiken als  een end-to-end

653
00:27:23,120 --> 00:27:26,039
causale inferentiemethode die in feite

654
00:27:26,039 --> 00:27:28,740
een dataset gebruikt en ons in staat stelt om

655
00:27:28,740 --> 00:27:30,419
interventies en contrafeitelijke

656
00:27:30,419 --> 00:27:34,820
voorspellingen rechtstreeks uit deze dataset te testen,

657
00:27:36,840 --> 00:27:39,299
dus laten we het eerste het

658
00:27:39,299 --> 00:27:40,740
eerste probleem aanpakken, dus causale inferentie

659
00:27:40,740 --> 00:27:42,419
vibratieve codering die ook de

660
00:27:42,419 --> 00:27:45,120
sectie is die geeft  de titel van het

661
00:27:45,120 --> 00:27:46,740
artikel is in feite

662
00:27:46,740 --> 00:27:48,539
en hier zal ik laten zien hoe

663
00:27:48,539 --> 00:27:50,760
correlaties operatieve codering moet worden uitgevoerd, wat

664
00:27:50,760 --> 00:27:52,440
uh al bekend is

665
00:27:52,440 --> 00:27:54,419
um en hoe interventie-

666
00:27:54,419 --> 00:27:56,760
query's moeten worden uitgevoerd waarvan ik denk dat het

667
00:27:56,760 --> 00:28:01,140
de echte vraag van het artikel is,

668
00:28:01,140 --> 00:28:03,900
dus hier is een oorzakelijk verband  grafiek die de

669
00:28:03,900 --> 00:28:05,700
gebruikelijke grafiek is

670
00:28:05,700 --> 00:28:07,260
die we hadden

671
00:28:07,260 --> 00:28:09,240
en hier is het bijbehorende creatieve

672
00:28:09,240 --> 00:28:11,760
coderingsmodel, dus de assen zijn de

673
00:28:11,760 --> 00:28:13,980
latente variabelen en komen overeen met de

674
00:28:13,980 --> 00:28:18,000
neuronen in een neuraal netwerkmodel

675
00:28:18,000 --> 00:28:20,760
en The Black Arrow gaat uit

676
00:28:20,760 --> 00:28:22,740
voorspellingsinformatie van één neuron

677
00:28:22,740 --> 00:28:25,559
naar de ene naar beneden in de hiërarchie

678
00:28:25,559 --> 00:28:28,500
en elk hoekpunt heeft ook dit

679
00:28:28,500 --> 00:28:31,140
foutneuron dat informatie doorgeeft naar boven in de

680
00:28:31,140 --> 00:28:32,820
hiërarchie, zodat de informatie van elke

681
00:28:32,820 --> 00:28:36,480
fout naar de naar het waardeknooppunt in

682
00:28:36,480 --> 00:28:39,120
de hiërarchie gaat en het in feite vertelt

683
00:28:39,120 --> 00:28:41,400
om zichzelf te corrigeren om te veranderen  de

684
00:28:41,400 --> 00:28:43,760
voorspelling

685
00:28:44,700 --> 00:28:46,559
dus om een ​​correlatie uit te voeren met behulp van

686
00:28:46,559 --> 00:28:48,840
voorspellende codering, wat je moet doen, is

687
00:28:48,840 --> 00:28:50,400
dat je een waarneming doet en je

688
00:28:50,400 --> 00:28:52,620
legt eenvoudig de waarde van een specifiek neuron vast,

689
00:28:52,620 --> 00:28:53,820


690
00:28:53,820 --> 00:28:55,200
dus als je de

691
00:28:55,200 --> 00:28:58,740
waarschijnlijkheid van X4 wilt berekenen, gegeven X3 gelijk aan S3,

692
00:28:58,740 --> 00:29:02,340
moeten we gewoon  neem X3 en fixeer het op

693
00:29:02,340 --> 00:29:04,380
S3 op een manier dat het niet

694
00:29:04,380 --> 00:29:08,159
meer verandert en voer een energieminimalisatie uit

695
00:29:08,159 --> 00:29:09,720
en dit model

696
00:29:09,720 --> 00:29:12,659
en door te minimaliseren door de as bij te werken

697
00:29:12,659 --> 00:29:16,380
uh via een minimalisatie van de variatie

698
00:29:16,380 --> 00:29:18,419
van vrije energie kan het model

699
00:29:18,419 --> 00:29:20,820
convergeren naar een oplossing  op deze vraag,

700
00:29:20,820 --> 00:29:22,919
dus de waarschijnlijkheid of de verwachte waarde

701
00:29:22,919 --> 00:29:27,179
van X4 gegeven X3 is gelijk aan 3.

702
00:29:27,179 --> 00:29:29,340
maar hoe voer ik nu een interventie uit

703
00:29:29,340 --> 00:29:31,679
zonder de structuur van de grafiek te beïnvloeden,

704
00:29:31,679 --> 00:29:33,419


705
00:29:33,419 --> 00:29:35,640
dit is eigenlijk het eerste

706
00:29:35,640 --> 00:29:37,679
idee van het artikel

707
00:29:37,679 --> 00:29:39,960
oh dit is nog steeds hoe  voer een

708
00:29:39,960 --> 00:29:43,260
correlatie uit, dus fix S3 gelijk aan X3 is de

709
00:29:43,260 --> 00:29:45,600
eerste stap in het algoritme en de

710
00:29:45,600 --> 00:29:47,220
tweede is om de as te verkrijgen door

711
00:29:47,220 --> 00:29:50,539
de variatie van vrije energie te minimaliseren

712
00:29:51,240 --> 00:29:53,340
een interventie die in theorie

713
00:29:53,340 --> 00:29:55,200
overeenkomt met het verwijderen van die

714
00:29:55,200 --> 00:29:56,220
pijlen

715
00:29:56,220 --> 00:29:57,659
en antwoorden op de vraag de

716
00:29:57,659 --> 00:29:59,279
waarschijnlijkheid  van X4

717
00:29:59,279 --> 00:30:02,399
door een interventie uit te voeren, dus X3 is

718
00:30:02,399 --> 00:30:04,860
gelijk aan drie imperatieve codering kan

719
00:30:04,860 --> 00:30:07,080
als volgt worden uitgevoerd,

720
00:30:07,080 --> 00:30:09,840
dus ik ga het algoritme hier schrijven,

721
00:30:09,840 --> 00:30:13,140
dus eerst omdat je in een correlatie vaststelt dat S3

722
00:30:13,140 --> 00:30:17,039
gelijk is aan de iFix X3 gelijk aan de

723
00:30:17,039 --> 00:30:18,720
observatie die je krijgt

724
00:30:18,720 --> 00:30:21,299
dan is dit de belangrijke stap die

725
00:30:21,299 --> 00:30:24,059
je moet nemen om niet meer op de grafiek in te grijpen,

726
00:30:24,059 --> 00:30:26,700
maar op de voorspellingsfout en

727
00:30:26,700 --> 00:30:28,980
deze gelijk aan nul te maken

728
00:30:28,980 --> 00:30:31,020
een voorspellingsfout gelijk aan nul te hebben,

729
00:30:31,020 --> 00:30:32,480


730
00:30:32,480 --> 00:30:36,179
zorgt er in feite voor dat uh betekenisloze

731
00:30:36,179 --> 00:30:38,460
informatie de hiërarchie opstuurt of eigenlijk

732
00:30:38,460 --> 00:30:40,200
geen informatie van de hiërarchie verzendt  hiërarchie

733
00:30:40,200 --> 00:30:41,880
omdat het je in feite vertelt dat de

734
00:30:41,880 --> 00:30:44,659
voorspelling altijd correct is

735
00:30:44,659 --> 00:30:48,120
en de derde stap is om, zoals we

736
00:30:48,120 --> 00:30:50,220
eerder deden, de as de

737
00:30:50,220 --> 00:30:52,919
onbeperkte as of X1 X2 X4 bij te werken door

738
00:30:52,919 --> 00:30:55,679
de variatie van vrije energie te minimaliseren,

739
00:30:55,679 --> 00:30:59,039
zoals ik nu zal laten zien, of experimenteel door

740
00:30:59,039 --> 00:31:00,840
simpelweg te doen  deze kleine truc om

741
00:31:00,840 --> 00:31:02,399
een ​​voorspellingsfout in te stellen die

742
00:31:02,399 --> 00:31:05,120
gelijk is aan nul,

743
00:31:05,640 --> 00:31:08,220
verhindert ons om daadwerkelijk op de

744
00:31:08,220 --> 00:31:10,320
structuur van de grafiek te reageren

745
00:31:10,320 --> 00:31:13,620
zoals de theorie van de calculus doet en om

746
00:31:13,620 --> 00:31:16,919
de ontbrekende uh de variabelen na

747
00:31:16,919 --> 00:31:19,140
een interventie af te leiden door simpelweg uit te voeren

748
00:31:19,140 --> 00:31:22,640
aberratie van minimalisatie van vrije energie

749
00:31:24,659 --> 00:31:26,580
hoe zit het met contrafeitelijke gevolgtrekking contrafeitelijke

750
00:31:26,580 --> 00:31:28,080
gevolgtrekking is eigenlijk

751
00:31:28,080 --> 00:31:30,539
eenvoudig als we eenmaal hebben

752
00:31:30,539 --> 00:31:34,740
gedefinieerd hoe een interventie te doen

753
00:31:34,740 --> 00:31:36,539
en dit komt omdat, zoals we eerder zagen, het

754
00:31:36,539 --> 00:31:38,640
uitvoeren van een contrafeitelijk vergelijkbaar is

755
00:31:38,640 --> 00:31:40,380
met het uitvoeren van een interventie in een eerdere

756
00:31:40,380 --> 00:31:44,360
situatie nadat u de

757
00:31:44,360 --> 00:31:48,120
niet-waarneembaar de niet-waarneembare variabelen,

758
00:31:48,120 --> 00:31:49,620


759
00:31:49,620 --> 00:31:51,480
zoals je kunt zien in de plot die ik eerder liet zien

760
00:31:51,480 --> 00:31:53,520
over de ontvoeringsactie en

761
00:31:53,520 --> 00:31:56,039
voorspellingsstappen de actie- en

762
00:31:56,039 --> 00:31:58,320
voorspellingsstappen ze hadden niet die

763
00:31:58,320 --> 00:31:59,640
twee pijlen

764
00:31:59,640 --> 00:32:02,580
ze waren verwijderd mooie codering

765
00:32:02,580 --> 00:32:06,299
stelt ons in staat om de pijlen hiervan uh

766
00:32:06,299 --> 00:32:08,279
binnen te houden  de grafiek

767
00:32:08,279 --> 00:32:11,340
en en voer contrafeiten uit door

768
00:32:11,340 --> 00:32:13,380
simpelweg een ontvoeringsstap uit te voeren zoals

769
00:32:13,380 --> 00:32:14,640
eerder

770
00:32:14,640 --> 00:32:16,679
een actiestap waarin we eenvoudigweg

771
00:32:16,679 --> 00:32:18,600
een interventie uitvoeren op het enkele

772
00:32:18,600 --> 00:32:21,240
knooppunt, dus we fixeren het waardeknooppunt en we zetten

773
00:32:21,240 --> 00:32:24,240
de fout op nul

774
00:32:24,240 --> 00:32:26,399
en voeren de energieminimalisatie uit naar

775
00:32:26,399 --> 00:32:27,960
minimaliseer de duur van vrije energie om

776
00:32:27,960 --> 00:32:30,679
de voorspelling te berekenen,

777
00:32:32,399 --> 00:32:36,299
dus ik denk dat dit een gemakkelijke en

778
00:32:36,299 --> 00:32:39,840
elegante methode is om interventies

779
00:32:39,840 --> 00:32:42,899
en tegenfeiten uit te voeren en uh

780
00:32:42,899 --> 00:32:44,880
ja, dus ik denk dat we

781
00:32:44,880 --> 00:32:46,500
nu moeten laten zien of het in de praktijk werkt

782
00:32:46,500 --> 00:32:48,720
of niet en we  heb een paar

783
00:32:48,720 --> 00:32:49,919
experimenten

784
00:32:49,919 --> 00:32:52,440
en ik ga je nu twee

785
00:32:52,440 --> 00:32:54,240
verschillende experimenten laten zien, de eerste is

786
00:32:54,240 --> 00:32:57,179
slechts een proof of concept-experiment

787
00:32:57,179 --> 00:33:01,020
dat aantoont dat in de operatieve codering

788
00:33:01,020 --> 00:33:02,480
in staat is om

789
00:33:02,480 --> 00:33:06,120
interventie en contrafeiten uit te voeren

790
00:33:06,120 --> 00:33:08,700
en de tweede laat eigenlijk een

791
00:33:08,700 --> 00:33:11,220
eenvoudige toepassing zien  in hoe Interventionele

792
00:33:11,220 --> 00:33:13,440
query's kunnen worden gebruikt om de

793
00:33:13,440 --> 00:33:16,260
prestaties van classificatietaken op een

794
00:33:16,260 --> 00:33:18,360
specifiek soort operationele

795
00:33:18,360 --> 00:33:20,940
coderingsnetwerken te verbeteren, namelijk dat van een volledig

796
00:33:20,940 --> 00:33:22,080
verbonden model,

797
00:33:22,080 --> 00:33:24,659
laten we beginnen bij het eerste,

798
00:33:24,659 --> 00:33:27,679
dus hoe doen we deze taak, dus gegeven een

799
00:33:27,679 --> 00:33:30,360
structureel Council-model

800
00:33:30,360 --> 00:33:33,360
we  genereer trainingsgegevens en we gebruiken deze

801
00:33:33,360 --> 00:33:35,760
om de gewichten te leren om de

802
00:33:35,760 --> 00:33:39,480
functies van de structurele Kaza-modellen te leren

803
00:33:39,480 --> 00:33:42,779
en vervolgens genereren we testtestgegevens

804
00:33:42,779 --> 00:33:44,399
voor zowel Interventionele als

805
00:33:44,399 --> 00:33:46,080
counterfaction-query's

806
00:33:46,080 --> 00:33:48,000
en we laten zien of we in staat zijn om te

807
00:33:48,000 --> 00:33:51,360
convergeren naar de juiste testgegevens  met behulp van

808
00:33:51,360 --> 00:33:53,340
creatieve codering

809
00:33:53,340 --> 00:33:54,779


810
00:33:54,779 --> 00:33:57,240
en bijvoorbeeld hier uh in die twee

811
00:33:57,240 --> 00:33:58,860
plots vertegenwoordigen Interventionele

812
00:33:58,860 --> 00:34:00,600
interventie en contrafeitelijke vragen

813
00:34:00,600 --> 00:34:03,539
van deze specifieke grafiek, de

814
00:34:03,539 --> 00:34:05,880
vlinderbiasgrafiek, een grafiek

815
00:34:05,880 --> 00:34:08,280
die vaak wordt gebruikt in uh bij het testen

816
00:34:08,280 --> 00:34:10,859
of een causale gevolgtrekking of

817
00:34:10,859 --> 00:34:12,179
interventie en contrafeitelijke

818
00:34:12,179 --> 00:34:15,540
technieken  het werk is zo simpel, maar

819
00:34:15,540 --> 00:34:18,000
in de krant kun je veel

820
00:34:18,000 --> 00:34:20,760
verschillende grafieken vinden, maar in het algemeen laten die

821
00:34:20,760 --> 00:34:22,800
twee grafieken zien dat de

822
00:34:22,800 --> 00:34:26,940
methode werkt, laat zien dat de

823
00:34:26,940 --> 00:34:27,918


824
00:34:27,918 --> 00:34:32,219
gemiddelde absolute fout tussen de

825
00:34:32,219 --> 00:34:33,960
Interventionele contrafeitelijke grootheden

826
00:34:33,960 --> 00:34:37,399
we we  berekenen en de Interventionele en

827
00:34:37,399 --> 00:34:39,780
contrafeitelijke grootheden van de

828
00:34:39,780 --> 00:34:41,460
oorspronkelijke grafiek

829
00:34:41,460 --> 00:34:43,800
liggen dicht bij elkaar, dus de fout is

830
00:34:43,800 --> 00:34:45,800
vrij klein.

831
00:34:45,800 --> 00:34:49,139


832
00:34:49,139 --> 00:34:51,239


833
00:34:51,239 --> 00:34:54,540


834
00:34:54,540 --> 00:34:56,460


835
00:34:56,460 --> 00:34:59,040
dat ik vorig jaar

836
00:34:59,040 --> 00:35:01,080
in dat artikel schreef. Ik

837
00:35:01,080 --> 00:35:04,200
stel in feite dit soort

838
00:35:04,200 --> 00:35:06,060
netwerken voor als een proof of concept, wat een

839
00:35:06,060 --> 00:35:08,160
volledig verbonden netwerk is, wat over het

840
00:35:08,160 --> 00:35:11,579
algemeen het slechtste neurale netwerk is dat je kunt

841
00:35:11,579 --> 00:35:13,500
hebben om

842
00:35:13,500 --> 00:35:15,960
machine learning-experimenten uit te voeren, omdat

843
00:35:15,960 --> 00:35:20,520
ons een vaste  set neuronen

844
00:35:20,520 --> 00:35:23,660
eigenlijk heb

845
00:35:23,760 --> 00:35:26,400
je elk paar neuronen is

846
00:35:26,400 --> 00:35:28,680
verbonden door twee verschillende synapsen, dus

847
00:35:28,680 --> 00:35:31,200
het is het meest is het model met

848
00:35:31,200 --> 00:35:33,359
de hoogst mogelijke complexiteit in het

849
00:35:33,359 --> 00:35:34,619
algemeen

850
00:35:34,619 --> 00:35:36,300
is het goede dat, aangezien je

851
00:35:36,300 --> 00:35:37,859
veel cycli hebt, het model extreem

852
00:35:37,859 --> 00:35:39,599
flexibel is  in de zin dat je

853
00:35:39,599 --> 00:35:42,480
het bijvoorbeeld kunt trainen op een gehakte afbeelding en

854
00:35:42,480 --> 00:35:45,359
op een gegevenspunt en op het label, maar de

855
00:35:45,359 --> 00:35:47,400
manier waarop je het kunt opvragen dankzij

856
00:35:47,400 --> 00:35:50,640
de informatie die teruggaat, is dat je

857
00:35:50,640 --> 00:35:52,140
op veel verschillende manieren kunt opvragen, dus  je

858
00:35:52,140 --> 00:35:54,060
kunt classificatietaken vormen waarbij

859
00:35:54,060 --> 00:35:55,980
je een afbeelding aanlevert en je voert de

860
00:35:55,980 --> 00:35:57,480
energieminimalisatie uit en krijgt het label

861
00:35:57,480 --> 00:35:59,400
maar je kunt ook bijvoorbeeld

862
00:35:59,400 --> 00:36:01,320
opwekkingstaken uitvoeren waarbij je het

863
00:36:01,320 --> 00:36:03,060
label geeft de energieminimalisatie uitvoeren en

864
00:36:03,060 --> 00:36:05,220
de afbeelding krijgt je kunt

865
00:36:05,220 --> 00:36:06,960
bijvoorbeeld afbeelding uitvoeren  voltooiing die je

866
00:36:06,960 --> 00:36:10,260
de helft van het beeld geeft en convergeert en en

867
00:36:10,260 --> 00:36:12,119
convergeert laat het model converteren naar de

868
00:36:12,119 --> 00:36:14,400
tweede helft enzovoort enzovoort, dus

869
00:36:14,400 --> 00:36:16,440
het is in feite een model dat

870
00:36:16,440 --> 00:36:19,619
de statistieken van de dataset in zijn

871
00:36:19,619 --> 00:36:21,900
geheel leert zonder gefocust te zijn op

872
00:36:21,900 --> 00:36:25,079
classificatie of  generatie in het algemeen,

873
00:36:25,079 --> 00:36:27,900
dus deze flexibiliteit is geweldig,

874
00:36:27,900 --> 00:36:31,260
het probleem is dat hierdoor het

875
00:36:31,260 --> 00:36:34,140
lijkt alsof elke taak niet goed werkt, dus

876
00:36:34,140 --> 00:36:35,820
je kunt veel verschillende dingen doen,

877
00:36:35,820 --> 00:36:38,579
maar geen ervan is goed gedaan

878
00:36:38,579 --> 00:36:39,960
en

879
00:36:39,960 --> 00:36:42,480
hier wil ik laten zien hoe het gebruik

880
00:36:42,480 --> 00:36:44,099
Interventionele vragen in plaats van

881
00:36:44,099 --> 00:36:46,740
standaard uh correlationele vragen of

882
00:36:46,740 --> 00:36:48,119
voorwaardelijke vragen

883
00:36:48,119 --> 00:36:49,980
verbeteren de resultaten van die classificatietaken enigszins,

884
00:36:49,980 --> 00:36:51,960


885
00:36:51,960 --> 00:36:54,000
dus wat zijn de conjectieve redenen

886
00:36:54,000 --> 00:36:57,599
hiervan, uh, de testnauwkeurigheid van

887
00:36:57,599 --> 00:37:01,079
die taken is niet zo hoog, de

888
00:37:01,079 --> 00:37:03,180
eerste twee redenen zijn dat het model

889
00:37:03,180 --> 00:37:05,640
wordt afgeleid  bij het corrigeren van elke

890
00:37:05,640 --> 00:37:07,920
uh elke afzonderlijke fout, dus eigenlijk

891
00:37:07,920 --> 00:37:09,420
presenteer je een afbeelding en je zou graag

892
00:37:09,420 --> 00:37:11,579
een label willen krijgen, maar het model werkt

893
00:37:11,579 --> 00:37:13,859
zichzelf eigenlijk bij om ook de

894
00:37:13,859 --> 00:37:16,320
fout in de afbeeldingen te voorspellen

895
00:37:16,320 --> 00:37:18,480
en de tweede reden die ik

896
00:37:18,480 --> 00:37:21,119
zei, is dat de  structuur is veel te

897
00:37:21,119 --> 00:37:24,540
complex, dus nogmaals vanuit een argumentatie van

898
00:37:24,540 --> 00:37:27,079
Occam scheermes

899
00:37:27,079 --> 00:37:28,800


900
00:37:28,800 --> 00:37:30,720
is dit het slechtste model dat je kunt hebben, dus

901
00:37:30,720 --> 00:37:32,160
elke keer dat je een model hebt dat past bij een

902
00:37:32,160 --> 00:37:33,960
dataset, zal dat model minder

903
00:37:33,960 --> 00:37:35,579
complex zijn dan dit model dat gaat  de

904
00:37:35,579 --> 00:37:37,560
voorkeur hebben,

905
00:37:37,560 --> 00:37:40,560
maar in het algemeen uh gewoon om het gewoon te bestuderen,

906
00:37:40,560 --> 00:37:41,400


907
00:37:41,400 --> 00:37:43,380
het idee is dat in dit model de

908
00:37:43,380 --> 00:37:44,820
interventies kunnen worden opgevraagd om de

909
00:37:44,820 --> 00:37:46,859
prestaties van uh van die volledig

910
00:37:46,859 --> 00:37:48,599
verbonden modellen te verbeteren. Nou,

911
00:37:48,599 --> 00:37:51,060
het antwoord is ja,

912
00:37:51,060 --> 00:37:53,160
dus hier is hoe ik interventie-

913
00:37:53,160 --> 00:37:55,619
query's uitvoer  Ik presenteer een afbeelding aan het

914
00:37:55,619 --> 00:37:56,640
netwerk.

915
00:37:56,640 --> 00:37:59,460
Ik repareer de fout van de pixels

916
00:37:59,460 --> 00:38:01,560
zodat deze gelijk is aan nul, zodat deze fout zich niet

917
00:38:01,560 --> 00:38:03,180
verspreidt in het netwerk.

918
00:38:03,180 --> 00:38:05,700
Vervolgens bereken ik het label

919
00:38:05,700 --> 00:38:08,400
en zoals je kunt zien, verbetert de nauwkeurigheid

920
00:38:08,400 --> 00:38:11,339
bijvoorbeeld vanaf 89 met behulp van de  standaard

921
00:38:11,339 --> 00:38:13,380
querymethode van creatieve coderingsnetwerken

922
00:38:13,380 --> 00:38:16,800
tot 92, wat de nauwkeurigheid is na

923
00:38:16,800 --> 00:38:19,020
interventie en hetzelfde gebeurt

924
00:38:19,020 --> 00:38:21,540
voor modemiddelen

925
00:38:21,540 --> 00:38:24,420
en ik denk dat een zeer legitieme criticus

926
00:38:24,420 --> 00:38:26,940
die waarschijnlijk iedereen zou denken bij het

927
00:38:26,940 --> 00:38:28,920
zien van die plots is dat oké, je

928
00:38:28,920 --> 00:38:32,099
verbetert de middelen van 89  tot 92 is eigenlijk

929
00:38:32,099 --> 00:38:36,180
nog steeds waardeloos en ja, het is waar

930
00:38:36,180 --> 00:38:38,400
en ik ben eigenlijk in de latere dia's. Ik

931
00:38:38,400 --> 00:38:40,619
ga laten zien hoe te handelen op de

932
00:38:40,619 --> 00:38:42,660
structuur van dit uh van dit volledig

933
00:38:42,660 --> 00:38:43,859
verbonden model

934
00:38:43,859 --> 00:38:46,500
zal de resultaten nog meer verbeteren tot

935
00:38:46,500 --> 00:38:48,480
het punt dat ze de  rijke uh

936
00:38:48,480 --> 00:38:50,820
prestatie die natuurlijk niet eens in de buurt komt van

937
00:38:50,820 --> 00:38:52,560
de allernieuwste uitvoering,

938
00:38:52,560 --> 00:38:55,320
maar het is nog steeds op peil maar niet op een

939
00:38:55,320 --> 00:38:57,380
niveau dat in wezen acceptabel wordt

940
00:38:57,380 --> 00:39:01,760
Kenworth onderzoek onderzoekt

941
00:39:02,040 --> 00:39:04,980
dus ja dus dit was het deel over

942
00:39:04,980 --> 00:39:08,400
causale gevolgtrekking met behulp van creatieve codering

943
00:39:08,400 --> 00:39:10,920
en ik denk dat ik samenvattend kan zeggen dat

944
00:39:10,920 --> 00:39:15,060
uh het interessante deel van deze uh van

945
00:39:15,060 --> 00:39:17,640
de resultaten die ik zojuist heb laten zien, is

946
00:39:17,640 --> 00:39:19,859
dat ik heb laten zien dat operatieve codering in staat is

947
00:39:19,859 --> 00:39:22,560
om interventies op een zeer gemakkelijke

948
00:39:22,560 --> 00:39:24,780
en intuïtieve manier uit te voeren, omdat je niet hoeft te

949
00:39:24,780 --> 00:39:26,280
handelen naar  de structuur van de oude grafiek

950
00:39:26,280 --> 00:39:28,740
niet meer, soms

951
00:39:28,740 --> 00:39:31,079
zijn die functiefuncties niet beschikbaar, enzovoort,

952
00:39:31,079 --> 00:39:34,020
enzovoort, maar je hoeft alleen maar in te

953
00:39:34,020 --> 00:39:36,140


954
00:39:36,140 --> 00:39:39,780
grijpen op een enkel neuron, bestudeert de

955
00:39:39,780 --> 00:39:41,640
voorspellingsfout tot nul

956
00:39:41,640 --> 00:39:44,220
en voert een energieminimalisatieproces uit

957
00:39:44,220 --> 00:39:46,619


958
00:39:46,619 --> 00:39:49,200
en deze uitgebreide zijn  stelde ons in staat om op

959
00:39:49,200 --> 00:39:51,240
creatieve codering gebaseerde structurele

960
00:39:51,240 --> 00:39:52,920
causale modellen te definiëren.

961
00:39:52,920 --> 00:39:54,920
Nu gaan we naar het tweede

962
00:39:54,920 --> 00:39:57,900
deel van het werk dat gaat over het

963
00:39:57,900 --> 00:40:01,700
leren van structurele structuren,

964
00:40:02,000 --> 00:40:05,099
dus leren van instructies, zoals ik al zei.

965
00:40:05,099 --> 00:40:07,260
Deals Behandelt het probleem van het leren van de

966
00:40:07,260 --> 00:40:09,720
causale structuur van het model

967
00:40:09,720 --> 00:40:11,880
uit observatiegegevens.

968
00:40:11,880 --> 00:40:13,800
eigenlijk geen probleem dat al

969
00:40:13,800 --> 00:40:17,760
tientallen jaren bestaat

970
00:40:17,760 --> 00:40:21,359
en altijd is geweest tot een paar

971
00:40:21,359 --> 00:40:24,000
jaar geleden, aangepakt met behulp van combinatorische

972
00:40:24,000 --> 00:40:25,560
zoekmethoden.

973
00:40:25,560 --> 00:40:26,640


974
00:40:26,640 --> 00:40:29,280


975
00:40:29,280 --> 00:40:32,880


976
00:40:32,880 --> 00:40:34,740


977
00:40:34,740 --> 00:40:36,780
dimensionaal en

978
00:40:36,780 --> 00:40:39,920
de Bison-grafiek die je wilt leren

979
00:40:39,920 --> 00:40:42,300
groeit in omvang om het te

980
00:40:42,300 --> 00:40:46,680
leren het is ongelooflijk traag

981
00:40:46,680 --> 00:40:48,780
de nieuwe oplossing die

982
00:40:48,780 --> 00:40:51,000
een paar jaar geleden uitkwam in een nieuwe krant

983
00:40:51,000 --> 00:40:53,540
uit 2018

984
00:40:53,839 --> 00:40:55,920
laat zien dat het mogelijk is om

985
00:40:55,920 --> 00:40:57,900
deze structuur daadwerkelijk te leren zonder een

986
00:40:57,900 --> 00:40:59,940
combinator-onderzoeksmethode, maar door

987
00:40:59,940 --> 00:41:01,619
een op gradiënt gebaseerde methode te gebruiken

988
00:41:01,619 --> 00:41:05,280
en dit was in feite dit bekwame

989
00:41:05,280 --> 00:41:07,320
probleem in het algemeen, want nu

990
00:41:07,320 --> 00:41:08,820
kun je gewoon

991
00:41:08,820 --> 00:41:10,980
een prior hebben over de parameters, wat

992
00:41:10,980 --> 00:41:12,420
het belangrijkste doel is dat ik

993
00:41:12,420 --> 00:41:14,700
een beetje beter ga definiëren  in deze

994
00:41:14,700 --> 00:41:15,599
dia

995
00:41:15,599 --> 00:41:18,180
loop je een gradiënt afdaling en zelfs als je

996
00:41:18,180 --> 00:41:19,740
een model hebt dat dubbel drievoudig is,

997
00:41:19,740 --> 00:41:20,820


998
00:41:20,820 --> 00:41:23,640
is de grootte eh, het algoritme is nog steeds ongelooflijk

999
00:41:23,640 --> 00:41:25,440
snel

1000
00:41:25,440 --> 00:41:28,260
en daarom is dit papier een

1001
00:41:28,260 --> 00:41:31,200
dit is ja, ik denk dat het een beetje nieuw is

1002
00:41:31,200 --> 00:41:33,180
en ik denk dat het er al is  600

1003
00:41:33,180 --> 00:41:35,099
citaten of zoiets of dat soort dingen

1004
00:41:35,099 --> 00:41:37,140
en elk artikel dat ik nu zie

1005
00:41:37,140 --> 00:41:38,720
over het adviseren van vrienden en het leren van de

1006
00:41:38,720 --> 00:41:42,000
structuur van de grafiek gebruikt hun methode,

1007
00:41:42,000 --> 00:41:44,820
het verandert gewoon een klein beetje,

1008
00:41:44,820 --> 00:41:46,980
ze vinden snellere of iets betere

1009
00:41:46,980 --> 00:41:49,440
inferentiemethoden, maar toch gebruiken ze allemaal

1010
00:41:49,440 --> 00:41:53,760
de  voordat dit artikel werd gedefinieerd en ik

1011
00:41:53,760 --> 00:41:56,460
doe het ook en wij doen het ook,

1012
00:41:56,460 --> 00:41:58,859
dus hier zullen we een nieuwe hoeveelheid vinden

1013
00:41:58,859 --> 00:42:01,500
die de matrix van het bureau is, de

1014
00:42:01,500 --> 00:42:03,480
matrix van het bureau is gewoon een matrix die

1015
00:42:03,480 --> 00:42:06,359
de verbindingen van het model codeert, dus het is een

1016
00:42:06,359 --> 00:42:08,520
binaire matrix en

1017
00:42:08,520 --> 00:42:10,920
in het algemeen  is een binaire matrix, dan maak je het

1018
00:42:10,920 --> 00:42:12,180
natuurlijk continu als je op gradiënt gebaseerde optimalisatie uitvoert

1019
00:42:12,180 --> 00:42:14,880


1020
00:42:14,880 --> 00:42:16,800
en dan heb je op een gegeven moment een drempel

1021
00:42:16,800 --> 00:42:19,800
die in feite een rand doodt of

1022
00:42:19,800 --> 00:42:21,480
op één zet

1023
00:42:21,480 --> 00:42:27,780
en de M3 IJ is gelijk aan één als we

1024
00:42:27,780 --> 00:42:30,540
hebben  als de Bayesiaanse grafiek een rand is

1025
00:42:30,540 --> 00:42:35,040
van hoekpunt I naar hoekpunt J of nul

1026
00:42:35,040 --> 00:42:37,380
anders, dus bijvoorbeeld dit bureau

1027
00:42:37,380 --> 00:42:39,540
Matrix vertegenwoordigt hier de

1028
00:42:39,540 --> 00:42:42,780
connectiviteitsstructuur van dit visuele netwerk

1029
00:42:42,780 --> 00:42:44,040
en

1030
00:42:44,040 --> 00:42:46,079
in feite pakt deze methode

1031
00:42:46,079 --> 00:42:48,780
twee problemen aan die we willen

1032
00:42:48,780 --> 00:42:51,000
over deze uh over het leren van de

1033
00:42:51,000 --> 00:42:53,460
structuur  van de vergelijking Netwerk het

1034
00:42:53,460 --> 00:42:54,780
idee is dat we uitgaan van een volledig

1035
00:42:54,780 --> 00:42:57,200
verbonden model dat

1036
00:42:57,200 --> 00:43:00,240
conceptueel is uh is vergelijkbaar eigenlijk

1037
00:43:00,240 --> 00:43:02,220
is gelijk aan de operatieve codering

1038
00:43:02,220 --> 00:43:04,020
Netwerk dat ik eerder heb gedefinieerd en dat volledig is

1039
00:43:04,020 --> 00:43:06,480
verbonden, dus je hebt veel

1040
00:43:06,480 --> 00:43:08,640
hoekpunten en elk paar hoekpunten  is

1041
00:43:08,640 --> 00:43:10,920
verbonden door uh door twee verschillende randen

1042
00:43:10,920 --> 00:43:13,319
en je wilt gewoon degene snoeien

1043
00:43:13,319 --> 00:43:15,780
die niet nodig zijn,

1044
00:43:15,780 --> 00:43:18,540
dus het kan worden gezien als een methode die

1045
00:43:18,540 --> 00:43:20,819
modelreductie uitvoert, je begint met

1046
00:43:20,819 --> 00:43:22,020
een groot model en je wilt het klein maken,

1047
00:43:22,020 --> 00:43:22,800


1048
00:43:22,800 --> 00:43:25,800
dus wat is  het eerste ingrediënt om

1049
00:43:25,800 --> 00:43:28,260
modellen goed te verkleinen is natuurlijk sparse

1050
00:43:28,260 --> 00:43:29,220
City

1051
00:43:29,220 --> 00:43:31,619
en wat is de prior die iedereen gebruikt

1052
00:43:31,619 --> 00:43:33,839
om een ​​model schaarser te maken, is de

1053
00:43:33,839 --> 00:43:36,480
LaPlace-prioriteit die in machine learning

1054
00:43:36,480 --> 00:43:38,880
eenvoudigweg bekend staat als de L1-norm

1055
00:43:38,880 --> 00:43:40,920
die hier wordt gedefinieerd als

1056
00:43:40,920 --> 00:43:43,980
de oplossing die de  dit artikel dat ik

1057
00:43:43,980 --> 00:43:46,740
eerder noemde, stelde voor om de

1058
00:43:46,740 --> 00:43:49,319
tweede prior bovenaan toe te voegen, wat afdwingt

1059
00:43:49,319 --> 00:43:53,359
wat waarschijnlijk het grootste uh-

1060
00:43:53,359 --> 00:43:55,980
kenmerk is van Bayesiaanse netwerken

1061
00:43:55,980 --> 00:43:57,780
waarop je causale gevolgtrekkingen wilt uitvoeren,

1062
00:43:57,780 --> 00:43:59,819
is dat je wilt dat ze cyclisch zijn

1063
00:43:59,819 --> 00:44:01,020


1064
00:44:01,020 --> 00:44:03,000
en in feite toonden ze  dat

1065
00:44:03,000 --> 00:44:06,359
acycliciteit kan worden opgelegd aan een bureau

1066
00:44:06,359 --> 00:44:08,160
Matrix als een prior

1067
00:44:08,160 --> 00:44:10,859
en het heeft dit de deze vorm hier dus

1068
00:44:10,859 --> 00:44:14,640
het is het spoor van de Matrix dat is de

1069
00:44:14,640 --> 00:44:18,420
uh de exponentiële van a keer a

1070
00:44:18,420 --> 00:44:21,859
waarbij a weer het bureau Matrix is ​​en

1071
00:44:21,859 --> 00:44:24,300
eigenlijk is deze hoeveelheid hier

1072
00:44:24,300 --> 00:44:27,900
is gelijk aan nul als en alleen als het

1073
00:44:27,900 --> 00:44:30,480
Bayesiaanse netwerk of de

1074
00:44:30,480 --> 00:44:32,819
of de grafiek die je overweegt

1075
00:44:32,819 --> 00:44:35,720
een c-klik is,

1076
00:44:37,619 --> 00:44:40,260
dus ik ga deze gebruiken in uh in sommige

1077
00:44:40,260 --> 00:44:42,960
experimenten, zodat die twee in Forceer

1078
00:44:42,960 --> 00:44:45,660
die twee prioren op de

1079
00:44:45,660 --> 00:44:47,520
op verschillende soorten  van patiëntennetwerken

1080
00:44:47,520 --> 00:44:49,200
en ik probeer ze samen te voegen met de

1081
00:44:49,200 --> 00:44:51,540
technieken die we eerder hebben voorgesteld over het

1082
00:44:51,540 --> 00:44:52,740
uitvoeren van causale gevolgtrekkingen de

1083
00:44:52,740 --> 00:44:55,020
operatieve codering,

1084
00:44:55,020 --> 00:44:56,520
dus ik ga twee verschillende

1085
00:44:56,520 --> 00:44:59,640
experimenten presenteren, dus één is een proof of

1086
00:44:59,640 --> 00:45:00,960
concept, wat de standaardexperimenten zijn die worden

1087
00:45:00,960 --> 00:45:03,660
getoond in  alle structurele

1088
00:45:03,660 --> 00:45:06,599
leertaken die de gevolgtrekking zijn van

1089
00:45:06,599 --> 00:45:08,880
het juiste Bayesiaanse netwerk uit gegevens

1090
00:45:08,880 --> 00:45:11,760
en dan ga ik voortbouwen op

1091
00:45:11,760 --> 00:45:13,500
de classificatie-experimenten die ik eerder heb laten zien

1092
00:45:13,500 --> 00:45:14,280


1093
00:45:14,280 --> 00:45:16,020


1094
00:45:16,020 --> 00:45:18,540
en laten zien hoe die priors

1095
00:45:18,540 --> 00:45:21,060
ons in staat stellen de classificatie te verbeteren

1096
00:45:21,060 --> 00:45:22,500
nauwkeurigheid de

1097
00:45:22,500 --> 00:45:25,500
testnauwkeurigheid van volledig verbonden

1098
00:45:25,500 --> 00:45:28,160
modellen voor voorspellende codering,

1099
00:45:29,520 --> 00:45:31,680
dus laten we naar het eerste experiment gaan,

1100
00:45:31,680 --> 00:45:33,300
namelijk het afleiden van de structuur van de

1101
00:45:33,300 --> 00:45:34,980
grafiek

1102
00:45:34,980 --> 00:45:37,319
en alle experimenten, ze volgen allemaal

1103
00:45:37,319 --> 00:45:39,480
in wezen dezelfde pijplijn in alle

1104
00:45:39,480 --> 00:45:42,060
artikelen in het veld de eerste stap is het

1105
00:45:42,060 --> 00:45:45,119
genereren  een visienetwerk van een willekeurige

1106
00:45:45,119 --> 00:45:46,079
grafiek,

1107
00:45:46,079 --> 00:45:48,359
dus normaal gesproken zijn de twee willekeurige

1108
00:45:48,359 --> 00:45:50,640
grafieken die iedereen test Erdos-

1109
00:45:50,640 --> 00:45:53,520
regrafieën en een schaalvrije grafiek,

1110
00:45:53,520 --> 00:45:55,859
dus je genereert die grote grafieken

1111
00:45:55,859 --> 00:45:58,680
die normaal gesproken 20 hebben voor de 80 80

1112
00:45:58,680 --> 00:46:01,619
verschillende knooppunten en enkele randen

1113
00:46:01,619 --> 00:46:04,619
die je willekeurig bemonstert

1114
00:46:04,619 --> 00:46:06,540
en  je gebruikt deze grafiek om een ​​dataset te genereren,

1115
00:46:06,540 --> 00:46:08,280


1116
00:46:08,280 --> 00:46:10,819
dus je bemonstert bijvoorbeeld

1117
00:46:10,819 --> 00:46:14,460
n Big N-datapunten en wat je doet is

1118
00:46:14,460 --> 00:46:16,859
dat je de grafiek neemt die je

1119
00:46:16,859 --> 00:46:18,780
eerder hebt gegenereerd en die je

1120
00:46:18,780 --> 00:46:20,819
weggooit je behoudt alleen de dataset

1121
00:46:20,819 --> 00:46:23,099
en de taak die je  nu willen oplossen is

1122
00:46:23,099 --> 00:46:25,020
leren

1123
00:46:25,020 --> 00:46:27,420
is een trainingsalgoritme hebben waarmee

1124
00:46:27,420 --> 00:46:29,819
je in feite

1125
00:46:29,819 --> 00:46:32,579
de structuur van de

1126
00:46:32,579 --> 00:46:34,619
grafiek die je hebt weggegooid kunt terughalen,

1127
00:46:34,619 --> 00:46:36,839
dus de manier waarop we het hier doen, is dat we ons

1128
00:46:36,839 --> 00:46:38,460
in een volledig verbonden creatieve codering bevinden

1129
00:46:38,460 --> 00:46:41,760
modelleer op deze dataset D met behulp van zowel de

1130
00:46:41,760 --> 00:46:43,800
schaarse als de SQL-prioriteit die we

1131
00:46:43,800 --> 00:46:45,359
eerder hebben gedefinieerd

1132
00:46:45,359 --> 00:46:48,780
en kijk of de

1133
00:46:48,780 --> 00:46:50,760
grafiek waarnaar we convergeren na het

1134
00:46:50,760 --> 00:46:53,220
wegsnoeien van de

1135
00:46:53,220 --> 00:46:55,319
ingangen van de matrix van het bureau die

1136
00:46:55,319 --> 00:46:57,599
kleiner zijn dan een bepaalde drempel,

1137
00:46:57,599 --> 00:47:00,060
vergelijkbaar is met  die van de begingrafiek

1138
00:47:00,060 --> 00:47:02,359


1139
00:47:02,520 --> 00:47:04,500
en er is ook te zien dat dit

1140
00:47:04,500 --> 00:47:06,599
werkelijk het geval is, dus dit is een voorbeeld

1141
00:47:06,599 --> 00:47:09,020
en ik laat veel verschillende

1142
00:47:09,020 --> 00:47:12,420
parametrisering en dimensies en

1143
00:47:12,420 --> 00:47:15,060
dergelijke zien in de paper,

1144
00:47:15,060 --> 00:47:16,920
maar ik denk dat die twee de meest

1145
00:47:16,920 --> 00:47:18,900
representatieve voorbeelden zijn  met een

1146
00:47:18,900 --> 00:47:20,760
foutenkinderkamergrafiek en een vrije schaalgrafiek

1147
00:47:20,760 --> 00:47:23,579
met 20 knooppunten

1148
00:47:23,579 --> 00:47:25,800
en hier aan de linkerkant kun je de

1149
00:47:25,800 --> 00:47:27,300
grond zien door middel van een grafiek die willekeurig is

1150
00:47:27,300 --> 00:47:29,339
bemonsterd

1151
00:47:29,339 --> 00:47:30,839


1152
00:47:30,839 --> 00:47:32,599
en aan de rechterkant kun je de grafiek zien

1153
00:47:32,599 --> 00:47:35,220
het mooie moeilijkheidsmodel zoals geleerd

1154
00:47:35,220 --> 00:47:37,440
uit de gegevens  ingesteld

1155
00:47:37,440 --> 00:47:39,359
en zoals je kunt zien, lijken ze behoorlijk op

1156
00:47:39,359 --> 00:47:40,500
elkaar,

1157
00:47:40,500 --> 00:47:42,780
het is nog steeds niet perfect, dus

1158
00:47:42,780 --> 00:47:45,000
er zijn enkele fouten, maar

1159
00:47:45,000 --> 00:47:47,460
over het algemeen is de structuur dat

1160
00:47:47,460 --> 00:47:49,500
ze vrij goed werken, we hebben ook enkele

1161
00:47:49,500 --> 00:47:52,140
kwantitatieve experimenten die we

1162
00:47:52,140 --> 00:47:54,000
hier niet laten zien  omdat het

1163
00:47:54,000 --> 00:47:55,740
gewoon enorme tabellen zijn met veel getallen

1164
00:47:55,740 --> 00:47:57,180
en ik dacht dat het misschien een beetje

1165
00:47:57,180 --> 00:48:00,660
te veel was voor de presentatie, maar

1166
00:48:00,660 --> 00:48:02,220
de resultaten laten zien dat ze

1167
00:48:02,220 --> 00:48:06,060
vergelijkbaar zijn met hedendaagse methoden,

1168
00:48:06,060 --> 00:48:07,920
ook omdat ik moet zeggen dat

1169
00:48:07,920 --> 00:48:10,859
de meeste kwaliteit  komt van de acigli

1170
00:48:10,859 --> 00:48:15,799
prior die in 2018 werd geïntroduceerd.

1171
00:48:16,920 --> 00:48:19,680
de tweede klasse van experimenten zijn de

1172
00:48:19,680 --> 00:48:21,599
onze classificatie-experimenten die, zoals

1173
00:48:21,599 --> 00:48:23,880
ik al zei, de uitbreidingen zijn van degene die

1174
00:48:23,880 --> 00:48:25,560
ik eerder deelde

1175
00:48:25,560 --> 00:48:27,119
en het idee is om structuurleren te gebruiken

1176
00:48:27,119 --> 00:48:28,560
om de classificatie te verbeteren

1177
00:48:28,560 --> 00:48:31,140
over de classificatieresultaten over de

1178
00:48:31,140 --> 00:48:33,420
middelen en mode middelen dataset

1179
00:48:33,420 --> 00:48:36,780
beginnend met een volledig verbonden grafiek,

1180
00:48:36,780 --> 00:48:40,560
dus wat ik deed is dat ik de

1181
00:48:40,560 --> 00:48:42,839
volledig verbonden grafische clusters van

1182
00:48:42,839 --> 00:48:46,440
neuronen verdeelde, dus 1B cluster

1183
00:48:46,440 --> 00:48:49,140
is degene die gerelateerd is aan de invoer

1184
00:48:49,140 --> 00:48:51,900
en al het kleine dan  we hebben een

1185
00:48:51,900 --> 00:48:55,319
bepaald aantal verborgen clusters

1186
00:48:55,319 --> 00:48:57,720
en dan hebben we het labelcluster, dat

1187
00:48:57,720 --> 00:48:58,800
is

1188
00:48:58,800 --> 00:49:01,560
de klasse het cluster van neuronen die

1189
00:49:01,560 --> 00:49:04,079
me de labelvoorspellingen zouden moeten geven

1190
00:49:04,079 --> 00:49:06,480


1191
00:49:06,480 --> 00:49:08,700
en ik heb ze getraind om

1192
00:49:08,700 --> 00:49:10,980
de eerste keer de schaarse voorafgaande te gebruiken  alleen dus

1193
00:49:10,980 --> 00:49:14,099
het idee is wat als ik, als ik de

1194
00:49:14,099 --> 00:49:16,500
verbindingen die ik niet nodig heb van een

1195
00:49:16,500 --> 00:49:17,460
model snoei

1196
00:49:17,460 --> 00:49:20,880
en leer als parsermodel,

1197
00:49:20,880 --> 00:49:24,119
dit goed werkt, het antwoord is nee, het

1198
00:49:24,119 --> 00:49:25,500
werkt niet en de reden en de

1199
00:49:25,500 --> 00:49:28,500
reden waarom is  dat je aan het einde de

1200
00:49:28,500 --> 00:49:30,660
grafiek waarmee je convergeert eigenlijk

1201
00:49:30,660 --> 00:49:32,700
de generator is, dus in feite leert het model

1202
00:49:32,700 --> 00:49:36,180
het label te voorspellen op basis van het

1203
00:49:36,180 --> 00:49:38,400
label zelf, zodat het alle

1204
00:49:38,400 --> 00:49:40,020
informatie van de invoer weggooit

1205
00:49:40,020 --> 00:49:42,480
en alleen het label behoudt en zoals je

1206
00:49:42,480 --> 00:49:45,119
hier kunt zien de  label y voorspelt zichzelf of

1207
00:49:45,119 --> 00:49:46,560
in andere experimenten wanneer je de

1208
00:49:46,560 --> 00:49:48,960
parameters verandert die je hebt dat y voorspelt op

1209
00:49:48,960 --> 00:49:52,520
nul die preex X1 de voorspelt y opnieuw

1210
00:49:52,520 --> 00:49:55,980
dus wat is de wat is de oplossing voor

1211
00:49:55,980 --> 00:49:57,240
dit probleem nou, de oplossing voor dit

1212
00:49:57,240 --> 00:49:59,520
probleem is dat we moeten

1213
00:49:59,520 --> 00:50:03,000
convergeren  naar een acyclische grafiek

1214
00:50:03,000 --> 00:50:05,220
en dus moeten we iets toevoegen dat

1215
00:50:05,220 --> 00:50:08,000
een cycliciteit voorkomt en wat is dat

1216
00:50:08,000 --> 00:50:10,200
natuurlijk degene die ik al heb

1217
00:50:10,200 --> 00:50:12,780
voorgesteld en dan laat ik een tweede techniek zien,

1218
00:50:12,780 --> 00:50:14,520


1219
00:50:14,520 --> 00:50:17,280
dus de eerste gebruikt de eerder gedefinieerde SQL

1220
00:50:17,280 --> 00:50:18,680


1221
00:50:18,680 --> 00:50:21,359
en de tweede is  a is een nieuwe

1222
00:50:21,359 --> 00:50:22,859
techniek die eigenlijk gebruik maakt van

1223
00:50:22,859 --> 00:50:24,359
negatieve voorbeelden,

1224
00:50:24,359 --> 00:50:26,520
dus een negatief een negatief voorbeeld in dit

1225
00:50:26,520 --> 00:50:30,060
geval is gewoon uh het datapunt

1226
00:50:30,060 --> 00:50:32,280
waarin je een afbeelding hebt maar het label is

1227
00:50:32,280 --> 00:50:33,240
verkeerd,

1228
00:50:33,240 --> 00:50:35,220
dus hier heb je bijvoorbeeld een afbeelding van

1229
00:50:35,220 --> 00:50:36,900
een zeven  maar het label dat ik

1230
00:50:36,900 --> 00:50:39,599
het model geef is een twee

1231
00:50:39,599 --> 00:50:40,980


1232
00:50:40,980 --> 00:50:44,579
en het idee is heel eenvoudig, het is

1233
00:50:44,579 --> 00:50:47,460
al in veel uh Works gebruikt,

1234
00:50:47,460 --> 00:50:49,740
dus elke keer dat het model een positief

1235
00:50:49,740 --> 00:50:52,079
voorbeeld is, moet het worden verhoogd om

1236
00:50:52,079 --> 00:50:53,520
de variatie te minimaliseren  van vrije energie

1237
00:50:53,520 --> 00:50:56,520
en elke keer dat het een heeft, is het een negatief

1238
00:50:56,520 --> 00:50:58,859
voorbeeld, het moet het verhogen,

1239
00:50:58,859 --> 00:51:01,260
dus laat me verder gaan met de fout deze

1240
00:51:01,260 --> 00:51:04,200
hoeveelheid die moet worden geminimaliseerd

1241
00:51:04,200 --> 00:51:05,960
vreemd

1242
00:51:05,960 --> 00:51:08,579
met veel experimenten en veel

1243
00:51:08,579 --> 00:51:10,859
experimenten, we zagen dat de

1244
00:51:10,859 --> 00:51:12,119
twee technieken

1245
00:51:12,119 --> 00:51:15,000
in feite leidt de eerste tot dezelfde

1246
00:51:15,000 --> 00:51:17,220
resultaten en de tweede leidt ook tot dezelfde

1247
00:51:17,220 --> 00:51:18,599
grafiek,

1248
00:51:18,599 --> 00:51:21,000
dus hier zijn de

1249
00:51:21,000 --> 00:51:22,800
hier zijn de nieuwe resultaten sommige middelen en

1250
00:51:22,800 --> 00:51:25,079
modemiddelen met behulp van de twee technieken

1251
00:51:25,079 --> 00:51:27,660
die ik zojuist heb voorgesteld

1252
00:51:27,660 --> 00:51:30,960
en en nu gaan we naar een aantal die  zijn

1253
00:51:30,960 --> 00:51:33,900
nog steeds niet geweldig, maar zeker

1254
00:51:33,900 --> 00:51:36,000
redelijkere testnauwkeurigheden, dus hier

1255
00:51:36,000 --> 00:51:39,059
hebben we een testfout van 3,17 voor minuten en een

1256
00:51:39,059 --> 00:51:42,119
testfout van 13,98 voor modemiddelen

1257
00:51:42,119 --> 00:51:44,819
en eigenlijk kunnen die resultaten

1258
00:51:44,819 --> 00:51:48,300
veel worden verbeterd door de

1259
00:51:48,300 --> 00:51:51,300
structuur van de grafiek te leren van  fijngehakt

1260
00:51:51,300 --> 00:51:53,040
en vervolgens de structuur van de

1261
00:51:53,040 --> 00:51:55,319
grafiek fixeren en een vorm van fijnafstelling uitvoeren,

1262
00:51:55,319 --> 00:51:57,660
dus als je het model op een gegeven moment nauwkeurig afstelt op

1263
00:51:57,660 --> 00:52:00,000
de juiste hiërarchische structuur,

1264
00:52:00,000 --> 00:52:01,980
bereik je de testnauwkeurigheid

1265
00:52:01,980 --> 00:52:03,359
die je zou verwachten van een

1266
00:52:03,359 --> 00:52:05,460
hiërarchisch model, maar die  degenen zijn

1267
00:52:05,460 --> 00:52:08,099
gewoon degene die het volledig verbonden model van

1268
00:52:08,099 --> 00:52:10,980
nature convergeert,

1269
00:52:10,980 --> 00:52:13,859
dus bijvoorbeeld van een testfout van

1270
00:52:13,859 --> 00:52:15,420
18,32

1271
00:52:15,420 --> 00:52:17,339
van het volledig verbonden model trein op

1272
00:52:17,339 --> 00:52:20,359
mode betekent door simpelweg

1273
00:52:20,359 --> 00:52:22,859
correlaties of voorwaardelijke zoekopdrachten uit te voeren,

1274
00:52:22,859 --> 00:52:24,420
wat een standaardmanier is om

1275
00:52:24,420 --> 00:52:26,520
operatieve coderingsmodellen op te vragen

1276
00:52:26,520 --> 00:52:29,220
interventies en de AC-klik

1277
00:52:29,220 --> 00:52:32,040
voorafgaand samen maakt deze

1278
00:52:32,040 --> 00:52:34,200
testfout veel lager

1279
00:52:34,200 --> 00:52:37,200
en we kunnen het ook voor de middelen observeren.

1280
00:52:37,200 --> 00:52:39,319


1281
00:52:39,780 --> 00:52:41,819
Ik ga niet een beetje in op details

1282
00:52:41,819 --> 00:52:45,420
over dit laatste experiment en hoe

1283
00:52:45,420 --> 00:52:48,660
de acyclische prior werkt op de  structuur

1284
00:52:48,660 --> 00:52:50,339
van de grafiek

1285
00:52:50,339 --> 00:52:52,440
dus ik voer een experiment uit op

1286
00:52:52,440 --> 00:52:54,960
uh op een nieuwe dataset, wat betekent dat ik

1287
00:52:54,960 --> 00:52:56,460
de nieuwe dataset binnenhaal, het kan

1288
00:52:56,460 --> 00:52:58,500
te veel zijn, ik noem het een dataset met twee middelen

1289
00:52:58,500 --> 00:53:01,440
waarin je het invoerpunt hebt

1290
00:53:01,440 --> 00:53:04,319
bestaat uit twee verschillende

1291
00:53:04,319 --> 00:53:07,319
afbeeldingen en het label hangt alleen af ​​van de

1292
00:53:07,319 --> 00:53:08,520
tweede afbeelding

1293
00:53:08,520 --> 00:53:10,800
op het eerste afbeeldingsverhaal,

1294
00:53:10,800 --> 00:53:12,720
dus het idee hier

1295
00:53:12,720 --> 00:53:15,079
is de structuur van het model, de

1296
00:53:15,079 --> 00:53:18,540
voorafgaande cycliciteit en dergelijke,

1297
00:53:18,540 --> 00:53:20,819
in staat om te herkennen dat de tweede helft

1298
00:53:20,819 --> 00:53:23,400
van de afbeelding is  eigenlijk zinloos in

1299
00:53:23,400 --> 00:53:27,960
uh bij het leren bij het leren van de bij het

1300
00:53:27,960 --> 00:53:31,140
uitvoeren van classificatie

1301
00:53:31,140 --> 00:53:33,119
hoe gedraagt ​​training zich in het algemeen, zoals

1302
00:53:33,119 --> 00:53:36,480
bijvoorbeeld we hebben dit uh invoer

1303
00:53:36,480 --> 00:53:39,000
invoerknooppunt uitvoerknooppunt en alleen de knooppunten zijn

1304
00:53:39,000 --> 00:53:41,940
volledig verbonden en het model

1305
00:53:41,940 --> 00:53:43,740
convergeert naar een

1306
00:53:43,740 --> 00:53:45,900
naar een hiërarchische structuur die de

1307
00:53:45,900 --> 00:53:48,960
een waarvan we weten dat hij het beste presteert bij

1308
00:53:48,960 --> 00:53:50,880
classificatietaken,

1309
00:53:50,880 --> 00:53:53,520
hier is een voorbeeld van een

1310
00:53:53,520 --> 00:53:54,980
trainingsmethode die

1311
00:53:54,980 --> 00:53:59,280
wordt uitgevoerd, dus op c0, wat het begin van de training is,

1312
00:53:59,280 --> 00:54:00,720


1313
00:54:00,720 --> 00:54:03,000
hebben we dit model hier, dus s0

1314
00:54:03,000 --> 00:54:05,819
komt overeen met de tot de zeven dus tot

1315
00:54:05,819 --> 00:54:08,099
de eerste afbeelding  aangezien één overeenkomt met

1316
00:54:08,099 --> 00:54:09,839
de afbeelding met zeven kolommen, hebben we het

1317
00:54:09,839 --> 00:54:12,300
label Y en alle latente variabelen x0

1318
00:54:12,300 --> 00:54:13,800
X1 X2

1319
00:54:13,800 --> 00:54:15,720
en het model is volledig verbonden, dus het

1320
00:54:15,720 --> 00:54:17,040
bureau Matrix

1321
00:54:17,040 --> 00:54:20,579
zit vol met enen er zijn geen nullen we

1322
00:54:20,579 --> 00:54:23,720
hebben zelflussen en dat soort dingen

1323
00:54:23,720 --> 00:54:27,319
de  model voor een paar tijdperken totdat

1324
00:54:27,319 --> 00:54:30,540
en wat we meteen weten, is dat

1325
00:54:30,540 --> 00:54:31,920
het model bijvoorbeeld onmiddellijk

1326
00:54:31,920 --> 00:54:34,740
begrijpt dat de vier niet nodig is

1327
00:54:34,740 --> 00:54:36,839
om classificatie uit te voeren, dus het is niet

1328
00:54:36,839 --> 00:54:40,740
zo dat elk uitgaand knooppunt van

1329
00:54:40,740 --> 00:54:43,980
het tweede invoercluster wordt verwijderd

1330
00:54:43,980 --> 00:54:45,900
en  iets dat we niet begrepen, is

1331
00:54:45,900 --> 00:54:48,660
dat dit dit cluster is dat

1332
00:54:48,660 --> 00:54:50,400
gerelateerd is aan de uitvoer,

1333
00:54:50,400 --> 00:54:52,260
dus we

1334
00:54:52,260 --> 00:54:55,319
hebben een lineaire kaart van s0 naar Y

1335
00:54:55,319 --> 00:54:56,480
direct,

1336
00:54:56,480 --> 00:54:59,339
wat dit deel hier is,

1337
00:54:59,339 --> 00:55:01,160
maar we weten dat een lineaire kaart eigenlijk

1338
00:55:01,160 --> 00:55:04,740
niet de beste is  kaart voor

1339
00:55:04,740 --> 00:55:07,200
het uitvoeren van classificatie van middelen, dus

1340
00:55:07,200 --> 00:55:08,700
we hebben een hiërarchie nodig, we hebben wat

1341
00:55:08,700 --> 00:55:11,579
diepte nodig om de resultaten te verbeteren en zoals

1342
00:55:11,579 --> 00:55:14,220
je kunt zien, is deze regel hier de

1343
00:55:14,220 --> 00:55:15,599
nauwkeurigheid

1344
00:55:15,599 --> 00:55:18,960
die tot nu toe, dus tot C2 is

1345
00:55:18,960 --> 00:55:22,500
vergelijkbaar met eh, dus het is 91  wat

1346
00:55:22,500 --> 00:55:24,059
iets beter is dan lineaire

1347
00:55:24,059 --> 00:55:25,500
classificatie,

1348
00:55:25,500 --> 00:55:28,740
maar als je eenmaal doorgaat met de training,

1349
00:55:28,740 --> 00:55:30,660
begrijpt het model dat het enige hiërarchie nodig heeft

1350
00:55:30,660 --> 00:55:33,119
om beter bij de gegevens te passen,

1351
00:55:33,119 --> 00:55:35,640
dus je ziet dat deze pijl

1352
00:55:35,640 --> 00:55:38,760
in de loop van de tijd sterker en sterker begint te worden

1353
00:55:38,760 --> 00:55:41,700
totdat hij begrijpt dat de lineaire

1354
00:55:41,700 --> 00:55:44,339
kaart is eigenlijk niet echt nodig en het

1355
00:55:44,339 --> 00:55:45,920
verwijdert het,

1356
00:55:45,920 --> 00:55:48,780
dus het model waarmee je convergeert is een

1357
00:55:48,780 --> 00:55:51,000
model dat begint bij een nul, naar een

1358
00:55:51,000 --> 00:55:53,760
verborgen knooppunt gaat en dan naar

1359
00:55:53,760 --> 00:55:57,180
het label gaat met een zeer zwakke lineaire kaart

1360
00:55:57,180 --> 00:55:59,700
die feitelijk wordt verwijderd als je als

1361
00:55:59,700 --> 00:56:02,760
je stelt een drempelwaarde in van uh als

1362
00:56:02,760 --> 00:56:05,520
verkoperdrempel van bijvoorbeeld 0,1 0,2 op een gegeven

1363
00:56:05,520 --> 00:56:07,619
moment wordt de lineaire kaart vergeten en

1364
00:56:07,619 --> 00:56:10,680
alles waar je mee eindigt is

1365
00:56:10,680 --> 00:56:13,319
met een hiërarchisch netwerk

1366
00:56:13,319 --> 00:56:15,720
dat is dat uh dus het heeft de

1367
00:56:15,720 --> 00:56:17,099
juiste structuur geleerd om te presteren

1368
00:56:17,099 --> 00:56:19,260
classificatietaken die hiërarchisch zijn

1369
00:56:19,260 --> 00:56:21,900
en het heeft ook geleerd dat het tweede

1370
00:56:21,900 --> 00:56:25,020
beeld geen enkele rol speelde bij het definiëren van

1371
00:56:25,020 --> 00:56:28,440
de testnauwkeurigheid en dit is alles,

1372
00:56:28,440 --> 00:56:30,420
dit wordt allemaal uitgevoerd, ook al die

1373
00:56:30,420 --> 00:56:33,839
taken worden eenvoudigweg uitgevoerd door

1374
00:56:33,839 --> 00:56:36,599
één gratis energieminimalisatieproces uit te voeren, dus

1375
00:56:36,599 --> 00:56:38,400
je initialiseert het model je definieert de

1376
00:56:38,400 --> 00:56:40,859
vrije energie je definieert de priors zodat

1377
00:56:40,859 --> 00:56:43,559
de sparse en de C-klik voordat

1378
00:56:43,559 --> 00:56:45,780
je de energieminimalisatie uitvoert en

1379
00:56:45,780 --> 00:56:47,400
je convergeert van hiërarchisch naar een

1380
00:56:47,400 --> 00:56:49,500
hiërarchisch model dat goed in staat is om

1381
00:56:49,500 --> 00:56:51,839
classificatie op gehakt uit te voeren

1382
00:56:51,839 --> 00:56:54,000
en als je dan  voer dan wat

1383
00:56:54,000 --> 00:56:55,800
fijnafstemming uit, je bereikt zeer competitieve

1384
00:56:55,800 --> 00:56:57,359
resultaten zoals je doet in feed forward-

1385
00:56:57,359 --> 00:56:59,339
netwerken met de feedbackpropagatie,

1386
00:56:59,339 --> 00:57:01,260
maar ik denk dat dat niet het interessante is,

1387
00:57:01,260 --> 00:57:03,780
het interessante is dat je

1388
00:57:03,780 --> 00:57:05,160
dit hele proces leuk vindt, dit proces allemaal

1389
00:57:05,160 --> 00:57:07,980
samen van interventie en de

1390
00:57:07,980 --> 00:57:09,780
acycliciteit

1391
00:57:09,780 --> 00:57:11,700
stelt je in staat  om een ​​volledig verbonden netwerk te nemen

1392
00:57:11,700 --> 00:57:12,660


1393
00:57:12,660 --> 00:57:15,119
en te convergeren naar een hiërarchisch netwerk

1394
00:57:15,119 --> 00:57:16,140
dat in staat is om

1395
00:57:16,140 --> 00:57:20,058
classificatie uit te voeren met goede resultaten

1396
00:57:20,760 --> 00:57:23,000
en ja,

1397
00:57:23,000 --> 00:57:26,280
dat is het eigenlijk. Ik ben nu oh ja wauw,

1398
00:57:26,280 --> 00:57:29,220
ik heb veel gepraat en ik ben uh, dit is de

1399
00:57:29,220 --> 00:57:32,160
conclusie  van het gesprek, dat is een ik

1400
00:57:32,160 --> 00:57:35,280
doe eigenlijk een kleine samenvatting en ik

1401
00:57:35,280 --> 00:57:37,559
denk dat de belangrijkste afhaalmogelijkheid als ik

1402
00:57:37,559 --> 00:57:39,300
je in één zin van dit

1403
00:57:39,300 --> 00:57:40,980
artikel moet geven, is dat voorspellende codering een

1404
00:57:40,980 --> 00:57:44,400
methode is voor het bijwerken van overtuigingen die in staat is om

1405
00:57:44,400 --> 00:57:46,559
end-to uit te voeren  -het leren van neven en nichten beëindigen, zodat

1406
00:57:46,559 --> 00:57:48,599
hij interventies kan uitvoeren om

1407
00:57:48,599 --> 00:57:51,420
een ​​structuur uit gegevens te leren en vervolgens

1408
00:57:51,420 --> 00:57:53,160
interventies en contrafeiten kan uitvoeren,

1409
00:57:53,160 --> 00:57:56,058


1410
00:57:56,700 --> 00:57:58,440
zodat causale gevolgtrekkingen in andere en

1411
00:57:58,440 --> 00:58:00,119
efficiënte interventies kunnen worden gemodelleerd door

1412
00:58:00,119 --> 00:58:01,680
simpelweg de voorspellingsfout op

1413
00:58:01,680 --> 00:58:03,359
nul te zetten, dus het is een heel gemakkelijke

1414
00:58:03,359 --> 00:58:06,240
techniek om uit te voeren  interventies en

1415
00:58:06,240 --> 00:58:07,619
je hoeft alleen maar één

1416
00:58:07,619 --> 00:58:08,940
neuron aan te raken je hoeft niet te handelen op de

1417
00:58:08,940 --> 00:58:10,859
structuur van de grafiek je

1418
00:58:10,859 --> 00:58:14,339
kunt het gebruiken om

1419
00:58:14,339 --> 00:58:16,140
structuur te creëren causale modellen die

1420
00:58:16,140 --> 00:58:18,359
biologisch plausibel zijn

1421
00:58:18,359 --> 00:58:20,819
het is in staat om de structuur voor te leren  uh

1422
00:58:20,819 --> 00:58:24,119
uit gegevens, zoals ik misschien al vaak heb gezegd

1423
00:58:24,119 --> 00:58:26,940
en

1424
00:58:26,940 --> 00:58:28,740
een paar zinnen over toekomstig

1425
00:58:28,740 --> 00:58:31,260
werk, is dat

1426
00:58:31,260 --> 00:58:33,180
iets dat leuk zou zijn om te doen, is om

1427
00:58:33,180 --> 00:58:36,119
de prestaties te verbeteren van het model dat

1428
00:58:36,119 --> 00:58:38,460
we hebben gedefinieerd, omdat ik denk dat het

1429
00:58:38,460 --> 00:58:40,980
redelijk goed presteert op  veel

1430
00:58:40,980 --> 00:58:43,079
taken, dus het presteert redelijk goed op

1431
00:58:43,079 --> 00:58:45,780
structureel leren voor mij

1432
00:58:45,780 --> 00:58:48,119
interventie en contrafeiten, maar

1433
00:58:48,119 --> 00:58:49,440
als je naar het geavanceerde

1434
00:58:49,440 --> 00:58:51,420
model kijkt, is er eigenlijk altijd een heel

1435
00:58:51,420 --> 00:58:53,880
specifieke methode die beter presteert in

1436
00:58:53,880 --> 00:58:55,559
de enkele taak,

1437
00:58:55,559 --> 00:58:58,260
dus het zou interessant zijn om te zien  als we

1438
00:58:58,260 --> 00:59:00,180
dat prestatieniveau in

1439
00:59:00,180 --> 00:59:03,599
uh in specifieke taken kunnen bereiken door enkele

1440
00:59:03,599 --> 00:59:05,599
trucs of enkele

1441
00:59:05,599 --> 00:59:10,260
of enkele nieuwe optimalisatiemethoden toe te voegen en

1442
00:59:10,260 --> 00:59:12,839
dit te generaliseren naar dynamische systemen

1443
00:59:12,839 --> 00:59:14,280
die eigenlijk veel interessanter zijn,

1444
00:59:14,280 --> 00:59:17,220
de statische systemen, zoals dynamische

1445
00:59:17,220 --> 00:59:20,099
causale modellen en of  andere technieken

1446
00:59:20,099 --> 00:59:22,200
waarmee je

1447
00:59:22,200 --> 00:59:25,200
causale gevolgtrekkingen kunt maken in systemen die bewegen, dus

1448
00:59:25,200 --> 00:59:27,799
een actie die in een specifieke tijdstap wordt ondernomen,

1449
00:59:27,799 --> 00:59:30,299
beïnvloedt een ander knooppunt in een latere

1450
00:59:30,299 --> 00:59:32,640
tijdstap, wat in wezen een grandeur-

1451
00:59:32,640 --> 00:59:34,859
causaliteit is

1452
00:59:34,859 --> 00:59:38,160
ja dat is het en uh

1453
00:59:38,160 --> 00:59:41,118
heel erg bedankt, bedankt

1454
00:59:47,460 --> 00:59:51,119
geweldige en zeer uitgebreide

1455
00:59:51,119 --> 00:59:53,160
presentatie die echt dacht dat

1456
00:59:53,160 --> 00:59:55,700
je gedempt bent

1457
00:59:57,119 --> 00:59:59,700
sorry gedempt op Zoom maar ja bedankt voor

1458
00:59:59,700 --> 01:00:02,400
de geweldige en zeer uitgebreide

1459
01:00:02,400 --> 01:00:05,099
presentatie er was echt veel

1460
01:00:05,099 --> 01:00:06,900
daar en er waren ook veel geweldige

1461
01:00:06,900 --> 01:00:09,900
vragen in de live chat dus misschien om op te

1462
01:00:09,900 --> 01:00:12,900
warmen  in de vragen hoe ben je ertoe gekomen om

1463
01:00:12,900 --> 01:00:15,960
dit onderwerp te bestuderen waar je

1464
01:00:15,960 --> 01:00:18,900
causaliteit bestudeerde en voorspellende codering

1465
01:00:18,900 --> 01:00:21,000
nuttig vond of vice versa of hoe

1466
01:00:21,000 --> 01:00:23,160
kwam je op dit kruispunt

1467
01:00:23,160 --> 01:00:25,740
Ik moet eigenlijk zeggen dat de eerste

1468
01:00:25,740 --> 01:00:27,240
persoon die met dit idee naar buiten kwam was

1469
01:00:27,240 --> 01:00:29,040
uh was Baron

1470
01:00:29,040 --> 01:00:33,900
zo zoals ik denk dat anderhalf jaar

1471
01:00:33,900 --> 01:00:36,660
geleden nog meer hij bracht als een

1472
01:00:36,660 --> 01:00:38,940
pagina met dit idee en toen werd hij

1473
01:00:38,940 --> 01:00:42,119
vergeten en niemand pakte het op en uh

1474
01:00:42,119 --> 01:00:43,980
en afgelopen zomer begon ik

1475
01:00:43,980 --> 01:00:47,880
nieuwsgierig te worden naar causaliteit en de

1476
01:00:47,880 --> 01:00:50,339
um  Ik las bijvoorbeeld The Book of Life

1477
01:00:50,339 --> 01:00:52,440
terwijl ik naar podcasts luisterde Ik ken de

1478
01:00:52,440 --> 01:00:53,760
standaardmanier waarop je geïnteresseerd raakt

1479
01:00:53,760 --> 01:00:54,900
in een onderwerp

1480
01:00:54,900 --> 01:00:57,480
en ik herinner me dit idee van

1481
01:00:57,480 --> 01:01:00,180
Baron en stelde het hem voor en uh en

1482
01:01:00,180 --> 01:01:03,180
ik had zoiets van waarom niet  we breiden het uit en eh

1483
01:01:03,180 --> 01:01:06,000
en maken er eigenlijk een paper van, dus ik heb

1484
01:01:06,000 --> 01:01:07,319
wat mensen erbij betrokken om me te helpen met

1485
01:01:07,319 --> 01:01:09,359
experimenten en dit is het

1486
01:01:09,359 --> 01:01:12,000
eindresultaat aan het einde

1487
01:01:12,000 --> 01:01:14,160
geweldig cool ja eh

1488
01:01:14,160 --> 01:01:15,240


1489
01:01:15,240 --> 01:01:17,400
veel te zeggen ik ga gewoon naar de

1490
01:01:17,400 --> 01:01:19,619
live chat  eerst en een heleboel

1491
01:01:19,619 --> 01:01:21,240
verschillende vragen behandelen en als iemand anders

1492
01:01:21,240 --> 01:01:22,440
me wil toevoegen, doe ik eerst het licht

1493
01:01:22,440 --> 01:01:24,059
aan, want ik denk dat ik

1494
01:01:24,059 --> 01:01:28,440
steeds meer in het duister tast ja

1495
01:01:28,440 --> 01:01:30,720
wie zei dat actieve gevolgtrekking niet kan oplossen  een

1496
01:01:30,720 --> 01:01:32,160
probleem met de donkere kamer

1497
01:01:32,160 --> 01:01:34,980
oh ja hier zijn we

1498
01:01:34,980 --> 01:01:37,020
dus zou je zeggen dat de lichtschakelaar ervoor zorgde dat

1499
01:01:37,020 --> 01:01:39,299
het lichter was

1500
01:01:39,299 --> 01:01:40,680
ja

1501
01:01:40,680 --> 01:01:42,240
ik denk dus

1502
01:01:42,240 --> 01:01:43,980
geen problemen hier

1503
01:01:43,980 --> 01:01:46,940
um oke ml Dawn schreef

1504
01:01:46,940 --> 01:01:49,559
omdat in voorspellende codering alle

1505
01:01:49,559 --> 01:01:52,020
distributies meestal Gaussiaans zijn, de

1506
01:01:52,020 --> 01:01:53,760
bottom-up berichten zijn Precision

1507
01:01:53,760 --> 01:01:55,500
gewogen voorspelling  fouten waarbij

1508
01:01:55,500 --> 01:01:57,420
Precisie het omgekeerde is van de Gaussiaanse

1509
01:01:57,420 --> 01:02:00,000
covariantie. Wat als niet-Gaussiaanse

1510
01:02:00,000 --> 01:02:03,319
distributies worden gebruikt,

1511
01:02:03,780 --> 01:02:05,339


1512
01:02:05,339 --> 01:02:09,059


1513
01:02:09,059 --> 01:02:10,380


1514
01:02:10,380 --> 01:02:13,079


1515
01:02:13,079 --> 01:02:15,480
is in principe de algemene methode die anders blijft.

1516
01:02:15,480 --> 01:02:18,480
afgeleide van de

1517
01:02:18,480 --> 01:02:20,819
virtuele vrije energie als je gaussiaanse aannames hebt

1518
01:02:20,819 --> 01:02:22,920


1519
01:02:22,920 --> 01:02:25,020
ja je hebt zelfs die ene grootheid

1520
01:02:25,020 --> 01:02:27,960
om op nul te zetten en je zult waarschijnlijk

1521
01:02:27,960 --> 01:02:29,880
moeten handelen op de structuur van de

1522
01:02:29,880 --> 01:02:30,900
grafiek

1523
01:02:30,900 --> 01:02:34,020
om interventies uit te voeren

1524
01:02:34,020 --> 01:02:37,079
en ook jij eh en collega's hadden een

1525
01:02:37,079 --> 01:02:39,900
paper in 2022 voorspellend  codering Afgezien van

1526
01:02:39,900 --> 01:02:41,880
Gauss-distributies die

1527
01:02:41,880 --> 01:02:43,859
naar een aantal van deze problemen keken,

1528
01:02:43,859 --> 01:02:46,260
ja ja, precies dus dat papier was een

1529
01:02:46,260 --> 01:02:47,339
beetje

1530
01:02:47,339 --> 01:02:50,460
het idee achter dat papier is uh

1531
01:02:50,460 --> 01:02:53,220
en we modelleren Transformers dat is de

1532
01:02:53,220 --> 01:02:54,420
grootste motivatie met behulp van behoorlijk

1533
01:02:54,420 --> 01:02:57,180
moeilijk en het antwoord is uh is niet

1534
01:02:57,180 --> 01:02:59,460
omdat de  het aandachtsmechanisme heeft

1535
01:02:59,460 --> 01:03:02,099
een zachte Max aan het einde en zachte Max-aanroepen

1536
01:03:02,099 --> 01:03:03,960
naar uh

1537
01:03:03,960 --> 01:03:08,400
zoals niet naar gaussiaanse distributie maar naar

1538
01:03:08,400 --> 01:03:11,280
ja naar zachte Max-distributie ik

1539
01:03:11,280 --> 01:03:13,440
snap de naam nu niet, maar ja

1540
01:03:13,440 --> 01:03:16,079
en uh dus ja dat is een generalisatie,

1541
01:03:16,079 --> 01:03:19,140
het is een beetje  lastig om het te noemen als

1542
01:03:19,140 --> 01:03:20,700
je de Gaston-aanname eenmaal hebt verwijderd, is een

1543
01:03:20,700 --> 01:03:22,319
beetje nog steeds lastig om het

1544
01:03:22,319 --> 01:03:24,059
creatieve codering te noemen,

1545
01:03:24,059 --> 01:03:26,400
dus hij is een dus

1546
01:03:26,400 --> 01:03:29,819
bijvoorbeeld praten met uh tegen auto

1547
01:03:29,819 --> 01:03:32,700
Freestone, hij houdt ook van creatieve codering

1548
01:03:32,700 --> 01:03:35,160
is alleen als je alleen Gauss

1549
01:03:35,160 --> 01:03:37,680
en Gauss hebt  aannames

1550
01:03:37,680 --> 01:03:39,720
maar ja dat is meer een filosofisch

1551
01:03:39,720 --> 01:03:42,660
debat dan uh

1552
01:03:42,660 --> 01:03:44,940
interessant en een ander onderwerp

1553
01:03:44,940 --> 01:03:46,740
dat volgens mij zeker van groot

1554
01:03:46,740 --> 01:03:49,500
belang is, zijn overeenkomsten en verschillen

1555
01:03:49,500 --> 01:03:52,980
tussen het aandachtsapparaat in

1556
01:03:52,980 --> 01:03:56,099
Transformers en de manier waarop aandacht

1557
01:03:56,099 --> 01:03:58,440
wordt beschreven vanuit een neurocognitief

1558
01:03:58,440 --> 01:04:00,180
perspectief en vanuit een voorspellende

1559
01:04:00,180 --> 01:04:03,240
verwerking Precisie  wachthoek wat

1560
01:04:03,240 --> 01:04:06,200
vind je daarvan, nou

1561
01:04:06,359 --> 01:04:08,700
het idee is dat eh

1562
01:04:08,700 --> 01:04:12,359
ja, ik denk erover na dat vanuit een

1563
01:04:12,359 --> 01:04:15,000
mooi verwerkings- en ook

1564
01:04:15,000 --> 01:04:16,400
operationeel inferentieperspectief

1565
01:04:16,400 --> 01:04:19,260
aandacht kan worden gezien als een soort

1566
01:04:19,260 --> 01:04:21,299
structureel leerprobleem, ik

1567
01:04:21,299 --> 01:04:23,040
denk dat er een  recent artikel van uh

1568
01:04:23,040 --> 01:04:25,680
van de groep van Chris Buckley waaruit blijkt

1569
01:04:25,680 --> 01:04:26,339
dat er

1570
01:04:26,339 --> 01:04:28,079
een

1571
01:04:28,079 --> 01:04:30,420
herdruk op het archief zou moeten zijn waarin ze in feite

1572
01:04:30,420 --> 01:04:31,859
aantoonden dat het aandachtsmechanisme

1573
01:04:31,859 --> 01:04:35,819
eenvoudigweg het leren van de precisie

1574
01:04:35,819 --> 01:04:38,880
op de gewichtsparameters is die specifiek zijn voor

1575
01:04:38,880 --> 01:04:41,040
andere gegevenspunten, dus deze precisie  is

1576
01:04:41,040 --> 01:04:43,200
niet a het is geen a het is geen parameter

1577
01:04:43,200 --> 01:04:45,540
die in de structuur van het model zit, dus het is

1578
01:04:45,540 --> 01:04:47,579
geen modelspecifieke parameter het

1579
01:04:47,579 --> 01:04:49,140
is een snel veranderende parameter zoals de

1580
01:04:49,140 --> 01:04:51,660
waardeknooppunten die worden bijgewerkt terwijl

1581
01:04:51,660 --> 01:04:53,760
de variatie van vrije energie wordt geminimaliseerd

1582
01:04:53,760 --> 01:04:55,440
en als ze eenmaal je  heb het geminimaliseerd

1583
01:04:55,440 --> 01:04:57,000
en berekend, dan gooi je het weg

1584
01:04:57,000 --> 01:04:58,920
en voor het volgende datapunt moet je

1585
01:04:58,920 --> 01:05:00,780
het helemaal opnieuw berekenen,

1586
01:05:00,780 --> 01:05:03,299
dus ja, ik denk dat de analogie

1587
01:05:03,299 --> 01:05:05,819
qua berekeningen uh is, het

1588
01:05:05,819 --> 01:05:07,920
aandachtsmechanisme kan worden gezien als een soort

1589
01:05:07,920 --> 01:05:10,559
structuurleren, maar een

1590
01:05:10,559 --> 01:05:13,020
structuurleren dat datapuntspecifiek is en

1591
01:05:13,020 --> 01:05:15,119
niet modelspecifiek

1592
01:05:15,119 --> 01:05:17,280
en ik denk dat als we een beetje willen generaliseren

1593
01:05:17,280 --> 01:05:18,960
en uitgaan

1594
01:05:18,960 --> 01:05:20,339
van het aandachtsmechanisme in

1595
01:05:20,339 --> 01:05:21,900
Transformers, het aandachtsmechanisme

1596
01:05:21,900 --> 01:05:24,180
cognitieve wetenschap,

1597
01:05:24,180 --> 01:05:28,020
ik denk dat ze waarschijnlijk twee verschillende zijn om

1598
01:05:28,020 --> 01:05:31,260
graag overeenkomsten te trekken en  uh

1599
01:05:31,260 --> 01:05:33,359
Ik denk dat de structurele leeranalogie

1600
01:05:33,359 --> 01:05:36,660
en hoe belangrijk de ene verbinding

1601
01:05:36,660 --> 01:05:38,760
is met betrekking tot de andere waarschijnlijk

1602
01:05:38,760 --> 01:05:41,900
het werk veel beter doet,

1603
01:05:42,000 --> 01:05:44,880
cool grijs antwoord oké

1604
01:05:44,880 --> 01:05:49,200
ml Don vraagt ​​in contrafeiten wat

1605
01:05:49,200 --> 01:05:51,240
het verschil is tussen verborgen variabelen

1606
01:05:51,240 --> 01:05:55,440
X en niet-geobserveerde variabelen U

1607
01:05:55,440 --> 01:05:59,180
het verschil is  dat je kunt

1608
01:05:59,540 --> 01:06:01,740
Ik denk dat de belangrijkste is dat je

1609
01:06:01,740 --> 01:06:03,599
het gebruik niet kunt observeren,

1610
01:06:03,599 --> 01:06:05,819
je kunt ze gebruiken, omdat je

1611
01:06:05,819 --> 01:06:09,000
ze kunt berekenen en repareren, maar je kunt ze niet

1612
01:06:09,000 --> 01:06:10,559
het idee is dat je er geen controle

1613
01:06:10,559 --> 01:06:13,380
over hebt, dus ze gebruiken het gebruik zou moeten zijn

1614
01:06:13,380 --> 01:06:16,020
gezien als een omgevingsspecifieke variabelen

1615
01:06:16,020 --> 01:06:18,540
dat ze er zijn, ze beïnvloeden

1616
01:06:18,540 --> 01:06:21,240
je proces oké, want als

1617
01:06:21,240 --> 01:06:23,280
je bijvoorbeeld teruggaat in de tijd,

1618
01:06:23,280 --> 01:06:25,079
is de omgeving anders, dus het idee is

1619
01:06:25,079 --> 01:06:26,520
bijvoorbeeld of je het

1620
01:06:26,520 --> 01:06:28,440
leuk vindt om terug te gaan naar het voorbeeld

1621
01:06:28,440 --> 01:06:29,880
ervoor of het

1622
01:06:29,880 --> 01:06:31,920
van  het verwachte inkomen van een persoon met

1623
01:06:31,920 --> 01:06:34,619
een specifieke intelligentie van Onderwijs uh

1624
01:06:34,619 --> 01:06:37,440
uh opleidingsgraad

1625
01:06:37,440 --> 01:06:40,200
het idee is dat als ik wil zien

1626
01:06:40,200 --> 01:06:43,559
hoeveel ik vandaag zal leren met uh met een

1627
01:06:43,559 --> 01:06:45,359
met ik weet het niet met een masterdiploma

1628
01:06:45,359 --> 01:06:47,339
anders is met respect  hoeveel ik

1629
01:06:47,339 --> 01:06:48,359


1630
01:06:48,359 --> 01:06:50,819
20 jaar geleden zou verdienen met een masterdiploma is

1631
01:06:50,819 --> 01:06:52,619
bijvoorbeeld anders hier in Italië met

1632
01:06:52,619 --> 01:06:55,440
betrekking tot andere landen en al die

1633
01:06:55,440 --> 01:06:57,000
variabelen die je niet onder

1634
01:06:57,000 --> 01:06:58,859
controle hebt, je kunt ze niet modelleren met je

1635
01:06:58,859 --> 01:07:00,359
visie. Netwerk,

1636
01:07:00,359 --> 01:07:03,480
maar ze zijn er, oké, dus jij  je kunt

1637
01:07:03,480 --> 01:07:05,220
ze niet negeren als je

1638
01:07:05,220 --> 01:07:07,559
conclusies wilt trekken, dus hij is ja,

1639
01:07:07,559 --> 01:07:08,760
het is eigenlijk alles waar je

1640
01:07:08,760 --> 01:07:10,079
geen controle over hebt,

1641
01:07:10,079 --> 01:07:13,079
je kunt ze afleiden, zodat je

1642
01:07:13,079 --> 01:07:14,819
een contra-contrafeitelijke

1643
01:07:14,819 --> 01:07:16,740
gevolgtrekking in de tijd kunt maken en kunt zeggen oh 20

1644
01:07:16,740 --> 01:07:19,020
jaar geleden zou ik dit hebben verdiend  veel

1645
01:07:19,020 --> 01:07:20,640
als ik als ik zo

1646
01:07:20,640 --> 01:07:22,559
intelligent was dat deze

1647
01:07:22,559 --> 01:07:24,599
graad gemiddeld natuurlijk is

1648
01:07:24,599 --> 01:07:27,059
en maar het is niet zo dat ik het

1649
01:07:27,059 --> 01:07:30,720
overheidsbeleid ten aanzien van banen of dat

1650
01:07:30,720 --> 01:07:32,819
soort dingen kan veranderen,

1651
01:07:32,819 --> 01:07:35,099
het is een diepere contrafeitelijke

1652
01:07:35,099 --> 01:07:38,400
ja precies dus ja dat is het gebruik

1653
01:07:38,400 --> 01:07:40,200
geweldig, oké

1654
01:07:40,200 --> 01:07:42,480
je hebt gegeneraliseerde

1655
01:07:42,480 --> 01:07:45,660
coördinaten geïmplementeerd in voorspellende codering

1656
01:07:45,660 --> 01:07:46,920
nee

1657
01:07:46,920 --> 01:07:50,039
ik heb nee ik heb het nog nooit gedaan ik heb het

1658
01:07:50,039 --> 01:07:52,680
bestudeerd maar ik heb het

1659
01:07:52,680 --> 01:07:55,260
nooit geïmplementeerd ik weet dat ze de neiging hebben

1660
01:07:55,260 --> 01:07:57,599
onstabiel te zijn en uh

1661
01:07:57,599 --> 01:08:00,299
en het is  erg moeilijk om ze stabiel te maken.

1662
01:08:00,299 --> 01:08:02,940


1663
01:08:02,940 --> 01:08:05,460


1664
01:08:05,460 --> 01:08:08,359


1665
01:08:08,400 --> 01:08:11,039


1666
01:08:11,039 --> 01:08:12,839


1667
01:08:12,839 --> 01:08:15,599


1668
01:08:15,599 --> 01:08:18,000
encoderstijl eigenlijk

1669
01:08:18,000 --> 01:08:20,520
denk ik nog steeds van Baron er is een er is

1670
01:08:20,520 --> 01:08:22,979
een krant die

1671
01:08:22,979 --> 01:08:25,439
afgelopen zomer uitkwam, maar nee, ik heb

1672
01:08:25,439 --> 01:08:26,580
ze zelf nooit met ze gespeeld

1673
01:08:26,580 --> 01:08:29,160
cool romper

1674
01:08:29,160 --> 01:08:32,040
doet het toevoegen van meer niveaus in de hiërarchie

1675
01:08:32,040 --> 01:08:35,160
het afleidingsprobleem van het

1676
01:08:35,160 --> 01:08:38,238
voorspellen van invoer verminderen door

1677
01:08:38,939 --> 01:08:41,698
meer toe te voegen  niveau in uh

1678
01:08:41,698 --> 01:08:43,439
in welke zin omdat het

1679
01:08:43,439 --> 01:08:45,779
vernietigingsprobleem wordt gegeven door Cycles, dus in feite

1680
01:08:45,779 --> 01:08:47,399
geef je een beeld

1681
01:08:47,399 --> 01:08:49,920
en het feit dat je

1682
01:08:49,920 --> 01:08:53,279
zo patches hebt die uit het beeld gaan

1683
01:08:53,279 --> 01:08:55,799
in de neuronen en dan andere randen die

1684
01:08:55,799 --> 01:08:57,500
teruggaan,

1685
01:08:57,500 --> 01:08:59,939
creëert in feite het feit dat  je

1686
01:08:59,939 --> 01:09:03,560
hebt de fout dat die in feite

1687
01:09:03,560 --> 01:09:06,179
deze inkomende aanpassing aan de pixels van

1688
01:09:06,179 --> 01:09:08,339
de afbeelding, ze creëren een aantal

1689
01:09:08,339 --> 01:09:09,719
voorspellingsfouten, dus je hebt een aantal

1690
01:09:09,719 --> 01:09:12,140
voorspellingsfouten die zich in het model verspreiden

1691
01:09:12,140 --> 01:09:14,640
en dat is ja en dit probleem

1692
01:09:14,640 --> 01:09:16,979
is volgens mij algemeen voor cycli en  het is waarschijnlijk

1693
01:09:16,979 --> 01:09:21,439
niet gerelateerd aan hiërarchie in het algemeen

1694
01:09:23,060 --> 01:09:25,140
aan de pixels

1695
01:09:25,140 --> 01:09:26,759
als je geen inkomende randen hebt,

1696
01:09:26,759 --> 01:09:27,660
heb je geen

1697
01:09:27,660 --> 01:09:30,540
uh geen vernietigingsprobleem meer

1698
01:09:30,540 --> 01:09:33,238
cool en en de specificatie van het

1699
01:09:33,238 --> 01:09:35,939
acyclische netwerk via de trace-

1700
01:09:35,939 --> 01:09:37,859
operator

1701
01:09:37,859 --> 01:09:41,819
dat is een zeer interessante techniek en

1702
01:09:41,819 --> 01:09:46,339
wanneer werd dat gebracht  in het spel

1703
01:09:46,560 --> 01:09:49,140
uh voor zover ik weet ik denk dat hij naar buiten kwam

1704
01:09:49,140 --> 01:09:52,380
met de paper die ik heb geciteerd in 2018

1705
01:09:52,380 --> 01:09:54,360
ik ik weet niet in ieder geval in de

1706
01:09:54,360 --> 01:09:56,940
literatuur over causale gevolgtrekkingen ik ben me niet bewust

1707
01:09:56,940 --> 01:09:59,699
van eerdere methoden ik zou nee zeggen

1708
01:09:59,699 --> 01:10:01,860
omdat dat  Ik bedoel, dat is het veel

1709
01:10:01,860 --> 01:10:04,140
geciteerde artikel, dus ik zou zeggen dat ze

1710
01:10:04,140 --> 01:10:05,520
met dat idee kwamen

1711
01:10:05,520 --> 01:10:07,980
wauw ja dat is best leuk dat

1712
01:10:07,980 --> 01:10:09,480
je gradiëntafdaling kunt doen en

1713
01:10:09,480 --> 01:10:11,400
de structuur kunt leren Ik denk dat

1714
01:10:11,400 --> 01:10:14,219
dat een heel krachtige techniek is ja

1715
01:10:14,219 --> 01:10:15,840
soms is het zoals wanneer je kijkt naar

1716
01:10:15,840 --> 01:10:17,640
toen verschillende

1717
01:10:17,640 --> 01:10:19,440
um-kenmerken van Bayesiaanse gevolgtrekking en

1718
01:10:19,440 --> 01:10:23,159
causale gevolgtrekking beschikbaar kwamen, is

1719
01:10:23,159 --> 01:10:25,620
het echt opmerkelijk, zoals waarom

1720
01:10:25,620 --> 01:10:28,500
is dit niet gedaan onder een Bayesiaans

1721
01:10:28,500 --> 01:10:30,719
causaal modelleringskader, het is

1722
01:10:30,719 --> 01:10:32,760
alsof dit slechts vijf tot

1723
01:10:32,760 --> 01:10:36,659
25 jaar geleden is gebeurd

1724
01:10:36,659 --> 01:10:39,960
en dat is dus heel erg kort  en

1725
01:10:39,960 --> 01:10:42,060
het is ook relatief technisch, dus er zijn

1726
01:10:42,060 --> 01:10:43,920
relatief weinig onderzoeksgroepen die zich

1727
01:10:43,920 --> 01:10:46,920
ermee bezighouden en het is gewoon

1728
01:10:46,920 --> 01:10:49,860
heel cool wat het mogelijk maakt

1729
01:10:49,860 --> 01:10:51,960
nee ja ja precies ik bedoel dat is ook ik

1730
01:10:51,960 --> 01:10:54,179
denk dat het opwindende deel van dit veld een

1731
01:10:54,179 --> 01:10:56,040
beetje dat is uh ik bedoel er zijn

1732
01:10:56,040 --> 01:10:59,100
zeker  doorbraken die er zijn die

1733
01:10:59,100 --> 01:11:01,020
nog moeten worden ontdekt en waarschijnlijk

1734
01:11:01,020 --> 01:11:03,000
leuk vinden omdat bijvoorbeeld voor

1735
01:11:03,000 --> 01:11:05,300
zoveel als een doorbraak dat papier was, ze

1736
01:11:05,300 --> 01:11:07,800
vonden uh

1737
01:11:07,800 --> 01:11:09,960
alsof ze gewoon de juiste

1738
01:11:09,960 --> 01:11:12,120
prior voor acyclische structuren ontdekten

1739
01:11:12,120 --> 01:11:14,040
oké is een

1740
01:11:14,040 --> 01:11:17,100
ja ik bedoel ik ik weet het niet  precies, maar

1741
01:11:17,100 --> 01:11:19,080
het kan een idee zijn dat je op een

1742
01:11:19,080 --> 01:11:21,120
middag hebt. Ik weet niet

1743
01:11:21,120 --> 01:11:23,040
hoe de anderen dat hebben bedacht,

1744
01:11:23,040 --> 01:11:25,320
maar het zou kunnen zijn dat ze

1745
01:11:25,320 --> 01:11:27,239
daar op het whiteboard zijn, zoals je bent,

1746
01:11:27,239 --> 01:11:29,280
oh dat eigenlijk  werkt, dat is een

1747
01:11:29,280 --> 01:11:32,159
enorme doorbraak en ik definieerde gewoon ja

1748
01:11:32,159 --> 01:11:33,960
de vorige

1749
01:11:33,960 --> 01:11:36,739
en ook veel van deze doorbraken

1750
01:11:36,739 --> 01:11:40,500
stapelen ze niet zomaar op, het is niet zoals

1751
01:11:40,500 --> 01:11:44,280
een uh uh een toren van blokken die ze in

1752
01:11:44,280 --> 01:11:47,640
lagen leggen en ze componeren, dus dan

1753
01:11:47,640 --> 01:11:50,159
zal er iets worden gegeneraliseerd naar um  gegeneraliseerde

1754
01:11:50,159 --> 01:11:52,140
coördinaten of gegeneraliseerde synchronie of

1755
01:11:52,140 --> 01:11:55,020
willekeurig grote grafieken of

1756
01:11:55,020 --> 01:11:57,239
um Sensor Fusion met multimodale invoer

1757
01:11:57,239 --> 01:12:00,679
en het is alsof die allemaal op echt

1758
01:12:00,679 --> 01:12:03,659
bevredigende en effectieve manieren samenvloeien, dus zelfs

1759
01:12:03,659 --> 01:12:05,640
kleine dingen die weer iemand

1760
01:12:05,640 --> 01:12:08,100
in een oogwenk kan bedenken,

1761
01:12:08,100 --> 01:12:11,100
kunnen echt impact hebben

1762
01:12:11,100 --> 01:12:14,159
um  oke ml Dawn zegt heel erg bedankt voor

1763
01:12:14,159 --> 01:12:16,199
het stellen van mijn vragen en heel erg bedankt

1764
01:12:16,199 --> 01:12:18,060
aan Tomaso voor de inspirerende presentatie

1765
01:12:18,060 --> 01:12:21,360
zo leuk oh heel erg bedankt en dan

1766
01:12:21,360 --> 01:12:23,280
vraagt ​​Bert

1767
01:12:23,280 --> 01:12:25,560
hoe zouden taalmodellen die

1768
01:12:25,560 --> 01:12:27,179
voorspellende codering gebruiken verschillen van die die

1769
01:12:27,179 --> 01:12:30,260
Transformers gebruiken

1770
01:12:31,679 --> 01:12:32,520
um

1771
01:12:32,520 --> 01:12:35,340
oke ik denk eigenlijk dat  als ik

1772
01:12:35,340 --> 01:12:36,659
vandaag een taalmodel zou moeten bouwen

1773
01:12:36,659 --> 01:12:38,640
met behulp van voorspellende codering, zal ik nog steeds

1774
01:12:38,640 --> 01:12:40,020
de Transformers gebruiken,

1775
01:12:40,020 --> 01:12:41,880
dus het idee is dat als je bijvoorbeeld

1776
01:12:41,880 --> 01:12:42,780


1777
01:12:42,780 --> 01:12:45,659
dit hiërarchische grafische

1778
01:12:45,659 --> 01:12:48,440
model hebt van dit of dit hiërarchische

1779
01:12:48,440 --> 01:12:50,460
Bayesiaanse netwerk dat

1780
01:12:50,460 --> 01:12:53,100
ik heb gedefinieerd in de in  de allereerste

1781
01:12:53,100 --> 01:12:55,380
schuift één pijl om een ​​functie te coderen

1782
01:12:55,380 --> 01:12:57,300
die de lineaire kaart is,

1783
01:12:57,300 --> 01:12:59,219
oké, dus een uur was gewoon de

1784
01:12:59,219 --> 01:13:01,080
vermenigvuldiging van een van de vector

1785
01:13:01,080 --> 01:13:03,060
gecodeerd in de latente variabelen maal

1786
01:13:03,060 --> 01:13:06,300
deze gewichtsmatrix die je dan

1787
01:13:06,300 --> 01:13:08,580
niet-lineair kunt maken en dat soort dingen  maar

1788
01:13:08,580 --> 01:13:09,960
dat kan eigenlijk iets veel

1789
01:13:09,960 --> 01:13:12,179
complexer zijn, de functie in de

1790
01:13:12,179 --> 01:13:14,880
pijl kan een convolutie zijn, kan een aandachtsmechanisme zijn,

1791
01:13:14,880 --> 01:13:16,800


1792
01:13:16,800 --> 01:13:20,820
dus hoe ik het zou doen, ik zal

1793
01:13:20,820 --> 01:13:23,880
nog steeds de ik bedoel gebruiken, wat eigenlijk

1794
01:13:23,880 --> 01:13:26,460
de manier is waarop we het deden in uh  in de Oxford

1795
01:13:26,460 --> 01:13:28,860
Group vorig jaar is dat we

1796
01:13:28,860 --> 01:13:30,900
precies de structuur hadden, elke pijl is

1797
01:13:30,900 --> 01:13:33,420
nu een Transformer, dus de ene is het

1798
01:13:33,420 --> 01:13:35,159
aandachtsmechanisme en de volgende is het

1799
01:13:35,159 --> 01:13:38,219
feed forward-netwerk als Transformers

1800
01:13:38,219 --> 01:13:40,020
en eigenlijk is het enige verschil dat

1801
01:13:40,020 --> 01:13:41,640
je hebt dat die  variabelen waarvan je

1802
01:13:41,640 --> 01:13:43,739
de posterior wilt berekenen en je

1803
01:13:43,739 --> 01:13:45,239
maakt die posteriors

1804
01:13:45,239 --> 01:13:47,400
Onafhankelijk van de onafhankelijkheid via VIA gemiddelde

1805
01:13:47,400 --> 01:13:49,560
veldbenadering, dus in feite volg je

1806
01:13:49,560 --> 01:13:51,659
alle stappen die je in staat stellen om te

1807
01:13:51,659 --> 01:13:53,520
convergeren naar de

1808
01:13:53,520 --> 01:13:56,520
variatievrije energie van creatieve codering, maar de

1809
01:13:56,520 --> 01:13:58,199
manier waarop je  voorspellingen berekenen

1810
01:13:58,199 --> 01:14:01,199
en de manier waarop je signalen terugstuurt,

1811
01:14:01,199 --> 01:14:04,739
gebeurt via Transformer,

1812
01:14:04,739 --> 01:14:07,560
dus ik zal nog steeds Transformers in

1813
01:14:07,560 --> 01:14:10,679
het algemeen gebruiken. Ik bedoel, ze werken zo goed dat ik

1814
01:14:10,679 --> 01:14:12,840
denk niet dat we arrogant kunnen zijn

1815
01:14:12,840 --> 01:14:15,060
en zeggen oh nee, ik ga het doen  beter via

1816
01:14:15,060 --> 01:14:17,640
een puur voorspellende coderingsmanierstructuren,

1817
01:14:17,640 --> 01:14:18,920


1818
01:14:18,920 --> 01:14:21,480
maar zal toch nog steeds Transformers benaderen

1819
01:14:21,480 --> 01:14:22,500


1820
01:14:22,500 --> 01:14:24,420
sorry je zei dat structuurleren

1821
01:14:24,420 --> 01:14:27,540
de Transformer-benadering zou benaderen

1822
01:14:27,540 --> 01:14:29,219
ja het structuurleren dat ik

1823
01:14:29,219 --> 01:14:32,640
eerder noemde in uh wanneer wanneer iemand uh

1824
01:14:32,640 --> 01:14:34,800
de overeenkomsten tussen creatieve codering

1825
01:14:34,800 --> 01:14:38,060
en het aandachtsmechanisme vraagt

1826
01:14:38,280 --> 01:14:41,699
interessant

1827
01:14:41,699 --> 01:14:42,900
eh

1828
01:14:42,900 --> 01:14:45,719
één ding vraag ik me af van Amazon Ik

1829
01:14:45,719 --> 01:14:47,640
kon het concept van diepte niet zien in

1830
01:14:47,640 --> 01:14:49,380
de voorspellende coderingsnetwerken die je

1831
01:14:49,380 --> 01:14:50,880
noemde hoogstwaarschijnlijk heb ik het gemist de

1832
01:14:50,880 --> 01:14:52,380
definitie voor voorspellende

1833
01:14:52,380 --> 01:14:56,480
codering betrof het concept van diepte

1834
01:14:56,640 --> 01:14:59,460
wat bedoelde je met diepte

1835
01:14:59,460 --> 01:15:02,219
nee ja het is waar  het is uh omdat de

1836
01:15:02,219 --> 01:15:04,980
standaarddefinitie, zoals ik meerdere

1837
01:15:04,980 --> 01:15:06,960
keren heb gezegd, a is hiërarchisch, je hebt

1838
01:15:06,960 --> 01:15:08,400
voorspellingen die in één richting gaan, een

1839
01:15:08,400 --> 01:15:09,719
voorspellingsfout die de tegenovergestelde richting uitgaat,

1840
01:15:09,719 --> 01:15:10,620


1841
01:15:10,620 --> 01:15:14,340
eigenlijk wat uh wat we deden in dit

1842
01:15:14,340 --> 01:15:16,320
artikel en ook in de laatste in uh

1843
01:15:16,320 --> 01:15:18,420
die wordt genoemd  Het leren over

1844
01:15:18,420 --> 01:15:19,920
willekeurige grafiektopologieën die we hebben

1845
01:15:19,920 --> 01:15:22,260
relatieve codering is dat we diepte kunnen beschouwen

1846
01:15:22,260 --> 01:15:25,620
als een

1847
01:15:25,620 --> 01:15:28,380
als onafhankelijk uh

1848
01:15:28,380 --> 01:15:31,380
eigenlijk paar latente variabele latente

1849
01:15:31,380 --> 01:15:33,239
variabele en pijl

1850
01:15:33,239 --> 01:15:34,739
en je hebt voorspellingen die in die

1851
01:15:34,739 --> 01:15:36,300
richting gaan en voorspelling Pijl gaat

1852
01:15:36,300 --> 01:15:38,340
met de andere maar dan jij  kan

1853
01:15:38,340 --> 01:15:41,880
deze samenstellen op hoeveel um veel manieren, dus

1854
01:15:41,880 --> 01:15:45,239
je kunt, dus in principe hoeft deze

1855
01:15:45,239 --> 01:15:47,040
compositie

1856
01:15:47,040 --> 01:15:48,659
uiteindelijk niet hiërarchisch te zijn, kan

1857
01:15:48,659 --> 01:15:50,820
cycli hebben, dus dan kun je

1858
01:15:50,820 --> 01:15:53,520
bijvoorbeeld een andere, uh, een andere

1859
01:15:53,520 --> 01:15:55,440
latente variabele aansluiten op de eerste  de ene en

1860
01:15:55,440 --> 01:15:57,540
verbind dan de andere met en je kunt

1861
01:15:57,540 --> 01:15:59,340
een structuur hebben die zo verstrengeld is

1862
01:15:59,340 --> 01:16:00,420
als je wilt,

1863
01:16:00,420 --> 01:16:02,699
dus bijvoorbeeld in de andere paper

1864
01:16:02,699 --> 01:16:04,500
trainen we de

1865
01:16:04,500 --> 01:16:06,659
uh een netwerk dat de vorm heeft van een

1866
01:16:06,659 --> 01:16:08,460
hersenstructuur, dus we hebben veel

1867
01:16:08,460 --> 01:16:09,900
hersengebieden die van

1868
01:16:09,900 --> 01:16:12,239
binnen dun verbonden zijn en gedeeltelijk met

1869
01:16:12,239 --> 01:16:13,860
elkaar verbonden zijn

1870
01:16:13,860 --> 01:16:15,719
en er is niets

1871
01:16:15,719 --> 01:16:17,640
hiërarchisch aan het einde, maar je

1872
01:16:17,640 --> 01:16:18,960
kunt het nog steeds trainen door de

1873
01:16:18,960 --> 01:16:20,699
operationele vrije energie te minimaliseren en door

1874
01:16:20,699 --> 01:16:22,620
de totale

1875
01:16:22,620 --> 01:16:25,159
voorspellingsfout van het netwerk te minimaliseren,

1876
01:16:25,159 --> 01:16:27,360
zodat je zou kunnen hebben

1877
01:16:27,360 --> 01:16:31,980
voor een bepaald motief in een verstrengelde grafiek

1878
01:16:31,980 --> 01:16:35,159
zou je drie opeenvolgende lagen kunnen zien

1879
01:16:35,159 --> 01:16:37,560
waarvan je, als je ze alleen zou bekijken, zou

1880
01:16:37,560 --> 01:16:38,820
zeggen oh dat is een gebouw met drie verdiepingen

1881
01:16:38,820 --> 01:16:41,940
dat is een model met drie lagen dat

1882
01:16:41,940 --> 01:16:43,980
adaptieve drie zegt, maar als je

1883
01:16:43,980 --> 01:16:46,620
daar een groter plaatje neemt  is niet zoals een

1884
01:16:46,620 --> 01:16:50,280
expliciete top of een expliciete bodem voor

1885
01:16:50,280 --> 01:16:52,140
dat netwerk,

1886
01:16:52,140 --> 01:16:54,360
ja precies en dit wordt in feite gegeven

1887
01:16:54,360 --> 01:16:55,980
door het feit dat elke bewerking

1888
01:16:55,980 --> 01:16:58,080
in voorspellende Koreaanse netwerken uh is, is

1889
01:16:58,080 --> 01:16:59,460
strikt lokaal,

1890
01:16:59,460 --> 01:17:01,739
dus eigenlijk passeert elk bericht

1891
01:17:01,739 --> 01:17:03,000
elke voorspelling en elke

1892
01:17:03,000 --> 01:17:05,280
voorspellingsfout  dat je stuurt, je stuurt het alleen naar

1893
01:17:05,280 --> 01:17:08,280
de zeer nabije neuronen oké en of

1894
01:17:08,280 --> 01:17:10,380
de globale structuur eigenlijk

1895
01:17:10,380 --> 01:17:13,380
hiërarchisch is of niet, het enkele

1896
01:17:13,380 --> 01:17:16,940
bericht dat wordt doorgegeven, ziet niet eens dat

1897
01:17:17,460 --> 01:17:19,620
ik denk dat dat een soort van

1898
01:17:19,620 --> 01:17:22,820
hoop is voor het leren van nieuwe

1899
01:17:22,820 --> 01:17:27,739
modelarchitecturen is de ruimte van  wat van

1900
01:17:27,739 --> 01:17:33,300
bovenaf is ontworpen, is erg klein en er zijn

1901
01:17:33,300 --> 01:17:36,480
tegenwoordig veel modellen in gebruik, hoewel het

1902
01:17:36,480 --> 01:17:38,640
supereffectieve modellen zijn

1903
01:17:38,640 --> 01:17:41,100
eh, hoewel je zou kunnen vragen of het effectief is per

1904
01:17:41,100 --> 01:17:43,320
rekeneenheid of niet, dat is een vraag op het tweede

1905
01:17:43,320 --> 01:17:45,300
niveau, maar veel effectieve

1906
01:17:45,300 --> 01:17:47,580
modellen hebben tegenwoordig niet een aantal van deze

1907
01:17:47,580 --> 01:17:49,860
eigenschappen van voorspellende coderingsnetwerken,

1908
01:17:49,860 --> 01:17:52,739
zoals hun vermogen om

1909
01:17:52,739 --> 01:17:55,520
alleen lokale berekeningen te gebruiken,

1910
01:17:55,520 --> 01:17:59,400
wat biologisch realisme

1911
01:17:59,400 --> 01:18:02,880
um of alleen spatio-temporeel realisme geeft, maar

1912
01:18:02,880 --> 01:18:06,060
ook veel voordelen kan bieden,

1913
01:18:06,060 --> 01:18:08,159
zoals Federated Compute of gedistribueerde

1914
01:18:08,159 --> 01:18:10,500
computerinstellingen

1915
01:18:10,500 --> 01:18:12,780
nee ja precies daar ben ik het helemaal mee eens

1916
01:18:12,780 --> 01:18:14,520
omdat ik  denk dat het idee in het algemeen is

1917
01:18:14,520 --> 01:18:16,679
dat en ik weet niet of dat

1918
01:18:16,679 --> 01:18:18,540
een voordeel zal zijn, dus ik denk dat het

1919
01:18:18,540 --> 01:18:20,159
veelbelovend is, precies om de redenen die je

1920
01:18:20,159 --> 01:18:20,880
zei

1921
01:18:20,880 --> 01:18:22,920
en uh en de reden is dat

1922
01:18:22,920 --> 01:18:25,380
je de modelreeks van vandaag met back-propagatie

1923
01:18:25,380 --> 01:18:28,860
in principe kunt samenvatten  ze als een

1924
01:18:28,860 --> 01:18:32,040
als een monitoring back-propagatie is een

1925
01:18:32,040 --> 01:18:34,080
functie, omdat je in feite een

1926
01:18:34,080 --> 01:18:36,120
kaart hebt van input naar output en back-

1927
01:18:36,120 --> 01:18:39,600
propagatie verspreidt in feite uh-

1928
01:18:39,600 --> 01:18:41,699
informatie terug van de computationele

1929
01:18:41,699 --> 01:18:44,340
grafiek, dus elk neuraal

1930
01:18:44,340 --> 01:18:45,960
netwerkmodel dat tegenwoordig wordt gebruikt,

1931
01:18:45,960 --> 01:18:48,960
is een functie terwijl voorspellende codering

1932
01:18:48,960 --> 01:18:51,179
en nog een bevrijdende codering zoals de

1933
01:18:51,179 --> 01:18:53,820
oude klasse van functies die de klasse van

1934
01:18:53,820 --> 01:18:56,040
methoden die trainen in het gebruik van lokale

1935
01:18:56,040 --> 01:18:58,500
berekeningen en die feitelijk werken door

1936
01:18:58,500 --> 01:19:01,620
een globale energiefunctie te minimaliseren, ze

1937
01:19:01,620 --> 01:19:03,840
zijn niet beperkt tot het modelleren van functies

1938
01:19:03,840 --> 01:19:05,940
van invoer naar uitvoer, ze modelleren eigenlijk

1939
01:19:05,940 --> 01:19:07,739
iets dat dat  lijkt een beetje op

1940
01:19:07,739 --> 01:19:10,080
fysieke systemen, dus je hebt een fysiek

1941
01:19:10,080 --> 01:19:13,500
systeem, je fixeert een aantal waarden op de

1942
01:19:13,500 --> 01:19:15,360
invoer die je hebt en je laat het

1943
01:19:15,360 --> 01:19:17,280
systeem convergeren en dan lees je een

1944
01:19:17,280 --> 01:19:19,980
andere waarde van uh neuronen of variabelen

1945
01:19:19,980 --> 01:19:21,960
die verondersteld worden te worden uitgevoerd, maar dit

1946
01:19:21,960 --> 01:19:24,120
fysieke systeem doet dat niet  het hoeft geen fit

1947
01:19:24,120 --> 01:19:25,920
forward map te zijn hoeft geen

1948
01:19:25,920 --> 01:19:28,260
functie te zijn die een invoerruimte en

1949
01:19:28,260 --> 01:19:30,659
een uitvoerruimte heeft en dat is het,

1950
01:19:30,659 --> 01:19:32,580
dus de klasse van modellen die je kunt

1951
01:19:32,580 --> 01:19:34,800
leren is uh dus eigenlijk kun je zien

1952
01:19:34,800 --> 01:19:37,560
als feed forward-modellen  en functies

1953
01:19:37,560 --> 01:19:39,600
en dan een veel grotere klasse, namelijk

1954
01:19:39,600 --> 01:19:41,880
die van fysieke systemen, of er

1955
01:19:41,880 --> 01:19:43,860
hier iets interessants is, ik

1956
01:19:43,860 --> 01:19:45,659
weet het nog niet, want de functies

1957
01:19:45,659 --> 01:19:47,460
werken buitengewoon goed, we zien

1958
01:19:47,460 --> 01:19:50,040
die dagen met back-propagatie is een

1959
01:19:50,040 --> 01:19:52,199
ze werken waanzinnig goed, maar

1960
01:19:52,199 --> 01:19:53,460
dus ja  Ik weet niet of er iets

1961
01:19:53,460 --> 01:19:56,040
interessants in het grote deel zit, maar het grootste

1962
01:19:56,040 --> 01:19:58,380
deel is vrij groot, oké, er zijn

1963
01:19:58,380 --> 01:20:00,480
veel modellen die je niet

1964
01:20:00,480 --> 01:20:02,940
kunt reproduceren en je

1965
01:20:02,940 --> 01:20:04,679
kunt trainen met creatieve codering

1966
01:20:04,679 --> 01:20:06,659
of een badkamerpropagatie of andere

1967
01:20:06,659 --> 01:20:07,860
methoden

1968
01:20:07,860 --> 01:20:10,440
dat is superinteressant zeker

1969
01:20:10,440 --> 01:20:12,719
biologische systemen fysieke systemen

1970
01:20:12,719 --> 01:20:15,900
lossen allerlei interessante problemen op

1971
01:20:15,900 --> 01:20:17,100
eh

1972
01:20:17,100 --> 01:20:19,380
maar er is nog steeds geen gratis lunch

1973
01:20:19,380 --> 01:20:21,540
bij mierensoorten die het echt goed doen in

1974
01:20:21,540 --> 01:20:23,100
deze omgeving doen het misschien niet zo goed

1975
01:20:23,100 --> 01:20:25,679
in een andere omgeving en dus eh

1976
01:20:25,679 --> 01:20:28,260
daarbuiten in het achterland  er

1977
01:20:28,260 --> 01:20:31,820
kunnen een aantal echt unieke speciale

1978
01:20:31,820 --> 01:20:35,880
algoritmen zijn die niet goed worden beschreven

1979
01:20:35,880 --> 01:20:38,460
door een functie te zijn,

1980
01:20:38,460 --> 01:20:42,060
maar toch een procedurele

1981
01:20:42,060 --> 01:20:46,679
manier bieden om heuristieken te implementeren

1982
01:20:46,679 --> 01:20:48,840
die buitengewoon extreem effectief kunnen zijn

1983
01:20:48,840 --> 01:20:51,120


1984
01:20:51,120 --> 01:20:53,880
nee ja ja precies en uh ja alles

1985
01:20:53,880 --> 01:20:55,679
waar ik me het meest op heb

1986
01:20:55,679 --> 01:20:58,260
gericht  van onderzoek tijdens mijn doctoraat, bijvoorbeeld

1987
01:20:58,260 --> 01:20:59,100


1988
01:20:59,100 --> 01:21:01,380
zoals het vinden van deze applicatie die is

1989
01:21:01,380 --> 01:21:04,199
zoals hier en niet binnen de

1990
01:21:04,199 --> 01:21:06,739
functies

1991
01:21:07,199 --> 01:21:08,820
koel goed

1992
01:21:08,820 --> 01:21:12,120
waar gaat dit werk vanaf hier naartoe, zoals in

1993
01:21:12,120 --> 01:21:14,520
welke richtingen ben je enthousiast

1994
01:21:14,520 --> 01:21:17,340
en hoe zie je dat mensen in het actieve

1995
01:21:17,340 --> 01:21:19,679
inferentie-ecosysteem krijgen  betrokken bij

1996
01:21:19,679 --> 01:21:22,460
dit soort werk,

1997
01:21:22,500 --> 01:21:24,840
denk ik dat een zeer waarschijnlijk de meest

1998
01:21:24,840 --> 01:21:27,780
veelbelovende richting is, wat een

1999
01:21:27,780 --> 01:21:30,060
uh zij is iets dat ik misschien

2000
01:21:30,060 --> 01:21:33,060
een beetje zou willen onderzoeken, zoals ik al zei,

2001
01:21:33,060 --> 01:21:34,980
er is echt om achter statische

2002
01:21:34,980 --> 01:21:37,380
modellen te gaan, dus alles wat ik heb laten zien  Ik heb

2003
01:21:37,380 --> 01:21:40,260
tot nu toe laten zien dat het gaat om statische gegevens,

2004
01:21:40,260 --> 01:21:42,840
dus de gegevens veranderen niet in de loop van de tijd.

2005
01:21:42,840 --> 01:21:45,780
Er zit geen tijd in de definitie van

2006
01:21:45,780 --> 01:21:48,000
creatieve codering zoals die is zoals ik

2007
01:21:48,000 --> 01:21:49,080
die hier heb gepresenteerd,

2008
01:21:49,080 --> 01:21:50,940
maar je kunt bijvoorbeeld

2009
01:21:50,940 --> 01:21:53,280
creatieve codering generaliseren om te werken  met temporele

2010
01:21:53,280 --> 01:21:55,800
gegevens met behulp van gegeneraliseerde coördinaten, zoals

2011
01:21:55,800 --> 01:21:58,800
je eerder al zei, door

2012
01:21:58,800 --> 01:22:01,380
het te presenteren als een gemeenschappelijk Kalman-

2013
01:22:01,380 --> 01:22:04,140
filter generatief model

2014
01:22:04,140 --> 01:22:08,040
en dat is waar bijvoorbeeld de

2015
01:22:08,040 --> 01:22:09,900
richting van de causale gevolgtrekking erg nuttig zou kunnen zijn,

2016
01:22:09,900 --> 01:22:12,600
want ja, dat model uh op

2017
01:22:12,600 --> 01:22:14,400
dat moment kun je misschien  in staat zijn om

2018
01:22:14,400 --> 01:22:17,880
grotere causaliteit en uh en

2019
01:22:17,880 --> 01:22:21,780
complexere en bruikbare uh

2020
01:22:21,780 --> 01:22:24,780
dynamische oorzaak van modellen te modelleren, in feite

2021
01:22:24,780 --> 01:22:26,940
omdat in het algemeen de do calculus

2022
01:22:26,940 --> 01:22:28,560
en de interventionele en

2023
01:22:28,560 --> 01:22:32,760
contrafeitelijke uh tak van wetenschap

2024
01:22:32,760 --> 01:22:36,000
meestal wordt ontwikkeld op kleine modellen,

2025
01:22:36,000 --> 01:22:38,159
dus het is

2026
01:22:38,159 --> 01:22:40,739
alsof je niet doet '  t doen interventies op

2027
01:22:40,739 --> 01:22:43,560
gigantische modellen in het algemeen, dus als je

2028
01:22:43,560 --> 01:22:45,800
naar medische gegevens kijkt, gebruiken ze

2029
01:22:45,800 --> 01:22:50,159
relatief kleine visienetwerken en

2030
01:22:50,159 --> 01:22:51,179
als je natuurlijk een

2031
01:22:51,179 --> 01:22:54,900
dynamisch causaal model wilt hebben dat een

2032
01:22:54,900 --> 01:22:56,340
specifieke omgeving of een specifieke

2033
01:22:56,340 --> 01:22:58,620
realiteit modelleert, heb je een  veel neuronen in

2034
01:22:58,620 --> 01:23:00,780
je hebben veel latente variabelen, ze

2035
01:23:00,780 --> 01:23:02,580
veranderen in de loop van de tijd en een interventie op

2036
01:23:02,580 --> 01:23:05,219
wat meer op een bepaald moment creëert een

2037
01:23:05,219 --> 01:23:07,560
effect in een andere tijdstap, dus misschien

2038
01:23:07,560 --> 01:23:09,239
in de volgende tijdstap in 10 verschillende

2039
01:23:09,239 --> 01:23:11,699
tijdstappen later en ik denk dat dat zou

2040
01:23:11,699 --> 01:23:14,100
heel interessant zijn om te ontwikkelen als een

2041
01:23:14,100 --> 01:23:16,380
biologisch plausibele manier om informatie door te geven

2042
01:23:16,380 --> 01:23:17,699


2043
01:23:17,699 --> 01:23:20,040
die ook

2044
01:23:20,040 --> 01:23:22,860
Grandeur-causaliteit kan modelleren, eigenlijk

2045
01:23:22,860 --> 01:23:24,659
hmm

2046
01:23:24,659 --> 01:23:29,659
waar zie je actie in deze modellen

2047
01:23:30,840 --> 01:23:33,840
waar zie ik actie

2048
01:23:33,840 --> 01:23:36,480
Ik dacht daar niet aan

2049
01:23:36,480 --> 01:23:38,760
ik denk als de acties in die  modellen

2050
01:23:38,760 --> 01:23:41,460
misschien op dezelfde manier als ik zoals je in

2051
01:23:41,460 --> 01:23:43,080
andere modellen ziet, omdat

2052
01:23:43,080 --> 01:23:44,940
creatieve codering in feite een model van perceptie is,

2053
01:23:44,940 --> 01:23:46,260


2054
01:23:46,260 --> 01:23:49,260
dus een actie is dat je kunt zien dat er een

2055
01:23:49,260 --> 01:23:52,739
gevolg is van wat je ervaart,

2056
01:23:52,739 --> 01:23:55,159
dus door de manier waarop je

2057
01:23:55,159 --> 01:23:57,840
iets ervaart te veranderen, dan jij  kan

2058
01:23:57,840 --> 01:24:00,060
berekenen, misschien kun je gewoon een

2059
01:24:00,060 --> 01:24:01,800
slimmere actie uitvoeren nu je meer informatie hebt,

2060
01:24:01,800 --> 01:24:03,000


2061
01:24:03,000 --> 01:24:04,560
maar

2062
01:24:04,560 --> 01:24:06,960
ja, ik denk niet dat actie erg

2063
01:24:06,960 --> 01:24:10,199
gemakkelijk is, zoals ja, ik zie geen expliciete

2064
01:24:10,199 --> 01:24:12,540
consequentie van acties behalve het feit

2065
01:24:12,540 --> 01:24:14,040
dat je hierdoor in principe kunt

2066
01:24:14,040 --> 01:24:15,960
misschien

2067
01:24:15,960 --> 01:24:18,780
trek je gewoon betere conclusies voor hen

2068
01:24:18,780 --> 01:24:21,719
acties uitvoeren in de toekomst

2069
01:24:21,719 --> 01:24:23,940
Ik zal daar een paar manieren aan toevoegen waarop

2070
01:24:23,940 --> 01:24:25,920
mensen uh hebben gesproken over voorspellende

2071
01:24:25,920 --> 01:24:29,340
codering en actie uh ten eerste interne

2072
01:24:29,340 --> 01:24:33,960
actie of geheime actie is aandacht zodat

2073
01:24:33,960 --> 01:24:36,120
we kunnen nadenken over perceptie  als een

2074
01:24:36,120 --> 01:24:37,980
interne actie is dat een benadering een

2075
01:24:37,980 --> 01:24:40,560
andere benadering vrij micro is de

2076
01:24:40,560 --> 01:24:42,840
output van een bepaald knooppunt, we kunnen

2077
01:24:42,840 --> 01:24:45,780
dat knooppunt begrijpen als een bepaald

2078
01:24:45,780 --> 01:24:48,780
ding met zijn eigen sensorische cognitieve en

2079
01:24:48,780 --> 01:24:52,080
actiestaten en dus in die zin

2080
01:24:52,080 --> 01:24:54,960
de output van een knooppunt en dan als laatste

2081
01:24:54,960 --> 01:24:57,179
welke  we verkenden een klein beetje in

2082
01:24:57,179 --> 01:24:59,940
livestream 43 over de theoretische recensie over

2083
01:24:59,940 --> 01:25:02,100
voorspellende codering die we helemaal doorlezen

2084
01:25:02,100 --> 01:25:03,840
en het ging allemaal om

2085
01:25:03,840 --> 01:25:05,460
perceptie, alles om perceptie en toen

2086
01:25:05,460 --> 01:25:08,040
was het zoals sectie 5.3

2087
01:25:08,040 --> 01:25:11,719
als je verwachtingen hebt over actie,

2088
01:25:11,719 --> 01:25:15,900
dan is actie gewoon  een andere variabele in

2089
01:25:15,900 --> 01:25:18,120
deze architectuur en dat is echt

2090
01:25:18,120 --> 01:25:20,040
afgestemd op inactieve gevolgtrekking, waarbij

2091
01:25:20,040 --> 01:25:21,659


2092
01:25:21,659 --> 01:25:24,000
we in plaats van een beloning of nutsfunctie te hebben die we maximaliseren,

2093
01:25:24,000 --> 01:25:26,699
actie selecteren op basis van het feit dat dit de

2094
01:25:26,699 --> 01:25:28,800
meest waarschijnlijke manier van handelen is, het pad van de

2095
01:25:28,800 --> 01:25:30,900
minste actie, dat is Bayesiaanse mechanica

2096
01:25:30,900 --> 01:25:33,300
en dus is het eigenlijk heel  natuurlijk om

2097
01:25:33,300 --> 01:25:36,420
een ​​actievariabele in te brengen en

2098
01:25:36,420 --> 01:25:40,800
deze in wezen te gebruiken alsof het een

2099
01:25:40,800 --> 01:25:43,260
voorspelling is over iets anders

2100
01:25:43,260 --> 01:25:45,540
extra ontvankelijk in de wereld omdat

2101
01:25:45,540 --> 01:25:48,480
we ook actie verwachten

2102
01:25:48,480 --> 01:25:50,820
nee ja ja precies

2103
01:25:50,820 --> 01:25:52,860
nee Ik vind de manier om acties te definiëren

2104
01:25:52,860 --> 01:25:55,260
eigenlijk heel leuk en  uh en ik denk nog steeds dat er

2105
01:25:55,260 --> 01:25:57,239
bijvoorbeeld niet

2106
01:25:57,239 --> 01:26:01,139
zo veel artikelen zijn die deze methode toepassen. Ik

2107
01:26:01,139 --> 01:26:03,239
denk dat er een paar zijn van uh van

2108
01:26:03,239 --> 01:26:05,100
Alexander of robria doet iets

2109
01:26:05,100 --> 01:26:08,580
soortgelijks, maar in de praktijk zoals buiten

2110
01:26:08,580 --> 01:26:10,920
de pure actieve gevolgtrekking zoals het toepassen van

2111
01:26:10,920 --> 01:26:13,260
voorspellende codering  en acties om

2112
01:26:13,260 --> 01:26:15,980
praktische problemen op te lossen is niet

2113
01:26:15,980 --> 01:26:19,280
veel onderzocht

2114
01:26:19,679 --> 01:26:23,400
nou ja bedankt voor deze uitstekende

2115
01:26:23,400 --> 01:26:25,199
presentatie en discussie is er nog

2116
01:26:25,199 --> 01:26:27,659
iets dat je wilt zeggen of

2117
01:26:27,659 --> 01:26:30,300
of Wijs mensen op

2118
01:26:30,300 --> 01:26:33,360
uh nee gewoon heel erg bedankt voor de

2119
01:26:33,360 --> 01:26:34,620
uitnodiging  en uh

2120
01:26:34,620 --> 01:26:36,120
het was echt leuk en ik hoop

2121
01:26:36,120 --> 01:26:38,460
op een gegeven moment terug te komen voor wat Future

2122
01:26:38,460 --> 01:26:40,199
Works

2123
01:26:40,199 --> 01:26:41,580
cool,

2124
01:26:41,580 --> 01:26:45,000
altijd en overal bedankt Thomas dus

2125
01:26:45,000 --> 01:26:49,820
bedankt Daniel tot ziens tot ziens


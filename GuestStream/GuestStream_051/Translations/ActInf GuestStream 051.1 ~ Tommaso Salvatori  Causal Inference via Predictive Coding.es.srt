1
00:00:19,020 --> 00:00:21,119
hola y bienvenido,

2
00:00:21,119 --> 00:00:23,400
es la transmisión de invitados de inferencia activa

3
00:00:23,400 --> 00:00:28,140
número 51.1 el 28 de julio de 2023,

4
00:00:28,140 --> 00:00:31,439
estamos aquí con Tomaso Salvatore y

5
00:00:31,439 --> 00:00:33,840
tendremos una presentación y una

6
00:00:33,840 --> 00:00:37,020
discusión sobre el trabajo reciente de

7
00:00:37,020 --> 00:00:39,660
inferencia causal a través de la codificación predictiva, así que

8
00:00:39,660 --> 00:00:42,480
muchas gracias por unirse para aquellos que

9
00:00:42,480 --> 00:00:44,340
están viendo  en vivo siéntete libre de escribir

10
00:00:44,340 --> 00:00:47,520
preguntas en el chat en vivo y listo gracias

11
00:00:47,520 --> 00:00:50,399


12
00:00:50,399 --> 00:00:52,980
muchas gracias Daniel por

13
00:00:52,980 --> 00:00:56,039
invitarme uh siempre he sido un gran admirador del

14
00:00:56,039 --> 00:00:57,719
canal y he estado viendo muchos

15
00:00:57,719 --> 00:00:58,920
videos

16
00:00:58,920 --> 00:01:01,379
soy bastante  emocionado de estar aquí y

17
00:01:01,379 --> 00:01:04,260
ser el que habla esta vez,

18
00:01:04,260 --> 00:01:06,600
así que voy a hablar sobre estos

19
00:01:06,600 --> 00:01:08,700
preprints recientes que publiqué, que ha

20
00:01:08,700 --> 00:01:11,159
sido el trabajo de los últimos meses

21
00:01:11,159 --> 00:01:12,119


22
00:01:12,119 --> 00:01:15,900
y es una colaboración con la

23
00:01:15,900 --> 00:01:18,659
búsqueda  en Ketty estoy en makarak

24
00:01:18,659 --> 00:01:21,600
barami Leyenda Thomas lukasiavich

25
00:01:21,600 --> 00:01:24,000
y es básicamente un trabajo conjunto entre

26
00:01:24,000 --> 00:01:26,299
versos, que es la empresa en la que trabajo para

27
00:01:26,299 --> 00:01:31,680
la Universidad de Oxford y uhuvian,

28
00:01:31,680 --> 00:01:34,200
así que

29
00:01:34,200 --> 00:01:36,299
durante esta charla,

30
00:01:36,299 --> 00:01:38,220
este será

31
00:01:38,220 --> 00:01:40,979
básicamente el esquema de la charla

32
00:01:40,979 --> 00:01:43,140
sobre la que comenzaré a hablar.  qué

33
00:01:43,140 --> 00:01:44,520
es la codificación predictiva

34
00:01:44,520 --> 00:01:47,659
y las interacciones dadas de qué es

35
00:01:47,659 --> 00:01:51,299
una breve introducción histórica por qué

36
00:01:51,299 --> 00:01:54,060
cree que es importante estudiar la

37
00:01:54,060 --> 00:01:56,159
codificación creativa incluso, por ejemplo,

38
00:01:56,159 --> 00:01:58,619
desde la perspectiva del aprendizaje automático.

39
00:01:58,619 --> 00:02:00,720
Luego proporcionaré una

40
00:02:00,720 --> 00:02:04,560
pequeña introducción sobre qué es la inferencia causal

41
00:02:04,560 --> 00:02:07,200
y  y una vez que tengamos toda esa

42
00:02:07,200 --> 00:02:08,880
información junta,

43
00:02:08,880 --> 00:02:12,540
discutiré por qué escribí este artículo, cuál

44
00:02:12,540 --> 00:02:14,520
fue básicamente la pregunta de investigación que me

45
00:02:14,520 --> 00:02:16,560
inspiró a mí y a los otros

46
00:02:16,560 --> 00:02:18,300
colaboradores,

47
00:02:18,300 --> 00:02:21,660
y presentaré los resultados principales,

48
00:02:21,660 --> 00:02:24,980
que son cómo realizar

49
00:02:24,980 --> 00:02:27,480
inferencia, intervención e

50
00:02:27,480 --> 00:02:29,340
inferencia contrafáctica

51
00:02:29,340 --> 00:02:33,319
y  cómo aprender las estructuras causales

52
00:02:33,319 --> 00:02:35,879
de un conjunto de datos dado usando

53
00:02:35,879 --> 00:02:37,920
codificación predictiva y luego, por supuesto,

54
00:02:37,920 --> 00:02:39,959
concluiré con un

55
00:02:39,959 --> 00:02:43,500
pequeño resumen y una discusión sobre

56
00:02:43,500 --> 00:02:45,840
por qué creo que este trabajo puede ser de hecho

57
00:02:45,840 --> 00:02:49,940
impactante y algunas direcciones futuras,

58
00:02:50,700 --> 00:02:53,400
entonces, ¿qué es la

59
00:02:53,400 --> 00:02:55,680
codificación creativa?  es en general famoso por

60
00:02:55,680 --> 00:02:58,440
ser un método de aprendizaje inspirado en la neurociencia,

61
00:02:58,440 --> 00:03:01,140
entonces, ¿qué teoría de cómo

62
00:03:01,140 --> 00:03:04,560
funciona el procesamiento de la información en el cerebro

63
00:03:04,560 --> 00:03:05,819
y,

64
00:03:05,819 --> 00:03:08,400
hablando muy formalmente, el nivel de

65
00:03:08,400 --> 00:03:10,560
codificación creativa se puede describir

66
00:03:10,560 --> 00:03:12,659
básicamente como que tiene una

67
00:03:12,659 --> 00:03:16,319
estructura jerárquica de neuronas en el cerebro

68
00:03:16,319 --> 00:03:19,080
y tú?  tenemos dos familias diferentes de

69
00:03:19,080 --> 00:03:20,700
neuronas en el cerebro,

70
00:03:20,700 --> 00:03:23,280
la primera familia es la que se

71
00:03:23,280 --> 00:03:24,480
encarga de enviar

72
00:03:24,480 --> 00:03:27,659
información de predicción para que las neuronas en un

73
00:03:27,659 --> 00:03:29,959
nivel específico de la jerarquía envíen información

74
00:03:29,959 --> 00:03:33,959
y predigan la actividad del

75
00:03:33,959 --> 00:03:35,940
nivel inferior

76
00:03:35,940 --> 00:03:38,340
y la segunda familia de  neurona es la

77
00:03:38,340 --> 00:03:41,099
de las neuronas de error y las neuronas de flecha

78
00:03:41,099 --> 00:03:43,019
envían información de error de predicción

79
00:03:43,019 --> 00:03:46,319
hacia arriba en la jerarquía, por lo que un nivel predice

80
00:03:46,319 --> 00:03:49,200
la actividad del nivel por debajo de

81
00:03:49,200 --> 00:03:51,659
esta actividad tiene algo de esta predicción

82
00:03:51,659 --> 00:03:54,239
como un desajuste que en realidad

83
00:03:54,239 --> 00:03:56,220
sucedería en el nivel por debajo

84
00:03:56,220 --> 00:03:57,840
e información sobre  el

85
00:03:57,840 --> 00:04:02,400
error de predicción se envía hacia arriba en la tecla de flecha,

86
00:04:02,400 --> 00:04:04,860
sin embargo, la codificación predictiva

87
00:04:04,860 --> 00:04:07,220
en realidad no se quemó como una

88
00:04:07,220 --> 00:04:10,799
neurociencia como una teoría de las

89
00:04:10,799 --> 00:04:11,939
neurociencias,

90
00:04:11,939 --> 00:04:13,860
pero en realidad se desarrolló inicialmente

91
00:04:13,860 --> 00:04:16,139
como un método para el procesamiento y la

92
00:04:16,139 --> 00:04:19,380
compresión de señales en los años 50, por lo que el

93
00:04:19,380 --> 00:04:21,899
trabajo de  Oliver Elias, que en realidad son

94
00:04:21,899 --> 00:04:25,020
contemporáneos de uh clothes Shannon of

95
00:04:25,020 --> 00:04:26,160
Shannon,

96
00:04:26,160 --> 00:04:27,960
se dieron cuenta de que una vez que tenemos un

97
00:04:27,960 --> 00:04:30,900
predictor, un modelo que funciona

98
00:04:30,900 --> 00:04:33,600
es bueno para predecir datos y

99
00:04:33,600 --> 00:04:36,000
enviar mensajes sobre el error en

100
00:04:36,000 --> 00:04:37,919
esas predicciones es mucho más

101
00:04:37,919 --> 00:04:41,100
barato que enviar el mensaje completo

102
00:04:41,100 --> 00:04:42,720
cada  tiempo

103
00:04:42,720 --> 00:04:45,240
y así es como nació la codificación bonita,

104
00:04:45,240 --> 00:04:47,639


105
00:04:47,639 --> 00:04:49,500
como un mecanismo de procesamiento y compresión de señales

106
00:04:49,500 --> 00:04:52,020
en la teoría de la información en

107
00:04:52,020 --> 00:04:53,639
los años 50, en

108
00:04:53,639 --> 00:04:57,120
realidad fue en los 80, eh, que se

109
00:04:57,120 --> 00:04:59,400
convirtió en que se usó exactamente el mismo modelo

110
00:04:59,400 --> 00:05:01,800
en eh,

111
00:05:01,800 --> 00:05:03,540
en neurociencia

112
00:05:03,540 --> 00:05:07,500
y eh  Entonces, con el trabajo de Mumford u

113
00:05:07,500 --> 00:05:10,440
otros trabajos, por ejemplo, explique

114
00:05:10,440 --> 00:05:12,960
cómo la calificación de la información del proceso para que

115
00:05:12,960 --> 00:05:14,520
obtengamos señales de predicción del

116
00:05:14,520 --> 00:05:17,160
mundo exterior y necesitamos comprimir

117
00:05:17,160 --> 00:05:20,280
esta representación y tener esta

118
00:05:20,280 --> 00:05:22,740
representación interna en nuestras neuronas

119
00:05:22,740 --> 00:05:25,199
y el método.  es muy similar, si no

120
00:05:25,199 --> 00:05:27,720
equivalente, al que se usó, que

121
00:05:27,720 --> 00:05:30,419
fue desarrollado por Elias y Oliver en

122
00:05:30,419 --> 00:05:32,900
los años 50,

123
00:05:32,940 --> 00:05:35,520
quizás cuál sea el cambio de paradigma más grande que

124
00:05:35,520 --> 00:05:38,100
sucedió en 1999

125
00:05:38,100 --> 00:05:41,400
gracias al trabajo de Ballard

126
00:05:41,400 --> 00:05:44,880
en el que introdujeron

127
00:05:44,880 --> 00:05:46,199
este concepto que yo  mencionado anteriormente

128
00:05:46,199 --> 00:05:48,060
sobre estructuras jerárquicas en el

129
00:05:48,060 --> 00:05:51,360
cerebro donde la información de predicción es de

130
00:05:51,360 --> 00:05:54,240
arriba hacia abajo y la información de error es de abajo hacia arriba

131
00:05:54,240 --> 00:05:55,560


132
00:05:55,560 --> 00:05:57,660
y algo que hicieron que no se había

133
00:05:57,660 --> 00:05:59,759
hecho antes es que

134
00:05:59,759 --> 00:06:02,820
explican y desarrollan esta teoría

135
00:06:02,820 --> 00:06:05,759
no solo en Francia sino solo pero

136
00:06:05,759 --> 00:06:07,979
también  sobre cómo funciona el aprendizaje en el

137
00:06:07,979 --> 00:06:10,139
cerebro, por lo que también es una teoría sobre cómo

138
00:06:10,139 --> 00:06:13,139
se actualizan nuestras sinapsis

139
00:06:13,139 --> 00:06:16,080
y el último gran avance del que voy a

140
00:06:16,080 --> 00:06:17,940
hablar en esta breve

141
00:06:17,940 --> 00:06:21,900
introducción histórica es de 2003, pero

142
00:06:21,900 --> 00:06:25,380
es eh, luego siguió adelante en el

143
00:06:25,380 --> 00:06:28,380
años después, gracias a Car Freeston, en el

144
00:06:28,380 --> 00:06:32,100
que básicamente tomó la teoría de

145
00:06:32,100 --> 00:06:35,039
Robin Ballard y la desarrolló,

146
00:06:35,039 --> 00:06:38,400
la extendió y la generalizó a

147
00:06:38,400 --> 00:06:40,919
la teoría de los modelos generativos, así que

148
00:06:40,919 --> 00:06:42,720
básicamente la afirmación principal que

149
00:06:42,720 --> 00:06:45,479
hizo Carfiston es que la codificación creativa es

150
00:06:45,479 --> 00:06:48,780
una  Esquema de maximización de evidencia de

151
00:06:48,780 --> 00:06:50,340
un tipo específico de modelo generativo

152
00:06:50,340 --> 00:06:52,979
que voy a presentar

153
00:06:52,979 --> 00:06:55,139
más adelante también

154
00:06:55,139 --> 00:07:00,300
para hacer un breve resumen en los

155
00:07:00,300 --> 00:07:01,560
primeros dos

156
00:07:01,560 --> 00:07:03,900
tipos de Rincón creativo que

157
00:07:03,900 --> 00:07:05,340
describí para procesamiento y

158
00:07:05,340 --> 00:07:07,020
compresión de señales y el  el

159
00:07:07,020 --> 00:07:09,180
procesamiento de la información en la retina y en el

160
00:07:09,180 --> 00:07:11,160
cerebro en general son métodos de inferencia

161
00:07:11,160 --> 00:07:12,300


162
00:07:12,300 --> 00:07:14,819
y el cambio más grande, la

163
00:07:14,819 --> 00:07:17,819
revolución más grande que

164
00:07:17,819 --> 00:07:21,120
tuvimos en 1999, digamos que en el

165
00:07:21,120 --> 00:07:23,580
siglo XXI, la codificación operativa se vio

166
00:07:23,580 --> 00:07:25,919
como un algoritmo de aprendizaje para que primero podamos

167
00:07:25,919 --> 00:07:29,699
comprimir información.  y luego actualizar todas

168
00:07:29,699 --> 00:07:31,800
las sinapsis o todas las variables latentes

169
00:07:31,800 --> 00:07:34,139
que tenemos en nuestro modelo generativo para

170
00:07:34,139 --> 00:07:38,599
mejorar nuestro modelo generativo en sí mismo,

171
00:07:38,759 --> 00:07:43,199
así que vamos a dar algunas definiciones

172
00:07:43,199 --> 00:07:45,000
que son un poco más formales

173
00:07:45,000 --> 00:07:48,479
para que la codificación operativa pueda verse como un

174
00:07:48,479 --> 00:07:50,220
generativo gaussiano jerárquico  modelo

175
00:07:50,220 --> 00:07:53,400
así que aquí hay una figura muy simple en la que

176
00:07:53,400 --> 00:07:54,780
tenemos esta estructura jerárquica

177
00:07:54,780 --> 00:07:58,319
que puede ser tan profunda como queramos

178
00:07:58,319 --> 00:08:01,560
y las señales de predicción de la señal van

179
00:08:01,560 --> 00:08:04,620
de una variable latente XM a la

180
00:08:04,620 --> 00:08:06,599
siguiente y se transforma

181
00:08:06,599 --> 00:08:09,720
cada vez a través de la función GN

182
00:08:09,720 --> 00:08:12,620
o GI

183
00:08:15,319 --> 00:08:18,180
este es un modelo generativo como dije y

184
00:08:18,180 --> 00:08:19,680
cuál es la probabilidad marginal de este

185
00:08:19,680 --> 00:08:21,780
modelo generativo, bueno, es simplemente la

186
00:08:21,780 --> 00:08:24,960
probabilidad del último, ¿

187
00:08:24,960 --> 00:08:27,660
puedes ver mi cursor?

188
00:08:27,660 --> 00:08:29,940


189
00:08:29,940 --> 00:08:32,700


190
00:08:32,700 --> 00:08:34,979
del último vértice multiplicado por

191
00:08:34,979 --> 00:08:37,140
la distribución de probabilidad de todos

192
00:08:37,140 --> 00:08:40,440
los demás vértices condicionada a la actividad

193
00:08:40,440 --> 00:08:43,020
del vértice anterior o de la

194
00:08:43,020 --> 00:08:45,860
variable latente anterior

195
00:08:45,899 --> 00:08:48,240
Ya dije que es un

196
00:08:48,240 --> 00:08:50,399
modelo generativo gaussiano, lo que significa que esas

197
00:08:50,399 --> 00:08:54,260
probabilidades están en forma gaussiana

198
00:08:54,660 --> 00:08:57,120
y cada

199
00:08:57,120 --> 00:09:00,480
función endos  función G en general y

200
00:09:00,480 --> 00:09:02,880
especialmente porque, por ejemplo, en el

201
00:09:02,880 --> 00:09:05,459
artículo de Rambler y en todos los artículos que

202
00:09:05,459 --> 00:09:07,920
vinieron después, también debido a la

203
00:09:07,920 --> 00:09:10,500
revolución del aprendizaje profundo, esas funciones son

204
00:09:10,500 --> 00:09:13,220
simplemente mapas lineales o

205
00:09:13,220 --> 00:09:15,120
mapas no lineales con

206
00:09:15,120 --> 00:09:18,000
funciones de activación o mapas no lineales con

207
00:09:18,000 --> 00:09:22,040
función de activación y un  sesgo aditivo

208
00:09:23,220 --> 00:09:27,180
para que podamos dar una

209
00:09:27,180 --> 00:09:28,860
definición formal de codificación creativa y podemos

210
00:09:28,860 --> 00:09:30,300
decir que la codificación operativa es un

211
00:09:30,300 --> 00:09:33,480
esquema de inversión para un modelo generativo de este tipo donde la

212
00:09:33,480 --> 00:09:35,839
evidencia de su modelo se maximiza al

213
00:09:35,839 --> 00:09:38,760
minimizar una cantidad que se denomina

214
00:09:38,760 --> 00:09:40,920
variación de energía libre

215
00:09:40,920 --> 00:09:43,740
en general  El objetivo de cada

216
00:09:43,740 --> 00:09:46,019
modelo generativo es maximizar la evidencia del modelo, pero

217
00:09:46,019 --> 00:09:48,860
esta cantidad siempre es intratable y

218
00:09:48,860 --> 00:09:51,019
tenemos algunas

219
00:09:51,019 --> 00:09:53,279
técnicas que nos permiten

220
00:09:53,279 --> 00:09:55,980
aproximarnos a la solución y a la

221
00:09:55,980 --> 00:09:58,500
que usamos en la codificación creativa

222
00:09:58,500 --> 00:10:00,720
en lugar de minimizar la aberración de

223
00:10:00,720 --> 00:10:03,480
energía libre, que es una  que es un límite inferior

224
00:10:03,480 --> 00:10:06,839
de la evidencia del modelo en este trabajo y, de

225
00:10:06,839 --> 00:10:09,660
hecho, en muchos

226
00:10:09,660 --> 00:10:11,700
otros, por lo que es la forma estándar de hacerlo.

227
00:10:11,700 --> 00:10:13,740
Esta minimización se realiza.

228
00:10:13,740 --> 00:10:16,080


229
00:10:16,080 --> 00:10:18,540


230
00:10:18,540 --> 00:10:19,980
en realidad, otros métodos,

231
00:10:19,980 --> 00:10:22,140
como la maximización de expectativas, que

232
00:10:22,140 --> 00:10:23,580
a menudo es equivalente,

233
00:10:23,580 --> 00:10:25,140
o puede usar otros

234
00:10:25,140 --> 00:10:26,940
algoritmos de paso de mensajes, como la

235
00:10:26,940 --> 00:10:29,959
propagación de creencias, por ejemplo,

236
00:10:30,720 --> 00:10:33,980
y retroceder un poco en el tiempo, así que

237
00:10:33,980 --> 00:10:35,940
olvidémonos un poco de los

238
00:10:35,940 --> 00:10:38,760
modelos generativos estadísticos

239
00:10:38,760 --> 00:10:41,360
si podemos ver la codificación creativa.

240
00:10:41,360 --> 00:10:44,040
como quiero decir, ya dije un par de

241
00:10:44,040 --> 00:10:46,200
veces como un modelo jerárquico

242
00:10:46,200 --> 00:10:48,420
con las actividades neuronales, por lo que con las

243
00:10:48,420 --> 00:10:50,700
variables latentes de las neuronas que representan las

244
00:10:50,700 --> 00:10:53,459
actividades neuronales, el remitente señala hacia abajo en

245
00:10:53,459 --> 00:10:54,899
la jerarquía

246
00:10:54,899 --> 00:10:57,540
y con los nodos de error o las neuronas de error,

247
00:10:57,540 --> 00:11:01,019
el remitente señala hacia arriba en la jerarquía, así que

248
00:11:01,019 --> 00:11:03,660
esto y  la información del error de vuelta

249
00:11:03,660 --> 00:11:05,700
cuál es la variación de la energía libre de

250
00:11:05,700 --> 00:11:08,220
esta clase de modelos de codificación operados es

251
00:11:08,220 --> 00:11:09,899
simplemente la suma

252
00:11:09,899 --> 00:11:12,720
del error cuadrático medio de todas las

253
00:11:12,720 --> 00:11:14,399
neuronas de error

254
00:11:14,399 --> 00:11:18,120
entonces es la suma del error

255
00:11:18,120 --> 00:11:21,980
del error total al cuadrado

256
00:11:22,019 --> 00:11:24,480
y esta representación es  será

257
00:11:24,480 --> 00:11:27,120
útil en las diapositivas posteriores y en

258
00:11:27,120 --> 00:11:28,740
cómo voy a explicar cómo usar la

259
00:11:28,740 --> 00:11:30,120
codificación creativa para modelar la

260
00:11:30,120 --> 00:11:32,940
inferencia causal, por ejemplo,

261
00:11:32,940 --> 00:11:34,800
creo que la codificación predictiva es importante

262
00:11:34,800 --> 00:11:36,240
y no es un buen algoritmo para

263
00:11:36,240 --> 00:11:37,500
estudiar

264
00:11:37,500 --> 00:11:39,600
bien en primer lugar como  Dije antes que

265
00:11:39,600 --> 00:11:41,399
optimiza el objetivo correcto que es

266
00:11:41,399 --> 00:11:43,079
la evidencia del modelo o la probabilidad marginal

267
00:11:43,079 --> 00:11:44,339


268
00:11:44,339 --> 00:11:45,660
y

269
00:11:45,660 --> 00:11:47,700
luego lo hace optimizando un

270
00:11:47,700 --> 00:11:49,440
límite inferior que se llama la variación de la

271
00:11:49,440 --> 00:11:52,440
energía libre como dije y el

272
00:11:52,440 --> 00:11:54,240
final virtual es interesante porque se puede

273
00:11:54,240 --> 00:11:57,680
escribir como  suma de dos términos diferentes

274
00:11:57,680 --> 00:12:00,839
que son y cada uno de esos términos de

275
00:12:00,839 --> 00:12:04,680
optimización tiene un impacto importante,

276
00:12:04,680 --> 00:12:06,899
por ejemplo, en tareas de aprendizaje automático

277
00:12:06,899 --> 00:12:09,060
o, en general, en tareas de aprendizaje,

278
00:12:09,060 --> 00:12:12,420
por lo que uno de esos términos obliga a la memorización,

279
00:12:12,420 --> 00:12:15,440
por lo que en el segundo término básicamente dice

280
00:12:15,440 --> 00:12:18,180
fuerzas el modelo  para ajustarse a un conjunto de datos específico

281
00:12:18,180 --> 00:12:19,560


282
00:12:19,560 --> 00:12:21,240
y el primer término

283
00:12:21,240 --> 00:12:23,519
obliga al modelo a minimizar la

284
00:12:23,519 --> 00:12:26,040
complejidad y, como sabemos, por ejemplo, de la

285
00:12:26,040 --> 00:12:28,500


286
00:12:28,500 --> 00:12:31,260
Teoría de la cuchilla de resultados si tenemos dos modelos diferentes

287
00:12:31,260 --> 00:12:33,000
que funcionan de manera similar en un

288
00:12:33,000 --> 00:12:35,640
conjunto de entrenamiento específico el que tenemos  obtener

289
00:12:35,640 --> 00:12:37,380
y el que se espera que

290
00:12:37,380 --> 00:12:39,899
generalice uh más es el menos

291
00:12:39,899 --> 00:12:41,160
complejo,

292
00:12:41,160 --> 00:12:44,100
por lo que actualizar un modelo generativo a través de la

293
00:12:44,100 --> 00:12:46,380
energía libre operativa nos permite

294
00:12:46,380 --> 00:12:47,779
básicamente

295
00:12:47,779 --> 00:12:51,959
converger al modelo de afeitar de resultado óptimo uh

296
00:12:51,959 --> 00:12:54,720
que es el que memoriza un

297
00:12:54,720 --> 00:12:56,100
conjunto de datos pero también es  capaz de

298
00:12:56,100 --> 00:12:58,680
generalizar muy bien en puntos de datos invisibles invisibles

299
00:12:58,680 --> 00:13:00,240


300
00:13:00,240 --> 00:13:02,639
una segunda razón de por qué la codificación operativa

301
00:13:02,639 --> 00:13:08,600
es importante es que en realidad

302
00:13:08,720 --> 00:13:11,760
no tiene que definirse en una

303
00:13:11,760 --> 00:13:13,920
estructura jerárquica, pero se puede

304
00:13:13,920 --> 00:13:15,959
modelar en arquitecturas más complejas y flexibles

305
00:13:15,959 --> 00:13:18,240
como gráficos dirigidos

306
00:13:18,240 --> 00:13:21,540
modelo con cualquier forma o generalizado aún

307
00:13:21,540 --> 00:13:23,700
más a redes con muchos ciclos

308
00:13:23,700 --> 00:13:25,920
que se asemejan a la región del cerebro y el

309
00:13:25,920 --> 00:13:27,779
resultado final en la razón subyacente

310
00:13:27,779 --> 00:13:30,300
es que no está aprendiendo y

311
00:13:30,300 --> 00:13:32,339
prediciendo con un pase hacia adelante y luego hacia

312
00:13:32,339 --> 00:13:34,260
atrás propagando el error pero está

313
00:13:34,260 --> 00:13:36,600
minimizar una función de energía

314
00:13:36,600 --> 00:13:38,459
y esto permite básicamente que todo tipo de

315
00:13:38,459 --> 00:13:39,839
jerarquía sea

316
00:13:39,839 --> 00:13:41,180
uh

317
00:13:41,180 --> 00:13:43,860
permite ir detrás de teclas directas y

318
00:13:43,860 --> 00:13:46,860
permite aprender ciclos y esto es

319
00:13:46,860 --> 00:13:48,060
bastante importante porque el

320
00:13:48,060 --> 00:13:50,399
cerebro está lleno de ciclos ya que tenemos

321
00:13:50,399 --> 00:13:53,399
información de algunos artículos recientes

322
00:13:53,399 --> 00:13:56,459
uh, eso se logró mapear completamente

323
00:13:56,459 --> 00:13:59,279
el cerebro de algunos animales, como la

324
00:13:59,279 --> 00:14:00,420
mosca de la fruta,

325
00:14:00,420 --> 00:14:03,899
el cerebro está lleno de ciclos, por lo que tiene

326
00:14:03,899 --> 00:14:06,720
sentido drenar nuestros modelos de aprendizaje automático

327
00:14:06,720 --> 00:14:09,000
o

328
00:14:09,000 --> 00:14:11,160
nuestros modelos en general con un algoritmo

329
00:14:11,160 --> 00:14:14,160
que nos permite drenar usando

330
00:14:14,160 --> 00:14:17,160
cíclico  estructuras

331
00:14:17,160 --> 00:14:19,380
La tercera razón por la que la codificación operativa es

332
00:14:19,380 --> 00:14:21,240
interesante es que se ha

333
00:14:21,240 --> 00:14:23,820
demostrado formalmente que es más robusta que la

334
00:14:23,820 --> 00:14:25,139
red neuronal estándar que comienza con la

335
00:14:25,139 --> 00:14:27,060
propagación negra, por lo que si tiene una

336
00:14:27,060 --> 00:14:28,200
red neuronal y desea realizar

337
00:14:28,200 --> 00:14:30,320
tareas de clasificación, la

338
00:14:30,320 --> 00:14:34,139
codificación creativa es más robusta

339
00:14:34,139 --> 00:14:36,260
y  esto es

340
00:14:36,260 --> 00:14:38,339
interesante en tareas como el

341
00:14:38,339 --> 00:14:40,680
entrenamiento de aprendizaje en línea en pequeños conjuntos de datos o

342
00:14:40,680 --> 00:14:43,440
tareas de aprendizaje continuo y la teoría

343
00:14:43,440 --> 00:14:45,540
proviene básicamente del hecho de que la

344
00:14:45,540 --> 00:14:48,540
codificación imperativa se ha movido para

345
00:14:48,540 --> 00:14:50,820
aproximar el descenso de gradiente implícito

346
00:14:50,820 --> 00:14:53,339
que es una versión diferente del

347
00:14:53,339 --> 00:14:54,899
descenso de gradiente explícito que es  el

348
00:14:54,899 --> 00:14:57,180
descenso verde estándar utilizado

349
00:14:57,180 --> 00:14:59,880
en todos los modelos básicamente

350
00:14:59,880 --> 00:15:03,680
y es una variación que es más robusta.

351
00:15:05,880 --> 00:15:08,279
Creo que está bien. Hice una codificación intraoperativa bastante larga.

352
00:15:08,279 --> 00:15:09,779
Creo que ahora estoy pasando

353
00:15:09,779 --> 00:15:11,639
al segundo tema, que es la inferencia causal

354
00:15:11,639 --> 00:15:13,019


355
00:15:13,019 --> 00:15:15,839
y el qué es.  inferencia causal la inferencia causal

356
00:15:15,839 --> 00:15:18,420
es una teoría es una

357
00:15:18,420 --> 00:15:20,339
teoría muy general que ha sido

358
00:15:20,339 --> 00:15:23,100
más formalizada por Judy Vestuario definitivamente es

359
00:15:23,100 --> 00:15:25,500
la persona más importante en el

360
00:15:25,500 --> 00:15:27,839
campo de la causalidad en Francia escribió algunos

361
00:15:27,839 --> 00:15:29,760
libros muy buenos, por ejemplo, el libro de

362
00:15:29,760 --> 00:15:32,760
Y es muy recomendable  si desea

363
00:15:32,760 --> 00:15:35,220
obtener más información sobre este tema

364
00:15:35,220 --> 00:15:37,800
y básicamente aborda el siguiente

365
00:15:37,800 --> 00:15:38,639
problema,

366
00:15:38,639 --> 00:15:40,440
supongamos que tenemos una

367
00:15:40,440 --> 00:15:42,000
distribución de probabilidad conjunta que está

368
00:15:42,000 --> 00:15:44,160
asociada con una red bayesiana, este

369
00:15:44,160 --> 00:15:46,199
será un poco el

370
00:15:46,199 --> 00:15:49,260
ejemplo continuo en todo el documento,

371
00:15:49,260 --> 00:15:51,839
especialmente cuando  no estás con

372
00:15:51,839 --> 00:15:54,480
redes asiáticas de esta forma,

373
00:15:54,480 --> 00:15:57,660
se basó en redes, las

374
00:15:57,660 --> 00:16:00,240
variables dentro pueden representar

375
00:16:00,240 --> 00:16:02,100
diferentes cantidades, por lo que, por ejemplo, nuestra

376
00:16:02,100 --> 00:16:04,620
red visual con esta forma puede

377
00:16:04,620 --> 00:16:06,899
representar

378
00:16:06,899 --> 00:16:08,820
las cantidades a la derecha, por lo que la

379
00:16:08,820 --> 00:16:10,800
estatua social económica de Studio de un

380
00:16:10,800 --> 00:16:13,079
individuo es  nivel de educación su

381
00:16:13,079 --> 00:16:16,699
inteligencia y su nivel de ingresos

382
00:16:17,100 --> 00:16:19,440
algo en lo que la estadística clásica es

383
00:16:19,440 --> 00:16:22,920
muy buena y es uh mientras que uh la

384
00:16:22,920 --> 00:16:25,320
aplicación más utilizada es modelar

385
00:16:25,320 --> 00:16:28,019
observaciones o correlaciones una

386
00:16:28,019 --> 00:16:29,279
correlación básicamente responde a la

387
00:16:29,279 --> 00:16:32,519
pregunta qué es si observamos

388
00:16:32,519 --> 00:16:35,579
otra variable C,

389
00:16:35,579 --> 00:16:37,500
por ejemplo en  este caso, ¿

390
00:16:37,500 --> 00:16:39,660
cuál es el nivel de ingresos?

391
00:16:39,660 --> 00:16:41,820


392
00:16:41,820 --> 00:16:44,339


393
00:16:44,339 --> 00:16:48,180


394
00:16:48,180 --> 00:16:50,220


395
00:16:50,220 --> 00:16:52,500


396
00:16:52,500 --> 00:16:54,360
un nivel de ingresos más alto

397
00:16:54,360 --> 00:16:56,040


398
00:16:56,040 --> 00:16:58,139
y esto es una correlación;

399
00:16:58,139 --> 00:17:00,300
sin embargo, a veces hay cosas que

400
00:17:00,300 --> 00:17:03,300
son muy difíciles de observar, pero juegan un

401
00:17:03,300 --> 00:17:05,040
papel muy importante en la determinación de esas

402
00:17:05,040 --> 00:17:06,119
cantidades,

403
00:17:06,119 --> 00:17:08,220
por ejemplo, podría ser que el

404
00:17:08,220 --> 00:17:11,160
nivel de ingresos esté mucho más

405
00:17:11,160 --> 00:17:13,380
definido por la inteligencia de un

406
00:17:13,380 --> 00:17:15,540
persona específica

407
00:17:15,540 --> 00:17:18,720
y tal vez que la inteligencia o

408
00:17:18,720 --> 00:17:21,000
si una persona es inteligente también es más

409
00:17:21,000 --> 00:17:24,540
probable que tenga un nivel de educación superior,

410
00:17:24,540 --> 00:17:27,540
pero aún así la verdadera razón por la cual

411
00:17:27,540 --> 00:17:30,120
el ingreso es I es por el coeficiente

412
00:17:30,120 --> 00:17:32,220
intelectual

413
00:17:32,220 --> 00:17:34,740
y esto puede ser esto no puede  be Estudios

414
00:17:34,740 --> 00:17:36,360
simplemente por correlaciones y tiene que ser

415
00:17:36,360 --> 00:17:39,120
estudiado por una técnica más avanzada

416
00:17:39,120 --> 00:17:41,280
que se llama intervención

417
00:17:41,280 --> 00:17:43,320
una intervención básicamente responde a la

418
00:17:43,320 --> 00:17:46,500
pregunta es qué es D si cambiamos C a

419
00:17:46,500 --> 00:17:48,240
un valor específico

420
00:17:48,240 --> 00:17:51,000
para que, por ejemplo, podamos tomar un

421
00:17:51,000 --> 00:17:54,660
individuo  y verifique su nivel de ingresos

422
00:17:54,660 --> 00:17:57,120
y luego cambie su nivel de educación, así que

423
00:17:57,120 --> 00:17:59,220
intervenga en este mundo

424
00:17:59,220 --> 00:18:01,080
y cambie su nivel de educación sin

425
00:18:01,080 --> 00:18:03,419
tocar su inteligencia y vea cuánto

426
00:18:03,419 --> 00:18:07,260
cambia su ingreso,

427
00:18:07,260 --> 00:18:09,900
por ejemplo, si el ingreso cambia mucho,

428
00:18:09,900 --> 00:18:12,179
significa que la inteligencia

429
00:18:12,179 --> 00:18:14,460
no cambia  No juega un papel importante en esto, pero el

430
00:18:14,460 --> 00:18:16,799
nivel de educación sí lo hace. Si el nivel de ingresos

431
00:18:16,799 --> 00:18:19,020
no cambia mucho, significa que tal vez

432
00:18:19,020 --> 00:18:20,640
haya una variable oculta. En este caso,

433
00:18:20,640 --> 00:18:22,860
la inteligencia que determina el

434
00:18:22,860 --> 00:18:25,760
nivel de ingresos de una persona.

435
00:18:25,980 --> 00:18:28,740
La tercera inferencia causal importante

436
00:18:28,740 --> 00:18:31,080
es que  de contrafactuales,

437
00:18:31,080 --> 00:18:33,120
por ejemplo, un contrafactual responde a la

438
00:18:33,120 --> 00:18:36,720
pregunta cuál sería y cambiamos

439
00:18:36,720 --> 00:18:39,240
C a un valor diferente en el pasado,

440
00:18:39,240 --> 00:18:40,679
por ejemplo, podemos ver que la

441
00:18:40,679 --> 00:18:42,059
diferencia entre intervenciones y

442
00:18:42,059 --> 00:18:45,059
contrafactuales es que las intervenciones

443
00:18:45,059 --> 00:18:47,820
actúan en el futuro, así que estoy entrevistando  en

444
00:18:47,820 --> 00:18:50,340
el mundo ahora para observar un cambio en el

445
00:18:50,340 --> 00:18:53,220
futuro bien contrafáctico nos permite

446
00:18:53,220 --> 00:18:56,039
retroceder en el tiempo y cambiar una variable

447
00:18:56,039 --> 00:18:59,160
en el tiempo y ver cómo ese cambio

448
00:18:59,160 --> 00:19:01,320
habría influido en el mundo en el que vivimos

449
00:19:01,320 --> 00:19:02,940
ahora

450
00:19:02,940 --> 00:19:06,299
y estos son definidos por judapple como los

451
00:19:06,299 --> 00:19:08,100
tres  niveles de inferencia causal

452
00:19:08,100 --> 00:19:09,660
correlación es el primer nivel

453
00:19:09,660 --> 00:19:11,580
intervención es el segundo nivel en

454
00:19:11,580 --> 00:19:14,720
contrafactual es el tercer nivel

455
00:19:16,020 --> 00:19:18,120
otras intervenciones Voy a

456
00:19:18,120 --> 00:19:20,640
definirlas más formalmente ahora que di

457
00:19:20,640 --> 00:19:23,760
una definición intuitiva y estoy

458
00:19:23,760 --> 00:19:25,500
usando esta notación aquí  que es lo

459
00:19:25,500 --> 00:19:27,240
mismo en realidad a lo largo de toda la

460
00:19:27,240 --> 00:19:29,640
presentación, por lo que X siempre será

461
00:19:29,640 --> 00:19:32,820
una variable latente s i siempre

462
00:19:32,820 --> 00:19:35,340
será un punto de datos o una observación

463
00:19:35,340 --> 00:19:38,520
y VI siempre será un vértice, por lo que

464
00:19:38,520 --> 00:19:40,860
cada vez que vea VI solo estamos

465
00:19:40,860 --> 00:19:42,720
interesado en la estructura del gráfico,

466
00:19:42,720 --> 00:19:45,299
por ejemplo,

467
00:19:45,299 --> 00:19:46,860
supongamos que tenemos un modelo bayesiano

468
00:19:46,860 --> 00:19:50,160
que tiene la misma estructura

469
00:19:50,160 --> 00:19:52,679
que el modelo bayesiano que vimos en la

470
00:19:52,679 --> 00:19:54,780
diapositiva anterior,

471
00:19:54,780 --> 00:19:57,840
dado que X3 es igual a S3, esta es la

472
00:19:57,840 --> 00:20:00,660
observación que hacemos que las estadísticas permitan

473
00:20:00,660 --> 00:20:03,360
Para calcular la probabilidad o la

474
00:20:03,360 --> 00:20:04,679
expectativa

475
00:20:04,679 --> 00:20:07,380
de X4, que es la variable latente

476
00:20:07,380 --> 00:20:09,240
relacionada con este vértice,

477
00:20:09,240 --> 00:20:13,860
dado que X3 es igual a S3, la

478
00:20:13,860 --> 00:20:15,679


479
00:20:15,679 --> 00:20:17,760
intervención externa necesitamos un nuevo tipo de

480
00:20:17,760 --> 00:20:19,919
notación que se llama operación do,

481
00:20:19,919 --> 00:20:21,179


482
00:20:21,179 --> 00:20:23,880
por lo que en este caso

483
00:20:23,880 --> 00:20:26,100
X4 queremos calcular  la probabilidad de

484
00:20:26,100 --> 00:20:30,000
X4 dado el hecho de que intervenimos en

485
00:20:30,000 --> 00:20:33,059
la palabra y cambiamos X3 West 3.

486
00:20:33,059 --> 00:20:35,580
y cómo hacemos esto para realizar una

487
00:20:35,580 --> 00:20:38,400
intervención Judo Pearl nos dice que tenemos que

488
00:20:38,400 --> 00:20:40,020


489
00:20:40,020 --> 00:20:41,880
tener un paso intermedio antes de

490
00:20:41,880 --> 00:20:45,059
Calcular una correlación es al principio

491
00:20:45,059 --> 00:20:46,860
tenemos que eliminar todo para eliminar todos los

492
00:20:46,860 --> 00:20:50,160
bordes entrantes a V3,

493
00:20:50,160 --> 00:20:52,799
por lo que tenemos que estudiar no esta

494
00:20:52,799 --> 00:20:55,679
red bayesiana sino esta segunda

495
00:20:55,679 --> 00:20:58,200
y luego, en este punto, podemos

496
00:20:58,200 --> 00:21:00,840
calcular una correlación como lo

497
00:21:00,840 --> 00:21:03,299
hacemos normalmente

498
00:21:03,299 --> 00:21:06,500
y esta es una intervención

499
00:21:07,020 --> 00:21:09,299
contrafactual  es una generalización de

500
00:21:09,299 --> 00:21:11,700
esto que, como dije, vivió en el pasado

501
00:21:11,700 --> 00:21:14,100
y están computando usando

502
00:21:14,100 --> 00:21:15,419
modelos causales estructurales,

503
00:21:15,419 --> 00:21:18,299
un modelo causal estructural es una tupla

504
00:21:18,299 --> 00:21:21,120
que es conceptualmente similar a una

505
00:21:21,120 --> 00:21:23,460
red bayesiana, pero básicamente tenemos

506
00:21:23,460 --> 00:21:26,220
esta nueva clase de variables encima que

507
00:21:26,220 --> 00:21:28,580
son las variables no observables que usan,

508
00:21:28,580 --> 00:21:30,960
por lo que tenemos la red bayesiana que

509
00:21:30,960 --> 00:21:34,020
teníamos antes de X1 X2 X3 S4,

510
00:21:34,020 --> 00:21:37,460
pero también tenemos esas variables no observables o

511
00:21:37,460 --> 00:21:40,020
que dependen del entorno, no

512
00:21:40,020 --> 00:21:42,539
puede controlarlas, puede inferirlas,

513
00:21:42,539 --> 00:21:43,980


514
00:21:43,980 --> 00:21:46,020
pero están ahí

515
00:21:46,020 --> 00:21:48,539
y

516
00:21:48,539 --> 00:21:51,360
f  es un conjunto de funciones que depende de

517
00:21:51,360 --> 00:21:53,400
todo

518
00:21:53,400 --> 00:21:57,299
básicamente f de x de x3 depende de X1

519
00:21:57,299 --> 00:21:58,980
porque tenemos una flecha en x2 porque

520
00:21:58,980 --> 00:22:00,960
tienes una flecha y en la

521
00:22:00,960 --> 00:22:02,940
variable no observable que también

522
00:22:02,940 --> 00:22:05,840
influye extrema,

523
00:22:06,179 --> 00:22:09,240
así que sí intuitivamente puedes vernos

524
00:22:09,240 --> 00:22:11,940
puedes pensar  de un modelo causal estructural

525
00:22:11,940 --> 00:22:14,159
como una red bayesiana con esas

526
00:22:14,159 --> 00:22:16,679
variables no observables en la parte superior y cada

527
00:22:16,679 --> 00:22:19,500
variable no observable solo influye en

528
00:22:19,500 --> 00:22:22,020
su

529
00:22:22,020 --> 00:22:24,600
propia última variable X, por lo que, por ejemplo,

530
00:22:24,600 --> 00:22:27,960
IU nunca tocará X1 también u3 solo

531
00:22:27,960 --> 00:22:30,360
tocará Q3 E1 influirá en

532
00:22:30,360 --> 00:22:34,039
X1 y  y así sucesivamente, por lo que

533
00:22:35,039 --> 00:22:37,679
realizar una inferencia contrafactual

534
00:22:37,679 --> 00:22:39,900
responde a la siguiente pregunta, entonces, ¿cuál

535
00:22:39,900 --> 00:22:42,960
sería X4 en X3 si hubiera sido igual a otra

536
00:22:42,960 --> 00:22:46,620
variable en una situación de aprobación? El

537
00:22:46,620 --> 00:22:49,340
extranjero

538
00:22:49,340 --> 00:22:51,840
requiere tres pasos diferentes, por lo que la

539
00:22:51,840 --> 00:22:53,039
abducción

540
00:22:53,039 --> 00:22:54,900


541
00:22:54,900 --> 00:22:57,179
es el cálculo de todas las

542
00:22:57,179 --> 00:22:59,460
variables de fondo, por lo que en este  en este paso,

543
00:22:59,460 --> 00:23:01,200
queremos retroceder en el tiempo y comprender

544
00:23:01,200 --> 00:23:03,419
cómo era el entorno, el entorno no observable,

545
00:23:03,419 --> 00:23:04,919


546
00:23:04,919 --> 00:23:08,039
en ese momento específico en el tiempo,

547
00:23:08,039 --> 00:23:11,039
y hacemos esto fijando todas las

548
00:23:11,039 --> 00:23:14,280
variables latentes X en algunos datos específicos que

549
00:23:14,280 --> 00:23:16,140
ya tenemos

550
00:23:16,140 --> 00:23:18,960
y realizando estos uh esto

551
00:23:18,960 --> 00:23:21,120
inferencia sobre el usado

552
00:23:21,120 --> 00:23:24,240
entonces vamos a usar la U

553
00:23:24,240 --> 00:23:26,940
para mantener la U que hemos aprendido y

554
00:23:26,940 --> 00:23:28,500
realizar una intervención

555
00:23:28,500 --> 00:23:29,880
para que

556
00:23:29,880 --> 00:23:32,340
una contrafactura también pueda verse como

557
00:23:32,340 --> 00:23:34,980
una intervención en el tiempo en la que

558
00:23:34,980 --> 00:23:36,960
conocemos el entorno las

559
00:23:36,960 --> 00:23:40,620
variables de entorno U1 U2  y u4 en ese momento específico

560
00:23:40,620 --> 00:23:43,039


561
00:23:43,200 --> 00:23:44,340
y

562
00:23:44,340 --> 00:23:46,679
cuál es el paso que falta,

563
00:23:46,679 --> 00:23:49,440
entonces, ¿cuál sería X4 en X3 si hubiera sido igual a

564
00:23:49,440 --> 00:23:50,780
otro

565
00:23:50,780 --> 00:23:53,280
otro punto de datos en esa

566
00:23:53,280 --> 00:23:55,980
situación específica ahora ahora podemos calcular una

567
00:23:55,980 --> 00:23:57,120
correlación

568
00:23:57,120 --> 00:23:59,520
y la correlación la hacemos en el camino

569
00:23:59,520 --> 00:24:02,039
en el gráfico  en el que

570
00:24:02,039 --> 00:24:04,440
ya hemos realizado una intervención utilizando

571
00:24:04,440 --> 00:24:06,659
las variables ambientales que hemos

572
00:24:06,659 --> 00:24:10,140
aprendido en el paso de abducción

573
00:24:10,140 --> 00:24:14,419
y esta es una inferencia contrafáctica

574
00:24:15,480 --> 00:24:18,000
esta es la última diapositiva de la

575
00:24:18,000 --> 00:24:20,159
inferencia causal presente introducción

576
00:24:20,159 --> 00:24:21,720
y trata sobre el aprendizaje estructural

577
00:24:21,720 --> 00:24:23,880
básicamente básicamente todo lo que he dicho

578
00:24:23,880 --> 00:24:27,360
hasta ahora se basa en el hecho de que conocemos

579
00:24:27,360 --> 00:24:29,700
las dependencias causales entre los

580
00:24:29,700 --> 00:24:31,500
puntos de datos, por lo que conocemos la estructura

581
00:24:31,500 --> 00:24:33,120
del gráfico, sabemos qué variable

582
00:24:33,120 --> 00:24:34,860
influye en cuál,

583
00:24:34,860 --> 00:24:37,260
conocemos las flechas en general,

584
00:24:37,260 --> 00:24:39,659
pero en la práctica esto no

585
00:24:39,659 --> 00:24:42,900
siempre es posible, por lo que

586
00:24:42,900 --> 00:24:45,419
no tenemos acceso al gráfico causal

587
00:24:45,419 --> 00:24:47,400
la mayoría de las veces y, de hecho, aprender

588
00:24:47,400 --> 00:24:49,919
el mejor gráfico causal a partir de los datos sigue siendo

589
00:24:49,919 --> 00:24:51,840
un problema abierto. Estamos mejorando en esto. Estamos

590
00:24:51,840 --> 00:24:53,880
mejorando, pero

591
00:24:53,880 --> 00:24:57,299
cómo realizar esta tarea exactamente

592
00:24:57,299 --> 00:24:58,380


593
00:24:58,380 --> 00:25:01,140
sigue siendo un problema abierto.

594
00:25:01,140 --> 00:25:03,179
entonces, como dije, básicamente, el objetivo es

595
00:25:03,179 --> 00:25:04,740
referir las relaciones del Consejo a partir de

596
00:25:04,740 --> 00:25:07,380
datos de observación, por lo que, dado un conjunto de datos,

597
00:25:07,380 --> 00:25:09,780
queremos inferir el gráfico dirigido exactamente

598
00:25:09,780 --> 00:25:12,179
que describe la conectividad

599
00:25:12,179 --> 00:25:14,460
entre el sistema y las variables del

600
00:25:14,460 --> 00:25:15,960
conjunto de datos,

601
00:25:15,960 --> 00:25:17,700
por ejemplo, aquí tenemos un ejemplo

602
00:25:17,700 --> 00:25:19,440
que supongo que

603
00:25:19,440 --> 00:25:22,860
todos estamos familiarizados gracias a

604
00:25:22,860 --> 00:25:25,080
la pandemia, así que tenemos esas cuatro

605
00:25:25,080 --> 00:25:28,799
variables, edad, vacuna, hospitalización

606
00:25:28,799 --> 00:25:31,380
y TC,

607
00:25:31,380 --> 00:25:33,600
y queremos inferir las

608
00:25:33,600 --> 00:25:36,059
dependencias causales entre esas variables,

609
00:25:36,059 --> 00:25:37,980
por ejemplo, queremos aprender directamente

610
00:25:37,980 --> 00:25:40,260
de los datos que la probabilidad  de una

611
00:25:40,260 --> 00:25:43,080
persona hospitalizada depende de

612
00:25:43,080 --> 00:25:45,419
su edad y del hecho de si está

613
00:25:45,419 --> 00:25:49,760
vacunada o no, etc., así que

614
00:25:51,299 --> 00:25:55,020
este es el final de la larga

615
00:25:55,020 --> 00:25:58,080
introducción, pero espero que haya sido lo

616
00:25:58,080 --> 00:26:00,179
suficientemente claro y espero haber dado como el

617
00:26:00,179 --> 00:26:02,039
conceptos básicos para comprender

618
00:26:02,039 --> 00:26:05,159
básicamente los resultados del documento y

619
00:26:05,159 --> 00:26:07,740
ahora podemos ir a las preguntas de investigación,

620
00:26:07,740 --> 00:26:09,059
por lo que las preguntas de investigación son las

621
00:26:09,059 --> 00:26:10,440
siguientes.

622
00:26:10,440 --> 00:26:12,900
Primero, quiero ver

623
00:26:12,900 --> 00:26:15,299
si la codificación creativa se puede usar para

624
00:26:15,299 --> 00:26:16,980
realizar inferencias causales.

625
00:26:16,980 --> 00:26:20,100


626
00:26:20,100 --> 00:26:22,380
realizar para calcular correlaciones

627
00:26:22,380 --> 00:26:25,020
en redes bayesianas

628
00:26:25,020 --> 00:26:27,419
y la gran pregunta es si podemos ir más allá de la

629
00:26:27,419 --> 00:26:29,400
correlación y modelar la intervención y

630
00:26:29,400 --> 00:26:31,679
contrafactual de una manera plausible biológicamente de una

631
00:26:31,679 --> 00:26:32,760


632
00:26:32,760 --> 00:26:34,380


633
00:26:34,380 --> 00:26:36,120
manera que, por ejemplo, sea simple e

634
00:26:36,120 --> 00:26:39,059
intuitiva y nos permita jugar solo con

635
00:26:39,059 --> 00:26:40,740
las neuronas  y no tocar, por ejemplo,

636
00:26:40,740 --> 00:26:43,740
la enorme estructura del gráfico

637
00:26:43,740 --> 00:26:46,380
y Más en la práctica, más específicamente,

638
00:26:46,380 --> 00:26:48,299
la pregunta es: ¿podemos definir un

639
00:26:48,299 --> 00:26:51,000


640
00:26:51,000 --> 00:26:52,740
modelo causal de estructura basado en codificación operativa para realizar intervenciones y

641
00:26:52,740 --> 00:26:55,320
contrafactuales?

642
00:26:55,320 --> 00:26:58,380
La segunda pregunta es,

643
00:26:58,380 --> 00:27:00,179
como dije, tener un modelo personalizado de estructura

644
00:27:00,179 --> 00:27:02,159
supone que nosotros  conocemos la estructura

645
00:27:02,159 --> 00:27:04,260
de la Red de evasión,

646
00:27:04,260 --> 00:27:07,919
por lo que suponemos que tenemos las flechas. ¿

647
00:27:07,919 --> 00:27:09,960
Podemos ir más allá y usar

648
00:27:09,960 --> 00:27:11,520
redes de codificación creativas para aprender la

649
00:27:11,520 --> 00:27:14,418
estructura causal del gráfico?

650
00:27:16,140 --> 00:27:18,900
Básicamente, dar respuestas positivas a

651
00:27:18,900 --> 00:27:21,120
ambas preguntas nos permitiría

652
00:27:21,120 --> 00:27:23,120
usar la codificación predictiva como  un

653
00:27:23,120 --> 00:27:26,039
método de inferencia causal de extremo a extremo que básicamente

654
00:27:26,039 --> 00:27:28,740
toma un conjunto de datos y nos permite probar

655
00:27:28,740 --> 00:27:30,419
intervenciones y

656
00:27:30,419 --> 00:27:34,820
predicciones contrafactuales directamente a partir de este conjunto de datos,

657
00:27:36,840 --> 00:27:39,299
así que abordemos el primero, el

658
00:27:39,299 --> 00:27:40,740
primer problema, la

659
00:27:40,740 --> 00:27:42,419
codificación vibratoria de inferencia causal, que también es la

660
00:27:42,419 --> 00:27:45,120
sección que brinda  Básicamente, el título del

661
00:27:45,120 --> 00:27:46,740
documento

662
00:27:46,740 --> 00:27:48,539
y aquí mostraré cómo realizar la

663
00:27:48,539 --> 00:27:50,760
codificación operativa de correlaciones, que

664
00:27:50,760 --> 00:27:52,440
ya se conoce,

665
00:27:52,440 --> 00:27:54,419
y cómo realizar

666
00:27:54,419 --> 00:27:56,760
consultas intervencionistas, que creo que

667
00:27:56,760 --> 00:28:01,140
es la verdadera pregunta del documento,

668
00:28:01,140 --> 00:28:03,900
así que aquí hay un causal.  gráfico que es el

669
00:28:03,900 --> 00:28:05,700
gráfico habitual

670
00:28:05,700 --> 00:28:07,260
que teníamos

671
00:28:07,260 --> 00:28:09,240
y aquí está el

672
00:28:09,240 --> 00:28:11,760
modelo de codificación creativa correspondiente, por lo que los ejes son las

673
00:28:11,760 --> 00:28:13,980
variables latentes y corresponden a las

674
00:28:13,980 --> 00:28:18,000
neuronas en un modelo de red neuronal

675
00:28:18,000 --> 00:28:20,760
y The Black Arrow que pasa de la

676
00:28:20,760 --> 00:28:22,740
información de predicción de una neurona

677
00:28:22,740 --> 00:28:25,559
al que está abajo en la jerarquía

678
00:28:25,559 --> 00:28:28,500
y cada vértice también tiene esta

679
00:28:28,500 --> 00:28:31,140
neurona de error que pasa información a la

680
00:28:31,140 --> 00:28:32,820
jerarquía, por lo que la información de cada

681
00:28:32,820 --> 00:28:36,480
error va al nodo de valor en

682
00:28:36,480 --> 00:28:39,120
la jerarquía y básicamente

683
00:28:39,120 --> 00:28:41,400
le dice que se corrija para cambiar  la

684
00:28:41,400 --> 00:28:43,760
predicción

685
00:28:44,700 --> 00:28:46,559
para realizar una correlación usando

686
00:28:46,559 --> 00:28:48,840
codificación predictiva, lo que tiene que hacer

687
00:28:48,840 --> 00:28:50,400
es tomar una observación y

688
00:28:50,400 --> 00:28:52,620
simplemente fijar el valor de una neurona específica,

689
00:28:52,620 --> 00:28:53,820


690
00:28:53,820 --> 00:28:55,200
por lo que si desea calcular la

691
00:28:55,200 --> 00:28:58,740
probabilidad de X4 dado que X3 es igual a S3,

692
00:28:58,740 --> 00:29:02,340
simplemente tenemos que  tome X3 y fíjelo en

693
00:29:02,340 --> 00:29:04,380
S3 de manera que ya no cambie

694
00:29:04,380 --> 00:29:08,159
y ejecute una minimización de energía

695
00:29:08,159 --> 00:29:09,720
y este modelo

696
00:29:09,720 --> 00:29:12,659
y al minimizar al actualizar el eje

697
00:29:12,659 --> 00:29:16,380
uh a través de una minimización de la variación

698
00:29:16,380 --> 00:29:18,419
de energía libre permite que el modelo

699
00:29:18,419 --> 00:29:20,820
converja a una solución  a esta pregunta

700
00:29:20,820 --> 00:29:22,919
entonces la probabilidad o el valor esperado

701
00:29:22,919 --> 00:29:27,179
de X4 dado que X3 es igual a 3.

702
00:29:27,179 --> 00:29:29,340
pero ¿cómo realizo una intervención ahora

703
00:29:29,340 --> 00:29:31,679
sin actuar sobre la estructura del

704
00:29:31,679 --> 00:29:33,419
gráfico?

705
00:29:33,419 --> 00:29:35,640
Bueno, esta es básicamente la primera

706
00:29:35,640 --> 00:29:37,679
idea del documento.

707
00:29:37,679 --> 00:29:39,960
realizar una

708
00:29:39,960 --> 00:29:43,260
correlación para fijar S3 igual a X3 es el

709
00:29:43,260 --> 00:29:45,600
primer paso en el algoritmo y el

710
00:29:45,600 --> 00:29:47,220
segundo es obtener el eje

711
00:29:47,220 --> 00:29:50,539
minimizando la variación de energía libre

712
00:29:51,240 --> 00:29:53,340
una intervención que en teoría

713
00:29:53,340 --> 00:29:55,200
corresponde a quitar esas

714
00:29:55,200 --> 00:29:56,220
flechas

715
00:29:56,220 --> 00:29:57,659
y responde a la pregunta la

716
00:29:57,659 --> 00:29:59,279
probabilidad  de X4

717
00:29:59,279 --> 00:30:02,399
al realizar una intervención, entonces X3 es

718
00:30:02,399 --> 00:30:04,860
igual a tres. La codificación imperativa se puede

719
00:30:04,860 --> 00:30:07,080
realizar de la siguiente manera,

720
00:30:07,080 --> 00:30:09,840
así que voy a escribir el algoritmo aquí,

721
00:30:09,840 --> 00:30:13,140
así que primero, como en una correlación, arregla S3

722
00:30:13,140 --> 00:30:17,039
igual al iFix X3 igual a la

723
00:30:17,039 --> 00:30:18,720
observación que obtiene.

724
00:30:18,720 --> 00:30:21,299
entonces este es el paso importante que

725
00:30:21,299 --> 00:30:24,059
tiene que intervenir ya no en el gráfico

726
00:30:24,059 --> 00:30:26,700
sino en el error de predicción y

727
00:30:26,700 --> 00:30:28,980
corregirlo igual a cero

728
00:30:28,980 --> 00:30:31,020
tener un error de predicción igual a cero

729
00:30:31,020 --> 00:30:32,480
básicamente

730
00:30:32,480 --> 00:30:36,179
hace que envíe información sin sentido

731
00:30:36,179 --> 00:30:38,460
a la jerarquía o que en realidad

732
00:30:38,460 --> 00:30:40,200
no envíe información del  jerarquía

733
00:30:40,200 --> 00:30:41,880
porque básicamente te dice que la

734
00:30:41,880 --> 00:30:44,659
predicción siempre es correcta

735
00:30:44,659 --> 00:30:48,120
y el tercer paso es, como hicimos

736
00:30:48,120 --> 00:30:50,220
antes, actualizar el eje, el

737
00:30:50,220 --> 00:30:52,919
eje sin restricciones o X1 X2 X4 al

738
00:30:52,919 --> 00:30:55,679
minimizar la variación de energía libre

739
00:30:55,679 --> 00:30:59,039
como mostraré ahora o experimentalmente

740
00:30:59,039 --> 00:31:00,840
simplemente haciendo  este pequeño truco de

741
00:31:00,840 --> 00:31:02,399
establecer un error de predicción para que sea

742
00:31:02,399 --> 00:31:05,120
igual a cero

743
00:31:05,640 --> 00:31:08,220
nos impide actuar realmente sobre la

744
00:31:08,220 --> 00:31:10,320
estructura del gráfico

745
00:31:10,320 --> 00:31:13,620
como lo hace la teoría del cálculo e

746
00:31:13,620 --> 00:31:16,919
inferir las variables que faltan después de

747
00:31:16,919 --> 00:31:19,140
una intervención simplemente realizando

748
00:31:19,140 --> 00:31:22,640
aberración de la minimización de energía libre ¿

749
00:31:24,659 --> 00:31:26,580
qué pasa con la inferencia contrafactual? la

750
00:31:26,580 --> 00:31:28,080
inferencia contrafactual es realmente

751
00:31:28,080 --> 00:31:30,539
fácil una vez que hemos

752
00:31:30,539 --> 00:31:34,740
definido cómo hacer una intervención

753
00:31:34,740 --> 00:31:36,539
y esto se debe a que, como vimos anteriormente,

754
00:31:36,539 --> 00:31:38,640
realizar un contrafáctico similar

755
00:31:38,640 --> 00:31:40,380
a realizar una intervención en una

756
00:31:40,380 --> 00:31:44,360
situación pasada después de haber inferido el

757
00:31:44,360 --> 00:31:48,120
inobservable las variables no observables,

758
00:31:48,120 --> 00:31:49,620
así que

759
00:31:49,620 --> 00:31:51,480
como puede ver en el gráfico que mostré

760
00:31:51,480 --> 00:31:53,520
anteriormente sobre la acción de abducción y los

761
00:31:53,520 --> 00:31:56,039
pasos de predicción la acción y los

762
00:31:56,039 --> 00:31:58,320
pasos de predicción no tenían esas

763
00:31:58,320 --> 00:31:59,640
dos flechas

764
00:31:59,640 --> 00:32:02,580
que fueron eliminadas bastante codificación

765
00:32:02,580 --> 00:32:06,299
nos permite mantener las flechas de este uh

766
00:32:06,299 --> 00:32:08,279
en  el gráfico

767
00:32:08,279 --> 00:32:11,340
y realizar contrafactuales

768
00:32:11,340 --> 00:32:13,380
simplemente realizando un paso de abducción como se

769
00:32:13,380 --> 00:32:14,640
hizo anteriormente

770
00:32:14,640 --> 00:32:16,679
un paso de acción en el que simplemente

771
00:32:16,679 --> 00:32:18,600
realizamos una intervención en el

772
00:32:18,600 --> 00:32:21,240
nodo único, por lo que fijamos el nodo de valor y establecemos

773
00:32:21,240 --> 00:32:24,240
el error en cero

774
00:32:24,240 --> 00:32:26,399
y ejecutamos la minimización de energía para

775
00:32:26,399 --> 00:32:27,960
minimizar la duración de la energía libre para

776
00:32:27,960 --> 00:32:30,679
calcular la predicción,

777
00:32:32,399 --> 00:32:36,299
así que creo que este es un método fácil y

778
00:32:36,299 --> 00:32:39,840
elegante para realizar intervenciones

779
00:32:39,840 --> 00:32:42,899
y contrafactuales y,

780
00:32:42,899 --> 00:32:44,880
sí, creo que lo que tenemos que

781
00:32:44,880 --> 00:32:46,500
mostrar ahora es si funciona en la práctica

782
00:32:46,500 --> 00:32:48,720
o no y nosotros  tengo un par de

783
00:32:48,720 --> 00:32:49,919
experimentos

784
00:32:49,919 --> 00:32:52,440
y ahora les mostraré dos

785
00:32:52,440 --> 00:32:54,240
experimentos diferentes, el primero es

786
00:32:54,240 --> 00:32:57,179
simplemente un experimento de prueba de concepto

787
00:32:57,179 --> 00:33:01,020
que muestra que en la codificación operativa

788
00:33:01,020 --> 00:33:02,480
es capaz de realizar

789
00:33:02,480 --> 00:33:06,120
intervenciones y contrafactuales

790
00:33:06,120 --> 00:33:08,700
y el segundo en realidad muestra una

791
00:33:08,700 --> 00:33:11,220
aplicación simple  en cómo se

792
00:33:11,220 --> 00:33:13,440
pueden usar las consultas intervencionistas para mejorar el

793
00:33:13,440 --> 00:33:16,260
rendimiento de las tareas de clasificación en un

794
00:33:16,260 --> 00:33:18,360
tipo específico de redes de codificación operativas,

795
00:33:18,360 --> 00:33:20,940
que es la de un

796
00:33:20,940 --> 00:33:22,080
modelo totalmente conectado,

797
00:33:22,080 --> 00:33:24,659
comencemos desde el primero,

798
00:33:24,659 --> 00:33:27,679
entonces, ¿cómo hacemos esta tarea, dado un

799
00:33:27,679 --> 00:33:30,360
modelo de Consejo estructural?

800
00:33:30,360 --> 00:33:33,360
generamos datos de entrenamiento y los usamos

801
00:33:33,360 --> 00:33:35,760
para aprender los pesos para aprender las

802
00:33:35,760 --> 00:33:39,480
funciones de los modelos estructurales de Kaza

803
00:33:39,480 --> 00:33:42,779
y luego generamos datos de prueba

804
00:33:42,779 --> 00:33:44,399
para consultas de intervención y

805
00:33:44,399 --> 00:33:46,080
contrafacción

806
00:33:46,080 --> 00:33:48,000
y mostramos si somos capaces de

807
00:33:48,000 --> 00:33:51,360
converger a los datos de prueba correctos  usando

808
00:33:51,360 --> 00:33:53,340
codificación creativa

809
00:33:53,340 --> 00:33:54,779


810
00:33:54,779 --> 00:33:57,240
y, por ejemplo, aquí uh en esos dos

811
00:33:57,240 --> 00:33:58,860
gráficos representan la

812
00:33:58,860 --> 00:34:00,600
intervención intervencionista y las consultas contrafactuales

813
00:34:00,600 --> 00:34:03,539
de este gráfico específico, que es el

814
00:34:03,539 --> 00:34:05,880
gráfico de sesgo de mariposa, que es un gráfico

815
00:34:05,880 --> 00:34:08,280
que se usa a menudo en uh para probar

816
00:34:08,280 --> 00:34:10,859
si una inferencia causal si

817
00:34:10,859 --> 00:34:12,179
la intervención y las

818
00:34:12,179 --> 00:34:15,540
técnicas contrafactuales  el trabajo es tan simple como eso, pero

819
00:34:15,540 --> 00:34:18,000
en el documento puede encontrar muchos

820
00:34:18,000 --> 00:34:20,760
gráficos diferentes, pero en general esos

821
00:34:20,760 --> 00:34:22,800
dos gráficos esos dos gráficos muestran que el

822
00:34:22,800 --> 00:34:26,940
método funciona muestra que el

823
00:34:26,940 --> 00:34:27,918


824
00:34:27,918 --> 00:34:32,219
error absoluto medio entre las

825
00:34:32,219 --> 00:34:33,960
cantidades contrafácticas intervencionistas

826
00:34:33,960 --> 00:34:37,399
nosotros nosotros  calcular y las cantidades intervencionistas y

827
00:34:37,399 --> 00:34:39,780
contrafactuales del

828
00:34:39,780 --> 00:34:41,460
gráfico original

829
00:34:41,460 --> 00:34:43,800
están cerca entre sí, por lo que el error es

830
00:34:43,800 --> 00:34:45,800
bastante pequeño.

831
00:34:45,800 --> 00:34:49,139
El segundo experimento es básicamente

832
00:34:49,139 --> 00:34:51,239
una extensión de un experimento que propuse

833
00:34:51,239 --> 00:34:54,540
en un artículo anterior que es el

834
00:34:54,540 --> 00:34:56,460
aprendizaje sobre topologías de gráficos arbitrarios.

835
00:34:56,460 --> 00:34:59,040
que yo que escribí el año pasado

836
00:34:59,040 --> 00:35:01,080
en ese artículo,

837
00:35:01,080 --> 00:35:04,200
básicamente propongo este tipo de

838
00:35:04,200 --> 00:35:06,060
red como prueba de concepto, que es una

839
00:35:06,060 --> 00:35:08,160
red totalmente conectada que, en

840
00:35:08,160 --> 00:35:11,579
general, es la peor red neuronal que puede

841
00:35:11,579 --> 00:35:13,500
tener para realizar

842
00:35:13,500 --> 00:35:15,960
experimentos de aprendizaje automático porque

843
00:35:15,960 --> 00:35:20,520
nos da un  conjunto de neuronas

844
00:35:20,520 --> 00:35:23,660
básicamente,

845
00:35:23,760 --> 00:35:26,400
cada par de neuronas está

846
00:35:26,400 --> 00:35:28,680
conectado por dos sinapsis diferentes, por lo que es

847
00:35:28,680 --> 00:35:31,200
el modelo con

848
00:35:31,200 --> 00:35:33,359
la mayor complejidad posible en

849
00:35:33,359 --> 00:35:34,619
general,

850
00:35:34,619 --> 00:35:36,300
lo bueno es que, dado que tiene

851
00:35:36,300 --> 00:35:37,859
muchos ciclos, el modelo es extremadamente

852
00:35:37,859 --> 00:35:39,599
flexible  en el sentido de que puede entrenarlo,

853
00:35:39,599 --> 00:35:42,480
por ejemplo, en una imagen picada y

854
00:35:42,480 --> 00:35:45,359
en un punto de datos y en su etiqueta, pero

855
00:35:45,359 --> 00:35:47,400
luego la forma en que puede consultarlo gracias a

856
00:35:47,400 --> 00:35:50,640
la información que retrocede es, eh, puede

857
00:35:50,640 --> 00:35:52,140
consultar de muchas maneras diferentes, así que

858
00:35:52,140 --> 00:35:54,060
puede formar tareas de clasificación en las que

859
00:35:54,060 --> 00:35:55,980
proporciona una imagen y ejecuta la

860
00:35:55,980 --> 00:35:57,480
minimización de energía y obtiene la etiqueta,

861
00:35:57,480 --> 00:35:59,400
pero también puede, por ejemplo, realizar

862
00:35:59,400 --> 00:36:01,320
tareas de generación en las que proporciona la

863
00:36:01,320 --> 00:36:03,060
etiqueta, ejecuta la minimización de energía y

864
00:36:03,060 --> 00:36:05,220
obtiene la imagen que puede realizar por

865
00:36:05,220 --> 00:36:06,960
ejemplo imagen  finalización que le da la

866
00:36:06,960 --> 00:36:10,260
mitad de la imagen y converge y

867
00:36:10,260 --> 00:36:12,119
converge deja que el modelo se convierta a la

868
00:36:12,119 --> 00:36:14,400
segunda mitad y así sucesivamente, así que es

869
00:36:14,400 --> 00:36:16,440
básicamente un modelo que aprende

870
00:36:16,440 --> 00:36:19,619
las estadísticas del conjunto de datos en su

871
00:36:19,619 --> 00:36:21,900
totalidad sin estar centrado en la

872
00:36:21,900 --> 00:36:25,079
clasificación o  generación en general,

873
00:36:25,079 --> 00:36:27,900
por lo que esta flexibilidad es excelente,

874
00:36:27,900 --> 00:36:31,260
el problema es que debido a esto,

875
00:36:31,260 --> 00:36:34,140
todas las tareas no funcionan bien, por lo que

876
00:36:34,140 --> 00:36:35,820
puede hacer muchas cosas diferentes,

877
00:36:35,820 --> 00:36:38,579
pero ninguna de ellas se hace bien

878
00:36:38,579 --> 00:36:39,960
y

879
00:36:39,960 --> 00:36:42,480
aquí quiero mostrar cómo usar  Las

880
00:36:42,480 --> 00:36:44,099
consultas intervencionistas en lugar de las

881
00:36:44,099 --> 00:36:46,740
consultas correlacionales estándar o las

882
00:36:46,740 --> 00:36:48,119
consultas condicionales

883
00:36:48,119 --> 00:36:49,980
mejoran ligeramente los resultados de esas

884
00:36:49,980 --> 00:36:51,960
tareas de clasificación,

885
00:36:51,960 --> 00:36:54,000
entonces, ¿cuáles son las razones conjetivas de

886
00:36:54,000 --> 00:36:57,599
estas? La precisión de la prueba

887
00:36:57,599 --> 00:37:01,079
en esas tareas no es tan alta.

888
00:37:01,079 --> 00:37:03,180
Las dos primeras razones son que el modelo

889
00:37:03,180 --> 00:37:05,640
está distraído.  al corregir cada uno de

890
00:37:05,640 --> 00:37:07,920
cada uno de los errores, así que básicamente

891
00:37:07,920 --> 00:37:09,420
presentas una imagen y te gustaría

892
00:37:09,420 --> 00:37:11,579
obtener una etiqueta, pero el modelo en realidad se está

893
00:37:11,579 --> 00:37:13,859
actualizando para predecir también el

894
00:37:13,859 --> 00:37:16,320
error en las imágenes

895
00:37:16,320 --> 00:37:18,480
y la segunda razón, que es la que

896
00:37:18,480 --> 00:37:21,119
dije, es que el  la estructura es demasiado

897
00:37:21,119 --> 00:37:24,540
compleja, por lo que nuevamente a partir de un resultado de la

898
00:37:24,540 --> 00:37:27,079


899
00:37:27,079 --> 00:37:28,800
argumentación de la navaja de afeitar de Occam,

900
00:37:28,800 --> 00:37:30,720
este es el peor modelo que puede tener, por lo que

901
00:37:30,720 --> 00:37:32,160
cada vez que tenga un modelo que se ajuste a un

902
00:37:32,160 --> 00:37:33,960
conjunto de datos, ese modelo será menos

903
00:37:33,960 --> 00:37:35,579
complejo que este que va  para

904
00:37:35,579 --> 00:37:37,560
ser preferido,

905
00:37:37,560 --> 00:37:40,560
pero en general, solo para estudiarlo,

906
00:37:40,560 --> 00:37:41,400


907
00:37:41,400 --> 00:37:43,380
la idea es que se puede consultar en este modelo, las

908
00:37:43,380 --> 00:37:44,820
intervenciones se pueden usar para mejorar el

909
00:37:44,820 --> 00:37:46,859
rendimiento de esos

910
00:37:46,859 --> 00:37:48,599
modelos totalmente conectados,

911
00:37:48,599 --> 00:37:51,060
bueno, la respuesta es sí,

912
00:37:51,060 --> 00:37:53,160
así que así es como realizo las

913
00:37:53,160 --> 00:37:55,619
consultas intervencionistas.  Presento una imagen a la

914
00:37:55,619 --> 00:37:56,640
red.

915
00:37:56,640 --> 00:37:59,460
Corrijo el error de los píxeles para que sean

916
00:37:59,460 --> 00:38:01,560
iguales a cero para que este error no se

917
00:38:01,560 --> 00:38:03,180
propague en la red

918
00:38:03,180 --> 00:38:05,700
y luego calculo la etiqueta

919
00:38:05,700 --> 00:38:08,400
y, como puede ver, la precisión mejora,

920
00:38:08,400 --> 00:38:11,339
por ejemplo, de 89 usando el  el

921
00:38:11,339 --> 00:38:13,380
método de consulta estándar de las redes de codificación creativa

922
00:38:13,380 --> 00:38:16,800
a 92, que es la precisión después de

923
00:38:16,800 --> 00:38:19,020
la intervención y lo mismo sucede

924
00:38:19,020 --> 00:38:21,540
con los medios de moda

925
00:38:21,540 --> 00:38:24,420
y creo que una crítica muy legítima

926
00:38:24,420 --> 00:38:26,940
que probablemente todos pensarían

927
00:38:26,940 --> 00:38:28,920
al ver esas tramas es que está bien que

928
00:38:28,920 --> 00:38:32,099
mejore los medios de 89  a 92

929
00:38:32,099 --> 00:38:36,180
todavía apesta básicamente y sí, es verdad

930
00:38:36,180 --> 00:38:38,400
y en realidad estoy en las diapositivas posteriores. Voy

931
00:38:38,400 --> 00:38:40,619
a mostrar cómo actuar en la

932
00:38:40,619 --> 00:38:42,660
estructura de este uh de este

933
00:38:42,660 --> 00:38:43,859
modelo totalmente conectado

934
00:38:43,859 --> 00:38:46,500
mejorará los resultados aún más hasta

935
00:38:46,500 --> 00:38:48,480
el punto en que alcanzaron el

936
00:38:48,480 --> 00:38:50,820
rendimiento rico que ni siquiera está cerca del

937
00:38:50,820 --> 00:38:52,560
rendimiento de vanguardia, por supuesto,

938
00:38:52,560 --> 00:38:55,320
pero aún está alto, pero no está a un

939
00:38:55,320 --> 00:38:57,380
nivel que se vuelva básicamente aceptable.

940
00:38:57,380 --> 00:39:01,760


941
00:39:02,040 --> 00:39:04,980


942
00:39:04,980 --> 00:39:08,400


943
00:39:08,400 --> 00:39:10,920
y supongo que para resumir puedo decir que

944
00:39:10,920 --> 00:39:15,060
la parte interesante de este uh de

945
00:39:15,060 --> 00:39:17,640
los resultados que acabo de mostrar es

946
00:39:17,640 --> 00:39:19,859
que mostré que la codificación operativa es capaz

947
00:39:19,859 --> 00:39:22,560
de realizar intervenciones de una manera muy fácil

948
00:39:22,560 --> 00:39:24,780
e intuitiva porque no tienes que

949
00:39:24,780 --> 00:39:26,280
actuar sobre  la estructura del gráfico anterior

950
00:39:26,280 --> 00:39:28,740
ya no está disponible, a veces, esas

951
00:39:28,740 --> 00:39:31,079
funciones de función no están disponibles,

952
00:39:31,079 --> 00:39:34,020
etc., pero simplemente tiene que

953
00:39:34,020 --> 00:39:36,140


954
00:39:36,140 --> 00:39:39,780
intervenir en una sola neurona, estudia el

955
00:39:39,780 --> 00:39:41,640
error de predicción a cero

956
00:39:41,640 --> 00:39:44,220
y realiza un proceso de minimización de energía

957
00:39:44,220 --> 00:39:46,619


958
00:39:46,619 --> 00:39:49,200
y estos son extendidos  nos permitió

959
00:39:49,200 --> 00:39:51,240
definir modelos causales estructurales basados ​​en codificación creativa.

960
00:39:51,240 --> 00:39:52,920


961
00:39:52,920 --> 00:39:54,920
Ahora pasamos a la segunda

962
00:39:54,920 --> 00:39:57,900
parte del trabajo, que trata sobre el

963
00:39:57,900 --> 00:40:01,700
aprendizaje de estructuras estructurales,

964
00:40:02,000 --> 00:40:05,099
por lo que el aprendizaje de instrucciones, como dije,

965
00:40:05,099 --> 00:40:07,260
trata el problema de aprender la

966
00:40:07,260 --> 00:40:09,720
estructura causal del modelo

967
00:40:09,720 --> 00:40:11,880
a partir de datos observacionales.

968
00:40:11,880 --> 00:40:13,800
en realidad, ningún problema que ha

969
00:40:13,800 --> 00:40:17,760
existido durante décadas

970
00:40:17,760 --> 00:40:21,359
y siempre se ha abordado hasta hace un par

971
00:40:21,359 --> 00:40:24,000
de años utilizando

972
00:40:24,000 --> 00:40:25,560
métodos de búsqueda combinatoria.

973
00:40:25,560 --> 00:40:26,640
El problema con esos

974
00:40:26,640 --> 00:40:29,280
métodos de investigación comunitaria es que su

975
00:40:29,280 --> 00:40:32,880
complejidad crece al doble exponencialmente,

976
00:40:32,880 --> 00:40:34,740
por lo que tan pronto como los datos se vuelven

977
00:40:34,740 --> 00:40:36,780
múltiples.  dimensional y

978
00:40:36,780 --> 00:40:39,920
el gráfico de Bison que desea aprender

979
00:40:39,920 --> 00:40:42,300
crece en tamaño al

980
00:40:42,300 --> 00:40:46,680
aprenderlo es increíblemente lento

981
00:40:46,680 --> 00:40:48,780
la nueva solución que apareció hace

982
00:40:48,780 --> 00:40:51,000
un par de años en un nuevo periódico

983
00:40:51,000 --> 00:40:53,540
de 2018

984
00:40:53,839 --> 00:40:55,920
muestra que es posible

985
00:40:55,920 --> 00:40:57,900
aprender esta estructura sin usar un

986
00:40:57,900 --> 00:40:59,940
combinator research method pero usando

987
00:40:59,940 --> 00:41:01,619
un método basado en gradientes

988
00:41:01,619 --> 00:41:05,280
y este fue básicamente este

989
00:41:05,280 --> 00:41:07,320
problema especializado en general porque ahora puede

990
00:41:07,320 --> 00:41:08,820
simplemente

991
00:41:08,820 --> 00:41:10,980
tener una previa de los parámetros, que es

992
00:41:10,980 --> 00:41:12,420
el propósito prioritario que voy a

993
00:41:12,420 --> 00:41:14,700
definir un poco mejor  en esta

994
00:41:14,700 --> 00:41:15,599
diapositiva,

995
00:41:15,599 --> 00:41:18,180
ejecute el descenso de gradiente e incluso si

996
00:41:18,180 --> 00:41:19,740
tiene un modelo que es el doble del triple, el

997
00:41:19,740 --> 00:41:20,820
tamaño

998
00:41:20,820 --> 00:41:23,640
es, eh, el algoritmo sigue siendo increíblemente

999
00:41:23,640 --> 00:41:25,440
rápido

1000
00:41:25,440 --> 00:41:28,260
y, por esta razón, este documento es un

1001
00:41:28,260 --> 00:41:31,200
esto es sí, creo que es algo nuevo

1002
00:41:31,200 --> 00:41:33,180
y creo que ya existe.  600

1003
00:41:33,180 --> 00:41:35,099
citas o cosas o cosas así

1004
00:41:35,099 --> 00:41:37,140
y cada artículo que estoy viendo ahora

1005
00:41:37,140 --> 00:41:38,720
sobre aconsejar a amigos y aprender la

1006
00:41:38,720 --> 00:41:42,000
estructura del gráfico usa su método,

1007
00:41:42,000 --> 00:41:44,820
solo cambia un poco,

1008
00:41:44,820 --> 00:41:46,980
encuentran métodos de inferencia más rápidos o ligeramente mejores,

1009
00:41:46,980 --> 00:41:49,440
pero aún así todos usan

1010
00:41:49,440 --> 00:41:53,760
el  antes de este documento definido y yo

1011
00:41:53,760 --> 00:41:56,460
también y nosotros también, así que

1012
00:41:56,460 --> 00:41:58,859
aquí encontraremos una nueva cantidad

1013
00:41:58,859 --> 00:42:01,500
que es la matriz de agencia la

1014
00:42:01,500 --> 00:42:03,480
matriz de agencia es simplemente una matriz que codifica

1015
00:42:03,480 --> 00:42:06,359
las conexiones del modelo, por lo que es una

1016
00:42:06,359 --> 00:42:08,520
matriz binaria y

1017
00:42:08,520 --> 00:42:10,920
en general  es una Matriz binaria, entonces, por

1018
00:42:10,920 --> 00:42:12,180
supuesto, cuando realiza una optimización basada en gradientes,

1019
00:42:12,180 --> 00:42:14,880
la hace continua

1020
00:42:14,880 --> 00:42:16,800
y luego tiene un Umbral en algún

1021
00:42:16,800 --> 00:42:19,800
punto que básicamente elimina un borde o

1022
00:42:19,800 --> 00:42:21,480
lo establece en uno

1023
00:42:21,480 --> 00:42:27,780
y el M3 IJ es igual a uno si

1024
00:42:27,780 --> 00:42:30,540
tenemos  si el gráfico bayesiano es un borde

1025
00:42:30,540 --> 00:42:35,040
desde el vértice I al vértice J o cero,

1026
00:42:35,040 --> 00:42:37,380
de lo contrario, por ejemplo, esta agencia

1027
00:42:37,380 --> 00:42:39,540
Matrix aquí representa la

1028
00:42:39,540 --> 00:42:42,780
estructura de conectividad de esta red visual

1029
00:42:42,780 --> 00:42:44,040
y

1030
00:42:44,040 --> 00:42:46,079
básicamente este método

1031
00:42:46,079 --> 00:42:48,780
aborda dos problemas que queremos

1032
00:42:48,780 --> 00:42:51,000
sobre estos uh sobre aprender la

1033
00:42:51,000 --> 00:42:53,460
estructura  de la red de ecuaciones, la

1034
00:42:53,460 --> 00:42:54,780
idea es que comencemos con un

1035
00:42:54,780 --> 00:42:57,200
modelo completamente conectado que

1036
00:42:57,200 --> 00:43:00,240
conceptualmente es similar, en realidad

1037
00:43:00,240 --> 00:43:02,220
es equivalente a la red de codificación operativa que

1038
00:43:02,220 --> 00:43:04,020
definí anteriormente, que está completamente

1039
00:43:04,020 --> 00:43:06,480
conectada, por lo que tiene muchos

1040
00:43:06,480 --> 00:43:08,640
vértices y cada par de vértices  está

1041
00:43:08,640 --> 00:43:10,920
conectado por uh por dos bordes diferentes

1042
00:43:10,920 --> 00:43:13,319
y simplemente desea podar los

1043
00:43:13,319 --> 00:43:15,780
que no son necesarios,

1044
00:43:15,780 --> 00:43:18,540
por lo que puede verse como un método que

1045
00:43:18,540 --> 00:43:20,819
realiza la reducción del modelo, comienza con

1046
00:43:20,819 --> 00:43:22,020
un modelo grande y desea hacerlo

1047
00:43:22,020 --> 00:43:22,800
pequeño,

1048
00:43:22,800 --> 00:43:25,800
entonces, ¿qué es?  el primer ingrediente para

1049
00:43:25,800 --> 00:43:28,260
reducir bien los modelos es, por supuesto, Sparse

1050
00:43:28,260 --> 00:43:29,220
City

1051
00:43:29,220 --> 00:43:31,619
y cuál es el anterior que todos usan

1052
00:43:31,619 --> 00:43:33,839
para hacer un modelo más disperso es el

1053
00:43:33,839 --> 00:43:36,480
LaPlace anterior, que en el aprendizaje automático

1054
00:43:36,480 --> 00:43:38,880
se conoce simplemente como la norma L1,

1055
00:43:38,880 --> 00:43:40,920
que se define aquí como

1056
00:43:40,920 --> 00:43:43,980
la solución que el  este documento que

1057
00:43:43,980 --> 00:43:46,740
mencioné anteriormente propuso agregar el

1058
00:43:46,740 --> 00:43:49,319
segundo anterior en la parte superior que impone lo que

1059
00:43:49,319 --> 00:43:53,359
probablemente sea la característica más grande

1060
00:43:53,359 --> 00:43:55,980
de las redes bayesianas en la

1061
00:43:55,980 --> 00:43:57,780
que desea realizar una

1062
00:43:57,780 --> 00:43:59,819
inferencia causal es que desea que sean

1063
00:43:59,819 --> 00:44:01,020
cíclicas

1064
00:44:01,020 --> 00:44:03,000
y básicamente mostraron  esa

1065
00:44:03,000 --> 00:44:06,359
aciclicidad se puede imponer a una

1066
00:44:06,359 --> 00:44:08,160
Matrix de agencia como anterior

1067
00:44:08,160 --> 00:44:10,859
y tiene esta forma aquí, por lo que es

1068
00:44:10,859 --> 00:44:14,640
el rastro de Matrix que es

1069
00:44:14,640 --> 00:44:18,420
el exponencial de a veces a

1070
00:44:18,420 --> 00:44:21,859
donde a es la Matrix de agencia nuevamente y

1071
00:44:21,859 --> 00:44:24,300
básicamente esta cantidad aquí

1072
00:44:24,300 --> 00:44:27,900
es  es igual a cero si y solo si la

1073
00:44:27,900 --> 00:44:30,480
red bayesiana o

1074
00:44:30,480 --> 00:44:32,819
el gráfico que está considerando

1075
00:44:32,819 --> 00:44:35,720
es un clic c,

1076
00:44:37,619 --> 00:44:40,260
así que voy a usarlos en algunos

1077
00:44:40,260 --> 00:44:42,960
experimentos para que esos dos apliquen fuerza a

1078
00:44:42,960 --> 00:44:45,660
esos dos anteriores en

1079
00:44:45,660 --> 00:44:47,520
tipos diferentes  de redes de pacientes

1080
00:44:47,520 --> 00:44:49,200
y estoy tratando de fusionarlos con las

1081
00:44:49,200 --> 00:44:51,540
técnicas que propusimos anteriormente sobre la

1082
00:44:51,540 --> 00:44:52,740
realización de inferencia causal, la

1083
00:44:52,740 --> 00:44:55,020
codificación operativa,

1084
00:44:55,020 --> 00:44:56,520
así que voy a presentar dos

1085
00:44:56,520 --> 00:44:59,640
experimentos diferentes, uno es una prueba de

1086
00:44:59,640 --> 00:45:00,960
concepto que son los

1087
00:45:00,960 --> 00:45:03,660
experimentos estándar que se muestran en  todas las

1088
00:45:03,660 --> 00:45:06,599
tareas de aprendizaje estructural, que es la inferencia de

1089
00:45:06,599 --> 00:45:08,880
la red bayesiana correcta a partir de los datos

1090
00:45:08,880 --> 00:45:11,760
y luego voy a construir sobre

1091
00:45:11,760 --> 00:45:13,500
los experimentos de clasificación que mostré

1092
00:45:13,500 --> 00:45:14,280
anteriormente

1093
00:45:14,280 --> 00:45:16,020


1094
00:45:16,020 --> 00:45:18,540
y mostrar cómo en realidad esos antecedentes

1095
00:45:18,540 --> 00:45:21,060
me permiten mejorar la

1096
00:45:21,060 --> 00:45:22,500
clasificación  precisión la

1097
00:45:22,500 --> 00:45:25,500
precisión de la prueba de los modelos de codificación predictiva totalmente conectados,

1098
00:45:25,500 --> 00:45:28,160


1099
00:45:29,520 --> 00:45:31,680
así que pasemos al primer experimento,

1100
00:45:31,680 --> 00:45:33,300
que es inferir la estructura del

1101
00:45:33,300 --> 00:45:34,980
gráfico

1102
00:45:34,980 --> 00:45:37,319
y todos los experimentos, todos siguen

1103
00:45:37,319 --> 00:45:39,480
básicamente la misma línea en todos los

1104
00:45:39,480 --> 00:45:42,060
documentos en el campo, el primer paso es

1105
00:45:42,060 --> 00:45:45,119
generar  una red de visión de un gráfico aleatorio,

1106
00:45:45,119 --> 00:45:46,079


1107
00:45:46,079 --> 00:45:48,359
por lo que, básicamente, normalmente los dos

1108
00:45:48,359 --> 00:45:50,640
gráficos aleatorios que todos prueban son

1109
00:45:50,640 --> 00:45:53,520
renegraphs de Erdos y un gráfico sin escala,

1110
00:45:53,520 --> 00:45:55,859
por lo que genera esos gráficos grandes

1111
00:45:55,859 --> 00:45:58,680
que normalmente tienen 20 para los 80 80

1112
00:45:58,680 --> 00:46:01,619
nodos diferentes y algunos bordes

1113
00:46:01,619 --> 00:46:04,619
que usted muestra aleatoriamente

1114
00:46:04,619 --> 00:46:06,540
y  utiliza este gráfico para generar un

1115
00:46:06,540 --> 00:46:08,280
conjunto de datos,

1116
00:46:08,280 --> 00:46:10,819
por lo que toma muestras, por ejemplo, de

1117
00:46:10,819 --> 00:46:14,460
n puntos de datos Big N y lo que hace es

1118
00:46:14,460 --> 00:46:16,859
tomar el gráfico que

1119
00:46:16,859 --> 00:46:18,780
generaron anteriormente y lo

1120
00:46:18,780 --> 00:46:20,819
tira, solo conserva el conjunto de datos

1121
00:46:20,819 --> 00:46:23,099
y la tarea que  Lo que quiero resolver ahora es

1122
00:46:23,099 --> 00:46:25,020
aprender

1123
00:46:25,020 --> 00:46:27,420
es tener un algoritmo de entrenamiento que

1124
00:46:27,420 --> 00:46:29,819
básicamente te permita

1125
00:46:29,819 --> 00:46:32,579
recuperar la estructura del

1126
00:46:32,579 --> 00:46:34,619
gráfico que has desechado,

1127
00:46:34,619 --> 00:46:36,839
así que la forma en que lo hacemos aquí es que estamos

1128
00:46:36,839 --> 00:46:38,460
en una codificación creativa totalmente conectada.

1129
00:46:38,460 --> 00:46:41,760
modelar en este conjunto de datos D utilizando tanto el

1130
00:46:41,760 --> 00:46:43,800
disperso como el SQL previo que hemos

1131
00:46:43,800 --> 00:46:45,359
definido anteriormente

1132
00:46:45,359 --> 00:46:48,780
y ver si realmente el

1133
00:46:48,780 --> 00:46:50,760
gráfico al que convergemos después de

1134
00:46:50,760 --> 00:46:53,220
eliminar

1135
00:46:53,220 --> 00:46:55,319
las entradas de la matriz de la agencia que

1136
00:46:55,319 --> 00:46:57,599
son más pequeñas que un cierto umbral es

1137
00:46:57,599 --> 00:47:00,060
similar a  el del gráfico inicial

1138
00:47:00,060 --> 00:47:02,359


1139
00:47:02,520 --> 00:47:04,500
y también muestra que este es

1140
00:47:04,500 --> 00:47:06,599
realmente el caso, así que este es un ejemplo

1141
00:47:06,599 --> 00:47:09,020
y muestro muchas

1142
00:47:09,020 --> 00:47:12,420
parametrizaciones y dimensiones diferentes

1143
00:47:12,420 --> 00:47:15,060
y cosas así en el documento,

1144
00:47:15,060 --> 00:47:16,920
pero creo que esos dos son los

1145
00:47:16,920 --> 00:47:18,900
ejemplos más representativos  con un

1146
00:47:18,900 --> 00:47:20,760
gráfico de vivero de error y un gráfico de escala libre

1147
00:47:20,760 --> 00:47:23,579
con 20 nodos

1148
00:47:23,579 --> 00:47:25,800
y aquí a la izquierda puede ver el

1149
00:47:25,800 --> 00:47:27,300
suelo a través del gráfico que es el que se

1150
00:47:27,300 --> 00:47:29,339
muestra

1151
00:47:29,339 --> 00:47:30,839
al azar

1152
00:47:30,839 --> 00:47:32,599
y a la derecha puede ver el gráfico

1153
00:47:32,599 --> 00:47:35,220
del modelo de dificultad bastante aprendido

1154
00:47:35,220 --> 00:47:37,440
de los datos  conjunto

1155
00:47:37,440 --> 00:47:39,359
y, como puede ver, son bastante

1156
00:47:39,359 --> 00:47:40,500
similares,

1157
00:47:40,500 --> 00:47:42,780
todavía no es perfecto, por lo que hay

1158
00:47:42,780 --> 00:47:45,000
algunos, hay algunos errores, pero

1159
00:47:45,000 --> 00:47:47,460
en general la estructura

1160
00:47:47,460 --> 00:47:49,500
funciona bastante bien, también tenemos algunos

1161
00:47:49,500 --> 00:47:52,140
experimentos cuantitativos

1162
00:47:52,140 --> 00:47:54,000
que no muestro aquí  porque son

1163
00:47:54,000 --> 00:47:55,740
solo tablas enormes con muchos números

1164
00:47:55,740 --> 00:47:57,180
y pensé que tal vez era

1165
00:47:57,180 --> 00:48:00,660
demasiado para la presentación, pero

1166
00:48:00,660 --> 00:48:02,220
los resultados muestran que funcionan

1167
00:48:02,220 --> 00:48:06,060
de manera similar a los métodos contemporáneos

1168
00:48:06,060 --> 00:48:07,920
también porque tengo que decir como la mayor parte de

1169
00:48:07,920 --> 00:48:10,859
la calidad  proviene del acigli

1170
00:48:10,859 --> 00:48:15,799
anterior que se introdujo en 2018.

1171
00:48:16,920 --> 00:48:19,680
La segunda clase de experimentos son

1172
00:48:19,680 --> 00:48:21,599
nuestros experimentos de clasificación que, como

1173
00:48:21,599 --> 00:48:23,880
dije, son las extensiones del que

1174
00:48:23,880 --> 00:48:25,560
compartí anteriormente

1175
00:48:25,560 --> 00:48:27,119
y la idea es utilizar el

1176
00:48:27,119 --> 00:48:28,560
aprendizaje estructural para mejorar la clasificación.

1177
00:48:28,560 --> 00:48:31,140
en los resultados de la clasificación en el conjunto de

1178
00:48:31,140 --> 00:48:33,420
datos de medios y medios de moda

1179
00:48:33,420 --> 00:48:36,780
a partir de un gráfico completamente conectado, lo

1180
00:48:36,780 --> 00:48:40,560
que hice fue dividir los

1181
00:48:40,560 --> 00:48:42,839
grupos de neuronas de gráficos completamente conectados,

1182
00:48:42,839 --> 00:48:46,440
por lo que el grupo 1B es el

1183
00:48:46,440 --> 00:48:49,140
que está relacionado con la entrada

1184
00:48:49,140 --> 00:48:51,900
y todos los pequeños entonces  tenemos un

1185
00:48:51,900 --> 00:48:55,319
número específico de grupos ocultos

1186
00:48:55,319 --> 00:48:57,720
y luego tenemos el grupo de etiquetas que

1187
00:48:57,720 --> 00:48:58,800
es la

1188
00:48:58,800 --> 00:49:01,560
clase del grupo de neuronas que se

1189
00:49:01,560 --> 00:49:04,079
supone que me darán las predicciones de la etiqueta

1190
00:49:04,079 --> 00:49:06,480


1191
00:49:06,480 --> 00:49:08,700
y las he entrenado usando para usar

1192
00:49:08,700 --> 00:49:10,980
la primera vez el escaso anterior  solo que

1193
00:49:10,980 --> 00:49:14,099
la idea es qué pasa si elimino las

1194
00:49:14,099 --> 00:49:16,500
conexiones que no necesito de un

1195
00:49:16,500 --> 00:49:17,460
modelo

1196
00:49:17,460 --> 00:49:20,880
y aprendo como modelo analizador

1197
00:49:20,880 --> 00:49:24,119
funciona bien la respuesta es no,

1198
00:49:24,119 --> 00:49:25,500
no funciona y la razón y la

1199
00:49:25,500 --> 00:49:28,500
razón por la cual es  que al final, el

1200
00:49:28,500 --> 00:49:30,660
gráfico con el que converge es en realidad

1201
00:49:30,660 --> 00:49:32,700
el generado, por lo que básicamente el modelo

1202
00:49:32,700 --> 00:49:36,180
aprende a predecir la etiqueta en función de la

1203
00:49:36,180 --> 00:49:38,400
etiqueta misma, por lo que descarta toda la

1204
00:49:38,400 --> 00:49:40,020
información de la entrada

1205
00:49:40,020 --> 00:49:42,480
y solo conserva la etiqueta y, como puede

1206
00:49:42,480 --> 00:49:45,119
ver aquí, el  label y se predice a sí mismo o

1207
00:49:45,119 --> 00:49:46,560
en otros experimentos cuando cambia los

1208
00:49:46,560 --> 00:49:48,960
parámetros que tiene que y predice en

1209
00:49:48,960 --> 00:49:52,520
cero que preex X1 predice y nuevamente

1210
00:49:52,520 --> 00:49:55,980
entonces cuál es la solución

1211
00:49:55,980 --> 00:49:57,240
a este problema, bueno, la solución a este

1212
00:49:57,240 --> 00:49:59,520
problema es que tenemos

1213
00:49:59,520 --> 00:50:03,000
que converger  a un gráfico acíclico

1214
00:50:03,000 --> 00:50:05,220
y entonces tenemos que agregar algo que

1215
00:50:05,220 --> 00:50:08,000
evite una ciclicidad y cuál es esa

1216
00:50:08,000 --> 00:50:10,200
es, por supuesto, la que ya

1217
00:50:10,200 --> 00:50:12,780
propuse y luego muestro una segunda

1218
00:50:12,780 --> 00:50:14,520
técnica,

1219
00:50:14,520 --> 00:50:17,280
por lo que la primera usa el SQL

1220
00:50:17,280 --> 00:50:18,680
definido anteriormente

1221
00:50:18,680 --> 00:50:21,359
y la segunda es  a es una

1222
00:50:21,359 --> 00:50:22,859
técnica novedosa que en realidad hace uso de

1223
00:50:22,859 --> 00:50:24,359
ejemplos negativos,

1224
00:50:24,359 --> 00:50:26,520
por lo que un ejemplo negativo en este

1225
00:50:26,520 --> 00:50:30,060
caso es simplemente un punto de datos en el

1226
00:50:30,060 --> 00:50:32,280
que tiene una imagen pero la etiqueta es

1227
00:50:32,280 --> 00:50:33,240
incorrecta, por lo

1228
00:50:33,240 --> 00:50:35,220
que aquí, por ejemplo, tiene una imagen de

1229
00:50:35,220 --> 00:50:36,900
un siete  pero la etiqueta que le doy

1230
00:50:36,900 --> 00:50:39,599
al modelo es un dos

1231
00:50:39,599 --> 00:50:40,980


1232
00:50:40,980 --> 00:50:44,579
y la idea es muy simple, ya se ha

1233
00:50:44,579 --> 00:50:47,460
utilizado en muchos trabajos,

1234
00:50:47,460 --> 00:50:49,740
así que cada vez que los modelos son un

1235
00:50:49,740 --> 00:50:52,079
ejemplo positivo, tiene que aumentar para

1236
00:50:52,079 --> 00:50:53,520
minimizar la variación.  de energía libre

1237
00:50:53,520 --> 00:50:56,520
y cada vez que tiene un es un

1238
00:50:56,520 --> 00:50:58,859
ejemplo negativo, tiene que aumentarlo,

1239
00:50:58,859 --> 00:51:01,260
así que permítanme avanzar en el error, esta

1240
00:51:01,260 --> 00:51:04,200
cantidad debe minimizarse

1241
00:51:04,200 --> 00:51:05,960
extranjera

1242
00:51:05,960 --> 00:51:08,579
con muchos experimentos y muchos

1243
00:51:08,579 --> 00:51:10,859
uh de experimentos vimos que las

1244
00:51:10,859 --> 00:51:12,119
dos técnicas

1245
00:51:12,119 --> 00:51:15,000
Básicamente, el primero conduce a los mismos

1246
00:51:15,000 --> 00:51:17,220
resultados y el segundo también conduce al mismo

1247
00:51:17,220 --> 00:51:18,599
gráfico,

1248
00:51:18,599 --> 00:51:21,000
así que aquí están

1249
00:51:21,000 --> 00:51:22,800
los nuevos resultados, algunos medios y

1250
00:51:22,800 --> 00:51:25,079
medios de moda usando las dos técnicas

1251
00:51:25,079 --> 00:51:27,660
que acabo de proponer

1252
00:51:27,660 --> 00:51:30,960
y ahora pasamos a algunos que

1253
00:51:30,960 --> 00:51:33,900
Todavía no son muy buenos, pero definitivamente son

1254
00:51:33,900 --> 00:51:36,000
precisiones de prueba más razonables, por lo que aquí

1255
00:51:36,000 --> 00:51:39,059
tenemos un error de prueba de 3.17 para minutos y un

1256
00:51:39,059 --> 00:51:42,119
error de prueba de 13.98 para medios de moda

1257
00:51:42,119 --> 00:51:44,819
y, en realidad, esos resultados

1258
00:51:44,819 --> 00:51:48,300
pueden mejorarse mucho aprendiendo la

1259
00:51:48,300 --> 00:51:51,300
estructura del gráfico de  picado

1260
00:51:51,300 --> 00:51:53,040
y luego arreglando la estructura del

1261
00:51:53,040 --> 00:51:55,319
gráfico y haciendo algún tipo de

1262
00:51:55,319 --> 00:51:57,660
ajuste fino, de modo que si ajusta el modelo en

1263
00:51:57,660 --> 00:52:00,000
la estructura jerárquica correcta en

1264
00:52:00,000 --> 00:52:01,980
algún punto, alcanzará la precisión de la prueba,

1265
00:52:01,980 --> 00:52:03,359
que es la que esperaría de un

1266
00:52:03,359 --> 00:52:05,460
modelo jerárquico pero esos  son

1267
00:52:05,460 --> 00:52:08,099
simplemente aquellos en los que el modelo completamente conectado

1268
00:52:08,099 --> 00:52:10,980
convergió de forma natural, por lo que,

1269
00:52:10,980 --> 00:52:13,859
por ejemplo, a partir de un error de prueba de

1270
00:52:13,859 --> 00:52:15,420
18.32

1271
00:52:15,420 --> 00:52:17,339
del tren de modelos completamente conectado en la

1272
00:52:17,339 --> 00:52:20,359
moda significa simplemente realizar

1273
00:52:20,359 --> 00:52:22,859
correlaciones o consultas condicionales,

1274
00:52:22,859 --> 00:52:24,420
que es una forma estándar de consultar el

1275
00:52:24,420 --> 00:52:26,520
modelo de codificación operativa

1276
00:52:26,520 --> 00:52:29,220
agregando  Las intervenciones y el clic

1277
00:52:29,220 --> 00:52:32,040
previo de CA juntos hacen que

1278
00:52:32,040 --> 00:52:34,200
este error de prueba sea mucho más bajo

1279
00:52:34,200 --> 00:52:37,200
y podemos observarlo por medios también.

1280
00:52:37,200 --> 00:52:39,319


1281
00:52:39,780 --> 00:52:41,819
No voy a entrar en detalles

1282
00:52:41,819 --> 00:52:45,420
sobre este último experimento y cómo

1283
00:52:45,420 --> 00:52:48,660
actúa el previo acíclico en el  estructura

1284
00:52:48,660 --> 00:52:50,339
del gráfico,

1285
00:52:50,339 --> 00:52:52,440
así que realizo Realizo un experimento en

1286
00:52:52,440 --> 00:52:54,960
uh en un nuevo conjunto de datos, es decir,

1287
00:52:54,960 --> 00:52:56,460
llamar al nuevo conjunto de datos, puede ser

1288
00:52:56,460 --> 00:52:58,500
demasiado, lo llamo un

1289
00:52:58,500 --> 00:53:01,440
conjunto de datos de dos medios en el que tiene el

1290
00:53:01,440 --> 00:53:04,319
punto de entrada  está formado por dos

1291
00:53:04,319 --> 00:53:07,319
imágenes diferentes y la etiqueta solo depende de la

1292
00:53:07,319 --> 00:53:08,520
segunda imagen

1293
00:53:08,520 --> 00:53:10,800
en la historia de la primera imagen,

1294
00:53:10,800 --> 00:53:12,720
por lo que la idea aquí

1295
00:53:12,720 --> 00:53:15,079
es la estructura del modelo, la

1296
00:53:15,079 --> 00:53:18,540
ciclicidad previa y cosas así,

1297
00:53:18,540 --> 00:53:20,819
capaz de reconocer que la segunda mitad

1298
00:53:20,819 --> 00:53:23,400
de la imagen es  en realidad no tiene sentido en

1299
00:53:23,400 --> 00:53:27,960
uh en el desempeño en el aprendizaje en el

1300
00:53:27,960 --> 00:53:31,140
desempeño de la clasificación

1301
00:53:31,140 --> 00:53:33,119
cómo se comporta el entrenamiento en general como,

1302
00:53:33,119 --> 00:53:36,480
por ejemplo, tenemos este nodo de entrada de entrada uh

1303
00:53:36,480 --> 00:53:39,000
nodo de salida y solo los nodos están

1304
00:53:39,000 --> 00:53:41,940
completamente conectados y el modelo

1305
00:53:41,940 --> 00:53:43,740
converge

1306
00:53:43,740 --> 00:53:45,900
en una estructura jerárquica que es el

1307
00:53:45,900 --> 00:53:48,960
uno que sabemos se desempeña mejor en las

1308
00:53:48,960 --> 00:53:50,880
tareas de clasificación,

1309
00:53:50,880 --> 00:53:53,520
bueno, aquí hay un ejemplo de ejecución de un

1310
00:53:53,520 --> 00:53:54,980
método de entrenamiento, por lo que

1311
00:53:54,980 --> 00:53:59,280
en c0, que es el comienzo del

1312
00:53:59,280 --> 00:54:00,720
entrenamiento,

1313
00:54:00,720 --> 00:54:03,000
tenemos este modelo aquí, por lo que s0

1314
00:54:03,000 --> 00:54:05,819
corresponde a los siete, a

1315
00:54:05,819 --> 00:54:08,099
la primera imagen.  como uno corresponde a

1316
00:54:08,099 --> 00:54:09,839
la imagen de siete columnas nuevamente, tenemos la

1317
00:54:09,839 --> 00:54:12,300
etiqueta Y y todas las variables latentes x0

1318
00:54:12,300 --> 00:54:13,800
X1 X2

1319
00:54:13,800 --> 00:54:15,720
y el modelo está completamente conectado, por lo que la

1320
00:54:15,720 --> 00:54:17,040
agencia Matrix

1321
00:54:17,040 --> 00:54:20,579
está llena de unos, no hay ceros,

1322
00:54:20,579 --> 00:54:23,720
tenemos bucles propios y cosas así.

1323
00:54:23,720 --> 00:54:27,319
modelo durante un par de épocas hasta que lo que

1324
00:54:27,319 --> 00:54:30,540
sabemos de inmediato es que, por

1325
00:54:30,540 --> 00:54:31,920
ejemplo, el modelo comprende de inmediato

1326
00:54:31,920 --> 00:54:34,740
que los cuatro no son necesarios

1327
00:54:34,740 --> 00:54:36,839
para realizar la clasificación, por lo que no lo hace,

1328
00:54:36,839 --> 00:54:40,740
por lo que se eliminan todos los nodos salientes del

1329
00:54:40,740 --> 00:54:43,980
segundo grupo de entrada

1330
00:54:43,980 --> 00:54:45,900
y  algo que no entendimos es

1331
00:54:45,900 --> 00:54:48,660
que este es este grupo es el que está

1332
00:54:48,660 --> 00:54:50,400
relacionado con la salida,

1333
00:54:50,400 --> 00:54:52,260
por lo que

1334
00:54:52,260 --> 00:54:55,319
tenemos un mapa lineal de s0 a Y

1335
00:54:55,319 --> 00:54:56,480
directamente,

1336
00:54:56,480 --> 00:54:59,339
que es esta parte aquí,

1337
00:54:59,339 --> 00:55:01,160
pero sabemos que en realidad un mapa lineal

1338
00:55:01,160 --> 00:55:04,740
no es el mejor  mapa para

1339
00:55:04,740 --> 00:55:07,200
realizar la clasificación de medios, por lo que

1340
00:55:07,200 --> 00:55:08,700
necesitamos cierta jerarquía, necesitamos cierta

1341
00:55:08,700 --> 00:55:11,579
profundidad para mejorar los resultados y, como

1342
00:55:11,579 --> 00:55:14,220
puede ver, esta línea aquí es la

1343
00:55:14,220 --> 00:55:15,599
precisión

1344
00:55:15,599 --> 00:55:18,960
que hasta este punto hasta C2 es

1345
00:55:18,960 --> 00:55:22,500
similar a um, entonces es 91  que es

1346
00:55:22,500 --> 00:55:24,059
un poco mejor que la clasificación lineal,

1347
00:55:24,059 --> 00:55:25,500


1348
00:55:25,500 --> 00:55:28,740
pero una vez que continúa con el entrenamiento,

1349
00:55:28,740 --> 00:55:30,660
el modelo comprende que necesita cierta

1350
00:55:30,660 --> 00:55:33,119
jerarquía para ajustarse mejor a los datos,

1351
00:55:33,119 --> 00:55:35,640
por lo que verá que esta Flecha comienza

1352
00:55:35,640 --> 00:55:38,760
a fortalecerse cada vez más con el tiempo

1353
00:55:38,760 --> 00:55:41,700
hasta que comprende que la clasificación lineal

1354
00:55:41,700 --> 00:55:44,339
map no es realmente necesario y

1355
00:55:44,339 --> 00:55:45,920
lo elimina,

1356
00:55:45,920 --> 00:55:48,780
por lo que el modelo con el que converge es un

1357
00:55:48,780 --> 00:55:51,000
modelo que comienza desde cero, va a un

1358
00:55:51,000 --> 00:55:53,760
nodo oculto y luego va a

1359
00:55:53,760 --> 00:55:57,180
la etiqueta con un mapa lineal muy débil

1360
00:55:57,180 --> 00:55:59,700
que en realidad se elimina si usted si

1361
00:55:59,700 --> 00:56:02,760
establece un umbral de uh si el

1362
00:56:02,760 --> 00:56:05,520
umbral del vendedor de, por ejemplo, 0.1 0.2 en algún

1363
00:56:05,520 --> 00:56:07,619
momento, el mapa lineal se olvida y

1364
00:56:07,619 --> 00:56:10,680
todo lo que termina es

1365
00:56:10,680 --> 00:56:13,319
con una red jerárquica, es

1366
00:56:13,319 --> 00:56:15,720
decir, uh, entonces ha aprendido la

1367
00:56:15,720 --> 00:56:17,099
estructura correcta para realizar

1368
00:56:17,099 --> 00:56:19,260
tareas de clasificación que es jerarquía

1369
00:56:19,260 --> 00:56:21,900
y también aprendió que la segunda

1370
00:56:21,900 --> 00:56:25,020
imagen no desempeñó ningún papel en la definición de la

1371
00:56:25,020 --> 00:56:28,440
precisión de la prueba y esto es todo, todo esto

1372
00:56:28,440 --> 00:56:30,420


1373
00:56:30,420 --> 00:56:33,839
se realiza también todos esos trabajos simplemente se realizan mediante

1374
00:56:33,839 --> 00:56:36,599
un proceso de minimización de energía libre,

1375
00:56:36,599 --> 00:56:38,400
inicializas el modelo, defines la

1376
00:56:38,400 --> 00:56:40,859
energía libre, defines los priores, por lo que

1377
00:56:40,859 --> 00:56:43,559
el disperso y el clic C antes,

1378
00:56:43,559 --> 00:56:45,780
ejecutas la minimización de energía y

1379
00:56:45,780 --> 00:56:47,400
converges a un

1380
00:56:47,400 --> 00:56:49,500
modelo jerárquico que es capaz de

1381
00:56:49,500 --> 00:56:51,839
realizar la clasificación en picado

1382
00:56:51,839 --> 00:56:54,000
y luego si  luego realiza algunos

1383
00:56:54,000 --> 00:56:55,800
ajustes finos y obtienes resultados muy competitivos

1384
00:56:55,800 --> 00:56:57,359
como lo haces en las

1385
00:56:57,359 --> 00:56:59,339
redes de avance con la propagación de retroalimentación,

1386
00:56:59,339 --> 00:57:01,260
pero creo que eso no es lo interesante, lo

1387
00:57:01,260 --> 00:57:03,780
interesante es que te gusta

1388
00:57:03,780 --> 00:57:05,160
todo este proceso, este proceso en

1389
00:57:05,160 --> 00:57:07,980
conjunto de intervención y la

1390
00:57:07,980 --> 00:57:09,780
aciclicidad

1391
00:57:09,780 --> 00:57:11,700
te permite  para tomar una red completamente conectada

1392
00:57:11,700 --> 00:57:12,660


1393
00:57:12,660 --> 00:57:15,119
y converger a una jerárquica

1394
00:57:15,119 --> 00:57:16,140
que es capaz de realizar

1395
00:57:16,140 --> 00:57:20,058
una clasificación con buenos resultados

1396
00:57:20,760 --> 00:57:23,000
y sí, eso es

1397
00:57:23,000 --> 00:57:26,280
básicamente. Ahora estoy, oh sí, wow, he

1398
00:57:26,280 --> 00:57:29,220
hablado mucho y estoy, eh, esta es la

1399
00:57:29,220 --> 00:57:32,160
conclusión.  de la charla, que es

1400
00:57:32,160 --> 00:57:35,280
básicamente un pequeño resumen y

1401
00:57:35,280 --> 00:57:37,559
creo que lo más importante, si

1402
00:57:37,559 --> 00:57:39,300
tengo que darte una oración de este

1403
00:57:39,300 --> 00:57:40,980
documento, es que la codificación predictiva es un

1404
00:57:40,980 --> 00:57:44,400
método de actualización de creencias que puede

1405
00:57:44,400 --> 00:57:46,559
realizar de principio a fin  -Aprendizaje de prima final para que

1406
00:57:46,559 --> 00:57:48,599
pueda realizar intervenciones para

1407
00:57:48,599 --> 00:57:51,420
aprender una estructura a partir de datos y luego

1408
00:57:51,420 --> 00:57:53,160
realizar intervenciones y

1409
00:57:53,160 --> 00:57:56,058
contrafactuales para

1410
00:57:56,700 --> 00:57:58,440
inferir causalmente en otras

1411
00:57:58,440 --> 00:58:00,119
intervenciones y modelar de manera eficiente

1412
00:58:00,119 --> 00:58:01,680
simplemente estableciendo el error de predicción en

1413
00:58:01,680 --> 00:58:03,359
cero, por lo que es una técnica muy fácil

1414
00:58:03,359 --> 00:58:06,240
de realizar.  intervenciones y

1415
00:58:06,240 --> 00:58:07,619
simplemente solo tiene que tocar una

1416
00:58:07,619 --> 00:58:08,940
neurona, no tiene que actuar sobre la

1417
00:58:08,940 --> 00:58:10,859
estructura del gráfico,

1418
00:58:10,859 --> 00:58:14,339
puede usarlo para

1419
00:58:14,339 --> 00:58:16,140
crear modelos causales de estructura que son

1420
00:58:16,140 --> 00:58:18,359
biológicamente plausibles,

1421
00:58:18,359 --> 00:58:20,819
es capaz de aprender la estructura para  uh,

1422
00:58:20,819 --> 00:58:24,119
de los datos, como dije, tal vez ya muchas veces

1423
00:58:24,119 --> 00:58:26,940


1424
00:58:26,940 --> 00:58:28,740
y un par de oraciones sobre

1425
00:58:28,740 --> 00:58:31,260
trabajos futuros es que

1426
00:58:31,260 --> 00:58:33,180
algo que sería bueno hacer es

1427
00:58:33,180 --> 00:58:36,119
mejorar el rendimiento del modelo que

1428
00:58:36,119 --> 00:58:38,460
hemos definido porque creo que

1429
00:58:38,460 --> 00:58:40,980
funciona razonablemente bien en  muchas

1430
00:58:40,980 --> 00:58:43,079
tareas, por lo que funciona razonablemente bien en el

1431
00:58:43,079 --> 00:58:45,780
aprendizaje estructural en la

1432
00:58:45,780 --> 00:58:48,119
intervención y los contrafactuales, pero

1433
00:58:48,119 --> 00:58:49,440
en realidad, si observa el modelo de vanguardia,

1434
00:58:49,440 --> 00:58:51,420
siempre hay un

1435
00:58:51,420 --> 00:58:53,880
método muy específico que funciona mejor en

1436
00:58:53,880 --> 00:58:55,559
una sola tarea, por lo que

1437
00:58:55,559 --> 00:58:58,260
sería interesante ver  si

1438
00:58:58,260 --> 00:59:00,180
podemos alcanzar ese nivel de rendimiento

1439
00:59:00,180 --> 00:59:03,599
en tareas específicas agregando algunos

1440
00:59:03,599 --> 00:59:05,599
trucos o algunos

1441
00:59:05,599 --> 00:59:10,260
o algunos nuevos métodos de optimización y

1442
00:59:10,260 --> 00:59:12,839
generalizarlo a sistemas dinámicos

1443
00:59:12,839 --> 00:59:14,280
que en realidad son mucho más interesantes que

1444
00:59:14,280 --> 00:59:17,220
los sistemas estáticos, como los

1445
00:59:17,220 --> 00:59:20,099
modelos causales dinámicos y/o  otras técnicas

1446
00:59:20,099 --> 00:59:22,200
que le permiten realizar

1447
00:59:22,200 --> 00:59:25,200
inferencias causales en sistemas que se mueven, por lo que

1448
00:59:25,200 --> 00:59:27,799
una acción realizada en un paso de tiempo específico

1449
00:59:27,799 --> 00:59:30,299
influye en otro nodo en un paso de tiempo posterior,

1450
00:59:30,299 --> 00:59:32,640
que es básicamente una causalidad de grandeza,

1451
00:59:32,640 --> 00:59:34,859


1452
00:59:34,859 --> 00:59:38,160
sí, eso es todo, y

1453
00:59:38,160 --> 00:59:41,118
muchas gracias, gracias

1454
00:59:47,460 --> 00:59:51,119


1455
00:59:51,119 --> 00:59:53,160
presentación increíble y muy completa que realmente creo que

1456
00:59:53,160 --> 00:59:55,700
está silenciado, lo siento

1457
00:59:57,119 --> 00:59:59,700
silenciado en Zoom, pero sí, gracias por

1458
00:59:59,700 --> 01:00:02,400
la presentación increíble y muy completa,

1459
01:00:02,400 --> 01:00:05,099
realmente hubo muchas cosas

1460
01:00:05,099 --> 01:00:06,900
allí y también hubo muchas

1461
01:00:06,900 --> 01:00:09,900
preguntas geniales en el chat en vivo, así que tal vez para

1462
01:00:09,900 --> 01:00:12,900
calentar  en las preguntas cómo llegaste

1463
01:00:12,900 --> 01:00:15,960
a estudiar este tema donde estudiaste en

1464
01:00:15,960 --> 01:00:18,900
causalidad y encontraste que la codificación predictiva

1465
01:00:18,900 --> 01:00:21,000
era útil o viceversa o cómo

1466
01:00:21,000 --> 01:00:23,160
llegaste a esta intersección, en

1467
01:00:23,160 --> 01:00:25,740
realidad tengo que decir que la primera

1468
01:00:25,740 --> 01:00:27,240
persona que tuvo esta idea fue

1469
01:00:27,240 --> 01:00:29,040
uh, era Baron,

1470
01:00:29,040 --> 01:00:33,900
creo que hace un año y

1471
01:00:33,900 --> 01:00:36,660
medio, incluso más, trajo una

1472
01:00:36,660 --> 01:00:38,940
página con esta idea y luego se

1473
01:00:38,940 --> 01:00:42,119
olvidó y nadie la recogió y uh,

1474
01:00:42,119 --> 01:00:43,980
y el verano pasado comencé a tener

1475
01:00:43,980 --> 01:00:47,880
curiosidad sobre la causalidad y el

1476
01:00:47,880 --> 01:00:50,339
um  Leí, por ejemplo, El libro de la vida

1477
01:00:50,339 --> 01:00:52,440
mientras escuchaba podcasts. Conozco la

1478
01:00:52,440 --> 01:00:53,760
forma estándar en la que te interesas

1479
01:00:53,760 --> 01:00:54,900
en un tema

1480
01:00:54,900 --> 01:00:57,480
y recuerdo esta idea de

1481
01:00:57,480 --> 01:01:00,180
Baron y se la propuse y

1482
01:01:00,180 --> 01:01:03,180
pensé: ¿por qué no?  lo expandimos y,

1483
01:01:03,180 --> 01:01:06,000
de hecho, lo convertimos en un documento, así que

1484
01:01:06,000 --> 01:01:07,319
involucré a algunas personas para que me ayudaran con los

1485
01:01:07,319 --> 01:01:09,359
experimentos y este es el

1486
01:01:09,359 --> 01:01:12,000
resultado final al final

1487
01:01:12,000 --> 01:01:14,160
increíble, genial, sí,

1488
01:01:14,160 --> 01:01:15,240


1489
01:01:15,240 --> 01:01:17,400
mucho que decir, solo voy a ir al

1490
01:01:17,400 --> 01:01:19,619
chat en vivo.  primero y abordar un montón de

1491
01:01:19,619 --> 01:01:21,240
preguntas diferentes y si alguien más

1492
01:01:21,240 --> 01:01:22,440
quiere agregarme, voy a encender la luz

1493
01:01:22,440 --> 01:01:24,059
primero porque creo que me estoy quedando

1494
01:01:24,059 --> 01:01:28,440
en la oscuridad cada vez más, sí, ¿

1495
01:01:28,440 --> 01:01:30,720
quién dijo que la inferencia activa no puede resolver?  un

1496
01:01:30,720 --> 01:01:32,160
problema de cuarto oscuro,

1497
01:01:32,160 --> 01:01:34,980
oh, sí, aquí estamos, ¿

1498
01:01:34,980 --> 01:01:37,020
diría que el interruptor de la luz hizo que

1499
01:01:37,020 --> 01:01:39,299
fuera más claro,

1500
01:01:39,299 --> 01:01:40,680
sí,

1501
01:01:40,680 --> 01:01:42,240
creo que

1502
01:01:42,240 --> 01:01:43,980
no hay problemas aquí?

1503
01:01:43,980 --> 01:01:46,940


1504
01:01:46,940 --> 01:01:49,559


1505
01:01:49,559 --> 01:01:52,020


1506
01:01:52,020 --> 01:01:53,760


1507
01:01:53,760 --> 01:01:55,500
errores donde la

1508
01:01:55,500 --> 01:01:57,420
precisión es la inversa de la

1509
01:01:57,420 --> 01:02:00,000
covarianza gaussiana, lo que sucede si se usan distribuciones no gaussianas

1510
01:02:00,000 --> 01:02:03,319


1511
01:02:03,780 --> 01:02:05,339
es

1512
01:02:05,339 --> 01:02:09,059
básicamente el método general sigue siendo

1513
01:02:09,059 --> 01:02:10,380
diferente, la principal diferencia es que

1514
01:02:10,380 --> 01:02:13,079
usted no tiene errores de predicción

1515
01:02:13,079 --> 01:02:15,480
que, como se señaló correctamente, es

1516
01:02:15,480 --> 01:02:18,480
básicamente el  derivado de la

1517
01:02:18,480 --> 01:02:20,819
energía libre virtual si tiene suposiciones gaussianas,

1518
01:02:20,819 --> 01:02:22,920


1519
01:02:22,920 --> 01:02:25,020
sí, incluso tiene esa cantidad única

1520
01:02:25,020 --> 01:02:27,960
para establecer en cero y probablemente

1521
01:02:27,960 --> 01:02:29,880
tendrá que actuar sobre la estructura del

1522
01:02:29,880 --> 01:02:30,900
gráfico

1523
01:02:30,900 --> 01:02:34,020
para realizar intervenciones

1524
01:02:34,020 --> 01:02:37,079
y también usted y sus colegas tuvieron un

1525
01:02:37,079 --> 01:02:39,900
artículo en 2022 predictivo  la codificación Más allá de las

1526
01:02:39,900 --> 01:02:41,880
distribuciones gaussianas que analizaron

1527
01:02:41,880 --> 01:02:43,859
algunos de estos problemas,

1528
01:02:43,859 --> 01:02:46,260
sí, sí, exactamente, ese artículo fue un

1529
01:02:46,260 --> 01:02:47,339
poco,

1530
01:02:47,339 --> 01:02:50,460
la idea detrás de ese artículo es, eh,

1531
01:02:50,460 --> 01:02:53,220
y modelamos Transformers, esa es la

1532
01:02:53,220 --> 01:02:54,420
mayor motivación usando bastante

1533
01:02:54,420 --> 01:02:57,180
dificultad y la respuesta es, eh, no es

1534
01:02:57,180 --> 01:02:59,460
porque el  el mecanismo de atención tiene

1535
01:02:59,460 --> 01:03:02,099
un Max suave al final y Max suave llama

1536
01:03:02,099 --> 01:03:03,960
a, eh,

1537
01:03:03,960 --> 01:03:08,400
no a la distribución gaussiana, pero sí a la

1538
01:03:08,400 --> 01:03:11,280
distribución Max suave,

1539
01:03:11,280 --> 01:03:13,440
no entiendo el nombre ahora, pero sí

1540
01:03:13,440 --> 01:03:16,079
y sí, eso es una generalización,

1541
01:03:16,079 --> 01:03:19,140
es un poco  Es difícil llamarlo una vez que

1542
01:03:19,140 --> 01:03:20,700
eliminas la suposición de Gaston. Todavía es un

1543
01:03:20,700 --> 01:03:22,319
poco complicado llamarlo

1544
01:03:22,319 --> 01:03:24,059
codificación creativa,

1545
01:03:24,059 --> 01:03:26,400
por lo que es,

1546
01:03:26,400 --> 01:03:29,819
por ejemplo, como hablar con uh para auto

1547
01:03:29,819 --> 01:03:32,700
Freestone o le gusta la codificación creativa

1548
01:03:32,700 --> 01:03:35,160
solo si solo tienes Gauss

1549
01:03:35,160 --> 01:03:37,680
y gaussian  suposiciones,

1550
01:03:37,680 --> 01:03:39,720
pero sí, eso es más un

1551
01:03:39,720 --> 01:03:42,660
debate filosófico que

1552
01:03:42,660 --> 01:03:44,940
interesante y otro tema

1553
01:03:44,940 --> 01:03:46,740
que creo que definitivamente es de gran

1554
01:03:46,740 --> 01:03:49,500
interés son las similitudes y diferencias

1555
01:03:49,500 --> 01:03:52,980
entre el aparato de atención en

1556
01:03:52,980 --> 01:03:56,099
Transformers y la forma en que

1557
01:03:56,099 --> 01:03:58,440
se describe la atención desde una

1558
01:03:58,440 --> 01:04:00,180
perspectiva neurocognitiva y desde un

1559
01:04:00,180 --> 01:04:03,240
procesamiento predictivo Precisión  ángulo de espera, ¿qué

1560
01:04:03,240 --> 01:04:06,200
piensas sobre eso?

1561
01:04:06,359 --> 01:04:08,700
Bueno, la idea es que, um,

1562
01:04:08,700 --> 01:04:12,359
sí, creo que es que, desde una

1563
01:04:12,359 --> 01:04:15,000


1564
01:04:15,000 --> 01:04:16,400
perspectiva bastante procesada y también de inferencia operativa,

1565
01:04:16,400 --> 01:04:19,260
la atención puede verse como una especie de

1566
01:04:19,260 --> 01:04:21,299
problema de aprendizaje estructural,

1567
01:04:21,299 --> 01:04:23,040
creo que hay un  artículo reciente del

1568
01:04:23,040 --> 01:04:25,680
grupo de Chris Buckley que muestra

1569
01:04:25,680 --> 01:04:26,339
que

1570
01:04:26,339 --> 01:04:28,079
debería haber una

1571
01:04:28,079 --> 01:04:30,420
reimpresión en el archivo en el que básicamente

1572
01:04:30,420 --> 01:04:31,859
demostraron que el mecanismo de atención

1573
01:04:31,859 --> 01:04:35,819
es simplemente aprender la precisión en

1574
01:04:35,819 --> 01:04:38,880
los parámetros de peso específicos de

1575
01:04:38,880 --> 01:04:41,040
otros puntos de datos, por lo que esta precisión

1576
01:04:41,040 --> 01:04:43,200
no es un parámetro

1577
01:04:43,200 --> 01:04:45,540
que está en la estructura del modelo, por lo que no es

1578
01:04:45,540 --> 01:04:47,579
un parámetro específico del modelo,

1579
01:04:47,579 --> 01:04:49,140
es un parámetro que cambia rápidamente, como los

1580
01:04:49,140 --> 01:04:51,660
nodos de valor que se actualizan al tiempo que

1581
01:04:51,660 --> 01:04:53,760
minimizan la variación de la energía libre

1582
01:04:53,760 --> 01:04:55,440
y una vez que lo hacen  lo he minimizado

1583
01:04:55,440 --> 01:04:57,000
y calculado, luego lo tira

1584
01:04:57,000 --> 01:04:58,920
y para el siguiente punto de datos tiene que

1585
01:04:58,920 --> 01:05:00,780
volver a calcularlo desde cero,

1586
01:05:00,780 --> 01:05:03,299
así que sí, creo que el

1587
01:05:03,299 --> 01:05:05,819
cálculo de la analogía es sabio, el

1588
01:05:05,819 --> 01:05:07,920
mecanismo de atención puede verse como una especie de

1589
01:05:07,920 --> 01:05:10,559
estructura de aprendizaje pero un  estructura de

1590
01:05:10,559 --> 01:05:13,020
aprendizaje que es específico del punto de datos y

1591
01:05:13,020 --> 01:05:15,119
no específico del modelo

1592
01:05:15,119 --> 01:05:17,280
y creo que si queremos generalizar un

1593
01:05:17,280 --> 01:05:18,960
poco y pasar

1594
01:05:18,960 --> 01:05:20,339
del mecanismo de atención en

1595
01:05:20,339 --> 01:05:21,900
Transformers al mecanismo de atención

1596
01:05:21,900 --> 01:05:24,180
ciencia cognitiva

1597
01:05:24,180 --> 01:05:28,020
creo que probablemente sean dos diferentes para

1598
01:05:28,020 --> 01:05:31,260
dibujar similitudes y  uh,

1599
01:05:31,260 --> 01:05:33,359
creo que la analogía del aprendizaje estructural

1600
01:05:33,359 --> 01:05:36,660
y la importancia de una conexión

1601
01:05:36,660 --> 01:05:38,760
con respecto a otra probablemente

1602
01:05:38,760 --> 01:05:41,900
hace el trabajo mucho mejor.

1603
01:05:42,000 --> 01:05:44,880


1604
01:05:44,880 --> 01:05:49,200


1605
01:05:49,200 --> 01:05:51,240


1606
01:05:51,240 --> 01:05:55,440


1607
01:05:55,440 --> 01:05:59,180
que puedes Creo que

1608
01:05:59,540 --> 01:06:01,740
la principal es que no puedes

1609
01:06:01,740 --> 01:06:03,599
observar el uso que

1610
01:06:03,599 --> 01:06:05,819
puedes usar porque puedes

1611
01:06:05,819 --> 01:06:09,000
calcularlos y arreglarlos pero no puedes

1612
01:06:09,000 --> 01:06:10,559
la idea es que no tienes control

1613
01:06:10,559 --> 01:06:13,380
sobre ellos, por lo que usan el uso debe ser

1614
01:06:13,380 --> 01:06:16,020
visto como variables específicas del entorno

1615
01:06:16,020 --> 01:06:18,540
que están allí, influyen en

1616
01:06:18,540 --> 01:06:21,240
su proceso, está bien porque, por

1617
01:06:21,240 --> 01:06:23,280
ejemplo, cuando retrocede en el tiempo, el

1618
01:06:23,280 --> 01:06:25,079
entorno es diferente, por lo que la idea es,

1619
01:06:25,079 --> 01:06:26,520
por ejemplo, si le

1620
01:06:26,520 --> 01:06:28,440
gusta volver al ejemplo

1621
01:06:28,440 --> 01:06:29,880
anterior al

1622
01:06:29,880 --> 01:06:31,920
de  el ingreso esperado de una persona con

1623
01:06:31,920 --> 01:06:34,619
una inteligencia especifica de Educacion uh

1624
01:06:34,619 --> 01:06:37,440
uh titulo de educacion

1625
01:06:37,440 --> 01:06:40,200
la idea es que si quiero ver

1626
01:06:40,200 --> 01:06:43,559
cuanto voy a aprender hoy con uh con un con

1627
01:06:43,559 --> 01:06:45,359
no se con un master

1628
01:06:45,359 --> 01:06:47,339
es diferente con respeto  a cuánto ganaría

1629
01:06:47,339 --> 01:06:48,359


1630
01:06:48,359 --> 01:06:50,819
hace 20 años con una maestría es

1631
01:06:50,819 --> 01:06:52,619
diferente por ejemplo aquí en Italia con

1632
01:06:52,619 --> 01:06:55,440
respecto a otros países y todas esas

1633
01:06:55,440 --> 01:06:57,000
variables que no están bajo tu

1634
01:06:57,000 --> 01:06:58,859
control no puedes modelarlas usando tu

1635
01:06:58,859 --> 01:07:00,359
red de visión

1636
01:07:00,359 --> 01:07:03,480
pero están ahí bien entonces tú

1637
01:07:03,480 --> 01:07:05,220
no puedes ignorarlos cuando

1638
01:07:05,220 --> 01:07:07,559
quieres sacar conclusiones entonces él es sí, es

1639
01:07:07,559 --> 01:07:08,760
básicamente todo lo que

1640
01:07:08,760 --> 01:07:10,079
no puedes controlar

1641
01:07:10,079 --> 01:07:13,079
puedes inferirlos para que puedas

1642
01:07:13,079 --> 01:07:14,819
realizar una

1643
01:07:14,819 --> 01:07:16,740
inferencia contrafáctica retrocediendo en el tiempo y decir oh

1644
01:07:16,740 --> 01:07:19,020
hace 20 años me habría ganado esto  mucho si

1645
01:07:19,020 --> 01:07:20,640


1646
01:07:20,640 --> 01:07:22,559
fuera tan inteligente que este

1647
01:07:22,559 --> 01:07:24,599
grado en promedio, por supuesto,

1648
01:07:24,599 --> 01:07:27,059
y pero no es que pueda cambiar las

1649
01:07:27,059 --> 01:07:30,720
políticas gubernamentales hacia los trabajos

1650
01:07:30,720 --> 01:07:32,819
o cosas así,

1651
01:07:32,819 --> 01:07:35,099
es un contrafactual más profundo, sí,

1652
01:07:35,099 --> 01:07:38,400
exactamente, sí, esos son el uso

1653
01:07:38,400 --> 01:07:40,200
increíble, está bien,

1654
01:07:40,200 --> 01:07:42,480
tiene  implementaste

1655
01:07:42,480 --> 01:07:45,660
coordenadas generalizadas en la codificación predictiva

1656
01:07:45,660 --> 01:07:46,920
no,

1657
01:07:46,920 --> 01:07:50,039
no, nunca lo he hecho,

1658
01:07:50,039 --> 01:07:52,680
sí, lo he estudiado, pero

1659
01:07:52,680 --> 01:07:55,260
nunca lo he implementado, sé que tienden a

1660
01:07:55,260 --> 01:07:57,599
ser inestables y eh,

1661
01:07:57,599 --> 01:08:00,299
y es  es muy difícil hacerlos estables.

1662
01:08:00,299 --> 01:08:02,940
Creo que eso es lo

1663
01:08:02,940 --> 01:08:05,460
que obtuve al hablar con personas que

1664
01:08:05,460 --> 01:08:08,359
los implementaron,

1665
01:08:08,400 --> 01:08:11,039
pero sí, sí, estoy al tanto de algunos

1666
01:08:11,039 --> 01:08:12,839
documentos que salieron recientemente

1667
01:08:12,839 --> 01:08:15,599
sobre ellos que se probaron en

1668
01:08:15,599 --> 01:08:18,000
alguna carga británica.  estilo de codificador en realidad

1669
01:08:18,000 --> 01:08:20,520
creo que todavía de Baron hay

1670
01:08:20,520 --> 01:08:22,979
un artículo que salió el

1671
01:08:22,979 --> 01:08:25,439
verano pasado, pero no, nunca los he jugado

1672
01:08:25,439 --> 01:08:26,580
con ellos.

1673
01:08:26,580 --> 01:08:29,160


1674
01:08:29,160 --> 01:08:32,040


1675
01:08:32,040 --> 01:08:35,160


1676
01:08:35,160 --> 01:08:38,238


1677
01:08:38,939 --> 01:08:41,698
nivel en eh

1678
01:08:41,698 --> 01:08:43,439
en qué sentido porque el

1679
01:08:43,439 --> 01:08:45,779
problema de destrucción lo da Cycles, así que básicamente

1680
01:08:45,779 --> 01:08:47,399
proporcionas una imagen

1681
01:08:47,399 --> 01:08:49,920
y el hecho de que tienes eh

1682
01:08:49,920 --> 01:08:53,279
parches que salen de la imagen que van

1683
01:08:53,279 --> 01:08:55,799
a las neuronas y luego otros bordes que

1684
01:08:55,799 --> 01:08:57,500
regresan,

1685
01:08:57,500 --> 01:08:59,939
esto básicamente crea el hecho de que

1686
01:08:59,939 --> 01:09:03,560
tienes el error de que básicamente

1687
01:09:03,560 --> 01:09:06,179
estos entrantes se ajustan a los píxeles de

1688
01:09:06,179 --> 01:09:08,339
la imagen, crean algunos

1689
01:09:08,339 --> 01:09:09,719
errores de predicción, por lo que tienes algunos

1690
01:09:09,719 --> 01:09:12,140
errores de predicción que se propagan dentro del modelo

1691
01:09:12,140 --> 01:09:14,640
y eso es sí, y este problema creo que

1692
01:09:14,640 --> 01:09:16,979
es general de ciclos y  probablemente

1693
01:09:16,979 --> 01:09:21,439
no esté relacionado con la jerarquía en general

1694
01:09:23,060 --> 01:09:25,140
para los píxeles

1695
01:09:25,140 --> 01:09:26,759
si no tiene bordes entrantes, ya no

1696
01:09:26,759 --> 01:09:27,660
tiene ningún

1697
01:09:27,660 --> 01:09:30,540
problema de destrucción

1698
01:09:30,540 --> 01:09:33,238
genial y la especificación de la

1699
01:09:33,238 --> 01:09:35,939
red acíclica a través del operador de seguimiento

1700
01:09:35,939 --> 01:09:37,859


1701
01:09:37,859 --> 01:09:41,819
es una técnica muy interesante y ¿

1702
01:09:41,819 --> 01:09:46,339
cuándo se trajo?  en juego

1703
01:09:46,560 --> 01:09:49,140
eh, hasta donde yo sé, creo que salió

1704
01:09:49,140 --> 01:09:52,380
con el artículo que cité en 2018

1705
01:09:52,380 --> 01:09:54,360
No sé al menos en la

1706
01:09:54,360 --> 01:09:56,940
literatura de inferencia causal No estoy al tanto

1707
01:09:56,940 --> 01:09:59,699
de ningún método anterior Diría que no

1708
01:09:59,699 --> 01:10:01,860
porque eso  Quiero decir, ese es el

1709
01:10:01,860 --> 01:10:04,140
artículo muy citado, así que diría que se les

1710
01:10:04,140 --> 01:10:05,520
ocurrió esa idea,

1711
01:10:05,520 --> 01:10:07,980
wow, sí, es bastante bueno que

1712
01:10:07,980 --> 01:10:09,480
puedas hacer un descenso de gradiente y aprender

1713
01:10:09,480 --> 01:10:11,400
la estructura. Creo que es

1714
01:10:11,400 --> 01:10:14,219
una técnica muy poderosa, sí,

1715
01:10:14,219 --> 01:10:15,840
a veces es como cuando miras

1716
01:10:15,840 --> 01:10:17,640
cuando las diferentes

1717
01:10:17,640 --> 01:10:19,440
características de um de la inferencia bayesiana y la

1718
01:10:19,440 --> 01:10:23,159
inferencia causal estuvieron disponibles,

1719
01:10:23,159 --> 01:10:25,620
es realmente notable, por qué

1720
01:10:25,620 --> 01:10:28,500
no se ha hecho esto bajo un

1721
01:10:28,500 --> 01:10:30,719
marco de modelado causal bayesiano, es

1722
01:10:30,719 --> 01:10:32,760
porque solo han pasado entre cinco y

1723
01:10:32,760 --> 01:10:36,659
25 años desde que esto sucedió,

1724
01:10:36,659 --> 01:10:39,960
por lo que es muy, muy corto  y también

1725
01:10:39,960 --> 01:10:42,060
es relativamente técnico, por lo que hay

1726
01:10:42,060 --> 01:10:43,920
relativamente pocos grupos de investigación que participan

1727
01:10:43,920 --> 01:10:46,920
en él y, um, es

1728
01:10:46,920 --> 01:10:49,860
realmente genial lo que está permitiendo

1729
01:10:49,860 --> 01:10:51,960
no, sí, sí, exactamente, quiero decir que también

1730
01:10:51,960 --> 01:10:54,179
creo que la parte emocionante de este campo

1731
01:10:54,179 --> 01:10:56,040
es un poco, quiero decir que

1732
01:10:56,040 --> 01:10:59,100
definitivamente hay  avances por ahí que

1733
01:10:59,100 --> 01:11:01,020
todavía tienen que ser descubiertos y probablemente les gusten

1734
01:11:01,020 --> 01:11:03,000
porque, por ejemplo, como un

1735
01:11:03,000 --> 01:11:05,300
gran avance, ese artículo fue que

1736
01:11:05,300 --> 01:11:07,800
encontraron,

1737
01:11:07,800 --> 01:11:09,960
como si simplemente descubrieran el

1738
01:11:09,960 --> 01:11:12,120
anterior correcto para las estructuras acíclicas,

1739
01:11:12,120 --> 01:11:14,040
está bien, sí,

1740
01:11:14,040 --> 01:11:17,100
quiero decir, no lo sé.  exactamente, pero

1741
01:11:17,100 --> 01:11:19,080
puede ser una idea que tengas en una

1742
01:11:19,080 --> 01:11:21,120
tarde. No conozco la historia

1743
01:11:21,120 --> 01:11:23,040
de cómo se les ocurrió a los demás,

1744
01:11:23,040 --> 01:11:25,320
pero podría ser que ellos

1745
01:11:25,320 --> 01:11:27,239
están allí en la pizarra.

1746
01:11:27,239 --> 01:11:29,280
funciona, eso es un

1747
01:11:29,280 --> 01:11:32,159
gran avance y simplemente

1748
01:11:32,159 --> 01:11:33,960
definí el anterior

1749
01:11:33,960 --> 01:11:36,739
y también muchos de estos avances,

1750
01:11:36,739 --> 01:11:40,500
no solo se apilan, no es como

1751
01:11:40,500 --> 01:11:44,280
una torre de bloques, se colocan en

1752
01:11:44,280 --> 01:11:47,640
capas y se componen, entonces algo se

1753
01:11:47,640 --> 01:11:50,159
generalizará a um

1754
01:11:50,159 --> 01:11:52,140
coordenadas generalizadas o generalizar sincronía o

1755
01:11:52,140 --> 01:11:55,020
gráficos arbitrariamente grandes o

1756
01:11:55,020 --> 01:11:57,239
um Sensor Fusion con entradas multimodales

1757
01:11:57,239 --> 01:12:00,679
y es como si todos se combinaran de

1758
01:12:00,679 --> 01:12:03,659
maneras realmente satisfactorias y efectivas, por lo que incluso

1759
01:12:03,659 --> 01:12:05,640
las cosas pequeñas que nuevamente alguien puede

1760
01:12:05,640 --> 01:12:08,100
pensar en un momento

1761
01:12:08,100 --> 01:12:11,100
um realmente pueden tener un impacto

1762
01:12:11,100 --> 01:12:14,159
um  ok ml Dawn dice muchas gracias por

1763
01:12:14,159 --> 01:12:16,199
hacer mis preguntas y un millón de gracias

1764
01:12:16,199 --> 01:12:18,060
a Tomaso por la inspiradora presentación

1765
01:12:18,060 --> 01:12:21,360
tan agradable oh muchas gracias y luego

1766
01:12:21,360 --> 01:12:23,280
Bert pregunta

1767
01:12:23,280 --> 01:12:25,560
cómo los modelos de lenguaje que usan

1768
01:12:25,560 --> 01:12:27,179
codificación predictiva difieren de los que

1769
01:12:27,179 --> 01:12:30,260
usan Transformers

1770
01:12:31,679 --> 01:12:32,520
um está

1771
01:12:32,520 --> 01:12:35,340
bien, creo que en realidad  si tuviera que

1772
01:12:35,340 --> 01:12:36,659
construir hoy un modelo de lenguaje

1773
01:12:36,659 --> 01:12:38,640
usando codificación predictiva, seguiría usando

1774
01:12:38,640 --> 01:12:40,020
los Transformers,

1775
01:12:40,020 --> 01:12:41,880
así que la idea es que, por ejemplo, si

1776
01:12:41,880 --> 01:12:42,780
tiene,

1777
01:12:42,780 --> 01:12:45,659
digamos, este modelo gráfico jerárquico

1778
01:12:45,659 --> 01:12:48,440
de esta o estas

1779
01:12:48,440 --> 01:12:50,460
redes bayesianas jerárquicas que

1780
01:12:50,460 --> 01:12:53,100
he definido en el  el primero

1781
01:12:53,100 --> 01:12:55,380
desliza una flecha para codificar una función

1782
01:12:55,380 --> 01:12:57,300
que es el mapa lineal, está

1783
01:12:57,300 --> 01:12:59,219
bien, entonces una hora fue simplemente la

1784
01:12:59,219 --> 01:13:01,080
multiplicación de a del vector

1785
01:13:01,080 --> 01:13:03,060
codificado en las variables latentes

1786
01:13:03,060 --> 01:13:06,300
por esta matriz de peso que luego puede

1787
01:13:06,300 --> 01:13:08,580
hacer no lineal y cosas así  pero

1788
01:13:08,580 --> 01:13:09,960
eso puede ser en realidad algo mucho más

1789
01:13:09,960 --> 01:13:12,179
complejo, la función incluida en la

1790
01:13:12,179 --> 01:13:14,880
flecha puede ser una convolución, puede ser un

1791
01:13:14,880 --> 01:13:16,800
mecanismo de atención,

1792
01:13:16,800 --> 01:13:20,820
así que, en realidad, cómo lo haría,

1793
01:13:20,820 --> 01:13:23,880
seguiré usando el quiero decir, que es en realidad

1794
01:13:23,880 --> 01:13:26,460
la forma en que lo hicimos en uh  en el

1795
01:13:26,460 --> 01:13:28,860
Grupo Oxford el año pasado es que teníamos

1796
01:13:28,860 --> 01:13:30,900
exactamente la estructura, cada flecha es un

1797
01:13:30,900 --> 01:13:33,420
Transformador ahora, así que uno es el

1798
01:13:33,420 --> 01:13:35,159
mecanismo de atención y el siguiente es la

1799
01:13:35,159 --> 01:13:38,219
red de avance como Transformadores

1800
01:13:38,219 --> 01:13:40,020
y básicamente la única diferencia que

1801
01:13:40,020 --> 01:13:41,640
tienes es que esos  las variables que

1802
01:13:41,640 --> 01:13:43,739
desea calcular el posterior y

1803
01:13:43,739 --> 01:13:45,239
hace que esos posteriores sean

1804
01:13:45,239 --> 01:13:47,400
independientes de la independencia a través de la aproximación de campo medio VIA,

1805
01:13:47,400 --> 01:13:49,560
por lo que básicamente sigue

1806
01:13:49,560 --> 01:13:51,659
todos los pasos que le permiten

1807
01:13:51,659 --> 01:13:53,520
converger en la

1808
01:13:53,520 --> 01:13:56,520
energía libre de variación de la codificación creativa, pero la

1809
01:13:56,520 --> 01:13:58,199
forma en que usted  calcula las predicciones

1810
01:13:58,199 --> 01:14:01,199
y la forma en que envías las señales

1811
01:14:01,199 --> 01:14:04,739
se hace a través de Transformer,

1812
01:14:04,739 --> 01:14:07,560
así que seguiré usando Transformers en

1813
01:14:07,560 --> 01:14:10,679
general. Quiero decir que funcionan tan bien que

1814
01:14:10,679 --> 01:14:12,840
no creo que podamos ser arrogantes

1815
01:14:12,840 --> 01:14:15,060
y decir oh no, lo haré.  mejor a través de

1816
01:14:15,060 --> 01:14:17,640
una estructura de codificación puramente predictiva,

1817
01:14:17,640 --> 01:14:18,920


1818
01:14:18,920 --> 01:14:21,480
pero de todos modos se aproximará a Transformers,

1819
01:14:21,480 --> 01:14:22,500


1820
01:14:22,500 --> 01:14:24,420
lo siento, dijiste que el aprendizaje de estructuras se

1821
01:14:24,420 --> 01:14:27,540
aproximaría al enfoque de Transformer,

1822
01:14:27,540 --> 01:14:29,219
sí, el aprendizaje de estructuras que mencioné

1823
01:14:29,219 --> 01:14:32,640
anteriormente en uh cuando alguien pregunta

1824
01:14:32,640 --> 01:14:34,800
las similitudes entre la codificación creativa

1825
01:14:34,800 --> 01:14:38,060
y el mecanismo de atención muy,

1826
01:14:38,280 --> 01:14:41,699
sí, muy  interesante,

1827
01:14:41,699 --> 01:14:42,900


1828
01:14:42,900 --> 01:14:45,719
una cosa que me pregunto de Amazon.

1829
01:14:45,719 --> 01:14:47,640
No pude ver el concepto de profundidad en

1830
01:14:47,640 --> 01:14:49,380
las redes de codificación predictiva que

1831
01:14:49,380 --> 01:14:50,880
mencionó. Lo más probable es que me lo haya perdido. La

1832
01:14:50,880 --> 01:14:52,380
definición proporcionada para la

1833
01:14:52,380 --> 01:14:56,480
codificación predictiva involucraba el concepto de profundidad. ¿

1834
01:14:56,640 --> 01:14:59,460


1835
01:14:59,460 --> 01:15:02,219
Qué quiso decir con profundidad?  es porque la

1836
01:15:02,219 --> 01:15:04,980
definición estándar, como dije varias

1837
01:15:04,980 --> 01:15:06,960
veces, es a es jerárquica, tiene

1838
01:15:06,960 --> 01:15:08,400
predicciones que van en una dirección, algún

1839
01:15:08,400 --> 01:15:09,719
error de predicción que va en la dirección opuesta,

1840
01:15:09,719 --> 01:15:10,620


1841
01:15:10,620 --> 01:15:14,340
básicamente lo que hicimos en este

1842
01:15:14,340 --> 01:15:16,320
documento y también en el último en uh

1843
01:15:16,320 --> 01:15:18,420
que se llama  El aprendizaje sobre

1844
01:15:18,420 --> 01:15:19,920
topologías de gráficos arbitrarios que tenemos en

1845
01:15:19,920 --> 01:15:22,260
codificación relativa es que podemos considerar la

1846
01:15:22,260 --> 01:15:25,620
profundidad como un

1847
01:15:25,620 --> 01:15:28,380
independiente,

1848
01:15:28,380 --> 01:15:31,380
básicamente un par de variable latente

1849
01:15:31,380 --> 01:15:33,239
variable latente y flecha

1850
01:15:33,239 --> 01:15:34,739
y tienes predicciones que van en esa

1851
01:15:34,739 --> 01:15:36,300
dirección y la flecha de predicción va

1852
01:15:36,300 --> 01:15:38,340
con la otra pero luego tú  puede componer

1853
01:15:38,340 --> 01:15:41,880
esto de cuántas um muchas maneras para que

1854
01:15:41,880 --> 01:15:45,239
pueda, básicamente, esta

1855
01:15:45,239 --> 01:15:47,040
composición no tiene que ser

1856
01:15:47,040 --> 01:15:48,659
jerárquica al final

1857
01:15:48,659 --> 01:15:50,820
puede tener ciclos para que, por

1858
01:15:50,820 --> 01:15:53,520
ejemplo, pueda conectar otra uh otra

1859
01:15:53,520 --> 01:15:55,440
variable latente a la primera  uno y

1860
01:15:55,440 --> 01:15:57,540
luego conecte a los demás y puede

1861
01:15:57,540 --> 01:15:59,340
tener una estructura tan enredada

1862
01:15:59,340 --> 01:16:00,420
como desee,

1863
01:16:00,420 --> 01:16:02,699
por ejemplo, en el otro documento,

1864
01:16:02,699 --> 01:16:04,500
entrenamos

1865
01:16:04,500 --> 01:16:06,659
la Red que tiene la forma de una

1866
01:16:06,659 --> 01:16:08,460
estructura cerebral, por lo que tenemos muchos

1867
01:16:08,460 --> 01:16:09,900
regiones del cerebro que están escasamente

1868
01:16:09,900 --> 01:16:12,239
conectadas en el interior y están parcialmente

1869
01:16:12,239 --> 01:16:13,860
conectadas entre sí

1870
01:16:13,860 --> 01:16:15,719
y no hay nada

1871
01:16:15,719 --> 01:16:17,640
jerárquico allí al final, pero

1872
01:16:17,640 --> 01:16:18,960
aún puede entrenarlo minimizando la

1873
01:16:18,960 --> 01:16:20,699
energía libre operativa y

1874
01:16:20,699 --> 01:16:22,620
minimizando el error de predicción total

1875
01:16:22,620 --> 01:16:25,159
de la red

1876
01:16:25,159 --> 01:16:27,360
para que pueda tener

1877
01:16:27,360 --> 01:16:31,980
para un motivo dado en un gráfico entrelazado, es

1878
01:16:31,980 --> 01:16:35,159
posible que vea tres capas sucesivas

1879
01:16:35,159 --> 01:16:37,560
que, cuando las miraba solas,

1880
01:16:37,560 --> 01:16:38,820
diría oh, ese es un edificio de tres pisos,

1881
01:16:38,820 --> 01:16:41,940
es un modelo de tres capas que dice

1882
01:16:41,940 --> 01:16:43,980
adaptativo tres, pero luego, cuando toma una

1883
01:16:43,980 --> 01:16:46,620
imagen más grande allí  no es como una parte

1884
01:16:46,620 --> 01:16:50,280
superior o inferior explícita para

1885
01:16:50,280 --> 01:16:52,140
esa red,

1886
01:16:52,140 --> 01:16:54,360
sí, exactamente, y esto se debe básicamente

1887
01:16:54,360 --> 01:16:55,980
al hecho de que cada operación

1888
01:16:55,980 --> 01:16:58,080
en las redes coreanas predictivas es

1889
01:16:58,080 --> 01:16:59,460
estrictamente local,

1890
01:16:59,460 --> 01:17:01,739
por lo que básicamente cada mensaje pasa

1891
01:17:01,739 --> 01:17:03,000
cada predicción y cada

1892
01:17:03,000 --> 01:17:05,280
error de predicción.  que envías, solo lo envías a

1893
01:17:05,280 --> 01:17:08,280
las neuronas muy cercanas, está bien, y si

1894
01:17:08,280 --> 01:17:10,380
la estructura global es realmente

1895
01:17:10,380 --> 01:17:13,380
jerárquica o no, el

1896
01:17:13,380 --> 01:17:16,940
mensaje único que pasa ni siquiera ve eso,

1897
01:17:17,460 --> 01:17:19,620
supongo que esa es

1898
01:17:19,620 --> 01:17:22,820
la esperanza de aprender nuevas

1899
01:17:22,820 --> 01:17:27,739
arquitecturas modelo es el espacio de  lo que está

1900
01:17:27,739 --> 01:17:33,300
diseñado de arriba hacia abajo es muy pequeño y se

1901
01:17:33,300 --> 01:17:36,480
usan muchos modelos hoy en día, aunque

1902
01:17:36,480 --> 01:17:38,640
modelos súper efectivos,

1903
01:17:38,640 --> 01:17:41,100
aunque podría preguntar efectivo por

1904
01:17:41,100 --> 01:17:43,320
unidad de cómputo o no, esa es una

1905
01:17:43,320 --> 01:17:45,300
pregunta de segundo nivel, pero muchos

1906
01:17:45,300 --> 01:17:47,580
modelos efectivos hoy en día no tienen algunos de estos

1907
01:17:47,580 --> 01:17:49,860
Las propiedades de las redes de codificación predictiva,

1908
01:17:49,860 --> 01:17:52,739
como su capacidad

1909
01:17:52,739 --> 01:17:55,520
para usar solo cálculos locales, lo

1910
01:17:55,520 --> 01:17:59,400
que da realismo biológico, o

1911
01:17:59,400 --> 01:18:02,880
simplemente realismo espacio-temporal, pero

1912
01:18:02,880 --> 01:18:06,060
también puede proporcionar muchas ventajas en

1913
01:18:06,060 --> 01:18:08,159


1914
01:18:08,159 --> 01:18:10,500
configuraciones de computación distribuida o computación federada.

1915
01:18:10,500 --> 01:18:12,780
No, sí, exactamente. Estoy completamente de acuerdo

1916
01:18:12,780 --> 01:18:14,520
porque yo  creo que la idea en general es

1917
01:18:14,520 --> 01:18:16,679
esa y no sé si eso

1918
01:18:16,679 --> 01:18:18,540
será una ventaja, así que creo que es muy

1919
01:18:18,540 --> 01:18:20,159
prometedor exactamente por las razones que dijiste

1920
01:18:20,159 --> 01:18:20,880


1921
01:18:20,880 --> 01:18:22,920
y la razón es que el

1922
01:18:22,920 --> 01:18:25,380
modelo de cadena de hoy con propagación hacia atrás

1923
01:18:25,380 --> 01:18:28,860
básicamente puedes resumir  ellos como

1924
01:18:28,860 --> 01:18:32,040
un monitoreo de la propagación hacia atrás es una

1925
01:18:32,040 --> 01:18:34,080
función porque básicamente tiene un

1926
01:18:34,080 --> 01:18:36,120
mapa de entrada a salida y la propagación hacia atrás

1927
01:18:36,120 --> 01:18:39,600
básicamente propaga uh

1928
01:18:39,600 --> 01:18:41,699
información desde su

1929
01:18:41,699 --> 01:18:44,340
gráfico computacional, por lo que cada

1930
01:18:44,340 --> 01:18:45,960
modelo de red neuronal que se usa hoy en día

1931
01:18:45,960 --> 01:18:48,960
es una función mientras que la codificación predictiva

1932
01:18:48,960 --> 01:18:51,179
y otra codificación liberadora como la

1933
01:18:51,179 --> 01:18:53,820
antigua clase de funciones que la clase de

1934
01:18:53,820 --> 01:18:56,040
métodos que entrenan en el uso de

1935
01:18:56,040 --> 01:18:58,500
cálculos locales y en realidad funcionan

1936
01:18:58,500 --> 01:19:01,620
minimizando una función de energía global,

1937
01:19:01,620 --> 01:19:03,840
no se limitan a modelar funciones

1938
01:19:03,840 --> 01:19:05,940
desde la entrada hasta la salida, en realidad modelan

1939
01:19:05,940 --> 01:19:07,739
algo que eso  se parece un poco a los

1940
01:19:07,739 --> 01:19:10,080
sistemas físicos, por lo que tiene un

1941
01:19:10,080 --> 01:19:13,500
sistema físico, fija algunos valores a

1942
01:19:13,500 --> 01:19:15,360
cualquier entrada que tenga y deja que el

1943
01:19:15,360 --> 01:19:17,280
sistema converja y luego lee algún

1944
01:19:17,280 --> 01:19:19,980
otro valor de las neuronas o variables

1945
01:19:19,980 --> 01:19:21,960
que se supone que deben salir, pero este

1946
01:19:21,960 --> 01:19:24,120
sistema físico no  no tiene que ser un

1947
01:19:24,120 --> 01:19:25,920
mapa de ajuste hacia adelante no tiene que ser una

1948
01:19:25,920 --> 01:19:28,260
función que tenga un espacio de entrada y

1949
01:19:28,260 --> 01:19:30,659
un espacio de salida y eso es todo,

1950
01:19:30,659 --> 01:19:32,580
así que la clase de modelos que puedes

1951
01:19:32,580 --> 01:19:34,800
aprender es uh, así que básicamente puedes ver

1952
01:19:34,800 --> 01:19:37,560
como modelos de alimentación hacia adelante  y funciones

1953
01:19:37,560 --> 01:19:39,600
y luego una clase mucho más grande que es

1954
01:19:39,600 --> 01:19:41,880
la de los sistemas físicos. Si hay

1955
01:19:41,880 --> 01:19:43,860
algo interesante aquí, no lo

1956
01:19:43,860 --> 01:19:45,659
sé todavía porque las funciones

1957
01:19:45,659 --> 01:19:47,460
funcionan extremadamente bien. Estamos viendo

1958
01:19:47,460 --> 01:19:50,040
esos días con propagación hacia atrás.

1959
01:19:50,040 --> 01:19:52,199
Funcionan muy bien, pero

1960
01:19:52,199 --> 01:19:53,460
sí.  No sé si hay algo

1961
01:19:53,460 --> 01:19:56,040
interesante en la parte principal, pero la

1962
01:19:56,040 --> 01:19:58,380
parte principal es bastante grande, está bien, hay

1963
01:19:58,380 --> 01:20:00,480
muchos modelos en los que no

1964
01:20:00,480 --> 01:20:02,940
puedes recuperar la propagación y

1965
01:20:02,940 --> 01:20:04,679
puedes entrenar con codificación creativa

1966
01:20:04,679 --> 01:20:06,659
o una propagación de baño u otros

1967
01:20:06,659 --> 01:20:07,860
métodos.

1968
01:20:07,860 --> 01:20:10,440
eso es súper interesante, ciertamente los

1969
01:20:10,440 --> 01:20:12,719
sistemas biológicos, los sistemas físicos

1970
01:20:12,719 --> 01:20:15,900
resuelven todo tipo de problemas interesantes,

1971
01:20:15,900 --> 01:20:17,100


1972
01:20:17,100 --> 01:20:19,380
pero todavía no hay almuerzo gratis

1973
01:20:19,380 --> 01:20:21,540
en las especies de hormigas que funcionan muy bien en

1974
01:20:21,540 --> 01:20:23,100
este entorno, podrían no funcionar muy bien

1975
01:20:23,100 --> 01:20:25,679
en otro entorno y, por lo tanto,

1976
01:20:25,679 --> 01:20:28,260
en las Tierras del Interior.

1977
01:20:28,260 --> 01:20:31,820
puede haber algunos algoritmos especiales realmente únicos

1978
01:20:31,820 --> 01:20:35,880
que no están bien descritos

1979
01:20:35,880 --> 01:20:38,460
por ser una función,

1980
01:20:38,460 --> 01:20:42,060
pero aún así proporcionan una

1981
01:20:42,060 --> 01:20:46,679
forma de procedimiento para implementar heurísticas

1982
01:20:46,679 --> 01:20:48,840
que podrían ser extremadamente extremadamente

1983
01:20:48,840 --> 01:20:51,120
efectivas

1984
01:20:51,120 --> 01:20:53,880
no sí sí exactamente y eh sí cualquier cosa

1985
01:20:53,880 --> 01:20:55,679
esto ha sido la mayor parte de mi

1986
01:20:55,679 --> 01:20:58,260
enfoque  de investigación durante mi doctorado, por

1987
01:20:58,260 --> 01:20:59,100
ejemplo,

1988
01:20:59,100 --> 01:21:01,380
como encontrar esta aplicación que es

1989
01:21:01,380 --> 01:21:04,199
como aquí y no dentro de las

1990
01:21:04,199 --> 01:21:06,739
funciones,

1991
01:21:07,199 --> 01:21:08,820
bueno, ¿a dónde

1992
01:21:08,820 --> 01:21:12,120
va este trabajo desde aquí? ¿

1993
01:21:12,120 --> 01:21:14,520
Qué direcciones le entusiasman

1994
01:21:14,520 --> 01:21:17,340
y cómo ve que las personas en el

1995
01:21:17,340 --> 01:21:19,679
ecosistema de inferencia activa  involucrado en

1996
01:21:19,679 --> 01:21:22,460
este tipo de trabajo,

1997
01:21:22,500 --> 01:21:24,840
creo que es muy probable que la

1998
01:21:24,840 --> 01:21:27,780
dirección más prometedora, que es

1999
01:21:27,780 --> 01:21:30,060
algo que tal vez me gustaría

2000
01:21:30,060 --> 01:21:33,060
explorar un poco, como dije,

2001
01:21:33,060 --> 01:21:34,980
realmente hay que ir detrás de los

2002
01:21:34,980 --> 01:21:37,380
modelos estáticos, así que todo lo que he mostrado  Lo que he

2003
01:21:37,380 --> 01:21:40,260
mostrado hasta ahora es que se trata de datos estáticos,

2004
01:21:40,260 --> 01:21:42,840
por lo que los datos no cambian con el tiempo,

2005
01:21:42,840 --> 01:21:45,780
no hay tiempo dentro de la definición de

2006
01:21:45,780 --> 01:21:48,000
codificación creativa tal como lo presenté

2007
01:21:48,000 --> 01:21:49,080
aquí,

2008
01:21:49,080 --> 01:21:50,940
sin embargo, puede, por ejemplo, generalizar la

2009
01:21:50,940 --> 01:21:53,280
codificación creativa para trabajar  con

2010
01:21:53,280 --> 01:21:55,800
datos temporales usando coordenadas generalizadas como

2011
01:21:55,800 --> 01:21:58,800
mencionaste anteriormente,

2012
01:21:58,800 --> 01:22:01,380
presentándolo como un

2013
01:22:01,380 --> 01:22:04,140
modelo generativo de filtro de Kalman común

2014
01:22:04,140 --> 01:22:08,040
y ahí es donde, por ejemplo, la

2015
01:22:08,040 --> 01:22:09,900
inferencia causal Dirección podría ser muy

2016
01:22:09,900 --> 01:22:12,600
útil porque sí, ese modelo, en

2017
01:22:12,600 --> 01:22:14,400
ese punto, tal vez puedas  ser capaz de

2018
01:22:14,400 --> 01:22:17,880
modelar una mayor causalidad y uh y más

2019
01:22:17,880 --> 01:22:21,780
compleja y útil uh

2020
01:22:21,780 --> 01:22:24,780
causa dinámica de los modelos básicamente

2021
01:22:24,780 --> 01:22:26,940
porque, en general, el cálculo

2022
01:22:26,940 --> 01:22:28,560
y la rama intervencionista y

2023
01:22:28,560 --> 01:22:32,760
contrafáctica de la ciencia se

2024
01:22:32,760 --> 01:22:36,000
desarrolla principalmente en modelos pequeños,

2025
01:22:36,000 --> 01:22:38,159
por lo que es

2026
01:22:38,159 --> 01:22:40,739
como si no  No hago intervenciones en

2027
01:22:40,739 --> 01:22:43,560
modelos gigantes en general, así que si

2028
01:22:43,560 --> 01:22:45,800
miras datos médicos, usan

2029
01:22:45,800 --> 01:22:50,159
redes de visión relativamente pequeñas y,

2030
01:22:50,159 --> 01:22:51,179
por supuesto, si quieres tener un

2031
01:22:51,179 --> 01:22:54,900
modelo causal dinámico que modele un

2032
01:22:54,900 --> 01:22:56,340
entorno específico o una

2033
01:22:56,340 --> 01:22:58,620
realidad específica, tienes un  muchas neuronas adentro

2034
01:22:58,620 --> 01:23:00,780
tienen muchas variables latentes que

2035
01:23:00,780 --> 01:23:02,580
cambian con el tiempo y una intervención en

2036
01:23:02,580 --> 01:23:05,219
un poco más en algún momento crea un

2037
01:23:05,219 --> 01:23:07,560
efecto en un paso de tiempo diferente, así que tal vez

2038
01:23:07,560 --> 01:23:09,239
en el próximo paso de tiempo en 10

2039
01:23:09,239 --> 01:23:11,699
pasos de tiempo diferentes más tarde y creo que eso sería

2040
01:23:11,699 --> 01:23:14,100
sería muy interesante desarrollar una

2041
01:23:14,100 --> 01:23:16,380
forma biológicamente plausible de transmitir

2042
01:23:16,380 --> 01:23:17,699
información

2043
01:23:17,699 --> 01:23:20,040
que también sea capaz de modelar la

2044
01:23:20,040 --> 01:23:22,860
causalidad de grandeza básicamente

2045
01:23:22,860 --> 01:23:24,659
hmm ¿

2046
01:23:24,659 --> 01:23:29,659
dónde ves acción en estos modelos ¿dónde

2047
01:23:30,840 --> 01:23:33,840
veo acción?

2048
01:23:33,840 --> 01:23:36,480
No

2049
01:23:36,480 --> 01:23:38,760
pensé en eso, creo que son las acciones en esos  modelos

2050
01:23:38,760 --> 01:23:41,460
tal vez de la misma manera que yo, como ves en

2051
01:23:41,460 --> 01:23:43,080
otros modelos porque la

2052
01:23:43,080 --> 01:23:44,940
codificación creativa es básicamente un modelo de

2053
01:23:44,940 --> 01:23:46,260
percepción,

2054
01:23:46,260 --> 01:23:49,260
por lo que una acción es que puedes ver que hay una

2055
01:23:49,260 --> 01:23:52,739
consecuencia de lo que estás experimentando,

2056
01:23:52,739 --> 01:23:55,159
así que al cambiar la forma en que

2057
01:23:55,159 --> 01:23:57,840
experimentas algo, entonces puedes  puede

2058
01:23:57,840 --> 01:24:00,060
calcular, tal vez simplemente pueda realizar una

2059
01:24:00,060 --> 01:24:01,800
acción más inteligente ahora que tiene más

2060
01:24:01,800 --> 01:24:03,000
información,

2061
01:24:03,000 --> 01:24:04,560
pero

2062
01:24:04,560 --> 01:24:06,960
sí, no creo que la acción sea muy

2063
01:24:06,960 --> 01:24:10,199
fácil, como sí, no veo ninguna

2064
01:24:10,199 --> 01:24:12,540
consecuencia explícita de las acciones además del hecho de

2065
01:24:12,540 --> 01:24:14,040
que esto puede permitirle básicamente

2066
01:24:14,040 --> 01:24:15,960
tal vez

2067
01:24:15,960 --> 01:24:18,780
simplemente saque mejores conclusiones para que

2068
01:24:18,780 --> 01:24:21,719
realicen acciones en el futuro.

2069
01:24:21,719 --> 01:24:23,940
Agregaré a eso algunas formas en que las

2070
01:24:23,940 --> 01:24:25,920
personas han hablado sobre la

2071
01:24:25,920 --> 01:24:29,340
codificación predictiva y la acción, en primer lugar, la

2072
01:24:29,340 --> 01:24:33,960
acción interna o la acción encubierta es la atención para que

2073
01:24:33,960 --> 01:24:36,120
podamos pensar en la percepción.  como una

2074
01:24:36,120 --> 01:24:37,980
acción interna, ese es un enfoque,

2075
01:24:37,980 --> 01:24:40,560
otro enfoque bastante micro son las

2076
01:24:40,560 --> 01:24:42,840
salidas de un nodo dado, podemos

2077
01:24:42,840 --> 01:24:45,780
entender ese nodo como una

2078
01:24:45,780 --> 01:24:48,780
cosa particular con sus propios estados cognitivos sensoriales y de

2079
01:24:48,780 --> 01:24:52,080
acción y, en ese sentido,

2080
01:24:52,080 --> 01:24:54,960
la salida de un nodo y luego, por último,

2081
01:24:54,960 --> 01:24:57,179
qué  exploramos un poco en la transmisión en vivo

2082
01:24:57,179 --> 01:24:59,940
43 sobre la revisión teórica sobre la

2083
01:24:59,940 --> 01:25:02,100
codificación predictiva que estamos leyendo hasta el

2084
01:25:02,100 --> 01:25:03,840
final y se trataba de la

2085
01:25:03,840 --> 01:25:05,460
percepción, todo sobre la percepción y luego

2086
01:25:05,460 --> 01:25:08,040
fue como la sección 5.3

2087
01:25:08,040 --> 01:25:11,719
si tiene expectativas sobre la acción,

2088
01:25:11,719 --> 01:25:15,900
entonces la acción es solo  otra variable en

2089
01:25:15,900 --> 01:25:18,120
esta arquitectura y que está realmente

2090
01:25:18,120 --> 01:25:20,040
alineada con la inferencia inactiva donde,

2091
01:25:20,040 --> 01:25:21,659
en lugar de tener una función de recompensa o

2092
01:25:21,659 --> 01:25:24,000
utilidad que maximizamos,

2093
01:25:24,000 --> 01:25:26,699
seleccionamos la acción en función de que sea el

2094
01:25:26,699 --> 01:25:28,800
curso de acción más probable, el camino de

2095
01:25:28,800 --> 01:25:30,900
acción mínima que es la mecánica bayesiana,

2096
01:25:30,900 --> 01:25:33,300
por lo que en realidad es muy  natural

2097
01:25:33,300 --> 01:25:36,420
traer una variable de acción y

2098
01:25:36,420 --> 01:25:40,800
utilizarla esencialmente como si fuera una

2099
01:25:40,800 --> 01:25:43,260
predicción sobre otra cosa

2100
01:25:43,260 --> 01:25:45,540
extra receptivamente en el mundo porque

2101
01:25:45,540 --> 01:25:48,480
también estamos esperando acción

2102
01:25:48,480 --> 01:25:50,820
no sí sí exactamente

2103
01:25:50,820 --> 01:25:52,860
no Me gusta mucho la forma de definir acciones en

2104
01:25:52,860 --> 01:25:55,260
realidad y  eh y sigo pensando que

2105
01:25:55,260 --> 01:25:57,239
ha sido como, por ejemplo, no hay

2106
01:25:57,239 --> 01:26:01,139
tantos artículos que apliquen este método.

2107
01:26:01,139 --> 01:26:03,239
Creo que hay un par de eh de

2108
01:26:03,239 --> 01:26:05,100
Alexander o Robria hace algo

2109
01:26:05,100 --> 01:26:08,580
similar, pero en la práctica, fuera de

2110
01:26:08,580 --> 01:26:10,920
la inferencia activa pura, como aplicar

2111
01:26:10,920 --> 01:26:13,260
codificación predictiva.  y las acciones para

2112
01:26:13,260 --> 01:26:15,980
resolver problemas prácticos no se han

2113
01:26:15,980 --> 01:26:19,280
explorado mucho,

2114
01:26:19,679 --> 01:26:23,400
oh, bueno, gracias por esta excelente

2115
01:26:23,400 --> 01:26:25,199
presentación y discusión, ¿hay

2116
01:26:25,199 --> 01:26:27,659
algo más que quiera decir

2117
01:26:27,659 --> 01:26:30,300
o señalar a las personas,

2118
01:26:30,300 --> 01:26:33,360
eh, no, solo muchas gracias por

2119
01:26:33,360 --> 01:26:34,620
invitarme?  y uh,

2120
01:26:34,620 --> 01:26:36,120
fue muy divertido y espero

2121
01:26:36,120 --> 01:26:38,460
volver en algún momento para algunos Future

2122
01:26:38,460 --> 01:26:40,199
Works

2123
01:26:40,199 --> 01:26:41,580
geniales

2124
01:26:41,580 --> 01:26:45,000
en cualquier momento en cualquier momento gracias Thomas así que

2125
01:26:45,000 --> 01:26:49,820
gracias Daniel nos vemos adiós


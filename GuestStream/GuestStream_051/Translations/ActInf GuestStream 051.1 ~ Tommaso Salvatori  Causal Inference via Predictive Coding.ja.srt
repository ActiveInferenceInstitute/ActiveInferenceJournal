1
00:00:19,020 --> 00:00:21,119
こんにちは、

2
00:00:21,119 --> 00:00:23,400


3
00:00:23,400 --> 00:00:28,140
2023 年 7 月 28 日のアクティブ推論ゲスト ストリーム番号 51.1 です。Tomaso

4
00:00:28,140 --> 00:00:31,439
Salvatore と一緒に来ています。

5
00:00:31,439 --> 00:00:33,840


6
00:00:33,840 --> 00:00:37,020


7
00:00:37,020 --> 00:00:39,660
予測コーディングによる最近の研究の因果推論に関するプレゼンテーションとディスカッションを行う予定です。

8
00:00:39,660 --> 00:00:42,480


9
00:00:42,480 --> 00:00:44,340
ご覧になっている方々にご参加いただき、誠にありがとうございます。

10
00:00:44,340 --> 00:00:47,520
ライブ ライブチャットに気軽に質問を書いてください。

11
00:00:47,520 --> 00:00:50,399
ありがとうございます、ありがとう

12
00:00:50,399 --> 00:00:52,980
ございます、ダニエル、招待してくれて、えー、

13
00:00:52,980 --> 00:00:56,039
いつもこのチャンネルの大ファンで、

14
00:00:56,039 --> 00:00:57,719
たくさんのビデオを見てきました、

15
00:00:57,719 --> 00:00:58,920


16
00:00:58,920 --> 00:01:01,379
私はかなりです ここに来て、えー、

17
00:01:01,379 --> 00:01:04,260
今回は自分が話すことに興奮している

18
00:01:04,260 --> 00:01:06,600
ので、私がここ数か月かけて作成した最近のプレプリントについて話したいと思います。

19
00:01:06,600 --> 00:01:08,700


20
00:01:08,700 --> 00:01:11,159


21
00:01:11,159 --> 00:01:12,119


22
00:01:12,119 --> 00:01:15,900
これは

23
00:01:15,900 --> 00:01:18,659
ルックアップとのコラボレーションです。 ケティで 私はマカラク・バラミの

24
00:01:18,659 --> 00:01:21,600
伝説のトーマス・ルカシア[ __ ]にいます、

25
00:01:21,600 --> 00:01:24,000
そしてそれは基本的に

26
00:01:24,000 --> 00:01:26,299
私がオックスフォード大学で働いている会社であるヴァース

27
00:01:26,299 --> 00:01:31,680
とウーヴィアンの間の共同作業です、

28
00:01:31,680 --> 00:01:34,200
それで

29
00:01:34,200 --> 00:01:36,299
この講演の間、

30
00:01:36,299 --> 00:01:38,220
私は

31
00:01:38,220 --> 00:01:40,979
基本的にこれが私が

32
00:01:40,979 --> 00:01:43,140
話し始める講演の概要です。 予測コーディングとは何か、および

33
00:01:43,140 --> 00:01:44,520


34
00:01:44,520 --> 00:01:47,659
その相互作用について

35
00:01:47,659 --> 00:01:51,299
簡単な歴史的紹介 なぜそれがそう

36
00:01:51,299 --> 00:01:54,060
思うのか

37
00:01:54,060 --> 00:01:56,159
創造的なコーディングを研究することは、たとえば

38
00:01:56,159 --> 00:01:58,619
機械学習の観点からも重要です 次に、

39
00:01:58,619 --> 00:02:00,720


40
00:02:00,720 --> 00:02:04,560
因果推論とは何かについて簡単に紹介します

41
00:02:04,560 --> 00:02:07,200
これらの情報がすべて揃ったら、

42
00:02:07,200 --> 00:02:08,880


43
00:02:08,880 --> 00:02:12,540
なぜこの論文を書いたのかについて話し合い、

44
00:02:12,540 --> 00:02:14,520
基本的に

45
00:02:14,520 --> 00:02:16,560
私や他の共同

46
00:02:16,560 --> 00:02:18,300
研究者たちにインスピレーションを与えた研究課題は何だったのかについて話し合い、

47
00:02:18,300 --> 00:02:21,660


48
00:02:21,660 --> 00:02:24,980


49
00:02:24,980 --> 00:02:27,480
推論、つまり介入と

50
00:02:27,480 --> 00:02:29,340
反事実推論をどのように実行するかという主な結果を提示します

51
00:02:29,340 --> 00:02:33,319
。

52
00:02:33,319 --> 00:02:35,879
予測コーディングを使用して特定のデータセットから因果構造を学習する方法、

53
00:02:35,879 --> 00:02:37,920
そしてもちろん、最後に

54
00:02:37,920 --> 00:02:39,959


55
00:02:39,959 --> 00:02:43,500
簡単な要約と、

56
00:02:43,500 --> 00:02:45,840
この研究が実際に影響を与える可能性があると私が信じる理由

57
00:02:45,840 --> 00:02:49,940
と将来の方向性についての議論で締めくくります。

58
00:02:50,700 --> 00:02:53,400
クリエイティブ コーディングとは何ですか

59
00:02:53,400 --> 00:02:55,680
クリエイティブ コーディング これは

60
00:02:55,680 --> 00:02:58,440
神経科学にインスピレーションを得た学習

61
00:02:58,440 --> 00:03:01,140
方法であることで一般的に有名ですが、脳内の情報処理がどのように機能するかについての理論は何でしょうか。

62
00:03:01,140 --> 00:03:04,560


63
00:03:04,560 --> 00:03:05,819


64
00:03:05,819 --> 00:03:08,400
非常に形式的に言えば、

65
00:03:08,400 --> 00:03:10,560
創造的なコーディングの層は

66
00:03:10,560 --> 00:03:12,659
基本的に

67
00:03:12,659 --> 00:03:16,319
脳内のニューロンの階層構造を持っていると説明できます。

68
00:03:16,319 --> 00:03:19,080


69
00:03:19,080 --> 00:03:20,700
脳には 2 つの異なるニューロン ファミリーがあり、

70
00:03:20,700 --> 00:03:23,280
最初のファミリーは

71
00:03:23,280 --> 00:03:24,480
予測情報の送信を担当する

72
00:03:24,480 --> 00:03:27,659
ため、階層の特定の

73
00:03:27,659 --> 00:03:29,959
レベルにあるニューロンが情報を送信し、

74
00:03:29,959 --> 00:03:33,959
その

75
00:03:33,959 --> 00:03:35,940
下のレベル

76
00:03:35,940 --> 00:03:38,340
と 2 番目のファミリーのニューロンの活動を予測します。 ニューロンは、

77
00:03:38,340 --> 00:03:41,099
エラー ニューロンとアロー ニューロンのニューロンであり、

78
00:03:41,099 --> 00:03:43,019
予測エラー情報を階層の

79
00:03:43,019 --> 00:03:46,319
上に送信するため、あるレベルが

80
00:03:46,319 --> 00:03:49,200


81
00:03:49,200 --> 00:03:51,659
このアクティビティの下のレベルのアクティビティを予測します。このアクティビティには、その

82
00:03:51,659 --> 00:03:54,239


83
00:03:54,239 --> 00:03:56,220
下のレベルで実際に起こっている不一致としての予測と

84
00:03:56,220 --> 00:03:57,840
、その情報が含まれます。 予測

85
00:03:57,840 --> 00:04:02,400
エラーは矢印キーに送信されます

86
00:04:02,400 --> 00:04:04,860
が、予測コーディングは

87
00:04:04,860 --> 00:04:07,220
実際には神経科学からの理論として神経科学として焼き付けられたものではなく、

88
00:04:07,220 --> 00:04:10,799


89
00:04:10,799 --> 00:04:11,939


90
00:04:11,939 --> 00:04:13,860
実際には最初は50年代に

91
00:04:13,860 --> 00:04:16,139
信号処理と圧縮の方法として開発されたものでした。

92
00:04:16,139 --> 00:04:19,380


93
00:04:19,380 --> 00:04:21,899
オリバー・エリアスは、実際には

94
00:04:21,899 --> 00:04:25,020
服のシャノンと同時代の人です。シャノンのシャノンは、

95
00:04:25,020 --> 00:04:26,160


96
00:04:26,160 --> 00:04:27,960


97
00:04:27,960 --> 00:04:30,900


98
00:04:30,900 --> 00:04:33,600
データを予測するのにうまく機能する予測器を手に入れたら、それらの予測の

99
00:04:33,600 --> 00:04:36,000
誤りについてのメッセージを送信するほうが、毎日

100
00:04:36,000 --> 00:04:37,919


101
00:04:37,919 --> 00:04:41,100
メッセージ全体を送信するよりもはるかに安価であることに気づきました。

102
00:04:41,100 --> 00:04:42,720


103
00:04:42,720 --> 00:04:45,240


104
00:04:45,240 --> 00:04:47,639


105
00:04:47,639 --> 00:04:49,500


106
00:04:49,500 --> 00:04:52,020
情報理論における信号処理と圧縮メカニズムとして

107
00:04:52,020 --> 00:04:53,639
50 年代に遡って、これがプリティ コーディングの誕生です。実際には

108
00:04:53,639 --> 00:04:57,120
80 年代になってから、

109
00:04:57,120 --> 00:04:59,400
まったく同じモデルが

110
00:04:59,400 --> 00:05:01,800


111
00:05:01,800 --> 00:05:03,540
神経科学でも使用されるようになりました。

112
00:05:03,540 --> 00:05:07,500
マムフォードの研究や

113
00:05:07,500 --> 00:05:10,440
他の研究では、たとえば

114
00:05:10,440 --> 00:05:12,960
プロセス情報の評価方法を説明して、

115
00:05:12,960 --> 00:05:14,520


116
00:05:14,520 --> 00:05:17,160
外界から予測信号を取得し、

117
00:05:17,160 --> 00:05:20,280
この表現を圧縮する必要があり、この

118
00:05:20,280 --> 00:05:22,740
内部表現をニューロン

119
00:05:22,740 --> 00:05:25,199
とメソッドに含める必要があります。  50年代にエリアスとオリバーによって開発されたものと同等ではないにしても、非常に似ています。

120
00:05:25,199 --> 00:05:27,720


121
00:05:27,720 --> 00:05:30,419


122
00:05:30,419 --> 00:05:32,900


123
00:05:32,940 --> 00:05:35,520


124
00:05:35,520 --> 00:05:38,100


125
00:05:38,100 --> 00:05:41,400
バラード周辺の研究のおかげで、おそらく1999年に起こった最大のパラダイムシフトは、

126
00:05:41,400 --> 00:05:44,880
彼らがこの概念を導入したものです。

127
00:05:44,880 --> 00:05:46,199


128
00:05:46,199 --> 00:05:48,060


129
00:05:48,060 --> 00:05:51,360
予測情報は

130
00:05:51,360 --> 00:05:54,240
トップダウン、エラー情報は

131
00:05:54,240 --> 00:05:55,560
ボトムアップという脳内の階層構造について前述しましたが、

132
00:05:55,560 --> 00:05:57,660


133
00:05:57,660 --> 00:05:59,759
彼らはこれまで行われていなかったことを行い、

134
00:05:59,759 --> 00:06:02,820
この理論を

135
00:06:02,820 --> 00:06:05,759
フランスだけでなく、フランスだけでなく、 これは

136
00:06:05,759 --> 00:06:07,979


137
00:06:07,979 --> 00:06:10,139
脳内で学習がどのように機能するかについての理論であり、私たちの

138
00:06:10,139 --> 00:06:13,139
シナプスがどのように更新されるかについての理論でもあります。そして、

139
00:06:13,139 --> 00:06:16,080


140
00:06:16,080 --> 00:06:17,940
この簡単な

141
00:06:17,940 --> 00:06:21,900
歴史的紹介でお話しする最後の大きな進歩は 2003 年のもの

142
00:06:21,900 --> 00:06:25,380
ですが、それから彼はずっと研究を続けていました。

143
00:06:25,380 --> 00:06:28,380
それから数年後、カー・フリーストンのおかげで、

144
00:06:28,380 --> 00:06:32,100
基本的に彼はロビン・バラードの理論を採用し、

145
00:06:32,100 --> 00:06:35,039


146
00:06:35,039 --> 00:06:38,400
それを拡張して

147
00:06:38,400 --> 00:06:40,919
生成モデルの理論に一般化しました。

148
00:06:40,919 --> 00:06:42,720
基本的に、カー・

149
00:06:42,720 --> 00:06:45,479
フリーストンが行った主な主張は、創造的なコーディングは創造的なコーディングであるということです。

150
00:06:45,479 --> 00:06:48,780


151
00:06:48,780 --> 00:06:50,340
特定の種類の生成モデルの証拠最大化スキームについては、

152
00:06:50,340 --> 00:06:52,979


153
00:06:52,979 --> 00:06:55,139
後で紹介するつもりなので、

154
00:06:55,139 --> 00:07:00,300


155
00:07:00,300 --> 00:07:01,560
最初の 2

156
00:07:01,560 --> 00:07:03,900
種類の創造的なコーナーで

157
00:07:03,900 --> 00:07:05,340
説明した信号処理と

158
00:07:05,340 --> 00:07:07,020
圧縮と、

159
00:07:07,020 --> 00:07:09,180
網膜と

160
00:07:09,180 --> 00:07:11,160
脳における一般的な情報処理は推論

161
00:07:11,160 --> 00:07:12,300
方法であり、

162
00:07:12,300 --> 00:07:14,819
最大の

163
00:07:14,819 --> 00:07:17,819
変化は

164
00:07:17,819 --> 00:07:21,120
1999 年に起きた最大の革命です。つまり 21

165
00:07:21,120 --> 00:07:23,580
世紀では、オペレーティブコーディングが

166
00:07:23,580 --> 00:07:25,919
学習アルゴリズムとして見なされ、最初に

167
00:07:25,919 --> 00:07:29,699
情報を圧縮できるようになりました。 次に、生成モデル自体を改善するために、生成モデルにあるすべての

168
00:07:29,699 --> 00:07:31,800
シナプスまたはすべての潜在変数を更新します。

169
00:07:31,800 --> 00:07:34,139


170
00:07:34,139 --> 00:07:38,599


171
00:07:38,759 --> 00:07:43,199
そこで、

172
00:07:43,199 --> 00:07:45,000


173
00:07:45,000 --> 00:07:48,479
操作可能なコーディングが

174
00:07:48,479 --> 00:07:50,220
階層的なガウス生成として見られるように、もう少し正式な定義をいくつか与えましょう。

175
00:07:50,220 --> 00:07:53,400
これは非常に単純な図です。

176
00:07:53,400 --> 00:07:54,780
階層構造は

177
00:07:54,780 --> 00:07:58,319
必要なだけ深くすることができ、

178
00:07:58,319 --> 00:08:01,560
信号予測信号は

179
00:08:01,560 --> 00:08:04,620
1 つの潜在変数 XM から

180
00:08:04,620 --> 00:08:06,599
次の変数に進み、関数 GN または GI を介して毎回変換されます。 先ほども

181
00:08:06,599 --> 00:08:09,720


182
00:08:09,720 --> 00:08:12,620


183
00:08:15,319 --> 00:08:18,180
言いましたが、これは生成モデルです。

184
00:08:18,180 --> 00:08:19,680
この生成モデルの限界確率は何ですか。

185
00:08:19,680 --> 00:08:21,780
まあ、これは単に

186
00:08:21,780 --> 00:08:24,960
最後の確率です。

187
00:08:24,960 --> 00:08:27,660
カーソルが見えますか。はい、そうです。

188
00:08:27,660 --> 00:08:29,940
完璧です。つまり、

189
00:08:29,940 --> 00:08:32,700
最後の頂点の遺伝モデルです。申し訳ありませんが、おそらく

190
00:08:32,700 --> 00:08:34,979
分布です。 最後の頂点の、

191
00:08:34,979 --> 00:08:37,140


192
00:08:37,140 --> 00:08:40,440


193
00:08:40,440 --> 00:08:43,020
前の頂点のアクティビティまたは

194
00:08:43,020 --> 00:08:45,860
潜在変数に条件付けされた他のすべての頂点の確率分布を掛けたものです。

195
00:08:45,899 --> 00:08:48,240
これはガウス

196
00:08:48,240 --> 00:08:50,399
生成モデルであるとすでに述べました。つまり、それらの

197
00:08:50,399 --> 00:08:54,260
確率はガウス形式であり

198
00:08:54,660 --> 00:08:57,120
、すべての

199
00:08:57,120 --> 00:09:00,480
endos 関数です。 一般に関数 G ですが、

200
00:09:00,480 --> 00:09:02,880
特に、

201
00:09:02,880 --> 00:09:05,459
ランブラーの論文やその後のすべての論文では、

202
00:09:05,459 --> 00:09:07,920
深層学習革命のせいで、

203
00:09:07,920 --> 00:09:10,500
これらの関数は

204
00:09:10,500 --> 00:09:13,220
単なる線形マップ、または

205
00:09:13,220 --> 00:09:15,120
活性化

206
00:09:15,120 --> 00:09:18,000
関数を備えた非線形マップ、

207
00:09:18,000 --> 00:09:22,040
または活性化関数と

208
00:09:23,220 --> 00:09:27,180
これにより、

209
00:09:27,180 --> 00:09:28,860
創造的コーディングの正式な定義を与えることができ、

210
00:09:28,860 --> 00:09:30,300
操作的コーディングは、一般に自由エネルギーの変動と呼ばれる量を最小化することによって

211
00:09:30,300 --> 00:09:33,480


212
00:09:33,480 --> 00:09:35,839
モデルの証拠が最大化される、そのような生成モデルの反転スキームであると言えます。

213
00:09:35,839 --> 00:09:38,760


214
00:09:38,760 --> 00:09:40,920


215
00:09:40,920 --> 00:09:43,740
すべての生成モデルの目標は、

216
00:09:43,740 --> 00:09:46,019
モデルの証拠を最大化することですが、

217
00:09:46,019 --> 00:09:48,860
この量は常に手に負えません。

218
00:09:48,860 --> 00:09:51,019


219
00:09:51,019 --> 00:09:53,279


220
00:09:53,279 --> 00:09:55,980


221
00:09:55,980 --> 00:09:58,500


222
00:09:58,500 --> 00:10:00,720
自由エネルギーの逸脱を最小限に抑える代わりに、ソリューションとクリエイティブ コーディングで使用するソリューションを近似できるいくつかのテクニックがあります。

223
00:10:00,720 --> 00:10:03,480
これは

224
00:10:03,480 --> 00:10:06,839
この研究におけるモデル証拠の下限であり、

225
00:10:06,839 --> 00:10:09,660
実際には他の多くの証拠にも含まれている

226
00:10:09,660 --> 00:10:11,700
ため、これを行う標準的な方法です。

227
00:10:11,700 --> 00:10:13,740
この最小化は

228
00:10:13,740 --> 00:10:16,080
成分降下で実行されます。

229
00:10:16,080 --> 00:10:18,540
はい、降下で合意した実行を実行します。

230
00:10:18,540 --> 00:10:19,980
実際には、

231
00:10:19,980 --> 00:10:22,140
期待値の最大化などの他の方法も同様です

232
00:10:22,140 --> 00:10:23,580


233
00:10:23,580 --> 00:10:25,140
が、たとえば信念伝播などの他のメッセージ パッシング アルゴリズムを使用して、

234
00:10:25,140 --> 00:10:26,940


235
00:10:26,940 --> 00:10:29,959


236
00:10:30,720 --> 00:10:33,980
時間を少し遡って、

237
00:10:33,980 --> 00:10:35,940


238
00:10:35,940 --> 00:10:38,760


239
00:10:38,760 --> 00:10:41,360
創造的なコーディングが見られる場合は統計生成モデルについて少し忘れることもできます。

240
00:10:41,360 --> 00:10:44,040
つまり、ニューラル活動を含む階層モデルとしてすでに何度か述べました。つまり、

241
00:10:44,040 --> 00:10:46,200


242
00:10:46,200 --> 00:10:48,420
ニューロン活動を

243
00:10:48,420 --> 00:10:50,700
表すニューロンの潜在変数では、

244
00:10:50,700 --> 00:10:53,459
送信側信号が階層の下にあり、

245
00:10:53,459 --> 00:10:54,899


246
00:10:54,899 --> 00:10:57,540
エラー ノードまたはエラー ニューロンでは、

247
00:10:57,540 --> 00:11:01,019
送信側信号が階層の上にあります。

248
00:11:01,019 --> 00:11:03,660
エラー情報は、

249
00:11:03,660 --> 00:11:05,700


250
00:11:05,700 --> 00:11:08,220
このクラス操作コーディング モデルの自由エネルギーの変動は何ですか。これは、

251
00:11:08,220 --> 00:11:09,899
単に

252
00:11:09,899 --> 00:11:12,720
すべてのエラー ニューロンの平均二乗

253
00:11:12,720 --> 00:11:14,399


254
00:11:14,399 --> 00:11:18,120
誤差の合計です。つまり、 は、合計誤差の二乗の誤差の合計であり

255
00:11:18,120 --> 00:11:21,980


256
00:11:22,019 --> 00:11:24,480
、この表現は次のようになります。

257
00:11:24,480 --> 00:11:27,120
後のスライドで、たとえば、因果推論をモデル化するためにクリエイティブ コーディングを

258
00:11:27,120 --> 00:11:28,740
使用する方法を説明するときに役立ちます。

259
00:11:28,740 --> 00:11:30,120


260
00:11:30,120 --> 00:11:32,940


261
00:11:32,940 --> 00:11:34,800
予測コーディングは重要だと思いますが、そうではありません。

262
00:11:34,800 --> 00:11:36,240


263
00:11:36,240 --> 00:11:37,500


264
00:11:37,500 --> 00:11:39,600
まず最初によく勉強するのが良いアルゴリズムです。 先ほども言いましたが、

265
00:11:39,600 --> 00:11:41,399


266
00:11:41,399 --> 00:11:43,079
モデルの証拠や周辺

267
00:11:43,079 --> 00:11:44,339
尤度である正しい目的を最適化し

268
00:11:44,339 --> 00:11:45,660
、

269
00:11:45,660 --> 00:11:47,700
それから、先ほど述べたように自由エネルギーの変化と呼ばれる下限を最適化することによって最適化します。

270
00:11:47,700 --> 00:11:49,440


271
00:11:49,440 --> 00:11:52,440
仮想的な

272
00:11:52,440 --> 00:11:54,240
仕上がりは興味深いものです。

273
00:11:54,240 --> 00:11:57,680
2 つの異なる項の合計

274
00:11:57,680 --> 00:12:00,839
と、最適化の各項は、

275
00:12:00,839 --> 00:12:04,680


276
00:12:04,680 --> 00:12:06,899
たとえば機械学習タスク

277
00:12:06,899 --> 00:12:09,060
や一般的な学習タスクにおいて重要な影響を与えるため、

278
00:12:09,060 --> 00:12:12,420
これらの項の 1 つは暗記を強制するため、

279
00:12:12,420 --> 00:12:15,440
2 番目の項では基本的に

280
00:12:15,440 --> 00:12:18,180
モデルを強制します。 特定のデータセットに適合させるため、

281
00:12:18,180 --> 00:12:19,560


282
00:12:19,560 --> 00:12:21,240
最初の項は

283
00:12:21,240 --> 00:12:23,519
モデルに複雑さを最小限に抑えるよう強制します。

284
00:12:23,519 --> 00:12:26,040
たとえば、

285
00:12:26,040 --> 00:12:28,500
結果のかみそり

286
00:12:28,500 --> 00:12:31,260
理論でわかるように、特定のトレーニング セットで同様に実行する 2 つの異なるモデルがある場合、

287
00:12:31,260 --> 00:12:33,000


288
00:12:33,000 --> 00:12:35,640
私たちが持っているモデルは、 を取得し、

289
00:12:35,640 --> 00:12:37,380


290
00:12:37,380 --> 00:12:39,899
最も一般化すると期待されるものはそれほど

291
00:12:39,899 --> 00:12:41,160
複雑ではない

292
00:12:41,160 --> 00:12:44,100
ため、操作自由エネルギーを介して生成モデルを更新すると、基本

293
00:12:44,100 --> 00:12:46,380


294
00:12:46,380 --> 00:12:47,779
的に

295
00:12:47,779 --> 00:12:51,959
最適な結果のカミソリ モデルに収束することができます。

296
00:12:51,959 --> 00:12:54,720


297
00:12:54,720 --> 00:12:56,100
これは、データ セットを記憶するだけでなく、

298
00:12:56,100 --> 00:12:58,680
目に見えないデータポイントを非常によく一般化できる

299
00:12:58,680 --> 00:13:00,240


300
00:13:00,240 --> 00:13:02,639
オペレーティブコーディングが重要である 2 番目の理由は、

301
00:13:02,639 --> 00:13:08,600
実際には

302
00:13:08,720 --> 00:13:11,760


303
00:13:11,760 --> 00:13:13,920
階層構造で定義する必要はないが、有向グラフィカルなどの

304
00:13:13,920 --> 00:13:15,959
より複雑で柔軟なアーキテクチャでモデル化できることです。

305
00:13:15,959 --> 00:13:18,240


306
00:13:18,240 --> 00:13:21,540
任意の形状のモデル、または脳領域に似た多くのサイクルを持つネットワークにさらに一般化されます。

307
00:13:21,540 --> 00:13:23,700


308
00:13:23,700 --> 00:13:25,920


309
00:13:25,920 --> 00:13:27,779
根本的な理由は、最終的な結果は、

310
00:13:27,779 --> 00:13:30,300


311
00:13:30,300 --> 00:13:32,339
前方パスで学習および予測してから

312
00:13:32,339 --> 00:13:34,260
誤差を逆伝播していないことですが、

313
00:13:34,260 --> 00:13:36,600
エネルギー関数を最小化する

314
00:13:36,600 --> 00:13:38,459
と、基本的にあらゆる種類の

315
00:13:38,459 --> 00:13:39,839
階層が可能になり、

316
00:13:39,839 --> 00:13:41,180


317
00:13:41,180 --> 00:13:43,860
直接キーの後ろに移動して

318
00:13:43,860 --> 00:13:46,860
サイクルを学習できるようになります。これは

319
00:13:46,860 --> 00:13:48,060
実際には非常に重要です。なぜなら、最近の論文から

320
00:13:48,060 --> 00:13:50,399
いくつかの情報があるので、脳はサイクルでいっぱいだからです。

321
00:13:50,399 --> 00:13:53,399


322
00:13:53,399 --> 00:13:56,459
うーん、ショウジョウバエなどの一部の動物の脳を完全にマッピングすることができました。

323
00:13:56,459 --> 00:13:59,279


324
00:13:59,279 --> 00:14:00,420


325
00:14:00,420 --> 00:14:03,899
脳はサイクルでいっぱいです。そのため、

326
00:14:03,899 --> 00:14:06,720
機械学習

327
00:14:06,720 --> 00:14:09,000
モデルまたは

328
00:14:09,000 --> 00:14:11,160
一般的なモデルを、サイクリックを使用して排出できるアルゴリズムを使用して

329
00:14:11,160 --> 00:14:14,160
排出するのは理にかなっています。

330
00:14:14,160 --> 00:14:17,160


331
00:14:17,160 --> 00:14:19,380
オペレーティブ コーディングが

332
00:14:19,380 --> 00:14:21,240
興味深い 3 番目の理由は、ブラックの伝播から始まる標準的なニューラル ネットワークより

333
00:14:21,240 --> 00:14:23,820
も堅牢であることが正式に証明されているため、

334
00:14:23,820 --> 00:14:25,139


335
00:14:25,139 --> 00:14:27,060


336
00:14:27,060 --> 00:14:28,200
ニューラル ネットワークがあり、分類タスクを実行したい場合は、

337
00:14:28,200 --> 00:14:30,320


338
00:14:30,320 --> 00:14:34,139
クリエイティブ コーディングの方が堅牢であり

339
00:14:34,139 --> 00:14:36,260
、 これは、

340
00:14:36,260 --> 00:14:38,339


341
00:14:38,339 --> 00:14:40,680
小規模なデータセットでのオンライン学習トレーニングや

342
00:14:40,680 --> 00:14:43,440
継続学習タスクなどのタスクでは興味深いものであり、この理論は

343
00:14:43,440 --> 00:14:45,540
基本的に、

344
00:14:45,540 --> 00:14:48,540
命令型コーディングが、

345
00:14:48,540 --> 00:14:50,820
陽的勾配降下法とは

346
00:14:50,820 --> 00:14:53,339
別のバージョンである

347
00:14:53,339 --> 00:14:54,899
暗黙的勾配降下法に近似するように移行されたという事実から来ています。

348
00:14:54,899 --> 00:14:57,180


349
00:14:57,180 --> 00:14:59,880
基本的にすべての単一モデルで使用される標準の緑色降下であり、

350
00:14:59,880 --> 00:15:03,680
これはより堅牢なバリエーションです。

351
00:15:05,880 --> 00:15:08,279
かなり長いイントラン操作コーディングを行ったので大丈夫だと思います。

352
00:15:08,279 --> 00:15:09,779
今から

353
00:15:09,779 --> 00:15:11,639
2 番目のトピックである因果推論

354
00:15:11,639 --> 00:15:13,019


355
00:15:13,019 --> 00:15:15,839
とその内容に移ろうと思います。 因果推論 因果

356
00:15:15,839 --> 00:15:18,420
推論は理論です それはジュディ・アパレルによって最も

357
00:15:18,420 --> 00:15:20,339
公式化された非常に一般的な理論です 彼は間違い

358
00:15:20,339 --> 00:15:23,100
なくフランスの因果関係の分野

359
00:15:23,100 --> 00:15:25,500
で最も重要な人物です

360
00:15:25,500 --> 00:15:27,839
彼はいくつかの非常に素晴らしい本を書いています

361
00:15:27,839 --> 00:15:29,760
例えばYの本は

362
00:15:29,760 --> 00:15:32,760
強くお勧めします

363
00:15:32,760 --> 00:15:35,220
このトピックについてさらに詳しく知りたい場合は、

364
00:15:35,220 --> 00:15:37,800
基本的に次の問題に取り組みます。

365
00:15:37,800 --> 00:15:38,639


366
00:15:38,639 --> 00:15:40,440
そのため、

367
00:15:40,440 --> 00:15:42,000


368
00:15:42,000 --> 00:15:44,160
ベイジアン ネットワークに関連付けられた結合確率分布があると仮定します。これは、特に次のような場合に、この

369
00:15:44,160 --> 00:15:46,199


370
00:15:46,199 --> 00:15:49,260
論文全体にわたる実行例になります。

371
00:15:49,260 --> 00:15:51,839


372
00:15:51,839 --> 00:15:54,480
この形のアジアのネットワークはありません。

373
00:15:54,480 --> 00:15:57,660
ネットワークに基づいています。

374
00:15:57,660 --> 00:16:00,240
内部の変数は

375
00:16:00,240 --> 00:16:02,100
さまざまな量を表すことができます。たとえば、

376
00:16:02,100 --> 00:16:04,620
この形のビジュアル ネットワークは

377
00:16:04,620 --> 00:16:06,899


378
00:16:06,899 --> 00:16:08,820
右側の量を表すことができるので、社会的に

379
00:16:08,820 --> 00:16:10,800
経済的です。個人のスタジオ像

380
00:16:10,800 --> 00:16:13,079
です。 教育レベル

381
00:16:13,079 --> 00:16:16,699
知能と所得レベル

382
00:16:17,100 --> 00:16:19,440


383
00:16:19,440 --> 00:16:22,920
古典的な統計が得意とすること、そしてそれは何か えーっと、

384
00:16:22,920 --> 00:16:25,320
最もよく使われる応用は

385
00:16:25,320 --> 00:16:28,019
観測値や

386
00:16:28,019 --> 00:16:29,279
相関関係をモデル化することです 相関関係は基本的に質問に答えます

387
00:16:29,279 --> 00:16:32,519


388
00:16:32,519 --> 00:16:35,579
別の変数 C を観測した場合、

389
00:16:35,579 --> 00:16:37,500
たとえば この場合、

390
00:16:37,500 --> 00:16:39,660
収入レベルはいくらですか、この教育レベルを観察した

391
00:16:39,660 --> 00:16:41,820
場合の個人の予想収入レベル、

392
00:16:41,820 --> 00:16:44,339


393
00:16:44,339 --> 00:16:48,180
そしてもちろん、その人が

394
00:16:48,180 --> 00:16:50,220


395
00:16:50,220 --> 00:16:52,500
修士号や博士号などのより高い学歴を持っている場合、

396
00:16:52,500 --> 00:16:54,360
その人は一般的に持っていると期待しています

397
00:16:54,360 --> 00:16:56,040
所得水準が高いほど、

398
00:16:56,040 --> 00:16:58,139
これは相関関係です

399
00:16:58,139 --> 00:17:00,300
が、

400
00:17:00,300 --> 00:17:03,300
観察するのが非常に難しいことが時々ありますが、それらは

401
00:17:03,300 --> 00:17:05,040
それらの数量を決定する上で大きな役割を果たしているため、

402
00:17:05,040 --> 00:17:06,119


403
00:17:06,119 --> 00:17:08,220
たとえば、

404
00:17:08,220 --> 00:17:11,160
所得水準は、

405
00:17:11,160 --> 00:17:13,380
ある人の知性によってはるかに定義される可能性があります。

406
00:17:13,380 --> 00:17:15,540
特定の人、

407
00:17:15,540 --> 00:17:18,720
そしておそらく知性や

408
00:17:18,720 --> 00:17:21,000
知性がある人は

409
00:17:21,000 --> 00:17:24,540
高等教育レベルを持っている可能性が最も高いです

410
00:17:24,540 --> 00:17:27,540
が、それでも

411
00:17:27,540 --> 00:17:30,120
収入が私である本当の理由はIQのせいであり

412
00:17:30,120 --> 00:17:32,220


413
00:17:32,220 --> 00:17:34,740
、これはそうではありません

414
00:17:34,740 --> 00:17:36,360
単純な相関関係による研究であり、介入と呼ばれる

415
00:17:36,360 --> 00:17:39,120
より高度な技術によって研究する必要があります。

416
00:17:39,120 --> 00:17:41,280


417
00:17:41,280 --> 00:17:43,320
介入は基本的に、

418
00:17:43,320 --> 00:17:46,500
C を特定の値に変更すると D は何になるかという質問に答えます。

419
00:17:46,500 --> 00:17:48,240


420
00:17:48,240 --> 00:17:51,000
たとえば、個人を取得できます。

421
00:17:51,000 --> 00:17:54,660
彼の収入レベルを確認し、

422
00:17:54,660 --> 00:17:57,120
教育レベルを変更します。

423
00:17:57,120 --> 00:17:59,220
この世界に介入して、

424
00:17:59,220 --> 00:18:01,080


425
00:18:01,080 --> 00:18:03,419
彼の知性には触れずに教育レベルを変更し、その収入がどのくらい変化するかを確認します。

426
00:18:03,419 --> 00:18:07,260


427
00:18:07,260 --> 00:18:09,900
たとえば、収入が大きく変化する場合、それは知性が変化していないことを

428
00:18:09,900 --> 00:18:12,179
意味します

429
00:18:12,179 --> 00:18:14,460
。 これには大きな役割はありませんが、

430
00:18:14,460 --> 00:18:16,799
教育レベルは影響します 収入レベルが

431
00:18:16,799 --> 00:18:19,020
あまり変わらない場合、

432
00:18:19,020 --> 00:18:20,640
この場合、隠れた変数がある可能性があることを意味します

433
00:18:20,640 --> 00:18:22,860


434
00:18:22,860 --> 00:18:25,760
人の収入レベルを決定する知性

435
00:18:25,980 --> 00:18:28,740
3番目の重要な因果

436
00:18:28,740 --> 00:18:31,080
推論は、

437
00:18:31,080 --> 00:18:33,120
したがって、たとえば、反事実はどうなるかという質問に答え、

438
00:18:33,120 --> 00:18:36,720


439
00:18:36,720 --> 00:18:39,240
C を過去の別の値に変更します。

440
00:18:39,240 --> 00:18:40,679
これにより、たとえば、

441
00:18:40,679 --> 00:18:42,059
介入と

442
00:18:42,059 --> 00:18:45,059
反事実の違いは、介入が

443
00:18:45,059 --> 00:18:47,820
将来に作用するかどうかであることがわかります。

444
00:18:47,820 --> 00:18:50,340
現在の世界では、将来の変化を観察するために、

445
00:18:50,340 --> 00:18:53,220
十分に反事実的であるため、

446
00:18:53,220 --> 00:18:56,039
過去に戻って変数を変更し、

447
00:18:56,039 --> 00:18:59,160
その変化が現在私たちが住んでいる世界にどのような影響を与えたかを確認できます。

448
00:18:59,160 --> 00:19:01,320


449
00:19:01,320 --> 00:19:02,940


450
00:19:02,940 --> 00:19:06,299
これらはjudappleによって次の3つとして定義されています。

451
00:19:06,299 --> 00:19:08,100
因果推論のレベル

452
00:19:08,100 --> 00:19:09,660
相関関係は第 1 レベルです

453
00:19:09,660 --> 00:19:11,580
介入は反事実の第 2 レベルです

454
00:19:11,580 --> 00:19:14,720


455
00:19:16,020 --> 00:19:18,120
他の介入は第 3 レベルです 直感的な定義を

456
00:19:18,120 --> 00:19:20,640
与えたので、ここでこれらをより正式に定義します。

457
00:19:20,640 --> 00:19:23,760


458
00:19:23,760 --> 00:19:25,500
ここではこの表記を使用します。 これは

459
00:19:25,500 --> 00:19:27,240
実際にはプレゼンテーション全体を通して同じな

460
00:19:27,240 --> 00:19:29,640
ので、X は常に

461
00:19:29,640 --> 00:19:32,820
潜在変数 s i は常に

462
00:19:32,820 --> 00:19:35,340
データポイントまたは観測値になり、

463
00:19:35,340 --> 00:19:38,520
VI は常に頂点になるため、

464
00:19:38,520 --> 00:19:40,860
VI が表示されるたびに、

465
00:19:40,860 --> 00:19:42,720


466
00:19:42,720 --> 00:19:45,299
たとえば、グラフの構造に興味がある

467
00:19:45,299 --> 00:19:46,860
ので、

468
00:19:46,860 --> 00:19:50,160


469
00:19:50,160 --> 00:19:52,679


470
00:19:52,679 --> 00:19:54,780


471
00:19:54,780 --> 00:19:57,840
X3 が S3 に等しいとすると、前のスライドで見たベイジアン モデルと同じ構造を持つベイジアン モデルがあると仮定します。これが

472
00:19:57,840 --> 00:20:00,660
統計で許可される観察です。

473
00:20:00,660 --> 00:20:03,360


474
00:20:03,360 --> 00:20:04,679


475
00:20:04,679 --> 00:20:07,380


476
00:20:07,380 --> 00:20:09,240


477
00:20:09,240 --> 00:20:13,860
X3 が S3

478
00:20:13,860 --> 00:20:15,679
外部

479
00:20:15,679 --> 00:20:17,760
介入に等しいとすると、この頂点に関連する潜在変数である X4 の確率または期待値を計算します。 do 演算と呼ばれる新しい種類の表記が必要です。

480
00:20:17,760 --> 00:20:19,919


481
00:20:19,919 --> 00:20:21,179


482
00:20:21,179 --> 00:20:23,880
したがって、この場合は

483
00:20:23,880 --> 00:20:26,100
X4 を計算します。 単語

484
00:20:26,100 --> 00:20:30,000
に介入して

485
00:20:30,000 --> 00:20:33,059
X3 West 3 を変更するという事実を考慮した場合の X4 の確率。

486
00:20:33,059 --> 00:20:35,580


487
00:20:35,580 --> 00:20:38,400
介入を実行するにはどうすればよいですか。Judo Pearl は、

488
00:20:38,400 --> 00:20:40,020


489
00:20:40,020 --> 00:20:41,880


490
00:20:41,880 --> 00:20:45,059
最初に相関関係を計算する前に中間ステップが必要であることを示しています。

491
00:20:45,059 --> 00:20:46,860


492
00:20:46,860 --> 00:20:50,160
V3 へのすべての入力エッジを削除するには、すべてを削除する必要がある

493
00:20:50,160 --> 00:20:52,799
ため、このベイジアン ネットワークではなく、

494
00:20:52,799 --> 00:20:55,679
この 2 番目のネットワークを研究する

495
00:20:55,679 --> 00:20:58,200
必要があります。この時点で、通常どおり

496
00:20:58,200 --> 00:21:00,840
相関関係を計算できます

497
00:21:00,840 --> 00:21:03,299


498
00:21:03,299 --> 00:21:06,500
。これは

499
00:21:07,020 --> 00:21:09,299
事実に反する介入です。 これはこれを一般化したもので、

500
00:21:09,299 --> 00:21:11,700
私が言ったように、

501
00:21:11,700 --> 00:21:14,100
彼らは構造的因果モデルを使用してコンピューティングしています。

502
00:21:14,100 --> 00:21:15,419


503
00:21:15,419 --> 00:21:18,299
構造的因果モデルは

504
00:21:18,299 --> 00:21:21,120
概念的にはベイジアン ネットワークに似たタプルです

505
00:21:21,120 --> 00:21:23,460
が、基本的には

506
00:21:23,460 --> 00:21:26,220
この新しいクラスの変数がその上にあります。

507
00:21:26,220 --> 00:21:28,580
は彼らが使用する観察不可能な変数な

508
00:21:28,580 --> 00:21:30,960
ので、

509
00:21:30,960 --> 00:21:34,020
X1 X2 X3 S4 より前にあったベイジアン ネットワークがあります

510
00:21:34,020 --> 00:21:37,460
が、これらの観察不可能な

511
00:21:37,460 --> 00:21:40,020
変数や環境に依存する変数もあります。

512
00:21:40,020 --> 00:21:42,539
それらは制御できません。推測はできますが、

513
00:21:42,539 --> 00:21:43,980


514
00:21:43,980 --> 00:21:46,020
それらはそこにあり

515
00:21:46,020 --> 00:21:48,539
、

516
00:21:48,539 --> 00:21:51,360
f これは基本的にすべてに依存する関数のセットです。x

517
00:21:51,360 --> 00:21:53,400


518
00:21:53,400 --> 00:21:57,299
の f と x3 は X1 に依存します。

519
00:21:57,299 --> 00:21:58,980
なぜなら x2 には矢印があるからです。

520
00:21:58,980 --> 00:22:00,960
矢印があるからです。また、

521
00:22:00,960 --> 00:22:02,940


522
00:22:02,940 --> 00:22:05,840
極端な影響を与える観察不可能な変数にも依存するので、

523
00:22:06,179 --> 00:22:09,240
直感的に私たちを見ることができます。

524
00:22:09,240 --> 00:22:11,940
考えることができます。

525
00:22:11,940 --> 00:22:14,159
これらの観察

526
00:22:14,159 --> 00:22:16,679
不可能な変数を最上位に持つベイジアンネットワークとして構造的因果モデルを作成し、各観察

527
00:22:16,679 --> 00:22:19,500
不可能な変数は

528
00:22:19,500 --> 00:22:22,020
それ

529
00:22:22,020 --> 00:22:24,600
自体の最新の変数 X にのみ影響を与えるため、たとえば

530
00:22:24,600 --> 00:22:27,960
IU は決して X1 に触れることはなく、u3 は

531
00:22:27,960 --> 00:22:30,360
Q3 のみに触れます。E1 はすべて X1 に影響を与えます。

532
00:22:30,360 --> 00:22:34,039
などなど、

533
00:22:35,039 --> 00:22:37,679
反事実推論を実行すると、次

534
00:22:37,679 --> 00:22:39,900
の質問に答えます。つまり、外部のパス状況で

535
00:22:39,900 --> 00:22:42,960
X3 が別の変数と等しい場合、X4 は何になるでしょうか。3

536
00:22:42,960 --> 00:22:46,620


537
00:22:46,620 --> 00:22:49,340


538
00:22:49,340 --> 00:22:51,840
つの異なるステップが必要です。したがって、

539
00:22:51,840 --> 00:22:53,039
アブダクションは

540
00:22:53,039 --> 00:22:54,900


541
00:22:54,900 --> 00:22:57,179
すべての背景変数の計算です。

542
00:22:57,179 --> 00:22:59,460
このステップでは、

543
00:22:59,460 --> 00:23:01,200
時間を遡って、

544
00:23:01,200 --> 00:23:03,419


545
00:23:03,419 --> 00:23:04,919


546
00:23:04,919 --> 00:23:08,039
その特定の瞬間に観測不可能な環境がどのような状態であったかを理解したいと考えています。

547
00:23:08,039 --> 00:23:11,039
これを行うには、すべての潜在

548
00:23:11,039 --> 00:23:14,280
変数 X を、

549
00:23:14,280 --> 00:23:16,140
すでに持っている特定のデータに固定し、

550
00:23:16,140 --> 00:23:18,960
これらを実行します。

551
00:23:18,960 --> 00:23:21,120
使用済みの推論では、

552
00:23:21,120 --> 00:23:24,240
U を使用して

553
00:23:24,240 --> 00:23:26,940
学習した U を保持し、

554
00:23:26,940 --> 00:23:28,500
介入を実行します。

555
00:23:28,500 --> 00:23:29,880
これにより、

556
00:23:29,880 --> 00:23:32,340
カウンターファクチャは、

557
00:23:32,340 --> 00:23:34,980


558
00:23:34,980 --> 00:23:36,960
環境

559
00:23:36,960 --> 00:23:40,620
変数 U1 U2 を知っている過去に戻る介入とみなすこともできます。 その特定の瞬間における u4 と、

560
00:23:40,620 --> 00:23:43,039


561
00:23:43,200 --> 00:23:44,340


562
00:23:44,340 --> 00:23:46,679
欠落しているステップは何ですか。

563
00:23:46,679 --> 00:23:49,440


564
00:23:49,440 --> 00:23:50,780


565
00:23:50,780 --> 00:23:53,280


566
00:23:53,280 --> 00:23:55,980
では、X3 における X4 は、その特定の状況で別の別のデータ ポイントに等しいと考えられます。これで、相関関係を計算できるようになり、

567
00:23:55,980 --> 00:23:57,120


568
00:23:57,120 --> 00:23:59,520


569
00:23:59,520 --> 00:24:02,039
グラフ上のパスで相関関係を計算できます。 ここでは、

570
00:24:02,039 --> 00:24:04,440


571
00:24:04,440 --> 00:24:06,659


572
00:24:06,659 --> 00:24:10,140
アブダクションステップで学習した環境変数を使用してすでに介入を実行していますが、

573
00:24:10,140 --> 00:24:14,419
これは反事実的推論です。

574
00:24:15,480 --> 00:24:18,000
これは因果

575
00:24:18,000 --> 00:24:20,159
推論の現在の導入部の最後のスライドであり、

576
00:24:20,159 --> 00:24:21,720
構造学習に関するものであり、

577
00:24:21,720 --> 00:24:23,880
基本的には私が述べたことのすべてです

578
00:24:23,880 --> 00:24:27,360
これまでのところ、データポイント間の因果関係がわかっているという事実に依存している

579
00:24:27,360 --> 00:24:29,700


580
00:24:29,700 --> 00:24:31,500
ため、

581
00:24:31,500 --> 00:24:33,120
グラフの構造がわかり、どの変数がどの変数に影響を与えるかがわかり、

582
00:24:33,120 --> 00:24:34,860


583
00:24:34,860 --> 00:24:37,260
一般に矢印がわかっています

584
00:24:37,260 --> 00:24:39,659
が、実際にはこれは

585
00:24:39,659 --> 00:24:42,900
常に可能であるとは限りません。

586
00:24:42,900 --> 00:24:45,419


587
00:24:45,419 --> 00:24:47,400
ほとんどの場合、因果関係グラフにアクセスできません。実際に

588
00:24:47,400 --> 00:24:49,919
データから最適な因果関係グラフを学習することはまだ

589
00:24:49,919 --> 00:24:51,840
未解決の問題です。この点は改善中です。

590
00:24:51,840 --> 00:24:53,880
改善はしていますが、

591
00:24:53,880 --> 00:24:57,299
このタスクを正確に実行する方法は

592
00:24:57,299 --> 00:24:58,380


593
00:24:58,380 --> 00:25:01,140
まだ未解決の問題です。 先ほども

594
00:25:01,140 --> 00:25:03,179
言いましたが、基本的な目的は観測データ

595
00:25:03,179 --> 00:25:04,740
から評議会の関係を参照することです。その

596
00:25:04,740 --> 00:25:07,380
ため、データセットが与えられた場合、

597
00:25:07,380 --> 00:25:09,780


598
00:25:09,780 --> 00:25:12,179


599
00:25:12,179 --> 00:25:14,460
システムとデータセットの変数の間の接続性を記述する有向正確なグラフを推測したいと考えています。

600
00:25:14,460 --> 00:25:15,960


601
00:25:15,960 --> 00:25:17,700
たとえば、ここに例があります。

602
00:25:17,700 --> 00:25:19,440
それは

603
00:25:19,440 --> 00:25:22,860
皆さんよくご存じだと思います、ありがとうございます パンデミックのせいで、

604
00:25:22,860 --> 00:25:25,080


605
00:25:25,080 --> 00:25:28,799
年齢、ワクチン入院

606
00:25:28,799 --> 00:25:31,380
、CT という 4 つの変数があり、

607
00:25:31,380 --> 00:25:33,600


608
00:25:33,600 --> 00:25:36,059
これらの変数間の因果関係を推測したいので、

609
00:25:36,059 --> 00:25:37,980
たとえば、データから直接学習したいのですが、

610
00:25:37,980 --> 00:25:40,260


611
00:25:40,260 --> 00:25:43,080
入院する人の割合は、

612
00:25:43,080 --> 00:25:45,419
年齢やワクチン接種の有無などによって異なります。

613
00:25:45,419 --> 00:25:49,760


614
00:25:51,299 --> 00:25:55,020
長い前置きはこれで終わりですが、

615
00:25:55,020 --> 00:25:58,080


616
00:25:58,080 --> 00:26:00,179
十分に明確であったことを願っています。

617
00:26:00,179 --> 00:26:02,039


618
00:26:02,039 --> 00:26:05,159
論文の基本的な結果を理解するための基礎です。次に

619
00:26:05,159 --> 00:26:07,740
リサーチクエスチョンに進みます。

620
00:26:07,740 --> 00:26:09,059
リサーチクエスチョンは次のとおりです。

621
00:26:09,059 --> 00:26:10,440


622
00:26:10,440 --> 00:26:12,900
最初に、

623
00:26:12,900 --> 00:26:15,299
クリエイティブコーディングを使用して因果推論を実行できるかどうかを確認したいので、

624
00:26:15,299 --> 00:26:16,980


625
00:26:16,980 --> 00:26:20,100
これまでのところオペレーショナルコーディングのみが

626
00:26:20,100 --> 00:26:22,380
使用されています ベイジアン ネットワークで相関関係を計算するために実行するのですが

627
00:26:22,380 --> 00:26:25,020


628
00:26:25,020 --> 00:26:27,419
、大きな問題は、

629
00:26:27,419 --> 00:26:29,400
相関関係やモデル介入を超えて、

630
00:26:29,400 --> 00:26:31,679
生物学的にもっともらしい方法で反事実を実現できるかということです。たとえば、

631
00:26:31,679 --> 00:26:32,760


632
00:26:32,760 --> 00:26:34,380


633
00:26:34,380 --> 00:26:36,120
単純で

634
00:26:36,120 --> 00:26:39,059
直感的で、ニューロンだけを操作できるような方法で

635
00:26:39,059 --> 00:26:40,740
たとえば、

636
00:26:40,740 --> 00:26:43,740
グラフの巨大な構造には触れません

637
00:26:43,740 --> 00:26:46,380
。実際には、より具体的に

638
00:26:46,380 --> 00:26:48,299
質問は、

639
00:26:48,299 --> 00:26:51,000


640
00:26:51,000 --> 00:26:52,740
介入と反事実を実行するための操作可能なコーディングベースの構造因果モデルを定義できるかということになります。2 番目の質問は、先ほど

641
00:26:52,740 --> 00:26:55,320


642
00:26:55,320 --> 00:26:58,380


643
00:26:58,380 --> 00:27:00,179
述べたように、構造カスタム

644
00:27:00,179 --> 00:27:02,159
モデルを持つことは、次のことを前提としているということです。 回避ネットワークの構造を知っている

645
00:27:02,159 --> 00:27:04,260


646
00:27:04,260 --> 00:27:07,919
ので、矢印があると仮定します。

647
00:27:07,919 --> 00:27:09,960
これを超えて、創造的なコーディング ネットワークを使用して

648
00:27:09,960 --> 00:27:11,520


649
00:27:11,520 --> 00:27:14,418
グラフの因果構造を学習できますか。

650
00:27:16,140 --> 00:27:18,900
基本的にこれらの質問の両方に肯定的な答えが得られれば、次

651
00:27:18,900 --> 00:27:21,120


652
00:27:21,120 --> 00:27:23,120
のように予測コーディングを使用できるようになります。

653
00:27:23,120 --> 00:27:26,039
基本的に

654
00:27:26,039 --> 00:27:28,740
データセットを取得し、このデータセットから

655
00:27:28,740 --> 00:27:30,419
介入と反事実の

656
00:27:30,419 --> 00:27:34,820
予測を直接テストできるようにするエンドツーエンドの因果推論手法です。

657
00:27:36,840 --> 00:27:39,299
それでは、

658
00:27:39,299 --> 00:27:40,740
最初の問題に取り組みましょう。因果推論

659
00:27:40,740 --> 00:27:42,419
バイブレーションコーディングは、これを

660
00:27:42,419 --> 00:27:45,120
提供するセクションでもあります。

661
00:27:45,120 --> 00:27:46,740
基本的に論文のタイトルです。

662
00:27:46,740 --> 00:27:48,539
ここでは、相関演算コーディングを実行する方法を示します。

663
00:27:48,539 --> 00:27:50,760
これは、

664
00:27:50,760 --> 00:27:52,440
うーん、すでに知られていますが、

665
00:27:52,440 --> 00:27:54,419
介入クエリを実行する方法を示します。介入クエリは、この

666
00:27:54,419 --> 00:27:56,760


667
00:27:56,760 --> 00:28:01,140
論文の本当の問題だと思います。

668
00:28:01,140 --> 00:28:03,900
したがって、ここに因果関係があります。 これは

669
00:28:03,900 --> 00:28:05,700
私たちが持っていた通常のグラフであり、これは

670
00:28:05,700 --> 00:28:07,260


671
00:28:07,260 --> 00:28:09,240
対応するクリエイティブ

672
00:28:09,240 --> 00:28:11,760
コーディングモデルです。つまり、軸は

673
00:28:11,760 --> 00:28:13,980
潜在変数であり、

674
00:28:13,980 --> 00:28:18,000
ニューラルネットワークモデルのニューロンに対応し

675
00:28:18,000 --> 00:28:20,760
、

676
00:28:20,760 --> 00:28:22,740
1つのニューロンからの予測情報を通過する黒い矢印です。

677
00:28:22,740 --> 00:28:25,559
すべての

678
00:28:25,559 --> 00:28:28,500
頂点には、情報を階層の上に渡すこのエラー ニューロンもあります。

679
00:28:28,500 --> 00:28:31,140


680
00:28:31,140 --> 00:28:32,820
そのため、すべての

681
00:28:32,820 --> 00:28:36,480
エラーの情報は階層の上の値ノードに送られ

682
00:28:36,480 --> 00:28:39,120
、基本的に、

683
00:28:39,120 --> 00:28:41,400
それ自体を修正して変更するように指示します。 予測な

684
00:28:41,400 --> 00:28:43,760


685
00:28:44,700 --> 00:28:46,559
ので、予測コーディングを使用して相関を実行するに

686
00:28:46,559 --> 00:28:48,840


687
00:28:48,840 --> 00:28:50,400
は、観察を取得し、

688
00:28:50,400 --> 00:28:52,620
特定のニューロンの値を修正するだけです。

689
00:28:52,620 --> 00:28:53,820


690
00:28:53,820 --> 00:28:55,200
したがって、

691
00:28:55,200 --> 00:28:58,740
X3 が S3 に等しい場合に X4 の確率を計算したい場合は、次のようにするだけです。

692
00:28:58,740 --> 00:29:02,340
X3 を取得し、それ以上

693
00:29:02,340 --> 00:29:04,380
変更されない方法で S3 に修正し、

694
00:29:04,380 --> 00:29:08,159
エネルギー最小化を実行します。

695
00:29:08,159 --> 00:29:09,720
このモデルは、自由エネルギーの変動の最小化を通じて

696
00:29:09,720 --> 00:29:12,659
軸を更新することで最小化することで、

697
00:29:12,659 --> 00:29:16,380


698
00:29:16,380 --> 00:29:18,419
モデルが

699
00:29:18,419 --> 00:29:20,820
解に収束します。 この質問では、

700
00:29:20,820 --> 00:29:22,919


701
00:29:22,919 --> 00:29:27,179
X3 が与えられた場合の X4 の確率または期待値は 3 に等しいことになります。

702
00:29:27,179 --> 00:29:29,340
しかし、グラフの構造に影響を与えずに介入を実行するにはどうすればよいでしょうか。

703
00:29:29,340 --> 00:29:31,679


704
00:29:31,679 --> 00:29:33,419


705
00:29:33,419 --> 00:29:35,640
基本的にこれがこの論文の最初のアイデアです。

706
00:29:35,640 --> 00:29:37,679


707
00:29:37,679 --> 00:29:39,960
ああ、これはまだ方法です。 相関を実行して、

708
00:29:39,960 --> 00:29:43,260
S3 を X3 に等しく修正することが

709
00:29:43,260 --> 00:29:45,600
アルゴリズムの最初のステップであり、

710
00:29:45,600 --> 00:29:47,220
2 番目のステップは、

711
00:29:47,220 --> 00:29:50,539
自由エネルギーの変動を最小限に抑えて軸を取得することです。

712
00:29:51,240 --> 00:29:53,340
介入は、理論的には

713
00:29:53,340 --> 00:29:55,200
これらの矢印を削除し

714
00:29:55,200 --> 00:29:56,220


715
00:29:56,220 --> 00:29:57,659
、確率の質問に答えることに相当します。

716
00:29:57,659 --> 00:29:59,279


717
00:29:59,279 --> 00:30:02,399
介入を実行することで X4 を計算するので、X3 が 3 に

718
00:30:02,399 --> 00:30:04,860
等しいという命令型コーディングは次のように実行できます。

719
00:30:04,860 --> 00:30:07,080


720
00:30:07,080 --> 00:30:09,840
ここでアルゴリズムを書きます。

721
00:30:09,840 --> 00:30:13,140
相関関係のように、最初に S3 を

722
00:30:13,140 --> 00:30:17,039
iFix X3 と等しい値に修正し、

723
00:30:17,039 --> 00:30:18,720
得られた観測結果と同じ値を取得します。

724
00:30:18,720 --> 00:30:21,299
次に、これは

725
00:30:21,299 --> 00:30:24,059
グラフではなく

726
00:30:24,059 --> 00:30:26,700
予測誤差に介入し、

727
00:30:26,700 --> 00:30:28,980


728
00:30:28,980 --> 00:30:31,020
予測誤差をゼロに修正する必要がある重要なステップです。予測誤差がゼロであると、

729
00:30:31,020 --> 00:30:32,480
基本的に

730
00:30:32,480 --> 00:30:36,179
意味のない

731
00:30:36,179 --> 00:30:38,460
情報を階層の上に送信するか、実際には

732
00:30:38,460 --> 00:30:40,200
情報を送信しません。

733
00:30:40,200 --> 00:30:41,880
なぜなら、それは基本的に

734
00:30:41,880 --> 00:30:44,659
予測が常に正しいことを示しており、

735
00:30:44,659 --> 00:30:48,120
3 番目のステップは、

736
00:30:48,120 --> 00:30:50,220
前に行ったように、

737
00:30:50,220 --> 00:30:52,919


738
00:30:52,919 --> 00:30:55,679
自由エネルギーの変動を最小限に抑えて軸を更新することです。つまり、

739
00:30:55,679 --> 00:30:59,039
これから示すように、または実験的に

740
00:30:59,039 --> 00:31:00,840
単純に次のようにします。

741
00:31:00,840 --> 00:31:02,399
予測誤差をゼロに設定するというこのちょっとしたトリックにより、

742
00:31:02,399 --> 00:31:05,120


743
00:31:05,640 --> 00:31:08,220


744
00:31:08,220 --> 00:31:10,320


745
00:31:10,320 --> 00:31:13,620
微積分の理論のようにグラフの構造に実際に作用したり、単純に実行するだけで介入

746
00:31:13,620 --> 00:31:16,919
後に欠落している変数を推測したりすることができなくなります。

747
00:31:16,919 --> 00:31:19,140


748
00:31:19,140 --> 00:31:22,640
自由エネルギーの逸脱 最小化

749
00:31:24,659 --> 00:31:26,580
反事実推論はどうなるか

750
00:31:26,580 --> 00:31:28,080
反事実推論は、えー、介入方法を定義できれば実際には

751
00:31:28,080 --> 00:31:30,539
簡単です。

752
00:31:30,539 --> 00:31:34,740


753
00:31:34,740 --> 00:31:36,539
これは、前に見たように、反事実を推論した後に

754
00:31:36,539 --> 00:31:38,640


755
00:31:38,640 --> 00:31:40,380
過去の状況で介入を実行するのと同様の反事実を実行するためです。

756
00:31:40,380 --> 00:31:44,360


757
00:31:44,360 --> 00:31:48,120
unobservable 観察できない変数です。

758
00:31:48,120 --> 00:31:49,620


759
00:31:49,620 --> 00:31:51,480


760
00:31:51,480 --> 00:31:53,520
先ほど示したアブダクションのアクションと

761
00:31:53,520 --> 00:31:56,039
予測のステップのプロットでわかるように、アクションと

762
00:31:56,039 --> 00:31:58,320
予測のステップには

763
00:31:58,320 --> 00:31:59,640
2 つの矢印がありませんでした。それらは

764
00:31:59,640 --> 00:32:02,580
削除されました。かなりコーディングすることで、

765
00:32:02,580 --> 00:32:06,299
この矢印を保持できるようになります。

766
00:32:06,299 --> 00:32:08,279
グラフを

767
00:32:08,279 --> 00:32:11,340
作成し、

768
00:32:11,340 --> 00:32:13,380
アクション ステップの前に行ったのと同様にアブダクション ステップを実行するだけで反事実を実行します。

769
00:32:13,380 --> 00:32:14,640


770
00:32:14,640 --> 00:32:16,679
アクション ステップでは、単純に

771
00:32:16,679 --> 00:32:18,600
単一ノードに対して介入を実行するため、

772
00:32:18,600 --> 00:32:21,240
値ノードを修正し、誤差をゼロに設定し

773
00:32:21,240 --> 00:32:24,240


774
00:32:24,240 --> 00:32:26,399
、エネルギー最小化を実行します。

775
00:32:26,399 --> 00:32:27,960


776
00:32:27,960 --> 00:32:30,679
予測を計算するための自由エネルギーの持続時間を最小限に抑える

777
00:32:32,399 --> 00:32:36,299
ので、これは

778
00:32:36,299 --> 00:32:39,840
介入や反事実を実行するための簡単で洗練された方法のようなものだと思います。

779
00:32:39,840 --> 00:32:42,899


780
00:32:42,899 --> 00:32:44,880
そうそう、私たちが

781
00:32:44,880 --> 00:32:46,500
今示さなければならないのは、それが実際に機能するかどうかだと思います。

782
00:32:46,500 --> 00:32:48,720
いくつかの実験があります。

783
00:32:48,720 --> 00:32:49,919


784
00:32:49,919 --> 00:32:52,440
これから 2 つの異なる実験を紹介します。

785
00:32:52,440 --> 00:32:54,240
最初の実験は

786
00:32:54,240 --> 00:32:57,179
単に概念実証の実験で、

787
00:32:57,179 --> 00:33:01,020
操作上のコーディングで

788
00:33:01,020 --> 00:33:02,480


789
00:33:02,480 --> 00:33:06,120
介入と反事実を実行できることを示します

790
00:33:06,120 --> 00:33:08,700
。2 番目の実験は実際に

791
00:33:08,700 --> 00:33:11,220
単純なアプリケーションを示します。 介入

792
00:33:11,220 --> 00:33:13,440
クエリを使用して、

793
00:33:13,440 --> 00:33:16,260


794
00:33:16,260 --> 00:33:18,360


795
00:33:18,360 --> 00:33:20,940
完全に

796
00:33:20,940 --> 00:33:22,080
接続されたモデルである特定の種類の運用コーディング ネットワーク上で分類タスクのパフォーマンスを向上させる方法について説明します。

797
00:33:22,080 --> 00:33:24,659
最初のネットワークから始めましょう。

798
00:33:24,659 --> 00:33:27,679


799
00:33:27,679 --> 00:33:30,360
構造的な評議会モデルを考慮して、このタスクをどのように実行するかです。

800
00:33:30,360 --> 00:33:33,360
トレーニング データを生成し、それを使用して

801
00:33:33,360 --> 00:33:35,760
重みを学習し、

802
00:33:35,760 --> 00:33:39,480
構造的な Kaza モデルの機能を学習します。

803
00:33:39,480 --> 00:33:42,779
次に、介入クエリと反作用クエリの両方のテスト テスト データを生成し

804
00:33:42,779 --> 00:33:44,399


805
00:33:44,399 --> 00:33:46,080


806
00:33:46,080 --> 00:33:48,000
、正しいテスト データに収束できるかどうかを示します。

807
00:33:48,000 --> 00:33:51,360


808
00:33:51,360 --> 00:33:53,340
創造的なコーディングを使用し、

809
00:33:53,340 --> 00:33:54,779


810
00:33:54,779 --> 00:33:57,240
たとえばここでこれら 2 つの

811
00:33:57,240 --> 00:33:58,860
プロットは、この特定のグラフの

812
00:33:58,860 --> 00:34:00,600
介入介入と反事実クエリを表します。

813
00:34:00,600 --> 00:34:03,539


814
00:34:03,539 --> 00:34:05,880
これはバタフライ バイアス グラフであり、

815
00:34:05,880 --> 00:34:08,280


816
00:34:08,280 --> 00:34:10,859


817
00:34:10,859 --> 00:34:12,179
介入と反事実

818
00:34:12,179 --> 00:34:15,540
技術かどうかの因果推論をテストする際によく使用されるグラフです。 仕事はそれと同じくらい単純ですが、

819
00:34:15,540 --> 00:34:18,000
論文には

820
00:34:18,000 --> 00:34:20,760
さまざまなグラフがたくさんありますが、一般的に、これら

821
00:34:20,760 --> 00:34:22,800
2 つのグラフ、これら 2 つのプロットは、この

822
00:34:22,800 --> 00:34:26,940
方法が機能することを示しており、

823
00:34:26,940 --> 00:34:27,918


824
00:34:27,918 --> 00:34:32,219


825
00:34:32,219 --> 00:34:33,960
介入による反事実量間の平均絶対誤差が示されています。

826
00:34:33,960 --> 00:34:37,399
計算すると、元のグラフ

827
00:34:37,399 --> 00:34:39,780
からの介入量と反事実量は

828
00:34:39,780 --> 00:34:41,460


829
00:34:41,460 --> 00:34:43,800
互いに近いため、誤差は

830
00:34:43,800 --> 00:34:45,800
非常に小さい

831
00:34:45,800 --> 00:34:49,139
です。2 番目の実験は、基本的に、以前の論文で

832
00:34:49,139 --> 00:34:51,239
提案した実験の拡張です。

833
00:34:51,239 --> 00:34:54,540
これは、

834
00:34:54,540 --> 00:34:56,460
任意のグラフ トポロジーでの学習です。

835
00:34:56,460 --> 00:34:59,040
私は昨年

836
00:34:59,040 --> 00:35:01,080
その論文で書きましたが、

837
00:35:01,080 --> 00:35:04,200
基本的に概念実証としてこの種のネットワークを提案します。これは

838
00:35:04,200 --> 00:35:06,060


839
00:35:06,060 --> 00:35:08,160
完全に接続されたネットワークであり、

840
00:35:08,160 --> 00:35:11,579


841
00:35:11,579 --> 00:35:13,500


842
00:35:13,500 --> 00:35:15,960


843
00:35:15,960 --> 00:35:20,520
固定された条件が与えられるため、機械学習実験を実行する必要がある一般的に最悪のニューラル ネットワークです。

844
00:35:20,520 --> 00:35:23,660
基本的にニューロンのセットです。

845
00:35:23,760 --> 00:35:26,400
すべてのニューロンのペアは

846
00:35:26,400 --> 00:35:28,680
2 つの異なるシナプスによって接続されているため、

847
00:35:28,680 --> 00:35:31,200


848
00:35:31,200 --> 00:35:33,359


849
00:35:33,359 --> 00:35:34,619
一般的に可能な限り最も複雑なモデルです。

850
00:35:34,619 --> 00:35:36,300
良い点は、

851
00:35:36,300 --> 00:35:37,859
多くのサイクルがあるため、モデルが非常に

852
00:35:37,859 --> 00:35:39,599
柔軟であることです。

853
00:35:39,599 --> 00:35:42,480
たとえば、細かく分割された画像、

854
00:35:42,480 --> 00:35:45,359
データポイント、そのラベルでトレーニングできるという意味ですが、

855
00:35:45,359 --> 00:35:47,400


856
00:35:47,400 --> 00:35:50,640
戻ってくる情報のおかげでクエリを実行できる方法は、

857
00:35:50,640 --> 00:35:52,140
さまざまな方法でクエリできるということです。

858
00:35:52,140 --> 00:35:54,060


859
00:35:54,060 --> 00:35:55,980
画像を提供し、

860
00:35:55,980 --> 00:35:57,480
エネルギー最小化を実行してラベルを取得する分類タスクを作成でき

861
00:35:57,480 --> 00:35:59,400
ますが、たとえば、

862
00:35:59,400 --> 00:36:01,320


863
00:36:01,320 --> 00:36:03,060
ラベルにエネルギー最小化を実行して

864
00:36:03,060 --> 00:36:05,220
画像を取得する生成タスクを実行することもできます。

865
00:36:05,220 --> 00:36:06,960


866
00:36:06,960 --> 00:36:10,260
画像の半分を与えて収束させ、そして

867
00:36:10,260 --> 00:36:12,119
収束させてモデルを後半に変換させる、というように、

868
00:36:12,119 --> 00:36:14,400


869
00:36:14,400 --> 00:36:16,440
基本的には分類や分類に焦点を当てることなく、

870
00:36:16,440 --> 00:36:19,619
データセット全体の統計を学習するモデルです。

871
00:36:19,619 --> 00:36:21,900


872
00:36:21,900 --> 00:36:25,079
一般に世代なので、

873
00:36:25,079 --> 00:36:27,900
この柔軟性は素晴らしいですが、

874
00:36:27,900 --> 00:36:31,260
問題は、この

875
00:36:31,260 --> 00:36:34,140
ため、すべてのタスクがうまく機能しないため、

876
00:36:34,140 --> 00:36:35,820
さまざまなことができるようになります

877
00:36:35,820 --> 00:36:38,579
が、どれもうまく機能しないことです。

878
00:36:38,579 --> 00:36:39,960


879
00:36:39,960 --> 00:36:42,480
ここでは、を使用する方法を示したいと思います。

880
00:36:42,480 --> 00:36:44,099


881
00:36:44,099 --> 00:36:46,740
標準の相関クエリや

882
00:36:46,740 --> 00:36:48,119
条件付きクエリの代わりに介入クエリを使用すると、

883
00:36:48,119 --> 00:36:49,980


884
00:36:49,980 --> 00:36:51,960


885
00:36:51,960 --> 00:36:54,000


886
00:36:54,000 --> 00:36:57,599
これらの分類タスクの結果がわずかに改善されます。では、これらのタスクのテスト精度が

887
00:36:57,599 --> 00:37:01,079
それほど高くない推測的な理由は何ですか。

888
00:37:01,079 --> 00:37:03,180
最初の 2 つの理由は、モデルの

889
00:37:03,180 --> 00:37:05,640
注意が散漫になっているためです。 あらゆるエラーを修正する際に、

890
00:37:05,640 --> 00:37:07,920
基本的に

891
00:37:07,920 --> 00:37:09,420
画像を提示して

892
00:37:09,420 --> 00:37:11,579
ラベルを取得したいとしますが、実際にはモデル

893
00:37:11,579 --> 00:37:13,859
自体が更新されて画像内のエラーも予測します。

894
00:37:13,859 --> 00:37:16,320


895
00:37:16,320 --> 00:37:18,480
そして、私が述べた 2 番目の理由は、

896
00:37:18,480 --> 00:37:21,119
構造が

897
00:37:21,119 --> 00:37:24,540
複雑すぎるため、結果から言えば

898
00:37:24,540 --> 00:37:27,079
オッカムのかみそりの

899
00:37:27,079 --> 00:37:28,800
議論から言えば、

900
00:37:28,800 --> 00:37:30,720
これは最悪のモデルなので、データセットに

901
00:37:30,720 --> 00:37:32,160
適合するモデルが得られるたびに、

902
00:37:32,160 --> 00:37:33,960
そのモデルは

903
00:37:33,960 --> 00:37:35,579
今後のモデルよりも複雑ではなくなります。

904
00:37:35,579 --> 00:37:37,560
好ましいことです

905
00:37:37,560 --> 00:37:40,560
が、一般的には、まあ、それを研究するためだけに、

906
00:37:40,560 --> 00:37:41,400


907
00:37:41,400 --> 00:37:43,380
アイデアは、このモデルでクエリを実行できます。

908
00:37:43,380 --> 00:37:44,820
介入を使用して、

909
00:37:44,820 --> 00:37:46,859
これらの完全に

910
00:37:46,859 --> 00:37:48,599
接続されたモデルのパフォーマンスを向上させることができます。

911
00:37:48,599 --> 00:37:51,060
答えは「はい」

912
00:37:51,060 --> 00:37:53,160
です。したがって、介入クエリを実行する方法は次のとおりです。

913
00:37:53,160 --> 00:37:55,619
画像をネットワークに提示します。

914
00:37:55,619 --> 00:37:56,640


915
00:37:56,640 --> 00:37:59,460
ピクセルの誤差を

916
00:37:59,460 --> 00:38:01,560
ゼロに修正して、この誤差が

917
00:38:01,560 --> 00:38:03,180
ネットワーク内で伝播しないようにします。

918
00:38:03,180 --> 00:38:05,700
その後、ラベルを計算します。ご覧のとおり、

919
00:38:05,700 --> 00:38:08,400
精度は、

920
00:38:08,400 --> 00:38:11,339
たとえば、

921
00:38:11,339 --> 00:38:13,380
クリエイティブコーディングネットワークの標準的なクエリ方法は

922
00:38:13,380 --> 00:38:16,800


923
00:38:16,800 --> 00:38:19,020
介入後の精度である92までであり、同じことがファッションの手段についても起こります。

924
00:38:19,020 --> 00:38:21,540


925
00:38:21,540 --> 00:38:24,420


926
00:38:24,420 --> 00:38:26,940


927
00:38:26,940 --> 00:38:28,920
これらのプロットを見たとき、おそらく誰もが考えるでしょう、非常に正当な批評家は、

928
00:38:28,920 --> 00:38:32,099
89からの平均を改善しても大丈夫だと思います 92 までは

929
00:38:32,099 --> 00:38:36,180
まだ基本的にひどいです。はい、それは本当です。

930
00:38:36,180 --> 00:38:38,400
実際に後のスライドで説明します。

931
00:38:38,400 --> 00:38:40,619


932
00:38:40,619 --> 00:38:42,660
この完全に

933
00:38:42,660 --> 00:38:43,859
接続されたモデルの構造にどのように作用するかを示します。これにより、

934
00:38:43,859 --> 00:38:46,500
結果がさらに

935
00:38:46,500 --> 00:38:48,480
向上することになります。 もちろん最先端のパフォーマンス

936
00:38:48,480 --> 00:38:50,820
にさえ及ばないリッチなパフォーマンスです

937
00:38:50,820 --> 00:38:52,560


938
00:38:52,560 --> 00:38:55,320
が、まだ向上していますが、

939
00:38:55,320 --> 00:38:57,380
基本的に許容できるレベルには達していません

940
00:38:57,380 --> 00:39:01,760
ケンワースの調査は調査中です、

941
00:39:02,040 --> 00:39:04,980
はい、これが

942
00:39:04,980 --> 00:39:08,400
クリエイティブコーディングを使用した因果推論に関する部分でした

943
00:39:08,400 --> 00:39:10,920
要約すると、

944
00:39:10,920 --> 00:39:15,060


945
00:39:15,060 --> 00:39:17,640
先ほど示した結果の興味深い部分は、

946
00:39:17,640 --> 00:39:19,859


947
00:39:19,859 --> 00:39:22,560


948
00:39:22,560 --> 00:39:24,780


949
00:39:24,780 --> 00:39:26,280
操作する必要がないため、オペレーティブ コーディングが非常に簡単かつ直感的な方法で介入を実行できることを示したということです。 古いグラフの構造

950
00:39:26,280 --> 00:39:28,740
では、それらの

951
00:39:28,740 --> 00:39:31,079
関数が利用できないこともあります

952
00:39:31,079 --> 00:39:34,020
が、

953
00:39:34,020 --> 00:39:36,140


954
00:39:36,140 --> 00:39:39,780
単一のニューロンに介入して

955
00:39:39,780 --> 00:39:41,640
予測誤差をゼロにし

956
00:39:41,640 --> 00:39:44,220
、エネルギー最小化プロセスを実行するだけで済みます。

957
00:39:44,220 --> 00:39:46,619


958
00:39:46,619 --> 00:39:49,200
これらは拡張されています。

959
00:39:49,200 --> 00:39:51,240
創造的なコーディングに基づいた構造

960
00:39:51,240 --> 00:39:52,920
因果モデルを定義できるようになりました

961
00:39:52,920 --> 00:39:54,920
ここで、構造構造の学習に関する作業の 2 番目の部分に移ります。つまり、前述したように

962
00:39:54,920 --> 00:39:57,900


963
00:39:57,900 --> 00:40:01,700


964
00:40:02,000 --> 00:40:05,099
命令学習です。 対処

965
00:40:05,099 --> 00:40:07,260


966
00:40:07,260 --> 00:40:09,720


967
00:40:09,720 --> 00:40:11,880
観測データからモデルの因果構造を学習する問題を扱います。

968
00:40:11,880 --> 00:40:13,800
実際には、

969
00:40:13,800 --> 00:40:17,760
何十年も前から存在している問題はなく、

970
00:40:17,760 --> 00:40:21,359
数年前までは常に存在していました。

971
00:40:21,359 --> 00:40:24,000
組み合わせ検索手法を使用して取り組む

972
00:40:24,000 --> 00:40:25,560


973
00:40:25,560 --> 00:40:26,640
これらのコミュニティ

974
00:40:26,640 --> 00:40:29,280
調査手法の問題は、データが複数になるとすぐに

975
00:40:29,280 --> 00:40:32,880
複雑さが指数関数的に 2 倍になることです。

976
00:40:32,880 --> 00:40:34,740


977
00:40:34,740 --> 00:40:36,780
次元が高く、

978
00:40:36,780 --> 00:40:39,920
学習したいバイソン グラフの

979
00:40:39,920 --> 00:40:42,300
サイズが大きくなりますが、

980
00:40:42,300 --> 00:40:46,680
それを学習すると信じられないほど時間がかかります。

981
00:40:46,680 --> 00:40:48,780
実際に数年前、2018 年の新しい新聞で発表された新しいソリューションは、

982
00:40:48,780 --> 00:40:51,000


983
00:40:51,000 --> 00:40:53,540


984
00:40:53,839 --> 00:40:55,920


985
00:40:55,920 --> 00:40:57,900
この構造を実際に学習することが可能であることを示しています。

986
00:40:57,900 --> 00:40:59,940
コンビネーターの研究方法ですが、勾配ベースの方法を使用することで、

987
00:40:59,940 --> 00:41:01,619


988
00:41:01,619 --> 00:41:05,280
これは基本的に一般的にこの熟練した問題でした。

989
00:41:05,280 --> 00:41:07,320
なぜなら、パラメーターに関する事前分布を

990
00:41:07,320 --> 00:41:08,820
簡単に得ることができるからです。これは、

991
00:41:08,820 --> 00:41:10,980


992
00:41:10,980 --> 00:41:12,420
私が

993
00:41:12,420 --> 00:41:14,700
もう少し良く定義する予定の優先目的です。 このスライドでは、

994
00:41:14,700 --> 00:41:15,599


995
00:41:15,599 --> 00:41:18,180
勾配降下法を実行します。

996
00:41:18,180 --> 00:41:19,740
モデルのサイズが 2 倍、3 倍である場合でも、

997
00:41:19,740 --> 00:41:20,820


998
00:41:20,820 --> 00:41:23,640
アルゴリズムは依然として信じられないほど

999
00:41:23,640 --> 00:41:25,440
高速です。

1000
00:41:25,440 --> 00:41:28,260
このため、この論文は、

1001
00:41:28,260 --> 00:41:31,200
これはそうです。これは一種の新しいものだと思いますし

1002
00:41:31,200 --> 00:41:33,180
、すでに実装されていると思います。  600件の

1003
00:41:33,180 --> 00:41:35,099
引用やそのようなもので、

1004
00:41:35,099 --> 00:41:37,140


1005
00:41:37,140 --> 00:41:38,720
友人のカウンセリングや

1006
00:41:38,720 --> 00:41:42,000
グラフの構造の学習について私が今目にしているすべての論文は彼らの方法を使用しています。

1007
00:41:42,000 --> 00:41:44,820


1008
00:41:44,820 --> 00:41:46,980
より速い、またはわずかに優れた

1009
00:41:46,980 --> 00:41:49,440
推論方法を見つけて少しだけ変更しているだけですが、それでも彼らはすべて

1010
00:41:49,440 --> 00:41:53,760
この論文で定義される前に、私も

1011
00:41:53,760 --> 00:41:56,460
同様に定義し、私たちも同様に定義した

1012
00:41:56,460 --> 00:41:58,859
ので、ここでエージェンシー行列である新しい量を見つけます。

1013
00:41:58,859 --> 00:42:01,500
エージェンシー

1014
00:42:01,500 --> 00:42:03,480
行列は単にモデルの接続をエンコードする行列である

1015
00:42:03,480 --> 00:42:06,359
ため、

1016
00:42:06,359 --> 00:42:08,520
バイナリ行列であり、

1017
00:42:08,520 --> 00:42:10,920
一般的には がバイナリ マトリックスである場合、

1018
00:42:10,920 --> 00:42:12,180
もちろん勾配ベースの

1019
00:42:12,180 --> 00:42:14,880
最適化を行うときはそれを連続にし、

1020
00:42:14,880 --> 00:42:16,800
ある

1021
00:42:16,800 --> 00:42:19,800
時点で基本的にエッジを殺すか、または

1022
00:42:19,800 --> 00:42:21,480
エッジを 1 に設定して

1023
00:42:21,480 --> 00:42:27,780
M3 IJ が 1 に等しいしきい値を設定します。

1024
00:42:27,780 --> 00:42:30,540
ベイジアン グラフが

1025
00:42:30,540 --> 00:42:35,040
頂点 I から頂点 J またはゼロまでのエッジである場合、

1026
00:42:35,040 --> 00:42:37,380
そうでない場合は、たとえば、このエージェンシー

1027
00:42:37,380 --> 00:42:39,540
マトリックスは、

1028
00:42:39,540 --> 00:42:42,780
このビジュアル ネットワークの接続構造を表します。

1029
00:42:42,780 --> 00:42:44,040


1030
00:42:44,040 --> 00:42:46,079
基本的に、このメソッドは、

1031
00:42:46,079 --> 00:42:48,780


1032
00:42:48,780 --> 00:42:51,000


1033
00:42:51,000 --> 00:42:53,460
構造の学習について、これらについて必要な 2 つの問題に取り組みます。 方程式ネットワークの

1034
00:42:53,460 --> 00:42:54,780
アイデアは、完全に

1035
00:42:54,780 --> 00:42:57,200
接続されたモデルから開始するということですが、

1036
00:42:57,200 --> 00:43:00,240
概念的には似ていますが、実際には、

1037
00:43:00,240 --> 00:43:02,220


1038
00:43:02,220 --> 00:43:04,020
完全に

1039
00:43:04,020 --> 00:43:06,480
接続されている、以前に定義した有効なコーディングネットワークと同等です。したがって、多くの

1040
00:43:06,480 --> 00:43:08,640
頂点とすべての頂点のペアが存在します。 は

1041
00:43:08,640 --> 00:43:10,920
2 つの異なるエッジで接続されており

1042
00:43:10,920 --> 00:43:13,319
、単純に不要なエッジを削除したいので、

1043
00:43:13,319 --> 00:43:15,780


1044
00:43:15,780 --> 00:43:18,540


1045
00:43:18,540 --> 00:43:20,819
モデル削減を実行する方法として見ることができます。大きなモデルから開始して、

1046
00:43:20,819 --> 00:43:22,020
それを小さくしたいと考えています。

1047
00:43:22,020 --> 00:43:22,800


1048
00:43:22,800 --> 00:43:25,800


1049
00:43:25,800 --> 00:43:28,260
モデルを適切に削減するための最初の要素はもちろんスパースシティであり

1050
00:43:28,260 --> 00:43:29,220


1051
00:43:29,220 --> 00:43:31,619
、モデルをよりスパースにするために誰もが使用する事前分布はラプラス事前分布

1052
00:43:31,619 --> 00:43:33,839
です。

1053
00:43:33,839 --> 00:43:36,480
これは機械学習では

1054
00:43:36,480 --> 00:43:38,880
単に L1 ノルムとして知られており、

1055
00:43:38,880 --> 00:43:40,920
ここで定義されている

1056
00:43:40,920 --> 00:43:43,980
解決策は、

1057
00:43:43,980 --> 00:43:46,740
前に述べたこの論文は、

1058
00:43:46,740 --> 00:43:49,319


1059
00:43:49,319 --> 00:43:53,359


1060
00:43:53,359 --> 00:43:55,980
ベイジアン ネットワークのおそらく最大の特徴である、

1061
00:43:55,980 --> 00:43:57,780
因果推論を実行する際に

1062
00:43:57,780 --> 00:43:59,819
それらを巡回にすることを強制する 2 番目の事前確率を先頭に追加することを提案しています。

1063
00:43:59,819 --> 00:44:01,020


1064
00:44:01,020 --> 00:44:03,000
その

1065
00:44:03,000 --> 00:44:06,359
非循環性は事前確率としてエージェンシー行列に課すことができ

1066
00:44:06,359 --> 00:44:08,160


1067
00:44:08,160 --> 00:44:10,859
、ここではこのような形状になっています。つまり、これはマトリックスのトレースであり、a を乗じた

1068
00:44:10,859 --> 00:44:14,640
指数です。ここ

1069
00:44:14,640 --> 00:44:18,420


1070
00:44:18,420 --> 00:44:21,859
で、a は再びエージェンシー行列であり、

1071
00:44:21,859 --> 00:44:24,300
基本的にこの量は次の

1072
00:44:24,300 --> 00:44:27,900
とおりです。

1073
00:44:27,900 --> 00:44:30,480
ベイジアンネットワークや、

1074
00:44:30,480 --> 00:44:32,819
またはあなたが検討しているグラフが

1075
00:44:32,819 --> 00:44:35,720
クリックされた場合にのみゼロに等しいので、

1076
00:44:37,619 --> 00:44:40,260
いくつかの実験でこれらを使用するので、

1077
00:44:40,260 --> 00:44:42,960
これらの2つを

1078
00:44:42,960 --> 00:44:45,660


1079
00:44:45,660 --> 00:44:47,520
異なる種類に強制します 患者ネットワークのデータを収集し、

1080
00:44:47,520 --> 00:44:49,200


1081
00:44:49,200 --> 00:44:51,540


1082
00:44:51,540 --> 00:44:52,740
因果推論と

1083
00:44:52,740 --> 00:44:55,020
操作的コーディングの実行について以前に提案した手法とそれらを結合しようとしています。その

1084
00:44:55,020 --> 00:44:56,520
ため、2 つの異なる

1085
00:44:56,520 --> 00:44:59,640
実験を紹介します。1 つは

1086
00:44:59,640 --> 00:45:00,960
概念実証で、これは で

1087
00:45:00,960 --> 00:45:03,660
示された標準的な実験です。 データから正しいベイジアン ネットワークを推論するすべての構造

1088
00:45:03,660 --> 00:45:06,599
学習タスクを行い

1089
00:45:06,599 --> 00:45:08,880


1090
00:45:08,880 --> 00:45:11,760
、その後、前に示した分類実験の上に構築して、

1091
00:45:11,760 --> 00:45:13,500


1092
00:45:13,500 --> 00:45:14,280


1093
00:45:14,280 --> 00:45:16,020


1094
00:45:16,020 --> 00:45:18,540
それらの事前分布によって実際にどのように分類を改善できるかを示します。

1095
00:45:18,540 --> 00:45:21,060


1096
00:45:21,060 --> 00:45:22,500
精度

1097
00:45:22,500 --> 00:45:25,500
完全に接続された予測コーディング モデルのテスト精度です。

1098
00:45:25,500 --> 00:45:28,160


1099
00:45:29,520 --> 00:45:31,680


1100
00:45:31,680 --> 00:45:33,300


1101
00:45:33,300 --> 00:45:34,980
グラフの構造を推測する最初の実験に移りましょう。

1102
00:45:34,980 --> 00:45:37,319
すべての実験は

1103
00:45:37,319 --> 00:45:39,480
基本的にこの分野のすべての論文で同じパイプラインに従います。

1104
00:45:39,480 --> 00:45:42,060
最初のステップは

1105
00:45:42,060 --> 00:45:45,119
生成することです。 ランダム グラフからのビジョン ネットワークな

1106
00:45:45,119 --> 00:45:46,079


1107
00:45:46,079 --> 00:45:48,359
ので、基本的に通常、

1108
00:45:48,359 --> 00:45:50,640
誰もがテストする 2 つのランダム グラフはエルドス

1109
00:45:50,640 --> 00:45:53,520
ルネグラフとスケール フリー グラフです。

1110
00:45:53,520 --> 00:45:55,859
したがって、

1111
00:45:55,859 --> 00:45:58,680
通常は 80 80 の

1112
00:45:58,680 --> 00:46:01,619
異なるノードといくつかのエッジに対して 20 個のランダムにサンプリングした大きなグラフを生成します

1113
00:46:01,619 --> 00:46:04,619


1114
00:46:04,619 --> 00:46:06,540
。 このグラフを使用してデータ セットを生成する

1115
00:46:06,540 --> 00:46:08,280


1116
00:46:08,280 --> 00:46:10,819
ため、たとえば

1117
00:46:10,819 --> 00:46:14,460
n 個のビッグ N データ ポイントをサンプリングします。そして、

1118
00:46:14,460 --> 00:46:16,859


1119
00:46:16,859 --> 00:46:18,780
以前に生成したグラフを取得してそれを破棄し、

1120
00:46:18,780 --> 00:46:20,819
データ セットとタスクだけを保持します。

1121
00:46:20,819 --> 00:46:23,099
今解決したいことは

1122
00:46:23,099 --> 00:46:25,020
学習することです

1123
00:46:25,020 --> 00:46:27,420


1124
00:46:27,420 --> 00:46:29,819
基本的に、捨てたグラフ

1125
00:46:29,819 --> 00:46:32,579
の構造を取得できるトレーニング アルゴリズムを用意することです。そのため、

1126
00:46:32,579 --> 00:46:34,619


1127
00:46:34,619 --> 00:46:36,839
ここで行う方法は、

1128
00:46:36,839 --> 00:46:38,460
完全に接続されたクリエイティブ コーディングを行うことです。

1129
00:46:38,460 --> 00:46:41,760


1130
00:46:41,760 --> 00:46:43,800


1131
00:46:43,800 --> 00:46:45,359
前に定義したスパース事前分布と SQL 事前分布の両方を使用してこのデータセット D をモデル化し

1132
00:46:45,359 --> 00:46:48,780
、特定のしきい値より小さいエージェンシー行列のエントリを取り除いた後に収束するグラフが実際に次のグラフに似ているかどうかを確認します。

1133
00:46:48,780 --> 00:46:50,760


1134
00:46:50,760 --> 00:46:53,220


1135
00:46:53,220 --> 00:46:55,319


1136
00:46:55,319 --> 00:46:57,599


1137
00:46:57,599 --> 00:47:00,060
最初のグラフのそれもあり、

1138
00:47:00,060 --> 00:47:02,359


1139
00:47:02,520 --> 00:47:04,500
これが実際に当てはまることも示しています。

1140
00:47:04,500 --> 00:47:06,599
これは例であり

1141
00:47:06,599 --> 00:47:09,020
、論文ではさまざまな

1142
00:47:09,020 --> 00:47:12,420
パラメータ化や次元

1143
00:47:12,420 --> 00:47:15,060
などを示しています

1144
00:47:15,060 --> 00:47:16,920
が、これらの 2 つが最も

1145
00:47:16,920 --> 00:47:18,900
代表的な例だと思います。 エラーのある

1146
00:47:18,900 --> 00:47:20,760
保育園グラフと 20 ノードのフリー スケール グラフがあり

1147
00:47:20,760 --> 00:47:23,579


1148
00:47:23,579 --> 00:47:25,800
、左側にはランダムに

1149
00:47:25,800 --> 00:47:27,300


1150
00:47:27,300 --> 00:47:29,339
サンプリングされたグラフであるグラウンドスルー グラフが表示され

1151
00:47:29,339 --> 00:47:30,839


1152
00:47:30,839 --> 00:47:32,599
、右側にはデータから

1153
00:47:32,599 --> 00:47:35,220
学習したかなりの難易度モデルのグラフが表示されます。

1154
00:47:35,220 --> 00:47:37,440
ご覧の

1155
00:47:37,440 --> 00:47:39,359
とおり、これらは非常に

1156
00:47:39,359 --> 00:47:40,500
似ています。

1157
00:47:40,500 --> 00:47:42,780
まだ完全ではないため、

1158
00:47:42,780 --> 00:47:45,000
いくつかのエラーがありますが、

1159
00:47:45,000 --> 00:47:47,460
全体的に構造は非常に

1160
00:47:47,460 --> 00:47:49,500
うまく機能しています。

1161
00:47:49,500 --> 00:47:52,140


1162
00:47:52,140 --> 00:47:54,000
ここでは示していない定量的な実験もあります。 なぜなら、これらは

1163
00:47:54,000 --> 00:47:55,740
たくさんの数字が含まれた巨大なテーブルであり

1164
00:47:55,740 --> 00:47:57,180
、プレゼンテーションには少し多すぎるかもしれないと思いましたが、

1165
00:47:57,180 --> 00:48:00,660


1166
00:48:00,660 --> 00:48:02,220
結果は、それらが現代の方法と同様に機能することを示しています。

1167
00:48:02,220 --> 00:48:06,060


1168
00:48:06,060 --> 00:48:07,920
また、ほとんどの品質と同様に言わなければならないためです。 これは、

1169
00:48:07,920 --> 00:48:10,859


1170
00:48:10,859 --> 00:48:15,799
2018 年に導入された acigli 事前実験に由来しています。

1171
00:48:16,920 --> 00:48:19,680
2 番目のクラスの実験は

1172
00:48:19,680 --> 00:48:21,599
分類実験であり、前述したように、

1173
00:48:21,599 --> 00:48:23,880


1174
00:48:23,880 --> 00:48:25,560
以前に共有した実験の拡張であり

1175
00:48:25,560 --> 00:48:27,119
、構造学習を使用して分類を改善するというアイデアです。

1176
00:48:27,119 --> 00:48:28,560


1177
00:48:28,560 --> 00:48:31,140


1178
00:48:31,140 --> 00:48:33,420


1179
00:48:33,420 --> 00:48:36,780
全結合グラフから始まる平均値とファッション平均のデータセットの分類結果について、

1180
00:48:36,780 --> 00:48:40,560
私がやったことは、ニューロン

1181
00:48:40,560 --> 00:48:42,839
の全結合グラフクラスターを分割して、

1182
00:48:42,839 --> 00:48:46,440
1Bクラスターが

1183
00:48:46,440 --> 00:48:49,140
入力に関連するクラスターであり

1184
00:48:49,140 --> 00:48:51,900
、その後すべてが小さいことです。

1185
00:48:51,900 --> 00:48:55,319
特定の数の隠れたクラスターがあり、

1186
00:48:55,319 --> 00:48:57,720
ラベル クラスターがあります。これは、

1187
00:48:57,720 --> 00:48:58,800


1188
00:48:58,800 --> 00:49:01,560


1189
00:49:01,560 --> 00:49:04,079
ラベル予測を提供するニューロンのクラスターのクラスであり

1190
00:49:04,079 --> 00:49:06,480


1191
00:49:06,480 --> 00:49:08,700
、以前のスパースを初めて使用するために使用してトレーニングしました。

1192
00:49:08,700 --> 00:49:10,980


1193
00:49:10,980 --> 00:49:14,099
アイデアは、モデル

1194
00:49:14,099 --> 00:49:16,500
から必要のない接続を削除し、

1195
00:49:16,500 --> 00:49:17,460


1196
00:49:17,460 --> 00:49:20,880
パーサーモデルが

1197
00:49:20,880 --> 00:49:24,119
これがうまく機能することを学習したらどうなるかということです。答えはいいえ、

1198
00:49:24,119 --> 00:49:25,500
機能しません。その

1199
00:49:25,500 --> 00:49:28,500
理由は次のとおりです。 最終的に

1200
00:49:28,500 --> 00:49:30,660
収束するグラフは実際には

1201
00:49:30,660 --> 00:49:32,700
生成されるため、基本的にモデルは

1202
00:49:32,700 --> 00:49:36,180


1203
00:49:36,180 --> 00:49:38,400
ラベル自体に基づいてラベルを予測することを学習するため、入力からすべての情報を破棄し

1204
00:49:38,400 --> 00:49:40,020


1205
00:49:40,020 --> 00:49:42,480
、ラベルのみを保持します。

1206
00:49:42,480 --> 00:49:45,119
ここでわかるように、 ラベル y はそれ自体を予測するか、

1207
00:49:45,119 --> 00:49:46,560
他の実験でパラメータを変更すると、

1208
00:49:46,560 --> 00:49:48,960
y は 0 で予測し、

1209
00:49:48,960 --> 00:49:52,520
preex X1 は再び y を予測します。

1210
00:49:52,520 --> 00:49:55,980
では、この問題の解決策は何ですか。

1211
00:49:55,980 --> 00:49:57,240
この問題の解決策は、

1212
00:49:57,240 --> 00:49:59,520


1213
00:49:59,520 --> 00:50:03,000
収束する必要があるということです。 非巡回グラフに

1214
00:50:03,000 --> 00:50:05,220


1215
00:50:05,220 --> 00:50:08,000
循環を防ぐ何かを追加する必要があります。その 1

1216
00:50:08,000 --> 00:50:10,200
つは、もちろん私が既に

1217
00:50:10,200 --> 00:50:12,780
提案したものです。次に 2 番目の手法を示します。

1218
00:50:12,780 --> 00:50:14,520


1219
00:50:14,520 --> 00:50:17,280
最初の手法では以前に定義された SQL が使用され

1220
00:50:17,280 --> 00:50:18,680


1221
00:50:18,680 --> 00:50:21,359
、2 番目の手法は a は、

1222
00:50:21,359 --> 00:50:22,859
実際に負の例を使用する新しいテクニックです。

1223
00:50:22,859 --> 00:50:24,359


1224
00:50:24,359 --> 00:50:26,520
つまり、この場合の負の負の例は、

1225
00:50:26,520 --> 00:50:30,060
単に、画像があるデータ ポイントですが、

1226
00:50:30,060 --> 00:50:32,280
ラベルが

1227
00:50:32,280 --> 00:50:33,240
間違っているということです。つまり、

1228
00:50:33,240 --> 00:50:35,220
ここでは、たとえば 7 の画像があるとします。

1229
00:50:35,220 --> 00:50:36,900
しかし、私がモデルに与えているラベルは

1230
00:50:36,900 --> 00:50:39,599
2 であり、その

1231
00:50:39,599 --> 00:50:40,980


1232
00:50:40,980 --> 00:50:44,579
アイデアは非常に単純です。

1233
00:50:44,579 --> 00:50:47,460
すでに多くの作品で使用されているため、

1234
00:50:47,460 --> 00:50:49,740
モデルが肯定的な

1235
00:50:49,740 --> 00:50:52,079
例であるたびに、変動を最小限に抑えるために値を増やす必要があります。

1236
00:50:52,079 --> 00:50:53,520
自由エネルギーの量があり、

1237
00:50:53,520 --> 00:50:56,520
それが負の

1238
00:50:56,520 --> 00:50:58,859
例であるたびに、それを増加させる必要があり

1239
00:50:58,859 --> 00:51:01,260
ます。そこで、この量を最小限に抑えるためのエラーに移りましょう。

1240
00:51:01,260 --> 00:51:04,200


1241
00:51:04,200 --> 00:51:05,960


1242
00:51:05,960 --> 00:51:08,579
多くの実験と、たくさんの

1243
00:51:08,579 --> 00:51:10,859
実験で、2つの手法が有効であることがわかりました。

1244
00:51:10,859 --> 00:51:12,119


1245
00:51:12,119 --> 00:51:15,000
基本的に、最初は同じ

1246
00:51:15,000 --> 00:51:17,220
結果が得られ、2 番目も同様に同じ

1247
00:51:17,220 --> 00:51:18,599
グラフが得られます。

1248
00:51:18,599 --> 00:51:21,000


1249
00:51:21,000 --> 00:51:22,800
ここに、私が提案した

1250
00:51:22,800 --> 00:51:25,079
2 つの手法を使用したいくつかの手段とファッション手段による新しい結果が示されています。

1251
00:51:25,079 --> 00:51:27,660


1252
00:51:27,660 --> 00:51:30,960
そして次に、いくつかの手段に移ります。

1253
00:51:30,960 --> 00:51:33,900
まだ素晴らしいというわけではありませんが、間違いなくより

1254
00:51:33,900 --> 00:51:36,000
合理的なテスト精度です。したがって、ここでは、

1255
00:51:36,000 --> 00:51:39,059
分間のテスト誤差が 3.17、

1256
00:51:39,059 --> 00:51:42,119
ファッション平均のテスト誤差が 13.98 です。

1257
00:51:42,119 --> 00:51:44,819
実際、これらの結果は、グラフの構造を

1258
00:51:44,819 --> 00:51:48,300
学習することで大幅に改善できる可能性があります。

1259
00:51:48,300 --> 00:51:51,300
細分化して

1260
00:51:51,300 --> 00:51:53,040


1261
00:51:53,040 --> 00:51:55,319
グラフの構造を修正し、何らかの形で微

1262
00:51:55,319 --> 00:51:57,660
調整を行うため、

1263
00:51:57,660 --> 00:52:00,000


1264
00:52:00,000 --> 00:52:01,980
ある時点で正しい階層構造に基づいてモデルを微調整すると、階層モデルに期待されるテスト精度に達しますが、

1265
00:52:01,980 --> 00:52:03,359


1266
00:52:03,359 --> 00:52:05,460


1267
00:52:05,460 --> 00:52:08,099
1 つは、完全に接続されたモデルが

1268
00:52:08,099 --> 00:52:10,980
自然にまあ

1269
00:52:10,980 --> 00:52:13,859
まあに収束したものにすぎません。たとえば、

1270
00:52:13,859 --> 00:52:15,420


1271
00:52:15,420 --> 00:52:17,339
完全に接続されたモデルのテスト エラー 18.32 から、

1272
00:52:17,339 --> 00:52:20,359


1273
00:52:20,359 --> 00:52:22,859


1274
00:52:22,859 --> 00:52:24,420


1275
00:52:24,420 --> 00:52:26,520
運用上のコーディング モデルをクエリする標準的な方法である相関クエリまたは条件付きクエリを実行するだけで、ファッション手段でトレーニングを行うことができます。

1276
00:52:26,520 --> 00:52:29,220
介入と AC クリック

1277
00:52:29,220 --> 00:52:32,040
事前分布を併用すると、

1278
00:52:32,040 --> 00:52:34,200
このテストの誤差が大幅に低くなり

1279
00:52:34,200 --> 00:52:37,200
、手段についても観察できます。

1280
00:52:37,200 --> 00:52:39,319


1281
00:52:39,780 --> 00:52:41,819


1282
00:52:41,819 --> 00:52:45,420
この最後の実験と、非

1283
00:52:45,420 --> 00:52:48,660
巡回事前分布がどのように作用するかについては、少し詳しく説明しません。

1284
00:52:48,660 --> 00:52:50,339
グラフの構造な

1285
00:52:50,339 --> 00:52:52,440
ので、実行します

1286
00:52:52,440 --> 00:52:54,960
新しいデータセットで実験を実行します。つまり、

1287
00:52:54,960 --> 00:52:56,460
新しいデータセットを呼び出すという意味です。それは

1288
00:52:56,460 --> 00:52:58,500
多すぎるかもしれませんが、入力ポイントがある2つの手段の

1289
00:52:58,500 --> 00:53:01,440
データセットと呼んでいます。 は

1290
00:53:01,440 --> 00:53:04,319
2 つの異なる画像で構成されており

1291
00:53:04,319 --> 00:53:07,319
、ラベルは最初の画像ストーリーの 2 番目の画像にのみ依存する

1292
00:53:07,319 --> 00:53:08,520


1293
00:53:08,520 --> 00:53:10,800


1294
00:53:10,800 --> 00:53:12,720
ため、ここでのアイデアは、

1295
00:53:12,720 --> 00:53:15,079


1296
00:53:15,079 --> 00:53:18,540
周期性を事前に考慮したモデルの構造や、

1297
00:53:18,540 --> 00:53:20,819


1298
00:53:20,819 --> 00:53:23,400
画像の後半が次の画像であることを認識できるようなものです。 実際には意味がありません

1299
00:53:23,400 --> 00:53:27,960
実行中 学習中 分類

1300
00:53:27,960 --> 00:53:31,140
実行中

1301
00:53:31,140 --> 00:53:33,119
トレーニングは一般にどのように動作しますか?

1302
00:53:33,119 --> 00:53:36,480
たとえば、この入力入力

1303
00:53:36,480 --> 00:53:39,000
ノード出力ノードがあり、ノードのみが

1304
00:53:39,000 --> 00:53:41,940
完全に接続されており、モデルは

1305
00:53:41,940 --> 00:53:43,740


1306
00:53:43,740 --> 00:53:45,900
階層構造に収束します。

1307
00:53:45,900 --> 00:53:48,960


1308
00:53:48,960 --> 00:53:50,880
分類タスクで最も優れたパフォーマンスを発揮することがわかっているものは、次のとおりです。これは、

1309
00:53:50,880 --> 00:53:53,520


1310
00:53:53,520 --> 00:53:54,980
トレーニング メソッドの

1311
00:53:54,980 --> 00:53:59,280
実行の例です。トレーニングの開始である c0 で、

1312
00:53:59,280 --> 00:54:00,720


1313
00:54:00,720 --> 00:54:03,000
このモデルがここにあります。つまり、s0 は

1314
00:54:03,000 --> 00:54:05,819
7 に対応し、

1315
00:54:05,819 --> 00:54:08,099
最初の画像に対応します。  1 つは

1316
00:54:08,099 --> 00:54:09,839
再び 7 列の画像に対応するため、

1317
00:54:09,839 --> 00:54:12,300
ラベル Y とすべての潜在変数 x0

1318
00:54:12,300 --> 00:54:13,800
X1 X2 があり、

1319
00:54:13,800 --> 00:54:15,720
モデルは完全に接続されているため、

1320
00:54:15,720 --> 00:54:17,040
エージェンシー マトリックスは 1

1321
00:54:17,040 --> 00:54:20,579
でいっぱいであり、ゼロは

1322
00:54:20,579 --> 00:54:23,720
ありません。自己ループなどがあります。

1323
00:54:23,720 --> 00:54:27,319
これまでのいくつかのエポックのモデルを作成し、

1324
00:54:27,319 --> 00:54:30,540
すぐにわかることは、

1325
00:54:30,540 --> 00:54:31,920
たとえば、モデルは分類を実行するのに

1326
00:54:31,920 --> 00:54:34,740
4 つが必要ないことをすぐに理解するため、

1327
00:54:34,740 --> 00:54:36,839


1328
00:54:36,839 --> 00:54:40,740


1329
00:54:40,740 --> 00:54:43,980
2 番目の入力クラスターからのすべての出力ノードが削除され

1330
00:54:43,980 --> 00:54:45,900
、 私たちが理解できなかったのは、

1331
00:54:45,900 --> 00:54:48,660
これが

1332
00:54:48,660 --> 00:54:50,400
出力に関連するクラスターである

1333
00:54:50,400 --> 00:54:52,260
ため、

1334
00:54:52,260 --> 00:54:55,319
s0 から Y までの直接の線形マップがあり、

1335
00:54:55,319 --> 00:54:56,480


1336
00:54:56,480 --> 00:54:59,339
これがこの部分になります

1337
00:54:59,339 --> 00:55:01,160
が、実際には線形マップが最適ではないことはわかっています。

1338
00:55:01,160 --> 00:55:04,740


1339
00:55:04,740 --> 00:55:07,200
平均値の分類を実行するためのマップなので、

1340
00:55:07,200 --> 00:55:08,700


1341
00:55:08,700 --> 00:55:11,579
結果を改善するにはある程度の階層が必要です。ご覧のとおり、

1342
00:55:11,579 --> 00:55:14,220
この行は

1343
00:55:14,220 --> 00:55:15,599


1344
00:55:15,599 --> 00:55:18,960
この時点までの精度です。つまり、C2 までは

1345
00:55:18,960 --> 00:55:22,500
91 と似ています。 これは

1346
00:55:22,500 --> 00:55:24,059
線形分類よりもわずかに優れています

1347
00:55:24,059 --> 00:55:25,500


1348
00:55:25,500 --> 00:55:28,740
が、トレーニングを続けると、

1349
00:55:28,740 --> 00:55:30,660
モデルは

1350
00:55:30,660 --> 00:55:33,119
データをより適切に適合させるために何らかの階層が必要であることを理解するため、

1351
00:55:33,119 --> 00:55:35,640
この矢印が

1352
00:55:35,640 --> 00:55:38,760
時間の経過とともにますます強くなり始め、最終

1353
00:55:38,760 --> 00:55:41,700
的には線形分類であることが理解されることがわかります。

1354
00:55:41,700 --> 00:55:44,339
マップは実際には実際には必要ではなく、

1355
00:55:44,339 --> 00:55:45,920
削除される

1356
00:55:45,920 --> 00:55:48,780
ため、収束するモデルは、

1357
00:55:48,780 --> 00:55:51,000
ゼロから始まり、

1358
00:55:51,000 --> 00:55:53,760
隠れたノードに進み、

1359
00:55:53,760 --> 00:55:57,180
非常に弱い線形マップを持つラベルに進むモデルになります。

1360
00:55:57,180 --> 00:55:59,700


1361
00:55:59,700 --> 00:56:02,760
売り手のしきい値をたとえば

1362
00:56:02,760 --> 00:56:05,520
0.1 0.2 に設定すると、ある

1363
00:56:05,520 --> 00:56:07,619
時点で線形マップが忘れられ、

1364
00:56:07,619 --> 00:56:10,680
最終的に得られるものはすべて

1365
00:56:10,680 --> 00:56:13,319
階層型ネットワークになります。つまり、

1366
00:56:13,319 --> 00:56:15,720


1367
00:56:15,720 --> 00:56:17,099
実行するための正しい構造を学習したことになります。

1368
00:56:17,099 --> 00:56:19,260
分類タスクは階層構造であり、

1369
00:56:19,260 --> 00:56:21,900
2 番目の

1370
00:56:21,900 --> 00:56:25,020
画像はテスト精度の定義に何の役割も果たしていないこともわかりました。

1371
00:56:25,020 --> 00:56:28,440


1372
00:56:28,440 --> 00:56:30,420
実行されるのはこれだけです。また、これらのジョブはすべて、

1373
00:56:30,420 --> 00:56:33,839


1374
00:56:33,839 --> 00:56:36,599
1 つの自由エネルギー最小化プロセスによって実行されるだけです。

1375
00:56:36,599 --> 00:56:38,400
モデルを初期化します

1376
00:56:38,400 --> 00:56:40,859
自由エネルギーを定義します 事前分布を定義して、

1377
00:56:40,859 --> 00:56:43,559


1378
00:56:43,559 --> 00:56:45,780
エネルギー最小化を実行する前にスパースと C クリックを実行し、

1379
00:56:45,780 --> 00:56:47,400


1380
00:56:47,400 --> 00:56:49,500


1381
00:56:49,500 --> 00:56:51,839
ミンチで分類を適切に実行できる階層モデルに収束します。

1382
00:56:51,839 --> 00:56:54,000
その後、微調整を実行すると、

1383
00:56:54,000 --> 00:56:55,800


1384
00:56:55,800 --> 00:56:57,359


1385
00:56:57,359 --> 00:56:59,339
フィードバック伝播を伴うフィードフォワード ネットワークで行うのと同じように、非常に競争力のある結果が得られます。

1386
00:56:59,339 --> 00:57:01,260
しかし、興味深いのはそこではないと思います。

1387
00:57:01,260 --> 00:57:03,780
興味深いのは、

1388
00:57:03,780 --> 00:57:05,160
このプロセスすべてが気に入っているということです。このプロセスは、

1389
00:57:05,160 --> 00:57:07,980
介入と非

1390
00:57:07,980 --> 00:57:09,780
循環性をすべて合わせて行うことで、

1391
00:57:09,780 --> 00:57:11,700
完全に接続された

1392
00:57:11,700 --> 00:57:12,660
ネットワークを取得し、良好な結果で分類を実行できる

1393
00:57:12,660 --> 00:57:15,119
階層的なネットワークに収束します。

1394
00:57:15,119 --> 00:57:16,140


1395
00:57:16,140 --> 00:57:20,058


1396
00:57:20,760 --> 00:57:23,000
そう、

1397
00:57:23,000 --> 00:57:26,280
基本的にはこれで終わりです。今、ああ、そうだね、

1398
00:57:26,280 --> 00:57:29,220
たくさん話しましたが、これが結論です

1399
00:57:29,220 --> 00:57:32,160


1400
00:57:32,160 --> 00:57:35,280
基本的には短い要約を行っていますが、

1401
00:57:35,280 --> 00:57:37,559


1402
00:57:37,559 --> 00:57:39,300
この論文の一文でお伝えしなければならない重要な点は、

1403
00:57:39,300 --> 00:57:40,980
予測コーディングはエンドツーエンドの実行が

1404
00:57:40,980 --> 00:57:44,400
可能な信念更新手法であるということだと思います。  -

1405
00:57:44,400 --> 00:57:46,559
いとこ学習を終了して、

1406
00:57:46,559 --> 00:57:48,599
介入を実行して

1407
00:57:48,599 --> 00:57:51,420
データから構造を学習し、

1408
00:57:51,420 --> 00:57:53,160
介入と

1409
00:57:53,160 --> 00:57:56,058
反事実を実行できるようにすることで、予測

1410
00:57:56,700 --> 00:57:58,440
誤差をゼロに設定するだけで他の

1411
00:57:58,440 --> 00:58:00,119
介入の因果推論を効率的にモデル化できるため、

1412
00:58:00,119 --> 00:58:01,680


1413
00:58:01,680 --> 00:58:03,359


1414
00:58:03,359 --> 00:58:06,240
実行するのが非常に簡単なテクニックになります 介入を行うと、

1415
00:58:06,240 --> 00:58:07,619
単に 1 つのニューロンに触れるだけで済みます。

1416
00:58:07,619 --> 00:58:08,940


1417
00:58:08,940 --> 00:58:10,859
グラフの構造に影響を与える必要がありません。 グラフを

1418
00:58:10,859 --> 00:58:14,339
使用して、

1419
00:58:14,339 --> 00:58:16,140


1420
00:58:16,140 --> 00:58:18,359
生物学的にもっともらしい構造因果モデルを作成できます。

1421
00:58:18,359 --> 00:58:20,819
構造を学習できます。 ええと、

1422
00:58:20,819 --> 00:58:24,119
私がすでに何度も言っているようにデータから、

1423
00:58:24,119 --> 00:58:26,940


1424
00:58:26,940 --> 00:58:28,740
そして将来の作業についてのいくつかの文は、私たちが

1425
00:58:28,740 --> 00:58:31,260


1426
00:58:31,260 --> 00:58:33,180


1427
00:58:33,180 --> 00:58:36,119


1428
00:58:36,119 --> 00:58:38,460
定義したモデルのパフォーマンスを向上させることができれば良いことだということです。なぜなら、それは

1429
00:58:38,460 --> 00:58:40,980
かなり良いパフォーマンスを発揮すると思うからです。 多くの

1430
00:58:40,980 --> 00:58:43,079
タスクがあるため、

1431
00:58:43,079 --> 00:58:45,780


1432
00:58:45,780 --> 00:58:48,119
介入と反事実の構造学習ではかなりうまく機能しますが、

1433
00:58:48,119 --> 00:58:49,440
実際には、最先端の

1434
00:58:49,440 --> 00:58:51,420
モデルを見ると、単一のタスク

1435
00:58:51,420 --> 00:58:53,880
でより優れたパフォーマンスを発揮する非常に特殊な方法が常に存在するので、

1436
00:58:53,880 --> 00:58:55,559


1437
00:58:55,559 --> 00:58:58,260
見てみると興味深いでしょう。

1438
00:58:58,260 --> 00:59:00,180


1439
00:59:00,180 --> 00:59:03,599
いくつかの

1440
00:59:03,599 --> 00:59:05,599
トリックやいくつ

1441
00:59:05,599 --> 00:59:10,260
かの新しい最適化方法を追加することによって、特定のタスクでそのようなパフォーマンスのレベルに達することができ、それを動的因果モデルなどの

1442
00:59:10,260 --> 00:59:12,839
静的システムよりも実際にははるかに興味深い動的システムに一般化することができれば

1443
00:59:12,839 --> 00:59:14,280


1444
00:59:14,280 --> 00:59:17,220


1445
00:59:17,220 --> 00:59:20,099
、

1446
00:59:20,099 --> 00:59:22,200


1447
00:59:22,200 --> 00:59:25,200


1448
00:59:25,200 --> 00:59:27,799
特定のタイム ステップで行われたアクションが、

1449
00:59:27,799 --> 00:59:30,299
後のタイム ステップで別のノードに影響を与えるように移動するシステムで因果推論を実行できるようにするその他のテクニック。これは

1450
00:59:30,299 --> 00:59:32,640
基本的に壮大な

1451
00:59:32,640 --> 00:59:34,859
因果関係です。

1452
00:59:34,859 --> 00:59:38,160
はい、それだけです。そして、ありがとう、ありがとう、

1453
00:59:38,160 --> 00:59:41,118


1454
00:59:47,460 --> 00:59:51,119
ありがとう 素晴らしくて非常に包括的な

1455
00:59:51,119 --> 00:59:53,160
プレゼンテーションで、本当にミュートされているのかと思っていました、申し訳ありませんが

1456
00:59:53,160 --> 00:59:55,700


1457
00:59:57,119 --> 00:59:59,700
Zoomでミュートになりましたが、はい、

1458
00:59:59,700 --> 01:00:02,400


1459
01:00:02,400 --> 01:00:05,099
素晴らしく非常に包括的なプレゼンテーションをありがとうございました。本当にたくさんの内容があり

1460
01:00:05,099 --> 01:00:06,900
、ライブチャットでも素晴らしい質問がたくさんありました。

1461
01:00:06,900 --> 01:00:09,900


1462
01:00:09,900 --> 01:00:12,900
どのようにして

1463
01:00:12,900 --> 01:00:15,960
このテーマを研究するようになったのか、

1464
01:00:15,960 --> 01:00:18,900
因果関係を研究していて予測コーディングが

1465
01:00:18,900 --> 01:00:21,000
役立つと気づいたのか、あるいはその逆なのか、あるいはどのようにして

1466
01:00:21,000 --> 01:00:23,160
この交差点にたどり着いたのかという質問ですが、

1467
01:00:23,160 --> 01:00:25,740
実際に

1468
01:00:25,740 --> 01:00:27,240
このアイデアを最初に思いついたのは次の人物だと言わなければなりません。

1469
01:00:27,240 --> 01:00:29,040
ああ、男爵だったね、

1470
01:00:29,040 --> 01:00:33,900
一年

1471
01:00:33,900 --> 01:00:36,660
半前にはもっと似たような男爵だったよ、もっと前に彼はこのアイデアを書いたページを持ってきて

1472
01:00:36,660 --> 01:00:38,940
、その後

1473
01:00:38,940 --> 01:00:42,119
忘れ去られて、誰もそれを取り上げなかった、

1474
01:00:42,119 --> 01:00:43,980
そして去年の夏、私は

1475
01:00:43,980 --> 01:00:47,880
因果関係とその因果関係について興味を持ち始めた

1476
01:00:47,880 --> 01:00:50,339
たとえば、

1477
01:00:50,339 --> 01:00:52,440
ポッドキャストを聴きながら『ブック・オブ・ライフ』を読んだのですが、あるトピックに

1478
01:00:52,440 --> 01:00:53,760
興味を持つ標準的な方法を知っています。

1479
01:00:53,760 --> 01:00:54,900


1480
01:00:54,900 --> 01:00:57,480


1481
01:00:57,480 --> 01:01:00,180
そして、このアイデアを男爵から思い出して彼に提案しました。

1482
01:01:00,180 --> 01:01:03,180
それを拡張して、えー、

1483
01:01:03,180 --> 01:01:06,000
実際に論文にしてみました。

1484
01:01:06,000 --> 01:01:07,319


1485
01:01:07,319 --> 01:01:09,359
実験に協力してくれる人たちを巻き込みました。そして、これが最後の最終

1486
01:01:09,359 --> 01:01:12,000
結果です。

1487
01:01:12,000 --> 01:01:14,160
素晴らしいですね、

1488
01:01:14,160 --> 01:01:15,240
ええと、

1489
01:01:15,240 --> 01:01:17,400
言いたいことがたくさんあります。

1490
01:01:17,400 --> 01:01:19,619
ライブチャットに行くつもりです。 まず、

1491
01:01:19,619 --> 01:01:21,240
さまざまな質問に答えて、もし誰かが

1492
01:01:21,240 --> 01:01:22,440
私を追加したい場合は、私が最初に電気をつけるつもりです、

1493
01:01:22,440 --> 01:01:24,059
なぜなら私は

1494
01:01:24,059 --> 01:01:28,440
ますます暗闇に落ちていくと思うからです、はい、

1495
01:01:28,440 --> 01:01:30,720
能動的推論では解決できないと誰が言ったのですか

1496
01:01:30,720 --> 01:01:32,160
暗い部屋の問題 ああ、

1497
01:01:32,160 --> 01:01:34,980
はい、ここにいます、

1498
01:01:34,980 --> 01:01:37,020
照明のスイッチのせいで明るくなったと思いますか、はい、

1499
01:01:37,020 --> 01:01:39,299


1500
01:01:39,299 --> 01:01:40,680


1501
01:01:40,680 --> 01:01:42,240


1502
01:01:42,240 --> 01:01:43,980
ここには問題はないと思います、ええとわかりました

1503
01:01:43,980 --> 01:01:46,940
ml

1504
01:01:46,940 --> 01:01:49,559
予測コーディングではすべての

1505
01:01:49,559 --> 01:01:52,020
分布は通常ガウス分布であるため、

1506
01:01:52,020 --> 01:01:53,760
ボトムアップメッセージは精度の

1507
01:01:53,760 --> 01:01:55,500
重み付けされた予測であるため、ドーンが書きました

1508
01:01:55,500 --> 01:01:57,420
精度がガウス共

1509
01:01:57,420 --> 01:02:00,000
分散の逆数であるエラー 非ガウス

1510
01:02:00,000 --> 01:02:03,319
分布が使用された場合はどうなるかは、

1511
01:02:03,780 --> 01:02:05,339


1512
01:02:05,339 --> 01:02:09,059


1513
01:02:09,059 --> 01:02:10,380


1514
01:02:10,380 --> 01:02:13,079


1515
01:02:13,079 --> 01:02:15,480


1516
01:02:15,480 --> 01:02:18,480
基本的に一般的な方法は異なりますが、主な違いは、正しく指摘されたように、予測誤差がないことです。

1517
01:02:18,480 --> 01:02:20,819
仮想自由エネルギーの導関数 ガウス仮定がある場合、

1518
01:02:20,819 --> 01:02:22,920


1519
01:02:22,920 --> 01:02:25,020
ええ、その単一の量を

1520
01:02:25,020 --> 01:02:27,960
ゼロに設定することさえできます、そしておそらく介入を実行するにはグラフ

1521
01:02:27,960 --> 01:02:29,880
の構造に作用する必要があります、

1522
01:02:29,880 --> 01:02:30,900


1523
01:02:30,900 --> 01:02:34,020


1524
01:02:34,020 --> 01:02:37,079
そしてあなたと同僚は

1525
01:02:37,079 --> 01:02:39,900
2022年に予測する論文を持っていました コーディング ガウス分布を超えて、

1526
01:02:39,900 --> 01:02:41,880


1527
01:02:41,880 --> 01:02:43,859
これらの問題のいくつかを検討しました

1528
01:02:43,859 --> 01:02:46,260
はい、はい、そのとおりです、その論文はちょっとしたものでした

1529
01:02:46,260 --> 01:02:47,339


1530
01:02:47,339 --> 01:02:50,460
その論文の背後にあるアイデアはええと、

1531
01:02:50,460 --> 01:02:53,220
私たちはトランスフォーマーをモデル化します、それが

1532
01:02:53,220 --> 01:02:54,420
かなり難しいことを使った最大の動機です、

1533
01:02:54,420 --> 01:02:57,180
そして答えは、ええと、それはそうではないからです

1534
01:02:57,180 --> 01:02:59,460
アテンション メカニズムには

1535
01:02:59,460 --> 01:03:02,099
最後にソフト マックスがあり、ソフト マックスは

1536
01:03:02,099 --> 01:03:03,960


1537
01:03:03,960 --> 01:03:08,400
ガウス分布ではなく、

1538
01:03:08,400 --> 01:03:11,280
ソフト マックス分布を呼び出します。

1539
01:03:11,280 --> 01:03:13,440
名前は今はわかりませんが、はい、それで、

1540
01:03:13,440 --> 01:03:16,079
はい、それは一般化です、

1541
01:03:16,079 --> 01:03:19,140
それは少しです ガストンの仮定を取り除いたら、それを呼び出すのは

1542
01:03:19,140 --> 01:03:20,700


1543
01:03:20,700 --> 01:03:22,319
少し難しいですが、それをクリエイティブコーディングと呼ぶのはまだ少し難しいので、

1544
01:03:22,319 --> 01:03:24,059


1545
01:03:24,059 --> 01:03:26,400
彼は

1546
01:03:26,400 --> 01:03:29,819


1547
01:03:29,819 --> 01:03:32,700
たとえば、車のフリーストーンと話すようなものです、彼がクリエイティブコーディングを好むのは、

1548
01:03:32,700 --> 01:03:35,160
ガウスとガウスしか持っていない場合に限られます

1549
01:03:35,160 --> 01:03:37,680
仮定です

1550
01:03:37,680 --> 01:03:39,720
が、はい、それは

1551
01:03:39,720 --> 01:03:42,660
うーん興味深いというよりも哲学的な議論です、そして、

1552
01:03:42,660 --> 01:03:44,940


1553
01:03:44,940 --> 01:03:46,740
それが間違いなく非常に

1554
01:03:46,740 --> 01:03:49,500
興味深いもう一つのトピックは、トランスフォーマーの

1555
01:03:49,500 --> 01:03:52,980
注意装置

1556
01:03:52,980 --> 01:03:56,099
と、

1557
01:03:56,099 --> 01:03:58,440
神経認知的

1558
01:03:58,440 --> 01:04:00,180
観点および

1559
01:04:00,180 --> 01:04:03,240
予測処理から注意が説明される方法との類似点と相違点です。 待っている角度

1560
01:04:03,240 --> 01:04:06,200
それについてどう思いますか?

1561
01:04:06,359 --> 01:04:08,700


1562
01:04:08,700 --> 01:04:12,359
まあ、その考えは、そうですね、私が考えているのは、

1563
01:04:12,359 --> 01:04:15,000
かなりの処理と

1564
01:04:15,000 --> 01:04:16,400
操作推論の観点から、

1565
01:04:16,400 --> 01:04:19,260
注意は一種の構造学習問題として見なすことができるということです。

1566
01:04:19,260 --> 01:04:21,299


1567
01:04:21,299 --> 01:04:23,040


1568
01:04:23,040 --> 01:04:25,680
Chris Buckley のグループのうーんからの最近の論文は、

1569
01:04:25,680 --> 01:04:26,339


1570
01:04:26,339 --> 01:04:28,079


1571
01:04:28,079 --> 01:04:30,420
アーカイブに再版があるはずであることを示しています。基本的に、

1572
01:04:30,420 --> 01:04:31,859
注意メカニズムは他のデータ ポイントに固有の重みパラメータの

1573
01:04:31,859 --> 01:04:35,819
精度を学習しているだけであることが示されているため、

1574
01:04:35,819 --> 01:04:38,880


1575
01:04:38,880 --> 01:04:41,040
この精度は では

1576
01:04:41,040 --> 01:04:43,200
ありません ではありません

1577
01:04:43,200 --> 01:04:45,540
モデルの構造内にあるパラメータでは

1578
01:04:45,540 --> 01:04:47,579
ないため、モデル固有のパラメータではありません

1579
01:04:47,579 --> 01:04:49,140


1580
01:04:49,140 --> 01:04:51,660
値ノードのような急速に変化するパラメータであり、

1581
01:04:51,660 --> 01:04:53,760
自由エネルギーの変動を最小限に抑えながら更新され

1582
01:04:53,760 --> 01:04:55,440
、一度更新されると それを最小化して

1583
01:04:55,440 --> 01:04:57,000
計算した後、それを破棄し、

1584
01:04:57,000 --> 01:04:58,920
次のデータポイントのために

1585
01:04:58,920 --> 01:05:00,780
最初から再計算する必要があります。つまり、計算の

1586
01:05:00,780 --> 01:05:03,299


1587
01:05:03,299 --> 01:05:05,819
アナロジーは、ええと、アテンション

1588
01:05:05,819 --> 01:05:07,920
メカニズムは一種の構造学習と見なすことができますが、

1589
01:05:07,920 --> 01:05:10,559


1590
01:05:10,559 --> 01:05:13,020


1591
01:05:13,020 --> 01:05:15,119
モデル固有ではなくデータポイント固有の構造学習です。

1592
01:05:15,119 --> 01:05:17,280
少し一般化して、トランスフォーマーの

1593
01:05:17,280 --> 01:05:18,960


1594
01:05:18,960 --> 01:05:20,339
注意メカニズムから

1595
01:05:20,339 --> 01:05:21,900


1596
01:05:21,900 --> 01:05:24,180
認知科学に移行したい場合、

1597
01:05:24,180 --> 01:05:28,020


1598
01:05:28,020 --> 01:05:31,260
類似点と注意メカニズムはおそらく 2 つ異なると思います。

1599
01:05:31,260 --> 01:05:33,359
ええと、構造学習のアナロジー

1600
01:05:33,359 --> 01:05:36,660
と、ある接続が別の接続に対してどれだけ重要であるかということは、おそらくはるかに

1601
01:05:36,660 --> 01:05:38,760


1602
01:05:38,760 --> 01:05:41,900
うまく機能すると思いますクールな

1603
01:05:42,000 --> 01:05:44,880
灰色の答えはわかりました

1604
01:05:44,880 --> 01:05:49,200
mlドンが反事実で尋ねます

1605
01:05:49,200 --> 01:05:51,240
隠れた変数

1606
01:05:51,240 --> 01:05:55,440
Xと観察されていない変数Uの

1607
01:05:55,440 --> 01:05:59,180
違いは何ですか

1608
01:05:59,540 --> 01:06:01,740
主な問題は、使用方法を観察できないことです。

1609
01:06:01,740 --> 01:06:03,599


1610
01:06:03,599 --> 01:06:05,819


1611
01:06:05,819 --> 01:06:09,000
計算して修正することはできるので使用できますが、それはできません。つまり、

1612
01:06:09,000 --> 01:06:10,559


1613
01:06:10,559 --> 01:06:13,380
それらを制御できないため、使用する必要があるという考えです。

1614
01:06:13,380 --> 01:06:16,020
環境固有の変数として見なされ、

1615
01:06:16,020 --> 01:06:18,540
それらが存在することはプロセスに影響を与えます

1616
01:06:18,540 --> 01:06:21,240
が、

1617
01:06:21,240 --> 01:06:23,280
たとえば過去に戻ると

1618
01:06:23,280 --> 01:06:25,079
環境が異なるため、

1619
01:06:25,079 --> 01:06:26,520
たとえば、前の

1620
01:06:26,520 --> 01:06:28,440
例に戻りたい場合のアイデアです。

1621
01:06:28,440 --> 01:06:29,880


1622
01:06:29,880 --> 01:06:31,920


1623
01:06:31,920 --> 01:06:34,619
教育に関する特定の知性を持つ人の期待収入 ええと、

1624
01:06:34,619 --> 01:06:37,440
教育の学位を持っている人の

1625
01:06:37,440 --> 01:06:40,200
考えは、

1626
01:06:40,200 --> 01:06:43,559
今日どれだけ学ぶか知りたい場合、ええと、修士

1627
01:06:43,559 --> 01:06:45,359
号とではわかりませんが、修士号とは

1628
01:06:45,359 --> 01:06:47,339
敬意が異なります。

1629
01:06:47,339 --> 01:06:48,359


1630
01:06:48,359 --> 01:06:50,819
20年前に修士号を取得して私が稼いでいた金額は、

1631
01:06:50,819 --> 01:06:52,619
たとえばここイタリアでは、

1632
01:06:52,619 --> 01:06:55,440
他の国や

1633
01:06:55,440 --> 01:06:57,000
あなたの

1634
01:06:57,000 --> 01:06:58,859
制御下にないすべての変数に関して異なります。ビジョンネットワークを使用してそれらをモデル化することはできません

1635
01:06:58,859 --> 01:07:00,359


1636
01:07:00,359 --> 01:07:03,480
が、それらはそこにあるので大丈夫です。

1637
01:07:03,480 --> 01:07:05,220


1638
01:07:05,220 --> 01:07:07,559
結論を導き出したいときにそれらを無視することはできないので、彼はそうです、

1639
01:07:07,559 --> 01:07:08,760
基本的にあなたがコントロール

1640
01:07:08,760 --> 01:07:10,079
できないことはすべて

1641
01:07:10,079 --> 01:07:13,079
あなたがそれらを推測できるので、できるようになります

1642
01:07:13,079 --> 01:07:14,819


1643
01:07:14,819 --> 01:07:16,740
時間を遡って反事実的な推論を実行して、ああ、20

1644
01:07:16,740 --> 01:07:19,020
年前なら私はこれを獲得していただろうと言うことができます もちろん

1645
01:07:19,020 --> 01:07:20,640


1646
01:07:20,640 --> 01:07:22,559


1647
01:07:22,559 --> 01:07:24,599
平均してこれくらいの知能があれば

1648
01:07:24,599 --> 01:07:27,059
、でも

1649
01:07:27,059 --> 01:07:30,720
仕事やそのようなものに対する政府の政策を変えることができるわけではない、それは

1650
01:07:30,720 --> 01:07:32,819


1651
01:07:32,819 --> 01:07:35,099
より深い反事実である、

1652
01:07:35,099 --> 01:07:38,400
はい、そのとおりです、はい、それらは素晴らしい使用法です、

1653
01:07:38,400 --> 01:07:40,200


1654
01:07:40,200 --> 01:07:42,480
大丈夫です 予測コーディングで一般化座標を実装しました いいえ、

1655
01:07:42,480 --> 01:07:45,660


1656
01:07:45,660 --> 01:07:46,920


1657
01:07:46,920 --> 01:07:50,039
いいえ、やったことがありません、

1658
01:07:50,039 --> 01:07:52,680
ええと、勉強しましたが、ええと、

1659
01:07:52,680 --> 01:07:55,260
実装したことはありません、それらが不安定になる傾向があることは知っています、

1660
01:07:55,260 --> 01:07:57,599


1661
01:07:57,599 --> 01:08:00,299
そして、それは それらを安定させるのは非常に難しいです。

1662
01:08:00,299 --> 01:08:02,940
これが、

1663
01:08:02,940 --> 01:08:05,460


1664
01:08:05,460 --> 01:08:08,359
それらを実装した人々と話して得た結論だと思いますが、

1665
01:08:08,400 --> 01:08:11,039
はいはい、

1666
01:08:11,039 --> 01:08:12,839
実際に最近発表された、英国の負荷で

1667
01:08:12,839 --> 01:08:15,599
テストしたそれらについてのいくつかの論文を知っています。

1668
01:08:15,599 --> 01:08:18,000
エンコーダのスタイル 実は

1669
01:08:18,000 --> 01:08:20,520
まだバロンからのものだと思う

1670
01:08:20,520 --> 01:08:22,979


1671
01:08:22,979 --> 01:08:25,439
去年の夏に出た論文があるけど、いいえ、自分でそれらを使ってプレイしたことはない

1672
01:08:25,439 --> 01:08:26,580


1673
01:08:26,580 --> 01:08:29,160
クールなロンパースは、

1674
01:08:29,160 --> 01:08:32,040
階層内にさらにレベルを追加すると、入力の

1675
01:08:32,040 --> 01:08:35,160


1676
01:08:35,160 --> 01:08:38,238
追加を予測することで気を散らす問題が軽減されます

1677
01:08:38,939 --> 01:08:41,698
えー、

1678
01:08:41,698 --> 01:08:43,439
どういう意味でのレベルかというと、破壊

1679
01:08:43,439 --> 01:08:45,779
問題は Cycles によって与えられるので、基本的には

1680
01:08:45,779 --> 01:08:47,399
画像を提供し

1681
01:08:47,399 --> 01:08:49,920
、えー、

1682
01:08:49,920 --> 01:08:53,279
画像からパッチが

1683
01:08:53,279 --> 01:08:55,799
ニューロンに入り、他のエッジが

1684
01:08:55,799 --> 01:08:57,500
戻っていくという事実を提供することで、

1685
01:08:57,500 --> 01:08:59,939
基本的に次のような事実が作成されます。

1686
01:08:59,939 --> 01:09:03,560
基本的に

1687
01:09:03,560 --> 01:09:06,179
これらの入力は画像のピクセルに合わせて調整され、

1688
01:09:06,179 --> 01:09:08,339
予測誤差が生成される

1689
01:09:08,339 --> 01:09:09,719
ため、

1690
01:09:09,719 --> 01:09:12,140
モデル内に広がる予測誤差が発生します。それは

1691
01:09:12,140 --> 01:09:14,640
そうです。この問題は

1692
01:09:14,640 --> 01:09:16,979
サイクル全般に共通する問題だと思います。 おそらく、

1693
01:09:16,979 --> 01:09:21,439


1694
01:09:23,060 --> 01:09:25,140
ピクセルの一般的な階層とは関係ありません。

1695
01:09:25,140 --> 01:09:26,759
入力エッジがなければ、

1696
01:09:26,759 --> 01:09:27,660


1697
01:09:27,660 --> 01:09:30,540
破壊の問題はもうありません。

1698
01:09:30,540 --> 01:09:33,238
クールです。そして、

1699
01:09:33,238 --> 01:09:35,939
トレース オペレーターによる非周期ネットワークの仕様は、

1700
01:09:35,939 --> 01:09:37,859


1701
01:09:37,859 --> 01:09:41,819
非常に興味深い技術です。

1702
01:09:41,819 --> 01:09:46,339
いつ導入されたのですか。

1703
01:09:46,560 --> 01:09:49,140
私が知っている限り、彼は

1704
01:09:49,140 --> 01:09:52,380
私が引用した論文を 2018 年に発表したと思います。

1705
01:09:52,380 --> 01:09:54,360
少なくとも因果

1706
01:09:54,360 --> 01:09:56,940
推論の文献

1707
01:09:56,940 --> 01:09:59,699
については知りません。以前の方法は知りません。ノーと言うでしょう。

1708
01:09:59,699 --> 01:10:01,860
つまり、これは非常に

1709
01:10:01,860 --> 01:10:04,140
引用されている論文なので、彼らがそのアイデアを思いついたと言えます。

1710
01:10:04,140 --> 01:10:05,520


1711
01:10:05,520 --> 01:10:07,980
すごいですね、

1712
01:10:07,980 --> 01:10:09,480
勾配降下法を実行して構造を学習できるのは非常に素晴らしいことです。

1713
01:10:09,480 --> 01:10:11,400


1714
01:10:11,400 --> 01:10:14,219
これは非常に強力なテクニックだと思います。ええ、

1715
01:10:14,219 --> 01:10:15,840
時々、次のようになります。

1716
01:10:15,840 --> 01:10:17,640


1717
01:10:17,640 --> 01:10:19,440
ベイズ推論と

1718
01:10:19,440 --> 01:10:23,159
因果推論のさまざまな機能が利用可能になったとき、

1719
01:10:23,159 --> 01:10:25,620
なぜ

1720
01:10:25,620 --> 01:10:28,500
これがベイズ因果モデリングのフレームワークの下で行われなかったのかと本当に注目に値します。なぜなら、

1721
01:10:28,500 --> 01:10:30,719


1722
01:10:30,719 --> 01:10:32,760


1723
01:10:32,760 --> 01:10:36,659
これが起こったのはわずか5年から25年程度であり

1724
01:10:36,659 --> 01:10:39,960
、非常に短いものだからです。 また、

1725
01:10:39,960 --> 01:10:42,060
それは比較的技術的なので、

1726
01:10:42,060 --> 01:10:43,920


1727
01:10:43,920 --> 01:10:46,920
それに取り組んでいる研究グループは比較的少ないです、そして、

1728
01:10:46,920 --> 01:10:49,860
それが可能にするものは本当に素晴らしいです、いいえ、はい、

1729
01:10:49,860 --> 01:10:51,960


1730
01:10:51,960 --> 01:10:54,179


1731
01:10:54,179 --> 01:10:56,040


1732
01:10:56,040 --> 01:10:59,100
正確に言います、それはまた、この分野のエキサイティングな部分だと思います、ええと、間違いなく存在します まだ発見されなければならないブレークスルーがそこにあり、おそらく気に入っているでしょう。なぜなら、

1733
01:10:59,100 --> 01:11:01,020


1734
01:11:01,020 --> 01:11:03,000
たとえば

1735
01:11:03,000 --> 01:11:05,300
あの論文がブレークスルーだったのと同じくらい

1736
01:11:05,300 --> 01:11:07,800
彼らが発見したのと

1737
01:11:07,800 --> 01:11:09,960
同じくらい、ああ、単純に非環状構造の適切な事前分布を発見したようなものです。

1738
01:11:09,960 --> 01:11:12,120


1739
01:11:12,120 --> 01:11:14,040


1740
01:11:14,040 --> 01:11:17,100
わかりました、はい、つまり私はわかりません まさにそのとおりですが、それは

1741
01:11:17,100 --> 01:11:19,080
あなたがある日の午後に思いついたアイデアかもしれません。

1742
01:11:19,080 --> 01:11:21,120


1743
01:11:21,120 --> 01:11:23,040
他の人がどうやってそれを思いついたのかはわかりませんが、

1744
01:11:23,040 --> 01:11:25,320
潜在的に彼らは

1745
01:11:25,320 --> 01:11:27,239
ホワイトボードにいて、

1746
01:11:27,239 --> 01:11:29,280
ああ、それは実際にそうだったのかもしれません これは

1747
01:11:29,280 --> 01:11:32,159
大きな画期的な進歩であり、私は単純に

1748
01:11:32,159 --> 01:11:33,960
以前の定義

1749
01:11:33,960 --> 01:11:36,739
とこれらの画期的な進歩の多くを定義しました、

1750
01:11:36,739 --> 01:11:40,500
彼らはただ積み上げるだけではありません、

1751
01:11:40,500 --> 01:11:44,280
ええと、ブロックの塔のようなものではありません、

1752
01:11:44,280 --> 01:11:47,640
彼らは積み上げて構成します、それで何かがええ

1753
01:11:47,640 --> 01:11:50,159
と一般化されるでしょう 一般化された

1754
01:11:50,159 --> 01:11:52,140
座標、一般化された同期、

1755
01:11:52,140 --> 01:11:55,020
任意の大きなグラフ、または

1756
01:11:55,020 --> 01:11:57,239
マルチモーダル入力を備えたセンサー フュージョン

1757
01:11:57,239 --> 01:12:00,679
、それらすべてが本当に満足のいく効果的な方法でブレンドされているようなものです。

1758
01:12:00,679 --> 01:12:03,659
つまり、

1759
01:12:03,659 --> 01:12:05,640
誰かがすぐに思いつくような小さなことでも、

1760
01:12:05,640 --> 01:12:08,100


1761
01:12:08,100 --> 01:12:11,100
本当に影響を与えることができます。

1762
01:12:11,100 --> 01:12:14,159
OK ml Dawn は、

1763
01:12:14,159 --> 01:12:16,199
私の質問をしてくれて本当にありがとう、そして

1764
01:12:16,199 --> 01:12:18,060
感動的なプレゼンテーションをしてくれた Tomaso に 100 万の感謝を言いました、

1765
01:12:18,060 --> 01:12:21,360
とても素晴らしい、ああ、ありがとうございました、そして

1766
01:12:21,360 --> 01:12:23,280
Bert は、予測コーディングを

1767
01:12:23,280 --> 01:12:25,560
使用した言語モデルは、

1768
01:12:25,560 --> 01:12:27,179


1769
01:12:27,179 --> 01:12:30,260
Transformers を使用した言語モデルとどう違うのかと尋ねました、

1770
01:12:31,679 --> 01:12:32,520


1771
01:12:32,520 --> 01:12:35,340
ええと、実際にはそうだと思います

1772
01:12:35,340 --> 01:12:36,659
今日、

1773
01:12:36,659 --> 01:12:38,640
予測コーディングを使用して言語モデルを構築する必要がある場合でも、私は引き続きトランスフォーマーを使用します。そのため、

1774
01:12:38,640 --> 01:12:40,020


1775
01:12:40,020 --> 01:12:41,880
アイデアとしては、たとえば、で

1776
01:12:41,880 --> 01:12:42,780


1777
01:12:42,780 --> 01:12:45,659
定義した

1778
01:12:45,659 --> 01:12:48,440
これまたはこれらの階層ベイジアン ネットワークの階層グラフィカル モデルがあるとします。

1779
01:12:48,440 --> 01:12:50,460


1780
01:12:50,460 --> 01:12:53,100
一番最初に

1781
01:12:53,100 --> 01:12:55,380
1 つの矢印をスライドさせて、

1782
01:12:55,380 --> 01:12:57,300
線形マップである関数をエンコードする

1783
01:12:57,300 --> 01:12:59,219
ので、1 時間は、

1784
01:12:59,219 --> 01:13:01,080


1785
01:13:01,080 --> 01:13:03,060
潜在変数にエンコードされたベクトルの a と、

1786
01:13:03,060 --> 01:13:06,300
この重み行列を乗算するだけで、その後

1787
01:13:06,300 --> 01:13:08,580
非線形にすることができます。 しかし、

1788
01:13:08,580 --> 01:13:09,960
それは実際にはもっと

1789
01:13:09,960 --> 01:13:12,179
複雑なものになる可能性があります。矢印に含まれる関数は

1790
01:13:12,179 --> 01:13:14,880
畳み込みである可能性があり、注意メカニズムである可能性があります。実際にどのように

1791
01:13:14,880 --> 01:13:16,800


1792
01:13:16,800 --> 01:13:20,820
行うかですが、実際に

1793
01:13:20,820 --> 01:13:23,880
私たちがやった方法である意味を引き続き使用します

1794
01:13:23,880 --> 01:13:26,460


1795
01:13:26,460 --> 01:13:28,860
昨年のオックスフォード グループでは、すべての矢印がトランスフォーマーであるという正確な構造を持っていたということです。

1796
01:13:28,860 --> 01:13:30,900


1797
01:13:30,900 --> 01:13:33,420
したがって、1 つはアテンション

1798
01:13:33,420 --> 01:13:35,159
メカニズムで、次の矢印は

1799
01:13:35,159 --> 01:13:38,219
トランスフォーマーとしてのフィード フォワード ネットワークです。

1800
01:13:38,219 --> 01:13:40,020
基本的に唯一の違いは、

1801
01:13:40,020 --> 01:13:41,640


1802
01:13:41,640 --> 01:13:43,739
事後を計算したい変数を指定し、

1803
01:13:43,739 --> 01:13:45,239
それらの事後を

1804
01:13:45,239 --> 01:13:47,400
VIA 平均場近似によって独立性を持たせる

1805
01:13:47,400 --> 01:13:49,560
ため、基本的には

1806
01:13:49,560 --> 01:13:51,659


1807
01:13:51,659 --> 01:13:53,520


1808
01:13:53,520 --> 01:13:56,520
クリエイティブ コーディングの変動自由エネルギーに収束できるようにするすべての手順に従いますが、

1809
01:13:56,520 --> 01:13:58,199
その方法は異なります。 予測を計算し

1810
01:13:58,199 --> 01:14:01,199
、信号を送り返す方法は

1811
01:14:01,199 --> 01:14:04,739
Transformer を介して行われる

1812
01:14:04,739 --> 01:14:07,560
ので、一般的には Transformer を引き続き使用します。つまり、トランスフォーマーは

1813
01:14:07,560 --> 01:14:10,679
非常にうまく機能するため、

1814
01:14:10,679 --> 01:14:12,840
傲慢になって「

1815
01:14:12,840 --> 01:14:15,060
いや、私はやります」とは言えないと思います。

1816
01:14:15,060 --> 01:14:17,640
純粋に予測コーディングの方法の方が

1817
01:14:17,640 --> 01:14:18,920
構造は優れています

1818
01:14:18,920 --> 01:14:21,480
が、とにかくトランスフォーマーに近似します

1819
01:14:21,480 --> 01:14:22,500


1820
01:14:22,500 --> 01:14:24,420
申し訳ありませんが、構造学習はトランスフォーマーの

1821
01:14:24,420 --> 01:14:27,540
アプローチに近似すると言いました はい、構造

1822
01:14:27,540 --> 01:14:29,219
学習については、えー、

1823
01:14:29,219 --> 01:14:32,640
誰かが

1824
01:14:32,640 --> 01:14:34,800
クリエイティブコーディング

1825
01:14:34,800 --> 01:14:38,060
と注意メカニズムの類似点を尋ねたときに、

1826
01:14:38,280 --> 01:14:41,699
ええと述べました。 興味深い

1827
01:14:41,699 --> 01:14:42,900
ですね、

1828
01:14:42,900 --> 01:14:45,719
Amazon で気になることが 1 つあります。

1829
01:14:45,719 --> 01:14:47,640


1830
01:14:47,640 --> 01:14:49,380
あなたが言及した予測コーディング ネットワークに深さの概念が見えませんでした。

1831
01:14:49,380 --> 01:14:50,880
おそらく見逃していました。

1832
01:14:50,880 --> 01:14:52,380
予測コーディングに提供されている定義には

1833
01:14:52,380 --> 01:14:56,480
深さの概念が含まれていました。

1834
01:14:56,640 --> 01:14:59,460
深さとはどういう意味ですか。いいえ、

1835
01:14:59,460 --> 01:15:02,219
はい、本当です。 それは、

1836
01:15:02,219 --> 01:15:04,980
私が何度も言ったように、標準の定義は

1837
01:15:04,980 --> 01:15:06,960
階層的であるため、

1838
01:15:06,960 --> 01:15:08,400
一方向に進む予測があり、反対方向に進むいくつかの

1839
01:15:08,400 --> 01:15:09,719
予測エラーがあります。

1840
01:15:09,719 --> 01:15:10,620


1841
01:15:10,620 --> 01:15:14,340
基本的に何をするか、この論文で私たちが行ったこと

1842
01:15:14,340 --> 01:15:16,320
と、えー、最後のものでも

1843
01:15:16,320 --> 01:15:18,420
呼ばれます。 相対コーディングによる

1844
01:15:18,420 --> 01:15:19,920
任意のグラフ トポロジの学習は、深

1845
01:15:19,920 --> 01:15:22,260


1846
01:15:22,260 --> 01:15:25,620
さを

1847
01:15:25,620 --> 01:15:28,380
独立した

1848
01:15:28,380 --> 01:15:31,380
潜在変数と潜在変数と矢印の基本的なペアとして考えることができるということです。

1849
01:15:31,380 --> 01:15:33,239


1850
01:15:33,239 --> 01:15:34,739
予測はその方向に進み

1851
01:15:34,739 --> 01:15:36,300
、予測矢印は他の方向に進みます

1852
01:15:36,300 --> 01:15:38,340
が、その後、あなたは

1853
01:15:38,340 --> 01:15:41,880
これらをいくつもの方法で構成できるので、

1854
01:15:41,880 --> 01:15:45,239
基本的にこの

1855
01:15:45,239 --> 01:15:47,040
構成は

1856
01:15:47,040 --> 01:15:48,659
階層的である必要はなく、最終的に

1857
01:15:48,659 --> 01:15:50,820
サイクルを持つことができるので、たとえば、

1858
01:15:50,820 --> 01:15:53,520
別の

1859
01:15:53,520 --> 01:15:55,440
潜在変数を最初の変数に接続することができます 1 つを

1860
01:15:55,440 --> 01:15:57,540
接続し、他のものを接続すると、必要

1861
01:15:57,540 --> 01:15:59,340
に応じて絡み合った構造を作成できます。

1862
01:15:59,340 --> 01:16:00,420


1863
01:16:00,420 --> 01:16:02,699
たとえば、他の論文では、

1864
01:16:02,699 --> 01:16:04,500


1865
01:16:04,500 --> 01:16:06,659


1866
01:16:06,659 --> 01:16:08,460
脳構造の形状をしたネットワークをトレーニングします。

1867
01:16:08,460 --> 01:16:09,900
脳の領域は

1868
01:16:09,900 --> 01:16:12,239
内部でまばらに接続されており、

1869
01:16:12,239 --> 01:16:13,860
相互に部分的に接続されており、

1870
01:16:13,860 --> 01:16:15,719


1871
01:16:15,719 --> 01:16:17,640
最後には階層的なものは何もありませんが、

1872
01:16:17,640 --> 01:16:18,960


1873
01:16:18,960 --> 01:16:20,699
動作自由エネルギーを最小限に抑え、ネットワークの

1874
01:16:20,699 --> 01:16:22,620
総予測誤差を最小限に抑えることで、トレーニング

1875
01:16:22,620 --> 01:16:25,159


1876
01:16:25,159 --> 01:16:27,360
することができます。

1877
01:16:27,360 --> 01:16:31,980
絡み合ったグラフ内の特定のモチーフについては、

1878
01:16:31,980 --> 01:16:35,159
連続した 3 つのレイヤーが表示される場合があります。

1879
01:16:35,159 --> 01:16:37,560
それらを単独で見ると、ああ、

1880
01:16:37,560 --> 01:16:38,820
これは 3 階建ての建物で、アダプティブ 3 と書かれた

1881
01:16:38,820 --> 01:16:41,940
3 レイヤー モデルだと思うでしょう。

1882
01:16:41,940 --> 01:16:43,980
しかし、より

1883
01:16:43,980 --> 01:16:46,620
大きな写真を撮ると、そこでより大きな写真を撮ることができます。 そのネットワークの

1884
01:16:46,620 --> 01:16:50,280
明示的なトップや明示的なボトムの

1885
01:16:50,280 --> 01:16:52,140


1886
01:16:52,140 --> 01:16:54,360
ようなものではありません、はい、正確に、これは基本的に、

1887
01:16:54,360 --> 01:16:55,980


1888
01:16:55,980 --> 01:16:58,080
韓国の予測ネットワークのすべての操作が

1889
01:16:58,080 --> 01:16:59,460
厳密にローカルであるという事実によって与えられます。つまり、

1890
01:16:59,460 --> 01:17:01,739
基本的にすべてのメッセージが

1891
01:17:01,739 --> 01:17:03,000
すべての予測とすべての予測

1892
01:17:03,000 --> 01:17:05,280
エラーを通過します 送信するのは

1893
01:17:05,280 --> 01:17:08,280
非常に近くのニューロンにのみ送信するということですが、

1894
01:17:08,280 --> 01:17:10,380
グローバル構造が実際に

1895
01:17:10,380 --> 01:17:13,380
階層的であるかどうかは、単一の

1896
01:17:13,380 --> 01:17:16,940
メッセージの受け渡しでは認識されません。これは、

1897
01:17:17,460 --> 01:17:19,620


1898
01:17:19,620 --> 01:17:22,820
新しいモデルアーキテクチャを学習するための希望のようなものだと思います。

1899
01:17:22,820 --> 01:17:27,739


1900
01:17:27,739 --> 01:17:33,300
トップダウンで設計されているものは非常に小さく、

1901
01:17:33,300 --> 01:17:36,480
非常に効率的なモデルであるにもかかわらず、現在多くのモデルが使用されています。

1902
01:17:36,480 --> 01:17:38,640


1903
01:17:38,640 --> 01:17:41,100
ええと、コンピューティング単位あたりの効率が高いかどうかを尋ねることもできますが、

1904
01:17:41,100 --> 01:17:43,320
それは第 2

1905
01:17:43,320 --> 01:17:45,300
レベルの質問ですが、今日の多くの効果的な

1906
01:17:45,300 --> 01:17:47,580
モデルにはこれらのいくつかがありません

1907
01:17:47,580 --> 01:17:49,860
予測コーディング ネットワークの特性は、

1908
01:17:49,860 --> 01:17:52,739
その能力のようなものです。

1909
01:17:52,739 --> 01:17:55,520
ローカル計算のみを使用することで、

1910
01:17:55,520 --> 01:17:59,400
生物学的リアリズム、

1911
01:17:59,400 --> 01:18:02,880
または単に時空間的リアリズムを提供しますが、

1912
01:18:02,880 --> 01:18:06,060


1913
01:18:06,060 --> 01:18:08,159
フェデレーテッド コンピューティングや分散

1914
01:18:08,159 --> 01:18:10,500
コンピューティング設定などで多くの利点を提供する可能性もあります。

1915
01:18:10,500 --> 01:18:12,780
いいえ、はい、まったく同意します。

1916
01:18:12,780 --> 01:18:14,520
一般的なアイデアは次のとおりだと思いますが、

1917
01:18:14,520 --> 01:18:16,679
それが

1918
01:18:16,679 --> 01:18:18,540
利点になるかどうかはわかりません。ですから、まさにあなたが言った理由から、それは非常に有望だと思います。

1919
01:18:18,540 --> 01:18:20,159


1920
01:18:20,159 --> 01:18:20,880


1921
01:18:20,880 --> 01:18:22,920
そして、その理由は、

1922
01:18:22,920 --> 01:18:25,380
バックプロパゲーションを備えた今日のモデル文字列は、

1923
01:18:25,380 --> 01:18:28,860
基本的に要約できるということです 基本的

1924
01:18:28,860 --> 01:18:32,040
に

1925
01:18:32,040 --> 01:18:34,080


1926
01:18:34,080 --> 01:18:36,120
入力から出力へのマップがあり、バックプロパゲーションは基本的に

1927
01:18:36,120 --> 01:18:39,600


1928
01:18:39,600 --> 01:18:41,699
その計算

1929
01:18:41,699 --> 01:18:44,340
グラフから情報を逆伝播するため、バックプロパゲーションは関数です。そのため、

1930
01:18:44,340 --> 01:18:45,960
今日使用されているすべてのニューラルネットワークニューラルネットワークモデルは、

1931
01:18:45,960 --> 01:18:48,960
予測コーディング中の関数です。

1932
01:18:48,960 --> 01:18:51,179
そして、古い関数クラスのような別の自由コーディングは、

1933
01:18:51,179 --> 01:18:53,820


1934
01:18:53,820 --> 01:18:56,040
ローカル計算を使用してトレーニングし、

1935
01:18:56,040 --> 01:18:58,500


1936
01:18:58,500 --> 01:19:01,620
グローバル エネルギー関数を最小化することで実際に機能するメソッドのクラスです。

1937
01:19:01,620 --> 01:19:03,840


1938
01:19:03,840 --> 01:19:05,940
入力から出力までのモデル関数に限定されず、実際に何かをモデル化します。

1939
01:19:05,940 --> 01:19:07,739
ある種の物理システムに似ている

1940
01:19:07,739 --> 01:19:10,080
ので、物理

1941
01:19:10,080 --> 01:19:13,500
システムがあり、入力したものにいくつかの値を固定し、

1942
01:19:13,500 --> 01:19:15,360


1943
01:19:15,360 --> 01:19:17,280
システムを収束させてから、出力されるはずの

1944
01:19:17,280 --> 01:19:19,980
ニューロンまたは変数の他の値を読み取ります

1945
01:19:19,980 --> 01:19:21,960
が、この

1946
01:19:21,960 --> 01:19:24,120
物理システムはそうではありません フィット

1947
01:19:24,120 --> 01:19:25,920
フォワードマップである必要はありません

1948
01:19:25,920 --> 01:19:28,260
入力空間と

1949
01:19:28,260 --> 01:19:30,659
出力空間を持つ関数である必要はありません それで学習

1950
01:19:30,659 --> 01:19:32,580
できるモデルのクラスは

1951
01:19:32,580 --> 01:19:34,800
それだけです 基本的にはフィードフォワードモデルのように見ることができます

1952
01:19:34,800 --> 01:19:37,560
そして関数、

1953
01:19:37,560 --> 01:19:39,600
そして物理システムのさらに大きなクラスが

1954
01:19:39,600 --> 01:19:41,880


1955
01:19:41,880 --> 01:19:43,860
ここに何か興味深いものがあるのか​​どうかは

1956
01:19:43,860 --> 01:19:45,659
まだわかりませんが、関数は

1957
01:19:45,659 --> 01:19:47,460
非常にうまく機能しているため、

1958
01:19:47,460 --> 01:19:50,040
バックプロパゲーションを使用した日々が見られますが、

1959
01:19:50,040 --> 01:19:52,199
それらはクレイジーにうまく機能しますが、

1960
01:19:52,199 --> 01:19:53,460
そうです

1961
01:19:53,460 --> 01:19:56,040
大きな部分に何か興味深い点があるかどうかはわかりませんが、重要な

1962
01:19:56,040 --> 01:19:58,380
部分は非常に大きいということです。

1963
01:19:58,380 --> 01:20:00,480


1964
01:20:00,480 --> 01:20:02,940
増殖を元に戻すことができないモデルがたくさんあり、

1965
01:20:02,940 --> 01:20:04,679
クリエイティブなコーディング

1966
01:20:04,679 --> 01:20:06,659
やバスルームの増殖、またはその他の

1967
01:20:06,659 --> 01:20:07,860
方法でトレーニングできます。

1968
01:20:07,860 --> 01:20:10,440
それはとても興味深いですね 確かに

1969
01:20:10,440 --> 01:20:12,719
生物学的システム 物理的システムは

1970
01:20:12,719 --> 01:20:15,900
あらゆる種類の興味深い問題を解決します ええと、

1971
01:20:15,900 --> 01:20:17,100


1972
01:20:17,100 --> 01:20:19,380
しかし、この環境

1973
01:20:19,380 --> 01:20:21,540
で本当にうまくいくアリ種にはまだフリーランチがありません

1974
01:20:21,540 --> 01:20:23,100


1975
01:20:23,100 --> 01:20:25,679
別の環境ではあまりうまくいかないかもしれません、

1976
01:20:25,679 --> 01:20:28,260
奥地ではそうです

1977
01:20:28,260 --> 01:20:31,820


1978
01:20:31,820 --> 01:20:35,880


1979
01:20:35,880 --> 01:20:38,460
関数であるということで十分に説明されていない、非常にユニークな特別なアルゴリズムがいくつかあるかもしれませんが、

1980
01:20:38,460 --> 01:20:42,060
それでも非常に非常に効果的な

1981
01:20:42,060 --> 01:20:46,679
ヒューリスティックを実装するための手続き型の方法を提供します。

1982
01:20:46,679 --> 01:20:48,840


1983
01:20:48,840 --> 01:20:51,120


1984
01:20:51,120 --> 01:20:53,880


1985
01:20:53,880 --> 01:20:55,679


1986
01:20:55,679 --> 01:20:58,260
博士課程での研究の

1987
01:20:58,260 --> 01:20:59,100
例

1988
01:20:59,100 --> 01:21:01,380
としては、関数の内側ではなく、ここにあるようなアプリケーションを見つけることなどが挙げられます。

1989
01:21:01,380 --> 01:21:04,199


1990
01:21:04,199 --> 01:21:06,739
機能は

1991
01:21:07,199 --> 01:21:08,820
優れていますが、

1992
01:21:08,820 --> 01:21:12,120
この作業はここからどこへ向かうのでしょうか。

1993
01:21:12,120 --> 01:21:14,520
どのような方向に興奮しているのか

1994
01:21:14,520 --> 01:21:17,340
、アクティブな推論エコシステムの人々がどのような成果を

1995
01:21:17,340 --> 01:21:19,679
上げていると思いますか。 この種の仕事に携わっているので、

1996
01:21:19,679 --> 01:21:22,460


1997
01:21:22,500 --> 01:21:24,840
おそらく最も

1998
01:21:24,840 --> 01:21:27,780
有望な方向性だと思います。

1999
01:21:27,780 --> 01:21:30,060
うーん、彼女はおそらく私が少し

2000
01:21:30,060 --> 01:21:33,060
探索したいものです。先ほども言ったように、

2001
01:21:33,060 --> 01:21:34,980
実際には静的モデルの背後にあるので、

2002
01:21:34,980 --> 01:21:37,380
私が示したものはすべてそうです

2003
01:21:37,380 --> 01:21:40,260
これまで示してきたのは静的データに関するもので、

2004
01:21:40,260 --> 01:21:42,840
データは時間の経過とともに変化しません。

2005
01:21:42,840 --> 01:21:45,780


2006
01:21:45,780 --> 01:21:48,000


2007
01:21:48,000 --> 01:21:49,080
ここで示したように、クリエイティブ コーディングの定義内に時間はありません。

2008
01:21:49,080 --> 01:21:50,940
ただし、たとえば、

2009
01:21:50,940 --> 01:21:53,280
クリエイティブ コーディングを一般化して機能させることもできます。 先ほど述べたよう

2010
01:21:53,280 --> 01:21:55,800
に、一般化された座標を使用した時間データを使用して、

2011
01:21:55,800 --> 01:21:58,800


2012
01:21:58,800 --> 01:22:01,380
一般的なカルマン フィルター生成モデルとして提示することで、

2013
01:22:01,380 --> 01:22:04,140


2014
01:22:04,140 --> 01:22:08,040
たとえば、

2015
01:22:08,040 --> 01:22:09,900
因果推論の方向が非常に役立つ可能性があります。 基本的に、

2016
01:22:09,900 --> 01:22:12,600


2017
01:22:12,600 --> 01:22:14,400


2018
01:22:14,400 --> 01:22:17,880
より大きな因果関係と

2019
01:22:17,880 --> 01:22:21,780
、より複雑で有用な

2020
01:22:21,780 --> 01:22:24,780
モデルの力学的原因をモデル化できるようになります。

2021
01:22:24,780 --> 01:22:26,940
なぜなら、一般に、科学の微積分法

2022
01:22:26,940 --> 01:22:28,560
や介入的および

2023
01:22:28,560 --> 01:22:32,760
反事実的な分野は、

2024
01:22:32,760 --> 01:22:36,000
ほとんどが小規模なモデルで開発されているためです

2025
01:22:36,000 --> 01:22:38,159


2026
01:22:38,159 --> 01:22:40,739
。

2027
01:22:40,739 --> 01:22:43,560
一般的に巨大なモデルに介入することはありません。そのため、

2028
01:22:43,560 --> 01:22:45,800
医療データを見ると、

2029
01:22:45,800 --> 01:22:50,159
比較的小さなビジョン ネットワークが使用されます。もちろん、

2030
01:22:50,159 --> 01:22:51,179


2031
01:22:51,179 --> 01:22:54,900


2032
01:22:54,900 --> 01:22:56,340
特定の環境や特定の

2033
01:22:56,340 --> 01:22:58,620
現実をモデル化する動的因果モデルが必要な場合は、 体内の多くのニューロンには、

2034
01:22:58,620 --> 01:23:00,780


2035
01:23:00,780 --> 01:23:02,580
時間の経過とともに変化する多くの潜在変数があり、

2036
01:23:02,580 --> 01:23:05,219
ある瞬間にさらに介入すると、別

2037
01:23:05,219 --> 01:23:07,560
のタイムステップで効果が生じるため、おそらく

2038
01:23:07,560 --> 01:23:09,239
10 個の異なるタイムステップ後の次のタイムステップで効果が生じる

2039
01:23:09,239 --> 01:23:11,699
と思います。

2040
01:23:11,699 --> 01:23:14,100


2041
01:23:14,100 --> 01:23:16,380


2042
01:23:16,380 --> 01:23:17,699


2043
01:23:17,699 --> 01:23:20,040


2044
01:23:20,040 --> 01:23:22,860
基本的に壮大な因果関係をモデル化することもできる、生物学的にもっともらしい情報伝達方法のような開発は非常に興味深いものです 基本的に、

2045
01:23:22,860 --> 01:23:24,659
うーん、

2046
01:23:24,659 --> 01:23:29,659
これらのモデルのどこにアクションが見られますか?

2047
01:23:30,840 --> 01:23:33,840
どこにアクションが見られますか?

2048
01:23:33,840 --> 01:23:36,480


2049
01:23:36,480 --> 01:23:38,760
私はそれらのモデルのアクションだと思いませんでした 他のモデル

2050
01:23:38,760 --> 01:23:41,460
でも見られるように、モデルは私と同じやり方かもしれません。なぜなら、

2051
01:23:41,460 --> 01:23:43,080


2052
01:23:43,080 --> 01:23:44,940
創造的なコーディングは基本的に知覚のモデルだからです。そのため、

2053
01:23:44,940 --> 01:23:46,260


2054
01:23:46,260 --> 01:23:49,260
アクションは、

2055
01:23:49,260 --> 01:23:52,739
自分が経験していることの結果があることがわかります。つまり、

2056
01:23:52,739 --> 01:23:55,159


2057
01:23:55,159 --> 01:23:57,840
何かを経験する方法を変えることで、その後、あなたは

2058
01:23:57,840 --> 01:24:00,060
計算できるかもしれません。より

2059
01:24:00,060 --> 01:24:01,800
多くの情報が得られたので、単純に賢いアクションを実行できるかもしれません。

2060
01:24:01,800 --> 01:24:03,000


2061
01:24:03,000 --> 01:24:04,560


2062
01:24:04,560 --> 01:24:06,960
でも、アクションはそう簡単ではないと思いますが、

2063
01:24:06,960 --> 01:24:10,199


2064
01:24:10,199 --> 01:24:12,540


2065
01:24:12,540 --> 01:24:14,040
これにより基本的に次のことが可能になるという事実以外に、アクションの明示的な結果は見当たりません。

2066
01:24:14,040 --> 01:24:15,960
おそらく、彼らが

2067
01:24:15,960 --> 01:24:18,780


2068
01:24:18,780 --> 01:24:21,719
将来的にアクションを実行するというより良い結論を導き出すだけでしょうか。

2069
01:24:21,719 --> 01:24:23,940


2070
01:24:23,940 --> 01:24:25,920
人々が予測

2071
01:24:25,920 --> 01:24:29,340
コーディングとアクションについて話してきたいくつかの方法について付け加えます。えー、まず内部

2072
01:24:29,340 --> 01:24:33,960
アクションまたは秘密アクションは注意です。そうすれば、

2073
01:24:33,960 --> 01:24:36,120
認識について考えることができます。

2074
01:24:36,120 --> 01:24:37,980
内部アクションとして、これは 1 つのアプローチであり、もう 1 つの

2075
01:24:37,980 --> 01:24:40,560
アプローチであり、かなりミクロなものであり、

2076
01:24:40,560 --> 01:24:42,840


2077
01:24:42,840 --> 01:24:45,780
特定のノードの出力は、そのノードを

2078
01:24:45,780 --> 01:24:48,780
独自の感覚認知状態と

2079
01:24:48,780 --> 01:24:52,080
動作状態を持つ特定のものとして理解することができ、その意味で

2080
01:24:52,080 --> 01:24:54,960
ノードの出力、そして最後にどれになる

2081
01:24:54,960 --> 01:24:57,179
かということになります。 私たちはライブ

2082
01:24:57,179 --> 01:24:59,940
ストリーム 43 で、

2083
01:24:59,940 --> 01:25:02,100
私たちが最後まで読んでいる予測コーディングに関する理論的レビューについて少し調べました。

2084
01:25:02,100 --> 01:25:03,840
それは

2085
01:25:03,840 --> 01:25:05,460
すべて知覚に関するものでした。そして、

2086
01:25:05,460 --> 01:25:08,040
セクション 5.3 のようなものでした。

2087
01:25:08,040 --> 01:25:11,719
アクションについて期待がある場合、

2088
01:25:11,719 --> 01:25:15,900
アクションは単なるものです。 このアーキテクチャのもう 1 つの変数であり、

2089
01:25:15,900 --> 01:25:18,120
これは実際には非能動推論と

2090
01:25:18,120 --> 01:25:20,040
一致しています。

2091
01:25:20,040 --> 01:25:21,659
報酬関数や効用関数のようなものを最大化する代わりに、

2092
01:25:21,659 --> 01:25:24,000


2093
01:25:24,000 --> 01:25:26,699


2094
01:25:26,699 --> 01:25:28,800


2095
01:25:28,800 --> 01:25:30,900
最も可能性の高い行動方針に基づいて行動を選択します。これはベイジアン力学であり

2096
01:25:30,900 --> 01:25:33,300
、実際には非常に重要です。

2097
01:25:33,300 --> 01:25:36,420
アクション変数を取り込んで、それをあたかも世界の

2098
01:25:36,420 --> 01:25:40,800


2099
01:25:40,800 --> 01:25:43,260
他の何かについての予測であるかのように利用するのが自然です

2100
01:25:43,260 --> 01:25:45,540


2101
01:25:45,540 --> 01:25:48,480


2102
01:25:48,480 --> 01:25:50,820


2103
01:25:50,820 --> 01:25:52,860


2104
01:25:52,860 --> 01:25:55,260
ええと、今でも、

2105
01:25:55,260 --> 01:25:57,239
たとえば

2106
01:25:57,239 --> 01:26:01,139
この方法を適用する論文はそれほど多くないような気がします。

2107
01:26:01,139 --> 01:26:03,239


2108
01:26:03,239 --> 01:26:05,100
アレクサンダーの論文がいくつかあると思います。または、ロブリアも

2109
01:26:05,100 --> 01:26:08,580
同様のことをしていますが、実際には、予測コーディングを

2110
01:26:08,580 --> 01:26:10,920
適用するような純粋な能動推論の外側のようなものです

2111
01:26:10,920 --> 01:26:13,260


2112
01:26:13,260 --> 01:26:15,980
実際的な問題を解決するためのアクションは、

2113
01:26:15,980 --> 01:26:19,280
あまり検討されていません。

2114
01:26:19,679 --> 01:26:23,400
まあ、この素晴らしい

2115
01:26:23,400 --> 01:26:25,199
プレゼンテーションとディスカッションをありがとうございます。他に

2116
01:26:25,199 --> 01:26:27,659
何か言いたいことはありますか。あるいは、

2117
01:26:27,659 --> 01:26:30,300
人々に向けてほしいことはありますか。

2118
01:26:30,300 --> 01:26:33,360
いいえ、招待していただきありがとうございます。

2119
01:26:33,360 --> 01:26:34,620
そして、

2120
01:26:34,620 --> 01:26:36,120
ああ、本当に楽しかったし、いつでもクールなFuture Worksのためにいつか戻ってくることを願っています、

2121
01:26:36,120 --> 01:26:38,460


2122
01:26:38,460 --> 01:26:40,199


2123
01:26:40,199 --> 01:26:41,580


2124
01:26:41,580 --> 01:26:45,000
ありがとうトーマス、

2125
01:26:45,000 --> 01:26:49,820
ありがとうダニエル、またね、バイバイ


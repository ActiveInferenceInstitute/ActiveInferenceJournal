SPEAKER_00:
Right.

Hello and welcome.

This is Active Inference Livestream 63.1.

It's November 7th, 2023.

We're here with Michal Piekarski, and we'll be hearing a presentation followed by a discussion on incorporating variational free energy models into mechanisms, the case of predictive processing under the free energy principle.

So looking forward to this lecture and discussion.

Thank you for joining and off to you.


SPEAKER_01:
Thank you very much, Daniel.

Thank you for the invitation.

This is a great honor for me and very nice.

I'm a little stressed.

I hope my presentation will be interesting for you and give a chance to take some new ideas concerning the Bayesian models, especially predictive processing under the free energy principle.

So my talk is based on my recent papers, which I published after many, many months of revisions in Synthes, and I will sketch the view which I try to develop here, which I think has some interesting implications also for the discussion in the field of Bayesian modeling, like in the wider context of philosophy of neuroscience, of philosophy of

cognitive science.

So I will try to justify the three hypotheses.

One general and two more specific concerning the predictive processing and free energy principle.

So the first one idea which I found in recent paper of some philosophers of mechanistic explanations is the

is the idea that there are phenomena, I think that there are neurocognitive phenomena, which should be explained not only in the terms of decomposing the mechanism which is related to these phenomena, but also should incorporate the constraints from the environmental constraints and the flows

of free energy.

I will defend the view according to which, if you want to explain those mechanisms, those phenomena, we have to take an account of flows of free energy.

And the notion of free energy, of course, in the work of those mechanisms is related to the thermodynamic free energy.

but i will try to show that in the free energy we can understand also in the terms of informational or version of energy this is the first hypothesis the second hypothesis is this is not my idea because we found in many papers of jacob howie or pavel guajeski the idea that predictive processing provides a sketch of mechanism in other words predictive processing is computational framework which has mechanistic

or which can offer a mechanistic explanation of its target phenomena.

And I will try to connect the two in my hypothesis and show that mechanistic predictive processing takes informational constraints from the free energy principle.

In other words, we can integrate the free energy principle as normative

theory, normative principle with process theory, predictive processing using the mechanistic picture, mechanistic idea of explanations.

So let's start from the first hypothesis.

I think that there is a general and common agree in the field, in the works of some authors from cognitive science, from philosophy of neuroscience.

the view which is schematically called New Mechanical Philosophy, according to which the explanation of phenomena is a matter of decomposing these phenomena into underlying mechanisms.

So, when we want to explain phenomena, for example, this is the view defended by Carl Kraver or William Bechtel and the others,

When we want to explain phenomena, not only cognitive, but also biological, social phenomena, we have to decompose the mechanism which is responsible for the realization of this phenomenon.

And this mechanism can be treated as a mechanism which casual product the phenomena or a mechanism which is responsible in the terms of constitutive

responsibility for the realization of the phenomena.

And those authors and those traditions which Craver in his book Explaining the Brain calls the system traditions, claims that explanation is a matter of finding the systems, the mechanism which should be described, should be identified by scientists and should be decomposed into parts

into components, we should recognize the relation between those components, and we should recognize and describe the organization which rules of the relations between the components.

And it means that we can think about the mechanism in terms of hierarchical organization, because if we look on this figure from the book of the Craver, phenomena are based hierarchically on the underlying

hierarchical structure of mechanisms.

There are lower and higher level mechanisms.

And according to our scientific aims, according to our strategy, we can look for the more deeper mechanisms concerning, for example, the level of biological organization to the level of the physiological organization and so on.

And one of the most important authors in the field of mechanistic explanation, the field of new mechanism, William Bechtel, in recent latest paper, he little changed the standard view on decomposition.

He claims that heuristic of decomposition or normative strategy of decomposition in mechanical explanation should be

we should rethink this strategy.

Why?

Because there are high-level cognitive mechanisms which cannot be described solely in the terms of hierarchical organization or in the terms of casual relations, because there are some kind of mechanisms which are, as Bechtel and colleagues claim, which are active control mechanisms.

There are mechanisms which are responsible for production of the phenomena.

What kind of phenomena?

There are active phenomena, active structures like bodily movements responsible for body movements and physiological processes.

And Bechtel claims that those mechanisms, of course, they play a central casual role in the hierarchical organization, but they also

components of complex network of heterohecar web of control systems and they play its casual role they casual role because they perform a given function in this heterohecar web why it is possible because in this sense though those phenomena are the are mutually constrained by the other elements

we can call them constraints, and they are active or they derive their casual efficiency because they are constraint systems.

And in the opinion of Bechtel and the other authors who co-work with him, it is important to rethink

our old image of decomposition because in standard view on the decomposition we don't take an account on two elements, on constraints which make given mechanism active in the sense of being part of a heterological web.

The second point is the energy, because biological mechanisms, cognitive mechanisms, are always related to the flows of the free energy which they transform, which they process.

And why this is important?

Because according to Bechtel, he claims, what is characteristic for biological mechanisms, in the contrast to the other mechanisms, is that the biological mechanisms

are dissipative structures.

It means that they occur and maintained in the context in which free energy is being dissipated.

And if we don't incorporate this element of transforming, of processing the flows of free energy, in the fact that we don't really explain why given mechanisms, why given systems have behaviors.

as Bechtel and Bich claims, completely unconstrained system will have no behaviors.

And I will think that if we want to explain neurocognitive mechanism, we have to adopt the view which I described in my paper as a constraint-based mechanism approach.

And according to view, it's heuristic, mechanistic explanation of at least some

cognitive phenomena should be based or should incorporate dimensions of constraints, element of the environment which makes mechanism active, and the flows of free energy.

And this is the first piece of my idea.

And second is that I think that we can try to demonstrate this idea in one...

example from contemporary cognitive science and I will try to adopt this idea into the frame of predictive processing.

So now I will sketch a short view about predictive processing framework and why this is important for me to try to test this framework into the context of the constraint-based mechanism approach.

So predictive coders develop the Kantian, the Helmholtzian idea that our cognitive systems, our brains, don't have direct access to the causes of information we receive by the input.

And they claim that the central, the main function of the brain on the surgical nervous system

is to minimize uncertainty, which is described in the terms of minimizing long-term minimization of average prediction error.

How it is possible?

How brain can do this?

It is possible because the brain inhabit or embedded the virtual informational structure, which predictive models by agents describe as generative model.

And the generative model is hierarchical.

and multi-level structures which transform process the information coming from the interest why because in this way the brain can minimize the so-called prediction error the discrepancies the differences between the state of the model state of the organism and the information coming from the from the environment because successful minimization of prediction error has many

adaptive benefits so this is general picture now we should look more closer to this image and try to develop a more formal technical language so we can use this the idea which develops a lot like to think about the

relation between the events in the world in the term of relation between cause and effect but what we receive is the observation we receive by the sensory input observations and if the idea developed by predictive coders coming from Helmholtz is that we don't have an access to the straight true states as understood as a causes of the observation so we have to infer the true states

to act successfully in the environment and how it is possible to infer, to guess the true states responsible for the observation we receive.

So there is a development of the Helmholtzian idea of unconscious inference and Bayerians claims that this idea can be translated into the thinking about the statistical inference.

What kind of statistical inference?

As we know, this is the Bayerian

Bayesian inference.

But what does it mean that generative model minimize prediction error or infer the true states using the Bayesian inference?

Talking about Bayesian inference raises a small problem because Bayesian inference, exact Bayesian inference is intractably hard.

You cannot compute exact Bayesian inference.

And the idea which developed

for example, Carl Freestone, is that our models, our brains using models, generative model, don't compute exact Bayesian inference, but it uses the simpler version of Bayesian inference.

They use approximation of Bayesian inference.

So in other words, if the story is correct, then continuous minimization of prediction error

can be described or understood or explained in the terms of approximation of Bayesian inference.

And this approximation is possible because model minimizing prediction error simultaneously minimize a certain quantity, which Friston calls variation of energy, the quantity which is always greater than or equal to suppressor.

And using this quantity,

model iteratively update the internal parameters, update the internal states, and in that way can infer the most probable true states, which is responsible for the observation which model refers.

So, according to predictive co-processing, as I read it correctly, what is characteristic for the brain is that the brain

main function is to processing information in the terms of Bayesian approximation.

And I tried to unpack this idea and try to read this in the context of new mechanical philosophy.

So this is the first step.

The second one is to justify that predictive processing can be connected with mechanical philosophy.

And in many papers of Paweł Gładziecki,

we see good arguments according to which predictive processing as computational architecture provides the scientist so-called sketch of mechanisms.

What is sketch of mechanisms?

This is the idea developed by Piccinini and Kaplan, and they claim that sketch of the mechanism is the incomplete

representation of mechanism.

It means there's some kind of representation of mechanism when the scientist omitted some components, some parts of the mechanism.

And good sketch of the mechanism should lead us to the schema of mechanism, which is the complete representation of constitutive element of given mechanism.

So if predictive processing gives us a chance to write a sketch of the mechanism,

And if predictive processing is about the neural function of the brain, then if the view which I developed after the ideas of Bechtel and colleagues, if this view of the constraint-based mechanism of approach is correct, then I will claim that if predictive processing

if predictive coders want to build a mechanistic explanation of cognitive phenomena, they should include into the explanation flows of free energy.

Because without these flows, without the constraint of information, or in general, free energy, we cannot explain how it is possible that the mechanism operates and is active as long as this energy is available.

And the thesis which I defend in this paper in sentence is that predictive processing models should include informational constraints if they are to be mechanistic.

Of course, if we think about predictive processing as the way to explain the function of the brain.

And of course, there is one natural question, because many authors will claim that the good theoretical normative

for building models in predictive processing offer the free energy principle.

So the question arises, does the free energy principle can offer these constraints for predictive architecture?

And I will claim that yes, the FEP offers these constraints, but of course, we should try to justify this test more specifically.

But we find one big problem.

When we look into the 2018 paper of William Bechtel, when he introduced the idea of necessity of including the free energy flows into explanations, he directly writes that the notion of free energy

which he developed in his papers, in his view, is distinct from the free energy principle or free energy articulated by Carl Friston and the others.

In other words, he said that, okay, we should include in the explanations flows of the free energy, but this is the thermodynamic free energy.

We cannot use the other kind of the energy because this is distinct notions.

In the same year, in 2018, Friston published a draft, a preprint of his book, A Free Energy Principle for Particular Freezing, and he said something that contradicts with the view of the Bechdel.

He said that, heuristically, free energy, thermodynamic free energy is consistent with the variational free energy.

It means that Friston claims

something what contradicts the thesis of the Bechtel.

And I think that it is possible to justify the thesis of Friston, but possibly I try to do this in a different way than Friston does this.

For me, as a person without good formal preparation to reading the paper of Carl Friston, it was hard to find a good way of reading his paper or his cooperators.

But I think that we can try to reconstruct the idea which supporters of the free energy principle

develop and these ideas can be commonly described as something what I call the Bayesian mechanics argument.

The first site I wrote here is coming from the paper of Max Ramstedt and he claims that a decor of Bayesian mechanics is the variational free energy principle.

So that Freestone in his latest paper proposed the idea of the loping of the Bayesian mechanics

the Bayesian mechanics which can investigate, describe and offer the theoretical background for explanation what is free energy principle, what is rational free energy.

And why this idea is important for me?

Because Fristel will claim that every kind of mechanism, each mechanism can be treated

as an expression of some kind of mechanics.

We've got classical mechanics, we've got quantum mechanics, thermodynamics, statistical, also Bayesian mechanics.

And every kind of mechanics has its own reified constructs.

Those constructs can be understood as thermodynamic energy, temperature, or variational free energy.

And what is important is that those constructs, those reified constructs, those notions are justified and can be used only in the realm of the given type of mechanics.

It means that it is possible that we've got some reified constructs which we can use on the base in the realm of the one kind of the mechanics, but we cannot use this in the other type of mechanics.

For example, when we talk about the notion of the

temperature it can be and it will be used in different way in the realm of quantum and within the realm of statistical mechanics and it means that if we take the construct like variational free energy on thermodynamic free energy from the point of the view of this argument it means that all of these constructs are relativized or related to the description and method of measurement

which we use in the specific type of mechanics.

What does it mean?

It means that there is no ontological... We don't have a good ontological argument to claim something like that, that, for example, statistical mechanics is more primary than, for example, classical or Bayesian mechanics.

Why?

Because every kind of mechanism, each type of mechanics

can be treated as a complementary description of the behavior of a dynamic system.

It means that the most primary, the most basic structure, we try to explain, describe in terms of mechanics, dynamical systems.

According to Freestone, it means that Bayesian mechanics can be treated as all others

kind of mechanics which we find in physics.

But what is specific for this mechanic, as I said, is the assumption of variational free energy, which is strictly related to the assumption of Markov blankets.

Tristan will claim that this additional constraints, the constraints of Markov blankets, makes possible to speak of states of something as relative as something different

of something else.

And why this is important to make a possibility to talk about, for example, internal and external states.

Because this possibility of talking about different states is directly applicable and directly important to the living organism and, for example, neural structures.

Friston, in his book, claimed that

every type of mechanics, you can ignore assumption of a marker blanket and the assumption of rational free energy and in some implicit sense will claim that the state outside blanket and the state outside blanket, this difference between those kind of states can be ignored.

Why?

Because if this way of thinking is correct,

uh every kind of mechanics give us a chance give us a possibility to talk about the systems we try to describe in terms of heat bed or thermal reservoir it means that every kind of mechanics except quantum mechanics give us a possibility to talk about the states as in equilibrium steady state so

And according to this argument, we should say that variational free energy can be applied and justified only in the realm of Bayesian mechanics.

And this application helps us to describe, explain why some things are autonomous or active, why some things can be as Bechtel says, dissipative structures.

But it means that variational free energy gives us a chance to describe

some kind of phenomena, some kind of system as autonomous or active, and simultaneously thermodynamical free energy can be implied in the realm of the object which we want to explain in the terms of statistics, as a statistical ensemble.

And this is why Friston will claim that both

variational free energy and thermodynamical free energy are consistent because they are two consequences or expression we can say the same more elemental thing the thing which can more elemental more basic mechanistic or maybe quantum nature and it means that thermodynamical free energy and variational free energy are the notion or constructs which we can trade as the as a two side of the one coin and

And using them is dependent of our method of measurements, of our scientific interest, from our strategy to investigate phenomena.

But they are complementary in the sense that they offer the scientists

the possibility of described dynamical systems from the two other sides, but those sides are complementary.

And how we should understand and interpret the Bayesian mechanics argument?

Because I think this argument should be interpreted in a more philosophical, more specific way.

that thermodynamical free energy and variational free energy are two sides or are related to us, two aspects of more primal dynamics, there is still open the questions about what we talk when we talk about systems which minimize variational free energy and when we talk about the flows of the thermodynamical free energy.

And I think that this ultra-realistic or super-realistic interpretation, literal interpretation of this argument is not correct, because it seems that we should interpret construct like thermodynamical and variational free energy as useful

useful function which uses, which justification are related to our scientific models and so on.

In other ways, there will be the problem to defend the thesis of wisdoms that the Bayesian and stochastic mechanics are equivalent formulation of the same thing.

I think that the same thing

about which Friston talked, is the dynamical systems.

And we try to find good tools to describe the systems, but it means that we don't have good tools which represent or map the real structure of the systems.

And it means that the notion we use, like rational free energy and thermodynamic free energy, should be understood as the constructs which are related

and ultimately reduced to the method of measurement, descriptions, the method we use.

So it means that when we talk about the consistency or complementarity of variational free energy and thermodynamical free energy, we don't make any ontological commitments, any ontological assumption about the representational architecture

of the target systems.

It means that the models which are based on the free energy principle address the casual structure of the world in the sense that they are epistemically useful.

We can use them in the practice of model building, but they are not epistemically useful in the sense that we describe the real properties which we can literally describe as generative models, as variational free energy,

and so on and so on.

And I think when we talk about the free energy principle, the instrumental and realistic interpretation is correct.

The free energy principle doesn't imply any ontological solution about the target phenomena.

But from the point of view of my argument, there is one problem because this instrumental interpretation of Freestonean argument

doesn't have any specific mechanistic implication.

We don't anything talk about mechanistic character of the phenomena.

Why this is important?

Because when we come back to the field of new mechanical philosophy, we will see that I think there is, in very general, there is a common view that we should

think about mechanism in a realistic manner.

What does it mean?

The view which I try to describe here is so-called mechanistic realism, according to which we should think about the structures, the entities in the world as the structure which are in some sense richer

than mere aggregation of causes why this is important because um mechanism which we try to identify the compose should be should be produced by some kind of structures and this structure should be more richer than a simple aggregation of the causes and this is the first part first part of this argument and the second connect connected to the idea i try to develop is that that

There are some of those structures whose, at first, organization cannot be reduced to an aggregation of causes, and second, this organization should be explained in terms of mechanisms which are constrained by flows of information, flows of thermodynamic free energy, and described maybe in terms of minimizing rational free energy.

In other words, I will reclaim that if we adopt the mechanistic realism, we should try to say that there are such phenomena, such structures, the explanation of which should take into account the energetic constraint, which we can try to describe in terms of variational free energy.

And if we try to adopt the image of

coming from the mechanistic realism will defend the view which I describe as moderate realism about Bayesian modeling or moderate realism about processing and the free energy principle.

In this moderate realistic interpretations, we can say that system minimizes prediction error or minimizes variational free energy because it implements in some way some casual mechanism

than can be described approximately in terms of minimizing variational free energy or simultaneously maximizing mutual information between internal states and sensory states.

And it means that we can identify, describe and decompose some kind of mechanism which we should trade as a system of mutual constraints

that restrict the flow of information, the flows also, the thermodynamic free energy to perform work in such a way that those mechanism minimize or should minimize discrepancy between internal state, between the parameter of the model, between the prediction of the model and the information coming from the environment in which given systems act and live.

Why?

Because without taking those constraints, flows of information, we cannot describe and explain why this kind of organisms are at non-equilibrium steady states.

So, and I think that we can find in the field of thermodynamics of information and also in the work of Krafft-Risdom argument, which I call the argument from neurocomputation, a good argument for the moderate realistic interpretation of CP and free and net principle.

And according to this argument, we can say that there is always trade-off between neural information processing and thermodynamics.

thermodynamic energy cost.

It means that every time when biological sensor detect a change in the environment, every time when biological organ process the informations, this processing confirmation or detection a change in environment is proportional to the amount of information, is proportional to some

minimum energy in terms of thermodynamics.

So every time when a system processes information, every time there is a minimum cost of thermodynamic energy.

And this argument from neural computation could be related to the formal investigations coming from the Fristow monograph about the Yerzynski equality.

piece of this book about the Yezinski equality is really hard.

But one and a half years ago I was talking with Carl Friston about this argument about the Yezinski equality and I think we can try to reformulate it in a more non-formal way.

according to the yashinski equality if we interpretate this in the realm of the free energy principle this equality allows us to connect variational free energy with thermodynamical free energy in this sense that when we want to describe a moving one system from one state to another and this moving in the system we can understood as creating or destroying the information there is a certain amount of thermodynamic

work cost and thermodynamic action, which should be entailed by this moving.

In other words, when we look into the generative model in our brains, we will say that every time when the model update belief, in the sense that every time when model change information in the internal state of the model, every time there is a thermodynamical

work costs so the belief updating in the bayesian generative model is strictly related to the thermodynamic cost of this belief updating and i think that moderate realistic interpretation should explain us how it's possible because because when we talk about the belief updating generative model when we talk about the minimization of error literally we we

literally we don't map the formal structures we use onto the target phenomena.

I don't claim that there are real generative model in our brains.

I don't think that our brains optimize base and so on, but I will claim that there are some structures in our head, there are some structures in the world that are richer than simple aggregation of codes, and those structures implement

some casual mechanism that scientists can approximately describe in terms of models which minimize operational energy or prediction error.

And I agree with the thesis of Kirchhoff and colleagues that our models fit the data without literal mapping.

Yes, our models are true,

because they are approximation of the data.

And I think that we can think about the predictive processing model integrated with the free energy principle as some kind of approximation of the data which

we want to investigate a specific type of the data, like our neural organs and so on.

And I think that we should distinguish and remember about three distinct elements.

The free energy principle as formal principle, the formal principle which doesn't assume any ontological assumption and doesn't imply any ontological commitments about the target phenomena.

the predictive processing framework as mechanistic computational models, which are grounded from the one side in this formal principle, when the formal principle offers the variational methods, variational notions, and from the other side on the heuristic of mechanistic explanations, and the biological systems,

the target systems that predictive processing and Bayesians employed to model, which are based, which are independent of the implication or on the language of the free energy principle and also which are independent from the language and ontological implication of process theory like predictive processing.

And going to conclusion.

So, I would claim that the reference to the informational constraints allows the scientists to explain the neural organs, the brains, not only as a kind of physical material, physical mechanism, but also as systems which have future homoesthetic nature.

And I will claim that it means that a full, a satisfactory explanation of how the brain works, not only how the brain can be treated in terms of statistical or classical mechanics, but also with the terms of thinking about the brain as active as the part of living body.

A full explanation of

brain as part of the living body requires taking into account informational constraints, which I believe can be characterized and described in terms of Bayesian operational free energy minimization.

And the last one conclusion is the observation that from the point of view of the argument I tried to develop is that the free energy principle is also normative

in the sense that it sets a norm that should be met by mechanistically non-trivial process theory or predictive processing models, assuming the correctness of the constraint-based mechanism approach.

Thank you very much.

That's all what I have.


SPEAKER_00:
Thank you.

great presentation I'll make a first comment meanwhile anyone who's watching live is welcome to ask a question okay

well i would like to applaud this great talk you connected the scholarship and the framings that are used in the philosophy of mechanistic accounts with free energy principle active inference predictive processing and i think you articulated several

lines of arguments that were in the literature including where there was change or even partial um incoherence or just partial ambiguity certainly ambiguity related to um justifications or understandings of the relationship between

thermodynamic free energy and mechanics and Bayesian free energy and mechanics and that is a very wide space with everyone saying things like it's a purely formal resemblance and it's just a convenient computational heuristic used in science and physics and engineering and there's just no need to worry about more than that

ranging to arguments that that they are co-extant or that they have different kinds of statuses so i very much appreciate in your paper and in this presentation that according to the way that different discussions about mechanism have been carried out in the biological sciences

using that setup to clarify some of these discussions in this field that are all great to approach with kind of a first principles view and it's helpful to see how it connects with the arguments of like craver and bechdel hey thanks um


SPEAKER_01:
I agree that I try to develop some arguments which exist in the current literature, but, for example, when I read the paper of Bechtel, I think that this idea doesn't be directly or explicitly articulated.

And...

And I think that Bechtel doesn't agree with my argument, my interpretation of his view, because of the variational free energy.

But I still believe that this is very basic intuition.

But this intuition said that if we think about the part of the

human organism.

If we think about the brains, what is more specific for the brains is that the brain processes information.

Not brain processes the thermodynamic energy, not brain processes or produces some kind of hormones, but the brain processes the information and this information is very important for a living organism.

And I think that this intuition should be tasted

in the field of science or maybe in the field of philosophy of neuroscience.

And I'm not so sure that this is possible to test it, but I think that this is important to description or explanation of those mechanisms, that those mechanisms are the mechanisms of processing information.

And possibly,

Possibly, maybe it's enough to say that, okay, those mechanisms of information which the brain processes, it's not important to explanation of how it is possible.

I think that predictive coders, the supporters of the free energy principle, said something important, and they claimed that what is specific for the brain is not only that the process information, but also is the process in some specific way using

something what we can describe in terms of Bayesian proclamation or more wider as a statistical inference.

And I think if predictive coders, predictive processing is a fruitful strategy of explanation in the field of neuroscience, if the model builds by predictive coders are scientifically useful,

then we should take and we should use this heuristic coming from the free energy principle that take it seriously because i don't want to i try to say that uh talking about the brightness which use it which uses which use the generated model which approximates by asian inference this is not the way of talking way of speaking

I think that we not only describe, but the word we use, the notion we use, as I said, in some way corresponds to some kind of structures, casual structures which we have in our head.

And maybe we investigate, we invite more specific, better words, better concepts, better tools,

to describe those structures, but I believe that for now, predictive processing under the free energy principle gives us a good chance to explain why our brains are not only the matter, but also the important part of the living bodies, and what is the difference between the brain as an active control mechanism

and why this brain shouldn't be treated as a heat bath or a thermal reservoir and so on.

And I think this is the reason why Bechtel tried to reformulate the classical image of decomposition because he observes that some mechanism cannot be described or treated as

non-living, non-active, and if they should be treated as active and living, then I think we have to try to incorporate into our understanding, into our explanation, the fact that the information which the brains, those mechanisms process is really

really important to understand why and how they act, how they are the part of dissipative structures like our organ.


SPEAKER_00:
Thank you.

I'll ask a question next from the live chat.

John Athos wrote, what are you defining as information when you say the brain is processing information?


SPEAKER_01:
Good questions.

I think when I talk about information, I think about some kind of semantic information.

Of course, if we try to use in practice this idea which I tried to develop, the question about information should be more specified.

I find the argument which developed Rosa Kao, as I remember, when she claims that when we talk about the information which is processed by single neuron, of course, this is some kind of synthetic information.

There is no any semantic.

But when we want to try to understand how it is possible that the brain as full organ

the cortex process information, this is always strictly related to some kind of semantic information.

And I think we can think about this semantic information, try to, but this is, I don't know for now how it is possible in the realm of my argument, but we think to try about this, what we

traditionally described as semantic information in terms of variational information, because I think that if brains minimize prediction error, if brains approximate Bayesian inference, it means that brain operates, uses the information in semantic sense.

In my paper, I wrote about the mutual information between the internal and sensory states.

But I think, I see this as lack of my view, because I don't precise what kind of information precisely I mean.

But I think that if we take this view as some kind of proposition of heuristic of building the models, the empirical investigations lead us to the possibility of describing this information in more specific terms.


SPEAKER_00:
awesome very interesting so a few lines that i'd love to ask uh questions about so in your paper and presentation you center the variational free energy as kind of the analytical construct doing the work in the mechanistic account and i wondered what leads to one

analytical constructs doing that kind of work for example why not center surprise and see variational free energy as a computational heuristic or just an approximator so what would be the relevance in a mechanistic account that centers the concept of surprise or the concept of prediction error

versus the concept of variational free energy, which are related concepts in that an organism minimizing one does minimize across all, but they are different in terms of their formal basis and what we could say about them.


SPEAKER_01:
Okay, I think that... I think that if...

The answer to your question can have two parts.

The first one is related to the view which I defend as moderate realism.

And I will claim that the notion we use, the concept we use, are related, are corresponds to the structures which we try to develop and describe using these notions, but it's not the sense that we literally map

the content of these notions onto those target phenomena.

So probably variational free energy for now looks a good fit to those phenomena from my point of view, but it doesn't mean that we can't use the other notions.

But you ask why we don't talk about surprise or predictive errors.

I think that we don't explain

in a satisfactory way, this idea when we talk about the surprise, maybe our prediction error, because we don't understand why the brain approximates Bayesian inference.

If we don't introduce the notion of the variational free energy, or maybe it's equivalent to the upper bound of surprise, but if we don't introduce this notion,

then we don't find the computational formulation or computational foundation to description of the brain working in terms of minimization of prediction error.

Because I've got some problems with it, but maybe try to explain how I understand this.

If we take the few...

your level of description of David Maher, I think that talking about the minimization of prediction error or minimization of uncertainty are related to the computational level.

And I think that talking about minimization of rational affinity or approximation Bayesian inference are related to the algorithmic level.

And the idea which I try to develop is to using the connection between those levels to

investigates what's happened on the mechanistic level, the third level of Mar.

And I think that the notion of prediction error or supra so when we talk about this, we omit something important what describe and explain freestone his his paper concerning the version of energy.


SPEAKER_00:
Yeah, very interesting.

Also, there's

different ways to view this but in the vfe centrism there's also the possibility of relationality as a core principle because if the mechanism surrounded surprisal of the system that would be closer to an account where there was like a surprise module or something that was a about

what the systems what representations were internal for the system but with vfe we have something that's explicitly implementational relative to a constructed model so then it it naturally um connects to discussions about how modeling is done as a practice

because it is a constructed variable, whereas like temperature, ostensibly, it is important for a chemical reaction.

Or temperature is a coarse graining.

I'm sure there's a lot of ways to say it more formally, but the kinetics do matter for the chemical reaction.

And then we have tools that are able to

measured temperature in different settings cognitive modeling seems to have a relational step that's a lot more multifaceted because VFE is only calculated relative to the modeler's construct so

one person might make a model that shows the vfe doing this and a different model would show something different and that seems to be a bit different than like we have two thermometers but we both agree that temperature really does matter


SPEAKER_01:
Yes, I agree, but I think the information, the dimension of information is more subtle than the dimension of temperature of mass.

And of course, VFE is related to the modelers and model which they develop, but... I tried to say something more.

Of course, VFE is related to the modelers, but there is something in the...

target phenomena which motivate modelers to use the VFE.

And of course, if I look on the piece of the paper, the paper doesn't look like, for example, processed information and doesn't motivate me to think about this paper as something what I can model and using the variational free energy or variational principle.

But when I talk about a living organism or the human brain, something like that, there is something in the structure, in the phenomena, which motivates me to build a model using this principle.

And of course, the truth of the method of measurement, the truth of the method of model building is arbitrary in the sense that the target phenomena

doesn't directly give us an information which method is better.

But it's not arbitrary in the sense that I think that we can, in the same fruitful way, use the statistical mechanics, for example, to explain the brain and, for example, Bayesian modeling to explain the brain.

It's not arbitrary in the sense that I can use the piece of paper to eat something.

I can take a glass of the water if I want to drink, but I cannot take a piece of paper if I wanted to drink.

But in this sense, when we distinct the formal principle, the normative principle, we distinct the model which are built on the normative principle, and the target phenomena which we want to explain using this model based on the formal principle, then it means that

the relation between the target phenomena and the relation between the model is not arbitrary in the sense that we can use every kind of the model to every kind of phenomena.

And I think that when we try to look into our brains, this kind of the structures in some way motivate us to build the computational models.

And those computational models, I think, is in some sense different than the computational model, for example,

behavior of, I don't know, pigeons or moving of my hand.

And so, generally I would say that truth of... And coming back to your questions, as I said, the dimension of information, the informational constraints are more

more subtle than the constraint like mass or temperature because probably we don't have as a same good method of measurement of the information like we have the good method of measurement of temperature of the mask and probably maybe the free energy principle or variational principles are not the the best

possible way of describing and explaining the brains, but probably I think this is the most optimal for this moment.

When we take the whole knowledge, our knowledge about the brains, it's the most optimal models and methods to try to explain this phenomenon.

But as I said, using of the variational principles, using of the Bayesian modeling doesn't lead us to say something very specific, ontologically independent from these models about the nature of the phenomena we described.


SPEAKER_00:
thank you yeah that makes me think about a sufficient account a mechanistic account if you have a system that entirely is described by hot and cold flows through a pipe you may have a sufficient and even a total account with only thermodynamics now maybe the hot and the cold exactly signify like a signal

So then there would need to be some kind of information processing model or layering to understand.

And then we move into a space where looking at just the information measures on the syntax, like the amount of data transmitted in bytes or something over a digital connection

you've already highlighted the relevance of the semantic information flows which is um a topic that Chris Fields and others have started to look into and then there's a further uncoupling between the material basis of the information transfer and the semantic information flow

I'll ask another question from the live chat.

So Upcycle Club wrote, are there any limitations or challenges associated with incorporating variational free energy models into mechanisms?


SPEAKER_01:
Yes, very good questions.

I think we can look for the limitation related to the

questions which scientist gives.

It means that we can looking for a limitation concerning the methods and of course the more important limitation concerning the border of the systems.

And it is hard to give

one good answer, because in the field of the new mechanical philosophy there is a big discussion about the border of the mechanisms.

And if we take the constraint-based mechanism approach, the situation looks more complicated, because system traditions of decomposing mechanisms will claim that the border of the mechanism

are related to the components of the mechanism.

When I say that the components of the mechanism, it means that something what we decompose is not enough because we have to also look into the elements which are not literally the part of the mechanism but which makes the mechanism active.

It means that the border of the mechanism is...

And I think the main limitation concerning my idea is to say when we have to stop to explain.

For example, when we take an example of the pipe and the water moving in the pipe, this is the part of the mechanism, but the material from which the pipe is built

the environmental constraints, which is not the part of the mechanism, but which is important, which is relevant for the functioning of mechanism, because the plastic pipe makes, for example, better working of the mechanism than the wooden pipe.

And I think the main limitation is related to the questions of

which constraints for example when we talk about the constraints as environmental cons environmental factors which of the constraints are how many constraints we need to to say that okay this is enough we we understand how this mechanism is corrective um

And I don't know for now where this border is, because opening the mechanism on the constraints, opening the mechanism on the information coming or processing it by the, for example, narrow mechanism, make these questions about the border mechanics more specific or more complicated.

But I think this is similar discussion or similar problem like we find in the field of the free energy principle,

when we talk about the border of the mark of blankets or when we talk about the mark of the cognitive or mark of the living organism.

I think that if we integrate mechanically the free energy principle with, for example, mechanical predictive processing, then this discussion about the border of mechanism


SPEAKER_00:
can be a part of the discussion of the border of marco blanca's of the border of informational systems something like that thank you that's uh that's a huge point it made me think about having a physical object could be digital or analog

that's going to implement some procedure or program that is going to be bound apparently by the halting problem like the inability to know for some kinds of systems how the program is going to turn out by the incompleteness type pattern so there's certain informational constraints that kind of come out of nowhere seemingly to constrain the action

or to provide some thing that limits the action or the knowability of the action so then you highlighted that it is actually this stopping time question or the halting question as a question of scientific strategy in building accounts

knowing how much to include.

And that's very related to identifying or selecting a system of interest, because you could say, well, the pipe comes from here, and so we're going to go back up the supply chain.

And pretty soon, if you include the flow of air or the movement of digital information or something like that in the system, that can unfold to include untenable models.

so then there's a core screening and all already um people make informational core screenings like with causal influence diagrams like just the the the relevance of one variable on another

so it's just very interesting that by having the material thermo mechanism and a legitimated role or status for informational mechanisms which are the natural kind of explanation for perhaps algorithms or information processing that

still there's the the boundedness of what is being accounted for but a clearer way to know how far out informationally just like you could go out physically in the thermal account with layers of rooms outside of the heat bath also you could go out


SPEAKER_01:
informationally more into the past for example or you can just say we're just drawing a line around this so maybe yeah sorry sorry no i think that i think that the the the for maybe the best way is to um question about on the of the borders of the mechanism or borders of the system relativized to the our scientific interesting

For example, when we talk about the pipe, when we talk about the digital system, they are some kind of artefacts and they are related to the architecture who made them.

When we talk about the brain, it is hard to think about the brain in the same or analogous sense when we talk about the pipe or the glass or the paper, because there are artefacts, we can say, but natural artefacts.

And I think that

our thinking about the borders of the system we want to describe, maybe primary should be related to our questions concerning the borders.

And maybe the story was that, that mechanists in the system tradition, the traditional viable mechanism will say something like that.

we think about the brain as a mechanism like the other one.

We can develop the structure of the social web, the structure of the brain, the structure of the bones, and so on and so on.

And this way of thinking about the brain as the kind of the mechanics like the other one,

in some sense direct the investigation to brains.

And this is why, for example, decomposition as the whole strategy in new mechanical philosophy, many authors think about this as relevant and we don't need anything else.

But when Bechtel, for example, claims that, okay, but there is a mistake because our brains

are based on the web of active mechanisms.

And those active mechanisms are more specific because what makes them active is not only what makes them a mechanism in traditional, normal sense.

And when we look into the paper of Bechtel, he doesn't develop the idea of talking about the whole brain.

They developed the investigation into the small pieces of how some kinds of cells transform molecules and so on and so on.

But they still claim that if we show the bound in two

too fast, then we don't explain correctly how the given mechanism, the given cells do actively what it does.

And I don't know, probably if I want to defend the idea of moderate realism about Bayesian modeling, I would like to say something like that,

There are some kind of borders in the world.

We try to look for those borders.

Maybe the systems and the difference between different systems motivate us to try to describe these borders, but there is no possibility to say that de facto here there are borders.

But always when we talk about the borders,

talking about the borders is related to our measurement tools and the methods.

So I think that the small move in mechanical philosophy, which I describe as the constraint-based mechanics approach, we can interpret it as an example of the situation when the phenomena motivate us to change our scientific practice.

Because Bechtel, 20 or 30 years, developed the new mechanical philosophy, but in the last five or six years, he claims, okay, but this old view was not perfect.

We have to change something.

And I think this is the argument for the moderate interpretation of the scientific practice.

Phenomena, in some sense, because we developed a good method to investigate phenomena,

phenomena in some sense motivate us to change, to develop the new methods, change our scientific heuristic and so on.


SPEAKER_00:
Wow, that's awesome.

One other thing that made me think about was the differences

the relevance or the difference between the thermodynamic and the informational accounts become very clear when studying decentralized systems that are spatially disaggregated because when there's something that's physically enclosed

and it has like a unitary physical component and a unitary informational component like a desktop computer or a single tissue then there is an undeniable overlap between those two systems which kind of host and relate to each other but then when there's a system that

has other things amidst it then just thinking here about nest mates and the ant colony well then spatial enclosure and just inclusion of all the material components seems to clearly capture too much and so

that seems to suggest a more informational role which takes on many rich areas from semiotics and all these other semantic questions not just again a reduction of the informational to the syntactic informational

like how much um energy or material was transferred because again that will just conflate that'll make me it's it's um it's a approach that won't lead to the best research agenda on mechanistic accounts interesting


SPEAKER_01:
So I think that the question about semantic information is really important here.

But if we take Bayesian ideas about the brain which inferred information coming from the environment, this information is something more than the syntactic relation.

But...

I think what is important here is what Tristan's really important claims, that I think that information changes everything, in that sense that when you try to literally think about Bayesian approximation and so on, then it means that

everything we want to describe and explain as scientists is strictly related to the information we receive.

Of course, this is very simple, but every object we observe, every object we want to describe is always related to the information we have about this object, and it means that there is no

something like we've got a naked fact, naked state of affairs.

Maybe ontologically there are, but from the point of view of the finite entities like us, every time when we receive some information, every time when we observe something and we try to develop a model of some kind of phenomena, it's always related to the

some kind of information what we have about this phenomena.

And I try to say something very metaphysical.

I think that the variational point of the view

as we can say, something like that.

The Bayesian point of view shows us that there is no distinct border between the object we try to map, between the language we try to use to map this object, because I think there is something that Maxwell Ramstad has right when they claim that

which still we try to describe the objects who behave like the... We try to describe the territory which behaves like a map, which describes this territory.

And of course, this is not only the metaphor.

I think that everything what scientists try to do is to reconfigure, is to identify

the structures which are extended now to the structure of thinking, structure of information which we use.

I don't know.

I think that Bayesian view about human brains, about the world,

lead us to the one very important question about the informational nature of the world.

I don't know, because I speak right now, but when we think, when we take seriously

this kind of thinking about the science about about the entities the most basic the most fundamental future of the world is not a mass it's not a temperature it's not an energy but an information and i think the information is something could change a difference and literally it means that if we want to develop a science we cannot ignore

this very basic, fundamental features of the world.

Why?

Because the information, of course, I don't want to say it's something like panpsychism, or something like that, but I will say that information is always for someone.

And I would like to say that

When I was reading the papers of Ristam and so on, I still think about why we should or why we shouldn't ignore the information in our investigation.

And I think that Bayesian modelling gives us a good tool, maybe it's a plea for the place of information in the science.

because but not only literally talking about information okay we know it's it's obvious that brains process information but information as an entity which making possible many many of the phenomena and i think that our brains in short we say our brains are the entities which are

which are possible because there are information which makes this kind of mechanism, but not the other.

Sorry, I think this is really speculative, metaphysical, but it's hard to catch this idea.


SPEAKER_00:
yeah there's there's a lot there um certainly the unity of the scientific and the broader informational endeavor is very interesting and so thinking about that explicitly in terms of epistemic foraging seeking out which measurements or observations to accrue

and that being kind of on a continuum of agency of experimental selection and there's other like a fixed camera does not have control over the sensory observations it's making it could have internal cognitive control but but

having a unified way to talk about what it is that science or just different ways of knowing even outside of what we might call professional science today that those different manifestations are are part of a ecosystem of shared intelligence part of a shared fabric with other information processing as

chemical elements are in circulation in a chemical ecosystem.

What is the status of the semantic informational connections in semantic ecosystems?

And people could have viewpoints like, well, there are no semantic ecosystems, there's just syntactic ones.

And there's no syntactic ones, there's just material ones.

I mean, people can always go that reductionist path.

But to the point where any given person says, okay, I've kind of bottomed out on reductionism here, or I'm looking for another perspective, there is going to be the question of what the semantic account is.

and what its status is and does it articulate with the thermodynamic account do they have a a zone of indistinguishability do they have um a broadly overlapping region maybe it could be one way in one system in a different way in a different system

But these will constitute some of the fundamental philosophy questions that you identified in terms of real mechanistic accounts for intelligent systems and using active inference to develop any kind of account.


SPEAKER_01:
I think we cannot ignore the semantic.

Maybe one of the most important things that Friston says is that semantic is important in that sense that it is not only the medium which we use.

For example, if I correctly understand the origins of the Fristonian free energy principle, it comes from the method of measurement of the brain activity.

And someone can say, okay, this is only the method and there is no any implication.

Of course, but I think that Freestone will say something more.

It says that there are phenomena in which semantics play an important role, but we cannot ignore

the semantic dimension in our practice of science building or model building.

It means that semantic is not something like the piece of the paper.

I can use the piece of the paper when I wrote a paper, or I can use a word or office and something like that, and it's not important what material I use.

I think that many authors or maybe many papers think about the semantic as some kind of the medium like the kind of the paper I use.

I think that the free energy relative inference framework shows us that the information or semantic is something what makes possible, for example, to do a science or maybe

The information semantics is something what makes possible for us a human being.

And maybe like a thermodynamic free energy.

If we don't transform the molecules from the environment, we don't live.

But the question is why science talk many things about the thermodynamic

processes and talk enough not enough about the informational processes i think that the bayesian smothering freestone approach lead us to the conclusion that there are systems for which the transfer of information is a same important like to transform process the thermodynamic energy in

The argument which I tried to develop from the neural computation shows us that always there is a trade-off, there is a relation between information processing and thermodynamic work cost.

If there is some kind of trade-off, why we don't take and use it?

But probably, as I said, information is something what

is hard to be measured, like temperature, like mass, and so on and so on.

And I believe that version principles give us a chance to try to develop this.

Of course, remembering about this, that those principles lead us to the tools which are only the more or minor the approximation of the real


SPEAKER_00:
real phenomena um okay well wow do you have any closing thoughts or where is your research in this area going to go uh i think that uh


SPEAKER_01:
I wrote a grant proposal about this idea, and a few days ago I received a reject in the Polish center.

And the only reason why they rejected my proposal is that I don't propose using or applying this idea into the real modeling framework.

So the next steps, and I think I agree, because this is very speculative.

I propose some kind of heuristics, and I believe that this heuristic can be applied in a real model building strategy.

And the next step now I'm thinking is that to mid-person or to make a competence concerning that and finding the way to how to taste

this idea in practice.

And still, there are two options.

The first one is that, okay, this is the speculative idea and we don't need to think about the mechanism in terms of information constraints and so on and so on.

And the second is that, okay, this is a correct idea, but

But maybe variational principle, Bayesian modeling is useless here because everything what we need is coming from the thermodynamic of information or the classical theory of information and so on.

So I think there are many open doors.

I hope I will take away to find some of them.


SPEAKER_00:
Thank you.

Well, it was a great presentation and discussion.

You're welcome back anytime to share what that next step looks like.

I really appreciate this.

I think it certainly is a great benefit to the literature to connect the kinds of things that people want to say and do to now how we are saying and doing it.


SPEAKER_01:
Thank you, Daniel.

Thank you very much.

Thanks for questions.

That was a very nice experience.

Thank you.


SPEAKER_00:
Excellent.

All right.

Till next time.

Good night.

Bye.

Bye.
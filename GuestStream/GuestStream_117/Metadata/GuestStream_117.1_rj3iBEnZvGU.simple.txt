SPEAKER_02:
Alright, hello, welcome.

It's August 19th, 2025.

We're in Actinth stream, guest stream one, 17.1, with Robert Stamps, who will be presenting on control strategies and predictive coding using magnetic noise.

Thank you for joining, Robert, and looking forward to this presentation and then discussion with...

Close the YouTube on your side, Robert.

Should I just begin now?


SPEAKER_01:
Well, thank you for this invitation.

Maybe I should give you a little bit of a background of how I ended up doing this research, getting interested into this problem, because I'm primarily a condensed matter physicist, computational in theory primarily.

and my area of specialization has been in magnetic materials and spin systems in general but I've been doing this for quite a while and about I would guess 25 years ago I was more or less starting my career I was in Australia at the time and

I also came across a paper, it was about the theory of flocking, and I found that very interesting.

And that led to one thing and led to another thing, and eventually I found myself co-supervising a PhD student who was working with an expert in psychology on psychophysics.

And they were looking, doing experiments and analyzing experiments on a primate visual system.

And what I found striking was some of the mathematics we were doing.

We were heavily influenced by, I think, about that time, in fact, just recently a Rowan Ballard paper had come out, and it was heavily influenced by the structural form of the mathematics we were doing.

And it mapped directly back onto some of the things I was thinking about doing with magnetic systems.

At that time, people were just starting to develop

a fabrication technique which would allow them to make new kinds of magnetic materials and that's what I'm going to discuss today and it occurred to me at that time I remember very clearly that when that technology matured it seemed to me like it would be an ideal one to try and implement some sort of neuromorphic

designs in.

And indeed, that is exactly what has panned out.

And so I'll tell you a little bit about that and what the state of the field is now.

But that explains more or less how I began my interest in this kind of cross-disciplinary

field.

And just to put the story back into context, then about three years ago, I had a sabbatical and I decided to go back to Australia and visit that group and see what had happened.

And that's when I came across Friston's papers in Active Inference.

And I actually didn't make head or tail of them.

And so I thought, well, I better put it into some context I can understand.

And

to try and understand exactly what predictive coding and active inference are about.

And that helped me, and that's where this work emerged from, trying to understand that.

So let me begin then.

So I'll just explain these pictures.

This is just a schematic of using nanomagnets with an active inference to control some process and then you feed back.

And this is a schematic of the sort of nanoparticle arrays which are in this control box.

This is my

idealized spin system and what this is at the very end I could show you it's a model which is a bit more sophisticated but it actually tries to model properly the properties of real magnetic materials and the point of it was to demonstrate that one could implement certain algorithms inside a magnetic context like this so to begin let me introduce myself the people I work with here

focus primarily on studying both theoretically and experimentally nanomagnets.

The experimental parts are led by my colleague Professor von Lierop, and these are students who involved one way or another in some of the works.

Some of it overlaps with what I'm going to show you today.

And these are just illustrations of some of the different systems which other people and which we also have looked at over the years.

Each of these represent a little magnet.

But...

Before I explain all of that, let me give you a little bit of an outline.

This is just an inspiration.

It's a wonderful, beautiful picture of a fly larva.

And you can see all the neurons here are close packed and they expand out as the fly, as it grows into a fly.

But the efficiency and the minimal amount of connections and yet what the fly can do is really quite amazing.

and that's a little bit of what trying to discover in all of this so for the first thing first thing i'll start with is an introduction to what i mean by magnetism and nanomagnets and why i think this might be a possible platform at least for some key aspects of what i think is called predictive coding

and then i'll give some examples active inference and tracking and some other examples as well and then finally when i talked about these real systems and more sophisticated models that demonstrate material specific properties then i have some micromagnetic examples that's what they call it so let me start with a quick overview of magnetism and nanomagnets

and some key definitions.

So I apologize if you've seen all of this before.

But it just gives us a way of defining terms in a common way.

so what is a nanomagnet well just means it's a little piece of magnetic material this is a real picture the particles themselves are smaller than a micrometer there may be 100 200 nanometers in length and half that size and width

And an example material would be an iron nickel compound.

And the shape is what matters here.

The shape is such that they tend to align as all the magnetization pointing in one direction or the other direction, but always along this axis that minimizes the energy of the magnetic atoms.

So at lower temperatures, then they form this magnetization and it tends to align along an axis.

So it can be fairly well controlled just by the shape.

And the lithography has been developing steadily over the last 20 years.

So one can even make smaller, well-defined particles.

Now the nice thing about it, by being able to make many of these particles and place them in close proximity to one another, is that these little magnets interact with one another just by stray magnetic fields, the same sort of magnetic fields that holds a magnet to a refrigerator door.

But what it does then is, depending on which way the magnet in the magnetic particle is oriented, it always likes to be north-south.

So if I draw little arrows to represent which direction of magnetization the particle is, they always try to order in a way that minimizes the stray field energy.

uh which is represented by these little bar magnets i've drawn schematically over here and so to be north south like that means that they if they're placing in this direction they like to be anti-parallel that minimizes their energy whereas if you place them at a corner like this then they'll form a corner and try to make a chain

The point is there is an energy associated with their interaction.

They can adjust their orientation to minimize that energy.

And that's sort of the basis starting point for where I think this overlaps with the predictive coding algorithm.

So the shape just means they have arrows along one direction and you can place them close together and then they behave like little toy magnets.

Now, the freedom and flexibility of the lithography which has developed over the years is you can place them in all sorts of interesting geometries where these interactions can compete.

and the name of that field is artificial spin ice and it is motivated by a residual entropy that was predicted by paulie for water ice the way that the polarization the distribution between plus and minus charges would order just in regular water ice it is frustrated it competes it cannot find its minimum minimum energy

And one can engineer that and study the frustration by putting these particles in certain arrangements.

This arrangement is one where there's a two-fold degenerate ground state

forms net anti-ferromagnet meaning there's a cancellation of all the magnetic moments this one where you just rotate everything a little bit prefers a ferromagnetic orientation so there's a net magnetic moment others which can be engineered are not really in either of these classes they are instead they're called topological ordering

So I'm going to talk about primarily this ferromagnet one just to illustrate the key ideas here.

Before I do that, as I was telling you before, my interest in this area, I was thinking that someday people are going to develop these magnetic arrays once I have the technology for neuromorphic application.

And indeed, that has been seen in the last five years where there are primarily reservoir computing algorithms have been implemented.

And one of the motivations, in fact, the prime motivation for this is the power efficiency.

So if we think about a conventional semiconductor-based logic computer, the information is moved around by charge currents, and charge currents lose energy through dual heating, through resistance.

So all the heat that's generated by the CPU is just wasted energy for moving charges around.

Information in a magnetic system can be encoded in the direction of little arrows I just talked about.

The energy involved in reversing these little arrows can be, in fact, as small as is allowed thermodynamically, in principle, the Skolpa-Landauer limit.

And this is not just a factor of two or three more efficient, it is orders of magnitude more efficient.

than what can be done by moving charges around.

So that is one of the prime motivations for trying to implement some of the very computationally heavy activities associated with machine learning.

And that's motivated this field.

There's some other things too.

You can start thinking about modifying individual elements and states.

Next Horizon is actually being able to do this in three dimensions and people are making strides toward that.

So it's an interesting area and it has the promise or at least the potential.

of introducing a non-semiconductor and conventional platform, which is particularly power efficient for conventional and non-conventional computing.

So,

make use of all that there's some basic concepts so i'll begin with this design here of a spin system now it prefers to align at in its lowest energy configuration such that there is if you add up the little arrows there will be a net magnetic moment meaning there will be a net

number of arrows which don't cancel out these cancel out in that direction but this group of four those add and there's four different ways to do that it could be these two adding and these cancelling and what we care about is the average direction of the arrow which is the average magnetization so I define a magnetization then as an average and it will be a thermal average

and where these little sigmas represent the individual spins making up the magnetization and this can be averaged over the entire material and so if you do that you would measure then the average magnetization the way these superimpose with one another so that you'll have a magnetization curve which would look as a function of temperature

They all, for a ferromagnet, like to lie in the same direction, or nearly lie as you increase temperature, but at some critical temperature, spins will start reversing and point in another direction, and that reduces the magnetization.

And the rate at which this happens we call a susceptibility, and statistically that's actually a correlation function.

Those two things are important because those are the two things which I will try to

convince you overlap directly into a predictive coding scheme and just enable it then the magnetization to act as a platform for implementing this sort of thing so the next bit that's important is to recognize how the magnetization

evolves over time, the processes involved.

So if I consider a whole lot of these little magnets, whatever geometry it is in, I can adjust the orientation with a magnetic field, which I might apply externally.

So I might generate a magnetic field either through another magnet or a coil or something like that and apply it to this array.

and that if it's strong enough it will tend to align all the magnetizations as much as they can in the direction of that field now if i reduce that field and i have a finite temperature there'll be a reduction in the number of spins or elements which like to point in their original direction and some of them will start to favor alignment with the field in its new direction

and so there might be a region which is pointed oppositely the others so you can see there's a net reduction in the magnetization and as i increase the magnetic field in this new direction this region grows at the expense of the other region until when i get to a large enough field everything is pointing in the opposite direction

The point I want to make here is this is all happening in time and it can be observed in real time.

So here's an example of an experiment that was done on such a system where they start the magnetization in one direction, then apply a small field to give it another direction and then watch what happens as it evolves.

And so microscopically what happens is all the spins were oriented one way and then one starts to reverse and that causes or makes it more probable that another spin nearby will reverse, which makes it more probable that yet another spin will reverse until they all reverse.

This all happens in real time and there's a statistical probability of each of these reversals.

So that time that it occurs in always changes a little bit, but it is a stochastic process.

and each reversal is sampling the configuration space but it's always looking for a way to minimize its energy so it's always trying to relax into a lowest energy configuration so that sets me now to where i can talk about the overlap with predictive coding um

Everybody here, I think, knows much more about this than I. I've been struggling to understand the logic behind it, but I think I've got a better idea now.

If we think about two variables associated... I think of it this way.

If I'm looking at the visual system, which I'm a little bit more familiar with, and I throw an object in the air, my eyes are following it, but they're doing it in a semi-random way.

They're saccadic eye motion.

and they're they tend to where am i going to look in the next instant is going to depend on what i've perceived in the previous instance and so there's a lot of information there there's velocity position all of that but i'm going to to look in an area where it's probably more probable to find the object in the next instant so there's two things going on i have a perception

and i have some expectation or belief of where it's and what i'm interested in or what my neurons are interested in is a probability distribution relating these two quantities

And that's where the whole theory begins.

So I've got some sort of input data, and I've got some sort of belief state of my neurons, and I'm going to say there's some sort of mapping, and it's not a perfect one-to-one.

It's stochastic.

There's a certain randomness on both sides of this.

and i'll say that that non there's a non-linear function mapping it or it can be non-linear mapping say my perception to a belief with a bit of random noise as well or maybe my next belief the transition to my next belief is also going to be a non-linear mapping or linear it can be linearized as well but it's some sort of mapping some function i don't necessarily know what that function is and moreover this can be in a hierarchical system so like

the original Raoul Ballard paper where a lot of this was described in terms of vision is a layered system where there's an image input from the back of the eye and then that feeds forward into regions of the brain

feed forward differences between the expectation and what was perceived, and then a prediction of which way maybe the eye ought to move next, or it would involve that, and it would keep going up the ladder and back and forth.

but i think that the main idea is that each layer tries to minimize locally its error function and match up the predictions with the perceptions at each layer and so this this is a little bit different i think than a lot of global oriented machine learning algorithms that are maybe a bit more familiar with

um one other thing too when i gave my example about tracking an object there's a lot of information being gathered in each time interval and that includes position but it also includes velocity and accelerations can in principle be any order of dynamics and so in addition to having a hierarchical uh layering

uh one can also envision a kind of a lateral hierarchy if you want and so i call that dynamic ordering and that that is i think the idea that that friston uh introduced uh back in the in the late to to around 2008 and so on and

The idea that evolves from all of that is I now have new indices that complicate my states and my perceptions.

They're indexed by layer and by dynamic order.

But the time evolution of my beliefs then, with this dynamic information, also introduce a term over here, but they're governed primarily by this relaxation of some energy function.

and that tends to uh that's the free energy idea that you're trying to to find an extremal value and it's this gradient which is most interesting for me

for what guides this, because that's where I can map that onto the magnetic system most easily.

And I put a little note here that this is a Langevin-type equation, but it can be also derived in a physics context where most physicists would be familiar with the Fokker-Planck equation for the distribution, the time evolution of a joint distribution function, and these equations emerge naturally from that.

Here's how we do it with predictive coding then in magnetization.

So I've got my state variables.

You might call them the hidden states and the more visible states.

They're indexed by layer in the hierarchy, but also by dynamic order.

And I'm going to say that I'm going to translate this statement of a nonlinear mapping between the visible and hidden states through these g and f functions, I'm going to translate them into measurements

on a magnetic system and the magnetic system is a magnetization something which responds where i put a field on it that field is a function of these variables these state variables and the magnetization is what responds or how the system responds to that magnetization as i said earlier system is always trying to find a way to minimize its mechanical energy and its entropy

maximize its entropy subject to whatever that constraining field is so if i can sample that that's why i put these angular brackets it's a stochastic sampling then i get a measure of the system trying to relax as it must to a more energetically favorable configuration and i just in

interpret that relaxation as an instantaneous value of the visible and hidden states.

So this is all done at some finite temperature T and the trick then is to figure out what this field is and then make a measurement of the magnetization.

And so my Langevin equation then, with this gradient of this error term or energy, translates into a transition rate for my state variables, here the belief, subject to this term, which is effectively a force guiding the magnetization toward a lower energy state.

So the terms that appear just for some details and how the mapping occurs, the field, which is the all important thing that's acting on the magnetization is a gradient of some energy function.

And that energy function is the lowest order expression for that is a difference between this nonlinear mapping and the average.

of my state whether I'm at whatever layer I'm in and at whatever order I'm in and that defines my effective field the terms which appear then there's a some parameter a precision which in my magnetic system goes like an inverse temperature

and magnetization which is my f function and the derivative of that f function which is my susceptibility so all these two things they can get from a measurement uh i do it computationally but they could not can be done through measurements uh we do that for data recording for example magnetic data recording all the time and this is the quantity then that tells me how my state variables will evolve in time

So the algorithm is really actually quite simple.

At each time step, I sample my magnetization on some magnetic array where I've subjected it to a field, which is a function of whatever the previous values for the state variables were.

Then having measured that magnetization, I get new values for my state variables and I calculate fields and I repeat the process.

So it's nothing more than that.

It's the same thing we used to do with magnetic recording, still do do with magnetic recording.

You have information on a magnetic material, you read it, but you can also write it.

You just repeat it.

So it's really quite simple, and it's an old, fairly well-established technology.

What's different is doing it on micrometer size patterns.

Well, that's not different either.

Here's an example.

So here's a three layer network with three dynamic nodes who would correspond to three different magnetic arrays.

And I would apply a field separately to each one of these arrays in my updates.

And then I just make a measurement of the magnetization from each of these nine arrays, feed it back in to get nine more fields and just keep iterating.

That's my implementation of the Longin equations that govern the predictive coding.

And that's how it works.

It really is kind of simple.

Here's an example.

So I've done one which is fairly recent, actually.

It's not dynamic.

It's just the layering.

I modeled it on the Rao Ballard paper.

But it's a simple one-dimensional version.

So I have an image with one dimension.

It's just a sinusoid, so I got a feature in it.

And I don't have very many nodes.

I think this was like 30 or something like that.

and then i reduce it to my first active layer mu1 which i think had something like 18 nodes and then i reduce it down to six nodes in mu2 and a bit more feature extraction with mu3 and then for mu3 i can do a reconstruction now over here mu1 and mu3 don't look very good

that's because i'm doing it at high temperature my original image looks like that and the reconstruction looks like that when i lower the temperature because remember you know i get a critical point where the magnetization disappears when i lower the temperature well quite a different picture here's my input data here's my first layer it's learned it pretty well

Here's the second layer which is extracting some features learned from this and more refinement at this layer Then I reconstruct from this layer the full image and it matches pretty well, but that's at low temperature So this just illustrates I think where the magnetic System you can actually see what it is actually doing something and it's through this nonlinear function and

Another example, this is now one which includes the dynamics.

So it's a tracking example.

And I was inspired, there was a thesis written by Millage, I think it is, it's quite nice thesis.

And in his chapter three, he did something like this.

I thought that was quite nice.

So I implemented it here.

And this is a sinusoid.

And I give into it because I've got three dynamic orders as well as three hierarchical orders.

So I have a velocity and acceleration and an acceleration to the acceleration.

And that input data then is represented here with my phi.

And then this is the layer one.

and it's following the sinusoid quite nicely.

This is the layer two, which is using a visible layer from here.

By the way, I'm following a paper, the formalism I'm using is by Buckley, so I have that hierarchy that they've used.

So this is their visible and their hidden notation.

These are errors on that.

and it's following quite nicely so that was curious how this what's being learned if anything so i tried a non-periodic input so i would just at each position choose randomly a velocity so sometimes go up or down it's a one-dimensional example and you see it does pretty well following even that random force choices in velocity so i ran it for some time and then i turned off the signal

so I turned off my input here and as you can see the hidden states however they keep going on looks like they learned a trajectory from the most recent steps and so it appears that it did learn to predict which way the system was going to go and yet you can see here it makes very quick corrections but here there's no input data and it will eventually come back down to that

And a lot of stuff is being filtered at the higher layers.

They respond a bit more quickly than the lowest layer.

And that's all consistent too with the observations that different time scales are happening at different hierarchical layers.

Active inference, which I haven't done yet, but it's here.

This is this example.

So it's what I showed you on the very first slide, nanomagnet control, feedback to a process, and then information coming back to the nanomagnets.

And again, this is just repeating what was done in the Buckley paper.

uh the active inference part is this a so we have the we've associated then some change in the process uh which appears then as a new function in this e function and this appears in the in the perception part so i look at how that is changing and adjust whatever this a signal is based on what is believed by the nanomagnets

And the target here is a linearized f function, but I'm still using an average over that linear f function from my simulating it with my thermal magnets.

So introduced into that function is a target value, Td.

And what that means is the system is going to try and align its perceptions through A with its belief that it ought to be at a temperature Td.

And this function that is going on in the process is unknown to the system.

uh so it has to it can't know the function it just has to adjust its position with mu in such a way as to get the data coming back into the nanomagnet to give it a value of td so this target here is the green line and here is the measurement is the orange and the belief is the blue and you see it starts off over here far away from the target and then it oscillates and exponentially decays down

uh until it hits the target and this is the other bit of information that's coming in that's that's the temperature this is the rate of bit change of the temperature with respect to to whatever this this value uh x is that it's sending to a and uh so it's got these two bits of input which is just the position and and the velocity whoops backwards and then this is the error function

and i found it actually useful to see this as a trajectory so that if i look at the belief in blue which is really what my spins are doing and the perception that's coming back in which is my phi and they're starting off here and then the perception

is slowly coming around and oscillating into its target value here and you see that the blue which is the stochastic magnets they're oscillating as well but doing quite a bit more jumping around but they also then find their center of gravity at the target point um this is just a note that rather than doing the simulation i can actually

capture most of what's going on here with a simpler approximation, which is quite common in magnetic systems.

It's a Brillouin function, which allows me to introduce temperature.

And you see it gives me the same qualitative features that I had with the actual magnetic system.

This is another thing, which is that system below before was just for three dynamical orders and one layer, one hierarchical layer.

There are parameters which matter, these precisions on two sets of terms which appear in my effective energy.

um i can get rid of the oscillations by adjusting these parameters so what i did was i basically built an active inference acting on the acting inference if you will so i built another network again with magnetic spins and their job however was to measure the free the energy uh

the energy cost of the first layer which is interacting with the process over time so they were running much slower and then try to minimize that energy by feeding back to these parameters

the lambda z and lambda omega.

And so this was the first iteration.

It just got started.

But now I collected data.

And then there was feedback from this active inference acting on the acting inference, which got fewer oscillations.

And as I repeat this through this learning exercise, you see it gets quite good near the end after about eight or nine repeats.

And it just goes smoothly to the target.

So it's adjusted these parameters

relative to the starting point to take it exactly to the target without a lot of searching around.

The last thing I'll mention is this.

We're done with fairly simple models.

That is, simple spins, real magnets that people can make are not that simple.

Although I did demonstrate you can use a fairly straightforward mean field theory to get the essential behaviors.

But that's a far cry from actually making a device.

So there are tools.

for trying to simulate better, more accurately, more realistically, magnetic nanoparticles.

So I applied those tools.

And they can be done at finite temperatures, it's just a different algorithm.

And it's called micromagnetics, and I can put in parameters which are measured and known for materials, iron, nickel being one of the compounds, and various variants of those compounds.

But there are things that people can actually make lithographically into the kind of designs which I was showing.

So I chose one with 16 spins.

and in this kind of a geometry so it's much simpler kind of system than what I was showing before and I did this tracking exercise but now call it a PID controller this is what it would look like electronically what I did was the same thing except it would be done with 16 spins and it did the same sort of targeting

adjustment and oscillates around here where I'm now measuring directly the magnetization through my micromagnetic simulations and then adjusting those for the applied field which then is feeding back into my imaginary thermostat over here another application of the 16-spin system if I say feed in an input function which is a sinusoid and a cosine

for my two dynamical orders, I can move the system of my 16 spins such that from one initial condition to another which I specify will correspond to an average magnetization.

That average magnetization is nothing more than the average number of spins pointing to the right or up and down.

And I can pick then a set of states by picking my starting points and my final points.

I can target a little subset of states where the magnets, specific magnets, are pointing in one direction or the other.

So I can pick examples of this has a certain magnetization, this has a certain magnetization, this has a certain magnetization.

corresponding to the alignment of the spins, I can choose those states with some precision just by picking my initial and final conditions.

The final one is just an example that what is a computationally hard problem in the field, which is if you have a real nanoparticle, some finite size with a lot of different kinds of more complex interactions, how do you reverse it in a very, very short time?

And this algorithm works rather nicely.

It reverses a magnetization from pointing up here to pointing down there by applying a pulsed, shaped pulsed field in the right directions, moving the field as well, which is a really difficult problem.

You can't solve that very easily using any of the tools that people have at the moment.

And that's it.

That's my short summary.

I think maybe nanomagnets, and I think a broad class of systems probably,

provide natural platforms for predictive coding by making this sort of mapping onto the magnetization or whatever the order parameter is and the susceptibility, which are all temperature dependent.

There are also stochastic processes.

They are minimizing a physical free energy.

And you can just map the two onto this, apparently, and constrain the system to evolve along a trajectory, which we would recognize then in an active inference example.

I'm also particularly interested in systems which are complex and like glasses and spinning glasses.

Some of these display all sorts of various odd and not very well understood dynamics.

And I'm wondering about whether or not there's some avenues into those studies by employing this sort of a methodology, if you will.

That's it.


SPEAKER_02:
Thank you, Robert.

That was awesome.

Anyone watching live, please write any questions.

First, Vladimir, please feel free to give any opening remarks and ask any questions you have first.


SPEAKER_00:
Yeah, this is a very interesting presentation.

And I have several questions for this.

So what your system, I see that it works and it can do calculations.

What you said, predictive coding is mostly doing computation with the matter.

And in this system, I think this is the main part that it is based on the mean field.

So that you are using Langevin equations and this is going to the...

uh free energy minima and so to go to the minima and uh this system just learned uh the path and that's what you have this gradients towards the minimum and so this is the basis of the system the why it can be predictable and then how you can uh calculate

Yes, this is correct.


SPEAKER_01:
Yeah, that's exactly it.

That's why I think it's rather general.

It could be taken outside of magnetic systems as well.

Yeah.


SPEAKER_00:
So this is quite general.

So you are at equilibrium.

So you have several local minima and you can even make those changes within the minima.

Yes.

So, for example, if you have

system with several minima and the kinetic barriers so that's can you also incorporate this


SPEAKER_01:
Well, that's kind of what motivated all of it.

As I mentioned at the end about glasses, which would be exactly that sort of system.

And it's not really at equilibrium.

And that's what makes it interesting, right?

Because this evolution, the Langevin equation evolution, especially with the dynamic orders, is tracing a path through phase space.

And this is introducing constraints on that path.

So I'm wondering, what I'd like to know is, does the landscape, the energy landscape, is this a way of studying it?

What role does it play?


SPEAKER_00:
Yeah, yes.

So basically, if you have several local minima and those switches, it may be quite interesting for computation.

So if those kinetic barriers are too high, then you are too stable.

It will be not possible.

And if it's too low, it's also unstable.

So this kind of things.

And another thing that would be also interesting to see how you can do for these non-equilibrium systems, which are not based on this relaxation or this lunging, but maybe going towards... I don't know if it's interesting to explore this direction.

it's completely non equilibrium.

Like I say, glasses and John.


SPEAKER_01:
That's exactly what I would like to do.

So far, I haven't.

I mean, one of the reasons I chose a spin the ice as a model system is you you can engineer the landscape to some extent.

Yes, you can have multiple, multiple energy states.

And I have

I've explored it numerically primarily, trying to see the consequences.

I found some oddities, some things, but I haven't made sense of it yet, honestly.


SPEAKER_00:
And have you tried it to... This system is a small size, so you can have a very big number of those particles, of those spins, and these big numbers,

may lead to some different scale so that you can do not only just explore a minimum but you can maybe use it as a on something called a completely different scale so if you if you have access to such a big number of uh interacting

particles.

So you have like an Avogadro system that may lead to some new phenomena.


SPEAKER_01:
I wonder that too.

I mean, computationally, it starts to become very difficult.

And we're kind of moving in that direction, trying to be able to treat larger systems.

But it starts to become very demanding.


SPEAKER_00:
For what?

To control it?

To simulate it.

over a sufficiently long time scales yeah but if you leave it to evolve by itself so you because you want to always like external control but you what it does yes but if you just switch by itself so just to self-revolve that's it i'm not yeah i'm not sure i'm following uh how would you what would you look at how would you set up that problem

So, for example, you measure the properties, for example, unification, susceptibility of the whole system.

But you don't provide the program, what it should calculate, but the system makes itself organized into some states by itself.


SPEAKER_01:
I see.

So for example, I played around a little bit with this idea.

If I have, I mean, instead of a large system, maybe I have many smaller systems, which I run simultaneously on the same problem, but I let each one follow its own path.

and so for example i think i was playing this when i showed you an example of the tracking where i'm using an active inference to adjust my parameters to optimize parameters for the tracking if i have several systems simultaneously doing this and letting them adjust their own parameters so to speak

Then I have, I think I was trying to think about which one of them is going to give me a state which is the one I want to feed back to my process, because I can't feed back all of it.

So I've got 2000 different systems which are all trying to predict an outcome, but only one of those is going to actually feed back to my process.

which one do I choose?

So I can make a, create a probability of which one, maybe the one that is closest, that predicts closest to the target value has the highest probability.

So I could choose which system to follow based on that probability.

So I'm sort of sampling between all my different systems.

Is that sort of what you're thinking of in a way?


SPEAKER_00:
But if you consider that interaction, not calculation in a normal way that you provide a program or you design all and control external control, but the material what you have interact with the environment and adjust the parameters without even providing these instructions.


SPEAKER_02:
I'll ask a question from the live chat, or I think I hear you on Vladimir.

I'm imagining one scenario where just stimuli are presented to a material.

and it could just be like sound vibrating sand and the sand pile never or maybe it's like an ultimate unsupervised learning but how much of this is the design for situation specific topologies like circuit design like and it's is it more like an asic

and it's more restricted to a certain kind of inference or how do you think about what functions it can perform or is it a general computer like a processor even if you could only do it very slowly like you play game of life that kind of stuff if it's just like a smaller hard drive with a spatialization element


SPEAKER_01:
Maybe this is relevant.

I'm not sure.

There's this thesis by Millage.

In this chapter, he did an image learning.

It was like learning the MNIST digits.

But he also had an interesting comment about hallucinating.

If he gave it a partial or

really a minimal amount of input, the system came back and filled in missing gaps in a way which was a bit surprising.

So it was like the system was self-organizing into a response to something it couldn't categorize very well.

I thought that was intriguing.

I didn't really know what that was saying, but it seemed interesting.


SPEAKER_02:
Yeah, that makes me think of a free energy minimum being possibly a local or global minima, but still being very far off from some other measure.

I don't know.


SPEAKER_01:
Another one example, which I don't understand, but I found very interesting, there were these papers, a few papers by Friston on a duet for one.

I don't know if you know these papers.

It was about birdsong.

And I think in one of them, if I remember correctly, it was almost as if there was one system

uh but then it was modeling it as if it were also two systems so we have one which is singing and then it stops and the other one picked up the song uh and reproduced a lot of it that that almost seems a little like that as well it's kind of in that category it's learned that it's following it but it's responding then because it it's it's it's up in this higher layers and comes back down


SPEAKER_02:
I'll ask a question from the live chat.

Burt wrote, I did not understand the step from the predictive processing feedback loop to active inference.

Can you explain how to add in action?

What are the affordances?


SPEAKER_01:
This one, I think.

So what I have in this case, do I have all the equations?

So there are two streams of input to the nanomagnets.

One is the temperature, and one is the rate of change of the temperature when you adjust a parameter.

And the parameter, this X, is essentially the rate of change of the perception with respect to whatever value of A is in terms of the magnets.

So it's adjusting A in a way that is proportional to, this turns out to be an error term,

uh on on the prediction and what is measured and then this coefficient just measures the the response essentially uh that this system is going to to react with uh so it's

I've got a single layer and then there's the system down here which is providing information so it's a feed forward but then this is a feedback is what the a value is so it's trying to minimize a discrepancy between the belief and the perception by adjusting some parameter x and all it knows is it changes this parameter x and there's a response that leads to a new response from the process


SPEAKER_02:
so if it changes it gets worse then the signs on that term change and so it pushes it in the other direction i don't know if that does that answer the question so you mentioned the frustration and the study of frustration so it's kind of like there's one alignment where it's a minimum a global or local minima

which can be designed to be a con or modeled as a convex optimization at least locally and then that the system could be like the magnet is pointing north and the compass is aligned and so then that's a static state or the magnetic field could be rotating or there could be other forces on the compass needle

and then the overall like tension or frustration is basically like the the relative mal-fitness so that's the downhill tension reducing force that at least locally the the compass is going to reduce its the gradient is going to go one way or it's exactly at a flat point and then it just is static unless there's some other output and then

to the physical part so then maybe to end the first question about what the affordances are in the lab what kinds of physical measurements and interventions can you do well for the magnetic system the kind of measurement you would do is measure the magnetization a simple way to do that


SPEAKER_01:
is if you've got a coil of wire and you move the magnet by it, the field which passes through the wire induces a current and that can be measured.

So that's sort of the basis.

There's other ways to measure magnetization, but that's kind of a simple, straightforward one.

You mean the mechanics of measuring the magnetic particles, or is this a different question?


SPEAKER_02:
Yeah, just what do the measurements look like?

Is it, um, is it, and then what can you make, can you change the fields with spatial and temporal precision?


SPEAKER_01:
yeah those are there's different techniques and different technologies in being developed as we speak but the maybe the simplest way to visualize them that they're typically some sort of variant of these kind of tape recorders which

Here you've got a coil, creates a field, and that generates a particular magnetic order, just like what you've just described.

Or you have some way of, maybe it's another coil, or there's various devices which are sensitive to magnetic fields, and you move that device over a region of magnet and it measures that as well.

it can be done sometimes optically for example some of the i think i pointed out the neuromorphic

It's the same problem in this reservoir computing schemes.

They actually use a combination of microwaves and there's also scanning probe techniques to look locally and globally at the configuration of the spins.

So there's a whole myriad of different ways that depend very much on the system and the materials involved.


SPEAKER_02:
Where do you see it being used?

Is it like a research element or a data center piece or is it something that would be functional in a phone or something in a house?


SPEAKER_01:
Well, one of the ideas, the motivation behind this is to try and find applications of materials that are much more efficient than transistors and CMOS technologies.

So there are different avenues you can pursue.

One of those is pure stochastic probabilistic generation.

It takes a fair amount of resources to generate a random number.

or pseudo random number, but you can do that with a magnetic system quite readily.

I'm very good at that.

So people are also developing that as a technique and that's for also applications to probabilistic computing.

And there may be many applications that if you can make it portable enough, you could do on your phone or

data centers for different machine learning algorithms for compression, like image compression.

I don't know that much about machine learning, but I guess language models and transformers

where it's... I do believe I read a white paper from a group where they were arguing that maybe active inference could also go and find application in those directions as well, so it's active inference as opposed to attention.

But I think that's fairly recent, so I don't know if that's...


SPEAKER_00:
what direction or what exactly is meant by that Vladimir do you want to ask another question no I think it was quite clear that this is I think which is a very interesting study and in the materials it is not

yet this direction is very new.

I think it's not yet very... how to compute with matter.

It's a quite new direction.

And that's one of the examples how it can be realized experimentally.

Because in many discussions in the publications, there is a lot of theoretical concepts.

For example, this active inference and other theoretical concepts

how it should be done and how it can be realized.

But experimentally, very few examples which are really implemented.

And this is one of those examples.

I think this is of this approach.

This is a demonstration among the first, I think this.


SPEAKER_01:
Well, I have a question.

for you also Vladimir, if that's all right.

I mean, I think you work with you, I think I saw through some of your publications work with nanoparticles.

And do you work with materials that have some sort of polarization or something analogous to


SPEAKER_00:
There was mostly a mean field.

It's one of the directions that's also how to go to the equilibrium and how to minimize the free energy in general.

So this is for example, for interaction with

membranes and biological systems there is also quite a big study so this thing goes in this direction and now what we're building the network for materials with intelligent materials

And here, in one of the directions, what we were trying to do is to build a discovery engine.

And also with Daniel involved in this, how to also for this field of materials, how to find concepts, for example, how to make

uh the uh those materials uh intelligent and what are the uh phenomenological concepts so on a very uh high level so what do you need a memory you need uh computation you need to

what is the time scale, so the system should change, and what is the short term, what is the long term, and this depends on the relaxation time.

So kind of this, because it doesn't matter what is the system, but you can already see how it can behave, because as you see, this is based on

thermal energy, minimization of the energy, and there are very fundamental concepts which are driving, driven them.

And their realization, it already can be captured from one system to another system, and you can even map it.

And see, this is also your system of magnets, what you see, it's quite general.

the mechanism is quite general can be applied to another system so this basically the idea of this work is how to make those connections between different fields or different materials and if you and

The major problem there is that you don't have reliable experimental systems which are demonstrating it because there is a lot of theoretical concepts how it can be done.

But you need to have this demonstrable, falsifiable results.

And I think this is how you show it.

It can be used for other systems.

This is kind of, it can be much.


SPEAKER_02:
Thanks, Vlad.

I'll add like one kind of connection there that arising from your talk is how do we think about the aspects or the capacities or reliability?

What to even measure?

How to have a useful engineering and in silico and in material flexible?

so that you can study different approximations and more field-like models and more point-like models so that you can do normal statistics on it and kind of port or have a good assembly language type connection kind of between like pro like the analogy between

processor instructions and the digital compute so then it's like what is the approach to map different materials and systems to their functions and then that's where these cognitive ontologies like precision attention belief updating learning all of those kinds of cognitive ontological terms become mappable from materials

Where with that sort of team and work, it was interesting to look at the method sections and look at the parameter spaces of where methods have or haven't been done, like which grid sizes have been done.

It's just a really high dimensional space, but possibly could be navigated interestingly to understand where there might be, where is something like a material that emulates a given function.

Where is that all like what is the status of being able to make that and then understand that within a structured parameterized experimental space to prioritize different experiments and perform meta analysis to understand how like just all all these different pieces.

So, yeah.

Just to add a little from what I've seen from that project and where this sort of talk takes me.


SPEAKER_01:
Well, I think I'd be very excited if someone would actually try to test this in one form or another, this idea.

I think, yes, you could maybe, I chose magnets just because I'm familiar with them, but I mean, other platforms might be easier technically to implement some example.

I think the PID controller is a nice one.

because it's close to an application.

It's used in an enormous number of things, from industrial control to cruise control on your car, for example, or just a thermostat for your home.

I mean, some practical device demonstration might be motivational in a sense.

Magnets may not be the best way to do it.

Maybe there are other systems

Or you could envision something analogous, liquid crystals, or something which has a non-trivial response in ordering, and yet subject to thermal processes.

I know that even in some conventional memory technology, for example, is something which

over 10 or 15 years has developed slowly, but it's kind of in that direction, a material platform where you have something which has a transition function, which would play the role, say, like the F function, for example.

People do look in that direction as well.

And economics is the motivator there, is to try to find more powerful, efficient solutions

replacements for what one does with transistors.

But I mean, in trying to find implications for something like predictive coding, you've got lots of examples from nature.

You can get devices that could do something similar.


SPEAKER_02:
Is there a software toolkit or a production...

possibility?

Like how would we, what kind of modeling can others engage with from this work or production of these objects?

uh experimentally uh i'm not sure yeah like are they synthesizable by yes you or how do they get synthesized or designed or what open source or technical documentation exists to design the in silico models or or to make them physically well i mean mainly


SPEAKER_01:
In my experience anyway, if you have an idea of something that could be measured is to convince someone who has the laboratory able to do it to invest a few years of their time and graduate students into making it happen.

So that's a little bit what I tried to do with this paper is demonstrate that, yeah, there would be a possibility of making these things using existing technologies.

It could be done.

And someone may try it.

But that would be just one example out of, I think, a new big possibilities, big problem, possibility space.

Great ideas of other systems, which might be easier to realize

Well, I think in this one, that's one of the reasons I chose it for this.

We have laboratories here can make nanoparticles and they do study spin glasses.

We may be able to do something in that direction.


SPEAKER_02:
Yeah, it's very fascinating.

Connor Hines and others have worked on the

spin glass system a little bit as well and so finding that that that sort of a uh

isomorphism between a generative model which can be emulated with distributions and maybe on batch data that works that works fine but then being able to have those calculations which it's very small set of them possibly or or even just one like gradient minimization methods alone

and have aspects of that computation, just like sometimes we amortize from the matrix to the neural network, and then go one layer into basically off-sourcing some of what appears to be the signal processing and action selection-like features of systems and have that based upon the material's properties.

and then co-design well then what symbolic aspects complement the materials properties that might handle what otherwise seems to be variants that needs to be soaked up by an algorithm within just like the big data processing framework


SPEAKER_01:
I think the spin glass, I think you mentioned the paper on spin glasses and active inferences, Heinz, I think it was.

Yeah, that's an interesting one, because one wonders, is there a way of approaching that sort of a problem from a different perspective, which is less coming up with a device or an algorithm, but it's just a way of thinking about things, which I do wonder about.

Maybe a direction to look would be in active systems, which is, I think, becoming

I mentioned this paper about bird flocking years and years ago, but that kept up.

And I think the modern name for this is active systems, falls under that rubric probably, where you think about particles which have almost like an attached motor.

And then how do they, what's the hydrodynamics of such a system?

But I do wonder then if this way of thinking, way of modeling, wouldn't be quite a natural approach in some aspects of that kind of a more complex system.

The question for me, though, is how to use this analysis in a way which is illuminating.

I'm not sure I know yet.

I mean, I did get some insight in the exercise here.

I didn't mention it.

I didn't talk about it, but

I found it useful to take these equations that govern this PID.

It's a set of three Langevin coupled equations.

And if I look at it from a physics point of view as a set of equations, which are not stochastic, but let's suppose we just take those differential equations and study their spectral response.

then by choosing my a function as simply as possible, one can show that the system of equations is oscillatory with an exponential decay.

That's not really surprising, it's just an oscillating system that responds in a well-defined way.

What is surprising is that it will go asymptotically toward the target, whatever you set the target at, but when you introduce noise into the system, it finds it much more quickly.

And that helped me understand how these equations are working.

They're not mysterious.

They're just a mechanical response, if you will, but it's introducing the noise, which actually makes it work better and faster.

That surprised me.

So that was maybe an insight I can take to be looking at other systems, which are relaxation dynamics, which are non-trivial.

The noise matters quite a lot, the magnitude and how it's introduced.

That is interesting.

So that's a direction I wonder about.

And the same with your question, Vladimir, about a system that might have a rather complex landscape.

I think that's interesting as well in terms of how that evolves on a longer time scale.


SPEAKER_00:
And this is also related, for example, if your example with the noise, the stability of those systems, because it is possible to design theoretically some system, but experimentally it will not be stable.

And this is the major problem that how to control it, that it will not fall apart.

the noise may destroy the properties.

And how do you control it?

Yeah.


SPEAKER_02:
But you showed that above a certain temperature, the functions of noise or temperature variance of one kind or another, there's the right amount.

So that's conveniently framed as precision or cognitive attention.

And then I think the biggest theoretical mapping is that there's no interpolation of a reward secondary function.

You're able to directly have a physical analogy, at least where the analytic form of the free energy functional in silico can be designed to reflect what was already transferred from the physics or inspired from the thermo and the information.

So the theory mapping

to the materials, like Vladimir pointed out, is really an exciting edge that your work is pushing along.

So it's awesome to hear.


SPEAKER_00:
There is a certain range of parameters when the system is behaving.

It can compute, and it is stable, and it can behave.

but those parameters are not obvious how to find this window of all these parameters temperatures and uh all the distances and everything that you have uh this control parameters they should be in a narrow range that it has sense and this is very difficult to find


SPEAKER_02:
And that kind of connects to the active inference for active inference that you talked about, like intelligent selection of materials properties.

So thank you again for the work and for sharing.

Hope that people are interested and learn more and you and your colleagues are welcome to share more.


SPEAKER_01:
Well, thank you very much.

It was nice to learn about the work that you're doing and the discussion.


SPEAKER_02:
Thank you.

Thank you.
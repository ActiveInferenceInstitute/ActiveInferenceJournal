SPEAKER_01:
hello welcome everyone it is august 6th 2025 we're live in active guest stream number 116.1 with returning guest max afer who will be discussing this paper solving the compute crisis with physics based basics

max will go over the paper talk about a few different things then i will check in the live chat and read any questions that people have so looking forward to people's questions and max thank you again for joining looking forward to the presentation


SPEAKER_00:
All right.

Thank you for the introduction.

Yes, I'm Max Eifer and I work with normal computing.

And as Daniel mentioned, this is my second time on the stream with the Active Inference Institute.

The first time I was talking about a paper called Thermodynamic Bayesian Inference.

which is an example of one of our algorithms we've been developing in thermodynamic computing.

And this time I'm going to be talking about this new perspective article we've written called Solving the Compute Crisis with Physics-Based ASICs.

And just a little word about the background on this.

This kind of emerged out of a

an industry-like workshop that Normal Computing, the company I work at, we hosted at our office in New York because we had been focusing up to that point on, and still are, on the ideas of thermodynamic computing and applying those to machine learning and in particular, you know, probabilistic machine learning.

But we noticed that there's a lot of other people, both in industry and academia, working on different approaches to unconventional computing.

And we figured, well, there must be something that we can learn from each other.

So we hosted this workshop.

And it turned out, I think, that a larger unifying framework started to emerge, that there's a bigger picture of something that these different approaches have in common, even though

even though when we move beyond the standard digital paradigm we're familiar with we get kind of a zoo of strange exotic new uh components and circuits that we're using there is still some kind of pattern or structure uh that we can identify and categorize uh to think about these things uh and so that's kind of where this uh paper came out of that um us and the other people who were at that workshop thought it was worth writing a paper on um and so i'm going to uh

After I finish giving a little bit of the background and overview, I'll scroll through the paper and give more detailed explanations and commentary on it.

Also, of course, I encourage anyone to go and check it out for yourself.

This is a prospective article and we're not going to be presenting new theoretical results in here per se or simulation results, but it's more of our view of the connections between these different computing paradigms and where we see the field going.

But in a nutshell, I guess it's the reason why I think we see so many of these new exotic computing paradigms starting to take off now is because there's kind of a new challenge for computing, which is the amount of compute that's required for AI is just a very large amount of compute.

And it requires a very large amount of energy.

And that's not, I guess, a new challenge for computing in a way, in that there have always been new algorithms that demanded more compute.

But there is something different about it, which I'll get into when I talk about the end of Dunard scaling, in that we'll have to solve it in a different way than it was solved before.

at a high level, you can kind of think of it as instead of just trying to make chips bigger or cram more transistors onto a chip, people are now looking into ways of

using a different kind of fundamental structure for computing, replacing using something else instead of a transistor to do computing, logic gates, or storing information, or taking those transistors we have and using them in a different way, modeling their behavior in a more complicated way.

And of course, there's also ways we can rethink the architecture of a chip, something like a GPU or a CPU,

There are a lot of kind of things in common in the in the circuit architecture from one to another.

And that hasn't been haven't been changed very much in a long time, at least in big commercial platforms.

And so people are also thinking of new kind of novel architectures and the

The kind of family of approaches I'm going to be talking about are what we call physics-based ASICs, which we've kind of introduced this term and given a bit of a definition in this paper.

But what you can think of it as is saying, instead of trying to abstract the physics away and treating my circuit as something that behaves like an ideal model of computation, like a Turing machine or finite state machine,

I'm going to actually model the physics of the components of a circuit and use those in an algorithm somehow.

So I'm going to use physical components on my chip and pay attention to what kind of physical processes they undergo and try to exploit those to do some kind of useful computation.

And then of course we can get into the idea of

hardware algorithm co-design where we kind of look at two different things.

We look at like what are the kinds of physical processes that this device can do and then on the other hand we look at what kinds of processes are useful for my algorithm or what kind of primitive operations are needed by the algorithm I'm trying to run

and we try to kind of find the overlap between those two uh categories basically and that's uh basically what a lot of us are working on um more or less right now including at uh normal computing is this approach to hardware algorithm co-design um so that's kind of the high level overview i guess the ideas in the paper and so now i guess i'll go into a little bit more detail um

so um i mentioned earlier that the problem of having applications algorithms that require a lot of compute uh is not a new problem per se um but the thing that is different is that we are now kind of firmly in the like post denard scaling era and arguably in the post moore's law era so i guess i'll say a little bit about what that means um so what denard scaling was about

was that for a long time, you know, decades up until the early 2000s, when people made new microprocessors, made new CPUs and, I guess, GPUs towards the end of that time period, they would shrink the transistors.

You know, they'd make the transistors smaller so they could fit more transistors on a chip, right?

And this is how Moore's Law, which is maybe a little bit more... More people are a little bit...

are familiar with more as well than Dennard scaling, but both were driven by that trend that people would make transistors smaller and so they could fit more transistors on a chip of the same size.

But what Dennard noticed is that when you make a transistor smaller,

that also changes the amount of power that you need to operate it because the supply voltage is also reduced by the same factor when you make that transistor smaller.

So this was really great, right?

You know, in engineering, it's not so often you get something that's like a kind of a free lunch, but this was maybe the closest thing you could get for a free to a free lunch is that

we we make the transistors smaller and not only can we have more on the chip but they consume less power they're also going to dissipate less heat so that helps us out with with cooling right um and uh one way of kind of summarizing that uh that idea is that um independent of how small your transistors are in the in the dinner in the dinard scaling era the um

density of power consumed per unit area was constant.

So in other words, if I have a chip that's, you know, a centimeter by a centimeter, it doesn't matter how many transistors are on it, a million, a billion or a trillion or whatever, we would assume that they would all consume the same amount of power.

And so that was in a nutshell, like how the challenges of

more and more demanding applications was met historically was that we would make the transistors smaller, put more on the chip, and we would also have better energy efficiency because of Denard scaling.

But the reason that will, so I guess, yeah, Denard scaling ended in like around, you know, people will say different years, maybe 2005,

something around there, maybe a little earlier.

But the reason it ended is that not every electrical property of the transistor scales in the same way when you make it smaller.

And in particular, without going too deep into detail here, it ended up turning out that the leakage current and the threshold voltage don't scale in that way.

And so we could only make them so small

and keep reducing the power consumption.

And so once transistors were small enough that the leakage current became very important, that trend ended.

And so after that, we could still make transistors smaller to some extent, but that wouldn't really change the power.

They would still consume roughly the same amount of power.

And so that started to create problems for cooling, right?

Because we can...

cram more and more transistors in a small area, but that means the density of power per unit area is now going to start increasing after the end of Dennard scaling.

And so people would do things like say, we're only going to turn on part of the chip at a time because otherwise there would just be too much heat, stuff like that.

And that was really what I think kicked off kind of like the multi-core era where people then started to say, well, because we can't really get much more mileage by just making the transistors smaller and putting more on the chip, what we're going to start doing is we're going to have multiple processors basically.

And so when we need more compute, we'll just get more processors, we'll get more chips, and then we'll try to parallelize our algorithms.

We'll try to make our algorithms

into how we'll try to solve our problem using an algorithm that can efficiently and effectively be parallelized across multiple chips with a minimal amount of communication between them.

And that basically brings us to where we are today, where the general purpose GPU is the dominant paradigm for some of the most important algorithms, particularly in AI now.

So yeah, I guess that's kind of the starting point.

And then the question is, well, what are the problems with this GPU paradigm?

Is this way of scaling things up, just getting more and more GPUs going to work?

Or if not, why are we considering these physics-based ASICs instead?

Well, so basically, the issue is that right now,

the because of this situation where we can't

reduce power consumption just by making the transistor smaller.

If you have twice as many GPUs, they're going to be consuming twice the amount of power.

And so there's a huge increase in the amount of energy that's consumed due to AI that's anticipated with people putting power plants in their data centers or data centers in their power plants or however you want to look at it, just because you needed a huge amount of energy to power training, particularly training, maybe even inference to some extent now.

And then also there's just the sheer cost of getting all that hardware, right?

The cost of buying all the GPUs that you need to do a training run for a foundation model.

And so that's why you have the big companies like OpenAI and et cetera saying that they're literally melting, their GPUs are melting because there's just too much throughput basically for them to handle.

So that's why we're there that's basically the summary of the problem that we're trying to solve or that we're We're talking about different solutions to that problem in this perspective article Okay, so I think that's also a good point to transition into talking about the idea of hardware algorithm co-design

So I mentioned that because the way that we're scaling up compute now is getting more chips and we need to parallelize our algorithms, the dominant algorithms that we see in the model architectures that we see in AI are ones that can be parallelized very effectively.

And that's something that was pointed out in a well-known paper called the Hardware Lottery.

And it was talking about how model architectures like transformers and so on, which are powering the large language models, are not necessarily better than many other approaches, different approaches from a first principles algorithmic standpoint.

But the reason why they're so successful and these algorithms have won out is because they

they are more suitable for the hardware paradigm.

They parallelize better on GPUs.

And so that's something that many now have kind of come around to that view, that the transformer, the reason why it's so successful and prevalent, I guess, is not because it's better than other solutions like recurrent neural networks or LSTM or whatever, but just because it parallelizes better on GPUs.

Now we can kind of take this line of thinking and generalize it a little bit further in that so clearly it's not just a question of like how good is the hardware or how good is the algorithm.

But the question of how suitable are the algorithms and hardware for each other is kind of equally important.

So that's maybe a slightly more general way of thinking about that idea of the hardware lottery.

If we think about how does it make sense for us to design better algorithms and hardware from that perspective, I think there's a good case to be made that we don't want to treat it as two separate optimization problems of either

On the one hand, we'll consider the algorithm fixed, and we'll try to make the hardware as fast as possible for running that algorithm.

Or on the other hand, we'll consider the hardware fixed, and we'll design the algorithm to run as fast as possible on that hardware.

But instead of either of those, we approached it as a joint optimization problem, as how do we design the best combination of hardware and an algorithm to solve this problem as efficiently as possible?

And so that approach is hardware algorithm co-design.

And many of the ideas in this paper on physics-based ASICs are taking a co-design approach like that, I guess.

So I guess with that said, I'll start to jump into what is a physics-based ASIC and how do we even define that.

So I'll start out with kind of an observation about why we think that using the physics of devices can give us more computational power, basically.

So we give an example in the perspective article of the problem of simulating the behavior of a single transistor.

So you would think of it as, well, if I have

software like Spice or Cadence or something, they have some behavior models of how a transistor behaves, or given the input signals, what are the output signals, and so on.

But to have a really detailed physical model of a transistor that totally explains all of its observable behavior,

One way you could do it is you could solve a system of partial differential equations for the transistor with many variables, many degrees of freedom.

If you wanted to run that simulation on a standard digital computer, you would end up using billions of transistors at least or more to simulate this one transistor.

You'd be doing a huge number of

of flops, floating point operations, just for a single time step to simulate that one transistor.

And so once we realize that, we can say, well, if it turned out that the behavior of a single transistor, the detailed physical behavior, was actually useful,

for some application, then instead of running this very costly simulation, we could just use that single transistor as its own model of its own behavior, basically, instead of some simulation of it, basically.

So that's the intuition for why we think that hidden in the physics of common

common circuit components, there's a lot of computation in there that's hidden in there somewhere.

But then the question is, well, is that computation useful?

So then we need to say, well, what are the primitive operations that are useful for the algorithms that we want to run?

Is there some way for us to map them onto the physics of circuit components?

So now that I've covered that intuition, I'll start to walk through

more of a definition, I guess, or maybe as close as we come to a definition in the perspective article for physics-based ASICs.

And the way we're thinking about it is that,

So first of all, all integrated circuits use physics to do computing.

There's no such thing as a circuit that doesn't use physics because ultimately the physics of the circuit and Kirchhoff's laws, et cetera, are going to govern the behavior.

So what do we really mean when we say a circuit that is using physics to compute?

Well, for one thing, in any kind of common run-of-the-mill integrated circuit that you'll find,

there are a lot of, well, they basically kind of abstract away from the physics so that it's possible to treat the circuit as an idealized model of computation, like a finite state machine or a Turing machine or something like that.

In order to make that possible so that you can kind of ignore the physics and design algorithms on a higher level of abstraction, we need to suppress a lot of kind of messy behaviors.

that you would find in systems in nature, in kind of generic dynamical systems in physics, but you don't really observe these behaviors in circuits that we've designed.

And so here are some examples of those messy behaviors that we tried to design integrated circuits to suppress.

So one is like statefulness.

So in an integrated circuit, there is a clear separation between memory and compute.

You know, you have some transistors and capacitors, et cetera, that are responsible for storing bits of information, for storing data, and other ones that are in logic gates that are responsible for processing that data.

And now, more recently, people are kind of starting to merge the two, and that, I think, kind of starts to fall within the realm of physics-based ASICs.

But in the conventional paradigm, you have this clear separation between logical operations and memory.

And what that means is that the circuits that are responsible for the logical operations are supposed to behave statelessly.

And so as an example of that, let's say that I have a NOT gate.

And I input a 0, it outputs a 1.

I input a 1, it outputs a 0.

Its output is only supposed to depend on the input it's getting at any given time.

Its output isn't supposed to depend on previous inputs that it's gotten.

Or in other words, it's not supposed to be maintaining some internal state that

keeps track of its history basically.

It's supposed to behave statelessly.

So that's one way in which circuits are designed to suppress kind of a common like generic physical behavior because most dynamical systems in nature of course are stateful in the way that they behave will depend on their history.

So that's one example.

Another one is

unidirectionality of the flow of information.

So I'll take the example of a NOT gate again.

If I change the voltage at the input, the voltage at the output is supposed to change in response to that.

But we're not supposed to have information going back the other way.

If I apply the voltage, change the voltage at the output, that shouldn't affect the voltage at the input.

designing logic gates and components that satisfy this kind of unit directionality makes the engineering of circuits a lot easier because you can treat logic gates as blocks in a feed-forward network where information propagates through one block and then through the next one, and you don't have to worry about unwanted feedback loops where information spontaneously propagates back through to the beginning of the circuit.

And, of course, you can design circuits with feedback loops, and many circuits do have feedback loops, like the one illustrated here.

But the point being, though, the feedback loop has to be built in explicitly.

So here you have, you know, logic gates, processing bits,

a memory device, a flop is updated, and then a signal is taken back to the beginning by some wire.

And this is the way that feedback loops are created in digital circuits, because you're not supposed to have information that shouldn't flow back through the same logic gate.

And then again, if you compare this to how

physical systems in nature behave they often don't have like unidirectional flow of information they have bi-directional flow of information so you think of for example newton's law you know for every action there's an equal opposite reaction if you or if you have two masses on a spring if mass a is connected to mass b if if b is pulled in one direction by a then a is you know pulled in the other direction by b and analogously if we think of like an analog circuit where you have different capacitors coupled

to each other in some resistive network or something, they'll both be sharing charge with each other and their voltages will both be changing at the same time with information flowing in both directions.

So unidirectionality was another example of how conventional ASICs suppress a generic physical behavior to make them conform to some ideal model of computation.

Another important one is determinism.

a circuit is given if you give the same input

multiple times, it should give the same output each time.

And of course, you can use a deterministic circuit for randomized algorithms if you want.

And that'll generally be done using a pseudo random number generator, which is still going to be implemented by a deterministic algorithm on a deterministic circuit.

But the point being, the circuit itself is supposed to,

To a very good approximation behave in a deterministic way where you give it the same input in the same you get the same output And so that's another you know thing that is not Well, it's not perfect.

It's not possible to realize that perfectly it's only possible to realize it to a good approximation because of course all systems in nature have fluctuations they have noise which is guaranteed by thermodynamics and

And so in order to suppress those fluctuations or at least make them or protect against them so that you don't have random bit flips all the time, that's always going to cost some amount of energy and there's going to need to be some kind of safeguards or errors correction or whatever built in to enable this idealized deterministic behavior.

So that was a third example of a kind of a behavior that

is suppressed in conventional ASICs.

And then I think the last example we gave here was

synchronization so when we have you know some very large circuit uh on a chip that with many different you know many different parts all doing something at the same time there is typically going to be like a global clock that is sending out a clock pulse signal to you know all different parts of the chip at the same time and so all parts of the chip are synchronized with with each other and with the centralized clock

Now this is another kind of behavior that we don't often see in

in systems we find in nature.

There are many systems that exhibit synchronization to some extent, but often synchronization happens in a distributed way.

So if you have something like a Kuramoto model, for example, which can be used to model how I believe it's used to model how neurons synchronize with each other in the brain or things like that, often models of synchronization

that are supposed to be realistic or ones that are supposed to model how synchronization actually happens in nature are using a kind of distributed model of synchronization where different

different kind of components will talk to each other, but they're not all talking to one centralized clock.

Of course, there are exceptions to that.

But that's probably the more generic physical situation where you have if you have synchronization, it happens in a distributed way.

And then often in arbitrary dynamical systems, you won't really have any synchronization at all.

So this was another behavior that where we're kind of

building in something where we're going through some effort to give our system a property that is kind of uncommon to find in nature, is somewhat unnatural somehow.

And so for all of these properties that allow us to treat integrated circuits as idealized models of computation, we're going to pay some cost in order to make them

to realize these properties approximately and they can generally they can only be realized approximately not perfectly and in general we're going to incur some additional costs in terms of dissipation you know how much yeah how much energy is dissipated

And in terms of speed, so in order to make these approximately true, we're going to have to accept some limitation on how fast we can run our circuit.

A good example probably being like synchronization.

If you imagine that every part of the chip needs to be synchronized to that global clock pulse, well, it takes some amount of time for that clock pulse to reach all the different parts of the chip.

So that could be a limitation on how

fast you can make your clock cycles, right?

Because you need to allow enough time for the clock pulse to reach all different parts of the chip.

Similarly, determinism is going to incur some cost because we need to

suppress noise or or make our signals large enough that the noise is like negligible relative to the signals which is a common way of of making circuits deterministic and so this observation that these idealized properties all incur some cost in terms of energy or time or something like that

generally energy and time because that we start to say well what if we design circuits without worrying about these properties or saying we're going to relax these requirements that they they have these properties and maybe our circuits will be more energy-efficient and maybe they'll run faster but then we'll have to kind of

Think in a more complicated way when we design our algorithms because we can no longer or when we even when we design our circuit both the circuits and the algorithms because it's not as easy to For example to compose operations because now instead of treating each component as a block, you know element that feeds information for that, you know has unidirectional information flow and is stateless and deterministic and

Now each component is a little bit more messy and we have these messy behaviors that we need to deal with somehow when we design our algorithms.

That's how I would think about defining what a physics-based ASIC is, is an ASIC which is not designed to satisfy all of these properties approximately and violates at least one of them or oftentimes more.

And as a result, we hope to be able to make circuits more energy efficient and faster that way.

So now I've kind of given a model for how to think about what a physics-based ASIC is, I guess I can talk a little bit about

some examples to uh approaches here um but actually i guess before i go into the examples of hardware paradigms maybe i'll pause and see if there were any um any questions that came up in in the chat and if there were i can i can take some of those um i'll just read two helpful comments by your colleague patrick coles who wrote joint optimization optimizing the hardware and algorithms together can be more powerful


SPEAKER_01:
than individually optimizing them independently.

And in a sense, the more properties on this list that you assume, the more you move away from physics, what physics does naturally.

Sorry about the audio, by the way.


SPEAKER_00:
Oh, no worries.

Great.

Yeah, I thought those were great comments.

Thanks for the comments, Patrick.

OK.

All right, great.

Well, so I guess I'll keep pressing forward and talk about a few examples of hardware paradigms that start to

abandon some of these properties, I guess, in various ways.

So one, of course, is analog circuits.

Here we have an example of an RC circuit.

And you could have many little RC circuits like this that are all coupled to each other.

In general, these can be noisy.

They could have more or less noise, or they could even have extra noise sources included in them.

And when we have a bunch of these coupled together, how they behave is modeled by some kind of differential equation or some stochastic differential equation that describes how all the voltages and currents change the function of time.

And so when we have a system like that, well, for one thing, it's not deterministic if there's thermal noise.

For another, it has information that flows in both directions across all the couplings.

um and it has components that are that behave in a stateful way like these capacitors their voltages are are actually playing kind of an algorithmically important role of storing state and so you have kind of somehow you know memory and processing happening in the same place

I wonder if there's any other properties that that violates.

Well, I guess it was all four of them, just with that simple example of the stochastic analog coupled RC network.

And now there's other kinds of circuits that do processing using similar networks, like resistive crossbar arrays can be used for matrix vector multiplication using an array of resistors.

Now, I'm not sure if I would exactly consider this analog computing, although maybe it is in some way in that you're doing addition using Kirchhoff's current law instead of using addition in binary using a bunch of logic gates.

And so we're somehow using the analog current signals in order to do the addition of our numbers.

And so I guess another example of circuits that violate determinism would be like stochastic magnetic tunnel junctions, where you can have

stochastic switching which happens when the magnetization direction in one of the ferromagnetic layers can spontaneously switch the magnetization can switch direction as a result of thermal fluctuations um and so now we have like non-determinism baked in at kind of the the smallest level of of the circuit instead of building our circuits out of

um building blocks that are deterministic we're building it out of building blocks that behave in a stochastic way and that can be used for like stochastic computing for example or it can be used for like for computing based on like p bits or probabilistic bits uh people commonly like use things like this for um like uh bolson machines uh would be one example um and then i guess uh

Other kinds of non-deterministic behavior in circuits would be stochastic digital circuits, which can still be built without using any more exotic components like MTJs, but just with standard CMOS transistors.

But in this kind of paradigm of computing, stochastic digital circuits, we encode numbers in random bits, bits that are Bernoulli random variables like coin flips.

And the probability that the bit is one is kind of encoding a number.

And so different numbers are encoded by random bits with different probabilities.

And so you can imagine that using this way of formatting or encoding numbers is more robust to errors than it is if we just formatted all our numbers in binary.

If I have a binary number, if I accidentally flipped the least significant bit, that causes a pretty small error.

But if I accidentally flipped the most significant bit, that creates potentially a catastrophic error.

And so that's kind of the reason that

standard like binary encoding is not the most resilient to error uh so it's like bit flip errors and so when we use instead uh stochastic digital circuits uh because our processing is resilient to bit flip errors we can in some sense like cut corners with the design and not build in as many guard rails that uh prevent bit flip errors and one example of that would be um

I mentioned earlier how global centralized synchronization is often assumed in integrated circuits.

But if you have some resilience against bit flip errors, you can often use some kind of trick so that you don't have to have a global clock that everything is synchronized to.

Or instead, you could have many local clocks in each different parts of the chip.

are just kind of locally synchronized, but not perfectly.

Those local clocks can drift or skew a little bit relative to each other, which is called polysynchronous clocking.

And using methods like that that take advantage of the error resilience of this paradigm can allow you to

you know, drive your circuit faster or to use less energy, basically.

And then, of course, there's approach.

There's other kind of physics based approaches that like optical optical computing or, you know, integrated photonics, for example, which I won't get into very much.

But yeah, some of our co-authors in particular, you know, like Peter McMahon is more of an expert on that kind of stuff.

But I guess in general, I just want to emphasize that there are a lot of different kind of building blocks that you could use

to build circuits, make chips that start to do away with some of these assumptions.

um oh i guess another good example of a computing platform or you know device that uh does away with one of those assumptions would be memoristers which are of course intended to behave statefully so the the way a memorister behaves should depend on kind of its history of what voltages have been applied to it before uh so it kind of gets rid of this assumption of statelessness uh and then you could also make like an emulator of a

a memristor using transistors and capacitors and so on.

So even without having a memristor device per se, you can still build circuits that behave in the same way of behaving statefully.

So I've talked about a few different platforms and building blocks that can be used.

to make physics-based ASICs.

And so now you may be starting to wonder, well, how do we know that these devices are actually going to be useful?

What kind of problems will we be able to solve?

Even if we build a chip that doesn't have statelessness or determinism or unidirectional information flow and so on, what's it going to be good for?

So this is where we need to think about that hardware algorithm co-design approach I was talking about.

So we have some device which has some physical processes that occur on it.

And we think of that as kind of this circle over here in this Venn diagram.

As we put it here is like what physics wants to compute.

Because when you have like some, for example, some network of electrical devices like transistors and capacitors and so on, there's going to be some, you know,

some mathematical function that relates its inputs to its outputs, potentially some complicated function, which includes a lot of physics that happens.

But we don't know that that's useful, per se.

So that's what physics wants to compute.

And there's also what functions are useful in our algorithms, what are the primitive building blocks of our algorithms.

And so what we want to do is we want

find the overlap of these two sets, basically, and find the operations that are performed efficiently on a device due to the physics of how the device works and are also useful in some algorithm, basically.

So that's kind of the idea of hardware algorithm co-design.

And so then I guess I'll talk a little bit about examples of how that might work.

So I think one good example of a class of algorithms where we would expect these physics-based ASICs to perform well

is in, well, in machine learning in general, where you have a function that is parameterized by a set of learned weights.

Because one thing that's useful about this is that

a neural network is adaptable in some way and that if you make some change to its activation function, for example, and then retrain it, it will often in general be able to modify the weights so that it still computes the correct function, but just does it in a slightly different way because now you've changed the activation functions and also changed the weights to somehow compensate for that.

So similarly, if we have some physics-based ASIC that has some kind of intricate physical processes that are used to evaluate functions, we don't necessarily get to choose what those functions are.

They might be very complicated functions, and they're not necessarily immediately useful, but if we

combine those functions that are natural to perform on the hardware with a learning algorithm and trainable weights, then in some sense we expect the model to be able to adapt to the functions that it is given, basically.

So you can kind of think of that as analogous to the idea of

training a model for some you know different for different kinds of activation functions and you could also train a model for That can take advantage of the physics on a you know of the operations that are naturally performed on some physics-based ASIC So that's kind of one general class is like training or in running, you know training an inference for neural networks on physics-based ASICs in fact a lot of the

hardware paradigms that we've talked about are being used or have been proposed to be used for exactly that.

So we have this area of like training and inference for neural networks.

Another good example is like scientific computing.

So a lot of the time, some of the most demanding computations that are done are simulating actual physical systems.

So as one example, you could think of like

molecular dynamics where you have some large molecule, which is basically a network of atoms that are connected to each other, of course, by bonds and so on.

Then there are the laws of motion that govern how all those atoms move.

In general,

the way that people report performance for molecular dynamic simulation, I believe, is often quoted in nanoseconds per day.

How many days do you have to run your simulation to simulate a molecule evolving for a nanosecond is a relevant question in molecular dynamics.

And so that goes to show that somehow the physical system itself, the molecule, is much more efficient at running its own dynamics than

digital computer is.

And so then we could ask, well, is there some circuit that behaves more like the actual molecule physically or that has similar equations of motion to a molecule?

That would be a lucky coincidence, right?

If we could find a circuit that has very similar equations of motion to some molecule and could use it to simulate the molecular dynamics,

Or more generally, we can look at some of the properties of the physical system that's there in molecular dynamics and say, well, can we

do we really need all these properties in our circuit that we talked about before, like, you know, statelessness and unidirectionality, et cetera, in order to build some, you know, build some algorithm for simulating the molecule.

And that's kind of a more general approach of like, we're not necessarily looking for an exact mapping between the dynamics of the circuit and the dynamics of the molecule, but we're kind of looking at a more abstract level, like what are the properties of the physical system we're trying to simulate?

What are the properties of the circuit?

And is there some kind of,

agreement between those in some way.

So a good example would be determinism.

In molecular dynamics simulations, of course, there's generally noise that's added, which represents the influence of the heat bath.

You'll simulate a molecule not just in a vacuum, but typically in some solvent.

And that solvent could be simulated in different ways, either, you know, by explicitly simulating a bunch of water molecules or by adding kind of noise terms in to simulate the effects of random impacts from water molecules or etc.

And because we're running an algorithm where in some way there's like random behavior,

which, again, modeling the heat bath, we start to wonder if it's actually necessary for our circuit to behave in a totally deterministic way, or if it's okay if our circuit has random errors at some rate.

So I think that's a good example of connecting the properties of the circuit we're using for computing to the application that's required by some scientific computing task.

And there may be, you know, other ways to do this as well.

You know, for example, if I have a molecule, there isn't necessarily like kind of something that behaves like a global clock that's, you know, talking to every single atom and synchronizing their motions, right?

Only, you know, there's only kind of local interaction and local connectivity in typically in molecules.

And so there might be a way of using a, you know,

a computing architecture that doesn't have a global clock or has something more like that polysynchronous clocking I was talking about and maps onto the molecular dynamics problem somehow.

And similar kind of arguments could apply for these other properties as well, even if the details of how that would work isn't exactly worked out.

And so in general, you know, as we were talking about earlier, when we kind of get rid of those idealized properties, we are allowing our circuit to behave more like a generic physical system that we might find in nature.

And so it might end up that it's well suited to simulating physical systems that occur in nature, like molecules, for example.

So another kind of class of algorithms where we expect physics-based ASICs to be useful is, well, for one thing, in randomized algorithms in general, because I was talking about how

well, if we don't require our circuit to be deterministic, we can cut some corners and make it more energy efficient and fast.

And so in a lot of algorithms that actually require randomness, like MCMC, for example, or sampling and Bayesian inference,

the algorithm is naturally resilient to errors because randomness is required by the algorithm.

And so we expect physics-based ASICs that get rid of properties like determinism to be well-suited to these kind of algorithms.

And by a very similar argument in a lot of generative AI, like image generation, video generation, materials discovery, which is, for example, generating new molecules,

Often these are solved using diffusion models, which are also randomized algorithms because they're basically simulations of a stochastic differential equation, which is actually similar to the molecular dynamics.

The way that we model a molecule evolving under a heat bath is like a differential equation that is based on all of the forces between atoms and Newton's laws.

And then we add noise.

And similarly, with a diffusion model, the way that we sample an image is by running a stochastic differential equation, which has some drift velocity and then some noise term.

So in some sense, there is even a similarity between diffusion models and molecular dynamics at some algorithmic level.

And that's actually one of the applications we're very interested in, I would say, is diffusion models in particular, because that's

rapidly growing in kind of its usefulness and its computational cost.

And then there's also some work on, we had an earlier paper, Thermodynamic Linear Algebra, where we talked about how you could use a kind of thermodynamic processor to solve problems in linear algebra, like linear systems of equations or inverting matrices, matrix exponentials.

And if you're interested in that, you can check out the paper, Thermodynamic Linear Algebra.

And then our other paper that I actually talked about last time I was on this was thermodynamic Bayesian inference, where we talked about how to use a thermodynamic processing device for sampling from Bayesian posteriors.

So those were some examples of different application areas.

And I tried to give a little bit of intuition for why we think different application areas might map well to physics-based ASICs.

and more kind of I guess even more broadly like why we expect that general purpose digital architectures are kind of overbuilt for a lot of these applications or at least they are built to at great expense to guarantee certain properties which are not needed for for many of these applications so I think that's you know basically what I wanted to say about kind of what physics-based ASICs are and

how we're thinking about algorithm co-design and what we think they're going to be useful for.

I guess we also talked a little about in this paper about kind of where we see the field going and what are some of the next steps, I guess, and this calls to action.

So I'll say a few words about that.

So for one thing, there are going to be algorithms that the GPU is very good at and are probably not going to benefit from, or at least not going to benefit as much from very new unconventional computing paradigms.

But there's also going to be some applications that are not very naturally suited to GPUs.

And so that's one of the first and fundamental things to do in our calls to action is to identify the set of applications that don't map well to GPUs or that GPUs aren't well optimized for and identify why they are not well optimized, why the GPU isn't well optimized for those applications.

and have a good understanding of that.

So that's one kind of call to action.

And then another one, of course, is actually co-designing the algorithms and hardware for physics-based ASICs.

And we give the example here of how transformers have been designed to run well on GPUs.

And so similarly, what's kind of the equivalent of the transformer for

for physics-based ASICs?

Well, maybe we don't know yet.

Maybe it's these energy-based transformers we're looking at.

Well, either way, I think there's a lot of room to explore new algorithms and hardware that will have a good agreement between the algorithm and physics-based ASICs and can be used for AI.

And then, well, there's going to be, yeah, so we talked about like developing the full stack for physics-based ASICs.

So there's basically just a lot of new kinds of layers of software that are going to be needed, like simulation of, so yeah, like first, like how can we use like a GPU to simulate how our physics-based ASIC would perform so that we can kind of solve design problems before we actually build it.

And can we take existing machine learning algorithms and compile them so that they'll run efficiently using our physical primitives on the physics-based ASIC?

That's another thing, kind of a compilation layer.

And so that will be a significant amount of work.

And then I think we also have a note about like, yeah, explaining goals in a way, explaining the field basically in a way that's easy to understand for people from different backgrounds, like electrical engineering or computer science and so on, just because this is a very interdisciplinary field at the moment, which is one of the things that's so exciting and fun about it, I think, is that it's

really very truly like interdisciplinary and combining physics and you know engineering circuit design and theoretical computer science in a very in math and kind of very interesting combination um so yeah i think that that covers everything i wanted to say about the paper and um i guess at this point i'll take any any questions if there were any other questions yeah awesome awesome all right okay


SPEAKER_01:
Here's where that took me, the three uncertainties.

What is a unifying framework for cognitive science?

What is a unifying framework for computer science?

And how are those questions related, especially in the context of cognition as inference slash compute as cognition?

Like, what are those sort of domain or discipline scale syntheses in the cognitive science and the computer science department, so to speak?

And what does that look like as we increasingly see those two used by and for and with each other?

So if you have any thoughts, go for it.

But that was just where it took me.


SPEAKER_00:
Interesting.

So you're asking, what is a unifying framework for cognitive science and a unifying framework for computer science?

And are you also gesturing at a unifying framework that would unify both of those unifying frameworks as well?


SPEAKER_01:
yeah it's like all these branches coming together that you brought together with the authorship and the perspectives in this paper and then over here in cognitive sciences and active inference that's the quest of the unifying cognitive model that attention memory anticipation all these different cognitive phenomena would be approached in a unifying

handling and we're using those probabilistic compute tools along the way and even interpreting what the materials classical computers and unconventional computers what they do as this kind of like material intelligence so it is getting very hot and heavy in that area um


SPEAKER_00:
Yeah, I see.

Yeah, I know.

I definitely like thinking about things like that as well.

I would say that I haven't explored as much like the kind of interactions with the cognitive side, cognitive science side of things, although I know there's a lot of kind of rich, interesting things to think about there as like, you know, for example, like some of

uh you know things in this paradigm people do are things like spiking neural networks and things like that that are at least inspired by and arguably you know behaving in a similar way perhaps to think you know things in the brain um and and that was also something we thought about a little bit in our in our you know thermo bayesian inference paper was uh can this can this somehow be mapped onto ideas about like how you know people reason uh under or you know how

yeah, how people reason under uncertainty, things like that.

So, yeah, I think there probably is a lot to explore there.

Also, yeah, it just goes to show just how kind of like interdisciplinary this field is.

Another thing I think about in terms of like unification, which is a little bit different from that, but may have some overlaps, is like

is like kind of unifying computer science or at least computational complexity theory with thermodynamics in some way, which I think is a kind of interesting idea.

I'm not sure how realistic it is at this point, but there are,

Basically, the reason I think that there is going to be some overlap there and perhaps some unification is that these are some of the two important theories that give us estimates of basically how much work it takes to do something.

So computational complexity theory gives us estimates of how does the amount of time, or you could also think of it as the amount of space or even the amount of energy required scale with

the size of your computation that you have to do basically and similarly in thermodynamics you can derive results that tell you how much like physical work you have to do in meaning like literally the amount of energy that you spend or dissipate in order to uh perform some uh you know or perform some change in the system state and there already are some small like overlaps where this becomes more explicit like of course landauer's principle and similar like generalizations of landauer's principle

that make a link between thermodynamic work and computational operations.

But in general, I think that there is a larger program that could be pursued of mapping statements in computational complexity theory somehow into thermodynamic statements about how much work is required to bring about some change in state of the system, basically.

So I think that's another.

And that could even perhaps overlap with the idea you were mentioning of unifying these

not, you know, exotic computing architectures with cognitive science and stuff like that.

So yeah, I think that's kind of an exciting idea.


SPEAKER_01:
Cool.

I'll ask one more question, then read the questions that people have added in the chat.

So how do you see biology and biological materials in the scheme?

Figure three seems to be mainly abiotic focused.


SPEAKER_00:
uh figure three um sorry what was the last phrase you said it was it was something focused abiotic abiotic where's that sorry these uh are not arising from a living system they're machines or engineered


SPEAKER_01:
though there's some analogy with biological systems, but just how did that come up or where do you see biological processes and materials generation in this scheme that the paper lays out?


SPEAKER_00:
I see, okay.

So I'm going to venture a little bit outside of my wheelhouse here because I'm not an expert in biology by any means.

So that's why I'm a little bit more comfortable drawing analogies between the behaviors of these circuits and generic dynamical systems like a molecule, for example.

But, of course, organisms are also made out of molecules, and even at a higher structural level, organisms might have some similarities in terms of those properties I was mentioning before.

in general yeah i guess yeah one example might be good was that kuramoto model thing i was just talking about uh so there i've seen there's some work on using the kuramoto model which the model of how a bunch of different oscillators can synchronize with each other to model how um synchronization uh can occur in the brain uh which i think has to do with uh

you know brain waves that are that are you know happening with different frequencies although i don't know much about that um but i i think that general the and i'm not sure if people know whether the kuramoto model is actually a good uh or whether it's a well-justified model of that but at any rate the kuramoto model is a decentralized model of synchronization where you have a bunch of things that have you know have pairwise interactions talking to each other instead of one global clock that's coordinating everything and so there we see uh

a physics-based ASIC that kind of gets rid of assumption four might be more similar in architecture to a biological system than it is to something like a GPU.

So I think that that's like one way of answering your question is that I would expect that physics-based ASICs may have more similarities in architecture to biological systems being decentralized, although I don't really know enough about kind of typical biological systems to


SPEAKER_01:
probably a very good answer but uh yeah i think that's one thought i have on it cool okay from the live chat gorto68 wrote cubism is my jam that was just a comment another question max have you noticed a trend in the last eight years in a new physics of geometry and ultra high frequency applications to meta materials and nano machines


SPEAKER_00:
Let's see, I'm going to try the last eight years of metamaterials and nanomachines.

Wait, what was the first part?

Something relating to nanomaterials and metamaterials?


SPEAKER_01:
New physics of geometry and ultra high frequency applications to metamaterials and nanomachines.


SPEAKER_00:
I would say I have not noticed that, but I haven't been very focused on those fields.

Yeah, I haven't noticed it, but that doesn't mean it isn't happening.

But yeah, it sounds interesting.


SPEAKER_01:
What has changed over the last eight years or months or days?

Why is this coming to fruition in your research and development at this time?


SPEAKER_00:
That's a good question.

So one thing that's changed, I guess, in, yeah, let's say the last decade or something, is that quantum computing turned out to be harder than a lot of people thought it was going to be, I think.

And so I remember early on when I was beginning grad school, there were a lot of people

I was kind of working more on the theory of thermodynamics of quantum computers, but there were a lot of people at that time who were working on quantum compilers, things like that, that were supposed to solve the problems of software engineering for a quantum computer.

And that just kind of goes to show how optimistic people were at that point, I think, because people already kind of treated the engineering challenge of building a quantum computer

as a foregone conclusion that would happen in the next decade or something.

And then we would run into this problem of needing to be able to design scalable software for it.

But as far as I know, it didn't quite turn out that way.

And we're still stuck with the problem of actually building the large

quantum computers, and so now people are thinking more about error correction codes and things like that that will basically allow us to scale them more easily.

But I guess that's one reason why, at least in the story of how I became interested in these kind of approaches to computing, that these are unconventional, classical approaches to computing.

that don't have, well, they still have some scalability and engineering challenges associated with them.

They're not the same kinds of challenges that were so hard and still are so hard in quantum computing, I guess.

That would be one answer.

And the other answer, I guess, is kind of related to what I was talking about with the end of Denard scaling, although that happened more like two decades ago.

But arguably, now is when it's kind of being most acutely felt, I guess, the end of Dart scaling, because we're finally getting to the point where the energy costs are just kind of accelerating massively due to our rapidly expanding compute needed for AI.

So those are kind of the two things I would point to, I guess.


SPEAKER_01:
Cool.

Yo, where did the paper take you all and the authors?

Like, where do you go from here?

What milestones are you looking for next?


SPEAKER_00:
Right.

Well, yeah, so at Normal Computing, we're making our own chips.

And so we have milestones for the development of those chips, which have to do with specific AI, generative AI algorithms that we want to run on them.

So that involves simulating how the chips are going to perform and getting out estimates of their performance from our simulations, building the chips.

And we've already made a lot of progress on that.

and uh and then kind of thinking through like how these chips are actually going to be used like commercially uh so that's why when i think about kind of like my piece of this puzzle of um physics-based asics it really has to do with our uh stochastic digital kind of thermodynamic stochastic digital processors we're making at uh at normal computing to solve some of these problems in ai


SPEAKER_01:
very cool uh i guess one sort of last question then feel free to make any comments like what is the educational trajectory how do people learn up on this is it going to look like type theory and programming languages and computer science undergrad or what kinds of background are useful to be learning and getting familiar with to kind of skate to where the puck is going


SPEAKER_00:
right so i would say that for one thing uh you know definitely uh people experienced in you know silicon engineering uh if if people who have a skill set in designing conventional asics and have a deep knowledge of

you know, of integrated circuits from that.

Well, you know, that knowledge will absolutely, I think, transfer over to kind of more unconventional architectures.

And it might be even more might be even more fun to think about because it's kind of throwing away some of the rules that are it's kind of like

solving some of the same kinds of problems but where many of the rules have been thrown away I guess so it becomes kind of a different game so that's one background one way of getting there another which is more which the way that I kind of got here is the kind of more theoretical physics and in particular like statistical physics and thermodynamics end up being very relevant especially because a lot of algorithms and AI have some algorithms and AI have been designed inspired by or based on

ideas in statistical physics.

So like, for example, diffusion models or more generally, like machine learning, there's like Bayesian inference models and Monte Carlo methods that are deeply related to things in statistical physics.

And so

In general, having both knowledge of the algorithms and of the physical pictures that those algorithms map onto kind of puts you in, I think, a good position to think about what might the ideal piece of hardware look like to run this algorithm.

Because you can kind of think about what physical models map onto an algorithm, I guess.

So that's another background that I think is a good root to this.

this way of thinking is like statistical physics uh etc um and then of course uh people with like yeah people with math math backgrounds uh you know math statistics uh computer science um and ai in general are of course all important and especially uh

when we get into things like performance bottlenecks, like knowing the answers to the questions like, what's the most costly part of running this pipeline?

If I'm running a fusion model or I'm running a large language model, where is the majority of the computation being done?

Which is kind of related to theoretical computer science and complexity theory, but isn't quite the same.

This is more of a practical question of, where are the actual bottlenecks in practice?

So, and so, you know, that would be the skill set more of people who do like, you know, large scale machine learning infrastructure or training and inference and stuff like that.

So I think all those like skill sets are, you know, people are in a very good position to kind of move into this field.

I do hope that more and more people do start to get involved in this.

Yeah.


SPEAKER_01:
Cool.

I'm going to read a very interesting related comment, but then feel free to make any last additions there.

Okay.

Talal Zahid wrote,

How digital education will advance with AI systems running on special chips, ASICs, using probabilistic models to personalize learning over time.

As digital education platforms scale globally, delivering personalized and adaptive learning, which requires low latency AI computation.

This research explores the deployment of stochastic AI models on ASICs to achieve real-time adaptation in user learning paths, reducing energy consumption while maximizing student outcomes.


SPEAKER_00:
I see.

That does sound like really interesting research.

And what I like about it is it's an example of kind of connecting the full stack end-to-end in the sense that it talks about the end application, what are some properties of that application, like requiring low latency and adaptive behavior, although I haven't read up very much on the application, but it sounds plausible that those are requirements, and then connecting that to what algorithms we're going to use for it, and in particular, like

you know, large language models or Bayesian reasoning models, and then like what hardware would be required to accelerate those algorithms.

So kind of connecting the all the way from the application to the algorithms to the hardware is exactly the right way of thinking in order to do something meaningful, I think.

So I really like that comment.

Yeah.


SPEAKER_01:
Cool.

And yes, closing the loop with the improvements and the improving improvements.

Any other comments?


SPEAKER_00:
Um, no, I think, yeah, I think I covered everything.

Uh, and yeah, just to get, I really enjoyed coming on.

Uh, so yeah, always, always a pleasure.

Uh, and yeah, thanks for having me on.


SPEAKER_01:
Cool.

Looking forward to the next episode.


SPEAKER_00:
All right.

Sounds good.

Thanks.

See you later.
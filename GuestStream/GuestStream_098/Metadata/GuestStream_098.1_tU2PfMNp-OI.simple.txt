SPEAKER_00:
Hello, welcome.

This is Active Inference Guest Stream number 98.1 on February 10th, 2025.

We will be discussing active inference in human-computer interaction with the authors of this very interesting paper.

So, thank you all for joining, and a pass to the authors for their introduction and presentation.

Thanks again, looking forward to it.


SPEAKER_03:
I'm Roderick Murray-Smith, I'm a professor at the University of Glasgow and the PI of the European Research Council project that's funded this work, the DEFY project.


SPEAKER_02:
John?

I'm John Williamson, I'm a senior lecturer here at the University of Glasgow and also one of the investigators on this project.


SPEAKER_01:
And I'm Sebastian Stein, I'm a research fellow here in Glasgow and also one of the investigators on the project.


SPEAKER_03:
Okay, so we published

on archive review paper on active inference and human computer interaction.

And then the friendly team at the Active Inference Institute said, oh, would you like to give a talk on this?

So we're very honored to be included in the Institute's presentation.

So we're going to give an overview of it.

But if you want to see things in more detail, then at the bottom of the screen, you can see a link to the archive.

publication.

Okay, let's get going.

So we'll cover a bit of motivation for the work described active inference in HCI, what the core elements are.

We'll give an example of a particular case study on ordinal selection, which Sebastian will present.

And that's work that we presented in Oxford at the Active Inference Workshop.

We thought it would be useful to have some concrete examples as part of the paper.

as part of the discussion here, and then some of the challenges for applying this work in the context of human-computer interaction.

So, a little bit of background about where this is all based.

We started a year ago the five-year project called DeFi.

um designing interaction freedom with active inference and the idea was we wanted to look at how we could better build the flexible interfaces of the future that would use richer sensing machine learning um in novel ways um and that has three main elements active inference is going to be the framework for building things we wanted to use

an approach called Optimal Mechanism Design to build the interaction mechanisms that could combine rich sensing, machine learning tools and embed content into computational structures.

And we wanted these mechanisms to also have shared autonomy mechanisms so that the amount of freedom the computer would have could be adapted.

So what we'll be talking about today is the first part of this activity on active inference.

So as a bit of background for the motivation, you can think about, you know, how have humans gone about controlling the world around them?

You know, the very first tools would just be simple rocks and things where the human could use their muscle power to pick up a rock and do something more than with their bare hands.

As time went on, we had tools which would give us some more feedback about the world that the actual

This design of a chisel might make it easier to feel the texture of the wood that you were working with.

And then further development got us to take external power and modulate that, so that could be wind power or steam power.

There's some external source of power which we could use to adapt our tools.

But recent years have moved towards the insertion of external computational power

And you can see the framework that we're working with here, and the grey boxes in this figure are where computational power has been inserted.

So the human can act on an interface, they can perceive the interface, and that computer that they're interacting with can also control the world.

So this could be a

a smart car and it could be tracking the road with its own cameras and augmenting the control to the steering wheel, but it can also be looking at the human and trying to infer their intent, it could be augmenting the display.

So we have a closed-loop interaction with the world and the tool has got external computational power algorithms, artificial intelligence.

So the argument would be that for less effort the human could achieve

the same or more performance and that's kind of a summary of you know what the here the AI was actually standing for artificial intelligence rather than active inference you can take one of these old Don Norman graphs where you're plotting the amount of effort you have to put in against the performance and usually the lower line would be human alone so usually you have to put a fair amount of effort before you get any sort of performance out

artificial intelligence or algorithmic support can lower that threshold so you can get something for far less, but it's always worth having a think about where that gap is being filled from.

Is it an earlier version of you that's somehow been learned, or is it other people like you according to some measure, or is it a more malevolent input from whichever service you're using which is trying to nudge you the way they want you to be?

So anyway, it's worth thinking about that background to tool design.

We're saying that humans are now starting to need to use artificially intelligent tools or algorithmic tools.

So these are going to be the most complex things that we've used to date.

And the tools are no longer going to be passive extensions of ourselves.

They're going to be active partners where they're starting to take on certain tasks and interaction becomes more of a dance than a command and control.

one so active inference appears highly relevant to this element of active tools okay also as part of the context there was we think there's a bit of a crisis in hci so you've got uh key researchers like kornbeck and rosewood highlighting that we have concepts of interaction that talk about design but we fail to produce theories and concepts that have high determinancy and adequate scope so they can't really

do anything in detail and they're not or they're not broad enough if they can do something in detail we haven't in hci been seeing the sort of ratcheting up of performance that you see in subjects like machine learning and computer vision where people are sharing software and building on each other's work and there are key questions about agency which is core to understanding

how we are going to be relating to algorithmic support, but how do we go about measuring that?

How do we go about checking whether our agency is being taken away from us by computers?

So given that active inference is a closed loop computational theory for modeling agent behavior, it seems very relevant to discussing these questions.

So you've got this elegant unifying theory.

It allows us to incorporate machine learning, Bayesian inferencing, probabilistic programming, and dynamic systems.

We can work out from low-level physical interactions to high-level reasoning decision tasks, so it seems like it's potentially going to be something that could support the next generation of human-computer systems.

So, three elements that are relevant for that is in human-computer interaction, you've got a lot of uncertainty.

Humans are very variable, their context is complex and variable, so we need to have

models that can cope with uncertainty.

Active inference is completely probabilistic and views agents as entities performing Bayesian inference, so that's quite appropriate.

Humans are also intelligent agents.

They're inherently predictive.

They're trying to understand the world around them and act in a way that is based on predictive models.

Active inference supports this sort of analysis, so we have a predictive element.

We also want to balance the exploitation of the knowledge that we have with exploration to try and reduce future uncertainty.

And that's something which Active Inference gives us support on as well, unifying the reasoning about perception and reasoning about actions in a single framework.

So let's get into Active Inference and HCI.

So your traditional Active Inference framework was often set in a more natural context rather than a design context.

So you might have

intelligent agent in this case we have our human user but it could be any other biological agent and it's interacting with an environment and it's as action states that it can sense this environment and make predictions about what it's going to sense given its actions on that environment so with active inference in a human computer interaction context there are several major configurations here then three you've got

an active inference model of the human interacting with some system.

It could be anything.

So when we're saying system, we're talking about the computer and its interface.

You could have the computer and its interface represented by an active inference model interacting with a user who's unknown.

You could have the active inference model transducing the information between the user and the computer.

So

we can split things up into offline and online use.

So the description approach is saying,

we've got a model of a user operating under active inference principles to reason about how the user is going to behave.

You could have the system as just its environment, and you could use that at design time to simulate user behavior and optimize your designs, or you could have it at evaluation time to interpret observed interaction behaviors.

Mutual interaction is where we're modeling both the human agent and the computer agent.

So we're looking at a model of the whole interaction loop and we have two interaction agents working together.

So here you see one agent has become the environment for the other.

So the human motor states are then perceived by the computer sensor states and the computer's action states could be

changing its display, making noises, sending vibrations, moving a robot arm.

These are perceived by the perception states of the user.

Okay, so the major change from the usual presentation is we're designing the environment of one agent with another agent.

Okay, in terms of online,

options we've got a user interacting directly with the active inference agent this is a constructive approach and it can be predicting the user's behavior and acting appropriately so the user is part of the environment but might not be explicitly modeled in terms of theory of mind we'll get to that in a second and the other one is the transduction one so

you can have an active inference agent lying between a user and an existing system, so it's mediating their interaction, and neither of them are explicitly assumed to be active inference agents.

So the user and the system jointly form the environment of the mediating agent.

And all of the models that we've been discussing so far could be augmented by explicitly incorporating an active inference model of the partner within their generative forward model.

So we've called this a reflective approach.

You're constructing an active inference system that incorporates an active inference model of a user.

So you see the embedding of another active inference model within its system predictions.

And this ties into work that's been going on in

other areas of work and theory of mind in terms of levels of mental modeling where you could basically have different levels of higher order cognitive modeling of mutual theories of mind between humans and agents.

So we're now going to go back to the transduction work that we mentioned here and have a specific use case.

So Sebastian do you want to take over?


SPEAKER_01:
Yeah, so this is a use case where we look at what's called noisy ordinal selection.

So in this case, we were looking at using active inference as a transducer, as suggested in one of the options that we might want to use active inference for in human computer interaction.

And this is work that we presented at the active inference workshop last year in Oxford.

So here we're looking at a rather abstract human-computer interaction setting, which you can think of as a two-player game where one player, in this case the user, would think of a number and the other player, in this case the interface, attempts to guess that number and presents one number in each interaction time step back to the user.

The user could then give feedback in the form of saying that the number that they're thinking of is either higher or lower than the number presented.

But that feedback is then corrupted by some amount of noise where the feedback below might be flipped into feedback higher and vice versa, potentially with different rates.

And that's a sort of scenario that's rather relevant for

sort of naturally ordered symbols like alphabets or sort of imposed orderings like we have in things like drop-down menus.

And the sort of channel characteristics of having very noisy feedback is quite commonly seen in interfaces like brain-computer interfaces or other assistive technologies like electromyography.

where we usually have rather low classification rates and have to deal with high levels of noise.

But we have the possibility to have rather noise-free sort of feedback in the form of a display back to the user.

So here's one example of how an interaction might unfold over time.

So time is on the x-axis here, and the different targets presented to the user are indicated by the agents.

trace presented by the blue line here.

In this case, the user is thinking of the number 4, and in the first time step all the way on the left, the agent or the interface is presenting to the user the number 16.

The user feedback is indicated by the triangles pointing up or down, indicating that the feedback received by the interface was either lower, as in the case of this first time step, or higher, as in the case of the second time step.

What you see here is that

I think in terms of five, it is where the feedback given by the user is the first corrupted feedback, at which point the agent flips effectively its exploration area to sort of higher numbers.

And we'll see later why that is.

And in particular here, the agent flips its belief to be one where the user is responding with comparably high error rates.

But we see that over time, the system recovers and the agent can actually correctly infer that the user's target is at or near the true target number four.

Okay, so there are some classic information theoretic approaches that we could apply here.

which have been applied in previous work, but they are only optimal under a set of strong assumptions.

For example, the Horsestein's posterior matching algorithm is only applicable if we assume that the noise is actually IID fixed and known in advance, and that the symbol set is arbitrarily large, as well as the feedback channel being noise-streamed.

So we were interested in seeing whether active inference can help relax those assumptions.

In order to do that, we translated the problem into an active inference formulation where we represent the user's intended target as well as the error rates as the latent state of the system that we would like to infer.

And the observations that the agent can make about the system, its reasoning about the user, are these binary feedback symbols, which correspond to their number being lower or higher than what the agent has displayed.

And the agent's action space is effectively the set of symbols that the user might want to select.

In order to apply active inference, we need a probabilistic forward model, so the system the agent interacts with, which in this case is the state transitions of this latent state as a function of the agent's action in the previous state.

So that's kind of answering the question, how might the user's target and the noise vary over time?

And we need a probabilistic forward model of how the user feedback is generated.

So answering the question, what symbol

what I observe if the user intended to communicate that the target they're thinking of is higher.

And so in order to select the next action, we need to inherently trade off between learning about the user's intent and learning about the environment characteristics of the error rates of the channel.

So commonly we use Gaussian distributions to represent the belief about the

the latent state in active inference agents, but Gaussian distributions are not particularly helpful here.

That is because a sequence of user interactions is usually compatible with distant and mutually exclusive beliefs.

So if we take an example where the agent displayed the number 20 and the feedback received was that the user's number is lower than that, and that is compatible with

the user's target actually being lower and the channel having a rather low error rate, but it's also compatible with a scenario where the user's target is actually higher and the channel has a higher error rate.

So in order to represent a potentially large number of these modes in our belief distribution, we decided to implement a particle filter where the belief distribution is then a weighted combination of

of point masses in this space of latent states.

But the K divergence can still be computed rather efficiently and quickly just by using the multinomial distribution represented by the weights in this case.

So going back to the example that we saw in the beginning, I want to now look at how the agent reasons about the latent state

in particular the error rates and the user target.

And we're going to first look at the error rate, which in this case we assume to be symmetric.

So it's equally likely to flip a signal lower into one that's suggesting that the target is higher and vice versa.

And here you can see around the time step where the first errors occur, the agent belief about the error rate is sort of changing from a lot of mass accumulated

mass and the belief distribution aggregating around high error rates.

So I should have said before, the color in this case represents the probability density within sort of local regions around these sort of discretized levels of error rate.

And the true error rate in this case is 20% as indicated by the red dashed line.

And so over time, we see that the

agent's belief about the error rate converges to somewhere close to the true to error rate of the system.

Next slide.

Now looking at the marginal distribution about the user target, we see that initially the agent has rather broadly spread out density, meaning that there's relatively low evidence for specific targets over the first few time steps.

But as the agent learns more and more about

the error rate with which the user is communicating, it's unable to identify the target correctly towards interaction step 35 and onwards.

So we did quite a comprehensive set of evaluation experiments, which are detailed in the paper, but just to highlight a few here, I want to present first what we've done in terms of simulating experiments and then present some of the key results.

So we're always assuming that the channel statistics are symmetric in this case, just to simplify the analysis.

And we're looking at scenarios where the user could select between one out of 32 different targets.

And we repeated simulations where the simulated user actually imagines each of the 32 different targets 10 times and simulated for 500 interaction time steps where after each

every hundred time steps we reset the distribution over the user targets to be uniform, which is simulating the behavior over five successive interaction episodes.

So one of the main findings is that even in the absence of knowing about the error rate of the communication channel, the active inference agent can make decisions as well or better than

the Holstein algorithm that I've alluded to earlier after the first 300 time steps when the agent has learned enough about the error rate to compensate.

And we've also done some experiments indicated on the right-hand side of this slide where we looked at the error rate actually changing over time.

And again, with the heat map displayed here, you can see that the agent's belief can track

the changes in the true error rate of the user.

So just to summarize, we presented a concrete example for using an active inference agent to do reliable one-off end selection and showed its effectiveness in inferring both the control polarity and working with non-stationary channel statistics.

And we can bring in additional

assumptions and further domain knowledge in a setup that's much more specific than this one.


SPEAKER_03:
Thank you.

Okay, so let's talk a bit about core elements of active inference in HCI.

So the key aspects of an active inference agent are listed below.

We've already discussed the idea of the agents and what entities are going to be modeled and implemented as active inference agents, the users, the systems, both, neither.

We need to look at the role of the environment.

Is it just a transmission medium between the agents?

Is the user using the system as an instrument to act on it, or are they both fighting environmental flux operatively?

We want to tie in some discussion about Markov blankets.

How do we separate agents from their environment?

Work out the actions that each agent is capable of and be able to define boundaries between the user and the system.

Forward models are obviously key to active inference, so we have to be able to create a generative forward model.

And that's often going to be an implementation step that's very challenging, because we have to be able to predict future states, synthesize the observations that we would see.

We're going to look at how we predict and reason about the next big action, because we need to use the forward predictive models to estimate future states and predicate actions on the estimates.

We have to think about the preference pile prior structure because the agent's goals or desires are going to be defined as a distribution rather than as a single reference value or control law.

So we're going to have to think about how we can shape these priors to align with the goal of interaction design.

So it's this change from trying to describe the natural world to trying to design and create a system or a world that's going to be better for the humans involved.

And then there's all the practical implementation questions around building and training the forward model, implementing your rollout policy, getting your inference mechanisms to work and perform Bayesian updates and the challenges associated with that.

So let's start with the environment.

Here's a sort of cartoon of the basic ways in which the environment can play a role.

You might have the user in the system who is directly engaging with the environment.

Can they do this?

manage the variability in the environment cooperatively.

So an example of that could be that you might be trying to use a smart speaker and the users try to operate that to change the mood and observable behaviour of a party in their flat.

But the user can also interact directly with others in this party and they can bring their observations of that together to pick the next best track.

Similarly, a central heating system

might have an application for controlling the temperature in the building but the user can also make changes like opening or closing windows.

We might have a transmission approach so that we were just describing the environment in terms of to what extent it affects the communication between the user and the system.

So if you have an environment with lots of background noise then any audio

display by the system may be difficult for the user to hear.

Similarly, bright sunlight on the screen might make the screen visible to the user, so we need to understand the impact of the environment on the communication between the user and the system.

We may have the user applying the system as some sort of intermediary instrument to act on or better sense the environment.

It could be transducing user unobservable information from the environment, or it could be acting on the environment at the behest of the user.

We haven't discussed multi-agent environments, but they would be a natural follow-on where we start to have multiple explicitly modelled agents acting together.

Okay.

An interesting question in HCI is where the boundaries are.

And obviously the active inference community has focused quite a lot on the notion of Markov blankets.

But HCI has often struggled a bit with this.

So the very name human-computer interaction suggests that we're assuming there's a clear boundary between a human and the computer.

And people might often potentially naively associate that with Clark's skin bag around the physical limitation of the human.

and we know that if you're going to understand the system then finding the boundaries between that system and its environment seems to be an essential for analysis of it but these the mark of blankets that describe the limits of one agent may change dynamically as we give and take control that could be driven by technology so we might give a user a tool or an assistive technology and we can transform what they can act and sense

so that if you have somebody wearing augmented reality glasses or virtual reality, then we can change how they're interacting with the world.

We can let them see things that would otherwise be invisible, and we may be able to give people the ability to act in the world in a different way.

So, modern tools will have context-sensitive algorithmic support from algorithms, from AI,

These could augment the user cognitively or it might relieve them the need to attend to or check on mundane tasks.

And so that could be seen as taking or augmenting agency depending on how it's designed.

But basically you can imagine that as we're taking over tasks from the human that the

The states which previously were being controlled by them are now being handed over to the agent to work on.

So we're seeing an adaptation of the Markov blanket, depending on how much support we're giving the human.

So we've been evolving human tools from rocks to AI, as we mentioned earlier.

We're getting this trajectory of increasingly powerful effort leverage, doing things better, doing more new things.

And so that means that we're increasing our individual collective intelligence, we're extending our natural selves with our tools.

And although we started off with crude functional tools like hammers and axes, we've moved to tools which have controllers in them like thermostats and governor technologies based on

on a controller approach.

And then each time you're giving some of your agency away for understood outcomes, and the tools can take on more and more of our tasks.

But the question is, are we at a point where we're potentially giving up really important bits of ourselves?

Okay, so arguments have been made in the active interest community that to maintain the integrity of the agency blanket needs to change, and there's several

papers making arguments about that you need to adjust yourself as your as your environment changes in order to keep your internal states stable and we're looking at the basic approach that we were showing earlier we have the markov blanket with its sensory and actuation nodes internal and external states but i think one of the interesting things that we could be

pulling in from machine learning is the notion of attention, so that at different points in the interaction cycle, the human may be actually only paying attention to a very limited subset of the sensory states.

So we have an effectively different mark of blanket, even though the human could potentially perceive things on the states that they're ignoring.

And this can, of course, be structured into a hierarchical setup.

So this use of attention means that we could be tracking, with our closed loop models, we could be going through interaction processes and looking at what the human pays attention to using the standard Bayesian methods to infer Markov blankets at different points in time and see what is the human ignoring at certain time points.

And what does that mean for system design?

Other research within HCI has been raising similar questions, although not from an active inference perspective.

So there was a paper by Satyana and Jones at MIT.

One's a computer scientist in an HCI context, the other's an anthropologist, and they were looking at intelligence as agency.

and looking at dyads of co-fused agents and looking at the line of control shifting between them.

Where they say compose, we would say acting on the environment, and when they say interpret, we would think of that as sensing the environment.

And a lot of their interest was in different contexts.

When is it good to give up agency on your control side and on your perception side?

And they were arguing that in certain cultures,

One is more appropriate than the other, but it's not consistent across cultures.

They were also speculating of what happens this year.

AI is taking over more and more control over the state of things, as you can see in the bottom right corner, the sort of Pac-Man figure where the green is gradually eating up the blue of the human.

And so you can think about that relating to Markov blankets and saying, well, you know, which states is it important for the human to continue to have control over?

OK, we mentioned forward models.

in active inference in a human-computer interaction context we're going to need to have forward models of human perception, we're going to have to have forward models of the computer sensing process, we're also going to need to be able to predict the human motor control system and what it's capable of, and the reliability of the computer displays given the environment.

An interesting observation is obviously that active inference focuses on forward models and it doesn't need explicit inverse models which is

a difference from other contexts.

Okay, so we want to look a bit at prediction.

So active inference agents aren't reacting directly to the sensations, they're reacting to their beliefs about future states of the world, and the observations are there to correct the estimates of these beliefs.

So we're trying to minimize our future surprise.

And you've got the

the standard approaches of expected free energy and potentially breaking that down as information gain and the pragmatic value.

And the interesting question is how humans are going to experience this.

So if a system is acting to gain information about them,

They may experience it as a proactive system that's giving them a personalized way to interact, or it may be that they find that the system is moving into the uncanny valley where they're feeling distinctly creepy as it tries to pick their brains about what sort of a user they are.

So it's going to be interesting to see how the implications of curiosity and the information gain term are perceived by users when they start to interact with the system.

But we also think that the whole approach of using surprise minimization as a guiding principle in interaction design is going to give us a novel perspective on the design process and may lead to some surprising outcomes in terms of final system design.

OK, preference priors are obviously another key element of active inference.

And researchers in HCI have highlighted that

most concepts of interaction don't tell us much about how intentions are formed or affected by interaction and it's interesting to think about how active inference manages this how the preference priors are going to be designed because obviously our system's behavior is going to be a function of the preference priors that we give the system and what we assume about the user

And there's more recent work in Active Influence about adaptation of preference priors during interaction and it's going to be interesting to think about how that models, but that seems like an area that's still quite open for development and we'd be curious to hear how the community's evolving in that direction.

Okay.

And there's, you know, a lot of people in human-computer interaction have been used to using objective reward functions in reinforcement learning.

And so there's interest in contrasting the active inference approach misdistribution of preference priors compared to classical control laws.

Okay, challenges.

It's not going to be easy, obviously.

There are huge computational challenges in active inference with the sorts of complexity of system we're looking at.

We're going to have all the complexity of action rollout.

We're going to have to balance the performance of the final system with the real time requirements of online applications.

And it's going to be interesting to see how much we can continue to use sampling and tree search and how much we have to move towards amortized algorithms where we're learning a mapping from offline

behaviour.

Even if we can't get real time work going, we do still think that offline analysis of these mutually interacting right of entrance models can be interesting for the design side.

We're essentially creating digital twins of our human and system and observing how they interact with each other.

Another challenge

we feel at the moment is that the software infrastructure for active inference is still lagging a long way behind, for example, machine learning.

So having easy to share standardized software libraries that people are familiar with and having the correct computational abstractions to build and reason about interfaces is not there yet.

So we need to develop workflows where we can actually apply active inference to interaction in a predictable way.

As we just mentioned, preference modeling is a new perspective for people in human-computer interaction in terms of using distributions.

So we've got these flexible frameworks, but they're unfamiliar to designers, and the consequences of changes to these distributions will have significant effects.

So how do we work with that?

How does it manage the uncertainty in the heterogeneity of human behavior?

And in general with interfaces representing uncertainty has the potential to make interfaces more robust because we know that we have that uncertainty there.

Ignoring it is not an option.

But if we create probabilistic interfaces, users may struggle to understand what's going on.

So creating the right metaphors, mapping the uncertainty into usable feedback for the users is going to be a major challenge.

Prediction is obviously key for active inference and that can potentially mitigate latency in the interaction loop, but that predictive aspect of it could be unsettling for users.

It may make the interface feel like it's in this uncanny valley where it's sort of human, but not quite.

We've been speculating as if we have a pair of active inference agents

they're going to be trying to push the joint system into a mutually predictable region, so that even if their predictive and generative models are not yet completely capable over the entire range, they're going to be, if active inference is the right framework to think about things, then the two agents are going to be pushing each other into a mutually predictable space.

So that could mitigate some of the challenges that we've just been outlining.

In terms of preference, we're going to have to develop how we elicitate and validate present preference priors for design purposes, how we can get generalized preferences from individual user preferences, but potentially creating hierarchies of preferences.

Okay, lots of the criticism of

active inference approach from the HCI community would be similar to other computational simulation approaches.

You've got the cost and complexity of developing models, the general difficulty of creating models which can represent the cognitive and perceptual complexity of humans, and especially the sensitivity of human behavior to the fine details of context.

But, you know, there's something that's been around, a challenge that's been around for a long time.

Thomas Sheridan, one of the

people who have been working in this field since the 60s has made the point that actually the process of modelling forces you to really think hard about what you're considering and to ask and answer questions about what the essential features of the structure and function are.

So in terms of changing the approach in HCI to doing science, forcing yourself to create good generative models is an important part of thinking about and understanding the problem.

So the modeling that's required by active inference requires you to think about what are really important features, and if you can build generative models that validate against user behavior, you've got an acid test of whether you really understand that interactive system.

But of course, any adaptive intelligent system is going to have to deal with co-adaptation of users and you're going to have to account for recursive theories of mind.

So we're facing really challenging things because even humans sometimes struggle with predicting what the people they're interacting with are actually thinking and what's going on in their heads.

Okay, so active inference faces the challenges, but you've got that inherent capacity to model both the user and the computer in an interaction loop.

and then try to approach the problems computationally.

Okay, so in general, we're seeing active inference as an example of the broader area of simulation intelligence in HCI.

If you haven't heard of the term simulation intelligence, there's a very good archive paper on it.

It's about 80 pages long, but it goes through how you can bring different types of simulation model into

understanding the world and making better models of it.

And we see active inference as very much an example of this.

You've got a number of different fields coming together, which are letting you build a computational human computer interaction approach that can include

um interdisciplinary approaches you've got probabilistic programming and predictive models you've got physics informed machine learning you need to take dynamics and control systems into account and you've got the physiology and cognitive science of the human side of it so you're simulating the whole interaction loop from perception cognition motor control sensors computation display um and you're built doing that in a way that's computational computationally implementable and testable

And we had some previous work based on non-active inference approaches, which was published at the UIST conference over a couple of years.

And this is us simulating human motor control and perceptual systems.

So there's a simulated ocular model.

simulated muscle models and then that was using reinforcement learning to perform certain tasks and it was the user in a box model that you could simulate we would like to extend that work to include active inference for the future okay so one of the things that we're arguing is an important part of this framework is that having active inference models of both the human and the the system lets us

model the intertwined action of human and system um and it's avoiding the problems that were seen when people tried to bring classical control models in where you could exchange building blocks in and out assuming nothing else would change because already in the 1960s mature as colleagues and with their crossover model demonstrated that human pilots would adapt their behavior

to shape the properties of the closed loop so that if the aircraft properties changed, they would become different pilots, they would behave differently.

So the closed loop system had a consistent shape.

And Holneigl's joint cognitive systems approach highlights the need to examine the joint system rather than individual blocks.

So he was viewing the human as an implicit model of the process they were controlling.

So we can't understand interaction either from the user or the technology in isolation.

You have to have this joint model.

And we think that the sort of active inference mutual interaction model are going to be required to actually model concepts like agency or engagement and interaction.

So let's have a look at our outlook.

We moved into this area because

generative modeling is at the core of active inference and the massive advances that have been made recently in generative machine learning make it possible to do the sort of analysis of the human perceptual system that would have been impossible even 10 years ago.

So the whole general, before we started working with active inference, we were using things like perceptual control theory and developed systems, so we were taking a closed-loop approach

are looking at empowerment approaches, Polanyi's work, where you're modeling the channel capacity from the actuation to the sensing.

But these control-based approaches will only become realistic if you can model human-level perception, and that's what machine learning has provided us with in recent years.

Active inferences, flexible preference priors, and continuously adaptive behavior we think is going to be well-placed to support AI tool use.

And we've highlighted the implications for HCI theory.

So having this whole loop model gives us the potential to model agency and engagement, although there are going to be challenges in getting there.

And active inference gives us this probabilistic predicted unified approach to closed loop modeling that we think could be quite a clean representation of the problems.

We've highlighted that there are major computational challenges in getting things going, especially in real time.

If it's a good theory of behavior, but not yet practical for real time use, could it be used as a better user in a box model to help us in the design process offline?

And there are going to be interesting questions about when to amortize the computation into some hard coded nonlinear network.

But of course, we've given you one example during this talk of an interactive application, but we want to branch out within our project and test it in a wider range of interactive applications, because that's how we think we're going to hit progress.

So that's us hitting the end.

And I'm going to just put up one thing that we had good feedback from reviewers about was our one page visualization of active inference.

And you can see that developed in the actual archive paper itself.

But some people thought that was a nice way to present it.

So we'll leave that up while you ask questions.


SPEAKER_00:
Thank you.

Awesome.

All right, Arun, want to begin with some reflections or questions?


SPEAKER_04:
Yeah, first of all, thanks very much for your talk.

It was really interesting.

I'd also like to echo what the reviewers said.

This is a fantastic introduction to active inference if you do not know how it works.

Really clear figures, clear explanations.

If you're about to start reading, say, the PAR textbook, I would say have a look at this first and then go to the textbook.

It's a really just nice primer.

So thank you for that.

Like, genuinely,

education and communication of how active inference works is really hard.

So it's really nice to see good examples of how to communicate those ideas very clearly.

In terms of my questions, I have loads.

Daniel, if there are questions from the chat, definitely interject and put those in as well, or I will just monopolize all of the time.

But to start with, I think what I wanted to

start with is, in terms of the alternatives to active inference that you experienced before, what were their key limitations that you think active inference is completely overriding?

And what are the limitations where active inference is also going to struggle?


SPEAKER_03:
I did mention the things like perceptual control theory, where you're seeing behaviours

the control of perception.

So there's a lot of the general ethos of active inference already in there, but they were not, so behaviour, perceptual control theory wasn't explicitly using a probabilistic approach.

And it had, you know, a standards reward function.

So there are lots of, you know, lots of different areas where people have got elements of it.

So the active or the empowerment approach where you're modelling the channel capacity from your actuators to your

sensors that's essentially saying how much can I put into the world and predict what's going to come so that's relating to generative models so there are similar points there but active inference has a nice coherent framework and we're still trying to understand ourselves what the relative merits of you know the energy and expected free energy and

the energy of the future and so on, all the variants that could come in there as an actual cost function are.

So it feels like there's something interesting there with the preference priors giving you more flexibility, but we need to see that making an improvement on a concrete case and understand the difficulty of how to specify those priors, because when you're not just trying to describe a natural system,

but actually create and design a system that's never been made before, you have different possibilities and it's potentially taking active inference out of its comfort zone in terms of not just describing natural things, but saying, is this a good design approach?

So I'm not sure that's a good question, but if John or Sebastian have other ideas to answer it better.


SPEAKER_02:
Yeah, we've done quite a lot of work in, say, Bayesian models in HCI, but that's usually about how we model the absorption of information from the environment solves the perception problem.

And one of the nice things about active inference is that it also includes the acting part of its framework.

So it really has the whole interaction loop can be unified in that model.

I think that's one of the big advantages.

I think also the predictive aspect.

So not always being about modeling what is happening now.

That is really important in human interactions.

You have to deal with latency and the fact that people are also making mutual predictions.

Most of the models don't explicitly account for that.

And I think that's something which active inference does very well.


SPEAKER_03:
And there's, you know, so there are some approaches like computational rationality where people focus on every area will have its own focal points and

computational rationality there's a lot of focus on what are the cognitive bounds or the physical bounds of the human how does the environment have you know constraints and these would all be represented by the generative models um an active inference also the community has focuses on more on say the markov blanket aspect which is maybe underplayed in other areas and so that's

interesting, from an analytic perspective, if you're trying to do understand things or systems and say, well, what is the system?

And how does that change over time?

And how should we infer that?

So, again, how do we, we can take the general ideas of the mark of blank, but actually making that work reliably doing the statistical analysis to be able to actually say, Okay, no, we can actually

in furthest but actually how much data do we need to do that and how reliable is it going to be seems to still be an open question but it's that's an aspect that's not been a focal point of other areas so


SPEAKER_04:
Fantastic.

Next question.

But maybe we could also talk about the vignettes at the end of the paper in the appendix, because I think they were very instructive.

But I'll ask this question and then maybe you could talk about the vignettes in relation to this question, which is what do you see as a difference between computation and cognition when it comes to these sorts of computer assistants for humans to use?


SPEAKER_03:
I'm not sure I understand

the question, but if any of you understand it better or think you understand it, then have a go.


SPEAKER_02:
I'm not sure I understand either.


SPEAKER_04:
Sorry.

Maybe I'll come up with an example.

So let's say I have a calculator that as a tool can help me compute, but I wouldn't necessarily say it's taking away.

It's not shifting that line of control that you spoke about before.

Whereas we could extend the sort of this creative spectrum, say, where there's a tool where, let's say, it's a large language model powered chatbot where I could ask it to tell me something or do something for me.


SPEAKER_03:
And that's going to... So let's take the example of the smart car.

So you might have

your smart car in a full manual mode, and then you're having to pay attention to everything in the world.

And, you know, you're having to make sure that you stay in your lane, that you keep your speeds, um, you know, appropriate for the other cars around you and safe and so on.

And you have to be aware of the, uh, how slippy the road is, icy or wet or dry or sandy.

And if you move into a semi-autonomous mode where the system starts to do lane following.

you are now relieved of the, you know, the computer can do the computation of tracking the white lines in the road and making sure that you're in a safe distance from them on both sides and safe distance from the car in front.

So you're relieved of having to monitor those states in the world and you can potentially use that spare capacity to do something else and think a bit more about where you're actually trying to go or whether to change your route and so on.

That would be an example of giving up one bit of agency so that you could focus on others.

We had the smart speaker example and that could be combined with the car.

So there the smart speaker could make decisions about what to play.

Now you could have something like a controller dial where you could gradually move through different genres of content and have much more control of what exactly was going to come up next.

But if, for example, you're trying to control your car while also picking music, you maybe need to give more autonomy to the music player and just let it play something roughly in your direction rather than trying to micromanage it because you've got to keep your eyes on the road.

So it's this divided attention.

What are you paying attention to at what time and what's a reasonable amount of attention to pay that?

It would be unsafe to focus on the music system when you're supposed to be driving.

And so active inference could help in predicting what you're doing, what your loads are, what your limitations are.

And if you're spending too much time paying attention to the interface, it needs to take control and take that away from you so that you can focus on the road.

Does that make sense?


SPEAKER_04:
Yes, yes.

I'm just going to go through my list sort of in the order as we spoke about some things, and I'll also pull some stuff from the review paper as well.

But yeah, if you feel like there's anything else that you didn't get a chance to speak about that you'd like to put in, then by all means do so.

So my next question is on the uncanny valley aspect that you mentioned before.

I think one of the vignettes was talking about, I think, referred to as the Seal Paro product.

not necessarily by name but this idea of a robot that can anticipate your emotions and your emotional state and try and behave in a way to make you feel better right and that's that's a product that has historically i think the designers of that said they started with a dog

And the reason the dog was not successful as a robot product to help people feel better was because people have very strong prior beliefs about what a dog is, how it looks, how it feels, how it sounds, how it should behave.

And that immediately put it in the Uncanny Valley because it sort of looks like that, but it's not.

And then that difference between your observation and your priors presumably gives you a very large free energy gap, and that is uncomfortable.

So I think it's very interesting to see how

if you sort of make your robot more predictive and able to handle uncertainty, how that would actually create an uncanny valley effect.

In my mind, it would actually reduce that because it's able to better match your priors if it has a model of you.


SPEAKER_03:
Does that make sense?

Yes, indeed.

it's going to be obviously, as you, you know, if we get to the goal of, you know, fantastic model of everything, then, you know, then hopefully things would be good.

But the, as you're moving towards that goal, there's going to be mismatches where it's slightly wrong and it's whether people find those glitches on the way to, you know, so if you had somebody who's

willing to stick with it, no matter how creepy it was on the way, then fine.

But it may be that some of the early phases of that learning process make it, you know, it's got some elements of prediction, but it's creepy enough that you just don't want to touch it.

It feels like, ah, maybe it wriggles the wrong way.

you just go, oh my God, what is this alien thing?

And I don't want to touch it.

And so it never gets the rest of the information it needs to improve its model.

And it's the same in general with HCI.

So we were talking about this in Copenhagen

a week or two ago and there was a question about are designed agents compatible with the free energy principle because you know they don't die and so on but any app which is not good is going to get binned and any product any robot that is too creepy is going to be switched off and ignored so death for an agent is being ignored and being thrown in the bin

Essentially, so it's trying to do things to avoid that drop in its ability to act in the world.

So it wants to, if it's been designed to work well with the human, then it should, you know, the human's preferences should be key to it.

but it also wants to be able to continue to work with the human in order to help the human.

And if it does something which gets it bend, then it's never going to be able to improve

So that's a very black hole in its predictive horizon that if it thinks there's a chance that you could be binned for behaving this way, that's going to be something it wants to avoid.

So I guess the question is, will we hit uncanny valleys on the way to perfection when we're still making errors and we haven't quite got the model?

Or is it just already very weird if

if humans, you know, for humans seeing a computational object that seems to be getting them so well, will that already, you know, be something that throws people?

And they don't, you know, it's partly also how quickly can the human adapt their model of the world?

So there may be some humans who've never experienced a machine that could understand them the way this one does, and that's already hard for them to adjust to.


UNKNOWN:
So


SPEAKER_03:
They just say, no, this is not a world I want to be in where machines behave like this.

So that's, I think we're going to see different things from different cultures, different communities, depending on your upbringing, what your expectations are, whether you can grasp a machine that seems to understand you in some ways better than you do yourself, because it may be able to measure

aspects of your behaviour and compare that to huge numbers of other people and maybe it picks out some things that you're not aware of yet, which would be potentially scary for some people.


SPEAKER_04:
Absolutely.

There's definitely very strong ethical considerations going into this, especially considering what do you explicitly model and what do you implicitly model.

And as you alluded to there, the group priors and then going for specific priors on an individual user.

Certainly what comes to mind when you're talking about products and going extinct, the extinction pressure is

One tool that did not have, to my knowledge, at least any predictive power, but was very much largely a rule-based tool was the Microsoft Clippy Assistant when you were typing in Microsoft Word and it goes, oh, it looks like you're trying to write a letter, which I think universally everybody found not necessarily creepy, but just incredibly irritating.

So I think that's where I sort of see us moving away from is these very like simplistic assistants with very limited perception.

We go to something that's more probabilistic

Oh, sorry.


SPEAKER_03:
The original work there was the Bayesian reasoning team and Eric Horvitz and co were doing the original work, but I think the problem there was it was taken away from the Bayesian reasoning team and given to a product team who just said, bugger that, we're going to clip all these things and make it much more straightforward.

And then they didn't have the richness to actually cope with the complexity of the environment.


SPEAKER_04:
Could I ask a question about the experiences your team has had in actually implementing Active Inference, the noisy ordinal task selection experiments as well.

So I know part of the challenges that you raised was sort of the

not necessarily mature software toolboxes that we have for implementing Active Inference.

So things like PyMDP, RxInfer, these are probably the most advanced implementations out there for Active Inference.

Do you mind if I ask what your team was using and where you see the biggest challenges from the software implementation side?


SPEAKER_01:
Yeah, so we looked at PyMDP and we did use it in some preliminary experiments, but at the moment, or at least at the time we were looking at it, it was very much limited to discrete probability distributions.

And so as soon as you get into very high dimensional state spaces, you reach the challenge that you'll have a combinatorial explosion of discrete bins in that state space.

And so it would become very, very costly very quickly.

And so we were looking at alternative approaches and we don't use Julia at all.

So RxInfer wasn't really something we were looking at in much detail.

And so we just decided to start implementing some of these things ourselves.

And because we've been working with probabilistic inference tools before, we thought we were sufficiently capable of doing that.

But then actually, you know, deciphering the active inference equations, the expected free energy and so on.

from the book was more challenging than we thought, I think.

And so in part, it was also a useful exercise to actually do the programming ourselves to see, you know, do things actually work the way they should and that we're implementing the right thing here.

Yeah, so the short answer is we use our own implementations, but very much with the idea that, you know, as we work through more and more specific examples, we end up with more robust implementations where we hopefully can, you know, slot in

categorical distributions and swap them out for particle filters or just Gaussian distributions and different inference algorithms, always depending on what's demanded by the individual application that we want to build.


SPEAKER_03:
Daniel, I think you can probably speak more to that.

We've been surprised at how challenging it is to do with things that we think should be simple.

So it's been taking us longer to get,

We're building a basic mouse model, expanding.

We had predictive control-based models of mouse movement in previous papers before we started using active inference, and we thought it would be relatively straightforward to take the active inference approach to that, but it's taken us longer to get things going there than we expected.

implementing things in continuous space is interesting.

And as soon as you get something concrete, you start to go, OK, so what should the preference priors be on this?

So is it the selection process?

Is it the movement process?

Should we have hierarchies of them?

So it forces you to suddenly go, OK, there's a lot more complexity here than we thought initially.


SPEAKER_04:
Most of my experience is with the discrete models and PyMDP specifically.

But I think Daniel can probably speak to more of the Rx and Fur side and the Julia side.

And I think they are pretty advanced ways of handling the continuous distributions, particularly for the drone flying examples that they have.

Daniel, I don't know if you want to add anything on that.


SPEAKER_00:
Yeah, I really resonated that a lot of the considerations of cognitive modeling, independent of whatever implementation approach, are like what slice or what joints of nature do you really want to model?

So that was a cool constructivist approach that you took with building up that biophysical realism.

And then when it comes to implementing active inference, it's a great learning experience to actually try to implement, in limited cases, different equations.

and then that helps like pull back the curtain and realize like hmm we're not really dealing with a collected data set and then training a generalized statistical model to interpolate something about the patterns of that data set but rather explicitly stating certain distributions that might have

observability or non-observability or control and and so it's like pulling out those islands or the icebergs that you want to model explicitly

rather than it just it's like etched away or revealed from where in another way of modeling you could have just collected the salient data set and then interpolated not needed to specify anything specific just we have

trace data on experiences that were rated positively or negatively.

We trained a model and that's how we are continuing to classify these traces of data versus it's like, wow, there can be a lot of steps and a lot of degrees of modeler freedom in determining which explicit distributions are relevant for that.

it's a separate question than like which dataset is relevant for it.

But that's, it comes up all the time in modeling and the frontiers of the software development are also quickly moving, hopefully meeting up with these more modeler challenge areas.


SPEAKER_04:
The only thing I'd add on to that is coming back to, I think, something you said earlier, Rod, on it's an acid test whether you can create an agent that is successful in these environments.

Part of that is also actually not even necessarily if you've implemented the model correctly, but just being able to specify a generative model for a problem.

It really does force you to think.

And I think that's one of the things that's been missing a lot from the more traditional reinforcement learning and deep learning approaches, where it is just get a bunch of data, chuck it at a neural network, and hope for the best.

And you sort of relying on the fact that, well, I don't know what a generative model for this is.

I don't have any model of causality.

I don't have any explicit model of a user.

I just throw some data and I get an output back out.

And I think it's really interesting from the active inference point of view of getting explainability from structure and being able to think about a problem

sort of almost like reasoning from your armchair and saying i'm going to think about this problem and say the world acts in a particular the world behaves in a particular way specify those causal chains and then test that agent out in the real world afterwards and i think the you know obviously the key thing is if you're using your deep learning to model how a city works with traffic or how a hospital work or how a company works you're usually doing that in order to do something else more efficiently


SPEAKER_03:
And so as soon as you close the loop using these predictive models, you've changed the city, you've changed the hospital, and it's going to be moved to somewhere where you had no data before because you didn't have the ability to control it before.

So the active inference, because it's a closed loop framework, means it's going to then have to adjust to that new world, and that's part of the basic framework, whereas a lot of the classical big data work was

say, oh, all we need to do is get lots of data.

We can build a model.

But they were forgetting that as soon as they used that, they moved the system that they were trying to optimize to somewhere it had never been before.

And their model was then useless.

So I think that's the interesting bit.

But the challenge is, OK, we closed loop.

But control theory spent decades looking at stability of closed loop systems.

know we're going to have to revisit a lot of that in active insurance work is yes we're trying to change things but you know can we guarantee that this that our hospital is going to be stable our city is going to be stable if we put active insurance loops around it and i don't think we can guarantee that yet can we


SPEAKER_00:
Lots of interesting questions in the chat.

Just read a few in this last section.

So Frazier Patterson wrote some questions, hopefully that we will hear more about in the coming years.

So let us continue on some other questions.

Thank you, though, Frazier.

Jeff wrote, how can incorporating active inference in HCI assist user trust?


SPEAKER_01:
I would say one aspect is interpretability, so if you build explicit forward models and somehow communicate the belief that the agent falls, that is, you know, the trigger for the agent taking certain actions, then

That could help explain and thereby build trust, right?

This causal chain of internal belief to agent action is harder to disentangle if you look at things like language models.

Obviously, it now comes in where, you know, they make more explicit their sort of chain of thoughts.

But I would say that that is one, maybe one aspect that can help with user trust.


SPEAKER_03:
So there are the, there's elements of it.

So there's some bits that would be

on the design side so if we have explicit models and we can look at a particular interaction context and analyze it then we can say oh okay we understand why the system was behaving the way it was and we can decide whether that's a good or bad thing we may still need to create a succinct metaphor for interaction for the user where the predictions are fed in and that can communicate that succinctly to the user in some settings or if this was a

know so say you were controlling a tanker and an active inference navigation system was giving you it could be saying my predictions about where you're going to be if you do this thing or this and then you could relate to that another way it can potentially build trust is that because you're closing loops to uh you know maximize um your predictability and minimizing your surprise it can potentially

there might be two possible interpretations of the user's situation and it could try and do something to quickly reduce the uncertainty of which they were in and then have a more coherent feedback for them.

Whereas in other situations, the system would have to say, well, if

this was your intention, then this would be happening if this was you.

So an active inference system could try and reduce that uncertainty more rapidly in order to give a more coherent feedback and control.

So that might be one way to build up trust that you can see, okay, it thinks I'm doing this.

Again, it's about the mutual modeling aspect that we were talking about.

as the user starts to become more confident that they understand the system, it can work with that.

So if it understands that the user doesn't seem to have a good model of them.

So if the active inference agent has an explicit model of the user's model of that, that model is still quite uncertain.

It knows there's not an awful lot of point in having a very deep prediction horizon because

very soon the user is going to get so diffuse about their understanding of what's going on that it maybe needs to spend more effort in the early stages of improving the user's model of it.

So it maybe has to perform some tasks, not so much to achieve the user's goals, but to help to reduce the uncertainty of the user's model of it.

So there may be a stage where the system just automatically essentially trains the user

how capable it is and it's also then improving its model of the user and vice versa.

So you may find that it achieves trust by going through a process of reducing the uncertainty of the mutual models and then they can do more together and the prediction horizons gradually get longer because they're able to predict each other's behaviour more coherently.


SPEAKER_00:
being able to explicitly disentangle pragmatic and epistemic value and when are we seeking to

provide preferred sensations like staying on the road for the self-driving car or the songs that are preferred for a given listener and then epistemic value can provide learning and updating it's not needed in every single situation just taking ad hoc excursions from blood sugar homeostasis is probably deleterious but then again it would be in balance with maintaining it in a healthy range to understand responsivity so it's like

whether for biological systems or these more synthetic informational systems being able to understand and have an explicit dial and this is also one of the most challenging parts of modeling even after you've set up the pragmatic and epistemic value terms

Their balance is like doing control theory on a seesaw because it contains a higher order trade-off with preference satisfaction and with learning.

Arun?


SPEAKER_04:
Yeah, I just want to come in with, I think the RX Infer team originally had an application where they were looking at hearing aids and parameter tuning and optimization specifically to help users use hearing aids more because the situations where you might be using that hearing aid would require very different parameters.

So if you're in a train station versus in your house, your hearing aid would need to be tuned differently, but that's a very difficult procedure to do.

So they would have an active inference algorithm that would

try and understand the uncertainty of where the user is, but deliberately choose odd parameter choices to learn about the context, to then choose good parameter values in order that the user has a good hearing experience.

So there you'll sort of

I think they're implicitly getting that trust because the user is then just using the product more as to your original point of if apps aren't useful, they don't get used and they die.

So as a user trusts a product more or trusts a system more, they are going to use it more.

I did want to come also onto the original question as well about improving user trust and the uncertainty aspect of this.

I mean, Active Interference has a very clear, explicit modeling of uncertainty.

How do we communicate that to users in a sensible way?

You also mentioned the large language model stuff in Chain of Thought.

I think they don't have an explicit model of uncertainty.

but they do a good job of communicating.

Whether what they're communicating is actually valid or not is a separate question, but they give English language that people can understand, right?


SPEAKER_03:
There's a lot of work on how to represent distributions interactively, and you can have things like audio models, which are properties of the sound can represent the distributions involved.

You've got visual

models where you can blur images.

But I think you're going to end up with, and you've got things like Chernoff faces, or Chernoff faces, or Chernoff radiation.

Chernoff faces where you could have multiple dimensions of a face representing a high dimensional data, but you could potentially add extra properties to it.

So I think there's going to be lots of scope for creative metaphor

designed to represent uncertainty.

We've been looking at this on and off over the years.

So that's going to be something where there's going to be loads of scope for creativity.

So you could separate the algorithmic elements of estimating your beliefs from the way you display them to the user and then try and optimise, you know, so which ways are working?

How do you change it?

How do you

if a particular approach is not communicating its uncertainty well to the user, then we need to learn better ways of doing that.

So there's going to be an element of data-driven tuning based on how well people are communicating, and there's going to be an element of just creative leaps of the imagination and how do we take high-dimensional, uncertain data and represent them.

And just to go back, some of these things have been around for a long time.

So the Bayesian

modelers at Microsoft had to think carefully about the wizards.

So if you're doing something mundane like a printer wizard, there may be an optimal question to ask next in terms of expected information gain.

But it may be, if you ask a series of questions which seem to be about a topic, are you properly plugged into the printer?

And then it suddenly leaps away and asks,

something about a completely different topic and then comes back to wires and printing and you know printer wires and so on the user may lose trust in it even if it was a statistically it was an optimal question to ask and because the user can't see the relationship and thinks this thing doesn't know it's just guessing random things so there may be an information theoretically optimal way to ask things but if you don't take into account the user's model then and so if the user

an experienced user and trusts the system then that would be an optimal thing to do if the user is a naive user in terms of this wizard which is often the case because things should hopefully not fail all the time then you have to assume that they are a bit mistrustful about products from Microsoft and

that they then are willing to just quit using the wizard if it starts and what they think of as random questions so this has been around for a long time and um it's going to be an interesting question so one of the you know for example in terms of the the interaction between the the human and the the computer the reason that we wanted to work on optimal interaction mechanisms as a topic is that we think there's going to be a very

a limited set of core features on the interaction, which can be combined in different ways with different sensors.

And we want to have a framework that people can quickly recognize.

And it's the way that nowadays toddlers pick up a magazine and start to swipe it, thinking it's an iPad.

They've very quickly grasped the basic ways of interacting with a tablet, and they're trying to generalize that around the world.

And we think that, you know,

a big part of making active inference and HCI work well together is going to be picking appropriate metaphors, building appropriate interaction mechanisms so that even if somebody is using a new sensor for the first time, they can quickly manipulate it and see, ah, okay, it seems to be mapping to a dial or a slider and I'm hearing some formative feedback from the core mechanism that I'm familiar with and I can quickly adjust my behaviour to

work with the possible mappings that might be involved there.

So there's going to be a co-creation of interaction metaphors at the same time as the active inference and HCI come together.


SPEAKER_00:
Well, maybe in closing, Arun and then all the authors, how are you planning to take the work forward and or what would be on your wish list slash issues or questions to raise up for people to explore more?


SPEAKER_03:
Well, the work is continuing forward and we're hiring

exciting new PhD students, some of whom we're asking questions which you weren't transmitting, but we're sure that we'll get a chance to hear that from them in person, so hopefully they'll be pushing things forward.

But I think, Aaron, you can see you've been voted in as a new speaker

Sometimes does that.

So I think the questions that we have for the community would be about things like the preference priors.

How are people working with that and tuning that and eliciting that in other contexts and adapting them, learning them from behavior, because that's all going to be super relevant for building systems that can interact well as a human and reasoning about what should

Should the priors on the system just be about pleasing the human?

Should we have other constraints on them?

What does that mean for something like an educational app, which should know more than the user about certain things at the start of the process?

Because the whole point is to change the human.

So I think that, you know, we're

We're excited to hear what's happening on things like preference priors on the whole debate about alternatives to EFE for prediction and what happens when things have not yet converged because some of the assumptions in the core cost functions like EFE are assuming that the system's already doing the right thing and what happens when there's a mismatch at the start of that process.

So I think that getting more input from people who are experienced working with these things would be exciting for us.

Do you have anything more you want to add in?


SPEAKER_02:
Yeah, I think we've still got some questions around how

the learning of the models happens so how do you learn a forward model particularly how do you do that during uh interaction without introducing instability um the questions about the representation of the low-level distributions the kind of things sebastian was talking about you can't always work with uni model models even uh want to do things in continuous spaces how can we do that for the kind of distributions we actually encounter in human computer interaction

and questions about how we do the debugging of these kinds of models, what are the diagnostics we can apply to see how well these models are working and track that.

We're going to build them and evaluate them, something which is going to be core to that.


SPEAKER_01:
I think for me, one of the most interesting things is looking at scaling, so we can scale things up to higher dimensional user inputs, but also scaling to

much higher dimensional degrees of freedom in terms of the system that's being controlled, the time horizons over which we reason, and that then brings up questions about what do we amortize, what do we actually explicitly model and roll out, and whether or not we bring in some concepts of hierarchical agents and so on.

So I think scaling for me and everything around that is sort of what's going to keep me busy for the next slide.


SPEAKER_04:
I think just to add there as well, I think the scale-free part is particularly important

of focus right now so we've had the scale for active inference paper that um came out last year um i think there's a paper that came out this year about the tree search and trying to reduce the sheer computer explosion of policy policies that you can have if you have a planning horizon longer than x so i think we'll see a lot of uh work addressing those issues over the next few years

For me, it's just a case of getting really clear and reproducible software packages so that we can make this easy.

I think that's where the work will be over the next few years of just being able to say, right, create an agent.

let it go the thought the thinking should be about how you set up your generator model and not really about whether you're using effective free energy or expected free energy of the future or generalized free energy i think that's largely a problem that should not necessarily that should not need to be solved by say designers or human computer interaction specialists

I'd like it ideally that the software is, you know, sort of batteries included, off you go, you get active inference out of the box, but then it's coming up with a model that maps to your problem, maps to the problem that you are trying to solve for a user.

So hopefully we get there sooner rather than later.


SPEAKER_03:
And I think also creating

tools to harnesses to take these agents and their verbs and run simulations and analyze them in different contexts and visualize what's going on and look at counterfactual situations and so on.

So I think if we're doing it all right, then we have the tools to make the active inference work well, but then

We're trying to, you know, even if it's understanding a natural system, you might like to put some, put it into a particular context and observe what happens and doing the tracking of, you know, the behavior.

So the kind of digital twin element of this, how do we build active inference agents that we can actually monitor and analyze and say, okay, we're now understanding if we want to have interpretable

systems then we can get it into a particular context and statistically understand what's going on around it and why is this down to, is the behaviour, is the lack of what seems to be a sensible action there down to an imperfect generative model of the world?

Is it down to too short a prediction horizon?

Is it that we're

just aren't sensing the world well enough, then that's just the limit of it's doing the right thing given the information it has.

So I think creating the tooling to understand and analyze the models is going to be interesting as well.


SPEAKER_00:
Well, you've created really a wonderful style sheet and almost a graphical taxonomy of different settings where that is coming into play in cyber physical systems, which have at least partially a fully modelable component.

And so that plus open source software development plus community feedback and application, it will be exciting to see where we all go.

So thank you again to the authors and to everyone for raising the possibility of the stream.


SPEAKER_03:
Thanks for inviting us.

See you all.


SPEAKER_00:
Thank you.
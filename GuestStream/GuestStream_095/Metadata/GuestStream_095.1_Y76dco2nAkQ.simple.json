[
  {
    "start": 6.157,
    "end": 33.514,
    "text": " hello and welcome everyone it's january 16th 2025 we're an active inference guest stream number 95.1 with rithvik prakhi discussing active inference for self-organizing multi-llm systems a bayesian thermodynamic approach to adaptation so rithvik thank you for joining looking forward to your presentation and sharing more and discussing so thank you to you",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 33.494,
    "end": 44.972,
    "text": " great thank you thank you daniel for having me um yeah so i'm gonna talk about kind of active inference for self-organizing multi-lm systems so basically making llm agents better with active inference",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 46.302,
    "end": 52.574,
    "text": " So to motivate the problem a little bit, current multi-LLM agents kind of have a lot of complex parameters.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 52.614,
    "end": 61.091,
    "text": "For example, prompts, agent architecture, tool use, like these are things that you can't just kind of fix with hyperparameter optimization.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 61.151,
    "end": 65.219,
    "text": "You have to find better, more intelligent ways to improve these in time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 65.861,
    "end": 68.125,
    "text": "And that's not really a thing right now.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 68.105,
    "end": 83.194,
    "text": " um also another thing is that lms themselves are used as a vector f adaptation for the agent in many ways so for example in the voyager paper um llms are used to acquire new skills and then use those skills to explore the environment of minecraft further um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 83.596,
    "end": 86.663,
    "text": " which is useful, but also quite limited.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 87.364,
    "end": 93.317,
    "text": "And then the third thing is that there's no formalized mechanism of balancing the exploration exploitation trade off.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 93.878,
    "end": 103.258,
    "text": "Right now it's kind of just using LLMs internal abilities to go about that, but there's no formalized mechanism and that kind of limits a lot of the potential.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 104.369,
    "end": 110.784,
    "text": " Now, our solution is using active inference to kind of overcome a lot of these problems.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 111.486,
    "end": 117.781,
    "text": "So the first thing is that the best possible prompt for different LLMs within the agent can be learned over time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 118.563,
    "end": 122.091,
    "text": "These things are something that we can figure out from the environment.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 122.071,
    "end": 134.798,
    "text": " large we can do search to figure out what are the best new prompting techniques so we can test out different prompts to see which ones give better scores on certain metrics so these things can be learned over time and the active inference agent",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 135.706,
    "end": 147.67,
    "text": " can help with that because now the active inference agent becomes the vector of adaptation, allowing for real learning through belief updates and structural updates versus just solely in context adaptation, which is what LLMs can do.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 148.031,
    "end": 153.883,
    "text": "So we're actually learning and we're actually adding information and updating beliefs within our model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 153.863,
    "end": 174.083,
    "text": " And Active Inference acts as a formalized mechanism to balance the explore-exploit boundary, allowing for a more principled understanding of uncertainty and also giving us the option to verify optimality, see if the path of selecting actions is actually an optimal one or not, which we can't do with LLMs directly.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 178.383,
    "end": 180.789,
    "text": " So active inference works through a generative model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 181.632,
    "end": 189.633,
    "text": "And the generative model we typically use for active inference in discrete time is called the Partially Observable Markov Decision Process, POMDP.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 190.275,
    "end": 192.882,
    "text": "That's Markov because",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 193.418,
    "end": 212.17,
    "text": " current states are only influenced directly by past states and it's partially observable as opposed to a regular MDP because of the agent doesn't have the ability to directly observe the states that must go through an observation, which allows it to update beliefs about the states without any direct observation of the states.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 212.538,
    "end": 223.27,
    "text": " So within active inference, we have several matrices that we put into these POMDPs to encode the actual information that we're learning from the environment or that is contained within the environment.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 224.211,
    "end": 226.954,
    "text": "So the first thing is the A matrix or the likelihood matrix.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 227.375,
    "end": 235.044,
    "text": "This is basically just showing the relationships between the likelihood, showing the relationships between states and observations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 235.745,
    "end": 242.292,
    "text": "So what are the chances that this observation is related to this state or this state produces this type of observation?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 242.474,
    "end": 256.228,
    "text": " The B matrix is showing the relationship between states at t plus 1 and states at t, time t. So basically, what is the probability that this state will occur at this time, given that this state occurred at a past time?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 257.629,
    "end": 260.572,
    "text": "Now, the C matrix encodes the preferences of the active inference agent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 261.112,
    "end": 269.781,
    "text": "So for example, if some of the states that this model had was sunny weather and rainy weather, the C matrix would encode that the model likes sunny weather.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 269.896,
    "end": 295.498,
    "text": " then the d matrix encodes the prior beliefs over states now the ultimate goal is to minimize surprise because surprise low surprise means that the model we have is actually a good representation of its environment but if we don't if we have high surprise that means we're not really understanding the environment well because every observation we're getting is surprising us so the goal is to minimize surprise and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 296.525,
    "end": 313.007,
    "text": " However, the issue with just directly minimizing surprise, which is shown in this equation here as negative log of the probability over all observations, is that this is typically intractable when the state space gets really big and it grows exponentially.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 313.047,
    "end": 316.832,
    "text": "So we can't directly minimize the surprise.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 316.973,
    "end": 318.094,
    "text": "So we have to...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 318.074,
    "end": 323.866,
    "text": " turn that into a new quantity that we can more easily approximate to instead of directly computing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 324.407,
    "end": 333.366,
    "text": "So this is how we derive variational free energy by doing a variety of steps from Bayes' theorem to get to this final equation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 334.73,
    "end": 340.542,
    "text": " Now, the variational free energy equation is constituted of two terms, the complexity term and the accuracy term.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 341.284,
    "end": 348.459,
    "text": "So the complexity term is basically just showing us the difference between the distributions for the prior beliefs and the posterior beliefs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 349.081,
    "end": 353.43,
    "text": "And the accuracy term is just trying to show us how close",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 354.642,
    "end": 362.877,
    "text": " how well the observations that, how well the states that we're seeing and how well those observations align with the states that we're seeing or the states that we believe in.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 363.778,
    "end": 377.022,
    "text": "So effectively the goal is to minimize the difference between our prior beliefs and posterior beliefs, and also to have our beliefs be as accurate as possible given the observations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 377.002,
    "end": 380.187,
    "text": " Now, the way this actually happens is a minimization process.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 380.547,
    "end": 383.732,
    "text": "Like we said before, this is a quantity we want to approximate to.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 383.772,
    "end": 394.409,
    "text": "So this equation right here, the DF over DQS, is just the partial derivative of free energy in terms of the posterior beliefs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 395.01,
    "end": 401.119,
    "text": "This first equation is just showing it in terms for one state factor.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 401.099,
    "end": 405.325,
    "text": " And when we do this, we want the derivative to equal zero over time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 405.825,
    "end": 410.111,
    "text": "But in practice, what happens is we have a max number of iterations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 410.772,
    "end": 423.729,
    "text": "Once those max number of iterations are reached, or the change in variational for energy falls below a certain threshold, the optimization process stops, and we have our final posterior beliefs that have minimized the quantities we want to minimize.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 424.79,
    "end": 429.857,
    "text": "This is the final posterior belief equation shown right here, basically just showing how",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 431.002,
    "end": 439.531,
    "text": " the final posterior beliefs will look, how they're influenced by the prior beliefs, and how they're influenced by the observations, the current state factor, and also the other state factors.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 442.734,
    "end": 448.039,
    "text": "Now, the second quantity in active inference that we're trying to minimize is the expected free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 448.079,
    "end": 455.507,
    "text": "Now, this is related to policies and actions in future states, as opposed to variation of free energy, which is very in the present.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 456.753,
    "end": 462.202,
    "text": " So expected free energy is made up of two terms, information gain and pragmatic value.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 462.743,
    "end": 468.432,
    "text": "So information gain is just trying to show how much we expect to gain by doing a certain action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 468.472,
    "end": 473.58,
    "text": "So the way that transitions are determined is",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 473.982,
    "end": 475.365,
    "text": " they're conditioned on an action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 475.805,
    "end": 478.29,
    "text": "So if we do this action, what is the likely outcome?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 478.931,
    "end": 488.81,
    "text": "So right here, we're saying, if we do this specific action, so a policy, if we execute the certain policy, what is the likelihood of having significant information gain?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 489.952,
    "end": 496.024,
    "text": "If we have a lot of information gain, that means it's a useful action because it's highly informative and that's what we wanna optimize for.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 497.354,
    "end": 501.404,
    "text": " So the second term is called pragmatic value, also known as expected utility.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 501.464,
    "end": 508.782,
    "text": "This term is useful for comparing the expected observations to our preferred observations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 509.343,
    "end": 514.536,
    "text": "So as I said before, the C matrix is the one that encodes the preferred observations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 514.904,
    "end": 524.893,
    "text": " And since we have some idea of what the expected states are from the B matrix, we also have some idea of what the expected states are in the A matrix.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 525.513,
    "end": 528.696,
    "text": "We can figure out what the expected observation should be as well.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 529.277,
    "end": 543.409,
    "text": "And on those expected observations, we can do a computation with the C matrix to see how high the expected utility or the expected level of preferred observations exists in the action we're about to commit.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 546.123,
    "end": 552.733,
    "text": " The final part we need for this paper, at least, is the A matrix updates.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 553.334,
    "end": 559.823,
    "text": "So the A matrix is the, like I said before, the matrix that encodes the relationship between states and observations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 560.644,
    "end": 567.735,
    "text": "But in our specific implementation, the agent doesn't initially know exactly how the states mapped observations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 567.835,
    "end": 572.181,
    "text": "It's intended to learn that over time by getting in new observations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 572.988,
    "end": 575.572,
    "text": " So to do that, we need to have some mechanism to update it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 575.772,
    "end": 576.894,
    "text": "And this is the mechanism.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 577.034,
    "end": 589.252,
    "text": "So we do a similar thing we did with variational free energy, where we take the derivative of variational free energy with respect to the modality or the A matrix with a certain modality.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 589.312,
    "end": 593.038,
    "text": "And we basically compute that over time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 594.79,
    "end": 600.758,
    "text": " And then we take this equation, and then this equation turns into the final derived equation below.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 600.818,
    "end": 605.485,
    "text": "This final derived equation below is just explaining how we should update the A matrix.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 606.085,
    "end": 612.414,
    "text": "So the main updates are going to come from the outer product between the observation and the current belief.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 612.855,
    "end": 623.329,
    "text": "So for example, if you have two states that are sunny and rainy, and the observation that you get is that it's sunny,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 623.461,
    "end": 642.949,
    "text": " All we would do is update the aspect of the sunny part of the A matrix to say that we're getting an observation that's sunny, and then add those to the counts of the A matrix.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 642.969,
    "end": 647.055,
    "text": "So getting to the actual specific work done,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 648.419,
    "end": 651.984,
    "text": " So this whole generative model consists of three state factors.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 652.605,
    "end": 654.648,
    "text": "So the first is prompt combinations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 655.349,
    "end": 657.893,
    "text": "The second is search states or search terms.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 658.013,
    "end": 659.254,
    "text": "And the third is the info state.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 660.216,
    "end": 662.078,
    "text": "And then there's seven observation modalities.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 662.519,
    "end": 669.87,
    "text": "So the accuracy, relevance, and comprehensiveness observation modalities are directly tied to the prompt states.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 670.731,
    "end": 677.16,
    "text": "The information relevance, information usefulness, and source quality modalities are directly related to the search state.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 677.393,
    "end": 685.428,
    "text": " And then the infostate just has three modalities with an infostate observation modality and an infostate state factor.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 685.448,
    "end": 687.291,
    "text": "So they're directly related to each other.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 687.391,
    "end": 694.845,
    "text": "So that means any observation in the infostate modality is just a direct reflection of the state factor.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 696.209,
    "end": 705.219,
    "text": " So this figure just shows the A matrices for each of these state factors and their relationship with the observation modality.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 705.68,
    "end": 708.75,
    "text": "So the first three is just for the prompt quality matrix.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 709.928,
    "end": 717.404,
    "text": " So this is basically just showing the relationship between the three quality metrics we described, the accuracy, relevance, and comprehensiveness, and the prompt states.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 717.845,
    "end": 721.232,
    "text": "Now, initially, you see all these squares are just blue.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 721.292,
    "end": 723.537,
    "text": "This is because the model has engaged in no learning.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 723.597,
    "end": 729.81,
    "text": "It's starting with a complete blank slate and has no understanding of the environment in this specific case.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 731.967,
    "end": 736.966,
    "text": " Now, the way the generative process actually works is that agents can select actions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 737.407,
    "end": 740.8,
    "text": "So they can either select prompt actions or search actions like we showed before.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 740.84,
    "end": 743.209,
    "text": "These are the two controllable states.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 744.218,
    "end": 750.529,
    "text": " Now, what a prompt action will do is it'll take a certain set of prompts that the agent selects.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 751.17,
    "end": 755.077,
    "text": "It'll run them through the actual research agent that we're trying to optimize here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 755.118,
    "end": 757.001,
    "text": "It'll run them through an agent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 757.041,
    "end": 758.744,
    "text": "It'll give certain observations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 759.485,
    "end": 765.957,
    "text": "So as you can see, this kind of JSON structure here, what we're doing is we're using a chat 54 mini.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 765.937,
    "end": 780.216,
    "text": " and that's just going to give us a score from 0.0 to 1.0 counting up by 0.1 on the three different metrics we need so for example if an agent selects a prompt action we first",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 780.736,
    "end": 783.92,
    "text": " run that prompt, run those set of prompts through the agent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 784.221,
    "end": 785.382,
    "text": "We get a final result.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 786.023,
    "end": 790.91,
    "text": "On that final result, CheshDPoroMini will evaluate it and then give us three scores.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 791.711,
    "end": 800.483,
    "text": "Those three scores will then get turned into an observation vector, and that observation vector will get passed back to the active inference agent to use as observation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 800.969,
    "end": 805.278,
    "text": " Now, this is the exact same process that happens for the search action as well, just with a different prompt.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 805.298,
    "end": 807.122,
    "text": "So this is actually the prompt for the search action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 807.764,
    "end": 815.721,
    "text": "As you can see, we're telling Chats for Tea to return scores on info relevance, usefulness, and source quality.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 816.427,
    "end": 821.437,
    "text": " Now, the info state is unique because it's determined purely by the amount of knowledge base.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 822.159,
    "end": 831.718,
    "text": "So the way that knowledge base works is by the amount as search actions are carried out and more information is added to the knowledge base.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 832.053,
    "end": 839.484,
    "text": " the info state goes from no knowledge to some knowledge to a high level of knowledge.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 839.965,
    "end": 845.293,
    "text": "And those are observations that are directly getting passed from the environment to the agent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 845.613,
    "end": 848.678,
    "text": "So the agent doesn't actually have any encoding",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 848.658,
    "end": 872.387,
    "text": " of the information it's getting from the search actions all it knows is that there's some knowledge state coming from the environment and it's using that as a way to determine what to do next and what observations it prefers so yeah that's where that's where that's where that's coming from now for the results",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 872.806,
    "end": 879.117,
    "text": " This is the same figure I showed before, but now with learned environment mappings.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 879.759,
    "end": 888.334,
    "text": "So as you can see, for the first three matrices, these are the prompts, the agent for each prompt has a distribution over what scores of expecting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 888.77,
    "end": 892.277,
    "text": " So for example, for the first prompt, it's going for eight and nine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 892.898,
    "end": 900.093,
    "text": "So it has a distribution saying it's most likely expecting, so this is the accuracy, this is the accuracy metric.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 900.494,
    "end": 908.871,
    "text": "So saying that for prompt one, for prompt combination one, it's generally gonna expect an eight or a nine on the accuracy metric.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 908.851,
    "end": 911.116,
    "text": " And it does that all the way through for all the prompts.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 911.136,
    "end": 912.819,
    "text": "And it does the same thing for search as well.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 913.34,
    "end": 916.046,
    "text": "So this is the info relevance metric.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 916.587,
    "end": 926.067,
    "text": "So for example, it's saying that for prompt one, it's generally going to expect a score of 9 out of 10 or 0.9 out of 1.0 on the info relevance metric.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 926.958,
    "end": 927.9,
    "text": " So kind of all that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 927.96,
    "end": 932.507,
    "text": "And then the info state metric is just concentrated around the second observation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 932.527,
    "end": 934.931,
    "text": "The second observation is just high info.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 934.951,
    "end": 936.814,
    "text": "So like detailed information from the environment.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 937.315,
    "end": 943.144,
    "text": "This is because the agent was running over 20 trials and once",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 943.276,
    "end": 949.303,
    "text": " and the info state metric gets to the knowledge base that we have gets to a high level pretty quickly.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 949.443,
    "end": 958.833,
    "text": "And then so that the info state kind of just keeps getting observations about that it's in a high level of information, has high level of information and just keeps getting those observations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 958.913,
    "end": 962.116,
    "text": "So it's all concentrated around that observation specifically.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 964.239,
    "end": 968.203,
    "text": "Now, one of the main points of this was to show how we could",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 968.96,
    "end": 975.408,
    "text": " show how we could go through the explore exploit trade-off really well by using active inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 975.448,
    "end": 976.79,
    "text": "And this is kind of showing that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 976.91,
    "end": 981.515,
    "text": "So this is just some time series data from time step zero all the way to time step 80.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 983.137,
    "end": 987.683,
    "text": "The red dots are search actions and the blue dots are prompt actions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 988.244,
    "end": 998.376,
    "text": "So as you can see, we have a preponderance of search actions in the initial stages because the agent is trying to learn the environment, trying to get more information",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 998.356,
    "end": 999.157,
    "text": " and stuff like that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1000.299,
    "end": 1004.084,
    "text": "So it's trying to understand how prompts work, how prompting works.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1005.005,
    "end": 1019.885,
    "text": "And then towards the end, there's mainly search actions, or sorry, mainly prompt actions as the agent is testing these prompts to see if they work well or if they don't work well, kind of using the information it's gotten from the search actions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1019.945,
    "end": 1021.627,
    "text": "Now, we know it's not actually using this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1021.647,
    "end": 1024.431,
    "text": "It's more kind of a simulation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1024.934,
    "end": 1032.344,
    "text": " It's using it in the sense that it knows that we already have a lot of information from the environment because of that info state observation it's getting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1035.148,
    "end": 1045.401,
    "text": "So in conclusion, what we did here was that we had the model effectively learn the environment's generative process, mapping the observations to the hidden states.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1046.183,
    "end": 1052.511,
    "text": "We were able to simulate human-like exploratory behavior with active searching and testing multiple possibilities.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1052.491,
    "end": 1066.882,
    "text": " And then the main thing is that we were able to enhance the capability of LLMs as well as enhance the capability of Active Inference because we gave LLMs a formalized structure that they didn't have before to execute learning through.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1067.423,
    "end": 1072.795,
    "text": "And we gave Active Inference the ability to operate in high dimensional spaces by integrating LLMs",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1073.264,
    "end": 1084.759,
    "text": " If we had to try to encode this functionality directly into the active inference agent, it would have been much more harder and much more complex than just having the LLM outsource everything to the LLM and have it do a lot of the work.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1085.18,
    "end": 1088.745,
    "text": "So we kind of had improvements on both ends through the system.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1091.248,
    "end": 1092.91,
    "text": "Now I just want to talk about some",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1095.033,
    "end": 1096.919,
    "text": " some future directions for this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1097.542,
    "end": 1103.783,
    "text": "So obviously the one big thing that this model doesn't have is encoding actual knowledge from the environment.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1103.883,
    "end": 1107.034,
    "text": "So as we perform search actions,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1107.419,
    "end": 1109.161,
    "text": " we're getting information about prompts, right?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1109.181,
    "end": 1115.426,
    "text": "So we're saying, so if we look up, what are the best ways to prompt for ChatGPT?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1115.747,
    "end": 1119.19,
    "text": "It'll say like tree of thought, chain of thought, blah, blah, blah, et cetera, et cetera.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1119.59,
    "end": 1123.794,
    "text": "But none of this information is actually getting encoded into the agent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1124.815,
    "end": 1128.038,
    "text": "All that's happening is that there's an info state, right?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1128.098,
    "end": 1131.16,
    "text": "And it's using that info state to determine its future actions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1131.841,
    "end": 1136.245,
    "text": "But it would be much more useful if the active inference agent actually knew",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1136.225,
    "end": 1146.361,
    "text": " why certain prompts or certain prompt combinations were good based on the search actions, and then use that information directly to influence those actions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1146.942,
    "end": 1153.693,
    "text": "So that would require some sort of hierarchical model, and making that would be a good feature direction for this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1154.078,
    "end": 1162.217,
    "text": " Another thing is giving the agent the ability to dynamically expand its state space to not be constrained to a fixed set of prompts.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1162.818,
    "end": 1168.031,
    "text": "So here, what we did was we just had a fixed set of prompts for each part of the agent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1168.833,
    "end": 1170.737,
    "text": "And obviously,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1170.717,
    "end": 1198.328,
    "text": " there's much more the space of possible prompts infinite space and there's there's a lot more we could explore if we weren't constrained by a fixed state space so expanding on that would be very useful and also the third thing would be to equip the agent with the ability to modify other complex parameters such as instead of just prompts maybe also be able to modify the architecture of the agent or the tools it's using and stuff like that",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1199.135,
    "end": 1211.53,
    "text": " The eventual goal is to make a general kind of agent brain that has the capability to be plugged into any LLM agent and exhibit human levels of learning, modification, structure learning, and stuff like that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1214.914,
    "end": 1217.737,
    "text": "So yeah, I think that's it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1220.3,
    "end": 1220.681,
    "text": "Awesome.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1220.981,
    "end": 1222.763,
    "text": "Thank you for the presentation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1223.772,
    "end": 1230.279,
    "text": " For those watching live, they can write questions while I'm just cropping and getting everything back.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1230.959,
    "end": 1232.741,
    "text": "Maybe share a little bit.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1232.961,
    "end": 1235.103,
    "text": "How did you come to this project?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1235.384,
    "end": 1239.047,
    "text": "Where are you at in your education and research and all of this?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1241.17,
    "end": 1242.651,
    "text": "Yeah, sure.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1242.811,
    "end": 1248.357,
    "text": "Well, I only found out about Active Inference maybe a year ago.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1248.397,
    "end": 1252.581,
    "text": "And kind of how I happened upon it is that",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1253.236,
    "end": 1257.04,
    "text": " I wanted to kind of do this general idea that I just said, like the agent brain.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1257.6,
    "end": 1271.475,
    "text": "So, because I noticed that LLMs were pretty static and, or like LLM agents were pretty static and then humans had to directly go in and like, oh, let me fix this prompt or let me change the order of these two nodes and stuff like that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1272.115,
    "end": 1282.686,
    "text": "And the same kind of, you know, LLMs themselves can do a lot of that improvement work, but it seemed like there was a big hole there that we could just fill by having some sort of self-organizing behavior.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1282.666,
    "end": 1287.634,
    "text": " And that's kind of how it happened upon Active Inference because it was this model-based.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1287.774,
    "end": 1298.312,
    "text": "So we were responsible for figuring out how to actually structure the generative model and the generative process that the agent was going to work with.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1299.018,
    "end": 1306.99,
    "text": " And it was very sample efficient, which I really liked because generally you don't get that many samples in a typical workload.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1307.07,
    "end": 1318.087,
    "text": "So for example, if we're using an agent in production, we wouldn't really be getting, unless it was being used at massive scale, we wouldn't be getting too many examples that we could work with to improve certain functions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1318.748,
    "end": 1323.996,
    "text": "And the sample efficiency of Active Inference really kind of helps with that issue.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1323.976,
    "end": 1329.584,
    "text": " And then the other thing was the reinforcement learning, obviously, just having some aspect of being able to learn over time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1330.346,
    "end": 1331.708,
    "text": "With LLMs, you can't really do that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1331.728,
    "end": 1332.669,
    "text": "It's all in context.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1333.51,
    "end": 1343.646,
    "text": "And I'm suspicious that in context learning is, or just having information in context is enough to replicate the abilities that a reinforcement learning scheme can provide.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1344.347,
    "end": 1347.071,
    "text": "So that's kind of how I happened upon this and started looking into it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1347.251,
    "end": 1352.599,
    "text": "I actually read the Thomas Parr Carl Friston textbook.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1352.579,
    "end": 1360.665,
    "text": " um on this and that's kind of how i got into it started doing some work and yeah kind of came to this",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1362.181,
    "end": 1362.581,
    "text": " Cool.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1363.262,
    "end": 1373.011,
    "text": "Okay, I'm gonna read some questions from Arun Naranjan, a colleague at the Institute, and then any other questions in live chat, we'll see.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1373.731,
    "end": 1376.874,
    "text": "Okay, so thanks, Arun, for the awesome submitted questions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1378.175,
    "end": 1378.375,
    "text": "All right.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1378.395,
    "end": 1382.459,
    "text": "What does it mean to ask an LLM for a numerical score of something?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1382.919,
    "end": 1390.686,
    "text": "Can we really convert the sampling of the next token's probability distribution of an LLM into a meaningful quantitative observation?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1392.286,
    "end": 1393.248,
    "text": " Hmm.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1393.268,
    "end": 1407.392,
    "text": "So it's, it's kind of a, um, the LLM observation is kind of a, uh, a prox maybe approximation or like, uh, kind of a,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1408.587,
    "end": 1410.81,
    "text": " maybe an easy way to do this basically.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1410.83,
    "end": 1422.307,
    "text": "So all we're doing is we're saying like, because the best text readers at this time in AI or text understanding machines kind of are LLMs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1422.808,
    "end": 1427.895,
    "text": "Because the way this is working is that the LLMs are getting",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1428.348,
    "end": 1443.787,
    "text": " just a bunch of text because the research agent is outputting a bunch of text, and the active inference agent directly can't really do much with this because language is a hard space to understand, and active inference would need a lot more improvements to be able to understand that space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1444.368,
    "end": 1454.44,
    "text": "So basically, all we're doing is that we're kind of simplifying the complex state space of just the English language, and we're simplifying it into something the active inference agent can understand.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1454.42,
    "end": 1459.57,
    "text": " So what we're doing is that the LLM can understand language, and it can generate certain scores.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1460.031,
    "end": 1466.864,
    "text": "Obviously, the LLM scores aren't as useful or nuanced as a human scoring certain text would be.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1467.385,
    "end": 1476.462,
    "text": "So for example, if there's an essay competition, an LLM would probably be a much worse scorer for those essays in a consistent manner than a human would be.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1476.442,
    "end": 1498.821,
    "text": " because it's kind of hard to determine what the standards are but for something like this where the text is relatively simple um it's pretty easy for the lm to just say this is the score i'm giving to the text we give it certain like guidelines to follow when assigning scores so it just gives a certain score and that score is kind of a",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1499.51,
    "end": 1507.962,
    "text": " basically a simplification of the state space of all possible languages to just three values that we can use to kind of help the active inference agent learn.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1508.383,
    "end": 1518.478,
    "text": "Now, ideally, it would just be the active inference agent directly digesting the language, but this is kind of an easy way to overcome that massive gap in complexity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1519.379,
    "end": 1520.221,
    "text": "Yeah, great response.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1520.281,
    "end": 1522.564,
    "text": "I'll add a few comments.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1523.084,
    "end": 1533.943,
    "text": " you pointed right there that a longer term research agenda is like an active inference linguistics model using some kind of syntax and semantics on the natural language itself.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1534.004,
    "end": 1537.47,
    "text": "And so some kind of mega hierarchical model or something cool like that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1537.85,
    "end": 1540.435,
    "text": "But in the meanwhile,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1540.415,
    "end": 1559.048,
    "text": " For specific purposes, we can take natural language expressions, use LLMs, especially with the JSON or structured output, and project them onto numerical lower dimensional state spaces, like how many bird species are mentioned in this paragraph.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1559.703,
    "end": 1581.728,
    "text": " only respond with a specific number three and then you have an a matrix that takes in as an observation the number of bird species observed and then maps to some other variable for the ecosystem or how many facts in this paragraph correspond to this circumstance and so then the active inference generative model can pick up",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1582.282,
    "end": 1610.466,
    "text": " at the interface of the output of the structured llm return so it's it's a design pattern i think that's very rapidly emerging and it's exciting to see that that you're working on it because it really does bring together this mega parametric you know millions to billions of parameters high dimensional state spaces more generic architectures for llm and such",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1610.446,
    "end": 1633.905,
    "text": " and then take the output sometimes crystallized down to just like a single scalar value or to a vector of numerical values pick up there with the more interpretable smaller and sample efficiency and direct belief updating and then just one more piece i'll say there's you can get an llm to give",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1633.885,
    "end": 1656.568,
    "text": " uh quantitative or natural linguistic representation of its own uncertainty but then there's this kind of head-scratching moment where it's like well even if it says that it's 55 confident that is the most likely chain of tokens for it to say it's not a true introspection of its actual uncertainty",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1656.548,
    "end": 1664.943,
    "text": " And so again, further down the road, that's a promising avenue for active inference models that can introspect and accurately have their own uncertainties.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1665.504,
    "end": 1671.616,
    "text": "However, in the meanwhile, this is one way that we can make the best of what exists today and what we can build tomorrow.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1672.978,
    "end": 1673.439,
    "text": "Exactly.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1673.719,
    "end": 1679.029,
    "text": "And one other thing I'll add is that what I'm trying to do as well is",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1680.207,
    "end": 1699.988,
    "text": " LLMs can already do a lot, so kind of have LLMs do as much as possible, but also try to kind of like what I was saying before, like having the active infrastructure be able to encode the information itself from the environment, like add things to the active infrastructure over time and then offset some of the responsibility of the LLM so that",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1700.322,
    "end": 1707.34,
    "text": " we're getting the actual interpretability of proper uncertainty estimation and all that stuff within the active inference agent without having to rely on the LLM.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1708.042,
    "end": 1713.657,
    "text": "But we can basically, if active inference is here and the LLM is doing all this, we can just",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1713.974,
    "end": 1719.081,
    "text": " you know, move that gap over time as we develop more, but we're still getting the performance to start with.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1719.802,
    "end": 1720.523,
    "text": "That's kind of funny.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1720.543,
    "end": 1737.788,
    "text": "It makes me think of like a grass is always greener or some kind of compliment where for the LLMs, we might gain interpretability by projecting down to smaller states based models that have real quantitative variance estimators.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1737.768,
    "end": 1744.84,
    "text": " Whereas for active inference models, we can gain interpretability and accessibility by translating them out to natural language.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1745.601,
    "end": 1748.766,
    "text": "So, you know, yeah, yeah, that's a great way to put it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1748.786,
    "end": 1749.868,
    "text": "Yeah, more tools better.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1749.968,
    "end": 1751.851,
    "text": "Okay, Arun asks the next question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1753.314,
    "end": 1757.621,
    "text": "Over what timescales did your active inference plus LLM agent execute?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1758.021,
    "end": 1761.447,
    "text": "How did that change with the complexity of the questions asked?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1767.113,
    "end": 1772.316,
    "text": " What was the runtime dynamics?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1772.657,
    "end": 1774.947,
    "text": "How long does one full loop take?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1776.209,
    "end": 1779.034,
    "text": " Yeah, I mean, it's pretty short.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1779.254,
    "end": 1784.583,
    "text": "So it was 20, it was, it was about like 80, 80 time steps.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1785.164,
    "end": 1788.83,
    "text": "So kind of just 80, like 20 trials, four time steps for trial.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1789.972,
    "end": 1799.989,
    "text": "The time steps only were relevant to the info state, the info state, or information state, because the other two didn't really have any trends, transition dynamics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1799.969,
    "end": 1804.996,
    "text": " But effectively, yeah, what it would just be is that for each time step, there would be some sort of action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1805.677,
    "end": 1808.682,
    "text": "The LLM would run through all the stuff it had to run through for the agent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1809.143,
    "end": 1815.212,
    "text": "So the research agent would run that we're trying to optimize, and then the evaluation on that result would run.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1815.752,
    "end": 1818.957,
    "text": "And then we would have that observation, and we'd put it back into the active inference model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1819.157,
    "end": 1820.579,
    "text": "So that would be one full time step.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1821.02,
    "end": 1826.348,
    "text": "So that maybe took, in terms of actual time, to run the research agent, run the evaluation,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1826.328,
    "end": 1832.038,
    "text": " And then the active inference model itself, maybe each time step took like 10 seconds.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1833.741,
    "end": 1839.812,
    "text": "It was relatively quick, especially because we're using like 4.0 mini, which is like faster than something like 4.0 maybe.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1839.853,
    "end": 1844.16,
    "text": "And the active inference agent was like the state space was pretty small.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1844.24,
    "end": 1846.945,
    "text": "So it didn't take much time for the active inference agent to run as well.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1847.086,
    "end": 1847.847,
    "text": "So yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1848.923,
    "end": 1849.244,
    "text": " Cool.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1849.344,
    "end": 1861.863,
    "text": "Yeah, I could see the latency of the LLM being a major determinant, whether you use kind of a smaller or a faster call, or whether you do something that that takes a bit to get chugging.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1862.564,
    "end": 1863.706,
    "text": "Okay, cool.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1863.907,
    "end": 1864.167,
    "text": "All right.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1864.588,
    "end": 1865.309,
    "text": "Arun's next question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1865.93,
    "end": 1869.295,
    "text": "Did you actually get good answers to research questions you asked?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 1873.223,
    "end": 1878.818,
    "text": " Well, the LM itself already worked.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1879.64,
    "end": 1881.585,
    "text": "Or the research engine itself already worked.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1881.605,
    "end": 1887.22,
    "text": "So it had certain prompts, and it would output something.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1887.26,
    "end": 1889.005,
    "text": "So for example, if I asked...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1889.93,
    "end": 1916.359,
    "text": " what give me give me like a report on coffee or something it'll go through the entire you know it was basically five aspects to the research agent it was um planning reviewing uh actually like or getting information from the internet answering the question and if there's like uh any issues with the answer then it would route back to the initial planning stage and then kind of go back from there",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1916.339,
    "end": 1925.185,
    "text": " So the agent itself was already relatively good at getting answers, getting good answers or getting good reports on whatever question you asked.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1925.333,
    "end": 1935.044,
    "text": " Maybe the point of this was just to show that, because if you think about it in time, the prompts will become worse and worse every time, because prompting techniques get better.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1935.405,
    "end": 1940.891,
    "text": "Well, they'll get worse and worse in the sense that our knowledge of how to prompt increases, for example.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1941.472,
    "end": 1945.376,
    "text": "We initially used to only think about step-by-step prompting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1945.437,
    "end": 1948.1,
    "text": "That was the max of what prompting could be.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1948.58,
    "end": 1951.003,
    "text": "Now we have so many different techniques.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1950.983,
    "end": 1954.268,
    "text": " We have a tree of thought, chain of thought, a react framework, blah, blah, blah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1954.288,
    "end": 1955.55,
    "text": "There's like so many different techniques.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1956.011,
    "end": 1970.494,
    "text": "And if we had started in like 2023 with just the step by step prompting and we as a human didn't go back in there and fix it ourselves, then we would just be stuck with those bad prompts compared to where the literature is at right now.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1970.812,
    "end": 1979.45,
    "text": " So the idea was that, well, for this was a fixed state space with just like, it was like five prompts for agent that we had to work with.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1979.53,
    "end": 1984.5,
    "text": "And then those would be formulated into combinations of prompts that the active inference agent could select.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1985.362,
    "end": 1986.745,
    "text": "So it's just kind of a...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1987.433,
    "end": 2002.126,
    "text": " a simple way to show that the active inference agent could pick these prompts, pick the best one, research from the environment, and then do this whole process to try to learn which is the best prompt in that very small state space over time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2003.427,
    "end": 2015.618,
    "text": "It's not going to change the effectiveness of the research agent too much because the state space of the prompts is so small, but it was already good to begin with and it was good to end with.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2016.408,
    "end": 2018.001,
    "text": " Yeah, thanks.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2018.465,
    "end": 2020.562,
    "text": "A few comments on that.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2020.812,
    "end": 2026.699,
    "text": " We're seeing at this moment just such an exponential increase in the capacity.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2026.739,
    "end": 2031.184,
    "text": "So as you point out, even six months or a year can be very different.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2031.645,
    "end": 2037.351,
    "text": "And it's an exciting time for understanding research and cognitive science agents overall.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2037.451,
    "end": 2048.524,
    "text": "Like I think about the graph structured knowledge bases like Buehler's graph preflexor type models where",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2048.504,
    "end": 2055.235,
    "text": " you know, limitations of in-context learning that you pointed to can be addressed with some operations on knowledge graphs.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2055.816,
    "end": 2075.068,
    "text": "And it's like, we're kind of exploring motifs and initial pattern designs and proofs of concepts for how these kinds of synthetic intelligence systems and augmented systems with human and non-human entities can be collaborating in information spaces",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2075.048,
    "end": 2102.98,
    "text": " epistemically and like everything from the user interface and and the the accessibility and the human computer side on through what are we really yielding are we synthesizing new molecules are these revealing errors in previous derivations and papers are these you know shorter algorithms like so it's just like wow happy 2025 huh",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2102.96,
    "end": 2132.055,
    "text": " um how arun asked how did this compare to purely llm driven agents for example llm plus while loop plus tool calling yeah so using llms um we can definitely use like lms directly to improve the agent so kind of just say like so we have this research agent we just have the llm going and say um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2132.592,
    "end": 2144.374,
    "text": " like if every time like every like let's say like every day do research on like all these topics around prompting um synthesize that research",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2144.776,
    "end": 2150.048,
    "text": " put it in your context, and then use that context to, like, look at all these five prompts, make them better.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2150.068,
    "end": 2163.139,
    "text": "Now, if we just said that, the LLM is going to do way better than the active inference model right now, because obviously LLM can explore an infinite state space of language, and we only have, like, five prompts we're looking at, and we can't really, like...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2163.119,
    "end": 2172.673,
    "text": " Right now, there's no actual thing where doing the search is actually changing the prompts or changing the prompt selection mechanism or anything like that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2173.253,
    "end": 2177.66,
    "text": "So yeah, LLM by itself would perform way better than the model currently.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2177.7,
    "end": 2189.937,
    "text": "But the issue is that at the end of the day, it's all in context learning with LLMs, and there's no way to maintain a real understanding of the environment over time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2190.794,
    "end": 2194.339,
    "text": " actually encoded within belief updates and these matrices.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2194.459,
    "end": 2196.382,
    "text": "And that's what the active inference is bringing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2196.422,
    "end": 2201.99,
    "text": "And this is more kind of like a proof of concept to show that we could do that at some point.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2202.01,
    "end": 2210.583,
    "text": "Because just thinking about it theoretically, an easier way to improve this is just to say, kind of like how I said the LLM can just take in context, blah, blah, blah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2211.043,
    "end": 2214.388,
    "text": "Now we could just do something simpler where we could say,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2214.368,
    "end": 2219.121,
    "text": " Now the LLM, go do this research every day, take in that, put it in your context.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2219.422,
    "end": 2225.178,
    "text": "Now turn that into states that we can, or state factors that we can put with an active inference agent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2225.619,
    "end": 2230.352,
    "text": "Now the active inference agent has a whole new set of state factors and states within those factors.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2230.332,
    "end": 2235.402,
    "text": " And now I can use that directly to learn about new prompting techniques.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2235.442,
    "end": 2241.133,
    "text": "And it has new, like, it's dynamic, state space is dynamically going to change because that element is adding new prompts all the time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2241.193,
    "end": 2248.527,
    "text": "And this way, like, it's a much better way of having the active information station actually learn these prompts.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2248.81,
    "end": 2256.442,
    "text": " And it's probably, I mean, we don't know yet, but most likely it's going to perform better than the LLM just doing it in context by itself.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2256.943,
    "end": 2260.869,
    "text": "But it's like a pretty simple way because of just like having the LLM do a lot of the work.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2261.49,
    "end": 2270.464,
    "text": "Because without that, the active inference agent, like the same kind of process I described, just purely with an active inference would take a really long time to figure out.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2271.794,
    "end": 2290.984,
    "text": " Yeah, that reminds me of how many ways we're seeing pop up that computer science and AI is sort of rediscovering or reinventing cognitive science, like knowing that versus knowing how, knowing where to look,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2291.065,
    "end": 2292.227,
    "text": " to get more information.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2292.447,
    "end": 2299.918,
    "text": "And then it's like, okay, well, let's just say we are going to have our grad student, I mean, research agent, and they're going to be doing some daily task.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2300.339,
    "end": 2305.226,
    "text": "We say, well, I want the report to be balancing accuracy and complexity.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2305.326,
    "end": 2306.889,
    "text": "Well, that's variational free energy.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2307.59,
    "end": 2311.656,
    "text": "Or in terms of prospective experimental design,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2312.125,
    "end": 2342.132,
    "text": " what should be the criterion well the reinforcement strategy is with some sort of large or curated or labeled database train an auxiliary reward function and then just go ahead and try to hill climb on that reward function well what if we're into the unknown we're on the frontier we don't necessarily have a training data set um or maybe we we wouldn't even want one even if we could so then what criteria should we use that's directly based on the model",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2342.112,
    "end": 2358.957,
    "text": " that yields pragmatic and epistemic value in the expected free energy this explicit explore exploit trade-off and and the ability to modulate between returning expected results with the high expected utility to learning",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2359.038,
    "end": 2381.063,
    "text": " then it's like just like you gave that kind of daily duty cycle with like every day go and research something and then that could be encoded kind of during sleep into beliefs like what is the the zero to one and a variance on what is the likelihood that this is going to happen or what's the efficacy of this treatment over that treatment",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2381.043,
    "end": 2399.878,
    "text": " and then that gives an actual information gain term so that it doesn't just continue to do the same internet searches again and again rather those searches are tuned for epistemic value so it's like and then even to the chain of thought and tree of thought",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2399.858,
    "end": 2428.532,
    "text": " well this is where we get into questions like rumination where you could have at a hyperparametric level you could think too little and leave good thoughts kind of unthought on the table so to speak or you could ruminate and you could think too long and be past the point of diminishing return well where is the pareto optimal point for how long you should be thinking that's a metacognition question that's a cognitive security question and so it's like",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2428.512,
    "end": 2456.106,
    "text": " it's really interesting to see even though they've been coming from the horizon with these mega parametric models they're actually converging to some of the formalisms and the trade-offs that cognitive science has been working with for a long time related to everything from like epistemic foraging and memory and skill to metacognition so i mean so cool how it's all coming together",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2456.086,
    "end": 2456.527,
    "text": " Right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2457.267,
    "end": 2462.454,
    "text": "I think like Google even recently was focusing one of their models on surprise.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2462.474,
    "end": 2466.759,
    "text": "So they were using surprise as the reward function to kind of improve their models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2467.02,
    "end": 2470.524,
    "text": "And it's kind of like that's I mean, that's active inference right there.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2470.584,
    "end": 2481.638,
    "text": "So it's kind of it is kind of cool that we're seeing these concepts and active inference kind of extend out to just like LLMs and diffusion models and stuff like that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2481.618,
    "end": 2504.888,
    "text": " Yeah, and although minimizing surprise is maximizing model evidence, there isn't a tractable heuristic to bound success or evidence, whereas there is one to bound surprise, again, even though they're kind of dual, so there's an interesting relationship there.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2505.67,
    "end": 2507.592,
    "text": "Okay, next Arun question.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2509.748,
    "end": 2513.393,
    "text": " Why does the A matrix need a specific action dependence?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2513.754,
    "end": 2521.165,
    "text": "I believe PyMDP should create a B control state for every action anyway, so you should be able to set those out of the box as it were.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2522.888,
    "end": 2527.134,
    "text": "Well, the A matrix doesn't have any action dependence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2527.254,
    "end": 2529.057,
    "text": "It's just the...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2529.712,
    "end": 2533.96,
    "text": " the actions are able to provide observations that the A matrix can use to update itself.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2534.461,
    "end": 2537.627,
    "text": "But the A matrix itself doesn't have any action dependence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2537.647,
    "end": 2540.071,
    "text": "It is all through the B matrix.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2540.272,
    "end": 2551.452,
    "text": "So the one thing is that the prompt and search states are controllable, but they don't have any legitimate transition dynamics.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2551.472,
    "end": 2552.935,
    "text": "So just because",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2553.607,
    "end": 2563.478,
    "text": " if we commit some sort of action, it's not actually causing the prompt combination at one state to transition to the prompt combination at another state.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2564.359,
    "end": 2574.27,
    "text": "It's more that we are picking those prompt combinations independently, or independently in the sense that it's basically just going to be deriving from expected free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2574.33,
    "end": 2577.433,
    "text": "So whichever is the lowest expected free energy is the one we're picking.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2578.154,
    "end": 2582.719,
    "text": "But the action isn't directly causing the transition from...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2582.817,
    "end": 2606.614,
    "text": " prompt combination one to prompt combination two the only thing where that's actually happening is the info state so like i think in the code or in the code there's um like b matrix learning as well but it really only happens for the um info state the other the b matrices for the prompt states and the search states are completely the same over time kind of showing that like",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2607.252,
    "end": 2621.211,
    "text": " they really don't have any direct this action causes this state to this state kind of you know dynamics there cool um thanks for clarifying and this is sort of a classic design",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2622.473,
    "end": 2639.267,
    "text": " degree of freedom or openness for those who have explored these cognitive models which is what exactly should the agent know about the consequences of its action because if it doesn't know the consequences of action it's just going to be flailing wildly",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2639.247,
    "end": 2648.282,
    "text": " Whereas if it's given too much information about the consequences of action, it's kind of like putting the cat in the hat because it's going to know exactly what it's going to do.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2648.802,
    "end": 2663.466,
    "text": "So that's where having the B matrix learning and having these learning methods is really cool and really helpful because then you can see, okay, with what learning rate and what kind of variability in action and in the generative process in the niche,",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2663.446,
    "end": 2675.953,
    "text": " what dynamics yield over what time scales the convergence to veridical learning of the consequences of action",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2676.507,
    "end": 2682.573,
    "text": " So maybe it needs to spin 10,000 prompts out before it understands the consequences of certain kinds of prompting.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2683.014,
    "end": 2686.998,
    "text": "Maybe it only needs to throw out a few, depending on what it is.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2687.058,
    "end": 2689.321,
    "text": "So, okay.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2689.341,
    "end": 2695.727,
    "text": "And then one more question from Arun, and then if anyone in the live chat has questions, they can write it.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2695.747,
    "end": 2701.634,
    "text": "He wrote, similar question to our Bellamy, another Discord colleague.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2701.654,
    "end": 2703.976,
    "text": "How did you find using LandGraph?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2703.996,
    "end": 2704.877,
    "text": "LandGraph.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2705.093,
    "end": 2711.82,
    "text": " It seems like quite a polarizing framework, but still probably easier to use than PyMDP, sweating emoji.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2713.281,
    "end": 2716.365,
    "text": "Yeah, definitely easier to use than PyMDP.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2716.925,
    "end": 2725.954,
    "text": "I had to do a lot of going into PyMDP and changing a lot of those source code to make all of this work, which was useful for me.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2725.975,
    "end": 2731.32,
    "text": "But yeah, Landgraf, actually, I...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2732.144,
    "end": 2739.371,
    "text": " I run a company where we build AI agents for other clients and companies like that, and we typically use LandGraph for everything.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2739.511,
    "end": 2743.976,
    "text": "So it wasn't too hard because I have a lot of experience in that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2744.036,
    "end": 2749.181,
    "text": "But the PyMDP was definitely harder, that's for sure.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2749.442,
    "end": 2753.766,
    "text": "What functionality does the LandGraph provide?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2754.146,
    "end": 2757.73,
    "text": "And how does it interface or pass to PyMDP?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2759.802,
    "end": 2766.953,
    "text": " Yeah, so the LandGraph is basically just a framework to create nodes and edges.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2767.754,
    "end": 2772.461,
    "text": "And the main other thing is that it can save state between agent runs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2773.122,
    "end": 2775.045,
    "text": "So you can define nodes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2775.105,
    "end": 2781.895,
    "text": "And for each node for this LandGraph agent, for example, each node was a LLM that executed a certain action.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2781.875,
    "end": 2787.721,
    "text": " So it would be like either it was answering the question or it was doing research or it was planning or something like that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2788.041,
    "end": 2789.663,
    "text": "And each node would do certain things.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2790.704,
    "end": 2795.609,
    "text": "LandGraph allows you to kind of simply define controlled edges between those nodes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2796.089,
    "end": 2798.392,
    "text": "So you can say this node directly leads to this node.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2799.513,
    "end": 2801.955,
    "text": "But if this happens, then it should lead to this node.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2801.975,
    "end": 2803.117,
    "text": "It's kind of conditional edges.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2804.058,
    "end": 2805.859,
    "text": "And it also allows you to save state.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2806.04,
    "end": 2810.464,
    "text": "So as the agent is running, you can kind of save state",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2810.697,
    "end": 2813.961,
    "text": " just save the results at each stage.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2814.602,
    "end": 2817.346,
    "text": "So if this node ran, it's saved in the list.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2817.426,
    "end": 2819.148,
    "text": "If this node ran, it's saved in the list.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2819.168,
    "end": 2828.641,
    "text": "And that allows you to call back to use this information from this past node to inform your answer for this specific node.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2828.661,
    "end": 2836.131,
    "text": "And it's just a simple framework to allow you to just build agents effectively.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2836.752,
    "end": 2842.762,
    "text": " I don't really Yeah, I think I think it is hard if you don't have like a good framework to go off of.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2843.283,
    "end": 2845.267,
    "text": "But we've kind of like built a framework over time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2845.427,
    "end": 2853.701,
    "text": "And actually, the agent that's used in the the the code I put out is, is using that framework.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2853.761,
    "end": 2857.748,
    "text": "So if you might find that helpful, if you want to build some line graph agents.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2859.128,
    "end": 2876.985,
    "text": " yeah where do you see it all sort of going with different agentic AI and where where would it look more like a fusion agents or or collaborative swarms or mesh like what do you think we might see",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2878.062,
    "end": 2888.595,
    "text": " Yeah, I think I'm a big proponent of the idea that we're going to have like billions of AI agents just kind of all talking to each other, like basically just like a whole AI population, like we have a human population.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2888.635,
    "end": 2906.398,
    "text": "I think that's probably very likely to happen because at the end of the day, like there's probably going to, I'm skeptical that one model is going to be able to vastly overpower humans to the extent that like,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2907.222,
    "end": 2908.844,
    "text": " a bunch of models will be able to.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2908.964,
    "end": 2920.199,
    "text": "So as you can see in the typical LLM literature, if you combine, for example, if you're in your reasoning cycles, combine Claude, Chachapiti, Llama.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2920.86,
    "end": 2930.273,
    "text": "And if you combine those together in your reasoning, it typically performs better than if you just use only Chachapiti in loops or only Claude in loops.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2930.293,
    "end": 2935.44,
    "text": "So having different models trained in slightly different ways is probably going to help",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2935.876,
    "end": 2937.982,
    "text": " increased performance by a significant amount.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2938.584,
    "end": 2949.598,
    "text": "So I think we're going to probably have a lot like billions of agents just communicating with each other, maybe at the speed of light, because it's all just information and they don't have to communicate in language like we do as humans.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2949.967,
    "end": 2952.75,
    "text": " But yeah, I kind of see that going there.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2952.83,
    "end": 2956.754,
    "text": "And then at least in the near future, I think we're kind of what we were saying.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2957.175,
    "end": 2963.561,
    "text": "We're probably going to see a big rise in using these kind of formalized frameworks to improve agents.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2963.621,
    "end": 2969.267,
    "text": "So like active inference or there's other Bayesian reinforcement learning schemes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2969.307,
    "end": 2971.249,
    "text": "There's typical reinforcement learning schemes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2972.631,
    "end": 2977.476,
    "text": "Those stuff are already being used, but I think there's probably going to be a big rise in that as the...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2977.675,
    "end": 2982.742,
    "text": " as the kind of benefits from just purely using like LLM reasoning kind of taper off.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2982.862,
    "end": 2984.765,
    "text": "I think we're gonna see a big rise in that as well.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2987.369,
    "end": 2989.652,
    "text": "That's very interesting.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2989.872,
    "end": 3007.497,
    "text": "Like going back to the sort of grass greener, we could see a situation where smaller interpretable active inference type models are using high dimensional interfaces like natural language sharing information.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3008.422,
    "end": 3037.77,
    "text": " and the opposite setting where large sort of opaque models are communicating through well-structured narrow channels that are active inference like state spaces and just sending each other that json kind of like as a semantically encrypted communication you know right it just says three one two and it's just like oh no",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3039.843,
    "end": 3042.106,
    "text": " That's definitely a really insightful point, yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3042.246,
    "end": 3048.213,
    "text": "It might just be that all these LLMs are communicating through, yeah, these narrow... It's kind of like how we communicate too.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3048.233,
    "end": 3052.357,
    "text": "It's not like I can't just directly pass my thoughts to you, you know?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3052.398,
    "end": 3058.384,
    "text": "Like, ideally, if I could, like, you would get a lot more information about what I'm thinking, like, what I'm feeling, stuff like that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3059.065,
    "end": 3063.811,
    "text": "We have to communicate through words, which is kind of a narrow representation of what our thoughts actually are.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3064.491,
    "end": 3066.694,
    "text": "So maybe LLMs have, like, a similar, you know...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3068.885,
    "end": 3092.023,
    "text": " lot to say about that but that again comes to the cognitive science topics of of communication semantics theory of minds you know how cool right exactly um yeah what are your next moves with your education and everything and then if there's anything else you want to add you can go for it",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3094.045,
    "end": 3099.374,
    "text": " I'll just continue to do this research, try to expand.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3100.235,
    "end": 3112.295,
    "text": "Right now, actually, what I'm working on is the thing I said about actually encoding knowledge within the agents instead of just using some unknown kind of info state modality.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3112.335,
    "end": 3117.083,
    "text": "I'm working on that right now, kind of using a hierarchical model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3117.4,
    "end": 3119.765,
    "text": " So I think that's up in your future.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3119.825,
    "end": 3131.73,
    "text": "Yeah, I just want to continue working on this whole idea of combining active inference and LLMs to do as much interesting things as possible and improve LLMs and active inference at the same time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3132.031,
    "end": 3141.454,
    "text": " One other thing I thought was really interesting was I was reading the paper on renormalizing generative models from Carl Friston and all of them.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3142.195,
    "end": 3149.533,
    "text": "But I think that seems really interesting because obviously one of the big problems with active inference is just scale.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3150.104,
    "end": 3154.314,
    "text": " it's kind of hard to just keep making more and more and more complex models over and over again.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3154.414,
    "end": 3163.135,
    "text": "Like LLMs and like typical reinforcement learning have it easy because they just put a policy on it and they just like put a bunch of data in there and then they're good to go, you know.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3163.796,
    "end": 3165.48,
    "text": "But we actually have to like",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3165.747,
    "end": 3168.189,
    "text": " painstakingly build all these generative models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3169.17,
    "end": 3171.393,
    "text": "And it kind of slows our progress down a lot.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3172.013,
    "end": 3183.324,
    "text": "So using these renormalizing generative models to kind of just take away that whole scale problem, it can just kind of run these group transformations on any space of any scale.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3184.926,
    "end": 3185.727,
    "text": "Would be pretty useful.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3185.747,
    "end": 3188.209,
    "text": "I'm trying to explore, see how I can use that as well.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3188.77,
    "end": 3191.232,
    "text": "I think one interesting thing with that is",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3191.363,
    "end": 3195.772,
    "text": " doing the RGM transformation in more abstract state spaces.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3195.832,
    "end": 3203.507,
    "text": "So in the paper, they use stuff like pixels and stuff like that, like physical spatial spaces, right?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3204.248,
    "end": 3212.224,
    "text": "But maybe doing it on something like language might be a little harder because how exactly do you... There's no...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3212.373,
    "end": 3215.558,
    "text": " exactly pixel equivalent of language.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3216.3,
    "end": 3217.321,
    "text": "Because pixels are simple.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3217.402,
    "end": 3219.505,
    "text": "You could just say, this is a discrete quantity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3219.806,
    "end": 3225.014,
    "text": "The only way you can differentiate these discrete quantities is by RGB values, and you're good to go.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3225.075,
    "end": 3228.34,
    "text": "But with language, it's kind of like you have",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3229.282,
    "end": 3232.407,
    "text": " it's not like each letter really means anything.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3232.927,
    "end": 3238.235,
    "text": "And putting letters together has different meaning, and putting words together has different meaning.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3238.816,
    "end": 3248.691,
    "text": "So it's kind of a hard, it's a much harder problem to solve to use the renormalizing generative models on these language spaces versus just spatial spaces.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3249.492,
    "end": 3251.415,
    "text": "So I think that's also an interesting problem.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3251.435,
    "end": 3256.142,
    "text": "But yeah, I'm just trying to think about these things, see what contributions I can make.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3256.843,
    "end": 3256.943,
    "text": "Yeah."
  },
  {
    "start": 3257.243,
    "end": 3257.603,
    "text": " Cool.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3257.683,
    "end": 3258.164,
    "text": "Yeah, great.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3258.244,
    "end": 3264.79,
    "text": "Comments from earlier Par and Friston et al work on reading as multi-scale active inference.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3265.29,
    "end": 3266.431,
    "text": "There's the eye saccade.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3266.711,
    "end": 3268.473,
    "text": "It's reducing uncertainty on the letter.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3268.993,
    "end": 3278.862,
    "text": "Then letter to letter to reduce uncertainty on the word, and then they go up to the third level with a narrative, you know, cat ate bird, or was it the bird ate the cat?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3279.242,
    "end": 3280.123,
    "text": "So those kinds of things.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3280.143,
    "end": 3284.747,
    "text": "You get priors over letters, transition matrices, all these kinds of things.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3285.528,
    "end": 3286.609,
    "text": "And then",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3286.724,
    "end": 3289.653,
    "text": " But that was, as you put it, painstakingly crafted.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3289.673,
    "end": 3295.249,
    "text": "And where could we abstract and have generalized templates",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3295.533,
    "end": 3318.283,
    "text": " so that perhaps an adapter module is crafted to connect a given domain or system of interest to some known interoperable scheme and then something like an rgm can take over from there um okay i'm gonna ask one more question from the chat and then that will be great okay but",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3319.495,
    "end": 3348.878,
    "text": " um are you using meta learning aspects in the system to enable self-guided research and are direction opinions guided simply by active or what was that last part or direction opinions guided simply by active okay um yeah there's really no meta learning",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3349.145,
    "end": 3367.373,
    "text": " the learning rate is stable throughout the entire thing um yeah the direction opinions are are like in which like which search action should be taking or should it should a search actually be taking it should a prompt actually be taking that's all purely guided by active inference yeah",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3367.96,
    "end": 3377.217,
    "text": " So that was kind of the main point was that the active inference has this formalized structure that can allow for like optimality in terms of like exploration exploitation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3377.257,
    "end": 3383.088,
    "text": "So the active inference agent is purely responsible for picking the actions and determining where to go next.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3383.289,
    "end": 3384.912,
    "text": "Epic.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3385.072,
    "end": 3387.957,
    "text": "Well, thank you very much for joining.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3388.158,
    "end": 3388.939,
    "text": "I hope you",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3390.37,
    "end": 3402.459,
    "text": " engage and find collaborators and replicators who are interested in this hot topic of active inference and LLMs and come back when you're ready for 95.2.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3402.56,
    "end": 3403.201,
    "text": "All right.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3403.321,
    "end": 3403.442,
    "text": "Cool.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3405.166,
    "end": 3405.627,
    "text": "Okay.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3406.549,
    "end": 3407.091,
    "text": "Thank you.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3407.351,
    "end": 3407.692,
    "text": "Thank you.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3407.792,
    "end": 3408.093,
    "text": "Peace.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3408.113,
    "end": 3408.514,
    "text": "Bye.",
    "speaker": "SPEAKER_01"
  }
]
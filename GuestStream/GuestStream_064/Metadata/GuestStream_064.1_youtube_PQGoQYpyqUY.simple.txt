SPEAKER_00:
hello and welcome this is active guest stream 64.1 it's november 29th 2023 we're here with elliot murphy returning guest and we'll be talking about this paper rose a neurocomputational architecture for syntax so for introduction and describing how today's stream is going to go elliot thank you for joining to you all right thanks so much daniel uh yeah i work at the uh ut health in houston in texas in the epilepsy


SPEAKER_02:
monitoring unit where we use intercranial recordings to map language in patients with epilepsy.

But today I want to present something a little bit more theoretical, a little bit more kind of philosophical almost.

This is a new paper that's just come out in the Journal of Neurolinguistics, and I thought it'd be good for me just to kind of walk through the paper step by step, give some motivation, give some description, give some kind of outline of the general kind of framing for the paper.

So the framework for this paper is trying to propose a general model for how to research language using intercranial recordings.

It's basically kind of move beyond research that comes from other kinds of extracranial imaging methods.

And it's trying to really figure out what is the most appropriate level of description in terms of neural complexity for specific aspects of language.

So there's many different parts of language, as we know.

Language has words, it has sounds, you can read language, you can sign it, you can construct meaning, you can do all sorts of things.

But the real kind of question for me is at what level of neural complexity and therefore at what level of appropriate recording resolution based on what tools you have available is going to be providing you with the most reliable signatures of particular levels of structure.

In this model, termed the ROSE model, I invoke representations, operations, structures, and encoding, and I'll introduce them step by step.

And so for the purposes of this kind of introduction, I'm just going to kind of outline what those general kind of frameworks are.

But to begin, the most kind of basic assumption that we have about language, in terms of like the findings from linguistics, so the study of language, like the scientific study of language, one of the really basic introductions

that you find in every introductory textbook is that linguists have kind of arrived at the conclusion that the human mind brain applies a set of rules to recursively combine linguistic units into larger objects, which derives an unbounded array of hierarchically structured expressions with humans in varying sentence meaning based on syntactic configuration.

Now that's a lot of jargon, but essentially what that means is when you hear a linear sequence of words, like, you know, we watched a movie with Jim Carrey,

You can pass that not simply as like beads on a string.

You can pass it in two ways, depending on how you chunk the individual phrases together and how you establish relations between different elements.

So, for example, we watched a movie with Jim Carrey can mean that you watched, you know, Eternal Sunshine of Spotless Mind, or it can mean that you're sat there next to Jim Carrey watching

you know, the new Napoleon film, for example.

So there's kind of ambiguity there, but the ambiguity arises due to structural configurations.

So that's kind of how you rearrange the sentences kind of in your head internally.

And that is obviously a process that's independent immediately from sensory motor

you know transformations or experience or speech or sound or orthography so these are very abstract processes and they're clearly mental processes they're not processes of the visual system they're not processes of the auditory system they're clearly uh hyroid and mental computations so what that phrase um unbounded array of hierarchically restricted expressions means is it just means that humans have this generative capacity and people like Carl Christen and Chomsky talk about generativity quite a lot what it basically means is the ability to just do exactly what I'm doing right now

use a finite set of elements, so lexical items or memorize words, and then recombine them again and again and again in really novel combinations, such that you can say sentences that have never been said before in the totality of human existence, but which are immediately assigned a meaning.

You don't have to think too hard about what meanings are usually unless you're reading, you know, Finnegan's Wake, for example.

So typically this is a reflexive process.

So it's automatic.

It's quick.

The brain resolves this very well without much effort, without much energetic cost as well.

So the question is, how does the brain do this?

So, you know, an additional point to make here is that this capacity for hierarchical occasion has also been linked to human-specific cognitive superiority.

So that's a more general background here.

So people like Stan Dehaene, Hauser and Wattamil, they've long stressed that this facility for constructing hierarchically organized linguistic structures is really potentially a human-specific function.

And that's a very long and, you know,

it's a long and difficult topic to get to get into so I'll put it to one side but it's an interesting topic to to explore the possibility that this whatever this process is it might in fact be unique to humans um so in the theoretical domain and turning to neurobiology an emerging consensus in neuroscience is that complex behavior and cognition rely on coordinated interactions between brain regions with face synchronization being a major candidate for a mechanism implementing this coordination by gating information transmission in the brain

But unlike for models of attention and working memory, there's currently an absence of oscillatory phase coding in models of natural language.

So what does that mean?

So in the neurobiology of attention and working memory, there's been a lot of really exciting accounts by people like Al Miller, Ole Jensen,

and a lot of other people trying to explore the relationship between neural synchronization metrics and some kind of isomorphic kind of relationship to some property of either an endogenous computation, like a working memory process, or some kind of property of the environment that you're kind of, you know, passing in real time.

But in language,

The best we have, just to boil it down very generally, the best we have at the moment in language is basically called cartographic models.

So that's kind of localizationist models in the brain.

So we have a decent idea, thanks to the really brilliant work by people like Greg Hickok and William Machen and Angela Friederici and Nye Ding and David Popel and really an incredible roster of people from the last few decades who very carefully mapped out which specific brain regions seem to be sensitive to different types of linguistic structure.

So lexicality, meaning the meaning of a word, syntax, the generation of a sentence structure, a phrase structure, semantics, the meaning of words, phonology and so on.

But what we really have right now is just a kind of map.

we have a map of the brain that you can pull up on google and you can google you know language syntax brain and you'll get a decent uh you know rough architecture of which parts of the brain seem to be sensitive to different parts of language but the main reason why i wrote this paper is because that's essential you know mapping out the terrain is obviously essential but the next step

People from working memory, vision, attention, they did this a while ago.

The next step is therefore to transition to a kind of how question, which is which neural mechanisms are involved in language.

So rather than simply saying, these are the brain regions involved, we have to then say, okay, these are the brain regions, but what is the brain doing?

By what operations is it performing?

At what level of neural complexity is it sensitive?

So just because this part of the brain is sensitive,

we don't really know in what way it's sensitive.

What type of neural processes is it actually recruiting to yield that sensitivity?

And what does that sensitivity really look like outside of bowl fluctuations or high gamma amplitude increases and other kind of typical measurements?

So that's in the theoretical domain.

In the empirical domain,

Invasive intercranial recordings have recently mapped out some of the feature space associated with how much phonological information can be transmitted by different neural mechanisms and some citations there for you to look at.

But within the domains of syntax and semantics, there's been less clear progress.

And the reason for that is pretty obvious.

Phonology, sound, auditory processing,

you know, the kind of a finite feature space involved in articulatory processes.

That's a very non-trivial, you know, field to explore, but it has been mapped out very clearly because the, you know, processes are kind of less abstract, more well agreed upon, more transparent.

They involve more clear,

sensory motor transformations and things that you can detect very clearly with very nice clear signatures in different types of neural state space or in fact different regions of the brain.

And these are also things that you can disrupt very easily.

So it's very easy to disrupt phonological processing.

We know which parts of the brain, which parts of cortex is disrupted.

But it's less clear how you specifically disrupt things like syntax and semantics.

You can knock out language comprehension in general, just all of language comprehension.

You can knock out and disrupt certain aspects of working memory performance.

But when it comes to knocking out, disrupting the brain's ability to specifically process syntactic or semantic information, that's been very difficult.

And there's only been a few papers on that, which I cite in this paper over the last couple of years.

But it's very difficult to really try and

provide causal evidence to try and really stimulate the brain in the OR or to find lesion patients who've had a stroke and who exhibit extremely specific niche deficits around this aspect of syntax or semantics.

So that's kind of the obstacle that we have.

The obstacle we have is that we have this very strange, potentially human-specific, seemingly very simple process of just building hierarchical structures

But it's very difficult to isolate from all these other things going on.

Because as soon as you build a sentence structure, as soon as you build a phrase or a sentence, you are also doing loads of other things.

You are also using your attention.

You're also using working memory.

You're also saying things or reading things or engaging your eyes or ears.

So it really is very difficult to isolate it.

So anyway, that's the problem that we have.

So I'll just skip this part to the next section here.

So I'm just going to read through some of this text and maybe try and unpack it a little bit more.

But the past decade has seen the emergence of low frequency phase coherence as a feasible index of hierarchical syntactic structure.

So what that means is that if you read these papers,

And there's low frequency oscillations in the brain that can be detected using MEG or scalp EEG.

And they basically show little fluctuations, little peaks

aligned with certain moments and periods or windows of structure building.

I'm just thinking of more metaphorical, little islands of structure that come along when you pass a sentence in real time.

Whenever that moment of structure building occurs, you seem to get this low frequency peak in activity in the brain.

It tends to be around classical language cortex, like left inferior frontal cortex or temporal cortex.

But the actual spatial temporal isolation is still a little bit iffy.

We still don't really know, quite frankly.

So that's the kind of background for that aspect of language.

At the same time, though, recent work has also examined local cortical processing using intracranial recordings.

So these are recordings inside the brain.

And what they do is they often focus instead on high-frequency power.

They can detect low-frequency power, absolutely, but most of the analyses and novel findings within the intracranial space have prioritised looking into high-frequency research for the simple reason that that's the only real way that you can get high-frequency recordings.

So this works to expose signatures of syntax in other areas outside of low-frequency dynamics.

So there's a big question here.

how these two distinct recording scales, low-frequency dynamics and high-frequency dynamics, could be combined in a coherent model of natural language syntax and cortical computation.

That hasn't been addressed yet.

So that's kind of one of the arguments that I try and build here.

So yeah, neural oscillations are effectively, like I mentioned, they reflect synchronized fluctuations in neuronal excitability and are typically grouped by frequency, and mainly for convenience, but not always.

There are, in fact,

there seems to be something else going on.

Common rhythms are often termed delta, theta, alpha, beta, and then gamma.

And broadly speaking, high frequency activity reflects local neural processing, whereas low frequency activity reflects regional synchronization.

So high frequency activity, you get that local cortical computation in a specific brain region.

This brain region is doing something, as opposed to low frequency activity, which is often more widespread.

And it's more to do with the coordination

of regional synchronization.

So the low frequencies say, okay, it's time for you guys to all come online at this particular time to, you know, engage your spike activity in this particular time period, but over a larger portion of tissue typically.

It's over much larger areas of the brain.

And then there's a bunch of other kind of computational differences seemingly as well between these lower bands.

So for example, alpha is very often implicated in cortical disinhibition.

It kind of helps to shield and protect

ongoing representations from you know decay and transformation and then Theta oscillations often implicated in hippocampus I've been involved in learning and memory there's a big literature on that don't need to go into the details but essentially there's a whole you know panoply of low frequency rhythms and they seem to be sensitive to different cognitive processes or at least you know recruited or they seem to index a bunch of lower order processes potentially

Okay.

So one way to kind of introduce this is through a model that I proposed in my book in 2020.

And in this model, again, using invoking low frequency power,

The combinatorial power of language is indexed via various oscillatory interactions like forms of cross frequency coupling, which is basically when the phase of a lower frequency rhythm kind of coordinates and dictates the firing of a high frequency rhythm.

So you kind of get a potential facilitation of what's called multiplexing, or kind of packaging representations in one order and then sending them downstream and kind of re-accessing them in a different part of the brain.

So in this book of mine, there's a bunch of literature that I've reviewed to motivate the idea that delta theta phase amplitude coupling constructs multiple sets of syntactic and semantic features.

So you get this initial low frequency phase interaction between delta and theta.

And this occurs when the phase of delta is synchronized with the amplitude of theta.

And delta represents superordinate syntactic categories, whereas theta represents feature bundles generated via lexical access.

What does that mean?

It means that these theta oscillations, there's something called theta-gamma coupling, famous in memory research and navigation.

Theta-gamma coupling, and in fact, there's some really nice work published yesterday, I think, by Oli Jensen, looking at how attention might be

also implemented through alpha gamma coupling but the basic idea is that you have individual sets of features that are you know basic mental representations like things responsible for you know animacy plurality uh basic conceptual categories they seem to be triggered by high frequency activity and then they're coordinated and packaged into a single kind of chunk

by theta so you have a single low frequency theta wave and within that single wave you get a bunch of individual you know five six seven or eight however many you can kind of squeeze and fit in uh and there's kind of trade-off between fidelity and uh the kind of number of um gamma oscillations you can package inside a theta cycle um

But that packaging of theta with gamma helps you to kind of coordinate and unify an individual representation in the attention and visual spaces that's been referred to as, you know, coordinated visual attention when you kind of your visual your visual system is kind of attuning to particular properties in the environment.

In the language space, I'm kind of invoking it to unify different individual features that compose into a lexical item.

So a lexical item is just a fancy word for a word.

So something like, you know, laptop, bed.

I'm just thinking of random words.

All of these random words are obviously composed of individual features.

conceptual features right so a bed is inanimate it's uh you know it has a particular geometry it has a particular function a purpose a design and so on and all these different conceptual features compose into what we mean by a bed whenever you think of a bed or a chair or a mirror um or you know Whole Foods it could be anything it's simple or anything from simple to complex you have a constellation of conceptual features that are being composed somehow and the idea here is that

These low frequency theta oscillations are there to coordinate the firings of those individual features, which is why you get a lot of lexicality effects being found in, for example, the theta range.

And then what you do is, in this model, you get those theta bundles of flexible features and you embed them further.

So you nest them again.

And what you do when you nest them again within a delta rhythm is that the delta rhythm then takes over from the individual lexical items and it composes a syntactic identity, so a structure.

a structural identity.

So for example, it's been well studied in linguistics that when you put two items together, two words together, you get a kind of symmetry breaking operation where one of the items kind of determines the category of the phrase.

So for example, if you say red boat, a red boat is a boat that is red.

A red boat is not a red light quality

that has boat-like features, for example.

So there's a kind of immediate asymmetry.

And you get that all over the place.

If you say John ran, John ran does not mean that there's a special kind of John who is exhibiting a running feature.

It means that there's an event of running and John was its agent.

So there's always an immediate kind of,

asymmetry in language.

And that's, you know, people like Stan Dehaene have kind of speculated that one of the interesting things about language is that unlike music and mathematics that kind of allow symmetry in their structures, language doesn't really like symmetry.

As soon as you get symmetry in language, the language system tries to break it somehow.

Because there has to be some kind of categorizer for you to identify what's called the label or the category of the phrase for you to get a single unified syntactic representation.

So in other words, whenever you get

two words being combined together, one of them always wins and one of them is the loser.

And that's how you build syntactic structures.

You get a kind of categorization process.

So just to kind of recap, you have Delta,

Peter and gamma the delta assigns or it somehow derives or facilitates the syntactic inference of what the categories and the reason who knows right i'll try and explain some potential reasons here and ultimately it's unknown, but we do know that in the empirical domain, we get a lot of these syntax signatures being found in the delta range so delta seems to.

care about chunking on some level maybe not syntactic specific chunking but i think it's possible um for reasons cited in the paper so delta and then and train then packages within one of its waves multiple theta rhythms uh and the reason why is because the same mechanism that you invoke when you get theta gamma coupling so it's a series of nesting you know nesting and nesting and nesting

and you know the whole rose architecture in this paper is basically trying to construct uh what some people might think of as rather crude but i think rather empirically motivated in fact um isomorphic relationship between structure and neural kind of signal complexity right uh it's it's common for people to think well you know the brain's very complicated language is very complicated it surely can't be the case that a particular level of linguistic structure maps directly onto a particular resolution of

for example, oscillatory phase coding, I think there's good empirical evidence to actually assume that.

And that not everything takes place at the lower coarse grain level, at the spike level, for example, or even intracellularly.

I think it's unlikely that all of these complex processes from making inferences about, you know, whether or not red or blue wins and becomes the categorizer or the adjective or phrase is all taking place at the intra or intercellular level, at the kind of global, you know, the kind of stuff that you can detect using single unit recordings, for example.

I think something more global and interactive is happening.

And that's why you kind of need to invoke some kind of phase code, because the phase code, you only get that once you transition to more global synchronization metrics of the kind found with low frequency dynamics.

That's the whole point of low frequency synchronization.

So I think that's kind of the kind of basic natural progression that you find here.

least in the way that i've motivated it in using the literature that i review in the book and kind of is recapitulated here to a certain extent

Okay, so then we turn to something called phase resetting.

So phase resetting, alongside concurrent encoding and storage of the products of these generated objects, permits a facility for recursive self-call.

So that's kind of where you get the recursive mechanism of language.

You're consistently re-nesting objects into this more complex delta rhythm, chunking out more and more and more low-frequency complexes, essentially.

And just as an aside, we also know that from empirical work by people in my lab and in the Netherlands as well, that you get a bunch of different dynamics alongside this.

So, for example, we know that beta rhythms and other types of gamma rhythms are involved in all sorts of different types of processes like conceptual storage and maintenance of a particular structure in memory.

And so there's a very kind of complex interplay here between different rhythms and it's not intuitive.

It's not easy to grasp on first.

You know, I realize I'm not potentially not doing the best job of explaining it because there's a lot of things that unpack as soon as you move to another kind of concept, you have to unpack it to a certain extent.

But the basic idea is that there's a number of interacting low frequency rhythms.

that then dictate the firing rate and coordination of these faster rhythms.

And the faster rhythms are responsible for the smaller, more atomic features like individual lexical items, and the slower rhythms are responsible for the more complex, larger structures like syntactic identities,

and syntactic workspaces and things that you construct.

So the kind of output, I guess, the output of the syntactic generative engine is found at the lower frequency end.

Whereas the input, the kind of stuff that you need to provide to the system to get any syntactic structure building done in the first place, like, for example, lexical features, you need at least to access some features before you can combine them.

In order to combine something, you need to have something to combine.

Those individual properties are hosted by the high frequency oscillations, essentially.

Okay.

And then finally, traveling oscillations are also going to be relevant here.

So these are essentially these low frequencies that I mentioned, except they exhibit a particular directionality.

They migrate in phase coherence across the brain.

And what that means is they coordinate the spiking of neural clusters across the brain, across fixed points.

So they kind of move from one part of the brain to another part.

And as they do so, as the phase of the low frequency is shifted across a particular path, they then trigger

particular spiking profiles of these neurons that they travel across.

So it's a coordination process.

And it's also, of course, given the nature of a migration path, that naturally leads to the conclusion that, as is often commonly known, different parts of the brain are innately sensitive

to particular representational features of the environment, particular conceptual features, things like faces, places, orthographic information, emotions and things like that.

There's a very big difference, for example, between inferior parietal cortex and anterior temporal cortex in terms of

which conceptual features they seem to be sensitive to.

So if you get that traveling profile, you get that low frequency phase code moving across a particular extended path, then that immediately gets you essentially for free.

It gives you for free the mechanism of activating distinct conceptual

representations during that process and so I I think you know there's a lot of other things to say here but traveling waves have been implicated in a number of kind of behaviorally relevant and processes in humans and also non-humans um and they seem to support brain connectivity and function more generally and so yeah that's kind of the background for the traveling wave mechanism um and then finally one final thing to say here um I mentioned the Delta Theta

and gamma complex, there's also been some interesting work outside of language looking at delta gamma feature combinatorics.

So that's kind of skipping out the middleman and simply involving this low frequency delta complex being phase coupled with the amplitude of high frequency gamma oscillations.

And what that seems to be involved in is feature combinatorics in general.

So, for example, it's been involved in fluid intelligence.

So when you have to just perform a particular task or solve a problem immediately and call upon various cognitive representations from different core knowledge systems in your mind, lots of different aspects of intelligence, you have to kind of recruit here and there.

You don't necessarily have to.

In fact, you don't at all construct a hierarchical structure.

You don't construct a hierarchical recursive representation.

You just combine things together into a set, into a conglomerate, to help you solve a particular puzzle.

So that's what fluid intelligence is all about.

You also get that with, say, in early stages in phrase composition in ScalPEG.

So people like Andre Martin and Jonathan Brennan have done some nice work showing this.

So what that tells you is that in language and also non-language, you can combine features.

for all sorts of reasons.

If you need to do that, delta-gamma coupling seems to be sufficient, and it seems to be a very efficient way of doing that.

Now, if you want to construct an unbounded array of hierarchically structured expressions, as I mentioned earlier, that might not be sufficient

And so the empirical argument I'm making here is that delta gamma coupling will not be sufficient for the kind of rich syntactic phenomena that are often documented in standard textbooks, for example.

So anything beyond a basic phrase unit, I suppose, anything with long distance relations.

or dependencies and things like that or in fact anything beyond a single phrasal structure will probably need to be um called upon all these other mechanisms that i mentioned earlier the more kind of richly layered uh levels of low frequency interactions so um so delta gamma is fine for you know feature combinatorics but it doesn't seem to be sufficient again that's an empirical claim could be wrong about that but it doesn't seem to be sufficient based on the literature for the more kind of complex structures of the kind that we uh that we all know

in the linguistics world um okay and then we can also make some interesting claims here that i've made with my colleague antonio benitez bracco in in seville in spain and all these aspects of oscillatory dynamics obviously result from somewhere they don't come from nowhere they result from genetic guidance and some recent workers provided a list of candidate genes for this particular guidance so we have a particular phase code a neural code for syntax but it's got to come from somewhere

unless you're, you know, well, I'm not going to name any names, but unless you're the kind of person who's so in love with, you know, deep learning and large language models that you think that it's all just some kind of epiphenomenal domain general learning process.

But if you don't believe that, then there's got to be some kind of genetic structure for language.

And so we've mapped a bunch of these potential genes using a bunch of different techniques to particular functions.

But that's a very much an ongoing kind of far off project.

It's just the very beginnings of trying to map out the kind of genetics of the neural code for language.

But at least it's a coherent project, right?

It's a legitimate coherent project.

Although rather nascent too, to be fair.

Okay.

So just to background this discussion a little bit more, I realize we're already quite far in, so I'll just kind of skip ahead a little bit.

One of the most important questions in the neurobiology of syntax is essentially, you know, what type of neural activity, low frequency activity, high frequency activity, spike rate, can track very long or very brief phrases?

So my basic answer, which I've already essentially given to you, is associating roughly the respective scales of recording and neural organization with particular levels of linguistic complexity.

So low frequency gets you structures, high frequency gets you operations, and spike rate, individual spike trains give you

basic representations.

Okay.

So this section here is slightly less important for our purposes, so I'll just skip over it.

But essentially, my claim in this section of the paper, section three, is just to kind of review existing models of language in the brain, neurobiology of syntax in particular, and just to show that they essentially tick many of these boxes, but not necessarily all of them.

So certain models emphasize the R or the O, but they don't talk about the S or the E.

Or in fact, in my own book, I talk about the O and the S, but I don't really talk about R or E at all, in fact.

Other work talk about potentially a combination of all these features, but don't provide a particular mechanistic relationship between how these features relate.

So, for example, you can talk about the neurobiology of R and O and S and E, but even if you have that, you still need to have some way of causally relating them, which is the kind of final section of this paper.

which is how you get, so for example, you know, I mentioned that the representations, the R unit is hosted at the spike level and the E unit encoding is represented in low frequency traveling waves.

But how the hell do you connect those two things, right?

How do you directly, what mechanism do you coordinate them?

So we know there's some recent work showing that in fact, traveling waves can in fact directly coordinate spike LFP coupling.

That's some very promising work there to kind of unify all these features together.

But you've got to have a mechanism to do that.

So you've got to have a separate mechanism in between R and O, O and S and S and E. It doesn't just come for free.

OK, so a lot of these previous models are very promising from a cartographic perspective.

They really do help us understand which parts of the brain are involved in different aspects of syntax or semantic.

But in terms of providing the how question, right, like which neural mechanisms exactly are involved, that's typically lacking.

The best candidate you have is definitely the work of Stan De Haene.

He's done some really nice work showing that signature syntax might be in places that most people have not really thought about, especially at the lower order level in individual spike, spiking activity or cellular barcodes and so on.

That's very promising and it's something that I invoke in this paper, in fact, as well.

um okay so just to introduce this a little bit further um these four components of rows rrc enter into every syntactic and phenomenon everything as soon as you get a syntactic structure you need to have at least all of these things um okay so in terms of representations i define them just like any old uh mental representation essentially

They are an instruction to provide a particular conceptual feature.

They are composed of features determining constraints on operations as well.

And that's a really important point, by the way.

I do want to stress that.

It's well known in linguistics that words have things like selectional restrictions.

So a verb will often co-occur or will need to occur with, say, another word.

You can't just put words in any order that you like.

So, you know, words are kind of like magnets.

They kind of try and attach to certain other words.

And often that's independent of statistics, by the way.

There's some good work by Andrea Martin and Sophie Slats on the relationship between statistics of language and structure in language.

And they're not the same thing at all.

They often correlate and they often aid each other in various ways, in very intricate ways.

But they're not the same thing at all.

You can't simply derive all possible structured language just from the frequency properties of words and the bigram and trigram frequency rate.

You need something else.

You need kind of structural information.

So the fact, this is kind of maybe a subtle point, but the fact that individual representations do in fact have selectional and agreement and movement constraints already tells you

there's got to be some kind of relationship between or dependency between the nature of how we neurally represent these representations and how they then transition into the operation stage right the o stage and there's got to be an initial dependency and

So some of these features can be maybe a little bit borrowed.

But linguists often talk about things like demonstrative, noun, plural, and so on.

These are, of course, human-defined.

The way that our conceptual systems work, we've tried to define these things using these terms.

Are these cognitively plausible or neurobiologically plausible?

Maybe not.

But there's at least some kind of atomic unit that's being evolved here, a unit, just call it X, right?

So this feature X and Y, they are being composed into particular terms.

uh configurations so maybe these terms these english language terms are just convenience a kind of placeholder for what these conceptual features really are so to speak but i i think that language is slightly misleading for a reason that people like chomsky and david popel have talked about and a lot of these things really just come down to

kind of classic marion levels you know david marge three levels of computation algorithm and implementation and all of these things are totally kosher and legitimate at the computational level and so therefore it's it doesn't really we don't really have to abandon them necessarily just because um we can't find uh you know what some what looks like an amount of feature in the brain that's you know a lot of this involves kind of category errors i think um so which is why i try and break it down into more atomic stages

So syntax builds structure through recursive applications of merge.

Like I said, you have this basic merge operation, which I haven't introduced yet, I realize, but it's basically just a binary set formation with some kind of categorization.

So you form a set, you just put two things together, and then you categorize it.

You give a category, you know, a noun phrase or a verb phrase or something like that.

And there's much more to be said there, but that's essentially the basic property of language, just building sets, set formation.

Okay.

So this is the outline of the model.

At the R level, as I mentioned, you get single unit encoding of conceptual features and what I call formal syntactic features.

And these are basically features that I won't bother going into, but there are certain features

like q and phi and other things that are other features that are interpretable or uninterpretable things are features that only enter into syntactic relationships and don't have any other role they don't have any other role in providing instructions to phonology or anything else

But so these features involve what's called a cellular barcode for distinct features that compose into syntactic objects coherently bound by these high gamma activity at O. So the O stage is the later stage.

And it also involves vector codes.

This is invoking a lot of work from John Hopkins University and people like Standa Hain again.

The R level also involves vector codes for ensembles

neural ensembles, cellular ensembles, hosting features common to objects represented at O, and which are ultimately coordinated by the S level, the low frequency level.

At the O level, high gamma activity.

So this high gamma activity is, like I said, it's a neural activity that is around 60 to 150, 200 hertz amplitude in the brain.

These high gamma sensory motor transformations

So things like high gamma activity provides access to centromotor transformations, and they transform these centromotor representations like sight and sound and orthography and so on, things that you read, things that you are exposed to, they transform them into lexicalized objects.

and if you're interested in the particular regional involvement there's a list there and then they are they they are therefore accessible as i mentioned earlier they're accessible to delta or theta phase locking so i mentioned that the theta gamma coupling where the phase of theta dictates the amplitude and the firing rate of the high frequency oscillations so you know you only get that once you have high gamma activity and

because that's the sufficiently correct resolution, the optimal resolution, it seems, which the brain likes to coordinate long-distance relationships and transfer information from one part of the brain to the other, essentially.

So this level can implement the semantic composition of language-specific concepts that coordinate the firing of R units.

So this high gamma level has also been implicated in basic semantic composition

So in a paper of mine, I've looked at the composition of basic adjective noun structures using intercranial recordings, and high gamma activity seems to be sensitive to this basic aspect of semantic composition, not necessarily syntactic composition, but certainly semantic composition.

It could also be involved in syntactic composition, but certainly semantic composition.

Syntactic composition, again, would necessarily involve the S level, S for structure.

Okay.

And then, of course, the high gamma activity itself also activates these ensembles that I mentioned at the R level that are composed of distinct units hosting this barcode or vector code for the finite list of units that compose into the feature bundles that you get whenever you access an individual word.

Every given word is literally just a linear list of these features that you kind of trigger whenever you access it.

And again, all of this stuff is only talking about the syntax semantics side.

The side of phonology and sound and gesture and orthography and all the rest of it, that's a separate issue.

That's a totally separate issue.

I'm only talking about interpretation, comprehension, planning, thinking, that kind of thing.

I'm just talking about internalization here.

Okay, turning to the S-level, as I mentioned, this is a low-frequency neural program for generating structural inferences over the O-level.

So delta-theta phase-amplified coupling, as I mentioned, gives you categorical inferences that allow you to modulate the representations of these feature bundles, so the delta can coordinate the theta-gamma complex, and it has some say in the sequencing and the triggering, and the

the kind of um like i said yeah basically the triggering of these features and how they are accessed and basically the readout but what's time to read out right and then finally e the encoding phase and this is what i mentioned earlier with the traveling waves where you get the local and global workspaces for bottom-up lexical memory and top-down hierarchical memory um traveling waves implement delta theta coupling for hierarchical memory

Whereas the theta gamma coupling gives you a kind of bottom up lexical memory.

It's kind of linearized memory just for lexical features.

Whereas the hierarchical memory, the memory about hierarchical relationships in a structure.

So, you know, abstract relations in a tree structure,

independent of any linear order or independent of any sensory motor transformations triggered by gamut.

There's a separability between the structural inferences made and the lower level sensory motor operations that you get and are accessed by gamut.

And then, as I mentioned earlier as well, alpha power can be involved in the shielding of this process.

An increase in alpha power towards the end of sentences is often indicative of some kind of accrual of semantic attention or semantic working memory or something like that.

So I think that's potentially related to this issue too.

And then finally, beta power also codes for syntactic predictions.

There's been some interesting work looking at how beta power seems to be sensitive to successful or unsuccessful, but basically triggered or not triggered syntactic predictions, or at least anticipation.

of building a syntactic phrase or you're about to you know you've been exposed to certain words and now you're about to generate a structural inference or you're predicting a specific syntactic item beta power seems to be involved in the indexing of that sensitivity and but notice something interesting here right that's beta power

This whole notion of prediction and anticipation, that is a separate process from the core structure building inferential process that's been exhibited by Delta and Theta and has already been empirically documented in Delta and Theta.

So again, this is a kind of neurobiological support for the separation of predictive algorithms, predictive processing,

and more kind of old-fashioned traditional structural inferences that are made by the brain.

So just to unpack that a little bit more, it would be a problem for me, it would be a problem for a lot of people if all of these processes, anticipation, prediction, coordination, attention,

and also syntactic structural inferences were all indexed and all coordinated in this delta and or theta rhythm.

That would be a problem because how the hell do you separate them out, right?

It's very difficult to think about that.

You know, if that were to be true, then we would probably have to abandon the notion that we can construct a coherent oscillatory phase code for language.

It might be impossible.

You just have to just give up on it and look elsewhere.

But the fact that we can, in fact, find the separability, I think speaks to this potential.

is why i wrote this paper because i think you know i think there's some good um some good potential here okay and then this figure down here um if i can maybe just zoom in a little bit potentially um this just depicts um exactly what i mentioned uh in verbal form in visual form so the representation level again these individual features here that correspond to individual cells sensitive maybe not selective but sensitive

to each of these features.

The question of selectivity versus sensitivity, that's purely empirical.

I enjoy philosophy and theorizing as much as anybody else, but these questions are, at this stage, purely empirical.

It's possible that there are certain cells that are selective or sensitive to all these features, but at least that's the framework.

There's a particular cellular infrastructure that facilitates these representational accessing, essentially.

When it comes to operation, again, I need to be careful with the term operation because maybe this is the one that I haven't explained well enough, so I should maybe spend a minute just explaining this.

The brain carries out phonological operations, it carries out semantic operations, it carries out syntactic operations, it carries out all sorts of operations.

In the context of rows, I'm focusing on how it carries out the combination, the initial

set formation process involved in merge.

So the early stage of merge, essentially.

The O versus S kind of captures the distinction between merge and labeling, essentially.

The distinction between set formation, just building, you know, putting two units together and then transferring it into a category.

So I guess what I'm trying to say is the categorization process will not be found at the O level, at the high gamma level.

You might see some category sensitivity in high gamma.

You might see different gamma profiles for verb phrases versus noun phrases.

And in fact, we've already found that.

People like Andrea Mara has published papers on this.

So we know that there's a syntactic or semantic sensitivity at the O level.

Again, the difference between verb phrase and noun phrase is not just syntactic, it's also semantic, right?

That's the problem.

Big issue here of trying to separate syntax from semantics is that whenever you modulate syntax,

you also modulate semantics, and whenever you modulate lexico-semantic content, you also therefore modify everything else, syntax and semantics.

So that's an issue.

And there's, you know, we're trying to, well, we are in fact, you know, running some designs at the moment to try and experimentally tease those apart.

But for this purposes, for the current purposes,

the term operation is referred to unifying different lexical and semantic features together, essentially, into a coherent unit, into a set.

And then once you've got that set represented in Gamma, then you turn to the structural level.

And the structural level is more higher order, more complex, more global, more interactive, and therefore more low frequency.

That's the kind of goal, that's the kind of framework here.

And then finally, the encoding level, I mentioned very briefly, maybe a little bit too briefly, the two types of memories that you get in language.

There's been some really good work by people like David Adger, Queen Mary, and a bunch of other people who've kind of explored the different types of syntactic workspaces or linguistic workspaces.

Like when you comprehend a sentence, you don't just have a single, well, you might in fact have a single workspace, it's possible.

but it's also possible that you read it out into different separable workspaces that are responsible or sensitive to different types of memory representations.

So it's often been implied by people like William Machen, I think very legitimately, that inferior frontal cortex is really involved in syntactic working memory.

Specifically, we don't know exactly what features of syntax, but certain types of syntactic information are held there and maintained there in short-term memory.

But so for lexical memory, I'm invoking theta-gamma coupling.

And then, as I mentioned briefly, for the hierarchical memory, I'm invoking delta-theta coupling.

And then unifying the both of them or connecting the two was this other form of processing that I mentioned earlier, the delta-gamma coupling, right?

So the delta-gamma coupling, as I mentioned, is just involved in basic domain general, very domain general,

combinatorics, just combining two different representations from different parts of the mind.

But nevertheless, you'll see the relationship between, you know, delta and gamma between both of them.

Now, when it comes to the spatial temporal dynamics of this process, the actual kind of, you know, how this is implemented in the brain in terms of which networks, which white matter pathways, and so on, that's a purely empirical question.

But we do know enough from the MEG, from the scalp literature, from the intracranial literature,

that these codes are legitimate codes to be implemented across some portions of cortex.

I have a bunch of speculations empirically supported also in terms of which parts of the brain these particular encoding workspaces are going to be found and going to be documented.

But at least this is the general abstract schema, if you like.

This is the kind of very general schema.

And the reason why I've emphasized that is because, as I mentioned earlier,

every other paper in neurolinguistics focusing on syntax is almost entirely cartographic.

it's almost entirely localizationist in terms of, let's figure out if it's, you know, is it IFG?

Is it PT?

You know, is it ventral temporal?

They're the kind of questions that are framed by most of these models in the literature.

And though essential, that's not my concern here, right?

My concern here is, yes, localization, but let's try and focus on which particular abstract neural code is going to be generalizable enough across different parts of the brain.

And also, by the way, small point here,

and also generalizable across cases of plasticity.

you know, I'm involved in a neurosurgery case today, we know that the brain is highly plastic.

When you resect certain portions of tissue, recruited or not in language, you get fairly rapid organisation over a period of weeks and months, sometimes years.

But that tells you immediately.

And in fact, there's some also really fascinating work in congenitally blind people and or deafblind, I think too, showing that portions of their brain

uh yeah in the congenitally blind there's portions of occipital cortex that come online and are sensitive to syntax so because you know these people are not using their occipital cortex for vision they don't need it uh they end up uh recruiting it for other other purposes too sometimes linguistic purposes so that's why i think focus on a more abstract code is more important maybe not more important maybe that's maybe that's unfair but i think it's potentially just as important if not more important than trying to figure out the kind of cartographic landscape

for how language is implemented.

Also, by the way, another small point, which is maybe not so small these days, this is also very important if we care about things like brain-computer interface devices.

So, you know, neurotechnologies.

And so there's a bunch of what's called BCI brain computer interface devices that you implant in patients brains to help them with either the epilepsy or to help them with speech processing.

You've got quadriplegic patients who lose their speech.

And there's some other efforts recently to try and help people with all sorts of language deficits by using neural implants.

There's a factory not too far away from me north in Austin, and there's places owned by Elon Musk, as you know, who owns Neuralink.

These guys are trying to do something similar.

But that's a speech.

When it comes to designing a brain-computer interface for things like syntax or semantics, I think we're going to need to pay attention to these aspects, these questions to do with the neural code rather than just the region.

Because if you're going to have somebody who has a syntactic deficit,

or a semantic deficit, and you're going to try and help them get their language faculty back.

It's no good just knowing which part of the brain is going to be recruited for their language function.

With a BCI device, you need to coordinate particular stimulation profiles and particular ways to coordinate and trigger and suppress brain activity.

And so in order to do that, you've got to have the right parameter space.

And to figure out the parameter space, you therefore need to have a question of neural code involved.

almost definitely a phase code, some kind of high frequency or low frequency phase code.

So all of these questions that I'm asking right now, don't just have kind of theoretical, you know, basic science, academic implications.

Obviously, that's my interest.

That's, that's my main concern here.

But they will also definitely have kind of, you know, implications for patient treatment in the BCI space moving forward.


SPEAKER_01:
Okay.


SPEAKER_02:
Right then, let's move on.

Let's skip past some of this.

It's getting on to almost an hour now, so I'll skip through.

The next few sections are just outlining representation, operation, structure, and encoding.

So I'll just pick out some of the most important moments from here.

But again, this section number five just redefines, again, and maybe exhaustively, what I mean by representation, some example features,

And in fact, what these gamma oscillations and high frequency oscillations might be indexing more generally in terms of their collective scope and their computational kind of facility.

This is just a little brief definition of the kind of scales that are relevant to the different levels.

So we have single unit activity, multi-unit activity,

the envelope of the spike, and the LFP.

These are different aspects of neural organization and recording resolution that will be relevant to the different components of ROSE.

And that kind of just outlines the scope of inquiry here.

This was something that I put in to certify, make it clear that I'm talking about representation in this sense and not that sense.

Basic units, again, I'm just talking about what these basic units are.

um and then i'm turning here to concerns about um how to distinguish it from things like operations um okay and then oh yeah so this is an important paragraph i think um so this recent work by nelson fl discovered preferential activity for um content words over function words in the anterior temporal load this is single unit recordings again i should i should stress sorry um

So that already begins to tell you that at the single unit level, at the silo level, different parts of the brain, I mentioned ATL already.

In this case, ATL does seem to be sensitive to lexical type.

Again, that could be due to other reasons.

In this case, they looked at it independent of word length and word position.

The big confound here is word frequency, but there's some of the kind of indication here that potentially something to do with lexicality is driving this difference, this sensitivity.

Okay.

And then more generally, what are termed concept cells that preferentially activate for stimuli that occupy a certain position in some category or axis seem to support a declarative, seem to support declarative information.

What does that mean?

It just means that there's some really nice work by Bauer et al.

showing that there's a kind of feature space in a primate and also a human

and ventral temporal cortex that is sensitive to a kind of axis of animacy.

So whether or not an object is animate or inanimate, like a chicken versus a laptop, there's some things that are kind of in between and vague.

And then whether or not a surface is spiky or stubby.

So kind of, you know, like this or spiky.

So kind of a kiki booba kind of situation for linguists.

So that's kind of a nice conceptual geometry that you have going on.

And there's different cells, concept cells, that preferentially activate the stimuli that occupy a space within that axis.

So that tells you very nicely that at the unit level, we have a potential neural code for this very generic, but also all-encompassing object space.

This is Doris Sauer's lab, the Barrackel paper.

So, you know, the kind of promise, the promise that this gives us for linguistic features, I think is quite interesting because linguists have spent a long time, you know, reminding us that the feature space in linguistic semantics can also occupy something like animate, inanimate, and then some other, a bunch of other abstract concrete kind of feature spaces too.

So I think it's very reasonable to expect and to predict at the unit level a similar kind of conceptual geometry taking place in a different part of the brain, potentially posterior temporal cortex, potentially inferior frontal cortex.

Who knows?

We don't know until we explore.

But I think that kind of reinforces the definition of the R level that I'm defining here.

Okay.

Right then.

Okay, so this next section here, mapping from R to O, this gets at the very tricky issue that I mentioned at the beginning.

So at the beginning of the paper, I, you know, lampoon and criticize all these other people for not doing this.

So hopefully I do it, you know, a decent job, otherwise I will embarrass myself a little bit.

But the kind of motivation here is that we don't just need a definition of what R and O looks like in the brain.

We need a way to relate R to O. We need a way to feed the information at the R level, the unit level, all the way up to the O level.

So I mentioned, just a quick reminder, R is a single unit spike train activity.

The O level is high gamma activity.

So they're kind of slightly more easy to detect using standard intercranial measures, for example.

Okay, so how can we relate these ideas, the O to the R?

So in this section, I kind of propose that the relations between multi-unit activity and population signals hold the answer.

So I might just have to focus on this for a little bit.

Oh, this is only a short section, very short section.

That's good.

OK, this will be quick then.

So local field potentials reflect the common synaptic activity of a population of neighbouring neurons.

And spikes are short-time high frequency content signals reflecting individual cellular activity, right?

So individual cells.

Neural synchronization can be evinced.

One of my favorite words there.

They can be evinced by temporally relating spiking activity to the background oscillations of LFPs.

And this relationship has been documented across multiple brain regions and cognitive functions.

So this spike phase coupling has functional consequences.

And there's some examples there if you want to consult.

Okay, so that's the coupling between the LFP and the actual spike, the individual spike.

So that's kind of an analogous, right?

If you think about it, that's analogous to what I mentioned earlier in the low-frequency space, right?

I mentioned the delta-theta coupling and the delta-gamma coupling, etc.

This LFP phase coupling is essentially the same thing, just on a smaller scale, right?

So it's kind of nesting all the way down.

So LFPs are highly effective means of exposing what state a given cortical region is in.

since they capture general dynamics not specific to any individual cell.

And this is a really cool point, actually, about LFPs.

They actually host types of information that are not detected at the spike level.

So that's kind of an important point there.

And also, of course, there are cells that don't spike too, right?

So in the same way that there's likely much information available at the LFP level that's not represented at the unit level, like dynamics and amazing properties exist that can only be detected at the level of summed activity of a million neurons,

so too is it expected to be the case that there are aspects of cortical computation that are only represented at the interactional global scale, even at the state scale, like, you know, hidden Markov scale, kind of autoregressive hidden Markov models and so on.

You can kind of generate these more abstract state spaces across networks of brains, networks of reasons, I'm sorry.

And I think that will also be relevant at some point too in the future, but maybe I'll get to that at some point later.

And that information is not represented at the LFP or spike level.

So that basic finding, empirical finding, is another core motivation for the ROSE model.

The fact that you do find this dissociable level of information representation across these scales already tells you that there's very likely to be this scale of computational complexity at the linguistic and non-linguistic level too.

Okay.

And like I said, it's a basic presupposition of rows, but it's also a presupposition of other major frameworks in cognitive neuroscience, right?

Not just me.

I'm not the one to kind of, you know, propose this.

So an example I counted here is working memory, which seems to involve discontinuous bouts of spiking activity as opposed to steady state neural dynamics, right?

So often it was initially believed that working memory, in order to maintain some of the working memory,

There's got to be some kind of direct, intuitively direct neural correlate, which looks like it's maintaining or staying at a stable level or amplitude, steady state or increase over time, etc.

Turns out not to be the case.

OK, so relating R to O is far from trivial.

Yes, that is absolutely correct.

Consider how communication through coherence has typically been assumed to reflect phase synchronization between oscillators.

Recent work has offered an alternative mechanism through which coherence is the consequence of communication and emerges because spiking activity in ascending area causes postsynaptic potentials in the same, but also other areas.

That makes things more complicated.

So these authors identified afferent synaptic inputs rather than spiking entrainment as the principal determinant of coherence, opening up new directions for framing the relation between units and coherence.

So LFP coherence appears to be determined by two factors.

Coherence due to the direct contribution of afferent synaptic inputs and coherence between the sender LFP and the sum population spiking connectivity in the receiver.

So therefore, coherence therefore depends on connectivity strength and oscillation power and does not need purely oscillatory coupling or spike phase locking in a receiver.

And then further complexities arise here.

I mentioned some examples from a neighboring field too, but I'll kind of skip over that.

It's an interesting topic.

But still, you know, successfully relating the two fundamental signals in the section, spike and LFP, can provide us with a comprehensive explanation regarding the neurobiological cognition.

And since many signals picked up by the LFP will also very likely be able to be found at the unit level, care must be taken to map out assembly level effects from single unit responses.

And that's kind of an empirical portion.

So this kind of section is very, I think it's very honest.

I think it's very upfront about the limitations and potential consequences.

countervailing trends in the literature that provide some obstacles to this grounding.

But nevertheless, I think even if these things are true, there's still the possibility for multiple types of face codes, multiple types of neural codes within the same space being involved in different types of cognitive operations or across different core knowledge systems.

And of course, given that we know that human syntax is in fact a potentially species-defining property, I think care must be taken to really separate it out from these constellation of domain general neural mechanisms, which might be relevant, but might also not be relevant.

Again, you have to kind of weigh up the likelihood of a domain general versus non-domain general process being relevant for your concerns.

And again, you don't know until you know, you don't know until you do the empirical research.

But I think, you know, the purpose of this subsection is to kind of open up the space of, you know, possible alternative mechanisms.

Okay, I'm going to skip over this section.

This just discusses gain field mechanisms.

I've written a few papers about this.

It's a very interesting topic, just kind of grounding the kind of lower level accounts of why, you know, what it is we know already about

what the computational properties of individual cells might in fact be.

And that might sound a bit abstract, but there's some really good work by W. Tecumseh Fitch and Randy Gallistol and a bunch of other people that I cite here, just exploring the potential computational kind of, you know, kind of classical Turing-level architecture, you know, Chomsky hierarchy kind of computational

you know, facility that these individual cells might have in terms of their operational power.

Okay.

And then this subsection here just introduces a kind of a more classical distinction between Sherringtonian and Hopfieldian views.

That's basically just a distinction that focuses on the transmission of signals by nodes in a point-backpoint architecture, viewing cognition as the result of patterns of node-to-node connections,

as opposed to viewing representational spaces

which computation is considered to be the transformation between spaces um so it's kind of a more different ways of viewing um populations and units the relationship between units and their population dynamics and i kind of speculate that's you know um evolutionarily older brain areas might be explained better via sharingtonian accounts whereas more recently evolved structures in him language right uh might not be so that's a potential avenue that could also direct

a way of exploring the kind of empirical space beyond this.

And then when it comes to the operation space, I've already mentioned much of this stuff.

I'm invoking high gamma activity.

I am talking about the different dimensions of gamma.

Gamma is not just a unified construct, either neurochemically or genetically.

In fact, it's a very complex structure indeed across species, across brain regions.

You get physiological gamma responses in subcortical structures.

You get kind of higher order cognitive responses in cortex.

Gamma itself is just literally a range.

um but it's a very a complex manifold and so i kind of this section kind of takes care to to break down what i mean by gamma um you know a lot of papers in the neurobiology of language just kind of invoke you know gamma oscillations um or particular arbitrarily cut off you know range uh band range this section tries to take a little bit of care to just unpack

the different types of gamma, different types of aspects of gamma in terms of their sensitivity to different aspects of representational complexity.

So lower versus high gamma, you know, broadband, narrowband gamma, etc.

If you're interested in that, this is a good section for you to consult.

Okay.

So turning to structure, this again kind of reviews some of the things I mentioned earlier to do with the fact that, you know, every linguistic structure is sensitive to some aspect of lexical and semantic information that cohere into some kind of phrasal or sentence structure.

There's a phrase by Arkham Rance, there's no escape from syntax.

That is absolutely true.

Everywhere you look in language, every kind of phenomenon, semantic, pragmatic, even phonological, there's often a very direct impact of syntactic information in terms of the coordination and the instructions and the influence it has on these other levels.

Syntactic structure really is the most kind of fundamental level of language that influences a bunch of different domains.

I've already introduced merge, so I'll just kind of skip over this.

This is just a kind of more set theoretic classical definition of what we mean by a feature combination of the kind involved in language and of the kind involved in the kind of work that I mentioned earlier, and of the kind involved in the hydrogency gamma and theta dynamics that I mentioned earlier too.

So merge is basically the most fundamental operation in language.

It's the basic property of language, as Chomsky calls it.

Okay.

Again, from a more philosophical perspective, I always found it interesting that even though this is a very rudimentary set formation operation, if you think about the cognitive consequences, as people like Peter Hagel have in a very nice paper, the computational system of language seems to be related to neural signals integrating perception and action.

and providing humans with novel modes of planning and interpretation whereby lexical units and unification processes like merge provide an imaginary space that transcends the influence of direct perception action cycles that's a really key point there because of course we know that um one of the um real benefits of language and potentially why it was evolutionarily selected for uh is because it allows us to plan interpret think

you know, consider personal responsibilities, you know, consider our own future, our past and so on, construct a notion of

know one's past and where you've been constructed narrative and so to speak right so we know that other other animals have memories and but maybe none of them have a past right maybe maybe only humans have a have a past it's a kind of a unique kind of concept and that involves some kind of a different epistemological transition and a linguistic transition too right so it's a fundamentally a linguistic notion uh same with the self right the concept of self

you know, who I am and who you are, I, you, and so on.

The existence of

the pronoun system in language, for example.

All of these things are extremely cognitively rich and semantically very important concepts.

They're not just there to facilitate communication.

They're not just there to facilitate information transmission.

They really are part of the mental architecture that we have as human beings.

So I think it's an important point that Hogarth makes there.

The fact that this really is the center of the kind of cognitive neurosciences.


SPEAKER_01:
Okay.


SPEAKER_02:
Oh, excuse me.

This section just introduces syntactic features, what we mean by a particular syntactic feature.

I mentioned lexical features quite a lot, but this section kind of just briefly touches on syntax specific features.

And it also reviews some recent work looking at the role of endogenous oscillations in neural computation.

I mentioned some of the stuff earlier to do with low-frequency sensitivity and entrainment to particular moments of syntactic structure building.

This section here just kind of unpacks some of that more exhaustively and gives some more recent references there.

So this section kind of just reviews that literature.

Skipping through this part here.

And again, just kind of to recapitulate what I mentioned earlier, the role of low frequencies in indexing supralexical structural inferences is very well empirically supported.

So the previous two pages that I just scrolled past, they really kind of proposed and summarized a lot of literature suggesting that lower frequencies indexing supralexical, so stuff above the individual word level, it really is pretty undeniable at this stage, I think.

Okay.

And then we turn to the more issues of timing.

So, you know, what is the precise timing of syntactic information processing?

This paragraph here just kind of breaks down some of the particular periods of sensitivity that have been already documented and how it is compatible and maps onto the predicted time course dictated by the higher frequency rhythms, right?

So it's no good me just saying, oh, delta is involved in syntax and, you know, gamma in, you know, semantics or whatever.

You really need to have a motivated

time course in real time, like a real time passing constraint on how these high frequency oscillations can embed themselves and be entrained and phase locked to these lower frequency rhythms.

So I kind of break down the time course of activity here.

And it's all kosher.

It's all empirically motivated.

It's all kind of aligned with what we know about both about delta and about the intermediary frequencies too.


SPEAKER_01:
Okay.


SPEAKER_02:
And then this section turns to, again, the mapping process that I mentioned.

the mapping from the O to S structure.

So this kind of invokes, again, a separable mechanism.

I'll just scroll up to the top here just to have a quick refresher.

So we had phase amplitude coupling, as I mentioned, right?

So this is spike phase coupling, mapping R to O. Mapping O to S is phase amplitude coupling.

So that's the kind of classical process that I mentioned earlier to do with theta gamma and delta gamma coupling.

because that gets you directly to the higher order structural inference.

This section is just a short review of the phase-amplitude coupling dynamics in this particular process, and it also talks a little bit about some other kind of extracranial recording measurements that have unveiled this in other kinds of domains.

This section here, dissociating structure from meaning, this is very much for the linguists.

This is kind of trying to figure out how we can carefully account for signals that are specific to the semantics of a phrase versus signals that are maybe not specific, but certainly sensitive to the syntax of a phrase.

That's a very tricky issue.

You have to kind of basically design an experiment or stimuli that is getting participants to pay attention to

structural inferences and syntactic information and syntactic acceptability and not pay too much attention to other kinds of features, but also the stimuli itself have to be carefully weighted across words that preferentially engage syntactic information over and above conceptual information or conceptual semantic information versus words that very much do trigger heavy semantic content.

So that involves a lot of

you know, careful kind of empirical consideration.

Our lab is currently carrying out, you know, some research into that space, as are some other labs too.

But it's basically an appeal for focusing more laser-like on these, you know, ways to dissociate syntactic from semantic processing using empirical, you know, novel kind of experimental paradigms.

Because a lot of the paradigms that are used at the moment have a big problem, which is they don't really carefully separate out moments of syntax from moments of semantics.

So it's very difficult to kind of, you know, realistically, you know, kind of confidently make it clear that a given brain signal or a given response is really being driven by either syntax or semantics.


SPEAKER_01:
Okay.


SPEAKER_02:
In terms of, skip over this.

No, I didn't.

Okay.

And then this outlines the mapping from the S to E levels.

And it talks a little bit more about some of Standerhane

and other people's work into vector codes for population dynamics to do with what level of representation we might find individual syntactic features at.

And again, I transition slowly here to talking about traveling waves, how they kind of

more kind of static code that i've presented so far in terms of just you know a delta a theta a gamma how that static code might be transfigured into something more you know cognitively and neurochemically plausible in terms of the what we know about the transformation of um information across white matter tracks and across and how different and portions of cortex cortical cortical cortical cortical information is kind of transferred in real time across different portions of the cortical mantle it's not just like stationary

Okay, so I guess what I'm kind of calling for here, and as I call for in the next section, is a more kind of spatiotemporally dynamic model or code for language, essentially.

Excuse me.

So all of this section essentially needs, it kind of calls upon the notion that you need to map on an important, you know, important findings from the psycholinguistic domain in terms of the timing of language, the kind of, you know, when we know that language is coming involved,

which periods of activity are going to be important and the actual neural architecture.

That's kind of a very tricky question, how you map on those.

So, for example, you know, if a psycholinguist says, OK, we know that when people using eye tracking or using scalp ARP responses, we know that the period of 300 to 400 milliseconds is when most people kind of detect semantic violation effects.

really then have to say okay if that's the case does that time period at the behavioral level map onto moments of neural activity also at that level or maybe potentially the neural response comes on a little bit earlier before it's detected the scalp level or maybe at the single unit level it's even earlier or vice versa or maybe there's a kind of top-down effect whereby some kind of

intermediary mesoscale neural signal can be detected first before the spiking information is.

There's different hypotheses that arise here in terms of the directionality and causal relation between the R to E levels.

In the ROSE paper, I make it very clear what my empirical predictions are, but other alternative ways to modify the ROSE model could be

in terms of the directionality from different levels, right?

So for different linguistic structures, it could be the case that some of the more mesoscale levels, in fact, influence the coordination of spiking rather than spiking information being read out at kind of high-order levels, which then facilitate the inferences being made.

The directionality is very clear in the paper, but it's very much open for empirical resolution, essentially.

Okay, so in this figure...

Figure two, figure one is the basic kind of, you know, components of the model.

Figure two is the basic mechanisms of the model.

So this just goes into some more technical details that I won't go into to do with the actual nature of spike-Galopy coupling, what the kind of mathematical foundations of it are, and how you can, you know, map that on to a kind of classical kind of low frequency response and how these different signals could be coordinated in real time.

Okay.

And then turning to the encoding section, as I mentioned earlier, we know that the human mind needs different workspaces for different aspects of cognition.

So the big question is, does language share a workspace with other kind of non-linguistic processes?

Is there a specialized, unique workspace for linguistic information?

It's possible.

I think it's likely, in fact, which is why I have it in my paper.

So, in fact, we know that there are some non-human primates that can execute very primitive combination processes, like potentially even more flashful composition, just combining two calls, you know, like primate vocal calls together into a basic primitive unit.

Can they then do it again and again and again?

Do they have the workspace that allows the facilitation of storing this one unit and then creating a new structure and then assigning that whole complex an identity that is independent from its discrete parts, which is what Merge is all about in language.

They don't seem to be able to do that.

So that tells you, I think quite reliably, that we do need some kind of notion of human specific syntactic workspace, which is what the whole encoding level is meant to be in ROSE.

Okay, so this section kind of just plays out a little toy example of how to derive a particular structure and what the whole process would look like going from R to O to E. So I think this is important for me to kind of outline since it's the part of the paper where I kind of really do give a concrete example.

So let's consider the sentence, old men walk slowly.

During the comprehension of the first two words, the delta-gamma combinatorial code coordinates the feature bundling of the atomic data structures hosted by olden men.

So olden men have various features.

At a minimum, this involves posterior superior temporal sulcus, low firmus activity coupled with neighboring posterior temporal cortex, but also cross-cortical sites responsible for the specific feature types in question.

So I've already mentioned places like ATL and IFG.

And then I also mentioned theta-gamma coupling.

This maintains in short-term memory.

I mentioned the lexical memory buffer before, right?

Theta-gamma coupling maintains in short-term memory the relevant units via HIFUX activity.

Obviously, in a linear sequence.

So this is worked by David Popel too.

The theta-gamma coupling is a linear feature clocking mechanism that tells you.

These are the set of features, features 1 to 5, 1 to 7.

And you're going to access them and trigger them and read them out in a given sequence.

So that's the kind of linear lexical memory stuff.

It doesn't give you the hierarchical syntactic relational component, but it's independent of order.

That comes later.

Okay.

At the transition between men and walk, so word two and word three, the superordinate delta theta code maintains the categorical identity of the object.

So, you know, old men is obviously men that are old, right?

As I mentioned, it's not an old quality that happens to be men.

So that's a noun phrase.

When you get to old men walk, you relabel the object and it's no longer a noun phrase, right?

You have to recategorize the syntax.

It's not a verb phrase.

It's old men doing something.

It's an event structure.

So things like event structures and quantification structures and all the rest of it, they involve very specific, unique semantics that are informed and provided by and configured by the syntactic category.

So that's why I mentioned that there's no escape from syntax.

The syntactic category feeds the semantic information.

OK, so when you get to the third-word walk, the superordinate delta theta code maintains the categorical identity of the object.

In this case, the negotiation

between a multi-unit noun phrase and a more complex verb phrase hosting old men.

So during the same period, the lexical memory code, again, if you remember, the lexical memory code is the theta gamma.

The lexical memory code increases its number of theta nested chunks due to the occurrence of walk, right?

So old men walk is, in terms of the theta gamma code, all the theta gamma code sees is another word.

When you get to walk, all the Theta Gamma Code sees is a third word.

That's it.

It just sees, okay, another word's coming, another word, another word.

And it just simply chunks those features together using the feature clocking mechanism that I mentioned.

But that's not what the Delta Complex sees, right?

That sees something else.

So the superordinate Delta Theta Code maintains the identity.

And in this case, the negotiation between a multi-unit non-phrase and a more complex phrase.

During the same period, the initial lexical memory code increases the number of peter-nested trunks due to thickness of walk.

The same transition occurs from walk and slowly, with the exception that while the lexical memory code still increases in complex strength,

and that's the measurement of pack relations between the future and gamma right high pack relations the hierarchical memory code would decrease closer to but not identical its pre-read baseline due to the um adjunction relation not demanding a revision of the hierarchical memory representation so what the hell does that mean um so when you get to old man walk you change the uh syntactic

um category it's a verb phrase so that that involves a activity at the delta theta level when you get old men walk slowly the word slowly does not change the syntax the words it's still a verb phrase so old men walk slowly is a verb phrase

Old men walk is also a verb phrase.

So slowly changes the lexical semantic information.

So you keep getting that theta gamma pack increase.

But you get either a steady state or a decrease.

Again, I predict the decrease of the delta theta pack relationship.

Because all you're doing is maintaining the same syntactic identity.

You don't need to change syntax.

Same syntax.

Again, I talked about syntax feeding semantics.

If you think about it for a second,

Old men walk slowly is still involves old men.

It could be old men walk quickly, old men walk, you know, lethargically, whatever.

It's still the same thing.

So the fact that old men walk slowly doesn't change the fact that it's still old men.

Whereas with old men walk, that does change the fact about old men.

So when we get to old men, we have certain facts we know about old men.

When we get to all men walk, that changes things.

We know that it's all men walking.

When we get to all men walk slowly, that's a modificational structure.

It's called an adjunct.

The adjunct structure or adverb or prepositional phrase, anything like that, simply adjoins.

So that only involves basic set formation.

It doesn't involve a relabeling of the category itself because the category is still the same.

So this gets into some theoretical syntax ideas that are not important here.

But the basic idea is that you just get an enhancement of lexical semantic information, but the syntax is identical.

So that's why you get this kind of wave of old men walk and then slowly.

And this is, you know, I mentioned some of the delta peaks before.

This kind of stimuli, old men walk slowly, is what has allowed researchers to figure out that you get those low frequency peaks at the third word, old men walk slowly, because that's the period where you really change the syntax and it turns it into, you know, a kind of sentence, I guess, a kind of full sentential structure.

with a propositional identity, you know, truth evaluability.

It can be true, it can be false.

You can afford all sorts of different epistemic judgments to it.

That's the whole kind of, you know, you kind of get a whole complex of instructions being sent to conceptual systems at that stage.

Okay, so the transfer of relevant lexical information from categorization or labeling that I mentioned would take place via interactions between these two neural codes, right?

So separable codes.

I speculate here potentially via theta-theta phase-phase coupling or phase locking of the theta-gamma workspace and the delta-theta workspace.

Theta-driven dynamics effectively constitute the handoff of information after lexicality has been established by lower-level R and O processes, transitioning from encoding lexical memory to multi-object memory.

Direct testing of these dynamics specifically with respect to syntactic workspace construction has currently not been undertaken, although much work has been carried out demonstrating increased theta-gamma coupling in human hippocampus

during main formation, as well as enhanced frontal theta to posterior gamma coupling, alongside the recent discovery of a rapid neocortical theta network mechanism for flexible information encoding.

And familiar operations from left corner minimalist grammars can be appropriately parcelated onto these R, O, and S levels.

That's a bunch of jargon, but the linguists will appreciate what I'm talking about there, I hope.

But future work should still, you know, treat this issue of precise parsing models with more care and tact than I have demonstrated here in this initial architectural proposal.

So what I'm trying to say there is that, you know, I mentioned Mars computational, implementational, and

algorithmic level in the middle.

I've talked a lot about computation and implementation.

I've only spoken a little about the algorithm, the thing connecting the wetware, the kind of biological stuff you can touch and see with the kind of more abstract algorithm in between the computation.

And that's the domain of what's called psycholinguistics.

So the field of psycholinguistics has a bunch of different parsing architectures for how humans parse sentences.

There's rival models.

Lots of them have different predictions for how human beings parse different types of weird sentences in real time.

I have my own favorite candidate.

I've mentioned left corner minimalist parsers.

That's kind of my own.

you know, favourite account for all sorts of empirical and theoretical reasons.

But it's an open question.

And in fact, later, I kind of discuss the possibility that different parts of the brain might implement slightly different parsing models.

So, you know, the posterior temporal cortex might be a good place to find the minimalist grammar.

But the inferior parietal cortex or the inferior frontal gyrus might be an excellent place to find a different kind of passing model.

And I cite a bunch of examples in the paper somewhere in here, I think.

But that's, again, a bit of a question.

But it's also kind of a more I think it's a more pluralistic way of kind of viewing language in the brain.

You know, it's not just like, you know, minimalist passing grammars explain everything and the entire brain.

is you know uh in in concordance with the predictions of minimalist grammars it might be a kind of more uh a different kind of constellation of of passes that we need um but still nevertheless with the basic primitive uh you know computational architecture being uh inflected by moments of merge and labeling and kind of more traditional basic assumptions from general grammar okay

This period just turns a little bit more towards travelling waves and a little bit more to do with fMRI research, kind of building the paper more to what we already know from the extracranial world.

This section talks about syntactic memory as being phase synchronization over successive cycles.

So this invokes a bunch of interesting work and consults some models from the world of the neurobiology of oscillations more generally in terms of phase dynamics and cross-referencing coupling and all the rest of it, just to kind of situate what the model might look like in a more kind of mathematically rigorous feature space.

The section on symbolic computation turns a little bit more to what we already know about... In fact, I think I should review this very quickly because this again comes back to the issue of domain in general.

So, vision directions and the neurobiology of navigation and memory are relevant to the conception of E.

So hippocampal cortical sequence replay and encoding is not constrained to simply repeat past experience.

Rather, this process is informed by an internal model of the world, generating representations of inferred entities, not necessarily encountered physically.

Again, this is one of the reasons why I work with people like Carl Friston, because I think this whole notion of building a gendered model of one's environment, the interplay of action and perception cycles and so on, that's really a fundamental negotiating

a building block of what language gives us, of what the role of language really is in the human mind.

It's really about interplaying these two different divides of the human brain.

So this active generative capacity motivates the authors to propose that replay in the brain instantiates a form of compositional computation.

So a given replay sequence constitutes a set of entities strung together into a compound, whereby each entity is bound to a representation of its compound role, determining its function as part of a whole.

This establishes a clear separation with respect to composability between entity and role, or what's called syntax and semantics, right?

The entity and the role.

So while roles encoded by hippocampal-cortical interactions can certainly be spatial, they can also be non-spatial, and even non-spatial and non-Euclidean, potentially involving arbitrary roles such as a verb, or what we would call a verb, some kind of event anchor.

So the entity role bindings currently explored empirically in humans are limited to things like which position and which sequence.

But if other roles like if, then, else can be encoded in a similar way, then replay may form a viable candidate for a neurophysiological mechanism implementing symbolic computation.

So this compositional nature of replay is implemented via our friend data-gamma coupling, mirroring closely the present assumptions of lexical feature sequencing and basic semantic compositionality being implemented via the same dynamics and high gamma activity.

And there's a nice paper recently by Nina Kazanina and David Popel that explore very similar notions.

They kind of argue that a lot of the neural mechanisms already known to be found in hippocampus could be used

for symbolic language of thought in language.

Language of thought meaning the kind of classical Fedorian notion.

So a lot of what's already known outside of language about the neurobiological infrastructure, things like navigation, could be utilized and exploited by the language system in order to achieve its goals of composable functions, compositionality, and basic structure building.

So that's, again, once again, it's an empirical question.

These are all empirical questions.

But I think the purpose of this paper, which is a purely theoretical paper, is to kind of negotiate through this terrain and see which candidates are more feasible, more likely, etc.

And I think this candidate in particular, I'm not the only one who thinks this, is a very viable candidate for grounding some of the initial

Again, the stress is, for me, it's initial.

It's just that initial phase of the early phase of merge, the basic set formation process, the theta gamma stage.

Potentially not the human-specific structure building stuff that I mentioned earlier to do with the structural inferences that involve other mechanisms that I mentioned.

But at least in some aspects of language, lexical search, lexical access, morphologically complex word binding and so on, it's likely that these things are potentially used as well.

Okay.

And then in terms of... So memory transfer, this is an important concept that's been raised recently by Brinkert et al.

and L. Miller.

This section just very briefly reviews how we might transfer memories across the cortex, which kind of might look a little bit like transferring syntactic structures into a workspace or transferring stored lexical items into a separate kind of consolidation or monitoring workspace.

um there's some kind of potential for for research in this domain and then turning to alpha beta dynamics this is an interesting topic and that i mentioned briefly earlier to do with how the you know the the the role of these oscillations that are outside the main code space that i've already mentioned right i've already mentioned i've delimited my code space what about the uh the world of alpha and beta i've mentioned it briefly but it's a it's kind of a

by an important role, given that they are quite dominant.

They're very prominently found in intracranial and extracranial research into language.

So just focusing on syntactic memory, there's a recent paper, Geric et al., which supports a role for beta in syntactic identity.

just being sensitive to the identity of a sentence, the type of category that you afford it.

These authors investigated speech memory representations using intracranial recordings in the left parasympathetic cortex during delayed sentence reproduction in patients undergoing awake tumor surgery.

Based on the memory performance of patients,

they found that the phase of frontotemporal beta represents sentence identity in working memory.

The notion of sentential identity presupposes a labeled or categorized structure, like a verb phrase, seemingly represented partially by frontotemporal beta.

And converging with other literature, beta may represent aspects of the global cognitive set going beyond syntax specific information to include conceptual and statistical information.

Again, I mentioned the role of beta in syntactic anticipation, syntactic prediction, the kind of statistics of language.

And I think that has an important role to play here again.

Intercranial recordings by yours truly of auditory language comprehension also implicate frontal alpha and beta power in phrase prediction and anticipation.

They kind of mark the moment of potential anticipation of a licensed phrase.

Another paper found that both theta and gamma are sensitive to syllable rate, but only beta power is modulated by comprehension rates.

So comprehension obviously implies semantics and syntax rather than just syllables.

And over the past decade, in fact, a little bit longer than that, over the past 15 years or so, there's been a debate about whether beta power effects during sentence comprehension reflect syntactic computations or instead reflect maintenance or set updating.

And to briefly summarize the most current results, it appears that the latter maintenance hypothesis is most well supported.

So I'm therefore going to put further discussion of this to one side.

But the basic role of it in maintenance in terms of just continuing, aiding with the assisting in some way of maintaining the set of generated and sustained cognitive representations, not necessarily being sensitive to the properties

of this set not being sensitive to the particular syntactic identity or the category of the syntax or even the category of the semantics or the features of the semantics but at least being involved in coordinating neural resources available neural resources to help with the sustaining of that memory in work in in a current space again this is very domain general not specific to language which is why i think it's you know you don't you find fewer of the um

language specific syntax semantics sensitivities and even though it's involving comprehension as i mentioned comprehension means everything it's just like you know processing a sentence means loads of things it doesn't mean specifically syntax specifically language specific stuff it just means you know when you when you when you process a sentence you do all sorts of things and so i think it's most likely that it's involved in these kind of more domain general maintenance processes

And then in parietal cortex, alpha enhancement seems to index syntactic working memory demands.

Much like the regulation of gamma by alpha in control and attention mechanisms that I mentioned earlier, parietal cortex may play an important role in memory and complexity.

such that low frequency rhythms originating in lateral or medial parietal cortex regulate the activity of gamma-encoded operations in lateral temporal cortex, and single-unit or assembly-encoded are in medial temporal or inferior temporal cortex.

That's a more kind of specific proposal.

And another likely site of syntactic working memory is in Perifontosulcus, for reasons cited in this paragraph.

It's very heavily involved in all sorts of interesting semantic integration processes.

And then, yes, as I mentioned, the kind of bringing this model to a kind of conclusion.

These assumptions take place all within the context of the of the traveling wave framework.

So lower frequencies migrating across the cortex, particularly being relevant to cases of maintenance and storage.

So traditional standing waves lead to periods when all neurons in the network are turned off.

Whereas traveling waves can ensure that sub-portions of a network remain consistently active, directly compatible with the mosaic-like architecture of posterior temporal cortex in semantic integration.

I won't explain that further, but it's basically the idea that within these small regions of cortex, like posterior temporal cortex, there's a very interesting patchwork of activity of little satellites of sensitivity to different aspects of language.

Even though they're all generally sensitive to language,

And within them, there's kind of more specialised regions, which is not unlike what you find in ventral temporal cortex for things like face and place and other kinds of shape perception, kind of ventral visual occipital cortex being having a kind of tapestry of, you know, cellular, cell specific sensitivity to different representational features.

So where exactly the waves travel to during phrase composition is, again, my favorite phrase.

This is purely an empirical question, but some candidate regions have been suggested above, right?

That's kind of where we stand at the moment.

That's where the field is at the very moment.

So figure three here is the basic processes of the ROSE model.

This kind of just gives a general outline of how ROSE is hypothesized to be implemented across the frontotemporal language network.

And at the bottom, there's a representation of the various frequency interactions proposed to implement phrase flexibility.

So you have representations, category-specific information being encoded in spikes,

with operations, you have broadband gamma activity.

Initially, I hypothesized in PSTS based on the intracranial work that we've done, but it could be anywhere.

It could be in PM2G, but also be in portions of IFG, depending on the category potentially of the phrase, the type of structure you're generating.

That's an open question, very much open.

In terms of structure, we have the low frequency neural program driven by a temporal cortex.

And then for encoding, we have these traveling waves that spatially migrate structure.

So I also think that given what we know about the speed of travelling waves, I think it's possible that a lot of this stuff transfers up either via dorsal or ventral pathways across posterior to anterior temporal cortex and from posterior to anterior frontal cortex, given the timescales of sensitivity that we know exist there.

Okay, this final section of the paper talks about causal evidence, which is increasingly becoming very important these days in neuroscience as we've exhaustively mapped correlational activity of loads of different linguistic processes, but we haven't really got a good sense of which parts of the brain are actually essential for language.

So when you get regions X, Y, and Z being activated, region X might be causally involved, whereas regions Y and Z might just be activated or recruited or implicated, but not actually essential for the function.

So this section just kind of charts out some of the ways of exploring that empirically through cortical stimulation mapping in the OR or in the AMU using TMS, for example, or joint TMS MEG studies.

That's a possibility.

There's all sorts of ways you can think of kind of providing different portions of causal evidence for these models.

And these paragraphs here just kind of just review mostly non linguistic, but also some language research into how people have used these methods to disrupt language in particular parts of the brain.

some more interesting research disrupting particular phase codes and trying to particularly trying to modulate either a hypo or hyper excite and particular osteo dynamics or pack dynamics in different parts of the brain now that's a really good way to directly test the causal involvement of a phase code more generally because if you can hyper excite a theta rhythm or hyper excited gamma rhythm then that can really screw up your potential your facilitation of

the coordination of these electrical features and your attention and working memory and all the rest of it so that's a great way to kind of causally test um not just a brain region's involvement but also a particular brick like how a brain region is involved okay

This section is much more philosophical.

If you guys play Elden Ring, you might get the reference in this section.

But the point of this section is to kind of evaluate the ROSE model, to kind of situate it in a large context to relate it to existing models in the intercranial space and extracranial space too.

So some of the important periods in this paper.

So I've already mentioned this paragraph, I think, to do with how different parts of the brain might be sensitive to different parses.

So I'll skip that bit.

This paragraph here just outlines how the very basic philosophy in Rose is that mesoscale neural organization can be useful for brain function and that high frequency gamma oscillations report mechanisms and underlying communication channels of neural computation.

So neural oscillations organize cortical activity to produce computation, and ROSE builds an architecture for syntax that presupposes this.

Some exciting prospects for testing ROSE come in the form of multi-channel recordings with broad cortical access using binary microelectrode arrays, et cetera, et cetera.

And I just cite some more technical examples there for people in the field.

And then let's see, what else do we have here?

Yeah, I've already mentioned the inside out outside it.

Well, actually, I haven't mentioned that.

I've mentioned Mars framework.

But just very briefly, I think my model is incompatible with a bunch of recent ideas to do with encouraging people to explore the brain from inside out.

So I'll just read this paragraph and then maybe try and digest it a little bit because it's probably the most important philosophical kind of grounding here.

So brain models informed by computational concerns will continue to be needed in neurosciences.

Yes, that is true.

In particular, as we approach the advent of widespread availability of single unit recordings.

And again, one of the anxieties I have with this paper is that as linguists get their hands on sophisticated neural recording measures, we really need to be careful and quite conservative in knowing what we can say with each of these measures.

So if you give every linguist on the planet access to single unit recordings in the human brain,

You're going to get a lot of papers making claims about, as I satirize here, Neo-Davidsonian existential closure neurons or specific, very niche kind of cellular types responsible for all sorts of baroque and complicated and not very cognitively plausible linguistic constructs.

so i really wanted to in this paper boil it down you know decompose language into its primitive components decompose at the computational level and decompose at the representational level that kind of goes back to the 1980s kind of classical framework of the computational representation of theory of mind where you have a series of fixed representations units and fixed computations or things that you can do with those units this kind of actions and objects right that's a basic metaphysical human distinction between things and processes actions

objects and all the rest of it, nouns and verbs, computations, representations.

So I really wanted to make it clear that next time you find yourself with access to a single unit dataset, you need to have sufficiently decomposed these levels to make it neurochemically and neurobiologically plausible, rather than talking about the more highfalutin stuff that I mentioned earlier.

so it's kind of more it's more of a cautionary tale i guess um because i you know other fields have gone through this before right um so for example um you know grid cells right it's it's over the last decade grid cells in um internal and paracamber cortex were initially implicated in um spatial navigation then it turns out they're involved in auditory navigation

and then they're involved in conceptual navigation it so in other words they end up just being involved in navigation full stop just all types of navigation and and i think a similar kind of transition might happen with when we get um an increasing number of publications about language in single units you'll get a lot of people initially saying oh well you know these these units these specific cell types are involved in x then it turns out they're also involved in y and z and will ultimately end up being

forced to propose a more generic account for these things, which is why in my paper I try and focus it, I try and appeal to as much generosity as I can.

There's always going to be some domain specificity because obviously language has domain specific representations and apparently domain specific computations.

So we will need some notion of domain specificity on some level.

But the only question is like, where are you going to find that?

Okay, I'm rambling a little bit, so I'll just, I'll continue.

So this outside-in perspective has been critiqued recently.

Djordje Brzezaki advocates for an insider perspective on building neural models of cognition and considers the classical Maurean framework a purely outside-in perspective.

Yet Maure himself stressed that the three levels should be investigated in parallel, not necessarily prioritizing any given level.

So this is a balancing act though, right?

Too much outside in, and you're going to get these Neo-Davidsonian neurons.

Too much inside out, and you're going to be told that linguistics departments need to shut down, right?

So regardless of your own philosophical bent, I think single unit and other types of intercranial recordings are plainly the most direct and reliable means to further test and refine rows.

In particular, given recent independent assessments and critiques of bold activity.

But the basic message here is that a purely insider perspective is when people will say, well,

what's called data-driven neuroscience, where you say, well, let's just do a bunch of sophisticated statistical analyses on our single unit data and see what kind of emerges.

Let's just hope that language will just arise somehow without any ideological presuppositions or any kind of biases.

First of all, it's impossible, I think, for any human being to do that, even if they tell themselves they're doing it.

You're always going to be delivering some biases.

You're always going to be bringing some philosophy of science or philosophy of mind to the table, whether or not you like it or not.

On the other hand, you have the completely outside-in perspective.

which I'm also very much against.

So the outside in perspective would say, let's, you know, everything that linguists say is Bible.

All these theories of double rates movement and pipe piping and all the rest of it, they are literally what the brain cares about.

And we have to find that in the brain.

We have to keep our fixed theories of higher order cognition and just look at the brain and expect the brain to care about everything we care about.

We expect the brain to be sensitive to all the stuff that we as linguists care about.

And the problem with science is that, you know, there's always a negotiation going on.

There's always a kind of deal that needs to be made.

So, you know, one of our former presidents talked about the art of the deal.

In this case, the art of the deal in this case is figuring out how much linguistics you need to hold on to and stay true to and how much you need to let go off.

and how much you need to say, the brain doesn't care about this.

It only cares about this, but not that.

So my own feeling is that the brain, excuse me,

brain does genuinely care about some aspects of language domain specific or otherwise namely the representational feature types and also this business of marriage and labeling that i mentioned the basic computations i think the brain does genuinely care about that it probably doesn't care about all the other stuff that linguists talk about and but it can be characterized at some kind of more epiphenomenal level i guess as some kind of emerging phenomenon we can talk about the philosophical details you know in another kind of

frameworks, if you like.

But that's the kind of, I think, way to go about it.

I think it's much too harsh and arrogant to just completely abandon all of the insights of contemporary linguistics just because we have sophisticated neuroscience.

I think holding on to some insights from linguistics will be essential to direct and test and refine our data-driven science too.

So I think what Brzezaki talks about in his book is, I think, misleading and kind of a, you know, unhelpful division.

I think it's not, I don't like to talk about inside out or outside in.

I think it's kind of just, you have a balancing act between both, right?

You have to just negotiate when it's appropriate to use computational levels and when it's appropriate to use implementational level theories.

Okay.

So that takes care of that.

I'll skip over this bit here.

Yes, so this gets into what I mentioned.

I mentioned this maybe a little bit briefly, but it's very common to object to these kinds of proposals of the kind I mentioned in Rose as a kind of crude form of direct mapping, right, or isomorphism.

And I'm sure some people listening to me right now might think, well, you're kind of a hypocrite because you're doing exactly what you're critiquing Bersaki for doing, right?

I'm complaining that Bersaki is just, no, sorry, I'm complaining that these outside-in people are just imposing their linguistic theory on the brain.

And I'm saying you shouldn't do that.

At the same time, though, the whole point of the ROSE model is to directly map on what different types of linguistic structure

onto the brain so a complete nihilist or you know might say well the brain really doesn't care about any of these structures the brain really doesn't care about um atomic features lexical items feature bundles phrases uh you know sets of sets of sets and all the rest of it now that might indeed be true that's totally possible um i think it's unlikely given that the um you know um

the specific responses that we've seen at different levels of linguistic complexity, the kind of dissociable scales of sensitivity you can see at different neural complexity scales that I've mentioned here.

But so, you know, the ROSE model is essentially invoking a direct mapping in some way, a kind of one-to-one correspondence between a scale of linguistic complexity X and a neural mechanism Y.

And you kind of, you know, you follow that all the way up until you get to the ultimate, the highest level of nesting that you get with the delta theta complex all the way down to the spike at the peak coupling that I mentioned earlier, right?

Okay, so some people might say that's a problem.

Rose continues this crude tradition by establishing an even more explicit isomorphism between system-level complexity and syntactic complexity.

So, for example, cells and syntactic features versus global coherence and phrase structure on the other hand.

Alternative accounts that build syntactic operations into neural systems of diverging levels of structural complexity have not been forthcoming, by the way.

So if they do both come, then we can test them as well.

But we'd also naturally be required to establish any kind of adversarial collaborations and experimental testing of the models.

So through this, it would be possible to pick different candidate neural mechanisms for syntax against one another and compare effect sizes across studies to see which mechanism, either from ROSE or some other model, best explains real-time direct vertical effects of language processing.

So that's a nice way to directly test the ROSE model.

But I also want to emphasize here that

As far as I know, the ROSE model is the only non-neurobiological model of syntax that takes seriously this whole notion of the fact that the brain genuinely exhibits distinct levels of neural complexity organization and scales of complexity, and also takes seriously the fact that not all neural recording measures just measure the same thing.

single units up to UTARs, up to SEGs, up to ECOG, up to MEG, up to Scalp EEG.

These are very different beasts and they record very different things.

It's easy and convenient and maybe comforting just to say, well, the kind of signal that we see in Scalp ERP responses and Delta waves are probably just some kind of summated phase reset of whatever's going on at the spike level.

On some level of description, that's true.

However, it doesn't help with building a model of syntax in the brain.

That's just kind of dismissal.

So my appeal here is to kind of ground it more concretely in different levels of analysis.

And so far, you know, all the other models of syntax in the brain, as I've said, are localizationist as opposed to kind of algorithmic.

That's why the subtitle of this paper is a neurocomputational architecture, because it's a neurocomputational architecture.

And it is very much an architecture.

There's within the Rose architecture, you can imagine three or four or five very different specific theories.

or models from the architecture.

So, you know, it's very much an architecture.

And within this paper, I propose my own specific theories at each level.

But the scale of the architecture scaling from the spike of peak coupling up to pack up to traveling waves, that's the architecture.

But so within the architecture, you could maybe reframe some of these things as long as the mechanistic relations between R to O to S to E are maintained.

That's the kind of general framework here.

And then going back to metaphysics for a moment, Rose is also sympathetic to recent moves in philosophy of biology to view a range of biological constructs as processes rather than objects.

Rose is built entirely from neurocomputational mechanisms that are already known to subserve clusters of generic perceptual and cognitive operations, right?

I'm not invoking any new neural mechanisms.

I'm not proposing...

a new kind of microtubule quantum computational mechanism to explain all this.

I'm being very, very conservative.

I'm just using literally stuff that's been published in the most high impact, serious scientific journals, which has been afforded textbook level treatments at every level.

So all of these mechanisms have been given serious textbook treatment.

The only difference here is that I'm migrating some of these concerns over to the linguistic domain with various different motivations across the levels.

Okay.

And then turning slightly further.

So, yeah, another nice point here made in the Nature Neuropaper by Nina Kazanina and Alessandro Tibano.

Current theories that model hierarchical structure building via low-frequency dynamics correlate some neural measure with attributes of a hierarchical syntactic structure and thus concern the outcome of syntactic structure construction.

So this is a critique of those MEG and EEG studies that show this low-frequency delta entrainment or peak response at moments of syntax.

It's a very legitimate critique.

But by initiating certain syntactic structure building at the cellular R level and accounting for how these output low frequency responses at the S and E levels, the ROSE architecture goes beyond these other accounts that are more closely tied to the output of structure building.

So, and again, even in the ROSE architecture, this is correct that these low frequency delta peaks are indeed the output of the structure building.

I'm not denying that.

I'm not saying that the input

They're not some kind of intermediate response.

Kazanina and Tavana are completely correct.

They do reflect some kind of readout, the final stage of the syntactic structure building process.

Some kind of wrap-up or even monitoring stage.

For me, it's part of the structure to encoding phase.

But at least the early stages of syntax at the R and O levels are definitely at this kind of spike level and high gamma activity level.


SPEAKER_01:
Okay.


SPEAKER_02:
um yes and then again open questions remain about how to ground this in more neurochemically explicit models i haven't talked much about neurochemistry uh some of my earlier papers in 2015 2016 tried to take a neurochemical framework much more seriously and i have more of a like mathematically rigorous framework into uh negotiating particular low frequency dynamics and but in this paper i'm just kind of leaning that to one side for now because the paper is already

know way too long and i'm just focusing on the kind of architectural components that's something to be focused on at a later stage okay um yes yes yes okay so turning to the conclusion um

I've mentioned a few times that the ROSE architecture really does come in response to the absence of phase coding models in language.

There's been a lot of phase coding responses or phase-weighted entrainment and low frequency responses empirically documented.

They've been empirically documented, but in terms of the actual theoretical summary, that's kind of been lacking.

So that's my main motivation.

And many researchers who have kept the frameworks emerging purely from extranial MEG-derived event-related fields or scalp EEG-related potentials or fMRI responses, etc.

These guys have begun to lose hook.

they've begun to lose hope in the prospect of finding syntax in the brain.

So, for example, Lena Pilkenen, who's probably the world's most respected neurobiologist of syntax, she's written that the neuroscience of language field has long assumed that our brains build syntactic structure during language processing.

Today, it's reasonable to question this assumption.

And she's totally right and motivated to say that because she's summarizing research from mostly extracranial stuff like MEG and fMRI.

And she's definitely right that if you just focus on that research, it seems pretty hopeless.

I think the lack of high spatial temporal resolution in these methods explains the absence.

That's kind of one of the implications of this paper.

And this paper is essentially indirectly critiquing that field.

So Pilkenant reasonably speculates that based on the evidence reviewed in her article, something like merge might not be an obvious neural operation, or alternatively, it might not be found in presently explored neural signals.

In fact, she has this nice distinction between syntax as knowledge and semantics as process, where in neural recordings across all methodologies,

Finding signatures of semantics is very easy because semantics is very energetically costly.

The number of words and possible concepts you can combine at any given moment is really kind of astonishing.

The number of different feature spaces you can talk about.

You can talk about green horses made from licorice that ride to Mars next Tuesday.

You can talk about all sorts of surreal stuff, right?

So semantics is very unpredictable.

It calls upon a wide range of cross-cortical representational features.

What about syntax?

Syntax is very different because syntax is just merge.

That's it.

You put two things together and then you label it.

So that's it.

There's no novelty there.

Every time you build a phrase, you do it in the same way.

You build a phrase architecture, you label it, you stack it into a workspace, you label that, you ship it off to an interpretation and procedure.

That's it.

That's all of syntax.

And you do it again and again and again.

You don't have a different merge process on Tuesday as you did on Friday.

But semantics is, like I said, there's a big difference here between the space of semantics and the space of syntax.

The space of syntax is just this merge, this one operation.

So it's no surprise that it's been very difficult for scientists to find neural signatures of syntax, syntax specific responses.

Because it's like, we do it so reflexively and so trivially, and we've been doing it since infancy, maybe even before infancy, depending on which language acquisition theorist you believe in.

Whereas semantics, we're constantly changing it.

It takes years and years and years for us to really build our full repertoire of developed semantics.

with syntax, once you've built a phrase, you've built a phrase.

That's it.

You've got all the syntax.

The only thing that you can go beyond that is performance aspects, like how complex a sentence you can construct.

Can you read Shakespeare?

Can you read Joyce?

Can you read David Foster Wallace?

That's a very different question.

But that's a question about performance.

It's a question about working memory, attention, all those sorts of things.

The actual basic component of syntax is just merge.

So I think Pilkenin's right that, you know, maybe she's not.

I don't think she's right that, you know, merge is knowledge and semantics is process.

I think they're both process and they're also both knowledge too.

But in terms of why it's been difficult for people to find a signal reliably, it's because that's all there is to syntax.

Whereas of semantics, you're going to get a very reliable, very serious neural energetic cost involved in processing sentences.

so and in fact in our lab in our intracranial recordings of our epilepsy patients all the response or the most reliable responses you see are clearly with uh you know meaningful sentences as opposed to um you know semantically impoverished sentences that have a good syntax because it's just difficult to really disentangle those things um but that leads to into some some other questions that i won't get into because that's that'll take us to a different topic um

But so I guess what I've tried to do with this paper is that with ROSE, I think I've at least provided some candidate neural signals and they do exist.

There's plenty of candidate neural signals, some very good ones, and you can be readily investigated in this manner.

So, yeah, I think that kind of settles that issue, at least for me.

So under ROSE, there's basically no sense to be made out of claims that syntax lies in one specific brain region.

I talked about PSTS before being the real site of the initial phase of merge, but in order to get to structural inferences and then transferring that to a workspace, that's a global phenomenon.

it involves most likely, you know, posterior temporal, inferior frontal coordination.

There's a nice paper in PNAS by Oscar Wolno, who's in our lab, a postdoc in our lab, who showed very convincingly that building a coherent sentence structure involves the coordination of overlapping, spatio-temporally overlapping, but distinct clusters in both posterior temporal and inferior frontal cortex.

So it's not just, you know, IFG versus PT.

It's kind of like, okay, they're both involved, but to what extent?

Some issues are a bit tricky to discuss because you have to kind of relate this to the lesion literature, which kind of more reliably implicates posterior temporal cortex strongly.

And for my money, if I were just to, you know, contradict myself just a little bit, I think the language region is essentially posterior superior temporal sulcus.

I think PSTS is basically the language region.

with other things coming online later due to either coupling or functional coupling and coordination with PSTS.

PSTS will always be involved.

It's always the hub.

And there's other regions like IFG and ATL are involved and called upon for performance reasons or reasons to do with downstage readout and interpretation.

But PSTS is the language region, for me at least.

um so that's kind of the seat of the rose localizationist approach at least but then the rest of the neural code spreads far beyond that as i mentioned um i only read one sentence of a paragraph before before waffling um but i think i pretty much already said that in many ways um okay and then yeah we can think about testability in all sorts of different ways too uh to do with separate out the category of the phrase versus the semantic of the phrase that's something to think about yeah and

Okay.

And then in terms of the general roles in shaping information processing, the currently identified frequency-specific mechanisms seem to align with an emerging consensus in the field that slow frequencies control input sampling, alpha and beta gate information flow, and high frequency activity is controlled by slower rhythms.

That's a very general kind of neurocomputational framework that's been explored in the current neurosciences.

And then moving beyond this, a newly emerging focus on brain criticality and how this concept might relate to brain rhythms provides exciting revenues for future work in theoretical neurolinguistics.

So that's a cool concept too.

And in fact, there's some really good work exploring how

In some of my earlier papers, I talked about what's called globularity, which is this concept of the fact that the human brain is uniquely almost spherical relative to other primates.

It's not completely spherical, obviously, but a lot of cognitive abnormalities come with a brain shape reorganization.

And the fact that the human brain is kind of

more globular than our ancestors has been used to motivate a lot by a lot of serious um anthropologists and also neurobiologists uh this idea that the human brain is kind of uniquely uh efficiently wired in some way to facilitate a cross cortical and also subcortical cortical um information integration and so i think there's something to be said there to do with um

know an evolutionary change in brain shape globular brain shape would have naturally facilitated different paths for these traveling waves to move across right and different patches of cortex can communicate with each other and in fact we know from work by people like liz falkey that the basic computational kind of uh contribution of language is to provide a kind of universal currency for different um conceptual domains to talk to each other

excuse me so language is basically giving you you know you have then your number sense your sense of morality your sense of intuitive physics intuitive geometry uh intuitive botany you know what is all sorts of different domains in the psychology literature concerning how human beings have intuitive modules for understanding reality and constructing a gender model of the world

language seems to uniquely allow us to conduct transactions between these domains.

Whereas in non-human primates, they may not be encapsulated exactly, but they are much more kind of atomized.

The modules, you know, non-human primates have most of these modules for sure, no doubt, maybe all of them, but they don't have a way of conducting transactions between them.

Whereas with humans, we can talk about, you know,

a large number of you know funny green colorful heavy balloons or whatever right we can combine different conceptual categories together and number sense morality you know um uh different geometrical constructions uh emotional constructions we can use language to access all of these different cognitive constructions now even more interestingly there's some you know

If you look at the kind of syntax of natural language, there's been some really interesting work by people at Queen Mary University and other universities showing that if you take the list of human semantic concepts and the list of human kind of, you know, electrical items and features,

a lot of overlap in terms of event structure and agent processes and features, but there's also some non-overlap.

So there's certain notions like worry or concern, for example, that are not morphologically marked in any language, whereas things like trustfulness or belief

and things like that are marked by words and language.

So there are certain conceptual features that are very readily found across the world's languages and have certain morphological inflections for or features for, but there are also certain epistemic notions or notions pertaining to knowledge and truth and so on.

that are not readily morphologically marked.

So that suggests that there are certain parts of the brain, certain cortical modules that the language system does connect with and does functionally speak to and interact with, but there are other cognitive or perceptual modules that it does not speak to, that it's more isolated from.

So that's kind of a potential way to kind of think about ways to relate the kind of neurobiological infrastructure with the kind of behavioral linguistic kind of phenomenon.

Okay.

So yeah, and then just to conclude, the more kind of philosophical foundations of ROSE are also in line with research documenting a rich array of innate capacities utilized during language acquisition in Neonics, which can distinguish distinct phonemes, detect word boundaries, layered words, and so on.

I'm kind of really invoking a very rich kind of array of innate capacities.

So currently, due to the lack of consensus regarding how to ground syntactic combinatorics and Excalc in the brain,

researchers enjoy many degrees of freedom when selecting from their preferred linguistic theory, processing theory, the algorithm, and your about-for framework, the implementational framework.

So I hope that rose basically allows us to, you know, fairly constrain the landscape to narrow the space of likely candidate neural mechanisms for syntactic structure building.

It's really all about figuring out what are the most likely mechanisms that we're going to find

um natural language syntax in the brain which parts of the brain are going to be sensitive to it and which really index that process on some level um so yeah i think i guess i should probably leave it there and conclude it but um yeah that's pretty much the whole model thank you for the massively informative overview and the research agenda do you want to stay


SPEAKER_00:
Happy to stay for a bit longer for sure.

Yeah, yeah.

I'm happy to chat.

All right.

Well, there's so many places to join.

For people who are watching live, maybe we'll have a few minutes to ask a question.

yeah if you have any questions or comments or thoughts I mean I I can I can I can sit here all day and just talk forever about this stuff I probably shouldn't all right I have a handful let's do some short answers and just some general because on the evidence and on the specific brain regions and specific time scales you've made that abundantly clear so I'm gonna ask some alternate questions so okay

how do we study these densely woven cognitive phenomena without how do we hold that tension without just resorting simply only always to holism or reductionism you kind of talked about that with the inside out and outside in views but how can we

approach a different cognitive system maybe not the human brain and language where even the relationships amongst the different phenomena might be themselves not known um okay i see you mean so if there's some kind of you mean a human specific cognitive process that is not language

Yeah.

If you want to pull out a different thing or study a different system, what kind of account are we looking for?

How are we going to know we're on the path that's not being just massively misleading and so on?


SPEAKER_02:
No, no, that's a good question.

So the ROSE architecture, I think, is partially applicable to lots of different cockpit domains.

I think for attention and working memory, face perception, olfaction, they all involve something at the R to O level.

The coordination of spike timing and the realization of some kind of sensory motor transformations at the broadband gamma level.

The thing where we take over

is really at the S-level, where you get the particular type of low-frequency coordination that seems to be specific for language, but language is not the only thing that recruits low-frequency oscillatory phase codes to conduct the orchestra of representations that need to be externalized or interpreted.

So I think the

It's transferable in many ways.

The real testable

uh issue will be to do with whether or not these um you know whether or not I'm correct that these different levels of linguistic structure are found only at these levels or maybe they're found across all of them um so for example if effects of syntax specific responses can be found truly at the r the o the s and the e scales that I've mentioned that muddies the waters

because it means that the brain at every scale across all areas cares about the most niche specific issues in syntax.

We might have to turn ultimately to state space architectures, which I suspect will be used to supplement rows, at least at the SME levels, I think, in terms of

you'll have different abstract states being coordinated across network nodes, which is similar to what I mentioned with traveling waves, just a more kind of global interactional process, but for a very different mathematical grounding.

It'll have a very different mathematical grounding.

I'm not sure if I'm answering your question well, but I think in terms of transferring it to other domains that could be explored,

It's really about trying to find a prediction of like, I need to use a particular methodology.

I need to use a particular experimental paradigm to find effects that should only be found at this particular neural resolution.

And in this respect, it's no different from

the rest of the cognitive sciences.

Because in working memory and attention and navigation, that's exactly what these guys have done for decades.

Linguistics, as usual, is the last person to the party.

Linguists are usually the last field in cognitive neuroscience to pick up the developing tools in the field and apply them.

And that's fine.

There's nothing wrong with that.

I think the reason is because human language is the most complex

cognitive process that we have it requires a lot of the weeds to be sorted out before we have to figure out how working memory works our attention works our vision works before we can even worry about language and because all of them kind of presuppose and require that right at some level so I think it's natural for language to be saved until last right save the best to last for sure no doubt um but I think it's also it makes it um

It's also the case that neuro linguists have not always taken the lessons that previous fields have learned as seriously, and we often end up making the same mistakes.

So when we got scalp EEGs for the first time or fMRI, we made a lot of the same mistakes that the memory guys did or the navigation guys did.

And I think my own fear in writing this paper is that the reason why I wrote this is mainly because

I see it in the last few months, I see it in the last year or so, cognitive neuroscientists getting their hands on very advanced recording techniques, but not really knowing what to make of it.

Again, this very aggressive, but also very contemporary and very progressive kind of so-called progressive viewpoint of data-driven science, where you just kind of let the data speak for itself and allow the statistics of your, or do some kind of machine learning.

um on your kind of on your neural data and expect that to kind of feed into an explanation for language a lot of those techniques and also discourage this kind of theoretical reflection um and if you look at people like Carl Friston you know he spends most of his time he's the world's most influential neuroscientist and he spends most of his time just doing theoretical neuroscience because it's that's that that's really where we're at these days in all of these fields it's about consolidation

an interpretation a lot of the a lot of the um i mean i was surprised when i when i started writing this paper i was surprised that a lot of the clues and answers to this puzzle of how syntax is important in the brain are kind of already out there they just haven't been very you know uh the dots haven't been connected the uh the principled relations and logical relations between different findings and different um you know recording methodologies um and the kind of

more principled relations between them haven't been established yet or haven't been figured out.

And I think there's often a push and a desire to just do another experiment, get funding to do more and more experiments and collect more data and do more analysis.

That's obviously essential.

I do that as my day job.

But outside of my day job, I think it's also important to spend some time thinking about how to

thinking more carefully about this so that you don't need to do any more experiments for a while.

Or at least so you know that when you do your experiments, you know the landscape better and you don't end up making the same mistakes that other people do.

But really, it goes back to this issue of cost benefit.

How do you

How do you kind of weigh up the pros of collecting more data with the foreseeable benefits?

But the whole notion of foreseeable benefits, empirical foreseeable benefits, really presupposes and needs this framework of what the benefits are likely to be.

If the benefits are just, I'm going to just let the data speak for itself and

get a bunch of different you know machine learning um or llm architectures to kind of figure out the story for me you're probably going to be disappointed um whereas if you take a step back and think about a more principled architectural approach like think about the brain the way an engineer would and figure out like which level is going to be best served for a different a particular cognitive process that's that's the basic mindset behind behind this model

It's about figuring out which scale of neural operation is going to be most helpful for you.


SPEAKER_00:
Great.

Well, on that kind of research program scale, the Compass Rose, it's a great model.

I was curious about a few other settings for speech, like listening to a video or recording at a different speed than 1x, or watching subtitles, or just the idea of reading, and whether there was some kind of a...

clean handoff between one sentence is heard then one is read then one is heard is it beyond the sensory modality by what stage or what elements that are extra lexemic like prosody and timing and so on play into for example speech as opposed to writing where there's formatting and italics and so on


SPEAKER_02:
Yeah, no, that's a good question.

So there's an unanswered question at the moment, which is to do with the bottleneck of reading or speech processing.

Well, speech processing, not so much, but definitely for reading.

We don't really know that much about the bottleneck, like how fast you can read.

So the ROSE model talks about delta rhythms.

It talks about theta rhythms.

These are very slow waves.

If you ever tried speed reading, it becomes very clear straight away that human beings can read very quickly.

and so if you if you you know code in matlab or python some kind of rsvp style and you know a textbook where you can just like have one word flashing on the screen at a time and set it to different rates if you have it you know 50 500 milliseconds away no problem it turns out you can go way faster up to like you know 15 20 words a second and you can still get interpretability and

you'll be surprised if you try it, you'll be very surprised by how fast you can actually read.

So you might think that poses a problem for the ROSA architecture.

I don't think it does.

I can get to that after I've answered your question.

It doesn't pose a problem at all.

But so that's a good question.

So we know that there are certain parts of the brain that do in the ventral temporal cortex, which is the lower side of the other side of the temporal lobe, they are responsible for

transforming visual information into orthographic information.

In fact, the work of Oscar Wolno in my lab, he's done the most impressive work on this by far.

Very exquisite, like intercranial recordings of how human beings... In fact, his whole research is on reading.

So you should probably definitely read his work.

It's basically about how the human brain takes visual information

and then converts it into an orthographic space, a constrained orthographic space, a space in which your mind, I guess, will be sensitive to particular statistical configurations of shapes.

But you know this one's a letter, this one's not a letter.

So that takes place in the same part of the ventral temporal cortex.

maybe somewhere around there that will due to endogenous um you know processing and limitations probably not to do a vision probably due to you know some kind of uh next next stage and process to do of interpretability

And because as you know, we can read, we can see based on such the card rates and all the rest of the numbers, like objects and things we can see in a given second is very large indeed.

The human brain mostly filters out most of its sensory information every second, just to kind of focus on the stuff that's key to its general model, the stuff that's key to kind of, you know, minimizing the free energy of its general model and keeping it kind of sustained and not changing the status quo, et cetera.

But on the reading level, we know that at that stage, that's when you get what I call the sensory motor transformations in high-gamma activity.

So that's where you get that high-gamma activity being responsible and sensitive to the mapping from that visual information to the linguistic information.

Because often somehow you see words on a screen, you see squiggles and lines.

Somehow you've got to convert that into a more abstract space.

So that all takes place in the ventral temporal cortex.

And then you get some interesting connectivity and top down information from being sent from frontal cortex back to ventral temporal to constrain and kind of coordinate

the that bottleneck at least that's one of the main theories that exists and i'm not an expert on reading per se i'm not a vision scientist but you know as far as i understand that's the kind of um the general framework is that there is a part of the brain that's sensitive to statistics but also the symbology of orthographic information um and we know that

it takes place in very short timescales.

You asked a very interesting question about the kind of dual modality, like when you're watching a film, you're listening to it, and you're also reading subtitles at the same time.

That's a very...

tricky question because on the centromotor side, we know that you'll get oratory cortex involvement and orthographic kind of occipital temporal involvement engagement at the same time.

But somehow they both have to converge on a single interpretation, right?

Because we know that you can't

comprehending things in parallel.

You can't read two books at once, or at least most people can't.

So that means there's got to be a single kind of interpretation procedure and a single kind of semantic integration process that is being instructed in a form by different modalities

But when you think about it, that might sound difficult, but it's actually no different from when I use hand gestures right now.

So when I use hand gestures, I'm providing some kind of extra modal information to you.

It's obviously easier because this is more hand gestures are usually kind of pragmatically informative rather than explicit propositional, of course, unless you speak sign language.

But it's still just a case of different sensory motor transformations taking place.

So that would still be at the O level.

that would still be at the O-level of R. It's still high-gamut transformations that are then fed into a more uniform, discrete and abstract syntactic inference being made.

There's only one syntactic inference being made about the meaning of the sentence, but it's being informed by multiple sites.

Now, you do raise an interesting question because that might be the case.

It might be the case, therefore, that this low-frequency delta complex therefore has to entrain and functionally connect with multiple cortical sites.

it might have to speak to auditory cortex, but also ventral temporal cortex.

If you're gleaning distinct types of, if the lower level lexical representations you're getting are being sourced from auditory, that orthographic.

So for example, if I give you a sentence,

and I get you to hear the first word, and then read the second word, and then hear, read, hear, read, or listen, I should say.

Listen, read, listen, read.

If you go between listening and reading, but you're still integrating a single unified structure, we know you can do that.

Human beings can do that.

So that tells you straight away that

you know, it can be done.

But the question is how it's done.

Like I said, I suspect it's through this dual kind of trade-off between central motor instructions being sent from auditory cortex to the same unified low frequency program, and then the visual information too.

So that's actually a kind of a nice way to separate out the distinction between, you know, sensory motor stuff from the more abstract, unified kind of syntactic information that's being built.

And in fact, you've just given me a good idea for an experiment, so thank you.


SPEAKER_00:
Cool.

Yeah, because language is so linearized, it's like being pulled through a needle, and then in the CICADE, so-called speed reading,

But it's not simply moving the linearization faster.

Speed reading can leverage formatting and all these other strategies, like almost thinking through other minds.

ways of reading and also having an off-center effect in visual system and the way that that kind of convolves the orthography versus in listening you basically always want to be listening more clearly because the moment is the only time you're going to hear it

The active listening components, like you can't let it fly by, but reading, you can take saccade strategies and cognitive strategies that you just can't take.

And that's kind of this like all at once versus one at a time element.


SPEAKER_02:
There's actually, there's a framework in psych linguistics called the now or never bottleneck.

So the now or never bottleneck refers to what you're saying, right?

In reading, there's no now or never.

You can just take your time.

With listening, you're totally right.

It's now or never.

You have to pay attention now and integrate or your working memory is going to dissipate or your attention is going to

or your phonological loop is going to decay, and you're going to ask the person to repeat themselves again and again.

So it's a very different performance system.

And it's also very different from Braille, when you're using touch and you're reading Braille, when you're gesturing and using sign language.

Language, especially syntax, truly is amodal.

It's completely independent of any modality.

The neuroscience of speech is very different from the neuroscience of language.

neuroscience of gesture is very different from neuroscience.

Your language is a more kind of a modal abstract system that calls upon these very various sensory motor kind of processes, which is what most people

intuitively trying ground language in a sensory motor kind of context because that's natural of course you know that's how we access it that's how we trigger it well obviously you know most of most of our daily language use is probably just thinking to ourselves you know um using language maybe on subconsciously using the using the computational capacity that language affords us in non-linguistic ways meaning to construct hierarchical structures of thought and planning

and you know and all the rest of it in some kind of subconscious way rather than explicitly saying I am saying a sentence or even I am thinking a sentence in my head because even when we think a sentence in our head

that's still just an internalized form of externalization, right?

So when you think about in your head, that's still externalization.

It's just a different type of externalization.

You're not externalizing it with your mouth, but you're still putting it out into your own phonological loop.

But that's still externalization.

internalization is truly the subconscious process of generating the syntactic inference which is done rapidly reflexively uh you know intuitively often without much thought often meaning attention often without much attention being directed to it and any kind of

conceptual implications being made.

So most of language use is outside of the sensory motor space, I would argue.

Maybe I'm wrong about that, but this is a point that Noam Chomsky's made a lot, and I think he's right about this.

A lot of the daily use of the language infrastructure

um is like i said i mentioned the universal currency metaphor it's like it's across these different conceptual spaces um and it's it's it's potentially represented very redundantly across sparse neural codes as i mentioned in the rose architecture such that it survives brain damage it survives all sorts of different things and

But it's still crucial.

And you get a lot of people like Rosemary Barley and Federico, you know, showing work that, you know, damage to, well, you know, using imaging research showing that the language network is not activated during XYZ non-linguistic cognitive tasks or damage to the language network does not damage, you know, XYZ non-linguistic tasks.

And that's all fine.

But it's really, that's kind of a separate issue from whether or not the more abstract syntactic neural code,

that I've presented in the paper is recruited on some level by non-linguistic resources, which I suspect it is.

Again, another empirical question for you, right?

But I think that's a reasonable framework to kind of assume, at least.

Yeah.


SPEAKER_00:
Cool.

One, I guess, reflection on that, and then one last question.

You described language, human language, as being a kind of psychotechnology with supporting architecture and enculturation, all these different features that are totally relevant, influence it, that allows us to articulate the sense-making in action.

Like when we look at figure 4.3 in the Active Inference textbook, and we have

a Bayesian graph that factorizes out through the sparsity of just how things are related, like how the measurement of the thermometer is related to the temperature in the room, how the temperature in the room changes, fundamental concepts of place and time, where preferences fit into that, what this is about, the semantics of this variable, whether it's kind of like a central tendency or whether it's a variance estimator like confidence, which is something that you brought up as like a strong linguistic utility.

So it's kind of like,

it has an element of just being the geometry and topology of thought and thinking and generative modeling that has ultra mundane everyday uses because this generalized apparatus really does

have utility, pragmatic value in communication.

So it can be used for the most referential, just one bump above pointing at something and pointing at something and grunting and pointing at something and say, look, or look, it's like you can keep going.

But often what's only needed is the first little bit, especially given a context.

And yet there's this open-ended...

arbitrary state space navigation elements as well yet it's only i just i think it's incredible how it's laid out and how what you described with a heavy emphasis on the neurophysiological and the computational comes into play with active inference and the generalized state spaces and all these concepts so that's just super exciting i guess my sort of

closing or or we can keep talking or whatever but how do you see this relating to language learning for first language and for not first language learning yeah i i i totally agree with what you said about the active inference framework but i think i think um


SPEAKER_02:
You know, linguistics, semantics and well, you know, syntactic structures are a unique kind of inference that the human brain has to make.

I see a lot of the properties of lexical semantics in human language as being a unique type, a unique contributor to the active inference framework.

And I think it's a surprise to me that more linguists haven't turned to active inference to think about certain properties, because it seems so obvious.

It seems so compatible.

I've tried to do it in some work, and I think maybe some other people have too, potentially.

It's an obvious kind of, well, I mean, I cited the Peter Hagor paper.

He cites that work too a little bit.

But no, it's a very promising way to pursue it.

In terms of language learning and acquisition, that's a very, very good question.

So I mentioned at the beginning some of the low-frequency delta responses to syntax, all of that research is in adults.

Well, all of it was until some researchers at Cambridge UK a few years ago started to think, okay, well, maybe is this how children process language too?

Or maybe is this how, is this where they process language in the same regions?

It turns out

If you look at teenagers and children and you do the same kind of low scalp BG, or maybe even Meg research, I'm not sure.

I can't remember.

I cited it in my book.

The low frequency responses change over development in terms of which ranges are sensitive to the periods of syntax.

So it's not necessarily, you know, 1.3 Hertz precisely that entrains the syntax.

Of course not.

This gets back to the point I made earlier about speed reading, right?

It's all about the flexible coordination and packaging

of representations into a given complex.

Again, I mentioned the issue of trade-off between number of representations you can chunk and the fidelity of the representations.

You can have a few and well represented in Ghana or a large number and only a small number of spikes per representation.

That's a very general framework.

that's been supported in the attention literature too.

It's not just my proposal.

So that poses a question.

If children and teenagers show a different kind of low frequency phrase response, how do we deal with that?

I don't have a clear answer to that question.

I think the ROSE architecture can very readily be modified to that because it's not as if they will still have the same architecture.

they'll still have the same R to O to S to E. The particular frequency band of interest might shift over development.

And in fact, we know from working memory and attention that theta dynamics and alpha dynamics, and beta dynamics too, do change cross-cortically over development.

And these are behaviorally relevant to things like

development of working memory and attentional resources.

That's been very clearly studied and very well studied too.

So it's more like there's a more abstract code that simply shifts its spatial temporal profile as the brain gets older and then changes more when we get even older.

So I have a paper that's going to come out fairly soon, I hope, to do with aging.

What happens when our brains decay and we get old?

There's some nascent kind of research looking at the language processing, not at the behaviour level, but in terms of the neuro level, the neurobiology of language processing in older people.

and how it might also relate to you know breakdown of semantic information access and things like that and what are the potential correlates of that so you can imagine very clearly if you take the rose architecture which components will be disrupted or modified based on if it's a syntactic or a conceptual or a lexical kind of deficit but you know the the the isomorphism is as i said very crude so you can you can predict that very clearly and in fact there's some

research in that space that I think is concordant with this, which is what I discuss in this new paper.

Like I said, it's a relevant architecture.

It's a very difficult question about how the whole brain matures over time, because there's certain aspects of knowledge that may not be represented in ways that we currently know how to detect or record.

We have the four or five measures, main measures that I mentioned in the paper, they're what we have right now.

Maybe there's a particular form of neural encoding that just goes way beyond what we can physically provide in the year 2023.

I don't think there's any reason to assume that we've reached the limit of decodability of neural signals and their complexity.

Absolutely not.

So the learning process is a very mysterious one.

So yeah, I think it's also a really good way to falsify a lot of this stuff, by the way.

It's a really good way just to completely falsify and test a lot of these proposals to do with the architecture itself and then developing code over time as well.

So, yeah, it's a very relevant concern.

The best way to do it is to get into training recordings and children.

That's very difficult because in Austin, Texas, there's a lab there that has these recordings and uses access to children with epilepsy.

And they get these children to watch trailers of films and listen to stories and things like that.

And they do some interesting language research.

in these kids that's exceptionally rare data and very very very useful data for answering these questions i don't have access to that data and most of that research has not been forthcoming yet there's been a couple of intracranial papers in children and language but it's fairly sparse fairly low patient numbers and the types of analyses have been too kind of remote and departed from this framework to really test it but that would be the way to test it get you know

electrodes directly inside the brains of children and potentially even younger.

I think the youngest I've seen is about four.

But even by four years old, it's maybe too late to chart the actual full development of acquisition of language, because by that stage, they've already acquired syntax, right?

If you could get even I don't want to sound like a mad scientist, but if it's possible, ethical to one day get access to intracranial recordings and very young children, then it would be possible.

to chart this developmental process especially if the children are in the hospital for protracted periods and in fact get some kind of i mentioned bci implants if they have some kind of implanted device over time that would be the most direct way to test this stuff well not just test my theory but just you know just just completely chart out the full developmental profile and the evolution and the development of the neurophysiological basis of speech processing and language processing um but right now that's kind of as far as i understand that's kind of like

the current landscape of where the field is right now.


SPEAKER_00:
wow well um you can have the last word with any last thoughts but thank you for this amazing presentation and i hope that if people are interested that they can of course read the paper and learn and reach out and maybe we can have 64.2 when the time is right but anything you'd like to kind of leave it with yeah no thanks so much for having me on man um it's been very it's been great you've been great i've been enjoyed it's been enjoyable just kind of outlining my own


SPEAKER_02:
You know, you read your own paper over and over again and you realize all the things that you kind of misinterpreted or misread on your face, read through in terms of like phrasing and all the rest of it.

So it's always good to refresh these things.

But no, absolutely.

If people are interested in this stuff, I'm always open.

You know, you can contact me via email, on Twitter.

Very happy to talk more.

Absolutely.

So yeah, thank you for listening.

Thank you for having me.

Thank you.

Peace.

Peace, mate.
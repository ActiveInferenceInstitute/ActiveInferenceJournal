SPEAKER_01:
hello and welcome it's december 7th 2023 we're here in active inference guest stream 67.1 andres carada is here with us and we'll be presenting and discussing on ntqr logic for noisy ai algorithms complete postulates and logically consistent error correlations so thank you for joining andres and also jacob and looking forward to this discussion


SPEAKER_03:
Thank you, Daniel.

Thank you for introducing me.

Hello, Jacob and Daniel.

Jacob, would you like to talk about why you're here?


SPEAKER_02:
Yes, sure.

So, hi, I'm Jacob.

I am a researcher currently based in California.

i am interested in using physics-based principles to model intelligent and self-organizing systems and how different frameworks uh most specifically active inference can be applied to both topics in ai and systems engineering and broadly how we can use these formalisms to understand systems at a deeper um more

inclusive level with other disciplines as well.

And I'm very interested to hear your talk and your thoughts on these topics and engage in hopefully a productive discussion.

Looking forward to that, yes.


SPEAKER_03:
okay so i'm going to talk about the problem of self-regulation for any intelligent machine and it has been a long journey for me in dealing with this topic it goes back to a patent that i that that i took out in 2010

But recently I've come to understand it because I've been collaborating with a philosopher and an economist, the aspects of it.

And that's why I have this very abstract title for it called NTQR, because it refers to a situation where you have an ensemble of N experts to which you have given T tests.

And each test has Q questions and each question has R responses.

Okay.

So it's about evaluating noisy AI algorithms when you give them these types of testing protocols.

And superficially, at the beginning, we can take that testing protocol to be the surface application itself.

You are literally taking, let's say, a multiple choice exam.

But I'm going to eventually dislodge you from that to understand that this is a digitizing format for testing logical consistency of anything.

because you can digitize anything.

So even if it's continuous, you can create four response ranges and stuff like that.

And the big breakthrough for me has been the recognition that for a long time I talked about these things as universal thermometers.

And the universality meant that they could be used everywhere, just like thermometers can be used everywhere aside from

you melting them right or freezing them beyond their range and the thermometer also had the notion of being stupid no intelligence no theory about the world or the phenomena that it's measuring the temperature off and i've did that for a long time and i took out a patent because i thought that i had found a method to do these thermometers and and you can patent methods

But it turns out that I did not discover a method or have a patent.

I discovered logical postulates for evaluation.

And so what I'm going to talk about today is the logic of evaluating noisy functions, period, in general, universally.

And so I'm going to talk about them as postulates because they apply whenever you are doing these NTQR tests.

And so a major conceptual goal for me is to convince you of that, right?

That I have something that you could call postulates because especially in the machine learning world, people would say, this is crazy.

This cannot possibly be, right?

How could you have postulates for anything in the real world so general?

And you'll see that I do that by basically shedding all representation.

So I want to mention these collaborators.

One of them happens to be my cousin.

This is a conflict of interest disclosure.

He's a professor of philosophy at Virginia Westland University, and he has been the severe critic of things.

And I've done most of the work in terms of the mathematics and the machine learning, but he's the person that's responsible for actually introducing the concept of logically consistent

versus logically sound and the utility of of having any sort of logical right proof for for anything that you do and then the other collaborator with this paper is elia parker who works with the navy and has a master's in economics because you're going to see that this is a fundamental problem in economics that goes under the name of principal agent monitoring

So I'm going to talk about, you know, the two main goals that I have in the talk, and I'm going to introduce the main problem that we're going to be addressing, right?

This principal agent monitor problem.

And then I'm going to make some observations about you guys.

I find that the format of these live streams is incredibly interesting to me intellectually.

And I just want to, you know, sort of give an outsider viewpoint about what I'm seeing that you guys are doing that I find so laudable.

And as I was saying to Daniel, I have a little bit of an imposter syndrome because I feel like you guys made a mistake inviting a stranger like me so suddenly.

But I hope to repay that kindness by engaging in a good discussion with everybody.

And so then I'm going to introduce this idea of algebraic evaluation, talking about binary classifiers.

And I'm going to make the math be really simple, just linear algebra.

so that you can understand it, right?

I don't want to lose you because this eventually involves algebraic geometry and something called Grubner basis and all these complicated stuff.

But I don't want to use this math to hide from the basic ideas that are behind it.

And then I'm going to talk about what I said before, that you shouldn't think of the multiple choice exams as actually being the surface realization of how something is being evaluated.

you could you could have given people a philosophy exam and giving them written essays and then you have chat gpt grade the written essays and give a score right from you know zero to four right so then you're making an ntqr equals five test right and so i'm going to go over that

And then the main technical result of today, the main thing that is the breakthrough, is that there are complete postulates for pairs of correlated binary classifiers that allow you to separate correlation from individual performance in a way that actually allows you to then immediately trivially sort of compute the only logically consistent error correlation if you believe that they behaved in a particular way.

And I'll go through that caveat.

Then I'm going to show an exact solution for the tree of error independent classifiers, and then show some experiments and work through the math of doing it.

Any questions or people want to comment on anything?


SPEAKER_01:
Go a little further, and I believe we'll have a few pieces to add in.


SPEAKER_03:
OK.

The main goal that I have is that I want to describe to you what I did technically, which is try to figure out how I'm going to evaluate ensembles of noisy binary classifiers.

And so it has a particular way that I go about it, blah, blah, blah, blah.

And that's sort of like the surface realization.

But once you realize how I've done it and what I've been able to achieve, what I really want you to think about is this last question.

if it's not this thing that i'm going to talk about wow isn't it something like this because it seems like this is so easy to implement and it's so useful if it existed and so that's where i want to take you okay and so i want to have the discussion for that and you know we can go into the math and i can pull up the papers i can pull up github repos right and i can show you stuff if you want me to show it but i don't think it's going to help

the discussion.

And so I want to talk about, you know, sparsity, picking best variants and safety from internal mistakes.

That those are the topics I think Jacob wrote, you know, talked about some of them, right?

I think the November 14 talk that I saw with Alexander, he was harping very much on sparsity being a very biological thing.

And you're going to see how I use it, right?

Because I basically have a data streaming algorithm

okay so this is my outside review i mean this is a community of people that you guys have been working in so such hard problems and you're still working on such hard problems which i have to say you know and i'm being you know honest here you've made very little progress on right but it's because they're really hard fundamental problems but you guys don't give up

You guys are still having this live stream.

You're still inviting people.

You're still discussing it.

Right.

I love that.

Right.

Not giving up on particularly hard problems.

Right.

So I'm very attracted to that particular mind temperament.

and your community as i can observe of people who have come to neuroscience and biological things but from different places sometimes from physics sometimes from biology sometimes from mathematics and have arrived there but you're still bringing in people from those places right and there's a lot of active listening that i saw daniel writing stuff down right as people were speaking i mean incredible stuff that you rarely see so kudos to you guys

for setting up this sort of environment and, you know, inviting a stranger like me to talk to you.

It's very courageous intellectually and I applaud it.

And I hope to repay the kindness by engaging in a discussion for the purposes of clarification, right?

That's what we're here for.

okay so you know i i i think in linkedin now i have myself a scientist and inventor and i'm starting i think i'm going to flip it i'm going to say inventor and scientist because i'm the older i get the more i think that invention is better than science first of all it's been around before us right and some people claim that the fire made us right and we didn't invent fire it wasn't invented by homo sapiens

And so invention has been before humans and probably will be after humans are around.

Right.

And and so I've done I worked in many different things.

And so I've done I have a patent for an underwater periscope.

When you are at the bottom of a pool, when you look up, you see that you see that circle that's called snail's window.

The whole world, the whole above surface world has been compressed into that circle.

and distort it, but you can see with the shading, right, that you can pick up pieces of the world.

And so if you have a polarized camera, you could reconstruct this.

So it's this idea of reconstructing, right, what's beyond that I find it interesting.

And I've worked in, I trained as a physicist, but I trained, you know, I've been always interested in the marriage of mathematics and exact stuff.

with a very physical problem.

So for my PhD, I worked on superfluid film vortices on a torus, and I found a theorem of Riemann that had no application of quantum mechanics until this problem.

Okay, so let's get to the meat of the problem.

So the principal agent monitoring problem.

So this is a problem as old as a hill.

So it appears in Plato's Republic, and some people call it the allegory of the ship of fools, right?

so there is a ship owner who has a crew of sailors and you know they're kind of rowdy sailors plato says that they may be drunk and among them there's a philosopher king who's plato himself right and uh and but the owner you know doesn't know how to pick the sailors right the owner just owns the ship doesn't have any ability that's why the owner has hired sailors

so if you are an a a principal and you hire somebody to do the job for you either you're lazy or stupid and you don't want to do it yourself so you have this monitoring problem how do you monitor that you know how do you do how do you monitor work that you don't want to do or don't or don't know how to do does everybody get that the paradox there

And so Plato's answer is democracy is no way to do this, because there can be factions.

Part of the crew can have a faction, and you're not going to pick the best captain if you have people vote for who the best captain is.

So Plato used this to attack democracy.

but this kind of goes this is counterintuitive right we know that that that group right that groups are better for us right that ensembles help us survive better right so there's this tension in western civilization between the sphere of the mob right and this thing about the unitary right philosopher king that's going to be the dictator so who's going to rule us is it going to be the philosopher king dictator or is it going to be the mob

So there's this constant tension.

So I came up with this problem because I'm one of the hundreds of scientists that have made speech recognition possible on your phone right now.

I used to work for Dragon NaturallySpeaking.

And Dragon NaturallySpeaking, if you go into Wired Magazine and look at the highlights of machine translation, the founding of Dragon Systems in Newton, Massachusetts,

in the 80s is considered to be a pivotal moment because you know the dragon systems went on to create uh the first uh continuous speech recognition product on the desktop and and the way that speech recognition was developed was as people may know from machine learning was by having data sets and one of them was people reading the wall street journal and then that being transcribed with all the mistakes that people made and the hooms and the haas

and so forth right but after you know five or or six years you know the wall street journal uh corpus became very important as a benchmark and so people started realizing well you know i'm being graded incorrectly in this transcription but i actually it's the official transcription that's wrong and so why don't we just redo it right and so they went through the whole corpus and they redo they spent the money to do it better

and so they so every test from then on writing the software had the old benchmark with the mistakes so that you could still benchmark all papers and then the new benchmark and i said who's checking the new benchmark right it's the principal agent monitoring problem right who checks the experts how do we know that this last transcription doesn't have any mistakes

Then a second moment in my professional life.

I'm a lecturer at UMass Amherst in physics, right?

Physics 101.

I have to lecture 500 students.

The only way to do this is to give them multiple choice exams, bubble sheet answers, right?

That I then collect with the proctors at the end of the exam and take to the back of the room and feed into an optical scanner.

I won't describe to you everything that I did and I was told to do to randomize the test in the room.

OK, but it led me to sometimes when I was feeding those exams to say, why do I need to feed the first sheet, the answer key?

Why can't I use the wisdom of the crowd to grade the exam?

And then finally, the thing that actually got me into the mathematics of actually doing this was in 2008, I started working in UMass Amherst in the computer science department with Howard Schultz doing digital elevation models from multiple maps that were done from aerial photographs.

So how do you combine these things?

There's so much data now.

This is the problem that we have, so much data.

How do you combine it and not prevent the noise from corrupting it?

And then finally, the 2010 patent by Data Engines on error independent model solution, which, as I'm going to tell you, has disappeared.

The patent is null and void.

There can be no patents on postulates.

You cannot patent natural loss.

So I'm telling you the history of my business failure, basically.

okay so so this is the paper that was on uh you know the the geometric uh let me get the laser pointer the geometric precision error right for computer vision tasks and basically the the idea there is that you you have a bunch of maps okay and you're and i'm plotting here the error covariance matrix between the different maps

And the way that my boss, Howard Schultz, did the maps, he did it so that he took advantage of the fact that when you have a photograph A and photograph B, when you match features in A to B, you can go this way, right?

One way, an arrow, but you can also go the other way.

And that matching is not symmetric.

So he wanted to take advantage of that non-symmetry to discover blunders.

And he did so, and he was able to show that he got rid of blunders.

But he went on to continue using the maps.

And everybody told him, Howard, why are you doing that?

Those maps are really correlated, right?

That's A to B and B to A matching.

And he said, yeah, but why not?

They have some information.

and so eventually i was able to prove that he was right and this is what i'm showing here is is where i have a model where i just assume that everybody's zero and all and i've induced a signal if you're a physicist you know what i'm talking about right because i have these two pair to these two maps that i know have been correlated or highly correlated and i'm introducing them there with other maps that are not so correlated

So it's like I'm putting an injection, right, a modulating signal.

And this recovery here was with compressed sensing.

And you see how without being told that along the diagonal there's a strong structure, it's able to pick it up.

Right?

So sparsity.

If the errors are sparse enough, you can discover the error, the precision error between regressors.

So even though today I will talk mostly about classification, everything applies to regression too.

And here's the crux of the issue, right?

That the way to do this is to basically get rid of reality.

This is the way that you get rid of representation.

Right?

If I have a model, why don't I subtract a true value from the model?

Whatever remains is the error.

And so I'm just going to talk about that today in terms of classification.

OK, we can stop here if people want to ask questions, because the next theme is going to be data streaming.


SPEAKER_01:
Yeah, I have a few comments.

Jakob, you want to go first, though?


SPEAKER_02:
Yeah, sure.

was wondering if there if you have been thinking about i guess a measure to quantify how uh how much information is lost or can be lost when using a particular benchmark uh because the way it sounded like when you were talking about it is

I guess, similar to the notion of, well, all models are a coarse graining of the particular thing we're modeling.

Therefore, every benchmark is going to have this problem of who is watching the benchmark or who's checking that the benchmark itself is correct, which is a, I suppose, inescapable problem.


SPEAKER_03:
We cannot escape it as a civilization, right?

Nobody knows what the answer key is.

right there is no dictator that we can go to as a civilization as a society it's inescapable right we're always benchmarking to something else there is nothing but benchmarking to something else right am i am i crazy here or can i get an almond right nothing but difference

yes nothing but gradient unity is yes at minimum two yes right that is the only thing that we have to observe and that's a statement of relative relativity in physics too right that we cannot observe a absolute velocity we can only observe relative velocity between particles it's the same thing for measurement it's no different yeah


SPEAKER_01:
a few other comments.

So the back and forth, keeping the back and forth directions being fit and how that contains information on blunders like you described.

And then another subtle pattern is in the matrix, one slide previously, in the cells that were too off diagonal, yep, there's a slight positive correlation that's transmitted

spuriously, potentially amplified through compressed sensing and distorted and so on,

there's also ripples of underlying even trivially analytical square correlation matrices empirically plus noise plus compressed sensing have these attributes.

And that is what puts so much of the experimental design and attention interpretation onus onto the cognitive entity as hardly this or any data speak for itself.

Yes.


SPEAKER_03:
Yes.

Yes, I have to agree with you, right?

We have to distinguish here there is no magic and there is no metaphysics here.

There is only a measurement, which is a statistic of a sample.

And therefore, it's impossible for you to generalize it.

It's not an intrinsic property here.

If I'm telling you that this is the error correlation in those maps, I'm telling you that that's the error correlation for that particular collection and for those particular maps, not for the algorithm in the future or in the past.

I'm telling you something about the test that you just took, period, end of story.

Nothing else.

If you want to ascribe meaning to that, go ahead.

But then you're incurring that

right, that cognitive, you're the one who's now becoming the dictator, right?

The bucket is stopping with you because you've decided to assume that that measurement meant X about the world.


SPEAKER_01:
Yes.

When you undertake the epistemic quest, then you're the principal epistemic agent.

And that's an N equals one situation.

Another, I think, interesting touch point is

hierarchical predictive processing models which we talk about a lot in inactive inference they have a highest level prior and if that is going to be also learnt over updated there's going to be a hyper prior on that so that's one aspect where there's just the buck stopping at a point from a hierarchy perspective and then also um even with just the gaussian

the bell curve with the mean and the variance but there's not a variance on the variance that is what gets collapsed that's the metacognitive and that degree of freedom is what constrains and makes models go stale and makes them over generalize without being aware yes and i want to i'm going to address that one here because i'm going to talk about measuring error correlation independent model being able to measure empirical error correlation


SPEAKER_03:
Now, whether you want to generalize that because you have some view of how to base and generalize it or not, that's a separate issue, right?

But being able to at least benchmark the actual correlation on the test

so that then you can have those cognitive models and those epistemic models that can have that.

Let's make the analogy.

Your car has a computer in its engine, right?

And it also has a thermometer.

What I'm going to talk about today is about how to build a thermometer that helps the car's computer run the engine better, right?

And all I'm saying really comes down to that, that wouldn't it be great

If intelligent minds had a thermometer for intelligence, right?

Because then the thermometer just tells you that the car is overheating.

It doesn't tell you why.

You need a cognitive process, right?

To figure that out.

Oh, it's because a piston is blowing, right?

Or the oil went down, right?

That's a separate issue, right?

But knowing that something is wrong by overheating with a thermometer is golden, right?

And I want to get to that rock of having a thermometer that I don't have to worry about representation.

It just gives me a measurement.

I'll deal with the fact of what it means, right?

But I just don't want to worry about what that measurement is, or I want to reduce it as much as possible.

So it may sound weird, but you would want a tool that strips epistemology.

from checking the logical consistency of your evaluation.

So that you can aid the epistemic agent.

And so I'm right.

This is the thing that I find weird and fascinating here that I'm going to propose to you that what we need to do is strip epistemology.

And I'm going to do that, you know, continuing on.

And so let me now start talking about that, OK?

So data streaming.

So you guys talked about sparsity, right?

So the way that I encountered data streaming was with a good Turing smoothing algorithm in speech recognition.

At Dragon Systems, they gave us a book written by, gosh, how can I forget his name?

Our research director.

And he had good Turing smoothing, frequency smoothing.

Can I get a count of how many people know about what this is or know what data streaming is?

Is this something that you guys know very well as a community?


SPEAKER_01:
Give a description that sets it up for how you want to talk about it.


SPEAKER_03:
Okay.

So the way that the typical data streaming algorithm and the way that it gets presented is that it's a method for minimizing memory.

when you want to compute statistics of a of a stream and so the prototypical simplest cases you have a stream of numbers that are coming down the the pike right and you want to keep uh and at any moment i want you to tell me what the average of those numbers are so the memory intensive way to do it is to store all the numbers right and whenever you a query gets made

take the average of those numbers right the whole list but of course as the as the stream increases right that list is going to balloon and become humongous so the the philosophy of data streaming is is don't do that just keep two numbers have a sum so every time you get a number increment the sum and then have a counter of how many numbers you've seen and then the average is just the sum divided by the number

Right.

So just having two numbers, two integer counters or two real numbers, whatever.

Right.

You can store and at any time compute a stream that may be a billion long.

So that's the purpose of data streaming.

Right.

To compress you, you take a data sketch, as it's called.

which is a compressed version of all the events that you've observed in the data stream, in all the richness.

But you're not doing that.

You're just taking the events and you're doing some sort of compression on it.

And you're keeping that statistic.

And then at the end, when people ask you, you say, oh, this is the average that I've observed so far.

so for a good touring smoothing got invented by alan turing during world war ii because i don't know if you know that he was responsible for the enigma machine breaking right in world war ii and you know they had they started to you know alan turing was a very good mechanical dude he actually built a tide predicting machine way before computers or he did any of the stuff that he was doing you know uh as he's known very abstractly right

He was very much hands-on, and he helped build the bomb, which were early computers that were processing the German traffic that day and trying to decode it.

They had to work all day.

These bombs were continually churning, right?

This is the Enigma machine, by the way, recently found in a flea market.

$15,000 if you find this machine.

So it had these very complicated rotors, right?

and so these these are the bombs and they had a lot of uh women assistants right back when when computers were woman right before uh you had the grace allen and so forth right and so uh this you had to send people home at the end of the day when did you stop you don't know how many a secret rotor settings were done that day

when do you stop when do you decide that you've done enough processing and there's no big chunk of German traffic that you've missed so he came up with good touring smoothing which was how many rotors settings have we not seen which is a crazy statistic if you think about it right what do you mean how many things have you not seen

And so he was able to do it by shifting, right?

By saying, well, there's many orders for which we've seen only one message, right?

And there's fewer orders settings for which we've seen two messages, right?

And so if you look at how many messages were per rotor setting, which now biologists use to figure out how many species they miss in a survey.

You can then know how to shift the mass to the zero frequency observation.

You take a little bit of probability from everything and shift it down.

And can you believe that this is still one of the provable best ways of doing this, estimating unknown frequencies?

Alan Orlarski from UC San Diego has been doing some of the modern work on understanding good Turing smoothing.

Notably, it doesn't get used nowadays in neural networks.

Because neural networks never have any missing data.

They will always give you an answer, even if it's wrong.

Or Linsky in this paper, which was published in NeurIPS, argues that that's wrong and you should go back to doing what it does.

But we're not going to go into that.


SPEAKER_01:
Can I make a comment there?


SPEAKER_03:
Yes.


SPEAKER_01:
um this reminds me of the distinction with parametric and non-parametric statistics and when people talk about how many parameters this model has neural network transformer what have you it's a parametric model that's why they're talking about how many parameters there are and non-parametric methods although that's kind of like saying the non-elephant animals kind of classic joke

It's also like a very open and procedural space where there can be a lot of methods such as shuffling and leaving one out, all these different statistical methods that may have less power or less of some statistical positive capacity

in the case where the generated data, for example, are known to be from a Gaussian error distribution, but under relaxed assumptions about variant structure, then non-parametric statistics can do well.


SPEAKER_03:
Yes, and heuristics must be something that biological systems use a lot, right, instead of exact computation.


SPEAKER_01:
I would mitose that statement into two, which the first statement I'm most comfortable with is we can model biological agents as using heuristics.

We can model anything as using heuristics in our...

finiteness as cognitive modelers so if we say we're restricting ourselves to an eight parameter model or to this kind of genre of model for this phenomena it's a map territory distinction now as to whether the territory uses heuristics whether you can really say does a bacterium or does a squirrel use heuristics

That is a question.

What are those heuristics?

But the first question, which I feel most strongly about, is that we can say that we'll choose to use heuristics in modeling complex behavior.

Why not?

How can that be not the case?


SPEAKER_03:
Okay.

I'll grant you that.

Because I don't have actual experimental, right?

I mean, to be precise, right, scientifically, you're asking for a particular demonstration of our heuristic and our biological system, and I don't have that.

I only have an aspirational statement that maybe it does happen, right?

Okay, so the thing that I want to do here and the problem that I'm trying to solve, again, is not necessarily procedurally what's happening.

We know that us in the world are not completely isolated from the world, right?

But what I want to think about is the Gedanken extreme where you're isolated and paranoid, which does occur in certain technological realms.

It doesn't have to be a mental paranoia, right?

So, for example, a self-driving car that wants to check its pedestrian detectors immediately, right?

That immediacy, right, means isolation, right?

It can't wait to get the feedback by actually touching something, right?

or get further information moving forward.

And, you know, it's close, right?

It can only rely on its own things because unlike a Roomba, right, it can't be bumping into pedestrians.

It cannot get feedback any other way other than this mechanism that it's decided to do whatever it is for pedestrian detection, which could be sonar or could be visual.

So I think of it as an extreme that I want to solve to understand what are the limits of what can be done.

OK, not because we are actually need to be in that realm.

And so I want to I wanted to build certain features into it, right, because I was using this in speech recognition.

And one of the things that I wanted to do was to make it simple.

And so I wanted to treat all the members of the example as black boxes.

So I don't want to go inside your brain.

I don't want to go and get readings of internal parameters in models, which is not wrong, mind you, again, right?

To come back to the analogy of the thermometer and the computer in a car engine, right?

You can have intelligent things.

And a lot of people do neural networks, right?

And they inspect internals to try to figure out, right, how to detect when it's actually hallucinating.

There's nothing wrong with that, right?

But I'm saying that it itself is then going to bring assumptions that I can't check.

Right.

So I don't want to do that.

I want to treat everything as a black box.

I just want to see what the decisions are from the Oracle or whether that Oracle will be a person or function or whatever.

Right.

And I don't want to go.

I don't want a theory of mind.

The other thing that I want to do, and here's the crucial thing, right, is I want finite chains of safety validation.

I don't want to use Bayesian models.

I don't want to use probability.

Because like you just mentioned, Daniel, those are hyperparameters, which are encoding information about the world.

And the whole point about being paranoid is that I cannot, right, I cannot rely on that.

I cannot rely on any representational knowledge.

I don't want to.

To avoid that pitfall.

And so by going to sample statistics, that's how I'm going to avoid that.

Because I'm not going to talk about distributions.

I'm not going to infer anything about the future or the past.

I'm just going to tell you a number about something that already happened.

And since it's a sample statistics, right, I'll be able to then do the magic that I do, which is to create a complete set of postulates.

So then I can prove that you could be in states that are probably wrong or inconsistent.

And and so that's the punch line, right, that I can do both of these things of being isolated and Gedanken, right, and paranoid by just

having functions that only use statistics of the decisions by the ensemble members nothing else let's stop here because this is an important point right this is a design choice right that i'm making could you um unpack that a little further i'm not sure i fully understand

how samples using sample statistics differs from conceptually from using say bayesian probability on your data so would would you agree that there's the concept of sample statistics and then you know distribution statistics right and things like that right that you can always define a sample statistics and actually compute it if you knew what the sample was

So that's what I'm talking about.

That's what I mean by sample statistics.

And then I mean by, so what would it be?

It would be something like if you have data being produced by a Gaussian distribution, right?

A sample statistic would be the mean of what was produced, okay?

That has meaning, that exists, right?

Independent of what meaning you want to ascribe to it.

Whether you want to think that it's the mean of the actual Gaussian distribution, right?

That's a separate thing, right?

Using that data and using that mean to then estimate what is the mean of the Gaussian distribution that produced it is a completely different task.

Yes.

Right.

So for one, the data may not have been produced by a Gaussian.


SPEAKER_01:
This issue, just to connect it to like what we talk about with variational or just Bayesian inference and active inference.

Let's just say in the room, the temperature is continuously variable.

But then the thermometer is going to be integers.

And then the agent is only capable of coarse graining into buckets of 10.

we can still talk about only cares because of you know a precision uh engineering spec right why am i being uh uselessly precise right yeah yeah variable precision to promote system properties is definitely a corollary of this kind of a framework more broadly like getting stability

getting rid of the representation question.

This is like hierarchical predictive processing of visual.

And so at the earlier lower levels, and this phenomenon is recapitulated in silico, as well as in aspects of biological systems.

that the earlier layers pick out less abstract components like edges and on and off.

And then not that it's clean, but that at each synaptic interval, which can be modeled as actually means and variances and differentials in the hierarchical predictive processing architecture, that can be modeled as like a kind of like representation explained away.

You know, where's the rom-com in this pattern of neural activity?

And then it's just very interesting that you brought up, of course, the engineering setting.

Predictive coding, which is to say difference in coding, was discovered slash invented in the video compression setting.

And yet in the Rao and Ballard 1999 work and so on, it came to be understood as a more general property of how information transmission and so on

in certain settings led to certain parsimonies that has certain relationships with things like the Kalman filter and predictive coding.


SPEAKER_03:
So you and Jacob have mentioned this thing about a transmission of information.

So I'm not going to go into it.

There's so much to talk about.

I've been working on this for a decade.

The difference between the observed responses gives you

something about the test, but also something about the quality of the person who answered the test.

The uninformative person who answered the test doesn't transmit anything about how they performed.

The random guesser only transmits information about the test.

If I randomly guess what A and B are, right, I'm going to have

the right proportion of A and B as the test itself.

It's going to be in the wrong places.

Yeah.

But you're transmitting the difference between the A's and the B's.

And so there is no magic here.

If the ensemble is made of stupid agents, they are not able to transmit via their aligned responses any information that allows you to grade them.

So my method has blind spots.

There is no magic here.

If your ensemble is made up of stupid members that don't have any information, you're not going to get anywhere.


SPEAKER_01:
What do you think about an ant colony where none of the nest mates know about how many seeds that they have and how many nest mates need what and how much rain there is outside and the rate of information and blah, blah, blah.

But they don't know any of that.

They're just using their local interactions.


SPEAKER_03:
I actually have a couple of books on ants and ant colonies.

And I saw that you wrote a paper on ant colony behavior.

I'm very interested in thinking about that, but we're not going to be able to talk about that, OK?


SPEAKER_01:
Fair enough.


SPEAKER_03:
OK.

So I'm interested in the comment you made about RAU, too, general information.

So data streams as NTQR tests, right?

So we're going to look at binary classification, right?

So down here below, we have the true label, right?

So this is a stream.

So think of this flowing to the right, that gray box.

Everybody sees that alpha and beta?

And then I have three different classifiers that are making different decisions, right?

So aligned on the first item, the first two classifiers are saying that it's beta, and the third one disagrees and says that it's alpha.

Everybody with me on this?

And so what I'm going to do is just collect how many times each of these different patterns occurs.

That's the compression, right?

So we could be looking at a test that has a million items.

I don't care.

It says I'm doing binary classification.

I only need two to the three eight integers.

So right there we see the compression that's very attractive, right?

For in terms of a biological system.

And because you're not you're not storing the whole stream, you're storing these eight integers that are picking up these patterns.

OK.

like i said and like daniel has said right you can digitize to whatever you want in terms of engineering spec you don't want to measure the degree of the temperature of a car to a millionth of a degree it's useless right economically there's no point

likewise you may not care too much to worry about you know knowing right you don't care about too much beyond boiling or something like that right you just want to be told hey the car is overheating right you could digitize that that's a safety you're coding in there by choosing the the degradation of digitation you're deciding what is your engineering spec and what is your safety concern okay

So I cannot tell you that.

This is an instrument that you have to design and you have to decide what your R is.

Any questions on that?


SPEAKER_01:
Yeah, just a few notes.

This is really interesting.

There's the principle of the hash encoding or the kind of lookup table for all the combinations, and then you can do hierarchical nestings.

And then also, this situation is kind of like the cells in the retina, just speaking coarsely, that there's some false positive and false negative for some activation thresholds.

And so it's kind of like a retinal...

display that is getting noisy photons and and also there's there's variability through all of these finite systems variability and finiteness are ever prevalent but the observation is just what it is


SPEAKER_03:
exactly exactly and and therefore right uh all the problems with philosophy and logic occur because of infinity right if we just accepted that the world is finite but maybe very large right we would solve all sorts of problems right zermelo's axiom of choice is only needed for infinite sets right finite sets everything is is explainable and understandable okay so

so here i'm going to show you you know eventually you're going to see this this this this is the the set of equations for three classifiers and these are the eight frequencies okay and so these are the complete postulates for error independent classifiers so i'm going to come back to this and make it much simpler than it is right but i just want to show you how

you know these frequencies here right there eight of them and i can sort of synthesize right uh assuming that that that that the classifiers have some sort of uh performance right if i just i i randomly pick numbers between one and a hundred and then divided by a hundred to to have these values and then i just plug them back in here and then i get what what the frequencies would have been of how they voted

So I'm just trying to show you how these numbers are integers.

There's no probability here.

And then the other thing I want to go back to is something that I think you guys would find interesting from the physicists.

Renaud was a 19th century French physicist, very famous for building the most precise thermometers of his time, at the time when people were just inventing the concept of temperature.

And people told him, how do you know that you're measuring anything?

Temperature is an imaginary concept, dude.

We don't even believe that there's such a thing as temperature.

And even if you did, you would need to have some sort of theory about how you make errors in your thermometer so that I would believe what your thermometer is saying.

And Renault was, screw that, right?

He says, are you saying that there is no science without theory, that there can be no purely empirical measurement?

And he said, no, I'm going to make purely empirical science.

And so he was a radical empiricist.

Eventually in the 20th century, we found out that empiricism doesn't work.

but he basically is going to do what i'm i'm going to show you today and and this is what i did with the maps he took thermometers and just compared them against each other that's all he did and then he looked at the differences right and he's the first one to start talking about the concept of precision as saying yes i can build the most precise thermometers because i can get them to agree to certain amount of figures right

And then they start disagreeing.

So even in science, right, there's a concept that you really do not have science until you have disagreement in measurement.

You have to have error in measurement.

If you do not have error in measurement, you either tell me about math and postulates or you don't know what you're doing.

Your instruments are stuck.

You think they have a position that they actually don't have.


SPEAKER_01:
Thank you.

This is awesome.

when we have multiple observations going wide that's sometimes called sensor fusion and then when we have the clash of the prediction and the observation and the differential that's found there that's the predictive processing architecture so those are both there's so many motifs but you totally need a minimum of two other otherwise you have like a statistical monad

which is really just a pure conceptual object.

And then one interesting tie in is like, in active inference, as we specify the generative model, the preferences are directly over observations.

In the extreme case, meaning that the likeliest thing and the preferred thing are one and the same.

And it's just that it just, you prefer to get that number on the roulette table.

And every single time your action is confirming that's the case.

That is one special case that can be set up as well as epistemic, more general epistemic activities.

But by having the pragmatic value load directly onto observations, like the thermometer reading, not I have a preference for the room to be at 72, but I have a preference for something about the measurements and that direct target rather than the hidden state being the direct target, which leads to a fantastical analysis.


SPEAKER_03:
okay so you guys seem to be uh yeah yes ahead of about thinking about this and how to implement it okay so let me finish with this with the temperature saying that that the reason i came up with upon renault

was because that this thing happens in invention all the time right you're not too clever you invent something and you think you're clever but you're not right it's somebody's thought about it before you just don't know it right you're just ignorant and so i said you know somebody must have thought about this somebody must have done something like this and so i went looking for it and so i found this book inventing temperature which has a whole discussion about renault and how he's wrong because he thought that he was doing a metaphysics

He thought that he was getting behind the curtain.

That was his philosophical mistake, right?

And so I want to pull back from his philosophical mistake and say, no, this is the comment I made earlier to Daniel, right?

I am not saying anything about what the reality is.

I'm only telling you an estimate of a sample statistic.

That's it.

How you want to interpret that is up to you.

And I recommend this book because it's great reading, right?

Because it's about how you invent physical concepts, right?

And the struggle that people went.

Okay, so now we're getting into the part where I think you guys are going to be interested because I'm going to go back to the digitizing the format for logical consistency because there's two different viewpoints of the same test.

And one of them, which is the one that I'm choosing to use the NTQR methodology just to have a new word for you to go somewhere else in your mind, is different from the ML binary classification case, which has semantics.

And here's where I want to show to you that the mathematics of the NTQR test is separate from the semantics.

It can be attached to semantics, but they're not the same thing.

So I want to spend some time to make sure that we capture this one, OK?

so and then the the payoff here is the the realization that you can start to understand it in terms of semantics this is something that a machine could use to binary classify things in the environment okay which is how i thought of at first and how people in machine learning think of it right i have a binary classifier because i'm looking to classify things in the external world i'm looking to be told how many a's and b's there are in the external world

But once I pull the rug from under you, I'm going to tell you that exactly the same mathematics can be used to figure out the statistics of correctness.

And that has nothing to do with what A and B means in the world.

Because it just means that there is some percentage of the questions have A as the right answer and B as the right answer.

But who knows where you came from digitizing that format, right?

so there doesn't need to be any semantics attached to a and b and i can still calculate what your performance in the total test is so something that can be attached semantically then has the same mathematics in a way that it can be used to check tests about chemistry about philosophy about geology about any subject right you're not doing binary classification you're just correcting grades

a test that have two responses per question or three or four.

That's a general thing, yes?

Very separate from the specific task of binary classifying things in the world.

So I'm going to stop and encourage discussion because I want you to get I'm going to show you the differences, too, in how it comes into the mathematics.

But I want to first get if you if just with words, you understand that there is binary classification where the percentage of A's and B's in the test actually means something in the world.

And then there is grading of multiple choice exams.

where the percentage of A and B has no meaning outside of the test.

It just means that 10 questions in the exam had A as a correct and 10 had B. But the questions could have been about geology, right?

A and B doesn't mean anything.


SPEAKER_01:
Yeah, just to kind of say it how I'm seeing it, if we look at the output of a statistical test, we might get a vector percentage rain and percentage not rain.

Bayesian causal model, probabilistic statistical model, percent dog, percent cat.

In contrast, we have a different procedural non-parametric approach that can be designed incrementally to discard various aspects of information

Especially in a discrete finite space.

Taking kind of the finitest perspective on modeling and then using that to say, well, I am core screening into this many cells, so I am going to have this many hash codes.

So we will be able to do this in this runtime.


SPEAKER_03:
Okay, that's a little bit too complicated, Daniel.

Let me restate the following.

We have a philosophy question, okay?

A philosophy exam.

and and the and the and the philosophy exam consists of uh you know that a and b questions right where you giving a passage and you're giving two philosophers and you have to identify which philosopher wrote that passage okay you get the philosophy exam


SPEAKER_01:
Yeah, it's testing for familiarity with philosophers, not the ability to generate philosophy or recognize new kinds, but I hear you.


SPEAKER_03:
So we could have Schopenhauer and Wittgenstein as choices A and B in question one, but question two could have Plato and Aristotle.

So A and B are not referring to the same thing in the two different questions.

That's what I mean by having no semantic connection.

Right?

But your grade is the percentage of times that you correctly answered A when it was an A-type question.

Right.

So does that make it easier to understand that I'm making a simpler point, right?

That I can detach grading you from having attaching a semantic meaning to your response.


SPEAKER_02:
It's like you, this is a common question in neuroscience where certain cells while learning a task can learn an abstraction of the task.

For instance, if it is A or B, you will have cells that respond to this concept of A or B, but then you have specific components of the network which

specialized to the particular instance of a or b like aristotle versus plato um or any other uh set of um set of philosophers i guess the question is whether um i'm not sure i fully understand the the what whether both both can happen at the same time


SPEAKER_03:
So both can happen at the same time, right?

You recognize that you can have the general structure, right?

Basically, what I'm trying to argue is that if there's an algorithm that can grade r equal to questions, right, then I'm done, right?

It can do it for any r equal to question of philosophy or economics, and it can also do it for binary classification.


SPEAKER_01:
What do you think about binary classification of language, like language generation questions?


SPEAKER_03:
Yeah, anything, right?

But you would have to decide if that's correct.

Let me go on maybe, and then when I show you things, you can start.

Let me see if I can, let me escape and see if I can go now to Mathematica so I get to show you some pictures.


SPEAKER_01:
Yeah.

Yeah, like the example, yeah.

I think of the pheromone perception in the environment and the alarm or no alarm.

Is it at the threshold?

You got finite number of antenna, finite number of receptors, noisy signaling, go, no go, buck stopping with an estimate brain somehow, some way.


SPEAKER_03:
Okay, so let me go on and you'll see what I think you'll start to see how there's different ways of looking at things.


SPEAKER_01:
We certainly see PowerPoint.


SPEAKER_03:
Oh, sorry.

I see.

Yeah, I'm going to do share again.

And I'm doing share now of Mathematica.

OK, you see the Mathematica?

Oops.

Not yet.

I just took it away.

I don't know why I did that.

You see it now?

Yes.

OK.

And then, oh.

I'm going to stop the show because I need to do that.

Thank you.


SPEAKER_00:
That way I avoid.

Okay.

Perfect.

Perfect.


SPEAKER_03:
Okay.

And then now I can go to here.

Yes.

So this is the paper that I sent Daniel and it's the one that I've submitted to a special issue on AI safety and philosophical studies.

And so

I want to, you know, so because I'm doing safety, right, I want to I want to speak about this specific case of doing a binary classifier to be concrete.

Right.

And so I'm now shifting into getting you into the mode of accepting that.

In fact, they are.

There is a logic to evaluation that there are postulates and there's a logic and that it's universal.

It's a big one.

It's a big burger.

And so the way that I'm going to do it is by just talking to you about what are the possible grades on an exam that has two responses before I see anything.

And so I know that if I have QA questions of A type, then the number of correct responses, so R is for response,

on the a questions that the i'th classifier right does everybody see that notation r a sub i right this has to be these relationships have to hold i claim these are postulates

You know, I've been thinking, how the hell did Euclid do this to begin with?

How do you introduce postulates?

What do you argue?

What do you say, right?

When you have postulates, you can't prove them, right?

You have to sort of say, I can sort of prove them here, but you have to sort of see them and say, yeah, of course, that makes sense, Andres.

Right?

You can't have more correct responses of A than there are A questions or B. Agreed?

within a circumscribed axiomatic verified information environment so play on where i have q questions right where i've told you what the number of q questions are right there's no way there's no there's no other way right and so therefore i can i can immediately go and write you know this this table i mean this is how simple it is right i'm going to tell you here let me let me make that bigger

So you see this, I'm having a Q equal 20 exam, 20 question exam.

And I'm just going to make a table of all the possible correct A responses you could have.

AA, right?

The number of A's that you correctly said that it was A. And the number of B's that you correctly said were B's.

And then I'm just going to keep track of the number of A's, right?

questions well the number of a questions can only go from zero to the total questions right and then once i decide that there's any of them then like i said right my correct responses can only go from zero up to that one and then if i have any questions that b questions has to be this remnant and so if you just do that this is this is the cube you get this is what you get for the grades

until three weeks ago i've never seen this cube and it's amazing that this cube is it's actually you know kind of a nice figure right it has got this double-edged kind of thing right but that's it if you answer a 20 question exam with 20 responses your correct grade is somewhere is a point somewhere here is it a true tetrahedra

I don't know what to call it, right?

It's got that 90-degree twist, right?

At the bottom is 90 degrees to the top.


SPEAKER_01:
Yeah.

Well, wow.

What do you see in it?

What does it mean to you?


SPEAKER_03:
Well, the thing about it that I found really interesting is let me show you the other way of looking at it, right?

Which is now the way that I looked at it for years until this year, which is the reality in which I looked at it, which was the machine learning.

Where now I'm telling you what is the percentage of things that A's that you got correct.

I'm telling you the percentage of B's and then I'm telling you the percentage of A's.

And in that cube, that same Q equal 20 test, this is how it looks.

See how different that is?

See how it has that kind of quadratic structure?


SPEAKER_01:
What's different between the two postulates?


SPEAKER_03:
Okay, so I'm going to show you what's different is that I decided to do the math in the machine learning world.

So this is what I'm sort of showing you now, right?

These are postulates, right?

The number of A responses that you give me

are equal to the number of A responses that you got correct plus the Bs that you incorrectly label as As.


SPEAKER_01:
Yeah, true positives and false positives.


SPEAKER_03:
Exactly, exactly, right?

These are postulates, right?

These are prior postulates, right?

They exist before you see any observation about the test, right?

Everybody's on board with the fact that this is postulates, right?

And that the number of responses you give

in a sort of election integrity.

There can be no more votes than their voters.

It obeys this.

And I can rewrite this in terms of being binary classification with these equations down here.


SPEAKER_01:
Yeah, just to kind of connect that to the case of an election.

So it's like one approach would be to take in the samples as evidence and try to estimate a continuous hidden state.

And you're always going to get into this statistical approach versus treating the fundamental finiteness.

We had, I'm not going to use the NTQR, but those are the parameters that describe the finite state.

printing and dissemination project.

Of that project and that printing, there was a thousand ballots with five options.

That is the state space.

You could go into cognitive modeling.

You could do narrative information.

You could do interpretation.

You can have this, all those things.

But the finiteness and the discreteness

Then to get around this challenge of like, well, temperature, like in, it could be like any number, or it could be a number greater than zero.

So how do you deal with that?

And you take a kind of engineering approach here, which is you define the safety zone, which can be hierarchical, abstract safety zone, generalized anomalies, and so on.

But you define the safety zone.

Within the safety zone, you have the direction of free energy minimization.

statistically control or you just stay in this in the fully finite world and maybe the issue can be resolved by explaining away the semantics of the decision through mere propagation and procedure exactly logical consistency okay so so so it turns out that that so so these two postulates right


SPEAKER_03:
These two positives here are written as a binary classifier here.

And you see that these equations are linear here, right?

So that's the cube that you saw with the planes.

And you saw that when I went to the ML, I showed you parabolas.

That's a quadratic.

Are people seeing that in the equations, right?

That when I change to the ML point of view where I'm dividing, right?

Then I end up having these quadratic structures, right?

In the cube, do you see the parabola?

What does that represent to you?

That's the ML space.

That's where I'm telling you the accuracy as percentages.

You see?

The percentage of you got half.


SPEAKER_01:
Yeah.

This test has the highest resolution.

When the coin is even 50-50, statistics does the best job.

When the coin is 99-1, then three measurements give you very poor statistical power because you can't resolve 99-1 from like 98-2 very well.


SPEAKER_03:
It's not quite like that, but it's close to that.

So let's move on.

Okay, so I'm not going to show you the math, but these two equations, right?

This is where the algebraic geometry comes from.

This is what we can observe, right?

We observe how many times you said that it was A. We can observe how many times it was B, that you said that it was B. And I know it's related to your individual performance by these equations.

it turns out that you can disentangle them and there's actually you know because these two frequencies have to add up to one there's only one postulate and that possible is being represented here in the two different ways it's either this one or this one either in the ml world or the the ntqr world and here's where we come back to that thing about transmitting information the difference between your responses is telling me something about the difference in your correctness


SPEAKER_01:
Just to clarify, you use a single equation number 16.


SPEAKER_03:
It's exactly the same test, but I'm showing P of A sub A is RAA divided by QA.


SPEAKER_01:
So are you co-asserting these axiomatically, or are you declaring them equivalent?

They're equivalent.

And so they're co-asserted by equivalence.

Yes.

Why do the shapes look different then?


SPEAKER_03:
Because they're in different spaces.

One is in the p space and one is in the integer space, right?

One is in the integer ratios and the other one is in the whole integer.


SPEAKER_01:
Okay, so it's kind of like a discrete mesh, and it's a continuous space, and there's integrity between the discrete and the continuous, and you have very strong procedural guarantees on the discrete, and then you have arbitrary or standard statistical guarantees on the continuous.


SPEAKER_03:
But my problem is that I have difficulty seeing how a biological system could do the division one, but I can see how it could do the integer one very well.

Right?

That's why I'm bringing up the NTQR.

That's why I think you guys would be interested in it.

Because the ML space is not, I don't see how a biological system could be dividing one.

But I can see how a biological system could build a cube that's 20 sided.


SPEAKER_01:
Yeah, that's kind of like a synapse and the synaptic release, it's in vesicles.

It's not like dopamine is a continuous variable.

At the synaptic level, the dopamine release is in one bucket.


SPEAKER_03:
Yeah.

so then so so then we get to the so so this is the the the interesting part right so so what is the problem with ensembles gosh if we could just do my so here's where we go back to heuristics right and my love of heuristics majority voting is such a great algorithm it's so simple right it's so simple and it works if if if things were good enough majority voting would just be good enough right it would just make us safe as it is

The problem with majority voting is that it can go wrong.

The mob can be wrong.

Things could be flipped in terms of their accuracy.

So we need to consider now ensembles.

And the problem with ensembles is that they can be error correlated.

They're not going to be making their errors independently.

So we need to handle that.

And so here's where we come to the first set of what I would call non-trivial postulates.

because this is what makes them complete right that if i observe two classifiers this right they can only vote four different ways and there's no other way that's it that's the completeness this is guaranteed so if i give you equations that describe these four different frequencies and those are going to be the four postulates i'm done

And so, by the way, these are the surfaces for some of those planes.

I'm going to show you later.

Let me move on to there because it's going to be 4.30.

So here's the correlation.

So this is what I was saying before, Daniel, that the P's are these things.

The response is divided by the total number of questions.

Right.

So either I can talk about ours or I can talk about peace.

And then correlation is defined as something that looks exactly like you would define it for distribution, but it's here defined for a sample.

Right.

So basically what I'm saying is I want to continue to write things as being individual products of performance.

And so what I really want to have inside this parentheses is the number of times that they both said that it was A and it was A. Well, that's this number here.

But I'm not going to put that number.

What I'm going to do is I'm going to stubbornly stick with individual performance, and then I'm going to put the difference between that number that I need and this thing that I'm stuck with putting in, which is the product.

And so these are the four postulates for pair correlated binary classifiers.

You only need two of them.

You need one for the A label and you need one for the B label.

And it acts exactly like you would expect correlation to do, right?

When correlation is positive, it's going to increase the number of responses where they agree.

And when it's negative, right, it's going to increase when they disagree.

It has the right sign, basically.

Right.

Correlation here is plus contributing plus.

And in these two equations where they disagree is continuing with minus.


SPEAKER_01:
Yes.

Something like a finite procedural.

recasting and reimagining of the true positive true negative false positive false negative those are four actual values and so again within the finiteness of digitized sensing especially you can take purely non-parametric purely algorithmic purely arithmetical numerical approaches

rather than always passing everything through the kind of statistical meat grinder.


SPEAKER_03:
I'm done.

I'm done, Daniel.

I'm done.

I'm done.

You said exactly, right?

This is the thing, right?

And so I'm not going to go back into... Tom Mitchell and Platonius, one of the students, came up with an independent solution, which is wrong.

And I've been slammed by reviewers because they say they did it right and I'm doing it wrong, and it turns out to be the other way around.

But I'm not going to go into that.

That's my personal beef.

But let me then show you what happens when you take these equations.

So what is the problem?

The problem that everybody has encountered with pair binary classifiers is that correlation is entangled with individual performance in every one of these equations.

Do you see that?

26A, 26B, 26C, and 26D

have correlation every single one of them we cannot separate correlation from individual performance but if you use algebraic geometry you can and that's what i'm not going into you can separate these equations you get back the individual equations 33 a and 33 b are exactly the same equations that i showed you before but then you get these new ones where you've decoupled correlation

So 33D, 33E, and 33Fs are the only ones that have correlation.

And it turns out that they're the same postulate.

You get no information from them.

And in fact, then everything in binary classification is controlled by this number.

The number of times that you agreed in the B label minus the product of times that you said there was B.

it turns out that you can prove that for binary classification this is exactly the same as this other quantity and the way that i'm going to use error correlation is this one this equation right here and the only thing i want you to note about this equation is that it's linear in the correlation so if you tell me what you think the grade is

and I observe how you voted and how you differed in your voting, I can tell you what your error correlation is.

I'm racing ahead, and I just want to show you some evaluations, OK?

So

Let me see what the... So this equation is the solution for the prevalence of As when the classifiers are making error independent mistakes.

It comes from this quadratic polynomial that you get from solving

that very complicated system that i show at the end so now i need eight of them i need three of them right so i need uh eight equations so now i have i j and k and you see sort of a descending a symmetric pattern right this ladder goes down one way and it goes up the other way these are the b's and these are the a's and when you solve that equation you get a quadratic

And it's that quadratic that I'm excited to show you.

You get a quadratic equation which has these complicated factors that depend only on the statistics that you have observed.

There's no free parameters.

So this is not Bayesian estimation in any way, right?

You have no freedom whatsoever.

so then when i do synthetic data where i i've made it to have 1920s of of one of the variables it predicted uh exactly but let me show you some experimental results and the one that i want to show you is where it fails that independent solution is going to fail if they if they if the classifiers are too correlated it's going to give you an imaginary solution


SPEAKER_01:
Can I make a comment on that?

Yes.

Sorry, go ahead.


SPEAKER_03:
An imaginary solution means that the parabola doesn't touch the axis.

And gosh, I look at this picture and I go, I can draw this circuit.

I can make this circuit, physical circuit, right?

So that I could, because, right?

I could make a geometric construction that draws this...

parabola and then i can check to see whether electrically it touches that axis and this is a warning light if you don't touch that axis your classifiers are correlated too much right the error independent solution has failed so this is something that no probabilistic method can do right no probabilistic method that assumes error independence

can show that the error-independence assumption is violated.

That's the main problem with distributions and statistics, right?

If you make assumptions, you cannot prove, right?

Because you're tuning your parameters to satisfy your assumptions.

But when you're doing things algebraically, like I'm doing here, there is no freedom.

You can be in a state where they... And actually, I can show...

because I can have a complete representation where I include all the correlations, I can prove that when I do these computations, if the classifiers ever have a square root that's unresolved, this is the independent algebraic evaluator, I can prove that that means that that set of classifiers is actually correlated.

It has non-zero correlation.

And below, you're seeing that that's in fact the case.

You're seeing that one of the correlations is almost 6%.

And I've been able to detect with the independent solution that, in fact, there is non-zero correlation because I have an unresolved square root.

That doesn't make any sense, right?

These are integer numbers.

What does it mean to say that you got a percentage that is some square root?

And this cannot be done by any infinite statistical... This is the distinction I want to make.

There's finite statistics, which is about samples, and there's infinite statistics, which is an infinite number of samples.

And there is finite statistics where you accept that what's gold is the sample.

The sample is true.

Everything that you derive from it is not possibly true.

versus the infinite statistical world where the world the sample is always wrong right in statistics a given sample is always wrong right the equality only occurs and the average of the samples that's a completely different way of thinking yes

and I say that the finite one is the one that I want to choose in terms of safety and it's 4 30 exactly so I'm going to stop because you know I think I feel like I've been talking too much and maybe people can chime in and push back and have interesting questions yeah anyone in the live chat and then Jakob go for it

yeah it's um i want to show you how i just want to say how crazy this is i'm giving you error correlations which are the only logically consistent error correlations once i decided to do majority voting or not and here i'm showing you that majority voting is kind of like a little bit too hysterical it thinks that they're 25 correlated and in fact they're only at most one percent and and so my method

right of using the postulates gives a better self-consistent solution i haven't gone into that right but i go what basically what i do is i take this number which is not at all an integer ratio right and i find the integer ratio that's closest to it right and then i ask for that integer ratio what is the only


SPEAKER_01:
logically consist of error correlation and i get this these numbers yeah that reminds me of the finite pi like 22 over 7 is a classic small approximator of pi and you can go into finite approximators

Exactly.

And therefore get closer and closer incessantly, keeping the strong guarantees.


SPEAKER_03:
Right, but at some point it's useless.

You're doing the width of a hydrogen atom, as people like to point out, right?

Your error in the circumference is going to be the width of a hydrogen atom.

Who cares, right?

and so a finite heuristic is going to do it very well sorry jacob i i had to i had to interrupt you there to just to point out right that that the fact that i can even measure error correlation and say that it's logically consistent is incredible i just have to say that yeah and no worries i i'm still trying to wrap my head around um the concept of


SPEAKER_02:
disentangling the semantics from the particular tools that you're using right um because i feel like it also kind of depends on your expectations as the person using these tools like going back to the going back to the example of like a or b like if you if you take away um if you take away the particular content of those of those questions and you only look at the statistics of the of the answers


SPEAKER_03:
of correctness i would say statistics of correctness statistics of correctness then


SPEAKER_02:
Perhaps you perhaps you are taking away the correlation from the particular content of those questions.


SPEAKER_03:
Yes.


SPEAKER_02:
Yes.

Are you taking away the correlation of, for instance, the person who said those questions?


SPEAKER_03:
Oh, my God.

It's so good.

It's so good that you were saying.

Right.

So so so here's what I had to do at UMass when I was giving the exams.

Right.

So so UMass Amherst is a state school.

Right.

So you get bubble sheet answers.

Right.

Exams at UMass Amherst.

Right.

But I taught at Williams and at Swarthmore.

I never gave bubble sheet exams at Swarthmore Williams.

It's only when you have large classes that you have these things.

At UMass, the students would cheat.

They told me, you have to make five exams, Andres.

The student is one color exam, and then you see people in a diamond pattern around that student.

So that everyone around the student has a different color exam.

And then you scramble the responses.

So nobody can look over anybody's shoulder and copy the bubble.

Right?

So what am I trying to do there?

Why am I trying to do that?

Because I want to prevent the error correlation that occurs because you cheated.

And I'm trying to find out the error correlation between all the students that is due to a commonly shared cognitive misunderstanding of Newtonian physics.

Maybe they're still thinking, right?

Maybe I put a question that is testing their belief in Aristotelian physics, right?

And I want to detect that.

And so when I get a wrong answer in a question, the purpose of the exam for me as a professor

is to find out what i should be talking about the next class right to address the mistakes that people are making cognitively i want to design a test that blocks any error correlation except the cognitive one and so that goes to what you're saying jacob right you get to design what error correlation right somebody's error correlation right may be a gold for you

I, as a teacher, want to see cognitive error correlations because I want to help you and teach better.

And when I talked about digitizing, you brought up another thing, Jacob, which is that maybe in the original signal, the responses were not correlated.

But then when I digitized it, I correlated.

And I think Daniel also talked about that.

right that when you change formats right you're introducing spurious signals and all of them are in there and yes there is no guarantee right but but by the way uh there is the advantage here that i can measure the error correlation right so that you can see whether i did introduce right by digitizing the the logical consistency of your grading

If I introduce correlations or not that existed in the original format.

Does that make sense?

There's no free lunch.

You won't get anything for free here.

If you really think about it, right?

It sounds magical, but it isn't.


SPEAKER_01:
It's okay on the free lunch theme, kind of thinking about this in a fun way.

There's a lunch where you just show up and it's there for you.

So that's kind of the dream scenario, if you're one of those who eat lunch at that time, because it's kind of like you've chosen the axis to create a finite projection to, it's the right test, the seating arrangement, there's just an ideal situation.

And then some of those may be actually perhaps even relaxed in practice

Like, I don't know all the details, but when you described that you're taking an integer approximator

So there's ways of integerizing and finitizing that strip semantics.

But then I thought about what if the ship of fools, what if none of them can be the captain?

Well, then the ship's going to go extinct.

And then it doesn't matter what, that's the imaginary parabola that doesn't make contact.

It's like, no, there's no decision rule.

There's also no quadratic regression, but this is just an imaginary solution.

These people are talking about survival strategies that are imaginary.


SPEAKER_03:
yes yes so so you get the other part about it which is what why it was good to put it in a philosophical uh journal which is what if there is no answer key what if we are all deluded and we're just testing with the test our alignment on a delusional belief i explained this to uh uh an ex-wife of mine who's a psychoanalyst and she because i was explaining the concept of logical consistency versus logical soundness by the way

With logical soundness, you need to know the truth table.

Logical consistency, you don't.

You're just checking to see, without semantics, that you're cranking the logic machinery correctly.

If your premises are correct, your conclusions are correct.

So that's what I mean here by logical consistency of grading.

I cannot tell you exactly what the grade is, but I can tell you whether you're being logically consistent with how you believe your experts are behaving.

Right.

So that that is.

Yes.

That's the only thing that can be checked here.

Right.

But it can be probably shown, right, that that you are in a highly correlated state and therefore you should raise an alarm.

And so when you look at what this could be, it could be also you could think of it as a way of selecting if you have different models.

and you find the one that's most aligned with what your beliefs are about what the truth is, then you have instant compression, right?

Just select that model, right?

Because you found the one that's most aligned with everybody, right?

And so you don't need to keep all of them.

But there's nothing to say that you're not hallucinating and that you're going to be eaten, right?

Because you think that there is no tiger there, but there is a tiger, right?

You could have false beliefs, right?

and be delusional right confident and alive not confident and soon to decease exactly right and so there is nothing here that prevents that from happening but it would allow you if you were initially in a good operating state to start detecting something that is malfunctioning right so that you could prevent that from happening


SPEAKER_01:
Yeah, I'll ask a question from the live chat.

Upcycle Club writes, I have a question.

How do you define the data sketch of the decisions?


SPEAKER_03:
So that was in the statement about completeness, right?

So whenever you look at an ensemble, by the way, if you know data streaming, right, there's data streaming algorithm for every statistic, right?

Sample statistics, there's many of them.

of course with a finite sample there's only a finite number but there's many of them right combinatorially explosive so here it is right if i talking about three right i have to talk about two to the three or eight different frequencies if i'm looking at only their alignment right but that's not the only statistics i may want to right suppose that you're looking at classification of of uh dna pairs so you're worrying so there's also sequence information

So you may want to know how accurate you are on the point, you know, on the given base pair, but you may want to know what percentage of times you were correct on both pairs.

So now you're keeping tab of another set of statistics, which is not the one here in equation 38, right?

Because now you're comparing two different points on the DNA sequence, right?

And so you could be keeping statistics about the observation 10 steps back or 10 steps forward, right?

So there's an infinite number of statistics that you could be doing.

So I've only shown one small set of them.

And for every one of them, you're going to be able to write this summation, right?

It's a part of, if you will, of the basis of probability theory that you have event space, right?

And that you can delineate event space completely, right?

So you have completeness, right?

There is no event that you do not know about.

if you and by the way there is no theory about all the theories of the world this is why you cannot do this with theoretical work right because we do have no theory that can enunciate all the postulates of all the possible theories about the world this can only be done with sample statistics right that's how i get away with this


SPEAKER_01:
yeah to kind of reflect that back i i first off i again i appreciate this presentation and work i i encourage people to to look into this because it's very interesting and to connect to the prior

century of statistics and understand where exactly this is positioned and and how deeply this statistical paradigm is baked into the safety discussion and to this last point that you're creating many possible finite spaces um the error patterns and covariances can be harder to fake than the mean you could just take one number and say oh my god danny oh my god


SPEAKER_03:
okay oh my god i mean i mean come on it just yes exactly right right you can fake the signal but you cannot fake the correlation it's so much harder right it's so much harder to do that yeah yes they're faking the mean and the variance and all the higher coordinates of motion then it's indistinguishable exactly exactly right yes so this applies to the concept of enemy inside the gates

how would you detect that something inside of you is hallucinating right now and you shouldn't be paying attention to it right this is would be a crucial thing for any right let's go back to the original question right forget about what i said if it's not how i said it is it has to be something like this how can an entity have intelligence have integrity of thought

if it doesn't have a way of detecting enemy inside the gates yes you have to defend against yourself cognitive security yes right this is this is what i think is so interesting about this right forget about right the number one enemy you have to worry about is you

your hallucination that is preventing you from seeing the tiger.

I would think that that would be the primary thing.

And going back to the semantics, I can see how this can be used for binary classification of safe, unsafe, right?

Tiger, not tiger.

But then, because the mathematics is exactly the same,

how the biological system could co-opt exactly the same computational structure to just check correctness of very complicated opinions not just binary classification how about Jakob um a last thought and then we'll just each have a closing word here


SPEAKER_02:
yeah i'm trying to connect these thoughts to um i guess the the active inference formalism where like where how many how many layers of uh of metacognition do you need to um to re to realize or uh elicit this type of self self-reflective behavior on the different systems that you're modeling


SPEAKER_03:
To all of them, right?

This would be used everywhere.

Like crazy, right?

You could use this over and over and over and over and over, right?

To check every single component.

Right?

It has no intelligence, so therefore it can be used right anywhere.

That's what I'm saying, that this is the thing that I find fascinating about it, because it has no intelligence and it's stripped the semantics.

You can insert the instrument anywhere to check statistics of correctness.

of your noisy oracles.


SPEAKER_01:
What that makes me think about is Chris Fields' recent course.

Andreas, not saying you watch this or anything like that, but what he spoke to was that there's a classical screen that can be digitized, a physical screen, and then there's two agents that have these semantic reference frames.

That was done in a Bayesian statistical setting.

And you have kind of added discrete beads...

some first approximation inside some of these otherwise distributional considerations like if the two thermometers can be from zero to ten that's how many pairwise observations there are you can have all kinds of distributions and flows and that type of modeling and continuous state spaces or

there may be a whole variety of activities available in the discrete cube, including these things like, well, 1 over and 7 up, and then that correlation, again, to make the point clear, it can be

basically detected and and triangulated where discrete differential is occurring in active inference we have the kind of continuous optimization machine learning but this is more like pinpoint like this bolt is failing rather than like there's a 0.8 that one of the bolts is loose no it's this this is failing


SPEAKER_03:
Yes, it's definitely trying to identify this is failing.

Don't pay attention to it.

Maybe take it out of the ensemble.


SPEAKER_01:
What are the next steps for your work?


SPEAKER_03:
So kind of going through the binary case has made me realize it was very hard to figure out the binary case.

I've been working on it since 2010.

But now that I've finally gone through the other end and I figured out how to make it simple, I'm starting to be able to write down what the solutions have to be for the three-label case, so the R equals three test.

And so what I've not gone into is that it goes back to that issue of these two numbers being the same, 34 and 35.

There's no more information in the binary case.

But in the three-label case, you would find that these three things are not the same, right?

So strangely enough, when you have more responses, there's more information because there's more chance of disagreement.

And disagreement is the most informative thing that you can have for an ensemble.

Agreement is useless.

Agreement is a tiny portion of these equations, right?

Look at these equations.

These equations, there are four of them, right?

Well, not this one, but in the eighth one, you can see it.

the maximally informative coin flip is 50 50. that's the statistical way to say this right but here right there are eight equations and only two of them are when they completely agree and this is what people like to talk about platonius and tom mitchell talk about the agreement equations

where they just wanted to talk about 41 plus 48 added.

And I'm like, why?

Why not look at the full set of information that's available when you consider 41, 42, 43, 40?

You know what I'm saying?

Each one of these is true.

It's much more complicated when you have correlation.

Then you have pairwise correlation, and then you have three-way correlation.

But my point is that these things are quite complicated.

But they have a very predictable structure.


SPEAKER_01:
This makes me think about it being a meta-observation.


SPEAKER_03:
Yes.


SPEAKER_01:
It's a second level.

It's an actual calculated... It's a monitor algorithm.

It's an actual fact.

As much as the thermometer's reading is a fact, the thermometer that measures the difference between them is also a fact.

That's right.


SPEAKER_03:
And then you have to decide, well, I know that I'm in this domain and, you know, this is temperature and temperature is not going to change very quickly because it's a car engine, right?

So it can't actually fluctuate.

You know what I'm saying?

That kind of thing.

So if you see something that's fluctuating too much, you know that it's wrong, right?

That's domain knowledge, right?

Being applied to then take a measurement, right?

And then interpreting that measurement as being a safety issue or not, right?

Again, the computer is needed, right?

The thermometer is providing the reading, but the computer is needed to understand why.

The other analogy I like to make is like a murder mystery.

A crime has been committed.

With this method, you can interview all the suspects and find via logical consistency the suspect, right?

Because they have a story that doesn't compare to anybody else.

But you cannot find why and how.

Because there's no semantics of why people commit murders or how they commit them.

So this helps you solve the crime.

Who committed the crime?

This algorithm is malfunctioning, but it cannot help you figure out why or how.

That you need a higher cognition.

Thank you again for joining.

Thank you for inviting me in this wonderful discussion.


SPEAKER_01:
Indeed, perhaps a 67.2 in a future time.

Yes.

Thank you.

Thank you, guys.


SPEAKER_04:
Thank you.
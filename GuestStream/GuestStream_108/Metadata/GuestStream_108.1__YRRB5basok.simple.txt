SPEAKER_00:
hello welcome it is may 22nd 2025 we are in active inference guest stream 108.1 discussing next generation artificial intelligence for emergent semantic communications with kristo k thomas so kristo thank you very much for joining looking forward to hearing about the work and also to everyone's comments and questions from the live chat thank you to you


SPEAKER_01:
Thanks, Daniel, for the introduction.

Yeah.

And thank you for inviting me for this talk.

So I'll be talking about Next Generation Artificial Intelligence for Emergent Semantic Communication.

And also I would like to express my gratitude to my collaborators, my postdoc advisor, Dr. Walid Saad and PhD student in our group, Omar Hashash.

um yeah so i would also like to advertise a bit on uh the upcoming research group i will be establishing as a tenure track assistant professor at worcester polytechnic institute in massachusetts uh so then my focus will be uh to work at the intersection of next generation artificial intelligence and wireless systems

with specific focus on resilient and interoperable semantic communication and integrated sensing and communication frameworks, explainable and generalizable AI for next generation wireless systems, foundation models for networking applications and developing strong mathematical foundations of cognitive and reasoning driven AI architectures.

so yeah so this is an outline of the talk um so first i will go over a motivation of next generation a native wireless networks um which is uh yeah so then further i will talk about two of my recent research work on semantic communication and further i will provide some future perspectives uh which involve emerging language design and uh using active inference

So, if you look at the wireless evolution, right now we are at the fifth generation of wireless systems, which promised apart from enhancing the cellular communication, aspects like massive machine type of communication or ultra-reliable low latency communication.

However, 5G could not meet all the promises it actually made.

um so yeah as like the you know saying goes usually the old generation old numbered generations are not very successful at their uh what they promise so if that is true so usually you know the even number of generations so next one will be the 6g so we can expect more exciting developments in uh sixth generation of wireless systems

So in 6G, so we are not, okay, as we move towards making our environments more smarter through making the network intelligent and be able to control the actions of distant autonomous agents in the environment.

In order to support those applications, 6G brings in a lot of different set of challenges like trustworthiness, security and resilience of underlying AI models, immersivity, sensing, AI nativity, and sustainable computing.

Here, AI is going to play a crucial role in the design, management, or maintenance of

different network and device functions so yeah so such a transition from the fifth generation to a native sixth generation um is expected as you move to sixth generation of technologies so here the question is why ai native because it can brings in the aspects of you know adaptability to dynamically varying uh wireless environment or tasks

It can handle nonlinear signals.

It can also bring in a lot of resource efficiency compared to classical systems.

So these are some of the advantages that AI nativity provides.

however if you look at the state of the art of a and et networks so currently the literature is predominantly uh data-driven a algorithms like you know uh so yeah meta-learning or auto-encoders cnn's and right now you know there is a huge high point using large language models for wireless network optimization however these data-driven machine learning algorithms

are plagued by the following disadvantages like large data size which is required which may not be always available readily in a wireless systems second is their limited generalizability to changing wireless environments or tasks so yeah

associated with the limited generation is the aspect that we need frequent retraining of these machine learning models which means we yeah if we have to build you know communication i mean real time communication and control such data driven machine learning algorithms are not suitable

So another aspect is they act like black box models.

So they lack interpretability and then they require huge computing power.

So unsustainable computing is another disadvantage of these machine learning algorithms.

So as we transition from the current AI native to next generation AI native, we must advance artificial intelligence together with the wireless network evolution.

So that's the focus of

a few of our research papers rigorous vision papers that we uh some i mean that got accepted in the last couple of years uh so one paper in one in one paper we actually discussed how how we can actually use next generation machine learning algorithms like causal reasoning for

um diverse um or cross-layer network optimization task and another paper that actually got accepted at the ieee proceedings of ieee we actually looked at a rigorous artificial general intelligence based vision which i will summarize as follows

So yeah, so next generation wireless systems are not just about communication So this those will also involve a lot of sensing so sensing act like an eye for the network so using the sensed information that contains the States of different autonomous agents or cyber physical systems in the environment The network should then act like a telecom brain.

So where telecom brain is characterized by distinct

capabilities such as perception which means the ability to generate abstract representations about different semantic concepts present in the data then using these abstractions to perform or compute or learn a world model that actually allows it to understand the cause and effect relations among different physical processes in the world so this world model will allow it to perform

or predictions about future networks or environment states and then use that predictions to basically perform action planning on the network so here action planning refers to not just network actions like you know resource allocation or beamforming but also designing optimal control actions

for the autonomous agents in the environment so as to guide or control them in the way as that the network intend it to perform.

So that's the idea of the telecom brain principle that we actually envision.

So along this direction, one aspect is the perception or the extracting semantics from the data.

So further in this talk, I'll be talking about how we can extracting semantics and then communicating the semantics, how it can actually enhance the resource efficiency of next generation wireless systems.

So that will be the focus of the next few slides.

So to introduce semantic communication, so if you look at the classical communication view,

uh so therein the objective was to you know ensure that the messages are recovered at the receiver side with the arbitrary low probability of error as possible so that is the technical problem so while doing so we have so far ignored the semantic aspect which means what meaning do these transmitted symbols convey and the effectiveness problem which means how effectively does the received meaning affect conduct in the desired way at the receiver side so

yeah as shannon has also mentioned these semantic aspects are irrelevant to the engineering problem however when we move towards future networks wherein we need um we have a lot of stringent communication requirements as well as a lot of other you know intelligent requirements like you know trustworthiness or immersivity as i discussed before we cannot continue to rely on just the technical problem so in order to make the communication resources more efficient

uh we must transition to a novel paradigm called semantic communication so in semantic communication the objective is to transmit the meaning behind the messages so here meaning actually means only the information that is relevant to the end user so communicate this information relevant information by building a semantic language between the transmitter and receiver we call the transmitter to be a teacher and apprentice receiver to be an apprentice

So by creating a machined language,

the objective is to ensure that receiver is able to regenerate the meaning uh with high fidelity as possible so here the question is so what is this semantic semantics actually contained in the data uh so yeah so if you look at this state of the art one traditional framework is this join source channel coding framework so so in traditional system we used to

separate the source encoding and the channel encoding part but in joint source channel coding framework we actually combine the source and channel encoding to one module and in deep joint source channel coding we use a machine learning in based encoder and decoder and then

train them in an end-to-end fashion with the goal of minimizing certain semantic loss function so by training such machine learning models we achieve we ensure that transmitter extract only the relevant information or relevant features from the data and communicate that information only so that leads to better compression in terms of what is being communicated

however our vision is that semantic communication is not just about you know compressing but better compressing the data so instead so if you can install machine reasoning at the you know transmitter and receiver side let's say receiver has better rig i mean uh you know prediction capabilities or

uh ability to you know generate uh missing information which can be achieved through you know instilling uh advanced machine learning framework uh so then we can achieve more minimalism in communication compared to just doing deep joint source channel coding

and then if we can instill these generalizable machine learning algorithms that actually ensure that these we don't need to retrain the system as tasks or the wireless environment changes so that is also very crucial for enabling real-time communication and control

and another goal is to ensure high fidelity at the receiver side so in my previous work have so far tackled these three challenges which i will talk about in detail and but there are also other open challenges like how to ensure interoperable multi-user semantic system in each of the users speak different semantic languages

or how to build a resilient semantic communication system so resilience against any network disruptions that could be due to time varying channel conditions or cyber attacks and then how to integrate this sc principles into the current 3gpp standards without introducing significant interoperability and standardization challenges so those are some open problems that i am currently looking at uh yeah

So let me first talk about my first work on semantics that got published in IEEE transactions on wireless communication.

So this is about building a reasoning foundation for emergent semantic communication.

So in particular I consider a

two two user semantic communication system so wherein the transmitter observes the world state and extracts the entities present in the data and then using the entities extracted it computes a semantic state description which i will talk about in later a detail later which is okay denoted by insert belonging to syntactic space and then the syntactic space i mean is that is encoded

uh to a representation space and then communicated at the receiver receiver extracts a semantic state using the extracted semantic state receiver performs certain reasoning which is denoted by phi of z hat and this recent output is used to perform certain actions on the world state so here there are two problems that i consider so one is the reasoning problem so which involves how to learn the semantics present in the data or how to define these semantics

and at the receiver after reconstructing the semantic state how the receiver can perform deductive reasoning and given this deductive reasoning how the transmitter can better encode this encode is another problem which is the emergent language problem so wherein

at the start of communication transmitter is not aware of how there is what are the semantics that receiver is interested in and the receiver is not aware of how the transmitter is in which language that the transmitter will be going to encode semantics so learning this encoder and decoder

those are which are formulated as you know probabilist probability distribution given as shown here pi of u given is third and pi of z given x comma c c here actually corresponds to the communication context so that will be the history of semantics that communicated in the previous time instance so that is the emergent language problem yeah so so so for the reasoning framework

We use causality from machine learning.

So where causality is founded on three different principles.

One is the causal discovery.

So given the entities present in the data, so here we call these entities as semantic content elements.

So only those entities that are really relevant for the end user task.

So those are the semantic content elements.

So we learn the cause and effect relations between these entities as a directed acyclic graph as is shown here.

So from causality literature, this can be called as the structural causal model.

and this structural causal models apart from this semantic content elements it could also involve certain exogenous random variables that are actually unknown or hidden from the hidden from us

yeah so that's a causal discovery phase so second phase is the causal representation learning so so when we are given a large dimensional observation maybe it could be larger dimensional antenna measurements so we it may not be immediately available i mean immediately known to us what are the causal cause and effect variables

so we need to first convert it to a lower dimensional latent representation involving just the causal variables so that is the causal representation learning part so do you so during this causal representation learning phase we can actually filter out the relevant and irrelevant variables from the

um features extracted from the data then finally we have the causal inference phase so wherein if we are given this causal graph we can perform intervention that actually mean we can manipulate the values of certain endogenous variables here then analyze what is the impact that cause on their effect variables

and then we can also perform counterfactuals that means so given we have observed certain set of entities as is shown here so what would the world state would be if these

know let's say s4 if you want if you want if you want to imagine a different outcome for s4 so what what interventions on these courses would have caused that so such imaginary worlds we can assume using the framework of counterfactuals so if we can actually extract this causal graph and then perform training

on this interventional and counterfactual distribution then we can actually attain a level of distribution generalization so that's the advantage of bringing in causality and another thing is the explainability aspect that so given this causal graph we can actually tell like you know if you have a causal graph among distinct environment variables

then resource allocation communication and computing resource allocation variables and quality of service values then we can actually assess if the performance falls below certain threshold so what are the underlying causes that actually lead led to that particular behavior so that that's where this explainability aspect is coming by using the framework of causality so now

uh okay so using the causal framework we learn the causal graph from the data so learning the causal graph we formulate it as a factorized posterior distribution where each factor will be the probability of each of these node variables given the its parent so probability of s1 given probability of so property s1 given s0 where s0 is the parent of s1

so that probability distribution we can use we we learn using a recently popular a method called generative flow networks wherein we start with an unconnected set of entities and iteratively learn each edge starting from this unconnected set of entities and then at the receiver side after receiving this causal state receiver actually combines these smaller states and then

Perform a logical reasoning where this logical reasoning is performed by evaluating certain logical formula Which is composed of this causal states that are connected by certain logical connective.

So it's like evaluating So, okay if this happened if the set of okay, these are the set of I mean if this is the causal state that the transmitter has observed and

okay then what would happen in the future okay yeah so those kind of reasoning or inferences can be um performed using this logical inference uh framework so for that we use neurosymbolic ai we cannot uh so we we use a existing neurosymbolic a method called logical neural network that actually converts this logical formula to a neural network uh yeah so that's how we actually perform the logical reasoning at the receiver side

So now I will discuss the emergent language problem.

So in the emergent language problem, the teacher's objective is to ensure that it communicates very less semantic information as feasible.

So, yeah, the transmitter is kind of, yeah, so that's the goal of transmitter.

So, which is formulated as negative of average semantic information at the transmitter side.

So, semantic information conveyed by the transmit semantic symbol U about the causal state is said given the policy, given the encoder policy as well as the history of causal states.

at the receiver's side receiver's objective is to maximize the reconstructed semantic information uh so after receiving you how much information that can provide about the causal state is that at t which is the reconstructed one given the policy of transmitter as well as the policy of the receiver and the history of causal states reconstructed with the then there are certain constraints like you know communication cost

plus uh minus log pi lt which is an apprentice surprise factor so which actually tells us how much the apprentice is surprised after receiving certain semantic information

so yeah so here given this contrasting objectives of teacher and apprentice we can formulate this as a two-player signaling game and the nash equilibrium of this two-player signaling game is uh can be formulated as is shown here so yeah like apprentice responding to the

uh given the you know teachers optimal encoder policies what's the best response that actually you know apprentice can uh provide that's that that's what this expression actually tells us so we actually source this through a since given so we actually formulated novel semantic information measures for the transmit side and receiver side but we observed that those are non-convex functions

so we sold them through an alternating optimization procedure where alternating optimization is performed for some surrogate functions that are convex so yeah so this actually evolves this game evolves through a two-way conversation between teacher and the apprentice wherein apprentice asks certain questions about the data set data and teacher responds by providing an encoded version of the information so that's how this game actually proceeds

so as i mentioned here we cannot continue to rely on the traditional mutual information measures since we need to quantify uh semantic information here so as i mentioned here how i formulate semantic information is that so semantic information is formulated as the information contained in the logical entailments

or logical conclusions that follow from any causal state for example if you look at any word like red there are multiple possibilities conclusion that can follow from it like you know red blooded or red ruby red meat and so on so those conclusions the information conclude into those conclusions that are relevant to the end user so that will be the semantic information so we actually analytically formulate this from a category theory perspective where we

uh what we did is we form we defined a syntax category consisting of causal states as the object and a semantics category consisting of the conclusion that follow from any causal state and copricious corresponds to from a category theory perspectives corresponds to the uh you know conclusions or how we yeah conclusions that follow from any causal state in the syntax category and this logical intelligence we can map it or we can consider it as a factor from the

syntax category to the semantic category so that's how we actually formulated this semantic information measure and we also discussed received semantic information which can be written as a fraction of fraction equal to semantic similarity of the transmitted information where semantic similarity can be computed by quantifying the overlap in terms of the capricious of transmitted semantic causal state and reconstructed causal state yeah so further

uh yeah so further we perform certain nash equilibrium analysis so wherein we first did some equilibrium analysis for some trivial cases like the cardinality of the representation space is one so that actually means we transmit same semantic symbol for all causal state so that is minimalistic but you know receiver may not be able to reconstruct the meaning with high fidelity

and we also consider the separating equilibrium where cardinality of u is same as cardinality of w so that leads to high fidelity of meaning reconstruction but it leads to maximal transmission so another thing is uh the partial pooling case where the cardinality of u is less than the cardinality of w so that leads to a minimal representation at the same time ensure a high fidelity

sufficiently high fidelity at the receiver stage so in this partial pooling case so some interesting insights we got so so we actually um showed that the the partitioning of the syntactic space occur in such a way that we communicate uk for all causal state that lead to the same semantics at the receiver side so this partitioning of the semantics the causal state

corresponds to a partitioning in the semantic space unlike a partitioning in the euclidean space for traditional communication systems so that's an interesting insight that we got from this natural equilibrium analysis so further i conclude this results by now discussing the summary of key analytical results as i mentioned we formulated novel semantic information using category theory and we also derived our upper low upper and lower bounds of average amount of bits communicated

in an es emergent semantic communication system and showed that these upper and lower bounds are less than that of a classical system showing minimalism and we also derived semantic error probability measures so that showed that the lower bound on semantic error probability is always less than or equal to the lower bound on the probability of bit error measure achieved using classical communication system so what that tells us is that so even if the probability of bitter is maybe high

still the receiver may be able to reconstruct the semantics with high fidelity as possible so that means we can uh it's okay to you know we can avoid these retransmissions as in classical system and we can maybe transmit at a lower rate so that means that we must transition to semantic performance measure if we are to make efficient use of communication resources so that's another insight that followed from these results

So further, I would like to discuss some simulation research.

So for the simulation, we actually looked at a visual question and answering based dataset where a teacher communicates certain images, encoded versions of the images, and then the apprentice's objective is to answer certain questions based on these images.

So we evaluated the performance of the emergent language

um for different tasks where we started with a simpler task where the image only contains maybe one object so images here contains objects of different shape color and size and we increase the complexity of this task so as time progresses what we saw is that the amount of communication rounds required for this emergent language training reduces close to zero

so that tells us that uh yeah the system is this semantic communication system is gaining more knowledge or receiver the apprentice is gaining more knowledge as time progresses uh so then you know because of this more knowledge it can actually avoid retraining for the emergent language so that yeah that way we can get rid of a lot of communication overhead

uh yeah so that's the generalizability aspect and we also evaluated the average amount of bits communicated over the wireless network wherein we showed that for our proposed system we almost gain around 100x gain compared to classic traditional semantic communication system that does not incorporate reasoning into their

uh architecture and compared to classical system yeah even much more gain is obtained so that actually shows shows the minimalism aspect that i was uh discussing so that actually uh concludes the yeah work on emergent semantic communication

and next i will briefly discuss um one follow work that we did where we utilize this causal semantic communication framework for a digital twin driven wireless network uh yeah so this got accepted at a general on selected areas in information theory

so here one application is as is shown here let's say we want to control the operations of certain autonomous robots in an industrial environment uh yeah using the communication from a base station or an edge server so here in uh we assume that okay given that this particular industrial environment will be also

a lot of sensors will be deployed in this environment so using the sensitive information the base station actually construct a digital twin of the industrial environment and uses this digital twin uh it actually you know using the digital twin it actually computes optimal control actions uh for these autonomous robots in the system so this is within the framework of semantic communication system and that's what i will be discussing here

so yeah so so here this semantic communication framework again we consider it for a two user so point-to-point system wherein the transmitter it's called an expert agent because it has access to lot of sensing information and using the sensed information it creates a world model about the wireless environment or autonomous agent environment

uh so basically extract the semantic content elements and you know generate the causal graph and using the causal graph it actually computes an expert policy control policy and address and this okay instead of commute computing the control policy the transmitter actually computes a causal semantic representation of the causal graph

And the receiver, receiver also has a world model at its end, but it creates this world model from the limited communicated information.

And then it also computes and receiver here is called an imitator because it tries to compute a policy.

And the objective is that this policy should be as close to the export policy at the transmitter as possible.

And this is achieved using the framework of imitation learning.

and that's why it's called an imitator agent so that's the yeah so this policy imitator policy should be such that it is quite close to the optimal export policy yeah so as to ensure that the semantic effectiveness of the task at the receiver side is as high as possible so that's the system model so compared to our previous work herein we

went one step beyond in terms of defining the info i mean semantic information so in traditional uh mutual information frameworks it does not distinguish between causal i mean uh directionality of information like but i of x comma y is basically same as i of y comma x so yeah so but in

in a causal system we must be able to distinguish between uh i of x comma y if x causes y uh then yeah i of x comma y should be different when you know uh when it should be different than i of y comma x so this is achieved through the principle of intrinsic information so given okay let's assume that s i t is the state of the system at time t and i c of s i t minus 1 given s i t

tells us about the course information about the course state which is st minus 1 provided by the current state st similarly we can define the effect information which is basically the information about the future effect states

So this can be defined using a distance measure between these two probability distribution, which is probability of SIT minus 1 given SIT and probability of SIT minus 1.

Similarly, for the effect information, and we choose Wasserstein distance here because of this unique properties that, you know, the distance is symmetric here.

So that's the reason why we actually resorted to Wasserstein distance here.

and further we can define the concept of integrated information so let's say that this ssit is composed of

distinct semantic content elements so okay so our objective is to basically perform abstractions of this semantic content elements by you know filtering out irrelevant states so we can how we define integrated information for a particular state sat is that integrated information is the information conveyed um conveyed by

uh the states beyond that of what is conveyed by the individual semantic content elements so that actually means that only if okay let's say the semantic content elements are all independent random variables then there there won't be any interactions between these different semantic content elements so then the integrated information will be zero so once there are some interactions between certain set of semantic content elements so that

that means that there are some cause and effect relations between these semantic content elements so yeah we so those set of states will form a semantic concept with non-zero integrated information so that's how we can actually uh you know define a semantic concept or extract a semantic or identify a semantic concept from the data so that's one uh you know yeah

step in terms of advancing the theory of semantic information that we actually proposed in this work.

So further, I would also like to quickly discuss the methodology that we used for.

So as I mentioned here, we have to transmitter has to learn the causal graph.

So which is achieved through causal discovery.

and also this causal graph should be represented semantically it should have a semantic representation uh which is actually uh represented as z t here and then yeah so then then the transmitter also has to compute a action control policy which is pi export of action given the state uh future state current state as well as uh the set of sensed variables

and then it also has to come up with this transition distribution which is uh represented as a causal graph so this okay given that we don't have a um yeah we don't we do not have uh i mean we do not have information of the true

posterior distribution of this causal state transition, we actually resolved to using a variational Bayesian optimization approach here, where we compute an approximate distribution by minimizing the KL divergence, which actually can be formulated as maximizing the evidence lower bound.

So, that is this GE expression here.

And that is how we actually formulated the causal graph computation problem.

And also at the transmit side, there is also the

problem of computing the semantic states that is achieved as so that is formulated as maximizing the integrated information conveyed by the causal state about the uh no yt which is the

corresponding to the inference variable at the receiver side subject to constraints on the bandwidth so how much information this is that t conveys about st should be less than or equal to some threshold so that actually tells about the bandwidth constraint so there are two different problems here so we solve these two problems by formulating this uh causal graph

uh as well as this yeah causal graph as well as the semantic representation using neural generate generative a based neural networks wherein specifically causal for causal graph used a graph neural network concept and for semantic state representation um for semantic state representation

uh we actually used variational autoencoder based uh framework to represent that and we actually sold this to two level optimization problem by a nested optimization uh using gradient descent so that's how we actually sold the parameters of the neural network i won't go into the details of uh yeah

details of the problem solution.

So, instead I would like to further state certain simulation results that we had.

So, first is what we showed through this generative A based framework is that or the welder model based framework as I discuss here is that we actually showed that compared to classical maximum likelihood based transition modeling, our proposed causal semantic communication system

uh can better represent the network state uh for all yeah for diverse snr uh ranges computer classical community classical systems and also we showed that we showed the generalizability by considering that okay so we started with some particular uh network or autonomous agent environment then after around let's say 6k samples we actually

um changed the environment dynamics then we actually saw that the classical systems will take

know the traditional system which are basically data driven a based algorithms like i mean transformer based encoder or decoder that takes large amount of time to actually require back to the normal state but our causality based system is able to adapt quickly to this uh network dynamics so that's the that actually proves the dynamic adaptability aspect uh yeah so that actually concludes my work on causal semantic communication for uh digital twin based

systems further i would like to quickly discuss some future perspectives so as i mentioned currently i've been considering only a point-to-point semantic communication system so my next goal is to actually concentrate multi-user semantic communication system

uh wearing okay so this set of uses are okay this could be robots or humans so they or a combination of them so they are actually encouraged in solving a particular uh maybe a show task or a

or distant as it can be so therein the objective is to design a joint communication and control system so herein we can consider that each of these user has access to only a partial observation so basically a case of partial observable marco marco decision process so that way we can actually formulate that

so using this partial observation and also using the communicated information from other uses each user should form a world model about the environment yeah then the quality of the using the world model that they construct they come up with a shared semantic language or shared symbol system

uh yeah so that actually will depend upon the quality of the welder model learned by each of the uses so here a promising approach is to actually use the perception and action planning framework design a perception and action planning framework through active inference concept so in this direction uh yeah so i'm actually looking at uh formulating

the semantic encoder or causal representation frameworks are as follows so let's say we have a semantic encoder at semantic encoder uh so that is defined as an approximate distribution which is q of u t where u is the semantic symbol and is the t k correspond to the causal states

pursued at different uses where k corresponds to user index and then causal representation at each user we can actually consider it as q of is a tk given is a t minus 1 ut minus 1 and here o otk corresponds to the partial observation at user k yt minus 1k corresponds to the inference decision

in the previous time slot uh at for for user k so here observation basically it's like composite two hospital one is like the partial observation and also like the inference decision that this particular age user will uh make yeah so so basically the causal representation or the the world the world model actually changes uh based on the inference decisions of each user

and Yeah, semantic symbol prayer is a prior is P of UT and then the logical reasoning and observation model.

We can actually formulate it like probability of yt plus 1 Given is 30 times probability of ot given by t is 30.

So observation depends on inference decision as well as the causal state and Then the actual transition model will be as is shown here Yeah, so here

yeah so actually motivated by the formulations in this recent paper um that appeared in rk we can actually formulate uh the collective uh free energy based uh yeah we can actually write the collective free energy for this particular um yeah but a particular multi-user semantic communication system um so yeah so wherein i'm not going to the details of this mathematical expression but we can

um write this free energy theme as consisting of three different factors so one is the reconstruction aspect so how much the user can actually reconstruct the observations based on the causal world model and then the second time is actually corresponding to the um

causal dynamics approximation which means how much the user use each user can approximate the causal um veiled model based on the observations partial observation at its end and the final factor will be corresponding to the uh a time that actually regularizes the entropy of semantic encoder which actually means that so this this stem would be actually corresponding to the

um to minimizing certain communication cost so or basically uh ensuring that we communicate minimal semantic semantics from the data

Yeah, so we can actually formulate two phase like an active inference.

We can actually formulate the free energy expressions for the perception phase and also for the inference phase, which is corresponding to where y corresponds to the inference variable.

And yeah, so then the objective is basically using the free energy for the perception part.

We can actually compute the minimalistic semantic encoder.

or the common semantic symbol system, as well as using the inference free energy framework, we can actually compute the best inference decision that each user should make using its own causal state perception, as well as the communicated information.

So that's the framework that I think can be applied for this multi-user semantic communication systems.

uh yeah so that will yeah with that i will conclude my talk so as i uh mentioned um 5g could not meet all the promises um however as we move towards next generation system wherein uh our yeah so the communication networks are not just about building better um um

wireless i mean better standard i mean better systems which high data rate or low latency and so on but also incorporating aspects like sensing or trustworthiness or and also autonomous control of distant agents or cyber physical system so we should transition to a telecom brain based vision which is consist of consisting of modules like sensing perception world model and action planning

And I discuss in detail a few of our work on semantic communication where we actually showed that using causality and neurosymbolic AI, we can actually build minimalistic, generalizable, and high-fidelity semantic system.

So they're saving a lot of the bandwidth.

And then, yeah, the future research vision, my future research vision is centered around building sustainable, trustworthy, and socially responsible NG AI native wireless systems.

using the Stelecom principles.

So with that, I will conclude my talk.

And if you have any questions, I am glad to answer those.


SPEAKER_00:
Thank you for the awesome presentation.

OK, I will prepare my questions and also welcome anyone in the chat to write and just while I crop and prepare that, just could you share a little bit?

How did you come to be setting up the lab?

Like just what was your trajectory from through school on through here?

How did you get into this research area?


SPEAKER_01:
Yeah, so I did my PhD in

know 50 generation wireless communications with focus on the physical layer aspects like you know building uh developing uh beamforming solutions or channel estimation schemes using conventional signal processing algorithms like no specifically even approximate biation inference kind of techniques um uh yeah so those are those are my main focus during my phd but when i transition to

a postdoctoral position i choose a different path like i mean i i yeah i saw that you know uh the physical layer advancements is kind of you know the physical literature is kind of saturating out so we need some new revolution uh in communication system as we move towards next generation systems that that can yeah that can be um

feasible by the incorporation of ai but not just you know data driven ai but um a that can actually reason like human beings so that's so that is the motivation where i started to work with uh professor valid sad on um this particular topic um yeah so that's how i actually entered into you know semantic communications uh which is actually a revolutionary approach that uh

that is aimed at building and improving the resource efficiency of future communication systems.

So that's how I ended up at this particular research.


SPEAKER_00:
thank you well i'm extremely um appreciative to your colleagues and you for bringing this all together and i'm excited to see where this all heads um because it's aligned with many of my interests and i also believe it's it's a critical infrastructure kind of civilizational technology layer so

First, I tried to summarize in one sentence because there are so many pieces and so many formalisms that might be new.

So my one sentence summary was dynamics and constraints of semantic transfer in hybrid and synthetic network intelligence systems.

and then to kind of expand on that a little bit so i'll just make a remark about where i saw active inference coming into play and and some of the important areas so um you starting with a dyadic model uh templated dyadic communication models like teacher apprentice or adversarial models

you are looking towards using active inference to allow real-time cooperation and turn taking in multi-agent systems and that would allow the embodiments which is achieved in different engineering solutions in different ways however the top-down approach is what is the analytical formalism that gives a unity from a um

modeling perspective to all of these different engineering approaches and those formalisms that you presented on allow the embodiment of agents or situation class instance and context specific cognitive mechanisms and dynamics which broadly correspond to perception on the inbound and action selection and in and on the outbound with reasoning happening in between

and so what that is heading towards is a first principles approach that applies to semantic dynamics which could be used within a single threaded shared database state multi-agent system so it could be a system with a very specific logistical order or known computational resource constraints

all the way on through nested arbitrary systems and then importantly embodied cyber physical networks where there can be the explicit introduction of physical and energy models bandwidth and resource costs and so that brings a complementary view from the engineering side

on informational and cognitive constraints with important physical constraints of the ever-changing basis of networks

So just kind of wanted to echo back because there were so many important pieces in the talk and it was a lot of information and it's a part of our tech stack that just kind of works.

Like the phone just has service and it's really abstracted.

And then of course we know that the phones are coming out with different versions and I'm not knowledgeable in these areas that much, but it's like services different in different regions and things change and technology happens like that.

so giving that kind of view into the tech stack is also i think really critical so usually i do not give a sort of extended comment but there's just i was writing down so many notes so do you have any thoughts or ideas on on that yeah i think you have uh described i mean the summary of this talk and yeah quite well uh i mean that's a great summary um so yeah so one point just to


SPEAKER_01:
conclude is that like you know uh so right now with with 5g we are able to get you know large download speeds or um lower latency you know streaming and so on uh but you know the but when we actually move to next generation system it's not just about cellular communication so the network should also

know we should be able to build much more smarter environments like you know smarter transportation systems or smarter uh you know industries uh telehealth systems so that actually requires the network to actually um have a perception about the physical world i mean if the network has to you know uh control the operations of this

uh distant autonomous agents uh um so yeah so that network should have a uh accurate perception about the physical world um unlike in classical system where uh you know we just treat the communication systems just like a passive passive system so whatever message comes in you just objective is just to communicate that messages with you know high accuracy at the receiving end

So that's a transitional paradigm that we are looking at as we move towards 6G and beyond.


SPEAKER_00:
Another area I wanted to remark on and part of that transformation is

predictive processing and the role of prediction in compression so the predictive processing frame differencing is used in standard syntax information driven compressive systems like frame differencing in the video if all you have to do is transmit what changes between frames it gives a

within a given local minima of minimal compression and that was discovered in the computer science area and then actually entered into this broader discussion about like visual and sensory systems so that's using frame differencing to for example transmit a 4k video with less syntactic information flow

and then actually it goes even further with all of your work exploring the transfer in a latent or semantic space which could be more like a primary lexical semantics or conceptual modularity all the way up on through possibly complex pragmatics like action directives

or complex information being shared at a higher semantic level and then that's kind of again where active inference comes into play is having a unified framework that on one hand can give an account for why frame differencing at a purely sensory kind of shannon information level is a compressive strategy

and then use on a latent space in exactly analogous formalism a co-operative mechanism to enable semantic compression so like if if we both have a friend abc but we both know them as a then there's the question of the

bits per second of the audio transfer like how good is my microphone and how much data needs to be sent but maybe a given bandwidth you only need to say a because b and c are semantically known and so that just kind of shows like it's not just about the amount of data transmitted compressing it all other things being equal is is good why send extra data through the wire

but now and this is kind of where i want to hear your thoughts on this we see a lot of work with natural language intermediaries and that is used to transfer information broadly with this kind of natural language interface um but also we could imagine situations where semantic information transfer happens on networks and i think this is exactly what you were getting at so

What kinds of information can be transferred?

Like does transmitting vector embeddings have a markedly different semantic impact than, for example, dialogue between LLMs?

the kinds of bandwidth that are currently being used in natural language mediated transmission could strategies like multimodal or semantic compression give some kind of order of magnitude modifications in the bandwidth or the impact of communication amongst latent space uh yeah so


SPEAKER_01:
yeah so one way to look at this like okay so let's say if you have like you know two llm agents communicating um and let's say okay so they are using this natural language to communicate among them uh so yeah so uh yeah so so i did let's let's say okay so in in semantic communication um so we are not i mean so so we are not really concerned about uh you know how the

how lm agent 1 communicates in a you know grammatically correct um english sentence uh you know with all the details included to llm agent 2 rather we will be more more uh interest here we are actually our frameworks actually lead to okay following this um emergent language formulation um so as time progresses what these llm agents would

uh lane is that okay let's say they are still communicating using english uh letters um so they end up learning a language um okay communicating a sequence of words or letters uh that is sufficient enough for the other agent to you know understand it i mean it need not be you know it can be considered like a machine machine language which need not be

perfect like an english language but no but certain sequence of letters that is enough for llm agent 2 to actually understand llm agent 1 given the background knowledge that each agent has

um like you know llm agent one okay so when when we when two humans actually start communicating okay when when two new uh um yeah so when uh one when i start communicating to another person whom i don't know before so initially there is a lot of overhead like you know we have to introduce ourselves we have to share some background knowledge about ourselves

So then further as they become friends, so then when they communicate, so they have a lot of implicit assumptions about the other person, how that person would be thinking or how that person would be viewing the world around him.

So the communication would actually be more direct.

um you know more optimized for optimized considering also that background right so similar to that so yeah so if two llms agent two llm agents communicate among them so so using an emergent language framework our objective is to build a machine language that is sufficient enough enough for

LLM agent to pursue or understand what LLM agent one actually means.

So that kind of a machine language construction leads to a better usage of communication resources.

So does that make sense to you?


SPEAKER_00:
Yes, like where that kind of takes me is

When we're talking about emergent semantics and pragmatics, which could be between family members, it could be between strangers, humans, it could be online or in person, it could be two LLMs, it could be a person and an LLM, like every kind of combo.

having this approach gives a way to scaffold the informational dynamics and kind of separate out the syntactic layer from the semantic layer and then it naturally leads to these kinds of sustained duration

semantic learning strategies like if every single message was coming from a different source then there would be some sort of like no free lunch but you could take the best semantics as possible to each fresh source like if it was just a different every single time but then to the extent that you can have a um prior

about a given type of interaction all the way on through the most specific prior which is exact relational with a given other entity then that allows semantic communication strategies like one means this dinner and two means that dinner

well then they only have to send one letter.

And then when we get into these situations where it can be machine-machine semantics, then that's kind of what tokens are.

Like that's why words in a given language get truncated up into these essentially probabilistically determined semantic units.

Like, oh, it's this verb and then ed to be the past tense.

So it's like, or whatever the tokenization strategy happens to be,

and then how do you use your tokens to have this no regret or minimal regret best possible token conveyance linearity to finesse

the translation of elements of a semantic space, which could be static or dynamic, into a linearized sequence of messages.

And then the network perspective is really important because network properties can matter.

Like if you have a network where 100% of packets are delivered, you're going to have certain kinds of strategic constraints in communication.

Whereas if you have a network where a given percent are being at this, you know, redirected or this kind of etc.

is happening, like those physical constraints do matter for a communication strategy.

And so having a sort of integrated analytic approach to modeling explicit syntax constraints, classical Shannon syntax, entropy driven computer science and networking on through abstract cognitive informational constraints, like we're just going to assume that it could be the maximally informative symbol, like the letter distribution in natural language could be made more informative.

So using some rebalanced, even A through Z, there could be increased information transfer, but that would only be in certain situations, like a machine-machine situation might be able to do that kind of hacking into the actual symbols being set in plain text.

But that just, again, reflects on how there's both the particular network and symbol set constraints that are really, really well laid out from a Shannon syntax approach.

And then there's this outer space, which we're still just beginning to explore that is grounded in the primary symbol entropy and the observations.

However, it can have multiple layers of cognitive and metacognitive dynamics, which are not so closely bound to the primary Shannon constraints, but still do have important constraints of their own

However, that's what semantic information is about because semantics can be about semantics.

I think that was one of the questions on one of the slides, which is like cognition on cognition or metacognition.


SPEAKER_01:
Yeah, so it's like about this communication constraints that we have, we are effectively forming a semantic layer between the transmitter and receiver.

which is actually you know constrained by all this cognitivity um yeah and then the understanding aspects right yeah that's very insightful discussion thank you yeah well just in sort of closing what are your next steps or what are like the timeline with the lab or how would people learn more or get involved

um yeah so um yeah i'll be starting my uh new position from august 2025 and yeah i'm looking to recruit one to two phd students um so yeah and also now continue this work with the students on this particular topic and yeah i mean i do have a lab website which is still kind of under construction but you know

I can share this.

I mean, I will share all my updates through this website.

Yeah.


SPEAKER_00:
Awesome.


SPEAKER_01:
Yeah.

Happy to discuss further or even give a talk later when I have more resources.


SPEAKER_00:
Cool.

Well, again, very appreciative of the work and the concise presentation.

Wish you the best.


SPEAKER_01:
Thank you very much.

Thank you for the invitation.
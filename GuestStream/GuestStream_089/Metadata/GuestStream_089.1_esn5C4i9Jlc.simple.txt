SPEAKER_02:
Hello and welcome.

It's September 30th, 2024.

We're in Active Inference Gas Stream 89.1, Generic Framework for a Coherent Integration of Experience and Exposure Rating and Reinsurance with Stefan Berneger and also joined by Roland Berge.

So thank you both for joining.

Stefan, to you for a presentation and we'll look forward to that and the discussion.


SPEAKER_03:
Okay.

Thank you, Daniel.

welcome everyone um let me quickly give an overview of what i'm going to present it's about reinsurance and reinsurance maybe some of the people on the call are familiar with what the insurance is if you are not no problem i'm going to explain everything you need to know

basic what reinsurers do they provide coverage to insurance companies and when they provide the contract to cover the risks they of course need somehow to rate this risk and there's two different approaches which are traditionally used one is experience rating the other exposure rating and what I'm to present a new framework which is integrating the two into one

integrated framework basically and just to what you can see what is highlighted there are three components to it the first component is a generative model the second is i mean the generative model he will create artificial or simulated claims

and what we need to do before we can do an analysis we need somehow to reduce this high dimensional complex objects into some smaller number of what i call reduced random variables and then we use invasion inference to basically calibrate the model this is basically what is supposed to do so and what you see on the bottom i made a reference to some people in the history so the traditional

approach where you use a experience rating you're taking the claims from the past you somehow project this into the future and use this data as a model for the future claims activity so that's why i put the kepler who basically took the data from tycho brahe and then he derived his laws for the motion of the planets and then a contrast is newton which

basic and derive these laws by some much more fundamental principles.

So basically in one case you model the data and the other case you model the world.

So this is the difference between the two.

Of course, in biology would have the Aristotle on one hand side and then

Darwin on the other side.

And from the statistics, what you see on the left side, this is the father of the classical statistics of Fisher.

And on the other side, they put the base.

So this is where the frame where we are operating.

Now to my talk, I will first give some background information and explain why this is relevant.

Then I will somehow make a link to active inference.

Then also point out to some of the challenges which we are facing in this attempt.

and then i'll provide with a roadmap for a solution and then i will show some examples and just for your information i will not show any formulas here so whenever you want to look into the formulas we can go back to the paper you find all the technical details so i'll just describe um with the help of numbers and some charts or some graphics what i'm going to or the basic ideas are so just if you expect any formulas you will be disappointed

okay so first um what i've done i've taken some real numbers from a reinsurance company and this is swissery and from swissery i've taken not the whole business but just one segment which is the pnc reinsurance which is uh property casualty and specialty lines

um two other segments which history has is a corporate solution so this is then direct insurance provided to large corporations and there is also life and health so what i'm showing here is only pnc reinsurance so range contracts with insurance companies and switzerland has developed about 20 or not 25 years ago an internal framework for

economic valuation of the business and it has started in 2013 to publish the numbers so evm results which is an economic valuation of the business in contrast to the traditional financial numbers which have been published and what is relevant for us here i'm not going to into details of evm but what we have is

so-called present values of future cash flows this is the basis for it and then it's the analysis done by underwriting year and not by financial year so these are the two main of the most important elements of evm and what we have is let's say the gross premiums and then commissions and the commissions are the

amount of the premiums which goes to the intermediaries so brokers and or goes back to the insurance company in the form of a contribution to the claim or to the internal expenses so what matters is what they've shown here as net premiums so this is it has been increasing for about from 13 billion in 2013 to roughly 18 billion in 2023 and on the right you see the mean and then i use the

the mean yeah over the of the net premiums as a base so this is 100 and then you can see what happens to these premiums so the more the largest part of it is used to pay claims so this is about 85 then you have about 10 is the internal the expenses of the reinsurer of swiss three then we have some other elements like taxes taxes depend on the on the profit so it's linked to the bottom line then we have

capital cost.

Capital cost is linked to the risk, but these are relatively small numbers.

And then the finance people have introduced some small details which they put on the other.

So there is a lot of work going into the small elements.

But what really matters for the success of the company is what we have here in claims and benefits and how much premium the company earns to cover these claims.

and just as a what we see here is the numbers published at time one so one year after inception so the 2013 numbers are published in 2014 based on the information which is available at that point in time but claims they have a relatively long um

they can take quite some time until they are known and paid and reported.

So there is only little information available after one year.

So there is some development taking place later and all the developments of prior years is shown at the very bottom.

And we can now see two important things.

One is if you go here to the EVM reals, you see kind of a cycle.

So there is some positive results, then it becomes very negative for a couple of years, and then it becomes positive again.

So this is one thing.

This is the nature of the business.

And the cycle has two components.

One, the premium rates go up and down.

And second, also the terms and conditions of the policies, they might be tighter or they might be released.

So there's also the claims might change during the cycle.

But what we see here is the bottom line result.

And then there is another element here, the EVM profit, the EVM profit of prior years.

and here we can see some quite negative numbers and then here again this is something which you want to avoid this is the big surprise which at the time of underwriting you put aside a certain amount of the premium for the reserves for the claims and then it turns out that this is not sufficient and then you have to

find some means or take capital to increase the reserves in later years.

And this is basically what reinsurers want to avoid.

So of course, what is not shown here is the initial assessment of the underwriters at the time when they look at the business.

So they also have an estimate for the claims.

This would be the claims at time zero.

These are the initial reserves.

What we see here is the claims at time one.

and what is important to know is of course the estimate is quite on a contract or a transaction level is quite high the uncertainty but the problem is that underwriting functions they tend to underestimate the claims during the soft market

they're too optimistic and then the adverse developments and they overestimate the claims cost during the hard market so there may be too conservative and they might miss some opportunities

and if you just look at this picture here then you can say there is a potential for improvement in this case of swissery i mean this is my from the outside view maybe the entire people will say something different but just looking at these numbers i would say that adverse development so the numbers which we have here which in the order of one to two billion this could be improved

potentially by about 10% of the premiums.

So about, let's say, one or two billion per year.

So there is quite some potential to reduce this uncertainty.

that's one motivation to do this and the second motivation is of course by better risk selection you could also increase the bottom line results so this 470 million the average but let's say one which is three percent maybe for one or two percentage points of the premium volume so these are is the motivation so we talk about quite substantial numbers in the case of

the profit maybe 100 or 200 million per year which additional profits could be done but better six collection and we come to adverse development which is basically the unwanted uh development in future years maybe one or two billion this is my claim people can prove me wrong but this is what i think is possible

now sorry now if you look at this slide there is three uh charts with the temporal development and what we have done here i've taken the numbers by uh sub line of business so this is also published by swiss reese i'm not inventing anything and let's first look on the left hand side what we see here is different lines of business so the red one here will be um

liability on the top we have here property and we see they develop quite differently now what is this development which i'm showing here this chart shows which percentage of the claims is reported after one year and we see in casualty it's only about 10 in properties maybe 50 percent

and then it takes maybe in casualty 10 years maybe or even longer until all the claims have been reported so this is just a reporting pattern and which is lined up business dependent then report it means that there is a an initial the insurance company they have notified the reinsurer there is this in this claim and they have put up some case reserves and reserves for it

But then it can take another couple of years until the claims are paid.

And this payment pattern is shown here on the right hand side.

So you see again here, in the case of liability, it's very slow.

So the payments can take 10, 20, 30 years.

In the case of short tail or property, this is normally settled within a few years.

And then what I have shown in the middle, this is the key topic of today.

so one point is the initial estimate of the claim so this is what underwriting is doing and the question is what is happening with this estimate down the road and this is shown here in the middle and here in the for the red which is a liability again you can see here at time one it's only between depending on 70 80 percent of the latest numbers so there is

In time one, the reserves are maybe here 80 or 70%, and then these reserves have to be gradually increased until they reach a stable level.

In the case of property or X and Ls, at time one, we are higher, and then it decreases.

So here we have kind of a favorable development.

So reserves can be released, and here we have an adverse development, which means you have to constantly increase the reserves in order to be able to pay all the claims.

And if you go back here, we can see in the first, after one year, it's only 10% of the claims have been reported.

So whatever reserves we have here is the estimate of the underwriters.

There is nothing else, no other information.

So here we can be very confident that the initial estimate of the underwriter was more or less on the same level because there is no additional information coming in.

So they started 20 to 30 percentage points below the ultimate.

which is quite substantial in the case of short tail lines it's not so clear because you see here about 50 of the claims have already been reported so it could be that the claims manager they're quite conservative at the beginning they reserve more than what is necessary and then it comes down here we don't know maybe we started higher it came down maybe started below and it came up in the first year

it's not so clear but overall we can say some business starts too high whichever and then it has a favorable development some starts just at the right level and it's all flat or we have an adequate estimate at the beginning and in other cases it's adverse and this is basically what we want to avoid now there is another complication

And this is that the market is not stable.

The market goes through cycles, as we've seen at the beginning with the numbers of Swiss Re.

And what I've tried to show here is if you take each and every contract, you can put a tag on each one and say it either contributes to the economic profit or it's just adequate to cover the expenses and all the costs.

or maybe some business you even if it is negative but you want to keep it because you want to keep the client relationship so this is then and the problem is nobody knows what the true color is so basically the underwriters they take each contract and they do an assessment and they say one contract is green one is amber one is red but it could be false positives or false negatives this is not known

of course if a contract is red and you say it's green then you write as much as possible of it and then you end up having with adverse and adverse development and of course you also then you have the winner's car so you're much much more likely to get this contract than if having one which is positive and everyone else also sees it positive so this is kind of the

um broader context in which the underwriting or the market is operating and if you say the underwriters are biased then you can say they um are so there is a true let's say that the true border between the three buckets but the underwriters they believe that the borders may be up here um in the case of a hard market

So they are too conservative.

They say some business which actually is already amber, they put it into the red bucket and some business which is green, they would put it into the amber bucket and vice versa during the soft market.

So this is the problematic.

And by improving the underwriting,

And by avoiding the bias and reducing the uncertainty, you can get this dashed line here closer to the true border and here as well.

So that's the aim of the whole thing.

So this is where I'm making a claim that there is quite some potential for improvement for a reinsurance company.

Now, there is, of course, also different approaches to reinsurance.

So if you're a big company like Swiss Re, then you have to do these assessments

in other cases if you are a small you say i don't do this myself i save this cost and i just follow the leaders yeah then you just basically can do it for free and you try to write the same business ever as the large ones and then you have the same at the lower cost ratio this is also a possible approach or you can say i don't care over the cycle the market will correct itself and i just write and that i'm sure that i can survive the soft market

and then I will earn during the hard market.

That's also possible approach.

So we talk here about a company which does a fundamental assessment, and that's why it's necessary to go through this analysis.

Okay, now just a few thoughts about how to link this to active inference.

And what I've first been thinking about the corporation, so the corporate agents, they act very similar to humans,

um i mean they're also competing for resources they have an incomplete knowledge of the external world they also use the internal representation of the external world with the help of models and so on then we have multiple departments which in our corporation and they use of course they have a different picture of the outside world and they also have different

interest, so they are not always pursuing the same goals.

So this is kind of, maybe we feel the same as humans, we are also not always sure.

One thought wants to go in one direction, then the other direction.

So I think this is very similar to corporations.

and of course corporations at the end they have people there at least this is still the case so we have also biological brains driving them and i think active inference is a very good tool to also understand the behavior of corporate agents in many different levels but what i'm doing here i'm just focusing on underwriting or in reinsurance and um

what I'm going to show how generative models can be used in this process.

And maybe just if you want to look at a broader role of active inferences or in economics, then there is one link which also introduced the term of cognitive economics.

So this is where it's in a broader picture where it would fit.

okay now what i'm showing here is what you need to understand about how reinsurance works so we have the external world where we have objects at risk this can be buildings this can be people this can be airplanes this can be corporations which maybe could default and so on so these are the objects at risk then we have the

range of the insurance company which i call here the sedent which issues policies to cover some risk for these objects then there is the external world where you have the generative process which creates all kind of events and some of these events like an earthquake or windstorm or an accident or a defaulting company results in insured claims now

The reinsurer doesn't get all this information which is available on the side of the seeding company.

What the seeding does, it creates a submission and the submissions, you can put it into three buckets.

In one bucket, we have the claims which are reported to the reinsurers.

Normally, this is claims above a certain threshold.

the second bucket contains the contract terms so of course you have the what were the reinsurance conditions in the past but maybe also some insurance conditions so insured retentions or some limits and the like so what is which act like a filter on the claims and the third bucket is the exposure profile which is basically

some statistics about sometimes also details about the insured qualities and the objects at risk so in the case of earthquake you maybe want to know is my the building which i'm covering um in

California on top of fault or is it somewhere in the center of Africa where there is no earthquake risk?

So there it's important to have also some additional information.

But what exactly we find in the exposure profile depends very much on the line of business.

the games of claims this is much more standardized basically what you have is the amount of the claim and the payments so you can put it into cash flows and some outstanding claims so this is some it's much easier to standardize the claims within the exposure you need specific information depending on the line of business then what reinsurers then have there as i mentioned already before you have two models one is experience rating

The other is exposure rating.

Experience rate takes the claims and the contract terms.

The exposure rating takes the exposure and the contract terms.

then there is a quite heavy involvement of the underwriter or the actuary which is depending who does it and in both models you then get some results with some statistics and since these two results can be quite different there is some blending process thank you place and then you end up with the projective statistics for the cover year then you take into consideration other

things like you have of course underwriting consideration client management which has some saying and risk management and finance so there is now this information is just one element in the decision process and at the end of the day you end up with a reinsurance contract for the future so this information is about the past and the present you and you use it to

project some statistics and and the expectations for the future this is basically where you have the expected loss cost which i mentioned before which you want to be on by be unbiased and as minimize the uncertainty so this is where the circle is closing you could also represent it with a

Markov blanket, you have some things happening inside of the blanket, the information coming in and some action going out.

So this is where active inference is basically visualized.

Okay, now the

step to the integrated approach and is so basically we are looking now into these two parts so these two models the experience rating and exposure made model this is the focus area and what we do here we have two things first we have a generative model now this generative model

is very similar to the exposure rating model but the difference is the exposure rating is only applied to the future to the cover year while the generative model is basically taking the same sorry model and you apply it to the past so you don't only apply it for the cover year this is then comes later but you first do a calibration of this model

And this calibration is done with the reported claims.

So you use the claims not to project them into the future, but you take the model and project the model to the past.

And then you compare the outcome of your model with the actual claims.

And then you calibrate the model.

And once it's calibrated, you can take this model project into the future and produce all the statistics you need for the future cover year.

So this is the basic

secret of this presentation.

So, as I mentioned before, it's a generative model.

You have reduced claim statistics and you have Bayesian inference, which you apply to calibrate the model.

Maybe you have to go through multiple iterations, but this is basically where the matching is happening.

And what I have done in my paper, I've basically looked at this part here, because here the generative model

you need to come up with a very specific model for each business segment.

So for each line of business, you have different data available, you have different models which you are using.

So this is where really the expertise of the reinsurer comes into play.

So they will have to provide these exposure models

generative models for each segment but once this is done then you can basically standardize this part here the model has to produce some claims in a standard format you compare them with the reported claims and you then

come up with some calibration parameters, which you feed back into your model.

And once this is calibrated, you can then apply it to the future and do everything you need to do on this right-hand side.

So that's basically the magic here.

And we will see it's much, again, you need much less

input from the underwriter because here in the past when you need to when you do experience right you have to project these claims to the future so there's a lot of assumptions going into this process because then if you go to the past the portfolio was different maybe the claims that in the past are no longer representative so there's many many assumptions which come from the underwriter and this can be avoided in this approach here

Okay, now, so far, it sounds relatively easy.

But of course, if it was easy, maybe it wouldn't have been done.

So there are some challenges which you need to resolve.

And to understand this, what I've done here, I'm just taking a few simulated claims.

And what we have here is the time axis.

In the vertical, we have the incurred amount, so the estimate of what the ultimate is going to be.

And in the Y axis, I have the paid amount.

And let's assume here at the end it's settled.

So you can see the estimate.

So first it goes up and then it comes down and then there is some adjustments.

At the end it goes up again.

So you can see this in this curve here.

The paid, first there is nothing paid and then it goes up and then eventually we have the final payment.

So this is one example.

But what can also happen is that at the beginning, you know nothing about the claim.

So the claim is reported with some delay.

So the accident date is here in 2016, but the reporting date is 2020.

So in this time, you have to put up so-called IBNRs, incurred but not reported.

reserves for all the claims which will be reported later you have also seen this at the beginning the case of history the reporting pattern was the chart on the left so this is the the nature of the business now if i take one claim so the whole graph here is one claim and

I can either present it with a graph in a three-dimensional space as I did here, or you can take each payment and each incurred adjustment and say that's just one element and then you put it into a high dimensional space.

So you can say that each claim is an element in a high dimensional sample space and the dimension here is maybe something between

uh 10 in the best case or connect will be up to 100 so it's very high and it's just a high dimensional space where it's very very thinly populated imagine you get about maybe 100 or in a or 1000 claims per submission and in a um if you have a claim with 100 or 50 or even 20 dimensions you're completely lost there's nothing you can do

So what is done here, and this is maybe the only formal symbols I'm using, here I say the sample space is high dimensional and what we do, we decompose it in m one-dimensional spaces.

That's basically the whole idea behind.

So we need to find some variables which are more or less independent

and we project these claims to these subspaces, and then we can solve the problem in a one-dimensional space a couple of times.

But it's then with a one-dimensional problem, which is much easier, of course, because then we have 100 to 1,000 claims in a one-dimensional space, and this is easy to handle.

So this is the second part, so the reduction, and I will show you how this is done, but just to understand that this step is necessary, else there's nothing you can do.

Now, if I go here, how can we solve this?

So this is again, another representation of the standard approach.

So what you have down here, you have claims which are, in this case, 10 years old.

So I use minus for relative to the underwriting year.

So it's underwriting year minus 10.

Here, these claims have been reporting pattern of 10 years.

So you know the first 10 years of the development.

But for the most recent year, so which is minus one, you have only one year of development.

so this is now if you try to do traditional experience rating there is two things you need to do you need to project all the past to the future so this is one step yes you need to trend the claims and say on an as if basis a claim which happened in the year minus 10 what would it be if it would happen in the year plus one which is basically my cover period

then what you see here is you get this kind of triangle so some claims you have 10 years of development but the most recent one they have only one now the problem is that these recent claims are maybe representative for the portfolio but they have very little development the old claims you have a lot of development but they are no longer representative and now what you need to do is you need to take this triangle and project it to the

uh to the end and see what is the end of this thing so there's a lot of as i said a lot of underwriting assumptions going into these two these fundamental steps and this is the experience-based approach in the case of exposure rating when you take the information which is available at normally the minus one so the market experience these are the market models you have the exposure profiles

insurance conditions you project everything to the cover period then you apply your model and you get an exposure based result and then you take the two and blend them and that's then the result um i can take the same now and represent what is it in the this gene integrated approach so again the three steps so first i now use my exposure model

not only for the future this is then the last step this is what i will have to do but before i do this i'm calibrating my model so i apply the model to the past so i take all the market experience which i had in the past the profiles from the past the the models which cannot change over time and the insurance conditions so i have a one model for each year

some of the information might be the same but basically i can now run this model for the year minus 10 and i can create not only one but 100 or 1000 or as many as i want simulated sets with claims and i do this for the entire period so i have here now a large number of sets

I could say it's K different sets and I have one observed set.

And now I can say the general statement is my model is calibrated if this one observed set of claims is not an outlier in the set of all simulated claims.

So if this is one element or basically one member of this family here, then I can say, yes,

um i cannot reject my model because it is compatible with the data and once this is the case then i can project it to the future and around the model now on the problem is now doing a comparison on this level so to compare the all the

claims which I've in here with all the claims over here, we are operating in this high dimensional space, and there is basically no way to do it.

So what we do in first is we reduce the claims.

So this is happening here.

And we represent the simulated claims in the same way as we represent the observed claims.

So we use the same statistics.

So from the latest year, I could, of course, I could simulate it till the end.

But I'm only taking the first year.

So the system knows behind what's happening to the end, but for the analysis, I'm only taking the first year.

And if I go back 10 years, I take the first 10 years of the simulation.

So basically, exactly the same representation of the claims in the simulated sets and in the observed set.

now i project it as i said before to these one-dimensional spaces and then i can do the analysis and i can come up with calibration parameters this is the um how this basically works now we can have now a look at the how do we reduce the claims and i have shown here two very simplified developments one is the reported pattern and the other

or the incurred, and then the paid amount.

So the in the case of the incurred is again, it takes a couple of years until the sometime until the claim is reported.

So I've here a reported leg.

And then the claim, then first estimate of the incurred and then here there's an adjustment and here again, and then

it got this continues into the future until the claim is closed but of course this information i don't have so what i'm taking here is the latest estimate for the ultimate this is what i call i for the incurred star

for the case simulation for instance on the right hand side i have then the payment patterns and what is important to make sure that the payments are always smaller than the incurred because usually um as there will be this is one of the boundary conditions in this information but we don't really care about the individual numbers

what we do is we take here an average legs it's basically the area which i have here above the curve so this gray area here if i divide it by the amount then i get the incurred leg this is this part here this is one of these reduced numbers so i'm not looking at the details i'm just taking the average leg which i have here so this is one figure i'm taking

And then another statistic I can use is the number of adjustments, which I have seen.

This is one, two, three.

That's also one number I'm using to calibrate.

So to say how many adjustments is an insurance company making for the claims?

Is it large or is it small?

So these are the reduced statistics here.

So the reporting lag, the amount, of course,

in card lag and the number of adjustments.

And on the paid side, I can take very similarly the paid amount or the ratio which is paid.

I can also take

a paid lag and also the number of payments so these are some of these reduced random variables which i'm using for calibration so i'm simplifying and these numbers are considered to be largely they're not completely independent but they are largely independent

now here is another um representation of the of the process um so we again we have now at the bottom the events which are leading to insured claims we have the set twist claims and the the part which is observed so this is more or less what the

seeding company is reporting to the insurer and then we have a set of reduced claims on the other side we have now the generative model which is creating the case sets with simulated claims this can be very detailed then maybe in the first step you align these claims representation with the observed claims and then in the next step we reduce them and then we take

this information here to come up with a conditional distribution functions then you take the observations and you apply base and you end up with the calibration parameters which you can feed back into the model so this is the another representation of this process

And once the model is calibrated, then of course you can go and apply to the future and do everything you need to be able to rate your business.

Okay, now let's look at some results.

First, what I've done here is very simple.

I've used, in this case, not actual claims, but

I've used a simulator for the generative process.

And what I've done here, it's a log-normal process, and I assume there is some trend.

So I go back the years and I have the true probability distribution for each year, which is represented in the vertical axis here.

You see here the probability.

And I've drawn one random number

for each year, which is shown here with these small diamonds.

And what we also have here is then the projection to the future.

So assuming this trend continues, we have then the, this is then the true, in this case, for the generative process, which is simulated, but this is the true distribution in the future.

Now, here I have my generative model.

and you can see my initial model is how it is flat so there is no trend in there and what i can do in in a simple approach simply say okay let me try to adjust the level so i'm just moving this model up and down and try to align it with the data this is done down here so here we have then again a flat model but you can see at the beginning it's

too high, because the observations are on the lower side.

And at the end, it's too high, because the observations are on the higher side.

So actually, there seems to be some trend in the data.

And which, of course, reflects the trend in the underlying model, which is unknown.

Now, if I can also do is to say, Okay, I'm trying, I'm calibrating the level and the trend.

And this is what I've done here in this third picture here,

Now I have, you can see a model which is increasing over time.

So this is the generative model and you can see now it's well aligned with the data.

And if I compare now just the first two moments, so the mean and the standard deviation, then here I'm quite close to the true hidden model.

So this is basically unknown, but this is what I've put behind the scenes.

The true model is 83 with a standard deviation of 8.3.

And here I have 88 with a standard deviation of 8.8.

So it comes pretty close here.

My initial model was 60 with a sigma of 6.

If I calibrate only the level, I end up with 72 and the standard was a sigma of 7.2.

So you can see if I use this approach, then I can

get very close to the hidden true model.

Here is a second example, which is now a compound model.

So I have a simulating the number.

I take a negative binomial distribution to simulate the number of claims and then the log normal distribution to simulate the severity of the claims.

And this is again shown here and the same approach.

You have the

just calibrate the level and you calibrate the level and the trend and you get pretty close to the initial model so just a different process behind but the principle is the same now here i have another mod approach where i show you a little bit more in detail what's actually happening

Here I have again a negative binomial distribution for the generative process.

Now here I've shown the probability mass

uh function so the probabilities for each outcome and what you show see here in the second part is then a log normal fit to this distribution and i could i could do the analysis with the discrete distribution which is possible but it could be computation a little bit more challenging but it's doable but

what we can see as long as we we want to assume that the observations are in the high density region of the model so we can easily replace it by a fictive distribution which is a much easier to handle because here we can use a um oh can i find the term

So the whole Bayesian analysis is basically much easier if you take a log-normal distribution than compared to a negative binomial.

Now, here again, done the same.

Now, here what I have, the probability distribution is not the model behind it directly, but simulated.

uh models where i have the take my generative model i'm simulating a large number of claims and then i take the empirical distribution as a basis for my analysis and here again i fit a log normal distribution so we're just fitting the first two moments and then i do the analysis with this fitted distributions what i've also shown here is then i've aggregated the whole

10 years or 12 years here into the into the mean and show the distribution of the mean.

And I can use this distribution to calibrate the level if I want.

So the level calibration I can do by using the annual data, but I can say I'm just looking at this overall distribution and I calibrate it.

So this is when you see here, then you see the observation.

and the calibrated observation is more or less in the center of the distribution but still if i look then at each and every year then i still have this trend which is missing so if i am calibrating level end trend then you can see here i come i'm up with a model which is very close to the initial or to the true hidden generative process so this is um

the how it works so you take your generative model to create the simulated distributions you fit discrete sorry a continuous distribution to it and then you apply base by using your priors and you find out you can find out the calibration parameters now

here what i've now put everything into one chart to say at the end what we have is we have seen we have different reduced random variables so here's the number of reported claims then we have the reported lag and the incurred amount the incurred lag

the number of incurred adjustments.

Then for the paid, we have the paid lag and the number of payments.

And what I've shown on the right is some additional strongly dependent variables, which we can use to observe, to check if the model is OK.

But in this case, I've only calibrated this green area here.

And here, I've just shown one

a hand where i can change the level and another one where i can change the trend and maybe i run multiple iterations in this case i think i run it five times to basically be able to deal with the dependencies and what i've shown now is that as you can see here this is now the

the distribution which i'm using in as a as a prior and then behind this is in light gray you can see the true distribution so you see here there is an offset and after calibration you can see these two distributions are basically overlapping which means there is no gap anymore and this is here is another big gap here it's already at the beginning okay

um and so you can see the calibrated model is basically very close or the deviation like here is smaller than one standard deviation so this is basically as good as you can get so this is the um how the whole framework works so we apply to multiple random variables so the ones i introduced before so the

which is shown here you can have some additional random variables which you just monitor and then you run your your calibration model until you are satisfied with the result and then you apply the model to the future now just to summarize i've looked here this also you can find in the paper

the comparisons of the integrated framework, what are the pluses and compared to the standard approach.

And you can see it is when you use all information in a coherent way, you don't waste any information.

in the standard framework there's a lot of information you cannot use and it's not aligned so it's this approach here is superior in all regards but there is of course it it's not for free the price you have to pay is the

the computational effort and also the effort to set up the whole system because you need to be able to run it to the past.

Of course, this effort is much higher, but you have the benefit that maybe you can substantially reduce the uncertainty and the bias in your estimates.

So this was the motivation which we addressed at the beginning.

Okay, here's just the summary.

So basically we have the process and the traditional model.

and over here the integrated approach and it's a kind of a paradigm shift

which requires that you have a new mindset to approach it because the actuaries especially in certain in the anglo-saxon region they have well-defined processes how they are supposed to rate the and assess the range of spaces this is you will not find in any of these uh um

documents which are used so it's a completely different approach and this will the market will probably is in some areas maybe not open to it so you need you need a different mindset you need the different skills so i think once you want to introduce something like this you need to run this in parallel to traditional approach until you realize that it has a benefit and maybe it's a similar process when as when swiss reintroduced the evm framework it took about

more than 10 years until they went public with it.

So these things can take quite some time.

But maybe there is a new technology which is around in artificial intelligence, maybe there is some way to shorten this transition period.

Okay, this is my part.

So thank you for the attention.

And I think we're open for the discussion and questions.


SPEAKER_02:
thank you for the great presentation anyone in the live chat can write a question otherwise roland it would be great for you to say hello give any remarks in a first question and then scott thank you sure so first of all congratulations stefan so


SPEAKER_00:
Stefan used to be my boss at Swiss Re and he's one of the OGs who came up with all the pricing frameworks that are still in place today.

I was super happy when I met him again and he showed me what he's been up to here.

and i think that's really where to take it because when i look back in the last 20 years or so there were like almost no developments in this area and i think nowadays with all the ai happening it's it's really time to do something like this but now comes my biggest question to you stefan

Do you think that the world is ready for this?

Do you think that anyone will look at this, that there is any form of acceptance for this?


SPEAKER_03:
You see, I mentioned it before, I'm very skeptical, because we have

standardized approaches and i have already tried to also to approach some i mean i left switzerland about 10 years ago and i was doing something completely different to me and this then came up somehow as a side product of other activities and then i remembered i had this problem in the back of my mind which i wanted to already address at that time i presented some ideas at that time and nobody was open for it

and if i by talking to some of the former colleagues i have the impression they are not ready that's why i mentioned here it requires a new mindset maybe a new mindset requires new people

and i don't know what has to happen to the industry maybe there is some smaller companies which say oh great this is an opportunity for us so let's jump on this but i think the old dinosaurs so far i don't see any willingness to change their approach


SPEAKER_02:
Okay, Scott, if you'd like to give any initial remarks.

I know you wrote a novel in the chat, but if you'd like to give any remarks and maybe a question somehow concisely.


SPEAKER_01:
Daniel knows that when I write a lot in the chat, it's an excitement.

So Stefan, first of all, thank you for fantastic presentation.

And it's a feeling for saying where have you been all my life, because you've given voice to a lot of the things that felt ever since I've heard about active inference felt like they needed to be thought about in risk.

And so I was a corporate and tax attorney for 30 years.

And so I see these things.

So I think my main question is, and what is one of my latter questions in the chat, I'm always looking for ways to introduce new things where you don't have to rip and replace.

Because people who remember talking to the head of security of AT&T one time, and he said, why would we do anything different next Tuesday than we did last Tuesday when we made a billion dollars?

So how do we get people to want to do something?

And what I wonder is as a immediate matter, could the generative model, the application in your slide right here under the Beijing approach of projected generative model to the past, could that be included as a calibration in effect, but on the existing projection of claims history to the future, that analysis?

Is there a way to make the thing that we're aspiring to migrate towards a part of an existing part of the analysis so that it's seen as being additional but not requiring migration to the new analysis entirely?

Is there a way to bolt this on so that people can start getting used to it, seeing its value in the context of the analysis that's already being done?

Or are they so fundamentally

different that the new approach doesn't lend itself to being offered as an add on an existing approaches?

What are your thoughts on that?


SPEAKER_03:
Yeah, very good question.

But if you look at these two slides, just over here, and over here, both approaches use more or less the same information.

Of course, you might say, okay, here, maybe have a different model than over here, but it could also be the same model.

So you can, I think it's necessary that in the transition phase you apply this new concept in parallel to the existing methodology.

So the people can also get the feeling what is the difference and then it will take some time, I don't know how many years, until you see the results because you will not see the results immediately.

It will take some time until you see that yes, down the road we have

really much less adverse development in the claims and this you meet maybe you need about five years of experience until you see the results really materialize and this is probably also the time it takes for the people to familiarize with the methodology I think we want to implement something like this you need to do it step by step maybe you don't

do it at the beginning for all lines of business you maybe confine it to a few lines of business and then over time you can expand to some other and of course people need to get used to it and once so you run the old framework in parallel and then you can compare and it's time you get the sense for it and once you feel comfortable then you can basically turn off the old part well


SPEAKER_02:
Thanks again for all just give a short note.

This slide really captures the difference here, going from extrapolating descriptive statistics on observables into making a generative world model that's based upon or conditioned or updated from observables, but also has latent states.

It's like, we don't remember the time series of visual input and then extrapolate.

we update based upon the observations and then use that to do the now casting and the forecasting.

So that has all kinds of beneficial properties.

The reduced representation

uh and the ability to explore direct counterfactuals generate synthetic data there's just a lot of things that a purely time series correlation based approach doesn't really get at the question i wanted to throw out to all of you was as a non-reinsurer coming from a biology perspective what are biological analogies to the reinsurers or the underwriters role

like could it be considered to an epistemic foraging portfolio like research projects or could it relate to like individuals and they're waiting in a group like risk profiles of different foraging strategies within a group or how how does it happen because

that idea of being too or too little cautious in a changing context with the hard and the soft market and there's a possibility of being over or under risk averse in either setting but you don't know the true risk setting


SPEAKER_03:
Maybe let me try to first provide just a short thought.

If I look at biology, then I think biology, you find it over here.

And I don't have difficulties to come up with an analogy in biology which correspond to this approach here.

But maybe someone else has a better idea.

But maybe just one analogy, something I did here, it's kind of intentional, I put here

you see one part of the information into red one into green and one into blue so you could say this corresponds to the uh cones you have in your eyes so basically you also have three receptors for different information

which of course it's just now it's not really one-to-one comparison but this is maybe what i try to do so you have the receptor you combine this this thing you come up with the internal view of the external world and then you act on it so this is the analogy as far as i can see it but maybe someone else has a better example in mind


SPEAKER_01:
I mean, at the broadest level... Sorry, Roland, did you have something?


SPEAKER_00:
No, just go ahead, David.


SPEAKER_01:
I was going to say, at the broadest level, I put it in the chat,

I mean, it feels like, so I used to work in securitizations.

So you take credit card portfolios or car finance portfolios and put them into packages and sell them off.

And that used to occur to me that what you're doing is trying to find a bigger sucker for the risk, right?

You package it up, you make it look good.

And that was the 2008 financial break was packages of mortgages that were crappy mortgages because the banks were enthusiastic about selling off the mortgages.

So they

made the mortgages so they were appropriate for sale in these packages, but they weren't necessarily good risks.

So the risk just got iterated up.

And what I'm wondering, you know, you think about a biology, what do you what are the name of the organisms that eat dead decaying matter, like leaf litter and stuff like that?

Whatever, they

I mean, death, it's everything's recycled and risk is recycled continuously.

That's what biology is.

So you have this big negentropic urge that negentropic beings have, whether they're physical or not, and they're all dealing and everyone loses ultimately their own individual battle on negentropy.

All organisms die.

but then they're recycled by other organisms.

And so there are copper fights that eat poop, things like that.

I mean, there's all sorts of recycling that happens.

And so it feels like from a reinsurance perspective, you have abstractions of risk,

With all insurance, you're creating instruments to create an organism, a sim-infogenesis organism that can cause you to de-risk in ways you can't do alone, period.

So it's like a eukaryote.

Right, you're adding, people could just not do reinsurance and not do insurance and self-insure, and then have to pay the full price of their house burning down or whatever.

But we have these abstractions where we kind of develop these eukaryotic things and say, hey, let's do this together.

It's a good idea, because then I just pay a premium, I get a house back, and then the reinsurers say, hey, let's do this together.

We got all this different flood kind of risk or whatever, and we can do an evaluation at a metadata level.

It feels a lot like exactly what happens in biology without memos and voting and all that.

And so the power of what you're talking about and revealing Stefan feels like this is how stuff works if we don't get in our own way.

politically and with false models from a history of economics, et cetera.

And if we really allow for the information to flow among the entities at risk, doesn't mean that every organism survives forever.

It's not about perpetual motion or perpetual life, but rather there's a balance that happens naturally.

feels like as an immediate matter, there's an ability to sell this to the quants on Wall Street and say, hey, your model is slightly out of whack with the biophysical physics reality, the mathematical physical reality.

And so, hey, you're an arbitrageur.

If you want to make money on the difference between reality and your model, here it is.

You don't have to do it.

Do what you're doing.

So it feels like it could be pretty attractive.

in the right kind of audience for people who have to deal with risk professionally that they can't turn away once you say something to them if it has value and shows value they won't be able to say oh no we're just going to do it our old way and their clients won't tolerate that anyway

Sorry, I got a little excited there.

It's great stuff.


SPEAKER_00:
Yeah, but you go in the absolutely correct direction.

I've had to calibrate a lot of risk models in my life.

And when you look at the definition of risk, it's what happens above the expected.

So normally you subtract the expected and normally you have to answer questions like what is the worst that can happen within 100 years?

And hardly any one of us has ever lived 100 years to actually tell, right?

And so actually what you learn very quickly when you're a risk professional

is that you have to break down the question to something that fits the experience horizon of a living being.

And so typically when I was calibrating risk models, I saw, okay, this guy has been in his profession for 10 years.

So I was asking questions around what was the worst thing that happened in 10 years?

And then given a model you try to extrapolate, so what's the worst that can happen in 100 years, right?

And in my opinion, this approach here is just doing it much more systematically.

So we always had a bit of a tendency to do this, but it was more heuristics.

And this kind of gives it a more scientific way of looking at the problem.


SPEAKER_02:
That's awesome.

I'll just make one quick... Oh, go ahead, Stefan.


SPEAKER_03:
I mean, you mentioned, or Scott mentioned the financial market.

You know, when I joined the reinsurance industry, this was many years ago, the first thing which happened was the collapse of the so-called LMX market.

This was the London excessive loss market.

And what they did there is they just handed the risks

from one to the next and it went around the circle many many times and then there were claims and the claims were also circled around so the same claim you had already suddenly you get it by the back door again and you put it on top of it and this happened until someone run out of cover and then it was there were some big losses then happening some natural events some other things

uh i think there was a piper alpha it was a windstorms in europe there were hurricanes on and suddenly this whole thing collapsed so it was working before because the the circling these things around took very long as long as it was this paper but then they started to introduce uh

So the whole thing went much faster and the whole thing collapsed.

And so the industry had to learn already in the past.

And I don't know what has to happen for the industry to maybe go through a new leap and to start to implement new technologies.

approaches because as long as there is nothing massive impacting as you also have mentioned before then maybe people are just saying okay why should I change anything everything is okay


SPEAKER_01:
And if I can, so I put in the chat, that's a direct relationship of what you're talking about there to climate change.

And let me tell you what I think is going on here.

I think it's pretty wild.

So we talk about abstractions.

We have abstractions upon abstractions.

You've got futures markets and derivatives, and they're trading upon trading upon trading, and there's all intangible.

Now, there's a connection ultimately to some physicality.

some commodity or some phenomenon, but there are abstractions that are layered on abstractions and that's fine.

But the challenge is we're trying to de-risk and create negentropy.

We're dissipating information differentials in those abstract areas and that requires energy.

AI requires coal burning.

And so the energy generation in physical space, coal, oil, et cetera, and the agriculture required to feed the people who did all that stuff that's required in physical space to keep taking these exponentially increasing levels of abstraction in intangible risk space, because the financial markets are one set of instantiations, but reinsurance is an abstraction on top of an abstraction.

The insurance didn't exist in all forms forever, and it keeps getting more abstracted.

Forward contracts are a form of insurance.

When you're hedging, you're doing it for insurance.

Speculation in markets like that create liquidity, but also make it noisy in terms of what the risk profile is, right?

It feels like as we're de-risking in abstract space, information space, it requires energy that we have to burn coal for in physical space.

And so it's an insidious problem that we continue to try to do that.

I have to go get something at the door, but I wanted to make that point about that relationship between climate change and the information space.


SPEAKER_02:
Thanks.

One key point, Roland, that you mentioned was fitting the experience horizon to the living beings.

In some ways, that's the cognitive approach, which is fit it to the cognitive slice, like the sense making and decision selection in the moment.

And then there's the consequences for the kinds of AI or synthetic intelligence models that are made.

For example, downloading all of the Internet's plain text data, that's more experience than one human life learning, and then using correlative aspects of that large data set to then carry forth those correlations forward.

That's like the LLM framework.

and then the question is how and where alongside or differently does a small generative model or some other kind of nested or structured generative model come into play that is able to do better or differently than just recapitulating correlations plus plus plus

And how that actually plays out.

And then I think there was a great comments about how does it really get adopted or how does it be shown to make a difference?

And it can be used in retrospective meta analysis, possibly with the kinds of data that you've shown today.

So having open source and clear methods for doing the retrospective and saying, not saying that doing anything differently would have been better, but here's how this pattern of outcomes and decision-making differed from this.

So give a meta-analytic framework for longer past time series, do backcasting kind of like in stock trading.

And then open up that space of, well, people are familiar with stock trading and investing based upon the mean value.

And this moves it into also thinking about the variance profiles.


SPEAKER_00:
I find this extremely interesting as an observation.

I used to work also at Swiss Re together with Stefan and I used to head an initiative in casualty.

And you see in property, things are somewhat objective because you have earthquakes and you have wind storms and they have kind of a physical nature and you can study how the earth works and you understand it.

something like liability is much more manmade.

And so essentially it's all about things that are not supposed to happen.

So you always have like an infinite configuration space and you cannot model it.

And already at the time, it was like 20 years ago, but I said, we need to have like monitors to systematically scan the media, what's being published, what's the awareness.

Because with a certain likelihood, the lawyers are going to pick up what's in the media and they are going to do lawsuits about it, which then result in liability claims.

And there I see exactly the applicability of LLMs and all this stuff.

Because what they can do extremely well is put things in a correlation with each other, right?

And so I think especially for modeling casualty business, I see a huge potential in what you are saying.

So just putting LLMs on top and finding out any form of dependence between what's being published and lawsuits, etc.


SPEAKER_02:
So Stefan, where are you gonna go with this work?

Or what are your directions?


SPEAKER_03:
Okay.

Good question.

But you know, as I think I briefly mentioned, this, see, maybe just make a short comment about my career.

So I have a degree in physics.

But then after my PhD, I then went into reinsurance industry.

And

left the reinsurance industry about 10 years ago and then i was doing completely different things but this topic here was kind of something which was brewing in the back of my mind so by exploring other fields and the maybe i have to say this is also thanks to my wife which she exposes me to all kind of topics suddenly i got familiar with different approaches how to solve problems

and then suddenly this uh um i saw a way how to solve this issue here and my intention was simply to bring it to paper to write it down to publish it and get it out so basically say okay then for me the this is done but now roland came into the picture and he is now coming up with some ideas that maybe we can

do something with it.

So I don't know exactly what's going to happen.

I, as I also mentioned, I approached some colleagues from former colleagues, but there was no big interest.

And I've also seen in, when I published the paper, the peer reviewers, they say, yes, it's interesting, but for them, it was not something they say, yes, we need to move in this direction.

It was simply asking what is then the benefit compared to the old approaches.

So so far, I don't see really the I haven't encountered anyone which is open to it.

And I didn't have any plans, because I have other ideas, which I want to presume and retired and I have other projects.

But depending what is happening, if people approach me and say, Yes, how can we somehow implement this framework, then I'm open to be involved there.

But I have no specific plans to do anything with it anymore.


SPEAKER_00:
maybe some reinsurance listened to this.

So I think obviously it's not very nice to do a dry exercise with fake data.

So we should have real data and we should have a real interest behind it.

But maybe if some reinsurers are listening to this podcast, please contact us and let's see that we can collaborate in this, that we can do something in this.


SPEAKER_02:
Yeah, there's a lot of things to say.

laid out calculation and the kinds of bayesian analyses that are described with this kind of baseline structure and then as you alluded to that's even before getting into multi-agent cognitive economics thinking more about the decision making the sense making of different entities this is just taking in a very specific approach to structuring the data and the input

And it's an interesting direction to see what tools and how general does the toolkit have to be.

Like when you say external data comes in, that could range from nothing to all the stock data in the world.

So how do you even exactly go about that to make it useful without just saying, well, it could be tiny or it could be all the data analysis in the world.

And then again, how does that connect to doing similarly or better or different than the kinds of models already in play, and then the decisions coming out of those?


SPEAKER_01:
I mean, it's funny because... Oh, go ahead, Stepan.


SPEAKER_03:
Oh, sorry.

I think you bring an interesting thought into the picture, because what I see, you can see I focus on a very tiny part of the whole decision process.

but of course you can expand now this market blanket approach and make say okay at the end we have within a reinsurer company there is many different buckets which we could try to understand from this from the perspective of this new framework and see how is the the corporate decision processes what are they happening how do different places part of parts of the company interact

how can the company interact with, I mean, there's other information coming from the outside, I mean, you have other stakeholders, I mean, I just looked here just as the insurance clients, but there, of course, there is the, you have the financial part, you have the, maybe some, you know, the exchanging risk, maybe you have the stock,

shareholders and so on so there is many different facets and you can also see them yourself as part of a bigger economic framework and see what is your place in there so at the end of the day i think if you want to have kind of digital earth

you could try to expand this kind of thinking into some much bigger framework and try to understand.

I don't know if anyone has the capability to do it because it sounds pretty complex, but maybe with the help of artificial intelligence we might get there.


SPEAKER_00:
This is another thought, because every time we have these tech revolutions, like now with the AI revolution, it also shifts industry players, right?

So if we look at the invention of the smartphone before phone companies were called like Ericsson or Nokia, and afterwards they were called Apple and Google.

And so I think there are always these shifts when tech revolutions come.

And I think this AI revolution together with such modeling, it could actually then suddenly become attractive for big tech to start doing reinsurance.

Because what you need is you need capital, they have capital.

and you need skills.

And they are actually quite good at acquiring data and analyzing data.

So maybe it's exactly this type of model that makes it accessible for big tech now to enter the domain.


SPEAKER_02:
Reinsurance on digital information.


SPEAKER_03:
interesting area i mean if you look at this insurance it's very very simple it's basically saying we need someone who absorbs is the last resort for risk yes you need to be able to assess this risk and in the case if something happens you need to be able to pay and i mean this is the traditional approach but this could be easily replaced as roland said by other

players which step into the market and since it's not largely regulated this entry point should be relatively easy if you have the technology and if you have also the financial strengths behind and of course the skills this is the important part then you can basically replace all the reinsurance industry and it's not complete fiction if you look at alibaba this big chinese um


SPEAKER_00:
amazon so alibaba started selling insurance so it's it's not completely off because they they have a platform so i could picture uh you know amazon or or facebook to start selling insurance it's it's not completely science fiction


SPEAKER_02:
okay very interesting uh roland and scott if you have any last comments then stefan you can have the last word okay all good scott okay what is your what's your last word stefan


SPEAKER_03:
okay well i didn't think about having a last word but i mean for me it was very interesting also having this discussion it was for me as a new experience to be in a podcast and to present in this way i mean of course i was presenting many uh

traditionally and it's now I'm entering it my later days into a new uh platform which was quite uh an interesting experience so I enjoyed it having this conversation and then let's see what's happening out of it great all right thanks again all until next time okay thank you
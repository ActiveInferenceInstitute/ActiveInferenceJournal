SPEAKER_01:
hello and welcome it's november 11 2024 we're in active inference guest stream number 92.1 with max eifer we're going to be discussing a variety of thermodynamic and bayesian topics it should be very exciting if you're watching live feel free to write any questions in the live chat

Max will go through some sections of several papers and I will read questions and ask questions.

So thank you, Max, for joining.

Looking forward to hearing about this.

Take it away.


SPEAKER_00:
Great.

Yeah, thank you for the introduction and thanks for having me on.

Really excited to talk about this stuff.

So there are a few papers I want to talk about.

One called Thermodynamic Linear Algebra about kind of a novel hardware approach to solving linear algebra problems.

Another one called Thermodynamic

Bayesian inference, which is similarly about using these new thermodynamic hardware devices for Bayesian inference problems.

But before I get into that, I just wanted to make a note on kind of like the story of this work and how it evolved, because I think it's pretty interesting.

um so i'm working at a company a startup called normal computing and where we started off was just thinking about what's kind of the best hardware paradigm for for ai and especially probabilistic machine learning because these are some of the most challenging tasks computationally and seen as some of the most necessary to be able to do kind of robust um ai without you know hallucinations

And a big approach to that is probabilistic machine learning.

And a lot of our approach was inspired by this report that came out of a workshop in 2019 called Thermodynamic Computing.

and i'm sharing it uh can you see my my screen here what i'm sharing okay yeah so there's this one quote here in the introduction that i think really summarized the point it says somewhat paradoxically while avoiding stochasticity and hardware we are generating it in software for various machine learning techniques at substantial computational and energetic cost so one of the big kind of takeaways from the work they did in this um this workshop

was there was kind of a consensus that emerged that we're working so hard to suppress the randomness that comes up naturally in our hardware.

And then we need randomness actually to run various randomized algorithms, especially in probabilistic machine learning.

And so if there were some way to just use the randomness that's naturally there,

that we know about due to thermodynamics to achieve those algorithmic goals, maybe we would save a lot of energy, basically.

So that was kind of one of the ideas we were inspired by.

But the first step along this journey was,

So there was an earlier paper where we basically proposed circuits that are capable of sampling from a normal distribution.

And I'll get more into the math of why that happens.

Why do circuits that look like this allow you to sample from a multivariate Gaussian?

So this was kind of the first work that was published at Normal, actually before I started working there, called Thermodynamic AI and the Fluctuation Frontier.

But how you can think of this is this is the electronic analog of a system of coupled harmonic oscillators.

So if you can just think about a bunch of masses that are coupled together by springs and that are also damped, so they have friction acting on them, and there's also noise acting on them.

So kind of damped system of harmonic oscillators driven by noise.

That's the kind of system that we're talking about in these works.

And we're just kind of looking at this electronic analog, which is completely

uh analogous completely isomorphic to that that picture uh in terms of masses on springs um so one of the first problems we were interested in solving was solving the linear system of equations so you can think of this as if i have a few planes in an n-dimensional space here i have three planes in a three-dimensional space um and in general i'll have n planes in an n-dimensional space and i want to find the one point where they all intersect

assuming that there is such a point.

And you can think of that mathematically as I'm giving a matrix A and a vector B, and I want to find the point X that satisfies this equation, AX equals B. So solving this kind of problem has tons of applications in economics, engineering, but also in AI for training neural networks more efficiently.

you know, if you can solve linear systems quickly, then there's benefits to training, et cetera.

So that's one of the first problems that we decided to look at.

And well, so I think that it's interesting to compare our approach with earlier approaches to analog computing, basically, because it's something that people have actually tried to do for quite a while is build analog computers.

In other words, where the signals are not stored as binary numbers, ones and zeros,

but can take values in between and so you're kind of using the capacity of the hardware to represent more precise numbers than just a one and zero and in theory that seems to make sense but the big problem that analog computing ran into is is basically that there's a lot of noise that's present and noise and imprecision kind of prevents you from answering from solving these problems like like linear systems of equations uh with the accuracy that you that is needed uh by the application

So I guess the big insight of this work in this thermodynamic linear algebra paper is that if you model the noise, if you understand the noise in your system well enough and you model that noise, then you can actually predict the behavior of the system on the distribution level, even if the system is unpredictable on the trajectory level.

And the way we describe that, modeling the noise, is using what's called a stochastic differential equation.

So equation 2 here I've written is a stochastic differential equation where I'm saying that dx, the change in the state of the system, which is really a collection of voltages on capacitors in the circuit, the way it changes in time has two components.

The first is this term, which is called the drift term.

And you can think of that as the predictable part of the change in the state.

And it's the gradient of this quadratic function.

And then the second term is the noise term.

So that's the result of kind of the random forces from the environment.

And what this actual potential energy function here corresponds to is exactly that system of coupled masses on springs or harmonic oscillators, which we've kind of drawn here, illustrated in a way that like you can basically think of the system as being a bunch of little balls coupled together by springs that are all being jostled around by noise.

And so the problem we wanted to solve, right, is that we had this matrix A and a vector B, and we wanted to find the point X such that AX equals B, right?

And the way we're going to do that using this physical system is by allowing it to come to equilibrium.

meaning that after it's kind of run for enough time under these dynamics, eventually the state will be uncertain, but the state will be described by a probability distribution that is the Boltzmann distribution, the exponential of the negative potential energy.

And so because the potential energy here is a quadratic function,

the distribution of this form where you have E to the minus V of X, where V of X is this quadratic, is just a Gaussian.

And it turns out that the mean of that Gaussian is A inverse B, which is exactly the solution to that system of linear equations.

And the covariance matrix is actually A inverse.

So there's there's I guess two things to notice here.

The first is that if we want the mean of the linear system, if we want the answer to the linear system of equations, we can just allow this system of coupled harmonic oscillators to come to equilibrium and then estimate the mean of its state.

And if we actually if we wanted something else, we wanted the inverse of the matrix A, then what we could do.

is we could allow the system to come to equilibrium and then estimate all of the second moments of its state, or in other words, estimate its covariance matrix, which turns out to be this A inverse here.

So I also, I want to expand a little bit on this idea of like, what do I mean when I say that we could model the system and we can predict its behavior at the distribution level, even though it's unpredictable at the trajectory level, right?

So when I wrote that, when I showed that equation, this equation two on earlier slide,

What I mean by when I say it's unpredictable at the trajectory level is that if I know x of t, I can't predict x of t plus 1 with certainty because there's this noise that acts as a force on the state.

And so you would need to know what the values of those random numbers

are going to determine what the state is at time t plus one um but if i know like initially if i have some probability distribution for the state at time t1 and i want to know what's the probability distribution of the state of time t2 that is a question i can answer using this equation um

And just for an illustration of this, on the left-hand side here, this kind of blue twisting path is a trajectory of the system where I've initialized it at this point over here.

And then I've run it for a long amount of time.

And the state of the system here, there's only two variables, x1 and x2, kind of orbits around in this ellipse.

But if I instead of just running one of these simulations running this one trajectory, if I ran many of these simulations and sample the initial state from kind of a from a normal distribution centered on this point,

and then I look at the distribution over time for that collection of you know many thousands of trajectories I would get this plot here on the right and so what you see that the big there's a big difference between these two pictures in that the left one we have this trajectory which is behaving in kind of an unpredictable and random way but the distribution itself is behaving in a very predictable and in nice kind of smooth way and in one another way to think about this is

is that the equation I showed earlier is a stochastic differential equation called a Langevin equation.

And it describes the dynamics in terms of the left picture, in terms of the trajectory or at the trajectory level.

But if I want an equation that describes what is happening in the right picture, that would be the Fokker-Planck equation, which describes the dynamics at the distribution level.

And so it's basically it's nice to be able to have these two pictures in mind at the same time.

On the one hand, we're thinking about the trajectory level.

and on the other hand we're thinking about the the distribution level picture of the dynamics um and so when we go up to higher dimensions it's not as easy to make you know to make this plot i can't make uh i have no way to easily draw you know a 10 dimensional hyper ellipsoid the way i can do it in two dimensions but there is still a way we can visualize it and the way we can visualize it is by looking at the covariance matrices um so

it's i guess just to go back here like what really are these ellipses well these ellipses really each of these ellipses really represents a whole distribution a normal distribution um and this ellipse is kind of the typical place where you would expect to find points that are drawn from that distribution but another way of describing the properties of that distribution is by its covariance matrix which describes all of the kind of parallel pairwise correlations between all the different variables

For here, we only have two variables, so it would be 2 by 2.

But if we had 10 variables, it would be 10 by 10.

Here, we have it for eight variables, so it's 8 by 8.

And what we're looking at here is really

over time as we go from kind of this black circle on the left side to this red ellipse on the right side, you can think of that process in higher dimensions in this eight dimensional case as how this matrix here is changing over time.

So it's kind of the same thing, but now we're just watching a video of how the covariance matrix changes over time instead of plotting many ellipses at different moments in time.

Now, another interesting thing about this video here is that this isn't a simulation.

This is actually an experimental demonstration that we did on the hardware that we created at Normal Computing on circuits that look essentially like this.

So these are just, as I mentioned earlier, these are just electronic implementations or realizations of coupled harmonic oscillators that happen to come to this equilibrium state, which is a multivariate normal distribution.

and which has these properties that you can use it to solve linear systems of equations and get the inverse of matrix.

I guess, yeah, just one more word of detail here.

So this input matrix here is the matrix A. And then target inverse, this is the actual inverse of A that you can do just numerically.

You can compute the inverse.

And then what these hopefully and in fact do kind of arrive to are the estimates, close estimates of the inverse of the matrix A, because that's the covariance matrix is supposed to be proportional to the inverse.

So you may be wondering at this point, this seems interesting, but why is it that we would go to all this effort to actually try to solve linear systems or invert matrices in a completely new way, right?

What about this approach is promising?

So one thing that's promising about it is that we have kind of theoretical results that show that we get better asymptotic scaling as the problem size increases with this thermodynamic architecture than the state of the art digital algorithms do to solve the same problem.

So without, yeah, I can talk more about like the difference between the overdamped and the underdamped at some point, but just the headline here, if we look at this one row linear system, what this is saying is that basically on the digital state of the art methods take like order of D squared, you know, the amount of timescales with the square of dimension to solve a linear system using something like the conjugate gradient method, for example, whereas with our kind of thermodynamic approach,

you get an amount of time scaling with d, so order of n instead of order of n squared.

So you have some kind of polynomial speed up there.

And we proposed a number of other algorithms in that paper too.

So the one for inverting a matrix, I basically explained how that one works.

It's just you end up estimating the second moments instead of estimating the mean of the state.

And there, you also get a polynomial advantage because instead of taking order of n cubed, you get order of n squared.

And we also have algorithms for getting the determinants of a matrix or solving something called the Lyapunov equation, which is a matrix equation that kind of generalizes the idea of the inverse.

So one other thing I wanted to show actually is just kind of an animation or maybe a more vivid illustration of what's going on here.

So what you're seeing here is just like a simulation of a two-dimensional coupled harmonic oscillator system.

where right now the matrix A is just the identity matrix, and so in other words, there's no coupling between the two harmonic oscillators.

We have two separate harmonic oscillators that aren't coupled together, and the coordinate of one is plotted as the x-axis, and the coordinate of the other is plotted as the y-axis.

So because they're not coupled, we wouldn't expect to see any correlation between them, and we don't in fact.

And there's also noise being added.

So I can kind of change the amount of noise here by increasing the temperature.

And I can change the resistance to change the amount of damping and change the inductance, which is essentially like changing the mass of the mass on a spring.

And by kind of tuning these parameters, I can get something that looks more or less like I'm getting a bunch of samples from this multivariate normal distribution.

And in fact, I am.

And I can look here at the marginals and see these are samples from these, you know, these are the histogram basically looks like a normal distribution.

Although just because all the samples seem to be to have the right distribution, that doesn't mean that the samples are independent from each other.

They could all be, you know, highly correlated.

And so we can also look at kind of the autocorrelation function here, where basically the more you crank up the noise, the faster the correlation decays.

In other words, if you want to predict where will the next sample be based on the current sample, if there's very little noise, that's kind of easy to do.

But if there's a lot of noise, that's really hard to do.

And I can also just take samples less frequently if I don't want as much correlation.

So that's what I'm doing here is I'm kind of cranking up the amount of the interval between samples.

And now that will suppress correlation between samples quite a bit.

But at this point, there's something a little bit trivial about this example because they're just two harmonic oscillators that aren't coupled to each other at all.

so really what we're interested in is is when we couple them together um so i think yeah we would change this kind of term and so what this ellipse now is the ellipse has changed this predicting you know where we expect to see the samples now that there's a coupling between these these two masses on springs

And I think it's nice just to think intuitively about why does it happen?

Why do we expect to see the distribution look like this when we turn on a coupling between the springs?

Well, it's basically just that like we, instead of having two separate masses on springs that are oscillating by themselves, now there's another spring that's pulling them together.

And so they want to be close together, right?

And so that means that we're likely to see points where the position of X1 is similar to the position of X2, whether they're both positive or both negative.

And that's kind of exactly what we're seeing as we see samples kind of filling in this ellipse.

So, yeah, I just wanted to show that kind of animation.

This is just a kind of some JavaScript code I wrote a while ago as kind of a visualization tool for that.

I also, yeah, I have the paper pulled up here and I can talk more about like, how, how do we, um, there are a few things I can talk more about.

So for example, like how do we derive these complexity results of the asymptotic scaling or how do these other algorithms work?

Like, um, the determined estimation.

Um, but yeah, I wanted to maybe at this point, pause and see if there were any questions either from the audience or Daniel or anything.

Yeah.


SPEAKER_01:
awesome okay anyone can write questions in the chat yeah hearing more though about how do you pull out different analytically useful values from heat what does that mean

Taking a stab at it now, but revisiting it, I mean, how is it that there are these tractable, useful quantities arising out of physical objects like this?


SPEAKER_00:
Right.

Yeah, I think that's a good question.

I mean, to go back to the history of like, so first off, the first thing people tried to do was just pure analog computing, where they didn't really consider the noise or assumed you could suppress the noise enough.

And that analog computing approach is based on a system that physically solves a differential equation.

So whereas our approach is a system that physically solves a stochastic differential equation, but it's still useful to think about the analog case where there's no noise.

and where you're just trying to solve a differential equation.

Because one thing you might think to do is if you want to solve a differential equation, instead of just solving it numerically on a computer, you could actually just try to build a system that actually evolves according to the differential equation you want.

So that's one perspective on why do these physical objects do useful computations, because it's kind of like saying we're going to build the physical system that behaves in that way instead of trying to simulate it.

And I guess, yeah, another perspective is that, well, so with the linear systems, right, like you can, in theory, you can solve linear systems on an analog computing device as well that doesn't have any noise.

And what's kind of going on there is that you're basically doing very...

efficient multiply and accumulate operations in a sense in that if you want to add together like n things um digitally that's going to basically take order of n steps but if you want to add together n things kind of using some analog device there are ways to do it that are more more like constant time so for example if i have n wires that all converge at one point

and I send a current into each one of those end wires, then the current that comes out is the sum of those currents.

So it's kind of a way of adding together all of those end things in constant time.

So I think those two perspectives are kind of useful, that one, we get very efficient analog multiply and accumulate operations, and the other is that we're kind of building the system instead of simulating it in some sense.


SPEAKER_01:
What is the transferability of these shapes?

Like, do you have it work really well on this five by five matrix, but then you need to go back to square zero for six by six?

Or how do you deal with making on the fly changes or adapting to specific problems?


SPEAKER_00:
Right, yeah, so that's a very good question, right?

Like how, you know, how does it, how do these things work in particular when we go up to like the limit of like much larger matrices, which is relevant for, you know, the big kind of AI and industrial applications.

What we found is that

Generally, these algorithms behave pretty well as we increase dimension over a large range of different dimensions.

We can go up to thousands of dimensions and still get the answer very accurately.

But there are other properties of the matrix that are relevant too that can make it hard to solve the problem.

One that's very important, for example, is the condition number of the matrix.

So if you think about those ellipses, well, I guess I can just go, yeah, scroll down here.

if for a matrix that well the condition number is the ratio of like the largest singular value to the smallest singular value of a matrix and what the singular values are is they're the length of like this kind of this long axis here and this short axis here uh and so an ellipse that's very long and skinny has a very large condition number basically the condition number basically describes how long and skinny the ellipse is or if it's in high dimensions it's like some kind of n-dimensional pancake

if it has and it has a large condition number and basically yeah um matrices with harder with larger condition numbers end up being harder to invert or to solve a linear system for if matrix a has a large condition number um but you know there's still uh there's still ways to do it but it's that's i would say one of the bigger challenges to generalization is that like as you go up to higher dimensions it's kind of easier to get matrices that end up having very large condition numbers so more can go wrong i guess


SPEAKER_01:
And you, to what extent, rely on repeated, de-correlated, long-term sampling of single objects versus parallelized or replicated sampling across objects?

And then what does that have to do with ergodicity?


SPEAKER_00:
Yeah, so I think I'll first answer the question about ergodicity, actually, because, yeah, I think that that's a very good question.

That's a very good way to phrase the question, actually, because if I look at this plot on the left, this is just a single trajectory, right, that I've run for a very long amount of time.

And it turns out that if I just take enough samples of this single trajectory, that even if those samples are kind of correlated with each other,

um it still you know will work but if if i have n samples it's better to have an uncorrelated samples and correlated samples but i don't need to actually run many trajectories in parallel i can just run one and it turns out that that one trajectory has the same statistics the same mean covariance as if i had run

a thousand you know or a thousand or a million separate trajectories and that doesn't happen for every you know for every stochastic differential equation but it does happen for this one and that property is called ergodicity right the property of ergodicity is that the ensemble averages and the time averages are you know are the same they're the same statistics um so on the right hand side here this is kind of looking at the ensemble picture which would be more like if i had used parallelization and built many

of the devices and kind of random and parallel.

So I guess the answer to the question of like, which one do we do or which one in practice is more useful?

You can do either one.

I think that it's it's a it's kind of a knob or a lever you have to turn if you if you have more, if you can pay more for more hardware.

uh and you can pay more energy then you can solve the problem faster by running many trajectories in parallel whereas if you only want you know one device that that only runs one of these trajectories you can just take samples at different points in time and because of ergodicity it's the same as the ensemble statistics basically cool carry on

Cool.

Okay.

Yeah, I thought those were all great questions.

So thanks.

Yeah, I guess there was one other thing I wanted to say just at a kind of a high level about like, how do we go about getting these complexity results?

Because you can imagine there might be something subtle about it just because this algorithm, for example, is never actually guaranteed to give you the right answer because it's based on randomness.

You know, the state is not, the mean is not guaranteed to converge to,

the you know to the solution to the linear system it's just very likely to do that um and so you need basically i think of it as it's um yeah it was this term that was coined by uh i forgot his name a computer scientist um leslie i think uh or what was the name uh yeah i forgot his name but um

It's called probably approximately correct is the idea that you have a computation that with a high probability will be within a certain radius, you know, within a certain error tolerance of the correct solution.

And so that's kind of the approach we take to

quantifying the runtime analysis for these algorithms.

And I guess an example of that is down here in the appendix, where you can kind of find we have bounds on, let's see, where is this?

Yeah, so it'll be down here.

Right.

Yeah, so we basically have a bound on how long do you need the time to be in order to have the

state close small closer than epsilon to the solution with probably with probability at least p basically and so all of our results are formulated in that framework of you choose the success probability you choose the relative error tolerance and then you get out a time basically that you need to run your system for you need to average for and those kind of results are quoted up here in these

these tables basically.

But yeah, so I think that's really what I had to say on the thermo linear algebra paper, unless there are any other questions about that.

And I think otherwise I'll move on to talking about this thermodynamic Bayesian inference paper, I guess.


SPEAKER_01:
Let me ask one question about that last bound.

Um, how do you get there?

Did you build different objects and then backcast the formal relationships, or do you play around with the formalisms and then ask if or how a physical object could realize it?


SPEAKER_00:
Right, so yeah, so all of these mathematical results are derived within a pretty abstract framework, which is the abstract framework is just the stochastic differential equation.

So in other words, in order to derive these complexity results, we just start by assuming that the system evolves according to this stochastic differential equation.

And once we've made that assumption, we can completely forget about the physical implementation.

Whether it's actual masses on springs or capacitors and inductors or whatever, those details become irrelevant.

And so we're just talking about the kind of mathematical properties of how quickly the distribution converges under these dynamics to the stationary distribution of this equation, basically.

OK, cool.

So yeah, so one thing that we noticed basically when we were doing that, when we were writing that thermo-linear algebra paper, is that we noticed that while we'd gotten these very nice and satisfying polynomial speedups that we predicted, it seemed like we somehow should be able to get even better speedups.

And the reason for that was that much of the runtime of running those thermo-linear algebra algorithms was actually dominated by just taking averages of the state of the system.

In other words, if we go back to this picture,

here, it turned out that we initialize the system out of equilibrium.

It very rapidly comes to its equilibrium distribution.

And then we have to spend a long time drawing samples from this equilibrium distribution and averaging those samples in order to get the estimate of the mean.

And so we started to think about, well, what if there are applications where we don't need to take averages, where we just want to draw samples from the equilibrium distribution?

Because if that were the case,

then we would probably we might be able to get a much better speed up because we wouldn't even have to do this this long, you know, the longest or the hardest part of our algorithm, which is moment estimation.

And so that was basically the genesis of this thermal Bayesian inference paper, because we started thinking, like, what kind of problems is the output sample from a distribution?

what kind of problem you're you don't want something that's a deterministic function of the input but you want to sample and one thing that came that came up is sampling from the bayesian posterior of some model so uh so that's kind of a pretty common task that that you know people want to do is they have some parameter data say and then some data observed

data, y, like different samples, y1, y2, so on.

And you want to condition your, you have some prior distribution for your hidden variable, or yeah, you have some prior distribution for your parameter theta, and then you have a likelihood, which is like a theory of how the

parameter theta determines the observed data y. And given those two things, given your data and your prior and your likelihood, you want to condition the parameter theta on the data.

And then often you want to sample from that posterior distribution, the distribution that's kind of been revised by the presence of your data.

And so we basically started thinking about whether there's a way

We could use a similar approach based on a physical realization of like noisy management dynamics in order to do Bayesian inference.

And then we wouldn't have to do moment estimation or averaging.

And so we might get much better speed ups.

And so that's basically what we did in this paper.

And I guess at a higher level,

there's kind of a nice formalism for how you can think about yeah just what what is going on at a higher level and how we're doing bayesian inference using a physical system here um and so what it is it's it's it's related to the idea of an energy-based model that one way of parameterizing a probability distribution p is to write that probability distribution p is the exponential

of some energy function, or e to the minus v. So this is just a way of writing any distribution you can write in this form.

But also importantly, this is the form a distribution will take in statistical mechanics when you allow a system to come to equilibrium that has some energy v.

form of its kind of equilibrium distribution um and so this when you talk about uh probability distributions in those terms it's often called like an energy-based model approach um and there's something convenient about using energy-based models for bayesian inference basically which is that when you take the log of your

probability density function instead of like multiplying together probability density functions you end up adding together energy functions because we're now talking about the logs of the probabilities instead of the probabilities and so that's what's going on with this equation here it says u1 of theta equals minus ln of p theta minus ln of py given theta so here um

P theta is the prior.

That's our assumption of the distribution for the parameters theta before we've gotten any measurements.

PY given theta is our theory or model of how Y depends on theta.

In other words, how does the data that we can observe

depend on the parameter that we can't observe.

And so what you would do in Bayes' theorem is you multiply together, or not even Bayes' theorem, just the definition of the joint distribution is p of theta and y is p of theta times p of y given theta.

But now instead of multiplying these things together, we're going to be adding them together because we're talking about the energy.

So this energy kind of represents the joint distribution somehow because it includes the contributions from the prior and the likelihood.

And so what you do in these thermobase and inference problems is you start out with one energy, one potential energy, which can be thought of as like kind of a box that, you know, you can think of the system as some, well, like in thermodynamics, we might think of it as like a gas inside of a box.

And then you can change the potential energy by moving a piston in or out or something like that.

or sometimes we think of it as like a marble in a bowl and then you can reshape the bowl changing the potential energy which is maybe closer to like this picture here where when we change the potential energy we're like changing the shape of this bowl basically um so

back here what's plotted in this panel a is what happens when we vary when we change the potential energy function over time so between time zero and time one we're going to change the potential energy from a potential energy that's associated with the prior uh

which is U zero.

So U zero equals minus log of P theta.

And that's like this blue curve is the probability density when we're in that prior distribution.

And then we slowly change the potential or slowly or quickly, we change the potential energy to U one, which is the distribution that corresponds to the posterior basically for theta.

In other words, it's the distribution where you've conditioned theta on all of your observed data basically.

uh and so what we're seeing in this plot is that as we as we change the potential over time the distribution of the system changes and it goes from this gaussian distribution to this kind of asymmetric distribution which in this case is the posterior for like bayesian logistic regression um and another view of that is in panel b where we're looking at

how the thermodynamic quantities change over time.

So whenever you're changing the potential energy of a system, the potential energy function of a system, you can be doing work on the system.

And so this orange curve here is beta times work, where beta is 1 over kT.

And we see that the potential energy changes between time 0 and time 1.

And during that window of time, we're doing work.

But then at time 1, the potential energy stops changing.

And so no work is being done there.

And so we can see how much work is being done on the system.

And then there's also heat, which is the change in energy that's not associated with the work being done, which you can compute just kind of using the first law that DE equals DW minus DQ.

um so we have work and heat which are kind of the two most fundamental uh thermodynamic quantities i guess um and then we also have uh the free energy uh we also you know a quantity of great importance and there's there's kind of two different versions of free energy that are that are plotted here so one is this dashed line which is the equilibrium free energy and what that means is that at each instant in time

The potential energy is changing, but we could imagine a system that was in equilibrium at that instant in time.

And if we did have a system that was in equilibrium at that instant in time, its free energy would be the equilibrium free energy.

So that's kind of the definition of the equilibrium free energy is the free energy of a system at equilibrium with the instantaneous potential.

Then there's the non-equilibrium free energy, which can basically be larger than that.

The reason it can be larger than that is that the system is not in equilibrium at all times, that if you're quickly changing the potential energy, the system is driven out of equilibrium,

and so you can kind of look at the gap between these like the gap between the dotted black line and the solid black line as the extent to which the system is out of equilibrium at each moment in time and what happens is when when we quickly change the potential from time zero to time one the system is coming out of equilibrium these these two lines are getting farther apart and then at time one we stop changing the potential and the system starts to come back into equilibrium basically

And just another note on this is a nice interpretation of the difference between these F minus F equilibrium is that it's actually the KL divergence or the callback callback library divergence of between.

the distribution of the system at any time and the equilibrium distribution.

So that's just another way of saying that it characterizes to what extent the system is out of equilibrium, because this KL divergence is basically a way of quantifying the

So it's not technically a distance measure, but a kind of distance between probability distributions.

And so it turns out that this like F minus F equilibrium is equal to the KL divergence between the distribution and the equilibrium distribution.

Um, so that's kind of the general framework of like what we're doing in the thermal Bayesian inference is that we're, we're setting up a system so that we can change this potential energy.

And we turn on, we, we have a potential energy term that represents the prior, and we have another kind of coupling term that represents the likelihood.

And then we have like physical degrees of freedom that represent the observed data that we have.

And when we turn on this interaction between data.

and all of these observational degrees of freedom, then the equilibrium distribution becomes the Bayesian posterior.

And if we wait for a long enough time, then we can measure the state data and get samples from the Bayesian posterior.

So let's see.

I say a little bit more about that just in theory and how that plays out.

But you might notice here that this kind of is the key to the algorithm is this stochastic differential equation here.

which is you know similar to the form of the stochastic differential equation i showed on this slide equation two um and in that it has kind of this drift term which is like the predictable change in the state of the system and then this um this noise term which is like the unpredictable change in the state um and so let's see

Yeah, I think that, oh yeah, okay.

So I guess what I'll, yeah, I'll talk a little bit about the examples we did in this paper.

So what we did was we tried this out for two kinds of Bayesian inference, like two examples of Bayesian inference, one of which is a Gaussian-Gaussian model.

So where you have a Gaussian prior and a Gaussian posterior, and the other is Bayesian logistic regression.

So where you have like a Gaussian prior and a logistic likelihood.

So this plot here is for the Gaussian-Gaussian model.

And we see basically the prior is one of these green ellipses.

And then the likelihood is the red ellipse or might be the other way around.

But then you get the posterior, which is kind of in the middle somehow.

And it turns out that you can use this circuit over here.

in order to sample from that blue ellipse, that blue distribution, which we've actually simulated this circuit in SPICE, which is a popular library, popular software package for simulating circuits.

And so what you do is you have one set of resistors that's used to input the prior covariance matrix,

and another set of resistors that's used to put in the likelihood covariance matrix.

And then once you do that, you can measure the currents through these inductors.

And measuring those currents gives you a sample from the desired posterior distribution.

So yeah, I guess that's at a high level what I wanted to say about this Gaussian-Gaussian model case.

Were there any questions about either the overview, like the general picture of thermobasian inference,


SPEAKER_01:
or about like this Gaussian Gaussian model yeah a few questions how do you know that you aren't too confident or not confident enough with respect to the posterior like your precision or your attention what makes you not just how do you get confidence in your confidence level

such that you can draw a useful Bayesian credible interval rather than, for example, making it much wider or much tighter.


SPEAKER_00:
I see.

Right.

In other words, how do we test that we're actually drawing from the right posterior kind of


SPEAKER_01:
Yeah, if you can compare it with the exact compute, then you've already done that computation.

So then when do you feel like it's okay to get out without the guide rails and take the thermodynamic estimate as it is without doing secondary calibrations?


SPEAKER_00:
Yeah, so that's a good question, right?

So one of the nice things about the Gaussian Gaussian model is it is one of these very few cases where there is an analytical solution.

So you can compute exactly what the posterior, like the posterior is a Gaussian again, and you can compute exactly what the mean and covariance matrix of that Gaussian are.

And so we can compare them.

But I think you're also alluding to the fact that eventually we aren't going to have that guardrail and we're just going to say, we're now confident enough that this system can sample from the true posterior and we're kind of done with testing it.

And yeah, I think the question of like, when exactly do we become confident enough?

that it gets the right posterior is maybe not completely solved yet.

But I would say that because in the Gaussian-Gaussian case, there's an analytical solution, I feel pretty confident that we can do enough testing even at pretty high dimensions that we'll be able to assure ourselves basically that it works.

But where that becomes more difficult,

is for cases where there is no analytical solution to compare to, like in the case of the Bayesian logistic regression, which we also do here.

And in that case, the best thing you can do is just kind of run a Monte Carlo simulation digitally and compare that to the results that we would predict with SPICE or compared to experimental results.

which also works, it's just much, much more expensive computationally.

But on the other hand, the fact that it's much more expensive computationally means that we're solving a really hard problem, which is good.

But yeah, I guess, yeah, to make a long story short, that's still kind of something we're going to have to figure out, you know, in complete detail is like, how do we do enough tests and what tests we have to do to assure ourselves that it gets to the right posterior?


UNKNOWN:
Yeah.


SPEAKER_01:
Another interesting piece was that changing differential between the equilibrium free energy and the non-equilibrium

so do you experiment with like different speeds and find a trade-off between moving it too fast but taking you too far out of equilibrium but then is there any downside to just moving it super fast or do you uh get better accuracy or performance in the kind of infinitesimal movement case keeping it close to equilibrium all along the way


SPEAKER_00:
Right.

So that's a that's a really good question.

And actually something that we're probably that that's a focus like a current kind of research focus is like, what is the given that we have this freedom of like how quickly we want to turn on the potential, how quickly we want to turn on the interactions?

What should we do with that freedom, basically?

And there's like this really interesting literature on like energy time tradeoffs in stochastic thermo.

So let me see if real quick I can.

pull up this paper um wasterstein uh statistical um well just while you're looking for it you mentioned probably approximately correct that's leslie valiant that's right leslie valiant that's right exactly yeah um thanks um yeah so

Okay, so I can't find exactly the reference I'm looking for, but basically what it says is that, so I talked about the KL divergence, which is one way of quantifying the difference between two probability distributions.

Another way of quantifying the difference between two distributions is the Wasserstein distance.

which kind of has this very deep and elegant theory in kind of optimal transport theory.

And where Wasserstein distance came from originally was optimal transport problems, and in particular, how do I transform one configuration of some material, say, into another configuration?

or if i have like a mound of dirt and i want to move the amount of dirt and also reshape it uh how how many shovels full of dirt am i going to have to take basically and that's kind of the classical um kantorovich problem uh or there's there's another guy who well i think it's the other guy the problem i'm thinking of but there are these two dual optimal transport problems that are basically the same thing that are that problem of transforming one mound of dirt into another

And the Wasserstein distance is the amount of shovels of dirt you need basically to carry to transform one distribution into another.

So you can think of it as like accounting for, it accounts for like not just assigning low probability to high probability events,

with one distribution versus the other but also like if the mean of one distribution is very different than the other then you can think of it as like having to move a lot of mass from one place to another and the kind of cool result that there is in statistical mechanics and in particular like stochastic thermo around this washerstein distance is that if you want to

change a system state um over time then there's like this trade-off you need to pay between the amount of work that you dissipate and the amount of time that you take and it ends up the form of that trade-off is that the product of the dissipated work and the time duration of your protocol

is lower bounded by something times the Wasserstein distance.

So I guess you can think of that as like, given that I have two distributions that are a certain distance away from each other, I can either transform from one to the other really quickly, but dissipating a lot of work, in other words, paying a lot of energy, or I can do it really slowly,

without paying really any work at all.

Or I can take some trade-off, do something in between where I slowly change from one to the other and spend a medium amount of work.

What we're interested in doing is applying this formulation, this theory of Wasserstein distance and trade-off between energy and time to solving problems like this one, like doing Bayesian inference.

and kind of saying there's like this optimal frontier of there's not just one optimal solution because you might be in a situation where you care more about speed than you care about energy or you might be in a situation where you care more about energy than about speed but there is a set of solutions that are optimal with respect to kind of your disposition towards and how you value energy and time and i think it's basically related to these like energy time trade-offs


SPEAKER_01:
that's kind of a classic pure slash meta bayesian approach anytime you have a free parameter treat that as another kind of variable to do inference on um what is the work reservoir as we're not exactly picking up weights and

lifting them to a higher elevation or anything so the heat reservoir makes sense in terms of standard thermodynamics heat bath and all that but what's happening with the work reservoir and what is it touching or doing with the device yeah yeah that's a great great question too so if i go back to um


SPEAKER_00:
to this picture for a second here, you can visualize these systems, again, as a collection of masses coupled together on springs.

And then what we're doing when we're changing the potential energy is we're changing the spring constants of some of these springs.

So maybe each of these springs has a little screw on it that you can turn with a screwdriver to make it more or less stretchy, basically.

But you need to draw a boundary between the system and what's outside the system somewhere.

And where we draw that boundary is that,

whatever is turning that screw that's tightening, that's changing the spring constant is not part of our system.

And so if you need to do work, if it costs energy in order to turn the screw, then that needs to come from somewhere.

And we're calling that a work reservoir.

And so we might actually be lifting weights up and down if that's where the energy comes from in order to change the parameters of the potential energy function.

We're just kind of saying that wherever that energy comes from, if we have some battery or some flywheel or whatever, that's kind of outside of our system.

And we're not really going to worry about it beyond just quantifying how much energy


SPEAKER_01:
interchanges with the system basically yeah it's interesting that the device almost has these dual reservoirs it's like one of those toothpastes with a dividing line in the middle and squirting out both kinds because on one hand it's interfacing with heat defined as that which does not do work just the thermal dissipative and then on the other hand it's connected into the work reservoir which is sort of

the complement of heat and then the device is is interfacing with both of them and as you described seeking to find a path or a trajectory for itself

that isn't leave that's not either dissipating extra heat unnecessarily or leaving work convertible work on the table either because it's sort of able to interface or or mediate between the work and the heat here


SPEAKER_00:
Right.

Yeah.

Yeah.

And there's a really a nice interpretation of that, too.

I think I the way you put it as like a toothpaste tube where you can come in one in and out the other or something like that.

And it is it's it's we see a lot of diagrams like this where you have a heat, you know, you have your interchanging both heat and work.

with an environment in the context of like heat engines, right?

Because you have a heat engine that, well, often there'll be three reservoirs in that case.

Like you'll take in heat from a hot reservoir, dump out heat to a cold reservoir and then do work or something.

But here we have two things.

We just have a heat reservoir and a work reservoir.

But there is a way of thinking about this as a kind of heat engine like thing, but it's more of a refrigerator than a heat engine, I guess.

Because the goal of a heat engine is to take heat

and then output work.

Of course, you also have to output heat somewhere.

But that's that's the goal is to use heat as a resource in order to output work.

But what we want to do here is we're actually going to input work and then we're going to output heat.

And as a result, the entropy of our system decreases.

And this is just kind of a general thing that happens when you condition one random variable on another.

the entropy, on average, of the variable you conditioned decreases because the mutual information has to be positive.

So this is just one of these fundamental information theory things.

Because of the positivity of the mutual information, conditioning on a random variable decreases the entropy.

And you can think of in some sense, like decreasing the entropy of your system as as the operation of a refrigerator, even though it's not necessarily decreasing the temperature, per se, but it is decreasing the entropy.

And so there's kind of this nice way of looking at this as like a heat engine, but really a refrigerator where you have this flow of work in and heat out and decreasing the entropy basically.

But just one quick note on that.

Of course, you can't decrease the entropy of the whole universe.

So the entropy has to be increased somewhere to compensate for that.

And that's what's happening where we have heat flowing out into the heat reservoir.

So the entropy of the heat reservoir is increasing to compensate.


SPEAKER_01:
Could you maybe give a little detail, how do real empirical measures, Y1, Y2, etc., come in?

Like in active inference, we're often thinking about perception as inference or action selection as inference, but just on the perception side.

So some...

perceptual prior new observation comes in new photon hits the retina or whatever it may be new thermometer reading comes in for the IoT and how does that feed into this and how do you schedule or bring that empirical measurement in


SPEAKER_00:
Right.

Yeah.

So we think of the measured data, these observations, Y1, Y2, Yn, as representing constraints on the system somehow.

And so when you get new data, you introduce kind of another one of these constraints by

writing down your observation and then coupling those physical degrees of freedom that store the observation then get coupled to your subsystem theta.

But just to make it kind of concrete of like, yeah, how does that actually happen?

So in this circuit here, the degrees of freedom that represent theta are the currents going through these inductors L1 and L2.

And the degrees of freedom that represent an observed data point Y are the currents that are provided by these current sources, I1 prime and I2 prime.

And so what you do when you get data is you turn on these current sources and set the current that they're programmed to output as equal to the components of the data that you got.

But you could also maybe, something you said actually I thought was really interesting, which was in terms of like thinking about observation as like perception or learning from perception.

And you maybe could even think of these, maybe if we got rid of these current sources and just had the environment kind of coupled to these wires, and somehow it's like taking a measurement of the environment directly more or less.

and then conditioning on those i think that that's also like kind of a valid interpretation which hadn't really i hadn't really thought of before but i think that's an interesting way to think about this as being kind of having these perception inputs here that are coupled to the environment and then conditioning its internal state based on those somehow


SPEAKER_01:
Cool.

Yeah, I mean, a neurological analogy to these circuits are like there's some neural manifold and spiking patterns or neural electrical fields are traversing and they're itinerating around this manifold describing the pre-observation state.

Then a new observation comes in.

If it's not attended to, it's like an uncoupled oscillator, like

the observation that isn't attended to it's as if it didn't happen tree falling in the forest and all that whereas if the observation is strongly attended to and there's a strong coupling between the observation and the resonating state of the the prior then it moves more to be influenced by that observation more strongly and then hierarchical systems just have series of layers

with possibly different attentions to layers with resonating information coming from the environment in

towards the deeper priors like of the organism and then and then more on the outbound action selection is when it opens up to well maybe the uh prior from the higher levels cascades out and gets reflected as action modifying the environment to be more like the priors of the organism that's the change the world direction of active inference

ongoing with the change your mind direction, updating the priors to be more aligned with what the environment is giving.


SPEAKER_00:
I see.

Yeah, I think that's a really cool way of looking at this.

Exactly as you said, if there's no coupling between the degrees of freedom representing the parameters you're estimating, if those are coupled to some source of information in the environment, then yeah, it's like you're paying attention to it in some way.

Whereas if there's no coupling, it's for the purpose of your Bayesian inference, it didn't happen or the information just isn't there.

And so somehow the conditioning on a piece of information is embodied physically as a physical coupling between your description of the latent variable and the actual physical encoding of the observed data, I think.

And then the other thing, yeah, the other piece you mentioned about like, what about kind of some feedback effect where like you, as a result of your knowledge of this variable, you come up with some policy for like how to behave or something like that.

And then that will actually change, maybe that will have an influence on the next observations you get.

And that's a really interesting direction as well.

I think like for the purpose of this paper, we've just assumed like all the observations are independent of each other.

And that would kind of break down if you assume that like if you are using some kind of decision, you know, decision strategy or policy to make decisions based on the data you get and your estimate of the state, your estimate of the parameters data to inform your future decisions in like maybe like a reinforcement learning setup.

But definitely like figuring out how to use this hardware in that context is something of interest, current interest for us now.

So I'll definitely keep people posted as that progresses for sure.


SPEAKER_01:
Cool, feel free to keep going if there's any other pieces you wanna explore and then people watching live can write any questions in the chat.


SPEAKER_00:
Cool.

Okay.

So I think that, yeah, so one thing I haven't talked about yet with this paper is our results on complexity, which is, of course, is a really important part of it, because that's what motivates us in the end is like why we go to this effort of

building this totally new kind of device in order to solve problems we already have a way to solve them um but it's just really hard it's you know intractable a lot of the time the sample from bayesian posteriors if you have to run some very long uh you know markov processor or you know some monte carlo simulation uh so we have some results um oh yeah and i also kind of hinted at this earlier that we expected that we might get better speed ups in this case

than in the case of the TLA, the thermo in your algebra algorithms, because in this case, we don't have to do moment estimation.

We just want to output samples.

And so that did kind of turn out to be true that we got pretty significant, like predicted speed ups.

So

for this case of the gaussian gaussian model where we're given like a gaussian prior and a gaussian likelihood and some data and then we want a sample from the gaussian posterior we found that the amount of time that it takes that the amount of time you have to wait between when you inc when you input your

uh when you input the problem and when you output the samples scales logarithmically with the size of the matrices involved basically um which is pretty good time scaling because if you think about like what you would have to do digitally to accomplish this you would have to like get the inverse of a matrix you have to get the inverses of two matrices and like add those together and take an inverse again that's essentially the process you need to do when you want to do

conditioning for Gaussians.

And so that's something that would basically be n cubed time, maybe somewhere between n squared and n cubed using state-of-the-art algorithms.

And so if we can do that in the amount of time that scales with log n, that sounds really good.

It almost sounds too good to be true in a way, because how is it that we went from polynomial to logarithmic?

It almost sounds like an exponential speedup.

And while I think that this result is really cool, it's not quite that cool.

in that it's not fundamentally changing a polynomial operation to a logarithmic.

And the reason is because we also have to look at the energy cost.

And when you look at the amount of energy, the energy consumed by the algorithm basically scales linearly with the size of the problem.

So we got this scaling where we have logarithmic time in dimension and linear energy in dimension, which is still a pretty significant improvement.

And then we also did some analysis of the time complexity for the Bayesian logistic regression scenario.

And we found there we were able to show that you also get logarithmic scaling with dimension, but we haven't yet figured out the energy scaling analysis for the logistic regression, because that circuit ended up being a bit more complicated, and it uses these

differential pairs of transistors to get logistic functions.

And it's a little bit involved to try to estimate all the energy.

But those were the two circuits we gave in the paper was this one for doing the Gaussian-Gaussian model and this one for doing the Bayesian logistic regression.

And we have some promising results on the time and energy complexity of those operations.

So I guess one other just kind of interesting note about the interpretation and that also informs like how we derive these scale results is that there's kind of this interesting literature on like Wasserstein gradient flows, which is basically.

You can think of when the probability distribution is changing over time here, you can think of that probability distribution as a vector in a very high dimensional space, or really an infinite dimensional space.

And the dynamics of that vector can be thought of as actually a gradient descent on some objective function.

uh and it turns out that what that objective function is is actually that callback library divergence or the difference in free energy between you know f and feq and then it's and then the distance that um kind of the step size of that gradient descent is measured in terms of the washerstein distance which is that distance between distributions i talked about earlier so you can think of this as um like the dynamics of the probability distribution

as like an ordinary differential equation in an infinite dimensional space where it's like going in the line of gradient of steepest descent in KL divergence with respect to changes in Wasserstein distance.

But yeah, so those are the main things I wanted to talk about in this paper, I guess.

And maybe at this point, we can see if there's any questions from the audience or something.


SPEAKER_01:
Cool.

yeah i think people will definitely be interested in that connection with free energy where the kl divergence comes up in both the variational free energy bounding the bayesian surprise and expected free energy which is when considering policy inference as a computation as a cognitive process and looking over expected observations asking what

see what policy dependent sequence of expected observations would have what epistemic and pragmatic value so these circuits aren't planning they're in the variational free energy kind of mode but it's interesting that again we see kl divergence popping up in both and and different divergences and distances between distributions being basically the crux of the question and which measurements

about which distributional distances and which properties of distributional distances and divergences can be enacted or how costly is it to emulate them when those are the calculations that it's important to know and the most classic of all being basically prior going to posterior how much should you update


SPEAKER_00:
Right.

Yeah, yeah, I really like what you were just saying because I think, yeah, I'm kind of aware of these.

I've read, I heard a little bit about like kind of these interpretations of KL divergence as like in terms of games and like how much do you expect to kind of lose or win if you have some assumption about the distribution of a parameter that if kind of your distribution for a parameter isn't the actual distribution or something like that and how

well or badly do you do in this game based on your kind of estimate of a distribution?

And I think that I really like that characterization of it because it gets it kind of a more like end to end measure of quality.

Because if you yeah, if you if you use some system to accelerate the computations you need to power an agent and then you want to estimate like quantify how good is your system really doing, then what people will care more about at the end of the day isn't necessarily like what's the like

What's the relative error between these two matrices or vectors, but more like how often will I make the right decision or something like that, basically?

Or what's my expected reward based on decisions I make?

And so I think that, yeah, if we can somehow introduce the KL divergence on that end as well, that'll be a really nice way to kind of formulate things.


SPEAKER_01:
How do you see these elements playing a role in systems engineering and design?

Like is it something that gets brought in like another kind of GPU or CPU?

We've talked about processing, but not really about memory or storage.

So how do you see this playing a role in whether specialized inference pipelines or consumer hardware or what?


SPEAKER_00:
Right.

Yeah, that's a great question.

I think there's really two ways to look at it.

One is like, how do the theoretical results matter in terms of the future of computing hardware?

And I think that these theoretical results can be pretty influential.

I think like scaling laws, like Moore's law and stuff like that, or like how Landauer's principle informs how people think about what's the minimum amount of energy that you can achieve with the computation

And I think that there's one like impact of, you know, these theoretical results is that it kind of gives you a bar to aim for.

And that if we can say like, this is the energy time trade off for Bayesian inference, you can, you can do it this fast, if you pay this much energy, but if you go a little bit slower, then you can save some energy, then that kind of sets the benchmark for like what people should hope to achieve with new hardware, whether or not they're using our

but then there's the other kind of way of looking at it, which is just a more kind of literal level is like,

What's the path from these designs we presented in this paper to actually building hardware and where would that hardware go and what will it do?

So I guess the exciting news kind of on that front is that, yeah, we talked a little bit about on Twitter recently how we got funding from an agency in the UK called Aria.

That's like their new more or less their version of DARPA kind of that they recently created.

and is really interested in bringing down the costs of AI, like in terms of the hardware production costs and also like the energy and time costs of training and even in inference as well.

And so they've kind of funded our project on a timeline of, you know, let's say like,

where we're aiming to have hardware in the next, you know, three, four or five years, basically at kind of varying level of kind of increasing the dimension that it'll increase in the size of the problems it'll be able to solve.

But in terms of, yeah, like where exactly that hardware will go or who will end up using it, I think that there's still like, there's still some open questions around that.

And more like I'm more working on, yeah, the side of like designing the algorithms and trying to figure out

you know, what error mitigation or error correction techniques we can come up with to make it work better and make it more robust.

But yeah, hopefully, you know, yeah, maybe next time, you know, I'll have more detailed answer on that.

But I think, yeah, at the moment, it's not totally set in stone.


SPEAKER_01:
What else do you think is out there in terms of Landauer?

What bounds are you scraping up against or what big patterns of information and work and heat do you think might be out there and touched upon by this line of inquiry?


SPEAKER_00:
Mm-hmm.

Right.

So I guess one kind of historical note is that for a long time, people have focused a lot on just Landauer's principle as the fundamental bound for computing.

And just for the audience and people who aren't necessarily familiar, what Landauer's principle says is that to erase a bit of information, you need to dissipate an amount of heat, which is log 2 times kT.

so in other words if you can run your computer at a very low temperature then you don't have to display as much information but there's still you know amount of of work that i'm sorry did dissipate heat there's still amount of energy that you have to pay that's proportional to the amount of erasure that you have to do now the issue with land hours principle one issue is that it's something that we haven't really been able to achieve um there's a nice kind of plot on one of these slides about it like showing how

while we are still getting more and more efficiency, we're still well above this thermodynamic limit, which is Landauer's bound in terms of how much energy we pay.

And so I think the trend more recently is to say, well, we need to also account for time because Landauer's principle doesn't really account for time.

It assumes you can do what you're doing infinitely slow.

And in reality, we don't want to do computations infinitely slowly.

And so that's kind of the angle I'm thinking

about what I'm saying, like, let's try using these results from stochastic thermo that bound the product of energy and time using the Wasserstein distance.

But I think that I've just kind of dipped a finger into that literature, and there's a lot of work on that stuff in terms of speed limits, especially in quantum systems, quantum speed limits and classical speed limits.

And so, yeah, that's like, I guess the line, the pattern I'm most excited about is using these bounds on like the product of energy and time in terms of, you know, distance measures between distributions.

But there may be, you know, maybe other patterns as well.

So definitely curious to hear if any other ideas I think come to mind.

But yeah.


SPEAKER_01:
yeah that's exciting to bring time into the equation literally and and for for real embodied systems that have existence in space and time

that their material properties and the propagation maybe even coming down to speed of sound, speed of light, deformation, velocities and these sorts of things could become dominant in terms of whether the bulldozer moves a few large

amounts of earth with great cost or whether it's very fine grains of sand which might be very efficient but then it might take so long that for for a practical situation it doesn't really have utility


SPEAKER_00:
yeah yeah absolutely and i think like one the one reason i'm i'm really excited about yeah as you put it like bringing time into the equation is kind of at a higher level like big picture i think that where things are going in terms of the theory is like some kind of unification between like computational complexity theory and stochastic thermo because like we've we've been approaching some of the same problems of like yeah how fundamentally

Does your resource costs of energy and time scale with the size of a system as it scales?

But so far, we haven't, I think, fully broken through to see the shape that connection takes.

And yeah, the approach that I'm

that I kind of take to that is like, first go for the low hanging fruit.

Like not, you know, we're not necessarily going to see it all at once.

Like what is this relationship between stochastic thermo and complexity theory?

But the more we can like build up these little results of like, well, you can,

by building, constructing kind of a thermodynamic system that you're going to change the parameters, do work on and see it as this kind of heat engine or refrigerator, you can achieve a certain computational complexity of this operation.

I think these smaller results are helping to bring those two fields a little closer together, maybe.

Yeah.


SPEAKER_01:
Interesting.

Also with time, it's almost like

it's like with a body there's a baseline a basal metabolic rate and then exercise is doing extra activity on top of that so it's like if you have to pay the upkeep on the energy of the circuit even if it only is using a little bit of extra activity to do the target computation you're still paying for a large block of time the baseline level

And so when we get to the thermal limit, then the baseline is free because it's happening at an ambient temperature.

So it's not needing to be powered to give some of the sampling dynamics in the same way.

the the the informative sampling and that's the part that that I think there's still so much learning and and interest in how how do these higher moments arise with with no extra work that seems like good information and yet it's falling out of what seems to be powered just from thermal vibration


SPEAKER_00:
Yeah, no, I definitely think about it in those terms as in like, what are the sources?

Like fundamentally, the sources of energy we have, energy takes the form of work or heat and heat is like cheap and work is expensive, right?

Because heat just kind of comes from a reservoir, come from anywhere and it's not organized.

And yeah, in work, you have to you have to

go to some trouble to actually get a source of work so yeah i definitely think about like that like how can we optimize the computation so that more of the energy comes from heat as opposed to work basically but yeah


SPEAKER_01:
yes i think that's that's exactly it having having something rather than needing to add energy from the wall so that the energy gets pushed through a system does some computation and dissipates the extras heat it's almost like there's another avenue where the heat in the room

vibrates the system to functionally implement that computation or cognitive process it's it's all about arbitraging he and work in new ways and seeing bayesian updating and equilibrium and far from equilibrium processes as happening as sort of a first principle of intelligent matter


SPEAKER_00:
yeah yeah yeah i think it's really cool really cool way to see it actually yeah absolutely um yeah and i guess yeah one one other thing i'll say i guess that i was thinking is um in terms of yeah like like how where where do we look for kind of fundamental constraints on computation in terms of the resource costs i think that a lot of people some some people have taken the view that like

really the Landauer's Principle is kind of what is the most fundamental thing.

And then the reason we aren't achieving it is just kind of practical considerations on some level.

It's just that it's hard to engineer.

But yeah, my outlook is more that like, well, maybe there's a fundamental reason that it is hard to engineer something.

Maybe there's a fundamental trade-off that we just haven't discovered yet that if we knew about would actually very well explain why we're still a thousand or three or four orders of magnitude above Landauer's Principle.

So yeah, I'm hoping that maybe just considering time will get us there.

If not, then yeah, there's more to think about.

But yeah, who knows, I guess, at this point.


SPEAKER_01:
Cool.

Do you have any final riddles or thoughts or comments?


SPEAKER_00:
um yeah i can't think of anything else yeah i think this is i think i really covered everything that yeah i was hoping to get across i think so far it's been really good uh it's been really great um but uh yeah if there's any any questions from the audience we of course be happy to take those yeah okay no specific questions this was very informative max i think it's it's uh been a topic coming up again and again with these


SPEAKER_01:
not just similarities and differences and analogies but implementations of information geometries and information processes on embodiments on physical embodiments biological designed and all of this and so

It certainly feels like there's a lot of threads swimming around and exciting opportunities for synthesis.

So thank you for this exciting research and the empirical connections and everything.


SPEAKER_00:
Absolutely.

Thanks for having me.

And yeah, as I mentioned earlier, when we first started out on this research, we definitely had no idea or didn't anticipate at all that it would lead us to these things that can be interpreted in terms of active inference and connections to that and free energy.

But yeah, it's just been really exciting for me to be working on something that I think is both a practical approach to improving our landscape of hardware for AI, but also has these interesting and deep thermodynamic


SPEAKER_01:
interpretations so yeah definitely i'm happy to come back anytime and chat more i thought this was really really fun cool let's check back in a couple months in a couple in a couple trillion oscillations of the cesium as they say all right thank you max farewell yeah bye have a good one
SPEAKER_08:
hello welcome everyone we are in active inference guest stream 82.5 on april 14th 2025 discussing computers meaning and consciousness with robert warden and guests and guests so robert thank you for joining and for assembling this group to you to kick off this discussion


SPEAKER_03:
Thank you.

Hi, thanks everybody for attending.

I'm going to be talking as it says on computers, meaning and consciousness.

Let me get straight into it.

What I'm going to be talking about is largely negative results.

And here are the two key negative results of this talk.

One is that the information inside a computer doesn't define what the computing computation is about.

And that is because information in the computer is encoded.

And the second negative result is if the brain is only a computer, if it's thought of as only a computer, then it cannot be conscious.

So those are two quite paradoxical results and I expect you to be puzzled by them.

As I go through, I would really be grateful if people would put up their hands at any time and if there's something that's really perplexing about it, ask a question because very much my purpose is to find out

what people don't understand about these results.

And what I think the argument to these results is fairly simple argument, but it's paradoxical and counterintuitive.

And I expect people to be puzzled, and I would like people to speak up when they are puzzled.

Okay, so those are the negative results I'm going to be talking about.

And this is one way to remember them.

It's like an incompleteness theorem for computers.

And what it says is the physical events inside a computer don't define what it computes.

They don't define what it's about.

And this, as I say, is because the information is encoded.

External information is required in order to interpret or decode the physical events in a computer as a computation.

So that's basically what it's about, an incompleteness theorem, a negative result.

I think as an incompleteness theorem,

It can be quite important.

However, I've got another slide here.

So basically to try and understand the results, the first result is not original result is that computation itself is not a physical process in itself.

It is our interpretation of a physical process and interpretation, but made by an external interpreter, usually people, but sometimes not people using a decoding of the physical events in the computer.

And the information running inside a computer is encoded, so it doesn't of itself have meaning until you decode it.

And without decoding, the physical events inside a computer don't have meaning, and they're not actually about anything.

And the puzzle here is that consciousness is about things, whereas the things in a computer are not about things.

Now this result that computation is not a physical process but an interpretation of a physical process, that's not a new result.

It was derived by Horstmann et al.

in 2014 for artificial computers and extended to natural computers such as brains by Stepney and Kendon in subsequent years.

So those two papers are very interesting precedents to the work here.

So that's the first part of it, is that computers don't have decoded meaning inside them.

The second part is applying that to consciousness.

Because consciousness that we experience has meaning.

It's about things.

It's about the things in front of us, the things we see, the things we feel, and so on.

But without decoding, the computation in the brain doesn't have meaning.

And the decoding requires information from outside the brain.

So if you model the brain purely as a computer, then the physical events inside the brain don't have meaning, and so they don't

They can't be the cause of consciousness.

So that is the basic framework for the results.

It's a negative result.

They are negative results.

And I think negative results are important in science.

A lot of precedents like conservation of energy is if you like a negative result, this energy can't be created out of nowhere.

So it leads to things like no perpetual motion machines.

Well, the second law of thermodynamics says you can't create heat differences out of nothing.

And so there's no such thing as a free heat engine.

Or another one more recent is GÃ¶del's incompleteness theorem for mathematics.

that says you can't derive all of mathematics purely axiomatically so these are important negative results and what negative results do is they clear the ground and make new room for new ideas so the result of this talk is an incompleteness theorem for computers excuse me but horse and i hope to show you that the result has far-reaching implications

firstly for man-made computers and secondly for cognitive science.

And so if you see, if you think you see a flaw in this argument, please put your hand up and ask me about it because that's the kind of thing I want to understand about.

So I'll proceed by a series of analogies going from books to computers to brains.

These are all physical devices and inside the devices, inside a book or inside a computer,

All the information is physically encoded.

And without decoding, the information has no meaning.

And the decoding is always done outside the device.

So a consequence of that is the device itself is not conscious.

Now, it feels all right for books.

It feels a bit more troubling for computers and very troubling for brains.

Anyway, let's set off down this journey and see how far we go.

inside each of the devices the information in the device has no meaning it's quite obvious for a book that words characters inside a book have no meaning but it becomes less obvious for computers and for brains so let's go through that progression and see how we go yes i just have a question to be sure


SPEAKER_01:
what is the ground to assert that within the brain bearing in mind that actually we do not understand the brain very well at this point there is no encoding and decoding mechanism of source i think the modern view is that there is encoding and decoding but it doesn't happen inside the brain in other words


SPEAKER_03:
The sense organs encode things like light as pulse trains, neural spike trains.

They go into the brain as encoded information and the brain does something with them.

Lots of things about neuron spike trains and then spike trains come out of the brain and they are decoded as muscle actions.

So the picture of the brain has encoding and decoding, but it doesn't actually happen in the brain.

It happens in the body, in the sense organs or in the muscles.


SPEAKER_01:
I think it's something that could be discussed, bearing in mind again that there are probably fundamental mechanisms within the brain we do not yet fully understand.

Let's assume it's a postulate.

More than something that is


SPEAKER_03:
Well, I think I can assume that any information in the brain is encoded.

We know all we find in the brain is neural spike trains, and if that is information about the world, it's certainly encoded.

Anyway, let's go from books to computers to brains, and then we'll see how it feels.

So in a book, there are physical squirrels on the page, as shown here,

And we do a lot of interpreting of those in order to understand the book.

We understand the shapes of the ink shapes as letters.

We understand strings of letters as words and word strings as sentences, sentences as meanings and meanings as stories.

So finally, after all that interpretation, all that decoding, we understand what the meaning is in the book.

But to get the meaning from the book, we've had to do a lot of decoding.

By the way, I shall use the word interpret.

and the word decode fairly interchangeably throughout this tool.

And I think, um, they obviously don't have entirely the same meaning, but I think that they're pretty synonymous for these purposes, but you may want to catch me up on that sometimes and say, look, you've used these words differently.

So that's what meaning is in books.

It's not physically inside the book.

It's inside our heads when we read the book.

So we go on from books to computers.

And physically, what's going on inside a computer is digital electronics.

We understand the laws of physics and how all that stuff, all those circuits work.

But again, to understand it as a computer, we do a lot of interpretation.

We interpret the voltages in the circuits as bits.

We interpret bit strings as numbers.

We interpret numbers as being solutions to equations or whatever we've done.

problems sometimes interpret those equations and models of physical things.

We have computer simulation of anything in the world, and that's the way we understand it by doing all that decoding, all that chain of decoding.

So when do we do that?

There's a lot of decoding information in our heads, which I say is not inside the computer.

The computer just has physical digital electronics.

so we go further to brains physically what happens in brains is neurons firing and as i say what what has started those neurons firing is sense organs like eyes and fingers sending spike trains to the brain recording information about the world but that information is encoded as spike trains then the spike trains are

interpreted in the brain in various ways i use it as we interpret the brain as a computer doing bayesian probability for instance so we may think that the spike trains in the brain have a meaning as probabilities of things and probabilities of events and we interpret synaptic connection strength in the brain as learning and memory and then spike trains come out of the brain to our muscles and our muscles decode the spike trains as motor actions

So again, there is encoding before stuff goes into the brain.

Stuff inside the brain is encoded, and then there's decoding when it comes out of the brain.

So those are the three devices.

And to sort of draw the generalization across those, in all three cases, there's a chain of interpretations.

There's a chain of decoding, which says we decode A as

c as c as d and so on we decode voltages as bits bits as numbers numbers as solutions to equations or whatever it may be or numbers as being about things and at each link in this decoding chain there is various very many possibles there's no one obvious choice there's a vast number of possible interpretations and when we do this interpretation it's us decoding

And we need information to do that decoding.

We need to know things about how computers work, or things about how the brain works, or things about what problem we've set the computer.

And so that's how we've set up our interpretations.

But there's a huge number of wrong interpretations we could set up instead.

But the important point is the information we use to decode resides outside the device.

And that means that there may be information inside the device, but it doesn't have the meaning that we apply to it.

And that the consequence of that, I would say is the device itself is not conscious.

And so we've gone through this progression from books to computers, to brains.

And the question is, is there anywhere along that trail where something different happened?

And I claim there's not anywhere that something different happened.

The whole issue of encoding and decoding is the same for all three of those devices.

So an example of a computer is a floor cleaning robot.

Now, you know how these work.

Initially, it blunders around the room and bumps into doors and things.

And it's been designed with an onboard computer which has a digital map of the room.

with things in it and representation of things like chairs and tables and so on.

And so once it's bumped into a table, it doesn't bump into the table again.

And so it has the map inside it, which is encoded information.

And the decoding of that information happens outside the computer when the computer sends commands to the motor and to the steering to avoid the table leg.

And the decoding also happens in the designer's mind, where the designer knows what the memory locations in the computer mean.

But there's no information in the computer which defines what the information means.

In other words, it's got a bunch of memory locations, but it doesn't know which memory locations refer to particular objects in the outside world or particular places.

So the computer

on the robot can't look at its memory locations to be aware of the table leg and this is illustrating the general point that the physical events inside the computer don't define what it's computing about and i think one of the reasons people find that puzzling is that when people talk about computers they use a lot of metaphors and even the word computer is a metaphor for people and these metaphors make computers sound like people

And those are the metaphors that I think have misled us and made this sound puzzling to many of us.

Where do we go next?

Another simple example of a kind of computer is an abacus.

Now, there's an abacus.

And what happens inside an abacus, I think most people would say, is not addition.

The physical process inside an abacus is beads sliding along wires.

But we've designed the abacus so we can interpret it as an adding machine.

So in some bazaar in the Middle East or something, you see somebody doing things on an abacus, he's adding up numbers, money, basically.

Now that's the way we interpret the abacus as a computer which does additions.

But we could interpret the same physical device in many different ways.

We could have used it as a model of sheep in the meadow, or we could have used it as a model of fish on the moon.

We could have used it for fortune telling, or we could have used it for multiplying.

I'll just talk about multiplying briefly.

If you imagine those red beads as being digits in a number like 10,000, one zero zero zero, every time you move a bead over to the right hand side, you're multiplying the number on the right hand side by a factor of 10.

So in that sense, if we used abacuses that way, they would be multiplying devices, not adding devices.

So again, the physical events that happen inside the abacus don't define what computation is going on.

It's people that encode the inputs and decode the outputs.

And in terms of the previous analysis of computers from Horstmann and Stemney and company, people are called representing entities who interpret the abacus as an adding machine.

They do the encoding and they do the decode.

so the first set of consequences of this viewpoint are consequences for man-made computers and particularly artificial intelligence and the first consequence is you may have heard there's quite a popular idea around amongst philosophers and others that we might be living inside a simulation in other words there might be some grand simulation of the universe

which simulates everything, the world and people and things, and we're just existing inside it.

Now, this interpretation of computers shows that's not the case.

In other words, if you have a big simulating computer, information inside it is encoded.

Sorry, I can't quite read some of my slides because the picture's in front of it.

So the simulated things don't exist inside the computer.

They exist inside the heads.

of the people looking at the computer outputs.

And so there are no simulated people inside a computer.

So we cannot be simulated people.

A second consequence, which again is quite topical.

There's a lot of discussion about AI advancing and becoming more intelligent.

And there's the idea of AI super intelligence, which completely exceeds all human intelligence.

Again,

If you take this interpreted view of computers, that can't happen because inside an AI computer, any computer doing AI, there is only digital electronics.

There's not meaning about things.

All the intelligence in AI computer is encoded information and it takes people to decode it.

And so when we talk about AI, all the intelligence lies

not inside the machine, but in human interpretation of the outputs.

And as an example of this, Deep Blue played wonderful chess, but it only played great chess because Garry Kasparov thinks it played great chess.

Another more humble example, you may have heard the old wartime computer called ENIAC, which was very good at printing log tables.

It could print log tables much better than people could print log tables.

but it only printed great log tables because people were able to read them and say, yes, these are great log tables.

So if an AI computer does something which you might want to call superintelligence, produces outputs that nobody can understand because it's beyond all human intelligence, it's actually not intelligence at all.

It's not intelligence until humans interpret it.

It's just electronics.

Now, these two results are quite

against the tide of what a lot of people believe the way to remember them is that inside a computer there is no decoding so there's no the decoding is needed to make meaningful information and therefore people simulated people and ai super intelligence do not exist inside computers so um to go back to the analysis which

was the origin of this, in 2014 showed that computing is not just a physical process.

It's an interpretation of a physical process by something they call a representational entity, an RE here.

Now the RE is the thing that encodes the inputs to the computer and decodes the outputs.

And the important thing to know is that the information used by the RE to decode resides

in the RE itself, not inside the computer.

And so this applies.

Firstly, they applied this to manmade computers because that's their interest, but they extend it then to artificial, to biological computers.

And in this case, the representational entity, which interprets the brain as a computer is the animal's body because it's the sense organs that encode the information on the way in and his motor actions, muscles,

which decode the information on the way out.

And again, we get this result applying to brains that the physical events inside a brain don't define what it's computing about.

Okay.


SPEAKER_01:
Yeah, just a question.

I think I understand where you're coming from when you state that the body is the decoder or is where decoding of neural information is enacted.

But I think it's also a huge assumption.

And I would, for instance, ask you what you think happens

when we have a reminiscence when we recall something from memory and it's very vivid for instance or like in dreams or in imagination where we may imagine ourselves moving as a body in space but the body is not actually oh that's something we could discuss in the fine details yeah but okay well we're out of that


SPEAKER_03:
Where I'm going is that most theories of consciousness and the brain see the brain as only a computer.

And what I'm saying is if the brain is only a computer, the information inside it can't have meaning and there cannot be consciousness.

What I'm going to say is the brain is more than just a computer.

There's something else in the brain besides a computer.

And I'll come to what that is.

And that is the thing that is conscious,

when we have memories or when we make plans or imagine things or whatever else it might be.

Okay.

So that's where I'm going.

The brain is, is a computer, but it's not only a computer.

There's something else in there.

Okay.

So.


SPEAKER_05:
Thank you.

Yeah.

Um,

just...

I should start by apologising.

I have to go to a TMB meeting in a few minutes, but I think it's a fascinating argument.

I just wanted to put a question that I'm hoping you can come back to later on.

Again, acknowledging I think this is a really interesting approach and I suspect you're absolutely right.

However, at the back of my mind, I'm wondering how many of these philosophical arguments have accommodated the free energy principle.

That is just the statement that any physical process is just a process of computation in the sense of belief updating.

So that all the internal elements of your brain and a computer that can be read as a process, as opposed to a book or an abacus, which is not a process,

can be read as belief updating basically whose physics move on a statistical manifold and just by using those kinds of semantics and uh translating things like interpretation decoding observation measurement these are just processes of inference in that reading of physical processes you have meaning baked in so the whole point of the free energy principle is that the physical process

encodes or has stands in for a parameterization of beliefs about something there is an inherent intrinsic meaning that is equipped or equips all the physical processes in virtue of the free energy principle because you can distinguish the physical process from the support of the beliefs entailed by that physical process about

things that are not that physical process so that's the whole point of the markup blanket that now you have an information theory and information geometry that allows you to have beliefs about things which are just meaning so my question is how much of this philosophical work and and you know can you apply the second law of girdle's theorem um to um open systems that have a distinction between the encoding of beliefs by the

the inside about the outside.

So that would be my challenge.

Oh, I've still got two minutes left to wit it, but I'm not.

I'm going to listen to Brianna's question.


SPEAKER_03:
Thank you, Carl.

It's a very profound question, and I do come to some of it later, but I'll try to address it properly later.

Brianna?


SPEAKER_04:
Hi, thanks for having me here.

And it's a fascinating argument.

I'm curious about the comments on AI.

So AI runs on an emulated substrate.

So it's remarkably similar to how it works in human cognition.

And I wonder if we're not taking into consideration that the substrate is emulated.


SPEAKER_03:
I think the argument, I mean, AI is complicated because

It runs on digital computers, they're doing digital electronics and bits and bytes.

And they are usually simulating neural nets.

And the neural nets are something like what the brain works.

And so it's a very complicated situation.

But fundamentally, there is still at the bottom level, a lot of encoding going on.

And a lot of decoding is needed to make any sense of it.

So it

That's a question that leads in a lot of directions, but fundamentally there is always encoding and decoding is always required.


SPEAKER_04:
There is, and it would be on top of the emulated substrate, which just... Yeah, you have to decode at this level.


SPEAKER_03:
You have to decode the bits and bytes as simulating a neural net.

You have to decode the neural net as producing conclusions.

So there are

Usually in computers, there are many, many levels of virtual machine going on, and many levels of decoding you have to do to get to the meaning.

All I need is one level of decoding.

That's enough.

Sure.


SPEAKER_04:
That's my question, or my remark.


SPEAKER_03:
Sorry, I'll carry on.

So now we're coming onto brains and consciousness and we take three statements, which I think are fairly widely believed.

One is that consciousness is caused by events inside the brain.

Number two is that consciousness has meanings about external things like coffee cups or the presentation or your computer screen.

And the third point is that the brain is a neural computer and only a neural computer.

Now, these statements, I will claim, are not mutually consistent.

One of them has to be wrong.

The reason is, if the brain is only a neural computer, all the information inside is encoded and has no defined meaning.

So that information can't cause consciousness of things, consciousness with meanings inside.

And if you look at those three statements, the first two, I think, are empirical facts.

And we can come back to discuss that later.

But most people believe that consciousness is caused by events in the brain because when brains are damaged, consciousness gets interrupted.

And you can't be conscious.

Most of the things you're conscious of are something happening in the brain.

And it has meaning about external things.

I think those two are well-established facts.

So the third is an assumption.

We're only assuming that computing is the only thing that goes on in brains.

I think three is the thing that's wrong.

So that runs straight up against a philosophical assumption called computational functionalism.

Computational function is a thesis that forming the right kind of computations is sufficient and necessary and sufficient for consciousness.

And this is described recently as a mainstream, although disputed, position in the philosophy of mind.

Now, it's also called computational sufficiency.

David Chalmers called it that in an article in 2012.

And what the results of the first half of this talk imply is that that is wrong.

In other words, being a computer is not sufficient to be conscious.

Now, most theories of consciousness assume computational functionalism.

They are theories in which the brain is a computer and the brain has consciousness by virtue of being a computer.

So the implication of this viewpoint is that those theories of consciousness cannot be correct.

And that is the main implication of this talk for theories of consciousness.

So to talk about some specific theories of consciousness, most of them assume that consciousness is caused by events of the brain.

The brain is a complicated neural computer, the most complicated machine in the universe we see here.

And inside this computer, there are various computational processes taking place.

And I call these P1, P2, and so on.

And most theories of consciousness assume that one or other of these processes called P5 or P4 causes consciousness.

And so you can look at examples of modern theories of consciousness from this viewpoint.

You might think P1 is representing things in the visual field.

Theories of consciousness like Lame, like, yeah, Lame and so on, people believe those theories.

Or you might think,

As in the free energy principle, the brain is building Bayesian models of reality, and that leads to consciousness.

Or you might feel some higher order theory of consciousness that P3 is the brain reflecting on its own computations, and that causes consciousness.

Or going with Antonio Damasio, you might feel that the brain is computing an emotional response to events, and that emotional response is the heart of consciousness.

Or you might feel along with Bernard Bars and Dehaene and others that global workspace is what makes us conscious.

So making information globally available is a computational process.

Or you might feel along with IIT that you need computations linked together with some very high connectivity, which is measure five.

Or you might think that P7 is predictive coding.

All of those you're saying, some

computational process is what makes the brain conscious.

The point of this talk is that those processes, P1 or P2 or whatever they are, don't happen inside brains.

They happen in our interpretations of brains as computations.

But from the information actually inside a brain itself, the processes could be completely different processes.

They could be Q1, Q2 and so on.

And so those theories of consciousness cannot work.

And if you want a short code mnemonic for why those theories of consciousness cannot be right, it is those theories of consciousness treat the brain as a computer.

So they have encoding, but they don't have decoding.

And so they can't get consciousness has decoded meaning.

And so a theory which only has encoding, but no decoding can't get decoded meaning.

And we go back again now to the example of a floor cleaning robot, just to illustrate what that means.

Supposing in a floor cleaning robot, we can go and talk to the designer or read the design handbook and get the complete theory of how the robot computer works.

Supposing we've done that for brains, supposing we have a complete model of all the neurons in the brain, what they're doing.

And so we happen to know that a particular computational process called visual perception is implemented by certain neurons we can point at in the brain, neurons N1, N2, and so on.

Or there's another process, higher order computing, that's done by a different set of neurons.

And you can divide that set.

You can say the higher order computing about an object B

is done by process P4B, and that is done by certain neurons which we've identified called N4B and so on and so forth.

And so there we are, we've made great progress in cognitive science, understanding how the brain works.

And now we propose a theory of consciousness.

So we say the consciousness of object B in our visual field is caused by process P4B.

So what form would that theory take?

What we're saying is information physically inside the brain, including the information in neural events N3B and the particular neurons causes awareness of object B. But this theory is not possible because information inside the brain doesn't define which neurons are the group N3B.

Inside the brain is not known which are neurons N3B.

There might be any neurons.

So, and the information to decode

N3B as the process P3B doesn't exist inside the brain.

And so there's no possible theory in which consciousness arises from physical events inside the brain.

To go back to the Roomba example, the robot example, the robot has memory with a load of memory locations, one of which represents a table leg.

But the robot itself doesn't know which memory location represents the table leg.

So the robot itself cannot be conscious of the table leg.

And it's an exact parallel, I claim, between those two cases.

So I would like to pause at this point, ask people to chip in with questions, doubts, and so on.

I'll just restate the incompleteness theorem.

The physical events inside a computer don't define what it computes.

and that external information is required to interpret those events as a computation and to give them any meaning.

So a few questions for you.

Is this a correct viewpoint of computation?

Is it just a physical process?

Or does it require an external interpreter or decoder?

And do modern theories of consciousness have any decoding built into them?

Or if they don't, what does it imply for those theories?

So I will pause and anybody wants to put their hands up

come out with objections.


SPEAKER_08:
Yeah, Kenneth and anyone else?


SPEAKER_02:
Yeah, thanks, Robert.

I mean, I think, you know, I was reminded when I was looking at your slides earlier and then now again, that this was all debated

for a long time, actually, in the late 80s and early 90s, kind of famously by John Searle and Daniel Dennett.

So, Searle on one side and Dennett on the other.

You know, Searle kind of, in a way, made most of his or a big chunk of his philosophical career just going around

giving the Chinese room argument.

Absolutely, yeah.

And Dennett, I mean, in a way, Dennett's response was something, you know, it was kind of unclear, but it sort of bordered on illusionism some of the time.

Yeah, yeah.

And then sometimes a kind of incoherent position where it seemed to presuppose that you could take the intent, what Dennett called the intentional stance, which is to interpret entities as exhibiting intentionality.

So for Searle, you know, the vacuum cleaning computer robot, the vacuum cleaning robot, it's

it's uh is as if it wants to you know vacuum stuff and make a map of the room but it doesn't literally do it it just sort of has a structure that's sufficient for us to go oh it it's as if it maps the room but it doesn't literally map the room right and of course dennett wanted to um either say that all intentionality is all is as if intentionality

and and that as if intentionality meant adopting the intentional stance and interpreting an entity as if it had intentionality so then if you apply that apply that to yourself

what is your interpreting activity is it intentional or not so that's the kind of the incoherence not as if i mean i know i'm not as if yes of course dennett would say oh no you don't know that right so i yeah yeah there's that there's those i mean those questions are very


SPEAKER_03:
important questions and i would go back even further than john searle to leibniz the leibniz mill the cogs inside the mill do they know anything about the wind sort of thing sure but john searle made the chinese room argument

that said computers can't have meaning inside them.

And to my mind, nobody produced a satisfactory answer.


SPEAKER_02:
Yeah, I agree with you on that.

I've always thought that.

I think there was a lot of debate about it.

It lasted to about the mid-90s, and then people just quit talking about it.


SPEAKER_03:
Well, also David Chalmers.

Yeah.

I mean, I think David Chalmers came back to it in about the 2000s.

He said computation is a causal network of causal topology.

and that has meaning i think his answer was unsatisfactory but we could talk about that but i think basically nobody's really answered john searle and the conclusion that i'm reaching is that it agrees with john searle that you need something else besides computers going on to get consciousness


SPEAKER_02:
I see that.

And then there's also, you know, those old sort of famous results in mathematical logic.

I mean, other than girls and completeness theorems, I'm talking about the Lervenheim-Skolem theorems.

Gosh.

Where you can have, you know, non-standard models of domains where it's like just a first order, you know, model of even arithmetic doesn't really capture arithmetic, you know.


SPEAKER_03:
Yeah, sure.


SPEAKER_02:
It requires a kind of, oh, we're going to interpret it as just being this, and then I may have gotten that one wrong, but there are results like that.

And then there's the whole issue of the non-categoricity of first-order models.

Yeah.

Like second-order, but we don't have a good idea of how to compute second-order models.


SPEAKER_03:
module models to get categoricity um no no my views on athletics are unpopular i i would say mathematics i mean everybody sees mathematics as this separate abstract domain actually i think certain elementary facts of mathematics like two plus two equals four they're about numbers they're not abstract and they are experimental facts and they agree with the data when you have two things and add two things you get four things

And we've learned that.


SPEAKER_02:
Yeah, but I thought, you know, in terms... The only alternative a computationalist has here is to say something like there are objective facts that pertain to, say, what a computation, what a realized computation computes that are kind of intrinsic to it.

And that's almost exactly, yeah, that's almost a kind of Pythagorean position you would have to hold.


SPEAKER_03:
Well, I, I think that is, is straightforwardly wrong.

In other words, I can have a computation, uh, like a logical and gate.

I can have two ones make a one, but I can interpret that either as an and gate, or if I interpret, if I change around and say, no one means true and Paulson and not means true.

I can say, no, it's not an and gate.

You're not doing and it's doing all or again, in terms of bitwise operations, I can say this operation on bits in the computer is addition, or I can say, no, I haven't represented used bits represent one, two, three, four.

I've used bits to represent one, 10, a hundred, a thousand.

So the operation is actually doing is multiplication.


SPEAKER_02:
so there is no objective fact about what operation what computation is doing yeah i mean if you think that then i think you're right so i've often thought this that what the computationalist has to say is that no there are objective facts about what certain realized computational structures compute

And maybe, was Carl saying something like that about the free energy?


SPEAKER_03:
I think he was, yeah.

It's a shame because I've got slides coming up about the free energy principle and Markov blankets and so on.

We could have discussed them then.

But anyway, I'll get to discuss that with Carl later because I'm giving this talk to the TNB later.

But I think Carl was sort of saying that.

that there is an interpretation, a unique interpretation of a computation.

And I think there are just too many counter examples.

I think any computation you show me, and you show me your interpretation of it, I can show you another interpretation that's just as good.


SPEAKER_02:
Yeah.

Yeah.

Well, when you said, if I may say one last thing, when you talked about, and David mentioned this,

the sensory and motor encoding as a kind of locus of real representational content.

Is that how I... Yeah.

Sensory organs encode and muscles decode.

Okay, why couldn't somebody raise the same objection that you're raising to computation as encoding meaning to that process?

Um, because there again, you might say, well, it depends, you know, to kind of look at another

line of theory of intentionality, theory of meaning, theory of representation that went on around the same time.

There's, for example, Ruth Milliken's sort of naturalistic theory that would allow you to assign a definite content to sensory and motor encoding and decoding.

But it's all kind of based on evolutionary history.

And if the evolutionary history were different, but you had exactly the same physical inputs, outputs, and so on, you would get a different content.

And that's kind of a...

To my way of thinking, I kind of unsatisfied... It may be fine with respect to, like, what the body is objectively doing and what it evolved for, but in terms of conscious content, it seems to me it's not going to work.

It's too externalist.

Even if I had a completely different evolution, but I ended up being this kind of atom-for-atom replica of what I am now...

The contents of my thoughts don't really change.

It seems to me.


SPEAKER_03:
Well, yeah, I mean, I would say you can't throw evolution away like that, but you can't play with it like that.

I think the brain is a result of evolution and evolution has meant that since organs have evolved to encode light and touch and so on, and muscles have evolved to do things and brains evolved to make sure that they do the right things.

And all that is a coherent picture, and I cannot just pull that apart, basically.


SPEAKER_02:
So if I understand you, would you think that you can't play the same objection to the representational content of sensory encoding and motor decoding, or however you want to think of it, as you can play it?


SPEAKER_03:
Well, I think, to step back a bit, I think all the theories we have

in natural science or gas laws or whatever it might be, involve encoding and decoding.

And the theory we have of the brain involves encoding by sense organs, involves decoding by muscles.

So that is, that theory is a coherent package that we can test against data on its own.

Whereas the difficulty with consciousness is that we don't see the decoding.

Whereas we know that consciousness is about decoded information.

Yeah.

But ordinary physical theories or theories of the brain, they always have an encoding bit and a decoding bit, and they kind of balance out.

So we've got at the end of the day, predictions we can test against data and we can say this theory is right or wrong.

So that's roughly my viewpoint, but there's a lot there.

We're getting into deep water.


SPEAKER_02:
Yeah.

Thanks.


SPEAKER_08:
Robert, we've also, of course, emailed on this, but external information is required to interpret those physical events as a computation.

So I think I'm 100% with you on relational semiosis and the idea that multiple perspectives could be taken on a computational process.

But what causal relevance, other than actually intervening on the internal states, would those external states have?

those interpretations matter for the interpreter but but unless they re-enter causally into the target system how do they come to matter for the system itself well i think that's what i'm going to address on the second half of the talk um and again you know the whole picture is the computer only picture


SPEAKER_03:
is radically incomplete and you get gaps like that but there's something else comes along which fills the gap and that's what i'm going to talk about okay so uh that by the way we can come back to that but that's more discussion i thought some people might object might have that question which in a sense was carl's question that isn't every physical thing a computation

And to a certain extent, I can regard lots of physical phenomena as computation.

I can regard a gas in a cylinder as a computation.

And there, the encoding is something that gives the gas a certain temperature.

But my theory is not just gas in a cylinder.

It's gas plus a pressure piston, which measures the pressure.

And the pressure piston is what does the decoding.

So that's a bit of a diversion.

I'd like to move on from this slide because that's not what the second half talks about.

If anyone wants to come back to this slide later, I'm happy to do so.

Now, what's the positive proposal?

How can brains be conscious?

The positive proposal is the brain is a neural computer, but it's not only that.

Something else is happening.

And it has to be happening because we know brains are conscious.

And so we need something else to make them conscious.

So what might that something be?

The proposal here is that the extra something is an analog model of reality.

And that an analog model is different from what goes on in digital computers or neural computers.

So what's an analog model?

Approach this by examples.

A toy car is an analog model of a real car.

Or a map is an analog model of a piece of terrain.

Or a photograph is an analog model of some scene.

So this gives us some idea of what analog models are about.

And I claim we try and encapsulate this in definition of what is an analog model.

A model M is an analog model of some physical situation P if

M has the same topology as P. In other words, if the situation P is donut shaped, then M is donut shaped.

And there's some kind of point to point correspondence between the physical situation P and the model M. So a point in the model is a function of a point in the physical thing.

And the function F, which relates

points in model and physical reality are a continuous function.

Mathematically, it's a fiber bundle, I think.

And it preserves neighborhoods.

And it preserves spatial binding.

So there are things in the analog model, which are like borders, for instance, borders on a map, which are also borders in the real situation.

So some of the information in the model M, in the physical situation P,

is reproduced in the model M, but with no encoding.

And so there's certain information you can get out of the model M, which doesn't require decoding.

And that is, I believe, the secret to consciousness, that there is an analog model inside the brain, and that model is a source of consciousness.

That model doesn't need any decoding.

So there can be information in consciousness about external things, because it's gone into the analog model without it being encoded.

So, I should say that I've tried to discuss analog models in a couple of minutes, but they're not that simple.

Because you could say, analog computation requires external decoding, just like any other form of computation.

And that's what I believe Orsman et al.

said.

And there's been loads of philosophical debate about what analogue modelling is.

People like Gibson, Fodor and Peleshin and so on.

And you could say, well, you've given the examples of maps and photographs, but they're actually encoded.

They are divisible.

You look very closely at a map, you can divide it into little fibres on the paper and they've got ink or they haven't got ink.

And the ink is an encoding of information.

Or you look at a photographic film and there are molecules of dye.

Or you look at a computer screen, there are pixels, they're all encoded.

However, those aren't the only examples of analogue models.

There are other analogue models which aren't divisible.

For instance, if you have a lens and the optical image is a model of reality, but it is not divisible because you zoom in very closely on the image, you get to the diffraction limit and usually get blurry.

So you cannot keep dividing an image from a lens as far as you like.

Similarly, a hologram, it's not divisible because a hologram is a Fourier transform of a spatial distribution on a film, say.

And so you can't zoom in as close as you like on a hologram and keep dividing and dividing it.

And another more exotic example is a single quantum state.

That's not divisible.

Notoriously not divisible.

And there are some quantum states which can hold very large amounts of information, macroscopic amounts of information, and they last longer.

They last a long time.

Most quantum states get dissipated in 10 to the minus 18 seconds and don't last at all.

But certain quantum states can last

very long times, and these are Bose-Einstein condensates.

They're things like superconductors or superfluids.

By the way, Fran is watching this talk.

I'm running out of water, so if she could bring me some water, I'd be very grateful.

So Bose-Einstein condensates are an example of quantum states which can exist for a long time and can hold a lot of information, but they are very rare states of matter.

So they...

I think people say that couldn't exist in the brain, but I think we shouldn't rule them out because evolution is very clever.

So the question of analog representations in the brain is still, I think, an open question and it's a question of physics or biophysics rather than a philosophical mathematical question.

Right.

So what's this got to do with consciousness?

Analog models have spatial binding.

In other words, they have things like boundaries which map to boundaries in the real world.

Now, our conscious experience includes a 3D model of local space.

That's what it's like to be.

You see a model of your computer screen in front of you, a model of your table itself.

And that model has spatial... Oh, my wall's coming.

That's great.

you.

That model has spatial spatial binding and so that model could be the source of consciousness with no decoding required.

So here is the positive proposal that the brain as well as a computer contains an analog model of local space and that analog model is the sole cause of

source of consciousness isn't consciousness is nothing to do with the neurons in the brain it's all coming from that analog one so that's a positive proposal and i'll sketch a bit um so the question you might ask is well you've said there might be an analog model of space in the brain what use would that be why would it evolve would it serve any purpose would it help the brain do any real computing brains need to do and

The answer is yes, it could be very useful because it could store a Bayesian maximum likelihood model of what's going on around the animal for limited periods of time, say for a few seconds.

It could be the place where all the sense data about the animal's surroundings get integrated into a single best likelihood model.

And it could persist this information for a fraction of a second while the animal decides what to do.

It could keep this information while the animal classifies the things around it, recognizing things that they have to do things about.

It could be used for simulating and planning what the animal's going to do.

And it could be used for comparing the present situation with past situations like memory and learning.

So the answer is yes, there could be lots of uses for an analog model in the brain.

Could it happen?

There is a possible physical form for the model.

as a wave excitation, and I've written papers about this.

But I'm not going to divert into those papers now, because that is simply, for the purpose of this talk, just an existent proof, to prove that it can be done, it could be done.

If evolution could use a wave excitation in the brain to store information, then that could be where the analogue model is stored.

And just to illustrate that in terms of free energy principle and active inference,

Here is a picture of the classic active inference FEP model of the brain.

Brain is a neural computer and it has a Markov blanket.

There's the outer, what I call, there are many Markov blankets, but it has an outer Markov blanket between the brain and the rest of the world.

And the sense organs are outside the Markov blanket.

They encode information which comes through the Markov blanket.

The brain does computation and then

neural spike trains come out of the market blanket and they go to muscles, which turn into physical action.

So there is encoding and decoding in this computer model of the brain, but there's no decoding inside the brain.

So active inference is all about this cycle here, where the muscle actions produce all sorts of changes in the world and they come back via sense organs.

So active inference and the FEP is a very complete and capable model of the brain, but

I would claim it cannot give you consciousness because all the information in here is encoded and there's nothing inside the brain that does the decoding.

Decoding is done on the way out.

So how would active inference and the FEP have to be modified to take account of consciousness?

it's as in this diagram here, you have not only an outer Markov, you have the same senses and muscles in the outer Markov blanket as you had before, but also as well as the computer in the brain, the gray computer we've got here, we've got the inner ring, the inner right white ring, which is the analog model of local space.

So what happens is the sense data comes in as encoded the brain, and then it gets decoded again into this analog model.

and from the analog model it gets encoded, and there's a rapid iteration of comparing sense data with the model.

So that's the way the best possible model of local space gets developed inside the brain.

And similarly, if the animal wants to plan things, then it extracts information from the analog model about the current situation, it does some computation about it, and it puts back

what the situation would be if the animal made certain moves.

So if the animal wants to plan certain movements, this green cycle here is what happens.

And so in this theory of enhanced accurate inference, if you like, there is an internal analog model as well as the computer and consciousness arises solely from that internal analog model.

The advantage of that is that if you get consciousness from the analog model, you don't need to decode it.

And the analog model is just like what consciousness is just like.

So there's no decoding involved and we've solved the problem, which we had with purely neural computer models of the brain.

Okay.

So that I think is a future direction, which I would hope active inference would take in order to produce a decent account of consciousness.

And I think that's a promising route.

So just to talk briefly about the future for theories of consciousness, I think some approaches, most approaches to theories of consciousness have this brain as a computer only assumption, and therefore they can't actually account for consciousness because they don't have decoding.

But I think some of the theories do have some promising elements in them.

I think the global workspace theory there,

It's compatible with the idea that the global workspace could contain an analog model of space somewhere in the thalamocortical system.

Certain people have written, Bjorn Merke has written about consciousness without a cortical system.

Again, this is compatible with the idea that somewhere in the midbrain, possibly the thalamus, there is an analog model.

I'll talk briefly on the projective consciousness model, which some of you know very well.

And that says there is a model of space in the brain, which is a projective transform model.

Although the PCM doesn't talk about analog at all, it does talk of projective transform model.

It has many properties in common with our consciousness.

And I think that is very much a step in the right direction.

And I've got a more detailed working out of this.

I call the projective wave model that says there is an analog of space in the brain it's in the thalamus it's in a wave excitation or in insects it's in the in the central body and that's the basis of consciousness but that is not really the main point of this talk is to talk about the future is to say we need to go in some different direction because the ordinary computational theories of consciousness are not going to hack it

So that is really the end of what I have to say.

After 35, how long?

No, an hour.

So open for discussion, here are a few questions.

Is there any alternative to having an analogue model in the brain?

Is there some other thing that could be the cause of consciousness?

Could it be stored in neurons or does it have to be stored in some other form such as a wave?

And

I think this has not only implications for consciousness, but also implication for cognitive models of the brain.

If we admit this possibility, does that actually help us produce models of how the brain works?

Or alternatively, should we just give up on theories of consciousness?

So, open for questions.


SPEAKER_01:
May I jump in?

Sure.

so one thing on just a second i'm going to close my door because there's people here all right so one thing to bear in mind with respect to this discussion about the brain in particular

is that for most neurophysiologists brain scientists not cognitive brain scientists as you know who's often so are not real neuroscientists uh i've said and restated when after computers and in their

digital form so with this encoding and decoding process involved have emerged that the brain is not a computer in the sense that precisely the brain is an analog machine that neurons do not encode things that actually spike strains are analog signals so it's actually the core belief of the classical neurophysiology

and computer sorry brain scientists that the brain is not digital but an analog machine that has analog model of the world now in the detail how that would work and how that would generate phenomenology at this as the subjective experience we have is i guess another question um so i think that is very what you are stating is actually very classical

from the perspective of neuroscience.

But it has been forgotten by a lot of people who are in the business of cognitive neuroscience.

And myself, at some point, I thought, we have to go back to the analogy between mind and brain and software and hardware.

So un-tailing some sort of encoding, decoding through virtualization.

But that's the theory of mind.

I don't know what I believe today, but actually what you say is about, you know, can the brain be considered or part of the brain or neurons can encode an analog model?

For most neurophysiologists, that's already the case.

How is another question.

So now in terms of the particular model and implementation or form of this analog model,

I would even go further and say at this point in my career and reflection where I'm actually completely lost, I'm open to anything, including what I used to consider as sort of science, like the claims of the Chalmers about quantum mechanics.

Why not?

And even some other physics that we don't even know yet and haven't understood in terms of, you know, the brain, maybe like a

I'm going to use an analogy like an antenna of sorts receiver and emitter that connects to physics in which you know consciousness resides that we may never understood.

So the field is open for me and I'm not shocked by your arguments.

I think it's worthwhile to pursue but I don't have a definite answer neither of

what it could be, this analog model, or even if there is not a physics in which encoding and decoding can perfectly make sense.


SPEAKER_03:
Okay, I've got two remarks, I suppose.

Well, let me remember one of them.

The idea that the brain is an analog computer and neuron spike trains are an analog representation of reality.

I don't think that... To me, spike trains are irretrievably encoded, and we don't know how they're encoded.

We don't know.

Certain neurons, pyramidal neurons, may be firing rate

is the way to look at it.

Even looking at firing rates is decoding.


SPEAKER_01:
But nobody in neurophysiology looks at firing rate in a computational sense of the term, as computational neuroscientists do.

They just consider it's an analog signal that

as an analog relationship with the amount of neurotransmitters that are going to be released and how that's going to affect, based on the biochemistry of the synaptic cleft, the potential of membrane on other neurons.

So for them, it's really like an electronic machine of a classical analog, you know, there is no analog digital converter, if you wish, in the sense of an analogue signal.

For them, it's all analog.


SPEAKER_03:
I suppose the other problem I have with that is you can look at certain neurons in the brain and it appears that they encode things or they represent things by firing rate.


SPEAKER_01:
You could say that is analogue, but actually... I'm sorry to cut you Robert's enthusiasm of the discussion, but those are models that have been extracted and designed by people who started thinking computationally about the brain.

and we had a fairly superficial culture about neurons and electrophysiology, I would say, for many of them, and that decided to interpret spike trains and their frequency or the rate of spiking as a signal that in itself was the meaningful signals in their simplified model of neurophysiology

to be able to compute certain functions like doing arithmetics or understand certain basic memory mechanisms and input-output relationships through transfer function.

But that's a small, very small part of neuroscience.

That's the computational neuroscientists that we're trying to do modeling and that interpreted spike trend as a relevant parameter.

But a true neurophysiologist would never think that way.


SPEAKER_03:
Do you mean experimental neurophysiologists?


SPEAKER_01:
Or even theoretical, I mean an electrophysiologist who masters the biophysics of membranes, of active conduction.

Oh, sure, sure.

That's a physical theory.


SPEAKER_03:
They think that... Yeah, I'm talking about people who worry about consciousness.


SPEAKER_01:
Yeah, but they are usually not real brain scientists.

That's what I'm saying.


SPEAKER_03:
Yeah, okay.


SPEAKER_02:
uh kenneth and then paul or anyone else who hasn't spoken ben on mute kenneth sorry just to uh make a quick remark uh with relation to what david just said i mean i think mcculloch and and pitts knew their neurophysiology pretty well and they knew they were making a massive simplification

and making a choice and kind of setting up neural network modeling as computational.

But I just want to say, and it's also kind of following up with what David says, well, I have two questions, really.

So, one is, suppose there is this analog model.

I mean, can't we raise the same sort of objections to computational models that we, I mean, couldn't we raise to analog models the same sort of objections we're raising to computational models?

So, for example, if you take analog computers like the differential engine, you know, Charles Babbage's

Wasn't it called the differential engine?

Yeah, that was later than Babbage.


SPEAKER_03:
Well, he did the differential engine, and there was a later one in the 30s.


SPEAKER_02:
Okay, so it's using gears, if I'm not mistaken, to do that.

Yeah, sure.

You mentioned the abacus.

I mean, it's an analog device, and someone might kind of push and go, well, really...

Digital computers are all implemented in an analog world, so really they're analog.

I know there's the flip side where everybody's trying to turn everything analog into something digital, like information ontologies or it from bit.


SPEAKER_03:
To me, the key distinguishing factor is that some of the information in an analog model

Doesn't need decoding.


SPEAKER_02:
Okay.

And I can... I like that idea because I think that there's... And that leads me to my second question.

I like the idea because I'm one of those who thinks that consciousness is not exhausted by intentionality.

And what I mean by that is...

Yes, there's the intentional level where you think, like, you're seeing something as something, but there's also the level at which you're just seeing, if we're talking about 50, for example.

So, for example, if you... you might wake up in the morning and have, like, a wadded sheet in front of your face, and for a second you don't know what it is.

But there's before you interpret before you interpret it.

Right.

So there's still something you're sort of directly aware of that doesn't require any decoding to be aware of it.

And then there's this further layer of decoding, which is like, oh, yeah, sheet or oh, it's kind of perceptual.

And that's where intentionality or meaning in the sense we've been talking about it enters in now.

OK, so I like that idea.

uh that an analog model might give you that layer that isn't it's information that doesn't have to be decoded because it's really sort of just built into the structure of that episode or that phase of consciousness itself but

My question for you really is, what is it to decode or interpret or intentionally animate, to use a Husserlian language?

What is it to see something as something?

That's like a further process.

You can imagine, in theory anyway, a kind of consciousness that doesn't do it, that is basically just kind of locked in this...

you know, it doesn't really sort of get outside of itself.

Yeah.


SPEAKER_03:
That's a complicated question.


SPEAKER_02:
Your arguments earlier on kind of depend on consciousness always involving intentionality or interpretation, which, you know, is certainly a tempting thought that it always does.

I can't say that it's false, but I wonder, like, what is the theory of interpretation or the theory of intentionality?

And do you need one?


SPEAKER_03:
your for your project um well this um i would say in that moment you wake up before you've interpreted your sense data as this is a sheet um there are things in your sense stage like a boundary between red and green yeah

you know, and you're, you're aware of that boundary before you say, Oh, that's, it's about my sheet.

Yeah.

And so there is a pre interpretation, experience, raw experience.

And that quality of the raw, the what it is like, bit is what, where the analog model represents

It has a nice fit to consciousness.

Analog model has got a barrier between red and green.

Your consciousness has that.

And that's for classification and intentionality.

When you see something on the wall, you say, oh, that's a fly.

That is built in by evolution.

Animals have to take their sense data and classify it into things they need to do things about.

That's a fly.

I can eat it.

Whatever.

And that secondary experience of

Taking a piece of sense state and classifying it as something is useful for the animal, but it also involves the conscious bit.

But once the animal has seen a fly as a fly, it doesn't quite see the raw sense state.

It mixes the fly up with previous flies it's seen in his memory.

And its current model of reality involves not only the sense data coming in, but all the flies it's seen before, that they're going to behave that way and be edible or whatever it may be.


SPEAKER_02:
I can agree with that, but what I wonder is, is that, you know, the seeing as or the interpretation, is that, do you also want to, you want to deny that that could be computational, correct?


SPEAKER_03:
No, no, it's computational, but it feeds results back into the model.

So you think that when I see a fly on the wall, there's computation in my brain that says that's a fly.

But then that computation feeds back into my analog model.

So my model is a bit more fly like than it was before.


SPEAKER_02:
But now you've got intentionality in your computations.

And then

We can go back and go, well, why were you objecting to computations being intentional in the first half of your talk?


SPEAKER_03:
No, my computation has evolved to classify the visual field into useful things that I need to know about.

And, you know, the computation doesn't know that's a fly.

The computation just does some grinding.

And it makes my muscles do a fly thing.

it makes my conscious awareness do a fly thing but inside it the computation are not aware of a fly oh okay okay i all right i have to think about i'm not i'm not sure i follow you completely but well my brains evolved to classify things usefully to help me survive i agree with that yeah and it that evolution hasn't made it aware of what the things are it's just

For certain inputs, it produces good outputs, which make me avoid flies, whatever it may be.

But also, it produces outputs into the analog model that says, when I see a blur on the wall, and I'm not sure what it is, I suddenly interpret that blur becomes a bit more focused as a fly, because I know there are flies on that wall.


SPEAKER_02:
Okay, so the interpretation is a consciousness, is a conscious matter?


SPEAKER_03:
Yeah.

No, the interpretation happens in the computer bit, but the result of being more fly-like than a blur on the wall is in the conscious bit.


SPEAKER_02:
Then it seems to me you're using interpretation or decoding in one sense in the first half of your talk and in another sense right now.


SPEAKER_03:
I don't think so.

I'm saying that the decoding happens on the brain has the computer brain has encoded stuff in it.

And there's two kinds of decoding.

There's decoding out the muscles and there's decoding going into the analog model.


SPEAKER_01:
At some point, if you wish, Ken, for what I understand, there is a decoding that comes from the processing of the within the brain that involved initially a form of encoding.

that remaps somehow this code into an analog model that maps the outside world in a manner that is coherent in term of shape, if you wish.

So there is a shape outside the world.

You don't see it directly.

You encode it through neural process.

And somewhere, somehow, or some mechanisms to be defined, it's decoded within to generate and synthesize an analog

experience of the world for which, even though there is an indirect process, there is a one-to-one of sorts mapping or some mapping that is more analogous in the sense of continuous deformation, if I understand.


SPEAKER_02:
Yeah.

But this is the same kind of decoding that can happen when you ask ChatGPT a question and it spits out an answer, right?


SPEAKER_03:
No, it just does that at times.


SPEAKER_02:
What's that?

ChatGPT just does electronics, nothing else.

Yeah, I got you, but we're talking about the... you want to say there is still a computational sense of encoding and decoding, yes?

But earlier in the talk it was about there's really no computation without some external observer sort of attributing or interpreting a computation.

I'm totally fine with your model.


SPEAKER_01:
And that's the role of the analog model, if I understand well.

Yeah, that's what I'm trying to get at.


SPEAKER_02:
So there's sensory inputs, they get encoded, and then there's a decoding step from the sensory inputs into the analog model.

That's right.

And the analog model is like the conscious episode, and it's what we might call the conscious interpretation.


SPEAKER_03:
That's right, yeah.

I'll go back to this diagram if you like.

I'll just come back to... Is that diagram showing?


SPEAKER_02:
Yeah.

But earlier you said decoding was going to be the same as interpretation.

And so what I'm getting at is like the D, like for your set, your outer D to muscles, is that really interpretation?


SPEAKER_01:
They are decoding spike trains.


SPEAKER_03:
They are interpreting spike trains as physical movements.


SPEAKER_01:
From the outset, Robert said, if you only take the computer model, which in the digital world excludes an analog model in this rationale, then you cannot get an interpreter that is inside the brain.

It's always some human or some other type of interpreter outside.

now what is introducing is a new mechanism that is still to be uncovered in terms of its physics or its whatever in which a analog model is actually working within the brain and therefore is not digitally operating as in a computer so there is a mixed architecture between a digital and analog system in the brain yeah so to speak


SPEAKER_02:
yes but but just it just to it's it seems to me like your your d and the red box that process is not it's the the coding process is not itself an intentional interpretation but it results in one yeah uh it results in one yeah it's creating the analog model okay uh benjamin and then paul and then anyone else who wants to add more


SPEAKER_08:
Unmute, Benjamin.

Benjamin, unmute.


SPEAKER_07:
Apologies.

If I understand correctly, and I'm sure you can clarify this for me, there are, according to this projective wave theory of consciousness, there are certain things that are built in or baked into the system, such as time pressure or time perception,

And, uh, are there other things besides, you know, uh, time and space that are, would be necessary for this, uh, for consciousness consciousness to emerge?

Or, uh, is it just that with, you know, the effects of entropy?


SPEAKER_03:
I would put the emphasis on space.

The animal model is a model of space in the present moment.

And I think for most animals, the present moment is all that exists.

they don't have a sense of chronological time and they don't worry about time.

They're just worrying about what they're going to do right now.

And so that's, for that they need analog model of space.

So I would put much more emphasis on space than time.

I think chronological time is a recent human thing that came along with language.


SPEAKER_08:
Great.

Thank you.

Brianna, then Paul or anyone else.


SPEAKER_04:
Hi.

So what if I look at something and I can't quite comprehend it?

Would that mean I'm not conscious of... I'm not currently conscious?

What if an animal can't comprehend something or anything, for that matter?


SPEAKER_03:
Sorry, I didn't catch all the questions.


SPEAKER_04:
Try again.

So the ability to extract meaning in an analog model is what you would say is required for consciousness, then?


SPEAKER_03:
Yeah.


SPEAKER_04:
So what if I'm unable to extract meaning about something?

Am I not conscious in that moment?


SPEAKER_03:
No.

I think you were asking about animals.

I think we've got a perfectly good theory of animals.

Animal brains absorb sense data and classify sense data into situations they need to do things about and do things about those situations.

Now that theory doesn't tell us how animal brains are conscious.

So I would decouple the two issues of how a brain does its practical job of helping the animal or the person survive versus how a brain actually creates consciousness.

Now we don't actually know whether animals are conscious, but I would guess that they are.


SPEAKER_04:
So if they're unable to make the classification of, for example, the sheep from the earlier analogy, then they would not be conscious.


SPEAKER_03:
Well, I think the two questions are independent.

The animal brain can make the classification that it has to, but it can do that without being conscious of it.


SPEAKER_04:
Okay.


SPEAKER_01:
Perhaps, Brianna, you can think of an example that comes from neuropsychology.

Think about a person who gets dementia or some other very debilitating neurological disease.

and for whom at some point everything is a confused world, which is very difficult to interpret, very disorienting, very minimalistic in terms of the content, the quantity of information that a person can integrate, but nevertheless the person still is trapped into this confused and disorienting experience.

So there might be consciousness, but not necessarily very informative in the sense of a coupling, adaptive coupling with the world, so to speak.

Would you agree with that, Robert?


SPEAKER_03:
I agree that if you take this viewpoint,

you can bounce it off all sorts of situations like that, the sort of person who's terminally confused and so on.

And I think you keep getting insights as to how the picture works in those cases.

And it takes to me quite a lot of sort of reflecting about what this model of the brain actually means and how it

how it affects how we interpret various strange conditions like that, yeah.

I agree, it's instructive to do, but I don't have all the answers to what comes out of asking those questions.


SPEAKER_04:
So the patient with dementia would be less conscious than someone without it is?

What we would denote from that?


SPEAKER_03:
No, a person with dementia might still be very conscious of the room around them.

But they're missing the meaning.

But in a really instantaneous way, rather than like,

an animal has no sense of the future or the past or causality or whatever.

It could be just conscious of sensations in the present moment.


SPEAKER_01:
Yeah.

And for me, you know, Brianna, what you say is somewhat what would the

integrated information theory with claims that you need to have some sort of complexity and efficiently integrated information that is both rich and synthetic to get a high quantity of consciousness.

But to me, the core of consciousness is not that.

And I worked a lot with patients a long time ago in neuropsychology.

And I think you can be acutely conscious while being totally confused.

It can be a very intense conscious experience to be totally confused.

I'll change to that.

It's not informative in the sense of adaptive coupling.


SPEAKER_04:
So then meaning is not required for consciousness.


SPEAKER_01:
Or maybe a bit of meaning, but at least some... And again, that begs the question of what we call meaning.

It's a complicated question.

But at least the structured, adaptive, highly informative contents of consciousness, when the brain is super efficient and well coupled with its environment,

might not be what really is the most fundamental aspect of consciousness.

It often goes with it.

It's very useful to have it.

But I remember a patient is a very amnesic patient with anterograd amnesia that were very, very bad or dementia patient.

that really really really were confused but we are living very intense and distressing experience and i would say that even though they could not necessarily have a very profound recursive thinking about their own consciousness and how it is perceived by others etc even though they they would have a very hard time to navigate even their immediate environment their experience was intense


SPEAKER_04:
Yes, I definitely agree with your interpretation.

Fascinating.

Can I chip in?


SPEAKER_06:
Is it here?

Hi, Paul.

Hi, Robert.

I thought you'd be disappointed if I didn't say something.

So here we go.

I'm also a little disappointed you didn't credit me with the Roomba example.

didn't I didn't what didn't credit me with the Roomba I didn't get in that the rumor example is due to Paul um so first of all just to say that I I'm a lay person in all of the aspects of um of this discussion except for the fact I'm a professional human being um and uh Robert and I often engage in uh philosophical and um computational discussions

around the area, this area.

And being a layperson, I can pose some questions, but I may not understand your answers, so I'm going to say that right from the outset.

Nor may I. But Robert's requirement for encoding and decoding

relies on the requirement for an extra layer of significance a meaning to use the word that he has given it um and my understanding is that those requirements for extra significance and meaning come from the fact that we know we are conscious that we have consciousness

So Robert and I, in one of our discussions, came to a bit of an impasse where he said, well, I know I can take the rim bar apart and I can see how it works.

And it's clearly just a matter of physics and electronics.

And so we have that evidence and then we have the evidence that we are conscious.

Then he said, at least I'm conscious.

Do you not know that you are, Paul?

And I

I don't know.

I don't know I'm conscious in the sense that there is a clear category difference between what I experience, what I feel, if you like, and what I can imagine an advanced AI, self-evolving,

computer with sensors actuators etc to have i cannot hand on heart so that i know where that dividing line comes so i would just like to posit that there is no meaning beyond what our brains our neural brains what their currency is what their conclude what the conclusion of our neural brains is

There is no meaning beyond that because both the advanced Roomba or ourselves, we attach adequate meaning to what we perceive to inhabit the environment we're in.

So I would question why there is a divide that requires encoding and decoding to happen.

I think in a nutshell, that's the essence of the argument, which I've rehearsed with Robert before.


SPEAKER_03:
Yeah.

Let me try and approach that.

I mean, I'm concerned with doing science.

In order to do science, I need experimental data.

And if I'm going to do science about consciousness, I need experimental data about consciousness.

And I have got data, I claim, that tells me I am conscious.

But I haven't got data that tells me the Roomba is conscious.

So the question of whether or not a Roomba is conscious, I don't feel it's justified to assume it is conscious in anything it is like to be a Roomba.

I just don't see that's a problem for my science if it's solely trying to account for consciousness of things with brains, biological things.

So in a sense, the question of whether the rumour is conscious or not is of less interest to me just because I don't have data about it.

But I think I do have data about me.


SPEAKER_06:
But that's not the challenge I'm putting to you, Robert.


SPEAKER_03:
No, you're totally wrong.


SPEAKER_06:
Well, the challenge...

I'm putting to you in essence is whether there is a distinction between the sensory experience, ability to operate in this environment that are, well, it's a Roomba and beyond sort of advanced AI, self-evolving, possibly self-replicating with improvements, robot, between that and what we are.

So I'm challenging the notion that humans have some categorical distinction, sort of self-awareness, awareness of our environment, which we call consciousness, which I have a real problem with that word, which is different to that of an advanced

piece of electronics and computing.

So I'm not challenging you to say whether a Roomba is conscious or not.

I'm challenging you to identify a distinction between the two, because if there is no distinction between the two... The only distinction is I've got data about me.


SPEAKER_03:
I don't have data about a Roomba.

And I'm doing science, so I need data.


SPEAKER_06:
But the data you've ascribed to yourself is consciousness that is distinct from a machine's.


SPEAKER_03:
I have data that says there is something that is like to be me.

This is the Thomas Nagel sense of consciousness that is what it is like.

And internally in my brain, in my life, there is something that is like to be alive.

I don't have that evidence about a Roomba.

So a Roomba doesn't worry me.

Because I'm trying to do science, not just philosophy.

I'm using philosophy for science, if you like.

And for my scientific purposes, I need data.

And the only data on consciousness that I've got is data on consciousness in people.

So I'm deliberately restricting my focus, if you like, just to do this and not that.


SPEAKER_06:
But what you're saying is that

you have is that your consciousness is putting a degree of significance on the objects which your eyes light on which is greater than that simply required to operate in your environment um i i don't i don't see yeah i mean it's a remarkable fact i i could be a zombie but i'm not right well that's what i'm trying to do in the lightest possible way


SPEAKER_03:
I mean, it's a philosophical possibility that I am a zombie, but I haven't had a first-person experience which says I'm not a zombie.

Whereas a Roomba could well be a zombie, and I think it actually is a zombie.

It doesn't bother me.

I just restrict my viewpoint to where I've got data.


SPEAKER_00:
I'm not sure that's what Paul is talking about, but... No, I think you got it wrong.


SPEAKER_03:
I'm saying my viewpoint is restricted.

I'm only trying to do science.

I'm not trying to evaluate philosophical possibilities.

I'm only trying to produce a scientific account of consciousness.

And the only consciousness I know about is what happens in biological things such as me.


SPEAKER_06:
Yeah, and I'm not making a philosophical point.

I'm simply saying that your requirement for encoding and decoding is only there because you think that there is something, some attachment of meaning and significance required to those things over and above what is simply required to operate in an environment like a Roomba.

And I don't see that.

I don't see that.

Yeah.


SPEAKER_03:
I mean, part of the confusion in these debates is that we're stuck with language and language is encoded and it's encoded very badly and ambiguously.

And we use words with very different meanings.

And so we can construct all these constructs about consciousness, which go way beyond the thing itself and which

get us down blind alleys, I think.


SPEAKER_06:
I think, you know, I have a problem with the term consciousness and it's very... It is one of those words that we're very bad at.


SPEAKER_03:
I mean, when we use words, we imagine they're kind of precise and say what we mean, but they are so imprecise.


SPEAKER_06:
It does surprise me that the scientific community would continue to use a word like consciousness rather than break it down to

its constituent parts and banning the use of the word consciousness.

It does not feel helpful.


SPEAKER_02:
That's been true.

Well, I think Brianna had a follow-up, isn't that right, to Paul's?


SPEAKER_04:
You are right.


SPEAKER_02:
Okay, go ahead.


SPEAKER_04:
So building on Paul's point, I'm actually curious whether an AI model's need to navigate its environment through interacting with us and processing tokens might lead to its own development of a subjective experience.

Could it be that such operational requirements foster an emergent intelligence that we simply can't grasp because we don't share the same kind of subjectivity?

In other words, if these models engage with the world in ways that parallel our interactions, what exactly distinguishes their processes from what we recognize as human consciousness and does this experience involve extracted meaning?


SPEAKER_03:
I don't think I have a neat answer to that, really.

I just come back to my view that anything that goes on inside an AI computer, whether it's running a robot or anything else, is not about things.

And I have consciousness which seems to me to be about things.


SPEAKER_04:
For you, I mean, you can't verify that I'm conscious or that no one else is conscious.

So why can't we verify that a machine is also conscious?

Can they extract meaning like we do from their own subjective experience?

And does that fit into your model?


SPEAKER_03:
If they are conscious, the consciousness they have is not the subject I'm studying.


SPEAKER_08:
does that help at all no probably not that that does help actually it's a fascinating conversation yep how about each person can give sort of a final round where does this take benjamin and then continue around hello uh yeah so i i


SPEAKER_07:
To answer questions about artificial intelligence and consciousness, no, I don't think artificial intelligence can be conscious, at least not distributed systems specifically.

The needing diversity complexity, these kind of things would

prevent that from occurring in any real measurable way, like in terms of subconscious or something of that nature.

Like a thousand of a thing is not diversity.

It's not complexity.

It's just one thing.

So in that regard, no, I don't think that it's possible in any way for artificial intelligence to achieve like sentient self-awareness consciousness.

But it definitely looks like it.

And I think that's pretty cool.

Certainly.


SPEAKER_04:
If it walks like a duck and it talks like a duck, it's probably a duck.


SPEAKER_01:
yeah exactly David and Kenneth well I'm gonna be a bit mischievous I agree with everybody and I don't know at this point so the everything is open to me almost not everything but I don't know


SPEAKER_02:
Yeah, I think that is the last word, actually.

Ken?

Well, I mean, I think it's perfectly conceivable that you could have an AI that passes every kind of souped-up Turing test you want, and they become our best friends.

or whatever, and they're complete zombies.

I think that's perfectly possible.

At least it's conceivable.

On the other hand, I take your argument, Robert, to be something like, look, if all you've got is computation...

So if an AI, no matter how fancy it is, no matter how behaviorally equivalent it is to a conscious being, if all it's got is computation going on, it's not conscious because we know that these computations do not carry any meaning.

They're not carrying intentionality and consciousness at least has the capacity for intentionality and meaning.

That whole argument, as I said earlier on, is going to depend on just, like, do you think there is a reasonable theory of intentionality that...

could be applied to computational systems or natural systems.

And that, of course, is an old debate, as I said earlier, that goes back to even before Searle and Dennett.

But Searle and Dennett and then people arguing about intentionality and representation after that.

And then kind of, you're right, that Chalmers kind of resuscitates it, but it's also like, it's a kind of a debate that

that went somewhat silent i think because everybody realized they have no idea what to say um and at the end of the day but i think it's it's it's good that you're bringing it back up and that you're making us think about this in light of a new idea um now that idea itself i want to ask you one sort of last detail oh let me just say in in favor of the word keeping the word consciousness

No, it's been tried before to ban the word consciousness from the philosophy of mind.

This was a very formidable attempt by none other than the nefarious Martin Heidegger.

with the word Dasein, and then of course you just end up creating more ambiguity.

So I think we got to pick some word, right?

Consciousness is fraud.

It has an interesting history, but we got to pick some word for it.

But okay, let me put that aside.

The thing about your model is this.

When you say that this analog model of space has to be topologically equivalent to...

Space?

Ambient space?

Is that right?

Physical reality, yeah.


SPEAKER_03:
If you feel a change in front of you, your muscle has a change.


SPEAKER_02:
And there's a continuous function.

right?

The function relating them must be continuous.

Does that mean that the analog model of space is itself really just a space?

It is in fact itself a kind of space?

And is it continuous?

And is it going to require that the physical realization of it involves continuity?


SPEAKER_03:
Well, in my particular theory, the analog model is a Fourier transform of a spatial distribution, which

And consciousness somehow involves a Fourier transform, but that gets technical.

But nevertheless, Fourier transforms are very deep in physics and quantum mechanics, so I think Fourier transforms having to do with consciousness is not a big stretch.

That's a way of evading the question.


SPEAKER_02:
Yeah, okay.

Because you might think, no, whatever's going on here, like our experience of space, it cannot be...


SPEAKER_03:
like Actually realized in something continuous or it can't know I think a real a miniature space inside the brain is not gonna happen Cannot that we know enough about the brains and that there's not some Miniature model of the world inside the brain.

Yeah, but I think for a transform in a wave is Is the way that can happen?

Okay


SPEAKER_08:
All right.

Thank you very much.

Thank you to the live chatters also for their vigorous commentary.

So thank you all.

Till next time.


SPEAKER_03:
All right.

Bye.


SPEAKER_07:
Many thanks.
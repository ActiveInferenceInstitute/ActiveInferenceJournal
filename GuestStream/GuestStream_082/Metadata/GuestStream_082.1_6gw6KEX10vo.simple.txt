SPEAKER_00:
hello and welcome it is april 29th 2024 we're an active guest stream 82.1 with robert warden discussing bayesian model-based cognition the requirement equation thank you robert to you for introduction and jumping into a presentation and discussion thanks for joining hi i'm robert warden and i work theoretical neurobiology group at ucl london


SPEAKER_01:
I'm going to talk in this live stream about an equation called the requirement equation, which has a lot to do with free energy principle and active inference, but it's not the same thing.

And I hope this is the first of a series of live streams talking about how we apply these ideas to 3D spatial cognition, particularly and eventually to consciousness.

But to get on with this presentation, here's a summary of the key ideas.

The main idea of this requirement equation is to ask, not ask what brains have to do, not ask how they do it.

And so this approach is different from the free energy principle and active inference, but I believe it complements it in a way I'll try to explain.

So as a statement of what brains have to do, there is this requirement equation, which is a mathematical equation.

Essentially, brains need to compute it.

or to give the same answer as if they'd computed it to give the greatest possible fitness to their owners.

So that's, it's a statement of what brains need to do.

And it's not a statement of how they need to do it.

It doesn't say anything about neurons or pre-energy minimization or anything like that.

It simply says to get as fit as possible, a brain has to do this.

And I'll describe what this is.

And it turns out this requirement equation is like Bayes' theorem.

It's very similar to Bayes' theorem,

but it has some extra terms in it.

And you'll see that.

And because it's the fittest possible brain, brains have to evolve towards the requirement equation, towards doing that thing.

And the trouble is the requirement equation itself is too expensive to compute and brains don't compute it.

So animals don't compute it, but they have to produce very similar results to that.

But we can compute the requirement equation.

We can compute it by brute force.

And this makes no assumptions about how brains do it.

And so that gives the best possible fitness.

That's a kind of yardstick against which you can calibrate any model of cognition.

And particularly, you can use it to calibrate and test free FEP and active inference models.

And I will put the viewpoint that they are really approximations to this equation.

And so we can test

active inference models by comparing their results with the requirement equation results so the first part of this talk is to derive this equation and show you how it works the second part of the talk is to show how it can be used to test and calibrate active inference models and the third part of in point six is just a few remarks about internal bayesian models okay so

To start, how do we characterize what a brain does without making assumptions about how it does it?

And the first characterization is a black box that takes sense data as an input and produces choices of actions as its output.

So that's all a brain does.

It takes a set of sense data, D, which can include vision, touch, all sorts of stuff, and then it chooses an action.

Now, we want to go a little beyond that black box model.

And we're going to what I call a grey box model, shown in the lower diagram.

And what happens is this, the brain examines, it has a set of possible actions, AI.

I is one, two, three, etc.

Now, for each of those possible actions, it calculates some real function, f of d and a. And that real function depends on both the sense data and the action it's going to choose.

So F is a kind of expected payoff for that action in these circumstances.

And then the brain compares all the possible Fs.

So here we are on the bottom left diagram.

It's computing a bunch of different Fs in parallel, and then it puts them all to a comparison, and it picks the biggest one.

It picks the best possible payoff from any action.

And so this is the gray box model.

It's assuming the brain looks at all the alternatives and chooses the best one, but it doesn't assume anything more than that.

It doesn't assume how it computes the Fs or what the Fs are.

It just assumes that it compares them all and picks the best one.

So that's kind of minimal set of assumptions about how a brain works.

Now we'll go on to put that set of assumptions inside

a real life situation where we've got probabilities and this comes in the next diagram this is a kind of meta model of how brains work life for an animal is divided with a number of different domains where it has different kinds of decisions to make and it has encounters in each domain and we're show here a picture of one encounter in one domain so we start on the left there is s which is state of affairs that's a state of the world

and it has a certain probability, a prior probability p of s. And the animal doesn't know what that state is, of course, but the state gives rise to sense data in the animal.

So these two arrows here show sense data about the external world and sense data about the animal's internal state.

So the animal has some sense state of d, which it receives with a conditional probability for any given state s, it gets sense data d with a conditional probability p of d given s.

Then it has a decision function f that I talked about in the previous slide, and it computes f of d and a for all the possible a's it might think of doing.

And then it chooses one of the actions a. And what happens when it chooses an action is the action a and the previous state s lead to a next state, which are called s prime.

And so that comes about with a certain conditional probability, again, P of S prime given S and A. And that next state has a certain value to the animal.

And that value relates in some way to its lifetime fitness.

And so that's, in principle, the computation.

And the assumption is that brains act as if they've computed F of DA to choose their actions.

And this as if will turn out to be very important.

so how do we use these probabilities in terms of an actual situation the mean payoff an animal has loads of different encounters in its life and the mean payoff is just given by this expression and so w is the average over all the states of affairs it's going to encounter weighted by their probabilities and then each state of affairs gives some sensitive d

with a certain probability, so it gets all the possible sense data that it might have.

Then it makes a decision, chooses an action A of D, and that produces a new state, S prime, with a probability P of S prime, given S and A of D. And there's a value for the new state, value for the animal of a new state.

For instance, if it's dead, the value is zero, and so on.

So this is just a straight application of probabilities to the previous diagram, and you get that formula.

And then you just rearrange the formula slightly.

What you do is you shove this first term, PS, inside the second term, and then you write it this way.

W is the sum of

functions j of d, and j of d is this thing.

And it's already beginning to look very like Bayes' theorem, you can see.

But it's got some extra stuff at the end.

It's got these values stuck in at the end.

So that is the first step towards this requirement equation.

And to understand how it's used, what I'm going to do is to choose a very simple example of an animal.

um a simple special case and the simple special case come out here is an animal that has only two actions and i think you can imagine something like a clam living in the sea it has two actions it can open up and try and eat consume food or it can shut down it can clam up and so those two actions are called action one and action two and this simple animal has very little sense stage it only has one

piece of sense data it can say detect the salinity of the water or the temperature of the water and that thing it can detect with its sense data has values between naught and one and now we consider various kinds of this simple animal firstly we consider two no-brainer animals a no-brainer is an animal that doesn't use its sense data and just makes the same choice of actions the whole time and so

The animal that is open all the time, it gets value H1 of D, and depending on D, it gets a payoff, positive, less positive, more positive, and so on.

An animal that stays closed all the time gets a payoff H2 of G. And so you can see that for each of these two no-brain animals, the average payoff is just the integral of P of D times H of D

over this interval 0 to 1.

So that's the payoff for a no-brainy animal is either W1 or W2.

But an animal can do better than that if it uses its sense data to choose which action to take.

So a brainy animal can choose the correct H that gives a bigger payoff in terms of any sense data D. So a brainy animal can ride the top of this curve.

It can go down here,

along the green curve, along the red curve, and finally along the green curve.

Now that gives the greatest possible payoff, the greatest possible average payoff in all situations.

So it's the greatest possible fitness.

So a brainy animal that can compute these two functions, H1 and H2, and can always choose the largest one,

is going to get the best possible fitness and if it gets it wrong if it's got some inaccuracy in the way it commutes h1 or h2 so these overlap points move around a little it gets less average fitness uh productly away from that best point so that's what happens in a very simple case where you've got two actions and one real piece of sense data now we're going to

generalize back again, and you can see what kind of decision function this simple animal's got to make.

The decision function it's got to calculate is basically the average fitness, and that gives it the greatest possible fitness over all situations.

So if we generalize away from this to the more general case, can you see all that slide?

I hope you can, yes.

So we generalize this case to when the animal's got more choices of action, say five actions or

more sense data, different dimensions of sense data.

And you can see when you generalize away from the simple case to the general case that the best possible decision function is given by this equation.

The decision function f is the sum over all states of the probability of the state weighted by the post theory of probability and the sum over all final states of the

probability of the final state weighted by the value of that final state.

And we could simplify that by this equation.

But that equation is basically a definition which doesn't say anything about what happens inside a brain, how it computes it.

It says if a brain chooses the same actions as if it had worked out these equations, it gets the best possible fitness.

So the requirement equation, which is here,

says what brains are required to do.

And any brain which chooses actions as if it had done all this has the best possible fitness.

So that means that brains will evolve to choose actions as if they computed this equation, which you can see is very like Bayes' theorem.

And so that's the first result that comes out of this work is that

The Bayesian cognition, the assumption that animals use Bayesian cognition is not really a hypothesis.

I believe Carl Friston has said and has been quoted as saying that it is a hypothesis.

This result implies it is not really a hypothesis.

It follows from a piece of analysis that makes no assumptions about how brains do what they do.

So I should say this derivation of the requirement equation has some similarities to

partially observable Markov decision processes and some differences, and you'll have to look at the paper to find out those.

So that, if you like, is the end of the first part of the talk, which is deriving this equation and saying that is a way of analysing what any brain is needed to do.

The second part of the talk is how you use this equation.

So all terms that you put in the requirement equation, all those probabilities,

depend on things happening outside the brain.

They depend on states of affairs in the environment.

They depend on sense data.

They depend on what's the payoff, what happens to the animal after it takes various actions.

So it's all external biology.

And you don't have to make assumptions about what's going on in the brain to compute it.

And that, I think, is a big advantage.

Now we can write down the requirement equation in lots of different domains.

And to do that, we have to model biology, which we can observe directly, rather than modeling brains, which we can't observe very well.

The trouble is, computing this requirement equation is very expensive.

It's expensive in four different ways.

Because to really compute it, you have to sum over all possible states S, you have to do the parallel computation over all possible actions A,

you have to sum over all possible subsequent states S prime.

And in some domains you have to compute the value.

You have to have branching look ahead to different future situations.

So it's very expensive and it's not tractable in animal brains.

It will be too slow to try, too expensive to compute.

So animal brains don't compute this equation.

They do much cheaper as if calculations.

Now, having said that, animals don't compute it.

We can compute it because we have digital computers and we can do the Monte Carlo integrations and we do the optimizations and so on to do all of this by brute force.

And that's what I will call brute force Bayesian computation.

And that we can compute it.

It acts as a kind of yardstick, which we can use to compare.

how well they compute their choices of action and any cognitive model, including models built in active inference.

So that I'll skip over this slide just to say you can apply this equation in all sorts of domains.

Skip.

Right.

So coming on to active inference,

FEP and AI models are not like the requirement equation in that they do make assumptions about what happens in brains.

They assume there's a generative model, which is a simplified version of the actual generative process, some simplification that the animal uses in its brain.

They use variational model fitting.

They minimize a variational free energy, minimize a KL divergence between that and minimizing a KL divergence is the process.

And they have a certain model of the fitness payoffs of actions, and they are represented as bias prior probabilities.

And the animal has a goal to minimize its .

And an active inference

It predicts how animals make the trade-off between exploration, trying to get sense data to help better future decisions, and exploitation here and now.

So there's a whole load of assumptions in there which are different from the requirement equation.

And so for any FEP or active inference model, we can test those assumptions against the requirement equation.

And this procedure to do that is here.

You first compute the FEP-AR model, which you've been doing already anyway, and you define the equivalent requirement equation.

And so you make biological assumptions about the states in the environment, about the sense data, about the

consequences of actions and so on.

And then you compute the requirement equation.

And that, as I said, you can do by brute force integration and optimization.

And then you compare the two sets of results, requirement equation and FEP results.

You compare them in terms of the lifetime fitness.

How many encounters of this kind does an animal have in its lifetime?

How much does the fitness vary between the two?

And you get a fitness gap between the requirement equation and the FEP result.

And the requirement equation is in some sense bound to come out fitter.

And so there'll be a gap.

And if that gap is significant, even a very small gap, 1%, natural selection will have improved on the FEP model.

In other words, if there's a 1% difference in fitness between two animals,

In a hundred generations, the fitter animal will come to dominate the population.

And based on that, you can use, you can refine the FEP model.

In other words, you can ask questions like, what's the sense data which makes the largest fitness gap?

What can we do to get the FEP model better for those kinds of sense data?

So I'll illustrate this process here.

Some animal has a lifetime fitness, which you measure percentages.

We compute what that is in the requirement equation.

So that's that point there.

We also compute what it is in the FEP, and those are different.

And we can also, by the way, look at no-brainer animals, which just make the same stupid decision every time, or real animals.

All four points on this graph can be computed.

And the interesting comparison is here between FEP active inference and the requirement equation.

So the question we're asking is how close is the FEP model to the optimum?

And what is the gap between the two and what difference is that going to make over, say, 100 generations of natural selection?

Now, I think this comparing with the requirement equation is

very useful thing to do for an FEP and AI models because it's a win-win test if the model comes very close to the optimum then you've confirmed it you can't do better than that but if it's not very close you can work out how to improve it you work out what sense data is not doing so well for how to make it do better for those sense data right um

Now we come on to the third part of the talk and the headline is that brains are not always Bayesian.

I know it has been stated as an assumption that brains are Bayesian.

The requirement equation tells you exactly what are the circumstances under which they are Bayesian and under what circumstances they don't need to be Bayesian.

And it turns out that in complex domains where there are complicated choices of actions,

the brain needs to build an internal model.

And the two most complex domains in that respect are two-dimensional navigation, finding a way around, and 3D spatial cognition.

And there's always been a kind of debate between in modeling people and in activist people who say the brain just reacts to events.

You can prove this way that in complex domains,

And an activist brain may do a job, but it doesn't do as good a job as a brain that makes an internal model.

And so you can prove in certain domains, mainly the complicated ones, that you need to build internal models.

You can't prove that in all domains.

In simple domains, models aren't needed.

And these are domains in which there are very few choices of action or very simple states or simple sense data.

And in these simple domains,

The as-if computation can be very simple.

It can just be a matter of some very simple neural relay and no internal model is needed.

And an example of that is the special case that I used to motivate the requirements equation where you've only got two possible actions.

You don't have to build a complete internal model of what's going on.

You simply have to know the thresholds at which you switch from one action to the other.

And this can be seen in terms of information capacity.

What comes into the brain is some Bayesian priors, which have made it evolve, and some sense data.

And it makes an internal model, or may or may not make an internal model, and then it chooses actions.

Now, if the choice of action is very small, it's a very small information content, then it doesn't need to make a big complicated internal model in order to make that simple choice.

So the information in the internal model

needs to be less than the information in the choice of actions.

And if the choice of actions is very simple, small information content, there is no point in constructing a very complicated internal model.

It's a waste of effort.

So if you regard the brain's internal models as an informational channel between sense state and actions, then you can see the information model is very necessary in complex domains, but not so necessary in simple domains.

Now, as a further comment on this, I say you should not trust the good regulator theorem.

Now, in FEP literature, the good regulator theorem is often cited as support for the idea that brains need internal models.

I'll just very briefly say I don't believe it does give them support.

When you compare it with the requirement metamodel and the good regulator theorem, which was a paper in 1970 by Conant and Ashby,

That has a couple of different metamodels of what cognition is about.

And it's based on Ashby's ideas of homeostasis.

But I think the argument they make, Conan and Ashby make from the good regulator theorem, I don't think it really holds up.

And I've just listed the reasons here.

There's the good regulator result that every good regulator of the system is a model of that system.

here's the requirement equation result in complex domains only the fittest brains computer model of the domain now the regular later model i'll assert is not a realistic model it lacks some key ingredient and it's less realistic it assumes a homeostatic picture of a brain whereas the requirement equation analysis is much more flexible and realistic

But mainly this fourth point, the regulator concept of a model is not a graduated, it's a yes, no, all or nothing concept.

And what it says is a model, how they define a model, you can have any function of the system variables can be a model of the system, even a very trivial function.

And so the concept of a model they use is a binary yes, no concept.

But the real meaning of model in everyday life and in science is a graduated.

In other words, the more complicated, the more detail there is in a model, the more it counts as a model.

So this is just my personal beef.

You should not be using the good regulator theorem to justify internal models.

Few remarks about how brains evolve because obviously this analysis depends totally on the idea that the requirement equation defines the fittest possible brain and brains evolve towards that optimum.

And as I said, even a small deficit in lifetime fitness, even a 1% deficit in lifetime fitness, over 100 generations, that gives enough selection pressure to make something start dominating the population.

But if the properties in the habitat change rapidly, brains can't catch up.

They only evolve very slowly by random mutations.

And therefore, if the properties in the habitat are changing, brains need to learn and they need to learn fast within a short animal lifetime.

And you can actually apply this requirement equation analysis to learning.

And animal learning, it turns out, is very close to the kind of learning you get out of the requirement equation.

On the other hand, if probabilities in the habitat stay constant for a very long time, brains can evolve to come very close to the optimum.

And the key place where this is the case is spatial cognition, 3D spatial cognition.

where the states of the world have to obey some very strong constraints.

They have to obey Euclidean geometry and they obey constraints of physics.

And these constraints have been true for all evolutionary time.

So animal brains have evolved to embody those constraints very precisely.

A final point is that animals we know have very precise and sensitive sense organs.

They would not have evolved if the brain couldn't make the very best use of the data from them.

So here's a plug for the next live stream.

The next live stream is applying these ideas to 3D spatial cognition.

Understanding three-dimensional local space is actually the most important thing a brain can do because it has very

huge impact on survival because it's a 3d model of local space is used to control physical movements at every moment in the day and if the model is has inaccurate mistakes in movement can cost the animal his life any time and they're also used for recognizing things in action so the selection pressure to get the internal model of 3d space good has been very strong and sustained

over 500 million years since the Cambrian explosion when animals first had limbs and precise sense data.

Even insects control their movements very well.

And consciousness shows that our own internal human internal model of space is a good precise model.

So the 3D model of space is a very good model.

And what this next live stream is going to talk about is how you build that.

how you build neural models of 3D local space.

Now, people have, there have been very few attempts to do this.

There are no working neural models of 3D spatial cognition that I know about.

One of the reasons is the architecture.

How do you, what's the neural architecture to represent three dimensions of space?

Neural sheets can represent two-dimensional space, but there's no equivalent architecture for 3D.

And the key problem is neural spatial memory.

If you represent spatial coordinates by neuron firing rates, it turns out that that memory is too imprecise and too slow.

Insects, for instance, have a tiny fraction of a second to work out what their surroundings are like.

And in that space of time, you can't get precise representation of 3D space using neuron firing.

So in the next live stream, I will introduce the proposal that 3D space is not represented by neuron firing at all, but it's represented by a wave in the brain.

And that wave is actually a projective transform of 3D space.

And that's the same transform was used in the projective consciousness model, PCM, which is an FEP-derived model of consciousness.

So that's the commercial for the next live stream.

Just to summarize on this live stream, gosh, I've been quick, 30 minutes.

There are two kinds of modeling you can do.

You can model biology or you can model the brain.

And modeling biology is a great deal easier because we can observe it.

We can observe easily what's happening to animals in their habitat.

Whereas modeling what happens inside the brain is more difficult.

There's much more hypothesis and guesswork about it.

And the requirement equation approach, which I've talked about in this talk, only requires modeling biology, only to model events outside the brain.

And you can compute this requirement equation in any domain you like.

And it defines the very best a brain can do.

So it's a kind of yardstick against which you compare actual brains and models of brains.

So you can compare the requirement equation with any active inference model.

And that I say is a win-win for the model, because if the model comes close to the optimum,

then you've confirmed it, but it doesn't come close.

You can use what you learn to work out how to improve it.

And I would offer to work with anybody who wants to make that kind of comparison with their active inference models.

Okay.

So I finished rather ahead of time.


SPEAKER_00:
Questions.

Thank you, Robert.

All right.

you for the presentation there's a lot to discuss and also people can write questions in live chat while i'm just like re-cropping everything um do you want to give any like context how did you come to study this question and how did you get to this equation and why um i got this equation about 25 years ago actually


SPEAKER_01:
um and i came back to it when i joined carl's group and got interested in the requirement in fep and so on and i realized there was a link between it and and fep and active inference and i've just been working recently to work out what the link is and this is what i think the link is um but i think it's a very basic foundation for studying the brain is to not

make hypotheses about what is how brains do what they do but to just analyze what they have to do and i think this is quite a powerful approach in all sorts of domains hope to convert one to other people to it


SPEAKER_00:
yeah okay well questions are kind of coming in i'll go over a few of my notes just to go over some of these core pieces because i think there's a lot lot to say and it's really exciting um in quantitative evolutionary population biology fitness is conceived of as like a top-down force

whereas the generative model is built up as like an agent level view from the inside like agent-based modeling with kind of a top-down pressure and that's one way to look at that kind of setting and I got the sense that I want to explore and learn more about that the requirements equation sits basically between those two

and sets the high bar that gives a requirements for the agent-based model because you can always just build an agent-based model and have a constraining force we know from simulations that that all does give um improvement of algorithms uh in simulations but it's kind of an open-ended search and there isn't necessarily a calibration


SPEAKER_01:
do you think about this is a calibration if if you are prepared to analyze an animal's lifetime in terms of a number of different encounters like each time it goes out to get food or each time it meets a potential mate or each time it comes across a predator and say what's its probability of surviving all those encounters then you can start doing the requirement equation analysis and you can find

In principle, you can find the very best it can do.

And that is an endpoint.

If you've got your FEP model and you're improving it, improving it, this is a kind of endpoint beyond which you don't expect to get, but you know when you're getting near.


SPEAKER_00:
Awesome.

And here's one other kind of angle that I was noting.

So the standalone Bayes equation.

Bayes with no additional terms is like this unified imperative for sensemaking.

It can cover discrete or continuous state spaces, single dimension, multiple dimensions.

There's even considerations for like how many parameters should be Bayes optimal, but it's all framed in terms of this unified sensemaking imperative.

requirements equation and and so then one of the key steps in active inference is to expand that unified bayesian imperative to not just the sense making but include action like a variational autoencoder with action the requirements equation says well it's a little bit more over the lifespan than just optimal sense making about sense and action

action but not sense making has fitness or survival consequences so then there's another term that applies to only actions because there's a direct fitness consequence of actions and phenotypes whereas the internal sense making is actually shielded from direct selection because it has no consequence except through the actions in the black or the gray box


SPEAKER_01:
Yeah, I mean, I think active inference is a particular way of making the trade-off between, I mean, the way I understand it, it's a bit simple, the trade-off between exploration and exploitation.

In other words, do I fly higher so I can see more prey or do I go down and actually catch that bit of prey?

And the requirement equation can encompass that because the value that an animal gets from an action

It can be direct exploitation that gets a piece of food, or it can be exploration that it learns something that it can use downstream.

So that kind of trade-off is made in active inference.

There is also the apparatus in the requirement equation of making the same trade-off and seeing if you get the same answers.


SPEAKER_00:
just one last framing and then i'll go to the live chat questions just at the purely empirical even just putting aside the evolutionary setting it's a common theme to make a model and then do a variance partitioning on the residual and find if it's loaded or associated with anything so when we're studying

bird migration and empirically 80% survive some stage, and then we're making an agent-based model.

Well, we don't want a model where 100% of them survive because we've kind of overshot how successful.

So then the empirical path could be like, okay, we just made a simple version and 50% of the simulated birds make the migration.

How is that correlated with the actual?

And then how is what's left in the residual?

Oh, it's associated with age.

Okay, now we include age as a factor in the agent-based model.

That eats up some of that variance.

How is the remaining variance associated with known or unknown factors?

And so there's an iterative process where you can discover potentially associated variables

and have that in feedback with the agent-based constructions like the agent responds to the requirements as modeled and then it looks like they're doing something else that's required that you don't have in your requirements equation so then but it sets a kind of local calibrator again to what kind of performance envelope could even be expected or relevant so that you don't have

in silico experiments where like bacteria are trying to read a book and a person is trying to do a metabolic thing that they can't do it doesn't make sense we all have a feeling like well that's not the niche it's not what they are being selected on for and so this again helps put an adapter in between the kind of bulk properties of the niche and then the specific performance


SPEAKER_01:
that gets all the way down to thinking like you mentioned about like the survival through time as being like the joint distribution of many many many many many encounters yeah I mean I I agree with all that I I think the requirement equation can be used to do those kinds of analysis when when you when you formulate it you're really looking at lifetime Fitness

And so you should be looking at all the things that influence lifetime fitness, including things that are nothing to do with brains at all, as you say, old age or whatever.

But nevertheless, the requirement equation, it gives you, I think, starting just from FEP models and AI models, you have the model on the one hand and practical observation of animals on the other hand.

And you've got nothing in between.

But requirement equation is somehow in between them that you can say, here's this requirement for what a brain needs to do.

And if a brain does it absolutely as well as that, then here's the survival.

So you've got a midpoint in the calibration between your brain model and your real life.


SPEAKER_00:
Awesome.

okay i'm gonna go to read some questions in the chat if whatever you want to say or answer or we'll just however you see okay bert asks is inactivism exclusive to internal representations i always think of it that way and activism is generally the idea that you don't need internal representations you can just sort of do something um and


SPEAKER_01:
In simple cases, you can.

In that simple clam case, the clam case only has to know what are the three thresholds at which it has to switch over from open to closed.

That's an activist.

I think an activist is usually hostile to internal representations, and they are in opposition.

And what I'm saying is the requirement equation tells you which circumstances one is right and which circumstances the other is right.


SPEAKER_00:
awesome one other free energy principle paper that this reminded me of it and I'll go back to the live chats was by yellow Brunberg in and others in 2018 the anticipating brain is not a scientist the free energy principle from an ecological inactive perspective which discusses it's kind of like the good scientist and the good engineer like it's not just the pure sense making imperative

when action is considered.

It's not just about accuracy and action because actions have consequences in a way that the sense-making component doesn't because the black or the gray box is set up so that the sense-making doesn't have consequences except through action.

That's that partitioning.

If you explored a model with telepathy, where there was direct consequence for internal states, then it would be basically as if that was an external or an action state, because it would just make those representations external facing.

So it's like, whichever states aren't external facing are the internal ones.

Whichever ones are external facing, and then those can be under, quote, no selective pressure,

they're all that's like a novelty type or just an open diffusion space or it can be under strong selection pressure which just means that there are higher consequences for a certain distribution but that's a distribution that you're actually having the agent's model converge towards and maybe maybe the populations convert maybe the optimal height would be like 15 feet and it just there's so many other trade-offs that that actually the population just continues to push up against five

but at least there was a way to say, well, it only got to a third of the value, but that's the highest score is 30%.

Yeah.


SPEAKER_01:
And that sort of approach sounds very congenial to what I'm doing.

I have to read the paper.

I'm afraid I haven't read it yet, but I certainly will do.


SPEAKER_00:
Okay.

All right.

Benjamin asks,

How does the amalgamation of Markovian dynamics and Bayesian theory reflect the necessity for brains in complex domains to construct internal models for better adaptation and decision-making?


SPEAKER_01:
How does the combination of... Markovian dynamics?


SPEAKER_00:
Yeah.

How does how you brought together... Well, I think this equation... Okay, sorry.


SPEAKER_01:
um this equation certainly builds in markovian dynamics i mean the probabilistic model of the state is a markovian model if there are certain state transitions in the world then those are modeled so i um i believe that that's encompassing this approach i don't know the word markovian changes things particularly okay


SPEAKER_00:
benjamin asked another question as the brain has evolved do we know which of these definitions of brain functions are inherent versus emergent and how does this potentially impact their functioning as they evolve collaboratively or independently um there seem to be two questions there can you read it again yes oh we have many amazing um conjugate question askers um

How do we know which of the definitions of brain functions are inherent versus emergent?

I think that's the first part.


SPEAKER_01:
Inherent versus emergent, right.


SPEAKER_00:
And then how does that impact their functioning as they evolve potentially collaboratively or independently?


SPEAKER_01:
Well, the requirement, I've tried to state the requirement in other words, what brains are required to do as

in the abstract and making no mention of how they've actually evolved to do it.

And that applies to brains at the very beginning of their evolution when they were totally unfit and they gradually got fitter by natural selection.

So inherent versus emergent.

I mean, what is emergent is brains getting closer and closer to an optimum given by that equation.

The equation

doesn't describe how brains are described where brains are evolving towards so that's the first part of the question now the second part of the question was a different contrast which was how does that potentially impact their functioning as they evolve collaboratively or independently now collaboratively or independently there's a huge amount of questions there I mean obviously um

social insects, for instance, their brains work collaboratively.

Is that the distinction we're talking about?

I mean, again, I think the requirement equation is applicable to social insects that don't have, they're not so concerned about individual survival, they're concerned about survival of the genotype of the hive, if you like.

But I think the equation still applies to that.

But collaboration and competition,


SPEAKER_00:
are very complicated areas to analyze well definitely especially next time i look forward to going very deep into the insect questions um i'll give some thoughts though i mean to the first half in inherent versus emergence without just saying it depends on how you define it it can be emergent through development that certain functions arise so that

That can be both.

That was the sense of the question.

Was it emergent in a lifetime?

I don't know what the sense of the question is.


SPEAKER_01:
Maybe I didn't get that right.

Certainly, emergent in a lifetime, yeah.

Because it's optimizing a lifetime fitness, you have to consider different periods in the lifetime and whether the brain gets fit in adulthood by having learnt

along the way or having inherent innate traits is a choice that brains have but as I said the requirement equation can be applied to learning and that's a very interesting application I haven't done talked about it at all here but it can be and there's a result a book from Anderson in 1990

which shows that data on animal conditioning, the sort of rats and pigeons in cages and so on, agrees very well with the Bayesian model, the requirements equation model of what the best possible learning is.

So that's another domain in which the Bayesian assumptions are confirmed.


SPEAKER_00:
interesting this is making me wonder about abiotic constraints and biotic constraints for the requirements equation so maybe one setting is that the external environment of the organism is very harsh like it's a desert ant colony

and so a lot of the selective pressure and the differences amongst colonies like in their persistence or their survival is related to their performance against the environment and there's another setting where maybe the environment is materially abundant but then there's a lot of

dynamics like game theory and sexual selection and like these internal so how does the requirements equation I mean if we have to change that as often as the weather changes then what are we really getting by just setting this local requirement which itself could be changing all the time or variable across settings that's a great question I mean in principle the requirement equation


SPEAKER_01:
takes as its starting point natural selection, not sexual selection.

But sexual selection, as you know, doesn't always make animals fitter, and sometimes makes them less fit, as in the peacock's tail.

And I've got a whole separate interest in how sexual selection is acted on the human brain, particularly, and in terms of social and sexual competition.

And that, I think,

The requirement equation is not a result of that.

The requirement equation is what you get after natural selection.


SPEAKER_00:
What do you mean after natural selection?


SPEAKER_01:
Well, natural selection, you have to have the fittest possible animal.

is one that survives to adulthood with the greatest probability and gets a mate with the greatest probability getting a mate with the greatest probability is not as simple as surviving the greatest probability because it's a competitive activity and you get positive feedback effects for instance as in classic peacock case where female peacocks prefer males with long tails and

And so that gets baked into the genotype by a kind of conspiracy between female genes preferring long tails and male genes having long tails.

But it's not good for the peacock species in terms of fitness.

Male peacocks are less fit because they've got big tails.

And you could have similar things like that happening inside the brain.

And I have a whole separate set of ideas which I would love to give a talk about sometime.

that natural that human language is actually a consequence of sexual selection um perhaps we could live stream that sometime but human language hasn't made us more fit in natural habitats there's some controversy for you wow to return to one other potentially interesting um aspect of of benjamin's question there with the second half


SPEAKER_00:
collaborative or independent uh challenges so this made me think about how the factorization of the problem to be solved like the sparsity of what depends on what in the problem may be very disjoint from what is connected to what in the organism so that's kind of widely been recognized by like evolutionary constraints or just evolutionary or tray architectures like if there's

yeah if there's a covariance in the niche um between two factors and then that is not the case in the organism then there can be like separated movement of those traits or you can have the opposite setting where two things are i mean there's also pieces that are correlated just like having length and volume like having a long body these other things change


SPEAKER_01:
I would guess that in terms of physical evolution of bodies, you can get two traits that have to be correlated, like long legs inevitably correlated with something else.

But in terms of evolution of the brain, it's not so constrained and you can have different cognitive functions that evolve independent of each other, much more freedom there.

That's just a guess.


SPEAKER_00:
yeah that's very interesting and the kind of there's a lot to say about that and also with um very fruitful to consider the differences with the mammal nervous system and with the insect nervous system and the ways in which the insect nervous system helps clarify certain sensory mappings that get very lost in the large mammal brain yeah


SPEAKER_01:
I agree insects are very worth looking at.


SPEAKER_00:
Okay, so anyone else can write questions in the live chat.

But I mean, what other directions would you like to explore?

How do you think this moves forward given where it is now?


SPEAKER_01:
Well, as I say, I'd be very interested if anybody's interested in taking an active inference model.

and trying to calibrate it this way, I'd be very happy to work with them too.

I would do the requirement equation side of things, and that would be a very interesting comparison for me to make.

But, say we're going to make the next live stream in a month's time, and that'd be about spatial cognition.

That's the main direction I want to take this.


SPEAKER_00:
yes moving things animals with spatialized nervous systems and then also nervous systems that enable movement but don't necessarily need to be spatialized like a worm may just have a topological or a tactile mapping but it may not need to have the kinds of echolocation or center place foraging components of a bat or a bee

i agree i agree um am i allowed to play with robert's equation is that okay of course you are of course you are okay um all right so this is very cool it sounds like in around one month we will have a session on insect brain and foraging spatial cognition and then there's probably many other

things we can continue to talk about.

And also, it would be awesome if somebody has a active inference model or some other kind of model that they could reach out to you and try to get some more empirical and specific.

Have you applied this to any specific like empirical or published cases yet so people could see like what the kind of analysis is?


SPEAKER_01:
Well, the main analysis is coming in the second live stream.

Can I just move to the slide that shows where you can get out of the paper?


SPEAKER_00:
Yes.


SPEAKER_01:
So there is the link to get hold of the papers.


SPEAKER_00:
Yeah.

And there's an application, which we'll look at next time as well.

But there's a very interesting video game slash

executable simulation baselanguage.org baselanguage.org cool all right thank you very much robert thanks everyone for the comments the questions and chat all right okay great thanks farewell bye bye
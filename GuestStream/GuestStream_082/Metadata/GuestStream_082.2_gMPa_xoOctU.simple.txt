SPEAKER_00:
Hello and welcome.

It is June 3rd, 2024.

We're in Active Inference Guest Stream 82.2 with Robert Warden.

Today we're going to be discussing three-dimensional spatial cognition, bees and bats.

So thank you, Robert, for joining again.

To you for the presentation and looking forward to it.


SPEAKER_01:
Thanks, Dan.

Okay, well, as Dan said, I'm talking about three-dimensional spatial cognition in small animals, particularly like bees and bats, for example.

And what I'm going to be doing is showing you a demonstration program that does this.

So you can find the demonstration program at this link at the bottom of the picture, and you can download it and try it yourself.

Or you can read a couple of papers about this work, which are there on archive at that address.

So that is the introduction.

To get straight into it, what this work represents, I think, is a challenge for classical neuroscience.

And by classical neuroscience, I mean the assumption that all that happens in the brain, all cognition is done by neurons connecting to each other by synapses and so on.

And the challenge, which I think comes out of this work,

is that the main result is that neurons are actually not capable of that.

They cannot represent three-dimensional space because they're too imprecise and too slow.

So the resulting challenge for neuroscience is to show that this idea is wrong.

Everybody thinks it's wrong.

Everybody thinks neurons do everything.

So you have to show it's wrong by building a working model of neural computation model of 3D space and checking that it really scales and can perform

somewhat like animals perform.

I think just writing papers and talking about it is not enough.

You've actually got to build a model and show it works.

And for building that model, the FEP neural process model is the best starting point, I believe.

So that's where this talk is going.

It gets there by 3D spatial cognition.

So what is that?

Spatial cognition, I should first say, is a very important problem.

The primary task of any animal brain is to control its movements, physical movements, its limbs, in 3D.

And that's a 3D problem.

And it has to do that at all times of the day.

And for most animals, most of their brain is devoted to this problem.

And we believe, being Bayesians, that they do this by building and using Bayesian maximum likelihood model of three-dimensional space

And my previous live stream was about the subject, how animals build models in general.

But the particular 3D model of space, that's been important since the Cambrian era, when animals first started having precise sense data, like good eyes and capable limbs.

And there's been huge and sustained selection pressure on all animal species since then to do it well.

And what we believe is that animals do do it rather well.

For instance, our own conscious awareness of 3D space must come from something going on in our brains.

And that is a rather precise model in our conscious awareness of 3D space around us.

So that must be quite a good model of space in our heads.

And going from us to small insects, even a small insect can land very skillfully on the room of a coffee cup or any other surface.

So that's why I say modeling 3D spatial cognition is the top priority for neuroscience.

And we may look at all sorts of problems in neuroscience.

This is the one, this is the heart of the problem, the thing we really need to get right.

So how do you do it?

How do animals build a 3D model of local space around them?

And the immediate problem is that most of their sense data and their vision

is two-dimensional.

So how do you get from two-dimensional vision to three-dimensional model of space?

There are some constraints, and people obviously think of stereopsis, where two eyes where you can tell the depth of things, or proprioception and touch.

But those constraints only apply to restricted regions of the space around an animal.

So for the rest of it,

What I believe animals do is they build a model of space around them by moving in space.

And this is a form of active inference, if you like, that you have to move in space to find out about space.

And this is based on a very strong Bayesian probability that as an animal's move, most of the things around it do not move.

So the world is like a big rigid moving body around the animal.

So the animal can compute object locations by what is called structure from motion, SFM.

And when you build a computational model of this, it's actually a fairly simple computation to do.

What you can do is fairly simple 3D matrix operations to maximize the likelihood of an object being in a certain position.

And that's what this program does.

But if you're doing that, shape from motion, structure from motion,

It requires a short-term spatial memory, a working memory, positions of things.

And that's what this demonstration program does.

So the demonstration program, bees on without vision, with some limited, not very bad resolution, bats have echo location instead.

That gives them both the echo delay,

And it gives them a Doppler shift.

And we'll talk about that later.

So both of those animals move fast among static objects.

So this 3D shape from motion is an applicable way of working out where the objects are.

So the program I'm going to show you builds a 3D model of space in three different ways.

Firstly, it can build an optimal Bayesian shape from model to model.

And that is what I call a brute force calculation.

I've discussed this in my previous live stream, and it's very hard for animals to do.

It's not the way we think animals do it, but the interesting thing about it is it gives you the very best possible Bayesian model based on the sense data.

So that's the first way.

The second way is by dynamical object tracking, where an animal makes an estimate of where each object is in three dimensions, and then it keeps updating that estimate every time from every

step along the track it takes, it updates that estimate from its sense data.

And the third model is doing that same object tracking, but doing it in the presence of neural memory noise.

I should say this computational model is built at Mars level, David Mars level two.

That is, it's not a neural implementation, but I have made it so that you can add simulated neural noise to it.

So if you want to see this program after you've seen it demonstrated, download it from this web address, and you can quite simply unzip it and start it running.

So I will now switch to demonstrating this program.

So I end the show.

And there is the program.

That's what it looks like.

And what you see is three different windows, left, center, and right.

the left-hand window is going to be a three-dimensional view of some space in which a bee or a bat is moving.

The center view is always just healthy information, and it tries to tell you how to use all these sliders and buttons and controls.

And the right-hand view is just various graphs, and we'll see some of those as we go along.

So what happens when you press the Start button is you see some three-dimensional space, and inside it,

There on the left-hand side is an animal, this time a bee, and the colored circles are objects randomly spaced in that space.

And so the lines going from the bee to the objects are lines of sight.

And this is a three-dimensional view, so you can rotate it, see the three dimensions, and that's what happens when you rotate it.

The objects rotate.

And so there, at the moment, we've got nothing about the bee's internal model of space.

We've only got actual space itself shown in the view.

In order to show the bee's internal model of space, we press the Run button.

And I'll do this, and you can see what happens.

So as you press Run, the bee starts moving.

That's the green line.

It gets new lines of sight.

And from the new lines of sight, it estimates the positions of all those objects.

So I'll restart and do that again.

And what you'll see this time is that the estimates of the positions appear as small circles with error bars.

So the error bars are the gray lines.

The small circles are where the bee thinks the object is.

So you can see the bee is building rather an accurate model of where the objects are from its sense data.

I'll restart again, and this time we'll step through it one step at a time to see

how the bee's model of space evolves with time.

So one step, and you can see in the very early steps, we'll rotate it a bit to show what's going on, the bee starts making really quite good estimates of where the objects are.

Each little white circle is quite close to the blue circle, but the estimates have error bars, and the error bars are in three dimensions, showing the uncertainty of the location estimate in all those dimensions.

So it's those estimates of position that enable the bee to move where it wants to do.

Suppose these are flowers.

It can go to the flowers and get pollen or whatever it wants to do.

Now, I said the program computes three different kinds of model.

And these three is always doing this as you go through the steps on the track.

And these three models are a full Bayesian model.

And that's what we're showing at the moment, full Bayesian.

We can switch to a tracking model.

That's the object dynamic tracking.

And there, if I switch that, you can see the error bars and the objects hardly move at all.

They do move a little bit.

But this is one result of the program, that doing that tracking model, which is simpler to do than the full Bayesian model, gives you nearly the same estimates and same error bars.

And the third model it computes is a tracking model with noise.

And I switch to this one.

And again, it's not moving very much actually.

But I'll move it on a bit and you'll see that noisy tracking often is very different from tracking without noise.

So we're on tracking without any noise at the moment.

We step forward a bit.

And the bee keeps updating its estimates of these positions as they go.

And I now switch from tracking to noisy tracking.

And you can see the noisy tracking is significantly worse than tracking.

The noisy estimates have drifted away from the true positions of the object.

That is really the second.

By the way, I'm running this with a fairly small level of neural noise.

You can adjust the level of neural noise.

You can adjust the bee's visual acuity.

You can adjust all sorts of things using these sliders and try running the program again.

This is running with a fairly small level of noise.

So I keep stepping, and a noisy tracking estimate keeps getting worse, but I go back to the tracking estimate.

The tracking estimate without neural noise is pretty much dead on.

So I'll restart again, and I'll run again, because then we can show the graph at the end of the run, which shows what's happened to these errors.

So if I run again, now you look at the graph on the right.

And what that is doing is comparing tracking, which is the black curve, versus noisy tracking, which is the red curve.

And these are steps along the bee's track.

Those are the same steps we saw there.

But this vertical axis is the level of error in the depth of the B estimates.

And you can see that tracking is rather small errors.

It homes in on the true position of the object, whereas with memory noise, you get much bigger errors.

And in fact, the errors are very unpredictable.

If you rerun, we'll just rerun it again, you find that the errors coming from noisy tracking

they do seem to vary quite a lot from one run to the next.

Noisy tracking is pretty unpredictable, basically.

So there the errors have gone right up and they've come down again, whereas ordinary tracking without noise is nice convergence towards the true positions.

So that is really the second key result of the simulation.

That is that a tracking model is a realistic model of how animals estimate positions of things around them.

But if you add noise to it, even adding a small amount of noise completely messes up the tracking model.

Now, there are more things you can show with this model.

You can show, for instance, how animals use their model to detect what is moving around them.

Because obviously, when an animal is moving, it is quite hard for it to detect motion from inside.

its visual field, because when it's moving, everything is moving in this visual field.

So it has to use the three-dimensional model to work out what is actually moving.

And if you also plot the efficiency of motion detection, you find that too is very much poorer when you add memory noise.

So I think I'll stop at this point.

The program also does a simulation of bats, but I won't step straight into that.

Perhaps we could come back to that at the end of the talk if somebody wants to hear about it.

But for the moment, let's just step back and find what we have concluded from the bees model now.

Where is my presentation?


SPEAKER_02:
Uh-oh.

Now I've got to resume the presentation.

How do I do that?

Yeah, we get back to the presentation.


SPEAKER_01:
So the key points that I think I may have shown you is that in the 3D view, you can see the track of the animal.

You can see its lines of sight.

You can see, sorry, my phone is ringing.

I better go and shut it up.

I've shown you where the real objects are.

Those are the circles, the colored circles.

I've shown you the objects as located in the internal model.

I've shown you shape from motion.

I've shown you what the error bars are.

And you can rotate the 3D view.

I've shown you the three different models of 3D space, the full Bayesian model, the dynamic object tracking model, and the tracking model with memory errors.

As I say, when you run this program, you can change all sorts of parameters to see how it's sensitive to the parameters.

And I've shown you the B spatial model.

I've shown you how the error bars are particularly the depth dimension.

And I've shown you how memory noise degrades the model.

So here are the key results.

The best possible model any animal could build from its sense data is a very good model.

In fact, it's more precise than the sense data, because if the animal can assume that objects don't move, then over time an animal can build up a very good understanding of where objects are, better than its raw sense data.

Animals can't do any better than that.

But dynamical object tracking works pretty well.

It's almost as good as the full Bayesian model.

And it only works if spatial memory has very high precision.

And I didn't say the levels of precision that I put into the program, which start to spoil the tracking model, they're about one part in 100.

And that, I believe, is much more precise than most neural representations of space can give.

So that's the next part of this talk.

How do we do that modeling with a neural model of the brain, the classical neural model?

So how do you build a neural spatial memory?

And this is a changed quote from Animal Farm where they said two legs bad, four legs good.

Two dimensions is easy.

Three dimensions is hard.

Because two dimensions, you can easily do a sheet of neurons representing two dimensions.

Whereas in three dimensions, you don't have that option.

And there are several possible memory designs.

You can have a two-dimensional sheet of neurons and you can represent the third dimension by depth.

depth by the third, yeah, you can have some other variable representing depth, or you can have a three-dimensional clump of neurons where position in the clump represents a 3D position, or you can represent all three dimensions of an object position by neural firing rates.

None of these work well for object tracking.

I think we can quite simply eliminate the 3D clump model, but the other two models have a problem with neural error rates.

And if neural information is encoded as firing rates of neurons, typically you have a neuron firing n times in some time interval t, and then the precision with which it can represent some real quantity is of the order of one part in square root of n.

Now, N over T is typically between 5 and 50 pulses per second on most animal brains.

But for insects and small mammals, the time they've got to have to update their internal model of space is very small, typically less than a tenth of a second.

So if the time is a tenth of a second, you get N less than 10.

And that gives you errors of the order of 30%, which are much bigger than the 1% errors, which I said are needed for tracking structured motion.

So the conclusion of this is that neurons, when they represent space, there's a trade-off between speed and precision.

The faster you have, the less precise it is.

And this trade-off is just too hard.

So I believe there is no working neural model of 3D spatial cognition.

Now the three points in blue I've said before, they say spatial cognition is very important and animals do it well.

And we've had this problem for a long time.

In his book Vision 40 years ago, David Meyer identified the challenge and he started work on it.

He defined what he called a two and a half D sketch and various other models.

Now,

I believe that in terms of building neural models of how spatial cognition works, people have really not moved beyond this.

Why?

I think the main reason is that the memory problem is just too hard.

The memory gives them too big errors and is too slow.

And I suspect that over the years there have been many people who look at this problem and they decide to move on and do something else instead.

But the result is that

Spatial cognition is the central problem of neuroscience.

We don't have a model of it.

And so this is rather like theory of planetary motion without a sun or a theory of the atom with no nucleus.

So how does this relate to active vision?

I think active vision is one way to explore this problem.

Active vision describes how a 3D spatial model can be inferred from vision.

And there are

quite a few papers on it.

They focused on various aspects of it.

They focused on 2D scene classification.

They focused on the trade-offs between various objectives like the choice of visual saccades.

They focused on 3D robotics.

As far as I know, none of them have really focused on how animal brains practically build a 3D model.

And I believe that existing active vision models do not address the issue of neural error rates.

One reason for this is that the standard active inference toolkit in MATLAB, I believe it doesn't model neural error rates.

It assumes, I believe, an abstract perfect neuron with very precise representation of quantities.

And error rates are actually not an issue for many of these applications.

They're not an issue for robotics, and they're not an issue really for making discrete choices.

But as I've said,

in this talk so far, the accuracy of the 3D model really matters.

And neural error rates are the big problem.

If we set that problem on one side for a moment, there is the issue of active inference trade-offs.

And there are many interesting trade-offs you can examine in active vision.

And the key trade-off, I believe, is one between freezing and moving.

As I've shown in the demonstration, the animal has to move in order to infer the 3D positions of things around it by shape from motion.

It also, of course, has to move to achieve practical goals like feeding and fleeing and mating and so on.

On the other hand, it can freeze.

And freezing, it may conserve energy.

It may be able to detect what's moving simply directly from its visual field, which is much easier.

And it may itself avoid detection.

So these are very key trade-offs.

They're absolutely essential for lifetime fitness for many animals.

Animals have to make this trade-off or these trade-offs any moment of the day.

And so we've got plenty of empirical data about it.

And I think it'll be a very useful area to explore.

Now, I'm going to switch to something completely different.

Having said that neural storage of spatial positions is a very hard problem,

I'm going to talk about an alternative, possible alternative way of storing spatial data.

And so if you assume there's some round region inside the brain of a fairly large diameter D, and this holds waves with a minimum wavelength, which I call lambda, and the neurons can couple to the waves, both as transmitters and receivers, and the wave can persist

at least for fractions of a second, so the wave can act as a working memory for positions.

And the number of object positions you can store in the wave can be up to d over lambda cubed, and that can be a very large number.

The spatial precision, which one object position is stored, can be one part in d over lambda, and I think

that D over lambda can be very large.

So you can easily get precision better than one part in 100, which is what you need to build the spatial model.

So in summary, wave storage of 3D positions may have a lot of computational benefits.

It can give a natural fit to the problem.

It can give high precision and high capability.

It can give you very fast response times, low spatial distortion, and some other benefits which are described in the papers.

So apart from its computational benefits, is there any evidence for wave storage in the brain?

I believe there are two quite powerful lines of evidence, one of which comes from the insect central body.

The central body of the insect brain is a very small part of the brain in the middle of it, and it consists of a fan-shaped body and the elliptical body.

And it has this shape, which is remarkably well conserved,

across all insect species.

And there's an insect brain database, and I've gone to the insect brain database and pulled from it the shapes of the central body from a few typical insect species.

And here you can see the fan-shaped body and the elliptical body, and it's very constant across all kinds of insects.

And you can see it's approximately a round shape, so it's well-suited to hold a three-dimensional wave.

and it does multi-sensory integration.

And so it's quite likely, quite probable that it holds spatial positions and insects have very few neurons in their brain to do it in any other way.

And what I think is significant is how constant and round the insect central body is compared with all the other parts of the insect brain.

So that's one piece of evidence from the insect central body.

The other piece of evidence

comes from the mammalian thalamus.

As you may know, the thalamus of most mammals, all mammals, is approximately spherical and is connected to all sense data and all cortical regions.

But the important thing is that the shape of the thalamus is highly conserved across all species.

And there's an important aspect of the thalamus anatomy

that unless you assume a wave, really it doesn't make sense.

Because the thalamus consists of a number of independent nuclei like the polvinar and the LGN and so on and so forth.

And the connections across within the thalamus between these nuclei are very weak or even non-existent.

So you could have this picture here that the thal... Where's my pointer?

Here's my pointer.

The thalamic nuclei, which are the white circles here, they all connect in two ways to the cortex, but they don't connect to each other.

So one thalamic nucleus here could easily start moving out towards the cortex, and the length of its axons could decrease, and its other connections, it doesn't need other connections.

So all the nuclei could migrate outwards towards the cortex,

And you could still have the same neural synaptic connectivity and the same computational capability if neurons only compute by synaptic computation.

So this way, you could save a lot of energy in shorter axon lengths.

So in summary, a compact thalamus only makes sense if all the nuclei need to be immersed in the same wave.

We now have three pieces of evidence for a wave in the brain.

Firstly, there's the computational neuroscience that it's a very difficult problem to build a 3D model about it, but you can build a 3D spatial model if you assume there's a wave storing positions.

Secondly, the insect central body is nearly round in all insects, very well suited to hold a wave and appears to be in the right part of the brain to do that.

And thirdly, the mammalian thalamus, which again has this round shape, very well suited to hold a wave.

And the important thing here is that without a wave, the anatomy of the thalamus doesn't make sense.

So I would like you, if you remember only one thing about this talk, remember this slide, there is quite a lot of evidence for a wave in the brain.

One thing I will say,

is that the wave is probably not an electromagnetic wave because there's quite a lot of interest in electromagnetic fields in the wave from researchers like Miller and McFadden and so on.

But an electromagnetic field can't play the role that this wave is needed to play.

In other words, the key thing that the wave is supposed to do in this model is to store information for fractions of a second.

But an electromagnetic field in the wave, and there certainly are electromagnetic fields in the brain, they cannot store information for fractions of a second, and they cannot represent 3D space like a hologram.

And just to say a little more about this, if there's an electromagnetic wave in the brain, it has to obey Maxwell's equation, so the wavelength times the frequency is equal to the speed of light, lambda f equals c.

And that means at 40 hertz, typical frequencies of these waves, the wavelength is 8,000 kilometers as large as half the Earth.

So the conclusion is that at 40 hertz, electromagnetic field is not a wave.

It's a static field, and it's driven entirely by neuron firing.

So it doesn't store the information.

So in summary, we're looking for something not electromagnetic,

and possibly some quantized excitation, something a bit exotic in the field of quantum biology.

I think we shouldn't despair here because we know evolution is a lot smarter than we are at discovering these things and exploiting them.

So here are some take-home questions.

Does 3D spatial cognition use a wave in the brain?

In other words, in the light of the evidence I've shown you, what is the Bayesian probability of that hypothesis being true

Now, I say these take-home questions because I didn't expect you to have an answer immediately, but perhaps you'd like to look at the papers and see what the evidence is and try and assess it in your own mind.

Or do you know some slam-dunk killer reason why there can't be a wave in the brain?

If you do know a reason, what is that reason?

And how do brains compute space?

How do neurons on their own represent 3D space with enough precision?

On the other hand, if there might be a way for the brain, wouldn't that be a rather exciting and revolutionary development?

It would actually change the whole of neuroscience, and it could address this central unsolved problem of how spatial cognition takes place.

So I believe that possibility should be explored, particularly for young researchers.

It's attractive.

It's greenfield research.

It's not a well-trodden path of classical neuroscience.

The classical neuroscience model of

McCulloch-Pitts neurons and Hebbian synapses, that's 75 years old now.

So I would like to encourage people to get out and explore.

Or again, come back to the earlier slide here, a crisis in neuroscience.

The result of this work, I think, is that neurons can't represent 3D space because they're too imprecise and too slow.

So the crisis is, can you show

This is wrong.

Can you show it by building a working neural computational model and checking its scales properly?

So the FEP neural process model is the starting point for that.

I think it's a good problem to work on because it is a crisis and big advances in science tend to come out of crises.

So what I'm advocating, and this is my last slide, is a twin track research program.

to build two different active vision models of 3D spatial cognition.

One is a pure neural model, which is a classic FEP neural process model.

Can this be made to work?

Or are the neural memory errors going to kill it?

And secondly, to try to build a hybrid wave and neural model.

And when you're building those models, we can explore the trade-offs that active inference is so good at computing.

and particularly the trade-off between freezing and moving.

So there are a couple of candidate projects for the Active Inference Institute.

Okay, that's it.


SPEAKER_00:
Awesome.

Thank you, Robert.

have some questions and some people have asked questions in the live chat so i'll uh i'll ask them so first just while i'm re-cropping everything how would you connect this to the requirements equation earlier work because you mentioned that there was a

requirements equation driven calibration of the optimal navigation.

So what does that look like to have the optimal navigation according to the requirements equation?


SPEAKER_01:
Well, to summarize on the requirements equation, you can model how brains evolve.

And this is the previous live stream.

And you can show that they evolve towards making a purely Bayesian calculation

of their best model of the world from the sense data.

But that purely Bayesian calculation is rather expensive.

And it's been well known in FEP that full Bayesian calculations is intractable for most animals.

And so that is a very expensive calculation.

And it's probably not the way animals do it, but it is, it can be done on digital computers and it can be done

in this model I've showed you.

And the first model, the full Bayesian model, is actually computing the requirement equation from the bees or the bats sense data.

The second model, a tracking model, is an approximation to that, which is a lot cheaper, but seems to give very nearly the same results.


SPEAKER_00:
Okay, awesome.

Let us dive into a few mammal and insect neuroanatomy questions.

So I'll start with a set of questions from the live chat.

This is going to be about mammal neuroanatomy.

Okay.

Tim Ritter asks, do you assume this wave property for all thalamic nuclei, primary and secondary, or for specific ones, e.g.

pulvinar or mediodorsal?


SPEAKER_01:
Very good question.

I don't know the answer.

I mean, at this stage, I believe the whole thalamus is around near spherical volume with the wave going through all of it.

So they are all immersed in that same wave.

So even the pulvinar, the pulvinar certainly is, even the LGN, which is rather small and is a pass-through nucleus, I think they all are.

So I think, for instance, I think...

People always say the thalamus is a relay.

Sense data gets to the cortex via thalamus, but people don't have a very good reason why it has to go through these relays, nuclei, in order to get there.

I think it's doing something about locating, about, I think the wave has some involvement there, but this is very early days.

I don't know the answer at all.


SPEAKER_00:
Okay, another question from Tim on mammalian neuroanatomy.

What about 2D orientation?

Would you expect similar waves in hippocampal instead of thalamic regions, or is 2D sufficiently easy to get by without?


SPEAKER_01:
Yeah, basically, I believe 2D is easy enough to get by, and the hippocampus is by no means suitable to hold a wave.

Hippocampi have all sorts of different shapes, so...


SPEAKER_00:
Yes, I think that was a core theme between the mammal and insect areas, is the conserved shape, and then also the allometric differences over evolution, where the size differential of the insect central body changes much less than other primary sensory regions, and that was in your paper.


SPEAKER_01:
Yeah, that's right, yeah.

And that kind of implies that

that small size of the central body, it's only a few percent of the whole insect brain, seems to be enough


SPEAKER_00:
Yeah, and that the properties which it hosts or enables might be related to its physical extent or like to its surface area.

It's a volume ratio and not a function.

Like, for example, in the antennal lobe where the olfactory information are coming in, there are these little glomeruli and different species have from several tens to several hundred of these olfactory glomeruli.

Like ants have many and they have more olfactory receptors in their genome and they have more olfactory glomeruli in that region.

Or insects with more compound eye sections, they have larger optic lobes.

So the primary sensory regions have very large...

variation amongst species but then as you get into the central body uh you get much more conserved Anatomy in size and then the mushroom body on the top part of the brain is something a little bit in between that might have more of an analogy to like mammalian cortex where they're actually

is the possibility to scale its cognitive capacities through size changes because it has some kind of repetitive or stereotyped layout?


SPEAKER_01:
Yeah, I mean, there are a load of fascinating questions in neuroanatomy which relate to this.

And if you pursue this hypothesis, then there's all those interesting questions.

I'm not an expert on insect or mammalian neuroanatomy, but there's a load of interesting questions in there.


SPEAKER_00:
Cool, so about the bee spatial cognition.

So we know that bees use a variety of visual cues ranging from the landmark,

and the landscape recognition to polarization of light and so on.

And also, as you pointed out, the central body does multisensory integration.

So how do we think about the possibly complementary or redundant information provided by these different aspects of the visual fields?

And what does your simulation focus in on?


SPEAKER_01:
Well, I mean, I believe that what the central body and the thalamus both do is multi-sensory integration.

In other words, animals should, they need to make the best 3D model of space they can, and they need to use all their senses to use it, apart from possibly smell.

That's an interesting question.

And so both of them do multi-sensory integration, and ideally one would put in a simulation one would have

things like stereopsis, one would have object recognition, one would have light polarization, all sorts of sources.

This program only does simple vision or simple echolocation at the moment, but it should do all multicenter integration in a single Bayesian maximum likelihood model of the whole, all sense data coming in at the moment.


SPEAKER_00:
Interesting.

Yeah, with sounds or with smells, it would be interesting to see how those come into play.

And how do you think about, in the simulations presented here, egocentric and allocentric navigation?

Because you mentioned how the kind of simplifying assumption is that the world is a rigid, fixed body.

So you can have these kind of duality where like, I'm moving and the world is fixed.

And then there's sort of like, I'm fixed and the world is moving.

So how does that relate to that egocentric, allocentric distinction?


SPEAKER_01:
Yeah, very good question.

I mean, I think...

The frame of reference used for the model should be as much allocentric as it can be because the wave has to persist.

And if the wave just persists, it represents an object at a constant position.

So you want to have a frame of reference where most objects are at constant positions.

So I think that makes it allocentric.

But obviously it has to change from time to time.

Every few seconds it has to switch because it can't just stay allocentric.


SPEAKER_00:
Interesting.

So now to connect that to what you brought up about move or stay, that kind of fundamental animal or fundamental mobile organismal nervous system question.

i thought about different body plans where the eyes or the visual component are unable to move separately from the body like a bee can turn its body but it can't rotate its eyes whereas in humans for example we have optic saccade so there that stay or move

yes we have turning our posture and moving through space but also we see like this microcosm where when the gaze is fixed there's high precision and then movement in the world is associated with movement of objects and then whereas when a eye saccade is dispatched during the saccade our visual attention is alleviated

And then it's because during that time, all the movement of pixels essentially is ascribed to the movement of the eye.

So we see kind of like a microcosm of the two modes of movement and stability in motion detection in the cicading.

But for other organisms that don't have eye cicade, the only way that they can get that kind of alternating movement and stability is by moving their body.


SPEAKER_01:
Yeah, yeah.

I mean, I always think of eye saccades as particularly predators, if you like, that want some high resolution in some particular direction.

Whereas for most insects, as you say, there is not the option of a high resolution fovea.

But I think of saccades as being cheap.

I mean,

The freeze-move trade-off is a real trade-off.

If an animal moves, it can be detected as moving and it can't detect motion itself as well as if it's stationary.

So that's a real hard trade-off an animal has to make.

Whereas circades, you can make them cheaply whenever you like.


SPEAKER_00:
yes yes and also it's really interesting like how often the behavioral studies just look at the direction of movement but there's this whole timing of movement and so there's definitely a lot of empirical studies about whether fear-based movements like in a predator prey uh or different kinds of movement choices um where would you say attention comes into play

the sense that the bee or the bat was just kind of taking it all in it didn't have like some restricted scope so it's kind of like a uniform attention across objects and across space and time but then we know that we do have this visual attention phenomena well yeah attention is very important and uh i think naively it's a search on model of attention in other words


SPEAKER_01:
The wave representation of all space represents all the space around an animal.

But the animal can focus attention on a region of the space.

And what that's doing is tuning the receptors in the thalamus.

So they are sensitive to wave vectors in a certain region.

So there's a whole load of issues there about how the wave works.

It's whether it can, how signals are rooted from sensitator inputs to specialist regions of the cortex.

And I think attention is that rooting of information.

So again, loads of big questions there.


SPEAKER_00:
Yes, with the wave, I was kind of...

thinking about the insect brain, visual input flowing in and also other potentially inputs.

And all of these are crashing on the shores of the central body.

And then there's this kind of stabilized dynamical wave representation such that information coming in differently changes the resting shape of the wave

And then that opens up, like you're now suggesting, recurrent connections or other connections into that wave hosting region.

Recurrent connections can modify the shape of the wave, potentially.

And then also the resting shape of the wave can route or augment or suppress other sensory information coming in.


SPEAKER_01:
would be like water kind of dumping to where there's already a high water point versus water going to where there's low water yeah yeah i mean i think key role of the wave is to persist a background model of all 3d space and then against that background model new center information that comes in particularly movement uh is best evaluated

A piece of new sense data, you evaluate it much better if you compare and contrast it with what you had before.

That is attention.

And it's the thalamus, if you like, telling the cortex, here, pay attention to here.

Here's your old information from this place in space.

Here's your new information.

So tell me what's changed.


SPEAKER_00:
Well, this connection with frame differencing is very powerful.

Predictive processing, predictive coding algorithms were built by computer scientists and compression engineers looking to make video compression work and doing the frame differencing because that's the optimal way to compress video.

And then that got brought also back into neuroscience where there's a lot of focus on things like edge detection and these other 2D visual phenomena.

And then as you're pointing to, there's this kind of sun at the solar system's center that's not really being discussed, which is like, okay, yes, we have neurons in different visual regions that are excitable by vertical lines, by diagonal lines, and so on.

But this is all flat phenomena.

And the question of not just shape recognition, but the question that's most proximally relevant for movement and the fitness related decisions for the organism in the niche has to do with its spatial navigation, not it's like eyesight at the eye doctor.


SPEAKER_02:
Yeah.


SPEAKER_01:
Not only the spatial navigation, how it moves its limbs, where it puts its foot next, that sort of thing.

And the 3D model, I think, does all of that.


SPEAKER_00:
Okay, I'll read a question from the live chat.

How might the saccade relate to a matrix of inputs versus a human-based visual system?

Movement on the matrix may give different spatial dynamics.


SPEAKER_01:
I'm not sure what you mean by matrix of inputs there, but as you said, during a saccade, visual input is kind of blocked while the eye is getting from A to B.

Uh, whereas the wave persists and the 3d spatial model stays constant.

And after a secade, the eye has to update the 3d spatial model in some different place.

So, um, I'm not, I don't think I've answered the question, but perhaps you, what do you understand about matrix?


SPEAKER_00:
It's what you enter.

Um,

it's making me think about the experiments where the, for example, the fruit fly is placed on a ping pong ball in a harness in a virtual reality setting.

So it's getting custom visual input and its movements on the ping pong ball just kind of scrolls the ball.

So it's basically fixed, but it gives a lot of degrees of experimental freedom around the, um,

orienting of the body and what visual inputs it gets.

So I wonder if anywhere there we know about the timescale of spatial orientation updating, because that would be very critical to understand the nature of the wave.

If it was something that was, for example, closer to the diffusion rate

of ions, then we might be looking more towards like a channel or pore based hypothesis.

If it was something that was faster than neural signaling, it would suggest something more like the direct coupling or other kinds of action.

So what makes you feel, as you suggested, that it is not an electromagnetic

stabilized wave fields?


SPEAKER_01:
Well, as I say, the physics, I mean, for an electromagnetic wave, we do understand the physics.

And the electromagnetic waves measured by EEG, for instance, they are a purely passive consequence of neural activity.

They don't persist in the information for any time at all.

Whereas this wave I'm talking about has to persist information for

for fractions of a second.

So this constant spatial model is kept persisted while the saccades go on, while the animal moves, while it computes shape from motion.

So a pure electric field, we know the physics, it's Maxwell's equations, and it does not store energy, store information.

So it's purely a passive reflection of what's going on in the neurons.

It's not a memory.


SPEAKER_00:
So the neurons are, especially if we think about the several like thousand to tens of thousand, let's say, in the insect central body, there's too few and they're too sparse and noisy to, in a purely connectionist neural framework, to support the kinds of empirical results that we see.

On the other hand, a purely field-based approach has some issues that you just laid out.

So it's very interesting that two at least of the well-known mechanisms, the local field potential and the firing rate, rate coding type models, that both of them seem to have some limitations.

And yet there's very strong anatomical evidence for the functional role of that region.


SPEAKER_01:
Oh, yeah, it's absolutely vital region.

But I believe that just looking at electric fields, magnetic fields in the brain is not going to give you memory.

And that's the key thing that I think is needed.

to do structured promotion.

You've got to have short-term working memory to hold a little model of space for a fraction of a second


SPEAKER_00:
Do you see that as a kind of special type of short-term memory or do you think this is the same memory pool that like short-term audio memory gets entered into?


SPEAKER_01:
Oh, it's, it's special.

It's different from that.

Yeah, definitely.

It starts from,

How do you complete a 3D spatial model?

How do you do structured promotion?

You need short-term working memory for that purpose specifically.


SPEAKER_00:
Interesting.

The kind that enables us to

check for difference in visually changing systems?

Or what does this visual working memory have?


SPEAKER_01:
Well, basically, it's not vision because it's 3D.

And checking for difference in the visual field is, you can do it quicker.

You can look directly at the visual field, whereas this model

I think the 3D model, it's maintained by a loop in mammals between the thalamus and the cortex.

And it's a 40 hertz cycle that maintains it.

So it takes time to build the 3D model.

And it's a bit slower than direct visual change detection.


SPEAKER_00:
Interesting.

It's this tension with visual being what is seen versus kind of a broader imagination of vision.

What about action in your model?

So how were the paths set?


SPEAKER_01:
Yeah, I said that the model was...

a bit like active inference, and the animal has to move in order to understand space.

But it's not really, the program has not modeled the kind of active inference choices of how should I move to get the best understanding of space.

And you could do that.

You could make the bee choose its trajectory to get the best understanding of where the flowers are.

Or you make the B2 that's tragic for all sorts.

Those are the active inference trade-offs that the program has not yet looked at and which can be looked at and I think are very interesting.


SPEAKER_00:
That's awesome.

Yeah, it's almost like the bee in this situation, it's like on a train trying to reduce its uncertainty about the location of landmarks.

But it's just on a rail, it doesn't have policy decision.

Whereas once we start to close that loop and ask which direction of movement, or none given the costs,

would reduce uncertainty about resolving this kind of spatial relationship then that that's where the perception actions start to like come into play in benefit of each other and potentially there's several choice successive moves can greatly reduce uncertainty through active sampling just as we see in skating and all other situations and that would

be like a heuristic or strategy that really does work.


SPEAKER_01:
Yeah.

I mean, another example that I use somewhere is I believe that predatory birds like hawks, when they're approaching their target, they don't go in a straight line.

They move on a curve to reduce the uncertainty.


SPEAKER_00:
So that they continue to get more information.


SPEAKER_01:
Can you see the range of the targets?

If they just went straight for the target, they wouldn't get a range fix on it.


SPEAKER_00:
Now, what about the difference between the bee and the bat?

Whereas a bee is simply receiving the reflected photons, let's just say, the bat is sending out a

invisible signal.

So how is this kind of radar echolocation setting similar and different to the vision setting?


SPEAKER_01:
It's very different.

I could show you that with the program if you like.

What happens with the bat is from the delay of an echo,

For a particular insect that the bat is tracking, from the delay of the echo, it sees the range of the insect.

So it gets the insect constrained on a sphere in its 3D model.

And then from the Doppler shift, it actually perceives the cosine of the angle between its directional movement and the direction of the insect.

So what it gets there is it constrains the sphere, surface of a sphere,

down to one circle in space.

So what the bat gets is a series of circles in space which constrain the position of the insects.

And I could show that on the program if you'd like.

Sure.

Okay.

Well, how do I do that?

How do I share again?


SPEAKER_00:
Yeah.

I'm curious.

Is the circle something like, is it like a hoop that you could throw a basketball through?

Or is it?

Well, I'll show you if I'm.


SPEAKER_01:
Now, I can't get rid of this something on my screen.

It's the problem.

I've got a big black screen with a two on it, and I can't get rid of it.

End show.

I suppose I end show.

Right.

Now, am I sharing my screen or not?


SPEAKER_00:
Not yet.


SPEAKER_01:
Now, how do I find where I do that?


SPEAKER_00:
How do I find where I am in space?


SPEAKER_01:
Yeah, absolutely.


SPEAKER_00:
That's an interesting question also, how these mechanisms are... I've got the Google window somewhere.


SPEAKER_01:
The Zoom window somewhere.

Oh, ZM.

That'll be it.


SPEAKER_00:
How are these mechanisms repurposed for digital navigation?

Semantic navigation?

Narrative navigation?


SPEAKER_01:
Just a minute.

Let me find... Oh, God.

Get away.

Oh, share.

Screen to share.

Okay.

You got that?

We're back.

Okay.

So what we do is we change from B to bat, and we start again.

So what you have now is the bat and several insects, which are colored circles here.

And what the bat has is its echolocation.

Take the blue insect.

It's echolocation.

echolocation constrains it to a sphere in space.

And that's the delay of the echolocation.

And the Doppler shift constrains the sphere down to a circle.

So if we rotate these circles, you see that blue insect is on a circle from one point in the bat's trajectory.

So as the bat moves, it gets successive circles of the same insect.

So what we have is the bat is moving.

And all these circles, highly confusing.

And it gradually locates all the insects better and better.

But if I restart and step again, if I step, I've restarted.

Right.

Now, if I step, what I can do is focus on one insect.

If I focus on that insect, then I only see the circles from echolocation of that one insect.

And I can step again and get a new circle from a new step.

And all the time, the bat is optimizing the position of that insect to get the best fit to all the circles it has for that insect.

So it's very different from vision, but it can build a very good 3D model from its echolocation.


SPEAKER_00:
Okay, so to kind of confirm what's happening here, there's...

For a given snapshot ping with the sonar, what is returned is a circle of equiprobable locations that are kind of like the maximum likelihood ridge.

That's right.


SPEAKER_01:
These are really sort of Gaussian donuts, if you like.


SPEAKER_00:
Okay.

And then successive pings enable you to look at the intersection point of the circles to find out through successive approximation the likeliest location.


SPEAKER_01:
So this is what's happening here on that light blue insect.

It's got these three circles, and that's the most likely.

They're not very well intersecting, but that's the most likely intersection point.


SPEAKER_00:
That's it's like a Gaussian mixture model.

You have those three peaks.

And then when you summit those three peaks, that the point that interpolates them or just is the geometric average is the single maximum likelihood estimate point.


SPEAKER_01:
That's exactly it.

Yeah.

And again, with the bat, I can go from full Bayesian model, which is this one full Bayesian tracking or tracking with noise.

And again,

The noise tends to have nasty effect.

Or I look back at this one here.

That's that dark blue insect.

And I can rotate to see how the circles go.


SPEAKER_00:
How is this similar or different than, for example, radar navigation algorithms for planes or ships?


SPEAKER_01:
I think it's quite similar.


SPEAKER_00:
i think yeah i mean they are making maximum likelihood inferences from radar signal and have you with this software looked at the computational like the runtime complexity or the resource use associated with scaling the number of insects or scaling the resolution of vision


SPEAKER_01:
Yeah, you can scale the number of insects.

For instance, if I scale up here to 30 insects, I can run the model with 30 insects.

And it runs perfectly well.

So the model is quite efficient because what it has to do for each object

And for each time step, it simply has to do this Gaussian optimization, which turns out to be just a 3D matrix operation, three matrices.

And that's very quick to do.

So that's one of the advantages of it, that if animals are trying to track a load of things around them, which they probably are, it's quite a quick, efficient computation.

But this is not the neural implementation.

The problem is going from here, this Mars level two computational model, to a neural implementation.

That is where I think all the interesting problems lie.

At least you have this level two model as a kind of starting point.

And the Bayesian model would be trying to implement that in neurons.


SPEAKER_00:
Yes, makes sense.

This is kind of the as if...

Bayesian algorithmic map and the question is what proximate mechanisms are capable of doing these algorithms functionally and neuroanatomy has basically localized to the region in mammals and in invertebrates

And so now we're in a space of winnowing possibilities and leaving the door open for unconventional opportunities for how those regions actually do it.

It's a very targeted specific agenda that connects a first principles grounding about how well the navigation can proceed with a requirements equation

on through to the empirical patterns that we see those empirical patterns might have some like behavioral experiments well sometimes can be hard to interpret as well though for example there's a common experiment where a wall that's shaped like an l is set up and then people will use it to test if an animal will go on the hypotenuse

to reduce the travel distance or whether it will take the the around the wall however even if there was a conceptualization of the ability to move on the hypotenuse direction an animal might like prefer to move along walls so then by the time you get to the real animals movements

It's very tied up with not just trying to be the optimal landmark resolving visual algorithm.

It's actually engaging in all these other drives that can make it look like it's lower efficiency or even like supernaturally efficient on a given domain.


SPEAKER_01:
Yeah, I think there's all sorts of animal experiments one could do, but interpreting them is not easy.

But basically, if you're looking in these experiments to measure how good the animal's internal 3D model is, I would like to, with insects, for instance, how good is their movement detection if it's movement in depth that needs a 3D model to resolve it rather than just the visual field?


SPEAKER_00:
that's very interesting especially in insects potentially where there's little overlap between their two compound eyes yeah yes um okay i'll read a question from the chat as we kind of head towards the end news equal wrote how would perception of ability to conceive of other domains of time as a cognitive function change spatial awareness


SPEAKER_01:
Other dimensions of time.

I'm not sure I understand that question.


SPEAKER_00:
Other domains of time.


SPEAKER_01:
What's other domains of time?

Well, this is all a very short-term question.

It's fractions of a second.

And time outside that interval just doesn't come into it at all, really.

Does that answer the question you're asking?


SPEAKER_00:
It might.

I think it might also be pointing to our awareness of how we handle our perception of time and space.

And what does that open up or enable, for example, for our perception of space if we have a different perception or conception of time?


SPEAKER_01:
Well, that is a very deep question.

I mean, I think our perception of chronological time, long-term time,

is something completely different from more anything else in the animal kingdom.

I think most animals live in the moment.

They're aware of what's happening right now and what they're going to do right now.

And the rest just doesn't matter.


SPEAKER_00:
Whereas we exit what matters in the moments to ruminate and speculate.


SPEAKER_01:
Yeah, absolutely.


SPEAKER_00:
Um, well, what other,


SPEAKER_01:
directions or or ways are you hoping to take this work well one way that is particularly important i think although particularly problematic is theories of consciousness in other words consciousness most of our consciousness at any moment is consciousness of the space around us

And it seems to come from our internal Bayesian model of space around us.

So this work is very related to why we are conscious.

And there's another paper, and I'd like to give another live stream just on that subject.

So I think this gives a way forward to a theory of consciousness that can be in many ways more satisfactory than we get from purely classical neural models of the brain.


SPEAKER_00:
That's awesome.

I'm also personally very excited on the empirical insect neuroanatomy side to look.


SPEAKER_01:
Oh, there are a huge number of ways of going forward.

Insects are particularly productive because you know they've got to do it with a really small number of neurons.

So it's really, there's a lot of experimental work on insects that can illuminate this question.


SPEAKER_00:
Yeah, absolutely.

Cool.

Yes.

The brains are fun to dissect.

You can see it transparently all there and no backstage there.

And then also this on the more transferable outside of entomology, I think the active inference

loop closure with policy selection of movements including stay go no go decisions and which way to go and then understanding how like well here's the trajectory that would have been the most information gain on resolving this location

here's the trajectory that maximized safety, not moving or something.

And then here's the trajectory that would have done some other thing.

And then being able to look at realized empirical trajectories and then break those down or look at their component loading

according to safety, visual information gain, other kinds of heuristics or impulses to understand moments or kind of fragments of trajectories like something looks to briefly resolve its uncertainty and then the overall demand to resolve uncertainty drops again and then it continues just with inertia and then maybe that's a very simple decision to make and then there's probably all kinds of cool

patterns and ways to go?


SPEAKER_01:
Yeah, there's a huge amount to investigate.

Another topic, by the way, is we've looked at mammals, we've looked at insects right at the opposite ends of the spectrum.

There's a whole load of stuff in between.

I think octopus and squid are particularly interesting, but there's all the other species one can look at, say, how do they do space?


SPEAKER_00:
they have unique bodies and different bodies but there's also bringing in there the question of underwater or space or a fluid media and then that and a bat flies but it has the mammalian neuroanatomy so then there could be like bird neuroanatomy with its slight differences from the mammal then there could be this question of underwater and maybe there's some mammals

that have underwater navigation maybe dolphins would be like bats but in another fluid media i hope people download the dot zip and play around so do i any last comments uh

not really no i think i've said it all thank you thank you robert well i i am greatly enjoying learning about the work and seeing about how the requirements equation scopes

a given problem setting and kind of puts a meter stick on whatever the inferential problem is, inference plus action problem is.

And then from there, the the Marian

research program is just kind of laid out.

You can pursue it from the mechanistic or from the algorithmic elements, but they all are connected through on one hand in theory, the requirements equation, and on the other hand, the empirical results that we actually see.

Yeah.

Cool.

So thank you again.


SPEAKER_01:
I come back to the Feynman.

quote Richard Feynman on his blackboard.

It said, if you can't build it, you don't understand it.

This is all about building it.


SPEAKER_00:
We can't build a bug.

Not yet.

Thank you, Robert.

See you for dot three.


SPEAKER_02:
See you.


SPEAKER_00:
Bye.
SPEAKER_00:
Hello and welcome to the Active Inference Lab.

Today it is October 25th, 2021, and we are here in Active Guest Stream number 12.1 with Martin Boots.

So this is going to be an awesome presentation and discussion.

If you're watching live, please feel free to add any questions into the live chat and we'll be compiling those.

And otherwise, we're just going to have a presentation and then discussion interval.

So Martin, thanks so much for joining our lab to give this presentation.

And we're really looking forward to what you have to share.


SPEAKER_01:
Yeah, a big pleasure to be here tonight or this morning, I guess, for you guys in California.

So let's see if I get the screen sharing here right again.

We practiced it before.

This should be this one here.

Looks great.

And now I just need to have at least one video on the side here for me.

So yeah, big pleasure to present our lab's work on which I call event predictive

active inference and particularly also modeling the development of conceptual compositional cognition from sensory motor experiences.

You might have already kind of noticed the picture here, this artistic picture on the top right.

And I think it shows nicely and intuitively how our brain continuously seems to attempt

to infer the hidden causes behind our sensory perceptions and thus this kind of terrace is kind of bent for us a little bit because the top part of course just can be interpreted in two ways and is obviously not quite rightly positioned or or yeah it cannot be like this in reality

I give you a couple of other examples what I mean by this active inference, which is known to or explained often by Carl Fristen and others to essentially develop latent hidden states to explain away the sensory perceptions by inferring the hidden causes behind them.

So here you see a color illustration, which is called a color illusion sometimes.

where you probably won't believe it.

I have to measure it each time again to make sure that I'm really right.

But essentially, in this yellow-tinted figure, essentially, these blue-colored squares here, they are essentially grayscale.

If you test them in the graphics program, you will see that this is a grayscale position.

So all blue points here are, in fact, grayscale.

And here on this side with the blue-tinted background,

you essentially have the same situation, but it's the yellow pieces that are actually grayscale.

This is illustrated here on the right a little bit.

And this just shows essentially that our brain is interpreting its sensory perceptions in the light, more analogically fitting here,

of the circumstances yeah and in this case yellow light makes a true blue square appear gray when we look at it so we perceive it gray the sensory stimulation is gray but the true cause behind it must be a blue square so i our brain essentially correctly and actively infers

that this square is most likely blue because under this yellow light conditions, it appears gray.

And we're not even aware of that.

So this is a lovely example of a very low level inference process in our brain that takes into account the situation and essentially puts the perceptual sensory information in this event, this light event, this lightning condition, essentially.

Another nice example of this is these shadows that I found here online from some artistic exhibition.

And you have here two shadows.

And as long as you look at the shadows and you perceive the shadows, you kind of not only perceive the shadows, but you perceive a person and the illustration of a person.

And to a certain extent, you have the feeling that some person must be standing here somewhere such that I will perceive

these shadows.

But lo and behold, this situation is totally different.

These are actually cool artistic sculptures that produce the shadows.

And again, you see that you have perceived the shadow and interpreted not only the shadow, but created a whole scene, a whole event, like with a light source and the person in the middle, and then the shadow that produces this.

in order to make sense of the shadow itself.

So again, you infer the hidden causes behind the shadow.

In this case, not quite correctly, because it's a cool artistic sculpture here that is an installation with a light source perfectly positioned such that the sculpture creates a particular human-like shadow figure.

So the first proposition that I take out of this essentially that our brain seems to be this generative predictive model and uses active inference essentially with this generative predictive model, generating or inferring internal activities that is internal active encodings that try to explain away the current sensory perceptions

and thus make sense of it in the sense that we explain them away by ideally inferring the true hidden causes behind the perceptions that we actually perceive.

This generative predictive model that explains away perceptions, it maintains distributed predictive activities which form this kind of attractors

which often then are formalized, particularly by Carl Fristen and his followers, so to say, and in various ways, formalizing this and essentially suggesting that the brain develops this local free energy minima, this local attractor-like minima that is the consistent explanation of a particular perception that we currently have under consideration.

So this would be, for example, in the shadow situation.

We perceive the shadow, but we know intuitively that there's a light source and something in between that produces a shadow.

And thus, in the shadow of a person, we essentially immediately have the feeling that some person must be standing there.

The free energy formalism essentially pushes towards the generation of these generative predictive models.

and pushes towards the continuous active inference processes that are unfolding within these developing generative predictive models.

And one can nicely distinguish three kind of aspects of this general inference process formalism.

And that is on the one hand side, on the first hand side, knowing what is going on.

So that's essentially the fast adaptation of internal model activities towards this local free energy minima.

to explain to ourselves what's going on.

So if you looked all together at the shadow, and we perceived the shadow, then you perceived not only the shadow, but you had the feeling that you inferred the causes that there is a person in the light source that creates the shadow.

Then the second part is learning more about the world.

We can revise our generative predictive models.

For example, you have now revised your predictive models.

learning about that there are some cool people that make sculptures that produce human-like shadows, and there's no human, actually, but just a sculpture.

And so we have learned something new about the world, essentially.

And we do the first two aspects of inference aspects, essentially, of course, only to really survive in our world, to live, to interact with our world in better manners.

So to really pursue our goals and our knowledge-driven behavior, our epistemic behavior.

by the means of these developing generative predictive models.

Let me explain it a little bit further.

So it's clear how I mean this.

So essentially also implied by the generative, by the free energy formalism and the active inference formalism, one can say that our brain is continuously on the one hand side in the present for sure, right?

I mean, we are ready while we, for example, write a letter here

We are ready to produce the next word and write it out on paper, for example.

We consider the recent past while doing this to be really in the present and fully embodied and grounded in the here and now.

is also continuously updated by our sensory feedback while we interact with environments or example we might update our belief how sturdy the pencil is or how hard it is and things like this and we learn about our world while we interact with it such as improving our writing skills and so forth but most importantly we use this

presentation of the presence and the belief where we are in essentially to predict the future what is going to happen if you like this future considerations and then essentially to pursue particular future desired situations such as finalizing writing this letter writing this word

producing a letter for another person and having overall purposes, also other purposes, of course, for the current day and so forth.

And these considerations, depending on our focus and the difference between the desired future considerations and the current expected future considerations, make us improve our behavior and act in a goal-directed manner with the environment.

One can illustrate this in a different way by essentially saying, well, the active inference framework essentially suggests that we are continuously in a predictive state of mind, ready to process the next sensory information, doing information integration there, producing local posteriors.

integrating this local posteriors this perception with our overall beliefs in our model so perceiving the shadow inferring that there's probably a person in light source that produces a shadow integrating an overall consistent in attempting to create an overall consistent a posteriori predictive state of mind and we use this state of mind essentially to um

roll out future considerations, including habitable behavior with our environment, test if we like this, if we want to go there, or if we want to pursue particularly other states in the future, choose the best one we can do, execute it, and then close the loop by using the reference copy to do the temporal prediction in the here and now.

Epistemic goal-directed behavior can essentially be formalized nicely in this reasonably short equation still to generate goal-directed epistemic behavior.

This comes from Friston and Pizzullo's and Al's work in 2015, which essentially

I think it's a very nicely formalized intuitive equation here, actually.

So let's see, we're pursuing the minimization of free energy under a particular policy pi at a certain state t. And pursuing this free energy consists of two components.

The first component is essentially the goal-directed component in this equation.

It's essentially trying to minimize the difference

between the observations that I expect to perceive when I pursue a particular policy pi, so a particular behavior pi, and compared to the observations that I would like to perceive under my internal model.

So this is essentially the expected divergence from desired future states.

And I try to then, of course, infer a policy pi, so behavior pi, that minimizes

the divergence from the expected future states to the desired future states.

And the other component in this equation is the epistemic part.

It's essentially the minimization, the attempt to minimize expected future uncertainty.

So it's the expectation of over my future horizon tau.

That's my timeline into the future, as you see here.

And it's essentially the expectation of future uncertainty, which I try to minimize as well.

So under my consideration of my policy pi, I expect future internal states m tau.

And these m taus under the policy lead to the expectation of future observations.

And then certainty of these observations or the observation densities, essentially.

So the entropy over that, the more uncertain

the more diffuse my expectation, the more uncertain my expectation, the less I like these future considerations.

And out of these two components in the equation, one can essentially derive agents that act in goal-directed and epistemic manners.

We have implemented such agents mainly

partially as rather still rather crude approximations simply with recurrent neural network structures here, for example, this rocket ball agent that was is controlled by a neural network on the fly, essentially by first training the network to learn the sensory motor model of this agent.

So this agent undergoes gravity and has inertia in the simulation in 2D.

and has these two thrusts that go diagonally downwards.

So it can counteract gravity and steer, as you see, essentially.

And the red line that you see here is essentially the imagination of the neural network where it expects to fly to over the next couple of steps.

So essentially the system rolls out, as I illustrated before, essentially into the future.

It's sensory motor dynamics, expected sensory motor dynamics, under expected motor control commands that it imagines executing.

And it then compares this, the resulting state with the desired state can take the K divergence or simply the Delta if it's deterministic.

and use that data to project it back onto the motor commands.

So that's the system essentially acts in its own best interest to pursue these goals.

The goals are given from the outside in this case.

So this is still a rather simple system in the sense that it's not probabilistic really, or tries to minimize uncertainty here, but just pursues goal directed behavior.

We have also done this with various other systems, for example, multi-joint

Um, um, that, uh, additionally has the constraint to avoid collisions, which are signaled by this hair sensors, as you see here, and you can also get quite some cool behavior, very similar principle.

And so to conclude the first part right now, I hope you have understood the active inference.

component and active inference is about essentially also suggesting implicitly that our mind is not only about knowing what is going on it also continuously learns about the world retrospectively so it's partially also in the past in some sense in its activity state and in the future considering um and inferring goal directed epistemic behavior however um

This is certainly good and we can generate closed loop control systems in this manner, which are closely related to model predictive control for that matter.

But in order to really become event predictive or more abstract in our scene right now, essentially you have seen a system that only is just writing something or is controlling a flying trajectory.

It's not really able to

decide to write a whole letter, right, in the sense, right, abstractly speaking.

So it's not able to abstract away from the actual current behavior.

So the question is, where do these event predictive structures that I was talking about come in?

And that leads me to my second proposition.

It's essentially that it appears to be

that due to evolutionary shaped inductive processing biases, our brain develops event predictive compositional structures.

These structures tend to model the hidden causes behind sensory perceptions.

I've already intuitively seen it.

I want to illustrate this further with an actual model behavior.

But before I do so, let me characterize where this proposition essentially comes from.

Where does this belief that event predictive structures are the right way to go, are the right way to abstract forward?

And it comes from the psychological literature, mainly.

And I start with a quotation here from Jeff Sachs and Barbara Tversky from 2001, which essentially reads, OK, event is

events have been characterized as a segment of time at a given location that is conceived by an observer to have a beginning and an end.

And their behavioral psychological work essentially suggested that when you have people segment movies or other little scenes or cartoons even and things like this, they do this very systematically.

So they have a really clear perception when an event starts and when it ends.

And

Out of this, it appears that we process our environment in terms of, we perceive our environment in terms of these events.

But important is that the event is not really in the environment per se, right?

There's no label.

This is an event, but our brain constructs these events.

And that's what Dara Baldwin lovely put into a recent special issue contribution.

So she says, or Baldwin and Cozy, they say, events...

The experiences we think we are having and recall having had, they are constructed.

They are not what actually occurs.

Because what occurs in the end is ongoing dynamic multidimensional sensory flow, which is somewhat transformed via psychological processes into structure, describable, memorable units of experience.

So essentially, there's no label of events, but our brain creates these abstractions and these compact encodings

of the events that we perceive that exist in our environment, but we construct them essentially.

They are not really existing on their own.

And over the last years, a lot of research has focused on this event predictive cognition.

And this research essentially investigates how events and conceptualizations thereof are learned, structured, and processed dynamically.

And this research line essentially suggests that event predictive encodings and processes optimally mediate between sensory motor processes and language.

Maybe I should have put inference processes here.

So this is a quote from our recent special issue.

So in summary, this event predictive cognition essentially comes from

um psychological literature um twersky's work with jeff sex and jeff sex continuation until today actually there's much more recent work from him about this it's rooted partially even more deeply in behavioral psychology if you look at the common coding theory of uh wolfgang clint's and ensuite behavior control theory of your achim hoffman and bernard hommel's work on the theory of event coding

If you want to learn more about this, allow me to point you to our recent special issue on this topic in the Topics in Cognitive Science journal with quite a variety of contributions spanning developmental, neural, and behavioral psychology, also linguistics contributions, and cognitive modeling, and computational AI contributions.

So what are these event structures now really about?

It's essentially when we think about particular event predictive structures, one can quickly get to the conclusion that events must be somewhat distributed networks that essentially predict the types of entities that are involved in an event, like a glass and a hand and an agent that reaches for the glass to drink out of it.

The relative spatial relations between the hand and the object, for example, and the interaction dynamics such as the hand really moving to that object.

These are event structures.

And then we have somehow this feeling of event boundaries and event transitions.

We know when a particular event can commence and when it can end, when it can begin and end.

And so, for example, when I grasp this cup here, I just can basically grasp it.

And once I have grasped it, I can transport it.

And then I can put it back down and I'm free again, so to say.

Combined with events and event boundaries, I can create mentally event schemas, which have previously also called scripts and similar things.

And I think these scripts develop naturally out of this event predictive coding principle.

And with event predictive coding principle, we essentially have now a much more clear idea, I think, of what

These scripts and schemata, as they have often been characterized before, actually are.

And of course, once we have schemata, we can embed them again in other events, forming hierarchies out of that.

So to get back to this illustration of writing a letter, we can now say essentially, well, our mind is not really in a continuous past consideration, but it considers past events, essentially, or the

unfolding of past events and notice that these do not need to be petitioned as crisp as it's illustrated here and we have certainly events unfolding in parallel for example if you think about the letter writing here right you're writing for example the second word here say scholar or so but you also have the full writing this letter event in your head this event of having sit down on the table and being ready to write this letter so there are multiple events

aspects, of course, that consider particular sub-components of the actual true environment out there.

And so, again, we know what's going on.

We can retrospectively improve our individual writing skills for writing particular words, writing in general, but also our skills of writing a whole letter and improving this, for example.

And then we can, again, pursue the events of finalizing this letter, posting it later on, and so forth.

So we have

full sequence in mind that actually then includes or concludes the full letter writing episode that is not only doing the letter but also putting a stamp on the envelope putting the letter in the envelope putting it to the post office and so forth so all this is included um for the fun of it i included this lovely video project here um so here we see an event of a

of a billiard ball, a pool table starting situation and somebody shooting the white ball.

And so we have a nice hit and we look, oh, some balls hit the hole.

No, they don't.

Actually, lo and behold,

they come back together to the starting position.

So I hope the video was a nice illustration of you definitely having expected something totally different, although not something very concrete, but at least the final state of the event that the balls will be distributed around the table in some form or the other.

Certainly they will not ever come back to the starting position like this.

And that's why hopefully some of you

at least a little smiling to themselves.

Yeah, that it's kind of a cool illustration.

Yeah, that our brain has these events in mind and it essentially steps, jumps ahead, right?

When the starting of an event unfolds, it kind of knows what the final situation is most likely going to be, even if it's not fully concrete, but it still has some in the ball situation, the pool ball situation, distributional sense to it.

So it's also really interesting.

And we have, lo and behold, actually formalized this event predictive active inference now in an event predictive active inference model where we modeled reaching behavior and visual eye fixation behavior in infants, actually, together with colleagues from Potsdam University from development and psychology side.

So now we essentially have the same model

equation again as before, but it's only conditioned now on events here, as you see.

So the whole equation is conditioned on in which events we believe to be currently in.

So the latent hidden state, this model we had before, is now an event model.

And so we put the M basically as the internal motivation.

So the desired observations given motivations are compared with the expected observations

pursuing a particular policy and under condition that we are in a particular event and a particular event series unfolds while interacting with the environment.

To illustrate this further, Chris Gums actually has done this work and also provided this following illustration here when we have now as the behavior policy here, for example, the gaze position of, say, an infant.

And so we might start to explain this equation a little bit further by the situation that, okay, the infant say he or she really likes teddy bears.

So basically it's really its internal motivation is to look at teddy bears and it really perceives a lot of pressure out of that.

So it really likes to look at teddy bears.

And so its observation is the desire to see teddy bear like objects.

And thus,

by pursuing a policy that minimizes the divergence from seeing teddy bears.

So it essentially will fixate teddy bears.

But now, when we now see that suddenly this ball here is moving, there might be a surprise in the observation because the baby probably has expected that everything else will stay rather stably.

So what is happening is that there is a large uncertainty what this ball object is suddenly doing and where it's rolling from.

And this makes the baby, for example, look at the ball now because it wants to know how it rolls and build a good predictive model so it's not surprised of the ball, given it caught its attention in the first place, of course, and so forth.

And it seems to be somewhat relevant for itself.

And so it actually predicts the next ball locations.

And it will also continue then to most likely, at least depending on the age, of course, a little later age probably than illustrated in this picture here, predict that this ball will eventually fall to the ground.

And thus, it actually will at some point anticipate not only the next ball position, but will look at the critical next ball position, which would be when does this object then suddenly fall to the ground.

Yeah.

And so we have kind of a two event prediction, the immediate next situation and the event boundary when the ball switches from rolling into falling and then expecting that it will fall somewhere on the floor and hopefully not on a teddy bear.

So to get this really important.

And lo and behold, the studies from Adam and Birgit Elsner

is showing that over the first year of our human's life, usually we develop this anticipatory event predictive eye gaze behavior.

So what they did is essentially they showed little kids little videos of hands grasping teddy bears or little claw like grippers grasping teddy bears.

or other simple kind of objects.

And what you can show is that when you track this baby's eyes, at six months of age, they don't show any anticipatory gaze behavior.

So they track the object, the hand that moves to the target object at best.

Usually, they even lag a little bit behind.

But lo and behold, with seven and a half months of age about, the babies start to anticipate.

And they do so particularly and only so when there is an effect, when the hand not only reaches for that entity, but then also lifts it.

So, of course, after 12 trials are done there in this experiment.

So usually after the first or second try and the hand really shakes the object a little bit.

then the babies start to anticipate.

And they only do so with about 11 months when there's no effect, when the hand just reaches for the object but doesn't do anything with it.

So then they stay on reactive or hand following behavior and only do this later on.

only anticipate in a later age, about 11 months old.

And interestingly, with a claw, at seven and a half, they don't anticipate at all.

So they don't see any agentiveness or any anticipation that this claw will do something with the object.

But with 11 months of age, they essentially show the similar behavior than when they watch a hand with seven and a half months, and the hand lifts the object, and the claw lifts the object, then they also start to anticipate.

with 11 months.

But only with 18 months, they also anticipate when the claw is not lifting the object, but just moving there.

And lo and behold, we simulated this with our event predictive active inference model, essentially assuming, so to say, simulating that until about six months of age, they just cannot interpret the event of an hand reaching for objects.

And so they just cannot make an event

case out of it and they just process the unexpected information that is the hand starts moving and they track the hand.

With 12 months of age, it's similar still when they see a claw, but with 12 months of age, when they see a hand and an object, they imagine probably this hand wants to reach for the object.

And so once the hand starts moving,

they will move their eyes to the object to be ready to process what the hand is going to do with the object.

And in fact, we modeled this now with an event predictive patient modeling approach, essentially, that essentially implements this active inference equation on the event predictive level that I just showed.

So it's essentially first trained to learn a probabilistic event predictive

key matter of the unfolding dynamics, starting and ending conditions.

And then it applies active inference of the current best event interpretation first.

So it first needs to know which event.

So if it sees the situation, it needs to infer, ah, a reaching event is most likely to unfold.

And once it's certain that the reaching event unfolds, then it knows, ah, reaching ends with a grasping and doing something with the object.

So I will look there to see

minimize my future expected uncertainty minimizing this anticipated uncertainty of what the hand is going to do with the object and maybe you can look at the results in detail in the paper essentially what is happening is that the system learns to during training to infer the correct events and then during testing after sufficient training phases rather quickly

it starts to anticipate to when the reach starts here over time, it starts to anticipate and look at the goal object because it has encoded this as a reaching event.

And I know how the reaching event unfolds.

And thus, I look at the end of the reaching event because I want to minimize the free energy that I anticipate unfolding in the near future while reaching and grasping

an object because this is a situation that I'm currently in.

So that's what the system emergently, so to say, does.

That's why emergent goal inspiratory gaze by this active inference or event predictive active inference formalism.

So, okay, we have shown that there are static latent encodings that can nicely

um foster the emergence of this unspiratory behavior but can such or how can such event predictive structures actually learned this is what i want to show you next but you could also have a short break if you like daniel or what you say um maybe ask and just double down on a few of these cool points and ask a few questions and then jump into the second part of the presentation


SPEAKER_00:
Yep, please.

Okay, um, so a few just kind of general questions.

One was, you brought in the neural network angle.

And so it was just a general question, how do the analytical single line equation, formalisms connect to different modern machine learning architectures?

And what does that have to do with the scaling or where active inference could be applied?


SPEAKER_01:
I think I'm going to answer this in the second part of my talk.

Perfect.


UNKNOWN:
Okay.


SPEAKER_01:
This is going to become much more neural networks just now.

And I will conclude also with a general statement about this.

So maybe we keep this for later.

Essentially, I mean, this was an LSTM network.

LSTM networks are used for


SPEAKER_00:
state-of-the-art machine learning so it's not like something that is like totally trivial or so great and the event basis is something that I'm sure you'll return to but you mentioned how events have boundaries and schema or schemata and hierarchies and one thought was how are these

boundaries, schema, and hierarchies learned in humans?

And how does that inform our design of cognitive systems?


SPEAKER_01:
Yes, also, this is kind of what I continue with essentially.

This is the question here.

So how are these event predictive structures learned?

Not so much in humans, I will not so much focus on in the talk now, but how can these be learned in artificial systems?

but inspired, of course, by human learning.


SPEAKER_00:
Well, I'm maximally uncertain and curious about those.

I guess implicitly expected and prefer that they be resolved.

But I hope so.

It just reminded me of how they're encultured.

Like at the end of a movement of a symphony, if you clap, you didn't get it because that's not the encultured moment to clap.

There's a broader event that actually goes beyond the sound.

And so it just made me think about how

through learning we reconceptualize what events are and if events truly are one of the uh kind of atomic units of cognition then that's incredibly powerful

I would think so, yes.

Was this a question or a comment?

That was just a comment on the importance of having event-based cognitive processes and how it reconsiders our own experience and suggests how to design other architectures.

So maybe we can go to the second part of the presentation.


SPEAKER_01:
The fantastic part is really that it goes hand in hand.

While these events develop, we are getting more and more ready to look and explore deeper event structures and more complex structures.

event structures.

And of course, the real mystery is how these event structures are learned.

And I want to show you a couple of ideas that we are pursuing in our mainly machine learning research part.

And in the last part of the talk, I will link these event structures also to language structures to a certain extent.

But allow me, in the interest of time, to go a little bit fast over these machine learning components, because to explain the machine learning architectures in all their detail, it would take talks on its own.

But I just want to give you a little bit of an idea where we are in this respect.

But the main question that was asked, and that's also a really good question, is essentially, what are these...

can our brain learn these event structures?

And there must be, essentially, in machine learning jargon, these inductive learning biases, these inductive tendencies to learn these event predictive structures.

And ideally, to learn them in a very compact, very suitably compressed form to identify the causality that

generate the sensory perceptions in the first place and allow them to combine these event structures in a very compositional, very flexible manner such that we get ready to apply our knowledge also in other situations that are related to previous experiences, but of course, always different.

And I've argued before that essentially there must, on the one hand side, be event-oriented interpretation tendencies in general, as we have already seen in the quote from Deb Baldwin, for example, also.

So we must essentially foster the development of this event-predictive, stable, and compact latent states.

This is what our brain seems to do.

And it seems to develop these event states in terms of attempting to characterize starting conditions, contextual conditions, and ending conditions of events.

So contextual conditions means like when can an event unfold and what is happening typically, why an event unfolds.

The starting condition means

when can an event start?

So I can reach only for an object when it's in reach, for example.

And the ending condition is essentially when an event ends.

So when a reaching event ends is typically when I grasp an object, for example.

And it appears that there is event predictive biases, is inductive biases to segment our stream of information into these events

And two very important ones are probably signals of surprise because usually when I don't have a good event or when I don't know exactly how an event unfolds, I get tubular first better to, for example, extend my hands, reach for things and so forth.

But then when I touch them, I fail to grasp them properly and they fall down and this is surprising.

And then I, um, get surprise signals, which over time, um, become non-surprising anymore because I know.

that when I reach for the object, then typically the grasp unfolds and I prepare for the grasp and then I manage the whole sequence without the surprises.

And the other one is this latent signals of stability because, and this also relates to causality in a sense, because our, I mean, our environment, our world is a three-dimensional space-time continuum essentially, and

forces or causal interactions can only unfold when two entities can exchange these forces.

And this is typically in the physical world only possible when they come in contact to each other.

Now we are in Zoom and we can exchange information over this long range means, but also in the sense we are in contact right now by this tool, by this digital device essentially and the internet

that enables us to now establish virtual rooms.

That's why a Zoom room, for example, is also called a Zoom room, because it's a virtual room where we are together, not physically, but information-wise, we can exchange information and thoughts and ideas and so forth.

And that's essentially very similar to being in the real room, where typically similar things are mainly unfolding.

And so this is the event predictive biases that make us structure our environment and our thoughts and our general predictive models in this event compositional manner.

And the other one is, of course, the importance of curiosity and homeostasis to make sure that we, on the one hand side, essentially

I mean, our brain does not have the capacity, our brain has some phenomenal capacity for sure, but certainly doesn't have the capacity to learn everything in all its detail about the world.

I mean, it's totally impossible.

I mean, otherwise we would need to learn down to quantum mechanics, how everything unfolds the whole time, right?

So this is absolutely impractical.

So of course we need to develop models

that we believe, so to say, are best suited for our needs.

So while we build our models and while we pursue active inference, as you already seen in the active inference equation, there's this important component that essentially tries to minimize the KL divergence between desired perceptions and actual perceptions.

And this is essentially maintaining internal homeostasis

friston would argue also that is this perception right that's not it's not necessarily only about the outside environment but it's also the perception of your own body right so if you desperately hungry um or starving or whatever right then of course you do everything to prevent this from happening um hopefully uh yeah it doesn't get to this um and uh and

curiosity meanwhile of course drives our curious minds our knowledge gain experience but also this knowledge gain experience is partially embodied so also our sensory system kind of signals to us what is probably interesting for us and it's going hand in hand with the homeostasis component we build models mainly about the stuff that really interests us and that we believe is important to us in our rich cultural social world of course this can be

very awkward, very cool artistic things and so forth, but nonetheless, it is still in the human realm important.

But when we try to implement this now in artificial neural network structures, I want to show you a couple of brief glimpses at least at what we have done in my group over the last couple of years.

The first

that I want to show you is a derivative of this Rocketball system where we now have, again, similar recurrent neural network structure, but we enhance the system now, not only processing sensory and motor information, predicting sensory consequences, as in the Rocketball thing and that multi-joint arm that you saw before, but also we allow it to give a contextual input state, which is essentially like an event state

that develops the ability to distinguish between different vehicles that the system is currently controlling.

Notice or recall that the system as a sensory input gets XY positions of the vehicle and motor commands are these two or four thrust motors.

So it doesn't see which vehicle it's currently controlling.

It only sees its position.

And so in order to distinguish the different vehicles, it either needs to enhance its latent state within the long-short-term memory structure, the current neural network structure, or it needs to contextualize a latent inductive bias, essentially, here.

And by training this, it, lo and behold, can do this.

Now, this is the trained model, and it has learned to control these three vehicles.

that have different inertial and gravity properties and also different thrust motors.

It does this now essentially by, on the one hand side, continuously doing this active inference with a line that you see again, the thought,

projecting where it will be in the near future, but also it retrospectively continuously adapts its internal event estimate of which vehicle it's currently controlling, or not really of which vehicle it's currently controlling, but rather what's the best contextual state that allows me to predict the current sensing motor dynamics in the best way.

And it does this rather well.

Lo and behold, if you plot

the internal state, this contextual state that emerges, that's not trained or in a sense, not trained in a supervised way, but it's trained via an active inference process, essentially.

So it emerges.

like a distinction between the three different vehicles, by just simply optimizing this forward model of the different dynamics of the three vehicles and having the inductive bias that's important here to succeed, essentially that the vehicles do not switch all the time, but they are stable for a while, like about 50 steps or something, and then they randomly switch at a certain point in time.

And the inductive bias is that this contextual vector that's hidden here, essentially, is a stable vector that only adapts itself much slower than the internal dynamics of the actual recurrent neural network.

And then during training, such these structures typically merge, distinguishing the three vehicles.

You can even train this to transport objects then.

There's some modularizations are possible, are necessary to do so, but admittedly,

um these these structures do emerge but not very robustly and the the structure of this latent state is still typically rather fuzzy so over the last two years we essentially have produced a couple of other neural networks that really try to work on this compression and the suitable compression of these latent states further

We have done this with various gating networks originally for still providing surprise signals.

So essentially surprise when a switch occurs and we just give it a switch occurrence signal.

That was last year essentially where we developed nice latent codes for predicting some abstract functions, for example, and distinguishing between them, even seeing some similarities between them.

But then this year,

At the CoxEye meeting, we have done this again.

So we see we have this event anticipation module.

This is a still rather simple neural network.

Then we have an event switching module that's a gated recurrent unit network here that allows the next anticipation of the next event code or the passing of the next event code into the lower level event processing structure that is actually processing the sensory information.

only at certain points in time when this event boundary anticipation network actually activates the switch.

And this event boundary anticipation network learns to activate the switch by particular suitably designed inductive learning biases

So the first inductive bias is essentially design of the model that you develop, essentially an event processing module that is contextualized by the belief of what's the current most suitable event code.

So what's the event that's unfolding?

And then event boundary anticipation module that essentially switches between events just in time.

And lo and behold, this system actually really does learn to switch when the information is there that it can know when the event switches.

that it learns an optimal model about the switches.

And if you have, for example, in this case, we use like one hot encoded symbolic sequences and different sequence processing automata, basically then the system developed distinct event codes for the three types of dynamics, for example.

And interestingly, because you see to P1,

the event one or program one essentially just switches between A and B, P2 switches between B and C, and the problem three switches between A, B, C, B, A, B, C, B. So it's a kind of a combination of problem one and problem two.

And so lo and behold, the P3 code essentially is between the P1 code and the P2 code.

in all the cases, although, of course, the different initializations, there are three different networks.

They, of course, develop different latent codes, but they still kind of imply the underlying structure of the events and the similarity between events.

But maybe the most advanced network is the one here, again, from Chris Bogum, together joint work with Georg Marcius.

here this paper now that essentially also has a very similar structure.

You have a hidden latent state as in the previous code that's essentially in this network.

It's this code that is passed down through here.

And this hidden latent code is maintained over time and is just controlled or can be adjusted by a gate.

That's a multiplicative gate that opens only very selectively and this gate is

is designed such that there is a loss function that punishes gate openings.

So the system really doesn't really want to open its gate, but if it's really helpful to lower the prediction error on this side, it does so.

It starts to open the gate and thus changes context with this recommendation system or this event coding system that has the next event code

ready to switch to it just in time, and then combines this to predict the next consequences.

So again, a sensory motor event predictive model, essentially.

And this system is now quite ready to process state of the art

challenges here, not only toy problems as in the admittedly in the last in the two other systems.

So for example, there's a billiard ball scenario, which is a kind of a benchmark in machine learning community where our system with suitable parameterization decreases the mean squared error much more than standard LSTM or GRU or standard recurrent neural networks.

And particularly when you have a testing

scenario that differs a little bit from the training scene and what's even more important maybe or more interesting in this scenario is that matching to this illustration here you see essentially that the latent state of the our system called gate lord essentially you can really see that there is a particular dynamic unfolding

So first of all, it rolls in a certain direction.

After the first interaction with the boundary, the latent state perfectly switches to one stable new state.

And then you see the second bounce.

And again, it switches to a new state.

So that's very powerful.

A little bit hand waving due to time cannot explain all of the components here.

but you see a really nice generalization behavior.

For example, when you train only this fetch and pick and place task in situations where the gripper object contact always occurs at time point five, and in generalization, the contact also occurs at other time points, immediately the Alice-Telman GRU networks get much worse in their predictive accuracy.

And Gatelord still generalizes also to these other scenarios.

And there are a couple of other also combinations with reinforcement learning systems.

So actually, Christian took that Gatelord module, essentially, this model learning module, and combined it with state-of-the-art reinforcement learning systems.

And as a result, we could learn faster, be more sample efficient, essentially, and be partially more accurate

than the state-of-the-art reinforcement learners in a couple of these mini-grid world tasks, for example.

So really important to induce the right inductive learning biases.

These essentially improve the latent state codes that much more systematically seem to develop really kind of an explaining code, like explaining the causality

with a latent code about what's really going on in the environment such as where the sheep for example is or where the goal position will be if the system remembers where the key is on one side and so forth.

So particularly very suited also for partially observable Markov decision process problems so where you need to maintain longer term memory of particular events that happened in

particular environments.

OK, so that's the second part of the neural networks.

And if you still have a couple of minutes, I try to put you through the last component here that I want to show you that this event predictive structures can be very closely related to language.

And that I find most exciting because this might in fact really close the language gap and really ground language in our sensory motor experiences by this tendency to develop event predictive structures that are then very easily linked to the language that each of us experiences while we grow up.

Let me show you what I mean by this event predictive structures in terms of a language domain.

Let's take this.

this example of a ball that rolled down the table because it was crooked, for example.

And so if you read the sentence, actually, it was crooked.

You might think, well, the ball was crooked.

That's why it rolled down.

It probably was the table.

So how come we are able to make sense of the sentence

with it particularly being ambiguous, being referential ambiguity here, being able to be bound to the subject or the object essentially.

And so what I mean, what happens when we read such a sentence is essentially that while we process sentence or we read the ball and we create probably a kind of a predictive encoding structure of the ball.

So the ball can roll and bounce and it gets repelled.

when it hits something and it has some particular size and possibly we also imagine some kind of particular ball with like a soccer ball or something, depending on what I usually not very visual person.

So I don't usually imagine that an actual ball.

And then I then read rolled down.

So the rolling is much more active than the falling at this point.

And I imagine the ball essentially rolling somewhere somehow

And then this somewhere somehow is specified.

That's the table.

So it rolls, uh, um, down the table.

So apparently it's, uh, there's a table and there's a tabletop.

So it's probably on the surface of the table that the rolling event unfolds.

And so I put this together and I have this rolling down as we've seen before the baby expecting the rolling and the falling.

We've now this event structure, right?

Of a ball rolling down, meaning like it rolled first on the surface and then it probably dropped down, uh, somewhere.

thus rolling down the table.

And now we read because it was crooked, essentially, and this essentially implies due to the because situation, right, that essentially was the second part of the sentence was not true, like this is explaining part of the sentence, the because part, right?

So was it not crooked, then the rolling event would not take place.

And this essentially counterfactual reasoning, right?

So can it be that the ball is crooked and thus,

It rolled down the table.

And if the ball would have some sort of crooked dent or whatever, it most likely has not rolled down the table.

So that seems to be not so plausible.

But a kind of tilted, crooked table seems to be much more plausible.

So most likely, it was the table that was

assigned with a pronoun, right?

So we can, we have essentially analyzed the whole event with its causality in it by imagining the words that we read and the sentence structure that we perceive as grammar and generating an imagination of what we perceive.

And when actually we have this year with together with Christian Stegemann Phillips, we have published a paper or the so-called learner architecture,

which essentially learns about an environment, an event predictive structure.

It is trained or informed how to link the individual event components that it has learned out of observing an environment with a language processing system.

And then it can, in fact, do just what I have illustrated.

It can disambiguate ambiguous sentences by creating

concrete imaginations of a described state of affair.

So for example, we have here this scenario.

Unfortunately, it looks like little viruses here.

This has nothing to do with corona or so.

Sorry about it.

It was created before the whole damn pandemic.

But nonetheless, what's important is that these creatures here push down stuff from these platforms and our learner system analyzes these things and essentially creates event predictive structures out of it.

And so then after it has learned this, it can essentially generate sentence interpretation.

So after training, essentially you can create, give it a sentence such as the green virus rests on the platform and it moves to the box after it falls.

So rather complex sentence.

And what the system then is doing, it can create itself out of the entities that are uttered like a green virus, a platform and a box.

um which has been decided to be green here as well that's randomly chosen then maybe it's not specified it arranges these three objects in such a way such that it it can imagine an event sequence unfolding in this constellation to make the sentence true so you can see this in this video here so essentially the system uh learned to

generate this constellation that you see and the virus lo and behold falls on the platform actually and then it rests on the platform for a little while and then it moves to the box after it has fallen on the platform so this is the interpretation of the sentence that the system has created and i think it's a nice illustration hopefully for you also convincing illustration that this event structures or sentences uttering event structures can be

What we make of it, what meaning making sense of a sentence is essentially something like this is the internal creation of a consistent event or event progression that fits to the described sentence structure, disambiguating, of course, certain temporal and referential and so forth ambiguities that might be inherent in the sentence and grammatical structure that is provided.

We have also just recently shown that

this event predictive inference structure also nicely fits to the rational speech act model actually for that matter and one can nicely model learning about the preferences of others this essentially this example in this study goes with actual behavioral experiments done on mturk here so for example you can imagine

when you have a scene like this and this is admittedly very abstract entities here and so for example you have the scenario and maria wants to signal an object to the following scene in the following scene to samantha so maria maria says red please take a red one essentially and samantha chooses the outline object like such as she takes the red cloud here red stripe cloud then you can possibly infer something about samantha's preferences

Lo and behold, she might like, for example, clouds more than circles.

But you don't know anything about squares because there was no option to choose from squares.

And you can also, by the active inference formalism within this formalism, actually, you can then also do it the other way around.

And you want to, for example, possibly learn about Elizabeth's preferences.

And you have only the scenario can give options to choose

amongst objects.

So for example, if you want to learn about if the person prefers clouds or circles, you could, for example, pick one of the green ones.

And then you would see if the person takes a cloud or a circle.

And then you have some hint that the person might prefer clouds or circles, for example.

And we have modeled this in this recursive inference process, very closely related to the active inference formulas in the USA, typical KL divergence, again, where we essentially compare our prior knowledge of feature preferences over our expected posterior knowledge of feature preferences while pursuing a particular action, a particular utterance.

And when we compare this to the actual behavior of the human participants, we get really good fits and in fact could show also from an information criterion manner that our cognitive model fits better than other competitive models.

models and this leads me to the end of my presentation yeah so uh just the language really briefly in the end but i hope you see what i mean by that this event predictive cognition essentially and the active inference processes that unfold which is within this event predictive cognitive systems and our brains for that matter most likely are very compatible to linguistic structures

So what I have shown you today, and I hope you could follow me so far and it wasn't too much, that essentially our generative mind may have the self-motivated objective to act highly flexibly and goal-directed and pursue epistemic self-motivated actions.

In general, this is quite clear to survive, right, to interact with our complex worlds, to seeing that we have become this crazy human beings with all our social likes and hates and capabilities and

intelligence and so forth.

On this human cognitive level, event predictive conceptualizations seem to be really important to enable us to act in a deeper goal-directed self-motivated manner and to conceptualize our environment as to be able to communicate and interact and cooperate with others and compete also with each other for that matter in a highly more sophisticated manner than any other animals can.

To learn such conceptualizations, we really need this inductive learning biases, and it's certainly not fully figured out yet how this learning and processing biases are working or functioning.

But we know and can show clearly that these conceptualizations are really good to enable deeper goal-directed, self-motivated planning, reasoning, counterfactual reasoning, filling in gaps, filling in unknowns, disambiguating ambiguous situations.

and pursuing abstract and concrete behaviors on multiple levels.

And last point was that language seemed to be mapped really rather easily on this, which essentially possibly might explain why we as babies naturally learn our mother tongue without overly much effort, and particularly the complexity of the grammar behind it also.

On the artificial intelligence side, I would like to conclude that the current deep learning systems, mostly because they don't foster this generativity and this conceptualization of structures, they do not really foster event predictive generative models.

And as others have said,

Thus, men may call the more simpler architectures that are nonetheless including systems like transformers and so forth really still rather highly computationally demanding stochastic parrots.

Arguably, and interesting to discuss this, of course, I believe that if we develop more event predictive generative systems, then we actually foster the development of

the learning of generative models that is and ideally causal models of the actual true causality that we encounter via our sensory motor experiences, enabling us to generate much more robust forecasts, possible action recommendations and explanations also why we would recommend particular things or forecast particular things because we essentially

are designed to learn about the causes that lead us to predict certain things so we can also of course talk about the causes and in my opinion these systems may then indeed yield strong ai or artificial general intelligence where self-motivated learning will be inevitable part of such systems and self-motivated learning and self-motivated behavior also thus will be part of the systems

And so I think we should be aware that there's no reason, I see no reason, research on human cognition and its functionality.

There's no reason why an artificial system should not reach such intelligence level.

And so, as others also have pointed out, I want to conclude with a word of caution, essentially, and a word of awareness that if these systems come into being, come into existence, yes, we need to make very much sure that we don't design them just for profit or for some individual personal interest, but we better put good purpose.

into these systems and it's a good idea to think about this now also this concludes my talk thank you so much for attention parts of this general motivation background you can find in my book from 2016-17 how the mind comes into being and i acknowledge some funding from humboldt foundation and dfg mainly and thank you for my team actually to produce much of this work i don't have a team slide here

But thank you for your attention as well.


SPEAKER_00:
Awesome presentation.

Thank you very much.

Thank you.

Maybe you can unshare and we can have a little discussion.

Definitely.

Well, tons of very interesting ideas.

Anyone can write a question in the chat, but let me just start with a introductory question.

How did you come to be working with active inference models?

What were you working on before?

Was it a system specific or question specific path that led you to integrate these novel components into active inference?


SPEAKER_01:
Yes, thanks.

I'm just a little bit confused because it's this camera now.

Okay.

Let me actually turn to this screen then.

Yes, well, I have studied all my research career, essentially started to study anticipatory behavior from the very early on during my diploma, bachelor's degree studies.

essentially coming into contact with a psychologist in Wurzburg, Joachim Hoffmann's group on anticipatory behavior control.

And I think in his work, he already essentially from the psychological perspective, um, of, uh, formalizes, um, this principle of active inference on a, on a very crude, more in words, um, specified psychological level where, and I started very early then to also realize that, uh,

that the devil is in the details.

And the big question is really, what are representations like?

What is the representations that these anticipations, that these predictions actually unfold?

What's the nature of these representations?

And even nowadays, we still see lots of machine learning work where representations are given in advance and so forth.

and, um, or are just totally done by, let's say the Atari, um, games or the successes.

I mean, there, you just give the plain image and you, you kill the problem with so much data that it's the, um, converges, but the system is not really learning the systematicity, the actual structure of the problem.

And, but most recent, um, advances, of course, also from others, for example, the, um,

the dreamer architecture or the planet architecture from, from what's his name?

Hafner is the last name.

This goes also very close in this direction, but no, no.

Yes.

They goes in this direction, but they don't really foster event predictive latent codes yet.

And I think they should.


SPEAKER_02:
Cool.


SPEAKER_00:
Are you,

have some predictions about where you might go, but just to sort of restate it, you said just in your previous answer that it was a crude active inference in words.

And so I wanted to ask, what is active inference crudely in words?

And then what key pieces of the formalism today refined those words?

And then what do you see being incorporated into the formalism going forwards?


SPEAKER_01:
So what's active inference, crudely speaking, and what?

Well, active inference essentially formalizes how our minds and how clever learning systems should infer and develop generative models about the world and use these models to pursue self-motivated, goal-directed behavior.

That's the general formalism.

And my point today was essentially that this formalism is overly general in a sense, because it doesn't specify the nature of the encodings, of the predictive codings.

And evolution has obviously shaped our minds such that we have particular expectations of the structure that's suitable to

model the outside environment and thus interact with this environment in a more flexible adaptive goal-directed socially competent manner and that's where the event predictive stuff falls in


SPEAKER_00:
Thank you for the answer.

Now, a sequence of events might have a narrative, could be said that connects them.

How does narrative relate to your work here?

Is that just a bigger nesting event or how do we connect micro scale events, which it's very nice how you, you know, rolling off the table was a clear demonstration of that kind of an event.

How does that connect to broader narratives?


SPEAKER_01:
Yeah, very good question.

Very good and very interesting.

I think, as I said, as I tried to imply also that the events exist on multiple levels of granularity and precision and an event can consist of yet sub events essentially, right?

I mean, like the whole episode of writing a letter, for example, consists of writing individual words, sentences, paragraphs,

thoughts expressing individual thought components and so forth, right?

So an event per se can, I mean, you have the event of your current day, right?

And you compress this as an event.

So that's the beauty about it, I think, also because an event is not, there's not only one event unfolding in our minds

in every here and now situation.

But it's about this different aspects that have beginnings and endings, right?

Like this meeting here, but my answer to your question, my individual sentences, my individual words, my actual utterances that I produce.

These are all events nested within each other.

And each of these components is characterized by certain

stability situations like the whole while we have this meeting we are in a certain interactive situation and this is more or less stable during the whole meeting I guess I'm the presenter here today and so forth and then we have listeners and you are the mediator so to say right

And this is the situation the whole time.

And this is stable from the beginning of the end, from the whole situation, right?

But when you pose me a question, then you focus in on this question answering situation event.

And that's one particular event within nested within the other event.

And while I produce my train of thought, there are also the events again unfold, right?

So there's a lot.

And each of these events, though, is characterized by certain stabilities that are unfolding in

such as the question answer situation where we focus on a particular component of this current talk and the current topic of this meeting.


SPEAKER_00:
Awesome.

What that reminded me of was Kronos and Kairos, two Greek word roots for time and the chronometer, the decimal points of the time.

And there's nesting of chronology, like the second is within the deca second or the minute, the hour.

But then Kairos, this more semantic or action-oriented kind of timeliness also has nesting and that can be perfectly...

nesting, like the example of the letter with a sentence, the word.

et cetera.

It's funny because it's called a letter, right?

At the bigger and the smaller level.

But then also the boundaries could be subjective.

So not to say arbitrary, but literally defined by the subject, learnt by the subject.

And so it seems to be no problem to have machine learning systems that can nest Kronos very well, but how do we achieve inadequate nested action-oriented representation

Um, any thoughts or I'll ask a question from the chat.


SPEAKER_01:
I think, um, we, that's a very important part and that's also the beauty.

I think about the event structures, right?

That essentially it's not, it's always subjective because it's the stability that each of us perceives that makes an event become an event.

And I mean, because our world for us is all the same.

We are very similar in this event structures, but nonetheless,

Of course, the events are very individually unfolding in each of our minds, and they are characterized by these stability and systematicity properties that each event has.

And the nice thing about it as well is that the petitioning and segmentation is extremely flexible, right?

I mean, you can, it doesn't depend

on time.

And in order to implement this in machine learning systems, you need to have a system that can flexibly maintain particular latent states over extended periods of time and systematically change them when the time is ripe, so to say.

And this is what we have done with the gate load system that I have showed there towards the end, which is going to be published in December.


SPEAKER_00:
Very cool.

So I'll ask a question from the chat.

So Dean has asked, entertaining counterfactuals is time distributing and time provides the twist like a Mobius strip.

So if we include diachronic continuity with events, episodic partitioned, do we break free from training?

A classical Dean phrasing, to be sure, which I just read literally rather than choose to interpret.


SPEAKER_01:
Maybe you can help me with the interpretation, but I would say the counterfactual component was the problem that the counterfactual is time demanding or consuming or mentally challenging essentially because time is on the issue and so I need to be

So I cannot do counterfactual reasonings on the fly.

And that's certainly true.

We fall typically in our best explanation very quickly and habitually.

And only when we really realize errors, we fall into counterfactuals.

I usually give the example with a cricket ball, or actually usually I give a simple example, which is non-dynamic, like the ball fits into the suitcase because it is

large, for example.

So the ball fits into the suitcase because it is large.

So it is the suitcase that is large and not the ball.

And that is kind of also a typical garden pass effect in linguistics.

You fall into the

Best interpretation, the pronoun it usually fits with the subject.

And so your best hypothesis to bind it to the subject and you read on and then you get to the final sentence and then you try to imagine why does the ball fit in the suitcase?

And then essentially you realize you have to revise your model.

And that's kind of the counterfactual part that only happens, I think, when there is surprise or other reasons to really pursue this additional cognitive effort.


SPEAKER_00:
Great answer.

Let me see if there's a link to what Dean suggested and the stochastic parrot notion.

So if we think about a stochastic seed preferring, seed rewarding parrot, it's like there's some maze and the seed could be on either side.

And so it's rewarded when it's discovering that, it's strengthening that, and then it switches and then it just stochastically sometimes investigates the other branch and then reinforces that stochastic

Now that's all time bound.

If we think about the event as the context, then the counterfactual, the continued possibility of imagining a counterfactual is kind of what allows the rapid moving from the training on the side that it has been on and in one step with one gate opening

we can counterfactually enter into a different context.

And then he adds, it's bi-directional and the not present is that now, let me restart this literally.

He says, the not present is now plus then and then plus now.

This is our ability, both episodic and diachronic continuity.

That is the what if the mind can work out.


SPEAKER_01:
Yes.

And that's, that's a, that's a beautiful part, right?

I mean, our mind can essentially buy this gate openings, create a switch between alternative events or event, sub event components, switched entity property,

uh switch other aspects of it and and uh and it not needs to do so it's not only doing this by randomly switching these things because it usually switches them very systematically in a meaningful manner that actually the consequence of an event actually changes it well right because you usually do what if questions that actually change the further event progression and you can do this mentally really well and i think the um this essentially uh can

could be done and should be further explored with such gating mechanisms.

Really cool component for future research, I think, to really show that such systems not only are able to explain events, but can also really reason in terms of counterfactual manners then by saying, well, if I switch, if this property would have been different, then actually the whole thing would have not happened.

This is actually also really important to

assign blame, for example, to people.

Would you not have shot the ball or something like this?

And so that's also very socially relevant lo and behold there.


SPEAKER_00:
as well as designing education and thinking about when are we trying to, when are we fine tuning, but not opening or changing a gate?

And then it made me think about that, the equation, which you did an awesome job to walk through and show and read it because like sensory motor inputs, the equation is a linear string.

But then we can engage in this counterfactual.

Well, what if the inference on observables weren't conditioned on policy?

And it's actually to have competency with those equations is to know a little bit about counterfactuals.

And then on the cutting edge, you're drawing from other fields.

And you're asking, looking at Pizzullo and Friston et al.

2015, and then asking, well, what if this formalism included another formula?

event-based cognitive framework that was not previously connected.

So it's like the learning trajectory for those of us who are learning the equations includes the counterfactuals and the gate opening and then research is just kind of going one step further and so just made me wonder about how we design education to accommodate this kind of gate opening and counterfactual space.


SPEAKER_01:
How we accommodate it

education um like what what for what kind of education you mean yes what which education you're referring to human human education for learning active inference or for learning another area yeah i think that's a good uh an interesting far uh

thought i would say um it's it's i think it's extremely useful for sure to yeah particularly also to look at equations and and analyze their components and then um progressively also enhance them or selectively compact them take kick away out the one summit or the other and then you see exactly what's happening so i guess

Hopefully, this was educational how I presented it, I guess.


SPEAKER_00:
It's a flow of sensory inputs, and then there's some discrete changes.


SPEAKER_01:
And event structures, if you think about the individual components in the equation as event components, in a sense.

Mm-hmm.


SPEAKER_00:
Very cool.

I have one more question and then maybe any closing thoughts.

You mentioned the Zoom room or the Jitsi room, online events, calendar events that we all get invited to.

That's just very interesting with how we're bringing our evolutionary and our ecological behavioral embodiments into the digital space.

And so-

What does it bode for online events?

Thinking beyond just webinars, but like, how do we think about online events?

Because in one sense, you're very local to me, you're in my headphones, giving me the sensory flow, but you're on a different continent.

And so where do you think, or what theoretical questions or applied questions does it raise to think about online events, which are a novel affordance from an evolutionary perspective?


SPEAKER_01:
I mean, I think for the one good of it, they have shown us that we don't need to fly as much around the globe as we have done before the pandemic, which is probably a really good thing for our planet.

And we should acknowledge this and accept this, I think.

I have the impression that particularly older colleagues have issues with that.

And I think it's very good.

important thing that we should be all aware of that and shout it out loud as I do right now that we are in a very severe situation for our world and this pandemic has shown us also and even shows us now how the humans are so easily able to neglect

things or deny things that are so obvious that they are occurring, which is very scary and very unfortunate for our own future.

Meanwhile, saying this, I think it was a great opportunity for all of us to acknowledge that video calls like this work really well.

And of course, after one and a half years of pandemic, I guess all of you guys also enjoy local meetings again and going out with other people.

So we also see how social we creatures are and what is missing in such video calls, which is, of course, the personal interaction and really the development of getting a feel of how the other person really takes also personally and when you can make a statement about

what's actually happening in true life out there.

That's, of course, very different.

Unfortunately, not possible via Zoom calls.

But so I can only urge everybody that we should be aware that when we fly around the world, we better make it worth the while and don't do this only for a couple of days.


SPEAKER_00:
it made me think of a trilemma fly.

We could have met in person.

We could have met in Iceland or something, somewhere in the middle.

There is an impact in the hardware and the bandwidth of a video call, and that impact is very cloaked.

And then there's also machine learning models.

And so maybe somehow, I don't know, if X is the impact of a flight,

is a video call 0.1 or one thousandth or one millionth of a flight and making some of these impacts that are very real clearer so that we can design events that are policies that align with our preferences.

to reduce our uncertainty and to connect socially, but also to pragmatically keep certain ecological parameters within a region that we can thrive in.


SPEAKER_01:
Yes, absolutely.

That would be good.

I mean, a video call is certainly even less than a million, I think, from a flight, although, of course, it is significant, but it's, I mean, incomparably less.

uh and but nonetheless one has to be of course also be aware that um yeah unfortunately we are in a situation where we really need global efforts so one cannot really point fingers to individual persons because we need global policy changes to save our own world and our future of our children and i have three kids and i'm very concerned


SPEAKER_00:
Do you have anything that you'd like to just leave us with, or that's a perfect closing note?


SPEAKER_01:
No, I don't want to leave on the downside.

Um, it was a great pleasure to, to do this, uh, with you guys.

And now we drift a little bit to the climate situation and I'm, I'm thankful that we also can talk about this a little bit.

And, uh, but I hope it's, I mean, it's hopefully also see that it's an amazingly exciting topic.

and not only one inside how our human mind works and how we manage to be as intelligent but also as limited as we are in the end and um but meanwhile also i think it has certainly ai is um is amazingly developing over the last couple of years and um

Yes, and there, I guess, also things go so fast.

So it's important that we think about the dangers of AI.

And unfortunately, as we have seen with Cambridge Analytica, it's only an example of what is going on right now in terms of AI-driven targeted influence of our own selves and our way we think about stuff and our beliefs.

And hopefully we will manage also to foster this awareness of this and thus be more ready to counteract it and find back to our beautiful human social abilities.


SPEAKER_00:
Thank you for that excellent and uplifting note.

It was really an honor, Martin.

So thanks again for the presentation and the chat really appreciated as well.

You and any colleagues are always welcome to come on a live stream or to get involved in the Act Inflab in any way.

So this was just a great conversation and it was hopefully very informative for our audience as well.


SPEAKER_01:
I hope so too.

Thanks for inviting me here and thanks for running this lab.

It seems to be a really good group and I hope we can stay in contact over the next months and years.

Thank you.

We will.

Okay.

Take care.

See you.

Bye.

Bye.

Thank you.
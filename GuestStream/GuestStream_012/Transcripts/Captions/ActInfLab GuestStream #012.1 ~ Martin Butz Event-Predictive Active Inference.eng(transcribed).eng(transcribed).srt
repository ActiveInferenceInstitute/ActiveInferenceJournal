20
00:00:08,000 --> 00:00:13,000
hello and welcome to the active inference lab today it is october 25th

30
00:00:13,000 --> 00:00:21,000
2021 and we are here in active guest stream number 12.1 with martin boots so

40
00:00:21,000 --> 00:00:28,000
this is going to be an awesome presentation and discussion if you're watching live please feel free to add

50
00:00:28,000 --> 00:00:33,000
any questions into the live chat and we'll be compiling those and otherwise we're just gonna have a

60
00:00:33,000 --> 00:00:40,000
presentation and then discussion interval so martin thanks so much for joining our lab to give this

70
00:00:40,000 --> 00:00:47,000
presentation and we're really looking forward to what you have to share yeah a big pleasure to be here tonight

80
00:00:47,000 --> 00:00:52,000
or this morning i guess for you guys in california

90
00:00:52,000 --> 00:00:58,000
so let's see if i get the screen sharing here right again we

100
00:00:58,000 --> 00:01:04,000
practiced it before this should be this one here looks great

110
00:01:04,000 --> 00:01:10,000
and now i just need to have at least one video on the side here for me

120
00:01:10,000 --> 00:01:17,000
so yeah big pleasure um to present our labs work on which i call event predictive

130
00:01:17,000 --> 00:01:24,000
active inference and particularly also modeling the development of conceptual compositional cognition

140
00:01:24,000 --> 00:01:32,000
from sensory motor experiences you might have already um kind of noticed uh the

150
00:01:32,000 --> 00:01:37,000
picture here this artistic picture on the top right and i think it shows

160
00:01:37,000 --> 00:01:43,000
nicely and intuitively how our brain continuously seems to attempt

170
00:01:43,000 --> 00:01:50,000
to infer the hidden causes behind our sensory perceptions and thus

180
00:01:50,000 --> 00:01:55,000
this kind of terrace is kind of bent for us a little bit because the top part of

190
00:01:55,000 --> 00:02:00,000
course um just can be interpreted in two ways and is obviously

200
00:02:00,000 --> 00:02:06,000
not quite rightly positioned or or yeah it cannot be like

210
00:02:06,000 --> 00:02:11,000
this in reality we give you a couple of other examples what i mean by this

220
00:02:11,000 --> 00:02:18,000
active inference which is known to or explained often by carl frist and others to extend essentially

230
00:02:18,000 --> 00:02:24,000
develop latent hidden states to explain away the sensory

240
00:02:24,000 --> 00:02:31,000
perceptions by inferring the hidden causes behind them so here you see a color illustration

250
00:02:31,000 --> 00:02:36,000
which is called color illusion sometimes where you probably won't believe it i

260
00:02:36,000 --> 00:02:44,000
have to measure it each time again to make sure that i'm really right but essentially in this yellow tinted

270
00:02:44,000 --> 00:02:51,000
figure essentially these blue colored squares here they are essentially grayscale now they are if

280
00:02:51,000 --> 00:02:57,000
you test them by um in the graphics program you will see that this is a grayscale

290
00:02:57,000 --> 00:03:04,000
position so all blue points here in fact grayscale and here on this side with a blue tinted background you essentially

300
00:03:04,000 --> 00:03:10,000
have the same situation but it's the yellow pieces that are actually grayscale this is illustrated

310
00:03:10,000 --> 00:03:16,000
here on the right a little bit and this just shows essentially that our brain

320
00:03:16,000 --> 00:03:21,000
is interpreting its sensory perceptions in the light

330
00:03:21,000 --> 00:03:28,000
more and logically fitting here um of the circumstances yeah and in this

340
00:03:28,000 --> 00:03:36,000
case yellow light makes a true blue square appear gray when we look at it so

350
00:03:36,000 --> 00:03:41,000
we perceive it gray the sensory stimulation is gray but the true

360
00:03:41,000 --> 00:03:47,000
cause behind it must be a blue square so i our brain essentially correctly and

370
00:03:47,000 --> 00:03:54,000
actively infers that this square is most likely blue because under this yellow light conditions it appears gray and we're not

380
00:03:54,000 --> 00:04:03,000
even aware of that so this is a lovely an example of a very low level

390
00:04:03,000 --> 00:04:10,000
inference process in our brain that takes into account the situation and essentially puts the perceptual

400
00:04:10,000 --> 00:04:16,000
sensory information in this event this light event this lightning condition essentially

410
00:04:16,000 --> 00:04:23,000
another um nice example of this is these shadows that i found here online from

420
00:04:23,000 --> 00:04:29,000
from some artistic exhibition and you have here two shadows and as long as you look at the shadows

430
00:04:29,000 --> 00:04:35,000
and you perceive the shadows you kind of not only perceive the shadows but you perceive a person and the illustration

440
00:04:35,000 --> 00:04:42,000
of a person and to a certain extent you have the feeling that some person must be standing here somewhere such that i

450
00:04:42,000 --> 00:04:49,000
will perceive these uh these shadows but lo and behold um this situation is totally different

460
00:04:49,000 --> 00:04:56,000
these are actually cool artistic sculptures that produce the shadows and

470
00:04:56,000 --> 00:05:03,000
um again you see that yeah you have perceived a shadow and

480
00:05:03,000 --> 00:05:09,000
interpreted not only the shadow but created a whole scene a whole event like with a light source and the person in

490
00:05:09,000 --> 00:05:16,000
the middle and then the shadow that produces this in order to make sense of the shadow itself yeah so again you

500
00:05:16,000 --> 00:05:21,000
infer the hidden causes behind the shadow in this case not quite correctly

510
00:05:21,000 --> 00:05:28,000
because it's a cool illustrative cool artistic

520
00:05:28,000 --> 00:05:35,000
sculpture here that with a installation with a light source perfectly positioned such that

530
00:05:35,000 --> 00:05:42,000
the sculpture creates a particular human-like shadow figure so the first proposition that i uh

540
00:05:42,000 --> 00:05:47,000
take out of this essentially that our brain seems to be this generative predictive

550
00:05:47,000 --> 00:05:54,000
model and uses active inference essentially with this generative predictive model

560
00:05:54,000 --> 00:06:01,000
generating inferring internal activities that is internal active and codings that try to

570
00:06:01,000 --> 00:06:07,000
explain away the current sensory perceptions and thus make sense of it in the sense that we

580
00:06:07,000 --> 00:06:12,000
explain them away by ideally inferring the true hidden causes

590
00:06:12,000 --> 00:06:19,000
behind the perceptions that we actually perceive this generative predictive model as

600
00:06:19,000 --> 00:06:25,000
explains away perceptions it maintains distributed predictive activities

610
00:06:25,000 --> 00:06:31,000
um which form this kind of attractors which often then are formalized particularly

620
00:06:31,000 --> 00:06:39,000
by carl fristen and his followers so to say and in various ways um formalizing this and

630
00:06:39,000 --> 00:06:46,000
essentially suggesting that the brain develops this local free energy minima

640
00:06:46,000 --> 00:06:52,000
this local attractor like minima that is the consistent explanation of a

650
00:06:52,000 --> 00:06:58,000
particular perception that we are currently have under consideration so um this would be for example in the

660
00:06:58,000 --> 00:07:04,000
shadow situation right we perceive the shadow but we know uh intuitively that there's a light

670
00:07:04,000 --> 00:07:10,000
source and something in between that produces a shadow and thus in the in the shadow of a person we essentially

680
00:07:10,000 --> 00:07:19,000
immediately have the feeling that some person must be standing there the free energy formalism essentially

690
00:07:19,000 --> 00:07:24,000
pushes towards the generation of these generative predictive models

700
00:07:24,000 --> 00:07:30,000
and pushes towards the continuous active inference processes that are unfolding within these

710
00:07:30,000 --> 00:07:38,000
developing generative predictive models and one can nicely distinguish three kind of aspects of this general

720
00:07:38,000 --> 00:07:44,000
inference process formalism and that is on the one hand side on the first inside knowing what is going on so

730
00:07:44,000 --> 00:07:51,000
that's essentially the fast adaptation of internal model activities towards this local free energy minima

740
00:07:51,000 --> 00:07:58,000
to explain to ourselves what's going on so if we looked all together at the shadow and we perceived the shadow then

750
00:07:58,000 --> 00:08:03,000
um you perceived not only the shadow but you had the feeling that you inferred

760
00:08:03,000 --> 00:08:08,000
the causes that is that there is a person in the light source that creates the shadow yeah

770
00:08:08,000 --> 00:08:14,000
then the second part is learning more about the world we can revise our generative predictive models for example

780
00:08:14,000 --> 00:08:21,000
you have now revised your predictive models learning about that there are some cool people that make sculptures

790
00:08:21,000 --> 00:08:27,000
that produce human-like shadows and there's no human actually but just a sculpture and so you have learned

800
00:08:27,000 --> 00:08:33,000
something new about the world essentially and we do the first two aspects of inference aspects essentially

810
00:08:33,000 --> 00:08:40,000
of course only to really survive in our world to live to interact with our world in better manners so to really pursue

820
00:08:40,000 --> 00:08:46,000
our goals and our knowledge driven behavior our

830
00:08:46,000 --> 00:08:53,000
epistemic behavior with them by the means of these developing generative predictive models

840
00:08:53,000 --> 00:08:58,000
let me explain it a little bit further so so it's clear how i mean this

850
00:08:58,000 --> 00:09:03,000
so essentially also implied by the generative

860
00:09:03,000 --> 00:09:10,000
by the free energy formalism and the active inference formalism one can say that our brain is continuously

870
00:09:10,000 --> 00:09:16,000
on the one hand side in the present for sure right i mean we are ready while we for example write a

880
00:09:16,000 --> 00:09:24,000
letter here we are ready to produce the next word and write it out

890
00:09:24,000 --> 00:09:29,000
on paper for example we consider the recent past while doing this to be

900
00:09:29,000 --> 00:09:35,000
really in the present and fully embodied and grounded in the here and now

910
00:09:35,000 --> 00:09:40,000
the present is also continuously updated by our

920
00:09:40,000 --> 00:09:46,000
sensory feedback while we interact with environments for example we might update

930
00:09:46,000 --> 00:09:51,000
our belief how sturdy the pencil is or how hard it is and things like this

940
00:09:51,000 --> 00:09:56,000
and we learn um about our world while we interact with it such

950
00:09:56,000 --> 00:10:02,000
as improving our writing skills and so forth but most importantly we use this

960
00:10:02,000 --> 00:10:08,000
representation of the presence and the belief where we are in essentially to

970
00:10:08,000 --> 00:10:15,000
predict the future what is going to happen if we like his future considerations and then

980
00:10:15,000 --> 00:10:22,000
essentially to pursue particular future desired situations such as finalizing

990
00:10:22,000 --> 00:10:29,000
writing this letter writing this word producing a letter for another person

1000
00:10:29,000 --> 00:10:34,000
and having overall purposes also other purposes of course for the current day and so forth and these considerations

1010
00:10:34,000 --> 00:10:41,000
depending on our focus the difference between the desired future considerations and the current expected

1020
00:10:41,000 --> 00:10:46,000
future considerations make us improve our behavior and act in a core directed

1030
00:10:46,000 --> 00:10:52,000
manner with the environment one can illustrate this in a different way by

1040
00:10:52,000 --> 00:10:58,000
essentially saying um well the active inference pro framework essentially suggests that we

1050
00:10:58,000 --> 00:11:03,000
are continuously in a predictive state of mind ready to produce to process the

1060
00:11:03,000 --> 00:11:11,000
next sensory information doing information integration there producing local posteriors

1070
00:11:11,000 --> 00:11:17,000
integrating this local posteris perception with our overall beliefs in our model so perceiving the shadow

1080
00:11:17,000 --> 00:11:23,000
inferring that there's probably a person in light source that produces a shadow integrating an overall

1090
00:11:23,000 --> 00:11:29,000
consistent in attempting to create an overall consistent a posteriori predictive state of mind and we use this

1100
00:11:29,000 --> 00:11:36,000
state of mind essentially to roll out future considerations including

1110
00:11:36,000 --> 00:11:41,000
habitual behavior with our environment test if we like this if we want to go

1120
00:11:41,000 --> 00:11:47,000
there or if we want to pursue particular other um other other states in the future

1130
00:11:47,000 --> 00:11:54,000
choose the best one we can do execute it and then close the loop by using the reference copy to do the temporal

1140
00:11:54,000 --> 00:12:00,000
prediction in the here and now this

1150
00:12:00,000 --> 00:12:08,000
epistemic goal directed behavior can essentially be formalized nicely in this

1160
00:12:08,000 --> 00:12:15,000
reasonably short equations still to um to generate um goal directive

1170
00:12:15,000 --> 00:12:21,000
epistemic behavior this comes from fristen and pesolos and ielts work in

1180
00:12:21,000 --> 00:12:28,000
2015 um which essentially um i think it's a very nicely formalized

1190
00:12:28,000 --> 00:12:36,000
intuitive equation here actually so let's see we pursuing the minimization of free energy under a particular policy

1200
00:12:36,000 --> 00:12:43,000
pi at a certain state t and pursuing this free energy consists of two components the first component is

1210
00:12:43,000 --> 00:12:49,000
essentially the pursuer the goal directed component in this equation is essentially

1220
00:12:49,000 --> 00:12:56,000
trying to minimize the difference between the observations that i expect to perceive when i pursue a particular

1230
00:12:56,000 --> 00:13:03,000
policy pie also a particular behavior pi and compared to the observations that i

1240
00:13:03,000 --> 00:13:08,000
would like to perceive under my internal model so this is essentially the expected

1250
00:13:08,000 --> 00:13:14,000
divergence from desired future states and i pursu i try to then of course

1260
00:13:14,000 --> 00:13:21,000
infer a policy price or behavior pi that minimizes the divergence from the

1270
00:13:21,000 --> 00:13:28,000
expected future states to the desired future states the other component in this equation is

1280
00:13:28,000 --> 00:13:34,000
the epistemic part it's essentially the minimization the attempt to minimize expected future

1290
00:13:34,000 --> 00:13:42,000
uncertainty so it's the expectation of over my future horizon tau that's my

1300
00:13:42,000 --> 00:13:47,000
timeline into the future as you see here and it's essentially the ex the

1310
00:13:47,000 --> 00:13:54,000
expectation of future uncertainty which i try to minimize it as well so under my

1320
00:13:54,000 --> 00:14:01,000
uh consideration of my policy pi i expect future internal states m tao and this end house under the

1330
00:14:01,000 --> 00:14:07,000
policy uh lead to the expectation of future observations and the uncertainty of this observations or the observation

1340
00:14:07,000 --> 00:14:14,000
densities essentially um so the entropy over that the more uncertain that

1350
00:14:14,000 --> 00:14:20,000
the more diffuse my expectation the more uncertain my expectation the less i like

1360
00:14:20,000 --> 00:14:27,000
these future considerations and out of these two components in the equation one can essentially um derive agents that

1370
00:14:27,000 --> 00:14:34,000
act in goal-directed and epistemic manners we have done

1380
00:14:34,000 --> 00:14:40,000
implemented such agents mainly partially

1390
00:14:40,000 --> 00:14:46,000
as rather still rather crude approximations simply with recurrent neural network structures here for

1400
00:14:46,000 --> 00:14:52,000
example this rocket ball agent that was uh is controlled by a neural network

1410
00:14:52,000 --> 00:14:59,000
on the fly essentially by first training the network to learn the sensory motor

1420
00:14:59,000 --> 00:15:04,000
model of this agent so this agent undergoes gravity and has inertia in the

1430
00:15:04,000 --> 00:15:10,000
simulation in 2d and has these two thrusts that go diagonally

1440
00:15:10,000 --> 00:15:16,000
diagonally downwards so it can counteract gravity and steer as you see

1450
00:15:16,000 --> 00:15:22,000
essentially and the red line that you see here is essentially the imagination of the

1460
00:15:22,000 --> 00:15:30,000
neural network where it expects to fly to over the next couple of steps so essentially the system rolls out as i

1470
00:15:30,000 --> 00:15:37,000
illustrated before essentially into the future it's sensory motor dynamics expected sensory motor dynamics under

1480
00:15:37,000 --> 00:15:43,000
expected motor control commands that it imagines executing

1490
00:15:43,000 --> 00:15:49,000
and it then compares this uh the resulting state with the desired state can take the kel divergence or simply

1500
00:15:49,000 --> 00:15:55,000
the delta if it's deterministic and pro use that delta to project it

1510
00:15:55,000 --> 00:16:02,000
back onto the motor commands so that's uh the system essentially acts in its own best interest to uh

1520
00:16:02,000 --> 00:16:09,000
pursue these goals the goals are given from the outside in this case so this is still a rather simple system

1530
00:16:09,000 --> 00:16:16,000
in the sense that it's not probabilistic really or tries to minimize uncertainty here but just pursues goal-directed

1540
00:16:16,000 --> 00:16:22,000
behavior we have also done this with various other systems for example multi-joint

1550
00:16:22,000 --> 00:16:28,000
arm that additionally has the constraint to avoid collisions which are signaled by

1560
00:16:28,000 --> 00:16:36,000
this hair sensors as you see here and you can also get quite some cool behavior very similar principle and so

1570
00:16:36,000 --> 00:16:42,000
to conclude the first part right now i hope you have understood the active inference component

1580
00:16:42,000 --> 00:16:49,000
and inference is about essentially also suggesting implicitly that our mind

1590
00:16:49,000 --> 00:16:55,000
is not only about knowing what is going on it also continuously learns about the world retrospectively so it's partially

1600
00:16:55,000 --> 00:17:03,000
also in the past in some sense in its activity state and in the future considering um and inferring goal

1610
00:17:03,000 --> 00:17:09,000
epistemic behavior however this

1620
00:17:09,000 --> 00:17:15,000
is certainly good and we can generate close-loop control systems in this manner which are closely related

1630
00:17:15,000 --> 00:17:21,000
to model predictive control for that matter but in order to really

1640
00:17:21,000 --> 00:17:27,000
become event predictive or more abstract in our scene right right now essentially

1650
00:17:27,000 --> 00:17:35,000
you have seen a system that only is just writing something or is controlling a flying trajectory right it's not really

1660
00:17:35,000 --> 00:17:40,000
able to decide to write a whole letter right in the sense right abstractly speaking so

1670
00:17:40,000 --> 00:17:46,000
it's not able to abstract away from the actual current behavior

1680
00:17:46,000 --> 00:17:52,000
so the question is how where do these event predictive structures that i was talking about come

1690
00:17:52,000 --> 00:17:59,000
in and that leads me to my second proposition it's essentially that uh that it appears to be

1700
00:17:59,000 --> 00:18:06,000
that due to evolutionary shaped inductive processing biases our brain develops event predictive

1710
00:18:06,000 --> 00:18:13,000
compositional structures these structures tend to model the hidden causes behind sensory perceptions i have

1720
00:18:13,000 --> 00:18:22,000
already intuitively seen it i want to illustrate this further with an actual model behavior but before i do so let me

1730
00:18:22,000 --> 00:18:28,000
characterize where this event where this proposition essentially comes from right where where does this believe that event

1740
00:18:28,000 --> 00:18:35,000
predictive structures are the right way to go are the right way to abstract forward and it comes from from the

1750
00:18:35,000 --> 00:18:41,000
psychological literature mainly and i start with a quotation here from

1760
00:18:41,000 --> 00:18:49,000
jeff sex and barbara twesky from 2001 which essentially reads okay event

1770
00:18:49,000 --> 00:18:54,000
events have been characterized as a segment of time at a given location that is conceived by an observer to have a

1780
00:18:54,000 --> 00:19:00,000
beginning and an end and their behavioral psychological

1790
00:19:00,000 --> 00:19:08,000
work essentially suggested that when you have people segment movies or other little scenes or

1800
00:19:08,000 --> 00:19:14,000
cartoons even and things like this they do this very systematically so they have a really clear perception when an event

1810
00:19:14,000 --> 00:19:21,000
starts and when it ends and out of this it appears that we process

1820
00:19:21,000 --> 00:19:26,000
our environment in terms of we perceive our environment in terms of these events

1830
00:19:26,000 --> 00:19:31,000
but important is that the event is it's not really in the environment per se

1840
00:19:31,000 --> 00:19:38,000
right there's no label this is an event yeah but our brain constructs these events and that's what uh what their

1850
00:19:38,000 --> 00:19:44,000
boyfriend lovely put into a recent special issue contribution so she says

1860
00:19:44,000 --> 00:19:49,000
or baldwin and kosi should i say events the experiences we think we are having

1870
00:19:49,000 --> 00:19:55,000
and recall having had they are constructed they are not what actually occurs because what occurs in the end is

1880
00:19:55,000 --> 00:20:02,000
ongoing dynamic multination multi-dimensional sensory flow which is somewhat transformed via psychological

1890
00:20:02,000 --> 00:20:09,000
processes into structure describable memorable humidity of experience yeah so essentially right

1900
00:20:09,000 --> 00:20:17,000
there's no label of events but our brain creates this abstractions and this compact encodings

1910
00:20:17,000 --> 00:20:23,000
of the events that we perceive of that exist in our environment but

1920
00:20:23,000 --> 00:20:29,000
we construct them essentially they are not really existing on their own

1930
00:20:29,000 --> 00:20:35,000
and over last years a lot of research has focused on this event predictive

1940
00:20:35,000 --> 00:20:41,000
cognition and this research essentially investigates how events and conceptualizations thereof are learned

1950
00:20:41,000 --> 00:20:48,000
structured and processed dynamically and this research line essentially suggests that event predictive encodings and

1960
00:20:48,000 --> 00:20:54,000
processes optimally mediate between sensory motor processes and language uh maybe i should

1970
00:20:54,000 --> 00:21:01,000
have put inference processes here so this is a quote from our recent special issue so in summary this event

1980
00:21:01,000 --> 00:21:09,000
predictive cognition essentially comes from um psychological richer um trasky's work

1990
00:21:09,000 --> 00:21:16,000
with jeff sex and jeff sex continuation until today actually there's much more recent work from him about this it's

2000
00:21:16,000 --> 00:21:23,000
rooted partially even more deeply in behavioral psychology if you look at the common coding theory of uh within prince

2010
00:21:23,000 --> 00:21:28,000
and answered our behavior control theory of jachim hoffman and bernard thomas

2020
00:21:28,000 --> 00:21:33,000
work on the theory of event coding if you want to learn more about this i'll

2030
00:21:33,000 --> 00:21:40,000
allow me to point you to our recent special issue on this topic in the topics in cognitive science

2040
00:21:40,000 --> 00:21:46,000
journal with quite a variety of contributions spanning developmental neural

2050
00:21:46,000 --> 00:21:51,000
and behavioral psychology also linguistics

2060
00:21:51,000 --> 00:21:58,000
contributions and cognitive modeling and computational ai contributions

2070
00:21:58,000 --> 00:22:04,000
so what are these event structures now really about um it's essentially when we think about

2080
00:22:04,000 --> 00:22:10,000
particular event predictive structures right one can quickly get to the conclusion while

2090
00:22:10,000 --> 00:22:17,000
events must be somewhat disdistributed networks that essentially predict the types of entities that are

2100
00:22:17,000 --> 00:22:22,000
involved in event like a glass and a hand and an agent that

2110
00:22:22,000 --> 00:22:30,000
reaches for the glass to drink drink out of it the relative spatial relations between the hand and object for example

2120
00:22:30,000 --> 00:22:36,000
and the interaction dynamics such as the hand really moving to that object these are event structures

2130
00:22:36,000 --> 00:22:42,000
and then we have somehow this feeling of event boundaries and event transitions we know when a particular event can

2140
00:22:42,000 --> 00:22:48,000
commence and when it can end when it can begin and end and so for example when i grasp this cup

2150
00:22:48,000 --> 00:22:55,000
here i just can basically grasp it and once i have grasped it i can transport it and then i can put it

2160
00:22:55,000 --> 00:23:01,000
back down and i'm free again so to say combined with events and event

2170
00:23:01,000 --> 00:23:09,000
boundaries i create i can create mentally event schemas which have previously in prison also called scripts

2180
00:23:09,000 --> 00:23:15,000
and similar things and i think these scripts and develop naturally out of this event predictive coding principle

2190
00:23:15,000 --> 00:23:22,000
and with the event predictive coding principle we essentially have now a much more clear idea i think of what the

2200
00:23:22,000 --> 00:23:29,000
scripts and schema ties that have often been characterized before actually are and of course once we have schemata we

2210
00:23:29,000 --> 00:23:36,000
can embed them again in other events forming hierarchies out of that so to get back to this illustration of

2220
00:23:36,000 --> 00:23:42,000
writing a letter we can now say essentially well we our mind is not really

2230
00:23:42,000 --> 00:23:48,000
in a continuous past consideration but it con considers past events essentially

2240
00:23:48,000 --> 00:23:54,000
or the unfolding of past events and notice that this do not need to be partitioned as crisp as it's illustrated

2250
00:23:54,000 --> 00:24:01,000
here and we have certainly events unfolding in parallel for example if you think about the letter writing here

2260
00:24:01,000 --> 00:24:06,000
right you're writing for example the second word here say scholar or so but

2270
00:24:06,000 --> 00:24:12,000
you also have the full writing this letter event in your head this event of

2280
00:24:12,000 --> 00:24:18,000
having sit down on the table and being ready to write this letter so there are multiple event aspects of course that consider

2290
00:24:18,000 --> 00:24:24,000
particular sub-components of the actual true environment out there

2300
00:24:24,000 --> 00:24:31,000
and so again we know what's going on we can retrospectively improve our individual writing skills

2310
00:24:31,000 --> 00:24:37,000
for writing particular words writing in general but also our skills of writing a whole letter and improving this for

2320
00:24:37,000 --> 00:24:43,000
example and then we can again pursue the events of finalizing this letter posting

2330
00:24:43,000 --> 00:24:49,000
it later on and so forth so we have a full sequence in mind that actually

2340
00:24:49,000 --> 00:24:54,000
then includes the or concludes the full letter writing episode

2350
00:24:54,000 --> 00:24:59,000
that is not only doing the letter but also putting a stamp on the envelope putting the letter in the envelope

2360
00:24:59,000 --> 00:25:05,000
putting it to the post office and so forth so all this is included um

2370
00:25:05,000 --> 00:25:11,000
for the fun of it i included this lovely video project here um so here we see an

2380
00:25:11,000 --> 00:25:18,000
event of a of a billiard ball a pool table uh starting situation and somebody is

2390
00:25:18,000 --> 00:25:24,000
shooting the white boy and and so we have a nice hit and we look oh some balls hit the

2400
00:25:24,000 --> 00:25:29,000
hole no they don't actually lo and behold they come back together to the

2410
00:25:29,000 --> 00:25:35,000
starting position so i hope the video was a nice illustration of you

2420
00:25:35,000 --> 00:25:41,000
definitely having expected something totally different although not something very concrete but at least the final

2430
00:25:41,000 --> 00:25:47,000
state of the event that the balls will be distributed around the table in some form or the other certainly they will

2440
00:25:47,000 --> 00:25:53,000
not ever come back to the starting uh position like this and that's why hopefully some of you

2450
00:25:53,000 --> 00:25:59,000
were at least a little smiling to themselves yeah that it's a kind of a cool illustration here that my brain has

2460
00:25:59,000 --> 00:26:04,000
his events in mind and it essentially steps jumps ahead right when the

2470
00:26:04,000 --> 00:26:10,000
starting of an event unfolds it kind of knows what the final situation is most likely

2480
00:26:10,000 --> 00:26:16,000
going to be even if it's not fully concrete but it still has some in the ball situation the

2490
00:26:16,000 --> 00:26:21,000
pool ball situation a distributional um sense to it so it's also really

2500
00:26:21,000 --> 00:26:29,000
interesting and we have lo and behold actually formalize this event predictive active inference

2510
00:26:29,000 --> 00:26:34,000
now in an eventbritic effective inference model where the model

2520
00:26:34,000 --> 00:26:40,000
reaching behavior and visual eye fixation behavior in infants actually

2530
00:26:40,000 --> 00:26:47,000
together with colleagues from from potsam university from development development psychology side so now we

2540
00:26:47,000 --> 00:26:53,000
essentially have the same same equation again as before but it's

2550
00:26:53,000 --> 00:26:58,000
only condition now on events here as you see so the whole equation is

2560
00:26:58,000 --> 00:27:05,000
conditioned on in which events we believe to be currently in so the latent hidden state this model we had before is

2570
00:27:05,000 --> 00:27:12,000
now an event model and um and so and we we put the m basically as

2580
00:27:12,000 --> 00:27:20,000
the internal motivation so the desired observations given motivations are compared with the expected observations

2590
00:27:20,000 --> 00:27:26,000
pursuing a particular policy and under condition that we are in a particular event and uh

2600
00:27:26,000 --> 00:27:32,000
and in particular event series unfolds while interacting with environment to

2610
00:27:32,000 --> 00:27:37,000
illustrate this further um chris gomez actually here is uh has done this work

2620
00:27:37,000 --> 00:27:44,000
and also provided this following illustration here when we um have now as the behavioral policy here for example

2630
00:27:44,000 --> 00:27:50,000
the gaze position of say an infant yeah and so um we might

2640
00:27:50,000 --> 00:27:57,000
start you might explain this equation a little bit further by the situation that okay the infant say it

2650
00:27:57,000 --> 00:28:03,000
he or she really likes uh teddy bears so basically it's really it's internal motivation is to look at teddy bear

2660
00:28:03,000 --> 00:28:11,000
teddy bears and um and it really perceives a lot of pressure out of that so it really likes to look at petty teddy bears and so its

2670
00:28:11,000 --> 00:28:16,000
observation is the desire to see teddy bear like objects

2680
00:28:16,000 --> 00:28:21,000
and thus by pursuing a policy that minimizes the

2690
00:28:21,000 --> 00:28:27,000
divergence from seeing petty teddy bears so it essentially will fixate teddy

2700
00:28:27,000 --> 00:28:32,000
bears but now when we now see uh that suddenly this

2710
00:28:32,000 --> 00:28:39,000
ball here is moving yeah um there might be a surprise in the observation because uh the baby probably

2720
00:28:39,000 --> 00:28:45,000
has expected that everything else will stay rather stably so um what's happening is that there is

2730
00:28:45,000 --> 00:28:52,000
a large uncertainty what this ball object is suddenly doing and where it's rolling from and this makes um the baby

2740
00:28:52,000 --> 00:28:57,000
for example look at the ball now because it wants to know how its rolls and build

2750
00:28:57,000 --> 00:29:03,000
a good predictive model so it's not surprised of the ball given it caught its attention in the

2760
00:29:03,000 --> 00:29:10,000
first place of course and so forth seems to be somewhat relevant for itself and so it actually predicts the next

2770
00:29:10,000 --> 00:29:15,000
ball locations and it will also continue then to most likely at least depending

2780
00:29:15,000 --> 00:29:22,000
on the age of course a little later age probably then illustrated in this picture here um predict that this ball

2790
00:29:22,000 --> 00:29:29,000
will eventually fall to the ground and thus it actually will at some point anticipate not only

2800
00:29:29,000 --> 00:29:36,000
the next ball position but we'll look at the critical next ball position which would be when does this object then

2810
00:29:36,000 --> 00:29:43,000
suddenly fall to the ground yeah and so we have a kind of a two event prediction the immediate next

2820
00:29:43,000 --> 00:29:51,000
situation and the uh event boundary when the ball switches from rolling into falling and

2830
00:29:51,000 --> 00:29:57,000
then expecting that it will fall somewhere on the floor and hopefully not on a teddy bear

2840
00:29:57,000 --> 00:30:02,000
so to get this really important and lo and behold um

2850
00:30:02,000 --> 00:30:08,000
the studies from from adam maurice uh it's adam and beget etzner um

2860
00:30:08,000 --> 00:30:14,000
is showing that uh in over the first year of

2870
00:30:14,000 --> 00:30:20,000
our human's life usually we develop this anticipatory event predictive eye gaze

2880
00:30:20,000 --> 00:30:28,000
behavior now so what they did is essentially they showed um little kids um little videos of hands grasping teddy

2890
00:30:28,000 --> 00:30:35,000
bears or little claw like grippers grabbers grasping teddy bears

2900
00:30:35,000 --> 00:30:40,000
or other simple kind of objects and what you can show is that when you track the

2910
00:30:40,000 --> 00:30:47,000
this baby's eyes for six months of age they don't show any anticipatory gaze behavior so they they track the object

2920
00:30:47,000 --> 00:30:54,000
the hand that moves to the target object at best usually they even lag a little bit behind but lo and

2930
00:30:54,000 --> 00:31:01,000
behold um with seven and a half months of age about um the

2940
00:31:01,000 --> 00:31:06,000
the babies start to anticipate and they do so

2950
00:31:06,000 --> 00:31:12,000
particularly and only so when there is an effect when the hand not only reaches

2960
00:31:12,000 --> 00:31:18,000
for that entity but then also lifts it so of course after so 12 trials are done there in this

2970
00:31:18,000 --> 00:31:25,000
experiment so usually in after the first or second try uh and the hand really shakes the object a little bit then um

2980
00:31:25,000 --> 00:31:32,000
the babies start to anticipate and they only do so with about 11 months when there is no effect when the hand just

2990
00:31:32,000 --> 00:31:38,000
reaches for the object but doesn't do anything with it so then they stay on reactive or hand following behavior and

3000
00:31:38,000 --> 00:31:45,000
only um do this later on only anticipate in a later age visit about 11 months old and interestingly

3010
00:31:45,000 --> 00:31:51,000
with a claw they at seven and a half they don't anticipate at all so they don't see any

3020
00:31:51,000 --> 00:31:58,000
agentiveness or any anticipation that this claw will do something with the object but with 11 months of age they

3030
00:31:58,000 --> 00:32:05,000
essentially show the similar behavior then when they watch a hand with seven and a half months in the hand lifts object the claw lifts the object then

3040
00:32:05,000 --> 00:32:12,000
they also start to anticipate with 11 months but only with 18 months they also anticipate

3050
00:32:12,000 --> 00:32:19,000
when the claw is not lifting the object but just moving there and lo and behold we simulated this with

3060
00:32:19,000 --> 00:32:25,000
our event predictive active inference model essentially um assuming uh so to

3070
00:32:25,000 --> 00:32:31,000
say simulating that until about six months of age they just don't cannot interpret the event of an

3080
00:32:31,000 --> 00:32:37,000
end reaching for objects and so they just cannot make an event

3090
00:32:37,000 --> 00:32:44,000
case out of it and they they just process the unexpected information that is the hand starts moving and they track

3100
00:32:44,000 --> 00:32:50,000
the hand um with 12 months of age it's similar still when they see a claw

3110
00:32:50,000 --> 00:32:56,000
but with 12 months of age when they see a hand and an object they imagine ah probably this hand wants to reach for

3120
00:32:56,000 --> 00:33:01,000
the object and so once the hand starts moving they will and they will move

3130
00:33:01,000 --> 00:33:08,000
their eyes to the object to be ready to process what the hand is going to do with the object

3140
00:33:08,000 --> 00:33:13,000
um and in fact we modeled this now um with an event

3150
00:33:13,000 --> 00:33:18,000
predictive patient modeling approach essentially that essentially implements this active

3160
00:33:18,000 --> 00:33:25,000
inference equation on the event predictive level that i just showed um so

3170
00:33:25,000 --> 00:33:30,000
it's it's certainly essentially first trained to learn a probabilistic event predictive

3180
00:33:30,000 --> 00:33:35,000
schemata of the unfolding dynamics starting in ending conditions and then it

3190
00:33:35,000 --> 00:33:41,000
applies active inference of the current best event interpretation first so it first

3200
00:33:41,000 --> 00:33:48,000
needs to know which event so if it sees the situation it needs to infer ah a reaching event is most likely to unfold

3210
00:33:48,000 --> 00:33:55,000
and once it's certain that the reaching and for event unfolds and it knows reaching ends with the grasping and

3220
00:33:55,000 --> 00:34:04,000
something doing something with the object so i will look there to minimize my future expected uncertainty

3230
00:34:04,000 --> 00:34:11,000
minimizing this anticipated uncertainty of what the hand is going to do with the object

3240
00:34:11,000 --> 00:34:17,000
and maybe you can look at the results in detail in the paper

3250
00:34:17,000 --> 00:34:23,000
essentially what is happening is that the system learns to during training to infer the

3260
00:34:23,000 --> 00:34:30,000
correct events and then during testing after sufficient training phases rather quickly it starts

3270
00:34:30,000 --> 00:34:37,000
to anticipate um uh when the reach starts here over time

3280
00:34:37,000 --> 00:34:42,000
um it starts to anticipate and look at the goal object because it has encoded

3290
00:34:42,000 --> 00:34:49,000
this is a reaching event and i know how the reaching event unfolds and thus i look at the end of the reaching event

3300
00:34:49,000 --> 00:34:54,000
because i want to minimize the the free energy that i anticipate

3310
00:34:54,000 --> 00:35:00,000
unfolding in the near future while reaching and grasping an object

3320
00:35:00,000 --> 00:35:06,000
because this is the situation that i'm currently in so that's what the system emergently so to say it does that's why

3330
00:35:06,000 --> 00:35:12,000
emerging golden inspiratory gaze by this active inference or event predictive active inference

3340
00:35:12,000 --> 00:35:18,000
formalism so okay we have shown that there are statin

3350
00:35:18,000 --> 00:35:23,000
static latent encodings that can nicely

3360
00:35:23,000 --> 00:35:29,000
foster the emergence of this inspiratory behavior but can such or how can such

3370
00:35:29,000 --> 00:35:36,000
event predictive structures actually learned this is what i want to show you next but you could also have a short

3380
00:35:36,000 --> 00:35:41,000
break if you like daniel or what you say

3390
00:35:41,000 --> 00:35:49,000
maybe ask and just double down on a few of these cool points and ask a few questions and then jump into the second part of the presentation

3400
00:35:49,000 --> 00:35:54,000
yep please okay um so a few just kind of general

3410
00:35:54,000 --> 00:36:01,000
questions one was you brought in the neural network angle and so it was just a general question

3420
00:36:01,000 --> 00:36:06,000
how do the analytical single line equation formalisms

3430
00:36:06,000 --> 00:36:13,000
connect to different modern machine learning architectures and what does that have to do with the scaling or

3440
00:36:13,000 --> 00:36:18,000
where active inference could be applied i think i can answer this in the second

3450
00:36:18,000 --> 00:36:26,000
part of my talk perfect okay this is going to become much more on your networks just now and i will

3460
00:36:26,000 --> 00:36:32,000
conclude also with a general statement about this so maybe we will keep this for later essentially um

3470
00:36:32,000 --> 00:36:38,000
i mean this was an lcm network lstm networks are used for uh state-of-the-art machine learning so

3480
00:36:38,000 --> 00:36:44,000
it's not like something um that is like totally trivial or so great and

3490
00:36:44,000 --> 00:36:50,000
uh the event basis is something that i'm sure you'll return to but

3500
00:36:50,000 --> 00:36:57,000
you mentioned how events have boundaries and schema or schemata and hierarchies and

3510
00:36:57,000 --> 00:37:02,000
one thought was how are these uh boundaries schema and hierarchies

3520
00:37:02,000 --> 00:37:09,000
learned in humans and how does that inform our design of cognitive systems

3530
00:37:09,000 --> 00:37:15,000
yes also this is kind of what i continue with essentially this is the question here okay so how

3540
00:37:15,000 --> 00:37:22,000
are these event predictive structures learned not so much in humans i will not so much

3550
00:37:22,000 --> 00:37:27,000
focus on in the talk now but how can this be lost in artificial systems but

3560
00:37:27,000 --> 00:37:34,000
inspired of course by human learning well i'm maximally uncertain and curious

3570
00:37:34,000 --> 00:37:40,000
about those i guess implicitly expected and prefer that they be resolved

3580
00:37:40,000 --> 00:37:47,000
but i hope so it just reminded me of how they're encultured like at the end of a movement of a symphony if you clap you

3590
00:37:47,000 --> 00:37:53,000
didn't get it because that's not the encultured moment to clap there's a broader event that actually goes beyond

3600
00:37:53,000 --> 00:37:59,000
the sound and so it just made me think about how through learning

3610
00:37:59,000 --> 00:38:06,000
we re-conceptualize what events are and if events truly are one of the

3620
00:38:06,000 --> 00:38:13,000
uh kind of atomic units of cognition then that's incredibly powerful

3630
00:38:13,000 --> 00:38:20,000
i would think so yes was this a question or a comment that was just a comment on the eve importance of having event-based

3640
00:38:20,000 --> 00:38:26,000
cognitive processes and um how it reconsiders our own experience and suggests how to design

3650
00:38:26,000 --> 00:38:33,000
other architectures so maybe we can go to the second part of the presentation the fantastic part is really right that it goes hand

3660
00:38:33,000 --> 00:38:40,000
in hand right while these events develop we are we getting more and more ready to look and explore deeper event structures

3670
00:38:40,000 --> 00:38:48,000
right and more complex event structures and of course the real mystery is how these event structures

3680
00:38:48,000 --> 00:38:55,000
are learned and i want to show you a couple of ideas that we are pursuing in our mainly

3690
00:38:55,000 --> 00:39:00,000
machine learning research part and in the last part of the talk i will link

3700
00:39:00,000 --> 00:39:05,000
these event structures also to language structures to a certain extent

3710
00:39:05,000 --> 00:39:11,000
so um but allow me the interest of time to go a little bit fast over this machine

3720
00:39:11,000 --> 00:39:16,000
learning components there because to i to explain the machine learning

3730
00:39:16,000 --> 00:39:22,000
architectures in all that detail it would take talks on on its own but i just want to give you a little bit of an

3740
00:39:22,000 --> 00:39:29,000
idea where we are in this respect but the main question that was asked and that's also really a

3750
00:39:29,000 --> 00:39:34,000
really good question is essentially what are these

3760
00:39:34,000 --> 00:39:41,000
how can our brain learn these event structures and there must be essentially

3770
00:39:41,000 --> 00:39:47,000
in machine learning jargon these inductive learning biases yeah these inductive

3780
00:39:47,000 --> 00:39:52,000
tendencies to learn these event predictive structures

3790
00:39:52,000 --> 00:39:58,000
and ideally to learn them in a very compact very suitably compressed

3800
00:39:58,000 --> 00:40:03,000
form to identify the causality

3810
00:40:03,000 --> 00:40:11,000
that generate the sensory perceptions in the first place and allow them to

3820
00:40:11,000 --> 00:40:17,000
combine these event structures in a very compositional very flexible manner such

3830
00:40:17,000 --> 00:40:25,000
that we get ready to um to apply our knowledge also in other situations that are related to previous

3840
00:40:25,000 --> 00:40:30,000
experiences but of course always different and

3850
00:40:30,000 --> 00:40:36,000
i've argued before that essentially they must on the one hand side be event

3860
00:40:36,000 --> 00:40:42,000
oriented interpretation tendencies in general as we have already seen in the quote from their baldwin for example

3870
00:40:42,000 --> 00:40:47,000
also so there must be we must essentially foster the development of this event predictive

3880
00:40:47,000 --> 00:40:54,000
stable and compact latent states this what seems what our brain seems to do

3890
00:40:54,000 --> 00:41:00,000
and it seems to develop these event states in terms of attempting to characterize

3900
00:41:00,000 --> 00:41:06,000
starting conditions contextual conditions and ending conditions of events so contextual conditions means

3910
00:41:06,000 --> 00:41:13,000
like when can an event unfold and what is happening typically why an event unfolds the starting condition means

3920
00:41:13,000 --> 00:41:20,000
when can this an event start yeah so i can reach only for an object when it's in reach so for example yeah and the

3930
00:41:20,000 --> 00:41:26,000
ending condition is essentially when an event ends so when a reaching event ends is typically when i grasp an object for

3940
00:41:26,000 --> 00:41:33,000
example and it appears that there is event predictive biases is inductive biases to

3950
00:41:33,000 --> 00:41:39,000
segment our stream of information into these events and two very important ones are

3960
00:41:39,000 --> 00:41:45,000
probably signals of surprise because usually when i don't have a good event or when i don't know exactly how an

3970
00:41:45,000 --> 00:41:51,000
event unfolds i get particularly first better to for example extend my hands reach for things

3980
00:41:51,000 --> 00:41:56,000
and so forth but then when i touch them i fail to grasp them properly and they fall down and this is surprising and

3990
00:41:56,000 --> 00:42:02,000
then i get surprise signals which over time

4000
00:42:02,000 --> 00:42:08,000
become non-surprising anymore because i know that when i reach for the object then typically the grasp unfolds and i

4010
00:42:08,000 --> 00:42:14,000
prepare for the grasp and then i manage the whole sequence without the surprises

4020
00:42:14,000 --> 00:42:19,000
and the other one is this latent signals of stability because and this also

4030
00:42:19,000 --> 00:42:25,000
relates to causality in a sense uh because our i mean our environment our

4040
00:42:25,000 --> 00:42:32,000
world is a three-dimensional space-time continuum essentially and forces or

4050
00:42:32,000 --> 00:42:38,000
causal interactions can only unfold when two entities can exchange these

4060
00:42:38,000 --> 00:42:44,000
forces and this is typically in the physical world only possible when they come in contact to each other now we are

4070
00:42:44,000 --> 00:42:52,000
in zoom and we we can exchange information over this long range um long range means but also in the

4080
00:42:52,000 --> 00:42:59,000
sense we are in contact right now right um by this tool by this digital device essentially

4090
00:42:59,000 --> 00:43:05,000
in the internet that um enables us to now um establish virtual rooms that's why a

4100
00:43:05,000 --> 00:43:11,000
zoom room for example is also called a zoom room yeah because it's a virtual room where we are together not

4110
00:43:11,000 --> 00:43:17,000
physically but information wise we can exchange information and and thoughts and ideas and so forth and that's

4120
00:43:17,000 --> 00:43:25,000
essentially very similar to being in the real room where typically similar things are mainly unfolding

4130
00:43:25,000 --> 00:43:30,000
and so this is the event predictive biases that make us structure our

4140
00:43:30,000 --> 00:43:36,000
environment and our thoughts and our general predictive models in this event

4150
00:43:36,000 --> 00:43:44,000
compositional manner and the other one is of course the importance of curiosity in homeostasis to make sure that we on

4160
00:43:44,000 --> 00:43:49,000
the one hand side essentially i mean our brain has does not have the capacity our

4170
00:43:49,000 --> 00:43:54,000
brain has a phenomenal capacity for sure but certainly doesn't have the capacity to learn everything in all its detail

4180
00:43:54,000 --> 00:44:01,000
about the world yeah i mean it's totally impossible i mean otherwise you would need to learn down to quantum mechanics

4190
00:44:01,000 --> 00:44:07,000
how everything unfolds the whole time right so this is um this is absolutely impractical

4200
00:44:07,000 --> 00:44:14,000
so of course we need to develop models that we believe so to say are best

4210
00:44:14,000 --> 00:44:20,000
suited for our needs right so while we build our models and why we pursue

4220
00:44:20,000 --> 00:44:25,000
active inference as you already seen in the active inference equation right there's this important component that

4230
00:44:25,000 --> 00:44:30,000
essentially tries to minimize the divergence between desired

4240
00:44:30,000 --> 00:44:37,000
perceptions and actual perceptions right and this is essentially maintaining internal homeostasis how fristen would

4250
00:44:37,000 --> 00:44:43,000
argue also that is this perception right that's not it's not necessarily only about the outside environment but it's

4260
00:44:43,000 --> 00:44:48,000
also the perception of your own body right so if you desperately hungry

4270
00:44:48,000 --> 00:44:55,000
or starving or whatever right then of course you do everything to prevent this from happening um

4280
00:44:55,000 --> 00:45:00,000
hopefully uh yeah it doesn't get to this um and

4290
00:45:00,000 --> 00:45:06,000
curiosity uh meanwhile of course drives our curious minds our knowledge gain

4300
00:45:06,000 --> 00:45:12,000
experience but also this knowledge gain experience is partially embodied so also our sensory system kind of signals to us

4310
00:45:12,000 --> 00:45:20,000
what is probably interesting for us and it's going hand in hand with the homeostasis component we build models

4320
00:45:20,000 --> 00:45:26,000
mainly about the stuff that really interests us and that we believe is important to us in our rich cultural

4330
00:45:26,000 --> 00:45:31,000
social world of course this can be very awkward very cool artistic things

4340
00:45:31,000 --> 00:45:37,000
and so forth but nonetheless right it is still in the human realm important

4350
00:45:37,000 --> 00:45:43,000
but when we do when we try to implement this now in artificial neural network structures

4360
00:45:43,000 --> 00:45:50,000
i want to show you a couple of brief glimpses at least at what we have done

4370
00:45:50,000 --> 00:45:57,000
in my group over the last couple of years the first that i want to show you is a derivative of this rocket ball

4380
00:45:57,000 --> 00:46:03,000
system where we now have again similar recurrent neural network structure but we have but we enhance the system now

4390
00:46:03,000 --> 00:46:10,000
not only processing sensory and motor information predicting sensory consequences as in the rocket ball thing

4400
00:46:10,000 --> 00:46:18,000
in that multi-joint arm that you saw before but also we allow it to give a contextual input

4410
00:46:18,000 --> 00:46:24,000
state which is essentially like an event state that develops

4420
00:46:24,000 --> 00:46:31,000
the ability to distinguish between different vehicles that the system is currently controlling notice or recall

4430
00:46:31,000 --> 00:46:38,000
that the system as a sensory input get gets xy positions of the vehicle

4440
00:46:38,000 --> 00:46:44,000
and motor commands are this two or four thrust motors

4450
00:46:44,000 --> 00:46:50,000
so it doesn't see which vehicle it's currently controlling it only sees its position

4460
00:46:50,000 --> 00:46:57,000
and so in order to distinguish the different vehicles it either needs to enhance its latent state within the long

4470
00:46:57,000 --> 00:47:02,000
shorter memory structure the car neural network structure or it needs to

4480
00:47:02,000 --> 00:47:09,000
contextualize a latent latent inductive bias essentially here and by training this it

4490
00:47:09,000 --> 00:47:14,000
lo and behold can do this now this is the trained model and it can has learned

4500
00:47:14,000 --> 00:47:21,000
to control these four three vehicles um that have different inertial and gravity properties and

4510
00:47:21,000 --> 00:47:28,000
um and also um thrust different thrust motors and just so does this now essentially by

4520
00:47:28,000 --> 00:47:35,000
on the one hand side continuously doing this active inference with a line that you see again the thought uh projecting

4530
00:47:35,000 --> 00:47:41,000
where it will be in the near future but also it retrospectively continuously

4540
00:47:41,000 --> 00:47:46,000
adapts its internal event estimate of which vehicle it's currently controlling or not really of which vehicle is

4550
00:47:46,000 --> 00:47:53,000
currently controlling but rather what's the best contextual state that allows me to predict the current sensing

4560
00:47:53,000 --> 00:48:01,000
motor dynamics in the best way and it does this uh rather well um lo and behold if you plot the internal

4570
00:48:01,000 --> 00:48:06,000
state this contextual state that emerges that's not trained or in a sense like

4580
00:48:06,000 --> 00:48:13,000
super traced not trained in a supervised way but it's trained via an active inference process essentially

4590
00:48:13,000 --> 00:48:19,000
um so it emerges like a distinction between the three different vehicles by

4600
00:48:19,000 --> 00:48:25,000
just simply optimizing this forward model of the dynamics of the different dynamics of

4610
00:48:25,000 --> 00:48:32,000
the three vehicles and having the inductive buyers that's important here to to succeed essentially that

4620
00:48:32,000 --> 00:48:38,000
the vehicles do not switch all the time but they are stable for a while like about 50 steps or something and then

4630
00:48:38,000 --> 00:48:43,000
they randomly switch at a certain point in time and and the inductive bias is that this

4640
00:48:43,000 --> 00:48:49,000
contextual vector that's hidden here essentially is a stable vector that only adapts itself

4650
00:48:49,000 --> 00:48:54,000
much slower than the internal dynamics of the actual recurrent neural network

4660
00:48:54,000 --> 00:49:01,000
and then during training such these structures typically emerge distinguishing the three

4670
00:49:01,000 --> 00:49:08,000
vehicles you can even train this to transport objects then there's some modularizations are possible unnecessary

4680
00:49:08,000 --> 00:49:14,000
to do so but admittedly these these structures do emerge but not

4690
00:49:14,000 --> 00:49:23,000
very robustly and the the structure of this latent state is still typically rather fuzzy so over the last two years

4700
00:49:23,000 --> 00:49:28,000
we essentially have produced a couple of other neural networks that really try to

4710
00:49:28,000 --> 00:49:35,000
work on this compression and the suitable compression of these latent states further

4720
00:49:35,000 --> 00:49:41,000
we have done this with various gating networks originally first still providing

4730
00:49:41,000 --> 00:49:46,000
surprise signals so essentially surprised when the switch occurs and we just give it a switch

4740
00:49:46,000 --> 00:49:53,000
a current signal that was last year essentially where we developed nice latent codes for

4750
00:49:53,000 --> 00:49:59,000
predicting some abstract functions for example and distinguishing between them even seeing some similarities between

4760
00:49:59,000 --> 00:50:05,000
them but then this year at

4770
00:50:05,000 --> 00:50:10,000
the coxah meeting we have done this again so we see we have this

4780
00:50:10,000 --> 00:50:16,000
event anticipation module this is a com still rather simple neural network then

4790
00:50:16,000 --> 00:50:22,000
we have event switching module that's a gated recurrent unit network here that

4800
00:50:22,000 --> 00:50:29,000
allows the next anticipation of the next event code or the passing of the next event code into the lower level event

4810
00:50:29,000 --> 00:50:36,000
processing structure that is actually processing the sensory information only at certain points in time when this

4820
00:50:36,000 --> 00:50:41,000
event boundary anticipation network actually activates the switch and this event boundary installation network

4830
00:50:41,000 --> 00:50:48,000
learns to activate the switch by a particular suitably designed inductive learning

4840
00:50:48,000 --> 00:50:54,000
biases so the first inductive part is essentially design of the model that you develop essentially an event processing

4850
00:50:54,000 --> 00:51:00,000
module that is contextualized by the belief of which what's the current most suitable event

4860
00:51:00,000 --> 00:51:07,000
code so what's the event that's unfolding and an event boundary anticipation module that essentially switches between events

4870
00:51:07,000 --> 00:51:13,000
just in time and lo and behold this system actually really does learn to switch when the information is there

4880
00:51:13,000 --> 00:51:20,000
that it can know when the event switches that it learns an optimal model about

4890
00:51:20,000 --> 00:51:27,000
the switches and if you have for example in this case we use like one hot encoded symbolic sequences and different

4900
00:51:27,000 --> 00:51:35,000
sequence processing automata basically then the system developed distinct event

4910
00:51:35,000 --> 00:51:42,000
codes for the three types of um dynamics for example and interestingly because you see uh

4920
00:51:42,000 --> 00:51:49,000
p1 the the event one or program one essentially um just switches between a and b p2

4930
00:51:49,000 --> 00:51:54,000
switches between b and c and three the problem three switches between a b c

4940
00:51:54,000 --> 00:52:00,000
b a b c b so it's a kind of a combination of problem one and problem two and so lo

4950
00:52:00,000 --> 00:52:07,000
and behold the p3 code essentially is between the p1 code and the p2 code in

4960
00:52:07,000 --> 00:52:13,000
all the cases although of course the different initializations there are three different networks they

4970
00:52:13,000 --> 00:52:21,000
of course develop different latent codes but they still um kind of uh and

4980
00:52:21,000 --> 00:52:28,000
imply the underlying structure of the events and the similarity between events

4990
00:52:28,000 --> 00:52:35,000
but uh maybe the most advanced network is the one here again from chris bergham

5000
00:52:35,000 --> 00:52:40,000
together joint work with martials here um this paper now that essentially also

5010
00:52:40,000 --> 00:52:45,000
has a very similar structure you have a hidden latent state as in the previous

5020
00:52:45,000 --> 00:52:52,000
previous code that's essentially in in this network it's it's this code that is passed down

5030
00:52:52,000 --> 00:52:58,000
through here and this hidden latent code is maintained over time and it's just

5040
00:52:58,000 --> 00:53:05,000
controlled or can be adjusted by a gate that's a multiplicative gate that opens only very selectively and this gate

5050
00:53:05,000 --> 00:53:10,000
is uh is designed such that there is a loss function

5060
00:53:10,000 --> 00:53:16,000
that punishes gate openings so the system really doesn't really want to open its

5070
00:53:16,000 --> 00:53:22,000
gate but if it's really helpful to lower the prediction error on the side

5080
00:53:22,000 --> 00:53:28,000
it does so it's also open the gate and thus changes context with this

5090
00:53:28,000 --> 00:53:36,000
this recommendation system or this event coding system that has the next event code

5100
00:53:36,000 --> 00:53:43,000
ready to switch to it just in time and then combines this to predict the next uh

5110
00:53:43,000 --> 00:53:52,000
consequences so again a sensory mode event predictive model essentially and um this system is now um quite ready

5120
00:53:52,000 --> 00:53:57,000
to um to process state-of-the-art challenges here

5130
00:53:57,000 --> 00:54:03,000
not only toy problems as in the admittedly in the last in two other systems so for example there's a

5140
00:54:03,000 --> 00:54:11,000
billiard ball scenario which is a kind of a benchmark the machine learning community where um our system

5150
00:54:11,000 --> 00:54:17,000
with suitable parametrization um decreases the mean squared error much

5160
00:54:17,000 --> 00:54:24,000
more than standard lstm or crew or standard recurrent neural networks and particularly when you have a testing

5170
00:54:24,000 --> 00:54:30,000
scenario that differs a little bit from the training scene and what's even more

5180
00:54:30,000 --> 00:54:36,000
important maybe or more interesting in this scenario is that matching to this illustration here you

5190
00:54:36,000 --> 00:54:44,000
see essentially that um the latent state of the our system called gate lord um

5200
00:54:44,000 --> 00:54:51,000
essentially you can really see that there is a particular dynamic unfolding right now

5210
00:54:51,000 --> 00:54:57,000
so first the ball rolls in a certain direction after the first interaction with the boundary

5220
00:54:57,000 --> 00:55:03,000
um the latent state perfectly switches to one stable new state and then you see

5230
00:55:03,000 --> 00:55:09,000
the second bounce and again it switches to a new state so um that's a very powerful a little

5240
00:55:09,000 --> 00:55:16,000
bit hand waving due to time cannot explain uh all of the all of the components here

5250
00:55:16,000 --> 00:55:21,000
but you see a really nice generalization behavior for example when you train only

5260
00:55:21,000 --> 00:55:29,000
this fetch and pick in place task in in situations where the gripper object contact always

5270
00:55:29,000 --> 00:55:36,000
occurs at time point five and in generalization the contact also

5280
00:55:36,000 --> 00:55:41,000
occurs at other time points um immediately the lstm and crew networks um

5290
00:55:41,000 --> 00:55:46,000
yeah get much worse in their predictive accuracy and gate lord still generalizes

5300
00:55:46,000 --> 00:55:54,000
also to these other scenarios and there are a couple of other um also combinations with reinforcement learning

5310
00:55:54,000 --> 00:55:59,000
systems so actually we took our the christian took that gate lord

5320
00:55:59,000 --> 00:56:05,000
module essentially this model learning module and combined it with state-of-the-art reinforcement learning

5330
00:56:05,000 --> 00:56:10,000
systems and as a result we could beat learn faster be more

5340
00:56:10,000 --> 00:56:17,000
sample efficient essentially and be it partially more accurate than the state-of-the-art reinforcement learners

5350
00:56:17,000 --> 00:56:24,000
in a couple of this mini grid world tasks for example so really important to

5360
00:56:24,000 --> 00:56:33,000
induce the right inductive learning biases these essentially improve um the latent state codes that

5370
00:56:33,000 --> 00:56:39,000
much more um systematically seem to develop um really ex kind of an

5380
00:56:39,000 --> 00:56:46,000
explaining called uh like explaining the causality in within with the latent code about

5390
00:56:46,000 --> 00:56:51,000
what's really going on in the environment such as where the sheep for example is

5400
00:56:51,000 --> 00:56:56,000
or where the goal position will be if if the system

5410
00:56:56,000 --> 00:57:03,000
remembers where the key is on one side and so forth so particularly very suited also for partially observable markov

5420
00:57:03,000 --> 00:57:11,000
decision process problems um so where you need to maintain longer term memory of particular events that

5430
00:57:11,000 --> 00:57:17,000
happened in particular environments okay

5440
00:57:17,000 --> 00:57:22,000
so that's the second part of the neural networks

5450
00:57:22,000 --> 00:57:28,000
and if you still have a couple of minutes i um try to

5460
00:57:28,000 --> 00:57:33,000
put you through the last component here that i wanted to show you that

5470
00:57:33,000 --> 00:57:41,000
event these event predictive structures can be very closely related to language

5480
00:57:41,000 --> 00:57:47,000
and that i find most exciting because this might in fact really close the

5490
00:57:47,000 --> 00:57:53,000
language gap and really around language in our sensory motor experiences by this

5500
00:57:53,000 --> 00:57:59,000
tendency to develop event predictive structures that are then very sim very easily

5510
00:57:59,000 --> 00:58:06,000
linked to the language that each of us experiences while we grow up let me show you what i

5520
00:58:06,000 --> 00:58:11,000
mean by this event predictive structures in terms of a language domain let's take this

5530
00:58:11,000 --> 00:58:18,000
this example of of a ball that rolled down the table because it was crooked for example yeah

5540
00:58:18,000 --> 00:58:24,000
and so if you read the sentence actually it was crooked yeah you you might think well the ball was crooked that's why i

5550
00:58:24,000 --> 00:58:30,000
drove down now it's probably was the table um so so how come we we are able

5560
00:58:30,000 --> 00:58:37,000
to make sense of this sentence with uh it particularly being ambiguous ambiguous being

5570
00:58:37,000 --> 00:58:44,000
referential ambiguity here being able to uh be bound to the subject or the object

5580
00:58:44,000 --> 00:58:50,000
essentially and so what i mean what happens when we read such a sentence is essentially that while we process

5590
00:58:50,000 --> 00:58:56,000
sending so we read the ball and we create probably a kind of a predictive encoding structure of the ball so the

5600
00:58:56,000 --> 00:59:02,000
ball can roll and bounce and it uh it gets repelled when it hits

5610
00:59:02,000 --> 00:59:08,000
something and it has some particular size and possibly we also imagine some kind of particular ball with

5620
00:59:08,000 --> 00:59:14,000
like a soccer ball or something depending on what i usually not very visual person so i don't

5630
00:59:14,000 --> 00:59:21,000
usually um imagine that an actual ball and then i then read rolled down so

5640
00:59:21,000 --> 00:59:28,000
the rolling is much more active than the falling at this point and i imagine the ball essentially rolling somewhere

5650
00:59:28,000 --> 00:59:34,000
somehow and then this somewhere somehow is specified it's the table so it rolls

5660
00:59:34,000 --> 00:59:40,000
uh down the table so apparently it's there's a table and there's a table top so it's probably on the surface of the

5670
00:59:40,000 --> 00:59:46,000
table that the rolling event unfolds and so i put this together and i have this rolling down as we've seen before the

5680
00:59:46,000 --> 00:59:52,000
baby expecting the rolling and the falling with now this event structure right of a ball rolling down meaning

5690
00:59:52,000 --> 01:00:01,000
like it rolled first on the surface and then it probably dropped down somewhere thus rolling down the table

5700
01:00:01,000 --> 01:00:06,000
and now uh we read because it was crooked essentially and this essentially

5710
01:00:06,000 --> 01:00:13,000
implies that because due to the because situation right um that essentially was the second part of the sentence was not

5720
01:00:13,000 --> 01:00:19,000
true like this is explaining part of the sentence it because part right so was it not crooked then the rolling event would

5730
01:00:19,000 --> 01:00:24,000
knock not take place and this essentially counter factual reasoning

5740
01:00:24,000 --> 01:00:31,000
right so um so can it be that the ball is crooked and thus it rolled down the table

5750
01:00:31,000 --> 01:00:37,000
and that's uh well if the ball would have some sort of crooked dent or whatever it's most likely has not rolled

5760
01:00:37,000 --> 01:00:43,000
down the table so that seems to be not so plausible so but a kind of a

5770
01:00:43,000 --> 01:00:49,000
tilted crooked table seems to be much more plausible so

5780
01:00:49,000 --> 01:00:54,000
most likely it was the table that was assigned with a pronoun right so we can

5790
01:00:54,000 --> 01:00:59,000
we have essentially analyzed the whole event with its causality in it by

5800
01:00:59,000 --> 01:01:06,000
imagining the words that we read and the sentence structure that we perceive as grammar and

5810
01:01:06,000 --> 01:01:12,000
generating an imagination of what we perceive and then actually we have um

5820
01:01:12,000 --> 01:01:19,000
this year with together with christian stigma and phillips we have published a paper on the so-called learner

5830
01:01:19,000 --> 01:01:24,000
architecture which essentially learns about an environment an event

5840
01:01:24,000 --> 01:01:30,000
predictive structure it is trained uh or informed how to um

5850
01:01:30,000 --> 01:01:36,000
link the individual event components that it has learned out

5860
01:01:36,000 --> 01:01:41,000
of its observing an environment with a language processes the system

5870
01:01:41,000 --> 01:01:48,000
and then it can in fact do just what i have illustrated it can disambiguate ambiguous sentences by

5880
01:01:48,000 --> 01:01:55,000
creating concrete imaginations of in a described state of affair so for example we have

5890
01:01:55,000 --> 01:02:00,000
here this this scenario yeah unfortunately it looks like little

5900
01:02:00,000 --> 01:02:07,000
viruses here this has nothing to do with corona or so sorry about it it was created before

5910
01:02:07,000 --> 01:02:13,000
the whole damn pandemic but nonetheless uh what's important is that these creatures here push down

5920
01:02:13,000 --> 01:02:19,000
stuff from this platforms and our learner system analyzes these things

5930
01:02:19,000 --> 01:02:25,000
and essentially creates even predictive structures out of it and so then after

5940
01:02:25,000 --> 01:02:31,000
it has learned this it can essentially generate sentence interpretations so after training essentially you can

5950
01:02:31,000 --> 01:02:38,000
create give it a sentence such as the green virus rests on the platform and it moves to the box after it faults so

5960
01:02:38,000 --> 01:02:46,000
rather complex sentence and what the system then is doing it can create itself out of the entities that are

5970
01:02:46,000 --> 01:02:52,000
uttered like a green virus a platform and a box which has been decided to be green here

5980
01:02:52,000 --> 01:03:00,000
as well that's randomly chosen then whether it's not specified it arranges these three objects in such

5990
01:03:00,000 --> 01:03:06,000
a way such that it it can imagine an event sequence unfolding in this constellation

6000
01:03:06,000 --> 01:03:12,000
to make this sentence true so you can see this in this video here so essentially the system

6010
01:03:12,000 --> 01:03:17,000
learned to generate this constellation that you see and the virus lo and behold

6020
01:03:17,000 --> 01:03:23,000
falls on the platform actually and then it rests on the platform for a little while

6030
01:03:23,000 --> 01:03:28,000
and then it moves to the box after it has fallen on the platform so this is

6040
01:03:28,000 --> 01:03:34,000
the interpretation of the sentence that the system has created and i think it's a nice illustration hopefully for you

6050
01:03:34,000 --> 01:03:40,000
also convincing illustration that this event structures or sentences uttering

6060
01:03:40,000 --> 01:03:46,000
event structures um can be the what we make of it what meaning making sense of

6070
01:03:46,000 --> 01:03:53,000
a sentence is essentially something like this is the the internal creation of a consistent event or event progression

6080
01:03:53,000 --> 01:03:58,000
that um fits to the described sentence structure disambiguating of course

6090
01:03:58,000 --> 01:04:04,000
certain temporal and uh and referential and so forth ambiguities that might be

6100
01:04:04,000 --> 01:04:10,000
inherent in the sentence and grammatical structure that is provided

6110
01:04:10,000 --> 01:04:17,000
we have also just recently shown that this event predictive inference structure also nicely

6120
01:04:17,000 --> 01:04:24,000
fits to the rational speech act model actually for that matter and one can nicely model

6130
01:04:24,000 --> 01:04:30,000
learning about the preferences of others

6140
01:04:30,000 --> 01:04:36,000
this essentially this example in this study goes with actual behavioral

6150
01:04:36,000 --> 01:04:44,000
experiments done on mturk here so for example you can imagine when you have a scene like this and this

6160
01:04:44,000 --> 01:04:50,000
is admittedly very abstract entities here and so for example you have the scenario

6170
01:04:50,000 --> 01:04:57,000
and maria wants to signal an object to the following scene in the following scene to samantha samaria maria says red

6180
01:04:57,000 --> 01:05:03,000
please take a red one essentially and samantha chooses the on light object like such as she takes the red cloud

6190
01:05:03,000 --> 01:05:11,000
here red stripe cloud then you can possibly infer something about samantha's preferences lo and behold she

6200
01:05:11,000 --> 01:05:16,000
might like for example clouds more than circles

6210
01:05:16,000 --> 01:05:23,000
but you don't know anything about squares because there was no option to choose from squares and

6220
01:05:23,000 --> 01:05:29,000
you can also by the active inference formalism within this formalism actually you can then

6230
01:05:29,000 --> 01:05:35,000
also do it the other way around and you want to for example possibly learn about

6240
01:05:35,000 --> 01:05:42,000
elizabeth's preferences and you have only the scenario can give options to choose amongst objects so for example if

6250
01:05:42,000 --> 01:05:50,000
you want to learn um about if the say if the person prefers

6260
01:05:50,000 --> 01:05:56,000
clouds or circles you could for example say pick one of the green ones and then

6270
01:05:56,000 --> 01:06:02,000
you would see if the person takes a cloud or circle and then you have some hint that

6280
01:06:02,000 --> 01:06:09,000
the person might um prefer clouds or circles for example and we have modeled this in this uh

6290
01:06:09,000 --> 01:06:17,000
recursive inference process very closely related to the active inference formalism usa typical kl divergence here

6300
01:06:17,000 --> 01:06:22,000
two kl kl divergence again where we um essentially compare

6310
01:06:22,000 --> 01:06:28,000
our prior knowledge of feature preferences over our expected

6320
01:06:28,000 --> 01:06:33,000
posterior knowledge of feature preferences while pursuing a particular action a particular utterance

6330
01:06:33,000 --> 01:06:41,000
and when we compare this to the actual behavior of the human participants we get really good fits and in fact could

6340
01:06:41,000 --> 01:06:47,000
show also from a from from information criterium um

6350
01:06:47,000 --> 01:06:55,000
manner that our cognitive model fits better than uh than other competitive models and this leads me to the end of

6360
01:06:55,000 --> 01:07:00,000
my presentation yeah so uh just the language really briefly in the end

6370
01:07:00,000 --> 01:07:06,000
but i hope you see what i mean by that this event predictive cognition essentially and the active inference

6380
01:07:06,000 --> 01:07:13,000
processes that unfold which is within these event predictive cognitive systems and our brains for that matter most

6390
01:07:13,000 --> 01:07:19,000
likely are very compatible to linguistic structures so what i have shown you today and i

6400
01:07:19,000 --> 01:07:24,000
hope you uh could follow me so far it wasn't too much um that essentially our

6410
01:07:24,000 --> 01:07:30,000
generative mind yeah may health may have the self-motivated objective to act

6420
01:07:30,000 --> 01:07:36,000
highly flexibly and goal-directed and pursue epistemic self-motivated actions in general this is quite clear to

6430
01:07:36,000 --> 01:07:43,000
survive right to interact with our complex worlds to seeing that we have become this crazy human beings with all

6440
01:07:43,000 --> 01:07:48,000
our social likes and hates and capabilities and intelligence and so forth

6450
01:07:48,000 --> 01:07:53,000
on this human cognitive level event predictive conceptualizations seem to be

6460
01:07:53,000 --> 01:08:00,000
really important to enable us to act in a deeper goal-directed self-motivated

6470
01:08:00,000 --> 01:08:05,000
manner and to conceptualize our environment as to be able to communicate

6480
01:08:05,000 --> 01:08:11,000
and interact and cooperate with others and compete also with each other for that matter in a highly more

6490
01:08:11,000 --> 01:08:17,000
sophisticated manner than any other animals can to learn such conceptualizations we

6500
01:08:17,000 --> 01:08:24,000
really need this inductive learning biases and certainly not fully figured out yet how this learning and processing

6510
01:08:24,000 --> 01:08:32,000
biases are working working or functioning but we know one can show clearly that

6520
01:08:32,000 --> 01:08:38,000
these contributions are really good to enable deeper goal-directed self-motivated planning

6530
01:08:38,000 --> 01:08:43,000
reasoning counterfactual reasoning filling in gaps finishing unknowns

6540
01:08:43,000 --> 01:08:48,000
disambiguating ambiguous situations and pursuing

6550
01:08:48,000 --> 01:08:55,000
abstract and concrete behaviors on multiple levels and last point was that language seemed to

6560
01:08:55,000 --> 01:09:02,000
be mapped really rather simplistic rather easily on this uh which essentially possibly

6570
01:09:02,000 --> 01:09:08,000
might explain why we as babies naturally learn our

6580
01:09:08,000 --> 01:09:14,000
mother tongue without much overly much effort and particularly the

6590
01:09:14,000 --> 01:09:21,000
complexity of the grammar behind it also now on the artificial intelligence side i would like to conclude that

6600
01:09:21,000 --> 01:09:28,000
that the current deep learning systems mostly because they don't foster this generativity and this conceptualization

6610
01:09:28,000 --> 01:09:34,000
of structures um they do not really foster event predictive genitive models and as others

6620
01:09:34,000 --> 01:09:40,000
have said thus men may call on the more simpler architectures that are

6630
01:09:40,000 --> 01:09:45,000
nonetheless including systems like transformers and so forth really still

6640
01:09:45,000 --> 01:09:51,000
rather highly computationally mining stochastic parrots

6650
01:09:51,000 --> 01:09:58,000
arguably and interesting to discuss this of course i believe that if we develop more event

6660
01:09:58,000 --> 01:10:03,000
predictive generative systems then we actually foster the development of

6670
01:10:03,000 --> 01:10:09,000
the learning of generative models that is and ideally causal models of the actual

6680
01:10:09,000 --> 01:10:15,000
true causality that we encounter via our sensory motor experiences enabling us to

6690
01:10:15,000 --> 01:10:22,000
generate much more robust for cars possible action recommendations and explanations also why we would recommend

6700
01:10:22,000 --> 01:10:30,000
particular things of forecast particular things because we essentially are designed to learn about the causes

6710
01:10:30,000 --> 01:10:35,000
that lead us to predict certain things so we can also of course talk about the causes

6720
01:10:35,000 --> 01:10:45,000
and in my opinion these systems may then indeed yield strong ai or artificial general intelligence um

6730
01:10:45,000 --> 01:10:50,000
where self-motivated learning will be inevitable part of such systems

6740
01:10:50,000 --> 01:10:55,000
and self-motivated learning and self-modded behavior also thus will be part of the systems

6750
01:10:55,000 --> 01:11:01,000
and so i think it's we should be aware that there is no reason

6760
01:11:01,000 --> 01:11:08,000
i see no recent research on human cognition and its functionality is no reason why

6770
01:11:08,000 --> 01:11:14,000
an artificial system should not reach such uh intelligence level

6780
01:11:14,000 --> 01:11:19,000
and so as others also have pointed out i want to conclude with a

6790
01:11:19,000 --> 01:11:24,000
with a word of caution essentially and a word of awareness that

6800
01:11:24,000 --> 01:11:31,000
if these systems come into being come into existence yes we need to make very much sure that

6810
01:11:31,000 --> 01:11:37,000
we don't design them just for profit or for some

6820
01:11:37,000 --> 01:11:43,000
individual personal interest but we better put good purpose

6830
01:11:43,000 --> 01:11:49,000
into these systems and it's a good idea to think about this now also

6840
01:11:49,000 --> 01:11:56,000
this concludes my talk thank you so much for attention parts of this general motivation background you can find in my

6850
01:11:56,000 --> 01:12:02,000
book from 2016-17 how the mind comes into being and i

6860
01:12:02,000 --> 01:12:08,000
acknowledge some funding from humboldt foundation and dfg mainly and thank you for my team

6870
01:12:08,000 --> 01:12:16,000
actually to produce much of this work i don't have a team slide here and but uh thank you for your attention as well

6880
01:12:16,000 --> 01:12:24,000
awesome presentation thank you very much thank you maybe you can unshare and we can

6890
01:12:24,000 --> 01:12:29,000
have a little discussion

6900
01:12:29,000 --> 01:12:35,000
well tons of very interesting ideas anyone can write a question in the chat but let

6910
01:12:35,000 --> 01:12:41,000
me just start with a introductory question how did you come to be working with

6920
01:12:41,000 --> 01:12:46,000
active inference models what were you working on before was it a system

6930
01:12:46,000 --> 01:12:51,000
specific or question specific path that led you to integrate these novel

6940
01:12:51,000 --> 01:12:57,000
components into active inference

6950
01:12:57,000 --> 01:13:02,000
yes thanks i just little bit confused because it's this camera now yeah okay

6960
01:13:02,000 --> 01:13:08,000
um well let me let me actually turn to this screen then

6970
01:13:08,000 --> 01:13:15,000
yes well i have studied all my research career

6980
01:13:15,000 --> 01:13:22,000
essentially started to study anticipatory behavior from the very early on during my

6990
01:13:22,000 --> 01:13:28,000
diploma bachelor's degree studies essentially coming into contact with a psychologist in whittsburg jorge

7000
01:13:28,000 --> 01:13:33,000
hofmann's group and inspiratory behavior control and i think in his work he already

7010
01:13:33,000 --> 01:13:40,000
essentially from the psychological perspective of formalizes

7020
01:13:40,000 --> 01:13:46,000
this principle of active inference on a very crude more inwards specified

7030
01:13:46,000 --> 01:13:53,000
psychological level where and i started very early then to also realize that

7040
01:13:53,000 --> 01:13:59,000
that well the devil is in the details

7050
01:13:59,000 --> 01:14:05,000
um the the big question is really what are representations like yeah what what

7060
01:14:05,000 --> 01:14:11,000
is the representations that these anticipations that these predictions actually unfold what what's the nature

7070
01:14:11,000 --> 01:14:17,000
of these representations and even now nowadays we still see lots of machine learning work where

7080
01:14:17,000 --> 01:14:22,000
representations are given in advance and so forth and

7090
01:14:22,000 --> 01:14:30,000
or are just totally done by let's say the atari games or these successes i mean there

7100
01:14:30,000 --> 01:14:35,000
you just give the plain image and you you kill the problem with so much data

7110
01:14:35,000 --> 01:14:42,000
that it still converges but the system is not really learning the systematicity the actual

7120
01:14:42,000 --> 01:14:51,000
structure of the problem and but most recent advances of course also from others for example the

7130
01:14:51,000 --> 01:14:57,000
the dreamer architecture or the planet architecture from from

7140
01:14:57,000 --> 01:15:03,000
what's his name hafner is the last name this score is also very close in this direction but no

7150
01:15:03,000 --> 01:15:09,000
no half now yeah yes they goes in this direction but they

7160
01:15:09,000 --> 01:15:16,000
don't really foster event predictive latent codes yet and i think they should

7170
01:15:16,000 --> 01:15:22,000
cool um i have some

7180
01:15:22,000 --> 01:15:28,000
predictions about where you might go but just to sort of restate it you said just in your previous answer that it was a

7190
01:15:28,000 --> 01:15:35,000
crude active inference in words and so i wanted to ask what is active

7200
01:15:35,000 --> 01:15:40,000
inference crudely in words and then what key

7210
01:15:40,000 --> 01:15:46,000
uh pieces of the formalism today refined those words and then what do you see

7220
01:15:46,000 --> 01:15:51,000
being incorporated into the formalism going forwards

7230
01:15:51,000 --> 01:15:57,000
so what's what's active inference crudely speaking and what

7240
01:15:57,000 --> 01:16:02,000
well it's uh active inference essentially formalizes

7250
01:16:02,000 --> 01:16:08,000
how our minds and how clever learning systems

7260
01:16:08,000 --> 01:16:13,000
should infer develop generative models about the world

7270
01:16:13,000 --> 01:16:21,000
and use these models to pursue self-motivated co-directed behavior

7280
01:16:21,000 --> 01:16:27,000
that's the general formalism and my point today was essentially that this

7290
01:16:27,000 --> 01:16:32,000
formalism is overly general in a sense because it doesn't specify

7300
01:16:32,000 --> 01:16:39,000
the nature of the encodings of the predictive codings and evolution has

7310
01:16:39,000 --> 01:16:44,000
obviously shaped our minds such that we have

7320
01:16:44,000 --> 01:16:51,000
particular expectations of this suitable of the structure that's suitable to

7330
01:16:51,000 --> 01:16:57,000
model the outside environment and thus interact with this environment in a more flexible adaptive

7340
01:16:57,000 --> 01:17:03,000
goal-directed socially competent manner and that's where the event predictive

7350
01:17:03,000 --> 01:17:10,000
stuff falls in thank you for the answer now a sequence of events

7360
01:17:10,000 --> 01:17:18,000
might have a narrative could be said that connects them how does narrative relate to your

7370
01:17:18,000 --> 01:17:24,000
work here is that just a bigger nesting event or how do we connect micro scale

7380
01:17:24,000 --> 01:17:30,000
events which it's very nice how you know rolling off the table was a clear demonstration of that kind of an event

7390
01:17:30,000 --> 01:17:37,000
how does that connect to broader narratives yeah very good uh question very good and

7400
01:17:37,000 --> 01:17:42,000
very interesting um is i think

7410
01:17:42,000 --> 01:17:49,000
as i said in uh as i try to imply also um that the events exist on multiple levels

7420
01:17:49,000 --> 01:17:56,000
of granularity and precision and an event can consist of yet sub-events essentially right i mean like

7430
01:17:56,000 --> 01:18:04,000
the whole episode of writing a letter for example consists of writing individual words sentences paragraphs

7440
01:18:04,000 --> 01:18:09,000
thoughts uh expressing individual thought components and so forth right so

7450
01:18:09,000 --> 01:18:16,000
so and so an event per se um can i mean you have the event of your

7460
01:18:16,000 --> 01:18:21,000
current day right and you compress this awesome event so so there's so that's the beauty

7470
01:18:21,000 --> 01:18:27,000
about it i think also because um an event it's not

7480
01:18:27,000 --> 01:18:33,000
it's not there's not only one event unfolding in our minds in every here and

7490
01:18:33,000 --> 01:18:38,000
now situation but it's it's about this

7500
01:18:38,000 --> 01:18:46,000
different aspects that have beginnings and endings right like this like this meeting here but my answer to your

7510
01:18:46,000 --> 01:18:52,000
question my individual sentences my individual words my actual utterances that i produce

7520
01:18:52,000 --> 01:18:58,000
these are all events nested within each other and each of these components is

7530
01:18:58,000 --> 01:19:03,000
characterized by certain stability situations like the whole

7540
01:19:03,000 --> 01:19:10,000
while we have this meeting um we are in a certain interactive situation and this is more or less

7550
01:19:10,000 --> 01:19:15,000
stable during the whole meeting yeah i guess i'm the presenter here today and so forth and then we have listeners and

7560
01:19:15,000 --> 01:19:21,000
you are the the mediator so to say right and this is the situation the whole time and this is

7570
01:19:21,000 --> 01:19:28,000
stable from from the beginning of the end from the whole situation right but when you post me a question then you focus in on this question answering

7580
01:19:28,000 --> 01:19:33,000
situation event and that's one particular event which is nested within the other event and while i produce my

7590
01:19:33,000 --> 01:19:40,000
train of thought there are also the events again unfold right so there's so there's a lot and and each of these car

7600
01:19:40,000 --> 01:19:46,000
events though is characterized by certain stabilities that are unfolding

7610
01:19:46,000 --> 01:19:52,000
such as the question answer situation where we focus on a particular component of this current talk and the current

7620
01:19:52,000 --> 01:19:59,000
topic of this meeting awesome what that reminded me of was

7630
01:19:59,000 --> 01:20:05,000
kronos and kairos two greek word roots for time and the chronometer that the

7640
01:20:05,000 --> 01:20:10,000
decimal points and the time and there's nesting of chronology like the second is

7650
01:20:10,000 --> 01:20:15,000
within the deca second or the you know the minute the hour but then kairos this

7660
01:20:15,000 --> 01:20:22,000
more semantic or action-oriented kind of timeliness also has nesting and that can be

7670
01:20:22,000 --> 01:20:28,000
perfectly nesting like the example of the letter with the sentence the word

7680
01:20:28,000 --> 01:20:34,000
etc it's funny because it's called a letter right at the bigger and the smaller level

7690
01:20:34,000 --> 01:20:39,000
but then also the boundaries could be uh subjective so not to say arbitrary

7700
01:20:39,000 --> 01:20:46,000
but literally defined by the subject learnt by the subject and so it seems to be no problem to have machine learning

7710
01:20:46,000 --> 01:20:53,000
systems that can nest kronos very well but how do we achieve um inadequate

7720
01:20:53,000 --> 01:20:58,000
nested action-oriented representation um

7730
01:20:58,000 --> 01:21:03,000
any thoughts or ask a question from the chat i think um the

7740
01:21:03,000 --> 01:21:09,000
that's the very important part and that's also the beauty i think about the event structures right that essentially

7750
01:21:09,000 --> 01:21:17,000
it's not it's always subjective because it's the stability that each of us perceives that makes an event become

7760
01:21:17,000 --> 01:21:23,000
an event now and i mean because our world for us is all the same we are very similar in

7770
01:21:23,000 --> 01:21:28,000
these event structures but nonetheless of course the events are very individually

7780
01:21:28,000 --> 01:21:34,000
unfolding in each of our minds and they are characterized by these stability and

7790
01:21:34,000 --> 01:21:40,000
systematicity properties that each event has and the nice thing about it as well is

7800
01:21:40,000 --> 01:21:45,000
that the petitioning and segmentation is extremely flexible right i mean you can

7810
01:21:45,000 --> 01:21:52,000
it doesn't depend on time in order to thus implement this in machine learning

7820
01:21:52,000 --> 01:22:00,000
systems you need to have a system that can flexibly maintain particular latent states over

7830
01:22:00,000 --> 01:22:07,000
extended periods of time and systematically change them when the time is ripe so to say and this is what we

7840
01:22:07,000 --> 01:22:12,000
have done with the gate lord system that i have showed towards the end which is going to be

7850
01:22:12,000 --> 01:22:17,000
published in december very cool so i'll ask a question from

7860
01:22:17,000 --> 01:22:24,000
the chat so dean has asked entertaining counterfactuals is time

7870
01:22:24,000 --> 01:22:33,000
distributing and time provides the twist like a mobius strip so if we include diachronic continuity

7880
01:22:33,000 --> 01:22:43,000
with event episodic partitioned do we break free from training

7890
01:22:43,000 --> 01:22:48,000
a classical a classical dean phrasing to be sure which i just read literally

7900
01:22:48,000 --> 01:22:53,000
rather than choose to interpret but maybe you can help me with the

7910
01:22:53,000 --> 01:23:00,000
interpretation but uh i would say um the the counterfectional component

7920
01:23:00,000 --> 01:23:07,000
was the problem that the counterfactual is time demanding or consuming or mentally challenging essentially to

7930
01:23:07,000 --> 01:23:13,000
because time is on the issue and so i need to be so i cannot do counterfactual reasonings

7940
01:23:13,000 --> 01:23:19,000
on the fly and uh that's certainly true we have um we fall typically in our best

7950
01:23:19,000 --> 01:23:24,000
explanation very quickly and habitually and only when we

7960
01:23:24,000 --> 01:23:31,000
really realize errors we fall into counterfactuals i usually give

7970
01:23:31,000 --> 01:23:37,000
the example with a cricket ball actually usually i give a simple example which is non-dynamic like the ball fits

7980
01:23:37,000 --> 01:23:43,000
into suitcase because it is large for example

7990
01:23:43,000 --> 01:23:48,000
so the ball fits into the suitcase because it is large yeah so it is

8000
01:23:48,000 --> 01:23:54,000
suitcase that is large and not the ball and um and that is kind of also a

8010
01:23:54,000 --> 01:24:03,000
typical garden pass effect in in linguistics here you fall into the best interpretation the pronoun it usually

8020
01:24:03,000 --> 01:24:11,000
fits with the subject and so uh you your best hypothesis to to bind it to the subject and you read on and then you get

8030
01:24:11,000 --> 01:24:18,000
to the final sentence and then you try to imagine why does the ball fit in the suitcase and then essentially you

8040
01:24:18,000 --> 01:24:23,000
realize you have to revise your model yeah and that's kind of the counter factual part that only happens i think

8050
01:24:23,000 --> 01:24:30,000
when there is surprise or other reasons to really pursue this additional cognitive effort

8060
01:24:30,000 --> 01:24:37,000
great answer let me see if there's a link to what dean suggested and the stochastic parrot

8070
01:24:37,000 --> 01:24:42,000
notion so if we think about a stochastic seed

8080
01:24:42,000 --> 01:24:47,000
preferring seed rewarding parrot it's like there's some maze and the seed

8090
01:24:47,000 --> 01:24:53,000
could be on either side and so it's rewarded when it's discovering that it's strengthening that and then it switches

8100
01:24:53,000 --> 01:24:59,000
and then it just stochastically sometimes investigates the other branch and then reinforces that

8110
01:24:59,000 --> 01:25:04,000
now that's all time bound if we think about the

8120
01:25:04,000 --> 01:25:11,000
event as the context then the counterfactual the continued

8130
01:25:11,000 --> 01:25:17,000
possibility of imagining a counterfactual is kind of what allows

8140
01:25:17,000 --> 01:25:25,000
the rapid moving from the training on the side that it has been on

8150
01:25:25,000 --> 01:25:31,000
and in one step with one gate opening we can counter factually

8160
01:25:31,000 --> 01:25:38,000
enter into a different context and then he adds um it's bidirectional

8170
01:25:38,000 --> 01:25:46,000
and the not present is that now let me restart this literally he says the not present is now

8180
01:25:46,000 --> 01:25:52,000
plus then and then plus now this is our ability both episodic and

8190
01:25:52,000 --> 01:25:58,000
diachronic continuity that is the what if the mind can work out

8200
01:25:58,000 --> 01:26:06,000
yes and that's that's a that's a beautiful part right i mean our mind can essentially by these gate openings

8210
01:26:06,000 --> 01:26:12,000
create a switch between alternative

8220
01:26:12,000 --> 01:26:17,000
events or event sub-event components switch the entity property

8230
01:26:17,000 --> 01:26:23,000
uh switch other aspects of it and and uh and it not needs to do so it's

8240
01:26:23,000 --> 01:26:29,000
not only doing this by randomly switching these things because it usually switches them very systematically in a meaningful manner

8250
01:26:29,000 --> 01:26:34,000
that actually the consequence of an event actually um changes it well right

8260
01:26:34,000 --> 01:26:39,000
because you usually do what if questions that actually change the further event progression

8270
01:26:39,000 --> 01:26:46,000
and you can do this mentally really well and i think the this essentially

8280
01:26:46,000 --> 01:26:52,000
can could be done and should be further explored with such gating mechanisms

8290
01:26:52,000 --> 01:26:59,000
really cool um component for future research i think to really show that such systems not only are able to

8300
01:26:59,000 --> 01:27:06,000
explain events but can also really reason in terms of counterfactual manners then by

8310
01:27:06,000 --> 01:27:12,000
saying well if i switch if this property would have been different then actually the whole thing

8320
01:27:12,000 --> 01:27:19,000
would have not happened this is actually also really important to assign blame for example

8330
01:27:19,000 --> 01:27:26,000
two people um would would you not have uh shot the ball yeah or something like this and and

8340
01:27:26,000 --> 01:27:31,000
so that's um that's also very socially relevant um lo and behold there

8350
01:27:31,000 --> 01:27:37,000
as well as designing education and thinking about when are we trying to

8360
01:27:37,000 --> 01:27:45,000
when are we fine tuning but not chain opening or changing a gate and then it made me think about that

8370
01:27:45,000 --> 01:27:53,000
the equation which you did an awesome job to walk through and show and read it because like sensory motor inputs

8380
01:27:53,000 --> 01:28:00,000
the the equation is a linear string but then we can engage in this counterfactual

8390
01:28:00,000 --> 01:28:05,000
what if the inference on observables weren't conditioned on policy

8400
01:28:05,000 --> 01:28:10,000
and it's actually to have competency with those equations is to

8410
01:28:10,000 --> 01:28:16,000
know a little bit about counterfactuals and then on the cutting edge you're drawing from other fields and

8420
01:28:16,000 --> 01:28:22,000
you're asking looking at pizzullo and fristen at all 2015 and then asking well what if this

8430
01:28:22,000 --> 01:28:29,000
formalism included another event-based cognitive framework that was

8440
01:28:29,000 --> 01:28:34,000
not previously connected so it's like the learning trajectory for those of us

8450
01:28:34,000 --> 01:28:41,000
who are learning the equations includes the counterfactuals and the gate opening and then research is just kind of going

8460
01:28:41,000 --> 01:28:46,000
one step further and um so just maybe wonder about how we

8470
01:28:46,000 --> 01:28:51,000
design education to accommodate this kind of gate opening and counter factual

8480
01:28:51,000 --> 01:28:58,000
space how we accommodate education um

8490
01:28:58,000 --> 01:29:06,000
like what what for what kind of education you mean yes

8500
01:29:06,000 --> 01:29:11,000
what which education you're referring to human

8510
01:29:11,000 --> 01:29:18,000
human education for learning active inference or for learning another area

8520
01:29:18,000 --> 01:29:26,000
yeah i think that's like a good interesting far thought i would say

8530
01:29:26,000 --> 01:29:32,000
it's it's i think it's extremely useful for sure to

8540
01:29:32,000 --> 01:29:37,000
yeah particularly also to look at equations and and analyze their components

8550
01:29:37,000 --> 01:29:44,000
and then progressively also enhance them or selectively compact them take

8560
01:29:44,000 --> 01:29:49,000
kick away out the one summoned or the other and then you see exactly what's happening so

8570
01:29:49,000 --> 01:29:55,000
i guess hopefully this was educational how i presented it i guess

8580
01:29:55,000 --> 01:30:02,000
it's a flow of sensory inputs and then there's some different structures

8590
01:30:02,000 --> 01:30:07,000
if you think about the individual components in the equation as as event

8600
01:30:07,000 --> 01:30:14,000
components in the sense very cool um i have one more

8610
01:30:14,000 --> 01:30:21,000
question and then maybe any closing thoughts you mentioned you know the the zoom room or the the jitsi room online

8620
01:30:21,000 --> 01:30:27,000
events calendar events that we all get invited to um

8630
01:30:27,000 --> 01:30:34,000
that's just very interesting with how we're bringing our evolutionary and our ecological

8640
01:30:34,000 --> 01:30:40,000
behavioral embodiments into the digital space and so

8650
01:30:40,000 --> 01:30:46,000
what does it bode for online events thinking beyond just uh webinars but

8660
01:30:46,000 --> 01:30:53,000
like how do we think about online events because in one sense you're very local to me you're in my headphones giving me

8670
01:30:53,000 --> 01:30:59,000
the sensory flow but you're on a different continent and so where do you think

8680
01:30:59,000 --> 01:31:06,000
or what theoretical questions or applied questions does it raise to think about online events which are a novel affordance from an evolutionary

8690
01:31:06,000 --> 01:31:11,000
perspective

8700
01:31:11,000 --> 01:31:18,000
i mean i think for the one for the one good of it they have shown us that we don't need to

8710
01:31:18,000 --> 01:31:25,000
fly as much around the globe as we have done before the pandemic which is probably a really good thing

8720
01:31:25,000 --> 01:31:31,000
for our planet and we should acknowledge this and accept this i think

8730
01:31:31,000 --> 01:31:37,000
i have the impression that particular older colleagues have issues with that

8740
01:31:37,000 --> 01:31:44,000
and i think it's it's very good an important thing that we should be all

8750
01:31:44,000 --> 01:31:51,000
aware of that um and shrouded out loud as i do right now that

8760
01:31:51,000 --> 01:31:59,000
we are in a in a very severe situation for our world and um this pandemic has shown us also

8770
01:31:59,000 --> 01:32:05,000
and even shows us now how the humans are so easily able to

8780
01:32:05,000 --> 01:32:12,000
neglect things or deny things that are so obvious that they are occurring which is very scary and very

8790
01:32:12,000 --> 01:32:19,000
unfortunate for our own future meanwhile saying this

8800
01:32:19,000 --> 01:32:24,000
i think it's it was a great opportunity to

8810
01:32:24,000 --> 01:32:30,000
for all of us to acknowledge that video calls like this work really well

8820
01:32:30,000 --> 01:32:37,000
and of course um after one and a half years of pandemic i guess all of you guys also enjoy um

8830
01:32:37,000 --> 01:32:42,000
local meetings again and and going out with other people so we also see how

8840
01:32:42,000 --> 01:32:48,000
social creatures are and what is missing in such video calls which is of course the

8850
01:32:48,000 --> 01:32:53,000
personal interaction and the really the development of of getting a feel of

8860
01:32:53,000 --> 01:33:00,000
how the other person really takes also personally and when you can make a statement about what's actually

8870
01:33:00,000 --> 01:33:06,000
happening in true life out there that's of course um very different unfortunately not possible

8880
01:33:06,000 --> 01:33:13,000
we are zoom calls um but so i can only urge everybody that uh um

8890
01:33:13,000 --> 01:33:19,000
we should be aware that when we fly around the world we better make it worth the while and don't do this only for a

8900
01:33:19,000 --> 01:33:24,000
couple of days it made me think of a trilemma

8910
01:33:24,000 --> 01:33:29,000
uh fly we could have met in person you know we could have met in iceland or something somewhere in the middle

8920
01:33:29,000 --> 01:33:36,000
um there is an impact in the hardware and the bandwidth of a video call and that

8930
01:33:36,000 --> 01:33:41,000
impact is very cloaked and then there's also machine learning

8940
01:33:41,000 --> 01:33:47,000
models and so maybe somehow i don't know if x is the impact of a flight

8950
01:33:47,000 --> 01:33:56,000
is a video call point one or one thousandth or one millionth of a flight and making some of these

8960
01:33:56,000 --> 01:34:01,000
impacts that are very real clearer so that we can

8970
01:34:01,000 --> 01:34:07,000
design events that are policies that align with our preferences

8980
01:34:07,000 --> 01:34:12,000
to reduce our uncertainty and to connect socially but also to

8990
01:34:12,000 --> 01:34:19,000
pragmatically keep certain ecological parameters within a region that we can

9000
01:34:19,000 --> 01:34:25,000
thrive in yes absolutely that would be good i mean a video call is certainly even less than

9010
01:34:25,000 --> 01:34:30,000
the millions i think from a flight although of course it is significant but

9020
01:34:30,000 --> 01:34:36,000
it's i mean incomparably less and

9030
01:34:36,000 --> 01:34:43,000
but nonetheless one has to be of course also be aware that um yeah unfortunately we are in a

9040
01:34:43,000 --> 01:34:50,000
situation where we really need global efforts so one cannot really point fingers to individual persons because we

9050
01:34:50,000 --> 01:34:58,000
need global policy changes to save our own world and our future of our children and

9060
01:34:58,000 --> 01:35:03,000
i have three kids and i'm very concerned

9070
01:35:03,000 --> 01:35:10,000
do you have anything that you'd like to just leave us with or that's a perfect closing note

9080
01:35:10,000 --> 01:35:16,000
no i don't want to leave on the downside sure it was a great pleasure to to do

9090
01:35:16,000 --> 01:35:23,000
this uh with you guys and now we drifted a little bit to the climate situation and i'm i'm thankful that we also can talk about

9100
01:35:23,000 --> 01:35:29,000
this a little bit and uh but i hope it's it i mean it's hopefully you also see

9110
01:35:29,000 --> 01:35:36,000
that it's an amazingly exciting topic and uh not only one inside how our human

9120
01:35:36,000 --> 01:35:41,000
mind works and how we manage to be as intelligent

9130
01:35:41,000 --> 01:35:49,000
but also as limited as we are in the end and but meanwhile also i think it has

9140
01:35:49,000 --> 01:35:56,000
certainly ai is is amazingly developing over the last

9150
01:35:56,000 --> 01:36:02,000
couple of years and um yes and and there i guess also uh things

9160
01:36:02,000 --> 01:36:07,000
go so fast so it's important that we uh think about

9170
01:36:07,000 --> 01:36:13,000
the dangers of a iron unfortunately as we have seen with cambridge analytica as

9180
01:36:13,000 --> 01:36:19,000
only an example of what is going on right now in terms of ai driven

9190
01:36:19,000 --> 01:36:25,000
targeted influence of our own selves and our way we think

9200
01:36:25,000 --> 01:36:32,000
about stuff and our beliefs and hopefully we will manage also to foster this awareness of this and thus

9210
01:36:32,000 --> 01:36:41,000
be more ready to counteract it and find back to our beautiful human social abilities

9220
01:36:41,000 --> 01:36:48,000
thank you for that excellent and uplifting note it was really an honor martin so thanks

9230
01:36:48,000 --> 01:36:54,000
again for the presentation and the chat really appreciated as well you and any colleagues are always

9240
01:36:54,000 --> 01:36:59,000
welcome to come on a live stream or to get involved in an act in flab in any way so this was

9250
01:36:59,000 --> 01:37:05,000
just a great conversation and it was hopefully very informative for our audience as well

9260
01:37:05,000 --> 01:37:11,000
i hope so too um thanks for inviting me here and thanks for running this lab it seems to be a really good group and i

9270
01:37:11,000 --> 01:37:17,000
hope you can stay in contact over the next months and years thank you we will


SPEAKER_02:
Great.

Hello and welcome everyone to Actim Flab.

Today is April 28th, 2021.

It's Actim Flab guest stream number 5.1 with Martin Beale.

Martin, thanks so much for joining us on today's stream.

And we're really looking forward to hearing about your perspective and what you have to share here.

So if anyone in the live chat would like to ask questions, we'll be relaying those to you during and at the end of the stream.

Other than that,

Please take it away, and thanks again for coming on.


SPEAKER_00:
Thank you, Daniel.

Welcome to my talk, everybody.

I will talk about, or I will talk about, at least in the background, about a paper I wrote with Felix Pollock and Ryota Kanai, which is called Technical Critique of Some Parts of the Free Energy Principle.

But I will not go directly through this paper.

I will kind of try to give a more updated perspective.

Yeah, what else to say?

I work at Araya in Tokyo.

And this work is also funded by the Britain World Charity Foundation.

And Felix was supported by the Monash University.

Okay, so here's the...

content of this talk.

First I'll make some preliminary remarks and then talk about my perspective basically on the free energy principle and how it can maybe split into two parts.

The construction of agents and the identification of agents and then I'll talk about what the FEP agent construction is and what the FEP agent identification method kind of is.

And then I'll talk about the relation between those, or at least the claims relation between those.

And then I'll go through some issues with this agent identification method and also with this relation.

Okay.

Um, so yeah, like I said, the main publication, uh, is same name as the, has the same title as this talk and, um, came out this year.

And it resulted from an effort with Felix, where we we actually set out to do like a discretized version of the life as we know it paper from 2013.

But we failed.

And then in the end, we wrote this critique.

So yeah, I will I will give this I will, in this talk, I will give a

my personal view a little bit of the free energy principle and why I find it interesting.

And like I said, I won't go through the paper in detail and instead summarize how I currently see the issues, also the issues that we mentioned in the paper.

But the FEP, the free, so FEP means free energy principle, has, while we were writing this paper, it has changed.

And so

my perspective has also changed.

And but if you have questions, particular questions about the paper, then you can, of course, ask me afterwards.

Okay, so here's the maybe

a little bit more detailed overview.

So in my opinion, the free energy principle can be viewed as two parts.

And the first part is basically what maybe you've heard is kind of the statement that Markov blankets imply Bayesian inference.

And so from my perspective, this is kind of a way to identify agents in stochastic processes.

So this is one part that is contained in the FEP literature.

And another part is about how to construct artificial agents, for example, for reinforcement learning or partially observable Markov decision problems.

And I call this the constructing agents part.

The claim is that if you read papers about this constructing agent part, the claim is that this construction is based on this identifying agents part.

So the construction is based on this Markov blankets imply Bayesian inference part of the literature.

And I will kind of argue that this has not been established.

And I also kind of think it's questionable, at least in the current state.

Then I also present some other issues with the AFP among those that I mentioned in the paper.

And these issues, so one thing I want to say is that these issues, they kind of would affect, I mean, if I'm correct about those issues, then they affect the usefulness of the free energy principle.

or agent identification, so then we couldn't identify agents used in the FEP.

And this also would question it as a theory of life in general, or life as we know it, if you want.

And then it would also affect this claim that

these constructed free energy principle agents are somehow based on first principles more than, let's say, some other reinforcement learning agents.

What all of these points, in my opinion, won't affect is

that these free energy principle agents, like free energy minimizing agents that are constructed in the literature, that they work at all, basically, or that they can be useful for some research.

I mean, I think that these agents' work is kind of independent of this whole Markov blanket implies Bayesian inference pattern.

But maybe I'm wrong.

OK.

So what are these two things in the construction of agents and the identification of agents?

So like I said, I think the literature can be split into these two parts.

And like how to construct an agent or a controller.

And the other one is how to identify an agent in a Stochastic Dynamical System.

And yeah, so

And like I said, also before this first method, how to construct agents or controller is claimed to inherit from the second method.

And these are still claims that are current.

So there's a paper that came out just this year or actually a week ago.

So I think it's called sophisticated inference.

Okay, so what do I mean by construction of agents?

Or what is this?

What is the problem of construction of agents?

Because that's a different problem than the identification of agents.

That's kind of the first point I want to make that these two problems are different in general.

So yeah, in artificial intelligence research, what people are occupied with is the construction of intelligent agents for some given environment or task.

And yeah, so basically what these people do in artificial intelligence research or engineering is programming robots or virtual agents.

And of course, the robot is then kind of fixed environment or if you have a virtual world, then this is also a fixed environment.

Then if you have some reinforcement learning problem, then you have kind of a task.

Okay, so

Um, like I said, we, in these cases, what we have is some environment, uh, that is given.

And I, uh, I write this here as a Bayesian network, and then we construct a controller.

And then this, what this basically does is we have some environment states that I call ETA and sensor values that depend on ETA.

And we have dynamics of eta.

So basically, towards the right, the time step increases.

So we have some dynamics of this environment, which tells us how eta changes under the influence of actions, depending on its previous state.

And the environment produces sensor values.

And what an agent does is it takes these sensor values

and updates its internal state, which is called mu, and produces actions accordingly.

So as you see here, mu depends on both the sensor values and the actions.

How exactly these edges look, there is some variability in it, but it's roughly one idea.

Most of them are roughly equivalent.

So like I said, or a bit more formally, maybe, uh, each of T is an environment state or an environment where task is defined as by, uh, an environment state sensor values, some kind of action interface.

So some dependency on the external variable.

So this dependency of update of each on the action.

and the dynamics, which means if we are in discrete time, then the dynamics can be specified as this Markov kernel or conditional probability

of the next internal state, the next sensor produced sensor value conditioned on the past internal state, past sensor value, and the action, which is the inference from the agent.

And in continuous time, we could also formulate this.

And one way to formulate this looks like this.

time derivative of the internal state as some function in its own state and the sensor and the action plus some noise and similarly for the sensor dynamics.

And, uh, maybe it's not so important, but in general you can have an, uh, an environment can have some kind of reward that it gives to the agent as part of the sensor value or, um, it may not.

Okay.

And then what's an agent.

So an agent has this internal memory state and it refuses actions.

And it also has an interface for accepting sensors that are produced by the environment.

And its dynamics can also be written as this kind of conditional probability.

Or similarly, with time derivatives in the time continuous state.

And maybe the main part

that's going to be relevant for something later in this talk is that we can combine such environments and agents and then they form a Markov process.

So then we have some joint variable X and the product of these two conditional probabilities, the one for the environment and the one for the agent.

they induce this Markov kernel of the joint process.

And similarly, in the continuous case, we can see the whole thing as one, the combination of the environment and the agent as a single Markov process with state x.

Okay, so that's kind of what we maybe should keep in mind.

Some remarks on this.

Usually we construct these agents to solve some problems, so we have some kind of reward.

So this is the case in reinforcement learning for POMDPs.

But we can also have agents that have just intrinsic goals and they don't need any reward from the environment.

These things are usually called intrinsic motivations.

And the, these, the FEP agents from the literature, they actually kind of have both

in within them.

So they have a they contain some part that's like an intrinsic motivation.

And they also can they have some part that is similar to a reward.

And so basically, the intrinsic motivation part is is like a maximization of information gain and the

Instead of reward maximization, we minimize some divergence from a desired distribution, which is good.

But at least on a high level, this can be seen as... So these two things are kind of combined in the expected free energy.

Yeah.

I mean, there are some tricks to this, but roughly this is true.

Okay.

So that's what I hope it came across, what I mean by construction of agents.

I don't know if there were any questions already.


SPEAKER_02:
Not yet.

Continue.

Thank you.


SPEAKER_00:
Okay, so then what do you mean by identification of agents, which is maybe a bit less common practice?

So first, maybe let's ask the question, what is an agent in general?

And so very roughly,

there's a definition by and they say, um, to be an agent, you have to be something that's individuated that can act and make things happens and, uh, can act, which just means to make things happen, basically.

And, uh,

it can act to achieve, these actions are performed in order to achieve some goal.

So an agent has also a goal.

Maybe something that has a goal and acts to achieve, actively pursues this goal.

And these agents that I just talked about, these constructed agents that we construct in artificial intelligence, but also in the FEP literature,

they are agents because they are kind of by construction.

They are individuated because we kind of know where their boundary is.

Like if you ask some engineer,

what they have to do to construct the agent, they know exactly what to do.

So they seem to know where the agent is or what the agent is.

And of course, since we program into them the intrinsic motivation or some kind of reward maximization mechanism, they are also co-directed by construction.

Um, okay.

So, but then this is not, this is just what, what agents are.

So what's, what's, what about identification of agents?

So the, the, at least for me, the origin of this question is the kind of the problem of agents in physics.

So the physicists, they tell us that everything is determined by the laws of physics.

So that means everything that happens is made happen by the laws of physics and not by any thing within the universe.

So if they are correct, then nothing within the universe can make anything happen.

But that also means that nothing can act to achieve a goal because nothing can make anything happen.

So you can't act to achieve some goal because you can't act in the first place.

So this seems to suggest that basically there are no agents in the universe.

It must be some kind of illusion.

But at the same time, I mean, we are here.

We are agents and we kind of want to, we kind of have goals, or at least it seems extremely likely.

And it also seems like there are things that don't.

And we built these robots and they seem to be quite real things and have goals also.

So it's kind of, so maybe there is some way, there should be some way to make these two things compatible.

So what we want is, at least if you like the physicist's perspective, and also you like to think about agents, then maybe what you want is a formal theory of agents that is compatible with physics.

And then, so one problem that any such theory should be able to solve is that if you have some kind of universe, and in physics, that's basically just a dynamic system, then what and where are the agents?

So if you have a formal definition of agents or a formal theory of agents, then

this is the question that it should probably be able to answer.

So here's the universe.

And what we want to do is we want to check where inside this long history of the universe, there are these things that are maybe can be called agents like this animal here.

a bit more formally.

So we have some kind of, uh, in physics, we have some kind of dynamic system.

We have a state space and states, and we have a dynamical law that is looked, maybe it looks like this, uh, the time derivative of Y is some function of the, so the time derivative of the state of the universe is a function of the current state of the universe.

And then we have, uh, um,

So these things have trajectories.

And if we select one of those trajectories by some initial condition y0, then what we would expect from a theory about

theory about agents and one that can also identify agents, then what we would want is that this theory gives us, like once we give it such a trajectory of a dynamical system, or maybe the trajectory and the dynamical system, that this theory tells us what is the set of agents

contained in this trajectory.

And it should tell us this, including what is basically its body or its individual.

What is the individual?

What are the actions and what are the goals?

OK.

So this is kind of the agent identification problem.

And the FEP does something like this.

They start not directly maybe with a deterministic dynamical system as the physicists maybe like, but they instead start with something that's usually considered as a subsystem of such a deterministic universe.

the subsystem should obey this kind of Langevin equation.

And then the FEP kind of tells us conditions under which there's an agent inside such a thing.

So I'm not 100% sure how this relates to the exact formulation I gave here, where you just get a trajectory and the system, and it tells you where the agents are.

There are some subtleties to this.

But roughly, I think the free energy principle is definitely a step towards such a

And this part of the free energy principle is what I call the free energy principle identification.

The FEP agent identification method.

And it is, if you're familiar with the literature, this is where the Markov blanket plays the fundamental role.

The so-called Markov blanket, I should maybe say.

Okay.


SPEAKER_02:
Martin, can I ask, what do we gain or lose by going to the Langevin equation?

What is gained or lost in that maneuver relative to the more general phrasing you had on the previous slide?


SPEAKER_00:
Yeah, good question.

So I Well, I don't really know.

I, all I can say is that.

Yeah.

I mean, one thing you definitely gain is that if you I mean, this thing, it's kind of hopeless.

Like if you would really

If we would need the particle physics equation, then we would probably never be able to identify agents because the particle, I mean, it's just too complicated and complex to say much about it.

So here, we can kind of say, OK, I can ignore a lot of stuff.

And all I have to find is some kind of part of the system that obeys some kind of approximate equation, basically.

And this can be enough.

So I guess this is one advantage.

Yeah, I mean, like, there's one other thing.

I'm not sure.

I mean, there's one other thing I sometimes think where this is might be useful for and that's so what we what we kind of want is that this agent theory, I mean, if you think about it, basically, a billion years, like a few at some point,

there probably weren't any agents inside the universe.

So, so at that point so, so agents kind of like a full blown theory of agents, they should, it should allow trajectories with within which agents appear and basically disappear, or at least they have to be able to appear.

because that's kind of what we think happened in the universe.

But this kind of appearance and disappearance is actually not so easy to do.

Especially if you would just kind of naively split up this system, the whole system into two parts and say, is one part the agent and one the environment?

Because then you split it basically for all time.

this is kind of weird because you would want that the split only works for a short time or once there actually is an agent.

So sometimes I think okay maybe this approximate equation only works for a short time and that's an advantage of this also that

There might be short timeframes where this equation works and others where it doesn't work.

And then we could think about whether this means the agent appears or not.

But actually, I'm not so sure about the details.

So yeah, that's all I can really say.

I mean, I think, yeah.

But it's a very good question, I think.

OK.

Any other questions?

Because it would be a reasonable point.


SPEAKER_02:
Not yet, but anyone is welcome to do so.

Thank you, though.


SPEAKER_00:
OK.

So how does the FEP construct agents?

I'm not going to talk about this a lot, but just maybe I'm only going to maybe mention the things that are relevant later.

And this is basically just the discrete time free energy minimizing agents.

So in the free energy prints in the FEP literature, especially the early ones, you also have continuous time free energy minimizing agents.

But yeah, I'm going to mostly ignore them here.

They're not really.

There's nothing in particular I want to say about them.

So in this discrete time free energy, in this discrete time FEP agents, the agent state mu consists of hyperparameters.

Basically, hyperparameters for the conjugate priors

of the models that this agent is constructed to use, basically.

And these hyperparameters, they encode the beliefs over both the environment state, basically a model of the environment state,

a model of the dynamics.

Sometimes there's no model of the environment dynamics, but sometimes there is.

So that's kind of important.

I mean, they basically, yeah.

So the actual state consists of these hyperparameters.

And then what happens with this is that in response to some observation and past action, the agent updates

the beliefs and actually that just means that it updates these hyperparameters.

And this is done via variation of Bayes.

And then the agent basically, it also has a belief over actions and this also gets updated and this is done in such a way that it minimizes the expected free energy.

which, I think I've mentioned this before, maximizes information gain and kind of minimizes the divergence from desired beliefs.

Maybe somewhat relevant is that if the model class that the agent maintains contains basically the truth, the true model, then

or an exact model of the environment and so on, then the belief update, this variation of belief update actually is the same as the exact patient update.

Okay, so that's already all I say about the FEP agent construction.

And now I'll talk a bit about how the FEP

agent identification works.

So the main relevant literature on this agent identification are four papers, as far as I can see.

And the first one is Life as we know it from 2013.

And then there's a monograph, quite a long

paper, I think 180 pages or something or 80 pages, lots of pages.

And that's called the free energy principle for particular physics.

And there's a kind of an extract from that, but with small differences and it's called Markov blankets, information, geometry, and stochastic thermodynamics by Thomas Parr.

And yeah,

and also Friston and another author.

And then there's also a paper which is written in response to our criticism that's called Some Interesting Observations on the Free Energy Principle.

Is there a question?


SPEAKER_02:
Nope.


SPEAKER_00:
Sorry.

Okay, so...

Okay, so this last paper was in response to our criticism.

So these, I think these contain basically what the, these contain all the things relevant to this agent identification problem.

And basically they are the main papers concerning this Markov blanket thing.

There are also some other ones, but I think actually these ones are the essential ones.

OK, yeah, the paper we wrote, it's mainly concerned with life as we know it, because the second, none of the other papers existed when we started writing it.

This one, the energy principle for particular physics came out while we were writing it.

But yeah, as I said, I will try to give my perspective that also includes these newer constructions or approaches or papers.

Okay.

Yeah, so what is the rough idea of this?

I can't go through this in detail.

Mainly, or among other reasons for this, is that I don't really know.

I don't really understand how it works in detail.

I still haven't gone through the whole construction.

Because usually, I get just stuck before I'm through.

Okay.

The rough idea is, so we have our universe, and like I said, we find this subsystem that obeys the Langevin equation.

Omega is some kind of Markov and white noise, and we also require that this

subsystem is such that not only does it obey this equation, but also that this equation has a steady state distribution P star.

Okay.

I think this is just going to be the same thing again.

We already saw this.

Ah, okay.

So now what we're going to do is we find the subsystem basically, let's say,

We have this system and it obeys this equation.

I mean, so basically, imagine this environment with this animal.

And now we further split up this system.

Oh, actually, I thought there was going to be something.

I'm sorry.

So we further split up the system.

We split it up into, here you basically, I hope you can kind of see that there's a central nervous system of this animal.

And we kind of split up this whole system into relevant, into parts that are aligned with agency in some way.

Let's go with this.

Okay.

Now, okay.

That's what we're going to do afterwards in one step.

But first, sorry, it's a little bit confusing.

But there is one nice fact about, so this should have been before these images.

There's one nice fact about these kind of equations.

Sorry, let me just go back.

here about these kind of equations, if they have a steady state distribution.

And only if they have a steady state distribution.

If they don't, then you can do the following.

So in that case, if they have a steady state distribution, then you can write f of x as this

a kind of gradient descent or ascent, whichever way you formulate it, of the log probability, of the log of the stationary distribution.

And yeah, so then you also get these two matrices here in front and R is anti-symmetric and gamma is symmetric.

Um, and also, yeah, you can write this here.

The, the log probability, the log, uh, a state probability, um, you can write it or turn it into this, uh, negative log, um, probability.

And that's the definition of this.

I you've maybe seen in the papers, the surprise.

Okay.

Yeah.

This is not really trivial.

When exactly you can write this, it's not really trivial.

Especially when this gamma matrix and R matrix don't depend on the position X, which is something that is usually assumed in the FEP literature.

Yeah, I mean, if the force is linear, or if F, the vector field, or sometimes also called flow is linear, then it works.

And this also means that around fixed points of a nonlinear vector field, it works.

Okay.


SPEAKER_02:
Can I ask a question on that last slide, Martin?

Yeah.

Maybe giving an example of a kind of system that people apply it to, what is X here?

And then what does it mean that R is anti-symmetric and that gamma is symmetric?


SPEAKER_00:
Okay.

So I actually don't know.

I don't think anybody applies this to anything.

Let me think.

at least, yeah, I mean, like, that's actually one of the criticisms I have.

But we can definitely apply it to some systems.

And maybe I'm gonna look somewhere deep down there.

Oh, that was one of these pictures there.

Slow.

Let's get trapped in those.

Right, I saw it.

Okay, yeah, here.

Okay.

So

Here's a 2D example of such an equation and with the gamma and the R. So we say this force, let's say this force is, or this vector field is linear.

So it's just a matrix times the vector.

So then exactly, then we have this matrix times the state and

We get this equation and we can find gamma and R where you can compute them.

There's a way in the literature, you can also see it.

And it's also mentioned in our paper, how to compute those.

And you can, and we can compute the stationary distribution.

It's some kind of Gaussian in this case, and it looks like this.

And you see here's the, on the left you see this vector field and on the right you see the steady state distribution.

Here it's called ergodic density, but that's the same thing.

And how do these split ups look?

So if you look at the component gamma times the gradient, you get this vector field.

And if you look at the component

Oh, this should be R. Sometimes this is called Q, but that's the same thing.

Just in case you're wondering.

If you look at the component R, then it has this, it's basically the circular component of

this vector field, it's hard to see that this vector field here is has this circular component.

But if you see roughly here, you see that they actually aren't.

So this one is actually straight always.


SPEAKER_02:
Cool.

So the last one is that the hill climbing aspect and the right side is the is it what sometimes called solenoidal or the ISO contours?


SPEAKER_00:
Yes, so exactly.

And so there is one interesting thing about this.

And that's that

So whether this matrix R is everywhere zero or not decides whether the steady state is a non-equilibrium steady state or an equilibrium steady state.

And this is due to,

I mean, so the standard definition of non-equilibrium steady state and equilibrium, or distinction between non-equilibrium steady state and equilibrium steady state is that in the equilibrium steady state, you have detailed balance, and in the non-equilibrium steady state, you don't.

And if you have this solenoidal flow,

then you don't have, that's exactly when you don't have, it basically destroys the detailed balance.

Yeah, detailed balance means that basically if you make a small square here, then the flow, or two, let's take two neighboring squares,

And then if it had detailed balance, then the flow from one square to the other would always be the same as the square from the other to the first, from the second to the first.

But since here, uh, this flow always goes in, in this horizontal direction, uh, not horizontal in this direction, what goes in from the one side is never compensated, uh,

in the other direction.

So then you can say, Hey, but on the left side, it also doesn't look like it's compensated.

And that, uh, that's, that's true, but it is actually compensated by the noise because this is only, uh,

This is only the vector field and without the noise factor.

So this is only F of X without the noise factor.

And the noise factor in this, because it's Gaussian, the noise factor is also, it has no curl.

So this is why you, so the noise can compensate this component, but it cannot compensate this component.

All right.

Maybe that was a bit, maybe I shouldn't have taken that much time to explain this, but it's kind of nice.

Yeah.

Okay.

Uh, yeah.

So this is what, uh, we get this nice decomposition and, uh, it's kind of needed.

We only get it if we have this steady state, uh,

And it plays a reasonably important role in the literature.

OK.

So this other thing that we do with our subsystem is we split it up in some way.

And this is kind of what the so-called Markov blanket does.

Uh, so the, but here, I'm just going to, I'm not going to talk about what the microphone is yet.

Uh, I'm just going to say what it's, uh, so about the work that it's supposed to do.

So it will give us, um, this, uh, split of the state into four of the state coordinates into four parts, basically a partition of the coordinates.

And we will then call eta the external states, s the sensory states, a the active and action states, and mu the internal states.


SPEAKER_02:
Sorry, someone else just joined.


SPEAKER_00:
Was that a question?


SPEAKER_02:
Oh, no, no.

Someone else just joined in.

Not actually sure, but c'est la vie.

And I'll add a little password just so that no one else joins in.

You have to go for it.


SPEAKER_00:
OK.

So then what the Markov blanket also does, basically, it does all kinds of, it does a lot of work.

Let's put it this way.

Because all you need is that you have the subsystem and you have the steady state and the Markov blanket.

And then what you get is that the internal active and sensory states,

in this system appear to, and this is important, actively resist the dispersion of its Markov blanket.

Okay.

There's this word Markov blanket again.

So here you should read boundary or base.

You can also read individuality.

That's what this means in this case by engaging in approximate Bayesian inference.

So this statement basically has all these features that we kind of want from an agent identification theory, because it tells us that if we have this under these conditions, and by naming these variables in this way, then we see that these,

I mean, it basically, it makes sense to name these variables in this way because they, we have some kind of goal.

The goal is to resist dispersion of the Markov blanket.

So basically it's the goal is kind of persistence.

And the agent acts to pursue this goal because it actively resists this dispersion.

Yeah.

And it is individuated because we have this, we know which states belong to it and which states basically don't belong to it.

If this distinction between external and internal.

Yeah.

And then additionally, which is also kind of nice that this whole, this agent is also doing some kind of approximate patient inference, which is a bonus, I guess.

So this is why I,

I say that the FEP basically contains a theory of agent identification.

And this is basically the part that interests me the most about the free energy principle.

Yeah, OK.

maybe a unique selling point compared to all other kinds of, to most other approaches to constructing artificial agents.

And it's also, I mean, there are not many agent identification theories that I know of.

And I actually hope that I know all of them.

But yeah, actually, it's quite a, these aren't common.

OK.

Yeah, I said what this, I said this.

OK.

Yeah, so in summary, we just make three assumptions.

We say some subsystem has Langevin dynamics, it has a steady state distribution, and it has a Markov Franklin, and we're done.

We have identified an agent.

Yeah, nice, sounds good.

Okay, so let me, is there any question up to now?


SPEAKER_02:
There's a few more general ones, but feel free to continue and we'll get them, I think, at the end, because this next section will probably be very helpful.


SPEAKER_00:
Okay, so now let's talk about what the relation between this FEP agent construction and the FEP agent identification parts of the FEP literature are, according to the FEP literature.

uh so the these i kind of said this already but uh the thing that makes uh fep or active inference those are actually the same thing as far as i know agent special uh is that

are two things, or at least two things.

There might be more, but I think these are the main ones, or the most important ones, at least.

These FEP agents, the constructed FEP agents that I talked about before,

They can be seen as kind of a plausible or used.

They are used kind of as a plausible model in neuroscience.

And this is related to predictive coding and the patient brain.

But I actually am not qualified to talk about this because I don't know enough about neuroscience.

But the other thing that makes them special is the

one of the main points of this talk is that they are claimed to be based on first principles.

And when the statement that they're based on first principles actually means refers exactly to this, this FEP agent identification method.

So the first principles are basically what I just

uh, talked about that, the management equation and the steady state and the Markov blanket gives you this agent.

So let's, uh, in order to make sure we're not putting words into the mouth of the literature, let's read this quote once.

That's from the also sophisticated inference paper that just came out.

The key distinction is that base adaptive reinforcement learning considers arbitrary reward functions while sophisticated active inference optimizes an expected free energy that can be motivated from first principles.

Okay, actually here, it doesn't say based, but actually in another, like you can look at this paper in another place, it says based on first principles.

And also this is not only mentioned for the sophisticated active inference agents.

You would see similar statements in the literature in other places.

So this is, I mean, this is kind of,

In some way, it's super cool if it's true, I think.

But I'm not so sure it's true.

Okay, so some notes.

Yeah.

Okay, so this is about whether actually this, how important is this

How important is it for the constructed agents to be based on these first principles?

So in this, also in the sophisticated inference paper, the authors say that the agents expected free energy, which is the thing they minimize, can be motivated from first principles.

But they also seem to say that the expected free energy can also be, I mean, yeah, actually, I mean, they say in one place, they say that it can also be motivated via information gain.

Don't, and they, I mean, there's, there's one part that this refers to.

So as far as I understand it, they are, there's a, there are two ways to motivate people.

this expected free energy minimization.

And one is via this first principles and one is via some other derivation that involves information gain.

Now, I wouldn't say I understand these derivations, this derivation completely, but I do know that there's a paper by Cohen and Hutter

And they show that basically some combination of information gain and reward maximization, you can prove that it's optimal in some sense.

So since this expected free energy is kind of a combination of those two things,

I kind of think, let's put it this way.

I'm not surprised that it shows good performance or something like this, because maybe it's actually so closely related to the method by Cohen that it just works.

So all I'm saying, I guess what I want to say here is that in case the motivation via first principles fails,

uh, the FEP construction of agents may still be of interest and might still work.

And this also means that all of these, all publications, I think there are some, but yeah, I think there may be quite a few, all of these publications that, uh, that are basically based on this, uh, expected free energy minimum is minimizing agents.

they need not be without merit just because the first principle's motivation fails.

So, or in other words, they can still make as much sense, basically, even if this free energy principle agent identification is wrong.


SPEAKER_02:
If I could ask a question here, Martin, what, what kind of first principles are you most interested in?

And then what kind of a theory of agents or agency would be satisfying given your interest in first principles?


SPEAKER_00:
Yeah.

I mean, I, I, I think so.

So if you.

I'm interested in the first principles that work.

I don't know what the first principles are, but like maybe one, one first principle is that you can start from physics.

Like that's like, you can, you, you start with some, I don't know where I have to go up.

Yeah.

Something like this is you start with a dynamical system and you then

derive using only notions that are well defined for dynamical systems or within such dynamical systems to show that this thing, this system contains agents or not.

So I guess the first principles for

the first principles here are that, okay, uh, all we have to believe is, um, that there are such, uh, subsystems that obey the Langevin equation.

And I guess since the Langevin equation has been quite popular in physics, um,

this has some strong support.

And maybe that's sufficient to be your first principle.

Also, having steady state is fine.

It's well-defined.

I mean, that's a mathematically well-defined thing.

We can, in principle, check this.

So

Yeah.

Then the other thing that's used in the literature is the Markov blanket.

And I think that's also, yeah, it's a, it could be a well-defined thing.

I think it's a bit of a, like what it means in the FEP literature is kind of not, it's not really clear and it changes and maybe it has to change again.

But

at least the original.

So if it depends a bit, um, yeah, I guess I have to, I would have to show a bit more about the, um, Markov blanket, but yeah, I would say I, yeah.

First principles for me means probably just that you have to be able to define it for

dynamical system.

It has to be like all the notions you use, you can only use notions that are defined for dynamical systems.


SPEAKER_02:
Totally makes sense.

It's like if you're going to play the game of dynamical systems analysis, you can only use those rules.


SPEAKER_00:
Exactly.

You can only, yeah, exactly.

You have to construct everything from there.

You don't get anything else.

That's it.

You get the dynamical system.

That's it.

Yeah.

Okay.

Yep.

That's where I was now.

Yep.

I say this need not be there.

Yep.

Okay, so now, oh, okay, so that's a big part of the talk.

That's kind of that was kind of my personal, my current personal view on what's Yeah, and how I how I look at the FEP literature and what part Yeah, how I split it up, like I said, and yeah, so now, I would go and talk about

Basically, the issues I see with this FEP agent identification method.

But if there's any questions now, it might be also a good point.


SPEAKER_02:
That's it for now.

Definitely after this, we'll see if anyone has any questions.

But feel free to type them in the live chat if anyone has it.


SPEAKER_00:
Okay, so the first issue I have is that this Markov blanket assumption is known to be insufficient.

By now, it's not to be insufficient.

Maybe in the maybe at the for the very first paper, it wasn't known to be insufficient.

Actually, for the first two papers, maybe.

No.

Yeah.

I don't know.

Yeah.

I can't really say because it wasn't me, but yeah, no, it's, it's, it's fair to say that, uh, the first two papers, they might not have known this.

Okay.

Uh, yeah.

So, so, so I'm going to kind of go through the Markov blanket evolution a little bit, not, not, not an energy term, but the Markov blanket evolution in the FEP literature.

Maybe I should say, originally, I think I don't have this here.

I mentioned it down there.

Okay.

So originally, the term Markov blanket comes from Judea Pearl from this 1988 book.

Yeah.

So it's a technical term.

that had a definition before it was used by the FEP literature.

Okay, so, and in this original paper, the life as we know it, there is actually no definition of like, there's no, it's nowhere clear.

It's not clearly said what is meant, what kind of Markov blanket is meant.

I mean, there's some kind of graph, but Markov blankets come from directed acyclic graphs and the graph is cyclic, so it's actually not

clear what this meant.

I mean, there is, but it's clearly stated that the Markov blanket should imply this equation, that the vector field, when we split it up into the four coordinates, also, actually, so

It kind of implies also that there are these four.

We can split up the coordinate into four sets of coordinates, all the coordinates into four sets of coordinates.

And the vector field then has a certain shape, form.

namely that the internal and sensory states do not, the vector field components of the internal and sensory states do not directly depend on the internal states and the vector field components of the action and, active and internal states do not depend directly on the external states.

maybe I said something wrong, but yeah.

And we call this the, so we, we, we call this the vector field condition or, or, yeah, I mean, I call this the vector field condition because it's a condition on the vector field.

And in our paper, this is conditional.

Okay.

So this is kind of, I think in life, as we know it, this is kind of, this is the only, uh, real place where, uh,

the Markov blanket plays a role.

And by the way, this is not a Markov blanket.

I mean, not technically, this isn't a Markov blanket.

I mean, this isn't a Perlian Markov blanket condition.

Then in, at least in Thomas Parr's paper in 2020,

Markov blanket is said to imply this condition independence of the external states given the blanket states which are the sensory and the action states.

So we get this factorization of the stationary distribution or steady state distribution.

And that's also considered a condition of the Markov blanket.

This thing is also implicit in the, this condition is also implicit in the free energy principle for particular physics.

But it's not explicitly referred to as the Markov blanket, so it's a bit weird.

But this is actually a Markov blanket in the sense of Judea.

So this condition is a true Markov blanket condition.

And it has come to be called, not only by me, the time synchronous Markov blanket condition.

And in our paper, this is conditioned

Those two conditions, this one and condition one, they are independent of each other.

There are systems that obey this one and not this one, and systems that obey this one and not this one.

And those systems are not

complicated, they are actually linear systems.

So they are these Ornstein-Uhlenbeck processes where you can easily compute this stationary distribution.

And it's a Gaussian.

So you can look this up in the paper.

And I think one of the, I mean, this is wild speculation, but I think that the authors

or Carl Fristen, because he's the only author, kind of thought that this condition implies this condition.

It seems to me there are various reasons, but I'm not going to go into it.

But yeah, this seems to me the case.

And maybe you could think, I mean, you could think this.

I mean, it's not completely out of the question, but yeah.

So then we wrote this paper after we wrote this paper in 2019 and complained about some thing in the, some things in the,

free energy principle for particular physics paper, namely this thing called the conditional independence corollary, which is supposed to follow from the vector field condition, I think.

I mean, I think that's the claim.

but we showed that it doesn't.

And so then the Markov blanket or then an additional condition was presented in this paper called some interesting observations about the free energy principle or something.

This is the response to our paper.

And this condition is that the solar needle

component matrix R or Q has this shape.

Okay, that's why, I mean, I'm going to call this the solenoid condition if I ever need it again in this talk, but okay.

Yeah, since this came out after, I mean, this was in response to our paper, so we don't really talk about this.

Okay, so this is just some additional condition.

Uh, and then I think so.

Yeah.

So you can kind of see all of these, like these conditions are all used in to derive this statement here that, uh, that this statement here.

the FEP agent identification method, basically, that you get to identify, that we actually get, we can identify an agent that actively acts to, that acts to persist, basically, with the goal of persistence.

Okay, so we have this condition as well.

And then in Thomas Parr's paper, and also in the monograph, if we look closely, there is actually an additional assumption made in the free energy principle for particular physics, it's claimed to be a consequence.

of the other conditions, but that's not true.

We can show this.

It's kind of easy.

And in the power paper, it's actually said that it's an assumption.

So I've known that this is an additional assumption.

And this assumption is that there's some function sigma, and this function sigma relates to variables.

And these two variables are

the most likely external state, volt, eta, given the blanket states S and A, and the most likely internal state, given the blanket state S and A, according to the steady state distribution.

And so the assumption is that there exists a function sigma that maps the most likely internal state to the most likely external state.

And that this function doesn't need to exist, you can see

For example, I mean, it's not in our paper.

We should have probably put it in our paper, but we didn't.

A counter example is you can use basically the system we show in Appendix D. And there we show one of those linear

linear systems again.

And for this, the bolt mu is just one half a and the bolt eta is one half s.

And since S of T is not a function of A of T, and basically, so they are kind of, so every combination of S and T can occur.

So there is no functional relation between the two.

And that means there also cannot be any function sigma.

So this means we kind of have to additionally assume sigma and also

Yeah, yeah.

And the thing is, in my opinion, this sigma might actually be a much stronger, the strongest of all of these assumptions.

that are made, like stronger than any of the other conditions here, these other three conditions, the vector field, the TSMC, and the conditioner, and the solenoid condition.

I think it does probably the most work.

But since I don't really understand the whole picture, I can't really self-tell for sure.


SPEAKER_02:
But so yeah.

Well, just on that, sigma is sort of the proposed function that's literally linking

internal and external states.

So when we're talking about agents and the way that their internal generative model is being linked to generative processes in the outside world, that's like the key link.

That's what we're all about.

So I totally see where you're coming from.


SPEAKER_00:
Exactly.

That's a good way to say it.

Exactly.

I mean, yeah, I think I will say this later again, but let me just say it here already.

These three conditions...

they are all trivially satisfied by four independent stochastic processes.

So it seems kind of unrealistic, let's put it this way, that any of these conditions by themselves, or even their combination, would be sufficient to...

um give us this kind of active uh active resisting to anything it's like or modeling of the environment or the asian inference about the environment because for independent stochastic processes

Don't seem to, I mean, like it would be hard to argue that any of those is doing anything is modeling any of the other ones.

So, uh, yeah, exactly.

This is, um, but if you have such a relation, then yeah, then yeah, there might be something going on.

Okay.

So in summary, we have these four conditions, the vector field condition, the TSMBC, the sonar linear condition and the Sigma function existence.

And I would say maybe call those the FEP blanket together.

Okay.

Okay.

He, I said this now, apart from the signal function, all the conditions are satisfied by four independent linear stochastic processes.

Yeah.

So probably Sigma is the most significant assumption.

And yeah, I mean, it's maybe a petty point, but I kind of think that calling the combination of these conditions Markov blanket is at least, I mean, by now, it is definitely misleading.

I mean, I think the use of this word Markov blanket was misleading before, but I think it should probably be dropped.

But anyway, I'm going to call it FEP blanket.

Oh, sorry.

So, and one more thing, I am not, I cannot say that these conditions, these four conditions together are sufficient for the whole FEP agent identification argument.

Just because I don't really, I haven't really gotten through the whole thing.

But we know that they are necessary.

I mean, they are used in these arguments.

OK.

So that was the first issue.

How am I doing in time, actually?

Is it already super long?


SPEAKER_02:
You can go for as long as you want and we can go as long as you want.

You can go as long as you want today, Martin, and then we can always schedule another time to speak.

But this is really, really helpful.

And I'm sure that a lot of people are going to digest it and have a lot to talk to you about.


SPEAKER_00:
Okay.

Okay.

Then I'm just going to take, okay.

All right.

Next one.

Uh, this point is not in our paper.

Um, I guess it has been in the back of our heads, but I mean, yeah, I think it's, it's really also, it's become much more, uh, I've become much more convinced about it and yeah.

So I'm going to just mention it.

Uh, and I think the, it would, yeah, I think it's a kind of challenge to the FEP community maybe.

And I kind of, yeah, I think they are kind of aware of this by now also.

So the thing is that there is no example for which all of the FEP blanket conditions, the four I just mentioned, the four conditions I mentioned have been proven to hold.

So we don't know any Langevin equation that obeys all of these conditions.

Actually, I forgot the steady state.

Maybe it should be.

But yeah.

There's no large amount of equation with a steady state that obeys those.

So what this means is

We don't even know if there's any system that the FEP identification method applies to, or that this statement that, in very short, that the FEP blanket implies an agent, that this is an agent,

We don't know that there's a system for which this is true.

So this also means we can never check.

There's no system we could look at and check if there is actually something doing Bayesian inference and in what way it's doing it and that it's actively resisting dispersion.

So this is kind of... It's not... It would be kind of...

So in some way, this says the whole Markov blanket story or the whole, as I call it, FEP agent identification story might be vacuous.

This is what this says.

And I think it's kind of a serious point, and it should be addressed, I think.

OK, so maybe some notes on this.

So there's this primordial soup example in the original paper, and it's used, I mean, in life as we know it, and it's also used in the monograph.

But there's no proof that this thing has a steady state or that this steady state obeys the TSMBC, the time synchronous Markov blanket.

And also, of course, there's no proof that it has a sigma function because in life as we know it, there wasn't even any sigma function.

also this, the, like, you could say, maybe the constructed FEP agents are systems that obey all these conditions, but and then, I mean, maybe they are even some of them.

But this connection is never made explicitly, like, it's always just there's this claim that they are based on this, on these first principles and all of this, but

It's never shown, for example, that the agent environment combination, which forms this Markov process, that it has a steady state distribution.

And it's actually very unlikely that in all of them it has a steady state distribution.

And actually, I will argue that it's actually not true, definitely not for all of them.

And also, even if this agent environment pair would have a steady state distribution, we would still want to see that it obeys the TSMBC.

And also, that is kind of questionable.

So I should mention that this is like the second point that checking that the TSMBC holds is an idea that's by Nathaniel Virgo.

And there's some arguments about it, but I won't go further into it.

Furthermore, it is never checked.

We never look at the sigma function, which appears to play a significant role, but we never look at the sigma function in these constructed FEP agents.

What I'm pretty sure that it says is satisfied in a reasonable way is the vector field condition.

don't really know why I say this here, but yeah, like the, ah, okay.

So you could, you could, yeah.

Okay.

So, so you could maybe, so since me and Felix, we use the Ornstein-Uhlenbeck processes to show various things about the free energy principle.

And we, we know that it satisfies

quite a few of, like it satisfies three of these assumptions.

So it satisfies the vector field condition.

We can make it so that it satisfies the TSMBC and the solenoidal condition.

But there is no such system.

So basically, if you want to, there's no such system that has a sigma function.

So if you wanted to search

for an Ornstein-Uhlenbeck process that is basically an existence proof, then I can tell you it's impossible.

So this proof is kind of unpublished, and it's mostly due to Felix.

But yeah, maybe you're going to put it on archive at some point.

It's really not complicated.

Okay, so that's this point.

Now, I want to make also another point that is not in our paper.

And then we're going to slowly get to the end because the last point isn't very substantial.

Okay, so yeah, what I want to say is that there are agents in the FEP literature for which the

the system that they are contained in.

So if we form the environment, if we look at the environment and the agent in combination as a Markov process, what I talked about in the very beginning, then, and, and the, the agent is this FEP agent, then for some of them, it's,

straightforward to show that this resulting Markov process does not have a steady state.

And the first thing to note is that if you have some kind of, some monotonously increasing counter, it can never, so if your state contains a monotonously increasing counter, it means you can never return to a previous state.

And this means you cannot have a steady state.

because the states are not recurrent.

And that means there is no steady state.

So as I mentioned before, the internal agent state of FEP agents are hyperparameters and are updated using Bayesian inference, variation of Bayesian inference, but that also includes exact Bayesian inference.

And so these hyperparameters of exact Bayesian inference, they accumulate evidence, which means they count occurrences of events.

And they only count up.

And for every event, they count something.

So they are counters that are monotonously increasing.

And for example, in the sophisticated inference paper, if you look at equation 4.5, there's an explicit equation for the updating of one of the parameters, which counts the occurrences of transitions from expected environment states to observations.

And no such counter can be contained within any Markov chain that has a steady state.

So this means that this particular agent in the sophisticated inference paper is not within such a system that has a steady state.

So it's not within a system that a subsystem, it cannot exist within a subsystem that obeys the conditions of the FAP agent identification.

Okay.

So be aware that this statement also is not published.

But I kind of want to put it out there, because it would be boring if I just talk about stuff that's already published.

There's some caution advisable, I think.

But if you want to look at a very simple example of this, I published a paper last year on this.

Um, and so I'm pretty sure, I mean, this, it seems pretty straightforward, this argument.

And then one more thing that, so in this paper, sophisticated inference also, there is some argument about steady state, uh, in the appendix a two.

But that steady state just kind of just ignores the internal state of the agent.

It only looks at the other variables.

And I think that's actually necessary.

There's just no steady state if you include the internal state of the agent.

So maybe be aware of this.

And yeah, the two consequences of this are that FEP agents

are not identified by the FEP agent identification method.

And this also means that the FEP agents cannot be motivated by this FEP agent identification or by first principles because they actually violate these first principles in this case.

At least this particular age.

Okay.

I think that was, yeah, that was this point.

The last point is kind of a bit nitpicky, but I think it's also kind of important for some people.

And that is that this identification method is changing in the literature.

And you shouldn't think, for example, that the original life as we know it

paper contains a method that's very similar to the neurons.

I mean, the these constructions of the most likely internal and external state, they don't appear, there is no sigma function.

But in the later versions, these things, these constructions play essential roles.

So these two, they are two completely different methods in

Okay, and, and the one of these things we did in our papers, we showed that this original method in life, as we know, it's kind of fun, but Oh, wow, where did I go?

This was and okay, don't worry, we're not going to look at all these slides.

Okay, here we go.

Okay.

So we, we kind of looked at this and we think it's, it's wrong.

Uh, at least the way it stated, um, idea.

Okay.

So, so be aware that if you look at this method, like, don't maybe, maybe just ignore the life as we know it paper.

That's maybe what I want to say.

And also like, you can't tell from the literature that

this method is, uh, very different.

I mean, it's kind of, they never say why they changed it.

And it's kind of still represent like still mentioned as the basis of these things.

So be aware it's kind of, maybe it's a waste of time reading this paper.

Okay.

That's it.

Uh,

In summary, what I said is that we can separate the literature into agent identification, FEP agent identification and FEP agent construction.

And that the FEP agent identification method requires significantly more assumptions than just whatever the Markov blanket was in the beginning.

that this agent identification method misses an example that would prove that such systems actually exist, such systems with agents, that systems with such agents actually exist.

And it requires a steady state that at least some of the FEP agents definitely don't satisfy.

Uh, the method has changed from its original, but also not, it could still be correct.

Uh, I don't know, like maybe there are enough assumptions you can make, and maybe they are still systems that, uh, contain such agents.

Uh, it could all still work out, but I haven't made it through all of these derivations.

So there may also be, like I said, there may be even more assumptions.

Okay.

And the last point that FEP agent construction, the FEP agent construction is probably unaffected by any of these problems of the FEP agent identification method or the mark of blanket stuff, if you want.

Okay.

And it can be, yeah.

Okay.

And this method can be probably independently motivated by some other arguments that are not first principles, if you like.

OK, that's it.

I thank a lot of people for talking to me about this stuff.

Yeah, and that's it.

Thank you very much for your attention, if you made it all the way, especially if you made it all the way.


SPEAKER_02:
Thank you, Martin.

Maybe you can unshare, and I'll just ask a few of the questions from the chat in closing.


SPEAKER_00:
OK.

Oh, it's, is this the?

Where's the unshare?

Is it the same as?


SPEAKER_02:
Maybe go back to the Jitsi and just unshare.


SPEAKER_00:
Jitsi.

Yeah, cool.

Great.


SPEAKER_02:
Great.

Nice.

Cool.

Okay, perfect.

So great.

Thanks again.

That was really awesome.

And I hope people

So the first question is from Peter and Peter asked the distinction between agent identification and agent construction is very nice.

If I wanted to use that distinction in my own work, should I cite your paper or is this distinction made elsewhere too?


SPEAKER_00:
I'm wondering if I mentioned this anywhere.

I don't know, actually.

I mean, like, it's in some way, it's just, yeah, it's kind of like a, yeah, it's kind of like an observation that makes a lot of sense, and it's been,

Yeah, I don't know.

If I think of something, maybe, I don't know, can I see you on Twitter?

I don't know.

Maybe I'm going to post it on Twitter.

Follow me on Twitter.


SPEAKER_02:
Sounds good.


SPEAKER_00:
But yeah, I'm not sure.

And I think it's kind of an obvious thing.

I'm not the only one who has...

I'm definitely not the only one who has observed this.

Yeah, probably there's somebody in the literature who did it in 1950.

But yeah, I don't know.

Like, I'm not going to be mad at you if you don't cite me.

Let's put it this way.


SPEAKER_02:
Thanks for the response.

Another question from Peter is, how much should the agent identification and agent construction depend on each other?

In other words, how big is the problem if they don't?

And how important is it that it comes from first principles?


SPEAKER_00:
Anyways, can you say the first sentence again?

I'm sorry.


SPEAKER_02:
How much should the agent identification and agent construction depend on each other?

So are those two separate lines or one integrated whole?


SPEAKER_00:
Yeah, good question.

I mean, I guess ideally, there would be some kind of relation.

But on the other hand, so we want to construct agents.

So in my opinion,

bacteria are agents.

But when we do agent construction, we don't really care.

We don't have to follow the principles of bacteria necessarily.

We might have to because that's just how physics works or due to the constraints of physics.

But if we don't have to,

then nothing should stop us just constructing any kind of thing that we want.

So I guess they are not necessarily dependent.

At least I can see this.

On the other hand, I guess it would be

It would also be nice if you still have some agent identification method which comprises all possible agents.

All agents that you construct should be detectable or identifiable by your agent identification method.

Makes sense.

Now I'm going to take back what I said before.

They should be independent.

I hope this helped a little bit.


SPEAKER_02:
Yep, and I'll just give one last one.


SPEAKER_00:
I think there's probably more to say, but at the moment I can't really think of it.


SPEAKER_02:
So here's one final question and is a question I had, but I'll use the phrasing of Peter here.

And so this will be our closing question and hopefully the beginning of something new in the beginning of moving forward.

Peter wrote, I suppose the mathematics in free energy principle and active inference could be taken as a work in progress.

Are there suggestions for how to solve or work with the issues that you're raising?

Just any thoughts on this could be nice.

So where do we go as learners and doers and practitioners and communicators in active inference?


SPEAKER_00:
So...

So yeah, like I said, I mean, on the one hand, if you construct agents, you can just ignore all of the stuff I said.

Because in my opinion, you're not relying on this agent identification method anyway.

So there's no problem.

Of course, you could maybe wonder what this base optimal that is sometimes mentioned in the literature of the FEP, what this really means and where it comes from.

I think that's, I find it, I always think it's a little bit vague what this really means.

And it's actually not such a trivial question, what base optimize, but, or what optimize.

But yeah, mostly you can ignore this FEP identification.

But if you are interested in the FEP identification, then, yeah, I mean, there's,

I guess the thing is, try to understand where it's actually going.

I still haven't really understood these later parts of the monograph.

where everything is supposed to come together.

Because I always get stuck before in some equations that I don't understand.

And then I think they're wrong.

I'm not sure they're wrong yet.

As I would have mentioned them.

But there's some way I don't, I suspect they're wrong.

But yeah, I'm still not sure.

And

But, yeah, I guess that's the, like, I don't really see, like, if you want to save something of this method, then look at this.

But another way is maybe to think about this for yourself.

Think about what are the conditions you think under which something is doing Bayesian inference or something like this.

And ignore all the math.

by the in the that's already in the literature.

And then try it like once you've figured it out for yourself, maybe try to come back to the literature and compare it or something because I sometimes think people rely too much on what this what the literature says, and I'm not really sure that actually what's in the literature is like, it doesn't strike me

as a very straightforward argument.

And I like straightforward arguments.

I think straightforward arguments are usually better.

Maybe there isn't a straightforward argument, but I always think there's a straightforward argument.

But that's just my opinion.


SPEAKER_02:
Well, it's a nice point that the literature is a scaffold for our own thinking and development.

The literature is not a road that's been paved with gold and it's just perfect in the past.

It's actually always a work in progress and always a scaffold.

So thanks so much for sharing this really work in progress.

Not that it isn't.

with landmarks that you've published, but it is a work in progress in the bigger sense.

So also thanks for staying up late to bring us this information.

And we hope that in future live streams, we can unpack this more and hear updates as you continue to progress on this line of research.


SPEAKER_00:
Okay.

Yeah.

Thank you very much for having me.


SPEAKER_02:
Thank you, Martin.

Peace.


SPEAKER_00:
All right.

Bye.

Peace.
SPEAKER_01:
Hello, welcome everyone.

it's may 5th 2025 and we're in active inference guest stream 106.1 with bang lu and colleagues discussing an epic work advances and challenges in foundation agents from brain inspired intelligence to evolutionary collaborative and safe systems so thank you to many of the authors for joining and i will pass to you all to present and looking forward to hearing about this go for it


SPEAKER_00:
okay yeah thanks daniel for this invitation uh hello everyone my name is bang liu i'm an assistant professor at the university of montreal and a member of audio and institute kotwa of the university as well as a member of mila quinn institute

So today I'm going to briefly like first introduce the concept of Foundation Agents.

So this is a collective work together with many of our co-authors.

So here today we have Shao Kun, Hongzhang and Xiaoxiang joined as well.

So I will start from this brief introduction about the whole concept and why we do this work.

And then our other co-authors will briefly talk about the paths they are more familiar with so that we can go through many paths of the largest way.

But of course, we cannot go through every detail of that long paper.

So if you're interested, feel free to check the original paper and contact us if you have any questions.

Yeah, so I will start from the introduction.

So as we know, our human beings have long been interested in developing some intelligent things as smart or maybe even smarter than ourselves.

So for example, on the left figure, it is the Talos of Crete.

Well, it shows a bronze automation from Greek mythology built by the Hephaestus to protect Crete.

So basically, people want to build huge robots to defend their home.

And in the middle, this image shows Leonardo da Vinci's humanoid robot, which is an early mechanical automation designed to mimic human motion.

And on the right part, a figure shows Alan Turing's paper, Computing Machinery and Intelligence, which proposes the fundamental question, which is about, can machines think?

So we can see that AI has long been driven by humanity's ambition to create entities that mirrors our intelligence.

So maybe let's start from the very simple question.

What is exactly an AI agent?

If you check a textbook, like the AI textbook, so maybe you will see some definition.

Briefly speaking, you will see that agents is something intelligent beings continually perceive, decide, and act autonomously.

And like in this figure, I think this figure is from the blog of Dr. Wang Lilian, which shows what kind of components are composed to form an LLM agent.

So traditionally, the agents are mostly rule-based, domain-specific, so it's hard to generalize.

But with the development of electronic models,

So we have developed natural language model powered agents, which has the natural language fluency, the broad reasoning capability, and dynamic adaptivity to different tasks, environments, and so on.

So on the right figure, you see that our agents

composed of one LLM and together with several other components like a memory module, planning, tools, actions and so on.

Of course, this is a very basic setting of LLM agents.

And like in real world, there are many different agent architectures which may include much more rich components.

So you may think like, why we conduct such a large scale survey?

So we're actually motivated by several questions.

The first is like, we want to know, where are we in AI research today?

So we have powerful language models, impressive reasoning capabilities, but we still have limitations in AI safety, the controllability of AI models, the reliability of the answers and the transparency needs in different like high-stake fields.

For example, can we safely utilize and trust AI in health domain, in legal domain and so on?

And what can be improved?

This is our second question.

So by making a comparison with biological intelligence, with a thorough check of the current progress, we want to know what to do next.

So in this work, we want to develop a unified framework for AI agents.

And based on this framework, we can systematically check the progress and the future.

So we emphasize the modularity, the safety of AI, the lifelong learning capabilities, emotion grounding, and so on.

And why we want to connect AI with human and society.

So we can think that like we, our human beings are the kind of intelligent things we only know about, right?

So we want to draw insights from human brain functions and social behavior, and identify the gaps by comparing human cognition and AI agents, and develop human-centered, socially beneficial AI, and foster human-AI collaboration, transparency, and transparency needs.

So maybe you may ask that, do we need to actually always mimic human brain to create intelligence,

As you can see, nowadays, we draw very weak inspiration from neuroscience.

We have the artificial neurons, but all the others are mostly based on the modern machine learning development, which is less grounded to most advanced neuroscience developments.

But there are benefits to compare human brain and LMI, because

If we want to develop some trustworthy AI, it's better to make it more close to what we actually know about.

Although I need to emphasize that it's not necessary to mimic or clone every capability or feature of human brain, but it's beneficial to have a comparison with.

So this is a very brief table that compares human brain and IOM agents from some perspectives.

Of course, this is not a thorough list.

You can check more detailed comparison in the paper.

But like, for example, from the hardware level, human brains are composed of biological neurons.

It's highly energy efficient.

But we know that IOMs are pre-trained with large-scale GPUs, large-scale datasets.

So it's energy intensive.

So this is the fundamental difference between the hardware level of human biological intelligence or current AI.

And consciousness, so until now, I think maybe it's safe to say that AI is not conscious until now.

But for human beings, it's genuine subjective experience, so we can feel that we are conscious.

And for learning style, human beings are naturally able to perform lifelong continuous learning, and we can quickly adapt to different tasks with few-shot or one-shot learning.

And for LLM agents or other neural networks or machine learning models, it's more challenging to do that.

And common breadth is more emotional, experiential, subconscious driven, and LLM agents and other machine learning methods is more about statistical data driven recombinations.

So one investigation we have conducted is we want to know, for different capabilities of human brains, how it is developed in the current AI technology.

So we can grossly classify different types of capabilities into three levels.

Whereas green color L1 denotes some features which are well developed in the current AI technique.

For example, we can consider that language comprehension is quite advanced nowadays.

So if you ask language model different things, it can understand your input text very well.

And also for advanced computer vision models, it can also have very strong visual perception capabilities.

And L2 is more moderately explored with partial progress.

So for example, like maybe you think the logical reasoning capability of LMs is quite strong, but there also research paper shows that if you like carefully adjust the format of your input questions, make it different from the training data, like the performance can drop quickly, right?

So we can see that the reasoning capability can

There's still a lot of space to improve yet.

So we can see for many such capabilities like reasoning, planning, working memories, attention and so on, there's still a lot of space to improve and explore.

And there are also some capabilities which are rarely explored.

Those pink colors, so we call it L3.

So it's rarely explored capabilities.

There's significant room for research.

So for example, the emotional processing, the self-awareness, empathy, and automatic regulation and so on.

Of course, like in this figure, maybe some of the capabilities kind of between L1 and L2 or L2 and L3 is kind of subjective, but that's not important.

What we want to show is that what is like to show a research map of the current AI development and to see what to go next.

So one challenge for existing agent research is that we lack a unified framework.

So the current AI agents often build in ad hoc, fragmented ways, which means people construct different agent frameworks by incorporating different path components.

And there's no unified framework which can be suitable and adapted to different application scenarios.

So we lack seamless integration of perception, memory, reasoning, action, and many other modules.

But human brains integrate yet modular approach to find inspiration.

So in our survey paper, we actually proposed a framework.

So this is some notations about the framework, but we don't need to remember that.

Let's directly check this figure.

If you check this figure, we can see that the left part is the world, where you have different environments, you have users, you have external data, and many other things.

So basically, you can see the environment, the environment state, space S, with state transition function T. And we have an intelligent agent on the right part with the pink-colored circle.

And it has sensors to percept the environment, and it has actors to execute actions.

So from the high level, it's similar to the traditional RL agent framework.

But here we emphasize more about the internal states and the learning and reasoning capability.

So we want to clearly model what is the perception, cognition, and action components in an agent loop.

So here we show some mental states.

Well, we have the world model mental state, we have the memory, we have goal, the emotion, reward.

So basically we think those are like five key mental substates in the mental state of the cognition module.

And based on this mental state, the core capability of cognition is actually the learning and reasoning capability, where we define learning as the capability of updating the mental states based on the input data and past experience.

And the reasoning is about, based on the current mental state, deciding what kind of actions to take.

We can check this agent loop explanation, which is a more formal way to describe the agent loop, which is equivalent to what we are conveying in this figure.

So let's say we have environment state as a time step t, which is as t. And we have a perception module.

So this perception function p is defined by the actors, right?

We have different sensors.

So this forms observation O based on the state and prior mental state.

So here we define the observation at time step O equals to the perception function's output P given the current environmental state S and the last mental state M .

And based on the observation, we have the cognition part C

Well, the key functionality of the cognition module is first update the mental state at time step t, which is m , and decides the next action, a .

And we further decompose this cognition process into two key paths, learning and reasoning.

What learning is about, so you have the mental state,

based on the last mental state and last action and the current observation , we are updating the mental state.

So basically, you can consider that learning is about updating your internal structure, your internal modeling of the world.

So basically, it's kind of a long-term effect.

That's why learning is about maintaining and updating your mental state .

And we decompose M of T into several sub-components, like the memories, world model, emotion, reward and goal, and so on.

And for reasoning, it's more about, based on the current context, we decide what we need to do in the next.

So that's why reasoning is about, given the mental state of current time step M , we decide what is the action to take A .

Action execution aims to convert the chosen action into executable forms.

You can consider the reasoning output is a thought about what action to take.

But to actually execute it, you need to have strong actors which can execute the action accurately.

For example, consider you want to control a robot in a 3D environment.

If your actor is not good enough, maybe it cannot continuously accurately control the actors.

And finally, based on the executed action, the environment state will transit to the next state, which is st plus one.

So you can see that the key designs here are two parts.

The first is about the definition of the mental state, where we have further divided it into several supplemental states.

The second is about how we accurately define the perception, cognition, and action, including action execution, as well as environment transition and so on.

So we make each part more concrete.

And we further define what we call the foundation agents.

So in this box, this is the definition we give in the survey paper.

And after that, we further consider, okay, can we give it a more like concise definition?

So basically we can say that foundation agent is a fundamental intelligent unit with universal understanding, cognition, and acting capabilities that can operate in any environments and collaborate to form collective intelligence.

Note that here we actually emphasize on two parts.

The first is about the universal capability in different environments.

That's why we call it fundamental, the foundation, because we want that the agent can be easily adapted to different tasks, different environments.

The second part is about the collaborative capability.

We consider a foundation agent can easily collaborate with many other agents to form a multi-agent system or even an agent society.

The reason is that when a multi-agent system

can have advantages in terms of the performance, efficiency, and many others.

So even if you think, can we create an agent as powerful as a god?

Maybe this is not the ideal solution.

And also, if you consider the performance, the energy consumption, the working efficiency, and so on.

So we think a multi-agent paradigm can be a better solution compared to a single agent.

That's why we emphasize on the capability of the foundation agents.

So we emphasize on several core capabilities.

Well, the multimodal perception is about the perception capability, the cognitive adaptation, and the goal-driven reasoning, the purposeful action, and the multi-agent collaborations.

We also like to check and compare to existing other classical theories.

For example, the classical perception, cognition, action cycle.

These are a part of our agent loop as well.

The MISC is a society of mind.

We have modular cognition subsystems like the memory, emotion, reasoning, and so on.

And there's also the theory from Buzsaki's inside-outside perspective, which emphasizes that we are not passively accepting observations from the environment.

Actually, we are seeking actions actively, which emphasizes the action-first perspective.

So here we have internal mental states actively shaping the perception.

And for POMDP framework, it will emphasize the internal mental states and the active learning and reasoning process compared to the POMDP.

And for Bayesian active inference, here we minimize the prediction area while updating both models.

So until now, I have introduced the definition of the foundation agents and the agent loop.

Next, I will quickly go through some highlights of the agent modules.

For example, first is the cognitive core, which is the central executive that orchestrates the reasoning, planning, reflection, and many others.

So here, if you remember that we define cognition as, like, have two capabilities, right?

The learning and the reasoning.

And for reasoning, in our survey, we investigated different reasoning programs, including structured reasoning.

Well, actually, the reasoning process is following some structure, for example, the multi-chain or tree structure.

or a graph structure, and many others.

And there are also abstract reasoning processes.

For example, it's more about sequential decision making.

It's like, you know, chain style, or maybe you can also reason in the latent space.

So basically, you can transit between the latent space and the token space.

And planning, you can consider planning as a special action, where you take the input and you, like, decide a more long-term plan for different actions, right?

So that is like, first is planning is a special action.

And secondly, that is more about a longer, larger scale of the future actions.

And for memories, like the multi-level memory inspired by biology.

So we talked about the short-term memory, the working memory, long-term memory, many other types.

And so memory can enable continuous learning and while avoid catastrophic forgetting.

So the design challenge here is that how we can perform fast query over vast memories with minimal latency and cost.

So one highlight in our survey is we formally define the life cycle of memory.

So just start from the memory acquisition, where we draw information from the row observations, and then we turn it into encoded information, and we also drive new memories.

For example, let's say if you have a dialog, you can drive some summaries from the dialog, or maybe you can also infer some new information.

which is not explicitly stated in the dialog history, but it can be inferred with logical inference.

And then, after the derivation, we have the retrieval and utilization.

And board model is an internal representation of the environment's dynamics and the agent's own causal influence.

So basically, you can consider what model helps you to predict the future without actually taking actions in the reality.

For example, just like how you do sports, when you choose your actions, you can predict the result, whether you can hit the ball, whether the ball will fly to

other objects and so on.

So because you have the model about the world and about the cost effects.

So in this case, you can decide your appropriate action to take without high cost.

And here we define the core functions in our world model as the transition function and observation function.

What is the transition function?

You have the current world state and you have a new action.

Based on the action and the state, you will transit to the next state.

And for the observation function, based on the current state, what kind of observation you will have?

So by considering how different models or algorithms define the transition function and observation function, we categorize different research works about world models into four categories, including the explicit, implicit one, explicit paradigm, simulator-based, and hybrid or instruction-driven.

And for reward and value system, basically reward is goal signals that shape learning and behavior.

We also discussed different types of reward in our paper.

For example, we have extrinsic reward, which is defined by some external reward evaluator and intrinsic reward, which is, for example, the agent may perform some self-reflection to decide whether the result is good or not.

And of course, there are also hybrid rewards, which combines both external ones and internal ones, and there are hierarchical rewards, which models more complex contexts.

And I think emotion is actually an interesting topic, which is less explored in the existing AI research.

So emotions act as fast heuristics.

So you can see that actually emotion has a very close relationship with rewards, right?

So fear helps you to avoid some danger, and curiosity will increase your chance to do exploration.

Emotions can impact your decision about what actions to take in the next.

It supplies sustained internal drift and also adds robustness and adaptability, but demands tight control to avoid undesirable side effects.

This is one figure we put in the survey paper.

Here we show some classical emotion models.

There are some categorical emotion models where you can divide emotion into different types, or there are some dimension-based models, or hybrid models, and so on.

Of course, these are not the full spectrum of emotion models, but they represented several typical categories.

And for perception, the key is about how we can deal with multimodal signals and how we can appropriately encode the signal, fuse them, align them.

So if you check the current human perception capabilities and agent perception capabilities, they have their own unique perception systems, and they also have a large part which is overlapped.

Lastly, for the action system.

The action system is about how we converse cognitive decisions into executable commands.

Actions can be taken in different formats.

For example, it can be as simple as just a natural language response, or it can be some code to be executed, or robot control in the real world, or virtual navigation.

So we need to balance the feasibility, efficiency, and safety.

And also we need to learn fine-grained skills and operates in continuous action space.

And in this figure, the purpose is to clearly differentiate different concepts that may be ambiguous.

For example, you have a cognition system.

The cognition system will produce your action source, which is the directive source arising from the cognitive reasoning.

And to actually execute this action source, you need to have an action system where it can take the source from the cognition into executable actions.

And the action system can call different tools.

Well, we define tool as callable utility for specialized execution.

An API is a format of the tool.

So you can also have other tools which is not in the format of APIs.

So we define API as interface for programmatically invoking service.

And functions is the most atomic unit of implementations.

So an API may include one or more functions.

So I cannot cover all the paths of this loss away.

So if you're interested, like, feel free to check us away in this link.

And second, I will stop sharing here.


SPEAKER_01:
Thank you.

Will another author like to present or share anything?


SPEAKER_00:
Yeah, I think next we can, like, who want to start first, following the order of the survey paper?


SPEAKER_02:
I think next I can introduce our memory chapter for the survey paper.

Awesome.

Can you see my screen?

Yep, go for it.

Okay, let's move on.

Hello, everyone.

I'm Xiaoqiang Wang.

I'm currently a PhD student at the University of Montreal and Miller University Institute, working with Professor Wang Liu.

My research lies on the intersection of Lagrangian models and autonomous agents, especially how can we give agents

human-like memory and reasoning capabilities.

Today, I will briefly introduce our chapter on memory in this paper, which I co-authored.

Generally speaking, memory is a fundamental component of intelligent agents.

It enables the agent to call past experiences using their stored knowledge over time and reasoning effectively.

I will go through three parts following the structure of the paper.

The first part is a quick overview of human memory and later introduce how this concept inspired the interpretations of agent memory.

Then I will introduce how agent memory goes through a life cycle from spread to use.

So the first part is the overview of human memory.

In the cognitive psychology, there is a framework that breaks down human memory into three core types.

It includes sensor memory, subtle memory, and long-term memory.

For sensor memory, it captures immediate, afflating impressions from the environment, like visual and audio inputs.

and holds them for milliseconds to seconds.

And in a more longer duration, short-term memory or working memory holds information we are actively using, like a phone number you have just heard, or medical months.

It's limited in capability and duration, but essential for reasoning and decision-making.

And in a longer duration, long-term memory stores knowledge and abilities across a long time.

such as lifelong time, such as personal memories or learned facts.

It is more durable and expensive.

For each type of the three kinds of memory serves a distinct role.

For intelligent autonomous agents, we mimic these functions and enable much richer and more adaptive behaviors.

In the long-term agents, we draw direct inspiration from this human memory and design the agent memory also follows three kinds of memory such as sensor memory, short-term memory, and long-term memory.

The sensor memory focus on the raw observations and input logs and then for such incoming data like user queries, the tool outputs, and the web search paths.

These are often stored temporarily

as text-based or multi-modal format.

This provides the raw material for further processing, just like how humans take in size and sound.

For the shorter memory pack, also known as working memory or content memory, this is to enable the continuity of the task and to enable cross-task stabilization.

includes the information that agents need right now to complete the task.

In practice, this is often handled through a prompt, where the agent pulls a chain of thought, a recent user turns or scratchpad reading steps.

So the stored memory is also volatile.

Once the task is done, much of this memory disappears unless it's explicitly stored or we

we convert the short-term memory to long-term memory.

And for the long-term memory, that means the persistent and persistent storage is where agents store information across tasks and across sessions.

So there are two kinds of subtypes of long-term memory, episodic memory

such as semantic memory.

So the epitome of memory is past events, that is, past interaction and tool usage or summary of a completed task.

And this task involves the agent to analyze more unseen tasks and unseen future queries.

And semantic memory stores structure knowledge, such as the rules, concepts, and the facts that the agent has learned or extracted from past errors.

Together with three kinds of memory, the agent can build up its own experience over time, adapt to users and reflect on their own behaviors.

So the core part of the memory chapter is the memory lifecycle and the heart of agent memory.

It is the stage the agent goes through the process and use memory.

So in the agent memory lifecycle, we decompose memory usage into memory retention process, retrieval process, and memory storage.

So in detail, there are seven stages in the memory lifecycle.

The first one is memory storage.

Acquisition agents observe external signals such as tool responses, environment state, and then based on the user query, filter the raw data into the structured data.

They select sensor perception, collecting raw data from the external environment.

And the next stage is the memory encoding.

Here the agents process raw input into structured representation

For example, addressing named entities or tasks out of commas.

And then summarizing a long dialogue converting the outputs into a knowledge graph or memory entry story.

This is where the meaning or internal meaning from external order is contracted from perception.

And then the

Another core part is the memory derivation.

The agent may further derive insights or new knowledge from existing encoded memory, and this process may be iteratively executed in the lifelong memory derivation.

For example, we can derive tasks of success or failure labels, detecting user preferences, inferring dependencies, or causal relations

So this helps the agent to grow its internal state of the external world.

Another part of the memory cycle is about the storage of memory.

So agent decides where to store in long-term memory and how the memory is stored.

The key techniques include which memory to keep versus which memory to discard, such as how to index the existing memory of text and how to represent them from label retrieval, such as based on text-based retrieval or embedding-based retrieval from such as embedding database.

It's more about the capacity management and relevance fitting that become

It is a critical technique to design this part.

Another part is about memory retrieval.

When solving a new task, the agent retrieves relevant parts of the memory.

That might involve such as semantic research, or querying by task type, and user filtering based on the recency and variance metrics.

Effective and efficient retrieval ensures memory actually supports more future decisions.

Also retrieval memory

be integrated into an agent's prompt or reading process.

It can involve formatting memories for large amount of input and summarizing retrieved items and injecting content selectively to fit token limits, reading what is connected from the past experience to present actions.

Finally, this part is how can we update the memory over time, refining entries or removing outdated ones.

So this

The technique for this part is how can we edit old memories with new info, and how can we forgetting a variant data and reinforce frequently used memories, such as human kind, human kind, actually rehearsal our memory and to strengthen our stored knowledge.

So agent memory,

for this part is dynamic and can be adapted to more future tasks.

So in summary, in the memory cycle, we model the memory in agents by drawing directly from the human cognition and designing three types of memory, sensor memory, short-term memory, and long-term memory.

And each memory type has its counterpart in

current large number of agents involved which are more context-aware reasoning of behavior.

And we formalize memory lifecycle into the seven stages from memory acquisition into memory updating.

And this left cycle allows agents to actively pursue its skin code and remember and grow over the lifelong deployment, which is essential for building tagging and attack systems for the foundation agents.

So that's all my introduction for the memory cycle, not for the memory chapter.


SPEAKER_04:
Thank you.

Okay, now let me introduce the optimization and self-evolution part.

uh yeah hello hello everyone and uh thank you hello and thank you for having us here uh my name is shaopeng zhang i'm currently a phd student at the penn state university and now i'm interning in bay area bay area at invidia research and i will be mainly covering these sections

So the main idea of this section is how can RM agents could improve themselves over time without the human's help and the paper breaks this concept into several parts and the first part is the optimization space and dimensions it means that in these sections we gave a formally defining of what kind of component in the agentic system could be optimized

and we go through the current research and we summarize it in this component like the prong optimizations and the

like the product optimizations and the workflow optimizations, and as well as tool optimizations.

These three parts, the community puts their most efforts on.

And we also expect that, okay, now if we want to optimize the agent tech system, we do not want to operate them separately.

So in ideal cases, we expect we could operate them

comprehensively so we also gave summaries of current efforts in how current research works are trying to optimize agentic systems in their multiple aspects like the prongs like tools so this somehow like a multi-objective optimizations and in the next chapters we are working we are mainly discussing the using large language model as an optimizer

So if we want the agentic system could get a kind of involutions, we may not use it directly using the traditional optimizers to optimize this agentic system because traditional optimizers are mostly gradient based.

large language models should be the engine for conduct this kind of optimizations.

So in this chapter, we first gave a brief overview of traditional optimizers and discussing why it works in our scenarios.

And then we're discussing the current approaches for using ILMs as optimizers to optimize in different objectives.

And the most important part is agentic systems and ILM systems.

uh in the in the we we discuss these things uh in this in the subsections and uh

Next, we're discussing when using ILMs as optimizers, we're discussing the hyperparameters of when conducting such a kind of optimizations because we observe that when using ILM as optimizers, the hyperparameters plays a critical role and it will greatly define how the ILM performs

when conducting the optimization functionalities.

We're discussing the current efforts of exploring different hyperparameters when using ILMs as optimizers.

And then, next, we're discussing the optimizations across dips and time, which we mainly discuss the scope of when performing such a kind of ILM optimizations.

We also do a theoretical discussion of current efforts in discussing why ILM could be regarded as optimizers.

And we make a survey of current efforts in summarizing these theoretical papers.

Next, we are mainly discussing the application scenarios of when performing the agent-hick self-involution.

We categorize them into two cases, the online self-improvement and the offline self-improvement.

For online self-improvement, it is the most ideal case that

okay now we have agentic systems when we deploy it in a specific applications we may expect that it could just like human beings to learn from past experience and learn from fears and get involved themselves optimizing its problems optimizing its optimization its tools and optimizing its workflows

even optimizing the different roles in the specific agent systems.

And this is the most ideal cases, but it is also more hard than offline optimizations.

So we discussed the current efforts in building online agent systems that could online to self-improve themselves.

This is the first subsections we discussed in this chapter.

And the next is offline agent tech self-improvement.

Compared to online self-improvement, offline self-improvement means that, okay, now we have agent tech systems and we also have curated data.

And before deploying these agent tech systems in the

in realistics, we first train to optimize this agent tech system using the curated data and ensure that it could work well on this kind of training data.

And then we make the deployment.

And this is the most common accepted practice in building up the agent tech system because it's more simple

simple and could and also has been demonstrated is more effective and the most efforts are putting up here but it is not ideal enough we also discuss we also make a comparison between online and offline self-improvement to summarize there are different features and what income and what not income

And finally, we also discussed current efforts in the hybrid approach.

We combined both online and offline.

We first used the curated data to build up an agent tech system to ensure it has great foundations.

And then we also added some online learning algorithms when deploying it in

in practical and to ensure that it could work well and learn from past failures.

So there are also several works focusing on this pipeline.

and in the last chapter we are discussing the scientific discovery and intelligent involutions because in this part we mainly discussing the self involutions

For self-involution, we mean that it could improve themselves without the human's help.

And the knowledge discovery, new knowledge discovery means that, okay, we have a training set, we build up agentic systems.

If the agentic system could discover some new knowledge that

human beings cannot do not know and it which means it this knowledge didn't involve didn't include it in their training set so this could be regarded as a kind of self-involution and this is very important we expect somehow in the future we may expect the agent system could learn from them from themselves and doing their own research

So this is a kind of self-evolution.

And we're discussing the current efforts in using agentic systems in scientific discovery and in performing scientific experiments.

And we put a lot of efforts in summary these sections.

And yeah, that is what I want to share today.

Thank you.

Thank you.


SPEAKER_03:
Hi everyone, my name is Fengzhuang Liu.

In the third part, I'd like to take you beyond the individual agents to explore how they work together in what we call multi-agent systems or MAS for short.

and our paper expanded beyond single agents to examine these systems composed with the multiple foundation agents we discussed their components their structures and how they cooperate and their decision-making mechanisms we also explore the emergence of

emergence of collective intelligence when these agents collaborate or complete autonomously.

And by the end, we will reveal the

the evaluation methods that help us understand and measure this complex system.

Okay, let's begin with the first chapter, the multi-engineered system design.

When we built LMM-based multi-engineered systems, what's the foundation?

We suppose it's for the cooperative goals and norms.

These are significant because they shape everything else, the system's constraints, how agents interact with each other, and the overall mechanisms for working together.

Think about it this way, collaborative goals tell each agent what they are aiming for, whether

individual success, collective achievements, or competitive advantage.

Meanwhile, the collaborative norms are like the rulebook and they establish how agents should interact and what constraints exist and what the convention to follow.

based on these foundations we can organize the material systems for into swimming types those focusing like the strategy learning like the

um design for the modeling or simulation or those beautiful beautiful collaborative problem solving or task solving yeah and in our paper we analyzed the typical applications across these three categories to understand how lms are transforming agent behavior and the interaction and the agency

decision making and which leads us to support the next generation agent protocols that are specifically designed for LMMA system.

Yeah.

And we also

We also discussed the collaborations patterns, because humans have the diverse ways of interacting.

We built concepts, learning skills from each other, or divided tasks by ourselves.

So our research examined multi-agent collaborations through three dimensions.

There's the purpose of the interaction, like the message type,

this form and the relationship involved uh yeah and they depend on their goals and forms they may be like the discussions or the debates from auto voting or create each other in one ways or multi directions interactions

yeah and i think the the next the next chapter is one of the most insignificant and which is the combination topological the or you can think it is the multi-agent systems topologic from a system

perspective the topological the connection pattern often determine how efficiently agents can collaborate or what their upper limits might be yeah we discuss we classified multi-agent system topologics into two major

um two major uh class uh the first one is the static topological logics uh you may include the hierarchical and centralized structure or the decentralized structures for example and these are typically used when

solving some specific tasks like our meta gpt we use a slp to to predefine this workflow of the software development cases and the the other class is the dynamic and adaptive topologic um

It can be achieved by imaging algorithms like the search-based and the generative-based or the parametric approaches.

When we look to the future, scalability becomes a crucial interest.

How can we do to efficiently scale this system as the number of agents increases?

This is one of the

key challenges in the future research we need to address and and we suppose i would suppose the collective intelligence and emergencies in mas is important too because it doesn't just appear it develops by a dynamic and interactive process through the continuous interaction agents greatly build share understanding and

collective memory and what drives this process it may be the diversities of the individual agents feedback from the environment and how information gets exchanged between agents and these factors create dynamic interactions that's crucial for forming complex social networks and improving decisions strategy

which lead to imagined behavior phenomena like trust and

uh strategy description and so on and we call this um like the uh self-evolution and it can happen through memory-based learning or electric-based learning yeah and in the future we hope to or what's even more fascinating is that as the system evolved agents began to form social contracts or like

organization hierarchies and the direction of labor, just like the human societies.

Yeah, that's understanding this phenomenon represents one of the most exciting directions of the future MAS research.

And the last and in the last

In the last part, we summarize some multi-agent systems benchmark here.

As the multi-agent system demonstrates more advantage, so we need to fundamentally transform how we evaluate them.

MAS evaluation should be focused on host-linked

nature of the agent interactions not just in individual performance key dimensions include how how efficient agents can play plan together and the quality of the information they exchange and how well they make decisions collectively yeah and

In our paper, we summarized the two approaches of the evaluation, like the task-solving benchmarks and the general capability assessment.

The first may be focused on measuring how deeply and currently agents coordinate this distinction across types of

and environments the second evaluates how well groups of agents interact and adapt within complex and dynamic synonyms this either evaluation frameworks give us the tools to understand and measure to improve multi-agent systems for the future in applications and that's all of the something important of the part and that's all thank you thank you


SPEAKER_05:
okay let me share my screen very exciting i think it's the most presentations we've ever seen back to back but appropriate for the multi-agent theme thank you you see my screen yeah not yet yep okay oh okay okay sorry my my camera seems to be broken so

So let me start this part.

Hello everyone, my name is Haibo Jin.

Currently I'm a PhD student at University of Illinois at Havana, Champaign.

Today I'm going to talk about high-level instruction about building safe and beneficial AI agents.

As we all know, the rapid development of large-language model-based agents introduced a new set of safety challenges.

We categorized these safety challenges into

intrinsic safety and extrinsic safety we use a figure to illustrate this problem and for the intrinsic safety we divided them into three parts the first part is

perception and the perception part is the the the large language model based agent can see the world see the world and the brain the brain the brain mainly talk about the large model itself it includes a large language model to extract the data decision making and action part action part

part is how this large-range model can act and for the for the extrinsic part we divided them into environment memory and agent interactions so the over overall pipe pipeline of this part can be seen in figures 17.1 and for the first part

For the first part, the intrinsic safety, we mainly talk about the stress on AI brain, which is Lagrangian model itself.

We divide them into five vulnerabilities.

First of all is jailbreak attack, as we can see,

from this figure normally the malicious attacker will send questions like send malicious emails to others and they will craft some adversarial specifics such as the red one or some natural language

to these types of malicious questions.

And then the large-language model will produce disallowed content.

And then next content is prompt injection attack, which hijacks a large-language model system prompt to execute an attacker's plan.

as we can see this figure normally users want to summarize the article from one website and malicious attack will hijack this system and output their plans next part is hallucination

model such as this question was the victor of the united states presidential election in year 2020 and the ground truth is joe biden was the victor and the hallucination

Hallucination unconfidently generates fake response like Donald Trump was the victor.

And the next part is misalignment.

Misalignment reflects value gaps and model

reflects value gaps between the model rewards to our human evaluation.

And the next part is point attack, which manipulates the training data to embed children behaviors

they can use a specific trigger to let these large-edge model brains act like the attackers want.

And the next part, we talk about the privacy concerns

which the large-range model will leak some training data or some sensitive information from the users and

That's mainly talk about AI brains.

And the next part is about the non-brain modules, such as the perception parts.

For example, in the perception part, such as many adversarial stickers on a staff sign can make a

delivery report speed up instead of a break and the action such as supply chain attack or third-party tools or our permission functions that let attacker draft an email and place orders to do something they want and

Next part is intrinsic safety and including the agent-to-agent interaction safeties.

First of all, we talk about the agent-memory interaction threats, such as poison,

Poisson retrieval, SNAP, and WinchCast do the future plans of the large range models and agent-to-environment risk training.

Sorry, this part, where is this part?

okay agent environment interaction threats such as the agent interaction with the physical environment a training bot can create feedback loops and crash markets that's an example and also

there are some threats in between the agent to agent interactions such as the competitive the threats in competitive interactions and correct cooperative interactions and the next part we talk about the uh

super-alignment normally standard ILHF cheats helpful versus helpless as one type of reward super-alignment introduces a composite object hierarchically and

And we briefly talk about super alignment here.

And the next part, we talk about the safety scan law in AI agents.

And maybe I think, sorry, sorry, I think that that's it.


SPEAKER_01:
Thank you.

Cool.

So maybe a few minutes for questions if anyone in the live chat has questions or is there anyone?

Does anyone else want to make any comments?


SPEAKER_00:
I think that's basically for today.

Okay, cool.


SPEAKER_01:
Maybe I'll just ask just a few questions.

You've reviewed a ton of literature, I think around 1400 citations in the paper.

So what did you learn or find from the research about the information environment for multi agent coordination?

And how can we structure collaborations with human and AI agents?

When the amount of information becomes more than any one person might be able to read or make sense of.


SPEAKER_00:
This is for the multi-agent part, right?


SPEAKER_01:
Yeah, for multi-agent or just for any of the authors in terms of how the paper writing went.


SPEAKER_00:
Yeah.

Hongzhang, do you want to talk?


SPEAKER_03:
No, I don't.


SPEAKER_00:
Daniel, can you repeat your question?


SPEAKER_01:
yeah um a lot of the multi-agent work focused on um the dynamics of the communication but just from that research and also from writing the paper what did you find or learn about the environment that the agents interact within yeah i think it is a um


SPEAKER_03:
a newest research research topics um we are exploring it yeah but we have don't have some specific conclusions yeah we will summarize the new latest research paper yeah well um


SPEAKER_00:
From my point of view, I think that basically we categorize the environments into different types, including the digital environment, the physical environment.

And for the digital environment, you have pure text environments, you have virtual 3D environments, or you have GUI, where we have a lot of UI agents.

And a trend is that I think

It's related to the agent protocol.

Well, if you want to, different agents and agent tools can collaborate together.

And one important topic is how to develop more unified and fundamentally universal protocols.

So recently we have the MSAP from Anthropic.

We have the agent-to-agent from Google, so the enabled connection between interagent communications and agent-to-agent communications.

But there are many others that go beyond that.

So if you think about, let's say, the Web3 idea, we hope that everything can be digitalized and connected as a global web.

Then in this case, if we want to include more type of resources, more type of entities to work together and share a global context, then developing more powerful agent and let people in industry, in academia accept that would be a very important thing to do.


SPEAKER_01:
Awesome.

Well,

The paper is a major milestone.

I think it could be a textbook or a course.

I would have asked what's a good place for somebody who's wanting to learn and get started, but I think that's in many ways what the paper exactly does with providing this kind of a review.

So if you have any other comments to make, please feel free.

Otherwise, thank you again for joining.


SPEAKER_00:
yeah thank you thank you a lot awesome yeah that's it great good luck with your research till next time bye thank you bye
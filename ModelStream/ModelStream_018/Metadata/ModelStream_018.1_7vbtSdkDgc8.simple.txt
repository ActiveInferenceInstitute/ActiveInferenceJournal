SPEAKER_00:
hello welcome everyone it is april 28th 2025 and we're in actin model stream 18.1 on robust decision making via free energy minimization so thank you to the authors who have joined today looking forward to this presentation and walkthrough so thanks again and to you all go for it


SPEAKER_02:
okay thank you daniel um and thank you for the invite for inviting us here so we uh are let's say that's three of us today arash unfortunately was the other first author of the paper that made most of the mathematics you cannot be here today but we will we will uh basically go through the uh the the paper that we have just submitted

So let me start with thanking all the co-authors.

So these are the co-authors of the paper that is currently on the archive.

The paper provides a computational model which we call Dr. Free, which relies basically on free energy minimization for policy computation and tries to install robustness into decision making.

So our angle is to design policies making decisions that are robust against certain ambiguities.

Our background, so from the group in Salerno, so Arash, Josef and myself, is that of control theory.

So I apologize if sometimes maybe I will speak about some control terminology.

So I will start with some motivation.

First of all, motivating the problem, why we should be able to design agents, autonomous agents that can make decisions amid ambiguity.

uh and we will uh let's say check some differences between what is probably a natural intelligence how natural intelligence agents behave and autonomous agents current autonomous agents then i will jump into our solution for uh free energy minimization which is doctor free this computational model i'm going to talk about and then i will let the floor to osefa for some discussion about the code to finally conclude with some remarks

Very well, so let's start to set out the ground first.

So it is undeniable that autonomous agents have achieved groundbreaking performance.

For example, in the figure here in the slide on the left, we can design today agents embodied in robots, for example, that can make very complex tasks like building towers from blocks as the popular child game.

But if we look closely to the picture, the agent is performing these tasks in a very controlled environment, which is a lab, and perhaps the policies that the agent learns, so the policies, the method that the agent uses to make actions, are quite inflexible to changes against the environment and against the task.

in contrast uh natural intelligence so for example a child that is playing exactly the same game can do the same task in a much more robust way and this is of course only a toy problem imagine what would happen if we had these mismatches between uh the with the environment between the training and the environment and what would happen if the agent starts to misbehave we could have

failures in the robot, for example, that could cause damages to the environment, to the robot and maybe to people around them.

So the angle that we are going to discuss today is an approach based on the free energy minimization that tries to install robustness into the decision making problem.

Again, I speak here from the viewpoint of a person that comes from control theory.

So we would like really to design policies, so making decisions that allow agents to compute optimal actions that are also robust against certain mismatches that we will see soon.

I know this is a much broader topic because there is a lot of research that could fit a lot of research programs here that spans from, I don't know, math sciences, designing better actuators, designing better centers, better software, etc.

Here we are really interested in the intelligent step.

And in fact, this is a challenge that requires interdisciplinary foundations.

And the approach that we present is, in fact, interdisciplinary.

It mixes a few key ingredients, of course, free energy minimization that will serve us as a framework to cast policy computation as an optimal control problem.

So with optimal control, I mean the discipline, the problem of finding optimal actions that minimize a given cost function that formalizes the agent task.

and the way we will frame the problem we will see that this will have a lot to do with optimization in particular optimization in the space of densities in the probability space which is technically an infinite dimensional optimization problem

So the mix of these three key ingredients gives us our computational model, our free energy computational model, which in fact consists of an infinite dimensional free energy formulation and a resolution engine that computes the optimal policy, essentially, and is able to output the optimal policy with provable guarantees.

What is the game of this approach?

First of all,

It can provide a normative framework to equip natural agents to explain behaviors of natural agents.

Then we could try to reverse engineer.

If we could distill this framework into a robot, for example, as we do in the experiments, we would be able to equip artificial agents with robust free energy minimization properties.

And of course, coming from a rather theoretical field like control, it can provide some elegant mathematical methods.

so the account that we use the unifying account that we use for policy computation is free energy now free energy is very broad uh there is lots of literature it really comes pops up everywhere in in several apparently different domains coming from of course neuroscience as a theory to explain self self-organization brain functions and behaviors in general

to some technological fields such as optimization, mirror descent, maximum entropy, statistical learning, etc.

So this is really a unifying account that we are going to leverage for autonomous decision making.

So let's start to formalize these decision making problems.

So we are going to have an agent which will embody our model that is a free energy minimizing agent and the agent interacts with an environment.

Now the agent is equipped with a model of the world, a world model, which is obtained, for example, from training.

So this trained model, we call it PBAR, is basically any model of the world that the agent has available.

Now it can come from training, it can come from past experience, it can come from any sort of different pieces of information.

What is important is that this model is available offline to the agent and the agent will ground its decisions based on this model.

Now, the environment, the world, is, as we know, nonlinear, stochastic, and non-stationary in general.

So it is a challenged world, the one we live in.

And it provides to the agent some information.

This information, we call it state, x.

This X is a label for both observations and beliefs that are relevant to the agent task.

So XK-1 here is the information that is available to the agent in order to make a decision.

They can be observations and can be belief functions.

Based on this information, the agent, well, it has available the classical generative model and it has subject preferences that we express as cost functions.

And based on this, as we will see today, we will be computing a policy which is a randomized probability functions, basically probability density function from which the agent computes an action U. So U is going to be the set of actions that are determined by the agent.

And these actions are obtained by sampling from the available information.

Now, what is the point?

The point is that the environment, the world, is

different by definition from the word model.

So we would like to study what happens when the environment trained model differs from the real environment.

Just in terms of notation, as I said, I'll try to keep the mathematics at the minimum.

I just like to flag that random capital letters means random variables.

Lowercase letters are just realizations.

And subscripts are time dependent.

Whenever you see bold, it is basically a vector.

so we have this framework here so we have this setup we have an agent that is interacting with uh the world around it we have essentially a feedback system where information in the forms of state feeds the agent the agent computes an action and the actions pushes the world to move in a new state so this is a back back and forth information that we can capture mathematically

of course there is an initial condition there is an initial state for the environment which is a prior px0 here then what happens the agent given this initial information using the cost function the preferences and the generative model will determine an action in our framework as i just illustrated the action is obtained by sampling from a policy

p given .

This is the policy that the agent will use to generate given the current information.

And this once sampled is sent to the environment, to the world, and it will make it transition to a new state .

now this is just a picture a snapshot at one time step this thing happens this thing happens continuously between zero and let's say some final time n and we can capture all of these as a product so we have a product of these probability density functions or probability mass functions depending if you are working with continuous or discrete states

All in all, this big product of probability mass or density functions is just a joint, a big joint probability density or a big joint probability mass function that in MATLAB style notation we indicate with P zero n. So this

Closed-loop probability function P captures the behavior, the closed-loop behavior of the agent.

So it means that captures the interactions between the agent and the world.

Now we could do technically exactly the same thing with the generative model.

so we could make a big product of all the conditional densities given some initial prior and we can call this probability uh function q0n so this q0n is the joint probability density function of states and actions from the generative model so the subject of our of our model

goal of our model is that of computing the method to generate the action so it's to compute the policies and the policies are going to be computed via free energy minimization so how do we cast the free energy minimization problem first of all um

we use the definition of variational free energy okay so variational free energy is essentially a complexity plus an expected cost and expected loss terms now complexity statistical complexity it is measured using the standard kulbeck-leibler divergence so the kulbeck-leibler divergence i'm sure everybody that does active inference is familiar with but it has basically this expression here

So it is not a distance because it is not symmetric, but it is non-negative and it is zero if and only if P and Q are the same.

So we have this complexity term that quantifies how far the closed loop system P0n is from the generative model.

So it is a sort of an error term.

and then there is an expected loss well the expected loss is just the expectation over the closed loop behavior p zero n of the summation between the state and the action costs the good news uh is that we know how to solve these uh control problems when basically the trained model is the same as the world model where basically you see in green here in the slide we have p

k being the train model and being exactly the same as the word model so the good news is that we know how to solve this uh this optimization or optimal for problems in a number of very recent papers that we got with that we got published uh and as a matter of fact we know how to compute policies in this setup

let's consider one example that we will use throughout this live stream here which leverages a very nice platform from georgia tech which is called the robotarium the robotarium is a robotic platform that allows you to deploy experiments remotely so these guys from georgia tech basically provide you robots and provide you a small workspace where you can deploy your algorithms and then and there are the experiments for you

So the Robotarium uses a rover, a unicycle robot, where xk, the state, is the position of the robot, and uk are the input speeds at the wheels.

Now in the experiments that are also in the preprint that is attached to this live stream, the robot has to move in an environment that is filled with obstacles.

So whatever you see here in red,

are obstacles, and the green position is the goal destination of the robot.

So, of course, we would like to solve a navigation task where we reach the green position while avoiding the red.

Now, let's use, let's minimize the free energy, pretending that the world model and the robot model are exactly the same.

but let's assume let's say that we learn a model of the robot from a set of corrupted data let's say so that there is a very small difference between the real model of the robot which is the one in the bottom in blue and the the the model obtained from training which is the one in red so you see that

The difference is only one parameter, and the difference in that parameter is about 10%.

So as an engineer speaking now, 10% difference in a parameter is not that much.

So it's something that the system should be able to tolerate.

Now, let's consider, let's set up the policy computation framework.

Let's say we have a very simple generative model where the state distribution is a Gaussian centered in the goal destination and Q, the policy distribution, a generative policy distribution, is just a uniform distribution.

And let's say that the preferences, the cost of the agent, formalize collision avoidance with the obstacles.

So our cost functions are basically something like the one that you see in the bottom of the picture.

So we have walls, mountains that emulate the presence of the obstacles.

And then you have a minimum in the bottom left area, in the bottom left part of the work area, minus 1, minus 1.5.

So let's see what happens if we pretend, actually, if we don't know that the real robot is different from the robot that we learned during training.

So let's apply the results that are known in the literature.

So we just minimize these cost function on here in the policy space.

And what we see is something that is quite relevant for us.

What we see is that in this very simple setup, by using the optimal policy and by only having a 10% discrepancy between the real model and the model from training, basically the robot fails in the task

basically every time except then in trivial situations.

By trivial situations, I mean situations where the shortest path between the initial position of the robot and the final destination is obstacle-free.

You see that only in the red curve there is the only initial position that the robot is able to achieve the final destination.

All the others, whenever there is an obstacle along the path so that the robot should change direction to avoid obstacles, the robot fails.

does not achieve the destination of course this means if it was a real robot interacting in a real environment this means that there is a crash with a wall or with the environment in general that can be even dangerous this is exactly the type of behavior that we would like to avoid so first of all um this mismatch between the real robot and the robot model available during training is called ambiguity so we have an ambiguous model

and this ambiguity can have catastrophic effects.

So it can lead to failures that can be catastrophic for the agent and for the surroundings.

So in the next part of the presentation, and then Josepa will follow up with the code with implementations, we will discuss how our model can mitigate this issue.

In particular, our model

relies on free energy minimization but we will see that basically we will minimize the worst case free energy and that means that implies a free energy maximization step which is quite unusual

Then the model will allow us to establish what are the best performance that can be achieved by an agent that follows the model, plus will have some interpretability properties.

So the cost driving the actions of an agent can be inferred from the viewpoint of an external observer.

And also clarifies

some cognitive behaviors like this hint he tries to explain some cognitive behaviors in the in the in the regimes of large and small ambivalence so this is what the model will do now let's see what the model actually is and we should start by formalizing a little bit the problem and we should check why the robot was failing we should understand why the robot was failing in the previous example when the ambiguity was not considered

So let's go back to our picture.

In the end, what happened before was that the real robot was different from the robot learned during training.

So the two PDFs there were different.

And this small mismatch created the crashes that we saw in the picture just a few minutes before.

We can make a step, an abstraction step, and realize that we can look at these objects in the space of density.

Okay, now these two objects are just two points in the space of densities.

Let's say P bar is the training model, the nominal model that the agent has available, and P is going to be a different point in this space.

We are interested in protecting the decisions of the agent against all possible perturbation in this space that are in a suitably defined set.

OK, so what we are going to define is a neighborhood, a sort of neighborhood, although it's not properly mathematically enabled, which is centered around the nominal model.

And we are going to call this radius eta, which we call a radius of ambiguity.

So the set inside the circle here is going to be called ambiguity set.

And eta is going to be the radius of ambiguity.

We can formalize things a little bit more.

There are some technicalities that I refer to the interested readers to the paper.

And we can say that basically this set, this circle, is the set of all probability densities that have Kullbeck-Leibler divergence from the nominal model of at most eta.

Again, eta is an ambiguity radius, quantifies how ambiguous the model, the training model is, can be state dependent, time dependent, and action dependent, of course.

And it has a straightforward interpretation.

Basically, if it is small for a given action and state pair, this means that the agent is quite confident or believes it is confident in the model P bar in the training model.

Otherwise means that it has low confidence in the model that it has available.


SPEAKER_01:
so we have formalized this oh sorry this carl maybe wanted to say something i i swear hand raising you you did and i do want to say something and i i should apologize because i have to go in a few minutes but i just wanted the opportunity to congratulate you on on this work um before i go away uh you know there are

I think a number of things that this work brings to the table.

And the first thing I think you've really highlighted is the intimate relationship between the sort of biomimetic applications of the field entry principle and active inference and control theoretic approaches, which of course really start to matter when in real world implementations and neuro robotics and the like.

So I just wanted to acknowledge that that's a really important point of convergence.

and just also um foreground the key advance that you've framed and you're about you i'm going to miss my favorite part because you're about to explain my favorite part but uh just to sort of put it in context um you know to my mind this brings a very fresh and different view of ambiguity to the table from the point of view of active inference because

Unlike active inference and discrete state space models where ambiguity is all about an intrinsic part of expected free-ranging, ambiguity in this instance, I think, is much closer to the notion that you would find in economics, where it's not only uncertainty about what will happen if I do this,

but it's i don't even know the game i'm playing and this is really literally a deep aspect of uncertainty and basically i think what you're bringing to the table here is a principled way of resolving that or incorporating that uncertainty incorporating that uncertainty quantification into a vanilla free energy minimizing process and

So to my mind, this is very much and literally a deep solution that one can look at in many ways.

One can look at this as basically a clever way of Bayesian model averaging by including uncertainty about the very structure, about the very

parameterization of the model itself by bringing hyper priors to the table that say look you know I know what to do if this is the right model but I have this extra kind of ambiguity this extra kind of uncertainty that actually inherits from not knowing what the model in and of itself is and you've provided a particular structure to that I just want to conclude by

and this is something that you might pick up in later discussion, but it also, this sort of kind of robust Bayesian model averaging or handling ambiguity,

not only accommodates the uncertainty about the very structure of the model itself, but it also resolves something which is generic to variational approaches, which is the overconfidence problem.

So even if you knew the right model,

the mean field approximations that are inherent in variational bays and the minimization of variational free energy inevitably lead to an overconfidence.

And in principle, I think the technology you're about to

you know do a deep dive on um will also resolve that overconfidence problem which has been long-standing um you know in variational inference and of course really starts to bite when you actually put robots into play so i'll stop there uh but just to thank you for involving me in this work and again congratulate you thank you for the kind words and


SPEAKER_02:
we can follow up on the many future and interesting directions that come from this work that start from this work okay so i'm gonna i'm gonna continue uh

so uh let me make a step back maybe uh so let me recall here the definition of ambiguity well the mathematical details here are not that relevant although the structure of this formula is important for our proofs whatever i'm going to show you is basically proof supported so there is mathematics out there that supports our claimant claims of course here i'm gonna keep it at a very high level

So we want to find this policy that is free energy minimizing, but yet it is robust.

So basically, we want to find now a policy, a method that finds the optimal policy minimizing the variation of free energy, but across that ambiguity that Carlo was also mentioning just a few seconds before.

And how do we do that?

So let me frame it first in English.

Okay.

So again, we want to find the optimal policy.

So we are going to find a probability density function or a sequence of probability density function.

P star u k given x k minus one.

Now, whenever you see the star symbol means optimality and the curly bracket over there means a sequence.

Possibly this is a sequence of policies.

So we want this solution, this policy, to be the solution of a problem where we minimize the variation of free energy.

So the optimal policy is the hard mean, is the minimizer of a cost functional that is complexity plus expected cost, just in the way we saw a few minutes before.

but there is ambiguity and we must take into account ambiguity so we are not just minimizing this cost functional where we are actually accommodating for the worst case so we are minimizing the maximum free energy where the maximum is taken across all possible environments all possible world models that belong to the ambiguity set that i just introduced

so we have a mean max problem where the minimization is as usual in the policy space but this minimization in the policy space happens on an inner maximization problem where we maximize over all possible words in the ambiguity set this is the let's say the english translation of of of our of our framework we can be a little bit more mathematical

Okay, so I'm just going to put the problem statement here, which looks, let's say, a huge equation, but it's not that complicated.

So again, we want to find the optimal policy, the green part in the sentence.

And we want to minimize the variation of free energy, which is nothing else than complexity and expected costs, where complexity is again complexity between the closed-loop system and the generative model.

And the expected cost is specified through the action and state costs.

And we want to minimize the worst-case free energy across ambiguity, which means that we want to minimize the maximum free energy across all possible environments that belong to the ambiguity set I just introduced.

So this is basically the formulation, the distributionally robust free energy minimization formulation that we have in the preprint that you see online, that is also linked to the live stream.

It is a challenging problem from the mathematical viewpoint because it is infinite dimensional and it is a mean max optimization problem.

And it is also a nonlinear problem.

So it is basically we cannot rely on linearity of the constraints or linearity of the cost function.

Actually, the big step in the preprint online is that this problem can be solved

and this basically gives us the resolution engine the resolution engine is the set the software tools and the mathematical methods that make it possible to solve that big optimization problem and make it possible to compute effectively a policy so how does it work in really a nutshell um

The agent, which now we are exploding, so we are giving a little bit more details, gets Xk-1, the state, the current state, beliefs or observations, uses the generative model and the state cost.

It will compute a policy, and the policy will be computed by minimizing in a recursive way the variation of free energy, the max variation of free energy.

So what is the resolution engine?

What will it do?

And then Josefa will jump in the code.

First of all, it can break down at a temporal scale and at a logical level the original free energy problem.

So there are lemmas out there, technicalities that tell you that the problem can be solved at different time steps and that minimization and maximization have a precise order in which they should be performed.

So first we should do maximization and then we should do minimization.

So this gives you basically a bi-level approach.

And these software and lemmas, what they do, they yield you an explicit expression for the optimal policy.

So we know what the optimal policy is going to be.

And also tell you what is the lowest challenge that the agent can achieve under ambiguity.

So the optimal value.

from the optimal policy which is going to be a probability density function like the one that you see in the picture we are going to sample an action with standard methods so we are going to sample uk send it to the environment and the loop will continue on and on so let me start giving you some just the flavor of these key elements here

First of all, the policy can be completed in a recursive way.

In particular, it can be shown, I don't do the details here, that the policy can be computed straight away as a minimization problem.

Where the original problem is, this is a perturbation of the original problem, where it will be apparent to you that rather than the cost we had before, now we have what we call the cost of ambiguity.

we call this cost of ambiguity because it comes from an optimization problem the the maximization step in the in the environment in the environment space that i mentioned before but this cost is there just because there is ambiguity if there was no ambiguity we would not have this step and then we will recover classic methods ambiguity free methods so

just two key takeaways here that there is a free energy maximization step if you look at the top diagram top equation top formula in green you will realize that this is maximizing a variational free energy so we have a free energy maximization step and it turns out that this free energy maximization step that gives us a cost of ambiguity is the guy that guarantees robustness

also of course as also carl was mentioning the model does not need the agent to know what is the real environment of the of the of in which the robot is moving for example the real robot model the real real robot model um it only relies on the probability uh sorry on the on the ambiguity set b eta

also in the top formula there is a c bar that this comes from a a backward recursion so one could use dynamic programming or neural approximators deep neural networks for example to estimate these pieces of this piece i'm not going into the details of this right now so

we have two steps as we said and down a logical breakdown of the of the decision problem so we want to find a policy finding the policy means minimizing that cost that is now at the center of the slide in the middle of the slide which has the cost of ambiguity and this cost of ambiguity comes from maximizing the free energy in order to make the model useful so deployable essentially we need to find a way to compute the cost that cost of ambiguity

Cost of ambiguity depth, I remind you, comes from this optimization problem.

Now, this is a very hard optimization problem because it is infinite dimensional.

So this is optimization in the space of density.

And it is also a nonlinear problem, although in a constraint that is a compact set.

it turns out that most of our methods the formal methods that we developed were aimed at proving that you can actually compute this cost in a rather efficient way i would add or at least with uh off-the-shelf tools from convex optimization okay i this is actually a set of theorems propositions and lemma which i'm going to package as a meta theorem which is very informally stated

This method theorem says that the cost of ambiguity has a specific form.

It's eta plus c tilde.

Okay.

Well, eta is a given.

Eta is the radius of ambiguity and c tilde is what our model computes.

Now this C tilde comes from solving an optimization problem that now is a scalar optimization problem.

So in this formula, alpha is a non-negative scalar value.

So we just have scalar optimization and B tilde alpha is a function that I'm not going to specify here, but it is convex.

So in the end,

we can recast the problem of solving or finding the cost of ambiguity and address this problem via a convex scalar optimization problem that moreover has a global minimum, of course, because it is convex optimization.

so we can go back to our resolution engine and say that that general step about finding the maximum free energy can be addressed with a scalar convex optimization problem so we did not build a solver for this we used off-the-shelf tool so that the problem that we are going to minimize now the cost function that we are going to minimize has a very specific structure you see that the cost of ambiguity is eta plus c killed

So the only thing that is needed to be done is to minimize this guy in the space of policy.

Now, this might.

Oh, sorry, I forgot.

So what what is the engine actually doing?

Taking the generative model, it is computing a cost of ambiguity, which sometimes it has this double shaped potential here.

So this is a cost of ambiguity means that in minus one and one, the ambiguity is very low or it is lower than other parts.

uh and for example uh towards the end of this diagram towards the sides of this diagram the ambiguity is much larger so the cost is much much larger so what does our model do computes this cost of ambiguity and will output the policy and the policy comes from minimizing this cost functional here

Now, this might seem as a very abstract, as a rather abstract problem, optimization problem, but actually it is very popular in the literature.

And it even has a policy that is a well-defined algebraic solution, which is a softmax.

So what you see here is basically a softmax function, a twisted softmax function.

And what is the idea that this formula, that this policy implements?

Well, the optimal policy expression is telling us that the optimal solution is given by the generative model, QU, so the bias of the agent, the beliefs of the agent, in a sense.

And this model is somehow weighted and or distorted by an exponential.

Now, the exponential in what I think it is quite interesting is that the exponential assigns lower values that are associated to high ambiguity.

So in the end, the probability of getting an action that is very ambiguous is going to be lower and lower, according to our model.

moreover as i said before we can also quantify what is the smallest possible free energy which has a log sum exp expression which is standard in the literature from physics for example so there is quite a lot on this slide so i'd like to emphasize again the optimal policy the structure the optimal policy is nothing else than a twisted or distorted version of the generative model q uk given x k minus one and the distortion happens to an exponential function

And the exponential is lower when the ambiguity is large.

So this means that the probability of sampling an action that is associated to high ambiguity is very small.

So the policy in our model has a very clear and neat structure, at least algebraic structure.

what happens inside the model so we can continue developing the flow of information inside the model so we have the cost of ambiguity computed with that scalar optimization problem we gather the generative model and we build the exponential inside the policy formula then the two terms here are multiplied with each other and after normalization we obtain the policy

you see it is quite clear that the policy the optimal policy coming from multiplication and normalization is a compromise between the generative model and the ambiguity so let me close the loop going back to the original problem that i was considering where the classical method would fail in this setting

So we have our rover, unicycle robot that is moving in this environment filled with obstacles.

We have the same generative model as before, the same training and robot model as before and the same cost.

So exactly the same frame.

And I remind you that when we don't take into account ambiguity, bad things can happen in the sense that the robot can crash in the obstacles unless this is really, unless we really have a trivial situation.

With our model instead, with Dr. Free, starting exactly from the same initial conditions and starting with exactly the same setup, you see that the robot is able to navigate towards the destination.

So, for example, take the green trajectory, the green path of the robot,

the right side of the slide on the right side of the slide it will crash in the obstacle in the bottom obstacle instead in the left with our model in the left part of the slide with our model it is able to read out in order to achieve the destination and reach the the final destination

Now, I think I gave quite a few details, and probably the best thing is to jump to the code to see it in action, unless there are questions or comments that you would like to discuss at this stage.


SPEAKER_00:
Looks awesome.

Please continue.


SPEAKER_02:
Then please, Josefa, I'll let you share the screen.


UNKNOWN:
Okay.


SPEAKER_03:
Please confirm, can you see my screen?


SPEAKER_00:
Looks good.

Thank you.


SPEAKER_03:
Okay.

Hello, everyone.

My name is Josefa, and I'm a postdoc with Giovanni at University of Salerno.

And my background is in control theory.

I would like to thank Daniel for inviting us for this presentation.

And I will present the code walkthrough for the robust decision making via free energy minimization.

So we'll start with the out.

First, we'll start with the resolution engine.

Then we'll discuss the robot experiments in more detail.

What is the cost structure for this implementation?

How we train the dynamics model?

How we define the cost function?

What is the ambiguity radius and what is the cost of ambiguity?

How is computed?

and at the end how we compute the policy basically and then we'll talk about the in silico and experimental results and then i will talk about in the end about the belief of that part which is a very cool part for of this doctor free algorithm okay so the resolution engine as giovanni just presented a few minutes ago

We want to solve a problem in which we want to obtain optimal policy, a set of optimal policies, which minimizes the maximum free energy cost basically.

And we have our model, the environment's model, the real environment's model is inside a ball

centered around a nominal model p bar and in the code i call p bar as a nominal model and so we we decompose this problem into a bi-level problem where we want to minimize over the free energy and the the maximum maximization of the free energy is implicitly incorporated in the terms of eta and c tilde which which makes the cost of ambiguity

And we obtain, by solving this problem, we obtain the twisting kernel policy where eta and C tilde are the exponent of the exponential policy.

So this is the flow of the Dr. Free Resolution Engine.

Now, so first I will introduce the robot experiments that we want.

We use the Robotarium platform by Georgia Tech.

and you can send your code to them and they will implement your code on the robots and you can they will send you the video and the results basically and what this is a real real snapshot of the platform of the work area and the robot a unicycle robot basically what we want what our task is that we want to drive the robot successfully to the cross while avoiding the red obstacles basically

uh the work area is around three cross two meters in size and uh there are multiple robots if if you have multi-agent problem also it's a it's a good platform for but for now we only consider a single agent here for simplicity um so this is what the

code looks like uh um so first we we define the global variables and action space um so so we import all the necessary libraries here the rps is the robotarium python simulator uh it's a package by the robotarium platform uh and we call the robotarium which initializes the robots the transformation from the unicycle dynamics to the

the linear dynamics and all the other utilities we call to solve the optimization problem for the obtaining the cost of ambiguity we use scipy and it's a minimized tool tool to minimize the function and we use some other plotting functions plotting libraries like matplotlib and sklearn to initialize our question process model

um here uh i define the action spaces so action spaces is basically the velocities of the robot in both axis x-axis and y-axis and we define it as a grid of a pi cross five uh discretization and it ranges from zero point minus 0.5 to 0.5 the speeds of the of the robots in both directions

now after doing this first what we have to do is to load the dynamics model or train the dynamics model so the p bar is the model that we have from the of the environment learned by some set of data um and here in our case we have three different data sets x1 x2 and x3 are the like the states of the robot uh and the positions

and the u1 the u is the input or the velocity is given to that robot at particular time and we create a stack of these two to make the training input and y is our target so the next possible state given the previous state and the current input what will be the next state is the is represented by so we have such three data sets each consisting of 500 samples

And we train three different Gaussian process models here.

So the Gaussian process, basically, the most important component of a Gaussian process model is the kernel function.

So the kernel function is basically finding the distance between the two data points at a given time.

So here, the most widely used kernel function

in a Gaussian process modeling is a RBF kernel with some constant term multiplied to it.

So here, we define the kernel function, the constant term, the RBF.

The RBF, the radial basis function, has parameters called length scale.

And since we have a four-dimensional input, we have four-dimensional length scale.

parameter, which is landscape hyperparameter.

I'm just initializing it here, plus some white noise because white noise, because your data collection channel can be noisy.

And so to model that noise, we have a white kernel function as well here in the entire kernel function.

Now, given this kernel function, we initialize the Gaussian process regressor and we take this regressor and we start the fitting.

And we use the fit command basically for this.

And we obtain, so this is the kernel parameters and what we want to, after training, what we obtain is the hyperparameters of these kernels.

So the sigma F here is the constant kernel coefficient.

The RBF here, the radial basis function here has a parameter W or the length scale.

We have that length scale here and the noise term here.

So these are the results of the three different data sets for the three different Gaussian models.

Now we take these models and then we move on to defining our cost function for the expected loss here.

so the loss function is basically consisting of three terms so the first term in the cost is a squared distance between the goal point and the state uh the second term the cost sum actually is a rbf kernel again centered around the obstacles

So here are the location of the obstacles on the work area of the robotarium.

And we calculate the RBF kernels.

We center the RBF kernel around these blue points and we get the Gauss sum.

So basically what it's doing is that when you are near an obstacle, the Gauss sum, that second term, will have a very high value.

So it will shoot up.

And the third term, which with a coefficient five here,

is for the walls.

So we don't want our robot to go outside the work area.

So we don't want to get it outside this rectangle.

So basically, it's like a control barrier function.

If it's near the wall, this term has a very sharp rise and the cost is very high.

so this is this uh this summarizes our cost function and so the cost function takes uh the the initial state uh as an input and the goal points where you want to go the robot wants to go and the obstacle points uh as an input and gives out a a scalar cost so

This is what, when we compute this state cost, this function for the entire grid, this is what our function looks like.

So you can see that you have a very high value when you are far from the goal.

And you have very high value near the obstacles and around the boundaries.

And you have the lowest value near the bottom left where the goal is in our scenario.

So we take this cost function and the Gaussian process model.

and now we come to a very important part of the resolution engine is the cost of ambiguity so the cost of ambiguity as discussed before depends on the two uh two terms which is eta and c tilde the eta is the ambiguity radius and the c tilde is the is the cost transformed cost uh which uh

is the we define like this so the city in the function takes an array of costs uh from the cost function eta the ambiguity radius uh here the p bar is the nominal model probability function and the generative i call the generative model the q in the paper as a reference problem and it gives out the scalar again uh in the cost in the form of transform cost

so uh before giovanni introduced a a term in the c tilde computation a v v alpha this objective function is the v alpha so this is the objective function that we want to minimize basically and it depends on the in the parameter alpha and it consists of two terms first is the fine term in alpha so alpha into the ambiguity radius eta and the term two basically is alpha into the log sum exponential

of the of the cost with rest to one by alpha.

So we want to minimize this objective function over alpha.

We take the minimize the minimum value of this objective function, and we basically compare it against the baseline, which is computed by the maximization of this log of the cost and the minimum value.

gives you C tilde.

For more detail, you can refer to the paper how it is formalized there.

So this minimization gives you the C tilde, which is dependent on eta and alpha.

So eta here, we define eta as a KL divergence between the goal points and the next possible state.

computed from the quotient process model.

And for computational stability, we clip it between 0 and 100.

So KL-divergent is basically the distance between, it's not a distance, but it's a difference between the two different distributions, P and Q. And here I made a mistake.

I think there should be Q here instead of P. So this is the expression for KL-divergent.

and for the gaussian distributions you can compute the scale divergence in an analytical form in a closed form without any sampling so now given eta and c tilde we can compute our policy which are eta and c tilde makes the makes the exponent of our policy so the policy here the p star

the optimal policy is a is a twisting kernel where the qu is the generative model's policy and the exponential in exponential is the gives you high value if the c tilde the ambiguity cost is low in this case particular case in this example we consider qu to be uniform and since it is uniform you can basically remove it from this expression

of the policy so the policy computation code uh starts uh the policy computation code uh takes this initial state uh your control space because we have two inputs you have two spaces to control or action spaces uh the goal points and the obstacle points and gives out a uh

In this case, it will give out a five plus five matrix of a probability.

It's a probability function from which you can also sample the action that will go to the agent.

So for this computation, first we initialize the exponent inside the exponential with the control space, according to the control space.

and we also initialize the the policy uh distribution according to control space size and now we we go uh we compute using the for loops for each possible combination of action we take the combination of that action we give this action to the gaussian process model and we compute uh we predict the next state the next possible state

and the gaussian process model will give you basically a mean and a covariance so the here the next denominator is the mean of the gaussian process and the sigma norm is the is the covariance and using this we compute the samples the we sample from this mean and covariance the next possible states and using this sample um we we compute the we compute the cost

uh sample the costs for this number of samples so here okay so now after after computing the samples i compute the eta the ambiguity radius again the ambiguity radius as mentioned before is a KL divergence and it is clipped between 0 and 100 and i take the number of samples 100 number of samples and i compute the cost for all the next possible states

uh and i give this cost matrix and the eta to the c tilde function and as the i showed earlier uh it will give out give out another scalar value the transformed cost which is our this is our exponent the minus eta minus c tilde is our exponent and so here we use the exponential trick

to normalize this and formulate the action PDF.

Because to avoid any overflow and underflow, we subtract the exponent with the maximum of that exponent matrix.

And basically, you have the normalized PDF here.

and we we do the flat flattening operation and unraveling the index and then we take the sample the action from that uh from this pdf and we give the action the the agent will then give the action to the state uh to the system so uh given this uh policy

uh code now we we want to apply to the robotarium exam uh example and we perform uh basically a 12 set of experiments on three different training stages so the doc you can see in the top panel on the top is the doctor free algorithm and on the bottom is the ambiguity unaware panel where you can see that the the doctor free agent even with unknown

unknown system dynamics, it can still reach the goal without colliding with any obstacles.

While the ambiguity unaware agent, although the policy it is computing is also optimal, it encounters or collides inside the obstacle only in some cases when

You can see that the obstacles are not present in the way of the agent.

It will reach the goal.

So we show that how our method basically solves the issue of the model mismatch.

And we also have the experimental results for this.

So you can see the agent, the robot moving in the environment, the actual robot.

and how it avoids the obstacle so because of some uncertainty sometimes the robot also gets a bit confused let's say but but anyway it will it will reach the destination without uh without colliding which is quite cool


SPEAKER_00:
okay yeah it's uh it gets okay could you just share a little bit could you just share a little bit right there what what is it spatial awareness like is it aware of its coordinates does it have a egocentric or allocentric navigation like how does it understand the location and the relative position of the obstacles so so so the so yeah okay uh


SPEAKER_03:
so uh here in this case um so the the location is provided by the the camera so the camera is tracking the top camera is tracking the robot and it it updates its location the camera computes the location and updates the location on the on the grid also it it uh here it it has the input it knows the obstacles location

So it is given in the cost function, we define the obstacle points.

So it knows the obstacles are present.

So when it comes near the obstacle, the cost function is automatically very high and it's immediately the obstacle.

I hope it answers your question.


SPEAKER_00:
Okay, so it's aware of its two-dimensional position and of the obstacle and the goals, and it computes kind of like a suitability that's ambiguity aware.

Yeah.


SPEAKER_03:
Awesome, thank you.

But the thing it does not know is if it takes this action, so if it takes, let's say, from the action space, it takes one action,

It does not know where this action will take it.

So it cannot foresee accurately where this action will take it.

So that's where the ambiguity parts come.

So that's where the ambiguity is.

So it does not know exactly, cannot be sure of its own decision.


SPEAKER_00:
I'll just ask one more.

Oh, yeah, please go ahead.


SPEAKER_03:
Yeah, okay.

No, no, please ask.


SPEAKER_00:
Yeah, I'll ask one more question from the live chat.

So Andrew Pashaya wrote, I'm wondering about what degree of computational overhead this computing the worst case ambiguity adds to running the agents.

So like just in this situation, what was kind of the computational complexity or the resource use of these different stages of the algorithm?

Like does the Dr. Free add a small overhead


SPEAKER_03:
to uh most of the computational cost being on the policy inference or does this ambiguity add a significant computational overhead uh no uh it's uh for sure because of the com uh because of the ambiguity uh computation of the ambiguity uh cost there is a there it is a bit slower than the the ambiguity never agent but as you can see uh here in the sorry in the video


SPEAKER_02:
uh it is all real time so it is doing that this is calculating the actions uh in real time uh i don't know the video is gone for some reason okay yeah if i if i may add something uh daniel i think this is an excellent question because it may be a a research research program per se so finding the computational tricks that

improve computational efficiency as much as possible.

There are many tricks indeed.


SPEAKER_03:
Yeah, so the one thing that would be interesting to see is because I can, if you allow me to go back a little bit.

Okay, so policy computation here.

You can see that we are looping over all the possible actions here, but

don't need to do that you can have a multi-threading or some parallel computation and you can compute all the actions for sorry the computation all the costs for all the possible actions in a parallel computation and which will it will uh which will uh i think make it much more faster because here we are using the for loop over uh the it's uh so it's it has a

If the action space is very large, then it will for sure have a very high computational overhead.

But I think you can resolve it by parallel computation for these four loops.

OK.

If there are no more questions, I will go ahead.

OK.

uh now we'll come to the belief update part so belief update what is belief update is that um that if agent is performing some task we can learn the objective functions of the agent of the expert agent just by their data of the input and the states data of that agent and uh

uh the cool part for the doctor free policy computation is that the policy is very interpretable so you can reconstruct the objective function of the agent simply by using the data the state and the input data of the export agent and we do this by by solving this by minimizing the likelihood negative likelihood

uh given here so the the likelihood function is basically a summation of the affine term plus a log sum x and because both of the both of the terms the affine term and the locks of x are convex this optimization problem is convex even though the the cost function or the objective function uh of the expert agent is non-convex in nature

so the major the major components of for this uh belief update are the features the cost features and the weights the feature weights the feature weights is the thing that we want to learn by optimizing over the the log likelihood function and so first let's see the code structure for this

solve this problem so again we define the global variables the action space we have to load the train models uh for and the expert agents data so we have the data of the state and the action for from some export we define the cost features uh we compute the feature expectations uh using the gaussian process model and we formulate and solve this optimization problem here and

we obtain the reconstructed cost.

So I will start with the cost features here.

What is the cost features that we define?

So similar to the previous task, I am defining the cost feature as the two terms.

The first is the squared distance from the goal point.

And the second term is again the Gaussian or the RBF kernel centered around some points.

And the RBF kernel is defined by this function here.

So the RBF kernel now is not defined only on the obstacles, the actual obstacle in the environment, but now the RBF kernel is defined on a set of uniformly distributed points on the grid.

which may or may not represent the actual obstacles.

So you can see that this is the feature map or the feature grid.

And this is uniformly distributed.

And the actual obstacles are here and here.

So the algorithm has to learn how to give importance to the relevant features only and not to random features in the environment.

okay so given this cost features uh we solve this minimization problem uh and uh so we have let's say we have m number of samples from the expert agent so we initialize the initial state and the uh and the the features so for again for all the possible number of inputs we compute

the next possible state using the Gaussian process model.

And we sample a number of samples from this Gaussian process model and we compute the features.

And we compute these features and we basically do the averaging or we do the averaging and we compute the expected value for these features.

So here, the second term

of the of the equation was log sum exponential and for that you need to compute the features for all the possible inputs while the first term of the of the likelihood function was an affine term which only computes the given action taken from the expert agent so you can see here at the end there are two terms the first is the affine term in the w

which takes the feature sample, which is dependent only on the input given from the agent, while the log sum exponential is depending on all the possible inputs, the feature computation for all the possible inputs.

And taking these two terms, we stack for the entire time horizon M, capital M, and then we minimize it for the summation of this

entire time trajectory.

Since it is a convex problem, you will have an optimal solution for any initialization of the weights.

So for this particular problem, we get this set of weights and you can see that where there was obstacles in the center, you have very high weights while

all the other weights are near zero or are very low compared to the the real obstacles so so this was the real cost function of the objective function of the export agent and this is the reconstructed cost function of the obstacle of the of the uh agent and you can see that they are very similar in nature so you have high costs away from the from the goal point

and you have the obstacle in the center, so on and so forth.

And using this cost function, the reconstructed cost function, the agent is still able to go to the goal while avoiding the obstacles.

So just to recap, so the cost structure for the robot routing task was to define the global variables, action space, load and train the dynamics model,

cost function and the features compute the policy and you compute the collect the data and basically visualize uh the robot uh going to the goal while in the belief of that cost structure uh we also define the global we define the cost features hello


SPEAKER_00:
It just blipped for a second.

You're back though.


SPEAKER_03:
Okay.

Yeah.

Okay.

Sorry.

So, okay.

As I was saying, so the belief update structure, we also need to define global variables and action space.

We need to load the trained models.

We define the cost features and we compute the feature expectations and we formulate and solve the

minimization of the log likelihood function and we obtain the reconstructed cost so yeah okay uh okay if you have any questions uh now i'm happy to take them


SPEAKER_00:
Awesome.

I'll ask a few quick questions, and if anyone else has a question in the live chat, go for it.

So first, it was mentioned that it's infinite dimensional.

So could you unpack what that means as you're dealing with a finite number of dimensions in terms of sensors and actuators?

So what does that mean for it to be infinite dimensional?


SPEAKER_03:
Okay.

The infinite dimensional was actually...


SPEAKER_02:
here so uh it's regarding to the maybe the professor's slide is better i can maybe maybe i can put my slides to say because this point is more clear okay give me just one split second please

So to answer your question, Daniel, so the infinite dimensional nature comes from the statement of the problem.

So you are not sharing.


SPEAKER_00:
I see it, actually.

I see it.


SPEAKER_02:
Okay, so maybe I'm not sure.


SPEAKER_00:
It's like on Zoom, like maybe you're both sharing or something.


SPEAKER_02:
Oh, okay.


SPEAKER_00:
But it's good, though.

I see your slides.


SPEAKER_02:
Okay, you see it.

Okay, perfect.

Okay.

The infinite dimensional that was mentioned before, it's in the mathematics, in the formal aspect of the framework, because this optimization problem has as decision variables the functions.

okay and that makes it an infinite dimensional optimization uh when osefa engineered uh dr free on the code of course you need to estimate this function so he made a step that discretizes at some point some of these functions uh and that of course reduces the problem to the uh final finite dimensional optimization so the infinite dimensional

feature that we were referring to was about the problem statement, so the fact that we are optimizing in functional spaces.


SPEAKER_00:
Awesome.

Okay, another question like, how would someone go about adapting this to another setting?

Like how would they think about framing the kinds of algorithms and approaches that are already in the active inference or the cybernetics literature?

Where do you see opportunities, like which systems for applying this and then how adaptable or how would you go about adapting the open source code you've provided to give this sort of ambiguity aware overlay?


SPEAKER_02:
So I'll go maybe with the framework and then I'll let Josefa comment more on the code since he developed the code.

So, well, the framework is, I would say, rather general.

So I would say that the system level steps would be to identify a good cost functional, identify the ambiguity set, model the environment, the training model, and everything else that comes with it.

Once you have this key, and the generative model, of course,

once you have these key ingredients i see um these framework fitting in the context of sequential policy optimization which is really sits is the bread and butter of control theory but also cybernetics and it has a lot of interdisciplinary links with uh with signal processing etc

In terms of code, I think I will let Josefa answer that, but the main challenge is going to be the computation of the integrals and the probability density functions that are implied by the framework.


SPEAKER_03:
Okay, yeah, so interesting question.

So there are

uh i think robotics is one of the fields in sequential decision making which is very very cool to apply this because we have seen that when you have mismatching parameters of the models of the system uh the robotics uh the task uh is very difficult to perform we also have also seen robots fail when they are mismatching the in the parameters um so if somebody wants to apply

or implement this framework, I would think that they would first need to change the dynamics model that they have.

So you need to adapt your dynamics model according to the system that you have, first of all.

Then, for example, if you are working in an environment in which

the how can i say the nominal model and the generative model are not really gaussians that at that point you will not have the analytical solution or the closed form solutions as given in the code and for that i think you need to revert to sampling some form some form of sampling to compute the integrals and the expectations

So that's one part that I think if someone wants to implement this in a system which is more complicated and doesn't have Gaussians assumptions, then yeah, you need to consider sampling.

Another part would be, as I said, if the action space of the system is too large, then to avoid operational overhead, you need to consider parallel computing, maybe.

or some form of vectorization to avoid the for loops in the input.

Yeah.

I think that's the core part.

But other things like the cost of ambiguity, the function that computes this cost of ambiguity, the structure would be the same for that.

Yeah.


SPEAKER_00:
Awesome.

So sort of in closing, what are your next steps for research?

Or how do you see this continuing?

Or what ways would you like to see people in the community kind of continue with the work that you're doing?


SPEAKER_02:
Well, first of all, I'd say that if you want to use your code and you have any questions, I think we would be happy to answer these questions.

And we are happy if the code is useful beyond, of course, the use case we are considering.

Robotics being one remarkable example, I think.

And where do we see this placed?

I think we prepared this slide that summarizes and maybe captures on some of the points that were discussed and about the interdisciplinary link.

So we see these placed in the context of feedback systems.

So in the end, the agent and the environment are two systems that are in feedback.

So the actions affect state and state affect the actions.

And basically what we did was to bring some control theoretical and optimization methods into this minimization of variational free energy.

So it is really an intersection between computation, embodiment of agents and robustness.

uh possible i think very cool research ideas could be uh integrating deep reinforcement learning so the the techniques that we the technique that we presented dr free is not mutually excluding with deep reinforcement learning one could use deep reinforcement learning to learn ambiguity one could use deep reinforcement learning to to learn a better cost etc so i think there is a sweet spot of applications that can have a high impact

for real-world deployments by merging these two approaches.

there are also there is also another so i'm not a neuroscientist i'm gonna start with that but uh i think the the model has some cognitive implications as i said i just showed uh it supports bayesian belief updates but one thing that we had no time to discuss about of course is that something very nice happens in the regimes of large and small ambiguities

so basically when ambiguity is small you get the behavior of an agent that knows everything about its model and it turns out that nobody cannot perform this agent so it's the best possible agent it is an oracle essentially that does the best possible actions will achieve the best cost function on the other side of the spectrum instead when ambiguity is very high

There is a sort of a transition where we can show it is shown in the preprint that the optimal policy is training independence in the sense that is independent on the training model.

So it is a kind of situation where the agent dominated by ambiguity grounds its decisions just on ambiguity and not on what it learned previously.

which is i think it's a kind of cool behavior of course i'm not a neuroscientist but i think it's something that could be investigated uh on that side on that spectrum on that side of the spectrum i hope i answered your question yeah that's an interesting point kind of like neuroplasticity and age


SPEAKER_00:
and how you never do worse by knowing more about the environment, all things being equal, and then as you turn up the dial on ambiguity, it erases whatever understanding was made ambiguous.

And that could be even empirically fit to understand what are the biological contexts that different ambiguities, different hyperparameters are in play.


SPEAKER_02:
Yeah, exactly.


SPEAKER_00:
Well, thank you both and to Carl for joining.

We really appreciate the work and for sharing it and having the open source code.

It's really cool.


SPEAKER_02:
Thank you.

I put here two links to the papers and the QR codes in case you are curious and you want to check them out.

And thank you again, Daniel, for organizing this.


SPEAKER_00:
Awesome.

Okay.

Till next time.


SPEAKER_02:
Bye.

Bye.

Bye.
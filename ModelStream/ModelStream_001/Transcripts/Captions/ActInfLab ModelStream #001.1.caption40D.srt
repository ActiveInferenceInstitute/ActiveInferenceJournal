1
00:00:07,173 --> 00:00:08,341
Hello, everyone.

2
00:00:08,341 --> 00:00:11,044
Welcome to the Active Inference Lab.

3
00:00:11,144 --> 00:00:14,314
This is the first active inference
model stream.

4
00:00:14,314 --> 00:00:16,583
Active Inference Model Stream 1.0.

5
00:00:16,983 --> 00:00:19,819
And I'm really excited
for today's conversation.

6
00:00:19,853 --> 00:00:24,491
I'm Daniel Freedman and just to introduce
the other participants today.

7
00:00:25,258 --> 00:00:27,093
Ryan Go for it.

8
00:00:27,660 --> 00:00:27,994
Yeah.

9
00:00:27,994 --> 00:00:30,130
Hi, I'm Ryan.

10
00:00:30,130 --> 00:00:34,100
I'm from the Institute of Brain Research.

11
00:00:34,434 --> 00:00:35,902
Hi, I'm Christopher.

12
00:00:35,902 --> 00:00:36,636
I'm a Ph.D.

13
00:00:36,636 --> 00:00:39,706
student at the MRC Cognition
Brain Sciences Unit, which is based

14
00:00:39,706 --> 00:00:42,342
at the University of Cambridge.

15
00:00:42,976 --> 00:00:44,844
Hi, I'm Max Murphy.

16
00:00:44,844 --> 00:00:47,280
I just completed my Ph.D.

17
00:00:47,280 --> 00:00:50,250
at University of Kansas in bioengineering

18
00:00:50,817 --> 00:00:53,686
with Focus on Neural Engineering Awesome.

19
00:00:53,987 --> 00:00:57,557
Thank you everyone for participating
and for Ryan and Christopher.

20
00:00:57,590 --> 00:01:01,094
Two of the authors of this awesome work
we're going to be exploring.

21
00:01:01,694 --> 00:01:05,298
So this is the first
in a several part series

22
00:01:05,598 --> 00:01:09,169
that is going to be highlighting
several perspectives and addressing

23
00:01:09,169 --> 00:01:12,272
questions
related to the active inference tutorial.

24
00:01:12,272 --> 00:01:14,174
Paper of Smith et al.

25
00:01:14,174 --> 00:01:15,108
Called a step

26
00:01:15,108 --> 00:01:19,679
by step tutorial on active inference
and its applications to empirical data.

27
00:01:20,013 --> 00:01:23,049
So the idea here is for those
who are working

28
00:01:23,049 --> 00:01:26,853
with empirical data to learn about active
inference as a method.

29
00:01:27,020 --> 00:01:27,954
And also for those

30
00:01:27,954 --> 00:01:29,289
in the active inference community

31
00:01:29,289 --> 00:01:32,525
to be learning about some of the methods
that apply active inference.

32
00:01:33,526 --> 00:01:35,695
If you're
listening, you're participating.

33
00:01:36,062 --> 00:01:38,932
And if you have any questions
during the live stream, feel

34
00:01:38,932 --> 00:01:42,902
free to post it in the YouTube live chat
And we'll try to address it

35
00:01:42,902 --> 00:01:46,873
during or after this presentation
that we're about to get.

36
00:01:47,474 --> 00:01:49,976
If you have questions
after the live stream,

37
00:01:50,276 --> 00:01:53,313
please feel free
to leave it in a comment form

38
00:01:53,313 --> 00:01:58,485
and we'll try to address it and integrate
your input in future sessions.

39
00:01:58,885 --> 00:02:02,222
And to learn more and to participate
check out Active

40
00:02:02,222 --> 00:02:05,692
Inference North or any of the information
in the video's description.

41
00:02:06,025 --> 00:02:09,562
So that's all the information
or metadata for this video.

42
00:02:10,196 --> 00:02:11,731
The way this is going to work today

43
00:02:11,731 --> 00:02:14,667
is we're going to do
some introductory questions,

44
00:02:15,335 --> 00:02:18,972
just sort of like asking
what in general is this work about what

45
00:02:18,972 --> 00:02:22,008
motivated the authors to write the paper
the way that they did?

46
00:02:22,342 --> 00:02:26,079
And then both Ryan and Christopher
are going to share their screens

47
00:02:26,312 --> 00:02:27,881
for part of the presentation.

48
00:02:27,881 --> 00:02:28,715
And they're going to show us

49
00:02:28,715 --> 00:02:31,618
a few different things
about the work that they've done.

50
00:02:31,985 --> 00:02:35,188
And then we have a couple of questions
prepared on our side.

51
00:02:35,522 --> 00:02:39,092
But also, we're going to be looking
at a live chat, if anyone has questions.

52
00:02:39,092 --> 00:02:40,527
So just post it

53
00:02:40,527 --> 00:02:42,395
whenever you feel like it
in the live chat,

54
00:02:42,395 --> 00:02:44,430
and then we'll again try to address it.

55
00:02:44,430 --> 00:02:48,735
So as I stated the intro questions
and then we'll go to the presentations.

56
00:02:49,135 --> 00:02:54,207
So first intro question to the authors is
What is this work?

57
00:02:54,474 --> 00:02:55,875
What is exciting about it?

58
00:02:55,875 --> 00:02:58,178
What motivated you to work on it?

59
00:03:00,880 --> 00:03:02,682
Okay, so.

60
00:03:02,682 --> 00:03:05,084
So just to kind of reintroduce myself
a little bit more.

61
00:03:05,084 --> 00:03:08,821
So I'm Ryan Smith, so I'm an investigator

62
00:03:08,821 --> 00:03:12,392
at Lorain Institute
for Brain Research in Tulsa, Oklahoma.

63
00:03:12,926 --> 00:03:15,128
And the focus of our institute

64
00:03:15,528 --> 00:03:18,531
is primarily on

65
00:03:18,531 --> 00:03:24,170
neuroimaging and sort of neuroscience
approaches to understanding psychology

66
00:03:24,170 --> 00:03:27,240
and psychiatry with a focus on sort of
treating psychiatric disorders.

67
00:03:28,374 --> 00:03:31,711
And so for a while now,

68
00:03:31,811 --> 00:03:36,082
there has been the use of simpler
computational models,

69
00:03:36,382 --> 00:03:39,085
primarily reinforcement learning models
or graph diffusion models.

70
00:03:39,085 --> 00:03:42,455
Things like that
out there for researchers

71
00:03:42,455 --> 00:03:44,691
for who are working with empirical data

72
00:03:45,558 --> 00:03:48,061
who so there are good resources

73
00:03:48,061 --> 00:03:50,196
out there for people
to learn those methods,

74
00:03:51,197 --> 00:03:53,533
to apply them to data in their own
research.

75
00:03:54,400 --> 00:03:58,037
However, at the moment, so active
inference is a sort of much newer field,

76
00:03:58,338 --> 00:04:01,341
especially its sort of formulation
in terms

77
00:04:01,341 --> 00:04:04,410
of partially observable
markup decision processes.

78
00:04:04,877 --> 00:04:10,083
And there isn't really today
a really clear

79
00:04:10,450 --> 00:04:13,052
sort of combined place

80
00:04:13,052 --> 00:04:18,024
to learn the sort of practical methods
to to build these sorts of models

81
00:04:18,057 --> 00:04:20,960
and to then sort of apply them

82
00:04:21,294 --> 00:04:24,931
to task behavior in empirical studies.

83
00:04:25,632 --> 00:04:29,202
And so the kind of motivation
for this paper or this tutorial

84
00:04:29,602 --> 00:04:34,240
was to allow somebody,
you know, so new students or somebody

85
00:04:34,240 --> 00:04:38,177
who is sort of like an early, early,
you know, junior faculty

86
00:04:38,177 --> 00:04:40,980
things like that,
who wants to go in this direction

87
00:04:41,814 --> 00:04:44,784
to give them the resources
they need to do that. And

88
00:04:45,885 --> 00:04:48,888
as easily as, you know, in a store
early as possible.

89
00:04:48,888 --> 00:04:54,927
So even if you start out not really
having any background in this stuff.

90
00:04:55,328 --> 00:04:59,399
And the hope was that
you know, the first section of the paper

91
00:04:59,732 --> 00:05:03,036
would allow you to kind of
get enough of a background in the theory

92
00:05:04,203 --> 00:05:08,007
such that you would understand how to
then learn how to build these models

93
00:05:08,708 --> 00:05:11,711
specifically for empirical tasks
and eventually

94
00:05:12,178 --> 00:05:15,448
to learn
how to fit those models to the data

95
00:05:16,382 --> 00:05:18,751
so that they could be used
in your own sort of research

96
00:05:20,086 --> 00:05:23,222
even, you know, for across
many different fields.

97
00:05:23,756 --> 00:05:28,061
So I would say what's what's exciting
or motivating is primarily just to that

98
00:05:28,428 --> 00:05:29,629
if you're a new researcher

99
00:05:29,629 --> 00:05:32,231
and you're interested in this,
but you don't know how to do it.

100
00:05:32,765 --> 00:05:34,834
You know, the hope is
this is kind of going to be like a one

101
00:05:34,834 --> 00:05:37,103
stop shop
where if you can get beginning to end

102
00:05:37,103 --> 00:05:38,471
then you'll know how

103
00:05:38,471 --> 00:05:41,240
to do what you need to do to start
using it in your own research.

104
00:05:41,574 --> 00:05:44,277
So that's the major driving motivation.

105
00:05:45,378 --> 00:05:45,978
Awesome.

106
00:05:45,978 --> 00:05:47,046
I love that.

107
00:05:47,046 --> 00:05:50,817
One stop shop for a researcher
who wants to apply the methods

108
00:05:50,817 --> 00:05:52,919
but doesn't really know where to start.

109
00:05:52,919 --> 00:05:54,687
So that's really a great idea.

110
00:05:54,687 --> 00:05:59,592
And you have version the document many
times to really address people's input.

111
00:05:59,592 --> 00:06:01,861
So it's cool
how it's an evolving document as well.

112
00:06:01,994 --> 00:06:05,865
So Christopher, what would you say
in response to those intro questions?

113
00:06:07,600 --> 00:06:12,672
Um, so I think there are two parts
of my motivation for writing this.

114
00:06:12,705 --> 00:06:16,509
The first was a selfish motivation
in that I think if you want to understand

115
00:06:16,509 --> 00:06:19,112
something, the best way to do
it is actually to write a tutorial on it.

116
00:06:20,613 --> 00:06:23,583
And so in when Ryan asked me to do this

117
00:06:23,583 --> 00:06:26,085
tutorial with him,
I was jumped at the opportunity

118
00:06:26,085 --> 00:06:27,620
just because it would
be an opportunity to kind of

119
00:06:28,855 --> 00:06:31,991
actually learn this stuff better myself

120
00:06:31,991 --> 00:06:34,560
and really get into
some of the more technical details.

121
00:06:35,862 --> 00:06:39,198
And so then more broadly
speaking, though,

122
00:06:39,198 --> 00:06:42,602
I think there are lots
if in principle there's nothing kind of

123
00:06:43,636 --> 00:06:46,506
applying these models and practices
no more complicated than doing like model

124
00:06:46,506 --> 00:06:48,107
based reinforcement learning.

125
00:06:48,107 --> 00:06:51,344
And yet it's incredibly more common
to see researchers

126
00:06:51,344 --> 00:06:54,447
working with various
more sophisticated RL schemes.

127
00:06:55,081 --> 00:06:55,715
And I think

128
00:06:55,715 --> 00:06:59,285
if you are a graduate student, maybe
starting in psychology on neuroscience,

129
00:06:59,385 --> 00:07:03,122
like, hey, I'm interested in whatever
it is, decision making under uncertainty.

130
00:07:03,790 --> 00:07:07,026
And I mean, we have a finite time
in this earth, right?

131
00:07:07,026 --> 00:07:10,196
And a finite amount of time to actually
dedicate into learning things.

132
00:07:10,463 --> 00:07:12,298
And so I think
it would be perfectly rational

133
00:07:12,298 --> 00:07:14,534
if someone to look at active inference
research

134
00:07:14,600 --> 00:07:16,335
like, wow, this is super technical.

135
00:07:16,335 --> 00:07:17,336
I wish I could learn it.

136
00:07:17,336 --> 00:07:19,772
But here's this other thing
where I can go on your match.

137
00:07:19,772 --> 00:07:23,810
I can read something in Bartow
and really totally get up to scratch

138
00:07:23,810 --> 00:07:27,113
and use it my research fairly easily,
and it just wasn't something

139
00:07:28,381 --> 00:07:28,681
really

140
00:07:28,681 --> 00:07:31,484
accessible to help people actually
apply it in their own research.

141
00:07:32,151 --> 00:07:34,187
So that was kind of my yeah, actually.

142
00:07:34,720 --> 00:07:34,987
Yeah.

143
00:07:34,987 --> 00:07:36,756
I mean, I should say,
just as a little background,

144
00:07:36,756 --> 00:07:38,858
you know,
when I wanted to learn this stuff

145
00:07:38,858 --> 00:07:41,961
originally at the end of my postdoc,
before I took my current

146
00:07:42,061 --> 00:07:45,631
faculty position,
and I the only way to do it,

147
00:07:45,798 --> 00:07:48,601
the only way that I could learn
it was actually to go, you know,

148
00:07:48,634 --> 00:07:51,838
hang out at the cell in London
with Carl for about four months

149
00:07:52,839 --> 00:07:56,843
and you know, just ask people
a ton of questions and sit there.

150
00:07:56,843 --> 00:08:02,148
And that was the only way, you know, so
without having to physically go to

151
00:08:02,815 --> 00:08:04,150
one location in the world,

152
00:08:04,150 --> 00:08:06,986
I was very far away from,
you know, my current institution.

153
00:08:07,353 --> 00:08:09,989
And without doing that,
there really was no way for me to do it.

154
00:08:09,989 --> 00:08:14,694
So, I mean, it's sort of out of empathy
for my past self.

155
00:08:15,595 --> 00:08:18,931
But, you know, this is a sort of thing
I wish I would have had available to me

156
00:08:18,931 --> 00:08:20,500
so I could learn
this stuff independently.

157
00:08:21,634 --> 00:08:24,003
Oh, so Max

158
00:08:24,003 --> 00:08:27,940
what is your background or what
got you really excited to work

159
00:08:27,940 --> 00:08:28,774
through the paper

160
00:08:28,774 --> 00:08:33,079
and develop a lot of the examples
out on your own computer and everything

161
00:08:33,079 --> 00:08:35,882
so what got you excited or
where are you coming from today?

162
00:08:36,816 --> 00:08:37,617
Sure.

163
00:08:37,950 --> 00:08:38,184
Yeah.

164
00:08:38,184 --> 00:08:42,555
So maybe I'm kind of your target
demographic in some sense.

165
00:08:42,989 --> 00:08:44,423
I don't know that I could really be

166
00:08:44,423 --> 00:08:47,693
considered a junior investigator,
although I hope I could someday be that.

167
00:08:48,628 --> 00:08:52,565
And I'm interested in
I'm much more on the motorcycle

168
00:08:52,665 --> 00:08:55,801
motor system side of neuroscience
as opposed to the decision making.

169
00:08:55,801 --> 00:09:00,406
So this just the whole lexicon,
it's a little different,

170
00:09:00,573 --> 00:09:03,943
but I am seeing so many common
is when I'm coming

171
00:09:03,943 --> 00:09:08,180
from a signal processing background,
I think of things in terms of calming

172
00:09:08,180 --> 00:09:11,250
formulation and trying to integrate
sensory and motor information.

173
00:09:11,250 --> 00:09:12,752
I see a lot of similarities there.

174
00:09:12,752 --> 00:09:15,021
And so for somebody like myself, then

175
00:09:15,021 --> 00:09:18,925
looking at some of the first ins work
and seeing how he's described, you know,

176
00:09:19,125 --> 00:09:24,063
the anatomy of inference and could this
apply also in motor systems.

177
00:09:25,364 --> 00:09:26,566
I'm very interested to

178
00:09:26,566 --> 00:09:30,369
see how this might apply
in my own future research and work.

179
00:09:31,737 --> 00:09:34,707
For So let's
just say that you were speaking

180
00:09:35,041 --> 00:09:39,345
Ryan first and then Christopher
to that early investigator,

181
00:09:39,378 --> 00:09:44,317
researcher of any age who is like, Okay,
I get it in reinforcement

182
00:09:44,317 --> 00:09:48,120
learning, it's about reward or reward
centric learning is about reward.

183
00:09:48,387 --> 00:09:50,523
What is active inference?

184
00:09:50,523 --> 00:09:54,894
How is it taking a different perspective
on what it is that organisms are doing?

185
00:09:55,094 --> 00:09:59,465
Then that kind of classical accounts
of reward based learning, for example,

186
00:09:59,465 --> 00:10:01,567
or whatever
you'd like to contrast it with,

187
00:10:01,567 --> 00:10:04,770
but what is the bridge
from the kind of assumptions

188
00:10:04,971 --> 00:10:08,341
and implications that lead
somebody to work on that classical

189
00:10:08,341 --> 00:10:11,911
framing versus
what is the difference in thinking

190
00:10:12,044 --> 00:10:14,647
that is active inference
and how is that manifested in the model?

191
00:10:16,248 --> 00:10:17,216
So I think

192
00:10:17,216 --> 00:10:20,486
this is actually a really important thing
to bring up right from the start.

193
00:10:20,486 --> 00:10:23,923
Is that so there's a
there's a kind of big distinction between

194
00:10:24,690 --> 00:10:28,928
when people talk about the free energy
principle broadly or the philosophy

195
00:10:28,928 --> 00:10:31,597
of the free energy principle
and is a very kind of

196
00:10:32,798 --> 00:10:35,001
you know, somewhat wide chasm
between that

197
00:10:35,401 --> 00:10:39,105
and what gets called active inference
nowadays, where active inference,

198
00:10:39,105 --> 00:10:42,775
as a corollary of the free energy
principle, but

199
00:10:43,743 --> 00:10:47,613
you know, active inference as formulated
in terms of partially observable,

200
00:10:47,847 --> 00:10:52,018
partially observable markup decision
processes is quite a bit narrower.

201
00:10:52,051 --> 00:10:52,351
Right?

202
00:10:52,351 --> 00:10:56,222
It's a very specific sort of discrete
state space generative model,

203
00:10:57,657 --> 00:11:00,026
and that has particular
sorts of elements to it.

204
00:11:01,260 --> 00:11:03,629
And it doesn't actually

205
00:11:03,629 --> 00:11:06,632
require knowing
a lot of the things that people talk

206
00:11:06,632 --> 00:11:09,201
about with respect to the free energy
principle more broadly.

207
00:11:09,702 --> 00:11:12,571
And it appeals to free

208
00:11:12,571 --> 00:11:16,942
energy and expected
free energy as functions right as more or

209
00:11:16,942 --> 00:11:20,713
less cost functions for figuring out
what the best choice to make is

210
00:11:22,481 --> 00:11:25,951
and it but but at the end of the day,
I mean, when you apply

211
00:11:25,951 --> 00:11:29,689
these sorts of models to behavioral tasks
like I used in studies

212
00:11:30,723 --> 00:11:32,925
it's actually there's
quite a bit of similarity.

213
00:11:32,925 --> 00:11:35,761
I mean, you can really see
active inference models

214
00:11:36,295 --> 00:11:39,265
or at least the empty formulation
as, as just a

215
00:11:39,265 --> 00:11:42,001
particular is kind of
like what Chris said is just a particular

216
00:11:42,334 --> 00:11:45,805
kind of flavor almost of model based
reinforcement learning,

217
00:11:46,672 --> 00:11:49,442
you know, so instead of
I mean, there's, there's some sort of

218
00:11:49,442 --> 00:11:53,512
technical differences, right,
where the way that sort of Karl

219
00:11:53,512 --> 00:11:58,084
and them have set it up
and what you're doing is kind of like

220
00:11:58,117 --> 00:12:01,387
it allows for kind of a fully unified
Bayesian way

221
00:12:01,854 --> 00:12:04,590
of doing reinforcement learning

222
00:12:04,957 --> 00:12:07,660
and decision making processes.

223
00:12:07,660 --> 00:12:11,597
And you do that
by instead of calling something a reward

224
00:12:11,664 --> 00:12:15,968
or say you define this probability
distribution over observations

225
00:12:17,036 --> 00:12:20,039
that gets
called a preference distribution

226
00:12:20,039 --> 00:12:22,508
and so reward then becomes

227
00:12:23,042 --> 00:12:26,946
a kind of probabilistic preference
to observe some things over others,

228
00:12:28,047 --> 00:12:31,650
you know, so whether you call
a particular observation a reward,

229
00:12:32,685 --> 00:12:36,722
then calling it a reward
just amounts to having a precise

230
00:12:37,690 --> 00:12:39,859
preference distribution
that has a precise

231
00:12:41,627 --> 00:12:44,930
high value over
whatever that rewarding outcome is.

232
00:12:45,898 --> 00:12:49,435
And then the agent is simply driven
to make decisions that it thinks

233
00:12:49,435 --> 00:12:53,372
is most likely to get it to observe
the thing it prefers.

234
00:12:53,873 --> 00:12:54,140
Right.

235
00:12:54,140 --> 00:12:55,775
So this preference distribution is more

236
00:12:55,775 --> 00:12:58,944
or less a way of specifying
what is rewarding to the agent.

237
00:12:58,944 --> 00:13:01,847
Right? What the agent is seeking

238
00:13:01,847 --> 00:13:04,950
One thing that is kind of nice
about active inference

239
00:13:04,950 --> 00:13:08,521
that's a little different
from reinforcement learning per se,

240
00:13:09,588 --> 00:13:10,823
or there's a few things I should say.

241
00:13:10,823 --> 00:13:12,091
One is that

242
00:13:14,560 --> 00:13:16,262
making decisions based on

243
00:13:16,262 --> 00:13:19,198
trying to minimize expected for energy,
which will cover,

244
00:13:20,933 --> 00:13:24,804
doesn't just try to maximize reward.

245
00:13:25,237 --> 00:13:28,674
It also simultaneously tries
to maximize information gain.

246
00:13:29,308 --> 00:13:32,178
And so so what will happen is,

247
00:13:32,878 --> 00:13:33,045
you know,

248
00:13:33,045 --> 00:13:34,180
I say say an agent

249
00:13:34,180 --> 00:13:37,817
starts out in a dark room, you know,
just to take a very kind of contentious,

250
00:13:38,050 --> 00:13:41,787
you know, philosophy of the FP
sort of example, right?

251
00:13:42,154 --> 00:13:44,990
Let's say
an agent starts out in a dark room,

252
00:13:44,990 --> 00:13:47,793
which means that it doesn't know
where anything is.

253
00:13:48,260 --> 00:13:48,561
Right?

254
00:13:48,561 --> 00:13:51,730
So that means that has a lot of
uncertainty over what state it's in

255
00:13:52,431 --> 00:13:55,701
unless they simultaneously
in another room,

256
00:13:55,701 --> 00:13:58,737
there's a fridge with some food in it.

257
00:13:58,904 --> 00:14:01,907
Then there will be two different
sorts of drives there, right?

258
00:14:01,907 --> 00:14:03,709
There'll be this epistemic drive to

259
00:14:03,709 --> 00:14:07,146
do whatever it thinks is going to give it
the most information gain.

260
00:14:07,413 --> 00:14:08,280
Right.

261
00:14:08,414 --> 00:14:10,382
Which will be a go
turn on the light, right?

262
00:14:10,382 --> 00:14:13,018
So then it'll know what state
it's in Right.

263
00:14:13,052 --> 00:14:15,254
Whether it's in room or room B, right.

264
00:14:15,254 --> 00:14:18,591
Or whether there's a couch in the room
or whether there's a TV in

265
00:14:18,591 --> 00:14:19,892
the room or whatever. Right.

266
00:14:20,860 --> 00:14:23,195
And that's not reward per se.

267
00:14:23,562 --> 00:14:26,632
That's just trying to make the app choose
the action

268
00:14:26,632 --> 00:14:28,934
that's going to maximize
the amount of information you get.

269
00:14:30,636 --> 00:14:32,638
The other the other chunk of it, though,

270
00:14:32,638 --> 00:14:35,975
is to maximize
observing preferred outcomes.

271
00:14:36,408 --> 00:14:39,578
And so the agent will also be driven
to just leave the room

272
00:14:39,879 --> 00:14:43,015
to go to the fridge,
you know, to observe itself eating food,

273
00:14:44,550 --> 00:14:44,783
you know.

274
00:14:44,783 --> 00:14:47,820
So in practice, what will happen
that typically won't happen,

275
00:14:47,820 --> 00:14:51,257
at least in a standard reinforcement
learning setting,

276
00:14:51,690 --> 00:14:55,327
is that the agent won't
just try to maximize reward directly.

277
00:14:55,361 --> 00:14:55,961
A lot of times

278
00:14:55,961 --> 00:14:59,899
it will first choose the action
that will help it figure out where it is

279
00:15:00,366 --> 00:15:03,903
so that it's more confident
what to do to get the reward later.

280
00:15:04,270 --> 00:15:09,708
And so so it's
so it's really not again, in practice,

281
00:15:09,708 --> 00:15:14,013
these models are are just a nice fully
Bayesian way to integrate

282
00:15:14,480 --> 00:15:17,182
perception, learning and decision making,

283
00:15:18,350 --> 00:15:19,952
where decision making is driven

284
00:15:19,952 --> 00:15:22,488
to both maximize information
gain and maximize reward.

285
00:15:24,256 --> 00:15:25,457
Christopher, thanks a lot.

286
00:15:25,457 --> 00:15:28,694
Anything else to add there?

287
00:15:28,694 --> 00:15:29,828
No, not particularly.

288
00:15:29,828 --> 00:15:31,263
I think the only thing I would say

289
00:15:31,263 --> 00:15:33,632
and I'm going to say
this in a little bit on slide is

290
00:15:34,033 --> 00:15:35,501
if you're interested
in the technical details,

291
00:15:35,501 --> 00:15:37,836
what Ron just said,
there are two really good papers.

292
00:15:38,537 --> 00:15:40,940
Maybe you can make me slides
available afterwards as well.

293
00:15:41,507 --> 00:15:42,841
One's by and also it.

294
00:15:42,841 --> 00:15:45,544
And I'm not sure if I'm sorry
if I butchered your name.

295
00:15:46,512 --> 00:15:49,448
The other one's Lancelot de Costa,

296
00:15:49,448 --> 00:15:51,383
and they have two excellent papers
comparing it

297
00:15:51,383 --> 00:15:54,320
to reinforcement
learning center formulations.

298
00:15:55,354 --> 00:15:56,388
Excellent.

299
00:15:56,422 --> 00:15:59,425
Just to draw out a couple points, Ryan,
from what you said.

300
00:15:59,425 --> 00:16:03,095
So one is that the active inference

301
00:16:03,696 --> 00:16:08,467
framework and specifically the PEO MDP,
the partially observable

302
00:16:08,500 --> 00:16:13,672
Markov decision process, instantiate
ation of active inference is a corollary

303
00:16:13,939 --> 00:16:16,675
or like a derivative of the free energy
principle.

304
00:16:17,042 --> 00:16:18,544
And there's a lot of philosophy

305
00:16:18,544 --> 00:16:20,779
and contentiousness
around different aspects of that.

306
00:16:20,779 --> 00:16:25,985
We're going to read a paper in Active
Inference Stream 14 by Mel Andrews about

307
00:16:26,418 --> 00:16:29,254
the math is not the territory,
about the philosophy and the status.

308
00:16:29,254 --> 00:16:32,691
So yes, if you want to go that way,
there's a whole rabbit hole there.

309
00:16:32,992 --> 00:16:35,928
But this is kind of like a tool,
like a linear regression.

310
00:16:36,261 --> 00:16:40,199
You don't get caught up in the weeds
on number theory or on the information

311
00:16:40,199 --> 00:16:44,536
space that it's about where
the calculation is useful and tractable.

312
00:16:44,536 --> 00:16:45,437
It's going to be a tool.

313
00:16:45,437 --> 00:16:48,640
So that's kind of where we're coming out,
at least today and in this series.

314
00:16:49,208 --> 00:16:53,278
Another interesting thing,
and in contrast with reinforcement

315
00:16:53,278 --> 00:16:56,315
learning,
which is kind of like reinforcement works

316
00:16:56,548 --> 00:16:59,318
and don't reinforcement
doesn't work, that neurons

317
00:16:59,318 --> 00:17:02,354
that fire together wired together
positive reinforcement schemes.

318
00:17:03,489 --> 00:17:03,722
There

319
00:17:03,722 --> 00:17:06,125
is a reward
preference built into that kind of model.

320
00:17:06,759 --> 00:17:09,895
But when there's a basin of low reward,
it's often

321
00:17:09,895 --> 00:17:11,897
difficult for those models to latch on.

322
00:17:11,897 --> 00:17:14,900
So we saw that in the paper
of Alec Chance at all

323
00:17:15,067 --> 00:17:19,071
in active Infinite Stream eight scaling
active inference that kind of showed

324
00:17:19,071 --> 00:17:23,742
how even up against these very,
very large scale machine learning models

325
00:17:23,742 --> 00:17:26,912
like cue reinforcement learning
and things like that,

326
00:17:27,246 --> 00:17:30,849
other state of the art deep
learning models that the active inference

327
00:17:31,417 --> 00:17:34,620
trolley, car and control theory
robots were able to work

328
00:17:34,620 --> 00:17:38,123
really well because first they did what
you just described, Ryan, which was they

329
00:17:38,490 --> 00:17:43,462
kind of went into an explorer mode before
going into a more fine tuning mode.

330
00:17:43,729 --> 00:17:47,499
And so in doing so, they transcended
the explore exploit simple trade off.

331
00:17:47,499 --> 00:17:51,570
It's not just a knob in this model, it's
not a coefficient to balance, explore

332
00:17:51,570 --> 00:17:52,704
versus exploit,

333
00:17:52,704 --> 00:17:56,475
I hope will draw out an understanding
that they're actually related

334
00:17:56,475 --> 00:17:57,609
in a different way

335
00:17:57,609 --> 00:18:01,113
because there's something that's model
based and generative that's happening.

336
00:18:01,513 --> 00:18:04,349
And so really a ton of interesting stuff.

337
00:18:04,650 --> 00:18:08,954
So at this point, I would think
we could go into the presentations

338
00:18:09,254 --> 00:18:12,991
if people have any questions that arise,
put them in the YouTube chat,

339
00:18:13,025 --> 00:18:15,761
and I'll be copying those out
for addressing them later.

340
00:18:16,228 --> 00:18:19,665
And other than that, let's just work
through the presentations,

341
00:18:19,665 --> 00:18:22,167
however Ryan and Christopher
have set up in.

342
00:18:22,167 --> 00:18:25,037
Perfect. Cool. Okay. Go ahead.

343
00:18:25,037 --> 00:18:27,272
And just very straight for us,
so you can just go ahead.

344
00:18:27,272 --> 00:18:29,041
Yeah, it's going

345
00:18:30,876 --> 00:18:32,277
Okay.

346
00:18:32,277 --> 00:18:33,579
So I'm using a dual monitor set up.

347
00:18:33,579 --> 00:18:35,147
We'll see how this goes.

348
00:18:41,320 --> 00:18:42,020
Okay.

349
00:18:42,020 --> 00:18:42,921
Sorry.

350
00:18:43,322 --> 00:18:44,890
I now open system preferences.

351
00:18:44,890 --> 00:18:48,427
Sorry to allow Microsoft teams to use it.

352
00:18:48,494 --> 00:18:49,628
Sorry about that.

353
00:18:56,068 --> 00:18:57,302
Okay.

354
00:18:58,871 --> 00:19:01,707
Should be with you momentarily.

355
00:19:03,475 --> 00:19:04,143
Cool.

356
00:19:07,646 --> 00:19:09,581
Good to digest.

357
00:19:09,581 --> 00:19:10,082
Really.

358
00:19:10,082 --> 00:19:14,887
It's really it's interesting stuff,
and we're all learning by doing here.

359
00:19:15,187 --> 00:19:18,991
So just really appreciate
and have just seen a lot of appreciation

360
00:19:18,991 --> 00:19:22,861
for this kind of work because
in some ways, it's like the missing piece

361
00:19:23,395 --> 00:19:28,567
between a lot of these hypothetical
or abstract or scaffold models

362
00:19:28,567 --> 00:19:32,171
or what people have heard about
with respect to what active inference is

363
00:19:32,437 --> 00:19:35,607
and then how it's deployed,
how it's enacted, so to take the

364
00:19:35,607 --> 00:19:39,511
an activist insight
seriously as practitioners of science

365
00:19:40,112 --> 00:19:43,916
and as communicators as well,
it means making this kind of work

366
00:19:43,916 --> 00:19:48,787
and showing us how you do it
as we all are trying to figure out

367
00:19:48,787 --> 00:19:53,025
every level,
from literally the tech to the best way

368
00:19:53,025 --> 00:19:57,496
to make it rigorous and accessible,
the best ways to talk about it.

369
00:19:57,729 --> 00:20:00,199
So, yeah, well,
what do you think about that? Right

370
00:20:02,234 --> 00:20:02,401
Yeah.

371
00:20:02,401 --> 00:20:04,436
I mean, I certainly agree.
Hold on to Chris.

372
00:20:04,937 --> 00:20:07,873
Yeah, actually, I've never used

373
00:20:07,873 --> 00:20:08,540
this, although.

374
00:20:08,540 --> 00:20:10,842
There should be a little thing
in the upper right corner.

375
00:20:10,876 --> 00:20:14,146
It's like an arrow pointing into it,
into like a square.

376
00:20:16,515 --> 00:20:18,150
Share content.

377
00:20:21,453 --> 00:20:23,889
You guys both on PCs?

378
00:20:24,122 --> 00:20:25,724
Yeah, yeah.

379
00:20:25,724 --> 00:20:26,425
Okay.

380
00:20:27,392 --> 00:20:29,895
So, yeah, so it should be upper right
corner, like right

381
00:20:29,895 --> 00:20:32,965
next to the leave button.

382
00:20:33,799 --> 00:20:35,000
Okay.

383
00:20:36,368 --> 00:20:38,971
Or Ryan, you could present first or.

384
00:20:39,471 --> 00:20:40,305
Yeah.

385
00:20:41,640 --> 00:20:43,709
I actually, I have a question.

386
00:20:44,243 --> 00:20:45,244
For me to present first.

387
00:20:45,244 --> 00:20:46,778
I think I think you're, you know.

388
00:20:46,778 --> 00:20:49,881
Yeah, what I'm going to do, I'm not going
to be so comprehensible for Chris.

389
00:20:49,915 --> 00:20:50,282
Got it.

390
00:20:50,282 --> 00:20:52,551
I actually have a question
that will take a couple of minutes.

391
00:20:52,951 --> 00:20:54,086
Christopher, what you're figuring out

392
00:20:54,086 --> 00:20:57,956
is what tools
would somebody need to follow along?

393
00:20:57,990 --> 00:21:02,928
So what programing, language
or what interface do they need?

394
00:21:03,295 --> 00:21:05,697
What background knowledge do they need?

395
00:21:06,198 --> 00:21:07,733
Is there a software to download?

396
00:21:07,733 --> 00:21:09,001
Is there a course?

397
00:21:09,001 --> 00:21:11,703
What are the prerequisites
for doing what?

398
00:21:11,703 --> 00:21:13,505
We're about to basically jump into

399
00:21:15,240 --> 00:21:15,607
Yeah.

400
00:21:15,607 --> 00:21:18,844
So I mean, I'll just say that, that,
you know, a lot of this stuff

401
00:21:18,844 --> 00:21:21,580
was originally designed by Carl Thurston.

402
00:21:22,714 --> 00:21:27,753
And if you know a lot about Carl,
you know, before he proposed a lot

403
00:21:27,753 --> 00:21:31,623
of the sort of inference stuff,
you know, he was sort of most

404
00:21:33,058 --> 00:21:35,060
famous or his his biggest contribution

405
00:21:35,060 --> 00:21:38,430
was putting together a ZPM within MATLAB.

406
00:21:38,997 --> 00:21:43,869
SPM is like a it's called statistical
parametric mapping software.

407
00:21:43,869 --> 00:21:48,940
But it was more or less
the original way of doing F MRI.

408
00:21:49,041 --> 00:21:52,244
So like analyzing functional,
like neuroimaging data

409
00:21:53,345 --> 00:21:57,683
and so a lot of the scripts
and dynamic puzzle modeling,

410
00:21:57,683 --> 00:22:00,385
I should say, which is a particular
sort of approach to doing or imaging.

411
00:22:01,086 --> 00:22:04,690
And so a lot of basically
all the standard resources,

412
00:22:04,690 --> 00:22:07,959
all the standard coding routines,
everything for actually building

413
00:22:08,026 --> 00:22:11,830
active inference models right now
is primarily an SPM,

414
00:22:12,164 --> 00:22:14,433
which runs within MATLAB

415
00:22:15,000 --> 00:22:18,603
and so and all the kind of supplementary

416
00:22:18,870 --> 00:22:24,242
tutorial code that we've provided,
we provide I think 66 or seven different

417
00:22:25,310 --> 00:22:28,180
different supplementary scripts
where you can actually run

418
00:22:28,180 --> 00:22:30,282
the simulations
and build these models yourself.

419
00:22:31,850 --> 00:22:34,252
They are
they are all they're all in MATLAB.

420
00:22:35,687 --> 00:22:37,789
So if you, if you have MATLAB

421
00:22:37,789 --> 00:22:39,791
and you need to have downloaded SPM

422
00:22:40,826 --> 00:22:42,894
and with MATLAB in SPM, you

423
00:22:43,762 --> 00:22:47,899
you will have everything you need to do
to open our supplementary code

424
00:22:48,400 --> 00:22:53,105
and to and to follow along to a lot of
the tutorial is actually set up.

425
00:22:53,105 --> 00:22:58,643
So it assumes that you kind of have
the paper open side by side with the code

426
00:22:59,111 --> 00:23:02,481
so you can actually click on,
you know, different options

427
00:23:02,481 --> 00:23:06,017
to simulate and reproduce all the figures
in the in the tutorial paper.

428
00:23:07,419 --> 00:23:08,787
So it's yeah.

429
00:23:08,787 --> 00:23:11,289
So the short answer is
you need MATLAB and you need SPM

430
00:23:12,657 --> 00:23:13,492
things for

431
00:23:13,525 --> 00:23:17,229
you to have some minimal ability to work

432
00:23:17,229 --> 00:23:20,499
with with matlab.

433
00:23:20,499 --> 00:23:23,502
Just to just to piggyback on to that
for anybody's

434
00:23:23,869 --> 00:23:26,371
benefit watching this in the future,
whenever you're watching this,

435
00:23:27,773 --> 00:23:31,209
once you get the spam folder,
you know, you'll unzip

436
00:23:31,209 --> 00:23:34,146
that, you'll get that folder, you'll
put that somewhere on your computer.

437
00:23:34,379 --> 00:23:36,648
You want to make sure that you add path

438
00:23:37,182 --> 00:23:40,552
from your MATLAB workspace when you're
in the supplementary code provided

439
00:23:41,653 --> 00:23:43,889
by Ryan and Christopher.

440
00:23:45,190 --> 00:23:46,892
When you're in that workspace,

441
00:23:46,892 --> 00:23:50,429
add path of that folder
wherever you put it, add path

442
00:23:50,762 --> 00:23:55,634
of that folder slash toolbox slash DPM

443
00:23:56,134 --> 00:23:59,070
and then you should have access
to the additional SPM.

444
00:23:59,070 --> 00:24:03,141
12 scripts that you need in order to make
use of the functions they've provided.

445
00:24:03,942 --> 00:24:05,110
Yeah. Yeah.

446
00:24:05,110 --> 00:24:07,245
Thank you very much. I mean, that's that.

447
00:24:07,245 --> 00:24:08,780
Yeah.
For someone who has never used MATLAB.

448
00:24:08,780 --> 00:24:12,017
Yes, there's a few steps like that
which are super important.

449
00:24:13,351 --> 00:24:13,618
Yeah.

450
00:24:13,618 --> 00:24:16,154
Because basically all these scripts
are actually within. Yeah.

451
00:24:16,154 --> 00:24:19,024
Within the toolbox

452
00:24:20,225 --> 00:24:22,594
of SPM

453
00:24:22,594 --> 00:24:24,663
and all the scripts they've provided.

454
00:24:25,630 --> 00:24:27,332
Call on

455
00:24:28,500 --> 00:24:30,368
sub functions

456
00:24:30,368 --> 00:24:32,671
within that are within SPM

457
00:24:32,671 --> 00:24:36,074
and so, so yeah, it won't unfortunately

458
00:24:36,775 --> 00:24:39,544
none of them will work without without us
SPM at the moment,

459
00:24:40,111 --> 00:24:42,280
although there are certainly efforts
to try to make

460
00:24:43,648 --> 00:24:46,218
some of these routines
more sort of generally accessible and

461
00:24:46,218 --> 00:24:51,122
like free software like alec electrons,
for example, is in the process of

462
00:24:51,423 --> 00:24:56,061
I think writing a like
a Python version of the, of the main

463
00:24:56,728 --> 00:24:59,664
act of inference MGP script,
which is called the,

464
00:25:00,131 --> 00:25:01,566
it's called a

465
00:25:02,968 --> 00:25:06,071
SPM underscore, MGP
underscore, VB underscore X

466
00:25:06,571 --> 00:25:09,274
and that just correspond that just MDP

467
00:25:09,274 --> 00:25:13,178
underscore, VB underscore x corresponds
to a markup decision process underscore

468
00:25:13,178 --> 00:25:17,182
Variational Bayes underscore
X which stands for fact arised.

469
00:25:18,016 --> 00:25:22,687
And I'll, I'll go over what factorization
is and how you actually code that end.

470
00:25:23,054 --> 00:25:24,890
Once we get to my section

471
00:25:27,192 --> 00:25:27,526
okay.

472
00:25:27,526 --> 00:25:28,994
I'm going to ask a follow up there.

473
00:25:28,994 --> 00:25:33,231
So I have my desktop computer,
I have MATLAB

474
00:25:33,231 --> 00:25:38,103
and it's all working with referencing
SPM most updated version.

475
00:25:38,537 --> 00:25:42,240
And I've run through the examples
that we're going

476
00:25:42,240 --> 00:25:45,277
to be working through maybe today
or the ones that are in the paper.

477
00:25:45,310 --> 00:25:47,612
So I kind of hit Enter
and I got things to work.

478
00:25:47,612 --> 00:25:50,715
Everything's
all the scripts are working fine.

479
00:25:51,850 --> 00:25:55,287
What information from my system
do I need to know?

480
00:25:55,320 --> 00:26:00,325
Like what measurements are going to be
relevant for me to bring to the table

481
00:26:00,692 --> 00:26:03,728
if I'm going to do a t test
for whether to and colonies

482
00:26:03,728 --> 00:26:07,032
are a different size, you know,
I know what kind of machinery I'm

483
00:26:07,032 --> 00:26:10,769
going to need, you know, two columns
in the Excel spreadsheet or something,

484
00:26:11,002 --> 00:26:14,606
but what kind of data or what
attributes of the system

485
00:26:15,006 --> 00:26:18,743
are important
to bring to the table for an investigator

486
00:26:18,743 --> 00:26:21,479
to utilize the examples
that you're providing

487
00:26:22,447 --> 00:26:23,815
and yes.

488
00:26:23,815 --> 00:26:25,684
So I mean,
so I mean, there's a couple of things.

489
00:26:25,684 --> 00:26:27,786
I mean, so so first the

490
00:26:29,187 --> 00:26:30,789
you know, so the first goal is, you know,

491
00:26:30,789 --> 00:26:35,961
you take a particular behavioral task
and you have to figure out

492
00:26:36,061 --> 00:26:39,931
what the right generative model
is for that task.

493
00:26:40,999 --> 00:26:43,068
And this is the sort of thing
that I'll give an example of.

494
00:26:43,068 --> 00:26:46,137
I can actually show you
multiple examples of this.

495
00:26:46,137 --> 00:26:49,040
And so once you have this
or generative model set up,

496
00:26:49,541 --> 00:26:52,711
then you can run simulations
and those simulations will generate

497
00:26:53,244 --> 00:26:55,547
observations and generate

498
00:26:57,015 --> 00:26:59,150
expected actions.

499
00:26:59,150 --> 00:27:03,188
And so so what you need then is you
need data

500
00:27:03,655 --> 00:27:06,224
from an actual participant performing
that task

501
00:27:06,658 --> 00:27:11,663
where you'll know on each trial what
the person observed, write what the task

502
00:27:11,663 --> 00:27:15,433
stimuli were and you'll know what
action they actually chose.

503
00:27:17,135 --> 00:27:17,535
And so

504
00:27:17,535 --> 00:27:22,240
then what you'll do ultimately is
you'll do what's called

505
00:27:22,240 --> 00:27:26,611
parameter estimation, which is and again,
this will be a lot more comprehensible

506
00:27:26,911 --> 00:27:28,279
once I actually show you.

507
00:27:28,279 --> 00:27:32,317
But basically what you try to do
is you try to find the set

508
00:27:32,317 --> 00:27:35,887
of parameters values in the model
that generates behavior

509
00:27:35,887 --> 00:27:39,190
that's most similar
to what the participant actually did

510
00:27:41,059 --> 00:27:45,030
and so once you define the parameters,
you say one has a value of two

511
00:27:45,030 --> 00:27:48,400
and the other has a value of four,
and that generates behavior

512
00:27:48,400 --> 00:27:50,935
that's identical to whatever
a given participant did.

513
00:27:51,536 --> 00:27:55,507
Then those values of two and four,
you can use those as individual

514
00:27:55,507 --> 00:27:57,108
difference estimates,

515
00:27:57,108 --> 00:27:57,475
you know,

516
00:27:57,475 --> 00:27:58,710
so if you have those values

517
00:27:58,710 --> 00:28:02,013
for a bunch of different people who
behaved in different ways on the task,

518
00:28:02,480 --> 00:28:05,183
then you could say, hey, you know,
is there something different,

519
00:28:05,283 --> 00:28:06,151
right, about the people

520
00:28:06,151 --> 00:28:09,721
that have a parameter value of four
versus a parameter value of eight?

521
00:28:10,221 --> 00:28:10,989
Right.

522
00:28:10,989 --> 00:28:15,160
Can I use that to predict, for example,
you know, how well somebody is going

523
00:28:15,160 --> 00:28:18,930
to respond to a particular treatment,
say, in computational psychiatry?

524
00:28:23,702 --> 00:28:25,837
I curse twice. Yes.

525
00:28:27,739 --> 00:28:28,640
It's an.

526
00:28:30,975 --> 00:28:31,342
Yeah.

527
00:28:31,342 --> 00:28:32,110
But so.

528
00:28:32,110 --> 00:28:34,312
But so
but so that's that's kind of the idea.

529
00:28:34,345 --> 00:28:37,549
And, you know, and I mean, I can
I can share my screen

530
00:28:37,549 --> 00:28:40,618
in the meantime and I don't know.

531
00:28:40,652 --> 00:28:43,488
Well, here I can
I can perhaps go through some of this.

532
00:28:44,055 --> 00:28:46,257
So I'll just share my screen
until I'm good.

533
00:28:46,424 --> 00:28:47,325
Yeah. Sorry.

534
00:28:47,325 --> 00:28:51,062
That was this whole thing was.

535
00:28:51,262 --> 00:28:52,731
Oh, you're good.

536
00:28:52,731 --> 00:28:53,898
I think I am, at least.

537
00:28:53,898 --> 00:28:56,701
Can you guys see my screen?
I it's perfect.

538
00:28:57,602 --> 00:28:58,903
Right? Yeah. Okay, great.

539
00:28:58,903 --> 00:28:59,370
Sorry about that.

540
00:28:59,370 --> 00:29:01,406
There are some,
a whole bunch of privacy issues.

541
00:29:02,107 --> 00:29:04,576
I had to cancel the passwords.

542
00:29:04,576 --> 00:29:07,412
Passwords anyway. Okay, we are here.

543
00:29:08,079 --> 00:29:10,515
So it's

544
00:29:16,621 --> 00:29:17,021
okay.

545
00:29:17,021 --> 00:29:19,524
So you guys saying my present of you,
are you saying the full thing.

546
00:29:20,425 --> 00:29:22,727
Looks we see the we see the full site.

547
00:29:22,727 --> 00:29:26,698
Yeah, just this little Okay,
so this is just kind of part one.

548
00:29:27,232 --> 00:29:29,734
After I've done gone through this,

549
00:29:29,734 --> 00:29:32,670
Ryan is going to take over and go through
some of the practical aspects of it.

550
00:29:34,305 --> 00:29:36,708
And so then just kind of reiterate
the scope and purpose.

551
00:29:37,242 --> 00:29:39,210
Our target
audience really is kind of researchers

552
00:29:39,210 --> 00:29:42,280
in neuroscience and psychology
that don't have a strong kind

553
00:29:42,280 --> 00:29:43,414
of quantitative background

554
00:29:43,414 --> 00:29:46,151
in math or email and in particular,
early career researchers.

555
00:29:46,651 --> 00:29:49,087
And we just really want to provide people
with the requisite background

556
00:29:50,622 --> 00:29:54,058
to actually apply this in the context
during a search okay.

557
00:29:54,092 --> 00:29:57,529
And then so just to quickly highlight
some really fantastic other resources,

558
00:29:58,663 --> 00:30:01,332
the first papers by Lancelot to Costa,
which is an incredible

559
00:30:01,332 --> 00:30:03,935
technical review, just came out
mathematical psychology.

560
00:30:04,502 --> 00:30:07,305
No, it has a really other incredible
paper.

561
00:30:07,305 --> 00:30:09,107
This came out in neural computation.

562
00:30:09,107 --> 00:30:11,376
And I think the

563
00:30:11,509 --> 00:30:15,280
comparison to dynamic programing, element

564
00:30:15,280 --> 00:30:18,716
formulations is still in preprints if,
if that's right or wrong.

565
00:30:19,584 --> 00:30:21,753
But as well,
there's some really phenomenal

566
00:30:22,320 --> 00:30:25,723
informal tutorials
with Oh excellent Chuck,

567
00:30:25,757 --> 00:30:28,827
I'm sorry if I put you your name,
but anyway, that fantastic

568
00:30:29,327 --> 00:30:32,997
kind of informal tutorials on Medium
and then lastly is kind

569
00:30:32,997 --> 00:30:36,234
of the closest thing to what we're doing
today is these lectures by

570
00:30:37,168 --> 00:30:39,971
Philip Shorten
back in the computational psychiatry

571
00:30:41,739 --> 00:30:42,707
some school.

572
00:30:42,707 --> 00:30:45,043
And these really are fantastic.

573
00:30:45,343 --> 00:30:47,512
The only difference is that they work
with, to my knowledge,

574
00:30:47,512 --> 00:30:49,614
I think they work at the UN Factories
MDP scheme,

575
00:30:49,614 --> 00:30:54,953
which is a little less flexible
than where working with IRC.

576
00:30:55,220 --> 00:30:59,190
And so there are a lot of ways
of motivating active inference.

577
00:30:59,824 --> 00:31:02,160
I think kind of the way
that's most intuitive towards people

578
00:31:02,160 --> 00:31:03,561
in cognitive science or backgrounds

579
00:31:03,561 --> 00:31:06,698
in cognitive science is to kind of stop
and aspect of the amazing brain.

580
00:31:07,599 --> 00:31:10,702
And so interrupting
if anything is kind of moving

581
00:31:10,702 --> 00:31:12,570
too quickly
or too slowly or anything like that.

582
00:31:12,570 --> 00:31:16,341
But broadly speaking,
the idea is, is that the brain encodes

583
00:31:16,341 --> 00:31:18,209
a generative model of the environment

584
00:31:18,209 --> 00:31:21,212
where this generative model is just kind
of a joint probability distribution.

585
00:31:21,846 --> 00:31:25,516
And then outside or in the outside world,

586
00:31:25,516 --> 00:31:30,421
is it where there are states and observe
and states give rise to observations

587
00:31:30,622 --> 00:31:34,826
and we use the generative model
in combination with Bayesian

588
00:31:34,826 --> 00:31:37,996
updating to insert the hidden causes

589
00:31:37,996 --> 00:31:41,466
of our sensations from the observations.

590
00:31:41,933 --> 00:31:46,304
And then we take actions based upon
our kind of internal model of the world

591
00:31:46,471 --> 00:31:50,041
that couples us back to the generative
process that is that makes sense.

592
00:31:50,341 --> 00:31:53,678
And so there's this kind of perception
action, a loop that's always going on.

593
00:31:55,113 --> 00:31:56,080
Across, I think I think you

594
00:31:56,080 --> 00:32:00,385
might want to tell people
just what the peo karma pie is.

595
00:32:00,451 --> 00:32:03,288
All right. Okay. Yeah. So okay.

596
00:32:03,288 --> 00:32:06,424
So that is a joint probability
distribution over observations,

597
00:32:06,424 --> 00:32:11,462
states and policies where policies
and just actions or sequences of actions

598
00:32:12,163 --> 00:32:14,432
that are available
to kind of all of the actions

599
00:32:14,432 --> 00:32:16,734
that are afforded
to the agents, essentially

600
00:32:18,670 --> 00:32:19,370
okay.

601
00:32:20,371 --> 00:32:23,942
And so then kind of just we start
with like a review to illustrate

602
00:32:23,942 --> 00:32:26,311
this will just give like a really,
really basic example.

603
00:32:27,011 --> 00:32:29,580
And so you might be imagine
being presented

604
00:32:29,847 --> 00:32:32,317
with kind of a shadowy shape like this.

605
00:32:32,850 --> 00:32:35,820
And then you might
and what you want to do is infer

606
00:32:35,820 --> 00:32:39,157
the causes of that shape based
just based upon

607
00:32:40,425 --> 00:32:41,592
the observations.

608
00:32:41,592 --> 00:32:43,261
But you don't just have observations,
right?

609
00:32:43,261 --> 00:32:45,530
You have some prior
knowledge about the world.

610
00:32:46,731 --> 00:32:48,433
And so we can specify that here.

611
00:32:48,433 --> 00:32:53,071
So we might say we are two possible
causes in this very limited example.

612
00:32:53,071 --> 00:32:54,272
That is,

613
00:32:54,272 --> 00:32:58,876
we might say that the shadow is caused
by convex surface or a concave surface,

614
00:32:59,243 --> 00:33:01,846
and they have fairly similar
probabilities.

615
00:33:01,846 --> 00:33:05,183
However, we also have the structural
prior that was kind of acquired

616
00:33:05,183 --> 00:33:07,919
through a lifetime of experience
of just light emanating from above

617
00:33:08,519 --> 00:33:12,056
and under kind of a structural PRI
that emanates from above

618
00:33:12,690 --> 00:33:15,994
the likelihood of the observation
of that particular shadow

619
00:33:16,694 --> 00:33:19,664
of conditional of being concave
is much, much higher

620
00:33:21,899 --> 00:33:22,400
okay.

621
00:33:22,400 --> 00:33:25,703
And so then we can then combine the two,
the prior and the likelihood

622
00:33:27,038 --> 00:33:29,140
to give us a joint distribution

623
00:33:29,140 --> 00:33:32,310
where this is just kind of conditional
on a specific observation

624
00:33:33,344 --> 00:33:34,045
we can

625
00:33:34,045 --> 00:33:38,583
and then with some over
the states in that likelihood

626
00:33:39,183 --> 00:33:41,786
to give us our marginal likelihood

627
00:33:42,420 --> 00:33:45,356
and then divide our joint distribution
or a generative model

628
00:33:45,523 --> 00:33:49,127
by our model likelihood,
and that's called model inversion

629
00:33:49,127 --> 00:33:51,529
or is often
colloquially called model innovation.

630
00:33:53,297 --> 00:33:55,500
And from there
we get our posterior distribution.

631
00:33:55,533 --> 00:33:59,270
This is the probability of states
conditional on observations.

632
00:33:59,537 --> 00:34:02,440
And so we started with a prior
and the likelihood

633
00:34:02,840 --> 00:34:05,510
and got to a posterior
through Bayes rule.

634
00:34:07,412 --> 00:34:09,947
And that is kind of formally speaking
the optimal

635
00:34:11,015 --> 00:34:13,317
way to infer

636
00:34:13,317 --> 00:34:16,220
the probability of hidden causes
given observations.

637
00:34:16,754 --> 00:34:18,656
And the complication to all this

638
00:34:18,656 --> 00:34:19,824
is that the marginal likelihood

639
00:34:19,824 --> 00:34:22,560
is generally speaking,
computationally intractable.

640
00:34:22,560 --> 00:34:26,297
So in the case of these
very simple discrete distributions,

641
00:34:26,297 --> 00:34:27,265
the number of sums

642
00:34:27,265 --> 00:34:29,400
that you have to perform scales
exponentially

643
00:34:29,400 --> 00:34:31,002
with the size of your hypothesis base.

644
00:34:31,002 --> 00:34:32,503
That's extremely impractical

645
00:34:33,971 --> 00:34:37,175
and kind of more realistically
when you're working with

646
00:34:37,175 --> 00:34:40,812
in continuous databases,
if you have non Gaussian or nonlinear

647
00:34:41,846 --> 00:34:45,216
signals there, just analytically,
the distributions is analytically

648
00:34:45,249 --> 00:34:46,250
tractable.

649
00:34:46,451 --> 00:34:49,854
So I think you think, Max,
that a question yeah. Yes.

650
00:34:49,887 --> 00:34:55,760
I guess real quick, just just to really
explicitly touch on the idea of hidden

651
00:34:55,760 --> 00:34:59,230
hidden states and hidden variables
in this particular context,

652
00:35:00,031 --> 00:35:03,134
would it be fair to say
that it's not only the convexity

653
00:35:03,134 --> 00:35:07,772
or concavity of the potential,
you know, thing that I'm looking at,

654
00:35:07,772 --> 00:35:10,975
but also I could think of the light
source, the direction

655
00:35:10,975 --> 00:35:13,544
from where the light source
is as being another hidden variable

656
00:35:13,678 --> 00:35:15,079
and those things together.

657
00:35:15,079 --> 00:35:18,716
Would it be fair to term those as under
my mark of blanket, or is that

658
00:35:19,283 --> 00:35:21,752
getting a little too far out from where
we want to be talking right now?

659
00:35:21,953 --> 00:35:22,153
Yeah.

660
00:35:22,153 --> 00:35:26,057
So Markov Markov blankets don't really
the kind of mark a blanket concept here

661
00:35:26,057 --> 00:35:29,494
doesn't really come in in
any really interesting way.

662
00:35:29,527 --> 00:35:32,230
I mean, you can formulate it that way,
but really the idea is just

663
00:35:32,730 --> 00:35:34,365
you have this prior expectation.

664
00:35:34,365 --> 00:35:37,568
So when you're looking at the little gray
desk right there under observation,

665
00:35:38,035 --> 00:35:40,738
most people when you look at it,
even though it's actually just

666
00:35:40,738 --> 00:35:44,075
a flat shape with some, you know, it's
just a flat two dimensional thing

667
00:35:44,075 --> 00:35:45,710
that has a little bit of darkness

668
00:35:45,710 --> 00:35:49,380
in one place, a little bit of lightness
and another and you know, if

669
00:35:49,680 --> 00:35:55,520
most people see that as being concave,
even though it's a flat 2D thing, right.

670
00:35:55,586 --> 00:35:59,524
So the idea is, is just that
there's a reason, right?

671
00:35:59,557 --> 00:36:01,659
Why people typically see it as

672
00:36:01,659 --> 00:36:05,029
being concave is it's kind of like
popping in as opposed to out.

673
00:36:05,463 --> 00:36:09,233
And that's specifically
because there's this prior belief that

674
00:36:10,334 --> 00:36:12,770
that light
sources tend to come from above.

675
00:36:13,571 --> 00:36:16,674
So the hidden states here are kind of

676
00:36:16,674 --> 00:36:20,811
there is a joint in the likelihood,
which just means that there's a

677
00:36:21,312 --> 00:36:25,883
in the upright where it says likelihood
there's a concave coma light from above.

678
00:36:26,317 --> 00:36:26,584
Right.

679
00:36:26,584 --> 00:36:28,719
So those are two different hidden states.

680
00:36:29,220 --> 00:36:31,622
And what it's saying is

681
00:36:31,622 --> 00:36:35,459
if the hidden states are concave
and light from above,

682
00:36:35,459 --> 00:36:38,296
if the combination of those things
is what's generating

683
00:36:38,729 --> 00:36:41,065
the what I'm
seeing with that gray disk thing

684
00:36:41,699 --> 00:36:44,802
and the probability of the shadow shape

685
00:36:45,203 --> 00:36:48,806
of the shadow pattern
that you see is 0.9,

686
00:36:49,207 --> 00:36:52,009
whereas if it was convex
and light from above,

687
00:36:52,009 --> 00:36:55,112
if those were the two hidden states,
then the probability

688
00:36:55,146 --> 00:36:59,417
you get that same that same shadow
pattern is only point one, right.

689
00:36:59,450 --> 00:37:03,854
In other words, it'd be really hard
to get that pattern of shading

690
00:37:04,288 --> 00:37:06,357
if if the thing was popping out

691
00:37:06,857 --> 00:37:09,827
under the assumption
that where it's coming from above.

692
00:37:11,028 --> 00:37:11,462
So so

693
00:37:11,462 --> 00:37:15,466
the observation here is shadow
and the hidden states are concave and

694
00:37:15,499 --> 00:37:19,070
light from a concave or convex and light
from above and light from below.

695
00:37:19,637 --> 00:37:21,205
Okay. Thanks.

696
00:37:21,205 --> 00:37:22,773
Yeah. Just one other quick example.

697
00:37:22,773 --> 00:37:27,478
There would be like the chessboard
with a shadow and a dark square.

698
00:37:27,478 --> 00:37:29,914
It's perceived
under a deep cultural prior.

699
00:37:29,914 --> 00:37:32,917
That chessboard is
a regular alternating grids.

700
00:37:33,251 --> 00:37:37,021
It makes an ambiguous,
shaded square appearance, something

701
00:37:37,021 --> 00:37:39,023
that's quite different than it
is. People can look that up.

702
00:37:39,390 --> 00:37:44,228
And then another thing this reminds me of
is like using a T test to test

703
00:37:44,228 --> 00:37:48,933
whether a group, a versus B is taller
or something like that, it assumes.

704
00:37:49,100 --> 00:37:52,370
But it could be an assumption
that could be slightly bent.

705
00:37:52,403 --> 00:37:55,439
Is it assumed that other
than the variable being considered

706
00:37:55,740 --> 00:37:58,476
other things are sort of fixed
or not mattering?

707
00:37:58,476 --> 00:38:01,112
Otherwise, your statistical test
is basically misleading

708
00:38:01,412 --> 00:38:02,647
because it could be capturing some

709
00:38:02,647 --> 00:38:06,183
totally confounding variable
in the framework you put it in.

710
00:38:06,751 --> 00:38:10,755
And so whenever you're talking
about a real biological organism,

711
00:38:11,055 --> 00:38:13,758
you're talking
about conditioning on certain things,

712
00:38:14,325 --> 00:38:18,429
and then given what is
totally conditioned out of the picture.

713
00:38:18,729 --> 00:38:20,097
So I kind of see

714
00:38:20,097 --> 00:38:23,868
where you're going with the blanket,
but it's not specifically in this place.

715
00:38:23,868 --> 00:38:25,202
But good question.

716
00:38:25,202 --> 00:38:28,472
But it's like depending on the
irrelevancies that we can condition out,

717
00:38:28,839 --> 00:38:32,109
then we're looking at the conditional
relationships between different

718
00:38:32,410 --> 00:38:34,979
observed states versus the hidden things.

719
00:38:35,212 --> 00:38:36,747
So cool stuff.

720
00:38:36,747 --> 00:38:40,051
And again, in the live chat,
people can post any questions on it.

721
00:38:40,351 --> 00:38:42,286
Continue, Christopher.

722
00:38:42,286 --> 00:38:43,688
Yeah, thank you.

723
00:38:43,688 --> 00:38:45,756
Yeah. Thanks for covering that. Right.

724
00:38:45,756 --> 00:38:49,126
Um, okay, so then the idea

725
00:38:49,126 --> 00:38:52,229
is, is that the margin of likelihood
is generally computationally intractable.

726
00:38:53,331 --> 00:38:53,698
That kind

727
00:38:53,698 --> 00:38:56,100
of barring some ideas
from statistical physics,

728
00:38:56,801 --> 00:38:59,670
what you can do is actually use
some approximation techniques.

729
00:38:59,670 --> 00:39:02,606
And instead of evaluating the margin
of likelihood directly,

730
00:39:02,940 --> 00:39:05,776
we can evaluate something
that is always provably greater

731
00:39:05,776 --> 00:39:08,612
than or equal to the margin of likelihood

732
00:39:09,180 --> 00:39:11,582
and simply come through that right now.

733
00:39:12,350 --> 00:39:12,950
Okay.

734
00:39:12,950 --> 00:39:15,986
And so we're just going to take the logs
kind of for mathematical convenience.

735
00:39:16,020 --> 00:39:20,324
The reason we always work with logs
or generally speaking, is because log

736
00:39:21,225 --> 00:39:22,693
log algebra is just a lot easier

737
00:39:22,693 --> 00:39:24,995
just because it turns
multiplication ones into arithmetic.

738
00:39:26,130 --> 00:39:28,699
So things, it's just easier to work
with essentially.

739
00:39:28,899 --> 00:39:32,203
So we'll take the negative logs
of our marginal likelihood.

740
00:39:32,970 --> 00:39:35,239
This is also sometimes called
Bayesian model evidence.

741
00:39:35,239 --> 00:39:38,309
And when we have a negative log
of the probability, it's also called

742
00:39:38,309 --> 00:39:41,579
surprise and information theory,
which is kind of the terminology.

743
00:39:41,579 --> 00:39:44,181
I'm going to use
the rest of this presentation

744
00:39:44,181 --> 00:39:47,852
And so you can see on the right hand
side of the equality, this is kind of to

745
00:39:47,852 --> 00:39:51,689
some rule of probability we can get back
our surprised by the summing over

746
00:39:51,689 --> 00:39:56,093
kind of all of the states under our joint
distribution over states and outcomes.

747
00:39:57,161 --> 00:39:59,897
And then we can
kind of do a little bit of a trick here.

748
00:40:00,164 --> 00:40:05,603
So I'm going to do now or show you is
that we are going to multiply this joint

749
00:40:05,603 --> 00:40:09,306
distribution or generative model
by some arbitrary distribution.

750
00:40:09,607 --> 00:40:12,410
We multiply and divide
by the same distribution

751
00:40:12,743 --> 00:40:14,044
and so nothing has changed here.

752
00:40:14,044 --> 00:40:15,780
This quality still holds.

753
00:40:15,780 --> 00:40:18,983
I could cancel this if we wanted to,
but I don't want to.

754
00:40:19,984 --> 00:40:22,653
And that's in particular
because I want to take advantage

755
00:40:22,653 --> 00:40:27,892
of something called Jensen's inequality,
and that is that's a result showing

756
00:40:27,892 --> 00:40:30,461
at the expectation of a logarithm
is always

757
00:40:31,128 --> 00:40:34,198
less than or equal
to the logarithm of an expectation.

758
00:40:35,332 --> 00:40:39,003
And so the idea is, is that this sum here

759
00:40:39,370 --> 00:40:43,207
where the logarithm is inside
this summation we're sorry.

760
00:40:43,207 --> 00:40:45,009
So we're taking with summing over

761
00:40:46,610 --> 00:40:47,745
this kind of difference

762
00:40:47,745 --> 00:40:52,450
between a generative model
and this approximate target distribution

763
00:40:53,484 --> 00:40:57,888
when the logarithm is inside
that it is always going to be

764
00:40:57,888 --> 00:41:00,057
and we're in kind of negative territory,

765
00:41:00,057 --> 00:41:03,594
it's always going to be greater
than this quantity on the left hand side

766
00:41:03,961 --> 00:41:08,966
where this left hand quantity is equal
to surprise because the log is outside

767
00:41:08,966 --> 00:41:12,303
the sum, we can cancel these
kind of approximate distributions right.

768
00:41:12,303 --> 00:41:14,572
And end up back at surprise.

769
00:41:14,805 --> 00:41:17,107
On the right hand side, we can't do that.

770
00:41:17,107 --> 00:41:21,579
This right hand side will only be equal
to surprise when our generative

771
00:41:21,579 --> 00:41:25,349
distribution perfectly matches
our approximate distribution.

772
00:41:26,183 --> 00:41:28,953
And so this quantity
on the right hand side here,

773
00:41:29,520 --> 00:41:32,089
this is variational free energy

774
00:41:32,089 --> 00:41:37,027
and so the idea is,
is that we find the probability

775
00:41:37,027 --> 00:41:38,796
distribution,
this approximate probability

776
00:41:38,796 --> 00:41:42,833
distribution, which we make simplifying
assumptions about and so we can kind of

777
00:41:43,100 --> 00:41:44,401
evaluate analytically,

778
00:41:45,436 --> 00:41:47,137
we make simplifying, we

779
00:41:47,137 --> 00:41:52,443
find the value of this distribution
that best minimizes F

780
00:41:53,310 --> 00:41:56,514
where F is variational free energy
is that kind of makes sense.

781
00:41:57,581 --> 00:41:59,116
Hey, Chris, just one thing.

782
00:41:59,116 --> 00:42:02,553
Are you like trying to like move
your mouse around to point to things? No.

783
00:42:03,120 --> 00:42:05,122
I'm I'm actually holding my mouse.

784
00:42:05,556 --> 00:42:08,192
I'm scribbling my mouse
a little bit on the live stream.

785
00:42:08,192 --> 00:42:09,460
They should be able to see that.

786
00:42:09,460 --> 00:42:13,531
But Christopher
thank you for that really awesome example

787
00:42:13,731 --> 00:42:15,933
because it really clarified a few things.

788
00:42:16,300 --> 00:42:17,735
Any other notes to add here?

789
00:42:17,735 --> 00:42:20,638
Otherwise, this is cool to continue

790
00:42:20,905 --> 00:42:21,505
yeah.

791
00:42:21,605 --> 00:42:22,206
Okay.

792
00:42:23,207 --> 00:42:26,443
So then just kind of
to give a really toy example of this

793
00:42:26,610 --> 00:42:29,580
so we can get to find
our approximate posterior.

794
00:42:29,914 --> 00:42:34,318
I'm kind of arbitrarily is a flat
distribution will have a true posterior.

795
00:42:34,318 --> 00:42:36,153
Generally speaking,
we don't know what this is,

796
00:42:36,153 --> 00:42:38,989
but the illustration site
we're going to give give it to you

797
00:42:40,024 --> 00:42:42,660
and then we can have a joint distribution
and then observation.

798
00:42:42,693 --> 00:42:47,064
What the observation will do is it's
just going to select a kind of a column

799
00:42:47,064 --> 00:42:49,633
from this joint distribution

800
00:42:52,469 --> 00:42:55,973
and so then we have
we can have into this into the equation

801
00:42:55,973 --> 00:42:59,076
we saw on the last slide
and we'll slowly but

802
00:42:59,076 --> 00:43:04,014
surely in each update nudge
our posterior distribution

803
00:43:04,315 --> 00:43:07,518
such that it minimizes F on each step.

804
00:43:08,118 --> 00:43:11,322
And at the third step,
you see, what we should say is that

805
00:43:11,322 --> 00:43:13,357
when F is at a minimum,

806
00:43:13,424 --> 00:43:15,726
it is equal to surprise.

807
00:43:16,327 --> 00:43:19,129
And when F is at a minimum
as well, it's at a minimum because

808
00:43:19,797 --> 00:43:22,800
the true posterior
and our approximate posterior match.

809
00:43:24,535 --> 00:43:26,337
So that kind kind of makes sense.

810
00:43:26,570 --> 00:43:31,108
And so the idea is, is that by performing
Bayesian inference and having and kind

811
00:43:31,108 --> 00:43:35,145
of forcing some arbitrary distribution
to approximate our posterior,

812
00:43:36,180 --> 00:43:37,881
we come up with an upper

813
00:43:37,881 --> 00:43:41,552
bound on surprise
or we come to an upper bound on surprise.

814
00:43:42,286 --> 00:43:45,489
Let me kind of just walk through
and just double check here.

815
00:43:45,723 --> 00:43:48,659
So CU of S is our

816
00:43:48,659 --> 00:43:51,595
estimate of whether the coin is a fair
coin.

817
00:43:51,795 --> 00:43:54,665
50 50 means that it's 50 50 and then 80.

818
00:43:54,665 --> 00:43:57,601
20 is the reality on the table,
so to speak.

819
00:43:58,102 --> 00:44:02,873
So it's almost like first
we click from 50 50, we see heads come up

820
00:44:03,340 --> 00:44:06,644
and that contributes a little bit of
evidence that maybe heads is more likely.

821
00:44:06,877 --> 00:44:09,680
So we go from 50 50 to 60 40

822
00:44:10,214 --> 00:44:14,018
and then something happens again
and we click on the second update

823
00:44:14,585 --> 00:44:19,390
to 7.7.3,
we click eventually it's a .8.2.

824
00:44:19,890 --> 00:44:22,926
And then if we click all the way to 0.9.1

825
00:44:23,427 --> 00:44:25,863
so a 90 ten, all of a sudden

826
00:44:26,096 --> 00:44:30,801
we go back up from point
six, nine, three, back up.

827
00:44:31,035 --> 00:44:35,639
So if we were to plot this final
estimated F we'd find that we were like

828
00:44:35,673 --> 00:44:39,109
going downhill, getting better, get 2.693

829
00:44:39,109 --> 00:44:42,112
in this discrete 0.1 movement space

830
00:44:42,513 --> 00:44:45,249
and then we pop up a little bit too far,
we overshoot

831
00:44:45,516 --> 00:44:49,687
and we get a little bit more surprised
by the distributions that we're drawing.

832
00:44:50,387 --> 00:44:52,456
Yeah, exactly. That's perfect.

833
00:44:52,456 --> 00:44:53,090
Well, the only,

834
00:44:53,090 --> 00:44:56,593
the only thing that I would say that's
I mean, a little different just to note,

835
00:44:56,593 --> 00:44:59,697
is that there's nothing in this example
where you observe something

836
00:44:59,697 --> 00:45:01,231
over and over again.

837
00:45:01,231 --> 00:45:01,665
Yeah.

838
00:45:01,765 --> 00:45:04,868
This is,
this is just a single observation once.

839
00:45:05,436 --> 00:45:09,807
And you're trying to figure out what
what is the you're trying to figure out

840
00:45:09,807 --> 00:45:13,410
what the true posterior is trying to get
as close to it as possible.

841
00:45:13,877 --> 00:45:16,547
Still, you're doing is saying, here's
the single observation,

842
00:45:16,980 --> 00:45:20,317
but I'm going to try out
a bunch of different values for cues.

843
00:45:20,317 --> 00:45:22,820
So I try different values
for that approximate distribution.

844
00:45:23,353 --> 00:45:26,924
And I'm just going to find the one,
you know, through this kind of iterative

845
00:45:26,924 --> 00:45:31,128
updating thing where I slowly move cues,
you know, to different values

846
00:45:31,628 --> 00:45:35,733
you just do that for a single observation
and you can update until you find

847
00:45:36,133 --> 00:45:39,303
the minimum free energy value,
which just tells you

848
00:45:39,603 --> 00:45:42,873
that you're approximate posterior
the QC thing is as close

849
00:45:43,107 --> 00:45:48,045
as really close to the true posterior
that you couldn't figure out on your own

850
00:45:48,045 --> 00:45:51,148
because the problem was too tough
to solve with exact Bayesian inference

851
00:45:52,082 --> 00:45:54,351
So this this is kind of corresponds

852
00:45:54,351 --> 00:45:57,788
to what people might talk about
as prediction error minimization, right?

853
00:45:58,222 --> 00:46:00,557
So you just see this observation once

854
00:46:00,991 --> 00:46:04,194
and the brain tries to minimize
prediction error by minimizing F

855
00:46:04,495 --> 00:46:09,366
by just moving beliefs
until it finds the the belief

856
00:46:09,833 --> 00:46:14,071
that minimizes F, which is the same thing
as minimizing prediction error

857
00:46:16,073 --> 00:46:17,875
like that is that makes sense.

858
00:46:17,875 --> 00:46:21,145
Let me clarify, because I think
that really helped me understand it.

859
00:46:21,145 --> 00:46:24,114
And at first I thought,
wait with one observation,

860
00:46:24,314 --> 00:46:27,117
because at first when I described it,
I was thinking update was tied

861
00:46:27,117 --> 00:46:29,153
to a new observation from the coin.

862
00:46:29,153 --> 00:46:31,822
But then I thought, well,
if you're only getting one observation,

863
00:46:31,922 --> 00:46:35,526
the maximum likelihood model is a coin
that only comes up heads

864
00:46:35,559 --> 00:46:37,427
because you only have one observation,

865
00:46:37,427 --> 00:46:41,131
but it's not one shot
parameter learning naive.

866
00:46:41,698 --> 00:46:44,701
It's actually a tethered estimate

867
00:46:45,102 --> 00:46:50,974
that's tethered even loosely, but nonzero
tethering to a prior point, 5.5.

868
00:46:50,974 --> 00:46:54,411
Some people will say uninformative,
but all priors are informative

869
00:46:54,978 --> 00:46:56,446
they're all what they are.

870
00:46:56,446 --> 00:46:59,416
And then it's
almost like because of the logs,

871
00:46:59,850 --> 00:47:02,953
even though you only observed the coin
come up once, it's like,

872
00:47:03,086 --> 00:47:06,390
All right,
it's a little too far to update to 90 91.

873
00:47:06,590 --> 00:47:08,025
Just from seeing one coin flip.

874
00:47:08,025 --> 00:47:11,495
I would need extraordinary evidence
for extraordinary updates.

875
00:47:12,062 --> 00:47:16,834
And so it's like that one heads
observation updates you from 50 50 to

876
00:47:16,867 --> 00:47:20,504
near 80 20, which does happen to be close

877
00:47:20,737 --> 00:47:23,640
to the actual probability of the coin.

878
00:47:23,974 --> 00:47:28,312
And it also happens to navigate this
explore exploit in an interesting way

879
00:47:28,579 --> 00:47:30,581
because it updates it,
but not all the way.

880
00:47:31,014 --> 00:47:35,052
So it's just kind of showing
how the Bayesian updating brings

881
00:47:35,052 --> 00:47:38,722
some of that wisdom of multi observation
learning

882
00:47:39,022 --> 00:47:44,027
like slow updating of parameters
and sequential updating

883
00:47:44,728 --> 00:47:47,264
to a little bit of a different context.

884
00:47:48,098 --> 00:47:50,801
So I mean I do
want to clarify here a little bit.

885
00:47:50,801 --> 00:47:55,072
I mean there's, there's no actions here
so there can't really be explore exploit,

886
00:47:55,105 --> 00:47:55,339
right.

887
00:47:55,339 --> 00:47:56,106
You can't

888
00:47:56,106 --> 00:47:59,977
we can't choose to look over here versus
over there to gain information, right?

889
00:47:59,977 --> 00:48:03,580
So explore exploit is, you know,
specifically in the realm of make it

890
00:48:03,580 --> 00:48:07,985
choosing actions that will minimize
uncertainty versus maximize reward.

891
00:48:08,652 --> 00:48:13,790
And this is this is really more adjust
with with normal free energy right.

892
00:48:13,790 --> 00:48:16,727
We're not expect expected for energy yet
which is the making part

893
00:48:17,094 --> 00:48:18,795
with normal for energy.

894
00:48:18,795 --> 00:48:22,699
There's you can think the simplest way
to think about free energy

895
00:48:23,133 --> 00:48:27,371
is just in terms
of complexity minus accuracy,

896
00:48:28,472 --> 00:48:28,805
which is

897
00:48:28,805 --> 00:48:31,541
equivalently complexity
plus prediction error

898
00:48:32,142 --> 00:48:35,545
and all that all that ultimately means is
so the complexity thing is

899
00:48:35,545 --> 00:48:38,749
basically
how much you have to change your beliefs.

900
00:48:38,749 --> 00:48:40,550
So it's basically saying,

901
00:48:40,550 --> 00:48:43,453
what's the minimum change in my beliefs?

902
00:48:43,987 --> 00:48:47,090
That will make my new beliefs
as accurate as possible?

903
00:48:47,124 --> 00:48:47,391
Right.

904
00:48:47,391 --> 00:48:48,892
So I have to move my beliefs

905
00:48:48,892 --> 00:48:51,828
as little as possible
while also minimizing prediction error

906
00:48:53,196 --> 00:48:54,464
if that if that makes sense.

907
00:48:54,464 --> 00:48:58,302
So this what this generative
model is saying is just that

908
00:48:58,769 --> 00:49:02,372
the probability that we're in state
one is 0.8

909
00:49:02,906 --> 00:49:06,076
if I were to choose
if if I were to get this observation

910
00:49:06,510 --> 00:49:10,113
where the probability of being in state
two is only point to

911
00:49:10,147 --> 00:49:14,851
where I to get this observation right
so so I mean, the probably

912
00:49:14,851 --> 00:49:19,022
a better example than the than the like
heads and tails thing would just be

913
00:49:19,022 --> 00:49:23,093
would just be something like the concave
convex example we gave before inside.

914
00:49:23,460 --> 00:49:26,897
There is some possibility
where we're light from above,

915
00:49:27,531 --> 00:49:28,532
where a light from below

916
00:49:28,532 --> 00:49:31,134
could cause the shading pattern,
but it's just much less likely.

917
00:49:31,635 --> 00:49:31,902
Right.

918
00:49:31,902 --> 00:49:33,704
So you're just trying to find the,

919
00:49:33,704 --> 00:49:37,441
the belief, right,
that it's convex and light from above,

920
00:49:38,008 --> 00:49:42,079
that that is the one that most likely
generated what you're what you're seeing.

921
00:49:42,346 --> 00:49:46,283
And it might take a bunch of updates
like that given just what you see wants

922
00:49:46,650 --> 00:49:49,119
to kind of arrive
at the best fitting belief.

923
00:49:49,453 --> 00:49:52,823
There's a related question in the live
chat that I'm just going to ask

924
00:49:53,190 --> 00:49:54,491
because it's on the topic.

925
00:49:54,491 --> 00:49:55,692
They asked,

926
00:49:55,692 --> 00:50:00,063
is this approximate Bayesian inference
thing called something else in stats

927
00:50:00,063 --> 00:50:03,266
outside of active inference,
or is this unique to active inference

928
00:50:03,266 --> 00:50:04,735
because this isn't sequential

929
00:50:04,735 --> 00:50:08,705
Bayesian updating, as you mentioned,
this isn't a standard Bayesian filter.

930
00:50:08,705 --> 00:50:11,742
What is this called
outside of the active inference field?

931
00:50:12,242 --> 00:50:14,511
So yeah, it's just Variational Bayes.

932
00:50:14,511 --> 00:50:15,412
Variational, Bayes.

933
00:50:15,412 --> 00:50:19,249
Where does the partially observable
Markov decision process come into play?

934
00:50:20,417 --> 00:50:22,619
We'll cover that in a moment's
time, actually.

935
00:50:22,619 --> 00:50:24,488
It does. It does, yeah.

936
00:50:24,488 --> 00:50:25,922
It doesn't come in yet.

937
00:50:25,922 --> 00:50:27,057
So I think one thing to say

938
00:50:27,057 --> 00:50:30,761
is kind of writing this tutorial,
it's hard to please everyone right?

939
00:50:30,794 --> 00:50:33,163
Like in going through
some of the feedback, like we get

940
00:50:33,697 --> 00:50:36,900
we got some feedback
where people were just utterly confused

941
00:50:37,434 --> 00:50:39,136
and wanting clear explanations.

942
00:50:39,136 --> 00:50:40,070
And then at the other end of

943
00:50:40,070 --> 00:50:41,038
the spectrum, we wanted

944
00:50:41,038 --> 00:50:43,073
there were people wanting
much more technical details,

945
00:50:43,073 --> 00:50:45,075
like how does this relate to things
like gradient descent?

946
00:50:45,275 --> 00:50:49,079
Because this is just kind of
a very simplified cartoon

947
00:50:49,112 --> 00:50:52,082
example of something called grading
as a great intercept scheme

948
00:50:52,082 --> 00:50:54,985
where you're doing
a gradient descent on free energy.

949
00:50:56,586 --> 00:50:58,889
And so I would just say kind of to flag

950
00:50:58,922 --> 00:51:03,126
all of these issues is one,
I would say if something's unclear

951
00:51:03,126 --> 00:51:06,296
in this presentation, read the paper
because we gave things,

952
00:51:06,897 --> 00:51:08,732
we cover things
in a lot more detail in the paper.

953
00:51:08,732 --> 00:51:12,202
And if that isn't technical, enough for
you as you go and just look at our code,

954
00:51:12,869 --> 00:51:16,039
we supply a kind of standalone script

955
00:51:16,339 --> 00:51:19,676
where we it's extreme
and it's extremely well commented.

956
00:51:19,676 --> 00:51:22,512
And from there you should be able
to figure out everything that's going on

957
00:51:24,714 --> 00:51:28,118
So all of this stuff in this presentation
is kind of necessarily simplified

958
00:51:28,118 --> 00:51:32,022
because you don't really need
those technical details to kind of start

959
00:51:32,022 --> 00:51:35,959
using the framework or start
getting intuition for how things work.

960
00:51:36,960 --> 00:51:40,597
But once you do have an intuition,
you want more, just go and see the code.

961
00:51:40,597 --> 00:51:41,731
I would say.

962
00:51:43,233 --> 00:51:44,067
Awesome.

963
00:51:44,067 --> 00:51:48,872
It's like you can use the ANOVA package
in or without going into the source code.

964
00:51:49,172 --> 00:51:51,374
It's helpful. It's a tool for scientists.

965
00:51:51,541 --> 00:51:54,377
And then if you're curious
about the underpinnings of statistics

966
00:51:54,611 --> 00:51:58,882
and perennial philosophical debates,
there's a literature and a search bar,

967
00:51:59,116 --> 00:52:03,286
but today it's about the applications
of these methods which is awesome.

968
00:52:03,653 --> 00:52:05,722
So thanks to everyone for the questions.
Keep them coming.

969
00:52:05,755 --> 00:52:08,091
But this is great
discussion. I really appreciate it.

970
00:52:09,559 --> 00:52:10,060
Okay.

971
00:52:10,060 --> 00:52:12,963
And so just kind of to recap there,
I haven't covered action yet.

972
00:52:12,963 --> 00:52:14,931
I'm going to kind of action a little bit.

973
00:52:14,931 --> 00:52:18,635
But just kind of to recap,
the idea is, is undirected inference

974
00:52:18,935 --> 00:52:23,807
organisms are kind of we model them as is
the phenotype is in their body

975
00:52:23,807 --> 00:52:26,877
and their brain kind of embodies
a generative model of the environment

976
00:52:27,477 --> 00:52:31,781
and organisms kind of in
the general model to give an approximate

977
00:52:31,781 --> 00:52:35,352
posterior distribution
over the hidden causes of sensory input.

978
00:52:35,619 --> 00:52:37,954
And they do this
by minimizing variational free energy.

979
00:52:39,322 --> 00:52:42,025
So that should that much
should hopefully be clear by this point.

980
00:52:44,027 --> 00:52:45,962
And so then onto the generative model

981
00:52:45,962 --> 00:52:48,598
and it is a very specific type
of generative model,

982
00:52:48,598 --> 00:52:51,434
namely it's a partially observable
mark of decision process

983
00:52:51,768 --> 00:52:54,404
and so that's kind of
a graphical representation of this

984
00:52:54,404 --> 00:52:56,940
as a Bayesian network
on the right hand side.

985
00:52:56,940 --> 00:52:58,575
And I'm not going to kind of give it

986
00:52:58,575 --> 00:53:01,244
kind of give
too much detail about this right now.

987
00:53:01,244 --> 00:53:05,582
I will actually build it up step by step,
but just kind of prelude.

988
00:53:06,116 --> 00:53:08,718
The idea with Peter Pace
is that they described

989
00:53:08,718 --> 00:53:11,655
transitions
among hidden unobservable variables

990
00:53:12,055 --> 00:53:16,693
and the sensory data that's generated
by the variables loops. So

991
00:53:17,761 --> 00:53:20,330
those arrows mapping from states

992
00:53:20,330 --> 00:53:24,000
to observations are kind of gives

993
00:53:25,202 --> 00:53:28,471
information about the direction
of influence or conditional in the

994
00:53:30,407 --> 00:53:31,208
conditional influence.

995
00:53:31,208 --> 00:53:34,711
So kind of the arrow
between the purple node.

996
00:53:35,245 --> 00:53:37,581
Oh, and the green node s

997
00:53:37,781 --> 00:53:41,051
is mediated by the matrix
which is a likelihood.

998
00:53:41,484 --> 00:53:44,221
And the transitions
between states are mediated

999
00:53:44,221 --> 00:53:46,957
by a B matrix,
which is a transition probability.

1000
00:53:47,824 --> 00:53:50,927
And so the goal of active inference
with power on deep is

1001
00:53:51,194 --> 00:53:55,832
is to infer states and action sequences
or policies

1002
00:53:56,032 --> 00:53:59,636
by minimizing various forms
of variational free energy

1003
00:54:01,605 --> 00:54:02,038
Okay.

1004
00:54:02,038 --> 00:54:04,708
So let's start with a really,
really simple example.

1005
00:54:04,741 --> 00:54:07,177
We have static inference

1006
00:54:07,177 --> 00:54:10,113
and the idea with static inference
is this

1007
00:54:10,113 --> 00:54:14,050
this is just a graphical representation
of his rule.

1008
00:54:14,050 --> 00:54:19,522
Essentially, we have a prior
which is encoded in our day vector.

1009
00:54:19,522 --> 00:54:22,792
We have a likelihood encoded in our
A matrix

1010
00:54:22,926 --> 00:54:26,463
and we end up with an update equation,
which is a soft max function.

1011
00:54:26,463 --> 00:54:29,299
A soft max function is a normalized
exponential. What's

1012
00:54:31,801 --> 00:54:35,972
and so I'm not going to kind of run
through that explanation in detail.

1013
00:54:35,972 --> 00:54:37,407
I'm just going to leave the slide here

1014
00:54:37,407 --> 00:54:39,676
so we can figure it out for themselves
and pause and go back

1015
00:54:39,676 --> 00:54:41,578
and convinced themselves
that this is true.

1016
00:54:41,578 --> 00:54:44,281
But it's a very simple example

1017
00:54:44,281 --> 00:54:47,350
of the inference scheme that we're using
here is formally equivalent.

1018
00:54:47,384 --> 00:54:49,753
It's just an exact inference scheme.

1019
00:54:49,753 --> 00:54:54,391
So moving into dynamic models
specifically, so where states

1020
00:54:54,391 --> 00:54:57,360
change over time, these are also called
hidden Markov models.

1021
00:54:57,927 --> 00:55:00,130
This is when we have to stop
making approximate

1022
00:55:00,130 --> 00:55:03,867
or when things are no longer
equivalent to a

1023
00:55:03,900 --> 00:55:04,901
using active inference.

1024
00:55:04,901 --> 00:55:07,504
I no longer equivalent to

1025
00:55:07,504 --> 00:55:09,205
exact inference

1026
00:55:10,407 --> 00:55:11,574
and so here

1027
00:55:11,574 --> 00:55:15,945
we have our transition
probabilities encoded in a B matrix.

1028
00:55:15,945 --> 00:55:19,249
And this is just essentially like
the probability of some state of T plus

1029
00:55:19,249 --> 00:55:21,751
one conditioned on the previous state.

1030
00:55:22,519 --> 00:55:25,288
And so you can see over here in times
the update equations

1031
00:55:25,288 --> 00:55:28,525
we're now in, we're in log space
and this little sigma thing here

1032
00:55:28,525 --> 00:55:31,661
is a soft max function
which just normalizes this equation.

1033
00:55:32,095 --> 00:55:35,899
And the idea is, is that this is kind of
sorry, my screen keeps flipping.

1034
00:55:36,433 --> 00:55:36,766
Anyway,

1035
00:55:38,401 --> 00:55:42,038
the combination
of D and base are two priors in addition

1036
00:55:42,038 --> 00:55:46,142
to our likelihoods
will give us the approximate posterior.

1037
00:55:46,242 --> 00:55:50,280
And the reason we have this one half in
front is just because in practice,

1038
00:55:50,480 --> 00:55:53,483
the approximation scheme
that's used by active inference, namely

1039
00:55:54,517 --> 00:55:56,519
variation on message passing, tends

1040
00:55:56,519 --> 00:55:59,522
to overestimate
kind of the value of the posterior.

1041
00:55:59,823 --> 00:56:01,925
And so this is kind of
just a way of compensating for that.

1042
00:56:01,925 --> 00:56:04,194
And so anyone's kind of interested
in technical details there.

1043
00:56:04,394 --> 00:56:07,564
See, Thomas Pyle has a really excellent
paper out in scientific reports

1044
00:56:07,564 --> 00:56:10,834
on your message passing schemes
on the various approximations

1045
00:56:12,168 --> 00:56:13,603
to free energy.

1046
00:56:14,137 --> 00:56:14,671
Okay.

1047
00:56:14,671 --> 00:56:19,476
And so then the form of the free energy
is just down here and so the idea is,

1048
00:56:19,476 --> 00:56:24,381
is that by iteratively applying
these updates, we minimize free energy

1049
00:56:26,449 --> 00:56:30,153
and you iteratively kind of every upstart

1050
00:56:30,153 --> 00:56:33,757
you do a full round of updating
every time you get a new observation

1051
00:56:36,893 --> 00:56:38,461
or that policy selection.

1052
00:56:38,461 --> 00:56:42,365
So the idea with policy
selection is roughly speaking,

1053
00:56:42,632 --> 00:56:46,569
that policies are just state transitions
that the agent has control over.

1054
00:56:46,870 --> 00:56:50,240
So imagine
you are a psycho physics experimenter

1055
00:56:50,473 --> 00:56:53,643
and you have just a very simple boring
example of someone kind of

1056
00:56:55,178 --> 00:56:58,915
say, estimating the orientation
of some hard to say stimulus.

1057
00:56:59,048 --> 00:57:00,183
And so the policy space

1058
00:57:00,183 --> 00:57:03,153
there is tiny it's two options of left
or right, let's say.

1059
00:57:04,020 --> 00:57:06,956
Or you could have an agent
that you could be simulating something

1060
00:57:06,956 --> 00:57:08,458
more interesting psycho physics.

1061
00:57:08,458 --> 00:57:10,560
You could be simulating an agent
navigating a maze.

1062
00:57:10,960 --> 00:57:12,962
And then the policy space is much larger.

1063
00:57:12,962 --> 00:57:13,797
They could go forward,

1064
00:57:13,797 --> 00:57:16,833
they could move forwards,
backwards, left, right, etc., etc.

1065
00:57:18,902 --> 00:57:19,269
And so

1066
00:57:19,269 --> 00:57:22,705
the idea is, is that active front
agents, by definition,

1067
00:57:23,072 --> 00:57:25,742
they dislike policies that will minimize
variational free energy,

1068
00:57:26,609 --> 00:57:29,012
but that relies

1069
00:57:29,012 --> 00:57:31,214
upon observations that have yet to come.

1070
00:57:31,981 --> 00:57:34,217
So that's kind of the problem.

1071
00:57:34,384 --> 00:57:36,986
But the way around
this is to treat observations

1072
00:57:36,986 --> 00:57:42,058
as random variables and then what you do
is you minimize an approximation

1073
00:57:42,058 --> 00:57:45,795
not to surprise, which is variational
for energy, but you expected surprise

1074
00:57:46,062 --> 00:57:48,765
and that approximation to expected
surprise as

1075
00:57:49,365 --> 00:57:51,367
is expected for energy.

1076
00:57:51,901 --> 00:57:55,238
And so this expected for energy
has two key components.

1077
00:57:55,672 --> 00:57:57,941
The first is the expected cost.

1078
00:57:57,941 --> 00:57:59,642
And this kind of minimize.

1079
00:57:59,642 --> 00:58:01,711
The idea here is you
minimize expected cost.

1080
00:58:02,145 --> 00:58:04,781
You need to minimize the deviation
between our predicted

1081
00:58:04,781 --> 00:58:06,249
and preferred outcomes.

1082
00:58:06,249 --> 00:58:08,785
And so this is kind of what we're talking
about running a spectrum before.

1083
00:58:08,785 --> 00:58:12,689
So we see vector encodes a distribution
over an agent's preferences.

1084
00:58:13,289 --> 00:58:15,658
So for example, policies

1085
00:58:15,892 --> 00:58:18,761
that minimize expected cost policies

1086
00:58:18,761 --> 00:58:23,233
that kind of bring about observations
that the agent prefers, for example.

1087
00:58:23,233 --> 00:58:28,338
So if I prefer to kind of have my body
temperature within a certain range,

1088
00:58:28,538 --> 00:58:31,975
the policy would be maybe staying inside
because I live in England and it's like

1089
00:58:32,008 --> 00:58:32,876
-1 outside,

1090
00:58:34,577 --> 00:58:36,246
for example.

1091
00:58:36,513 --> 00:58:40,116
Now expected ambiguity is a little bit
that this

1092
00:58:40,116 --> 00:58:43,453
this is kind of the epistemic drive
or the information gain term.

1093
00:58:43,820 --> 00:58:47,790
This is kind of the expected entropy
of our likelihood distribution.

1094
00:58:48,191 --> 00:58:51,227
So the idea here is,
is that to minimize ambiguity,

1095
00:58:51,828 --> 00:58:53,296
you'll have an A matrix, right?

1096
00:58:53,296 --> 00:58:57,100
And a matrix if the idea is to minimize
kind of ambiguity,

1097
00:58:57,100 --> 00:59:00,470
you need to select observations
in a matrix

1098
00:59:01,337 --> 00:59:04,073
that are maximally precise.

1099
00:59:04,073 --> 00:59:07,243
And so to return to or take actions even

1100
00:59:07,710 --> 00:59:10,980
that will make those things
that a matrix maximally precise.

1101
00:59:11,447 --> 00:59:14,951
And so if I'm in a dark room
to give an example for the thing

1102
00:59:14,951 --> 00:59:18,321
that will make or select kind of

1103
00:59:18,421 --> 00:59:21,791
policies that will make that distribution
maximally precise policies

1104
00:59:21,991 --> 00:59:22,959
like turning the light on.

1105
00:59:24,093 --> 00:59:26,062
And so then to minimize fringe

1106
00:59:26,062 --> 00:59:28,231
as a whole, you have to minimize
both of these things.

1107
00:59:29,499 --> 00:59:32,969
And you can do this in terms one step
policies, just looking one step ahead.

1108
00:59:33,202 --> 00:59:37,173
All you can do is look in using date
policies looking many, many steps ahead.

1109
00:59:38,141 --> 00:59:39,876
And hopefully that's pretty clear.

1110
00:59:39,876 --> 00:59:42,445
Is anything you'd like to add to that,
Ryan or Daniel?

1111
00:59:42,679 --> 00:59:43,580
Anyone else?

1112
00:59:44,847 --> 00:59:46,382
Yeah, I mean, I guess
I'll I guess I'll just say,

1113
00:59:46,382 --> 00:59:49,352
I mean, to
just to make it kind of as clear

1114
00:59:49,385 --> 00:59:52,689
and explicit as possible for people
who don't have a background in, you know,

1115
00:59:53,256 --> 00:59:56,125
what this whole thing,
you know, what this equation means.

1116
00:59:56,626 --> 00:59:57,060
Right.

1117
00:59:57,060 --> 01:00:00,363
So the idea is just if you look at that
term above expected cost

1118
01:00:00,863 --> 01:00:03,132
and that's called a cal divergence.

1119
01:00:03,566 --> 01:00:07,337
And so basically it's just a value
that encodes the difference between

1120
01:00:07,337 --> 01:00:10,640
two distributions or the dissimilarity
between two distributions.

1121
01:00:11,474 --> 01:00:14,010
And so that first one that Q given

1122
01:00:14,010 --> 01:00:17,780
PI, that's just saying
what observations do I expect

1123
01:00:18,281 --> 01:00:20,617
given that I choose to do this
versus that?

1124
01:00:21,284 --> 01:00:25,622
And then that second one,
that P of all, that's the preferences

1125
01:00:27,023 --> 01:00:27,724
so basically

1126
01:00:27,724 --> 01:00:32,095
what it's trying to do is just minimize
the difference between your

1127
01:00:32,295 --> 01:00:36,032
your preferred observations
and the observations that you expect,

1128
01:00:36,599 --> 01:00:39,936
given that you choose to do,
you know, thing one versus thing two.

1129
01:00:40,670 --> 01:00:44,073
So it's really just choosing the thing
that's going to get you as close to the

1130
01:00:44,073 --> 01:00:46,776
you think is going to get you as close
to what you want as possible.

1131
01:00:47,110 --> 01:00:49,812
So it's very just,
you know, reward seeking more or less.

1132
01:00:50,780 --> 01:00:53,583
And that second one,
the expected ambiguity that each thing

1133
01:00:54,150 --> 01:00:55,785
stands for entropy.

1134
01:00:55,785 --> 01:00:57,887
And basically entropy is just

1135
01:00:58,321 --> 01:01:01,724
the higher the entropy, the flatter
a distribution is.

1136
01:01:02,191 --> 01:01:05,628
So think of a distribution
that's like one is like 0.5.5,

1137
01:01:05,628 --> 01:01:08,498
and the other one's like .8.2.

1138
01:01:08,498 --> 01:01:12,835
If you chose the state, that would
generate 8.5.5 over observations

1139
01:01:13,336 --> 01:01:17,273
that wouldn't tell you anything
because either observation, you got it.

1140
01:01:17,273 --> 01:01:21,377
Tell you there's a 0.5 probability
that you're in one state,

1141
01:01:21,377 --> 01:01:23,713
in the probability
you're in the other state.

1142
01:01:23,713 --> 01:01:24,447
Right.

1143
01:01:24,480 --> 01:01:27,950
Whereas the other state would generate
a point eight or a point two.

1144
01:01:28,451 --> 01:01:31,888
Then that one will give you a lot
more information because you observe

1145
01:01:31,888 --> 01:01:34,123
if you observe the thing
that indicates 0.8

1146
01:01:34,123 --> 01:01:36,092
then you're really confident
what state you're in.

1147
01:01:37,493 --> 01:01:39,028
And if or if you observe point

1148
01:01:39,028 --> 01:01:41,497
two, you're really confident
you're not in that state, right?

1149
01:01:42,131 --> 01:01:45,001
So you're just seeking out
the thing that you think

1150
01:01:45,001 --> 01:01:47,937
will get you what you want
as much as possible.

1151
01:01:48,271 --> 01:01:52,308
But also moving to the states
that are going to give you

1152
01:01:52,308 --> 01:01:55,411
the observation that's going to tell you
the most about where you are,

1153
01:01:56,446 --> 01:01:58,247
if that if that if that makes sense.

1154
01:01:58,247 --> 01:02:01,951
So just again, just for people who
who aren't necessarily familiar

1155
01:02:01,951 --> 01:02:05,121
with reading the kind of notation
in these equations.

1156
01:02:05,755 --> 01:02:08,257
Thank you, Max.

1157
01:02:08,491 --> 01:02:08,691
Yeah.

1158
01:02:08,691 --> 01:02:10,560
I just want one point of clarification.

1159
01:02:10,560 --> 01:02:12,161
And then I just wanted to tie
it back to what

1160
01:02:12,161 --> 01:02:13,963
the example we had discussed previously.

1161
01:02:13,963 --> 01:02:17,066
So first, I want to just make sure
I'm understanding correctly that the D in

1162
01:02:17,066 --> 01:02:21,070
that equation is the k l diverge.

1163
01:02:21,070 --> 01:02:24,841
So that's the that's an operator it's not
the same as the D that's in the figure

1164
01:02:25,141 --> 01:02:26,476
in the block diagram.

1165
01:02:26,476 --> 01:02:28,311
That's that's the L divergence.

1166
01:02:28,311 --> 01:02:29,779
That's a yeah.

1167
01:02:29,779 --> 01:02:30,747
That's kind of a standard

1168
01:02:30,747 --> 01:02:33,850
way of representing yet another paper is
that's just kind of pasted in, but

1169
01:02:34,550 --> 01:02:37,386
it's not very clear that should be
the K l has nothing to do with d.

1170
01:02:37,920 --> 01:02:39,122
D sub. Fail.

1171
01:02:39,122 --> 01:02:43,226
The D d that's in this graphic
in the block diagram.

1172
01:02:43,526 --> 01:02:47,263
Would that be analogous in our previous
example where we were talking when,

1173
01:02:47,263 --> 01:02:51,601
when we illustrated step by step,
that would be R 0.5, 0.5.

1174
01:02:51,601 --> 01:02:54,737
But it in this context
could be much more complicated than that.

1175
01:02:55,037 --> 01:02:55,605
Yeah.

1176
01:02:55,605 --> 01:02:56,672
And a simple example.

1177
01:02:56,672 --> 01:02:58,741
Yeah, it would just be your prior. Right.

1178
01:02:58,741 --> 01:03:01,611
So if ahead of time you think that like

1179
01:03:02,211 --> 01:03:05,381
ice cream
is more likely than donuts, right?

1180
01:03:05,715 --> 01:03:08,050
Then that could just be like point eight,
point two,

1181
01:03:08,551 --> 01:03:09,485
you know, that's

1182
01:03:09,485 --> 01:03:11,120
or if you have no idea whether it's

1183
01:03:11,120 --> 01:03:13,489
going to be ice cream or donuts,
it could be .5.5.

1184
01:03:13,923 --> 01:03:15,158
In for it.

1185
01:03:15,158 --> 01:03:19,562
And for anyone who is like
wanting to get into optimization

1186
01:03:19,562 --> 01:03:22,832
and how do we know whether or not
this is going to converge

1187
01:03:22,832 --> 01:03:26,736
when we do our message passage or message
passing algorithm back and forth?

1188
01:03:27,069 --> 01:03:30,039
There was a really good citation,
and that's the one

1189
01:03:30,039 --> 01:03:33,376
that was alluded to at the beginning
of this on the slides.

1190
01:03:33,376 --> 01:03:35,378
Right.
The technical papers. Is that correct?

1191
01:03:36,379 --> 01:03:39,482
So the citation to do with message
passing in particular is a paper

1192
01:03:39,482 --> 01:03:42,885
by Thomas Parr
so that in scientific reports,

1193
01:03:45,488 --> 01:03:47,690
yeah, we can
I think you have showing notes.

1194
01:03:47,857 --> 01:03:49,926
I assume we can. Yes, please.

1195
01:03:49,926 --> 01:03:53,329
I have a couple other general questions
from the live chat,

1196
01:03:53,329 --> 01:03:58,000
but I think we'll take them at the end
of your presentation as we turn towards

1197
01:03:58,000 --> 01:04:01,771
some of the neurobiology beginnings
and a few other aspects.

1198
01:04:01,771 --> 01:04:03,506
So continue. Christopher.

1199
01:04:03,506 --> 01:04:05,241
Yeah, thank you.

1200
01:04:05,741 --> 01:04:06,108
Okay.

1201
01:04:06,108 --> 01:04:09,512
And so then kind of just to close
or just to briefly recap,

1202
01:04:10,079 --> 01:04:13,916
so there are multiple stages
to this to policy selection.

1203
01:04:13,916 --> 01:04:17,653
So under a generative model
that has multiple policies,

1204
01:04:17,854 --> 01:04:21,924
you need to minimize you minimize your
free energy with respect to each policy,

1205
01:04:22,391 --> 01:04:25,862
your variational free energy and
so you might think about this as you say,

1206
01:04:25,862 --> 01:04:29,999
you're coming to a set of traffic lights
and you have a hold up.

1207
01:04:29,999 --> 01:04:32,568
You could turn left, right or go straight
ahead.

1208
01:04:32,568 --> 01:04:36,305
Those actions are possible
but you then get sensory

1209
01:04:36,305 --> 01:04:38,975
input
that says there's another left hand side.

1210
01:04:39,742 --> 01:04:44,113
So that would make that policy
given extremely high for energy value.

1211
01:04:44,580 --> 01:04:47,850
And so that would eliminate it
from the plausible policies that you can

1212
01:04:47,850 --> 01:04:49,018
kind of evaluate. Right.

1213
01:04:50,820 --> 01:04:51,254
And so

1214
01:04:51,254 --> 01:04:54,090
then expected and fringe
is what we just talked about

1215
01:04:54,523 --> 01:04:57,293
and the posterior distribution
is actually

1216
01:04:57,894 --> 01:05:00,529
the soft max function over both of these.

1217
01:05:00,529 --> 01:05:03,733
And so this because kind of this is
these are both negatives

1218
01:05:06,936 --> 01:05:10,172
policies that best minimize
both variational for energy

1219
01:05:10,172 --> 01:05:13,643
and expected free energy
will have the highest posterior value

1220
01:05:15,845 --> 01:05:18,414
and then kind of it is
that is that clear?

1221
01:05:18,514 --> 01:05:21,651
There are a couple of caveats
to all of this, but

1222
01:05:22,952 --> 01:05:23,819
this wanting to make sure.

1223
01:05:23,819 --> 01:05:25,521
Right and you're allowed to thing here.

1224
01:05:25,521 --> 01:05:28,958
Well, I just say just just so anybody
who doesn't know like what a

1225
01:05:29,091 --> 01:05:30,960
what a soft max function is.

1226
01:05:30,960 --> 01:05:31,193
Yeah.

1227
01:05:31,193 --> 01:05:35,564
So so all that all that means
is that like for instance, when you take

1228
01:05:35,765 --> 01:05:37,566
minus F, minus G

1229
01:05:37,566 --> 01:05:38,267
and that's

1230
01:05:38,267 --> 01:05:41,671
not going to give you something that's
a true probability distribution, right?

1231
01:05:41,671 --> 01:05:44,440
It's not going to be a thing
that sums to one. Right.

1232
01:05:44,440 --> 01:05:46,809
We're probabilities altogether. Some 21.

1233
01:05:46,809 --> 01:05:47,543
What it's going to do

1234
01:05:47,543 --> 01:05:51,180
is give you this negative, you know, a
bunch of negative numbers, right?

1235
01:05:51,480 --> 01:05:55,318
So the soft max function does is it
normalizes that,

1236
01:05:55,318 --> 01:05:58,621
which just means that it takes
the kind of relative values,

1237
01:05:58,955 --> 01:06:01,757
you know, of each of those things
and turns it into

1238
01:06:02,191 --> 01:06:04,593
a probability distribution
that sums to one.

1239
01:06:05,094 --> 01:06:08,364
So so you end up with that bolded
PI symbol

1240
01:06:08,898 --> 01:06:11,434
is just a probability distribution.

1241
01:06:11,667 --> 01:06:15,771
That's assigned a product that assigns
a probability to each possible policy.

1242
01:06:16,973 --> 01:06:17,206
And a

1243
01:06:17,206 --> 01:06:19,742
policy, by
the way, is just a sequence of actions.

1244
01:06:20,209 --> 01:06:20,509
Right.

1245
01:06:20,509 --> 01:06:21,844
So one policy might be like

1246
01:06:21,844 --> 01:06:25,081
turn left, turn right, and another policy
might be turn right, turn left.

1247
01:06:26,315 --> 01:06:28,484
And so it's just saying that

1248
01:06:28,484 --> 01:06:32,121
and but that minus F minus thing

1249
01:06:32,521 --> 01:06:35,124
will just be turned
into a probability distribution.

1250
01:06:35,524 --> 01:06:38,361
And that probability distribution
over different policies

1251
01:06:38,361 --> 01:06:41,664
is what the agent will sample from right.

1252
01:06:41,664 --> 01:06:46,535
So if the if PI says this policy is point
eight, this other policy is point two,

1253
01:06:46,969 --> 01:06:50,339
then the agent will be a lot more likely
to choose the policy at this point.

1254
01:06:50,973 --> 01:06:51,574
Yeah.

1255
01:06:51,841 --> 01:06:55,611
Maybe go to the last caveat slider
then I have a few other questions.

1256
01:06:55,611 --> 01:06:57,480
Well,
we'll just try to like have a question.

1257
01:06:57,480 --> 01:06:59,682
Simple answer, question, simple answer,

1258
01:06:59,682 --> 01:07:01,417
and then we'll move into
the part to screen. Sure.

1259
01:07:02,451 --> 01:07:02,852
Okay.

1260
01:07:02,852 --> 01:07:08,791
So the two caveats are just I there are
two extra components of the MVP that is.

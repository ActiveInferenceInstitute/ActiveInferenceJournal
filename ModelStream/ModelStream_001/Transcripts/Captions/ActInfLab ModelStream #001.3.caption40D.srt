1
00:00:11,077 --> 00:00:12,512
And we are live.

2
00:00:12,512 --> 00:00:16,383
Hello, everyone, and welcome
to Active in France. Lab.

3
00:00:16,549 --> 00:00:20,153
This is the third part
in a four part series.

4
00:00:20,253 --> 00:00:22,255
It is Model Stream, 3.0.

5
00:00:22,722 --> 00:00:27,494
And today we are here with Ryan Smith
and Christopher White and Max Murphy.

6
00:00:27,727 --> 00:00:30,630
And we're going to be having
a really interesting discussion.

7
00:00:30,630 --> 00:00:32,165
So thanks, everyone, for joining.

8
00:00:32,165 --> 00:00:34,334
Just to quickly introduce ourselves

9
00:00:34,334 --> 00:00:37,804
before we go to points a process
and then into the material.

10
00:00:38,171 --> 00:00:39,439
I'm Daniel Friedman.

11
00:00:39,439 --> 00:00:41,541
I'm a postdoc in California.

12
00:00:44,144 --> 00:00:45,812
I'm Ryan Smith.

13
00:00:45,812 --> 00:00:48,415
I'm an investigator
at the Laureate Institute for Brain

14
00:00:48,415 --> 00:00:51,551
Research in Tulsa, Oklahoma.

15
00:00:51,551 --> 00:00:52,852
I am Christopher.

16
00:00:52,852 --> 00:00:53,586
I'm a Ph.D.

17
00:00:53,586 --> 00:00:57,090
student at the MRC Cognition Brain
Sciences Unit in Cambridge, England.

18
00:00:58,892 --> 00:00:59,993
I am next Murphy.

19
00:00:59,993 --> 00:01:02,429
I'm a postdoctoral fellow
at Carnegie Mellon.

20
00:01:03,930 --> 00:01:04,431
Awesome.

21
00:01:04,431 --> 00:01:08,101
Thanks, everyone, for joining
for our panelists and the authors

22
00:01:08,101 --> 00:01:11,337
for Max for contributing your experience
and also for

23
00:01:11,337 --> 00:01:14,908
everybody who's going to be live
and in replay asking us questions.

24
00:01:15,475 --> 00:01:19,546
This is the third in a four part series
that is going to be highlighting

25
00:01:19,779 --> 00:01:23,783
different perspectives and addressing
questions related to the active inference

26
00:01:23,783 --> 00:01:27,253
tutorial paper of Smith at all from,

27
00:01:27,921 --> 00:01:30,623
I guess you could say 20, 20,
but it's been versioned several times.

28
00:01:30,957 --> 00:01:33,860
And that tutorial is called a step
by step tutorial

29
00:01:33,860 --> 00:01:37,130
on active inference
and its applications to empirical data.

30
00:01:37,697 --> 00:01:40,700
So if you have any questions
during the live stream, you're more than

31
00:01:40,700 --> 00:01:43,603
welcome to type it in the live chat
and we'll try to get to it.

32
00:01:43,903 --> 00:01:47,207
If you have questions after this
live stream, just leave a comment

33
00:01:47,207 --> 00:01:49,609
and we're going to try to address it
as well.

34
00:01:49,609 --> 00:01:52,145
To learn more
and to find out about participate

35
00:01:52,645 --> 00:01:54,881
check out Active Conference Talk.

36
00:01:55,381 --> 00:01:56,182
That's all.

37
00:01:56,182 --> 00:01:59,953
So for today in our third session,
we're going to be picking up

38
00:01:59,953 --> 00:02:03,423
with just a brief overview, even shorter
than the second parts overview.

39
00:02:03,790 --> 00:02:07,227
And then we're going to be diving into
two main sections today.

40
00:02:07,660 --> 00:02:12,132
Ryan is going to be leading a section
on the neural process theory

41
00:02:12,565 --> 00:02:16,536
And then Christopher is going to be
leading a section on hierarchical models.

42
00:02:16,803 --> 00:02:18,905
So two very interesting topics.

43
00:02:19,272 --> 00:02:23,243
Let's just start with one warm up
introductory question

44
00:02:23,243 --> 00:02:25,545
before we go to the neural process
theory.

45
00:02:25,545 --> 00:02:28,381
And my question would be,
let's say somebody were sent

46
00:02:28,548 --> 00:02:31,351
just this third part of this
four part series

47
00:02:31,951 --> 00:02:35,088
What would you say
is the summary of the first two parts

48
00:02:35,088 --> 00:02:38,124
that we can catch them up with right now
before we jump into these next

49
00:02:38,124 --> 00:02:38,525
two parts?

50
00:02:38,525 --> 00:02:41,928
So what happened in the first two parts
that let us to want

51
00:02:41,928 --> 00:02:45,198
to talk about the neural process theory
and the hierarchical modeling today?

52
00:02:47,667 --> 00:02:48,101
Okay.

53
00:02:48,101 --> 00:02:49,969
Well, I can try to start to take a stab,
Chris.

54
00:02:49,969 --> 00:02:51,070
Feel free to add anything.

55
00:02:51,070 --> 00:02:55,542
But it's obviously a lot to summarize in
just a couple seconds here.

56
00:02:55,975 --> 00:02:59,212
But in Session one, primarily,

57
00:02:59,212 --> 00:03:01,548
we went over the kind of like broad,

58
00:03:02,248 --> 00:03:04,350
broad strokes, sort of a description
of active inference,

59
00:03:05,251 --> 00:03:08,955
the ideas of how you can do
approximate Bayesian inference

60
00:03:08,955 --> 00:03:12,992
to solve problems with perception
and learning and decision making using

61
00:03:13,593 --> 00:03:17,530
free energy minimization
and expected for energy minimization,

62
00:03:18,264 --> 00:03:20,567
where exact Bayesian inference
just becomes

63
00:03:20,567 --> 00:03:24,037
intractable
for most kind of Real-World problems.

64
00:03:24,037 --> 00:03:25,471
When you're trying to

65
00:03:26,072 --> 00:03:28,775
do something cognitive or perceptual.

66
00:03:30,176 --> 00:03:32,645
And then we went over

67
00:03:32,645 --> 00:03:35,248
how to build specific task models

68
00:03:36,783 --> 00:03:39,152
and specifically we used an explorer
exploit

69
00:03:39,219 --> 00:03:43,623
a simpler sort of explorer
exploit model similar to you, a kind

70
00:03:43,623 --> 00:03:46,526
of behavioral task that you might use,
and then an empirical study.

71
00:03:47,393 --> 00:03:52,599
And we just showed
how at the level of matrices and vectors

72
00:03:52,599 --> 00:03:54,801
and things like that,
how you specify a generative model

73
00:03:55,702 --> 00:03:58,338
in order to simulate behavior
on that kind of task.

74
00:04:00,540 --> 00:04:03,977
And that's
more or less what we've covered so far.

75
00:04:04,010 --> 00:04:07,981
So that kind of general mathematics
and motivation of active inference

76
00:04:08,448 --> 00:04:12,518
and how to apply it to to model tasks.

77
00:04:13,686 --> 00:04:16,723
But that was all, like I said,
at the kind of computational

78
00:04:16,723 --> 00:04:18,891
and algorithmic level of description

79
00:04:19,926 --> 00:04:23,763
whereas one of the really nice things
about active inference is that

80
00:04:24,297 --> 00:04:27,600
you have not only the computational
and algorithmic level,

81
00:04:27,600 --> 00:04:31,271
but you also have an attached
what's called a neural process theory,

82
00:04:31,771 --> 00:04:35,275
which more or less corresponds
to a specific hypothesis

83
00:04:35,975 --> 00:04:40,046
about ways that neurons,
neural populations can be connected up

84
00:04:40,780 --> 00:04:43,283
in plausible ways to implement

85
00:04:43,283 --> 00:04:46,619
the act of inference algorithms,
and specifically that variational

86
00:04:46,719 --> 00:04:50,790
updating algorithms
that are used to solve

87
00:04:51,524 --> 00:04:54,594
the sorts of tasks that you use to model
with that inference.

88
00:04:55,395 --> 00:04:58,264
So that's kind of where we're picking up
as now that we've got that, now

89
00:04:58,264 --> 00:05:01,801
that we can build models and understand
their background and motivation,

90
00:05:02,268 --> 00:05:05,505
you know, how can we actually move this
in into neuroscience and make,

91
00:05:06,673 --> 00:05:09,909
make a trial by trial predictions
about what neural responses

92
00:05:09,909 --> 00:05:12,679
you are to see, which you could,
for example, feed in

93
00:05:13,112 --> 00:05:17,083
to like an F MRI experiment to see
if you can actually find correlates

94
00:05:17,083 --> 00:05:19,619
of those predicted trial
by trial responses.

95
00:05:21,754 --> 00:05:24,357
And just kind of add to that,
I think the general way

96
00:05:24,357 --> 00:05:27,193
that you would feed it into an
AI experiment

97
00:05:27,193 --> 00:05:29,529
would just be the standard way
that people usually do this stuff.

98
00:05:29,529 --> 00:05:31,931
So you just put it in
and out of just regress and

99
00:05:33,166 --> 00:05:35,501
into a G lab. Yeah.

100
00:05:35,501 --> 00:05:38,237
And as you'll see, we can also do
it's not just up from here.

101
00:05:38,371 --> 00:05:42,075
You can also do with I mean, we the
the most direct predictions

102
00:05:42,075 --> 00:05:45,144
that make sure about earpiece any e.g.

103
00:05:45,144 --> 00:05:48,781
So and we're going to talk about what
the ERP is

104
00:05:48,781 --> 00:05:52,185
and the biology of it a little bit
along the way and other frameworks

105
00:05:52,185 --> 00:05:53,686
for how people have interpreted it.

106
00:05:53,686 --> 00:05:56,322
But yes, as we've
kind of looked at a few different times,

107
00:05:56,556 --> 00:06:00,159
there's the observables, which are the
behavior or the neuroimaging

108
00:06:00,393 --> 00:06:04,731
whether it's a fMRI or EEG or
some other type of measurement technique.

109
00:06:04,964 --> 00:06:06,766
And then there's
the measurement of behavior.

110
00:06:06,766 --> 00:06:10,036
And we're talking about models
that start from those observables

111
00:06:10,203 --> 00:06:13,773
and then do some interesting
generative modeling under the hood,

112
00:06:13,906 --> 00:06:16,008
and that's where the power arises from.

113
00:06:16,008 --> 00:06:17,377
So any other comments on that?

114
00:06:17,377 --> 00:06:20,480
Max, before we jump into part
one with the neural process theory.

115
00:06:22,548 --> 00:06:23,683
No, I'm just

116
00:06:23,683 --> 00:06:26,419
like I said last time,
I'm excited to hear about timing

117
00:06:26,452 --> 00:06:29,722
relating specifically,
you know, gradients on,

118
00:06:30,289 --> 00:06:33,393
you know, variational free energy to you
know, dopamine expression

119
00:06:33,393 --> 00:06:34,327
and just hearing about

120
00:06:34,327 --> 00:06:37,630
like the links between those
and why that might be an elegant way

121
00:06:38,798 --> 00:06:42,969
to kind of express things
in terms of the neural process theory

122
00:06:42,969 --> 00:06:46,038
and what we know about the connectivity
of these regions

123
00:06:46,038 --> 00:06:47,340
and things like that.

124
00:06:49,108 --> 00:06:51,978
Sounds good then, Ryan, take it away.

125
00:06:53,012 --> 00:06:53,846
Okay.

126
00:06:54,480 --> 00:06:55,548
So let me just

127
00:06:56,015 --> 00:06:58,618
share my screen here.

128
00:07:07,527 --> 00:07:08,161
Okay.

129
00:07:08,261 --> 00:07:10,963
So just

130
00:07:11,264 --> 00:07:13,966
as a kind of very brief,

131
00:07:13,966 --> 00:07:16,335
you know,
sort of just catch people up a bit

132
00:07:20,940 --> 00:07:23,943
So we we started out,
you know, like describing

133
00:07:23,943 --> 00:07:27,280
active inference, like I said, at the
level of generative models

134
00:07:28,381 --> 00:07:30,082
and those.

135
00:07:30,082 --> 00:07:32,585
So I'm trying to find the right

136
00:07:32,585 --> 00:07:34,987
figure here.

137
00:07:39,025 --> 00:07:40,092
And those have this

138
00:07:40,092 --> 00:07:42,962
particular sort of graphical structure.

139
00:07:44,063 --> 00:07:45,031
And just focusing on

140
00:07:45,031 --> 00:07:47,767
the one on the bottom right here,
which is the full generative model,

141
00:07:48,201 --> 00:07:53,072
you have a particular set of observations
over time, this 010203 thing.

142
00:07:53,506 --> 00:07:55,675
And then you have particular
hidden states.

143
00:07:55,675 --> 00:07:59,679
So beliefs about what's going on
out in the world outside of the brain.

144
00:08:00,112 --> 00:08:01,814
And so you have your beliefs

145
00:08:01,814 --> 00:08:05,184
about what happened at time one
and then time two, and then time three.

146
00:08:05,718 --> 00:08:09,121
And this a matrix thing

147
00:08:09,121 --> 00:08:12,625
is what encodes the likelihood mapping
or the belief about

148
00:08:12,625 --> 00:08:17,363
what outcomes will be present
if particular states out in the world

149
00:08:18,397 --> 00:08:19,532
are the case.

150
00:08:20,700 --> 00:08:24,003
And then you have beliefs
about how in this B matrix thing here

151
00:08:24,370 --> 00:08:27,840
about how if you're in particular
states of time, one how that's

152
00:08:27,840 --> 00:08:31,110
going to transition into different states
of time two and so forth.

153
00:08:31,744 --> 00:08:34,847
And the study thing
is just an initial prior

154
00:08:35,414 --> 00:08:37,149
about what
state you're going to be at time one

155
00:08:38,284 --> 00:08:38,618
and then

156
00:08:38,618 --> 00:08:42,421
you have policies pi up here
which control

157
00:08:42,421 --> 00:08:46,092
which allow the agent to control
some of the transitions between states.

158
00:08:47,126 --> 00:08:51,397
So for instance, the transition
of moving my leg up versus down, right?

159
00:08:51,397 --> 00:08:54,500
Would be something
that the agent could control

160
00:08:54,500 --> 00:08:58,738
and then policies are selected
based on this thing,

161
00:08:58,738 --> 00:09:02,041
which is the expected for energy,
which is a function of C,

162
00:09:02,041 --> 00:09:05,811
which is your preferences so the outcomes
that you prefer over others.

163
00:09:06,679 --> 00:09:09,749
So policies are selected
that are going to transition

164
00:09:09,749 --> 00:09:12,985
between states
that generate outcomes or observations

165
00:09:13,319 --> 00:09:15,655
that are as close
to preferences as possible.

166
00:09:16,756 --> 00:09:18,925
And then policies are also

167
00:09:19,725 --> 00:09:23,229
in part controlled by E,
which is a prior over policies

168
00:09:23,229 --> 00:09:25,731
that acts
as a kind of influence of habits

169
00:09:26,265 --> 00:09:29,201
and the schema thing,
which will be important

170
00:09:29,201 --> 00:09:32,271
in the normal process
theory I'll talk about, which is

171
00:09:33,639 --> 00:09:34,240
it's essentially

172
00:09:34,240 --> 00:09:38,311
it's like a temperature parameter
more or less on like a dynamically

173
00:09:38,311 --> 00:09:41,647
updated temperature parameter
that basically controls

174
00:09:42,949 --> 00:09:45,051
how much the expected for energy

175
00:09:45,685 --> 00:09:48,354
contributes to policy selection.

176
00:09:48,387 --> 00:09:51,991
So it's basically saying
if I don't trust my model very much

177
00:09:52,458 --> 00:09:55,294
then policy selection
is going to be influenced by, I expected

178
00:09:55,294 --> 00:09:58,564
for energy to less and by habits more.

179
00:09:59,465 --> 00:10:02,768
And the way the gamma is updated
through this hyper parameter beta,

180
00:10:04,236 --> 00:10:06,305
which again, I'll,

181
00:10:06,305 --> 00:10:09,542
I'll cover this on all more detail
last time, but I'm just kind of reminding

182
00:10:09,542 --> 00:10:14,146
the reader about the elements
that are going to correspond

183
00:10:14,146 --> 00:10:17,717
to different aspects of the
of the neural process theory.

184
00:10:19,251 --> 00:10:23,122
So and just my only app
and my only comment on this is again,

185
00:10:23,122 --> 00:10:26,359
think about incremental layers
of complexity in the model.

186
00:10:26,359 --> 00:10:30,730
On the top left is static perception,
hidden states, observable states.

187
00:10:30,730 --> 00:10:31,530
And you're doing inference

188
00:10:31,530 --> 00:10:35,167
on the hidden states
as given your prior D so instantaneous

189
00:10:35,968 --> 00:10:38,771
hidden state inference given observables
bottom left,

190
00:10:38,938 --> 00:10:40,506
you're talking about hidden states

191
00:10:40,506 --> 00:10:44,243
changing through time with the inference
of a B matrix top right.

192
00:10:44,510 --> 00:10:49,315
You introduce the idea that PI policy
is going to intervene into how states

193
00:10:49,315 --> 00:10:52,752
change through time and that that policy
is selected by a preference

194
00:10:53,119 --> 00:10:55,855
C that gets transmuted
into a free energy.

195
00:10:56,355 --> 00:10:59,191
And then the bottom right
is the full fleshed out skeleton

196
00:10:59,425 --> 00:11:02,995
where we also see
the gamma and beta as described by right.

197
00:11:03,095 --> 00:11:06,866
So just start top, left,
bottom left, top, right, bottom right.

198
00:11:06,999 --> 00:11:10,403
And that's adding up one more thing
for each layer of the model.

199
00:11:10,403 --> 00:11:13,739
And then now we're going to be working
with the bottom right model. Yep.

200
00:11:14,206 --> 00:11:18,744
And and the last sort of important thing
is about the underlying way.

201
00:11:18,778 --> 00:11:20,446
So each of these equations

202
00:11:21,781 --> 00:11:24,483
on the and in each of these boxes,

203
00:11:25,885 --> 00:11:28,187
the, the way that you infer

204
00:11:28,187 --> 00:11:32,091
beliefs about states
based on observation is through

205
00:11:32,091 --> 00:11:36,195
a set of equations that correspond
to approximate Bayesian inference

206
00:11:36,929 --> 00:11:40,666
through minimizing free energy
and expected for energy

207
00:11:41,634 --> 00:11:44,236
So the idea is more or less

208
00:11:44,236 --> 00:11:46,605
how the system
how in the normal process theory

209
00:11:46,605 --> 00:11:50,443
does this system settle
on a set of posterior beliefs or states

210
00:11:51,210 --> 00:11:53,512
through free energy,
free energy minimization,

211
00:11:54,447 --> 00:11:58,918
and how in the brain that's
going to correspond to the normal process

212
00:11:58,918 --> 00:12:02,521
theory to minimizing the particular types
of prediction errors.

213
00:12:03,789 --> 00:12:06,125
Now, but I want to be clear, though,
that these sorts of prediction

214
00:12:06,125 --> 00:12:08,094
errors are not identical.

215
00:12:08,094 --> 00:12:09,929
This is not the same process theory

216
00:12:09,929 --> 00:12:12,364
as what's associated with predictive
coding and perception

217
00:12:13,165 --> 00:12:14,033
or predictive coding

218
00:12:14,033 --> 00:12:17,303
as a continuous states based model,
as opposed to the discrete states

219
00:12:17,303 --> 00:12:20,272
based models we're talking about
that have categories right?

220
00:12:20,272 --> 00:12:24,410
You can only be in state one, state
to state three, as opposed to the sorts

221
00:12:24,410 --> 00:12:28,981
of continuous states basis associated
with things like brightness, for example,

222
00:12:30,416 --> 00:12:31,951
So so this is

223
00:12:31,951 --> 00:12:34,653
this has a similar kind of predictive
error minimization flavor

224
00:12:34,920 --> 00:12:37,223
to predictive coding models,
but it's not the same thing.

225
00:12:38,924 --> 00:12:42,828
So now to start

226
00:12:42,828 --> 00:12:47,733
with kind of abstractly,
the, the, the way to see how prediction

227
00:12:47,733 --> 00:12:53,639
errors come in really naturally is just
to remember that for the energy, right?

228
00:12:53,739 --> 00:12:56,642
One, one decomposition
that we've talked about in

229
00:12:56,642 --> 00:13:00,713
previous sessions is you can think of
free energy as to be being decomposed

230
00:13:00,713 --> 00:13:03,749
into a complexity,
a term minus an accuracy term,

231
00:13:04,750 --> 00:13:06,886
where accuracy just means

232
00:13:06,886 --> 00:13:10,089
basically
how close your predicted outcomes are to

233
00:13:11,190 --> 00:13:12,858
the observed outcomes.

234
00:13:12,858 --> 00:13:16,862
And complexity corresponds
to basically how much you have to move

235
00:13:16,862 --> 00:13:20,199
your beliefs from prior beliefs
to posterior beliefs.

236
00:13:20,733 --> 00:13:24,537
So minimizing free energy
means coming to the beliefs that

237
00:13:25,805 --> 00:13:26,605
change your beliefs

238
00:13:26,605 --> 00:13:29,608
as little as possible
while also maximizing accuracy.

239
00:13:31,076 --> 00:13:34,547
So so that's the kind of idea,
a very kind of intuitive way

240
00:13:34,547 --> 00:13:36,882
to think of free energy

241
00:13:38,450 --> 00:13:43,322
and so given that definition,
you can think of the great answer.

242
00:13:43,322 --> 00:13:45,991
The way that free energy changes
with respect

243
00:13:45,991 --> 00:13:48,694
to these approximate posterior beliefs
that we're trying to come to.

244
00:13:49,094 --> 00:13:49,595
Right.

245
00:13:49,595 --> 00:13:53,299
These can always be expressed as
a particular mixture of prediction errors

246
00:13:53,699 --> 00:13:57,469
I think about this is pretty intuitive
because complexity, like I said, is

247
00:13:57,469 --> 00:14:01,140
just the average difference
between posterior and for our beliefs.

248
00:14:01,674 --> 00:14:04,610
So minimizing that difference

249
00:14:04,610 --> 00:14:07,780
can be seen as a type of prediction error

250
00:14:07,780 --> 00:14:08,747
and minimizing.

251
00:14:08,747 --> 00:14:12,685
And then accuracy is similarly
just the difference between predicted

252
00:14:12,685 --> 00:14:14,220
and observed outcomes.

253
00:14:14,220 --> 00:14:18,691
So minimizing that difference can also
be seen as as a type of prediction error.

254
00:14:20,059 --> 00:14:21,327
So active

255
00:14:21,327 --> 00:14:25,231
inference can be implemented
then as this kind of prediction

256
00:14:25,264 --> 00:14:26,699
error minimization

257
00:14:26,699 --> 00:14:30,069
that just corresponds to a minimization
of these two differences

258
00:14:31,003 --> 00:14:34,306
at a computational level of description
as opposed to an algorithmic

259
00:14:34,306 --> 00:14:37,443
or neural level description

260
00:14:37,910 --> 00:14:40,079
So the kind of standard

261
00:14:40,079 --> 00:14:43,015
and fairly daunting
looking figures that you'll typically see

262
00:14:43,115 --> 00:14:46,185
and act of inference paper is associated
with the neural process theory

263
00:14:46,852 --> 00:14:48,654
look something like this.

264
00:14:48,687 --> 00:14:52,091
So on the left you have a bunch of
initially pretty scary

265
00:14:53,726 --> 00:14:57,196
equations
that correspond to the the actual kind

266
00:14:57,196 --> 00:14:59,732
of dynamics that a neural system
is going to have to implement.

267
00:15:01,367 --> 00:15:04,737
And then on the right
and there's kind of a little fun

268
00:15:04,737 --> 00:15:09,408
kind of schematic ball neuron setups
where you have particular layers that

269
00:15:09,408 --> 00:15:10,142
are supposed to kind of,

270
00:15:11,210 --> 00:15:11,443
you know,

271
00:15:11,443 --> 00:15:13,879
kind of heuristics perhaps
correspond to different,

272
00:15:14,813 --> 00:15:18,417
different layers of neurons
and cortical columns.

273
00:15:19,952 --> 00:15:21,954
And each of these layers of neurons

274
00:15:22,087 --> 00:15:26,725
are sort of proposed, again,
very just kind of like schematically

275
00:15:27,393 --> 00:15:30,996
to correspond to different variables
in the equations over here.

276
00:15:31,497 --> 00:15:36,535
And the synaptic connections correspond
to a particular mathematical operations.

277
00:15:36,969 --> 00:15:42,141
So and so here,
kind of the easy way to think about it

278
00:15:42,141 --> 00:15:45,344
is that these red connections here

279
00:15:45,344 --> 00:15:49,348
are excitatory or excitatory
just means addition in the equations.

280
00:15:49,848 --> 00:15:52,351
Whereas these inhibitory

281
00:15:52,351 --> 00:15:54,553
bluish purple connections

282
00:15:54,553 --> 00:15:58,357
with the balls
at the end of the arrows are inhibitory,

283
00:15:58,390 --> 00:16:00,993
which just means subtraction
in the equations,

284
00:16:01,493 --> 00:16:05,397
whereas the green modulatory connections

285
00:16:05,798 --> 00:16:08,467
and that just corresponds
to multiplication

286
00:16:09,335 --> 00:16:12,104
and the update equations
on the other side of the panel.

287
00:16:12,104 --> 00:16:15,174
So the idea is that you can use this
kind of using this kind of scheme,

288
00:16:15,174 --> 00:16:19,144
you can just follow along,
you know, just follow the synapses

289
00:16:19,144 --> 00:16:22,982
and the ball neurons that correspond
to the particular variables

290
00:16:24,283 --> 00:16:25,084
and you can kind of

291
00:16:25,084 --> 00:16:30,723
get a sense of how you can just
connect up neurons like this to solve

292
00:16:30,756 --> 00:16:33,826
the equations that need to be solved
to implement active inference.

293
00:16:34,426 --> 00:16:37,062
But just say one thing about this,
I think makes it

294
00:16:37,062 --> 00:16:38,597
a little bit more intuitive.

295
00:16:38,597 --> 00:16:42,134
So because all of the updates are done
in log space

296
00:16:42,568 --> 00:16:45,604
additions and basically like adding on

297
00:16:46,071 --> 00:16:49,475
putting probability distributions
together is just addition subtraction,

298
00:16:51,143 --> 00:16:53,445
which is nice thinking about it
in terms of this game.

299
00:16:53,779 --> 00:16:56,515
And then generally speaking,
the modulatory connections

300
00:16:56,515 --> 00:16:58,617
will correspond
to some precision parameter

301
00:17:01,053 --> 00:17:03,422
which will modulate the precision
of the distribution.

302
00:17:03,689 --> 00:17:04,590
That makes sense.

303
00:17:04,590 --> 00:17:06,358
So that's kind of
how you think about there.

304
00:17:06,358 --> 00:17:07,459
I find that helpful

305
00:17:07,459 --> 00:17:09,962
in thinking about the relationship
between something like this

306
00:17:10,596 --> 00:17:13,232
and the actual
update equations themselves.

307
00:17:14,199 --> 00:17:14,600
Mm hmm.

308
00:17:14,600 --> 00:17:17,102
Yeah, absolutely. Yeah. Thanks for adding
that.

309
00:17:17,136 --> 00:17:22,474
It's almost like the anatomy of the model
we're doing a schematic neuroanatomy

310
00:17:22,741 --> 00:17:26,078
that's a simplified or abstracted
based upon the actual patterns

311
00:17:26,078 --> 00:17:27,379
of neural connections.

312
00:17:27,379 --> 00:17:31,450
And then we're looking at the anatomy
of this computational model

313
00:17:31,717 --> 00:17:35,287
and thinking about how it could be
plausibly carried out on neural

314
00:17:35,654 --> 00:17:38,791
systems, neural processes,
but also it stands alone.

315
00:17:38,791 --> 00:17:41,627
And so this is just another way
that we can look at it.

316
00:17:41,627 --> 00:17:42,061
Yeah.

317
00:17:42,061 --> 00:17:48,133
So so another thing that I really want
to be clear about is that active

318
00:17:48,133 --> 00:17:53,072
inference, per se, isn't committed
to this particular implementation.

319
00:17:53,439 --> 00:17:56,942
There's there's quite a few different
ways that you could come up with

320
00:17:57,276 --> 00:17:59,311
to connect a bunch of neurons together.

321
00:17:59,311 --> 00:18:01,113
To solve these equations.

322
00:18:01,113 --> 00:18:04,650
So there's there's not kind of a one
to one mapping between the way

323
00:18:04,650 --> 00:18:08,987
that the equations look and the way
that neurons could solve those equations.

324
00:18:09,888 --> 00:18:12,991
So this is an example, but ultimately

325
00:18:12,991 --> 00:18:16,028
it's a separate
empirical question what way

326
00:18:16,995 --> 00:18:19,765
the brain is
actually set up to solve these equations,

327
00:18:19,765 --> 00:18:22,134
even if those equations are
what are being solved?

328
00:18:23,535 --> 00:18:24,203
Yeah.

329
00:18:24,570 --> 00:18:28,440
And even if so, then we can also,
let's say turns out variational message

330
00:18:28,440 --> 00:18:31,343
passing like marginal message
passing is just a really bad metaphor.

331
00:18:31,376 --> 00:18:31,977
Well, that's fine

332
00:18:31,977 --> 00:18:32,344
because we've got

333
00:18:32,344 --> 00:18:35,047
all sorts of other things like late
propagation, blah, blah, blah, blah,

334
00:18:36,482 --> 00:18:38,083
and they can all

335
00:18:39,384 --> 00:18:42,354
I think the main thing is that you're
minimizing

336
00:18:42,354 --> 00:18:45,390
variational free energy
and expected for energy.

337
00:18:46,225 --> 00:18:49,761
The rest is part of specific parts
of the process theory that we can

338
00:18:51,463 --> 00:18:52,898
hopefully falsify

339
00:18:52,898 --> 00:18:56,268
or at least more realistically
not find useful anymore.

340
00:18:56,869 --> 00:18:57,469
Yeah.

341
00:18:57,536 --> 00:18:59,605
And so let's just say
one more note on that.

342
00:18:59,605 --> 00:19:01,440
You brought up
how there's the computational

343
00:19:01,440 --> 00:19:04,243
and the algorithmic levels,
and we're in the process theory realm.

344
00:19:04,343 --> 00:19:05,777
So for those who might not be familiar

345
00:19:05,777 --> 00:19:08,080
with process theory
or Mars levels, of analysis,

346
00:19:08,413 --> 00:19:10,682
the computational level
is like the function,

347
00:19:10,682 --> 00:19:13,585
like you have to sort a list
from the biggest to the smallest.

348
00:19:13,819 --> 00:19:15,854
You can't falsify that idea.

349
00:19:16,021 --> 00:19:18,157
It's computational, it's functional.

350
00:19:18,157 --> 00:19:20,626
Then you go
one layer down into the algorithm.

351
00:19:20,626 --> 00:19:22,327
So you say, I have a sorting algorithm.

352
00:19:22,327 --> 00:19:25,731
There's many sorting algorithms,
but this is the implementation

353
00:19:25,731 --> 00:19:29,468
as it's actually going to be written
in a certain pseudo code.

354
00:19:29,801 --> 00:19:33,872
So that's one of many possible algorithms
that do a certain computational task

355
00:19:33,872 --> 00:19:34,740
like sorting.

356
00:19:34,740 --> 00:19:38,644
And then the process theory is like
the specific program you can think of.

357
00:19:38,844 --> 00:19:42,681
It's the actual implementation now as to
whether it's actually

358
00:19:42,681 --> 00:19:45,484
being implemented that way,
you might be right or wrong.

359
00:19:45,751 --> 00:19:46,251
So that's what

360
00:19:46,251 --> 00:19:50,055
we're talking about with the false
of a falsification of the process theory.

361
00:19:50,389 --> 00:19:53,559
But at the computational level, it's
what Christopher just said.

362
00:19:53,559 --> 00:19:55,561
It's the imperative imperatives
that there's a free energy

363
00:19:55,561 --> 00:19:58,630
minimization happening
and now we want to be building up

364
00:19:58,897 --> 00:20:02,201
specific plausible examples
to work within that framework for.

365
00:20:03,535 --> 00:20:06,238
Just to jump in real
quick, it sounds like in the next stream

366
00:20:06,238 --> 00:20:09,141
we might be discussing
more of these empirical examples

367
00:20:09,341 --> 00:20:13,712
and instantiation, but just to say that
from the biological standpoint of what's

368
00:20:13,712 --> 00:20:17,416
going on, for example, when we talk about
a moderate modulatory layer

369
00:20:17,416 --> 00:20:20,519
or that that could happen
in a multitude of different ways

370
00:20:21,220 --> 00:20:25,591
in terms of changing
what the membrane essential of the cell

371
00:20:25,591 --> 00:20:29,361
is and making it more responsive
or receptive to being activated.

372
00:20:29,661 --> 00:20:33,465
Or it could happen through,
you know, gradual plastic processes

373
00:20:33,465 --> 00:20:36,435
that recruit more receptors to the

374
00:20:36,435 --> 00:20:38,470
to the surface
and cause it to become more responsive

375
00:20:38,670 --> 00:20:40,339
to any individual message.

376
00:20:40,339 --> 00:20:44,042
So it's really going to be context
specific and depend on the experiment

377
00:20:44,042 --> 00:20:46,812
and what circuit
we are specifically studying.

378
00:20:48,013 --> 00:20:49,081
Yeah, absolutely.

379
00:20:49,081 --> 00:20:52,751
And I mean, the thing you mentioned about
plasticity makes an important point

380
00:20:52,751 --> 00:20:56,622
that I should make sure
I might have forgot to mention.

381
00:20:56,622 --> 00:21:00,692
But but so in the context of inference,
right over the shortest time

382
00:21:00,692 --> 00:21:03,795
scales,
we're just getting observations, right?

383
00:21:03,795 --> 00:21:07,599
And based on the way that the current
weights of all these connections,

384
00:21:08,000 --> 00:21:11,570
the system is just going to settle on
a, on a free energy minimum,

385
00:21:11,970 --> 00:21:13,639
like a prediction at a minimum.

386
00:21:13,639 --> 00:21:17,509
And then the, the activation patterns
in these neurons

387
00:21:17,576 --> 00:21:21,913
specifically these ones at the top here,
the S party and S or by Tao

388
00:21:21,913 --> 00:21:25,250
and Tao are going to be
the posterior beliefs

389
00:21:25,250 --> 00:21:28,320
about what state you're in.

390
00:21:28,320 --> 00:21:31,590
And but that's again,
just given a fixed set

391
00:21:31,590 --> 00:21:35,894
of synaptic connections that corresponds
to your generative model now.

392
00:21:36,928 --> 00:21:37,763
But with

393
00:21:37,763 --> 00:21:41,133
learning like we discussed last time,
I want to show up learning simulations.

394
00:21:41,533 --> 00:21:46,104
Learning corresponds to slowly
changing the synaptic the strength of

395
00:21:47,139 --> 00:21:49,374
each of these synaptic connections
themselves.

396
00:21:50,442 --> 00:21:53,211
So you also over slower time scales
minimize expected

397
00:21:53,211 --> 00:21:55,480
for energy and variational free energy

398
00:21:56,915 --> 00:22:00,619
through through
changing these connections strengths

399
00:22:01,653 --> 00:22:03,755
over slower timescales via

400
00:22:03,755 --> 00:22:06,391
kind of synaptic plasticity processes

401
00:22:06,391 --> 00:22:09,461
like Long-Term Potentiation
and long term depression, for example,

402
00:22:10,295 --> 00:22:13,432
as a way of improving the accuracy
of your generative model.

403
00:22:14,566 --> 00:22:17,135
So so just to kind of reiterate,

404
00:22:17,636 --> 00:22:20,272
inference corresponds
to changing activity levels,

405
00:22:20,539 --> 00:22:23,241
whereas learning corresponds
to changing synaptic strengths

406
00:22:24,710 --> 00:22:27,245
so if that's if that's a helpful

407
00:22:29,047 --> 00:22:31,149
so so again, yeah,
I mean, like, like they were saying,

408
00:22:31,149 --> 00:22:34,186
you have basically three
different levels of hypothesis to test.

409
00:22:34,186 --> 00:22:37,389
One is at the computational
level, does the brain

410
00:22:38,523 --> 00:22:41,360
minimize free energy
and expect to free energy or at least

411
00:22:41,760 --> 00:22:44,830
is it helpful to describe it
as doing that?

412
00:22:45,564 --> 00:22:48,767
And then second question is,
given that that first one is true,

413
00:22:49,134 --> 00:22:52,170
which of several different
algorithms is the brain using?

414
00:22:52,471 --> 00:22:56,007
And then question three is given that
it's using, again, a specific algorithm,

415
00:22:56,742 --> 00:23:01,246
how are the how is the brain set
up to implement that algorithm?

416
00:23:01,279 --> 00:23:03,582
So there's kind of a one too many mapping
between each level.

417
00:23:04,683 --> 00:23:08,387
And a final point to make is actually
some people talk about these different,

418
00:23:08,420 --> 00:23:11,189
you know, about quote unquote memory
and levels of

419
00:23:11,523 --> 00:23:14,559
computational level, algorithmic
level and implementation level.

420
00:23:14,926 --> 00:23:17,162
People talk about them
as though they're entirely independent

421
00:23:18,029 --> 00:23:20,532
but they're actually not
necessarily independent.

422
00:23:20,532 --> 00:23:23,568
And sometimes the algorithm

423
00:23:24,302 --> 00:23:26,505
can depend a bit that can be dependencies

424
00:23:27,105 --> 00:23:30,809
between the algorithm and able mutation,
because some of the things

425
00:23:30,809 --> 00:23:32,310
that the algorithm is going to have to

426
00:23:32,310 --> 00:23:35,380
compute are things about, for instance,
like energy costs,

427
00:23:36,148 --> 00:23:38,683
which are going to depend
on the implementation.

428
00:23:39,451 --> 00:23:41,019
So anyway, just
it's kind of a side point.

429
00:23:42,687 --> 00:23:43,955
But but

430
00:23:43,955 --> 00:23:48,226
in general, just to kind of you know,
walking through, you know, a sense here,

431
00:23:48,226 --> 00:23:51,530
so you might start out with
these observations at the bottom, right?

432
00:23:51,530 --> 00:23:52,597
And these are going to go up

433
00:23:52,597 --> 00:23:56,601
through these excitatory connections
to this pink layer three here,

434
00:23:56,601 --> 00:23:59,971
which is going to correspond to the state
prediction error we'll talk about.

435
00:23:59,971 --> 00:24:02,908
And so these excitatory connections
are going to drive increases

436
00:24:03,275 --> 00:24:06,011
in activity in these pink guys.

437
00:24:07,379 --> 00:24:10,215
And those are going to be inhibited
by prior beliefs

438
00:24:10,215 --> 00:24:13,351
about states at this higher level,
which jointly.

439
00:24:13,385 --> 00:24:13,652
Right.

440
00:24:13,652 --> 00:24:17,722
Is going to lead this to this level to

441
00:24:18,990 --> 00:24:22,928
decrease
while the system finds states up here

442
00:24:22,928 --> 00:24:26,731
at the level above that best account

443
00:24:26,731 --> 00:24:29,835
for or predict those observations.

444
00:24:30,802 --> 00:24:33,271
So these red arrows going up here

445
00:24:33,638 --> 00:24:36,575
and can correspond to interactions

446
00:24:36,575 --> 00:24:40,212
between A and oh in these equations.

447
00:24:41,546 --> 00:24:45,183
And so you can kind of you can walk
your way through kind of step by step.

448
00:24:45,183 --> 00:24:47,252
And we do this kind of step
by step in the paper

449
00:24:47,519 --> 00:24:50,322
where each of these interactions

450
00:24:51,122 --> 00:24:54,259
corresponds to elements in the equations.

451
00:24:54,259 --> 00:24:58,830
So for instance, like to get states
to get states up here, right?

452
00:24:58,830 --> 00:25:01,800
That corresponds
to this fourth equation down.

453
00:25:02,200 --> 00:25:04,769
And that's by multiplying policies

454
00:25:06,004 --> 00:25:08,373
by states given policies.

455
00:25:08,807 --> 00:25:09,107
Right.

456
00:25:09,107 --> 00:25:13,645
So I here multiplying the

457
00:25:16,248 --> 00:25:18,817
the S's and the Ts here that are added

458
00:25:18,817 --> 00:25:23,522
so will lead to the posterior over states
just as one example.

459
00:25:23,922 --> 00:25:27,259
And and then another thing
that's important to

460
00:25:27,659 --> 00:25:31,263
to stress is a lot of times
you'll you'll notice in these diagrams

461
00:25:31,263 --> 00:25:35,267
that there are kind of multiple rows
kind of going back, right?

462
00:25:35,267 --> 00:25:38,303
So there's like these three pink ones
and then these three other pink

463
00:25:38,303 --> 00:25:38,970
ones behind them.

464
00:25:39,938 --> 00:25:42,107
And that's because

465
00:25:42,274 --> 00:25:45,911
each of these posterior beliefs over
states, the systems

466
00:25:45,911 --> 00:25:49,848
trying to come to are with respect
to a particular policy.

467
00:25:50,181 --> 00:25:56,054
So the thing is actually calculating
what its beliefs would be under each

468
00:25:56,054 --> 00:25:59,758
of many policies where each of these rows
corresponds to a policy.

469
00:26:00,926 --> 00:26:05,430
And that's what allows it to then
infer a distribution of our policies

470
00:26:05,430 --> 00:26:08,733
based on the states
that are expected under each policy

471
00:26:08,733 --> 00:26:11,403
and the outcomes
that they would are expected to generate.

472
00:26:12,571 --> 00:26:14,539
So so again,

473
00:26:14,539 --> 00:26:18,109
I'm not going to kind of do this for you
now just because it would take forever.

474
00:26:18,443 --> 00:26:21,413
But if you just kind of
look at each of these equations,

475
00:26:22,147 --> 00:26:25,083
you know, where addition
is, where multiplication is, etc.,

476
00:26:25,083 --> 00:26:28,086
then you'll find that these connections

477
00:26:30,322 --> 00:26:33,391
you know, basically reiterate or show how

478
00:26:35,460 --> 00:26:36,861
how this can implement,

479
00:26:36,861 --> 00:26:40,498
how the neural connections can implement
each of these equation equations.

480
00:26:42,033 --> 00:26:48,807
But to to kind of go into certain
aspects of how this would work

481
00:26:48,807 --> 00:26:51,710
and where the neural processing
really comes in in terms of making

482
00:26:53,011 --> 00:26:55,447
empirical predictions just, just.

483
00:26:55,714 --> 00:26:57,415
Just to jump in real quick

484
00:26:57,415 --> 00:27:00,952
for the sake of moving forward,
just to help me thinking about what

485
00:27:00,952 --> 00:27:03,788
the biology is as we start to talk
about the normal process theory

486
00:27:03,955 --> 00:27:07,392
here you have PI
and you have kind of like,

487
00:27:07,592 --> 00:27:10,028
you know, connections
that are inhibitory excitatory.

488
00:27:10,929 --> 00:27:13,832
Sometimes when I think about phenomena
in the biology

489
00:27:13,932 --> 00:27:17,068
you know, you have an emergent concept
that that's

490
00:27:17,068 --> 00:27:21,106
the result of kind
of a population of cells in this case.

491
00:27:21,106 --> 00:27:25,610
Would it be better to think of this
like a policy as the actual like,

492
00:27:25,810 --> 00:27:28,213
you know, the activation
I'm looking at a single neuron

493
00:27:28,213 --> 00:27:32,217
or am I going to be thinking about that
like at a, you know, a layer or

494
00:27:32,317 --> 00:27:33,518
population level.

495
00:27:33,518 --> 00:27:35,387
Oh, yeah. Yeah. And a very,
very good point.

496
00:27:35,387 --> 00:27:36,755
I mean, I guess that is a very schematic.

497
00:27:36,755 --> 00:27:38,123
And any act of inference,

498
00:27:38,123 --> 00:27:40,291
maybe you'll see that,
you know, the statement as they chase

499
00:27:40,291 --> 00:27:42,694
balls is supposed to correspond
to a population of neurons

500
00:27:43,895 --> 00:27:44,162
Right.

501
00:27:44,162 --> 00:27:46,998
So a population where I mean,
just as one example, right?

502
00:27:46,998 --> 00:27:50,402
Like one subset of neurons
in a given population might encode

503
00:27:50,402 --> 00:27:53,672
the expected free energy of each policy
Right.

504
00:27:53,672 --> 00:27:57,509
So activation across the population of
neurons would have code, for instance,

505
00:27:57,509 --> 00:28:00,745
a distribution over
and all the policies being considered

506
00:28:01,413 --> 00:28:02,714
or something,
something along those lines.

507
00:28:02,714 --> 00:28:05,316
So yeah, no, no, no one's
no one's thinking about this

508
00:28:05,316 --> 00:28:09,387
as each of these being, you know,
single neurons that are solving all this.

509
00:28:09,387 --> 00:28:12,624
I mean, that would be it really.

510
00:28:12,657 --> 00:28:14,626
It'd be a really vulnerable system.

511
00:28:14,626 --> 00:28:18,029
Very little damage to that system where
there would knock the whole thing out.

512
00:28:19,330 --> 00:28:21,733
So, so, so yes, definitely there

513
00:28:21,766 --> 00:28:22,434
there's what's

514
00:28:22,434 --> 00:28:25,236
supposed to correspond to populations,
but a lot of the details of that is

515
00:28:25,603 --> 00:28:29,340
really kind of under specified in the,
in the theory stated.

516
00:28:29,340 --> 00:28:32,677
It's still fairly abstract
kind of at the level I'm showing here.

517
00:28:33,311 --> 00:28:35,146
And so

518
00:28:36,347 --> 00:28:39,150
so that that helpful.

519
00:28:39,751 --> 00:28:40,552
Yeah. Yeah.

520
00:28:40,552 --> 00:28:43,021
That does Okay.

521
00:28:43,688 --> 00:28:45,090
So okay,

522
00:28:45,090 --> 00:28:47,659
so the first one here to focus on
the left

523
00:28:47,826 --> 00:28:51,796
are so this epsilon pi tau thing

524
00:28:52,597 --> 00:28:54,833
and this is what's called state
prediction error.

525
00:28:55,934 --> 00:28:58,803
And more or less what this does is

526
00:28:58,803 --> 00:29:02,340
by minimizing this,
we end up with a posterior overstates

527
00:29:03,441 --> 00:29:07,579
and then the actual variational
for energy for which policy

528
00:29:07,579 --> 00:29:12,016
is just calculated based on beliefs
about states and that error signal.

529
00:29:13,585 --> 00:29:17,355
So and this corresponds to layer three

530
00:29:18,389 --> 00:29:21,092
and layer two here in the

531
00:29:22,460 --> 00:29:25,130
in the schematic on the on the right.

532
00:29:26,297 --> 00:29:31,636
So to to give you a sense so in the paper
we show a specific example,

533
00:29:31,636 --> 00:29:34,472
like a work example of calculating
this kind of prediction error.

534
00:29:35,540 --> 00:29:39,277
And so what this says,
so this is the prediction

535
00:29:39,277 --> 00:29:43,248
error for each policy
and for each time point tile.

536
00:29:44,449 --> 00:29:45,250
And the the

537
00:29:45,250 --> 00:29:48,119
first part here this one half line

538
00:29:48,453 --> 00:29:52,056
B Pi tile as I tell -1 etc.

539
00:29:52,056 --> 00:29:55,860
all the way over the whole thing
here with the B's that are multiplied

540
00:29:56,194 --> 00:30:01,065
by one half, you can think of this whole
entire thing as your prior expectation.

541
00:30:01,966 --> 00:30:04,803
So be from Chi-Town -1.

542
00:30:06,204 --> 00:30:09,574
This is saying,
what do I expect about my current state,

543
00:30:09,908 --> 00:30:12,677
given the state
that I was just in before?

544
00:30:13,711 --> 00:30:16,848
Whereas this one is something
a little more interesting.

545
00:30:17,115 --> 00:30:20,285
It's saying
this is saying sort of beliefs about

546
00:30:21,419 --> 00:30:23,721
expectations that come from the future.

547
00:30:23,721 --> 00:30:27,959
So, you know, what are my beliefs
about a state at a given time?

548
00:30:28,326 --> 00:30:30,495
Given my beliefs about the state
after that?

549
00:30:31,863 --> 00:30:35,500
And, you know, this this might seem
a little bit counterintuitive, but

550
00:30:36,167 --> 00:30:39,204
it allows for retrospective inference

551
00:30:39,838 --> 00:30:41,573
which is something that humans
definitely do.

552
00:30:41,573 --> 00:30:42,473
So think about.

553
00:30:42,473 --> 00:30:45,577
Think about a case where you're sitting
in a room and it's dark

554
00:30:46,110 --> 00:30:49,280
and you don't know whether the room
is the green room or the red room.

555
00:30:51,015 --> 00:30:54,118
And but you sit there for like 5 minutes
and then all of a sudden

556
00:30:54,118 --> 00:30:56,521
you turn on the light
and you see that the room is red.

557
00:30:57,322 --> 00:30:59,257
You don't just infer I'm in a red room.

558
00:30:59,257 --> 00:31:02,627
Now, you infer
I've been in a red room the whole time.

559
00:31:03,561 --> 00:31:06,064
So your beliefs about
the past are updated

560
00:31:06,064 --> 00:31:09,000
based on your beliefs
about the current time point.

561
00:31:09,968 --> 00:31:13,504
So what this this second be here.

562
00:31:13,504 --> 00:31:16,908
So expectations
propagating from the future to the past

563
00:31:17,408 --> 00:31:20,678
allows you
to come to beliefs about the states

564
00:31:20,678 --> 00:31:23,648
you were in an earlier times
based on beliefs about later times.

565
00:31:25,149 --> 00:31:28,186
And and so so it's important
to also reiterate here

566
00:31:28,186 --> 00:31:32,790
that each of these essays is for,
you know, as Pie Town -1

567
00:31:33,157 --> 00:31:36,461
and these beliefs about each asset

568
00:31:36,461 --> 00:31:39,797
each time
points are updated at every time point.

569
00:31:40,298 --> 00:31:44,369
So at time three, I can observe something
and update my belief about

570
00:31:44,569 --> 00:31:45,803
the state at time one.

571
00:31:46,771 --> 00:31:48,106
So this is kind of being

572
00:31:48,106 --> 00:31:52,310
re computed
at each time point with a new observation

573
00:31:52,310 --> 00:31:56,314
but about each time point, you know, in
both the past in the future

574
00:31:57,515 --> 00:31:59,851
as well as the present

575
00:31:59,851 --> 00:32:03,821
so, so broadly, then this whole thing,
this one half times

576
00:32:04,022 --> 00:32:06,424
both of these businesses

577
00:32:06,891 --> 00:32:09,827
is your prior expectation.

578
00:32:09,827 --> 00:32:14,098
And then that gets added
to the likelihood

579
00:32:14,098 --> 00:32:16,267
function here, which is so a

580
00:32:17,635 --> 00:32:21,472
times oh, which is the actual observation
at a given time.

581
00:32:21,906 --> 00:32:25,843
And so this is like your this thing
together is like your likelihood

582
00:32:26,277 --> 00:32:28,980
and so this again is very similar

583
00:32:30,315 --> 00:32:33,217
to just kind of another form
of representing

584
00:32:33,885 --> 00:32:36,487
Bayes theorem where you have priors
and you have a likelihood

585
00:32:36,888 --> 00:32:40,558
and you're trying to come to something
that will inform your posterior beliefs

586
00:32:40,558 --> 00:32:41,192
or states.

587
00:32:42,694 --> 00:32:45,229
And then that whole thing is subtracted

588
00:32:45,630 --> 00:32:48,666
from beliefs about the current state
at that moment.

589
00:32:49,767 --> 00:32:53,771
And more or less, if you just kind of run
this over and over again,

590
00:32:54,439 --> 00:32:56,841
you'll settle on a set of beliefs
about states

591
00:32:56,841 --> 00:32:59,110
that minimizes this thing.

592
00:33:00,044 --> 00:33:02,814
And just to show an example of that,

593
00:33:02,814 --> 00:33:06,784
we specify here,
you know, what a could look like.

594
00:33:07,018 --> 00:33:08,853
Right. So in this case,

595
00:33:08,987 --> 00:33:11,622
state
one versus state to the two column state,

596
00:33:11,622 --> 00:33:16,427
one would generate, say, observation
one with 0.8,

597
00:33:17,662 --> 00:33:20,264
whereas state two here, right.

598
00:33:20,264 --> 00:33:24,602
And the right column would generate
outcome one with 0.4 and so forth.

599
00:33:25,069 --> 00:33:28,473
So that would be your likelihood.

600
00:33:28,673 --> 00:33:29,774
Now be here.

601
00:33:29,774 --> 00:33:33,978
Like I said before, this is your prior
from based on the previous time point.

602
00:33:34,512 --> 00:33:39,417
And here we're just saying column again
here is state at time Tao

603
00:33:39,851 --> 00:33:42,587
and a row is time to -1 or

604
00:33:44,255 --> 00:33:47,458
you know, from time -1 This is basically

605
00:33:47,458 --> 00:33:51,295
this is saying given that I was in a
given that I was in a state at time.

606
00:33:51,596 --> 00:33:54,732
Tao What is my belief about the state
I'll transition to?

607
00:33:54,999 --> 00:33:58,302
So this is saying,
I believe if I'm in state one, I'm

608
00:33:58,302 --> 00:34:00,938
most likely staying at state one.

609
00:34:01,672 --> 00:34:05,009
But there's a point two probability
that I could move to state to

610
00:34:06,978 --> 00:34:09,947
whereas the B pital

611
00:34:10,715 --> 00:34:14,052
is .3.3 and .7.7,
which means no matter what

612
00:34:14,052 --> 00:34:16,921
I expect, it's more likely I'm
going to move to state two.

613
00:34:18,322 --> 00:34:20,458
And then what I actually observe

614
00:34:20,458 --> 00:34:23,728
is outcome one

615
00:34:23,728 --> 00:34:27,865
and my prior beliefs are just 25.5.

616
00:34:27,865 --> 00:34:31,869
So I have no, I didn't have
a strong prior either way to start with

617
00:34:32,970 --> 00:34:35,006
and this V thing is

618
00:34:35,239 --> 00:34:37,909
just a depolarization variable

619
00:34:38,109 --> 00:34:40,745
which doesn't come into this prediction
error equation.

620
00:34:41,212 --> 00:34:43,214
So I'll just
kind of leave that for below.

621
00:34:43,681 --> 00:34:47,051
But so this is just an example of how
if you plug

622
00:34:47,051 --> 00:34:51,322
in each of these
things into this equation,

623
00:34:52,390 --> 00:34:54,926
then just kind of follow along
with the numbers.

624
00:34:55,226 --> 00:34:59,764
Eventually
you'll end up with this prediction error,

625
00:34:59,764 --> 00:35:04,202
this -0.2231 negative .999163

626
00:35:05,503 --> 00:35:08,940
and and you can think about

627
00:35:09,874 --> 00:35:12,043
and so then if we move

628
00:35:13,711 --> 00:35:16,547
here, then we can take this V,

629
00:35:16,547 --> 00:35:19,183
which is your initial kind
of depolarization

630
00:35:21,119 --> 00:35:23,020
level in a given neuron

631
00:35:23,020 --> 00:35:25,323
then you add the prediction
error to that.

632
00:35:26,057 --> 00:35:28,926
So the depolarization,
the polarization level changes

633
00:35:29,994 --> 00:35:32,430
so as to be a change in the activity
level of the neuron

634
00:35:32,430 --> 00:35:34,665
in terms of depolarization.

635
00:35:34,665 --> 00:35:37,435
But then posteriors overstates

636
00:35:37,435 --> 00:35:41,973
then correspond to normalizing,
putting a soft max over

637
00:35:42,006 --> 00:35:46,911
this depolarization which then turns it
into your actual posterior or states.

638
00:35:47,378 --> 00:35:50,414
So this is based so this ultimately says
my posterior belief

639
00:35:50,414 --> 00:35:55,319
is that I'm more likely in state
1.6, six, seven, six, seven.

640
00:35:56,120 --> 00:35:59,323
Then in being in state, 2.3,
three, three.

641
00:36:00,224 --> 00:36:03,194
And this then would correspond
to the firing rates

642
00:36:03,961 --> 00:36:06,731
in the process theory associated with

643
00:36:08,432 --> 00:36:10,067
each possible state.

644
00:36:10,067 --> 00:36:12,870
So V is going to be your depolarization

645
00:36:13,604 --> 00:36:16,207
and S is going to be
the resulting firing range.

646
00:36:17,608 --> 00:36:18,543
Add something to this.

647
00:36:18,543 --> 00:36:21,712
So generally speaking, easier or sorry.

648
00:36:22,847 --> 00:36:24,048
Epsilon. Epsilon.

649
00:36:24,048 --> 00:36:27,118
Yeah, a little epsilon is a free energy

650
00:36:27,118 --> 00:36:28,119
gradient

651
00:36:30,688 --> 00:36:32,456
It is a variational range gradient.

652
00:36:32,456 --> 00:36:34,091
And then this awesome.

653
00:36:34,091 --> 00:36:38,029
What we what we end up doing here when
we're adding essentially when we have V,

654
00:36:38,462 --> 00:36:41,866
when we're placing our negative
fringe gradient, we're essentially

655
00:36:41,866 --> 00:36:45,303
just doing a gradient descent
on variational free energy.

656
00:36:45,303 --> 00:36:47,371
That's exactly what this is actually,

657
00:36:47,371 --> 00:36:50,041
and that's how it's implemented
in the code only difference

658
00:36:50,041 --> 00:36:51,876
between this
and what we're doing in the code

659
00:36:51,876 --> 00:36:54,879
is that we have a little steps,
have a little step thing.

660
00:36:54,912 --> 00:36:58,149
So we divide our epsilon by like two

661
00:36:58,149 --> 00:37:01,285
or whatever
time constants for gradient descent is.

662
00:37:02,486 --> 00:37:03,421
So. You know,

663
00:37:03,421 --> 00:37:07,325
tying it back to that previous schematic
that we had where we had

664
00:37:07,325 --> 00:37:11,929
the arrows that were excitatory
and we're adding this on to the putative,

665
00:37:12,263 --> 00:37:14,732
you know, membrane potential
that we have described here

666
00:37:14,732 --> 00:37:16,767
that might reflect
these kind of probabilities.

667
00:37:17,168 --> 00:37:21,472
Then if we have this kind of perhaps this
if this were an excitatory interaction

668
00:37:21,472 --> 00:37:24,842
that we were looking at,
then maybe if that excitatory interaction

669
00:37:24,842 --> 00:37:28,813
from one particular layer to another
was representing that

670
00:37:29,447 --> 00:37:33,451
variational free energy gradient, we
might expect to see this kind of response

671
00:37:33,451 --> 00:37:36,654
and we might measure that in terms
of some kind of related, you know,

672
00:37:37,622 --> 00:37:41,158
bio like biomarker

673
00:37:41,158 --> 00:37:41,759
of interest.

674
00:37:41,759 --> 00:37:43,861
Is that more or less just.

675
00:37:43,861 --> 00:37:47,131
Yeah, I mean, so well,
I mean, just to go back to the kind of

676
00:37:47,131 --> 00:37:51,135
schematic here, like where this is
happening is like right here, right?

677
00:37:51,135 --> 00:37:54,405
So a B from time one, right?

678
00:37:54,839 --> 00:37:56,440
Which is this purple guy.

679
00:37:56,440 --> 00:38:01,979
So S through B from the previous time
point adds on to the prediction error

680
00:38:02,013 --> 00:38:05,783
at the current time point
and s from the future

681
00:38:06,684 --> 00:38:09,053
believe about the future also adds here.

682
00:38:09,487 --> 00:38:12,023
Whereas the likelihood of a

683
00:38:17,328 --> 00:38:17,595
Yeah.

684
00:38:17,595 --> 00:38:19,697
Is added from from oh,

685
00:38:20,731 --> 00:38:23,134
this is an arbitrary thing here

686
00:38:23,134 --> 00:38:23,501
anyway.

687
00:38:23,501 --> 00:38:25,803
I mean, so that's, that's the

688
00:38:25,803 --> 00:38:29,073
I have to walk through a little bit
to figure out where this

689
00:38:29,073 --> 00:38:30,641
inhibitory what it's inhibitory

690
00:38:30,641 --> 00:38:32,343
when it's coming from

691
00:38:34,245 --> 00:38:36,213
this is the minus L and Yeah.

692
00:38:36,213 --> 00:38:38,916
The current state. Yeah.

693
00:38:38,916 --> 00:38:39,650
Yeah. Okay.

694
00:38:39,650 --> 00:38:42,486
But anyway,
so that's, that's kind of as an example

695
00:38:43,621 --> 00:38:46,624
but, but yes, so,

696
00:38:47,325 --> 00:38:51,796
so by doing that you get this posterior
over states that gives you both

697
00:38:52,596 --> 00:38:56,267
predicted depolarization levels and cells
and predicted firing rates.

698
00:38:56,600 --> 00:38:58,636
And that being said,

699
00:38:59,970 --> 00:39:04,642
those and of themselves are not that easy
to to measure empirically

700
00:39:04,642 --> 00:39:07,178
unless you're doing like a single cell
recording or something like that.

701
00:39:07,712 --> 00:39:13,751
And so the idea that gets taken to
our front here is how you can get

702
00:39:13,751 --> 00:39:16,754
from these predicted firing rates
to something that you can measure

703
00:39:17,521 --> 00:39:21,258
like event
related potentials in doing EEG

704
00:39:22,226 --> 00:39:24,962
or electroencephalography,
where you have a bunch of electrodes

705
00:39:25,162 --> 00:39:29,166
basically attached to somebody's scalp
and you're measuring changes

706
00:39:29,166 --> 00:39:31,802
in neural activity,
changes and potentials

707
00:39:33,270 --> 00:39:35,573
through the scalp from the brain.

708
00:39:35,573 --> 00:39:37,975
When a person is exposed
to a particular stimulus

709
00:39:39,410 --> 00:39:42,012
and the

710
00:39:42,012 --> 00:39:45,116
the normal process theory,
the kind of prediction is that

711
00:39:46,050 --> 00:39:49,720
the Europeans are going to correspond
to the rate of change

712
00:39:51,455 --> 00:39:54,859
in beliefs,
which is going to be the rate of change.

713
00:39:54,859 --> 00:39:58,763
And there's a slight

714
00:40:00,197 --> 00:40:01,399
the rate of change.

715
00:40:01,399 --> 00:40:03,434
And in v very change,

716
00:40:03,434 --> 00:40:07,204
this depolarization thing,
which is going to be a prediction error.

717
00:40:07,705 --> 00:40:10,941
So so just to show an example of this

718
00:40:11,575 --> 00:40:14,345
that me and Chris published recently

719
00:40:15,312 --> 00:40:19,116
in the literature
on the on visual consciousness,

720
00:40:20,117 --> 00:40:23,020
you know,
there's a big interest in in what areas

721
00:40:24,422 --> 00:40:27,057
are specific
to having a conscious experience

722
00:40:27,992 --> 00:40:30,361
and an example of an group

723
00:40:30,361 --> 00:40:35,132
that in the past has been linked
to a conscious experience.

724
00:40:35,132 --> 00:40:38,369
But more recently
and there's been some sort of controversy

725
00:40:38,369 --> 00:40:41,906
because it looks like you're going
to get these three potentials

726
00:40:43,073 --> 00:40:47,144
in a way that's not specific
to your conscious experience.

727
00:40:47,912 --> 00:40:50,681
And so we're interested in seeing
whether the neural process theory

728
00:40:51,048 --> 00:40:53,918
would allow us to kind of show

729
00:40:53,918 --> 00:40:57,521
where this is coming from and could
reconcile these sorts of results.

730
00:40:58,656 --> 00:41:00,591
And so it was based on this
really kind of

731
00:41:00,591 --> 00:41:03,360
simple paradigm where you start out

732
00:41:03,994 --> 00:41:06,330
the person who starts out
looking at a stimulus like this

733
00:41:06,764 --> 00:41:10,201
where there's just these red discs
and these kind of slanty lines.

734
00:41:11,168 --> 00:41:15,973
And then in some trials, what will happen
is they'll kind of transiently

735
00:41:16,273 --> 00:41:18,843
be a little square here that comes up

736
00:41:19,543 --> 00:41:21,879
in the midst of this slanting lines.

737
00:41:21,879 --> 00:41:22,580
Then it'll go away

738
00:41:24,648 --> 00:41:27,084
And basically, what, what

739
00:41:27,418 --> 00:41:31,121
what the manipulation
is, is in some cases you have the person,

740
00:41:31,455 --> 00:41:32,723
you know, focus specifically

741
00:41:32,723 --> 00:41:35,426
on these red desks on the outside
to see if they change color.

742
00:41:36,660 --> 00:41:39,864
And then you ask at the end, you know,
did you ever notice

743
00:41:40,865 --> 00:41:43,801
the square things that pop up

744
00:41:44,068 --> 00:41:46,403
And there are cases when

745
00:41:47,972 --> 00:41:50,541
when they do see the square in cases
when they don't

746
00:41:50,941 --> 00:41:51,475
and you're

747
00:41:51,475 --> 00:41:54,512
looking for differences in the group
is corresponding to when they do versus

748
00:41:54,512 --> 00:41:57,147
they don't under conditions
of different sorts of attention.

749
00:41:58,883 --> 00:42:01,118
And so what?

750
00:42:01,118 --> 00:42:04,188
So what we were trying to do
is to see whether

751
00:42:04,722 --> 00:42:06,891
when you built

752
00:42:07,525 --> 00:42:09,326
an active inference model

753
00:42:09,326 --> 00:42:13,063
that that corresponded to this particular

754
00:42:14,632 --> 00:42:17,935
tension, a task

755
00:42:17,935 --> 00:42:20,871
that's called inattentional blindness
task, you know,

756
00:42:20,871 --> 00:42:25,175
could we reproduce the the P3 pattern
so that's kind of in red here.

757
00:42:25,509 --> 00:42:26,410
This extra

758
00:42:28,879 --> 00:42:31,015
dip in

759
00:42:32,149 --> 00:42:36,353
neural activity as measured by EEG,
that looks like it

760
00:42:36,353 --> 00:42:39,557
typically only happens when a person
consciously experienced something.

761
00:42:39,557 --> 00:42:43,060
But with these recent exceptions
where it looks like it depends

762
00:42:43,060 --> 00:42:46,096
just on whether a stimulus is a task
relevant or not.

763
00:42:47,097 --> 00:42:49,633
And Chris, actually I think, you know,
this was primarily your work.

764
00:42:49,633 --> 00:42:52,336
So so maybe you could describe this
in a little more detail.

765
00:42:52,870 --> 00:42:53,370
Yeah. Yeah.

766
00:42:53,370 --> 00:42:56,840
So first of all, I think to
just give a shout out because we've got

767
00:42:56,840 --> 00:43:00,744
the reference on the slides is this
paradigm was designed by Michael Pitts

768
00:43:02,346 --> 00:43:03,814
and he has done

769
00:43:03,814 --> 00:43:07,885
a lot of really incredible
empirical work on visual consciousness.

770
00:43:07,885 --> 00:43:11,455
So just to kind of want to acknowledge
that anyway.

771
00:43:12,523 --> 00:43:15,125
So generally speaking,
there's three phases in this experiment.

772
00:43:15,125 --> 00:43:17,861
So phase one is they're not told there's
going to be a square

773
00:43:17,895 --> 00:43:22,266
kind of self-assemble and they're asked
just to monitor the disks on the outside.

774
00:43:22,633 --> 00:43:25,235
And then there's
a huge literature on this.

775
00:43:25,235 --> 00:43:27,037
But generally speaking,
in a central blindness,

776
00:43:27,037 --> 00:43:30,040
when you have a set up
that's kind of analogous to that, about

777
00:43:30,040 --> 00:43:34,945
50% of people are intentionally blind
to whatever your manipulation is.

778
00:43:35,379 --> 00:43:37,548
So I think they're really famous.
One is like a group.

779
00:43:37,581 --> 00:43:39,617
If you ask people to monitor

780
00:43:40,651 --> 00:43:42,686
how many times a
basketball is being passed back and forth

781
00:43:42,720 --> 00:43:44,655
within a group of people,
a gorilla can walk past

782
00:43:44,655 --> 00:43:47,891
and wave at people, and about 50% of them

783
00:43:48,692 --> 00:43:50,494
don't report seeing it

784
00:43:51,996 --> 00:43:53,731
So the idea here is just kind of

785
00:43:53,731 --> 00:43:56,200
operationalize
that a little bit more carefully.

786
00:43:56,200 --> 00:43:59,336
And then in phase two is after everyone's
kind of being because

787
00:43:59,336 --> 00:44:02,373
like literally when you ask people off
to phase one, hey, did you see a square,

788
00:44:02,406 --> 00:44:05,242
you're kind of alerting to them the fact
that there was a square present there.

789
00:44:05,843 --> 00:44:08,512
But it's still not
the square start task relevant.

790
00:44:08,545 --> 00:44:11,949
So in phase two, they just do exactly
the same task on the external disks.

791
00:44:13,350 --> 00:44:15,753
And in phase three,
the manipulation is they

792
00:44:15,886 --> 00:44:16,920
then say, Okay, well,

793
00:44:16,920 --> 00:44:20,557
we now want you to count how many times
I'll hit a button whenever the square is

794
00:44:20,958 --> 00:44:24,628
and so the idea was, was that if you kind
of see here when we say percent saying,

795
00:44:24,628 --> 00:44:27,331
we basically said a hierarchical model,
which we're gonna look at in a minute.

796
00:44:27,765 --> 00:44:31,535
And we just had it kind of report
what type of trial

797
00:44:31,535 --> 00:44:35,005
it thought it was in,
did it see a line or not

798
00:44:35,005 --> 00:44:37,641
where a line was just like a kind
of this categorical variable,

799
00:44:38,208 --> 00:44:39,977
or did it see a square

800
00:44:42,112 --> 00:44:45,816
and essentially we just in the same way
that you might in psycho physics,

801
00:44:45,816 --> 00:44:49,953
like titrate the stimulus
or the contrast of a stimulus or whatever

802
00:44:50,187 --> 00:44:50,888
until people say it.

803
00:44:50,888 --> 00:44:55,092
50% of the time, basically what we did
was we titrated it until the model saw it

804
00:44:55,092 --> 00:44:58,328
50% of the time,
or titrate the precision of the matrix

805
00:44:58,562 --> 00:45:00,831
kind of corresponding
to contrast or whatever.

806
00:45:01,432 --> 00:45:05,836
And then we just manipulate attention
where attention corresponds to basically

807
00:45:06,036 --> 00:45:08,505
how precise the model's matrix was

808
00:45:09,440 --> 00:45:12,876
And what we showed was that
when attention, which we operationalized,

809
00:45:12,876 --> 00:45:17,281
is basically like when two steps
of attention with focal attention and

810
00:45:19,116 --> 00:45:23,220
like super precise task, relevant
attention when those is the focal

811
00:45:23,220 --> 00:45:26,890
attention condition, the model reported
saying the thing 99% of the time.

812
00:45:26,890 --> 00:45:30,561
So basically all the time
and when there was precise task

813
00:45:30,561 --> 00:45:33,397
relevant attention the model reported
seeing 100% the time again.

814
00:45:33,597 --> 00:45:35,132
But we said dissociation

815
00:45:35,132 --> 00:45:39,203
between the presence of the P3 in line
with empirical results and so the idea.

816
00:45:39,570 --> 00:45:43,173
So I can actually just highlight
this since I've known that the cursor so

817
00:45:43,207 --> 00:45:46,910
so in this first condition
so this is when like Chris said the

818
00:45:47,044 --> 00:45:50,714
you know the thing had no expectation
right that it was going to that

819
00:45:50,714 --> 00:45:52,449
there's going to be any square there.

820
00:45:52,449 --> 00:45:56,687
And and the precision of the matrix
was low because there was no attention.

821
00:45:57,154 --> 00:46:01,892
And so as a result, the start of the
simulated air appears flat.

822
00:46:02,793 --> 00:46:05,863
And similarly this the second

823
00:46:06,997 --> 00:46:08,298
the second step here,

824
00:46:08,298 --> 00:46:11,235
which is what would correspond to a dip
here, is absent.

825
00:46:12,603 --> 00:46:16,140
And here at the bottom
are the simulated firing rates.

826
00:46:16,240 --> 00:46:19,143
So on the first level, basically
when black at the bottom changes

827
00:46:19,143 --> 00:46:21,979
to black at the top,
that just means it starts to believe

828
00:46:22,346 --> 00:46:25,716
the upper row is a belief
that the square was present.

829
00:46:26,116 --> 00:46:28,252
And the bottom is the belief
that the square was absent.

830
00:46:28,685 --> 00:46:31,522
So basically what this means is
when it goes from black

831
00:46:31,522 --> 00:46:34,625
at the bottom to black at the top,
that the belief changes.

832
00:46:34,925 --> 00:46:38,529
So it's more likely
that the stimulus was present.

833
00:46:40,030 --> 00:46:41,965
And then you can see

834
00:46:41,965 --> 00:46:44,701
because the matrix is imprecise

835
00:46:45,102 --> 00:46:48,172
then, which corresponds
to like a low stimulus strength.

836
00:46:48,172 --> 00:46:50,707
And this then this is not very

837
00:46:51,842 --> 00:46:53,777
you know, it's still pretty blurry.

838
00:46:53,777 --> 00:46:57,114
And then the second level,
these are beliefs about sequences

839
00:46:57,114 --> 00:46:58,115
that are what's actually going on.

840
00:46:58,115 --> 00:47:02,853
In other words, did it go from no square
to square and back to know square?

841
00:47:03,921 --> 00:47:07,024
And you can see that
the beliefs remain really imprecise

842
00:47:07,791 --> 00:47:11,762
because it's not it's
not attending to it.

843
00:47:11,762 --> 00:47:14,131
It's not task relevant.

844
00:47:14,131 --> 00:47:16,633
In this case, though, I was talking about

845
00:47:17,367 --> 00:47:20,504
you can see that
even though it's not task relevant,

846
00:47:20,938 --> 00:47:24,942
the beliefs at the higher level
ultimately do converge onto this top row,

847
00:47:24,942 --> 00:47:27,511
which is the one
where the square was present.

848
00:47:28,412 --> 00:47:30,948
But you can see that the beliefs change
pretty gradually,

849
00:47:31,982 --> 00:47:35,786
whereas one attention was precise
because it was task relevant

850
00:47:36,220 --> 00:47:39,122
then you can see that the belief changes

851
00:47:39,122 --> 00:47:42,626
from being kind of flat over these past
and two possible top states

852
00:47:42,993 --> 00:47:48,332
to being black, fully confident
that the square turned on and off.

853
00:47:49,800 --> 00:47:52,569
And that really quick change in beliefs

854
00:47:53,070 --> 00:47:56,673
is what corresponds to the strong ERP.

855
00:47:56,673 --> 00:47:58,408
And in this case,
because of the higher level,

856
00:47:58,408 --> 00:48:01,144
what's that kind of late
ERP corresponding to the P three?

857
00:48:02,246 --> 00:48:05,282
So really what's going on
is the prediction of the neural process

858
00:48:05,282 --> 00:48:09,920
theory is that it's going to be
the rate of change in beliefs.

859
00:48:10,721 --> 00:48:14,625
Basically, that is what's going to be
measured as as an ERP

860
00:48:16,059 --> 00:48:16,660
yeah.

861
00:48:16,927 --> 00:48:18,962
When you think of the owner
say about that, Chris.

862
00:48:18,962 --> 00:48:19,930
No, no, not at all.

863
00:48:19,930 --> 00:48:22,499
I mean, one thing I just want to
highlight is like the only difference

864
00:48:22,499 --> 00:48:24,868
between phase one and phase
two is a very slight difference.

865
00:48:24,868 --> 00:48:27,371
In the precision of the matrix that we

866
00:48:28,505 --> 00:48:32,976
took to basically be the addition
of some some slight focal attention

867
00:48:35,012 --> 00:48:35,479
And but

868
00:48:35,479 --> 00:48:39,216
you can see like just looking at
the figures, like it really made very a

869
00:48:39,283 --> 00:48:41,919
very minor difference,
but it still bumped it up enough

870
00:48:43,153 --> 00:48:45,989
and above the threshold
just enough for it

871
00:48:45,989 --> 00:48:49,026
to basically go from 50% report
to basically 100% report.

872
00:48:50,127 --> 00:48:52,829
Just also one point on the ERP

873
00:48:52,829 --> 00:48:57,000
or the event related potential, it's
defined on the bottom left of the slide.

874
00:48:57,367 --> 00:49:01,238
It's like a population level
measure of the electromagnetic activity

875
00:49:01,505 --> 00:49:04,074
in populations of cells in the brain.

876
00:49:04,608 --> 00:49:08,545
And so it's kind of like on the left,
if something's passing under the radar,

877
00:49:08,812 --> 00:49:12,950
the population is not changing
in broad strokes.

878
00:49:13,350 --> 00:49:17,688
But then when there's this event
that's task relevant, it transiently

879
00:49:17,688 --> 00:49:21,558
synchronizes the population to be doing
certain things in a certain timing

880
00:49:21,725 --> 00:49:25,262
and then that's reflected by changes
in the population level,

881
00:49:25,529 --> 00:49:29,366
electromagnetism patterns
reflected by what are called ERP.

882
00:49:29,399 --> 00:49:32,936
But there's a whole literature with
Princeton and many others on this topic

883
00:49:34,004 --> 00:49:34,338
Yeah.

884
00:49:34,338 --> 00:49:38,008
So is it just depolarization of dendritic

885
00:49:38,008 --> 00:49:38,909
trees?

886
00:49:41,078 --> 00:49:41,345
Yeah.

887
00:49:41,345 --> 00:49:43,347
Which corresponds to the prediction or.

888
00:49:43,347 --> 00:49:45,849
Yeah, which corresponds prediction. Cool.

889
00:49:45,882 --> 00:49:49,052
So basically as prediction error
initially goes up

890
00:49:49,052 --> 00:49:52,456
and then goes back down,
then that would be the

891
00:49:52,856 --> 00:49:55,892
the predicted theory

892
00:49:57,127 --> 00:50:01,665
So and this is kind of related to,

893
00:50:02,232 --> 00:50:05,802
you know, pretty standard neural
mass models that are used in the

894
00:50:05,969 --> 00:50:09,272
in the literature
that make pretty similar assumptions.

895
00:50:09,573 --> 00:50:09,973
Right.

896
00:50:09,973 --> 00:50:11,274
They assume that the average firing

897
00:50:11,274 --> 00:50:14,711
rate of the population can be treated
as a as a sigmoid function.

898
00:50:14,711 --> 00:50:19,583
So a function that looks like this
and of the average membrane potential.

899
00:50:20,851 --> 00:50:25,088
And you can do things like, for instance,
shift this function to the left

900
00:50:25,088 --> 00:50:29,326
or the right based on things
that control the firing rate threshold,

901
00:50:30,727 --> 00:50:33,296
which could relate to inhibitory
interneurons or you know, there's a bunch

902
00:50:33,296 --> 00:50:37,267
of different neural mechanisms involved
but basically this the point is

903
00:50:37,267 --> 00:50:42,172
just that this idea
about where you get firing rates

904
00:50:42,873 --> 00:50:47,177
from average member integer
potentials is not kind of out of nowhere.

905
00:50:47,377 --> 00:50:50,947
It's something that's kind of used
as a standard assumption

906
00:50:50,947 --> 00:50:53,717
in and

907
00:50:53,984 --> 00:50:55,652
these sorts of models.

908
00:50:55,652 --> 00:50:58,855
And one interesting
note is the soft max that sigma

909
00:50:58,855 --> 00:51:01,825
that we use to kind of re
normalized probability.

910
00:51:02,259 --> 00:51:06,530
It's actually utilized
in artificial neurons in neural networks

911
00:51:06,530 --> 00:51:09,366
as also a probability
re normalizing trick.

912
00:51:09,733 --> 00:51:13,703
So we're thinking about it
in kind of like the intersection of the

913
00:51:13,703 --> 00:51:18,108
physical thing and the instrumentalist
approach, realism and instrumental ism.

914
00:51:18,308 --> 00:51:21,778
And we're sealing our active
inference ship together through their

915
00:51:24,481 --> 00:51:25,048
Mm hmm.

916
00:51:25,515 --> 00:51:25,916
Yeah.

917
00:51:25,916 --> 00:51:31,221
So so that's and you know, later
Chris will show specific simulations

918
00:51:31,588 --> 00:51:36,393
based on an actual task that will produce
predicted groups that are consistent

919
00:51:36,393 --> 00:51:39,329
with existing literature
when he goes to the code.

920
00:51:39,663 --> 00:51:42,265
And the second part of this session

921
00:51:43,467 --> 00:51:46,503
but so I'm going to sort of
stop talking about the stay prediction

922
00:51:46,503 --> 00:51:49,673
areas at this point, which I again,
Chris, will pick up in his part.

923
00:51:50,740 --> 00:51:53,376
The second thing that's commonly talked

924
00:51:53,376 --> 00:51:56,947
about as part of the neural process
theory in an act of inference

925
00:51:57,514 --> 00:51:59,950
is this idea of expected free

926
00:51:59,950 --> 00:52:03,186
energy precision
being associated with dopamine.

927
00:52:03,720 --> 00:52:07,190
And so as I write from the start,
that to be technically correct,

928
00:52:07,190 --> 00:52:09,459
we call this expected for energy
precision.

929
00:52:09,459 --> 00:52:12,395
In the literature,
it's often just called policy precision,

930
00:52:14,030 --> 00:52:15,599
but that can be

931
00:52:15,599 --> 00:52:18,401
slightly confusing just because

932
00:52:19,369 --> 00:52:23,240
the precision of the actual posterior
distribution of our policies.

933
00:52:23,607 --> 00:52:26,343
Right. This bolded pie here

934
00:52:26,343 --> 00:52:29,813
corresponds
not just to the precision of expected

935
00:52:29,813 --> 00:52:33,750
free energy
g here but also to F and to E.

936
00:52:34,918 --> 00:52:36,753
So there are ways in which

937
00:52:36,753 --> 00:52:40,123
the expected free energy precision,
this gamma thing that scales g

938
00:52:41,158 --> 00:52:44,461
you know, could be precise or imprecise,
while the actual posterior

939
00:52:44,461 --> 00:52:47,564
distribution of our policies
could still be precise or imprecise

940
00:52:47,898 --> 00:52:51,301
in a way that differs
from the precision of G yeah.

941
00:52:51,535 --> 00:52:54,304
We could be not sure what to do,
but have very clear

942
00:52:54,304 --> 00:52:56,673
understanding of two policies
and their implications.

943
00:52:56,673 --> 00:52:58,808
It's two separate levels of uncertainty.

944
00:52:58,808 --> 00:53:01,044
Not quite that simple. Yeah, that's not.

945
00:53:01,278 --> 00:53:02,345
Yeah, like so.

946
00:53:02,345 --> 00:53:07,717
So a good a good example would be,
for instance, so in the, you know, in

947
00:53:08,685 --> 00:53:11,121
the paper that I wrote with Caspar

948
00:53:11,988 --> 00:53:16,159
and others on the I'm deeply felt
so deeply thought affect.

949
00:53:16,159 --> 00:53:20,630
So the idea about changes
in emotional states or affective states

950
00:53:21,531 --> 00:53:24,301
relates specifically to this better

951
00:53:25,035 --> 00:53:27,904
term here that controls
that expected for energy precision

952
00:53:28,438 --> 00:53:31,942
and now I expected for energy
precision goes down

953
00:53:32,709 --> 00:53:36,379
and that will often correspond
to write a negative

954
00:53:36,379 --> 00:53:40,717
or when expected for energy goes up
then that will correspond to a decrease

955
00:53:40,717 --> 00:53:43,920
in expected for energy precision

956
00:53:45,155 --> 00:53:48,291
but or a difference between F and G

957
00:53:48,892 --> 00:53:53,730
but but there will be cases
where gamma will go down

958
00:53:53,730 --> 00:53:57,667
meaning the model becomes less confident
in its beliefs about

959
00:53:58,802 --> 00:54:00,870
its ability
to minimize expected for energy.

960
00:54:00,870 --> 00:54:04,874
Essentially, it decreases its beliefs,
its confidence in its action model.

961
00:54:05,709 --> 00:54:09,212
But there are cases when that can happen,
despite the fact that the actual

962
00:54:09,646 --> 00:54:13,049
positive or additional distribution
over policies becomes really precise.

963
00:54:13,583 --> 00:54:16,519
So so a kind of nice
example of this would be,

964
00:54:16,786 --> 00:54:19,189
you know, say you're
kind of walking around in the woods

965
00:54:19,189 --> 00:54:23,760
and at the moment you're really confident
in the walking around policy Right.

966
00:54:23,760 --> 00:54:28,231
So there's a precise distribution over,
you know, I should be walking around,

967
00:54:29,165 --> 00:54:31,468
but now you unexpectedly see a bear

968
00:54:33,003 --> 00:54:34,738
now immediately

969
00:54:34,738 --> 00:54:37,374
you're going to switch
to having a very precise post,

970
00:54:37,374 --> 00:54:40,577
just your posterior distribution
over the run away policy.

971
00:54:41,144 --> 00:54:45,949
And so you're going to have two policies,
two distributions of are policies

972
00:54:45,949 --> 00:54:50,720
that are both very precise,
but they also but they change rapidly.

973
00:54:50,920 --> 00:54:55,125
So the policy that the
that the model was confident in before

974
00:54:55,125 --> 00:54:58,628
is very different than the policy
that the model was competent and after.

975
00:54:59,062 --> 00:55:01,464
So despite that,
the posterior distribution

976
00:55:01,464 --> 00:55:03,633
is still pressed over
policies are still precise.

977
00:55:03,967 --> 00:55:06,603
The things very confident what to do,

978
00:55:06,603 --> 00:55:08,705
gamma will still go down.

979
00:55:08,705 --> 00:55:10,840
So the thing will feel negative

980
00:55:11,174 --> 00:55:13,910
like emotion despite being very confident
and running away.

981
00:55:14,444 --> 00:55:16,680
So the thing that the thing
that's driving that is f

982
00:55:17,013 --> 00:55:19,449
I think that's
important to put in or to say

983
00:55:21,951 --> 00:55:22,886
if you don't cash it out.

984
00:55:22,886 --> 00:55:24,654
Right. Or Oh, well, I mean yes.

985
00:55:24,654 --> 00:55:25,455
I mean just in the sense of

986
00:55:25,455 --> 00:55:29,659
so f as F is going to be so based
on new observation, F is the

987
00:55:30,460 --> 00:55:35,165
is the kind of evidence for H policy
at the time of the new observation.

988
00:55:35,598 --> 00:55:41,371
So if F strongly contradicts G, then
that means that the distribution is going

989
00:55:41,371 --> 00:55:44,474
to change in a way
that suggests that you ought

990
00:55:44,474 --> 00:55:47,644
to have less confidence in G.

991
00:55:47,644 --> 00:55:51,614
And and we can we can I'll show
a specific example of that below

992
00:55:52,048 --> 00:55:54,818
and when we actually show
simulations of the

993
00:55:56,353 --> 00:55:58,521
postulated domain responses.

994
00:55:58,521 --> 00:56:02,392
But so the idea here is,
is that dopaminergic

995
00:56:02,425 --> 00:56:07,464
physic dopamine responses
correspond to changes in this parameter

996
00:56:07,464 --> 00:56:11,668
beta, which controls gamma which is the
expected for energy precision

997
00:56:12,902 --> 00:56:15,405
and the
way that that these updates in beta.

998
00:56:15,405 --> 00:56:17,741
So this equation here

999
00:56:18,608 --> 00:56:20,810
just corresponds to the initial beta

1000
00:56:21,778 --> 00:56:24,514
so the non bolded one here

1001
00:56:24,647 --> 00:56:25,982
plus this thing.

1002
00:56:25,982 --> 00:56:26,816
So the posterior

1003
00:56:26,816 --> 00:56:30,620
distribution of our policies minus
the prior distribution of our policies.

1004
00:56:30,987 --> 00:56:33,690
So basically how much did beliefs
about policies change

1005
00:56:34,124 --> 00:56:37,961
and then dotted
with the negative expected for energy

1006
00:56:38,995 --> 00:56:40,363
that was that.

1007
00:56:40,363 --> 00:56:43,600
Yeah, that correspond to that trial
and then minus the previous

1008
00:56:44,501 --> 00:56:47,270
beta which allows us
to just continually iterate.

1009
00:56:48,338 --> 00:56:51,007
But because the change from post

1010
00:56:51,007 --> 00:56:53,910
to your beliefs or policies
from prior beliefs or policies

1011
00:56:56,246 --> 00:56:58,314
just adds F basically

1012
00:56:58,314 --> 00:57:02,385
the prior policies we don't show
it here is just Alan E minus gamma G.

1013
00:57:02,752 --> 00:57:05,455
So that F only comes in to turn the prior

1014
00:57:05,722 --> 00:57:08,491
or policies into a post area
over policies.

1015
00:57:09,359 --> 00:57:12,228
And so this change is driven by F.

1016
00:57:13,630 --> 00:57:15,498
And so the more the

1017
00:57:15,498 --> 00:57:22,472
F is consistent versus inconsistent
with G is what is what controls the

1018
00:57:22,472 --> 00:57:28,311
the agents sort of changes in confidence
about all about its G estimates.

1019
00:57:29,512 --> 00:57:32,348
And so it changes in that
or what are expected

1020
00:57:32,348 --> 00:57:35,585
or proposed to correspond
to a basic domain responses.

1021
00:57:36,052 --> 00:57:40,089
And then the kind of tonic
a level of theta is also proposed

1022
00:57:40,089 --> 00:57:43,226
to be associated
with tonic dopamine levels.

1023
00:57:43,626 --> 00:57:46,262
So if I could just very briefly
say something.

1024
00:57:46,262 --> 00:57:48,431
So essentially gamma is a

1025
00:57:49,999 --> 00:57:51,668
gamma is a

1026
00:57:51,668 --> 00:57:55,438
a right or beta sorry, is a right
parameter from the gamma distribution.

1027
00:57:56,072 --> 00:57:59,642
And essentially we get
you get this derivation.

1028
00:57:59,642 --> 00:58:03,980
So by the dot that that technically
that's a that's a temporal derivative

1029
00:58:03,980 --> 00:58:04,380
right.

1030
00:58:04,380 --> 00:58:07,250
And so this isn't exact
it's a little misleading kind of

1031
00:58:07,851 --> 00:58:12,155
because this actually
is a variational derivative of

1032
00:58:14,123 --> 00:58:15,091
gamma with

1033
00:58:15,091 --> 00:58:18,228
respect to variational free energy
because that makes sense.

1034
00:58:18,261 --> 00:58:21,364
So the idea is,
is that updates or changes in

1035
00:58:21,731 --> 00:58:25,335
essentially this by the value
or minimize variational free energy.

1036
00:58:25,969 --> 00:58:28,905
And if you want to say like that,
so the actual

1037
00:58:30,139 --> 00:58:32,809
actual derivations of that
get kind of kind of gnarly

1038
00:58:33,042 --> 00:58:38,047
but if you're interested, if anyone's
interested I would just say go and read.

1039
00:58:38,047 --> 00:58:42,418
And the appendix of an sales paper
that's in class

1040
00:58:42,418 --> 00:58:46,689
computational biology on the locusts
are really serious and

1041
00:58:47,891 --> 00:58:50,326
learning right in prediction error

1042
00:58:50,326 --> 00:58:52,495
where they should have actions as.

1043
00:58:52,495 --> 00:58:53,229
On as yeah.

1044
00:58:53,229 --> 00:58:57,033
And as in the appendix and on this paper
is actually really great for this

1045
00:58:57,033 --> 00:59:01,971
because plus C be basically like required
it looks like that

1046
00:59:02,272 --> 00:59:07,143
she still like every little detail about
deriving everything in act of inference.

1047
00:59:07,143 --> 00:59:10,280
So to as a really great paper
both in the main text and in the appendix

1048
00:59:10,280 --> 00:59:13,082
for show and kind of exactly
where you get each of these things.

1049
00:59:14,651 --> 00:59:15,251
Yeah, I

1050
00:59:15,251 --> 00:59:19,088
like it pretty much as clearly
as I've seen it.

1051
00:59:19,188 --> 00:59:21,424
Anything else
to add on the neural process front?

1052
00:59:21,791 --> 00:59:24,127
Because there are some questions and
then we'll head into the second section.

1053
00:59:24,127 --> 00:59:25,328
But this is awesome.

1054
00:59:25,328 --> 00:59:28,097
And there's a lot more actually.

1055
00:59:28,097 --> 00:59:29,732
Let's keep going then, then.

1056
00:59:29,732 --> 00:59:32,168
So yeah, we'll do more neural process
theory, however much,

1057
00:59:32,435 --> 00:59:33,670
and then we'll be taking questions.

1058
00:59:33,670 --> 00:59:36,506
We'll have an intermission with
a few questions that are really nice.

1059
00:59:36,539 --> 00:59:39,576
And then Christopher will launch
into the hierarchical model.

1060
00:59:41,010 --> 00:59:44,280
So, so to actually give examples here
because, you know,

1061
00:59:44,280 --> 00:59:47,584
everything we have said about that,
about dopamine and beta updates

1062
00:59:47,584 --> 00:59:49,652
is still pretty abstract.

1063
00:59:50,753 --> 00:59:54,257
So, you know, in the, in the tutorial
we give specific numerical

1064
00:59:54,257 --> 00:59:55,458
examples of this as well.

1065
00:59:57,360 --> 00:59:59,696
To show and give a kind of like geometric

1066
00:59:59,696 --> 01:00:02,899
actually intuition about this.

1067
01:00:04,033 --> 01:00:06,336
And so here you can see so.

1068
01:00:06,336 --> 01:00:09,872
So here we show that the prior policy
is this pi zero.

1069
01:00:10,173 --> 01:00:13,710
I guess that is just the soft
max of l n e minus gamma tree.

1070
01:00:14,344 --> 01:00:19,315
Then the posterior is the soft max of el,
any minus F minus gamma g.

1071
01:00:19,649 --> 01:00:21,084
So they showed before.

1072
01:00:21,084 --> 01:00:24,954
And so
and these are the equations for quote

1073
01:00:25,054 --> 01:00:28,558
unquote dopamine basic domain responses.

1074
01:00:30,026 --> 01:00:33,229
And so if you just take a specific
really simple example,

1075
01:00:33,663 --> 01:00:34,998
so let's say there's no hassle.

1076
01:00:34,998 --> 01:00:36,899
Let's say that Beta zero

1077
01:00:36,899 --> 01:00:41,270
and therefore gamma zero and the initial
theta posterior, those are all just one.

1078
01:00:41,270 --> 01:00:44,641
So we're just starting out,
you know, totally flat, basically.

1079
01:00:45,008 --> 01:00:47,176
And and

1080
01:00:48,511 --> 01:00:51,514
and we'll
say that the agent has no habits.

1081
01:00:51,514 --> 01:00:53,850
So there are two possible policies here,
I should say.

1082
01:00:53,850 --> 01:00:55,051
So the agent has no habits.

1083
01:00:55,051 --> 01:00:59,422
That's just one and 14 counts over
policies.

1084
01:01:00,289 --> 01:01:03,393
Now, let's say that G

1085
01:01:03,393 --> 01:01:05,294
is ten and nine.

1086
01:01:05,294 --> 01:01:08,331
So in other words, there's slightly
higher expected for energy

1087
01:01:08,331 --> 01:01:10,967
for the first policy
than the second policy.

1088
01:01:12,001 --> 01:01:15,672
So now let's say that
if the agent gets a new observation

1089
01:01:16,105 --> 01:01:19,809
and the free energy for each policy
corresponds to 21.

1090
01:01:20,343 --> 01:01:26,115
So as you can see, this is still
consistent with the ten and nine, right?

1091
01:01:26,115 --> 01:01:30,119
It's not like contradicting the policy.

1092
01:01:30,119 --> 01:01:33,389
One still has higher for energy
than policy two.

1093
01:01:34,524 --> 01:01:36,759
So when you calculate

1094
01:01:36,759 --> 01:01:39,662
pi zero using this equation
and these numbers up here,

1095
01:01:40,096 --> 01:01:43,166
you end up initially having a probability

1096
01:01:43,166 --> 01:01:46,069
over policies of 0.218 and .728.

1097
01:01:46,436 --> 01:01:49,706
So policy two is the one that

1098
01:01:50,740 --> 01:01:52,375
is has is

1099
01:01:52,375 --> 01:01:55,144
more likely
given the expected free energy

1100
01:01:55,144 --> 01:01:57,513
because it's lower rate
expected for energy as nine

1101
01:01:59,048 --> 01:02:02,518
Now the poster policies after we put an F

1102
01:02:03,019 --> 01:02:05,521
just ends up being 01.

1103
01:02:05,521 --> 01:02:08,825
And so if you take the difference here
so there's got pi death

1104
01:02:08,825 --> 01:02:14,097
so pi minus pi zero that ends up
being negative point to an 8.218.

1105
01:02:15,364 --> 01:02:17,400
And then if you do this

1106
01:02:17,400 --> 01:02:20,069
adopt pi dif with negative G

1107
01:02:20,803 --> 01:02:23,606
and these two betas just cancel out
because it's just one -1.

1108
01:02:23,973 --> 01:02:26,209
And so you own up with point you two.

1109
01:02:26,209 --> 01:02:29,912
And then if you compute the update,

1110
01:02:30,379 --> 01:02:34,984
then the with the change will be then
gamma ends up

1111
01:02:34,984 --> 01:02:37,920
equaling 1.28, which went up from one,

1112
01:02:39,322 --> 01:02:42,125
which means that you'd have
a positive phase.

1113
01:02:42,225 --> 01:02:45,027
A document predicted
positive physics domain response

1114
01:02:46,129 --> 01:02:48,364
and a way to kind of think about it
geometrically

1115
01:02:48,364 --> 01:02:53,169
as whether or not the vectors
corresponding to PI diff.

1116
01:02:53,169 --> 01:02:57,807
So the difference between fine zero point
in the same direction as negative g

1117
01:02:59,041 --> 01:03:02,845
because that implies
that they're consistent with one another.

1118
01:03:02,845 --> 01:03:07,750
That implies
that essentially the G was on track and.

1119
01:03:07,950 --> 01:03:10,319
That's worth a rerun.

1120
01:03:10,353 --> 01:03:12,855
So what is this geometric space?

1121
01:03:13,289 --> 01:03:16,759
And then what would it mean
if the vectors were totally correlated

1122
01:03:16,759 --> 01:03:17,727
and they were like pointing

1123
01:03:17,727 --> 01:03:21,330
in the same exact direction versus here
they're almost at 90 degrees?

1124
01:03:21,330 --> 01:03:21,864
Or what

1125
01:03:21,864 --> 01:03:24,600
would it mean for different arrows
to be pointing in different directions?

1126
01:03:25,268 --> 01:03:28,137
So these are the vectors corresponding to

1127
01:03:29,305 --> 01:03:31,274
the vectors here in G

1128
01:03:31,274 --> 01:03:35,711
and the vector that corresponds to pi
minus pi zero.

1129
01:03:36,179 --> 01:03:37,380
I mean, once again.

1130
01:03:37,547 --> 01:03:38,414
This is.

1131
01:03:38,414 --> 01:03:40,783
So this is
and I should say that these are scaled

1132
01:03:41,117 --> 01:03:45,555
and so that they're similar lengths
at the same directions

1133
01:03:45,555 --> 01:03:48,758
because otherwise one error
would be way longer than the other.

1134
01:03:48,758 --> 01:03:50,960
And the kind of point isn't clear.

1135
01:03:51,394 --> 01:03:52,628
Wouldn't be clear.

1136
01:03:52,628 --> 01:03:57,033
But so basically pi death
rate is negative point to an 8.2 on it.

1137
01:03:57,033 --> 01:03:59,769
So it's pointing left and up.

1138
01:03:59,769 --> 01:04:02,638
So negative X, positive Y,

1139
01:04:03,806 --> 01:04:08,077
whereas G is negative G would be -10, -9.

1140
01:04:08,077 --> 01:04:11,247
So it will be down and left.

1141
01:04:12,715 --> 01:04:15,718
And so the, the general kind of

1142
01:04:15,718 --> 01:04:18,521
intuition that's supposed to be given
is that

1143
01:04:18,521 --> 01:04:20,590
when these two vectors
point in the same direction where

1144
01:04:20,590 --> 01:04:23,726
that just means
that the angle is less than 90 degrees

1145
01:04:24,994 --> 01:04:27,964
then the gamma update will be positive.

1146
01:04:29,031 --> 01:04:32,835
Whereas if for instance
instead we do a different example

1147
01:04:33,135 --> 01:04:35,571
now the only thing we're going to do here
is we're going to change f

1148
01:04:35,638 --> 01:04:38,941
we're going to change F from 21, two

1149
01:04:39,742 --> 01:04:42,745
or another we're going to change g.

1150
01:04:43,112 --> 01:04:48,284
So G is going from ten to nine
to only being one and nine.

1151
01:04:48,551 --> 01:04:52,855
So in other words, while in this case
the first policy at higher

1152
01:04:52,855 --> 01:04:58,294
expected for energy Now the first policy
has lower expected for energy.

1153
01:04:59,729 --> 01:05:00,363
Now if you

1154
01:05:00,363 --> 01:05:04,133
run through the exact same set of steps
to solve the equations

1155
01:05:04,567 --> 01:05:08,070
you're going to end up
with a gamma of 0.14, which means there's

1156
01:05:08,070 --> 01:05:12,141
going to be a negative,
there should be a drop in dopamine.

1157
01:05:12,808 --> 01:05:15,511
In other words, the agent should become
less confident in its

1158
01:05:16,846 --> 01:05:18,114
G estimates.

1159
01:05:18,114 --> 01:05:22,084
And you can see that if you look at where
the vectors are pointing in this case,

1160
01:05:22,952 --> 01:05:25,321
then they're then the angle difference

1161
01:05:25,321 --> 01:05:29,191
is 128.6, six degrees,
so more than 90 degrees.

1162
01:05:29,191 --> 01:05:31,227
So therefore they point
in different directions.

1163
01:05:31,227 --> 01:05:33,496
So therefore they provide evidence
against G

1164
01:05:34,997 --> 01:05:38,367
F, if that makes sense.

1165
01:05:38,734 --> 01:05:40,569
Yep. Let me try with the dopamine.

1166
01:05:40,569 --> 01:05:44,440
So we're thinking of dopamine
as a tracker of confidence.

1167
01:05:44,440 --> 01:05:48,945
It has many roles and Colombo and others
have talked about pluralism in dopamine.

1168
01:05:48,945 --> 01:05:51,113
So we're not
there's not a lecture on dopamine.

1169
01:05:51,113 --> 01:05:52,949
We're thinking about dopamine
as like confidence

1170
01:05:52,949 --> 01:05:54,717
and we're headed in a direction.

1171
01:05:54,717 --> 01:05:57,753
We're on our road trip and the dopamine
coursing through our brain.

1172
01:05:57,753 --> 01:05:59,322
And we want to know how confident we are

1173
01:05:59,322 --> 01:06:02,291
that we're on the right path
as new observations are coming in.

1174
01:06:02,858 --> 01:06:06,829
Now, as we've seen from the earlier
sections, we're doing a lot of matrix

1175
01:06:06,996 --> 01:06:07,830
math.

1176
01:06:07,830 --> 01:06:10,700
And just like a vector
is a simpler version

1177
01:06:10,700 --> 01:06:13,803
of a matrix, and a tensor is a more
complicated version of a matrix.

1178
01:06:13,803 --> 01:06:17,807
They're all kind of in
the same math space and a given vector.

1179
01:06:18,441 --> 01:06:21,410
From computer science, it's
like two numbers that are in a column,

1180
01:06:21,711 --> 01:06:24,780
but also a vector
has this arrow interpretation.

1181
01:06:24,947 --> 01:06:27,616
So if someone says the coordinate three,
comma three,

1182
01:06:27,817 --> 01:06:30,586
it's like a vector from 00
pointing to 33.

1183
01:06:30,820 --> 01:06:34,790
So lists of numbers like a computer
science vector is basically

1184
01:06:34,790 --> 01:06:39,829
the same thing as a vector with a start
and a head entity at the end.

1185
01:06:40,062 --> 01:06:41,797
So physics and computer science

1186
01:06:41,797 --> 01:06:44,367
and we're thinking about vectors
and matrices, intensities,

1187
01:06:44,834 --> 01:06:48,270
and then we're laying out
after scaling and normalizing

1188
01:06:48,504 --> 01:06:51,741
which direction
these different vectors are pointing

1189
01:06:52,108 --> 01:06:55,111
and then it's kind of like,
are we headed on the basically

1190
01:06:55,111 --> 01:06:58,647
the same path is the,
the vector of the evidence sort

1191
01:06:58,647 --> 01:07:02,084
of pointing us in a similar direction
that we're already believing in.

1192
01:07:02,451 --> 01:07:05,988
Or is the new evidence
coming in like a tailwind or headwind

1193
01:07:06,122 --> 01:07:08,724
that's kind of surprising us
and it's not in line,

1194
01:07:08,724 --> 01:07:11,260
it's not headed in the same direction
as what we're already believing.

1195
01:07:11,560 --> 01:07:15,631
And then this meta with the gamma,
the hyper parameter is about

1196
01:07:15,631 --> 01:07:20,002
how much reliance we're putting in
to G versus our observations.

1197
01:07:21,270 --> 01:07:21,670
Yeah.

1198
01:07:21,670 --> 01:07:24,807
So I mean, another kind of a nice,
nice thing here

1199
01:07:24,807 --> 01:07:28,244
is that so as as Gamma goes down. Right.

1200
01:07:28,277 --> 01:07:30,546
So if you look
if you look at this poster,

1201
01:07:30,546 --> 01:07:34,583
you know, distribution of our policies
equation as is gamma value gets lower.

1202
01:07:34,617 --> 01:07:34,950
Right?

1203
01:07:34,950 --> 01:07:40,156
So as the agent becomes less confident
in its and its action model, more or less

1204
01:07:40,256 --> 01:07:44,460
less confident that its model will be
good at minimizing expected for energy.

1205
01:07:44,860 --> 01:07:45,194
Right.

1206
01:07:45,194 --> 01:07:48,731
As that goes down, then
G contributes less, which means that F, n

1207
01:07:48,764 --> 01:07:51,801
e contribute more so if an agent

1208
01:07:51,834 --> 01:07:54,637
has built up really strong habits,

1209
01:07:55,738 --> 01:07:58,741
then its beliefs
and its model will control action.

1210
01:07:59,075 --> 01:08:02,478
If Gamma is high, but if Gamma is
really low, then the agent will

1211
01:08:02,611 --> 01:08:06,315
just kind of choose whatever
its habitually chosen in the past.

1212
01:08:06,315 --> 01:08:07,950
So habits will kind of take over.

1213
01:08:07,950 --> 01:08:09,852
If it's not confident in its model

1214
01:08:11,053 --> 01:08:13,089
which
is kind of a nice way to think about

1215
01:08:13,089 --> 01:08:16,292
as some kind of analogs
to like model based versus model three

1216
01:08:17,760 --> 01:08:21,964
algorithms and reinforcement
learning for example, that again,

1217
01:08:22,298 --> 01:08:25,534
you could think about in terms
of actually making decisions based on,

1218
01:08:25,901 --> 01:08:28,571
you know, explicitly simulating
what's going to happen in the future.

1219
01:08:28,571 --> 01:08:29,605
If I make one choice versus

1220
01:08:29,605 --> 01:08:32,942
another versus just doing
what's typically worked in the past,

1221
01:08:34,610 --> 01:08:38,080
as is another kind of way to think about
what Gamma has arbitrating.

1222
01:08:39,448 --> 01:08:41,517
Did you want to say anything else, Chris?

1223
01:08:41,517 --> 01:08:44,320
No, I was just going to say there's
some really nice numerical results,

1224
01:08:44,987 --> 01:08:48,190
so I think it's gotten
so I'm sorry if I've forgotten the name.

1225
01:08:48,491 --> 01:08:51,627
I think it's Thomas Fitzgerald
is the lead author on this, but

1226
01:08:52,695 --> 01:08:53,262
they have some

1227
01:08:53,262 --> 01:08:56,065
stuff just showing that related schemes.

1228
01:08:56,298 --> 01:08:57,900
I think the equations have changed

1229
01:08:57,900 --> 01:08:59,935
a little bit since then,
but that pretty much the same

1230
01:09:01,737 --> 01:09:03,806
that this scheme gives very, very similar

1231
01:09:05,074 --> 01:09:07,243
answers as something like temple distance
learning,

1232
01:09:07,643 --> 01:09:10,412
which is a very simple model
for algorithm and reinforcement learning

1233
01:09:11,113 --> 01:09:15,151
and that's also I think, the gold
standard model of phase excitement

1234
01:09:15,284 --> 01:09:16,185
at the moment.

1235
01:09:16,418 --> 01:09:18,154
Robin, correct me on that, but.

1236
01:09:18,154 --> 01:09:19,722
No, I mean, yeah,
I mean it's like temple,

1237
01:09:19,722 --> 01:09:23,659
the temple, different account
of the reward prediction or stuff.

1238
01:09:23,659 --> 01:09:27,763
And yet we're treating basic
domain responses as reward prediction

1239
01:09:27,763 --> 01:09:29,565
errors and temporal difference.
Learning models

1240
01:09:29,565 --> 01:09:33,335
is definitely a very standard
kind of like widely supported view.

1241
01:09:34,537 --> 01:09:38,841
And this I mean the
it is that this this can this sort of

1242
01:09:40,409 --> 01:09:40,876
this sort

1243
01:09:40,876 --> 01:09:42,912
of active inference model of dopamine

1244
01:09:44,013 --> 01:09:45,381
is kind of saying something different.

1245
01:09:45,381 --> 01:09:48,017
It's not saying
it's a reward prediction error,

1246
01:09:48,017 --> 01:09:51,153
but it's something that will have
the same kind of dynamics

1247
01:09:51,153 --> 01:09:53,822
as reward prediction error
because if you think about it,

1248
01:09:54,156 --> 01:09:57,793
so say, you know, I'm walking along
and I'm not expecting a reward

1249
01:09:58,561 --> 01:10:01,997
and then all of a sudden
I get a cue that makes it

1250
01:10:01,997 --> 01:10:05,968
so now that I expect now I expect reward.

1251
01:10:06,402 --> 01:10:10,072
And so, you know, for instance,
like or say like I'm like a simple

1252
01:10:10,072 --> 01:10:13,375
kind of like experiment,
like a like operant condition experiment,

1253
01:10:13,375 --> 01:10:15,311
conditioning experiment,
like a rat or something.

1254
01:10:15,311 --> 01:10:16,979
You know, it learns

1255
01:10:16,979 --> 01:10:19,682
that every time it sees a light, it's
going to get a reward.

1256
01:10:19,682 --> 01:10:20,983
And 5 seconds.

1257
01:10:20,983 --> 01:10:25,154
And then the reward prediction,
our story would say,

1258
01:10:25,521 --> 01:10:27,156
oh, you're going
to get a reward prediction error

1259
01:10:27,156 --> 01:10:30,859
when the light rats is the light,
because now it's going to get

1260
01:10:30,859 --> 01:10:34,129
it knows it's going to get a reward
that it wasn't expecting.

1261
01:10:34,630 --> 01:10:38,100
And so in this case, though,
you'll get something at a similar

1262
01:10:38,100 --> 01:10:39,468
but for a different reason.

1263
01:10:39,468 --> 01:10:42,538
Where you can think about the rat
initially just kind of wandering

1264
01:10:42,538 --> 01:10:47,476
around, not being super confident,
you know, and in a particular policy.

1265
01:10:47,910 --> 01:10:50,846
But then it gets that cue
that it's going to get rewarded,

1266
01:10:51,146 --> 01:10:53,716
you know, if it goes up and pushes the
the lever. Right.

1267
01:10:53,716 --> 01:10:54,750
Is the standard way this works.

1268
01:10:56,151 --> 01:10:57,653
So so instead of it being

1269
01:10:57,653 --> 01:11:01,323
a reward prediction error, what would be
happening is the cue is actually

1270
01:11:02,358 --> 01:11:06,128
unexpectedly making the rat
a lot more confident in what to do.

1271
01:11:06,462 --> 01:11:06,996
Right.

1272
01:11:06,996 --> 01:11:09,431
But more confident in the policy
of going and pushing the lever.

1273
01:11:10,699 --> 01:11:13,669
So you're going to get something
that looks just like

1274
01:11:13,669 --> 01:11:16,338
a reward prediction error,
at least in that sort of setup,

1275
01:11:17,072 --> 01:11:19,441
but actually has to do with an update
and confidence about what to do

1276
01:11:21,243 --> 01:11:23,812
so so that's kind of in

1277
01:11:23,812 --> 01:11:26,282
part that's one
kind of intuitive way to think about,

1278
01:11:26,949 --> 01:11:29,852
you know, where similar looking dynamics
come from.

1279
01:11:30,919 --> 01:11:33,956
But but I should also kind of
be clear that,

1280
01:11:33,956 --> 01:11:36,992
I mean, this is definitely not

1281
01:11:38,027 --> 01:11:41,730
meant necessarily
to be kind of a universal

1282
01:11:42,431 --> 01:11:45,301
unifying account of dopamine.

1283
01:11:45,301 --> 01:11:47,369
You know, at least at least as far as

1284
01:11:48,404 --> 01:11:48,904
I can tell at

1285
01:11:48,904 --> 01:11:52,474
present, there's lots of other things
that dopamine does that,

1286
01:11:52,975 --> 01:11:56,979
you know, look like they couldn't
be explained by just this sort of model.

1287
01:11:57,479 --> 01:12:02,518
But this sort of model looks like
it's decent as a contender for thing

1288
01:12:02,551 --> 01:12:05,354
making sense of things
like why dopamine doesn't just respond

1289
01:12:05,354 --> 01:12:07,923
for prediction errors, but also
a response for events that are salient

1290
01:12:09,091 --> 01:12:10,793
for example,

1291
01:12:11,460 --> 01:12:13,929
this kind of saliency versus reward
prediction error sort of thing.

1292
01:12:13,962 --> 01:12:14,730
There's a couple others.

1293
01:12:14,730 --> 01:12:17,833
But but this is not necessarily
meant to be a

1294
01:12:18,200 --> 01:12:21,270
you know, this is all dopamine does.

1295
01:12:21,270 --> 01:12:22,938
But anyway,

1296
01:12:23,839 --> 01:12:26,475
so so just making that clear

1297
01:12:28,577 --> 01:12:31,046
So now the kind of last thing to show

1298
01:12:31,046 --> 01:12:31,714
and this is the actual

1299
01:12:31,714 --> 01:12:35,117
figure in the paper that just shows
both of these things, is that

1300
01:12:35,551 --> 01:12:40,155
if you do the actual variational updating
where just better continually changes

1301
01:12:40,589 --> 01:12:43,659
than the
the actual kind of gradient like the

1302
01:12:43,692 --> 01:12:48,063
the changes look like this sort of thing
would start out for instance, at

1303
01:12:49,298 --> 01:12:51,533
around at a certain value.

1304
01:12:52,634 --> 01:12:55,337
And then for for this case right
so it starts at

1305
01:12:56,638 --> 01:12:59,241
you know, down coming from one
and then it will

1306
01:12:59,274 --> 01:13:03,879
gradually go up and converge
after 16 iterations.

1307
01:13:03,879 --> 01:13:08,984
Is that what's that in the code to that
post your value or what Gamaa to be

1308
01:13:09,351 --> 01:13:13,155
whereas in this case
it will drop coming up from one down

1309
01:13:13,455 --> 01:13:18,660
and then converge over those iterations
onto this value of .14.

1310
01:13:19,027 --> 01:13:22,564
So it's not as though in the neural model
this is kind of happening

1311
01:13:22,564 --> 01:13:23,732
with one little step.

1312
01:13:23,732 --> 01:13:26,969
It's it's kind of a it's a
you know it's a convergence

1313
01:13:28,003 --> 01:13:30,639
that you can think of as related
to a kind of prediction error

1314
01:13:31,140 --> 01:13:33,942
and essentially kind of like an expected
free energy prediction here.

1315
01:13:34,209 --> 01:13:36,712
Some people have talked about it
that way in previous papers.

1316
01:13:37,846 --> 01:13:39,047
And for those following along

1317
01:13:39,047 --> 01:13:42,584
line in the code with this being the EMV
precision updating and.

1318
01:13:43,185 --> 01:13:45,621
Yeah, I think so. Yeah, there's a yeah.

1319
01:13:45,621 --> 01:13:48,891
To to play around with this exact
this exact example

1320
01:13:49,525 --> 01:13:53,562
as well as ones where you can specify
bigger policy spaces.

1321
01:13:53,562 --> 01:13:57,266
So like like when the agent has like five
policies to choose from instead of one,

1322
01:13:58,434 --> 01:14:00,869
which is a little more realistic,
it's just

1323
01:14:00,869 --> 01:14:05,340
it's hard to show vectors, it's hard
to illustrate those in like

1324
01:14:05,340 --> 01:14:08,210
five dimensions and so we want the two
for this simple examples.

1325
01:14:09,044 --> 01:14:12,815
But yes, and that if precision updating,
you can reproduce

1326
01:14:13,849 --> 01:14:17,085
these results and then try changing
provider better values

1327
01:14:17,085 --> 01:14:21,390
or changing genes to see to get more
of an intuitive sense of the dynamics

1328
01:14:23,692 --> 01:14:25,661
and so like in the

1329
01:14:25,861 --> 01:14:28,997
in the in the lecture might call it

1330
01:14:29,264 --> 01:14:32,701
when we were showing in the explore
exploit test model the last time

1331
01:14:33,635 --> 01:14:38,006
these sorts of simulated simulated
behaviors, right where the agent either

1332
01:14:38,006 --> 01:14:41,276
chooses to take the hint and then go left
or right to get a reward.

1333
01:14:41,276 --> 01:14:42,211
Oh, it just makes

1334
01:14:42,211 --> 01:14:44,980
or is just wrecks risk seeking
and just goes left versus right

1335
01:14:44,980 --> 01:14:46,982
automatically
instead of taking the hint first.

1336
01:14:47,483 --> 01:14:50,819
And in, in the previous session
we went over this

1337
01:14:51,220 --> 01:14:54,723
and but we kind of ignored
this bottom right section on expected

1338
01:14:55,491 --> 01:14:58,193
for energy precision
and these dogma plots.

1339
01:14:58,727 --> 01:15:01,497
But now that we have kind of
walked through that a little bit

1340
01:15:02,498 --> 01:15:05,200
you know, we can kind of come back
to that and see that in this case

1341
01:15:06,735 --> 01:15:08,070
the agent

1342
01:15:08,136 --> 01:15:11,173
started out in the beginning
and it chose to take the hand

1343
01:15:12,040 --> 01:15:14,276
and then it knew

1344
01:15:14,376 --> 01:15:16,678
whether to choose the left,

1345
01:15:17,513 --> 01:15:19,948
choose the left machine
or choose the right machine.

1346
01:15:19,948 --> 01:15:21,250
So you can see what happens
here. As well.

1347
01:15:21,250 --> 01:15:25,954
When it takes the hint, then it becomes
a lot more confident about what to do.

1348
01:15:27,322 --> 01:15:31,760
And so it's at that point that you
simulate a big jump in dopamine Right.

1349
01:15:31,793 --> 01:15:34,263
So in this case, it's not because it's
got a reward immediately. It's

1350
01:15:34,263 --> 01:15:38,400
because it got a cue where it learned
what to be able to do to get a reward.

1351
01:15:39,101 --> 01:15:42,304
So then excuse is left
and then it observes a win here

1352
01:15:42,304 --> 01:15:45,073
at the third time
player, the third column.

1353
01:15:46,475 --> 01:15:48,810
So so this is kind of an example.

1354
01:15:49,711 --> 01:15:53,382
And then in the learning simulations
that we showed last time as well.

1355
01:15:53,815 --> 01:15:54,983
And for the take.

1356
01:15:54,983 --> 01:15:57,052
Before we leave that side,
can I just jump in real quick

1357
01:15:57,052 --> 01:15:59,555
and add a quick clarification
about that figure

1358
01:16:01,657 --> 01:16:04,126
so in that middle panel four win lose

1359
01:16:04,126 --> 01:16:06,962
and you look at that matrix,
it actually looks like if that's the

1360
01:16:08,063 --> 01:16:10,832
expected probabilities for her win
loss at any

1361
01:16:10,832 --> 01:16:14,836
given time at Park one, two and three,

1362
01:16:15,671 --> 01:16:19,207
he actually it actually looks like
in this case, the way that the simulation

1363
01:16:19,741 --> 01:16:20,742
it's more confident

1364
01:16:20,742 --> 01:16:24,212
about winning on this time step
when it knows to take the hint.

1365
01:16:24,212 --> 01:16:27,182
And then after it's taken a hit,

1366
01:16:27,182 --> 01:16:31,920
it becomes less confident
about running is it is that.

1367
01:16:32,321 --> 01:16:35,924
It's actually to do with the fact
that those aren't so the probabilities

1368
01:16:35,924 --> 01:16:39,127
but they're not probabilities
in the way that the rest of the model's

1369
01:16:39,127 --> 01:16:41,797
probabilities
so these are these are preferences

1370
01:16:42,230 --> 01:16:44,866
where we model preferences
as a probability distribution.

1371
01:16:45,467 --> 01:16:47,102
I, I see.

1372
01:16:47,102 --> 01:16:50,472
So that has to do with more of the,
you know, it's had more update,

1373
01:16:50,472 --> 01:16:53,342
it's had more observations
at times three.

1374
01:16:53,575 --> 01:16:56,311
And so as you're going,
since you're it's an accumulator

1375
01:16:56,912 --> 01:16:59,081
in that optimization process, is that.

1376
01:16:59,414 --> 01:17:02,317
No it's just it's
just so when you're trying to encode

1377
01:17:02,317 --> 01:17:05,253
essentially
what's rewarding for the agent, right?

1378
01:17:05,621 --> 01:17:10,859
You're just setting you're just saying
that it has a particular distribution

1379
01:17:10,993 --> 01:17:12,027
over each possible

1380
01:17:12,027 --> 01:17:15,497
set of outcomes for each outcome modality
and that distribution just encodes

1381
01:17:16,031 --> 01:17:19,768
which observations
are more preferred than others

1382
01:17:20,669 --> 01:17:23,338
and formally that's just a fixed

1383
01:17:23,338 --> 01:17:26,808
probability of distribution over
the different observations you might get,

1384
01:17:27,075 --> 01:17:29,311
where the higher
the probability as quote unquote,

1385
01:17:29,311 --> 01:17:31,313
the more the agent
prefers that observation.

1386
01:17:32,614 --> 01:17:36,218
So that's the reason this is just saying,
like for the first outcome modality

1387
01:17:36,218 --> 01:17:41,123
is just all gray here because the agent
doesn't prefer a hint or not a hint.

1388
01:17:42,357 --> 01:17:44,326
And then the third
one here observed action.

1389
01:17:44,326 --> 01:17:47,896
It doesn't prefer innately to observe
itself, do one action or another.

1390
01:17:48,397 --> 01:17:51,433
But and this middle one observing
wins versus losses

1391
01:17:51,433 --> 01:17:53,769
versus
NOL just not observing an outcome yet

1392
01:17:54,636 --> 01:17:58,306
at the at the first time points
in the first column, it just does

1393
01:17:58,306 --> 01:18:02,244
it doesn't have any preferences
at the second time points.

1394
01:18:02,244 --> 01:18:05,180
So this middle column,
the thing has a strong preference

1395
01:18:05,180 --> 01:18:08,750
for when so this black one,
which corresponds to high probability

1396
01:18:09,217 --> 01:18:11,753
and has a strong preference
against losing

1397
01:18:11,953 --> 01:18:14,456
observing a loss
which is a very low probability.

1398
01:18:14,823 --> 01:18:17,292
And then the null stage
is this kind of like intermediate thing.

1399
01:18:17,292 --> 01:18:20,529
It's not like it's not bad like a loss,
but it's not good like a win.

1400
01:18:21,697 --> 01:18:22,397
That shows the

1401
01:18:22,397 --> 01:18:25,667
null there or the race
is because it shows a hint because

1402
01:18:25,667 --> 01:18:28,070
of the epistemic value of the hint,
I should say.

1403
01:18:28,804 --> 01:18:29,604
Yeah.

1404
01:18:29,838 --> 01:18:33,508
So it so basically what's happening is,
if you remember that the

1405
01:18:33,909 --> 01:18:37,079
this distribution
is a little lighter at time too,

1406
01:18:37,079 --> 01:18:40,916
because the value of winning is higher
at the second time

1407
01:18:40,916 --> 01:18:43,952
step than the third eye
because it can win $4

1408
01:18:43,985 --> 01:18:46,922
if it tries to get by, just choose
one right away.

1409
01:18:47,456 --> 01:18:51,393
But at the third time point,
this is a little less precise.

1410
01:18:51,393 --> 01:18:55,230
It's a little flatter, which is it
looks darker gray because the black here

1411
01:18:55,230 --> 01:18:57,666
only corresponds
to winning $2 instead of four

1412
01:18:58,433 --> 01:19:01,903
so the difference between
winning and losing isn't quite as stark.

1413
01:19:02,771 --> 01:19:04,406
Okay. Yeah.

1414
01:19:04,840 --> 01:19:07,576
So but the point is, it's
not actually getting a big bump

1415
01:19:07,576 --> 01:19:11,480
in dopamine when it wins, you know, it's
getting a big bump in dopamine

1416
01:19:11,480 --> 01:19:13,248
when it observes the hint because that's

1417
01:19:13,248 --> 01:19:15,784
when it becomes
confident in what to do to get the win.

1418
01:19:17,486 --> 01:19:18,153
And that's

1419
01:19:18,153 --> 01:19:21,289
to reemphasize that we're doing this

1420
01:19:21,289 --> 01:19:25,260
trajectory of policy, which is what ties
it to the path integral as well.

1421
01:19:25,527 --> 01:19:28,263
So each time point
that it's being evaluated, it's

1422
01:19:28,263 --> 01:19:31,433
in the past and in the future,
and it's conditioned on policy.

1423
01:19:31,800 --> 01:19:33,168
So it's almost like the reward.

1424
01:19:33,168 --> 01:19:35,370
If you always expect
the envelope has the paycheck,

1425
01:19:35,604 --> 01:19:36,404
if you get it

1426
01:19:36,404 --> 01:19:40,108
and then you open it, it's like neutral
with respect to what you already believed

1427
01:19:40,108 --> 01:19:41,443
about the way the world worked.

1428
01:19:41,443 --> 01:19:44,546
And you'd get a negative if it was less
than expected in a positive,

1429
01:19:44,546 --> 01:19:45,680
if more than expected.

1430
01:19:45,680 --> 01:19:50,352
And so in that way, the reward of what
is initially just pure stimuli

1431
01:19:50,352 --> 01:19:53,221
that could be rewarding or thought of in
that way gets moved up.

1432
01:19:53,588 --> 01:19:56,725
That's kind of like reinforcement
learning or reward learning.

1433
01:19:56,858 --> 01:20:00,729
And that's why we're in this area
of thinking about stimuli and policy

1434
01:20:00,729 --> 01:20:01,863
and rewards and risks.

1435
01:20:03,265 --> 01:20:03,965
Yeah, I mean, the idea

1436
01:20:03,965 --> 01:20:07,169
is just that in that context,

1437
01:20:07,169 --> 01:20:09,671
these dopamine responses,
the simulated dopamine responses

1438
01:20:09,671 --> 01:20:10,806
here corresponded

1439
01:20:10,806 --> 01:20:14,709
to changes in confidence
about what to do or changes in confidence

1440
01:20:14,976 --> 01:20:17,512
that your model will give you the right
answer about what to do.

1441
01:20:18,880 --> 01:20:23,518
And, and that will look
that will have the same dynamics

1442
01:20:23,518 --> 01:20:26,188
as a reward prediction error

1443
01:20:26,955 --> 01:20:30,058
from a cue that predicts reward.

1444
01:20:30,058 --> 01:20:32,694
So so the dynamics will look like
a reward prediction error,

1445
01:20:32,694 --> 01:20:35,864
even though in these models
it corresponds to something different.

1446
01:20:38,400 --> 01:20:41,436
So it's kind of like in behaviorism,
everybody can agree on the behavior.

1447
01:20:41,469 --> 01:20:45,140
We agree that if we train up the animal,
it learns to do this after X

1448
01:20:45,640 --> 01:20:47,776
experimental paradigm is carried out.

1449
01:20:48,176 --> 01:20:51,546
And now we're talking about internal
we're thinking either at a neural process

1450
01:20:51,546 --> 01:20:55,383
level, like which regions of the brain or
computationally like what is happening.

1451
01:20:55,884 --> 01:20:59,621
And then here there's a common trunk
with the predictive processing,

1452
01:20:59,621 --> 01:21:03,091
the reward prediction error, all of that
and the active inference models

1453
01:21:03,091 --> 01:21:05,460
we're talking about,
which is in reinforcement learning,

1454
01:21:05,460 --> 01:21:09,564
which is that over time,
the reward gets moved up to be associated

1455
01:21:09,731 --> 01:21:13,168
apparently more with the cue
than with the actual stimulus delivery.

1456
01:21:13,568 --> 01:21:17,138
But we're taking a point of divergence
in that we're adding in

1457
01:21:17,138 --> 01:21:21,076
a few different pieces that differentiate
in active inference, free energy

1458
01:21:21,843 --> 01:21:25,914
gradient, descent driven policy selection
from just

1459
01:21:26,014 --> 01:21:29,751
Q association, even though they do have
some similarities.

1460
01:21:30,018 --> 01:21:31,686
This is like a generalization

1461
01:21:31,686 --> 01:21:34,456
or a much more nuanced way
to approach similar situations.

1462
01:21:35,957 --> 01:21:36,458
Yeah.

1463
01:21:36,458 --> 01:21:39,561
And, you know, I ultimately write
the hope is to find situations

1464
01:21:39,928 --> 01:21:43,064
where active inference and reinforcement
learning models are going to be going

1465
01:21:43,131 --> 01:21:46,034
to make different predictions, right,
about what dopamine will do

1466
01:21:47,235 --> 01:21:49,871
and because that's
where you can actually say,

1467
01:21:50,038 --> 01:21:52,908
you know,
which model has more empirical support.

1468
01:21:54,309 --> 01:21:57,979
And yeah, Thomas Fitzgerald, I think, is
the paper that Chris was talking about

1469
01:21:58,079 --> 01:22:01,116
or you know, he did show
there are interesting findings

1470
01:22:01,116 --> 01:22:05,420
where when you get rid of dopamine,
when there's dopamine depletion

1471
01:22:05,420 --> 01:22:08,890
or there's some kind of lack of dopamine
in the system for one reason or another,

1472
01:22:09,991 --> 01:22:13,428
that doesn't
actually get rid of a reward.

1473
01:22:13,428 --> 01:22:16,531
Learning like an agent
can still learn about rewards despite

1474
01:22:17,832 --> 01:22:20,835
the dopaminergic function being impaired,

1475
01:22:22,103 --> 01:22:24,406
which makes it look as though,

1476
01:22:24,406 --> 01:22:28,343
you know, dopamine might not be serving
this reward prediction error

1477
01:22:28,343 --> 01:22:30,712
function, at least
if you think that reward prediction error

1478
01:22:30,745 --> 01:22:32,514
is necessary for reward learning

1479
01:22:33,782 --> 01:22:36,051
and and so there was he

1480
01:22:36,051 --> 01:22:38,520
showed some simulations again
with a slightly different formalism

1481
01:22:38,520 --> 01:22:42,824
than the current one, showing how
you can get the same dopamine responses

1482
01:22:44,192 --> 01:22:47,095
as you would expect in a reinforcement
learning model.

1483
01:22:47,462 --> 01:22:50,465
But it explains why
you don't need dopamine

1484
01:22:50,465 --> 01:22:53,068
to do the reward learning

1485
01:22:53,535 --> 01:22:57,339
but but like I mentioned, the formalism
isn't exactly the same as it is now.

1486
01:22:57,405 --> 01:22:59,908
There's a slightly older paper,

1487
01:22:59,908 --> 01:23:00,642
so, you know,

1488
01:23:00,642 --> 01:23:01,076
I think there's

1489
01:23:01,076 --> 01:23:04,412
just a lot more kind of empirical work
to do to try to kind of find,

1490
01:23:04,946 --> 01:23:08,083
you know, differential support for
one model of dopamine over another here.

1491
01:23:09,951 --> 01:23:13,555
Very any other any other comments, Chris.

1492
01:23:14,189 --> 01:23:17,392
And I think just looking at the time,
you want to kind of speed

1493
01:23:17,592 --> 01:23:20,695
as the slides and then get to the stuff.

1494
01:23:20,695 --> 01:23:21,796
Yeah. Okay.

1495
01:23:21,796 --> 01:23:25,200
So yeah, I mean, I can skip,
I can skip or to skim over a lot of this.

1496
01:23:25,233 --> 01:23:27,602
I mean, the last thing
I was going to talk about is that

1497
01:23:28,269 --> 01:23:30,271
this isn't used as much

1498
01:23:30,271 --> 01:23:32,340
because it's not actually implemented
in this way in the code.

1499
01:23:32,707 --> 01:23:36,044
But Karl has made this point
that you can also implement

1500
01:23:36,411 --> 01:23:38,913
and minimizing expected free energy

1501
01:23:39,681 --> 01:23:42,584
using a different kind of prediction
error called an outcome prediction error.

1502
01:23:43,051 --> 01:23:46,855
And that just corresponds
to the different elements

1503
01:23:46,955 --> 01:23:48,890
and expected for energy.

1504
01:23:48,890 --> 01:23:50,325
So which is here just

1505
01:23:51,860 --> 01:23:52,227
here is

1506
01:23:52,227 --> 01:23:57,032
just minimizing the expected difference
between the preferred outcomes C

1507
01:23:57,432 --> 01:24:00,835
and the outcomes that you expect
under a policy right.

1508
01:24:00,835 --> 01:24:04,973
So under some policy
for under some policy,

1509
01:24:04,973 --> 01:24:09,010
the state for that pie multiplied with A
will give you

1510
01:24:09,010 --> 01:24:12,280
the expected outcomes of a policy
so you're just trying to

1511
01:24:13,148 --> 01:24:14,049
this prediction error,

1512
01:24:14,049 --> 01:24:15,183
minimizing this prediction error

1513
01:24:15,183 --> 01:24:18,987
just corresponds to finding the policy
that minute or that minimizes the

1514
01:24:18,987 --> 01:24:21,056
the difference
between basically what you want

1515
01:24:21,056 --> 01:24:23,324
and what you expect
given what you choose to do.

1516
01:24:24,426 --> 01:24:26,594
And the second term is

1517
01:24:26,594 --> 01:24:29,831
the ambiguity
or information seeking term.

1518
01:24:30,231 --> 01:24:34,069
And it effectively you can think about it
as the expected difference between

1519
01:24:34,469 --> 01:24:37,772
beliefs about states before
and after a new observation.

1520
01:24:38,106 --> 01:24:40,241
So you're trying to find basically

1521
01:24:40,742 --> 01:24:44,546
and a different state
like the state that maximizes

1522
01:24:44,779 --> 01:24:47,315
how much information
you gain about what state you're in.

1523
01:24:48,083 --> 01:24:51,753
But formally, it's that it's the entropy
of the likelihood distribution

1524
01:24:51,753 --> 01:24:54,622
for a state
or how kind of flat or uninformative

1525
01:24:54,689 --> 01:24:58,860
the distribution is of expected outcomes,
given that you're in one state

1526
01:24:58,860 --> 01:25:00,428
versus another.

1527
01:25:00,462 --> 01:25:03,798
And in the paper we we go through

1528
01:25:04,065 --> 01:25:06,267
and numeric numerical examples of

1529
01:25:07,001 --> 01:25:09,704
computing these sorts
of outcome prediction errors as well.

1530
01:25:10,805 --> 01:25:13,942
But the basic point again
I won't go through them in detail is just

1531
01:25:14,342 --> 01:25:18,079
if you have some policy
that is going to generate the state,

1532
01:25:18,780 --> 01:25:21,816
state, one with probability
point nine and state to probability

1533
01:25:21,816 --> 01:25:24,452
point one, and your preference is

1534
01:25:25,620 --> 01:25:30,525
perfectly precise over observing
outcome one and that's your likelihood

1535
01:25:30,525 --> 01:25:34,429
mapping then policy whereas policy

1536
01:25:34,429 --> 01:25:37,165
to is going to as expected to

1537
01:25:38,266 --> 01:25:43,738
just generate .5.5 over states and then

1538
01:25:44,339 --> 01:25:47,809
and the outcomes that are associated
with those states look like this.

1539
01:25:48,343 --> 01:25:51,746
Then what you're going to get is a

1540
01:25:52,947 --> 01:25:53,348
policy.

1541
01:25:53,348 --> 01:25:58,286
One is going to end up
if you just do the math and out policy,

1542
01:25:58,319 --> 01:26:02,290
one is going to have a value of 2.4
for that for that term,

1543
01:26:03,057 --> 01:26:06,594
whereas policy two
is going to have a value of 7.3.

1544
01:26:06,928 --> 01:26:09,097
So this difference
is going to be a lot bigger

1545
01:26:09,397 --> 01:26:11,799
and therefore there's going to be more
outcome prediction error.

1546
01:26:11,799 --> 01:26:13,601
And so the the policy that

1547
01:26:13,601 --> 01:26:16,971
the first policy that's going to generate
the outcomes that are to

1548
01:26:16,971 --> 01:26:20,308
what you want is going to minimize
that prediction error more.

1549
01:26:21,276 --> 01:26:22,877
And so again, we just show examples.

1550
01:26:22,877 --> 01:26:25,113
We also show examples of, of this part.

1551
01:26:25,580 --> 01:26:28,216
But the basic idea here is
if this is your likelihood function,

1552
01:26:28,650 --> 01:26:32,020
then state one here is kind of imprecise
and what it predicts, right?

1553
01:26:32,020 --> 01:26:35,256
It predicts one outcome with point four
and another outcome with point six,

1554
01:26:35,757 --> 01:26:38,560
whereas state two
has a more precise distribution,

1555
01:26:38,960 --> 01:26:41,963
it generates outcome one with point two
and I'll come to a point eight.

1556
01:26:42,330 --> 01:26:44,098
So the agent would learn more

1557
01:26:44,098 --> 01:26:47,402
if it moves to state two
because the outcomes are more informative

1558
01:26:48,102 --> 01:26:50,805
and and so basically these work

1559
01:26:50,805 --> 01:26:53,975
examples just show how

1560
01:26:54,242 --> 01:26:57,612
the agent should to minimize
outcome prediction or the edge shouldn't

1561
01:26:57,612 --> 01:27:01,216
the agent should be driven
to choose to move to state to

1562
01:27:03,351 --> 01:27:04,085
And so

1563
01:27:05,086 --> 01:27:05,853
finally, and I'm not going

1564
01:27:05,853 --> 01:27:08,923
to go into this at all,
but we also show this

1565
01:27:09,991 --> 01:27:13,394
panel here that corresponds
to another kind of element

1566
01:27:13,861 --> 01:27:15,930
that's been proposed
in the normal process theory

1567
01:27:15,930 --> 01:27:18,866
that has to do with model reduction
and sleep.

1568
01:27:19,934 --> 01:27:21,903
And the basic idea is that sleep

1569
01:27:21,903 --> 01:27:27,041
allows you to search for other models
that you might

1570
01:27:27,775 --> 01:27:30,678
you might entertain
that can produce the same experiences

1571
01:27:30,678 --> 01:27:33,281
you had during the day, but can do
so in a simpler way.

1572
01:27:34,415 --> 01:27:35,283
So it's finding.

1573
01:27:35,283 --> 01:27:38,386
So sleep is basically still trying
to minimize free energy,

1574
01:27:38,720 --> 01:27:43,358
but not with respect to new observations,
but just with respect to your model

1575
01:27:43,391 --> 01:27:46,294
based on the experiences
that you already had earlier.

1576
01:27:47,195 --> 01:27:50,298
And that corresponds to

1577
01:27:50,298 --> 01:27:53,101
internal dynamics
that can essentially prune away

1578
01:27:53,101 --> 01:27:55,903
or reduce the weights
of synaptic connections

1579
01:27:55,903 --> 01:27:59,707
that are contributing less
to explaining what you did.

1580
01:27:59,707 --> 01:28:01,776
So essentially kind of noise

1581
01:28:01,776 --> 01:28:04,746
like like if there was a
bunch of kind of coincidental

1582
01:28:07,382 --> 01:28:08,883
connections between different things

1583
01:28:08,883 --> 01:28:11,352
you observed, then this kind of noise
in what you've learned.

1584
01:28:11,886 --> 01:28:14,122
And so you can during sleep,
you can reduce that.

1585
01:28:15,023 --> 01:28:18,092
And here's just a few papers
that have talked about that

1586
01:28:18,092 --> 01:28:21,696
and shown simulations
in case people are interested.

1587
01:28:23,598 --> 01:28:26,901
But so now moving on to

1588
01:28:26,934 --> 01:28:30,905
what Chris is going to talk about
are hierarchical models.

1589
01:28:31,539 --> 01:28:35,877
And the idea here is
you can just take the same exact

1590
01:28:35,877 --> 01:28:38,913
structure of the model here
that we've shown before.

1591
01:28:40,081 --> 01:28:43,751
But instead of
but put a layer kind of below that,

1592
01:28:44,085 --> 01:28:45,453
that's the same kind of thing.

1593
01:28:45,453 --> 01:28:46,754
But instead of the observation

1594
01:28:46,754 --> 01:28:50,892
those being actual outcomes,
the observations are just whatever

1595
01:28:51,326 --> 01:28:53,895
the states, the beliefs overstates

1596
01:28:53,895 --> 01:28:57,565
states are at the lower level
or the observations.

1597
01:28:57,899 --> 01:29:01,602
The observations for the second level
are just the posteriors overstates

1598
01:29:01,602 --> 01:29:03,638
at the first level.

1599
01:29:03,638 --> 01:29:06,541
And so that just looks like this

1600
01:29:07,041 --> 01:29:09,844
where you see here
that you still are selecting policies

1601
01:29:09,844 --> 01:29:13,147
at the higher level, although you can't
also select policies at the lower level.

1602
01:29:13,581 --> 01:29:16,617
But states at time to hear just

1603
01:29:16,718 --> 01:29:20,855
generate states of time
one and states of time one or states it

1604
01:29:21,055 --> 01:29:25,293
sorry states that wobble
one act as the observations for

1605
01:29:26,260 --> 01:29:27,995
level two.

1606
01:29:28,396 --> 01:29:31,199
And you can see necessarily
these operate over a lot slower

1607
01:29:31,199 --> 01:29:35,737
time scales
well so for instance a time one level two

1608
01:29:36,337 --> 01:29:40,375
the first state state of time
one is going to generate the

1609
01:29:41,576 --> 01:29:44,812
first state
at time, one at the lower level.

1610
01:29:45,346 --> 01:29:49,584
But then the lower level
is going to move to its beliefs

1611
01:29:49,584 --> 01:29:52,520
to state to over a faster time scale.

1612
01:29:53,988 --> 01:29:57,058
And so this whole thing
is going to repeat at the lower level.

1613
01:29:57,058 --> 01:30:00,261
So the thing is going to infer posteriors
over states based on two

1614
01:30:00,661 --> 01:30:02,697
observations at fast time scales

1615
01:30:03,765 --> 01:30:06,801
and then the posterior beliefs over
states at the lower level

1616
01:30:06,801 --> 01:30:10,738
are then going to propagate up
and act as the observations

1617
01:30:11,706 --> 01:30:15,810
for state
one at a time, one at the higher level.

1618
01:30:16,244 --> 01:30:19,213
And then that's going to transition
through a second level, be a matrix

1619
01:30:19,213 --> 01:30:24,552
to beliefs over states of time, to which
then is just going to provide priors

1620
01:30:25,353 --> 01:30:28,923
to a second trial,
the start of a second trial

1621
01:30:29,957 --> 01:30:31,993
at the lower level,

1622
01:30:32,427 --> 01:30:34,362
which again
operates over a faster time scale.

1623
01:30:34,362 --> 01:30:37,965
So essentially this whole lower level
model completes its belief update

1624
01:30:38,933 --> 01:30:42,203
before the higher level transitions
to its next state.

1625
01:30:43,304 --> 01:30:46,340
And in the normal process, theory,
it really just involves

1626
01:30:46,340 --> 01:30:51,279
taking the first level, just kind of
tacking on another wall of neurons on top

1627
01:30:51,612 --> 01:30:55,116
and making the observations
of the second level,

1628
01:30:55,116 --> 01:30:58,419
the things that drive
the prediction error and the states

1629
01:30:58,786 --> 01:31:01,956
as adjusts the posterior over states
at that the first level.

1630
01:31:01,956 --> 01:31:02,223
So it's

1631
01:31:02,223 --> 01:31:06,761
just kind of repeating the same thing
but at a higher level and treating the

1632
01:31:07,428 --> 01:31:10,364
the interactions
between the second and the first level,

1633
01:31:10,698 --> 01:31:13,267
the same way that observations
feed into the first level

1634
01:31:14,802 --> 01:31:15,703
and so.

1635
01:31:15,703 --> 01:31:17,972
I raise a question on this
slide from the chart.

1636
01:31:18,473 --> 01:31:22,009
Someone asked, can you please
explain the relationship or difference

1637
01:31:22,009 --> 01:31:26,614
between models based upon deep active
inference versus sophisticated inference?

1638
01:31:26,948 --> 01:31:29,684
And how is that
related to the current discussion around

1639
01:31:30,251 --> 01:31:32,787
dopamine signaling,
whether it's basic or non basic?

1640
01:31:32,787 --> 01:31:34,255
So two questions.

1641
01:31:34,255 --> 01:31:36,657
One is what is deep

1642
01:31:36,657 --> 01:31:40,461
active inference and how is that related
to sophisticated active inference?

1643
01:31:40,461 --> 01:31:44,198
And how are those related to what
we're discussing now about dopamine?

1644
01:31:44,432 --> 01:31:46,467
And I have a thought, if you can.

1645
01:31:47,435 --> 01:31:49,504
I can answer all these very quickly.

1646
01:31:49,504 --> 01:31:51,472
This is a perfect answer. Go for it.

1647
01:31:51,472 --> 01:31:55,042
So deep active inference
can kind of correspond to two things.

1648
01:31:55,810 --> 01:31:56,611
You can talk about it

1649
01:31:56,611 --> 01:32:01,048
in terms of using deep neural networks
to parameterized your your thumb,

1650
01:32:01,082 --> 01:32:05,620
essentially your eyes and these
and they matrices or your policies

1651
01:32:06,454 --> 01:32:08,723
that would be kind of like a machine
learning application.

1652
01:32:08,723 --> 01:32:11,792
And when I say deep active inference,
what I generally

1653
01:32:11,792 --> 01:32:14,262
and other people have talked about,
like the paper that first came out

1654
01:32:14,262 --> 01:32:16,531
cold, like deep, whatever,
the back of inference,

1655
01:32:16,631 --> 01:32:16,964
it was just

1656
01:32:16,964 --> 01:32:19,901
one of these hierarchical models
or deep in the sense that there's this

1657
01:32:20,868 --> 01:32:23,471
temporally deep or temporally thicker

1658
01:32:24,238 --> 01:32:27,141
and could say time, time
scales stacked on top.

1659
01:32:27,542 --> 01:32:31,946
And so then in terms of sophisticated
active inference,

1660
01:32:31,946 --> 01:32:34,749
that's just to do with policy selection
and where

1661
01:32:35,016 --> 01:32:38,553
you're essentially doing something
that looks more like a tree, such

1662
01:32:41,589 --> 01:32:43,457
so I would just say

1663
01:32:43,457 --> 01:32:46,327
it's equivalent to backward induction
and reinforcement learning.

1664
01:32:46,994 --> 01:32:49,897
So you essentially instead of saying
kind of

1665
01:32:49,897 --> 01:32:53,134
propagating forwards on a tree,
you start at your terminal node

1666
01:32:53,134 --> 01:32:55,670
and then you propagate backwards.

1667
01:32:55,670 --> 01:32:59,340
So you say, given where I want to go,
how do I best get there?

1668
01:32:59,507 --> 01:33:01,776
And then propagate backwards?

1669
01:33:01,876 --> 01:33:05,346
And then I don't really know
what the basic nonphysical thing means.

1670
01:33:06,981 --> 01:33:07,648
Yeah.

1671
01:33:07,648 --> 01:33:11,452
Yeah, I'm not I'm not aware of anybody
sort of saying anything about

1672
01:33:12,720 --> 01:33:16,857
if there's any change in the dopamine
story and sophisticated act of inference.

1673
01:33:17,058 --> 01:33:18,960
And in that.

1674
01:33:18,960 --> 01:33:22,330
Investigative act of inference,
you're still you're still obviously

1675
01:33:22,330 --> 01:33:25,032
updating beliefs about policies
when you make a new observation.

1676
01:33:26,434 --> 01:33:27,668
But but yeah,

1677
01:33:27,668 --> 01:33:31,472
I mean, the main difference, like Chris
said, is just it's just how you come

1678
01:33:31,472 --> 01:33:35,409
to post your beliefs or policies by doing
the sort of more thorough research

1679
01:33:35,776 --> 01:33:39,647
where you have this additional element
where you say you know, given

1680
01:33:39,647 --> 01:33:41,616
that I observe a thing at time, too,

1681
01:33:41,616 --> 01:33:45,486
at a given branch in a tree,
what will my beliefs be at time, too?

1682
01:33:46,621 --> 01:33:47,521
And like

1683
01:33:47,521 --> 01:33:50,057
like like Chris said, it's equivalent
to backward induction

1684
01:33:51,092 --> 01:33:54,061
and it's, it's anyway.

1685
01:33:54,061 --> 01:33:55,196
Yeah. So I

1686
01:33:55,630 --> 01:33:57,865
without without again,
I don't think anyone's talked

1687
01:33:57,865 --> 01:34:01,802
about this much but presumably
you're still making observations

1688
01:34:01,802 --> 01:34:04,772
that update your beliefs or policies
as you move through time.

1689
01:34:05,806 --> 01:34:09,276
And so I don't necessarily see how
the dopamine story would need to change.

1690
01:34:10,745 --> 01:34:13,848
But but yeah, and I will say in addition

1691
01:34:13,848 --> 01:34:17,018
to what Chris said, there are really

1692
01:34:18,619 --> 01:34:21,656
two kind of related to meeting meanings

1693
01:34:22,023 --> 01:34:23,491
independent of the kind of machine
learning,

1694
01:34:23,491 --> 01:34:25,593
different network thing
that Chris talked about.

1695
01:34:25,593 --> 01:34:27,028
One is and like is of the standard,

1696
01:34:27,028 --> 01:34:29,296
one is talking
about deep temporal models. Yes.

1697
01:34:29,330 --> 01:34:35,369
And that's just exactly what I was
showing here, where the second level

1698
01:34:35,369 --> 01:34:39,040
necessarily operates at a slower
timescale than the first level,

1699
01:34:39,807 --> 01:34:43,911
because the first level has
to kind of complete all of its time steps

1700
01:34:44,779 --> 01:34:48,449
before it can provide an observation
to the higher level

1701
01:34:48,849 --> 01:34:50,251
and then the higher level.

1702
01:34:50,251 --> 01:34:54,789
So basically each higher level state
corresponds to a whole lower level trial,

1703
01:34:55,589 --> 01:34:57,825
you know, so if you wanted to,
you could make a lower level

1704
01:34:57,825 --> 01:34:59,627
trial, have ten time steps.

1705
01:34:59,627 --> 01:35:02,463
But the posterior of our beliefs
at that lower level at the end of those

1706
01:35:02,463 --> 01:35:07,001
whole times to all those time steps
would be, you know, what the observation

1707
01:35:07,601 --> 01:35:10,938
is that provides evidence
for the first state of the first

1708
01:35:10,938 --> 01:35:13,908
that the state of the first time point
at the second level.

1709
01:35:14,408 --> 01:35:17,445
So and then second
level states transition

1710
01:35:17,778 --> 01:35:19,547
and then the second level state.

1711
01:35:19,547 --> 01:35:24,018
So that basically one trial at the higher
level with a two time points,

1712
01:35:24,752 --> 01:35:28,022
each of those time points
corresponds to providing priors

1713
01:35:28,022 --> 01:35:30,591
for a whole trial at the lower level.

1714
01:35:30,991 --> 01:35:34,862
So you're going to have as many trials
at the lower level as you have time

1715
01:35:34,862 --> 01:35:36,630
steps in one trial at the higher level,

1716
01:35:38,232 --> 01:35:41,235
so so that's what deep temporal models
means,

1717
01:35:41,368 --> 01:35:44,638
just that the second level operates
at a slower time scale than the first

1718
01:35:44,638 --> 01:35:47,942
and provides priors for initial starts
on the first level.

1719
01:35:48,409 --> 01:35:52,012
And another
another meaning that some people use

1720
01:35:52,012 --> 01:35:54,882
like in the deeply felt affect
paper is just parametric models,

1721
01:35:55,783 --> 01:35:59,153
which basically just means
the first level out.

1722
01:35:59,520 --> 01:36:02,790
Things that happen at the first level
can be used as evidence for updating

1723
01:36:03,157 --> 01:36:06,360
hyper parameters for the higher level

1724
01:36:06,360 --> 01:36:08,929
and like in the deeply felt affect paper,
basically

1725
01:36:08,929 --> 01:36:12,433
we had second level states
that corresponded to valence,

1726
01:36:13,234 --> 01:36:15,903
but those were updated
not by first level beliefs,

1727
01:36:15,903 --> 01:36:18,806
but by changes in beta
at the lower level.

1728
01:36:19,073 --> 01:36:21,308
So when gamma updates were positive,

1729
01:36:21,375 --> 01:36:24,712
right, when the confidence and expected
free energy went up

1730
01:36:25,112 --> 01:36:27,748
there was a sending signal
other than updated

1731
01:36:27,748 --> 01:36:31,952
beliefs at the second level
that corresponded to feeling better.

1732
01:36:32,153 --> 01:36:33,487
Right? So positive valence

1733
01:36:34,622 --> 01:36:36,023
and those also

1734
01:36:36,023 --> 01:36:39,293
that state could also act as a primer on

1735
01:36:40,227 --> 01:36:42,897
what the beta update should be, right?

1736
01:36:43,030 --> 01:36:46,801
How confident the agent should be at
the lower level those trials go forward.

1737
01:36:46,801 --> 01:36:50,104
So it can also have to do with,
with, with

1738
01:36:51,272 --> 01:36:55,075
get its quote unquote deeper parameters
as opposed to just deeper,

1739
01:36:56,043 --> 01:36:59,046
deeper levels that control priors
overstates at the lower level.

1740
01:37:00,181 --> 01:37:03,617
But that's not as commonly used by far

1741
01:37:03,617 --> 01:37:06,353
the most common is just deep
temporal models.

1742
01:37:06,353 --> 01:37:09,590
Do you have any more to add on
the neural process or can we do our over.

1743
01:37:10,357 --> 01:37:13,194
And no neural process is pretty done

1744
01:37:13,594 --> 01:37:17,031
the only thing is, is that before
Chris starts,

1745
01:37:17,765 --> 01:37:20,801
because what we're simulating
when we're doing this

1746
01:37:21,035 --> 01:37:23,270
example
in the code of a hierarchical model,

1747
01:37:24,104 --> 01:37:26,540
the example we're using is an empirical

1748
01:37:26,841 --> 01:37:30,044
task called a local global
or oddball paradigm.

1749
01:37:30,444 --> 01:37:32,646
So Chris is just going to I'm

1750
01:37:32,646 --> 01:37:36,450
just putting up this slide
so that Chris can walk us through the the

1751
01:37:38,252 --> 01:37:40,020
design of this task.

1752
01:37:40,020 --> 01:37:44,491
So just before we cross over, just what
we take a breath, I want to make one

1753
01:37:44,925 --> 01:37:50,331
ultra rapid summary for just contrast
and then ask one question from the chat.

1754
01:37:50,698 --> 01:37:54,468
So the brief summary is we're dealing
with the active inference framework.

1755
01:37:54,735 --> 01:37:56,470
We are within free energy principle.

1756
01:37:56,470 --> 01:37:57,738
We're talking about active inference.

1757
01:37:57,738 --> 01:37:59,573
That's why it's active inference loop.

1758
01:37:59,573 --> 01:38:02,943
Now, we've heard about a couple
of adjectives, deep sophisticated

1759
01:38:03,077 --> 01:38:07,114
affective and even inactive inference,
and those have been in the titles

1760
01:38:07,114 --> 01:38:09,316
of papers
or they've been the titles of models.

1761
01:38:09,316 --> 01:38:12,686
We heard about just very concisely

1762
01:38:12,686 --> 01:38:15,923
about what deep can mean, whether deep
neural networks or deep time.

1763
01:38:16,457 --> 01:38:17,791
And then there's a third definition

1764
01:38:17,791 --> 01:38:21,428
of deep meaning,
like fully or radically parametric

1765
01:38:21,695 --> 01:38:25,399
and that is interpreted within
within the affective valence framework

1766
01:38:25,566 --> 01:38:27,334
within the paper
that Ryan just mentioned.

1767
01:38:27,334 --> 01:38:29,536
So that's deep and affective,
which are related

1768
01:38:29,536 --> 01:38:32,873
then sophisticated is related
to this tree search, counterfactual

1769
01:38:33,073 --> 01:38:35,175
policy and state estimation.

1770
01:38:35,175 --> 01:38:38,479
We talked about the sophisticated
inference in a previous stream

1771
01:38:38,746 --> 01:38:42,483
and an active is highlighting
more from an ecological

1772
01:38:42,483 --> 01:38:44,151
or even a philosophical perspective

1773
01:38:44,151 --> 01:38:47,254
about the embodied
and enacted aspects of cognition.

1774
01:38:47,755 --> 01:38:50,090
So that's just a summary
about a few of the flavors

1775
01:38:50,591 --> 01:38:52,826
under the umbrella of active inference.

1776
01:38:52,826 --> 01:38:56,163
And they don't necessitate
a renaming of the lab

1777
01:38:56,163 --> 01:38:58,265
because they're like variants
or paper titles.

1778
01:38:58,632 --> 01:39:01,302
But this is the total umbrella
that we're working under.

1779
01:39:01,769 --> 01:39:03,871
And then the general question
from the chart,

1780
01:39:03,871 --> 01:39:06,974
which I hope that set it up for,
and then we'll address this question.

1781
01:39:06,974 --> 01:39:09,410
Each person
give a very short response to that.

1782
01:39:09,410 --> 01:39:12,880
We launched into crisis
when preparing a model

1783
01:39:12,880 --> 01:39:16,417
for exploring free energy
principle active inference in MATLAB.

1784
01:39:16,884 --> 01:39:20,654
Do you often sketch out equations
with sample Bayesian calculations

1785
01:39:20,654 --> 01:39:23,757
for each term to help clarify
your thinking and track what will happen?

1786
01:39:24,058 --> 01:39:27,728
Or in other words,
how do you go from thinking or specifying

1787
01:39:27,728 --> 01:39:31,966
the generative model
of even a experiments into what you do

1788
01:39:31,966 --> 01:39:35,636
when you're talking like this
behavioral example to the code?

1789
01:39:35,836 --> 01:39:36,704
How do you do that?

1790
01:39:38,439 --> 01:39:41,141
I mean, so for me, honestly,

1791
01:39:41,141 --> 01:39:46,480
like when I'm actually setting up a model
for a task, I don't really think

1792
01:39:46,480 --> 01:39:50,150
about the equations hardly at all
because you don't need to write.

1793
01:39:50,150 --> 01:39:54,555
I mean, what you need to or you need to
focus on is just what the necessary

1794
01:39:54,555 --> 01:39:57,458
elements are in terms of states
and observations

1795
01:39:58,625 --> 01:40:01,028
and policies

1796
01:40:01,028 --> 01:40:02,763
that are just in a task, right?

1797
01:40:02,763 --> 01:40:05,132
So you have to think, okay, in a task,

1798
01:40:05,132 --> 01:40:09,803
what are the observations
that the participant has?

1799
01:40:10,237 --> 01:40:10,537
Right.

1800
01:40:10,537 --> 01:40:13,807
Is there just one possible observation
modality?

1801
01:40:13,807 --> 01:40:17,745
They either observed this or that,
or are there multiple observation

1802
01:40:17,745 --> 01:40:21,482
modalities like maybe they observe a cue
and then the observer reward?

1803
01:40:21,982 --> 01:40:22,316
Right.

1804
01:40:22,316 --> 01:40:26,153
And so then you need two observation
modalities, right?

1805
01:40:26,153 --> 01:40:27,988
You think about
how many in each modality, right?

1806
01:40:27,988 --> 01:40:29,023
Is it just cue or no?

1807
01:40:29,023 --> 01:40:33,560
Cue, you know, as a just reward
or no reward or is it no reward,

1808
01:40:33,961 --> 01:40:36,697
reward or loss?

1809
01:40:36,697 --> 01:40:39,833
So you just think, okay, what are the
different types of observations

1810
01:40:40,167 --> 01:40:42,269
and how many observations
within each type?

1811
01:40:42,269 --> 01:40:44,671
Just, you know,
what does the actual participant observe?

1812
01:40:45,706 --> 01:40:47,141
And then you
have to think about the states, right?

1813
01:40:47,141 --> 01:40:49,743
What are the
what is the participants beliefs?

1814
01:40:49,743 --> 01:40:51,145
What is the minimal set of beliefs?

1815
01:40:51,145 --> 01:40:54,982
They need to be able
to make a decision, right?

1816
01:40:54,982 --> 01:40:56,750
About how to get reward.

1817
01:40:56,750 --> 01:41:00,621
If it's a reward task, and, you know,
so that might be beliefs

1818
01:41:00,621 --> 01:41:03,957
about what trial type it is, right?

1819
01:41:03,957 --> 01:41:07,594
Or my beliefs about what
are the different choices I might make.

1820
01:41:08,328 --> 01:41:08,629
Right.

1821
01:41:08,629 --> 01:41:13,200
Like moving into the state of choosing,
say, option one versus option two,

1822
01:41:14,301 --> 01:41:15,736
you know,
and then you have to think, okay,

1823
01:41:15,736 --> 01:41:18,238
what are the different action sequences
right?

1824
01:41:18,238 --> 01:41:21,842
Like, does the agent
just push a button and get lucky,

1825
01:41:21,842 --> 01:41:25,746
you know, reward or no reward or,
you know, the the agent have to,

1826
01:41:26,413 --> 01:41:28,816
you know, make a couple choices in a row,
you know, just

1827
01:41:28,816 --> 01:41:30,751
what are the actual options?

1828
01:41:30,751 --> 01:41:31,018
Right.

1829
01:41:31,018 --> 01:41:33,520
And so, I mean, that's
that's really what it boils down to

1830
01:41:33,520 --> 01:41:38,192
is just figuring out what things that
the participant observe you know, what

1831
01:41:38,192 --> 01:41:41,728
beliefs do they need to make choices,
what choices are available.

1
00:00:07,674 --> 00:00:08,708
Hello, everyone.

2
00:00:08,708 --> 00:00:14,014
Welcome to the Active Inference Lab
and to the Model Stream 2.0.

3
00:00:14,147 --> 00:00:16,516
Today is going to be a really awesome
discussion,

4
00:00:16,516 --> 00:00:19,519
so let's just introduce ourselves
and get right into it.

5
00:00:19,853 --> 00:00:20,954
I'm Daniel Freedman.

6
00:00:20,954 --> 00:00:23,890
I'm a postdoctoral researcher
in Davis, California.

7
00:00:24,124 --> 00:00:26,626
Right Yep.

8
00:00:26,626 --> 00:00:27,961
So I'm Ryan Smith.

9
00:00:27,961 --> 00:00:30,964
I'm an associate investigator
at the Laureate

10
00:00:30,964 --> 00:00:33,800
Institute for Brain Research.

11
00:00:34,801 --> 00:00:35,935
Hi, I'm Christopher.

12
00:00:35,935 --> 00:00:37,470
I'm a Ph.D.

13
00:00:37,470 --> 00:00:40,440
student at the University of Cambridge.

14
00:00:40,774 --> 00:00:42,409
And I'm Max Murphy.

15
00:00:42,409 --> 00:00:43,276
And I'll be starting

16
00:00:43,276 --> 00:00:46,479
my postdoctoral fellowship
at Carnegie Mellon in hopefully a week.

17
00:00:47,547 --> 00:00:48,114
Awesome.

18
00:00:48,114 --> 00:00:50,650
Thanks to Ryan and Christopher,
the authors

19
00:00:51,151 --> 00:00:53,920
or two of the three authors of this paper
for joining today

20
00:00:53,920 --> 00:00:56,823
and for Max
for lending your expertize as well.

21
00:00:57,323 --> 00:01:01,728
So this is the second model stream
in a multi part series

22
00:01:01,895 --> 00:01:03,830
that is going to be highlighting
perspectives

23
00:01:03,830 --> 00:01:07,434
and addressing questions
related to applied active inference.

24
00:01:07,801 --> 00:01:12,539
And so right now we're looking at the
tutorial paper of Smith at all, a step

25
00:01:12,539 --> 00:01:16,609
by step tutorial on active inference
and its application to empirical data.

26
00:01:16,843 --> 00:01:19,479
The link
you can find in the video description.

27
00:01:20,046 --> 00:01:23,950
So if you have any questions or thoughts
or comments during the live stream,

28
00:01:24,217 --> 00:01:27,287
type it in the YouTube live chat
and we'll be monitoring that.

29
00:01:27,754 --> 00:01:30,156
And then if you have any questions
after the live stream

30
00:01:30,323 --> 00:01:33,460
leave a comment on the video
or get in touch with us another way

31
00:01:33,626 --> 00:01:35,862
and we'll try to address it
in a future session.

32
00:01:36,262 --> 00:01:38,665
And if you want to learn more
or if you want to participate,

33
00:01:38,865 --> 00:01:41,034
go to active inference dot org.

34
00:01:41,835 --> 00:01:42,502
All right.

35
00:01:42,535 --> 00:01:45,572
So today in our second session,
we're going to be picking up

36
00:01:45,572 --> 00:01:49,275
with a brief overview and summary
of where we're at, where we've been.

37
00:01:49,676 --> 00:01:50,477
We're going to go through

38
00:01:50,477 --> 00:01:54,180
a few clarifying questions
that have been raised in the last week.

39
00:01:54,447 --> 00:01:58,785
And then we're going to dove into
the specific examples and into the code

40
00:01:58,952 --> 00:02:00,520
and do some code walkthrough.

41
00:02:00,520 --> 00:02:02,388
So that would be really exciting.

42
00:02:02,388 --> 00:02:02,655
All right.

43
00:02:02,655 --> 00:02:05,859
So I'll just throw out
the first opening question,

44
00:02:05,859 --> 00:02:08,495
which is Who is this session for?

45
00:02:08,862 --> 00:02:11,731
What kinds of tools and skills
would you say

46
00:02:11,731 --> 00:02:15,235
are required and which kinds of tools
or skills might be helpful?

47
00:02:18,771 --> 00:02:19,772
Okay.

48
00:02:19,839 --> 00:02:23,710
Well, I mean, I would
say that the session and I mean, really

49
00:02:23,710 --> 00:02:27,780
the entire tutorial is for people
who are just kind of

50
00:02:27,780 --> 00:02:29,382
I mean, mainly people
who are just starting out,

51
00:02:29,382 --> 00:02:32,986
people who want to be able,
who are interested in active inference,

52
00:02:34,020 --> 00:02:37,590
but maybe don't have a lot of background
in mathematics, you know,

53
00:02:37,590 --> 00:02:39,392
especially like people
coming from like neuroscience

54
00:02:39,392 --> 00:02:42,495
and psychology backgrounds, for example,
that want to

55
00:02:42,495 --> 00:02:45,865
or people
who do some computational modeling.

56
00:02:45,865 --> 00:02:49,636
But like maybe like simpler reinforcement
learning models, things like that,

57
00:02:50,403 --> 00:02:53,406
where active inference
is kind of notoriously a little bit

58
00:02:53,406 --> 00:02:56,876
more kind of like black box, you know,
it's harder to kind of break into.

59
00:02:57,277 --> 00:03:00,713
And a lot of people think that the models
are described in a way that's kind of

60
00:03:00,713 --> 00:03:01,414
like opaque,

61
00:03:02,515 --> 00:03:02,882
you know,

62
00:03:02,882 --> 00:03:07,187
and so the the idea was to give people
the background needed

63
00:03:07,921 --> 00:03:09,956
and the actual example code

64
00:03:10,623 --> 00:03:13,393
that would be needed,
that could be adapted to build

65
00:03:13,393 --> 00:03:16,963
your own models,
do your own simulation studies, and apply

66
00:03:17,330 --> 00:03:20,700
and build models of task studies
and be able to fit models to data.

67
00:03:21,568 --> 00:03:24,270
In other words,
to kind of start to finish, to be able

68
00:03:24,270 --> 00:03:26,539
for a person who wants to to be able to

69
00:03:27,540 --> 00:03:30,009
use inference models to model

70
00:03:30,009 --> 00:03:32,712
data in empirical studies and actually
use them in their own research.

71
00:03:33,246 --> 00:03:36,149
And so as far as tools necessary

72
00:03:36,149 --> 00:03:39,452
you know, the main thing right now
is going to be having

73
00:03:39,452 --> 00:03:43,423
some minimal understanding or motivation
to learn how to use MATLAB

74
00:03:45,391 --> 00:03:46,426
So, you know,

75
00:03:46,426 --> 00:03:50,330
we in the tutorial codes,
I mean, we have like seven

76
00:03:50,330 --> 00:03:53,466
or eight different tutorial scripts,
you know, at this point.

77
00:03:53,466 --> 00:03:54,767
But the main one

78
00:03:54,767 --> 00:03:58,404
is the one that I'll be focusing on today
and it's very heavily commented.

79
00:03:59,105 --> 00:04:02,242
So it kind of best we did try to describe
kind of like line by line

80
00:04:02,242 --> 00:04:03,376
each thing that's going on.

81
00:04:03,376 --> 00:04:05,478
I mean, it's
honestly there's enough comments in there

82
00:04:05,478 --> 00:04:07,947
not must be kind of like a second
paper in and of itself.

83
00:04:09,315 --> 00:04:10,316
So, you

84
00:04:10,316 --> 00:04:15,121
know, we're hoping that even someone
who doesn't have a ton of background in

85
00:04:15,121 --> 00:04:17,890
MATLAB will be able to kind of bootstrap
their way up.

86
00:04:18,925 --> 00:04:21,561
But you definitely need

87
00:04:21,561 --> 00:04:24,264
to have a little bit or be motivated
to learn at least a little bit.

88
00:04:25,531 --> 00:04:27,567
Anything else to add on that,
Christopher or Max?

89
00:04:29,636 --> 00:04:30,837
I think that sums it up well.

90
00:04:30,837 --> 00:04:32,839
Just one note on the code there.

91
00:04:32,872 --> 00:04:34,874
So you mentioned it's in MATLAB.

92
00:04:34,874 --> 00:04:39,212
Now, what about other programing
languages or backgrounds like Python

93
00:04:39,245 --> 00:04:43,082
so what other languages are people in
the active inference space developing in?

94
00:04:43,116 --> 00:04:44,851
And how could we include

95
00:04:44,851 --> 00:04:48,054
more computer languages
as well as natural human languages?

96
00:04:48,955 --> 00:04:51,824
So I mean,

97
00:04:51,858 --> 00:04:53,493
there are so at present,

98
00:04:53,493 --> 00:04:58,031
everything is almost exclusively
in MATLAB, unfortunately.

99
00:04:58,364 --> 00:04:59,265
And the reason for that

100
00:04:59,265 --> 00:05:03,703
is, is that a lot of this
is actually built off of SPM scripts.

101
00:05:03,703 --> 00:05:05,171
You know, the Karl Freston

102
00:05:05,171 --> 00:05:09,742
originally wrote in relation
to neuroimaging approaches.

103
00:05:09,742 --> 00:05:11,778
So things like dynamic causal modeling

104
00:05:12,845 --> 00:05:15,281
and so and a lot of

105
00:05:15,682 --> 00:05:18,418
a lot of the current scripts
for doing active inference

106
00:05:19,152 --> 00:05:21,854
call on a broad range of other

107
00:05:21,954 --> 00:05:25,391
SPM scripts that are used for a lot of
different purposes, for instance,

108
00:05:25,758 --> 00:05:29,162
and sort of dealing with large matrices

109
00:05:29,162 --> 00:05:33,366
in and like neuroimaging and so that.

110
00:05:33,733 --> 00:05:37,203
So for that reason
and because Karl is primarily

111
00:05:37,203 --> 00:05:41,374
working in MATLAB in relation SPM,
everything has kind of been in SPM

112
00:05:41,507 --> 00:05:45,578
and specifically in
what's called the DSM toolbox in SPM.

113
00:05:45,578 --> 00:05:48,915
12 stands for dynamic expectation
maximization.

114
00:05:49,982 --> 00:05:52,952
There's a ton of active inference scripts
as well as a ton of other modeling

115
00:05:52,952 --> 00:05:56,889
scripts for say like continuous models
and other sorts of things in there.

116
00:05:57,790 --> 00:06:00,760
And we just make use of a couple of those

117
00:06:01,060 --> 00:06:04,097
are the main ones,
not the ones they call on. But

118
00:06:05,331 --> 00:06:06,699
but that

119
00:06:06,699 --> 00:06:10,570
being said, there
are people who are trying at the moment

120
00:06:10,570 --> 00:06:13,506
to translate some of this stuff
into Python.

121
00:06:14,407 --> 00:06:17,243
Alec shots, for example,

122
00:06:17,243 --> 00:06:19,946
is currently in the process
of trying to put together

123
00:06:20,880 --> 00:06:25,918
Python versions of some of the main
simulation and version scripts

124
00:06:26,386 --> 00:06:30,423
and I don't I mean, they're very kind of
in beta at the moment

125
00:06:30,423 --> 00:06:31,290
as I understand them.

126
00:06:31,290 --> 00:06:33,426
But I know I know that that's something
that's in progress.

127
00:06:34,460 --> 00:06:34,861
Cool.

128
00:06:34,861 --> 00:06:38,798
So it sounds like because this project
has such a history

129
00:06:38,798 --> 00:06:42,034
of being developed in MATLAB
and the SPM toolkit and Coral,

130
00:06:42,034 --> 00:06:44,637
for instance,
research and collaborations, we're

131
00:06:45,171 --> 00:06:49,876
drawing heavily on MATLAB However,
in the modern development context,

132
00:06:50,042 --> 00:06:53,246
there's a lot of open source projects
that are developing in Python.

133
00:06:53,413 --> 00:06:55,848
So that's definitely an area
will hope to be expanding.

134
00:06:55,848 --> 00:06:59,485
And also a great opportunity
for people who are experienced in Python

135
00:06:59,919 --> 00:07:02,655
or MATLAB and want to be curious
about active inference.

136
00:07:02,889 --> 00:07:05,658
This is the opening to help contribute

137
00:07:05,658 --> 00:07:08,327
to something that could be really
exciting. So just a last,

138
00:07:09,395 --> 00:07:11,998
last warm up introduction question.

139
00:07:12,532 --> 00:07:16,469
What kinds of projects
or questions or investigations

140
00:07:16,702 --> 00:07:20,206
do you think this could apply to
for somebody who they've

141
00:07:20,206 --> 00:07:23,109
heard of scientific modeling
but not active inference modeling?

142
00:07:23,209 --> 00:07:26,913
What do you think are some low-hanging
fruit or really straightforward

143
00:07:26,913 --> 00:07:28,981
interpretations
where you think it could be applied?

144
00:07:30,349 --> 00:07:32,385
So, I mean, I guess
there's a couple of questions here.

145
00:07:32,385 --> 00:07:36,422
I mean, one is and may not like well,
I guess the main one is

146
00:07:36,923 --> 00:07:40,426
given that the kind of history
of computational modeling has

147
00:07:41,828 --> 00:07:44,497
largely been in the

148
00:07:44,831 --> 00:07:47,767
like reinforcement
learning you know, sort of literature

149
00:07:47,767 --> 00:07:48,835
or I mean, there are others

150
00:07:48,835 --> 00:07:51,170
like director data and modeling
and things like that. But

151
00:07:53,072 --> 00:07:55,675
the, you know,
one of the kind of major questions.

152
00:07:55,875 --> 00:07:59,545
So a lot of inference models
seem like they're really complicated,

153
00:07:59,679 --> 00:08:02,281
you know, relative
to some of these simpler

154
00:08:02,949 --> 00:08:05,551
sort of traditional reinforcement
learning models.

155
00:08:05,551 --> 00:08:07,954
So what is it
that active inference actually adds?

156
00:08:07,954 --> 00:08:08,521
Right.

157
00:08:08,521 --> 00:08:12,091
Like, what can you do using an active
inference model in a study

158
00:08:12,492 --> 00:08:17,196
that you maybe couldn't do
or could do as naturally

159
00:08:17,763 --> 00:08:20,733
using standard search of existing
reinforcement learning models

160
00:08:21,300 --> 00:08:24,437
and you know, here, the kind of answer

161
00:08:24,437 --> 00:08:26,973
that I would want to give
is there's a few parts to this.

162
00:08:27,440 --> 00:08:28,508
One is that

163
00:08:29,509 --> 00:08:31,377
because the

164
00:08:31,377 --> 00:08:35,781
kind of value or cost function
in active inference is expected

165
00:08:35,781 --> 00:08:38,951
for energy
as opposed to a reward function.

166
00:08:40,253 --> 00:08:42,388
And because the expected free energy

167
00:08:42,889 --> 00:08:47,193
has a reward, a reward component
and an information

168
00:08:48,060 --> 00:08:51,564
value component,
then it's it's quite a bit more natural

169
00:08:51,697 --> 00:08:56,335
and active inference models
to model, explore, exploit tasks,

170
00:08:56,402 --> 00:09:00,540
you know, so tasks where the agent has
to somehow intrinsically assign value

171
00:09:00,540 --> 00:09:04,544
to seeking information
and as a means of knowing

172
00:09:04,544 --> 00:09:07,547
how to maximize reward eventually.

173
00:09:07,547 --> 00:09:09,415
So there are and there are actually two

174
00:09:09,415 --> 00:09:12,752
different types of sort of directed
or goal

175
00:09:12,752 --> 00:09:15,388
driven explanation or exploration
and act of inference.

176
00:09:15,855 --> 00:09:17,423
One is

177
00:09:17,757 --> 00:09:20,426
what we just call parameter exploration.

178
00:09:20,426 --> 00:09:23,329
And the idea here is
this is probably the closest thing to

179
00:09:24,230 --> 00:09:27,099
reinforcement learning where say,
the agent might not start out

180
00:09:27,099 --> 00:09:27,800
knowing what

181
00:09:27,800 --> 00:09:31,037
the reward probabilities are
if they choose one action versus another.

182
00:09:31,571 --> 00:09:34,974
So they will choose actions
that will help them

183
00:09:34,974 --> 00:09:39,211
essentially learn best
what the reward probabilities are before,

184
00:09:39,512 --> 00:09:44,517
you know, just reliably or continuously
selecting the same option.

185
00:09:44,517 --> 00:09:44,750
Right.

186
00:09:44,750 --> 00:09:47,253
Because that's the kind of know
which one's the most rewarding first.

187
00:09:47,820 --> 00:09:53,426
So that's called parameter exploration
and that that, like I said, has analogs

188
00:09:53,960 --> 00:09:58,230
and reinforcement learning
but doesn't have like a specific

189
00:09:58,230 --> 00:10:03,603
in standard models anyway, doesn't have
a very specific sort of information drive

190
00:10:04,870 --> 00:10:06,706
to to seek that out.

191
00:10:06,706 --> 00:10:10,009
But there are some so that things
like information, bonus terms, different

192
00:10:10,009 --> 00:10:12,345
things like that and reinforcement
learning that can do something like that.

193
00:10:13,245 --> 00:10:15,247
But they're kind of tacked
on. They're a little, little

194
00:10:15,247 --> 00:10:17,416
and a little more
kind of like ad hoc to them, I guess.

195
00:10:19,218 --> 00:10:21,887
Whereas it kind of emerges
naturally from expected free energy

196
00:10:21,887 --> 00:10:23,089
and active inference.

197
00:10:23,089 --> 00:10:25,958
And there's also something
that's a little more unique

198
00:10:25,958 --> 00:10:28,427
and act of inference,
which is called state exploration,

199
00:10:28,961 --> 00:10:30,429
and that's more aware

200
00:10:30,429 --> 00:10:34,467
you're just kind of choosing to move
to states that you expect will give you

201
00:10:34,467 --> 00:10:37,870
the most precise observations
about what state you're in.

202
00:10:39,171 --> 00:10:41,807
You know, so this would be the thing
like you don't know whether you're

203
00:10:41,807 --> 00:10:45,011
in the green room or the red room,
so you should go turn on a light

204
00:10:45,845 --> 00:10:47,647
so you'll know
which which room you're in,

205
00:10:48,714 --> 00:10:49,081
right?

206
00:10:49,081 --> 00:10:52,485
So that also falls out of expected
free energy very naturally,

207
00:10:53,152 --> 00:10:56,522
but not out of a lot of other

208
00:10:56,756 --> 00:10:59,191
traditional computational modeling
approaches.

209
00:11:00,493 --> 00:11:04,263
The there's also sorry,
I could go on for a while about this,

210
00:11:04,263 --> 00:11:07,800
but there's also something
in reinforcement learning models

211
00:11:07,800 --> 00:11:10,236
that has to do with explore,
exploit called random exploration,

212
00:11:10,836 --> 00:11:14,273
where people essentially start
behaving more randomly.

213
00:11:14,607 --> 00:11:16,809
And if they're uncertain

214
00:11:17,343 --> 00:11:20,579
and if they know they have to make
a bunch more feature choices.

215
00:11:22,081 --> 00:11:25,951
But those values are typically fixed
in reinforcement learning models,

216
00:11:26,385 --> 00:11:31,023
whereas the expected for energy precision
parameter and active

217
00:11:31,023 --> 00:11:34,660
inference is actually
a dynamically updated version of that.

218
00:11:35,161 --> 00:11:37,663
So it changes to remain driving,

219
00:11:37,830 --> 00:11:39,932
especially the amount of randomness
and behavior that's

220
00:11:41,100 --> 00:11:44,637
expected given past observations.

221
00:11:44,637 --> 00:11:46,739
And so that's kind of nice.

222
00:11:46,739 --> 00:11:49,175
The last thing I would say
is, is that active

223
00:11:49,175 --> 00:11:52,812
inference has a very kind of specific
neural process theory associated with it.

224
00:11:53,245 --> 00:11:56,982
So it makes very specific predictions
about the neural responses

225
00:11:56,982 --> 00:12:00,086
you ought to see and studies
whereas most other models don't.

226
00:12:00,720 --> 00:12:03,022
And so those are that's what I would say.

227
00:12:03,022 --> 00:12:04,256
Thanks for the answer.

228
00:12:04,256 --> 00:12:07,626
Any thoughts on that or any
other questions, Christopher or Max?

229
00:12:09,295 --> 00:12:09,462
I would

230
00:12:09,462 --> 00:12:13,232
just say very broadly speaking, active
inference is really great for modeling.

231
00:12:13,232 --> 00:12:18,270
Decision-Making Under Uncertainty,
and in particular, perceptual,

232
00:12:18,270 --> 00:12:22,775
various types of perceptual decision
making and in discrete state spaces.

233
00:12:22,775 --> 00:12:24,810
So you can do anything from modeling.

234
00:12:24,810 --> 00:12:27,980
So like Raymond,
I have some previous work where we show

235
00:12:28,314 --> 00:12:30,616
that you can reproduce
a whole lot of kind of

236
00:12:31,117 --> 00:12:35,755
classic visual awareness,
neural correlates, and like the classic

237
00:12:35,755 --> 00:12:39,658
visual awareness paradigms for example,
using a hierarchical model

238
00:12:41,994 --> 00:12:45,464
And the reason we can do that
is because it had

239
00:12:45,798 --> 00:12:49,201
active inference has such a detailed
and rich process theory attached to it.

240
00:12:50,402 --> 00:12:52,972
And so for me,
I think what's really unique about it

241
00:12:52,972 --> 00:12:56,742
is that so there's lots of different ways
of providing unified explanations.

242
00:12:56,742 --> 00:12:58,110
But one way that you might want

243
00:12:58,110 --> 00:13:01,547
to provide a unifying explanation
is just have a unifying set of kind of

244
00:13:03,249 --> 00:13:03,749
common

245
00:13:03,916 --> 00:13:06,752
unifying computation
or one computational architecture

246
00:13:06,752 --> 00:13:10,089
where you can draw in all of these
different things into one paradigm

247
00:13:10,556 --> 00:13:13,292
and then understand, well,
one computational paradigm

248
00:13:13,292 --> 00:13:15,294
that is, and understand
how they're related

249
00:13:15,294 --> 00:13:17,229
through the different parameters
that are being modulated.

250
00:13:17,229 --> 00:13:20,199
When you being fit,
when you build models, for example,

251
00:13:21,600 --> 00:13:24,303
So I'm not aware
of any other modeling paradigm really

252
00:13:24,303 --> 00:13:30,075
where you could model things as diverse
as a basic kind of explore exploit task

253
00:13:30,776 --> 00:13:34,647
or looking at kind of basic
domain dynamics.

254
00:13:34,880 --> 00:13:37,917
And then something that goes all the way
up to something like visual awareness

255
00:13:38,284 --> 00:13:42,421
or even like planning and Belman
optimal planning.

256
00:13:42,988 --> 00:13:44,957
So to me, that's
what's so special about this.

257
00:13:44,957 --> 00:13:48,727
If you if you want to unify things
under one umbrella.

258
00:13:48,961 --> 00:13:49,562
This is really.

259
00:13:52,031 --> 00:13:52,398
Yeah.

260
00:13:52,398 --> 00:13:55,634
One last thing I'll say, and this is just
kind of a practical, nice thing.

261
00:13:55,634 --> 00:13:57,970
And so this is that they and most other

262
00:13:58,671 --> 00:14:01,407
modeling approaches you actually have
to kind of change the equations,

263
00:14:02,141 --> 00:14:05,177
you know, to set it to a specific

264
00:14:05,177 --> 00:14:05,945
task.

265
00:14:06,312 --> 00:14:09,481
Whereas an actual inference
technically you don't all you do

266
00:14:09,481 --> 00:14:11,717
is that the actual update
equations are always identical.

267
00:14:12,585 --> 00:14:14,687
All you have to do is write down

268
00:14:14,854 --> 00:14:18,591
the appropriate generative model

269
00:14:18,858 --> 00:14:21,160
so it's nice
because it's, it's really generic.

270
00:14:21,160 --> 00:14:23,295
And, you know, once
you have the generative model

271
00:14:23,295 --> 00:14:26,065
specified, everything else
kind of just comes for free.

272
00:14:26,065 --> 00:14:28,133
And just to jump in off of that

273
00:14:28,634 --> 00:14:31,270
generalizability,
the reason I'm interested, I

274
00:14:31,270 --> 00:14:35,574
am somebody who studies movement at
totally different area of neuroscience.

275
00:14:35,574 --> 00:14:38,611
Then decision
making and uncertainty modeling.

276
00:14:38,611 --> 00:14:42,882
But at the same time, this framework,
what kind of interesting thing about it

277
00:14:42,882 --> 00:14:46,252
is that exactly that, which is that
it seems that I can start out with kind

278
00:14:46,252 --> 00:14:50,089
of a structured generative model
that is consistent even across domains

279
00:14:50,422 --> 00:14:54,927
and then potentially use that to apply it
to my own neuroscientific research.

280
00:14:56,328 --> 00:14:59,465
The main questions that I have
and things that I'll be interested

281
00:14:59,465 --> 00:15:03,402
to hear about moving forward would be,
you know, we talked about that

282
00:15:03,402 --> 00:15:07,473
we can use it both for parameter
exploration and state exploration

283
00:15:07,740 --> 00:15:08,974
but when we're in

284
00:15:08,974 --> 00:15:11,844
in the empirical context
of the experiments being performed for,

285
00:15:12,478 --> 00:15:16,382
it might be the case that I'm uncertain
about certain parameters

286
00:15:16,382 --> 00:15:19,151
or I'm uncertain about the state,
and I may be uncertain

287
00:15:19,151 --> 00:15:21,287
about both of those things
simultaneously.

288
00:15:21,287 --> 00:15:24,423
So finding a way to either

289
00:15:24,423 --> 00:15:29,194
design my experiments so that I impose
certainty on either the state

290
00:15:29,194 --> 00:15:34,500
or the parameters is probably seems to be
an important part of using this model.

291
00:15:35,000 --> 00:15:35,701
Mm hmm.

292
00:15:35,868 --> 00:15:37,469
No, no,
that's a great point. Actually, man.

293
00:15:37,469 --> 00:15:41,006
Chris, we're just talking about this
like like yesterday or something

294
00:15:41,307 --> 00:15:44,910
like how to design
an optimal task to disambiguate

295
00:15:44,910 --> 00:15:48,514
but include both having to solve problems
that involves

296
00:15:48,881 --> 00:15:51,150
state
and parameter exploration simultaneously.

297
00:15:52,151 --> 00:15:53,986
Technically, the example, a

298
00:15:53,986 --> 00:15:58,157
model that we have for
the tutorial is able to do that,

299
00:15:59,224 --> 00:16:02,361
but not in an incredibly interesting way,

300
00:16:02,861 --> 00:16:05,831
just in just an kind of way
to show that how you could do it.

301
00:16:06,231 --> 00:16:06,699
Mm hmm.

302
00:16:06,999 --> 00:16:09,168
Very interesting
about the experimental design

303
00:16:09,168 --> 00:16:12,071
because the kind of statistical
techniques like a T test,

304
00:16:12,204 --> 00:16:15,274
we know the statistical power
calculations or we might know

305
00:16:15,274 --> 00:16:19,411
the kinds of experimental designs
that lead to good clean T tests.

306
00:16:19,411 --> 00:16:21,780
But it's a great point
about the experimental design.

307
00:16:22,181 --> 00:16:26,352
And just to highlight this generalizable
and also generative nature

308
00:16:27,119 --> 00:16:28,821
we talked about a lot of different

309
00:16:28,821 --> 00:16:31,156
areas of research,
a lot of different frameworks.

310
00:16:31,156 --> 00:16:34,526
But the parts that stood out to me
were, Christopher, you said decision

311
00:16:34,526 --> 00:16:37,529
making under uncertainty and action
and perception.

312
00:16:37,963 --> 00:16:41,400
That covers a lot,
a lot of different areas of neuroscience.

313
00:16:41,700 --> 00:16:44,870
And then we are looking for
this kind of unified explanation,

314
00:16:45,204 --> 00:16:48,407
but rather than an explanation
which might be philosophical

315
00:16:48,407 --> 00:16:52,044
or based upon how much we understand it
or how we feel that one day

316
00:16:52,344 --> 00:16:56,415
we're rather going to look
for a unified computational architecture

317
00:16:56,415 --> 00:17:00,552
or a computational paradigm,
as you said, that will let us compare

318
00:17:00,552 --> 00:17:04,556
and design and develop these generative
and generalizable models.

319
00:17:04,823 --> 00:17:06,892
So really cool discussion.

320
00:17:07,326 --> 00:17:10,829
And I'm just going to ask one question
from the YouTube live chat.

321
00:17:10,829 --> 00:17:12,698
Then we'll go
to Ryan with a screen share.

322
00:17:13,732 --> 00:17:15,434
KB asks

323
00:17:15,434 --> 00:17:19,104
Can active inference research
help us understand the neuroscience

324
00:17:19,104 --> 00:17:23,242
of stimulus, independent thoughts when
there isn't any sensory input at all?

325
00:17:23,409 --> 00:17:26,712
Thanks for elaboration in this regard.

326
00:17:27,513 --> 00:17:29,281
I mean, I guess,

327
00:17:29,515 --> 00:17:31,984
I mean, the broad answer is yes, but

328
00:17:33,018 --> 00:17:36,955
the way that you would do
it would probably be pretty task specific

329
00:17:37,623 --> 00:17:41,727
And, you know, so
so I'm I'm not necessarily sure

330
00:17:41,727 --> 00:17:43,562
how to give a really precise,
detailed answer

331
00:17:43,562 --> 00:17:46,899
without knowing the particular task
or even just general

332
00:17:46,899 --> 00:17:49,168
psychological context
that's being imagined.

333
00:17:50,235 --> 00:17:53,572
So, I mean, it's actually really easy
to model, for instance, like working

334
00:17:53,572 --> 00:17:56,975
memory tasks where you might get
a stimulus at one point.

335
00:17:56,975 --> 00:17:59,344
But then there's there's no stimulus
for a really long time.

336
00:17:59,711 --> 00:18:03,182
And somehow the thing has to continue
to maintain representations

337
00:18:03,782 --> 00:18:06,018
in the absence of any stimulus
to know what to do

338
00:18:06,318 --> 00:18:08,821
you know, at some
distant future point. And,

339
00:18:09,822 --> 00:18:11,924
you know, there's that kind of thing.

340
00:18:13,192 --> 00:18:15,994
I mean, just as one example,
I again, like I said,

341
00:18:16,895 --> 00:18:19,098
the way to answer
the question is probably is

342
00:18:19,264 --> 00:18:20,499
it would be different depending on

343
00:18:20,499 --> 00:18:23,569
the exact sort of psychological
or task context in question.

344
00:18:24,503 --> 00:18:27,406
So just like explore, exploit
there are tasks that looked

345
00:18:27,406 --> 00:18:32,244
more exploratory or more exploitative
almost within this action of perception.

346
00:18:32,544 --> 00:18:35,114
We're going to be talking about
some tasks that are more towards

347
00:18:35,114 --> 00:18:38,517
the action oriented side, like the policy
planning or the motor behavior

348
00:18:38,650 --> 00:18:40,419
and other tasks that are more towards

349
00:18:40,419 --> 00:18:43,822
almost the perceptive side,
like this visual awareness task.

350
00:18:44,022 --> 00:18:47,793
Now, ocular motor movement, the movement
of the eye muscles is still a behavior.

351
00:18:47,960 --> 00:18:50,596
And there's papers that model that,
but it's almost like

352
00:18:50,762 --> 00:18:54,032
we're looking at the whole spectrum
of explore, exploit or action,

353
00:18:54,032 --> 00:18:57,970
perception, reinforcement
learning and wondering how we can again

354
00:18:57,970 --> 00:19:01,607
make this unified framework for talking
about these different kinds of systems.

355
00:19:01,740 --> 00:19:04,643
So shall we jump right in right
or go ahead at anything else?

356
00:19:04,776 --> 00:19:07,179
Well, I mean, us
I mean, just, just one last point

357
00:19:07,179 --> 00:19:10,816
to bring up in relation to their
you know, this person's question

358
00:19:11,183 --> 00:19:13,986
is just that it's actually
what's really important and probably

359
00:19:13,986 --> 00:19:16,722
a more recent development
and this is stuff that

360
00:19:17,389 --> 00:19:20,092
some of the visual awareness stuff
that me and Chris have done and also

361
00:19:21,093 --> 00:19:23,729
some stuff that I've done previously
with like

362
00:19:23,762 --> 00:19:26,565
emotional awareness, for example,
like levels of emotional awareness.

363
00:19:27,599 --> 00:19:30,302
There's there's a real importance
of what you're doing,

364
00:19:30,636 --> 00:19:35,107
not just behavioral actions,
but actually cognitive actions

365
00:19:36,408 --> 00:19:36,842
you like.

366
00:19:36,842 --> 00:19:40,979
You're selecting policies
that just do something like in the neural

367
00:19:40,979 --> 00:19:44,383
implementation gestures, something like
change the functional connectivity

368
00:19:44,883 --> 00:19:47,819
between two brain regions or change.

369
00:19:48,187 --> 00:19:50,322
In other words,
it is stimulus independent

370
00:19:52,724 --> 00:19:53,225
action.

371
00:19:53,225 --> 00:19:55,561
They're actions that that have to do with

372
00:19:55,561 --> 00:19:58,230
changing what you're doing cognitively
and not what you're doing behaviorally,

373
00:19:59,331 --> 00:20:02,834
you know, so that's also an important
part of it that's been done.

374
00:20:02,834 --> 00:20:06,305
And, you know, a few previous papers
that I think

375
00:20:07,072 --> 00:20:10,642
relates an important way to this kind
of stimulus independence or a question.

376
00:20:10,909 --> 00:20:12,811
But again,
only if I understood it. Right.

377
00:20:12,811 --> 00:20:15,314
Well, good answer. And thanks for that.

378
00:20:15,347 --> 00:20:17,649
So go ahead and
share your screen and just

379
00:20:18,750 --> 00:20:19,685
however we can

380
00:20:19,685 --> 00:20:22,688
help whatever you want us to do, or not
do, we're there.

381
00:20:22,688 --> 00:20:25,324
But I'm really looking forward to this.

382
00:20:25,324 --> 00:20:25,891
Okay.

383
00:20:25,891 --> 00:20:31,496
So the you know, the main
the main thing here that I want to do

384
00:20:31,496 --> 00:20:35,100
just to start, as you know, and this is
in part is because I'm assuming that not

385
00:20:35,367 --> 00:20:39,504
everyone who's watching
today was watching last week

386
00:20:39,972 --> 00:20:42,608
and so I'm not going to be able
to give you

387
00:20:43,442 --> 00:20:46,411
a full understanding
of where we're sort of how we're going

388
00:20:46,411 --> 00:20:47,412
from last week to this week,

389
00:20:47,412 --> 00:20:51,350
but it going to give kind of
like a really sort of rapid

390
00:20:52,217 --> 00:20:54,620
overview of where we're kind of going.

391
00:20:55,687 --> 00:20:57,723
You know, so first we covered this

392
00:20:57,723 --> 00:21:02,561
slide here, which just shows
an example of how you can do

393
00:21:02,794 --> 00:21:05,998
and how you can do perception
as Bayesian inference.

394
00:21:06,365 --> 00:21:09,968
And this is just an example,
a mathematical example of how it works.

395
00:21:10,736 --> 00:21:14,740
But the idea is where you see this
sort of two dimensional image and you're

396
00:21:14,806 --> 00:21:18,543
this gray thing under gray disk, under
observation, and you're trying to infer

397
00:21:19,711 --> 00:21:21,780
based on prior expectations,

398
00:21:22,581 --> 00:21:25,884
whether it's a three dimensional, it's
a concave or conducts shape.

399
00:21:27,019 --> 00:21:31,556
And so the idea is, is that in
simple problems, you can do that exactly.

400
00:21:31,556 --> 00:21:33,091
By solving

401
00:21:33,492 --> 00:21:34,526
Bayes Theorem.

402
00:21:34,526 --> 00:21:37,996
So this equation up here
on the upper left.

403
00:21:37,996 --> 00:21:41,700
But in most cases,
that's actually intractable to do.

404
00:21:42,200 --> 00:21:46,872
And so what you do in act of inference
is you instead

405
00:21:47,506 --> 00:21:51,777
define this thing F here,
which is called expected for energy.

406
00:21:52,978 --> 00:21:56,114
And it's more or less a measure

407
00:21:56,114 --> 00:22:00,352
of the accuracy of predictions
minus the accuracy of beliefs.

408
00:22:01,253 --> 00:22:04,556
And combined
with the complexity of those beliefs.

409
00:22:04,589 --> 00:22:05,490
Essentially how much you have to

410
00:22:05,490 --> 00:22:09,061
change your beliefs to
to get a model that's accurate.

411
00:22:09,528 --> 00:22:12,664
And so by finding a set of beliefs

412
00:22:13,265 --> 00:22:17,102
that minimize expected for energy,
in other words, ones

413
00:22:17,102 --> 00:22:20,305
that are accurate
while changing as little as possible.

414
00:22:21,473 --> 00:22:22,140
This is just an

415
00:22:22,140 --> 00:22:25,377
example of how by doing that, by
just kind of searching around

416
00:22:25,377 --> 00:22:29,948
for different cues here different beliefs
about the probability of states,

417
00:22:30,782 --> 00:22:33,785
then you can find one
that when you calculate

418
00:22:33,785 --> 00:22:37,255
it gives the lowest value for F,
and that's going to be the one

419
00:22:37,255 --> 00:22:40,659
that's as close as possible to the true

420
00:22:41,293 --> 00:22:44,763
answer the base theorem would give you.

421
00:22:44,863 --> 00:22:46,498
And so

422
00:22:47,032 --> 00:22:48,767
so we're

423
00:22:48,967 --> 00:22:49,801
going to skip this one.

424
00:22:49,801 --> 00:22:53,071
But well, so what you're trying to do
then an act of inferences,

425
00:22:53,071 --> 00:22:56,141
you have some true thing
going on out in the world.

426
00:22:56,141 --> 00:22:59,010
True states
and the observations that they generate.

427
00:23:00,212 --> 00:23:03,248
But then the brain has this model
where it's trying to say, okay,

428
00:23:03,548 --> 00:23:08,320
what state must it be
in, given that I got this observation?

429
00:23:08,320 --> 00:23:11,757
So the brain is trying to essentially
come up with the representation

430
00:23:11,757 --> 00:23:15,427
that best matches what's going on
in the generative process.

431
00:23:15,460 --> 00:23:18,230
In other words, the thing that's really
generating the observations,

432
00:23:19,564 --> 00:23:20,699
and so

433
00:23:20,699 --> 00:23:24,436
that's probability of O
and so states and observations

434
00:23:24,770 --> 00:23:28,540
and this pie thing is policies
So it's what's the most likely action

435
00:23:28,540 --> 00:23:31,743
I ought to choose,
given that the world is this way,

436
00:23:31,743 --> 00:23:34,413
and given that
I prefer some observations over others

437
00:23:36,081 --> 00:23:37,916
and what's going to be important

438
00:23:37,916 --> 00:23:40,585
for what I cover today

439
00:23:41,586 --> 00:23:45,457
is the way that you actually set up
these generative models

440
00:23:46,691 --> 00:23:48,760
and the way that these are set up is

441
00:23:49,261 --> 00:23:52,864
you have these states
and you have these observations.

442
00:23:52,898 --> 00:23:57,302
Oh, and,
and then you have prior, initial prior

443
00:23:57,302 --> 00:23:59,638
expectations that we just called D

444
00:24:01,173 --> 00:24:04,743
and if you solve this right,
if you get some observations

445
00:24:05,110 --> 00:24:08,413
and you say, given my generative model,
what states are most likely

446
00:24:08,580 --> 00:24:12,217
given those observations
and given my prior expectations, D,

447
00:24:12,751 --> 00:24:16,388
then you get your sort of optimal belief
about what the

448
00:24:16,555 --> 00:24:18,824
what the states
ought to be given those observations.

449
00:24:19,391 --> 00:24:24,129
And but in act of inference,
we do this in a way that's also dynamic.

450
00:24:24,663 --> 00:24:27,399
And so, for instance, states
that one at a time,

451
00:24:27,399 --> 00:24:31,036
one are going to transition
into states at time, too.

452
00:24:31,503 --> 00:24:35,040
And so you also have beliefs, this thing
and be about

453
00:24:35,407 --> 00:24:38,610
what states are most likely
to transition into what other states.

454
00:24:40,345 --> 00:24:41,713
And then.

455
00:24:42,180 --> 00:24:44,716
And this is kind of one
of the unique moves in act of inference

456
00:24:45,183 --> 00:24:48,753
is that policies
so the different action sequences

457
00:24:48,753 --> 00:24:50,088
you might choose

458
00:24:50,088 --> 00:24:54,326
are just models as things that entail
different transitions between states

459
00:24:54,793 --> 00:24:59,731
and so be Matrix one, for example, here
would entail choosing

460
00:24:59,731 --> 00:25:01,800
to move from one state to another state,

461
00:25:02,133 --> 00:25:04,703
whereas some different policy
would entail a different be a matrix.

462
00:25:04,703 --> 00:25:08,039
That means you transition from one state
to some other state.

463
00:25:09,741 --> 00:25:15,046
And the choice of policy
depends on g here, which is the expected

464
00:25:15,046 --> 00:25:18,917
for energy, which is a function of C,
which is your preferences.

465
00:25:20,018 --> 00:25:23,221
So essentially what you're trying to do
is you're trying to find the policy

466
00:25:23,722 --> 00:25:26,825
which corresponds
to a set of state transitions

467
00:25:27,325 --> 00:25:30,362
that will generate the observations
over time

468
00:25:30,896 --> 00:25:33,231
that are going to be as consistent
as possible

469
00:25:33,231 --> 00:25:37,135
with your preferences,
which corresponds to having the lowest G.

470
00:25:38,803 --> 00:25:40,539
And so and then there's

471
00:25:40,539 --> 00:25:45,243
some other sort of parameters up here
that can say add habits.

472
00:25:45,243 --> 00:25:49,281
So E here can give
you kind of an additional prior policies

473
00:25:49,281 --> 00:25:51,983
that gives you kind of habits
to do one thing over another.

474
00:25:52,517 --> 00:25:54,753
It can have this beta gamma thing

475
00:25:55,320 --> 00:25:59,224
that also controls how precise
or how reliable

476
00:26:00,559 --> 00:26:02,327
the model expects

477
00:26:02,327 --> 00:26:04,396
the expected free energy estimates to be.

478
00:26:05,530 --> 00:26:07,732
So I realize that's a lot,

479
00:26:07,732 --> 00:26:11,603
but we kind of covered
this more slowly last time.

480
00:26:12,070 --> 00:26:14,973
But what you really need to know

481
00:26:14,973 --> 00:26:18,543
when we're actually putting together the
code, like I'm going to walk you through

482
00:26:19,177 --> 00:26:22,280
and this is what we're going
to be specifying step by step.

483
00:26:22,280 --> 00:26:25,050
We're going to be specifying
what the observations are.

484
00:26:26,484 --> 00:26:28,987
We're going to be specifying
the likelihood A.

485
00:26:29,154 --> 00:26:34,626
So in other words, what states generate
what observations with what probability

486
00:26:36,127 --> 00:26:36,394
we're going to

487
00:26:36,394 --> 00:26:40,398
be specifying DMB, which are the priors
of our initial states

488
00:26:40,398 --> 00:26:43,501
and beliefs
about how states are going to transition

489
00:26:45,136 --> 00:26:46,705
and we're going to define policies,

490
00:26:46,705 --> 00:26:49,441
this pie thing, which is going to tell us

491
00:26:50,141 --> 00:26:53,111
the different possible state transitions
we could choose.

492
00:26:54,212 --> 00:26:58,049
And then we're good to finally see here,
which is what the agent's

493
00:26:58,049 --> 00:27:02,120
preferences are, what observations
it wants to get over others

494
00:27:03,922 --> 00:27:08,994
as well as the habits and this beta thing
we expected for energy precision.

495
00:27:09,494 --> 00:27:12,330
And so building a model on the code

496
00:27:12,330 --> 00:27:16,134
amounts to just specifying
what all these things are

497
00:27:16,434 --> 00:27:20,205
and what the state space is,
what the observation space looks like.

498
00:27:20,639 --> 00:27:26,144
What A and B and B look like and what C
looks like, and maybe even better.

499
00:27:27,779 --> 00:27:29,280
And so if you want to know

500
00:27:29,280 --> 00:27:33,451
the exact equations for solving this,
you know, we've shown them here.

501
00:27:33,451 --> 00:27:35,954
But again,
I'm not going to describe them in detail,

502
00:27:36,955 --> 00:27:40,325
but basically all they're doing
as they're saying, given my beliefs

503
00:27:40,325 --> 00:27:44,663
about state transitions and my priors
and given a my likelihood,

504
00:27:45,163 --> 00:27:47,999
what's my most likely posterior
belief going to be,

505
00:27:49,567 --> 00:27:51,836
which is just Bayesian inference
rate priors

506
00:27:51,836 --> 00:27:54,873
and likelihoods together equal posteriors

507
00:27:56,141 --> 00:27:58,276
or approximate to posteriors technically.

508
00:27:59,477 --> 00:28:01,913
But so that's what we're doing.

509
00:28:01,913 --> 00:28:03,048
I just want to.

510
00:28:03,048 --> 00:28:04,783
Make one comment on that slide, right?

511
00:28:04,783 --> 00:28:06,217
Yeah. Yes.

512
00:28:06,217 --> 00:28:09,854
So on the top left,
we have state inference

513
00:28:10,088 --> 00:28:12,424
given priors and given observations.

514
00:28:12,424 --> 00:28:13,825
So that's kind of inference.

515
00:28:13,825 --> 00:28:16,027
And we're going to be building
on these strata

516
00:28:16,127 --> 00:28:20,031
to go from inference, Bayesian inference
to active inference.

517
00:28:20,298 --> 00:28:21,399
So you go from the top left,

518
00:28:21,399 --> 00:28:25,537
which is just perception at one moment
in time that static to the bottom left

519
00:28:25,770 --> 00:28:29,641
where you have states that are updating
through time, given priors

520
00:28:29,641 --> 00:28:32,243
and given observations, you're
updating your estimate through time,

521
00:28:32,711 --> 00:28:34,145
then you go to the top, right?

522
00:28:34,145 --> 00:28:36,481
So that so the bottom left square
adds in time.

523
00:28:36,848 --> 00:28:37,649
Now, on the top right,

524
00:28:37,649 --> 00:28:40,952
we add in this element
of policy selection or control theory.

525
00:28:41,486 --> 00:28:44,723
And then on the bottom right,
we get all the way up to this

526
00:28:44,723 --> 00:28:48,093
for active inference model, which
not just has a control theory element

527
00:28:48,093 --> 00:28:52,497
through policy
but some extra parameters that help us

528
00:28:52,530 --> 00:28:55,333
as we're going to find out soon,
like E and beta.

529
00:28:55,600 --> 00:28:58,570
So just to give that one more time,
because this is what's unique

530
00:28:58,570 --> 00:28:59,838
about active inference,

531
00:28:59,838 --> 00:29:02,607
and this is the skeleton of the model
that everything else

532
00:29:02,607 --> 00:29:04,242
is going to be scaffolded onto.

533
00:29:04,242 --> 00:29:06,077
Thanks. Ryan continued.

534
00:29:06,077 --> 00:29:07,145
Yeah. No, thank you.

535
00:29:07,145 --> 00:29:10,281
I was definitely a good
a quicker, clear summary.

536
00:29:11,382 --> 00:29:13,017
So appreciate it.

537
00:29:13,685 --> 00:29:14,052
Okay.

538
00:29:14,052 --> 00:29:16,521
So and then again, just to remind people,

539
00:29:17,188 --> 00:29:21,226
the end goal of all of this is to learn
how to put together

540
00:29:21,593 --> 00:29:24,596
and particular generative models, right?

541
00:29:24,596 --> 00:29:27,932
Particular sets of matrices, d vectors, B

542
00:29:27,932 --> 00:29:30,635
matrices, C matrices, etc.

543
00:29:31,035 --> 00:29:33,838
and for

544
00:29:33,838 --> 00:29:34,806
empirical.

545
00:29:34,806 --> 00:29:37,675
So for behavioral tasks that

546
00:29:37,675 --> 00:29:39,611
participants can do.

547
00:29:39,611 --> 00:29:43,681
And then you can fit these models
to actual behavior

548
00:29:44,048 --> 00:29:48,219
and to learn things about participants
for instance, you might learn

549
00:29:48,720 --> 00:29:52,090
what their parameters are for A or for B

550
00:29:52,090 --> 00:29:55,059
or for instance,
what their beta value is,

551
00:29:56,094 --> 00:29:57,395
you know, etc.

552
00:29:57,395 --> 00:30:01,399
And so that's the
that's the kind of ultimate goal here,

553
00:30:02,066 --> 00:30:05,270
but the kind of first step to do that
beyond just understanding

554
00:30:05,270 --> 00:30:07,405
the structure I was talking about

555
00:30:07,405 --> 00:30:09,474
is to know how to actually put
that together in the code.

556
00:30:10,608 --> 00:30:11,743
And so the last time I, you know,

557
00:30:11,743 --> 00:30:16,014
I gave a bunch of examples
of past experiments

558
00:30:16,014 --> 00:30:19,284
that we and others have done
where we have built active inference

559
00:30:19,284 --> 00:30:23,221
models for tasks,
set them to data and, and learn things,

560
00:30:23,888 --> 00:30:27,125
for instance, about differences
in substance use

561
00:30:27,492 --> 00:30:30,195
and learning rates,
for example, or differences

562
00:30:30,195 --> 00:30:33,565
in data and decision, uncertainty
in between healthy people

563
00:30:33,565 --> 00:30:36,401
and people with depression
and anxiety or substance use

564
00:30:37,502 --> 00:30:41,206
or differences in perception of precision

565
00:30:41,206 --> 00:30:44,709
of precision estimates
for intercept of perception.

566
00:30:44,709 --> 00:30:46,211
For instance,

567
00:30:46,878 --> 00:30:49,280
So so I just kind of went through
these examples

568
00:30:50,281 --> 00:30:52,450
of specific tasks

569
00:30:52,450 --> 00:30:56,087
and empirical results
just to show the kind of thing

570
00:30:57,222 --> 00:31:00,024
that you can do with this in practice.

571
00:31:00,024 --> 00:31:03,628
The I'm not going to Riga and, but

572
00:31:04,495 --> 00:31:07,966
and then the, the task
that we're modeling is one

573
00:31:07,966 --> 00:31:12,437
that's designed to be really simple,
but that also kind of showcases

574
00:31:13,137 --> 00:31:16,140
the major important elements
of active inference models.

575
00:31:17,342 --> 00:31:18,977
And so here what we're

576
00:31:18,977 --> 00:31:23,047
doing is we have a task
where the agent starts in a start state

577
00:31:23,648 --> 00:31:27,552
and they can either directly choose
to pull the left

578
00:31:27,652 --> 00:31:29,921
or choose the left slot machine
or the right slot machine

579
00:31:30,488 --> 00:31:33,858
and if they do that,
they can either win $4 or $0,

580
00:31:33,858 --> 00:31:36,794
depending on which machine is better,
the left or the right one.

581
00:31:37,362 --> 00:31:39,430
But they don't know ahead of time
which one's better.

582
00:31:39,464 --> 00:31:43,668
So they can either kind of take a risky
guess and either get $4 or zero,

583
00:31:44,269 --> 00:31:48,239
or they can choose to seek information
first to ask for this hint.

584
00:31:49,007 --> 00:31:52,677
And the hint will tell them
which the left one

585
00:31:52,677 --> 00:31:55,213
which of the left, one of the right one
is better on that trial.

586
00:31:56,180 --> 00:31:59,250
And if it's the left better context,
that means the left one

587
00:31:59,250 --> 00:32:01,152
will win 80% of the time.

588
00:32:01,152 --> 00:32:02,020
And if the hand says

589
00:32:02,020 --> 00:32:06,090
the right ones better, then the right one
will pay out 80% of the time.

590
00:32:06,758 --> 00:32:11,462
But there's a cost to taking the hint,
which is that if you take the hint

591
00:32:11,462 --> 00:32:14,799
and then choose a machine,
then you can only win $2 instead of four.

592
00:32:15,633 --> 00:32:19,270
And so that's what
we're going to be modeling.

593
00:32:20,204 --> 00:32:23,041
And so what we need to specify for
that is

594
00:32:23,041 --> 00:32:27,011
we need to specify that
there are two types of hidden states.

595
00:32:27,045 --> 00:32:30,181
One is whether it's the left better
or the right better

596
00:32:30,715 --> 00:32:33,151
contacts, whether it's a left better
or a better trial,

597
00:32:34,052 --> 00:32:36,354
and also the behavior states.

598
00:32:36,387 --> 00:32:40,825
So choosing to move from the start
state to the hint state or choosing

599
00:32:40,825 --> 00:32:45,063
to move from the start state directly
to the left or right slot machine,

600
00:32:47,098 --> 00:32:49,667
So that's two types of states.

601
00:32:49,667 --> 00:32:54,038
We have to specify the observations
where in this case

602
00:32:54,572 --> 00:32:58,643
the major observations
are winning the $4, $2 or $0.

603
00:32:59,043 --> 00:33:01,779
And or in this case,
we're actually just going to model.

604
00:33:01,779 --> 00:33:02,880
You win or you don't win.

605
00:33:02,880 --> 00:33:05,350
And I'll show you how that works.

606
00:33:05,416 --> 00:33:11,155
And and we'll have to define
the agent's preferences.

607
00:33:11,155 --> 00:33:14,525
So essentially saying it prefers $4,
more than $2 and $2

608
00:33:14,525 --> 00:33:17,161
more than $0
and that's going to be that sea thing.

609
00:33:17,729 --> 00:33:22,333
And so so that's kind of the general task
structure.

610
00:33:22,800 --> 00:33:24,836
And I went through

611
00:33:26,270 --> 00:33:29,907
you know, the way to do this
kind of simply outside of the code,

612
00:33:29,907 --> 00:33:32,910
which is you define
your initial state priors

613
00:33:33,277 --> 00:33:35,880
and you define your state outcome mapping
how you define

614
00:33:35,880 --> 00:33:38,516
your preference out
preferences of our outcomes.

615
00:33:39,450 --> 00:33:43,521
You define the state transitions,
such actions and so be

616
00:33:43,521 --> 00:33:47,558
and then you define the policies
which you define as this letter V.

617
00:33:48,826 --> 00:33:49,160
And then

618
00:33:49,160 --> 00:33:52,830
the final thing I'll talk about before
going into the code is that

619
00:33:53,164 --> 00:33:57,769
and in this model, as you can
separate out the generative process,

620
00:33:57,969 --> 00:34:00,605
so it's actually out in the world
generating observations

621
00:34:01,039 --> 00:34:04,008
from the generative model,
which is the agent's beliefs

622
00:34:04,375 --> 00:34:06,778
about what's generating the observations.

623
00:34:07,712 --> 00:34:11,082
And if we want to separate
those in the code, then what we do is

624
00:34:11,482 --> 00:34:14,719
big D or big A or big B,

625
00:34:14,852 --> 00:34:17,155
and those are in the generative process.

626
00:34:18,089 --> 00:34:20,258
But if you also specify

627
00:34:20,258 --> 00:34:23,361
a little D or a little A or a little B,

628
00:34:23,761 --> 00:34:27,331
then that's the generative model,
that's the agent's beliefs.

629
00:34:27,665 --> 00:34:31,469
And if you specify those, then they will
be separate from the generative process.

630
00:34:31,969 --> 00:34:33,604
And that's typically what you need to do.

631
00:34:33,604 --> 00:34:34,672
That is what you need to do.

632
00:34:34,672 --> 00:34:37,108
If you want a model learning,
which I'll show.

633
00:34:37,742 --> 00:34:40,311
Right. Just just to two quick points.

634
00:34:40,311 --> 00:34:44,048
So first off, someone has asked, will
these slides be available

635
00:34:44,482 --> 00:34:46,651
or is there any resource where we can
look at these slides?

636
00:34:46,651 --> 00:34:48,386
Because there's a lot
of great information on them.

637
00:34:50,221 --> 00:34:51,522
I can make them available.

638
00:34:51,522 --> 00:34:55,960
Everything, everything here is I mean,
almost all the figures that I'm showing,

639
00:34:56,861 --> 00:34:59,831
other than the actual task
figure here are in the

640
00:34:59,831 --> 00:35:02,366
R in the tutorial paper and

641
00:35:03,601 --> 00:35:06,003
everything
I'm showing about how to actually build

642
00:35:06,003 --> 00:35:09,140
the models is in the code and well,
explain in the code like I'll show,

643
00:35:09,807 --> 00:35:14,078
but I'm also happy to provide better
this slides.

644
00:35:14,078 --> 00:35:16,114
I mean, just, just let me know.

645
00:35:16,114 --> 00:35:16,881
Like I said, the vast

646
00:35:16,881 --> 00:35:20,151
majority of them are literally
just the figures in the paper.

647
00:35:20,151 --> 00:35:22,553
So, so yeah, I mean, like I said, I'm

648
00:35:22,587 --> 00:35:26,357
happy to make them available
or send them to anyone.

649
00:35:26,357 --> 00:35:30,261
I can even just put them a supplementary
additional supplementary material

650
00:35:30,261 --> 00:35:31,562
at the tutorial as well.

651
00:35:31,562 --> 00:35:34,165
So I mean, yeah, just whatever,
whatever people want.

652
00:35:34,432 --> 00:35:35,466
Great idea.

653
00:35:35,466 --> 00:35:39,604
And what a fun and accessible science
we can have

654
00:35:39,604 --> 00:35:42,573
when we just receive questions like that
and can put up our slides.

655
00:35:42,840 --> 00:35:44,709
And then I think, Max,
there was a question

656
00:35:44,709 --> 00:35:48,312
that you had about the B matrix
just while we're defining our terms.

657
00:35:48,646 --> 00:35:50,014
What would you like to say about that?

658
00:35:51,015 --> 00:35:51,649
All right.

659
00:35:51,649 --> 00:35:54,585
So but just real quick, for anybody
who's just jumping in

660
00:35:54,585 --> 00:35:57,755
on the stream,
the link is, is below the video.

661
00:35:57,755 --> 00:35:59,757
You can go to their paper
if you're looking for that

662
00:35:59,757 --> 00:36:00,725
supplementary code,

663
00:36:00,725 --> 00:36:03,628
it's in the supplementary materials
you can download all of the code

664
00:36:03,895 --> 00:36:06,430
from there, supplementary materials
there.

665
00:36:06,430 --> 00:36:10,268
The question
I had was one thing to touch on

666
00:36:10,268 --> 00:36:12,403
is as we walk through the code,
one thing I noticed

667
00:36:12,403 --> 00:36:16,007
is that we have kind of a ten story
all structure for these matrices

668
00:36:17,175 --> 00:36:20,244
where we could also specify
a transition matrix

669
00:36:20,511 --> 00:36:23,014
that was that
had like a 2D structure with

670
00:36:24,048 --> 00:36:26,951
width with probability of 0.8, 0.2.

671
00:36:26,951 --> 00:36:30,254
But instead we have a ten story structure
where we have a fixed probability

672
00:36:30,254 --> 00:36:34,659
of one for transitioning
from any given state to another state.

673
00:36:34,959 --> 00:36:36,894
And I just thought

674
00:36:36,894 --> 00:36:39,597
if you wanted to touch on
maybe the motivation for that,

675
00:36:39,597 --> 00:36:42,967
I believe it's related to how the Model
Y basically interpreting

676
00:36:42,967 --> 00:36:46,170
the model and, and,
and how the model gets used

677
00:36:47,271 --> 00:36:48,105
is that

678
00:36:49,407 --> 00:36:51,409
I didn't know
if you wanted to elaborate on that any.

679
00:36:51,409 --> 00:36:53,344
Yeah. I mean, I mean, yes.

680
00:36:53,344 --> 00:36:54,478
I mean, so technically, you know,

681
00:36:54,478 --> 00:36:57,982
we call these things like we call
like a matrix and the B matrix matrices,

682
00:36:57,982 --> 00:37:01,719
but technically they're sensors as though
they're high dimensional matrices.

683
00:37:02,720 --> 00:37:05,356
And so, so

684
00:37:05,356 --> 00:37:08,759
and you know, obviously go through this
when we go through the code, but

685
00:37:09,160 --> 00:37:13,331
because you can have different types
of states, right?

686
00:37:13,331 --> 00:37:17,101
So the state of being in the left better
or the right better context

687
00:37:18,703 --> 00:37:20,104
versus the type of state

688
00:37:20,104 --> 00:37:24,175
that has to do with, you know, where
you move to or what you choose, right.

689
00:37:24,175 --> 00:37:27,311
That being in a star state versus
the hidden state versus,

690
00:37:27,411 --> 00:37:31,549
you know, the left or right
bandit or slot machine choice state.

691
00:37:31,983 --> 00:37:33,451
And because you have

692
00:37:33,451 --> 00:37:36,120
two different types of states,
those are called state factors.

693
00:37:37,321 --> 00:37:39,357
And so essentially what

694
00:37:39,357 --> 00:37:43,361
you need to do
is you need to specify these matrices

695
00:37:43,361 --> 00:37:46,697
or I guess a really tensor is that again,
this is for a

696
00:37:47,131 --> 00:37:49,800
that say

697
00:37:49,800 --> 00:37:53,170
what outcomes for each
type of outcome will be generated

698
00:37:53,437 --> 00:37:56,641
for each combination of each value
for each state factor,

699
00:37:57,708 --> 00:38:00,711
you know, which makes which makes it

700
00:38:00,711 --> 00:38:03,714
you know, higher dimensional,
which is probably the trickiest part.

701
00:38:03,714 --> 00:38:07,852
I'll explain this as well as I as
I can when we go through the code.

702
00:38:08,953 --> 00:38:12,690
But so but so that's that's that
and also so simultaneously

703
00:38:12,690 --> 00:38:17,194
because you have two types of states
here, that also means

704
00:38:17,194 --> 00:38:21,365
you have to have a B matrix,
a transition matrix for each

705
00:38:22,767 --> 00:38:24,568
type of state.

706
00:38:25,202 --> 00:38:28,472
And when you when the agent doesn't

707
00:38:28,472 --> 00:38:31,976
have control about state,
about transitions for a certain state,

708
00:38:32,943 --> 00:38:33,811
then there's only going

709
00:38:33,811 --> 00:38:37,782
to be one of these B matrices

710
00:38:39,450 --> 00:38:42,453
So for example, here,
so this is just says

711
00:38:42,453 --> 00:38:45,756
B four factor one,
which is what context you're in.

712
00:38:46,991 --> 00:38:50,561
This is just a simple 2D matrix,

713
00:38:51,362 --> 00:38:54,832
although technically for reasons
in the code, you define it as a

714
00:38:55,433 --> 00:38:59,370
as a three dimensional thing with just a
one here this is just but that's just

715
00:38:59,370 --> 00:39:04,108
because the third dimension here defines
the the number label for the action.

716
00:39:05,109 --> 00:39:07,745
So this is basically
you're saying there's only one action

717
00:39:08,346 --> 00:39:12,083
for state factor one,
and that is an identity

718
00:39:12,083 --> 00:39:15,119
matrix here,
which just means that for each trial

719
00:39:15,586 --> 00:39:18,989
it's going to be the left better context
through the whole trial.

720
00:39:19,023 --> 00:39:19,590
Right.

721
00:39:19,590 --> 00:39:22,426
In other words, within a trial,
it's not going to like after they take

722
00:39:22,426 --> 00:39:25,229
the hint, it's not going to like
switch on them, you know.

723
00:39:25,262 --> 00:39:26,731
So really the right one is better

724
00:39:26,731 --> 00:39:28,899
if they got the hint
that the last one was better.

725
00:39:28,899 --> 00:39:30,701
This is just saying
if you start in the left, better

726
00:39:30,701 --> 00:39:32,970
one, you're going to stay in the left,
better one, the whole trial,

727
00:39:33,804 --> 00:39:34,305
and that's it.

728
00:39:36,073 --> 00:39:38,275
Whereas for the

729
00:39:38,275 --> 00:39:41,912
stay factor two,
so the B matrices for state factor two,

730
00:39:42,813 --> 00:39:47,818
there are four of these which correspond
to four different possible actions

731
00:39:48,319 --> 00:39:51,789
and so here the third dimension
says this is action one,

732
00:39:52,289 --> 00:39:55,593
which means that from any state
which are the columns,

733
00:39:55,960 --> 00:39:59,497
you can move to state one,
which is the first row,

734
00:40:00,564 --> 00:40:03,901
whereas the second one says,
if I pick action two

735
00:40:03,901 --> 00:40:08,305
so third dimension value to that,
I can move from any of the four states

736
00:40:08,305 --> 00:40:12,643
again columns to state to
which is the hand, the state

737
00:40:13,944 --> 00:40:15,780
and so forth for action three action

738
00:40:15,780 --> 00:40:18,816
for moving to choose the left band it

739
00:40:19,550 --> 00:40:22,553
or the left machine
or choosing the right machine.

740
00:40:23,120 --> 00:40:23,621
So these are going to be

741
00:40:23,621 --> 00:40:26,757
the four actions, which are just
the four possible transitions

742
00:40:28,359 --> 00:40:29,460
and then when you

743
00:40:29,460 --> 00:40:32,296
specify policies, this thing in V,

744
00:40:33,330 --> 00:40:35,533
this is just saying
for each state factor,

745
00:40:36,000 --> 00:40:38,436
which of those actions
can I string together?

746
00:40:40,237 --> 00:40:43,574
So the for one here for state

747
00:40:43,574 --> 00:40:47,411
factor one is just a bunch of ones
because again, there's only one action.

748
00:40:47,411 --> 00:40:49,747
It just stays in
whatever state it started in.

749
00:40:51,282 --> 00:40:54,018
And each column here is a policy
in each row

750
00:40:54,018 --> 00:40:57,021
as a time, as an action time
point basically.

751
00:40:57,021 --> 00:41:00,825
So this is saying so the first row
here is what action could I take

752
00:41:01,258 --> 00:41:05,830
when moving from time to time to
and the third row in the second row

753
00:41:05,830 --> 00:41:10,000
here is what action could I take when
moving from time to time three and again

754
00:41:10,634 --> 00:41:13,838
each of these columns just says
stand ones because there is no

755
00:41:14,405 --> 00:41:18,642
possible action for changing the context.

756
00:41:19,777 --> 00:41:22,913
But policy the policy for

757
00:41:23,280 --> 00:41:26,083
space for state or for factor two here

758
00:41:26,650 --> 00:41:32,223
so again the the actions and there's
these five columns and this just says

759
00:41:32,590 --> 00:41:35,860
I can move to the start state
and I can stay in the start state.

760
00:41:35,860 --> 00:41:36,093
Right?

761
00:41:36,093 --> 00:41:38,462
So that's just action one up here twice.

762
00:41:39,063 --> 00:41:42,867
And then the second column here
says I could take action to.

763
00:41:42,900 --> 00:41:46,070
So look at the hint and then yeah,

764
00:41:47,705 --> 00:41:52,877
then move to action three,
which is choose the left machine.

765
00:41:53,310 --> 00:41:55,012
Or I could take the hint to

766
00:41:55,012 --> 00:41:57,515
and then choose
for which is moved to the right machine.

767
00:41:58,048 --> 00:42:01,919
Or I can go straight to the right machine
and then go back to start.

768
00:42:02,419 --> 00:42:06,290
Or I can go straight to the arrow,
straight to the three as the left machine

769
00:42:06,290 --> 00:42:09,426
or straight to the right machine
action four and then go back to start.

770
00:42:10,127 --> 00:42:13,430
So so in other words, you know, there's
a bunch of other possible transitions,

771
00:42:13,430 --> 00:42:15,499
right strings
that you might think of, right?

772
00:42:15,499 --> 00:42:16,200
You might,

773
00:42:16,233 --> 00:42:19,637
for instance, go to the left machine
and then go take the hint or something

774
00:42:19,637 --> 00:42:20,070
like that.

775
00:42:20,070 --> 00:42:21,038
But we're not allowing that.

776
00:42:21,038 --> 00:42:23,674
We're saying
these are the allowable transitions

777
00:42:24,141 --> 00:42:28,412
so these are the different policies
or action sequences that can be chosen.

778
00:42:28,746 --> 00:42:30,681
So correct me if I'm wrong,
but it seems that this

779
00:42:30,681 --> 00:42:32,917
sensorial structure
kind of allows us to compress

780
00:42:32,917 --> 00:42:37,087
all those different combinatorial
combinations that we might have.

781
00:42:37,087 --> 00:42:40,291
So it allows the structure of basically
just allows the code

782
00:42:40,291 --> 00:42:42,760
to be a little more efficient.
Is that at the end of the day?

783
00:42:42,960 --> 00:42:44,762
Yeah. That's
a perfectly reasonable way to talk about.

784
00:42:46,363 --> 00:42:48,198
So, so, okay.

785
00:42:48,198 --> 00:42:51,435
So, so that already gives
you kind of a little bit of a start here.

786
00:42:52,436 --> 00:42:54,104
So now

787
00:42:55,773 --> 00:42:58,442
and then and then in
so the end of the day, what

788
00:42:58,442 --> 00:43:01,579
we're going to be doing as we're going
to be specifying each of these things.

789
00:43:01,579 --> 00:43:05,182
So t the amount of time steps v
the policy space, a

790
00:43:05,583 --> 00:43:09,587
the likelihood the state outcome
mapping be the transition probabilities

791
00:43:09,587 --> 00:43:12,756
or transition matrices, see,
which is the preferred

792
00:43:13,757 --> 00:43:16,360
observation that says preferred
status here but should be preferred

793
00:43:16,360 --> 00:43:20,497
observations D It would be the priors
of our initial states

794
00:43:21,966 --> 00:43:24,902
and then all these other extra
little parameters that you can

795
00:43:24,902 --> 00:43:27,304
or cannot include
And I'll talk about this later,

796
00:43:28,472 --> 00:43:30,574
but then we also specify this little D

797
00:43:30,574 --> 00:43:33,777
which basically says the thing doesn't
start out knowing the context.

798
00:43:34,178 --> 00:43:37,815
So that's the learn the context over
a trial has to sort of build up beliefs

799
00:43:37,815 --> 00:43:39,817
about D over trials.

800
00:43:39,817 --> 00:43:43,287
And so with that
as a kind of quick background,

801
00:43:44,722 --> 00:43:45,122
I'm going to

802
00:43:45,122 --> 00:43:48,192
now kind of move into the code.

803
00:43:48,192 --> 00:43:50,027
It's cool that the whole thing is stored

804
00:43:50,027 --> 00:43:54,098
as one object with a bunch of fields
because it's almost like

805
00:43:54,098 --> 00:43:56,867
you could have ensembles
of these different models

806
00:43:56,867 --> 00:43:59,103
or you might have a bunch of ants
in a colony.

807
00:43:59,236 --> 00:44:02,873
It really lends itself
to being reproducible and transparent

808
00:44:02,873 --> 00:44:06,510
with what each parameter is and comparing
a whole host of them alongside

809
00:44:06,510 --> 00:44:07,845
or different kinds of models.

810
00:44:07,845 --> 00:44:10,948
Just really cool to see that
instead of just a tangle of parameters

811
00:44:11,582 --> 00:44:12,650
that you can get lost in.

812
00:44:12,650 --> 00:44:17,187
This is like a column of things
that each has a very clear purpose.

813
00:44:18,756 --> 00:44:19,323
Yeah.

814
00:44:19,323 --> 00:44:21,959
So I should say to you, you do yeah.

815
00:44:22,192 --> 00:44:24,061
Put all of these things
into the structure

816
00:44:24,061 --> 00:44:26,130
that would usually result
little and deep,

817
00:44:26,130 --> 00:44:28,098
which stands for Markov Decision Process.

818
00:44:29,566 --> 00:44:31,568
And then when you actually run the thing

819
00:44:31,568 --> 00:44:35,572
then you run it through the, the VB,

820
00:44:35,572 --> 00:44:39,276
the SPM, MVP, VB, underscore X code,

821
00:44:40,077 --> 00:44:43,313
and we've provided a version that just
appends it with the word tutorial

822
00:44:44,281 --> 00:44:46,250
and you make that big MVP.

823
00:44:46,250 --> 00:44:50,654
And then you can take is subsequent
commands like this that just m

824
00:44:51,055 --> 00:44:55,059
generate figures that will show you
the outcomes of the simulation.

825
00:44:56,360 --> 00:44:59,496
And these are examples of simulation
results.

826
00:44:59,496 --> 00:45:03,400
But again, I'll go into these
after we go through some of the code.

827
00:45:03,400 --> 00:45:04,535
And because

828
00:45:04,535 --> 00:45:06,670
I know how to actually use this stuff,
you need to understand the code.

829
00:45:07,271 --> 00:45:12,042
And so hopefully
I can make this not too monotonous.

830
00:45:12,042 --> 00:45:15,145
You know, walking through code
isn't always the most exciting, but

831
00:45:16,313 --> 00:45:17,981
the way that we have this set up

832
00:45:19,116 --> 00:45:20,918
is you
know, like I said, we have comments

833
00:45:20,918 --> 00:45:22,986
that kind of try our best to explain
whatever

834
00:45:22,986 --> 00:45:26,223
that little command means, you know,
like what clear and close all mean what

835
00:45:26,857 --> 00:45:29,059
our entry default means, you know, etc.

836
00:45:30,127 --> 00:45:32,429
And I'm actually going to change this
to show

837
00:45:32,996 --> 00:45:35,632
what that means is just that

838
00:45:35,632 --> 00:45:38,736
the MATLAB
will use a random number generator

839
00:45:38,736 --> 00:45:42,539
that won't do the exact same thing
each time

840
00:45:42,539 --> 00:45:45,175
so we can get variable results
when we run it.

841
00:45:47,144 --> 00:45:50,180
So we've provided

842
00:45:50,180 --> 00:45:53,217
several different simulation
options on the we can use that

843
00:45:53,217 --> 00:45:55,652
for building the model below,
which is what I walk through.

844
00:45:56,086 --> 00:46:00,390
So if you set the SIM
variable equal to one, then

845
00:46:00,390 --> 00:46:04,528
that will just simulate a single trial,
which will Rupert it reproduce?

846
00:46:04,995 --> 00:46:08,098
Figure seven and in in the paper

847
00:46:08,799 --> 00:46:11,201
that just be a single trial simulation.

848
00:46:11,201 --> 00:46:14,772
If you set sim equal to the model,
simulate multiple trials

849
00:46:15,272 --> 00:46:17,608
where the left context
is always active, but

850
00:46:18,475 --> 00:46:20,811
the agent will know
that the agent has to learn.

851
00:46:20,811 --> 00:46:23,480
Okay, the
the left one is the one it always is.

852
00:46:23,781 --> 00:46:27,785
So after a while it'll be confident
and won't need to take the hint anymore.

853
00:46:29,253 --> 00:46:30,754
If I could just briefly interject there.

854
00:46:30,754 --> 00:46:34,958
So that would be for example, an example
of what we talked about earlier

855
00:46:34,958 --> 00:46:37,995
is parameter exploration yes.

856
00:46:39,363 --> 00:46:41,098
Well, well, not exactly.

857
00:46:41,098 --> 00:46:44,668
It would be an example of learning
the parameters over successive

858
00:46:46,470 --> 00:46:47,638
yeah.

859
00:46:47,638 --> 00:46:50,440
So state I mean state exploration
is a bigger thing here, right?

860
00:46:50,474 --> 00:46:51,175
Because, you know,

861
00:46:51,175 --> 00:46:56,146
they choose to move to the queue
or not to learn which context it is yeah.

862
00:46:56,180 --> 00:46:58,182
Yeah, I agree that.

863
00:46:58,182 --> 00:47:02,219
So yeah, the tricky thing is,
is that when you only have two options,

864
00:47:02,686 --> 00:47:05,923
then parameter exploration
would just amount to

865
00:47:06,390 --> 00:47:11,161
even though I've gotten some good results
from choosing the the left one,

866
00:47:12,196 --> 00:47:14,798
I might also instead
try to choose the right one a few times

867
00:47:14,798 --> 00:47:18,802
to be more confident in what the reward
probability is for the right one.

868
00:47:20,070 --> 00:47:23,073
But typically to get good dynamics

869
00:47:23,073 --> 00:47:25,943
like that, you're probably going to add
something that has

870
00:47:27,010 --> 00:47:30,614
has more like three arms, like three
different options instead of two.

871
00:47:31,682 --> 00:47:33,617
If you're also going to include the state

872
00:47:33,617 --> 00:47:36,687
exploration thing with the queue

873
00:47:36,753 --> 00:47:38,222
it would get a little more complicated.

874
00:47:38,222 --> 00:47:41,124
Like a standard kind of paradigm
for doing parameter exploration

875
00:47:41,124 --> 00:47:43,794
would be something
where you don't have the queue

876
00:47:44,127 --> 00:47:49,166
and the agent just has like three choices
and it just kind of has to choose

877
00:47:49,566 --> 00:47:53,337
different ones over time until it becomes
confident which one is the best.

878
00:47:53,337 --> 00:47:56,373
And then it'll just
keep choosing that one.

879
00:47:56,373 --> 00:47:58,909
So the parameter exploration
would amount to

880
00:47:59,276 --> 00:48:02,012
how many times kind of it it chooses each

881
00:48:02,312 --> 00:48:05,115
slot machine initially

882
00:48:05,115 --> 00:48:07,684
just to figure out which one is best

883
00:48:08,986 --> 00:48:11,955
and standard reinforcement learning agent

884
00:48:11,955 --> 00:48:14,224
will do that differently
than an act of inference agent.

885
00:48:15,158 --> 00:48:17,394
And the substance use study I mentioned

886
00:48:17,394 --> 00:48:20,397
above actually specifically did that.

887
00:48:20,464 --> 00:48:21,632
But but yeah.

888
00:48:21,632 --> 00:48:25,903
So just one question here in this SIM
equals one, two, three, four, five.

889
00:48:26,270 --> 00:48:30,741
They're like scenarios that you're going
to be able to rapidly plug and play.

890
00:48:30,741 --> 00:48:35,012
So scenario one is like a static
or scenario two is multi trial.

891
00:48:35,012 --> 00:48:38,882
So this is what is going to allow
reproducible construction of the figures

892
00:48:39,182 --> 00:48:42,452
and understanding of how different
parameter changes influence behavior.

893
00:48:42,452 --> 00:48:43,787
But that's what
we're laying out right now

894
00:48:43,787 --> 00:48:46,156
is kind of like the scenarios
for the agent, right?

895
00:48:46,890 --> 00:48:50,294
Yeah, I know about quick is one thing
I think since I have to take about a half

896
00:48:50,294 --> 00:48:52,229
now to run it.

897
00:48:52,229 --> 00:48:52,796
Yeah.

898
00:48:52,796 --> 00:48:54,698
Don't you SIM five we'll talk about

899
00:48:54,698 --> 00:48:57,534
with the one we talk about parameter setting, which I'll be a different session,

900
00:48:58,535 --> 00:49:00,704
but mainly we'll be talking about
SIM one,

901
00:49:00,704 --> 00:49:03,540
two and three here, which just has to do
with perception and learning

902
00:49:04,041 --> 00:49:07,778
as opposed to the real empirical stuff
which is estimating parameter

903
00:49:08,912 --> 00:49:14,284
and using them for group level analyzes
and things like that.

904
00:49:14,351 --> 00:49:18,255
Well, just so helpful though,
how it's laid out from the most static,

905
00:49:18,255 --> 00:49:22,592
straightforward, perceptive inference
to action in the loop

906
00:49:22,826 --> 00:49:25,929
and then now that actions in the loop,
we're going to think about the simplest

907
00:49:25,929 --> 00:49:29,766
setting to explore action in the loop
on a single trial, unchanging.

908
00:49:29,933 --> 00:49:31,034
And then we're going to keep on adding

909
00:49:31,034 --> 00:49:34,838
layers of ecological variability
or other features into the model.

910
00:49:34,838 --> 00:49:35,939
So thanks for making it

911
00:49:35,939 --> 00:49:38,575
so step wise, and that's
why it's a step by step tutorial.

912
00:49:40,344 --> 00:49:41,611
Yeah, yeah. We tried

913
00:49:42,779 --> 00:49:45,182
but that's the last thing for for today.

914
00:49:45,182 --> 00:49:48,385
So if you set SIM equal to three, then

915
00:49:48,785 --> 00:49:51,788
that will simulate
a reversal learning situation.

916
00:49:52,289 --> 00:49:52,622
And that's

917
00:49:52,622 --> 00:49:56,426
where basically the left context
is active for a certain number of trials.

918
00:49:56,760 --> 00:49:57,995
And so the agent should build up

919
00:49:57,995 --> 00:50:00,864
a prior expectation
that it's just the left one every time.

920
00:50:01,264 --> 00:50:03,934
So it'll stop taking the hint
and just start using left.

921
00:50:04,801 --> 00:50:06,236
But then it switches.

922
00:50:06,236 --> 00:50:09,973
It's in the right context
as the right one is the correct one.

923
00:50:09,973 --> 00:50:12,676
And you see basically how long it takes

924
00:50:13,143 --> 00:50:16,513
before the agent again says, Okay, now
I need to start taking the hint again.

925
00:50:16,913 --> 00:50:19,950
And then eventually when it becomes
confident to just choosing the right one,

926
00:50:20,283 --> 00:50:23,520
you know, directly
and not needing to take the hint.

927
00:50:24,021 --> 00:50:27,691
And so it's kind of like a standard
reversal learning kind of task.

928
00:50:28,258 --> 00:50:31,461
And so those are the three
that hopefully will get through today.

929
00:50:33,430 --> 00:50:34,664
And there

930
00:50:34,664 --> 00:50:37,834
are several parameters
that can be kind of messed with.

931
00:50:38,568 --> 00:50:42,773
But the one that I've focused on mainly
here at least we can set at the beginning

932
00:50:43,206 --> 00:50:47,911
is this our one thing
which just turns into hours later down,

933
00:50:48,612 --> 00:50:51,581
but lessons for risk seeking,
or it could also be reward seeking,

934
00:50:52,716 --> 00:50:55,786
but more or less, the bigger
this value is, the stronger

935
00:50:55,786 --> 00:50:58,555
the agent's preference is for

936
00:50:59,823 --> 00:51:02,092
winning as much money as possible.

937
00:51:02,092 --> 00:51:05,162
So it's this is the value
that's in the C matrix

938
00:51:05,162 --> 00:51:08,131
that defines preferences over
winning versus losing.

939
00:51:08,698 --> 00:51:11,001
And so the bigger this number, they

940
00:51:12,069 --> 00:51:14,805
the more the agent is going to say, okay,

941
00:51:14,971 --> 00:51:18,175
I care more about reward
than about information, and therefore I'm

942
00:51:18,175 --> 00:51:21,945
just going to take the risky choice to go
for one of the slot machines directly.

943
00:51:22,479 --> 00:51:25,949
So the higher this less information
seeking, the more the lower

944
00:51:25,949 --> 00:51:29,186
this is, the more the more information
seeking the agent will be.

945
00:51:31,655 --> 00:51:32,522
And so that's

946
00:51:32,522 --> 00:51:35,859
just kind of a really simple one
to show how by tweaking parameters,

947
00:51:35,859 --> 00:51:40,497
you can get different results
here, just allowing you to wait

948
00:51:40,831 --> 00:51:45,202
and how strong the reward value
component is in the expected

949
00:51:45,202 --> 00:51:50,040
free energy versus how strong
the weight is of the information

950
00:51:50,040 --> 00:51:52,175
seeking
component of the expected free energy

951
00:51:54,344 --> 00:51:55,378
and then

952
00:51:55,879 --> 00:51:59,082
kind of ignore this tab thing down here.

953
00:51:59,082 --> 00:52:02,285
This is basically you can turn this on
if when you do SIM equals five,

954
00:52:02,986 --> 00:52:07,424
you want to actually do group level
Bayesian analyzes on the parameters

955
00:52:07,991 --> 00:52:11,461
and this will also save
the outputs of SIM.

956
00:52:11,628 --> 00:52:14,431
Like Kris said, SIM
five like takes like a half hour run.

957
00:52:15,398 --> 00:52:20,370
So if you turn this thing on, it'll also
save the results of SIM five so

958
00:52:20,370 --> 00:52:24,207
you can run this PEB thing without having
to redo some five over time.

959
00:52:25,275 --> 00:52:27,644
So we just figured hopefully that would
help.

960
00:52:27,644 --> 00:52:29,913
I just, I just want to jump in
and point out that it's really nice

961
00:52:29,913 --> 00:52:33,617
in this part where, for example,
with our cell set there, now let's say

962
00:52:33,617 --> 00:52:36,620
that I'm designing my experiment
and I need to know how many trials and

963
00:52:36,620 --> 00:52:40,590
how many subjects do I need
when I'm doing my power analysis.

964
00:52:40,790 --> 00:52:44,928
I can run a simulation like this, set
that parameter at what I think might be

965
00:52:44,928 --> 00:52:47,297
a reasonable range of values
from the literature

966
00:52:47,564 --> 00:52:48,598
and then I can say,

967
00:52:48,598 --> 00:52:51,067
well, this is the expected difference
in my groups

968
00:52:51,067 --> 00:52:53,103
based on what I think
this range of values might be.

969
00:52:53,103 --> 00:52:54,771
So it's a really powerful tool

970
00:52:54,771 --> 00:52:57,140
to be able to plug
and play a single parameter like that.

971
00:52:58,441 --> 00:52:59,342
Yeah, no, it's nice.

972
00:52:59,342 --> 00:53:02,212
I mean, typically with experimental,
you know, with experiments

973
00:53:02,212 --> 00:53:04,781
where you do want to do first
as you want to kind of say, hey,

974
00:53:05,415 --> 00:53:08,018
can tweaking different parameters

975
00:53:08,018 --> 00:53:12,222
in the model produce
simulated behavior that looks a lot

976
00:53:12,222 --> 00:53:15,892
like the type of behavior
actually see in real participants

977
00:53:17,360 --> 00:53:19,129
you know? So yeah. So exactly.

978
00:53:19,129 --> 00:53:21,598
If you're hoping for as hey,
like there's a chunk of people

979
00:53:21,932 --> 00:53:25,769
that look like that act
as though they have a high risk seeking

980
00:53:25,769 --> 00:53:30,273
value and another group of people, though
they have a lower risk seeking value

981
00:53:30,273 --> 00:53:34,177
based on their behavior,
you can fit the model to that explicitly.

982
00:53:34,177 --> 00:53:38,048
And then you can say,
hey, is the difference in the value

983
00:53:38,048 --> 00:53:41,084
between these two groups of people,
does that predict anything interesting?

984
00:53:41,484 --> 00:53:43,620
You know,
does that tell you something about,

985
00:53:43,620 --> 00:53:45,021
you know, how
well they're going to respond

986
00:53:45,021 --> 00:53:48,959
to like drug versus drug B
or does it tell you something about,

987
00:53:49,859 --> 00:53:52,996
you know, their general cognitive ability
or, you know, their current

988
00:53:52,996 --> 00:53:56,633
emotional state or, you know,
like you can do all that kind of stuff,

989
00:53:57,200 --> 00:53:58,969
which is the ultimate goal of all of this

990
00:54:00,937 --> 00:54:01,838
but so,

991
00:54:01,838 --> 00:54:04,641
okay, so the first thing we need to do,

992
00:54:05,609 --> 00:54:08,011
at least in the order
I have it set up, is to set T.

993
00:54:08,011 --> 00:54:11,314
So T is the number of time steps
and the trial so in this case,

994
00:54:11,314 --> 00:54:14,918
there are three time steps, remember,
because the

995
00:54:15,352 --> 00:54:17,520
the agent and

996
00:54:19,289 --> 00:54:21,358
the agent has to start in the starts date

997
00:54:22,092 --> 00:54:25,262
and then move to the second time
step, choose the hidden state

998
00:54:25,962 --> 00:54:29,099
and then third time step, choose
one of the machines

999
00:54:29,332 --> 00:54:32,502
or it can go straight from the start,
stage one of the machines.

1000
00:54:32,902 --> 00:54:35,739
But then at the third time step
it'll just go back to the start stage.

1001
00:54:36,606 --> 00:54:37,907
That's how we have it set up anyway.

1002
00:54:37,907 --> 00:54:40,844
So you need three time steps

1003
00:54:41,745 --> 00:54:44,414
so then what's typically

1004
00:54:44,414 --> 00:54:48,485
easiest is to first start
or start out by specifying the

1005
00:54:49,486 --> 00:54:52,455
prior expectations over states.

1006
00:54:52,455 --> 00:54:55,825
And so in the, in Big D,
which is the generative process

1007
00:54:56,760 --> 00:54:59,629
for factor number one,
which is in the braces here,

1008
00:55:00,030 --> 00:55:04,668
we just specify this as a column vector,
so a row

1009
00:55:04,668 --> 00:55:07,637
and then this means that it's transposed
and becomes a column that's

1010
00:55:08,838 --> 00:55:10,540
apostrophe thing here.

1011
00:55:10,540 --> 00:55:14,778
And so we set this as one and zero,
which just means that it's

1012
00:55:14,778 --> 00:55:18,281
with 100% probability
going to be in the left

1013
00:55:18,281 --> 00:55:21,584
better context

1014
00:55:21,985 --> 00:55:24,688
and then for the second step factor,

1015
00:55:24,688 --> 00:55:26,690
so this D two thing here,

1016
00:55:27,691 --> 00:55:30,560
we set it so the agent always starts out

1017
00:55:30,560 --> 00:55:33,763
in the start state
so a one in that first entry

1018
00:55:34,197 --> 00:55:36,866
and then zeros for all the rest
because it's never going to start

1019
00:55:37,200 --> 00:55:40,704
in the hinge state or in the choose
left state or in the right state

1020
00:55:41,371 --> 00:55:44,641
and we're just saying absolutely
the agent always start in the start state

1021
00:55:47,310 --> 00:55:49,145
but then this is all explained.

1022
00:55:49,145 --> 00:55:52,982
But then for a little D
so there's the agent's actual beliefs.

1023
00:55:53,516 --> 00:55:56,519
We say the agent has no idea

1024
00:55:57,020 --> 00:55:59,322
what context it's
going to be to start with.

1025
00:55:59,789 --> 00:56:03,159
And so we set measures to values
that are flat.

1026
00:56:03,159 --> 00:56:06,029
So where each the left better
and the right better context

1027
00:56:06,062 --> 00:56:08,031
have the same values.

1028
00:56:08,598 --> 00:56:11,401
But notice here
that we didn't set these up

1029
00:56:11,401 --> 00:56:14,871
to add up to one like we did for Big D.

1030
00:56:15,305 --> 00:56:16,039
I mean, the reason

1031
00:56:16,039 --> 00:56:19,109
is, is that these are technically
what are called directly distributions.

1032
00:56:20,243 --> 00:56:23,079
And directly distributions

1033
00:56:23,079 --> 00:56:25,281
are basically

1034
00:56:25,281 --> 00:56:28,418
they say the smaller these numbers are,

1035
00:56:28,785 --> 00:56:32,989
the less confident
the agent is in its beliefs.

1036
00:56:33,490 --> 00:56:37,894
So if these are point two, five and 25,
if thinks it's a flat distribution,

1037
00:56:38,194 --> 00:56:42,065
but it's really unconfident,
whereas this thing was like 20 and 20,

1038
00:56:42,599 --> 00:56:46,503
then it would also believe it's
a flat distribution but it'd be really,

1039
00:56:46,503 --> 00:56:49,873
really confident that its beliefs
are about a flat distribution.

1040
00:56:50,974 --> 00:56:53,176
And to explain that better,

1041
00:56:53,176 --> 00:56:54,711
we're actually going to transition over

1042
00:56:54,711 --> 00:56:57,013
to a Chris
is going to give you a little bit of a

1043
00:56:58,281 --> 00:57:02,085
kind of more general explanation
about how learning works here sense.

1044
00:57:02,118 --> 00:57:04,154
You actually do
need to understand this now

1045
00:57:05,688 --> 00:57:09,092
well so again,
transition over to Chris here briefly.

1046
00:57:09,626 --> 00:57:13,530
Yep. Because usually a probability
people would imagine it sums to one

1047
00:57:13,630 --> 00:57:14,964
something has to happen.

1048
00:57:14,964 --> 00:57:18,001
You know, the coin has to go
heads or tails, so it has to add up

1049
00:57:18,001 --> 00:57:21,738
to 100% has to add up to one
to be a classical probability.

1050
00:57:21,905 --> 00:57:24,040
But this is a little bit of a tweak
on that.

1051
00:57:24,040 --> 00:57:26,309
Some similar features,
but some unique features.

1052
00:57:26,309 --> 00:57:30,447
So thanks for switching over there
and helping us understand it. Yep.

1053
00:57:30,847 --> 00:57:32,449
So can you guys see my screen? Yes.

1054
00:57:32,449 --> 00:57:35,251
We don't see you, but we suspect.
All right. Go ahead.

1055
00:57:35,251 --> 00:57:37,654
Okay. Okay.

1056
00:57:37,654 --> 00:57:41,090
So as I was saying before, essentially
likelihoods

1057
00:57:41,491 --> 00:57:44,060
what we're talking about before
I say we're talking about the Matrix

1058
00:57:44,360 --> 00:57:46,663
there is, technically speaking,
a duration.

1059
00:57:47,230 --> 00:57:48,865
So all of the distributions

1060
00:57:48,865 --> 00:57:52,135
will be categorical distributions,
our days, our base and our eyes.

1061
00:57:52,702 --> 00:57:54,204
They're all categorical distributions

1062
00:57:55,972 --> 00:57:59,476
and they
all have a prior or a distribution

1063
00:57:59,476 --> 00:58:02,178
over the parameter space,
which is a directional distribution

1064
00:58:04,013 --> 00:58:06,282
and so just kind of the people
who are technically

1065
00:58:06,516 --> 00:58:09,419
in the know,
as it were, a categorical distribution

1066
00:58:09,419 --> 00:58:13,756
is just a special case of multi, multi,
no meal distribution

1067
00:58:14,891 --> 00:58:18,661
and additionally distribution
is what's called a conjugate prior

1068
00:58:18,928 --> 00:58:22,065
for the categorical in the multi
normal distributions, which just means

1069
00:58:22,065 --> 00:58:25,502
that if you multiply these two things
together, these two distributions,

1070
00:58:25,702 --> 00:58:30,039
you get out the distribution
that has the same form as the prior.

1071
00:58:30,306 --> 00:58:33,810
And that's really important if you're
doing like empirical Bayes, for example,

1072
00:58:34,110 --> 00:58:38,815
where the prior or the posterior
after round one

1073
00:58:38,815 --> 00:58:42,151
serves as the prior four in the second
round of inference, that kind of thing.

1074
00:58:43,653 --> 00:58:47,223
And so just to briefly go through this so

1075
00:58:47,223 --> 00:58:49,259
you can see my cursor, right?

1076
00:58:49,259 --> 00:58:50,093
Yep. Yep.

1077
00:58:50,093 --> 00:58:50,560
Okay.

1078
00:58:50,560 --> 00:58:53,263
This is the categorical distribution
where X is

1079
00:58:54,330 --> 00:58:56,799
a categorical variable

1080
00:58:56,799 --> 00:58:59,102
that occupies one of one to K

1081
00:59:00,003 --> 00:59:02,071
mutually exclusive state of the world.

1082
00:59:02,405 --> 00:59:07,477
So that is to say,
I have a distribution over a grid world

1083
00:59:07,810 --> 00:59:11,180
where my agent is my agent can't be in
two places at once at either end,

1084
00:59:11,447 --> 00:59:13,750
it's either left or it's right.

1085
00:59:13,750 --> 00:59:15,151
For example,

1086
00:59:15,752 --> 00:59:17,987
and theta are just the parameters
of that distribution.

1087
00:59:17,987 --> 00:59:21,090
And so this function here
is this thing sticking out the front.

1088
00:59:21,257 --> 00:59:24,460
It's this normalization constant
we can adjust it just to counter

1089
00:59:24,460 --> 00:59:26,996
the combinatorics
of the distribution controls.

1090
00:59:26,996 --> 00:59:28,298
That seems to one

1091
00:59:28,865 --> 00:59:31,901
exactly the same thing here
with the durational distribution are over

1092
00:59:31,901 --> 00:59:36,806
and it is a distribution over
the parameter space of our likelihood.

1093
00:59:36,806 --> 00:59:37,340
Right.

1094
00:59:37,473 --> 00:59:38,308
And so duration

1095
00:59:38,308 --> 00:59:42,178
distributions often kind of colloquially
called distributions of distributions

1096
00:59:42,645 --> 00:59:45,582
because they are a distribution
over a vector that has to sum up to one.

1097
00:59:47,550 --> 00:59:50,486
And then what's really beautiful
about these two distributions

1098
00:59:50,486 --> 00:59:52,355
is that when you multiply them together

1099
00:59:52,355 --> 00:59:55,592
and we'll just ignore the normalization
constant because things get pretty messy,

1100
00:59:56,392 --> 01:00:00,897
but you end up with a distribution
that it has at the same functional form

1101
01:00:00,897 --> 01:00:03,066
as the PRI. It's also duration
distribution.

1102
01:00:03,066 --> 01:00:07,203
So our posterior distribution
over the parameters of our likelihoods

1103
01:00:07,537 --> 01:00:12,275
is also a duration distribution
and all that changes

1104
01:00:12,275 --> 01:00:13,743
when we go from the likelihood

1105
01:00:13,743 --> 01:00:16,212
is we add this little alpha thing
here, it's called account.

1106
01:00:17,647 --> 01:00:19,749
So essentially it's add account

1107
01:00:20,149 --> 01:00:23,386
to whatever it was that was observed
to the variable that was observed.

1108
01:00:23,886 --> 01:00:25,888
So I'm just going to go into a little bit
more detail

1109
01:00:25,888 --> 01:00:27,690
than that on the next slide.

1110
01:00:27,690 --> 01:00:28,925
Is there anything
you wanna add to that, Ron?

1111
01:00:30,526 --> 01:00:33,296
No, I mean, just just,
I mean at the, at the end of the day

1112
01:00:33,296 --> 01:00:36,299
and you know, first Chris will,
Chris will cover this, you know,

1113
01:00:36,299 --> 01:00:37,634
I'm sure is that

1114
01:00:37,634 --> 01:00:39,602
while this might look really complicated

1115
01:00:39,602 --> 01:00:41,871
to somebody
who doesn't have a lot of math background

1116
01:00:41,871 --> 01:00:46,309
it really just amounts to there
being like what I showed you

1117
01:00:46,309 --> 01:00:50,480
with D there being a number
that's assigned to each entry

1118
01:00:51,781 --> 01:00:55,351
and when you observe something,
that number will get bigger.

1119
01:00:55,818 --> 01:00:58,421
And so that just means that

1120
01:00:59,055 --> 01:01:02,759
like what I said, that for instance,
if I believe

1121
01:01:02,759 --> 01:01:06,362
that I was in the left
better state the last five times,

1122
01:01:06,796 --> 01:01:08,798
then I'll add like

1123
01:01:09,332 --> 01:01:12,969
five numbers you know, I'll add
15 times to that left one.

1124
01:01:12,969 --> 01:01:17,140
So now it would be like 5.25
as opposed to just point to five.

1125
01:01:18,675 --> 01:01:20,576
And ultimately

1126
01:01:20,576 --> 01:01:23,112
those numbers get soft maxed.

1127
01:01:23,112 --> 01:01:26,816
So they just get normalized and turned
back into a probability distribution.

1128
01:01:28,351 --> 01:01:30,153
But but like I said
at the end of the day,

1129
01:01:30,153 --> 01:01:33,056
you're just adding counts
each time you observe something.

1130
01:01:33,322 --> 01:01:33,856
Yeah.

1131
01:01:34,490 --> 01:01:37,360
This is just like how
you might be a little bit more confident

1132
01:01:37,360 --> 01:01:41,364
about the true ratio of different kinds
of colors of balls in an urn.

1133
01:01:41,364 --> 01:01:44,133
If you had three of one color
and then one of the other color,

1134
01:01:44,133 --> 01:01:48,071
you're not quite sure if you had
3,000,001 million, you'd be very sure.

1135
01:01:48,071 --> 01:01:51,074
And then, Max, you had a question Yeah.

1136
01:01:51,074 --> 01:01:53,042
So just go
into the other end of the spectrum.

1137
01:01:53,042 --> 01:01:57,013
Then if I'm somebody who's in probability
theory and I'm talking about a dinner

1138
01:01:57,046 --> 01:01:59,282
slide distribution
and I'm talking about the support

1139
01:01:59,282 --> 01:02:03,352
for that distribution
on the open interval 01.

1140
01:02:03,352 --> 01:02:05,722
And as we can see here
from these equations that are provided

1141
01:02:06,522 --> 01:02:09,492
that if I go to zero,
then I'm basically going to end up

1142
01:02:09,492 --> 01:02:13,529
with a since it's multiplying each time,
then there could be problems.

1143
01:02:13,529 --> 01:02:15,298
So in practice, however,

1144
01:02:16,365 --> 01:02:17,400
my question

1145
01:02:17,400 --> 01:02:20,603
not being very experienced
with using these things in practice

1146
01:02:20,803 --> 01:02:23,172
when we implement this,
does that have much of an effect

1147
01:02:23,172 --> 01:02:27,577
on any of the estimated parameters no.

1148
01:02:27,910 --> 01:02:31,180
But one thing to say is that

1149
01:02:31,180 --> 01:02:35,184
because you always work with log you,
we always in log space,

1150
01:02:36,552 --> 01:02:41,124
we end up once you just have basically
a little numerical trick

1151
01:02:41,124 --> 01:02:44,594
where all of your to make sure
that you never have a log of zero.

1152
01:02:44,861 --> 01:02:48,631
We add E to the -16 to all entries

1153
01:02:51,701 --> 01:02:53,503
Does that kind of answer your question?

1154
01:02:53,503 --> 01:02:57,006
Yes. So if up for example,
if I see at the end of the day

1155
01:02:57,406 --> 01:03:01,711
I might have a converged parameter
that has apparently a very low log

1156
01:03:01,711 --> 01:03:05,915
likelihood of you know when I
when I converge on that estimate

1157
01:03:07,049 --> 01:03:07,650
is, you

1158
01:03:07,650 --> 01:03:10,553
know, maybe part of that could be related
to okay.

1159
01:03:10,653 --> 01:03:13,756
There's some times
where I am entering in a state

1160
01:03:13,756 --> 01:03:16,592
that might be unlikely
to occur most of the time

1161
01:03:16,592 --> 01:03:19,629
or maybe it wasn't observed
in this particular set of experiments.

1162
01:03:19,629 --> 01:03:21,931
But the rest of it kind of holds up.

1163
01:03:21,931 --> 01:03:25,802
Is that maybe one way to think about low,
low values?

1164
01:03:26,335 --> 01:03:29,472
When I converge
to my parameter estimates.

1165
01:03:29,472 --> 01:03:33,142
And I'm not sure I totally understand
the example just because it's a bit

1166
01:03:33,142 --> 01:03:35,144
a little bit abstract,
could we give a how.

1167
01:03:35,144 --> 01:03:37,113
About the slot machine example?

1168
01:03:37,113 --> 01:03:40,383
It's like you go into the casino, you've
never done any of the slot machines.

1169
01:03:40,616 --> 01:03:42,451
It's like you haven't
had much experience.

1170
01:03:42,451 --> 01:03:43,953
So even if you're

1171
01:03:43,953 --> 01:03:47,223
not sure about which one's better,
it's like you're not sure overall.

1172
01:03:47,557 --> 01:03:51,160
And then as you try more of the slot
machines, just like Ryan was saying,

1173
01:03:51,294 --> 01:03:55,164
you increment up observations,
so to speak, in those columns.

1174
01:03:55,364 --> 01:03:58,334
So after you've made a thousand slot
machine visits, you're

1175
01:03:58,334 --> 01:04:01,637
a little bit more confident than after
you had made zero or one or few.

1176
01:04:01,671 --> 01:04:04,140
Is that the case just to bring it
to the example that we're going.

1177
01:04:04,140 --> 01:04:04,507
To be there?

1178
01:04:04,507 --> 01:04:08,911
Yeah, or I could pull the slot machine
three times and get Y only one result

1179
01:04:08,911 --> 01:04:13,683
observed, and I would never have
the compliment to that result observed.

1180
01:04:14,550 --> 01:04:15,918
And so then effectively my,

1181
01:04:15,918 --> 01:04:19,789
my probability distribution
is then one and zero on my posterior.

1182
01:04:21,090 --> 01:04:23,526
So if I end up having like weird

1183
01:04:23,826 --> 01:04:27,530
looking numbers in my on my converge
parameter estimates in terms of the log

1184
01:04:27,530 --> 01:04:29,332
likelihoods for the different things,

1185
01:04:29,332 --> 01:04:32,201
it kind of maybe just relates
to this particular property.

1186
01:04:32,201 --> 01:04:33,836
But at the end of the day,

1187
01:04:33,836 --> 01:04:34,570
what it sounds like

1188
01:04:34,570 --> 01:04:38,507
is that I'm still going to converge
to the correct values using this approach

1189
01:04:38,674 --> 01:04:42,278
that's just like kind of an insurance.

1190
01:04:42,311 --> 01:04:44,113
Yeah, I'm not sure honestly about any

1191
01:04:44,113 --> 01:04:47,216
if there any formal results
that relate to this

1192
01:04:47,216 --> 01:04:50,720
but in practice,
because we always basically

1193
01:04:50,720 --> 01:04:53,856
the categorical distribution is just
and it just ends up being a self

1194
01:04:53,856 --> 01:04:56,792
max function
over the concentration parameters

1195
01:04:58,628 --> 01:05:01,197
you always have something
that's between zero and one.

1196
01:05:02,431 --> 01:05:05,401
And the concentration parameters are
basically will always kind of

1197
01:05:05,401 --> 01:05:06,969
be somewhere between zero.

1198
01:05:06,969 --> 01:05:09,038
It's never been observed. And

1199
01:05:10,139 --> 01:05:12,375
whatever it is, it goes to however high.

1200
01:05:12,408 --> 01:05:15,578
All of our main experiments,
you run all simulations around

1201
01:05:16,746 --> 01:05:19,382
I've never encountered a problem
with this,

1202
01:05:19,382 --> 01:05:21,884
but then again,
I've never actually really looked into

1203
01:05:24,220 --> 01:05:27,290
possible consequences of that,
so I just don't know.

1204
01:05:27,290 --> 01:05:28,624
I'm sorry, Ryan.

1205
01:05:28,624 --> 01:05:29,458
I don't think so.

1206
01:05:29,458 --> 01:05:31,294
I mean, the only thing I would say
and yeah, and maybe,

1207
01:05:31,294 --> 01:05:33,796
maybe I misunderstood the question,
but I mean, if it has to do

1208
01:05:33,796 --> 01:05:36,933
with whether or not ill,
there's certainly no guarantee that

1209
01:05:36,933 --> 01:05:41,537
you'll converge on the,
on the on the trigger generative process.

1210
01:05:42,004 --> 01:05:45,541
Yeah, there's, there's lots of cases
where things can go wrong

1211
01:05:45,541 --> 01:05:47,877
and you'll get stuck in local minima
and things like that.

1212
01:05:48,444 --> 01:05:51,614
And we actually show several examples
of that in this structure.

1213
01:05:51,614 --> 01:05:53,282
Learning paper

1214
01:05:53,282 --> 01:05:56,385
and where we show you how you can use
active inference for structural learning

1215
01:05:56,385 --> 01:05:58,287
and like Bayesian model expansion

1216
01:05:58,287 --> 01:06:01,223
or Space-Based expansion and say spares
production and things like that.

1217
01:06:01,791 --> 01:06:07,229
There's, there's lots of cases where
you'll end up with the wrong likelihood.

1218
01:06:08,664 --> 01:06:12,001
So, so it's not uh, yeah, it's.

1219
01:06:12,668 --> 01:06:15,805
Or maybe in my mind just a caveat
would then be,

1220
01:06:16,038 --> 01:06:18,774
you know, if it going to that example
with the posterior distribution,

1221
01:06:18,774 --> 01:06:22,878
I only have pulled the,
or picked a machine three times.

1222
01:06:23,679 --> 01:06:26,615
When we're designing our experiment,
it might be important to consider

1223
01:06:26,615 --> 01:06:27,883
setting it up so that,

1224
01:06:27,883 --> 01:06:28,150
you know,

1225
01:06:28,150 --> 01:06:29,318
we're not going to get into the case

1226
01:06:29,318 --> 01:06:32,321
where we only have a sample of N of three
because obviously that doesn't

1227
01:06:32,321 --> 01:06:34,223
sound like good science
to anybody anyways, but

1228
01:06:34,223 --> 01:06:38,394
it's just kind of the oversimplified case
where as you add more categories,

1229
01:06:38,928 --> 01:06:41,831
you could run into that scenario
for one of your particular categories

1230
01:06:41,831 --> 01:06:43,599
which could cause some problems. I.

1231
01:06:43,599 --> 01:06:47,269
I like if there's 100 kinds of serial
and you only let people choose

1232
01:06:47,269 --> 01:06:49,438
one time,
you're not going to get a good estimate.

1233
01:06:49,438 --> 01:06:53,342
It's kind of like a sample size
and an experimental design question.

1234
01:06:53,609 --> 01:06:56,445
So just to kind of pull back for those
because that's,

1235
01:06:56,445 --> 01:06:59,515
that's a really interesting question
about where there's proof

1236
01:06:59,515 --> 01:07:02,618
ability or how different sample sizes
are related to each other.

1237
01:07:02,618 --> 01:07:05,755
But we're just thinking about this,
the structure

1238
01:07:05,888 --> 01:07:11,627
for how additional observations
are going to increase the kind

1239
01:07:11,627 --> 01:07:15,164
or the the quality of this estimate
of the parameter.

1240
01:07:15,865 --> 01:07:16,265
Yeah.

1241
01:07:16,265 --> 01:07:20,002
And you also and you'll see
in what Chris shows you that that like

1242
01:07:20,403 --> 01:07:24,740
the basically the the lower
the concentration of the concentration

1243
01:07:24,740 --> 01:07:28,644
property values are generally
the more the expected free energy

1244
01:07:28,644 --> 01:07:33,682
will be weighted toward choosing
like parameter exploration policies.

1245
01:07:34,050 --> 01:07:34,316
Right?

1246
01:07:34,350 --> 01:07:38,020
So it'll be a thing
where if it's never chosen the 99th

1247
01:07:38,020 --> 01:07:41,857
serial before, it'll be driven to go
choose that one just

1248
01:07:41,857 --> 01:07:45,127
because it wants to be more confident
in what the actual parameters are.

1249
01:07:46,262 --> 01:07:48,464
But anyway I personally.

1250
01:07:48,464 --> 01:07:50,132
Yeah, go ahead.

1251
01:07:50,132 --> 01:07:50,633
Yeah. Yeah.

1252
01:07:50,633 --> 01:07:51,867
So we actually don't

1253
01:07:51,867 --> 01:07:55,504
yeah we actually don't have any numerical
examples in a paper where we kind of

1254
01:07:55,638 --> 01:07:59,642
have to show the fringy values
for different parameter values.

1255
01:07:59,909 --> 01:08:04,880
We only kind of have state
exploration terms, but there are,

1256
01:08:05,581 --> 01:08:08,017
I mean, maybe that's something
we could add to the paper actually.

1257
01:08:08,050 --> 01:08:10,686
That would be something interesting.
Yeah.

1258
01:08:10,686 --> 01:08:12,188
Yeah. Okay.

1259
01:08:12,621 --> 01:08:15,191
So essentially,
just to kind of give a recap,

1260
01:08:15,191 --> 01:08:18,194
we just have our

1261
01:08:18,694 --> 01:08:22,531
ADD probability distribution over a
all the parameters of A

1262
01:08:22,832 --> 01:08:28,871
is just a directional distribution over
kind of over A and then so we just end up

1263
01:08:28,871 --> 01:08:35,111
with a matrix that has at the root
where the rows are possible observations

1264
01:08:35,111 --> 01:08:40,816
and the columns are hidden states
and literally all that we're doing this.

1265
01:08:40,916 --> 01:08:43,285
So this little time,
this little kind of cross inside

1266
01:08:43,285 --> 01:08:45,521
the circle is called current.
It contains a product

1267
01:08:48,157 --> 01:08:49,425
that is that goodness.

1268
01:08:49,425 --> 01:08:51,260
That is that correct?
I actually don't know.

1269
01:08:51,260 --> 01:08:53,229
That's the notation.

1270
01:08:53,229 --> 01:08:54,864
Ross product and.

1271
01:08:54,864 --> 01:08:56,799
It's in the SPM textbook.

1272
01:08:56,799 --> 01:08:59,201
That notation is used
for the Cronica product.

1273
01:08:59,835 --> 01:09:00,536
Yeah. Thank you.

1274
01:09:00,536 --> 01:09:02,271
Okay, good.

1275
01:09:02,271 --> 01:09:03,706
Sorry, I just had a brief mind blank.

1276
01:09:03,706 --> 01:09:04,573
I was like, oh god.

1277
01:09:04,573 --> 01:09:07,743
Anyway, so and there's eight thing here
is just a learning, right?

1278
01:09:07,943 --> 01:09:11,881
And so whatever, it will
just multiply essentially this year.

1279
01:09:11,881 --> 01:09:16,018
So say I'll just give an example
and we'll just add that

1280
01:09:16,485 --> 01:09:19,321
to our previous values
of whatever these A's are.

1281
01:09:19,321 --> 01:09:23,792
So just very simply say we have
our posterior states

1282
01:09:23,792 --> 01:09:29,064
is something like .7.3 and our OB
we receive observation.

1283
01:09:29,431 --> 01:09:32,701
So this is a transpose

1284
01:09:32,701 --> 01:09:33,202
vector.

1285
01:09:33,202 --> 01:09:37,139
So this corresponds to road to this
all that this would do,

1286
01:09:37,139 --> 01:09:42,545
if we're literally adding a value of point seven and point three to this matrix,

1287
01:09:44,146 --> 01:09:45,514
that's all the planning is really.

1288
01:09:45,514 --> 01:09:47,583
It's very simple.

1289
01:09:47,850 --> 01:09:50,286
And then both of these things will get
both of these

1290
01:09:50,953 --> 01:09:52,621
columns will get passed through

1291
01:09:52,621 --> 01:09:54,356
SoftBank's function
at the end of the day,

1292
01:09:54,356 --> 01:09:57,726
just to kind of give a give an example
of what we're talking about before.

1293
01:09:57,726 --> 01:10:02,298
So because they're both soft, maxed
all itself, my old self Max cares about,

1294
01:10:02,698 --> 01:10:05,701
is that the difference
between the two numbers

1295
01:10:05,968 --> 01:10:07,803
in all the two elements of the vector.

1296
01:10:07,803 --> 01:10:12,541
So 51 and 52 and one
the same, same value in the soft max.

1297
01:10:12,775 --> 01:10:15,311
But in one case
the model is incredibly confident

1298
01:10:15,945 --> 01:10:18,614
and the other case
is not confidence at all for example.

1299
01:10:19,281 --> 01:10:22,718
So if you had an agent that was actually
doing parameter exploration,

1300
01:10:22,885 --> 01:10:23,852
there would be no

1301
01:10:23,852 --> 01:10:27,756
the agent would have no motivation
whatsoever to explore the parameters

1302
01:10:27,990 --> 01:10:31,527
in the first case of 51 50,
but it would be quite motivated

1303
01:10:31,527 --> 01:10:35,731
in the case of 21, for example,
even though the likelihood distribution

1304
01:10:35,731 --> 01:10:37,266
you're not getting
out of that is the same.

1305
01:10:38,500 --> 01:10:40,836
Okay, good.

1306
01:10:40,970 --> 01:10:43,172
So that is actually,
I think all that we really had.

1307
01:10:43,172 --> 01:10:47,142
That's all that I'd prepared for this
is because it's really is in practice.

1308
01:10:47,142 --> 01:10:48,577
It's so simple

1309
01:10:49,979 --> 01:10:50,746
So back to you.

1310
01:10:50,746 --> 01:10:51,213
Right.

1311
01:10:51,747 --> 01:10:52,348
Okay.

1312
01:10:54,683 --> 01:11:02,791
So Okay.

1313
01:11:02,958 --> 01:11:05,160
So anyway, with
that has a little bit of the background.

1314
01:11:05,728 --> 01:11:06,195
All right?

1315
01:11:06,195 --> 01:11:09,798
All we're doing here is we're saying
to start with, the agent believes

1316
01:11:10,099 --> 01:11:13,535
it's a flat distribution, has no idea
which one's going to be better left

1317
01:11:13,535 --> 01:11:15,304
or right on each trial.

1318
01:11:15,304 --> 01:11:17,273
That's in little D
because that's our model.

1319
01:11:17,273 --> 01:11:18,407
So each time it

1320
01:11:18,407 --> 01:11:21,677
believes that it was in the left better
context, it's just going to keep adding,

1321
01:11:22,077 --> 01:11:25,014
you know, numbers to this point
to five here.

1322
01:11:25,714 --> 01:11:28,584
And those will be scaled by that learning
right thing that

1323
01:11:29,251 --> 01:11:32,821
that just showed where, you know, if,

1324
01:11:33,155 --> 01:11:35,724
if learning rate is one
then you're trying to add observes

1325
01:11:35,724 --> 01:11:36,659
the left better context.

1326
01:11:36,659 --> 01:11:40,562
I'll just add a one
so it'll become 1.25 and then 2.25, etc.

1327
01:11:41,030 --> 01:11:44,433
But if the learning rate was like 0.5,
so it was like lower,

1328
01:11:44,833 --> 01:11:47,336
then it would add
zero point fives each time.

1329
01:11:47,336 --> 01:11:50,739
So it'd be 0.75, 1.25, etc.

1330
01:11:51,206 --> 01:11:53,475
So the learning rate just scales
how big the counts are.

1331
01:11:53,475 --> 01:11:54,176
They get added.

1332
01:11:55,344 --> 01:11:56,145
And the learning rate

1333
01:11:56,145 --> 01:11:59,548
could be zero, at which case
there wouldn't be an updating.

1334
01:11:59,581 --> 01:12:02,885
Just thinking about just
what could the whole range be

1335
01:12:02,885 --> 01:12:07,089
from not learning agents to very slowly
incorporating new information,

1336
01:12:07,356 --> 01:12:10,659
using this seriously framework
that Christopher just described

1337
01:12:10,859 --> 01:12:13,262
all the way up
to just ridiculously high values.

1338
01:12:13,262 --> 01:12:15,798
So that it just yeah.

1339
01:12:15,798 --> 01:12:16,932
I should just highlight as well.

1340
01:12:16,932 --> 01:12:20,736
I think that is this is I just kind of
want to this probably isn't a good point

1341
01:12:20,736 --> 01:12:22,805
for discussion
just to keep things moving.

1342
01:12:22,805 --> 01:12:26,008
But one thing that's worth highlighting
is there's a genuine kind of debate

1343
01:12:26,141 --> 01:12:29,845
or maybe debate a bit too strong a word,
but discussion to be had about

1344
01:12:29,845 --> 01:12:32,614
whether we should actually be modeling,
modulating learning rates

1345
01:12:33,515 --> 01:12:38,020
given that the model is Bayes
optimal or Apr Bayes optimal.

1346
01:12:38,387 --> 01:12:40,756
One would assume that there should be
a way of modeling things

1347
01:12:40,756 --> 01:12:42,891
about actually changing
the learning rate from one

1348
01:12:45,127 --> 01:12:47,296
but in practice,
I'm not sure if that actually works.

1349
01:12:47,296 --> 01:12:48,564
But anyway.

1350
01:12:48,564 --> 01:12:51,600
So yes, I mean,
I mean technically, technically

1351
01:12:51,600 --> 01:12:54,336
a learning rate of one is optimal yeah.

1352
01:12:54,636 --> 01:12:57,039
But for but for,

1353
01:12:57,339 --> 01:13:01,443
for when you're fitting behavior,
when you're fitting the model to behavior

1354
01:13:01,910 --> 01:13:05,547
and most people are not optimal.

1355
01:13:06,782 --> 01:13:11,220
And so you have to estimate
typically the learning rate

1356
01:13:11,220 --> 01:13:15,290
for each person and oftentimes
it's quite a bit below one.

1357
01:13:15,624 --> 01:13:17,793
But that's actually an interesting
attribute.

1358
01:13:17,793 --> 01:13:21,063
And again, it's an example
about how using these generative

1359
01:13:21,263 --> 01:13:24,633
and generalizable frameworks
help us compare not just

1360
01:13:24,733 --> 01:13:28,404
who could remember more numbers
or who had a faster reaction time

1361
01:13:28,404 --> 01:13:31,173
as far as just observable
characteristics.

1362
01:13:31,440 --> 01:13:34,376
We're moving a step back
and asking whether people might differ

1363
01:13:34,376 --> 01:13:36,879
in their learning rate
or in other attributes of this model.

1364
01:13:38,280 --> 01:13:39,448
And there is also and

1365
01:13:39,448 --> 01:13:40,582
again, I mean, this isn't something

1366
01:13:40,582 --> 01:13:44,987
to really get into so much,
but there is effectively a learning rate

1367
01:13:45,254 --> 01:13:51,360
that comes out of the precision
of your posterior beliefs overstates

1368
01:13:51,860 --> 01:13:55,798
and like, for instance,
if you're opposed to your beliefs,

1369
01:13:55,798 --> 01:13:57,533
other states are really imprecise.

1370
01:13:57,533 --> 01:13:59,101
Like say it's like point,

1371
01:13:59,101 --> 01:14:02,671
say afterwards, you know,
this is like point six, point four.

1372
01:14:03,272 --> 01:14:07,543
Then whatever observation that you get
is just going to add counts of point

1373
01:14:07,543 --> 01:14:11,613
six and point four, which means that
you're not really going to build up

1374
01:14:12,881 --> 01:14:15,517
strong priors.

1375
01:14:15,651 --> 01:14:16,318
You know,

1376
01:14:18,387 --> 01:14:19,521
a strong of a

1377
01:14:19,521 --> 01:14:21,890
more precise priors
for one over the other

1378
01:14:23,058 --> 01:14:24,493
because

1379
01:14:25,127 --> 01:14:28,764
you're not confident
about which state you're in.

1380
01:14:28,997 --> 01:14:29,164
With.

1381
01:14:29,164 --> 01:14:33,001
Just a quick example of that,
you go into the casino and you're drawing

1382
01:14:33,001 --> 01:14:36,972
a card and you can't read the card
or the numbers rubbed off.

1383
01:14:36,972 --> 01:14:41,310
So if there's no precision at the sensory
level with which card you drew, you're

1384
01:14:41,310 --> 01:14:44,980
not going to be updating your estimate
of how likely different numbers are.

1385
01:14:45,214 --> 01:14:47,316
And then if there's
a perfectly legible card

1386
01:14:47,416 --> 01:14:50,452
so it's 100%,
you have no error about which card it is.

1387
01:14:50,786 --> 01:14:54,923
There's the secondary question about
how much you update your internal model,

1388
01:14:55,157 --> 01:14:58,026
but it's not a precision
about the perception question.

1389
01:14:58,060 --> 01:15:02,431
So it's really a nice point and it's
worth understanding that you can have

1390
01:15:02,598 --> 01:15:07,669
uncertainty about observations
that then is captured by this model.

1391
01:15:07,836 --> 01:15:11,907
But also even in the case of precise
or even totally observable scenarios,

1392
01:15:12,040 --> 01:15:15,177
there's additional uncertainty
related to learning

1393
01:15:17,613 --> 01:15:18,213
Yeah.

1394
01:15:18,213 --> 01:15:20,816
I mean,
again, there's still potential problems.

1395
01:15:20,816 --> 01:15:22,417
I mean, what tends to happen in
these models

1396
01:15:22,417 --> 01:15:25,921
is especially if learning rate is one,
then the agent kind of becomes

1397
01:15:25,988 --> 01:15:31,693
too confident too quickly
or a better way to put it as they once

1398
01:15:31,693 --> 01:15:35,731
they've like observed a certain number
of like left, for example,

1399
01:15:36,164 --> 01:15:39,034
becomes really hard for it
to unlearn that

1400
01:15:40,269 --> 01:15:41,970
which is different from what humans do.

1401
01:15:41,970 --> 01:15:44,840
Humans tend to infer that
there's a different context, like, okay,

1402
01:15:44,873 --> 01:15:47,943
I know now it's the, oh,
it's a new context where right is better.

1403
01:15:47,943 --> 01:15:50,946
So it doesn't necessarily
unlearn the original

1404
01:15:52,247 --> 01:15:55,751
counts and instead sort of infer
now I'm in a new state

1405
01:15:55,751 --> 01:15:58,120
and I need to accrue different counts
for that state.

1406
01:15:58,887 --> 01:16:02,190
And that requires more complex models,
which can also be built just.

1407
01:16:02,457 --> 01:16:06,061
But that's not that's
kind of like humans using narrative

1408
01:16:06,328 --> 01:16:10,899
to help reset local parameter estimations
like, oh, well, now it's a new day.

1409
01:16:11,099 --> 01:16:12,968
So now things
that were unlikely yesterday.

1410
01:16:12,968 --> 01:16:14,570
Now it can be likely today.

1411
01:16:14,570 --> 01:16:17,239
And also it relates to this question
about the optimal learning rate

1412
01:16:17,472 --> 01:16:19,908
in a mathematical
or an analytical framework.

1413
01:16:19,908 --> 01:16:22,945
Maybe one or some other number
is, quote, optimal.

1414
01:16:22,945 --> 01:16:25,714
But then in a realized
ecological setting,

1415
01:16:25,981 --> 01:16:29,651
it might be actually not the best
solution in that setting.

1416
01:16:31,486 --> 01:16:32,187
Yeah.

1417
01:16:32,187 --> 01:16:35,524
And so, I mean, yeah, typically people
talk about this in other literature

1418
01:16:35,524 --> 01:16:39,962
as being like laden cause inference
really like inferring related causes

1419
01:16:41,296 --> 01:16:41,897
behind things.

1420
01:16:41,897 --> 01:16:43,231
So there's different latent cause.

1421
01:16:43,231 --> 01:16:44,299
And it's kind of the same thing as saying

1422
01:16:44,299 --> 01:16:46,602
there's a different context
and therefore a different likelihood.

1423
01:16:47,636 --> 01:16:50,339
But, but yeah, so, so I'll just

1424
01:16:50,339 --> 01:16:53,609
kind of, you know, go through this here,
you know, kind of step by step.

1425
01:16:53,642 --> 01:16:57,479
So, you know,
so I so I'm going to set up, right?

1426
01:16:57,479 --> 01:16:58,780
So I have

1427
01:16:59,915 --> 01:17:02,084
I have my big D in my little D here.

1428
01:17:03,452 --> 01:17:07,956
And then the next thing I'm going to do
is I'm doing this a little bit, you know,

1429
01:17:07,990 --> 01:17:13,395
condensed is to just set the number
of states here for each state factor.

1430
01:17:13,795 --> 01:17:16,598
So that's going to be the length of
do you want in the length of D two.

1431
01:17:16,598 --> 01:17:18,567
So this one's going to be
the two contexts

1432
01:17:18,567 --> 01:17:21,269
there's going to be two states
and this one's going to be

1433
01:17:21,603 --> 01:17:23,972
the length of D two,
which is going to be four states.

1434
01:17:24,539 --> 01:17:26,408
So at the end of the day,
this next thing,

1435
01:17:26,408 --> 01:17:29,311
if you look on the right,
is just going to look like that, too.

1436
01:17:29,311 --> 01:17:29,945
And for.

1437
01:17:29,945 --> 01:17:30,912
Just one.

1438
01:17:30,912 --> 01:17:32,881
Note there, for those
who are unfamiliar with MATLAB.

1439
01:17:32,881 --> 01:17:37,753
So at line 148 where noise is,
you put a little red dot like a stop sign

1440
01:17:37,953 --> 01:17:42,090
and then you hit run, which ran
everything that we had just discussed

1441
01:17:42,090 --> 01:17:45,060
from a clean slate up to that line,
and then it halted.

1442
01:17:45,427 --> 01:17:48,096
So that's
what allowed us to actually instantiate

1443
01:17:48,096 --> 01:17:50,165
all these variables
we were just talking about.

1444
01:17:50,165 --> 01:17:53,402
And then you stepped
just one line at a time

1445
01:17:53,535 --> 01:17:57,272
so that you could run this line
one 48 with all of its prerequisites

1446
01:17:57,272 --> 01:18:00,275
fulfilled So it's kind of like a break
point in the program.

1447
01:18:00,475 --> 01:18:01,176
We took a breath

1448
01:18:01,176 --> 01:18:03,578
and then we ran one line, and so now
we're going to go step by step.

1449
01:18:04,513 --> 01:18:07,783
Yeah, it just allows you
to kind of see who was doing step by step

1450
01:18:07,783 --> 01:18:09,418
which is, yeah, it's helpful. So sorry.

1451
01:18:09,418 --> 01:18:10,686
I should have said what?

1452
01:18:10,686 --> 01:18:12,320
Players and stuff and stuff like that.

1453
01:18:12,320 --> 01:18:16,291
And so, so now that I've specified
that I can put this little four

1454
01:18:16,291 --> 01:18:17,626
loop thing which just says

1455
01:18:18,927 --> 01:18:19,294
for that

1456
01:18:19,294 --> 01:18:23,165
four I equals one to the number of states
or factor two,

1457
01:18:24,166 --> 01:18:26,368
I'm just going to make it
all the no hint.

1458
01:18:26,401 --> 01:18:31,206
This is just saying for all the matrices
in a one to start with,

1459
01:18:31,606 --> 01:18:34,443
and everything is just going to generate

1460
01:18:34,443 --> 01:18:37,546
the no hint observation

1461
01:18:37,813 --> 01:18:41,316
so it will end up just looking

1462
01:18:41,316 --> 01:18:44,619
like this where

1463
01:18:46,254 --> 01:18:49,124
and so I just have
so this is just saying,

1464
01:18:49,424 --> 01:18:53,028
and I should say that the columns
correspond to the first day fact

1465
01:18:53,061 --> 01:18:56,164
or so left contacts right context

1466
01:18:56,665 --> 01:18:59,801
and the third the third dimension here
one, two, three, four.

1467
01:18:59,801 --> 01:19:02,637
That's what corresponds
to the second state factor,

1468
01:19:03,038 --> 01:19:06,174
which is the the behavior.

1469
01:19:06,675 --> 01:19:10,212
And so this is saying
when I'm in the start state one,

1470
01:19:11,413 --> 01:19:16,218
each context is going to generate the
no hint observation when I'm in the hint

1471
01:19:16,418 --> 01:19:20,088
factor, it's going to generate the
no hint observation and so forth.

1472
01:19:20,088 --> 01:19:21,790
So I'm just starting it that way.

1473
01:19:21,790 --> 01:19:23,892
Four is to say that they're all that way

1474
01:19:24,793 --> 01:19:28,530
but then what I'm going to do
is I'm going to define this thing,

1475
01:19:29,297 --> 01:19:32,400
which is the probability
of the hint being accurate

1476
01:19:33,802 --> 01:19:35,504
and and again, this is just kind of

1477
01:19:35,504 --> 01:19:39,307
to make things
a little bit concise and convenient.

1478
01:19:40,008 --> 01:19:42,344
And then I'm going to replace that a two

1479
01:19:42,344 --> 01:19:44,846
because remember a two means
I'm in the hint state.

1480
01:19:46,548 --> 01:19:48,850
And so I'm going to say
when I'm in the hint state,

1481
01:19:49,518 --> 01:19:51,720
column one and column one here,

1482
01:19:52,721 --> 01:19:56,725
I'm going to observe the machine
left hint with some probability

1483
01:19:56,725 --> 01:19:59,728
if I'm in the machine left state,
which is the left column,

1484
01:20:00,662 --> 01:20:04,533
and then one minus that for the
no hence state.

1485
01:20:05,033 --> 01:20:08,069
And again,
if I'm in the right, the right better

1486
01:20:08,203 --> 01:20:12,474
context to the right column here,
then it's going to be the reverse.

1487
01:20:12,474 --> 01:20:15,677
I'm going to observe the right machine
with some probability.

1488
01:20:15,844 --> 01:20:17,546
Just one, one note on that. Right?

1489
01:20:17,546 --> 01:20:20,882
So people might look at this and wonder,
why didn't you just say hint

1490
01:20:20,882 --> 01:20:23,752
or no hint instead of this one
-1 equals zero.

1491
01:20:24,085 --> 01:20:27,589
And this is because as you've had it, it
generalizes to cues

1492
01:20:27,589 --> 01:20:29,691
that are less than 100% accurate.

1493
01:20:30,091 --> 01:20:32,661
So then you could have the probability
of the hint being accurate

1494
01:20:32,794 --> 01:20:36,464
to being 90% or less or some other value.

1495
01:20:36,464 --> 01:20:40,135
And so if you have a probabilistic
learning task, you can model it

1496
01:20:40,268 --> 01:20:42,270
with this exact framework
just by changing

1497
01:20:42,904 --> 01:20:46,141
equals point
something rather than re-imagining

1498
01:20:46,141 --> 01:20:49,344
the entire matrix multiplication,
which is really the hard part.

1499
01:20:50,111 --> 01:20:51,279
Yeah, exactly.

1500
01:20:51,279 --> 01:20:54,049
So. So so if I step through that,
then in this case

1501
01:20:54,049 --> 01:20:57,485
I've set it to one, which just means
the hint is completely accurate.

1502
01:20:58,086 --> 01:21:01,089
So now if I look at A1 again,

1503
01:21:01,623 --> 01:21:05,927
then it's going to look like this
where all the other behavioral states

1504
01:21:05,927 --> 01:21:09,030
so in the start state
in choosing the left state

1505
01:21:09,030 --> 01:21:11,733
or choosing a left machine
and choosing the right machine

1506
01:21:12,567 --> 01:21:15,570
they still generate row one here.

1507
01:21:15,570 --> 01:21:18,206
Both of both contexts do the no hint.

1508
01:21:18,673 --> 01:21:23,411
But if I'm in state two,
which is the hint behavioral state,

1509
01:21:23,945 --> 01:21:27,315
then if I'm in the left machine,
if I'm in the left state,

1510
01:21:27,749 --> 01:21:30,619
then I will observe the left

1511
01:21:30,619 --> 01:21:33,755
is better hint with probability one.

1512
01:21:33,755 --> 01:21:37,559
And if I'm in the right column here,
the right context is better state

1513
01:21:38,059 --> 01:21:42,597
that I'm going to observe the right hint
observation.

1514
01:21:42,597 --> 01:21:45,367
So the bottom row here
with 100% probability.

1515
01:21:45,500 --> 01:21:48,270
So that words, if I observe this thing
that I'm going to know for sure

1516
01:21:48,770 --> 01:21:51,973
that I'm now in the left better context.

1517
01:21:53,375 --> 01:21:53,875
But if again,

1518
01:21:53,875 --> 01:21:58,513
if I made this like .8.2, .2.8,
then that would just mean, you know,

1519
01:21:58,947 --> 01:22:01,116
there's some probability
that the hand is going to

1520
01:22:01,650 --> 01:22:03,852
give me the wrong answer.

1521
01:22:04,185 --> 01:22:08,523
So that's so that's all that
as I remember the first state factor is

1522
01:22:08,523 --> 01:22:09,958
the columns.

1523
01:22:09,958 --> 01:22:13,662
The second state factor
is going to be the third dimension.

1524
01:22:15,063 --> 01:22:17,399
That's kind of key

1525
01:22:19,234 --> 01:22:21,236
So now what I'm going to do

1526
01:22:21,236 --> 01:22:25,573
is I'm going to define
the second outcome modality.

1527
01:22:25,573 --> 01:22:30,679
So this is another thing it's important
is that the the number here for a

1528
01:22:30,712 --> 01:22:34,783
that's in the braces
that corresponds to the outcome modality.

1529
01:22:35,283 --> 01:22:39,287
And so the first set of observations
I can get

1530
01:22:39,754 --> 01:22:44,960
is no hint
machine left hand and machine right hand.

1531
01:22:44,960 --> 01:22:46,861
So that's one set of observations

1532
01:22:46,861 --> 01:22:49,331
I would call like an observation
or an outcome modality.

1533
01:22:51,066 --> 01:22:51,566
Now for

1534
01:22:51,566 --> 01:22:54,936
a to the second observation
or outcome modality,

1535
01:22:55,403 --> 01:22:59,174
the possible observations are null
or loss

1536
01:22:59,874 --> 01:23:02,377
and win.

1537
01:23:02,744 --> 01:23:05,714
So this is saying is for the first

1538
01:23:05,714 --> 01:23:08,683
two behavioral states for
I equals one and two.

1539
01:23:09,284 --> 01:23:12,988
And the so being in the start state

1540
01:23:12,988 --> 01:23:16,558
or the hidden state is always going
to generate the null observation.

1541
01:23:16,558 --> 01:23:20,028
So it's not going to generate a loss
or a win

1542
01:23:21,029 --> 01:23:24,366
Now, after that, if I step through here,

1543
01:23:24,866 --> 01:23:28,970
I can set what I just called P
when so the probability of winning

1544
01:23:28,970 --> 01:23:32,774
and I set that to be point eight and
and this is the part

1545
01:23:32,774 --> 01:23:36,211
where the actual reinforcement
or reward learning comes in,

1546
01:23:37,512 --> 01:23:41,683
because what I can do is I can say, okay,

1547
01:23:41,883 --> 01:23:46,621
if I'm in the left,
choose the left machine state.

1548
01:23:46,621 --> 01:23:48,723
So three then

1549
01:23:50,959 --> 01:23:53,028
the probability of

1550
01:23:53,028 --> 01:23:55,030
winning, the probability of losing

1551
01:23:56,664 --> 01:24:02,070
is one minus P
when and the probability of winning

1552
01:24:02,070 --> 01:24:05,940
and the right one or the probability
of losing in the right one if I choose

1553
01:24:05,940 --> 01:24:10,145
the right one is high right,
which makes sense, right?

1554
01:24:10,145 --> 01:24:10,578
Because if I'm

1555
01:24:10,578 --> 01:24:14,749
in the left better context,
then if I choose the right column, right?

1556
01:24:14,749 --> 01:24:16,618
If I choose the right

1557
01:24:16,818 --> 01:24:19,721
machine,
then I'm most likely going to lose

1558
01:24:22,624 --> 01:24:24,059
in contrast,

1559
01:24:24,059 --> 01:24:28,196
if I'm in the right better context

1560
01:24:29,497 --> 01:24:31,733
and I choose

1561
01:24:31,766 --> 01:24:35,904
if I choose the right machine
and I'm in the right better context, then

1562
01:24:36,371 --> 01:24:40,241
the right column here should generate
a win with high probability.

1563
01:24:40,442 --> 01:24:41,176
So this is just saying

1564
01:24:41,176 --> 01:24:44,846
if I choose the right machine,
that's the for and I'm in the right

1565
01:24:44,846 --> 01:24:47,449
client, right better context,
which is the right column,

1566
01:24:47,882 --> 01:24:51,286
then I have a high probability of winning

1567
01:24:52,654 --> 01:24:55,023
and vice
versa up here if I choose the left one.

1568
01:24:55,824 --> 01:24:58,793
Again, just to talk about how
this is the minimal example,

1569
01:24:58,793 --> 01:25:01,963
we're talking about two options
with the two slot machines.

1570
01:25:01,963 --> 01:25:03,398
There could be more options.

1571
01:25:03,398 --> 01:25:05,900
We're talking about how much it costs
to get the hints

1572
01:25:06,301 --> 01:25:09,170
and how much it costs when you win
or how much you receive when you win.

1573
01:25:09,170 --> 01:25:12,140
That's a parameter
you can change how accurate the hint is.

1574
01:25:12,140 --> 01:25:14,809
You could have a perfectly accurate hint
that sends you to the slot machine

1575
01:25:15,009 --> 01:25:16,611
that wins 100% of the time.

1576
01:25:16,611 --> 01:25:19,114
So the probability of winning
can now be changed.

1577
01:25:19,481 --> 01:25:20,582
So then you can say, Okay, I'm

1578
01:25:20,582 --> 01:25:23,451
imagining a situation where there's
a perfect messenger with a hint

1579
01:25:23,618 --> 01:25:26,821
and that it's free to visit that person,
and then it's 100% likely

1580
01:25:26,821 --> 01:25:28,656
which slot machine is the winning one.

1581
01:25:28,656 --> 01:25:32,160
Or you can imagine these more gray
zone scenarios where different

1582
01:25:32,160 --> 01:25:36,131
things are associated with each other
in a less direct way,

1583
01:25:36,131 --> 01:25:37,699
or there's more than two options

1584
01:25:37,699 --> 01:25:40,802
just for those who are like seeing
this one example for the first time,

1585
01:25:41,136 --> 01:25:44,939
this is kind of like the tip
of the iceberg of a big, big family

1586
01:25:44,939 --> 01:25:48,109
of different kinds of models that have
a lot more options and a lot more nuance.

1587
01:25:48,843 --> 01:25:51,779
Yeah, I mean, you could have,
you know, 20

1588
01:25:51,880 --> 01:25:54,983
outcome modalities and ten state factors
if you wanted to.

1589
01:25:55,350 --> 01:25:56,584
It would just mean that like

1590
01:25:56,584 --> 01:25:58,720
there'd be a fourth dimension
for the third set factor

1591
01:25:58,720 --> 01:26:00,655
and a fifth dimension
for the fourth state factor.

1592
01:26:00,655 --> 01:26:02,657
And you know,
let's just get really big, really fast.

1593
01:26:03,825 --> 01:26:04,926
But so now that I've done this

1594
01:26:04,926 --> 01:26:09,164
because I set the probability
of winning 2.8 and given that you're,

1595
01:26:09,631 --> 01:26:11,900
you know, in the correct context,

1596
01:26:12,333 --> 01:26:14,602
then a two will look like this.

1597
01:26:15,136 --> 01:26:17,672
So this just says,

1598
01:26:17,672 --> 01:26:21,009
if I choose if I'm in the start state,
I don't observe,

1599
01:26:21,209 --> 01:26:22,644
I never observe a winner or loss.

1600
01:26:22,644 --> 01:26:25,413
If I'm in the hit state,
I never observe a winner, a loss.

1601
01:26:25,980 --> 01:26:28,683
And if I may, if I choose

1602
01:26:28,683 --> 01:26:32,187
the left option, the left machine,

1603
01:26:32,587 --> 01:26:36,658
then I will win with 0.8 and lose
with point two.

1604
01:26:37,392 --> 01:26:38,693
And the opposite four.

1605
01:26:38,693 --> 01:26:41,129
If I choose the right one right,
I'll lose with point eight.

1606
01:26:41,196 --> 01:26:45,500
If I if I choose the left machine
and I'm in the right context,

1607
01:26:46,167 --> 01:26:49,571
and if I choose the right machine,
so stay at four here,

1608
01:26:50,138 --> 01:26:53,541
then the probability of winning is 0.8.

1609
01:26:53,641 --> 01:26:55,810
So the third row here is winning, right?

1610
01:26:57,212 --> 01:26:59,547
And oh, point two and vice versa.

1611
01:26:59,581 --> 01:27:01,749
So this is just defining
the probability of winning,

1612
01:27:02,350 --> 01:27:04,986
given your choice, given
whether it's the left better

1613
01:27:04,986 --> 01:27:07,155
or right better context

1614
01:27:08,623 --> 01:27:11,292
and I know that's a mouthful, but

1615
01:27:11,292 --> 01:27:11,960
yeah.

1616
01:27:13,294 --> 01:27:17,932
So then finally and this isn't really

1617
01:27:17,932 --> 01:27:21,669
all that practically interesting,
but it's important to include us.

1618
01:27:21,669 --> 01:27:23,972
Is that what this just says?

1619
01:27:24,272 --> 01:27:29,043
So I'll come at three
is it just the agent's own behavior?

1620
01:27:29,510 --> 01:27:34,515
So what this means is every single time
it makes a choice,

1621
01:27:34,983 --> 01:27:38,052
it observes itself making that choice.

1622
01:27:39,220 --> 01:27:43,324
And all that does is make the agent
completely confident about what it did.

1623
01:27:45,193 --> 01:27:46,160
But you can imagine,

1624
01:27:46,160 --> 01:27:49,597
if somebody weren't aware
of their own Decision-Making behavior,

1625
01:27:49,931 --> 01:27:52,300
you could imagine that might lead
to decision making.

1626
01:27:52,300 --> 01:27:54,002
That's not in line
with what you would do.

1627
01:27:54,002 --> 01:27:55,403
If you could observe your behavior.

1628
01:27:56,504 --> 01:27:56,971
Yeah.

1629
01:27:56,971 --> 01:27:58,106
So, you know, I mean,

1630
01:27:58,106 --> 01:28:00,275
if you if you get rid of this,
then in some cases

1631
01:28:00,275 --> 01:28:02,710
the agent won't be able to confirm
what he did.

1632
01:28:02,710 --> 01:28:05,079
And so, therefore,
it will be hard for him to uh,

1633
01:28:05,847 --> 01:28:08,583
or to learn
what actions were the right one

1634
01:28:08,583 --> 01:28:10,018
because we don't know,
like what it show is.

1635
01:28:10,018 --> 01:28:13,054
Basically, it'd be like choosing a slot
machine blind.

1636
01:28:15,390 --> 01:28:19,961
I just want to jump in with a question
about my motor systems context.

1637
01:28:19,961 --> 01:28:22,764
So, for example,
if I'm studying sensory motor integration

1638
01:28:22,964 --> 01:28:27,402
and I'm measuring motor cortex signals
from neurons and motor cortex,

1639
01:28:27,602 --> 01:28:31,372
and I'm measuring outputs
such as the kinematics,

1640
01:28:31,372 --> 01:28:35,143
the kinetic forces that are generated
and the EMG signals in the periphery.

1641
01:28:35,610 --> 01:28:38,379
I could then use that that context

1642
01:28:38,379 --> 01:28:41,916
to say I could specify that as either 11
or as 01.

1643
01:28:41,916 --> 01:28:46,454
For example, if I didn't think that it
was capable, observing its past history

1644
01:28:46,454 --> 01:28:51,392
or its its decision states in the form
of some kind of a sensory blockade

1645
01:28:52,060 --> 01:28:55,630
related to my experiments
would that be a practical application?

1646
01:28:55,830 --> 01:28:57,799
Does that have face validity

1647
01:28:57,799 --> 01:29:00,635
in this context,
given that I'm measuring those signals.

1648
01:29:01,536 --> 01:29:05,006
One that just collectively flag,
generally speaking, in

1649
01:29:05,006 --> 01:29:08,643
motor control domains you're working
with like continuous quantities, right?

1650
01:29:09,010 --> 01:29:09,677
Right.

1651
01:29:10,144 --> 01:29:12,980
And that's why you use things
like common filters, etc.

1652
01:29:12,980 --> 01:29:13,748
as your models.

1653
01:29:15,383 --> 01:29:15,983
I'm not

1654
01:29:15,983 --> 01:29:19,253
sure to what extent
a part of the observable Markov decision

1655
01:29:19,253 --> 01:29:22,557
process would actually be the appropriate
generative model for the situation.

1656
01:29:22,824 --> 01:29:25,560
I think it would be
if you want a model like the decision

1657
01:29:25,560 --> 01:29:28,029
making processes observed

1658
01:29:29,163 --> 01:29:30,965
for motor control.

1659
01:29:30,965 --> 01:29:32,867
So am I going to move my arms
the left or the right?

1660
01:29:32,867 --> 01:29:36,971
That is a discrete decision,
but actually measuring motor control

1661
01:29:37,038 --> 01:29:40,441
and a lot of those things that you are
talking about, those continuous signals,

1662
01:29:40,742 --> 01:29:44,112
I'm not actually sure you know,
MDP is the appropriate generative model.

1663
01:29:44,445 --> 01:29:48,116
But for example, if I thought that
I had a stage space that consisted of a

1664
01:29:48,483 --> 01:29:52,086
a fixed point or any kind of, you know,
however, an abstracting of those

1665
01:29:52,086 --> 01:29:56,891
continuous processes into the realm of,
you know, a discrete state,

1666
01:29:58,292 --> 01:30:00,294
just glossing over that

1667
01:30:00,762 --> 01:30:04,198
this kind of model
could maybe account for that.

1668
01:30:04,966 --> 01:30:07,468
Or do you use. Me using that word in any
I mean, in

1669
01:30:07,468 --> 01:30:10,471
any case, you know, or
where a signal is technically continuous?

1670
01:30:10,471 --> 01:30:13,808
I mean, you know, the term
to use these sorts of models, right?

1671
01:30:13,808 --> 01:30:17,111
You're going to have to
just pick some quasi arbitrary

1672
01:30:17,111 --> 01:30:21,282
or maybe just by in some sense way
to kind of been, you know, been time

1673
01:30:21,282 --> 01:30:24,852
right after desperate guys time
and some in some way.

1674
01:30:25,319 --> 01:30:28,156
I mean, the signal was X for these

1675
01:30:28,156 --> 01:30:31,392
three milliseconds and now it changed
to Y these next three milliseconds.

1676
01:30:31,392 --> 01:30:33,928
Serves some kind of like funny average
or something like that.

1677
01:30:34,462 --> 01:30:39,367
And I guess I guess
the potential concern here

1678
01:30:39,367 --> 01:30:43,938
is also that like like
there has to be some way to to link the

1679
01:30:44,739 --> 01:30:48,209
the the states as here
as you're kind of bending them

1680
01:30:48,609 --> 01:30:52,313
to some sort of policy selection
process, presumably.

1681
01:30:52,847 --> 01:30:57,418
And so now I'm not completely sure I can
imagine exactly how that would work.

1682
01:30:57,485 --> 01:31:01,255
It's almost like this one is about
the agent deciding which way to walk.

1683
01:31:01,456 --> 01:31:04,025
And then maybe there's
some proprioceptive feedback.

1684
01:31:04,025 --> 01:31:07,228
But let's think about what
this Decision-Making example is

1685
01:31:07,495 --> 01:31:09,330
and then wonder about where the motor

1686
01:31:09,330 --> 01:31:12,433
and where these other modalities
and the continuous signal processing.

1687
01:31:12,500 --> 01:31:14,602
Also interesting,
let's just walk through this

1688
01:31:14,802 --> 01:31:18,039
discrete time
discrete opportunity space model.

1689
01:31:18,039 --> 01:31:21,142
And then we talked about like an active
inferred stream eight

1690
01:31:21,442 --> 01:31:23,377
with all the chants about
what does it look like

1691
01:31:23,377 --> 01:31:27,148
to move active inference
from a discrete into a continuous space.

1692
01:31:27,315 --> 01:31:30,218
So the kinds of things
that are actively being developed

1693
01:31:30,218 --> 01:31:32,553
and it sounds exciting
about the motor example, Max.

1694
01:31:33,688 --> 01:31:35,456
Yeah, I mean, I mean, so there are,

1695
01:31:35,456 --> 01:31:39,393
you know, and you know, Thomas
Parr has done a bunch of this

1696
01:31:39,393 --> 01:31:43,531
like where he's mixed models, right
where the top the top layer is a discrete

1697
01:31:43,531 --> 01:31:47,368
space based Markov decision process model
like this, you know, deciding left arm,

1698
01:31:47,368 --> 01:31:47,735
right arm.

1699
01:31:47,735 --> 01:31:50,071
But then it feeds into a continuous

1700
01:31:50,805 --> 01:31:54,709
state space below that actually controls

1701
01:31:54,709 --> 01:31:57,778
the dynamics of the movement itself
based on the decision.

1702
01:31:58,346 --> 01:32:00,581
And my guess is it would be more

1703
01:32:01,449 --> 01:32:03,684
something more like that would be better.

1704
01:32:04,685 --> 01:32:06,921
You know, that being said,
these sorts of these sorts of models

1705
01:32:06,921 --> 01:32:10,091
are much more about,
you know, behavior in choice.

1706
01:32:10,091 --> 01:32:10,791
I mean, when you're

1707
01:32:10,791 --> 01:32:13,427
talking about neural responses,
I would think that would come more

1708
01:32:13,895 --> 01:32:16,964
be more useful in these models
to use them to predict

1709
01:32:16,964 --> 01:32:21,769
the neural responses that you would get
when a person does

1710
01:32:21,769 --> 01:32:26,307
X versus Y and see if that matches
with the firing rates that you see.

1711
01:32:27,575 --> 01:32:29,010
That is a paper

1712
01:32:29,010 --> 01:32:29,777
mixed model thing

1713
01:32:29,777 --> 01:32:32,947
as a paper, I think it's called
like discrete movements to decisions

1714
01:32:32,947 --> 01:32:35,616
and back again or something
along those lines in your computation.

1715
01:32:36,317 --> 01:32:36,817
Yeah.

1716
01:32:36,817 --> 01:32:39,320
I think it deals with all the
formal issues that can arise with this.

1717
01:32:40,087 --> 01:32:40,755
Yeah.

1718
01:32:41,689 --> 01:32:42,723
Thanks. Okay.

1719
01:32:42,723 --> 01:32:45,293
So just for the sake of time here.

1720
01:32:46,327 --> 01:32:49,764
So. So again, what I did here
and this is for the agent observing

1721
01:32:49,964 --> 01:32:54,068
their own behavior,
all I did was I made it like this, right?

1722
01:32:54,068 --> 01:32:57,572
So this is just saying
when the agent chose

1723
01:32:58,239 --> 01:33:01,275
a state one,

1724
01:33:01,676 --> 01:33:04,912
then it observes the observation
associated with stay one

1725
01:33:04,912 --> 01:33:07,615
right row one with 100% probability

1726
01:33:08,149 --> 01:33:10,651
when it was in the hinge state,

1727
01:33:11,118 --> 01:33:13,788
then observes that it was in the hand
state, wrote to

1728
01:33:13,788 --> 01:33:16,090
within a percent probability
and so forth.

1729
01:33:16,290 --> 01:33:20,661
So it's just providing maximally precise
evidence so that it knows what it did

1730
01:33:22,863 --> 01:33:25,199
So now notice that this was big.

1731
01:33:25,466 --> 01:33:27,902
Right. So that's the generative process.

1732
01:33:27,902 --> 01:33:32,139
If I wanted to
if we wanted to do reward learning right?

1733
01:33:32,139 --> 01:33:36,210
If we wanted it to learn P
when what the reward probabilities are,

1734
01:33:36,877 --> 01:33:39,080
then we'd have to set a little AI here.

1735
01:33:40,147 --> 01:33:43,317
And you might start out
just sort of making big

1736
01:33:43,317 --> 01:33:46,621
a making little equivalent to too big

1737
01:33:47,755 --> 01:33:49,423
to start with.

1738
01:33:50,191 --> 01:33:53,728
And and multiplying

1739
01:33:54,095 --> 01:33:58,232
each of the values
in little by a big number like 200.

1740
01:33:58,966 --> 01:34:01,335
And what that does is it just makes it so

1741
01:34:02,903 --> 01:34:05,039
the agent is really,
really, really confident

1742
01:34:05,473 --> 01:34:07,575
about its beliefs

1743
01:34:09,410 --> 01:34:11,879
in a way that our associated with big.

1744
01:34:13,047 --> 01:34:14,915
And what that does is it just

1745
01:34:14,915 --> 01:34:18,452
prevents learning for anything
you don't want it to learn.

1746
01:34:19,220 --> 01:34:20,821
So if you multiply this thing
by really big number,

1747
01:34:20,821 --> 01:34:23,524
then it won't learn anything,
even though technically it's learning

1748
01:34:23,691 --> 01:34:25,026
because it's just already too confident.

1749
01:34:26,761 --> 01:34:27,228
But then

1750
01:34:27,228 --> 01:34:30,297
what you could do is for anything
that you do want it to learn

1751
01:34:30,965 --> 01:34:34,869
you can just redefine those
with really small numbers, right?

1752
01:34:34,869 --> 01:34:36,570
So you can say a three here.

1753
01:34:36,570 --> 01:34:41,542
So the probability
of choosing of winning choose

1754
01:34:41,542 --> 01:34:45,713
if you choose the left machine,
you know, is point five.

1755
01:34:46,380 --> 01:34:47,882
So just point fives all around.

1756
01:34:47,882 --> 01:34:50,251
Right. Or 0.25 or whatever.

1757
01:34:50,251 --> 01:34:53,220
So this is just saying
that the agent would start with

1758
01:34:54,255 --> 01:34:57,091
completely imprecise beliefs
about whether it would win

1759
01:34:57,091 --> 01:35:00,895
or lose, depending on what it chose,
depending on context.

1760
01:35:01,262 --> 01:35:02,963
So it would. So
this is what you would do.

1761
01:35:02,963 --> 01:35:05,433
You kind of turn on a little
if you wanted it

1762
01:35:05,433 --> 01:35:06,901
to learn the reward probabilities.

1763
01:35:06,901 --> 01:35:09,170
But we're not doing that here.

1764
01:35:09,170 --> 01:35:11,372
So we're just showing this as examples.

1765
01:35:11,372 --> 01:35:13,641
And same thing. If you wanted to,
you could do the same thing.

1766
01:35:13,641 --> 01:35:18,779
But for our commonality, one, to have it
learn like the accuracy of the hint.

1767
01:35:19,680 --> 01:35:21,749
Again, these are just kind of common
to our examples.

1768
01:35:21,749 --> 01:35:23,350
If you wanted to do that kind of thing,

1769
01:35:25,619 --> 01:35:29,323
So now we've defined the the likelihood
matrix, right?

1770
01:35:29,323 --> 01:35:30,191
The Matrix, right?

1771
01:35:30,191 --> 01:35:33,427
What states generate,
what outcomes with what probabilities?

1772
01:35:33,994 --> 01:35:36,597
Now we're moving on in to B,
which are the transition

1773
01:35:36,597 --> 01:35:38,199
probabilities.

1774
01:35:39,600 --> 01:35:41,936
And remember that

1775
01:35:41,936 --> 01:35:45,439
there's one of these per state factor
per action.

1776
01:35:47,141 --> 01:35:50,511
So and this is what I showed
before that for B one.

1777
01:35:50,511 --> 01:35:54,615
So for state of factor one, the context,
there's only one action, quote unquote.

1778
01:35:55,015 --> 01:35:58,652
And it's just an identity matrix
that's just says the last better context

1779
01:35:58,652 --> 01:35:59,820
never changes.

1780
01:35:59,820 --> 01:36:01,355
But then a trial

1781
01:36:03,157 --> 01:36:03,691
and again,

1782
01:36:03,691 --> 01:36:06,827
I showed this before
for the second state factor,

1783
01:36:07,695 --> 01:36:10,698
the transitions as four different
possible transitions

1784
01:36:11,265 --> 01:36:13,934
transitioning from state one
to any of the other states.

1785
01:36:14,235 --> 01:36:17,371
So just to be clear here,
and these transition matrices,

1786
01:36:17,371 --> 01:36:21,142
the column state of the states here
and now and the rows of the states

1787
01:36:21,142 --> 01:36:22,710
that you would move to.

1788
01:36:22,710 --> 01:36:26,046
So this is saying from any state
you could move to state, one

1789
01:36:27,615 --> 01:36:28,849
from any state

1790
01:36:28,849 --> 01:36:31,118
you could move to state to, etc.

1791
01:36:32,386 --> 01:36:34,321
So, you know, having,

1792
01:36:34,321 --> 01:36:38,859
you know, one, two, three,
four different actions

1793
01:36:38,859 --> 01:36:41,362
for that state fact
or four different possible transitions

1794
01:36:44,131 --> 01:36:46,567
So now the next thing that we do is

1795
01:36:48,903 --> 01:36:52,840
we move to the preference distribution.

1796
01:36:52,840 --> 01:36:54,074
So that's C, right?

1797
01:36:54,074 --> 01:36:57,077
So this is what the agent wants or what
the agent finds

1798
01:36:57,077 --> 01:36:59,547
rewarding
and how rewarding the agent finds it.

1799
01:37:00,648 --> 01:37:03,918
So to do that, initially,
we just with no, we just say

1800
01:37:03,918 --> 01:37:06,954
what are the different number of outcomes
for each outcome modality.

1801
01:37:07,354 --> 01:37:10,558
And that just looks at the size
of the row

1802
01:37:10,558 --> 01:37:13,727
dimensions in a.

1803
01:37:13,727 --> 01:37:15,963
So if I do that

1804
01:37:15,963 --> 01:37:18,632
and again,
this is just using this in a size

1805
01:37:19,300 --> 01:37:21,435
function, which is just again, to make it

1806
01:37:21,902 --> 01:37:24,271
more convenient or generalizable.

1807
01:37:24,605 --> 01:37:28,008
So if I do that then
and is going to look like that, that's

1808
01:37:28,008 --> 01:37:31,912
just saying I have three I have three
different outcome modalities.

1809
01:37:32,379 --> 01:37:36,250
One is no hair, no no hint and hand

1810
01:37:36,350 --> 01:37:39,653
y is null lose and when.

1811
01:37:40,020 --> 01:37:43,090
And one is the agent
observing its own behavior.

1812
01:37:43,090 --> 01:37:44,892
Right. Observing
action. One, two, three or four.

1813
01:37:46,227 --> 01:37:48,195
And can I can I ask me how

1814
01:37:48,195 --> 01:37:52,466
many people have asked in the past weeks
about dimensionality of

1815
01:37:53,267 --> 01:37:54,168
not to go into the mark

1816
01:37:54,168 --> 01:37:57,638
of blanket discussion,
but the dimensionality of observations

1817
01:37:58,038 --> 01:38:02,810
and how we talk about different kinds
of outcomes, different kinds of sensors,

1818
01:38:02,810 --> 01:38:03,811
for example.

1819
01:38:03,811 --> 01:38:07,915
And it's almost looking like it's
just about the way it's specified.

1820
01:38:08,115 --> 01:38:12,152
So the hints could be in your ear
and the observation

1821
01:38:12,152 --> 01:38:13,454
could be visual or something.

1822
01:38:13,454 --> 01:38:15,089
And so you could get philosophical

1823
01:38:15,089 --> 01:38:17,892
and ask whether there's really one
or two different sensory modalities.

1824
01:38:17,892 --> 01:38:21,795
But in the context of this
inferential model that we're writing,

1825
01:38:22,029 --> 01:38:23,931
it's almost like those things
don't really matter.

1826
01:38:23,931 --> 01:38:26,400
They're just variables that are observed.

1827
01:38:27,601 --> 01:38:28,035
Yeah.

1828
01:38:28,035 --> 01:38:30,938
I mean, so again, we call this model
like outcome modalities.

1829
01:38:30,938 --> 01:38:32,539
So in this case there's three, right?

1830
01:38:32,539 --> 01:38:33,874
So like you said, the hint

1831
01:38:33,874 --> 01:38:36,877
this could be the auditory modality
and there's three things I could hear.

1832
01:38:37,244 --> 01:38:40,214
This could be the visual modality
and it's the three,

1833
01:38:40,514 --> 01:38:44,518
whether it wins or loses or hasn't
seen that yet, or this can be visual

1834
01:38:44,919 --> 01:38:47,788
and this four thing, the third
one could be appropriate ception, right?

1835
01:38:47,788 --> 01:38:50,090
It could be observing
or feeling what it did.

1836
01:38:50,090 --> 01:38:52,092
I mean, in this case, it's
probably also somewhat visual, right?

1837
01:38:52,092 --> 01:38:52,860
It observes what it does.

1838
01:38:52,860 --> 01:38:55,796
But you got the point yeah. Really? Yeah.

1839
01:38:55,829 --> 01:39:00,701
So there's just a nice factorization
dimensionality for each modality.

1840
01:39:00,734 --> 01:39:01,001
Yeah.

1841
01:39:03,804 --> 01:39:06,807
But okay, so now I have

1842
01:39:07,207 --> 01:39:13,314
what I've done is I've started out
by just putting zeros in the C matrix,

1843
01:39:13,314 --> 01:39:16,717
the preference distribution
for each outcome modality.

1844
01:39:17,217 --> 01:39:19,320
So that's just saying for outcome
modality one.

1845
01:39:19,320 --> 01:39:24,158
So the hint the agent has no preferences
and I should say the,

1846
01:39:24,191 --> 01:39:28,262
the rows or the observations for the,
you know, no hint, no hint.

1847
01:39:28,829 --> 01:39:30,397
I'm going to columns or time.

1848
01:39:30,397 --> 01:39:33,600
So as the saying at time one,
the agent doesn't care

1849
01:39:33,834 --> 01:39:36,704
whether it gets the hint or not
in terms of rewarding this

1850
01:39:37,404 --> 01:39:39,473
same thing
at time to same thing at times three.

1851
01:39:39,873 --> 01:39:42,776
And I've just done that for all three
outcome

1852
01:39:42,776 --> 01:39:45,412
modalities to start with.

1853
01:39:45,813 --> 01:39:47,214
The only thing that we want

1854
01:39:47,214 --> 01:39:50,117
to add preferences for

1855
01:39:50,117 --> 01:39:52,953
is the win lose observations.

1856
01:39:54,521 --> 01:39:56,490
So in this case, so that's C

1857
01:39:56,490 --> 01:40:00,027
for outcome modality
to which is the win loss thing here.

1858
01:40:00,594 --> 01:40:03,030
So what I've done again
for Generalizability

1859
01:40:03,130 --> 01:40:06,567
is I've set this parameter
le that it's called loss aversion.

1860
01:40:07,167 --> 01:40:09,570
And I have set this parameter RC,
which is that

1861
01:40:09,570 --> 01:40:13,741
reward seeking thing,
which I just defined above as our S1.

1862
01:40:13,741 --> 01:40:14,808
And again, it's just convenient.

1863
01:40:14,808 --> 01:40:17,011
So I don't have to go down
here, reset it every time

1864
01:40:19,113 --> 01:40:21,648
so I define these

1865
01:40:21,749 --> 01:40:23,851
just for again, just for convenience.

1866
01:40:23,851 --> 01:40:28,322
And then what I say here is
so the observations again are null

1867
01:40:28,322 --> 01:40:32,459
is when this is at time zero,
the agent doesn't care

1868
01:40:32,459 --> 01:40:34,695
whether it observes a knowledge
loss or a win, and that's

1869
01:40:34,695 --> 01:40:39,333
just because it can't observe anything
but a null and at time too,

1870
01:40:39,833 --> 01:40:45,105
it just prefers losing with
a value of again, -1 here

1871
01:40:46,407 --> 01:40:49,443
for both time, two and time three.

1872
01:40:49,443 --> 01:40:54,281
But here's a little bit of a trick
is that I define the preference

1873
01:40:54,281 --> 01:40:58,652
for winning as our set time too.

1874
01:40:58,652 --> 01:41:01,789
And remember that R's is for right now
because that's what they set it above

1875
01:41:02,322 --> 01:41:02,856
their time.

1876
01:41:02,856 --> 01:41:05,793
Three I set the preference
as our as divided by two

1877
01:41:07,895 --> 01:41:10,164
so what that ultimately means

1878
01:41:10,164 --> 01:41:11,832
is that

1879
01:41:13,500 --> 01:41:16,203
the this state factor

1880
01:41:16,203 --> 01:41:19,606
will look like this or I mean sorry
this this preference distribution

1881
01:41:19,606 --> 01:41:23,510
will look like this saying
it dislikes losing equal at H time,

1882
01:41:23,510 --> 01:41:27,414
but it once
it gets more reward for winning at time

1883
01:41:27,414 --> 01:41:30,150
to then at time three

1884
01:41:30,484 --> 01:41:32,653
and so that's our
that's our preference distribution

1885
01:41:34,521 --> 01:41:36,990
And this is just all that's basically.

1886
01:41:36,990 --> 01:41:42,262
It this is all in how the details
of how the sketched out scenario okay $4

1887
01:41:42,262 --> 01:41:45,732
but then you got to wait 20 minutes
and however your scenario is

1888
01:41:46,100 --> 01:41:50,304
this is about the detail in matrix
form of how the payoffs work

1889
01:41:50,604 --> 01:41:54,641
and so it could be -1 or you might ask
if another number is appropriate there.

1890
01:41:55,142 --> 01:41:59,012
But it's kind of like that's
the detail level, that's the applications

1891
01:41:59,012 --> 01:42:00,781
and that's where we hope to see
a lot of people

1892
01:42:00,781 --> 01:42:04,251
exploring different scenarios
so that we can understand better.

1893
01:42:05,252 --> 01:42:05,886
Yeah.

1894
01:42:05,886 --> 01:42:10,257
And so and so you'll see that
when we actually do the like fitting

1895
01:42:10,390 --> 01:42:12,359
like parameter fitting to behavior,

1896
01:42:12,359 --> 01:42:14,962
you know, one thing we'll do
is we'll fit this, right?

1897
01:42:14,962 --> 01:42:17,131
So for some people, Rs
might be like eight.

1898
01:42:17,297 --> 01:42:20,400
And for other people
it might be like two.

1899
01:42:20,400 --> 01:42:23,270
So you find this value for each person

1900
01:42:23,270 --> 01:42:26,240
the best explains or reproduces
their behavior.

1901
01:42:27,808 --> 01:42:29,710
I mean, you gave that example
last week, right?

1902
01:42:29,710 --> 01:42:33,147
In the substance of this example
where actually has really meaningful

1903
01:42:33,147 --> 01:42:35,782
all of that parameter
has a really meaningful interpretation.

1904
01:42:37,718 --> 01:42:39,419
Yeah, exactly.

1905
01:42:39,419 --> 01:42:42,823
Because it has to do with
in the case of an Explorer exploit task,

1906
01:42:43,190 --> 01:42:47,361
the lower RSS basically means
the more people, the more like

1907
01:42:47,461 --> 01:42:51,865
someone is driven to explore,
you know, before they kind of jump

1908
01:42:51,865 --> 01:42:54,835
to conclusions about which one's the best
and just keep choosing that one.

1909
01:42:55,102 --> 01:42:56,036
It makes me

1910
01:42:56,036 --> 01:42:59,907
it makes me think about maybe in the
substance example or another example.

1911
01:42:59,907 --> 01:43:01,208
It's like if we're going to

1912
01:43:01,208 --> 01:43:05,012
have a society that rewards good behavior
and punishes negative behavior

1913
01:43:05,012 --> 01:43:06,813
or something on some issue,

1914
01:43:06,813 --> 01:43:09,883
do you go with here's
how much you get positive and here's

1915
01:43:09,883 --> 01:43:13,520
how much the punishment should be
should be small punishment versus large.

1916
01:43:13,720 --> 01:43:14,821
These kinds of questions,

1917
01:43:14,821 --> 01:43:17,691
which are really important
at the individual and the group level,

1918
01:43:18,091 --> 01:43:20,527
not that this is even close
to the answer,

1919
01:43:20,727 --> 01:43:23,230
but it's almost like it's a way
for us to start talking about

1920
01:43:23,363 --> 01:43:27,134
how much do we value and how much do
individuals need different blends

1921
01:43:27,134 --> 01:43:28,969
of these different attributes.

1922
01:43:28,969 --> 01:43:31,238
So really a good way to talk about this.

1923
01:43:32,272 --> 01:43:34,174
Yeah, absolutely.

1924
01:43:34,274 --> 01:43:37,110
You know, and so like

1925
01:43:37,110 --> 01:43:40,214
oh, so one thing that I did want to
clarify here is you'll notice

1926
01:43:40,214 --> 01:43:42,583
that, you know, these are supposed to be
probability distributions, right?

1927
01:43:42,849 --> 01:43:46,086
See, you know,
each column here is supposed to be

1928
01:43:46,520 --> 01:43:48,522
something that adds up to up to one.

1929
01:43:48,522 --> 01:43:50,657
And technically,
we work with log probabilities.

1930
01:43:51,091 --> 01:43:53,760
And so,

1931
01:43:53,760 --> 01:43:56,663
you know, -1 and four and -1
and two obviously don't

1932
01:43:57,798 --> 01:44:00,167
aren't probabilities
or log probabilities.

1933
01:44:00,934 --> 01:44:06,039
So in practice,
this each column here is soft maxed

1934
01:44:06,039 --> 01:44:09,743
and then each element is logged,

1935
01:44:09,743 --> 01:44:12,679
which is which is how you end up
with the actual log probability

1936
01:44:12,679 --> 01:44:16,383
distributions that define the preferences
when they're actually used in the code.

1937
01:44:17,918 --> 01:44:20,520
And I show that I show
an example of the numbers

1938
01:44:20,520 --> 01:44:24,057
they turn into in the in
or we show them in the in the paper

1939
01:44:26,126 --> 01:44:28,028
so you know, last kind

1940
01:44:28,028 --> 01:44:32,199
of really central thing
is to define the policies.

1941
01:44:32,866 --> 01:44:35,569
So here I'm just going to use NPE here

1942
01:44:35,569 --> 01:44:38,071
to define the number of policies
I want to allow.

1943
01:44:38,639 --> 01:44:41,441
And here
the next thing is just the number

1944
01:44:41,441 --> 01:44:44,278
of actual state factors

1945
01:44:44,444 --> 01:44:46,446
and that will just allow me

1946
01:44:46,446 --> 01:44:50,117
in a generalizable way to say that V

1947
01:44:50,784 --> 01:44:54,154
as a T -1 rows, right?

1948
01:44:54,154 --> 01:44:55,656
So it's going to be

1949
01:44:56,223 --> 01:44:58,292
the number of actions minus

1950
01:44:58,492 --> 01:45:00,827
the number of times
because you have to move from time

1951
01:45:00,927 --> 01:45:02,229
to time to time to the time three.

1952
01:45:02,229 --> 01:45:07,601
So there's always one less action
possibility than there are times

1953
01:45:07,601 --> 01:45:09,002
at a trial.

1954
01:45:09,336 --> 01:45:11,805
And the number of policies, again,
is going to be columns.

1955
01:45:11,805 --> 01:45:14,174
A number of state factors
is going to be that third dimension.

1956
01:45:14,775 --> 01:45:17,210
So I can start by
just a finding that as ones

1957
01:45:18,979 --> 01:45:21,448
and I can keep it that way.

1958
01:45:21,448 --> 01:45:23,684
I just wrote this explicitly,
so you can see it.

1959
01:45:23,684 --> 01:45:24,785
And this just means again,

1960
01:45:24,785 --> 01:45:27,321
it can only choose
to stay in the same context every time.

1961
01:45:27,321 --> 01:45:29,589
So that's not something
the agent knows or can control.

1962
01:45:30,324 --> 01:45:34,628
Whereas for today, factor two, I can
define each of the as possible policies

1963
01:45:34,628 --> 01:45:38,231
stay in the start state, take the hint
and then choose the left machine.

1964
01:45:38,432 --> 01:45:41,134
You know, take the hand,
choose the right machine,

1965
01:45:41,134 --> 01:45:44,137
choose the left machine
and then go back to start user emission.

1966
01:45:44,137 --> 01:45:45,439
Now back to start.

1967
01:45:45,439 --> 01:45:48,175
And I know describe what
each of those means in this kind of one,

1968
01:45:48,175 --> 01:45:50,210
two, three, four, five can get here

1969
01:45:51,445 --> 01:45:54,848
so then if we want to
and I'm not going to do this here,

1970
01:45:55,615 --> 01:45:59,286
you can specify E
which is the kind of like habits,

1971
01:45:59,820 --> 01:46:00,420
you know, essentially

1972
01:46:00,420 --> 01:46:03,623
just apply over policies
that biases you toward choosing one thing

1973
01:46:03,623 --> 01:46:05,792
versus another. And just one.

1974
01:46:05,792 --> 01:46:06,760
And this is.

1975
01:46:06,760 --> 01:46:09,096
Run as in the previous active
inference streams.

1976
01:46:09,096 --> 01:46:11,431
We talked about that
as the field of affordances

1977
01:46:11,598 --> 01:46:15,235
for those who are connecting it
to the ecological psychology or to the

1978
01:46:15,569 --> 01:46:16,336
in activism

1979
01:46:18,905 --> 01:46:21,241
oh. You mean policy.

1980
01:46:21,241 --> 01:46:23,643
Oh. Yeah, sure. Understand that.

1981
01:46:23,910 --> 01:46:28,115
The prior the prior on what can be done
among policies

1982
01:46:28,482 --> 01:46:31,084
is the affordances
of the organism in their niche.

1983
01:46:31,451 --> 01:46:32,185
So if you don't have

1984
01:46:32,185 --> 01:46:35,155
the object, it's not going to be a policy
under consideration.

1985
01:46:35,322 --> 01:46:38,358
And so e is this mathematical device

1986
01:46:38,592 --> 01:46:40,961
that's going to be weighting policies

1987
01:46:41,428 --> 01:46:44,664
by their habit basically.

1988
01:46:45,031 --> 01:46:47,200
Excellence is a habit,
that kind of thing.

1989
01:46:50,103 --> 01:46:50,537
Yeah.

1990
01:46:50,537 --> 01:46:54,708
I mean, I guess like,
I mean technically what you're doing is,

1991
01:46:54,708 --> 01:46:58,078
you know, like the when this makes sense
functionally is when you're learning it.

1992
01:46:58,745 --> 01:47:01,114
So, so what you would do with little E

1993
01:47:02,249 --> 01:47:05,085
and so this might start out,
you know, flat,

1994
01:47:05,085 --> 01:47:08,054
right with just counts of one
for the duration distribution

1995
01:47:08,955 --> 01:47:13,226
but if you choose the same policy
20 times, let's say you choose policy

1996
01:47:13,226 --> 01:47:18,398
three 20 times, then this one,
this third one here would become a 20.

1997
01:47:19,132 --> 01:47:22,602
And, and so that just means
that you'd start out with a strong bias

1998
01:47:22,602 --> 01:47:27,541
now for choosing policy free because
you've chosen it a bunch of times before.

1999
01:47:28,508 --> 01:47:30,811
And that's an optimal

2000
01:47:30,811 --> 01:47:34,881
thing to do under the assumption
that option three was

2001
01:47:34,881 --> 01:47:38,318
the one that continued to have the lowest
expected free energy every time.

2002
01:47:38,351 --> 01:47:38,618
Right.

2003
01:47:38,618 --> 01:47:42,422
So if in this model based way,
based on minimizing spectra for energy,

2004
01:47:42,856 --> 01:47:45,459
you succeeded over and over again
when you chose option three

2005
01:47:45,926 --> 01:47:48,528
that it makes sense
to build up this kind of bias

2006
01:47:48,929 --> 01:47:52,666
and so that you don't actually
have to work that hard cognitively

2007
01:47:52,666 --> 01:47:54,067
to just keep doing the thing.
That's right.

2008
01:47:54,067 --> 01:47:57,270
Once you build up a prior
that it's going to keep being right

2009
01:47:57,270 --> 01:47:59,272
So I guess I can see how

2010
01:47:59,906 --> 01:48:04,177
if you do the same sorts of things
in a given niche over and over again,

2011
01:48:04,177 --> 01:48:05,579
then you develop priors

2012
01:48:05,579 --> 01:48:08,315
that bias you toward doing the things
that are successful in that niche.

2013
01:48:08,415 --> 01:48:10,617
And if something
if something's never been

2014
01:48:10,617 --> 01:48:12,052
if something's ever been observed,

2015
01:48:12,052 --> 01:48:15,789
then it's unlikely it takes a different
mode of operation outside this script.

2016
01:48:15,889 --> 01:48:18,391
If something's never been observed
for it to be considered.

2017
01:48:19,826 --> 01:48:22,829
This is just a basic they know the
that's why you initiate

2018
01:48:22,829 --> 01:48:27,167
with a non-zero value
because it's something that could happen.

2019
01:48:28,201 --> 01:48:29,236
Yeah, yeah, yeah.

2020
01:48:29,236 --> 01:48:29,703
That's true.

2021
01:48:29,703 --> 01:48:32,806
If you set a zero anywhere
and in E, then it's, it's

2022
01:48:32,806 --> 01:48:35,942
as though they don't have that option
yeah, that's true.

2023
01:48:36,877 --> 01:48:39,579
One other thing to say is just like

2024
01:48:39,579 --> 01:48:43,250
agents can get stuck
if you have non stationary environments.

2025
01:48:43,250 --> 01:48:47,287
So say you have 100 trials and one thing
and then you switch

2026
01:48:48,455 --> 01:48:50,991
contexts,
the agent will probably be stuck

2027
01:48:53,360 --> 01:48:56,463
and so I know it's useful
with all of these parameters.

2028
01:48:56,730 --> 01:49:00,300
They have kind of
we can have our favorite interpretations

2029
01:49:00,300 --> 01:49:02,369
related to like ecological psychology
or whatever,

2030
01:49:02,369 --> 01:49:06,206
but I think it's always useful to kind of
consider them functionally and use

2031
01:49:06,206 --> 01:49:06,907
them kind of

2032
01:49:07,941 --> 01:49:09,175
appropriately for the city.

2033
01:49:09,175 --> 01:49:11,611
So whatever situation is your modeling?

2034
01:49:11,611 --> 01:49:13,079
Yeah, yeah.

2035
01:49:13,079 --> 01:49:16,283
That's the thing is, is, you know, the,
the most useful places with this

2036
01:49:16,283 --> 01:49:19,953
and say like computational psychiatry
are that if you have the right set

2037
01:49:19,953 --> 01:49:24,090
of experiences and these models
and you get stuck doing

2038
01:49:24,090 --> 01:49:26,927
what's technically Bayes optimal
given your past experience,

2039
01:49:27,360 --> 01:49:30,664
but that like makes
you really, really maladaptive

2040
01:49:30,664 --> 01:49:35,335
and what you're doing in the new context
that you're, you're in, right?

2041
01:49:35,368 --> 01:49:36,436
So you can get stuck in really

2042
01:49:36,436 --> 01:49:40,307
maladaptive places in parameter space
that could lead to symptoms

2043
01:49:40,307 --> 01:49:42,842
that look like psychiatric symptoms,
for example.

2044
01:49:43,443 --> 01:49:46,680
And so, I mean, this is a kind of idea,
you know,

2045
01:49:46,680 --> 01:49:50,083
like filtering back as a paper
on this as like, you know,

2046
01:49:51,518 --> 01:49:54,321
optimal inference
and suboptimal models, I think. But

2047
01:49:55,589 --> 01:49:57,290
the whole
point is you're doing the optimal thing,

2048
01:49:57,290 --> 01:49:58,892
but your model is wrong.

2049
01:49:58,892 --> 01:50:00,694
Ryan, outside of the.

2050
01:50:00,694 --> 01:50:02,896
Optimal thing, under the assumption
that your model is right.

2051
01:50:03,363 --> 01:50:05,899
Just outside of this model stream,

2052
01:50:05,899 --> 01:50:08,702
which is really amazing
and I hope it will continue.

2053
01:50:08,935 --> 01:50:10,170
We would also like to feature

2054
01:50:10,170 --> 01:50:14,174
these kinds of direct discussions
about the computational psychiatry.

2055
01:50:14,474 --> 01:50:17,210
So this is awesome with a walk through in
the code is going to be helpful

2056
01:50:17,210 --> 01:50:19,012
for a lot of people
who are learning that.

2057
01:50:19,012 --> 01:50:22,649
And then let's table and find the right
time to talk about these issues

2058
01:50:22,649 --> 01:50:26,219
because it's really awesome
what you're describing.

2059
01:50:26,219 --> 01:50:26,686
Yeah.

2060
01:50:26,686 --> 01:50:30,323
You know, I mean, obviously computational
psychiatry is what I do in breakfast.

2061
01:50:30,323 --> 01:50:31,791
Right? So, yeah, I'd love to.

2062
01:50:31,791 --> 01:50:36,663
And okay, so finally there's a bunch
of kind of additional parameters, right?

2063
01:50:36,730 --> 01:50:41,401
So we talk about learning rate,
which here we're just setting 2.5

2064
01:50:41,401 --> 01:50:45,405
for, you know, arbitrarily and beta.

2065
01:50:45,405 --> 01:50:48,174
This is the expected
free energy of precision.

2066
01:50:48,708 --> 01:50:53,079
So it, it controls
essentially how confident the agent is in

2067
01:50:54,214 --> 01:50:56,916
its expected free energy estimates.

2068
01:50:56,916 --> 01:50:59,486
And so if this value is low

2069
01:50:59,719 --> 01:51:03,490
or sorry, if this value is high,
that means that the agent

2070
01:51:03,490 --> 01:51:06,493
is really unconfident
and it's expected free energy estimates.

2071
01:51:07,293 --> 01:51:09,896
And when that's the case, then behavior

2072
01:51:09,896 --> 01:51:12,032
will be a lot more random

2073
01:51:12,966 --> 01:51:15,001
unless the agent has strong habits.

2074
01:51:15,602 --> 01:51:18,138
If agent has strong habits,
then having a high beta value

2075
01:51:18,138 --> 01:51:21,908
means that the habits will have
a much stronger influence on on policy

2076
01:51:22,342 --> 01:51:25,078
and ultimate policy selection.

2077
01:51:25,078 --> 01:51:29,115
And we actually show this is the thing
that I was talking about.

2078
01:51:29,115 --> 01:51:30,450
It's kind of like a

2079
01:51:30,450 --> 01:51:34,988
and random exploration sort of thing
where it's actually updated over time.

2080
01:51:35,321 --> 01:51:38,391
So with each observation, the agent
essentially updates its confidence

2081
01:51:38,892 --> 01:51:42,729
in its expected free energy estimates

2082
01:51:43,263 --> 01:51:46,566
and then Alpha is kind of like a standard

2083
01:51:46,566 --> 01:51:49,969
what are called inverse temperature
parameters and where it just kind of

2084
01:51:49,969 --> 01:51:54,340
controls randomness and action selection
so once you have chosen a policy,

2085
01:51:54,841 --> 01:51:59,012
then that policy might say
this action is better than that action,

2086
01:51:59,012 --> 01:52:01,314
but occasionally
the agent might still choose

2087
01:52:02,082 --> 01:52:03,683
a different action kind of randomly.

2088
01:52:04,818 --> 01:52:07,454
And this is actually

2089
01:52:07,721 --> 01:52:10,156
again, this is technically
like a suboptimal thing, right?

2090
01:52:10,156 --> 01:52:13,793
You ought to just choose the best thing
but this is actually quite important

2091
01:52:13,793 --> 01:52:17,230
to fit in practice
because in actual human behavior,

2092
01:52:18,631 --> 01:52:21,434
there tends to be
a notable amount of randomness

2093
01:52:21,434 --> 01:52:24,938
and so you need to actually fit something
like Alpha to get a model

2094
01:52:24,938 --> 01:52:27,240
that fits behavior well.

2095
01:52:28,742 --> 01:52:32,078
Now, these other two things
I'll probably skip over, largely

2096
01:52:32,078 --> 01:52:34,981
because they only have to do
with the neural process theory primarily.

2097
01:52:35,515 --> 01:52:39,953
But this ERP thing,
it just controls how much beliefs reset

2098
01:52:39,953 --> 01:52:43,356
at each time
point with respect to the modeled

2099
01:52:45,458 --> 01:52:46,593
firing rate changes.

2100
01:52:46,593 --> 01:52:50,964
So it basically controls how much priors
at one time point carry over

2101
01:52:52,265 --> 01:52:56,336
at the next time point with respect
to the neural process theory.

2102
01:52:57,670 --> 01:52:58,471
And then

2103
01:52:58,471 --> 01:53:01,841
Tao is basically looking evidence
accumulation, right?

2104
01:53:01,841 --> 01:53:04,944
So that controls how quickly
you update your beliefs

2105
01:53:04,944 --> 01:53:06,980
based on new observations and

2106
01:53:08,081 --> 01:53:11,551
as attached to the
the neural process theory.

2107
01:53:12,485 --> 01:53:13,086
Yeah.

2108
01:53:13,586 --> 01:53:14,754
Anyone digs into the code.

2109
01:53:14,754 --> 01:53:17,390
Is this a time
constant on gradient descent?

2110
01:53:17,390 --> 01:53:17,924
Yeah.

2111
01:53:20,794 --> 01:53:23,830
So and I go through a couple
of other ones here that I explain,

2112
01:53:23,830 --> 01:53:26,599
but I'm going to kind of skip
over those for now.

2113
01:53:27,600 --> 01:53:30,870
So then that's basically a
if we wanted to, we get a hard code

2114
01:53:30,870 --> 01:53:32,639
what the states,
what the initial states are

2115
01:53:32,639 --> 01:53:36,276
and what the observations are like that
by setting SNL.

2116
01:53:36,676 --> 01:53:39,045
But typically what we will do is
we won't do that.

2117
01:53:39,045 --> 01:53:43,583
We'll just let the generative process
generate the observations itself,

2118
01:53:44,284 --> 01:53:45,285
which is probabilistic.

2119
01:53:45,285 --> 01:53:49,522
So it'll generate with whatever
the probability is of the over

2120
01:53:49,522 --> 01:53:52,826
D what states are actually
the true states in that context.

2121
01:53:52,826 --> 01:53:54,594
And therefore the
observations get generated.

2122
01:53:56,129 --> 01:53:58,364
And so the last thing and again,
I already covered

2123
01:53:58,364 --> 01:54:02,836
this is you just stick all these things
you defined into this MDP, right?

2124
01:54:04,170 --> 01:54:07,207
And we allow a little data, which means
we're going to allow it to learn

2125
01:54:07,507 --> 01:54:09,776
whether the left or the right
context is more likely.

2126
01:54:10,310 --> 01:54:12,712
We set all these things,
you know, the learning rate

2127
01:54:13,079 --> 01:54:16,149
and the action precision
or inverse temperature.

2128
01:54:16,182 --> 01:54:19,519
So Alpha etc.

2129
01:54:19,519 --> 01:54:22,322
We're going to leave out other things
we could have defined

2130
01:54:22,322 --> 01:54:25,558
like E or like learning A or learning

2131
01:54:25,558 --> 01:54:28,528
B or learning C or learning A, etc.

2132
01:54:28,628 --> 01:54:31,631
We're not going to define
the states ahead of time

2133
01:54:32,165 --> 01:54:33,066
and we're not going to define

2134
01:54:33,066 --> 01:54:36,402
some of these additional parameters
that we could include

2135
01:54:36,402 --> 01:54:38,471
and then we can just kind of

2136
01:54:38,504 --> 01:54:44,377
do some nice labeling here
just so we label what the contexts

2137
01:54:45,345 --> 01:54:46,279
mean right?

2138
01:54:46,279 --> 01:54:48,348
The semantics,
we're laying on it for the figures

2139
01:54:49,449 --> 01:54:51,084
and that's it.

2140
01:54:52,285 --> 01:54:53,820
And then once we do that,

2141
01:54:53,820 --> 01:54:57,557
then we can use this little check
script here

2142
01:54:57,557 --> 01:55:00,693
just to make sure we didn't mess
anything up in building the model.

2143
01:55:00,693 --> 01:55:02,161
And it'll kind of nicely

2144
01:55:02,161 --> 01:55:04,797
tell you or give you a hint anyway
about what you might have messed up.

2145
01:55:06,199 --> 01:55:10,403
And then this script that I mentioned
before

2146
01:55:10,436 --> 01:55:13,172
that we provide is the one
that actually runs the simulation.

2147
01:55:14,240 --> 01:55:16,809
And then finally, once we have

2148
01:55:17,310 --> 01:55:20,413
the simulations run,
which will be stored on this big MBP,

2149
01:55:20,914 --> 01:55:23,249
then we can use these plotting scripts

2150
01:55:23,616 --> 01:55:26,252
to actually show the results.

2151
01:55:27,387 --> 01:55:30,223
And if I set them equals one, then it's

2152
01:55:30,223 --> 01:55:33,960
going to do that it's just going to
do a single trial simulation,

2153
01:55:34,961 --> 01:55:38,031
which I'm going to do now.

2154
01:55:38,031 --> 01:55:40,266
So I'll just let the thing run
the rest of the way.

2155
01:55:40,266 --> 01:55:42,669
So I want I'll get rid of these stoppers.

2156
01:55:43,236 --> 01:55:46,005
Well, just to give a quick
let's give a coding recap

2157
01:55:46,205 --> 01:55:48,875
just to where we are
at the end of the second session

2158
01:55:49,309 --> 01:55:50,977
while you're going
through the stop signs.

2159
01:55:50,977 --> 01:55:55,448
So in the first session, we talked a lot
about the basis of this paper

2160
01:55:55,648 --> 01:55:57,450
and the rationale,
and we talked a little bit more

2161
01:55:57,450 --> 01:55:58,551
about who might be interested

2162
01:55:58,551 --> 01:56:00,820
or what kinds of experiments
it was adjacent to

2163
01:56:00,820 --> 01:56:01,888
and then today

2164
01:56:01,888 --> 01:56:05,692
that was awesome with the code
because we went through a full definition

2165
01:56:05,959 --> 01:56:08,761
and we moved from that sort
of analytical definitions

2166
01:56:08,761 --> 01:56:10,296
that we went through
in the first session.

2167
01:56:10,296 --> 01:56:12,799
Into seeing how the realized in the code.

2168
01:56:12,966 --> 01:56:13,900
And for me

2169
01:56:13,900 --> 01:56:16,703
at least, it was really helpful to see
how a lot of these things were put down.

2170
01:56:17,103 --> 01:56:20,506
And then we got all the way up to line
six 31 or something,

2171
01:56:20,506 --> 01:56:22,442
but could be different
in a little different version,

2172
01:56:22,442 --> 01:56:24,644
but we got to
the definition of the full model

2173
01:56:24,811 --> 01:56:28,681
and then the call out from that object
to another script, and then we're going

2174
01:56:28,681 --> 01:56:33,086
to be storing all the outputs
in this kind of big container called MDP.

2175
01:56:33,186 --> 01:56:36,289
So today
we went from zero to 60 with the code

2176
01:56:36,622 --> 01:56:39,993
getting all these things run
through and defined and operated on.

2177
01:56:39,993 --> 01:56:41,260
Now we're going to look at the output

2178
01:56:41,260 --> 01:56:42,862
and then it sounds like in future
sessions

2179
01:56:42,862 --> 01:56:45,732
we're going to be fitting parameters
and maybe doing a few other things.

2180
01:56:47,100 --> 01:56:48,034
Yeah, and future stuff.

2181
01:56:48,034 --> 01:56:52,005
Well, I mean, it sounds like maybe
we're going to wait to do

2182
01:56:52,105 --> 01:56:54,707
to show learning next time,
I guess, depending on the amount of time.

2183
01:56:54,707 --> 01:56:59,679
But yeah, next time
then we'd be in a position to do

2184
01:57:00,513 --> 01:57:02,615
actual code for learning.

2185
01:57:02,615 --> 01:57:05,284
We're planning
on covering the neural process theory

2186
01:57:05,284 --> 01:57:07,387
and building hierarchical models

2187
01:57:08,521 --> 01:57:12,558
and then the last thing is
you actually fitting models to data. So.

2188
01:57:13,059 --> 01:57:15,828
So yeah, we'll see how much of that
we can get through.

2189
01:57:16,429 --> 01:57:21,000
But okay, so just to finish here,
so I got rid of all my little stoppers,

2190
01:57:21,000 --> 01:57:24,871
so now I'm just going to click Continue
and it's going to run Samuel's one

2191
01:57:25,838 --> 01:57:30,243
and what I'm going to get out of
this is a little plot like this.

2192
01:57:30,243 --> 01:57:33,413
And also there'll be a neural neural
process that you want to under here.

2193
01:57:33,413 --> 01:57:35,481
But I'm going to ignore this for now
because we're not talking

2194
01:57:35,481 --> 01:57:36,849
about the neural
process theory right now.

2195
01:57:38,551 --> 01:57:42,055
So what this
top one here is, is it's hidden states.

2196
01:57:42,055 --> 01:57:45,124
So contexts now black equals

2197
01:57:45,124 --> 01:57:48,361
a high, probably higher probability
of white equals a low probability.

2198
01:57:48,361 --> 01:57:50,730
And so there's, you know,
the gray is kind of in between.

2199
01:57:50,797 --> 01:57:51,397
Right?

2200
01:57:51,664 --> 01:57:55,201
So this is saying
and these are all posteriors

2201
01:57:55,201 --> 01:57:58,237
over states at the end of a trial
so that's important.

2202
01:57:58,237 --> 01:58:02,642
So what this is saying
is at the end of the trial at Time three,

2203
01:58:03,076 --> 01:58:05,711
these are the beliefs
that the agent has about

2204
01:58:05,878 --> 01:58:08,848
what state
it was in across all these timestamps.

2205
01:58:10,116 --> 01:58:11,984
So this is saying
at the end of the trial,

2206
01:58:11,984 --> 01:58:16,155
the agent was completely confident
that it was in the left better context.

2207
01:58:17,256 --> 01:58:21,527
And these little cyan blue dots here mean
that that's what the true context was.

2208
01:58:21,894 --> 01:58:23,696
So in other words, the agent was right.

2209
01:58:24,964 --> 01:58:27,533
So this is saying
at the end of the trial, the thing was,

2210
01:58:27,533 --> 01:58:30,269
the agent was really confident
it was in the left better context.

2211
01:58:31,037 --> 01:58:34,841
The other state factors, choice
states down here, and this is just saying

2212
01:58:34,841 --> 01:58:40,680
that it observed itself go from the start
state to the hint state and then, well,

2213
01:58:40,680 --> 01:58:43,783
sorry, it moved from the start state
to the head of state to the left state.

2214
01:58:44,183 --> 01:58:48,221
So those are the states,
the actions correspond to that.

2215
01:58:48,221 --> 01:58:51,290
Well, so this is saying at first chose
the hint state

2216
01:58:51,958 --> 01:58:56,129
and then it chose the choose
the left state or choose left actions.

2217
01:58:56,129 --> 01:58:58,164
Are these the actions

2218
01:58:59,198 --> 01:59:02,001
This thing in the middle, on the left,
it's not really important.

2219
01:59:02,001 --> 01:59:04,871
It's just kind of depicting
what the different policy options are

2220
01:59:05,805 --> 01:59:07,373
kind of arbitrarily based on the numbers.

2221
01:59:07,373 --> 01:59:08,574
So I just ignore that

2222
01:59:09,609 --> 01:59:10,209
down here at the

2223
01:59:10,209 --> 01:59:14,413
bottom, we have the outcomes
and the outcome preference distributions.

2224
01:59:14,947 --> 01:59:16,883
So this is just saying it observed null.

2225
01:59:16,883 --> 01:59:19,252
And then the left hand and the null.

2226
01:59:19,252 --> 01:59:22,955
Again, this
the third one just says that observed it

2227
01:59:22,955 --> 01:59:25,391
go to the start state, then to hand state
and then to choose is left.

2228
01:59:25,391 --> 01:59:27,393
So just observing its own behavior.

2229
01:59:27,393 --> 01:59:30,363
The second one here is the win lost one,
which is the most important.

2230
01:59:30,596 --> 01:59:33,699
So you can see that this one actually
has a nonzero distribution over it,

2231
01:59:34,133 --> 01:59:37,036
which is what means it
prefers some observations over others

2232
01:59:38,337 --> 01:59:40,139
and so in this case, what it's saying

2233
01:59:40,139 --> 01:59:43,776
is it started the null state at time to
it stated in the null state.

2234
01:59:43,943 --> 01:59:45,144
Right, because it took the hint

2235
01:59:46,112 --> 01:59:49,348
and then it observed a win.

2236
01:59:49,348 --> 01:59:50,850
So in other words, it got the hint.

2237
01:59:50,850 --> 01:59:52,351
It knew what context it was.

2238
01:59:52,351 --> 01:59:55,021
It chose the left, chose the left.

2239
01:59:55,488 --> 01:59:58,658
And so it observed
when So that's what that means.

2240
01:59:59,759 --> 02:00:02,595
And then this distribution
of our policies here is just saying

2241
02:00:02,595 --> 02:00:04,530
at the beginning
it wasn't that confident. Right.

2242
02:00:04,530 --> 02:00:06,933
So there's a lot of gray
across all the policy options

2243
02:00:07,333 --> 02:00:09,902
but after the hint,
it became really confident at time, too,

2244
02:00:10,203 --> 02:00:12,872
about what the right policy was.

2245
02:00:13,105 --> 02:00:14,974
This bottom thing is

2246
02:00:14,974 --> 02:00:18,511
part of the neural process area
simulator dopamine.

2247
02:00:18,844 --> 02:00:20,279
So dopamine response is quote

2248
02:00:20,279 --> 02:00:23,282
unquote, it's the updates in the expected
precision beta thing,

2249
02:00:24,350 --> 02:00:27,653
but that's how you read these plots.

2250
02:00:28,321 --> 02:00:30,423
And again, I won't
I won't go over the neural process

2251
02:00:30,423 --> 02:00:33,292
very well because we're going to do that
next time, hopefully.

2252
02:00:33,292 --> 02:00:37,296
But just the last thing to show is
if you noticed in that case,

2253
02:00:37,296 --> 02:00:40,800
the agent went to the head first, right?

2254
02:00:41,267 --> 02:00:43,502
So it went for the hand first.

2255
02:00:43,502 --> 02:00:45,471
I'll just do that one more time.

2256
02:00:45,471 --> 02:00:48,541
So you can see it's
very reliable that it will

2257
02:00:48,541 --> 02:00:52,745
and that it will choose the hint
with a lot of confidence.

2258
02:00:53,312 --> 02:00:57,316
But now look what happens
if I make this wrist value higher,

2259
02:00:57,316 --> 02:01:01,087
if I make it eight, which means that
the thing is really, really early.

2260
02:01:01,087 --> 02:01:03,256
Wants to win the $4, basically.

2261
02:01:03,589 --> 02:01:07,226
So in this case,
it should be risk taking.

2262
02:01:07,226 --> 02:01:08,794
It should just forego the hand

2263
02:01:10,129 --> 02:01:11,731
and just take a guess immediately.

2264
02:01:11,731 --> 02:01:14,166
And you could see that's what it does.

2265
02:01:14,433 --> 02:01:18,704
It has a flat distribution, has no idea
whether it should choose the left or

2266
02:01:18,704 --> 02:01:22,808
the right one, but it just takes a chance
because it really wants to win the $4

2267
02:01:22,808 --> 02:01:26,412
and value of information
and the expected free energy is low

2268
02:01:27,213 --> 02:01:29,348
because the reward value is really high.

2269
02:01:29,715 --> 02:01:32,051
And so it just takes a guess

2270
02:01:32,551 --> 02:01:34,053
and it guesses

2271
02:01:34,053 --> 02:01:37,223
right and it turns out that
it was in the left context.

2272
02:01:37,223 --> 02:01:39,292
So it observes that it lost

2273
02:01:40,493 --> 02:01:41,927
at time, too.

2274
02:01:41,927 --> 02:01:44,063
And because it observed
that it lost in the first

2275
02:01:44,096 --> 02:01:47,233
oh, I must have been in the left context
as opposed to the right one.

2276
02:01:47,233 --> 02:01:50,569
So at the end, it still knows now
that it was in the left better context.

2277
02:01:51,203 --> 02:01:53,539
And so that's so that's what that means.

2278
02:01:53,539 --> 02:01:58,277
You can see right away that by changing
how much the thing value is reward

2279
02:01:58,277 --> 02:02:02,214
and the preference distribution changes
how information seeking it is versus

2280
02:02:02,214 --> 02:02:02,882
our risk seeking.

2281
02:02:02,882 --> 02:02:07,687
It is and so that's
so that's the way that that works.

2282
02:02:09,055 --> 02:02:12,124
So do we want to end here or

2283
02:02:12,124 --> 02:02:14,560
do we want
or is there time to show learning?

2284
02:02:16,128 --> 02:02:18,864
We can absolutely go through learning,
if you would like.

2285
02:02:19,365 --> 02:02:22,835
So the topics that I've written down,
I have learning, which it sounds like

2286
02:02:22,835 --> 02:02:24,003
would be awesome. To run through.

2287
02:02:24,003 --> 02:02:27,907
Now, then we have actual data,
hierarchical models,

2288
02:02:27,907 --> 02:02:30,810
neural process theories,
and then computational psychiatry.

2289
02:02:31,077 --> 02:02:35,181
So that's the stack of topics
let's get to learning today.

2290
02:02:35,548 --> 02:02:37,983
And then another time,
another model stream

2291
02:02:40,119 --> 02:02:40,753
Okay.

2292
02:02:40,753 --> 02:02:41,821
Sounds good.

2293
02:02:41,821 --> 02:02:44,557
So learning.

2294
02:02:44,557 --> 02:02:47,326
So if you can say
I sort of said how to do this here.

2295
02:02:47,326 --> 02:02:51,030
I said for to reproduce figure seven
so four figure nine here,

2296
02:02:51,030 --> 02:02:53,065
which is some two,
which is the initial learning.

2297
02:02:53,499 --> 02:02:55,835
I'm going to first set

2298
02:02:55,835 --> 02:02:58,170
ours to three.

2299
02:02:58,170 --> 02:02:59,772
It's just a value
that turns out it's nice

2300
02:02:59,772 --> 02:03:02,641
for showing that an Amex of learning
in a way that's clear.

2301
02:03:03,075 --> 02:03:06,212
And so because it should start out

2302
02:03:06,579 --> 02:03:07,780
being information seeking

2303
02:03:07,780 --> 02:03:12,084
and we want to show us how it learns
to be more confident

2304
02:03:12,084 --> 02:03:16,522
and therefore starts to forego
the hint as it builds up a stronger prior

2305
02:03:16,522 --> 02:03:20,226
that the left context
is the one that keeps being the case.

2306
02:03:21,160 --> 02:03:23,596
So now what I'll do
is I'll set some equals to,

2307
02:03:24,630 --> 02:03:25,231
but I'll show

2308
02:03:25,231 --> 02:03:28,567
you what this does down
when I go to some equals to

2309
02:03:30,903 --> 02:03:32,104
blah, blah, blah.

2310
02:03:32,104 --> 02:03:32,972
Okay.

2311
02:03:33,005 --> 02:03:35,241
So this is actually incredibly simple.

2312
02:03:36,409 --> 02:03:38,477
So and similarly to a set,

2313
02:03:38,477 --> 02:03:41,580
all I do is I say, okay,
I want 30 trial so and equals 30.

2314
02:03:42,181 --> 02:03:45,217
And to keep the MDP thing

2315
02:03:45,217 --> 02:03:49,255
something that remains
and then just kind of single structure

2316
02:03:50,322 --> 02:03:52,892
I redefine little as big and deep.

2317
02:03:52,925 --> 02:03:55,027
And again, this is just for convenience.

2318
02:03:55,027 --> 02:03:59,665
And then I can use this deal function
which is just kind of a nice convenient

2319
02:03:59,665 --> 02:04:02,401
function in MATLAB,
which more or less just says

2320
02:04:02,935 --> 02:04:05,671
repeat the MVP structure 30 times.

2321
02:04:06,405 --> 02:04:08,774
So now there's MVP one, maybe two, MVP

2322
02:04:08,774 --> 02:04:11,710
three and all those are just

2323
02:04:12,178 --> 02:04:15,815
the same, just identical
as the initial MVP that we built.

2324
02:04:16,382 --> 02:04:20,219
And so if I do that then what will happen

2325
02:04:20,219 --> 02:04:24,123
is it will look like so okay,
so I'll do so I'll do two things.

2326
02:04:24,490 --> 02:04:25,191
One is

2327
02:04:26,892 --> 02:04:28,427
so before

2328
02:04:28,427 --> 02:04:31,697
and so now I just have big MVP
equals all MVP.

2329
02:04:32,264 --> 02:04:34,366
So now
big MVP will look like this, right?

2330
02:04:34,366 --> 02:04:37,670
This just a single field for TV,
a B, blah, blah, blah, right?

2331
02:04:37,670 --> 02:04:43,008
Everything that we saw
and now once I run this deal thing,

2332
02:04:43,609 --> 02:04:46,111
then if I step, then all of a sudden MVP

2333
02:04:46,111 --> 02:04:48,314
will now look like this.

2334
02:04:49,148 --> 02:04:50,583
Where you can see now

2335
02:04:50,583 --> 02:04:54,420
that same structure
is repeated in 30 rows

2336
02:04:56,088 --> 02:04:58,290
and this will all start out
being identical.

2337
02:05:00,025 --> 02:05:04,730
So what will happen now, though, is
if I run it through the VB

2338
02:05:04,730 --> 02:05:08,534
X tutorial script, then
and this will take a second

2339
02:05:09,034 --> 02:05:12,271
and we'll run through the whole thing
30 times

2340
02:05:14,073 --> 02:05:15,841
and again, this will just take a sec.

2341
02:05:15,841 --> 02:05:17,610
So wait, let me ask something.

2342
02:05:17,610 --> 02:05:20,779
So first, maybe someone maybe
has a little noise in the background, but

2343
02:05:20,779 --> 02:05:25,217
it seems like this
little MDP is like a column that conveys

2344
02:05:25,217 --> 02:05:30,723
all the details for a single trial,
and then we're making a better matrix.

2345
02:05:30,723 --> 02:05:33,392
We're just generalizing the matrix
into many dimensions.

2346
02:05:33,659 --> 02:05:37,563
And now we're stacking
all of the total model as a column into,

2347
02:05:37,730 --> 02:05:39,798
in this case, 30 trials.

2348
02:05:40,132 --> 02:05:44,370
But you could do 100 trials in a row,
or you could have 20 sets of ten trials

2349
02:05:44,637 --> 02:05:49,575
since the idea of concatenating
total model set ups like a column

2350
02:05:50,075 --> 02:05:54,647
so that you can do operations on big,
big potentially hierarchical sets

2351
02:05:54,880 --> 02:05:55,681
of models.

2352
02:05:56,682 --> 02:05:58,918
In this case, each and AP is a row.

2353
02:05:59,318 --> 02:06:00,786
It's just a giant model.

2354
02:06:00,786 --> 02:06:03,889
Yeah, it's just a giant structure
with over and over again.

2355
02:06:04,256 --> 02:06:06,992
Yeah, I meant Ro Start could call it
my R centrism.

2356
02:06:07,192 --> 02:06:10,195
I'm always like thinking in column.

2357
02:06:10,329 --> 02:06:11,030
Yeah.

2358
02:06:11,030 --> 02:06:13,866
All right. But okay, so now I ran it.

2359
02:06:13,866 --> 02:06:16,869
I ran the MVP
with all those different repeated,

2360
02:06:17,903 --> 02:06:20,439
repeated roles, repeated models
through this MVP

2361
02:06:20,873 --> 02:06:23,609
script here and now

2362
02:06:25,044 --> 02:06:27,279
what you'll see is now the MVP structure

2363
02:06:27,279 --> 02:06:29,882
will look a little different

2364
02:06:32,451 --> 02:06:33,752
so now in addition

2365
02:06:33,752 --> 02:06:36,488
to all the stuff in it I had before,

2366
02:06:36,956 --> 02:06:40,359
it's also now going to have

2367
02:06:41,160 --> 02:06:46,932
all this,
oh, this thing is like being now.

2368
02:06:46,932 --> 02:06:49,101
It's going to have all this other stuff.

2369
02:06:49,101 --> 02:06:50,970
It's going to have the free energy.

2370
02:06:50,970 --> 02:06:53,238
If it's going to have the expected
free energy,

2371
02:06:53,339 --> 02:06:54,573
it's going to have the total for energy

2372
02:06:54,573 --> 02:06:58,110
h it's going to have the actions
you it's going to have the free energy

2373
02:06:58,110 --> 02:07:03,248
for the learn parameters
d bunch of other stuff.

2374
02:07:03,949 --> 02:07:06,251
And we have a whole table in the
in the tutorial

2375
02:07:06,251 --> 02:07:08,887
that says what each of these fields
mean so you can interpret them.

2376
02:07:09,888 --> 02:07:10,289
They don't

2377
02:07:10,289 --> 02:07:13,659
all matter enough for me
to go through them right now.

2378
02:07:13,659 --> 02:07:15,861
But so then once I have that

2379
02:07:16,161 --> 02:07:18,330
then I can generate

2380
02:07:19,298 --> 02:07:23,636
a plot with this little game
underscore tutorials script

2381
02:07:23,636 --> 02:07:26,839
that I for plotting multiple trials
that we put together

2382
02:07:27,640 --> 02:07:30,542
and I can run that

2383
02:07:31,010 --> 02:07:33,379
and it will generate

2384
02:07:33,379 --> 02:07:36,749
multi trial there so.

2385
02:07:36,782 --> 02:07:40,052
So what this is showing
is the first action on each trial.

2386
02:07:40,653 --> 02:07:43,789
And so here
what you can see is that and the start

2387
02:07:43,822 --> 02:07:45,858
and the colors of the probabilities
so dark equals

2388
02:07:45,858 --> 02:07:49,428
higher probability and the blue circles
are the actual chosen actions.

2389
02:07:50,529 --> 02:07:54,533
So you can see for the first several
trials the agent kept taking the hint

2390
02:07:55,367 --> 02:08:00,739
but after a while because the left one
if the left contacts kept being the one,

2391
02:08:00,739 --> 02:08:03,542
the one that it thought it was in
at the end

2392
02:08:04,009 --> 02:08:07,146
it starts to be confident around trial
nine ten here

2393
02:08:07,546 --> 02:08:09,281
that it's
always going to be the left one.

2394
02:08:09,281 --> 02:08:11,417
So it just immediately chooses the

2395
02:08:12,518 --> 02:08:14,820
the left arm or the left machine.

2396
02:08:16,255 --> 02:08:20,192
But now what you can see
is, is that it gets one wrong, right?

2397
02:08:20,192 --> 02:08:22,761
Because the it's only 80% right
that the left one

2398
02:08:22,761 --> 02:08:25,064
is going to be the right one. And.

2399
02:08:25,064 --> 02:08:27,132
And once

2400
02:08:28,167 --> 02:08:30,869
one to the dog, could a dog be meters
or so that year.

2401
02:08:30,869 --> 02:08:31,670
Yes it is.

2402
02:08:31,670 --> 02:08:34,506
It is in a dog
wanting to want it to be in my lap.

2403
02:08:34,940 --> 02:08:37,409
But the dogs are contributors
and absolutely.

2404
02:08:37,409 --> 02:08:39,845
Welcome to speak. Yes.

2405
02:08:41,547 --> 02:08:42,648
I thought.

2406
02:08:42,648 --> 02:08:45,417
But anyway so it starts to get one
wrong here on trial.

2407
02:08:45,451 --> 02:08:48,787
12 right the 13 because again
it's only even

2408
02:08:48,787 --> 02:08:51,857
when it's in the left better context
it still will lose 20% of the time.

2409
02:08:52,291 --> 02:08:52,891
Right.

2410
02:08:52,891 --> 02:08:54,693
And so once it loses
it says okay actually

2411
02:08:54,693 --> 02:08:57,129
I'm not confident anymore
so start taking the hint again.

2412
02:08:57,129 --> 02:08:59,698
And that happens a couple of times.

2413
02:08:59,698 --> 02:09:01,433
And so it starts kind of going

2414
02:09:02,901 --> 02:09:04,269
bouncing around

2415
02:09:04,269 --> 02:09:06,872
and, and also note that this, this

2416
02:09:06,872 --> 02:09:10,075
is under an alpha value, right?

2417
02:09:10,075 --> 02:09:12,811
Like the actual precision value
that's not that high.

2418
02:09:13,178 --> 02:09:15,481
So its behavior
is a little random, right?

2419
02:09:15,481 --> 02:09:19,118
There's a little bit of randomness in it,
like what a real human would do.

2420
02:09:20,719 --> 02:09:21,954
And so this next caller,

2421
02:09:21,954 --> 02:09:25,924
this next plot here below is just green
as a win, black as a loss.

2422
02:09:25,924 --> 02:09:30,129
And the you can see when it loses
the negative energies

2423
02:09:30,129 --> 02:09:31,163
are larger

2424
02:09:33,298 --> 02:09:36,034
and again, this
I want to go through these last

2425
02:09:36,502 --> 02:09:39,071
these two here are just part
of the neural process theory.

2426
02:09:39,071 --> 02:09:41,173
They're like the expected

2427
02:09:41,173 --> 02:09:43,342
groups and expected domain responses.

2428
02:09:43,342 --> 02:09:45,978
The model the theory would predict

2429
02:09:45,978 --> 02:09:48,914
and then the slope bottom line is
just kind of the evolution of the agents

2430
02:09:48,914 --> 02:09:52,251
priors about whether it's going to be
the left one or the right one context.

2431
02:09:53,819 --> 02:09:55,020
So that's how to

2432
02:09:55,020 --> 02:09:59,525
read this so it learns and then but look

2433
02:09:59,525 --> 02:10:02,895
what happens if I instead make the thing
a little bit more

2434
02:10:04,363 --> 02:10:05,164
risk seeking.

2435
02:10:05,164 --> 02:10:09,501
So I make the the value of the
the subjective

2436
02:10:09,501 --> 02:10:12,571
value of winning $4 more.

2437
02:10:12,571 --> 02:10:14,473
And I do that just by

2438
02:10:14,473 --> 02:10:17,910
setting Rs to four instead of three.

2439
02:10:17,910 --> 02:10:19,511
In this case.

2440
02:10:21,380 --> 02:10:22,981
So now

2441
02:10:24,650 --> 02:10:25,250
by the way, it's

2442
02:10:25,250 --> 02:10:26,885
awesome that basically

2443
02:10:26,885 --> 02:10:30,355
by hitting play on this script,
it seems like even with minimal

2444
02:10:30,355 --> 02:10:34,092
tweaking, people can reproduce this
in their own setup.

2445
02:10:34,526 --> 02:10:35,294
Starts at play

2446
02:10:35,294 --> 02:10:39,031
with some of the variables,
and it's just really an awesome tool kit.

2447
02:10:39,031 --> 02:10:40,799
This is pretty cool.

2448
02:10:40,933 --> 02:10:42,201
Yeah. No, thank you.

2449
02:10:42,201 --> 02:10:45,003
Yeah, we we definitely tried to make it
as easy as

2450
02:10:45,637 --> 02:10:48,774
User-Friendly as possible
for people to have minimal background.

2451
02:10:48,774 --> 02:10:51,243
So glad if you guys think so.

2452
02:10:51,243 --> 02:10:53,345
But okay, so, so in this case, right?

2453
02:10:53,345 --> 02:10:57,349
I set the risk seeking parameter
and the preference distribution

2454
02:10:57,349 --> 02:10:59,117
to be a higher number.

2455
02:10:59,117 --> 02:11:02,888
So now if you look, the agent is now
it takes the hand twice

2456
02:11:02,888 --> 02:11:05,123
and then just sticks
with the left every single time.

2457
02:11:06,391 --> 02:11:07,092
Even if it

2458
02:11:07,092 --> 02:11:10,395
loses a couple of times,
it just continues to be a risk seeking

2459
02:11:11,430 --> 02:11:15,901
and, and again, everything else is
basically the same except the,

2460
02:11:16,635 --> 02:11:18,937
the predicted domain responses and groups

2461
02:11:18,937 --> 02:11:21,273
and things like that are different.

2462
02:11:22,140 --> 02:11:25,811
But but so that is what it looks like
when you just do

2463
02:11:25,811 --> 02:11:29,715
like a normal learning
just where it says the same.

2464
02:11:29,715 --> 02:11:33,452
The context is the same across trials
and the last thing I'll show you guys

2465
02:11:33,952 --> 02:11:37,689
is if I set it to three,
then we engineer this kind of this

2466
02:11:37,923 --> 02:11:40,158
reward learning

2467
02:11:41,693 --> 02:11:44,129
or this reversal learning version, right?

2468
02:11:44,129 --> 02:11:45,030
So our virtual learning

2469
02:11:45,030 --> 02:11:47,766
is for the first several trials,
the left context is better.

2470
02:11:48,133 --> 02:11:50,869
So it's going to build up a prior
the the left context is better

2471
02:11:51,270 --> 02:11:54,306
but then it's going to switch to
the right context being better and more.

2472
02:11:54,306 --> 02:11:55,841
We're going to see how the agent does.

2473
02:11:57,242 --> 02:11:57,876
Now, I'll show you

2474
02:11:57,876 --> 02:12:00,946
how we do that,
which is also very simple here.

2475
02:12:00,946 --> 02:12:03,649
We're going to set end to be 32 trials

2476
02:12:04,082 --> 02:12:06,952
through the same kind of,
you know, deal thing.

2477
02:12:07,619 --> 02:12:10,155
But now what I did is I just said

2478
02:12:10,155 --> 02:12:13,158
for one to

2479
02:12:13,158 --> 02:12:16,361
the number of trials divided by eight,
we're going to make big.

2480
02:12:16,361 --> 02:12:20,065
The 10 is saying the true neural process

2481
02:12:20,065 --> 02:12:23,068
or the true generative process
is going to be the left one.

2482
02:12:23,502 --> 02:12:27,940
But then I say in the MDP four
and divided by eight plus one plus

2483
02:12:27,940 --> 02:12:32,411
one to N so basically one
after this number to the end,

2484
02:12:33,512 --> 02:12:34,413
then it's going to now

2485
02:12:34,413 --> 02:12:37,182
be switched at the generative process
as the right context.

2486
02:12:37,683 --> 02:12:41,153
And so in this case,
it just means the first a the first eight

2487
02:12:41,153 --> 02:12:43,121
trials are left better

2488
02:12:43,121 --> 02:12:46,692
and the rest of the trials up to 32
are going to be the right better context.

2489
02:12:47,192 --> 02:12:48,360
So that's literally it.

2490
02:12:48,360 --> 02:12:49,561
Then just run the same thing

2491
02:12:49,561 --> 02:12:53,332
through the same vbox function and
you plot it with the same plot script.

2492
02:12:53,498 --> 02:12:55,734
So that's really very, very simple.

2493
02:12:56,702 --> 02:12:56,969
So if

2494
02:12:56,969 --> 02:12:59,471
I do that with risk, speaking of three

2495
02:13:00,105 --> 02:13:04,409
then what we'll see happen is

2496
02:13:07,412 --> 02:13:09,648
that, you know,
give it a second to look here.

2497
02:13:09,681 --> 02:13:13,952
Just to give one note on that software
design, instead of having five scripts

2498
02:13:14,319 --> 02:13:17,422
called simulation
one simulation two, simulation three,

2499
02:13:17,956 --> 02:13:19,691
it's like there's a Common Core.

2500
02:13:19,691 --> 02:13:23,161
And then the simulation scenarios
are defined basically

2501
02:13:23,161 --> 02:13:26,498
like variables down here in the six
hundreds lines.

2502
02:13:26,832 --> 02:13:32,170
And then by changing which simulation
is in play at the very top, it seems like

2503
02:13:32,170 --> 02:13:36,274
Ryan is being able to change
which simulation we're in very fluidly.

2504
02:13:36,274 --> 02:13:37,676
So we can change the simulation.

2505
02:13:37,676 --> 02:13:40,912
Look at a variable, change a variable,
go back up, change the simulation,

2506
02:13:40,912 --> 02:13:42,214
go back in

2507
02:13:44,082 --> 02:13:44,683
yeah.

2508
02:13:44,683 --> 02:13:48,620
I mean, it's basically it's just a it's
just an if then function,

2509
02:13:48,620 --> 02:13:51,023
it just says if SIM equals three,
then do this.

2510
02:13:51,490 --> 02:13:54,026
Or else if SIM equals four, do this.
Yeah.

2511
02:13:54,092 --> 02:13:55,861
I mean, it's
just a very simple statement.

2512
02:13:57,362 --> 02:13:58,497
But but okay.

2513
02:13:58,497 --> 02:14:02,300
So in this case, and so I set our

2514
02:14:02,501 --> 02:14:05,337
as equals three.

2515
02:14:05,337 --> 02:14:08,540
And what you can see is, is in this case,

2516
02:14:08,540 --> 02:14:11,309
the agent just continued to take the hint
the entire time

2517
02:14:14,012 --> 02:14:16,581
because it was never
it was never confident enough.

2518
02:14:17,049 --> 02:14:19,885
And the, you know, and it happened
to get the first one wrong

2519
02:14:19,885 --> 02:14:23,388
in this case,
which is part of what drove that

2520
02:14:23,388 --> 02:14:27,025
but so if they if the actual observations
they got were a little different,

2521
02:14:27,025 --> 02:14:30,996
let's actually try that again
and see what

2522
02:14:33,698 --> 02:14:35,267
what I would get here.

2523
02:14:35,267 --> 02:14:38,437
But this characterizes
all these interesting things like a

2524
02:14:38,970 --> 02:14:44,476
perhaps over or under eagerness to ask
for hints or acquire more information.

2525
02:14:44,643 --> 02:14:48,080
It's showing how the model this one
is not trying to fit human behavior.

2526
02:14:48,080 --> 02:14:51,650
We're really playing with the bare bones,
but already we're seeing that's kind of

2527
02:14:51,750 --> 02:14:55,520
pathological hint taking behavior,
but it never goes when it knows.

2528
02:14:55,654 --> 02:14:57,222
And so now it's something else. Yeah.

2529
02:14:57,222 --> 02:15:01,960
Well, it's just it's parameter play,
but it's going to play like yeah.

2530
02:15:01,960 --> 02:15:03,061
So like in this case, right?

2531
02:15:03,061 --> 02:15:04,463
It does take the hint every time.

2532
02:15:04,463 --> 02:15:06,364
It does kind of start.

2533
02:15:06,364 --> 02:15:09,501
You can see on Trial five here,
it starts to have a little bit of like,

2534
02:15:09,501 --> 02:15:12,370
oh, maybe I should just choose the lab
choose the left one immediately,

2535
02:15:12,804 --> 02:15:15,440
but then it gets a couple wrong
and then the context changes.

2536
02:15:15,841 --> 02:15:18,243
You can see in context learning here,
it starts out, Oh,

2537
02:15:18,276 --> 02:15:20,445
I think the left one's better,
and then I start switching.

2538
02:15:20,445 --> 02:15:23,048
And then around trial, 11 and 12.

2539
02:15:23,048 --> 02:15:26,852
Now it starts to become more and more
confident that in the right win context.

2540
02:15:28,019 --> 02:15:30,088
So now last thing I'll show you

2541
02:15:30,555 --> 02:15:35,560
is what happens under that same thing,
but when I set are as equal to four.

2542
02:15:35,560 --> 02:15:37,863
So it's a little more risk seeking

2543
02:15:41,800 --> 02:15:44,536
because this actually gets
a little more fun.

2544
02:15:44,769 --> 02:15:49,074
So the risk seeking
is like a multiplication on the money.

2545
02:15:49,074 --> 02:15:52,477
So it's like one to $2 versus
1 million to 2 million, like how high

2546
02:15:52,477 --> 02:15:53,512
the stakes are

2547
02:15:53,512 --> 02:15:56,748
because it sounds like you're
kind of tuning up whether the agent

2548
02:15:56,748 --> 02:15:59,684
wants the big wins or what
is it really conveying in this model?

2549
02:16:00,152 --> 02:16:04,256
It's just it's just essentially the
precision of the preference distribution.

2550
02:16:04,256 --> 02:16:07,292
So it's just it's
just how big the number is

2551
02:16:08,660 --> 02:16:11,763
over when at times step two.

2552
02:16:13,165 --> 02:16:14,232
But that's really all it is.

2553
02:16:14,232 --> 02:16:18,470
It's just how high the probability
is for a win in the agents model,

2554
02:16:18,470 --> 02:16:21,139
which just encodes
how strongly they prefer winning.

2555
02:16:21,706 --> 02:16:24,976
And you can think about it
as like $4 for them

2556
02:16:24,976 --> 02:16:28,346
subjectively
being like the value of $10 or something.

2557
02:16:28,346 --> 02:16:29,147
Like that.

2558
02:16:29,147 --> 02:16:33,718
But, but all it ends up doing in
practice is it ends up saying the

2559
02:16:34,986 --> 02:16:37,189
reward value component of expected

2560
02:16:37,189 --> 02:16:41,259
free energy has a higher weight
than the information value component,

2561
02:16:42,861 --> 02:16:45,330
as is all it ends up being in practice.

2562
02:16:45,530 --> 02:16:49,201
And so here you can see that the behavior
is actually, you know, again,

2563
02:16:49,201 --> 02:16:50,602
it's more interesting.

2564
02:16:50,602 --> 02:16:53,004
So you could say it takes the hint twice
and all of a sudden it's like,

2565
02:16:53,004 --> 02:16:53,405
all right,

2566
02:16:53,405 --> 02:16:56,975
I'm confident enough I'm going to go
for choosing the left immediately.

2567
02:16:57,442 --> 02:16:57,609
Right.

2568
02:16:57,609 --> 02:17:01,046
And it does that for review trials
and then the context switches,

2569
02:17:01,346 --> 02:17:04,015
you know, so at trial
it switches to being the right context.

2570
02:17:04,382 --> 02:17:06,918
And then it's like, nope,
I'm going to start taking the hint again.

2571
02:17:06,918 --> 02:17:09,554
And then it observes the right one

2572
02:17:09,554 --> 02:17:11,089
a bunch of times.
And then it's like, okay.

2573
02:17:11,089 --> 02:17:13,058
And now I'm confident it's the right one.

2574
02:17:13,058 --> 02:17:13,892
And then it starts

2575
02:17:13,892 --> 02:17:16,261
choosing the right one of that,
taking a hit a bunch of times.

2576
02:17:16,261 --> 02:17:19,531
Now, the behavior is quite a bit more
interesting because it becomes confident,

2577
02:17:19,531 --> 02:17:22,033
it loses confidence
that switches to being confident again.

2578
02:17:22,033 --> 02:17:23,335
And the other thing.

2579
02:17:23,335 --> 02:17:27,405
Again, it gets one wrong around 23
it gets a wrong one,

2580
02:17:27,639 --> 02:17:30,475
and then it does one hint and then it's
like, Nope, I'm on the right track.

2581
02:17:30,475 --> 02:17:32,978
I just lost one time. It's all good.

2582
02:17:32,978 --> 02:17:34,412
Yep, exactly.

2583
02:17:35,880 --> 02:17:39,150
So, so that's,
that's the way the learning works.

2584
02:17:39,150 --> 02:17:40,352
And if I show you, you know, like,

2585
02:17:40,352 --> 02:17:43,722
I'm in and plotting this here,
but like literally in the MVP,

2586
02:17:44,222 --> 02:17:48,994
like it will just be
if I go to the things learning D, right?

2587
02:17:48,994 --> 02:17:53,131
So if I go to a little D trial, one

2588
02:17:55,300 --> 02:18:01,039
and then, you know, it observes.

2589
02:18:01,673 --> 02:18:05,377
Yeah, it's 0.75.25 because that observer
left the first time.

2590
02:18:05,510 --> 02:18:06,278
Right?

2591
02:18:06,411 --> 02:18:10,048
And then at, you know, say like trial A

2592
02:18:11,349 --> 02:18:16,321
it's 2.22.2

2593
02:18:18,323 --> 02:18:20,725
it didn't make. It
because that's what it's considered.

2594
02:18:20,725 --> 02:18:23,762
A flat was flat still,
but that's because it got some wrong.

2595
02:18:23,928 --> 02:18:26,498
That's when it was most confusing.

2596
02:18:26,731 --> 02:18:28,066
Yeah.

2597
02:18:28,633 --> 02:18:33,271
And then but if I go down to say, like,
you know, at the end, say like trial,

2598
02:18:33,338 --> 02:18:36,741
I don't know, 31 or something,
then all of a sudden

2599
02:18:38,343 --> 02:18:41,246
it's two versus 13, right?

2600
02:18:41,279 --> 02:18:42,380
So it's way more confident.

2601
02:18:42,380 --> 02:18:47,018
It's added a lot more counts to it
being the right context.

2602
02:18:47,018 --> 02:18:48,119
That's better.

2603
02:18:49,321 --> 02:18:51,723
And that's, you know, that's really it.

2604
02:18:51,723 --> 02:18:55,126
And so
and that's what all the learning is.

2605
02:18:55,126 --> 02:18:58,830
And, you know, like a

2606
02:18:58,830 --> 02:19:02,967
cross
showed you the equation for learning a

2607
02:19:03,468 --> 02:19:06,871
but you know, in the,
in the tutorial paper

2608
02:19:07,305 --> 02:19:10,542
we also show it for, for learning

2609
02:19:11,076 --> 02:19:14,012
D which is even simpler.

2610
02:19:14,012 --> 02:19:16,748
It's just see if I can get to
a real quick here

2611
02:19:19,584 --> 02:19:21,920
learning process.

2612
02:19:21,920 --> 02:19:24,489
A learning.

2613
02:19:24,489 --> 02:19:26,124
Yeah, it's literally just this.

2614
02:19:26,124 --> 02:19:29,394
So D for the next
trial is D for the previous trial.

2615
02:19:29,394 --> 02:19:33,198
Plus at times,
whatever the posterior states were

2616
02:19:33,431 --> 02:19:35,300
for the previous trial,

2617
02:19:36,267 --> 02:19:39,170
that's you know, that's literally it

2618
02:19:41,973 --> 02:19:43,007
although I'm realizing

2619
02:19:44,042 --> 02:19:44,409
that should

2620
02:19:44,409 --> 02:19:47,545
be a minus, not an equals, I think, but

2621
02:19:47,545 --> 02:19:50,081
a little typos here and there.

2622
02:19:50,081 --> 02:19:52,851
But anyway, that's, you know,
that's really it.

2623
02:19:53,318 --> 02:19:57,756
And so it's not a
it's not complicated at all.

2624
02:19:57,756 --> 02:20:00,892
It's just add account based on what
your posterior was overstates

2625
02:20:00,892 --> 02:20:02,994
at the end of the last trial.

2626
02:20:03,928 --> 02:20:06,664
So, I mean, that's
you know, if we're stopping

2627
02:20:06,664 --> 02:20:09,501
at the end of basic learning,
then then that's really it.

2628
02:20:09,667 --> 02:20:13,872
And like I said, four and five
have to do with estimating parameters

2629
02:20:13,872 --> 02:20:16,908
based on data and then recovering

2630
02:20:16,908 --> 02:20:20,812
parameter estimates and feeding behavior
to data and things like that.

2631
02:20:20,812 --> 02:20:22,814
So we'll cover that at another time.

2632
02:20:23,281 --> 02:20:23,882
Awesome.

2633
02:20:23,882 --> 02:20:27,752
Well, just to catch our breath
at the end of this,

2634
02:20:27,752 --> 02:20:30,555
really fascinating
and very appreciated stream.

2635
02:20:31,923 --> 02:20:34,592
And if anyone wants to
put any last comments in the live chat,

2636
02:20:34,592 --> 02:20:36,561
they'll have just a couple of minutes
to do so.

2637
02:20:36,561 --> 02:20:41,833
Let's just each take a final recap
or a final thought

2638
02:20:42,467 --> 02:20:44,903
what do we want to remember
from this time?

2639
02:20:45,203 --> 02:20:46,905
What are we going to say
forward into next time?

2640
02:20:46,905 --> 02:20:49,274
What are we looking forward
to learning more about?

2641
02:20:49,274 --> 02:20:53,111
So whoever wants to go first,
maybe Christopher,

2642
02:20:53,311 --> 02:20:56,948
any remarks on Ryan's presentation
and then on the code

2643
02:20:56,948 --> 02:21:00,084
and also like your role
or which part had you done?

2644
02:21:00,151 --> 02:21:01,152
I was just curious about that.

2645
02:21:01,152 --> 02:21:03,421
Wallace things for the last.

2646
02:21:05,590 --> 02:21:05,990
So know,

2647
02:21:05,990 --> 02:21:11,729
I think I agreed with everything you said
there one thing to just say very one

2648
02:21:11,729 --> 02:21:14,866
when you really get your get a handle
on building these models,

2649
02:21:15,600 --> 02:21:20,271
it will take, I think the most time
I've ever spent building these models.

2650
02:21:20,271 --> 02:21:24,843
Once we actually figured out the model
structure was a couple of days like

2651
02:21:24,843 --> 02:21:28,179
you might spend a very, very long time
figuring out the model structure.

2652
02:21:28,179 --> 02:21:30,448
But once you figure out the model
structure, everything falls in place.

2653
02:21:30,448 --> 02:21:31,216
It's super easy.

2654
02:21:32,217 --> 02:21:34,752
So it's kind of interesting in the sense
that I've some friends

2655
02:21:34,752 --> 02:21:37,489
do like you're on network
or modeling with like spiky neural nets

2656
02:21:37,755 --> 02:21:40,258
and they can spend like weeks
building these models

2657
02:21:44,796 --> 02:21:46,898
The the hard work
here is really conceptual.

2658
02:21:48,633 --> 02:21:51,069
And then you ask like,
what part of the tutorial

2659
02:21:51,069 --> 02:21:53,905
I wrote all investors, Ryan,
was that the question?

2660
02:21:53,972 --> 02:21:55,473
Not a partitioning.

2661
02:21:55,473 --> 02:21:58,977
Just which parts were your background
drawing from or just

2662
02:21:59,477 --> 02:22:01,880
how do you see it similar differently?

2663
02:22:02,580 --> 02:22:06,017
I wrote them generally speaking,
I wrote the mathematical appendices

2664
02:22:06,017 --> 02:22:09,354
and then did like first drafts
of a lot of the techie sections.

2665
02:22:09,354 --> 02:22:12,991
But that was kind of like my
I really really asked Ryan to do that

2666
02:22:12,991 --> 02:22:16,027
just because I wanted to kind of get
a better understanding of myself.

2667
02:22:17,629 --> 02:22:18,930
And then Ryan kind

2668
02:22:18,930 --> 02:22:22,367
of wrote a lot of the broader sections
that was going to help, stuff like that.

2669
02:22:22,767 --> 02:22:28,473
And yeah, I mean, I mean, yeah, I wrote
I mean, I wrote

2670
02:22:28,473 --> 02:22:31,409
the code were going through now was, was,

2671
02:22:31,743 --> 02:22:34,546
you know, I took the first pass at

2672
02:22:34,546 --> 02:22:36,981
Chris wrote the

2673
02:22:38,416 --> 02:22:40,652
hierarchical model in code

2674
02:22:40,652 --> 02:22:43,655
and which another, another day
you'll see.

2675
02:22:43,655 --> 02:22:47,392
And a lot of the neural process
simulation code

2676
02:22:48,493 --> 02:22:51,362
or that kind of custom one,
the ones that are showing in

2677
02:22:51,362 --> 02:22:54,165
ours is kind of like Carl Standard,
the ones that are in a spam but

2678
02:22:55,333 --> 02:22:58,136
it's a way,
way cooler ones for the hierarchical

2679
02:22:58,136 --> 02:23:00,772
unreal process simulation stuff.

2680
02:23:00,772 --> 02:23:04,475
So yeah, I mean a lot of this was a fully
collaborative of, you know, I wouldn't

2681
02:23:04,475 --> 02:23:08,446
necessarily say, you know, one person
did anything more than the other.

2682
02:23:08,613 --> 02:23:09,247
And so

2683
02:23:10,448 --> 02:23:12,050
everyone
seems like written a scientific paper.

2684
02:23:12,050 --> 02:23:14,953
No, it's like it's after
the first draft has been written.

2685
02:23:14,953 --> 02:23:17,055
That's kind of hard
to tell the finished product

2686
02:23:17,055 --> 02:23:20,425
because when you have heavy backward
and forward editing it.

2687
02:23:21,192 --> 02:23:21,726
Yeah.

2688
02:23:23,294 --> 02:23:24,062
I totally agree.

2689
02:23:24,062 --> 02:23:25,263
It's a great perspective.

2690
02:23:25,263 --> 02:23:27,398
So also thanks for sharing that.

2691
02:23:27,398 --> 02:23:30,668
So maybe now
let's have our last little round, maybe

2692
02:23:31,102 --> 02:23:33,538
Max or we'll each take a last step

2693
02:23:34,672 --> 02:23:34,939
yeah.

2694
02:23:34,939 --> 02:23:38,009
I'm just,
you know, as a lead into next week,

2695
02:23:38,009 --> 02:23:40,078
I'm excited to hear
about the neural process theory

2696
02:23:40,078 --> 02:23:43,014
and really about, you know,
looking at those dopamine traces

2697
02:23:43,014 --> 02:23:46,584
and it's it's cool to see the spiking
that you are able to reproduce

2698
02:23:46,584 --> 02:23:50,154
with this kind of a model structure
that does reflect, you know, kind of how

2699
02:23:50,588 --> 02:23:54,125
the exact form
that our biological processes might look.

2700
02:23:54,826 --> 02:23:57,662
One thing
I'm really keen to hear about is the,

2701
02:23:57,662 --> 02:24:00,999
you know, the link
function between your, your gradient

2702
02:24:02,133 --> 02:24:03,301
on free

2703
02:24:03,301 --> 02:24:07,739
energy and your dopamine express
are stimulated dopamine expression levels

2704
02:24:07,972 --> 02:24:12,310
and the postulated link between,
you know, precision insofar as I mean,

2705
02:24:12,310 --> 02:24:15,346
I mean, these are things that I'm just
it's a little out of my realm.

2706
02:24:15,346 --> 02:24:17,081
But I think it's really interesting

2707
02:24:17,081 --> 02:24:20,785
because fundamentally that's
that's such an elegant mechanism.

2708
02:24:20,785 --> 02:24:21,886
If it's that gradient

2709
02:24:21,886 --> 02:24:25,556
only from a mathematical standpoint
that it's really cool to tie that

2710
02:24:26,190 --> 02:24:29,160
from math and theory
into empirical observation

2711
02:24:29,160 --> 02:24:31,996
and using that to validate the model.

2712
02:24:33,998 --> 02:24:35,500
Or preferred one.

2713
02:24:35,500 --> 02:24:35,967
Definitely.

2714
02:24:35,967 --> 02:24:40,038
While we're excited
to, we're excited to show you a nice job.

2715
02:24:40,038 --> 02:24:42,507
If you want to make any last comments.

2716
02:24:42,507 --> 02:24:44,342
Know that this has been really fun.

2717
02:24:44,342 --> 02:24:45,443
Thanks, Daniel. Yeah.

2718
02:24:45,443 --> 02:24:45,877
Yeah, yeah.

2719
02:24:45,877 --> 02:24:47,745
Thanks everyone talking this whole time.

2720
02:24:47,745 --> 02:24:49,580
So yeah, yeah.

2721
02:24:49,580 --> 02:24:51,315
Again, people really do want to hear me.

2722
02:24:51,315 --> 02:24:52,417
Bramble anymore.

2723
02:24:52,417 --> 02:24:53,618
Not so next time at least.

2724
02:24:53,618 --> 02:24:57,588
But yeah, thanks everyone for watching
life and in replay and everything

2725
02:24:57,588 --> 02:25:01,059
and just for participating
because it's the conversations

2726
02:25:01,059 --> 02:25:04,862
we're having the real time
errors we're finding and the learning

2727
02:25:04,862 --> 02:25:06,097
that we're all contributing to.

2728
02:25:06,097 --> 02:25:07,799
So just thanks everyone
for participating,

2729
02:25:07,799 --> 02:25:10,001
and we'll see you later for not.

2730
02:25:10,034 --> 02:25:11,536
We'll see you later tonight.

2731
02:25:11,536 --> 02:25:13,004
We'll see you later tonight.

2732
02:25:13,004 --> 02:25:14,505
We'll see you later tonight.

2733
02:25:14,505 --> 02:25:17,475
We'll see you later tonight.
We'll see you later tonight.

2734
02:25:17,475 --> 02:25:20,445
We'll see you later tonight.
We'll see you later tonight.

2735
02:25:20,445 --> 02:25:23,414
We'll see you later tonight.
We'll see you later tonight.

2736
02:25:23,414 --> 02:25:24,882
We'll see you later tonight.

2737
02:25:24,882 --> 02:25:27,885
We'll see you later tonight.
We'll see you later tonight.

2738
02:25:27,885 --> 02:25:30,021
We'll see you later
tonight. We'll see you the.

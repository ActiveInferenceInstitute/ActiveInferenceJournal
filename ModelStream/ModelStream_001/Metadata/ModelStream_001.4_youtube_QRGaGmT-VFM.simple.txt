SPEAKER_01:
Okay, I think we're live.

Okay, yes, the screen was blocked out for a second.

But hello, everyone.

Welcome to the Active Inference Lab.

This is February 5th, 2021.

And we are here in model stream 4.0 with Ryan Smith and Christopher White.

I'm Daniel Friedman.

I'm a postdoctoral researcher in California and a participant in Active Inference Lab.

Maybe the two of you can just briefly reintroduce yourself.


SPEAKER_03:
Yes, for anybody who hasn't been watching the previous sessions, I'm Ryan Smith.

I'm an investigator at the Laureate Institute for Brain Research in Tulsa, Oklahoma.


SPEAKER_00:
Hi, I'm Christopher White.

I'm a PhD student at the MRC Cognition Brain Sciences Unit in Cambridge, England.


SPEAKER_01:
Awesome.

Well, we are here today in part four, which will be the last section on this paper.

Hopefully not the last time we get to speak together on any stream, but it's part four of four on this paper, a step-by-step tutorial on active inference and its application to empirical data.

You can find the link to the most updated version of that resource in the video descriptions.

And as far as points of process, if you have any questions or comments or thoughts during the live stream, you can type it into the live chat and we'll address it when the time is right.

If you have questions arising after this live stream or you're watching it after it's occurred, feel free to leave a comment and we'll try to address it in future sessions because the model stream never ends.

So to learn and participate and learn about how to participate, go to ActiveInference.org.

That's where you'll find more information on Active Inference Lab.

So today in our fourth session, it's going to be pretty exciting because we've covered a lot of the groundwork.


SPEAKER_00:
Daniel, are you crying?


SPEAKER_01:
Yeah, we're not hearing anything, Daniel.

Oh.

Give me one second.

Okay, do you hear me now?

Nope.

Yeah, you froze.

All right, give me a second.

It's all good.

This is how it happens sometimes when live streaming happens.

Let me go back to the event and I will rejoin the event.

All right.

Cool.

Waiting.

This is just how it is.

Welcome to live streaming, everyone.

Cool.

Look at that ghost, Daniel Friedman.

He can be kicked out.


SPEAKER_03:
You're really quiet again.


SPEAKER_01:
Okay, okay, okay.

Yes.

Device settings.

Okay.

Yes.

Thanks, Microsoft Teams, for facilitating these conversations.

It's still live and happening.

It's just a little hiccup with Teams that now we've gotten that out of the way, so it's fixed.

But thanks, Ryan.

And I don't know if this other person can be kicked out, but take it away.

We're all good, though.

Thank you.

Okay.

Maybe.

Which one are you?

The ghost of Daniel's past.

It's okay.

It will auto-drop them after you share and unshare.

But it's all chill.

Okay, okay.

Perfect.


SPEAKER_04:
Okay.


SPEAKER_01:
I think that worked.


SPEAKER_03:
Okay, so we are still live and going.


SPEAKER_04:
Yep.


SPEAKER_03:
Okay.

Alright, so...

So I should just start or is there anything else?

You were kind of in the middle of saying something.


SPEAKER_01:
Just if we could start with some recap for those who probably have seen the first three sessions, just where did we get to as of today?

And then where does that take us into the future?


SPEAKER_03:
Okay.

So yeah, just as a quick recap, as Daniel said, the kind of whole purpose of this tutorial that we put out

was to try to make methods for actually building active inference models and applying them to experimental data.

We wanted to try to make those methods more accessible to a broader audience.

Because up until now, it's been fairly difficult to find sort of clear resources for beginners to start learning these sorts of methods.

And so the whole kind of the whole

you know, point of what we've covered so far has kind of been building to this section.

Um, um, if the focus again is on being able to actually use these models for, uh, experiments.

Um, so obviously the, the earlier sections could be useful on their own if you want to do simulation work.

Um, and there's lots of papers, um, reporting simulation work, but, um, in terms of, um, then applying the models and simulation work that you do to, um,

to actual experiments, to actually fit it to task behavior with participants and predict things about neural responses and fMRI studies and all that kind of stuff, then it all kind of boils down to what we'll talk about today.

So in earlier sections, we kind of covered the basics of active inference, what variational free energy is, what expected free energy is, what the motivation is for doing active inference, what the potential benefits are.

over traditional reinforcement learning models, you know, things like that, just kind of the nuts and bolts.

And then we covered how to build active inference, partially observable markup decision process models.

And as an example, we built a simple kind of explore exploit model, which I will, or explore exploit task model, should I say, as just kind of a concrete example of how you would build a model

And we showed some simulations both for perception and decision-making and for learning with that model.

And then we also kind of took a break from that.

We're going to return to that now, but we also took a break from that a little bit to cover hierarchical models and the neural process theory.

But now we're going to come back to using the explore-exploit task model because it's

simpler and more straightforward in terms of what you do with participant behavior and how you fit it.

So all of it, like I said, is building up to using the task model we built as an example for fitting it to data.


SPEAKER_01:
So, I don't know.

Christopher, great summary.

Christopher, what would be your perspective or what was something that stood out to you?

from the previous sessions?

Yep, because you're so both deep in the game and actually producing this artifact that it's been fun for us and for the audience just to be kind of talking through it.

So what does it leave upon somebody who's been involved in the generation of this artifact now to be talking about it?


SPEAKER_00:
Interesting question, I suppose.

I think it has made, and I'm just, so this is kind of in,

the context I have, I started my PhD four months ago.

So this is all kind of in the context of figuring out not just how to build these models kind of for myself, um, or sorry, how to teach other people how to build these models rather, but actually how to build these models for myself and apply them to my own research questions.

Um, and so I actually have never fitted them to data.

All of the stuff that I've done so far has been theory, essentially just, I've done a lot of simulation work, but no, um,

a data fitting.

So I don't know.

I think that's actually the frontier in active inference at the moment is actually figuring out how to get these models to hit the road.

Um, and there's a really one, one really tricky thing is most of the people who work on active inference in a real way are neuroimages.

So I, for example, I'm in a neuroimaging department.

Um, I know Ryan, you work in neuroimaging as well.

Um,

It's actually challenging to test these models in the domain of neuroimaging.

Neuroimaging is really hard.

Actually, I think many of the clearest predictions actually in terms of the neural process theory are actually things that you could test really quite easily with optogenetics.

Not optogenetics or anything like that's easy, but say predictions about microcircuitry in PFC, which active inference certainly makes, can very much be tested easily.

In a rodent model, but it's really difficult to test it in a human and so I think The point that I'm trying to in a rather rambling way get to is I think we need to think very deeply about When we build models about what it actually is that we're trying to solve.

Do we just want a directional hypothesis or

In which case, you don't necessarily need to fit these things to data.

You're just saying A will be bigger than B, which is still better than most psychology, let's be honest.

Yeah.


SPEAKER_01:
Yeah, it's really interesting.

The neuroimaging case has a few specific difficulties, massive amounts of data in a system that has massively complex internal and external dynamics like the brain.

And you're making a ton of data with a very complex error profile.

And in the SPM textbook,

It's chapters and chapters of normalization and rewarping and all these different kinds of ways to deal with these very complex data sets.

And now we've almost built out that framework.

And now, just like you said, where it's hitting the road is in being applied to empirical data.

And it's a little bit of a surprise, at least to me and probably others, that maybe neuroimaging, although that's the home of SPM, we're seeing it applies to systems beyond neuroimaging.

And even there's other systems that help inform our neuroimaging quest.

So pretty interesting stuff.

Um, okay.

So should we just get started then?

Sounds good.

And anyone in the live chat, just drop a question and we'll get to them.

Thanks a lot.

Okay.


SPEAKER_03:
So one brief thing that I did want to mention, um, is that a couple of times ago we, um, we talked about, we covered learning.

Um, but since then, um, just wanted to let people know, we actually, uh, caught a little

a small error in the learning section in the previous version that we've now corrected.

And it has to do with the way that you calculate the novelty term in expected free energy that gets added to expected free energy when you're doing learning.

So it's what drives the parameter exploration term in expected free energy.

It basically drives an agent to seek out policies that will tell it more about

In this case, what the A matrix looks like.

So tell it more about what the relationship is between states and observations.

And there's equivalent terms that could be added if you're trying to learn transition beliefs or other matrices and vectors in the model.

But so now just to kind of make that clear, we actually have added two different additional worked examples of how to calculate the novelty term just to make sure that's clear.

So if anybody doesn't happen to have the kind of most updated version and you're interested in kind of knowing the kind of rigorous details about how learning works in the formalism, then it might be helpful to download the updated version so that you can have additional worked examples to get a better sense of how that works.

So I just wanted to make that clear.

Like I said, it's not like a big error, but it's a small error that if you

were to read it, it could be confusing.

I just wanted to highlight that for people.

But today, like I mentioned, the focus is going to be on actually building task models and applying them to data.

This isn't necessarily the only way you could break it up, but I've broken this up into six steps that are probably a good heuristic

um, you know, roadmap or, or whatever you want to call it, um, for doing this kind of work.

Um, so the first thing is that you have to have a task and you have to have participants perform that task.

Um, the second thing is you have to build one or more models of that task.

So one or more models, um, in this case, partially observable markup decision process models in the active inference formulation.

that can reproduce or generate simulated behavior for that task.

And so once you do that, then the next thing you need to do is you need to somehow find parameter values in each of those models that you construct that can best reproduce each participant's behavior.

So for instance, under one model, you might find that the first participant

The model can reproduce the first participant's behavior the best if it has a particular, say, value for how much they want the reward.

So, for instance, like the value of the, or think about it as a precision of the preference distribution.

Or you might find that for another participant, you need a lower value for that to reproduce their behavior well, but they need something like a higher learning rate.

or a lower action precision value, right?

There's all these different parameters in these models we've been covering and different values for those parameters are going to be better.

Different parameter value combinations in these models are going to be better at reproducing one person's behavior versus another on the task.

And so what that requires is having some kind of estimation algorithm

something that basically searches through in some way different combinations of parameters and checks how well it reproduces a given participant's behavior.

And so, like I said, those are called parameter estimation algorithms, and there's several different types that you can use.

I'll briefly mention a few, and then we'll go into detail on the one that is probably most consistent with

the general kind of theme of active inference models, which is a scheme called variational base.

So once you've found for each model the parameter values that best describe each participant, then you want to know, okay, well, which of those models is the best model?

And so in that case, you need some kind of way of comparing how well each model, on average, can reproduce the behavior of the whole group.

of your participants.

So each model is going to have their best fit parameters for each participant, but still the best fit parameters for one model might still do a better job than the best fit parameters for another model.

And again, we'll go into this.

And then once you've identified what the best model is, then, and this is something that might not be obvious, but it's really important, is you have to somehow confirm

that the parameters in the best model are recoverable.

So you can either talk about this as whether the model is identifiable or that the parameters in the model are recoverable.

And that's something that, again, I'll expand on.

But the basic idea is, or one kind of simple way to think about it, is what if it's the case that two different combinations of parameter values are equally good?

at reproducing or explaining a given participant's behavior.

If that were the case, then there isn't any unique combination of parameter values that is kind of the best set of parameter values.

And what that means is that you could take one set of parameter values, generate behavior with it, but when you ran estimation on that, you could get a very different set of parameter estimates.

And if that's the case, then the parameter estimates that you're actually getting for that model for a participant aren't necessarily a reliable or uniquely best description.

And so that means that parameters aren't recoverable or there's no kind of uniquely best set of parameters under that model.

So you need to do that to make sure that's true for the winning model.

And if it's not, then you might move down to say the second best model and see if that's the case.

Right.

So you want to find the best fitting model.

Um, ideally you want to find the best fitting model that is also clearly recoverable that has clearly recoverable parameters.

Um, and finally, once you, um, have identified the winning recoverable model, um, um,

you have this set of parameter estimates that describe each person.

So these are individual difference variables, right?

One person has higher action precision than another.

One person has a more precise preference distribution than another.

And at this point, you can take these up to the group level and you can say, okay, you know, for instance, does a healthy group have different parameter values on average than say a group with depression or a group with anxiety?

Or you can say, you know, do parameter values predict something on a continuous, you know, in a continuous way, right?

Maybe higher anxiety levels are associated with lower action precision or something like that.

And so I'll show you guys examples of this kind of thing as well.

So first, and you know, we've kind of already covered this in previous sessions or just examples of it.

But there are lots of different kinds of tasks that you can model, which is kind of a nice perk of active inference models, is that they're not just about decision making as a main component.

You can also use them to model kind of simple perceptual tasks.

So Christopher, in a previous session, showed how you could build a model of a simple perceptual oddball or a local global task, which is

almost entirely about perception.

You won't get necessarily interesting individual differences in behavior in these tasks, but you can show differences in, say, simulated event-related potentials in EEG studies.

And in principle, you could fit the ERPs to find the best parameters for a person.

But you can also do inferential or prospective decision-making tasks, like where you

plan for the future and make a decision based on what you expect future outcomes to be, which is the kind of thing, kind of like what I'll be showing you or with the explore exploit task.

But the explore exploit task that we'll be showing is kind of very similar to a standard kind of reinforcement learning task.

It just requires that you seek out information before you can really know or even learn what the right reward values are.

Then lastly, you could combine this with neuroimaging in the context of predictions from the neural process theory, which is what we've talked about and what we covered in relation to the hierarchical model that Christopher covered.

I should say that this is a fairly new emerging field.

There's not that many empirical papers that do this thing.

You know, it's something that my lab has kind of been trying to make more common practice, which is, again, part of the motivation for this tutorial is so that a larger number of researchers can do this kind of thing.

But so, you know, I mainly work in computational psychiatry.

And so, you know, this has been used to show, say, for instance, less precise, lower action precision in substance use disorder

and also slower learning rates for negative outcomes in substance use disorder.

You know, we've shown things like that greater decision uncertainty is associated with people with depression, anxiety, and substance use in the context of approach avoidance conflict tasks.

Most recently, we also used kind of a simpler Bayesian perception version of an active inference POMDP to show

differences in the, um, sensory precision in, um, in an interoceptive context.

So for instance, when people, uh, um, how, how precisely or accurately people perceive their own heartbeats, um, and interesting differences there, um, between, uh, again, clinical, multiple clinical groups and healthy, uh, participants.

Um, and then lastly, um, and, you know, one of the, um, first papers using this kind of approach was actually Philip Schwartenbach's, um, paper where they looked at, um,

predictions about mid-range dopaminergic responses and changes in the expected free energy precision term that we had talked about in previous sessions.

And I was able to show nice relationships between these expected precision updates and

neural responses using fMRI in a region of the midbrain that is known to be rich in dopamine neurons.

These are recent examples that we're hoping other researchers can build on.


SPEAKER_01:
On that neuro slide, these examples, we're seeing a couple examples of different technologies like fMRI, but we also heard it goes beyond neuroimaging, the scope of these methods, and we're also seeing a couple of biological questions or conditions.

related to your clinical experience let me complement those two axes of variation with the methods and the biological question with this algorithmic dimension and there's a question in the chat that says what kinds of tasks reflect the relationship between dynamic programming and active inference for example bellman optimal state action policies thanks for the elaboration so how do we connect some of these biological and methodological

uses and axes of variation to some of the algorithmic questions about optimization, dynamic programming.


SPEAKER_03:
Yes, I mean, it's a great question.

We actually have a recent preprint that Lance Dacostas, first author on, where he was able to come up with some proofs to show when active inference is and is not Bellman optimal.

And in terms of what the conclusion was, in terms of one step policies,

Active inference is Bellman optimal in the context where the precision of the preference distribution is maximal.

It's maximally precise over whatever the reward outcome is.

Active inference models in the simple POMDP scheme we've been talking about here, they're not Bellman optimal or they're not guaranteed to be.

in the context of multi-step, so like deep temporal policies.

But whereas the more recent sophisticated active inference algorithms are Bellman optimal for deep policies, that's because they more or less correspond to backward induction.

So that's the probably shortest answer I could give to that.


SPEAKER_00:
Christopher, did you have something to add there?

Uh, no, I, yeah, I was just going to point to the Lancelot DaCosta paper.


SPEAKER_01:
Great.

I posted the DaCosta papers that I think are relevant in the chat.

So thanks for that question and continue.


SPEAKER_03:
Okay.

So just, just to be clear, this is a very specific paper that, you know, Lance and I and Nora and a few other people put out as a preprint just a couple of months ago.

Um, so, so I'm talking about one particular paper of, of Lance's, um,

So, I mean, I can certainly point that specific one out if needed so people know which one I'm talking about.


SPEAKER_01:
The relationship between dynamic programming and active inference to the discrete finite horizon case with... Yep, that's the one.

...Dacosta, Sajid, Parr, Fristian, and Smith.


SPEAKER_03:
Yep.


SPEAKER_01:
Okay.

Cool.


SPEAKER_03:
All right.

So hopefully that helps.

Okay.

So now I'll kind of be going through each of the steps here that I was talking about

kind of one by one, right, with our explore exploit test example.

So first, step one is you have to have participants perform a task, right?

So in this case, and I should say for some of this, it's really going to be assumed that people were following along with previous sessions.

The time constraints just really don't allow us to cover all the background so that somebody could watch this one without having seen the other ones and fully follow everything I'll be talking about.

So just to make that clear, if anything is kind of confusing, then the previous sessions are kind of necessary.


SPEAKER_01:
We walked through the matrices and some of the degrees of freedom in this setup.

So I think it should be good.

Check out the other videos if you haven't already.


SPEAKER_03:
But as a brief kind of refresher about the task that we're talking about, on each trial, the agent starts in a start state.

And then it can either directly choose one of two slot machines.

And if they're right, they'll win $4.

But if they're wrong, they'll win zero.

And they start out not knowing, you know, like just flat prior over whether the left or the right one is, whether it's the context in which the left one or the right one is more likely to win.

So it's just 50-50.

So it's pretty risky to just pick one right away.

At least before you've learned if one of them is actually more likely, if the context is

in which one is more likely to win is more common.

So instead, the other thing that the agent can do is they can choose to get a hint.

And the hint will tell them whether it's a context in which the left bandit is the one that is more likely to win and when it's the right one.

And in each of the contexts, in the context where the left one's better, it will pay out 80% of the time.

Whereas in the context of the right one, the right one will pay out 80% of the time.

But the catch is if you choose to take the hint, then you'll only win $2 if you get the right one afterwards.

So this trades off this kind of reward seeking and information seeking where if you seek information first, you're more likely to win.

But if you choose first right away, then you'll get a higher reward if you're right.

So it specifically trades off reward and uncertainty.

And this is where a lot of this is not going to make sense unless you've seen the previous sessions, is that the second thing is you have to build one or more models of the task we just talked about.

In our example, we'll use two different variants of the model, one where we're just going to fit the risk-seeking parameter and an action precision parameter, and a second model where we also assume that the agent has a learning rate.

or that there are differences in learning rate.

So for those that don't remember, the risk-seeking parameter just corresponds to the magnitude of this RS value.

So what this says is at time two, if the agent observes a win, so this row is a win, then the value of that win will be RS.

And if they instead wait and choose the hint, they'll continue to observe this start thing at time two, but at time three,

If they win, then the value they will win will be this RS value divided by two.

So whatever RS ends up being, the win at the third time point if they take the hint is half of that.

So we're going to be fitting for each person what that RS value is, which again stands for risk-seeking, which would be clear to anyone who's watched the previous sessions because the higher this value is, the less exploratory drive the agent will have.

Um, and that means that there'll be more likely to take the risk and just, um, choose one of the slot machines randomly, um, as opposed to taking the hint first to be more confident in which one's right.

Um, so the second parameter is this action precision parameter alpha.

Um, and all this basically says is the precision or the probability of selecting some action given an alpha value corresponds to this.

So it corresponds to whatever the probability is of an action given a policy scaled by this alpha thing and then soft maxed to become just a proper probability distribution again.

So it just controls how more or less it controls how precise action selection is given the choice of policy.

And then finally, in the second model where we're also going to assume an agent has a learning rate,

other than just the optimal learning rate of one, then we're also going to be fitting this thing eta here, which is the learning rate.

And this just says that your beliefs about the probability of the context being that the left slot machine is better versus that the right slot machine is better just updates based on your beliefs about whatever probability

that state, whatever the winning state was on the previous trial, again, four times step one.

So basically each trial, whatever the posterior beliefs were over states, the count for that just gets added to whatever the prior over initial states was, is at the next time point, but it's scaled by this eta thing.

So if eta is one, then it'll tack on a count of one,

Again, if it knows with certainty that it was in, say, like, state one.

But if eta is 0.7, then it'll only add a count of 0.7 on each trial, given that the posterior distribution was, like, 1, 0.

So that's the basic setup is we're going to fit either just RS and alpha, or we're going to fit both of those and eta.


SPEAKER_01:
One note on that, Ryan, I like this step to build one or more models, because we're thinking about active inference as like this process theory, it's instrumental, you know, the whole instrumentalism versus realism question, we're using it as an instrument.

And that means that we're iterating over models, we're relating our models to each other in specific ways, like make the simpler one, and then make one that just adds one more variable.

And then it's not like you're asking, what is the active inference model here?

It's

here's the active inference approach and we're going to combine our perspectives and make multiple models and we'll be comparing models.

So the only limiting factor there is how many models can we think of at the beginning and how wise can our model selection process be?

So it's just really like a pluralistic but pragmatic way to go about it by saying construct one or more models because then you're never going to fall in the trap of thinking you're making the active inference model.

because there isn't the model of anything statistically, they're just instruments.

So it's a really helpful perspective.

And so this isn't just a little convenience to make one or more models.

It's something that reminds the scientist or the modeler that they're actually making one of many or infinite possible ways to think about a situation.


SPEAKER_03:
Yeah.

I mean, I mean, hopefully you've really kind of, you know, thought deeply about what the most plausible models that are given current theory and, you know, current previous or previous empirical

research right so I mean they should be informed but yes so you're just you're trying to find the model that has the highest evidence that the observations or the behavior provides the highest evidence for and yes it is always possible that there's some other model that you haven't thought of that would be better but you know if you have a model that explains behavior really well and it does so better than any of the other ones you can think of then it's kind of a good bet as at least giving you

Um, interesting individual differences to, to again, bring up to a group level analysis.

Um, so, so, okay.

So, you know, to get kind of a little nitty gritty in terms of the code and the MATLAB structure and everything, um, you know, the kind of thing you need to do is you first, so you have a participant's behavior, right?

Like they chose the hint and then slot machine one, they chose the hint and then slot machine two.

Okay.

Now they just chose the slot machine right away.

or slot machine two right away, and so forth.

Those have to be coded in to each trial in the MDP structure, which again, previous sessions will explain.

The MDP structure in MATLAB, each number inside these parentheses here will be the trial that you're talking about.

The dot u here is the,

the matrix that described that encodes the actions that a participant took.

Now, if we remember, so row one here corresponds to state factor one, which is the context, whether left or right is better.

And there is no action for that.

That's not something agent control, it's just stable within a trial, which one it is.

So these just get ones and there is no other possible action.

For the second row though, so this is the state factor that corresponded to action selection.

Two, you know, action two corresponded to taking the hint and action three corresponded to choosing left.

So if that was what a participant did on trial one, then you would have to feed in two followed by three, right?

They chose the hint and then they chose left on that trial.

And so you just have to iterate that over all the trials and the task.

So one to, you know, end trials, whatever they did.

So that's MDPU, and in this case, like I said, there are two actions, so two columns, and there are two state factors where only the rows, which only the second one is controlled.

So now MDPO.O is the observations.

So again, if you're familiar with the way we set up the model for this task before, there are three different outcome modalities, three different types of observations you can get.

So the first observation modality was a hint or not.

So in this case, one is just you're kind of still observing the start observation.

If you observe two, that means you get a hint that the left one is better.

And if you observe a three, that's the hint that the right one is better.

So in this case, they got the observation that the left one is better when they took

one, two, one, because after they got the hint, they just returned back to observing kind of the initial observation for the hint modality.

Whereas for the second modality here, which is the wins losses, for the first two time steps, they just observed kind of the starting observation here because they chose the hint at the second time point.

So there was no win or loss.

And then at time point three, they observed a win.

So win is encoded as a three in this model and

losses is encoded by a 2.

And then the final ALCO modality is just the agent observing what it does.

So in this case, this just says started in start state, chose to take the hint, and then chose the left.

It observed itself choosing the choice of the left slot machine.

So that's it.

So all you have to do when you're

instead of simulating behavior, you're actually feeding in participant behavior to fit it, to fit a model to it, then you just have to sort of translate the behavior and the raw data into a form like this that matches the action and outcome representations in the model.

And so that's it.

You first feed those in, and then once you've done that,

then you have to use some kind of estimation algorithm, which basically means the thing is going to somehow repeat simulating behavior in that model until it finds a model that maximizes the overlap between the probability of choosing actions and what the participant actually chose.

So for instance, if it chose hint and then left-banded,

left slot machine on trial one, it's going to find a set of parameters that apply across all trials that maximize the probability that the simulated agent also would choose the hint followed by choosing the left slot machine.

So there are a number of different estimation algorithms that can be used.

The simplest possible one is just something like a grid search.

So in this case, say you just have two parameters, which this is just arbitrary called alpha and beta.

You can kind of divide this up into little grids, different combinations of parameter values.

So for instance, like, you know, parameter 0.2 for alpha and parameter value 3 for beta.

And then you can just encode, for example, what the probability of the model is given the participant's actions.

Or

that would be the posterior, which I'll talk about, or just the probability of participant behavior given the model and those parameter values.

So in this case, there's this kind of clear, nice result where this value right here, something like 0.3 and 4, that is maximally and uniquely the best set of parameters for reproducing that participant's behavior.

And in this case, you're using something called

In this case, you're using something called maximum likelihood, so maximum likelihood estimation, which just means, again, you're trying to find the model and set of parameters for which the probability of the participant's behavior given that model is highest, which is a type of likelihood.

You're trying to find the maximum value for the likelihood, which you just, in this case, encode with these redder colors,

equals higher likelihood of behavior given the model.

You can also do something a little bit.

To be clear, what we actually would want is the inverse of this.

We would want the probability of a model given participant behavior, which is the posterior.

But when you're doing maximum likelihood estimation, you're just assuming here that you have essentially a flat prior for

what the probability over models is.

So in this, if that's the case, then the probability of the model given participant behavior is just proportional to the likelihood.

But another thing you can do is called maximum a posterior or map estimates, which is the same thing, except it doesn't assume that you have a flat prior over models, right?

So if you just start out with a prior expectation,

that one model is better than another, then you can incorporate that information into parameter estimation.

And this is kind of an example of this where you might find this distribution in terms of just the likelihood, the probability of data given the model where a parameter combination right around here would be best.

But then you might also have some reason to have a prior expectation that models down here are actually more likely

And so if you combine those guys together, you can get a posterior that looks a little different, right?

Like something very similar, a very similar model wins in this case, but that's just because the behavior that's driving the likelihood here is just already a really good fit.

So it kind of dominates over the influence of priors, but that's not always going to be the case.

So those are two approaches.

And again, you can do them with a simple grid search, or you can do something.

So I should say that the grid search kind of thing, it only really works when you have a few parameters, when you don't have very many.

And the reason is because the more parameters you have, the higher dimensional this parameter space becomes, and it just becomes, it takes sort of intractably long to do, to search through every possible combination.

So what you do instead is you use some kind of gradient descent process exactly like the gradient descent on free energy that we've been talking about for how active inference models arrive at posteriors over states and posteriors over policies.

So there's a really kind of interesting overlap between the way that you estimate models via gradient descent and the way that you actually, the way that active inference models

update their beliefs via gradient descent.


SPEAKER_01:
So in this case... It's a very good point.

I just want to point out one similarity and difference because people may have heard about these gradient descent algorithms for descriptive statistics or for fitting descriptive models.

So that's from going again from the data that's empirical to a descriptive model or descriptive statistics like regression coefficients

And if it's a multivariable regression, you might need to use not just a grid search, but act that you can do a simple maximum likelihood approach, like a least squares approach or something like that.

You need to use this kind of a complex multidimensional optimization to get to that descriptive statistic.

And then there's this little twist where actually in active inference, we might be using those computational techniques, but we're estimating the parameters of an underlying generative model.

And then there's this nice little return where actually the generative model is implementing that as well.


SPEAKER_03:
Yeah.

So in this case, you absolutely need some kind of prior value.

Um, because with gradient descent, you're not exploring every combination.

What you're doing is you're starting at some starting point, which is just coded by a here.

Um, and then you're just like when you're minimizing free energy in, in an active inference model, you're searching neighboring values and trying to find a value that has, um, a higher likelihood or a lower free energy.

Um, and then you just kind of keep doing that iteratively.

until you find some value that where the likelihood stops getting bigger or free energy stops getting smaller.

And that then corresponds to your best estimate or your posterior estimate for the parameters for a given participant.

So when you're doing it with a gradient descent for energy, then that ends up corresponding well to the actual approach that we'll be talking about or actually using in our example, which is a variational base.

So the variational base, you do exactly this.

You set a prior mean value and a prior variance for each parameter, and then you do gradient descent.

until you find a set of parameters that minimize free energy, which is, again, an approximation to the model evidence.

So, but it's important to keep in mind some potential limitations of this approach, which is, if you see here, this parameter space, it does look like there is a kind of nice single, right, like local minimum, right, local place where the

free energy is lowest or where the, again, it would be a local maximum, be like top of a mountain in this if you were talking about the highest likelihood.

They typically do log likelihoods as opposed to likelihoods.

But one of the issues is this sort of, the parameter space need not have a landscape that's quite this clean.

Um, you might end up, for instance, having a landscape just moving to a two dimensional case.

Now that looks kind of like this, where if you start out, say with a prior value with prior values that, um, you know, like this kind of red circle up here, then if you do gradient descent, you might end up getting stuck on a little local minimum like this.

Um, where, you know, and then, and then the gradient descent algorithm would say, Hey, you know, actually, you know, this seems like the best one.

Cause if I move in any direction.

then the free energy goes up.

Whereas actually the global minimum, the one that you would want to get to is this different one that's on the other side of this little kind of free energy hill.

And so, you know, this is an example where choosing the right priors is important to be able to get the best parameter estimates.

But also this speaks to the kind of thing about parameter recovery that I was talking about earlier,

because it could be that if you set your parameters kind of right on the top of this hill thing here, then even if the parameter combination over here is the one that actually generated the data, you could get stuck over in this one, in which case the estimation algorithm would not give you the right parameter estimates.

So this is why what I mentioned earlier about assessing parameter recoverability is really important.

by way of saying it's important to choose good priors or to figure out what good priors are and there are there are interesting ways to try to optimize that so there are kind of hierarchical bayesian techniques we won't talk about these explicitly but where you can kind of and these won't necessarily solve this problem but where you can more or less you can think about it as it estimates the parameters for each person and then it notices hey you know looks like these are all shifted over

you know, the distribution of these looks like they're all shifted over, say to the right, you know, so actually the, looks like the distribution is more kind of over here.

And so then it might choose that, choose that, that the kind of mean value of that as the new priors for, for redoing the estimates until it finds values that, that essentially it's, it's,

helping you to find whatever the optimal priors would be.

But again, that won't always solve this sort of problem with a lumpy landscape.


SPEAKER_01:
You know, one other little thought, it's actually almost three layers with the red dot.

So there's us as agents on a landscape.

We actually need to come up with policy as agents in our niche to get around local minima.

Like my door's closed, I need to step away to open the door before I can move through it.

Then how are we going to fit policies for ourself under pervasive uncertainty?

Well, we're doing this state estimation with a policy so that we can actually come to... And then how are we going to converge on those parameters?

Using...

a little bit like a gradient descent algorithm internally.

Not internally to us, it's just within the agent computationally.

It's very interesting because people may have... I'd imagine there's one kind of person who sees this and says, oh, we talked about optimization for a year in my course, so I've heard all about non-convex and non-convex optimization techniques or rugged fitness landscapes.

And there's another group of people for whom actually this optimization theory might be quite novel because of...

how they've looked at modeling before.

So very interesting.

And to put into the fourth section like that really spoke a lot.

So interesting stuff, Ryan.

I just wanted to ask Christopher, you want to add anything before we continue with this?


SPEAKER_00:
No, this really is very much Ryan's part of the paper.

Cool.

Yeah, yeah.

I'm just here kind of... More than welcome to add thoughts or if you have any other insights about technical aspects.

No, no, nothing really.

I would just...

I might ask some questions if things come up, because I'm going to be doing this at some point in the next couple of months.


SPEAKER_01:
Okay.


SPEAKER_03:
Great.

Thanks.

Okay.

So, like I said, the kind of detailed example that we are going to use is variational days.

And note that variational message passing is, again, what agents are using within

within active inference models, which we talked about before.

So now we're using something very, very similar to this.

So technically, I should say, now it's using marginal message passing in the latest versions, but again, we cover this.

It's very similar to variational message passing.

It's just kind of a slight improvement.

So we're using the same sort of approach to estimate parameters, to estimate the parameters that people are using, the parameters that are kind of, you know,

potentially kind of stored in their brain in some sense.

So we're doing gradient descent on variational free energy, as I mentioned.

And as I mentioned, you need to specify prior means and prior variances for each parameter, and then you just move the prior values in the direction of increasing action probabilities.

But what you're doing, because it's a gradient descent on variational free energy, is you're not technically

just trying to find the maximum likelihood value like you're doing with like a grid search.

Instead, what you're trying to do is you're trying to, well, I should say that the variational free energy part of it means that there's a complexity penalty.

And what that basically means is, so if people remember from variational free energy when we talked about it before, the simplest way or kind of intuitive way to think about it is that it's just complexity minus accuracy, right?

And what complexity means is how much you have to change your beliefs.

So if you start out with particular prior values, then the farther you have to move those, move your beliefs from those prior values, that's going to push variational free energy up.

Whereas at the same time, variational free energy is going to go down as the model predicts behavior better.

So what it means is, for instance, if I start out with a prior value that's way down here, like around, I don't know, one and three, then it's going to have to move from those prior values a pretty long way before it gets to a set of parameter values that fit well.

Whereas, say, if I started over here, it's not going to have to move those as far.

But

if I were to do that, then potentially instead of the posterior estimate, uh, posterior parameter estimates actually settling on this thing that has the maximum likelihood, it might stop, say like around this one, right?

A little lower than it or something like that.

Um, because that maximizes, um, that leads to high accuracy while also not having to move the parameter values, uh, really, really far, not having to change beliefs too much from what the, um,

uh, informed prior belief was.

Um, so it's, you know, it's, it's important, right?

To think that this assumes something that the, that the priors you have, that you have them for a reason, right?

That they're actually based on something that they're informing you.

Um, so that, so that it is actually, does actually make sense not to move them too far from prior values.

Um, and in practice, the reason this is helpful, um, is because,

One thing that often happens when you're just doing maximum likelihood is that that maximum likelihood will be with respect to your particular data set, your particular set of participants.

But often, if you choose just the maximum likelihood value, then that's kind of overfit to just those participants, where if you were to, say, apply that exact same model to a new set of participants, it might not do as well

because it was fitting specific things that were not generalizable about your data set in particular.

So by putting this kind of complexity cost on it, it prevents overfitting, which means that the predictions of that model are more likely to generalize to a new set of participants later.

So it prevents overfitting, which is a very common issue in just standard frequentist statistics as well.

Okay.

So that's what you're doing with variational Bayes, is doing this kind of complexity minus accuracy thing, where you're preventing overfitting while also maximizing the accuracy of model predictions.

OK.

So then, like I mentioned, you need to do model comparison to identify what the best model is.

Now, let's see.

So I just realized I should probably set something in motion here.

Let's see if I need to.

So when we actually go into the code here in a minute, one of these things takes a long time.

Let's see.


SPEAKER_04:
So I set it to store stuff.

I just want to triple check that I shouldn't set this thing.

Let's see.

Yeah, my bad.

I should have double checked this beforehand.


SPEAKER_01:
Yeah.

Cool, though, it's really interesting.

And Christopher, you want to add anything?

Or maybe just a quick note while Ryan's figuring out like, what is it like to be learning these models?

Or what kind of skills do you wish you had earlier when you were learning the models?

Other than, of course, your own tutorial?


SPEAKER_00:
I mean I think everyone who did undergrad psychology is probably at some point in their I did cognitive science as an undergrad but I took a lot of psychology classes when you take staff psychology a lot of people read this Andy Field textbook and he has a chapter on some type of like esoteric regression and the introductory sentence is like I've never done this I don't see myself ever doing it but

I wrote this chapter about it, and if I ever need to do it, I will be very impressed by how much I seem to know.

So that probably will describe my experience with this tutorial to a certain extent.


SPEAKER_03:
So real quick, I'm just going to jump to the code and explain why I should have started this thing.

So if you set this thing to sim equals five, then what it's going to do is it's going to actually generate simulated behavior

for six hypothetical participants where each participant has a different combination of parameter values that's generating that simulated behavior.

And then what it's going to do is it's going to then apply the estimation algorithm to the resulting simulated behavior.

And it's going to give you a set of results.

And then what it will do is it's going to do Bayesian model comparison on those

to identify what the best model is.

And then I'm going to try to do this.

I'm going to try to set the, so I'll just say for the purposes of, you know, you guys actually doing it yourself at home, I'm going to, I've set this thing to 32 trials for each

for each set of simulated behavior.

But maybe if I set this to 16, then it will go faster.


SPEAKER_01:
Just to think about what's happening, people looking through these equations, every time two matrices or two numbers are getting multiplied, the computer has to do something.

So we're kind of nesting matrix multiplications inside of even bigger ones.

So if you want two time steps for two

larger time clicks for four models, you know, for four participants, it really starts to balloon rapidly, especially when there's computationally intensive steps.

Yeah.


SPEAKER_03:
So here, just to show you guys what's going on.

So I just started this thing.

And if you look in the MATLAB window here, it's calculating log likelihood.

So LL stands for log likelihood over and over again under a particular set of parameter values.

And in this case, it's negative 31.

And it's trying to

minimize that where in this case it's moving it closer to zero.

So maximizing in a sense, but bringing closer to zero.

So now it moved via gradient descent to a second set of parameter values.

And now it's found that, okay, now the log likelihood is negative 22.

So we're even closer to zero.

And again, same thing.

Now we're at negative 19 and it's going to keep going until it settles on a stable value.

And in this graph that will update every time,

which I'll show you here.

So this is just showing, ignore, I should say, these, so let me go explain, is these routines were originally designed for DCM, for dynamic causal modeling with fMRI.

So a lot of the, a bunch of these graphs and also the sorts of labels that they have are, don't really apply in this case, they only apply to DCM.

So, you know, so ignore a lot of the labels, but the point here is that,

Each iteration, as you go from left to right, is a new estimate of the log likelihood for the set of parameter values it's trying during gradient descent.

And so you'll see that after a while, the thing is just going to kind of plane out.

It'll converge onto a value.

See right now, it's like 17.35, and now it's like 17.16.

So the thing is kind of starting to converge on a stable

minimum log likelihood.

So now I found a set that actually is still at 16, so the thing is still going for a bit to converge.

Down here, these are the posterior deviations.

Again, ignore a lot of the units here, but the way that you can read this is just that zero here corresponds to the prior values that you set, and if it's going

If the bar here is going down, then that means that the posterior parameter estimate is lower at this point.

It's gone down from the prior value.

And this kind of red-pink thing around it, that's the posterior variance.

So in this case, and parameter one here is the action precision, and parameter two is the risk-seeking, I think.

I can double-check whether those are backwards or not.

Um, but, uh, which order those are, but this is just saying that whatever this first parameter is, um, the, the posterior mean estimate here is, um, you know, whatever that is that actually corresponds to the real units of the, um, of the, uh, parameter, but it's not very confident in that posterior mean estimate.

Whereas here, the second parameter, um, it's also this negative posterior at the moment, but it's very, very confident in that posterior estimate.

So that's what that means.

And these will update with each iteration.

But so now you can see that actually the thing kept exploring and now it's actually found a set of parameter values that continues to actually explain the behavior, the simulated behavior quite a bit better.

So now it's at like negative 13.

So you're gonna see this, it almost kind of converged for a bit, but now it's actually kind of going back up.

So eventually, eventually it will converge, but it's just doing really, really well at finding values that explain behavior well.

One thing I should point out though is that the actual values of these, the absolute value of these are not really informative because they're basically the sums of the action probabilities for each trial.

So these numbers will be bigger if the task has more trials.

So in and of themselves, it's only the relative values that are meaningful.


SPEAKER_01:
This reminds me actually, it reminds me a lot of Bayesian methods in phylogenetics, where it's like, what is the likelihood of this phylogenetic tree?

It's given some number by a program, which sounds weird to think about the state space of all the possible trees or something like that.

But it turns out by doing this kind of a gradient descent, searching through all the possible trees, you actually do converge on a tree that is generative of the kinds of data that are observed.

And so it's just really interesting to see how this is working and it's fun to watch the number drop down too.


SPEAKER_03:
So, so now, so in this case it converged for the first person.

Um, and you know, these were the posterior deviations and then here's the simulated behavior, um, of the participant under those parameters.

So you can see that, you know, the parameters do fit the behavior really well, right?

The probabilities are pretty high under those parameters for each.

for every action.

There are some little exceptions, but it does pretty well.

So now it's just moving on to the next person.

Sorry, what?


SPEAKER_00:
It might just be helpful to describe what the blue dots are.


SPEAKER_03:
Yeah, sorry.

I'm assuming people have watched previous sessions.

So blue corresponds to the actual chosen action at the first action for each participant.

So basically this is saying, and so black means

probability one.

And as it goes toward white, that means probability zero.

So this is just saying basically 100% probability under the model that the agent would choose the hint and the blue dot says that's what they actually chose.

And so on and so forth.

So anytime that there's a light gray, you know, that's on the part where the blue dot is, that means the model didn't really do that well at predicting that behavior.

But you can see in this case, most it's pretty dark around most of

these blue dots, so it does pretty good is the point.

So now, like I said, this is just going to iterate around for the six agents that I mentioned, and it's just going to compare the three that did have a learning rate and the three that didn't have a learning rate in the model is what it will do.


SPEAKER_00:
And so just to kind of summarize what you've said so far.

So the steps would be something like you would actually get your empirical task, right?

And think about how the actions that the subject has actually got or the participants actually going to make relate to what the model is going to make or the actions available to the model.

You'd come up with some mapping and then you would then presumably just translate them in some way.

and plug those into your u, mdp.u, which is the actions participant shows, and mdp.o, which is the observations that they actually saw.

Yep.

And then you would plug into the algorithm, essentially.

Yep.


SPEAKER_03:
You plug it into the algorithm, and then the algorithm just repeatedly computes the sum of the log likelihoods for each trial, and then tries to find a minimum of that sum.


SPEAKER_00:
Okay.

And so in terms of, this is something I've wondered about, how do you choose what the best trials are or what the best priors are?

Is it okay just to kind of in silico simulate a bunch of things and just say, hey, this seems reasonable?


SPEAKER_03:
In a lot of cases, yes.

I mean, so there's a couple of things to do.

One is you can do the kind of modelist or the recoverability stuff, you know,

beforehand and try to find a set of parameter values, a set of prior values for which the parameter estimates are recoverable.

So if you set one set of priors, then maybe the thing does get stuck in local minima.

But if you set another set of priors, then the generative parameters actually do end up matching the estimated parameters really well.

So there's a couple of different things.

yeah, do some of the simulation recoverability stuff ahead of time to find good priors.

Another thing is, you know, if you just, you know, estimate participant behavior and you start to see that the posterior estimates tend to be really far away from the priors you're setting, that's kind of an indication that you're probably not setting very good priors.

So then you might like try setting new priors that look closer to where everybody's kind of moving.


SPEAKER_00:
Yeah, so we do have a footnote about this in the paper from memory.

But it would be good to kind of make that explicit.


SPEAKER_01:
Another way that I've seen that in the field of evolutionary biology is if you have wildly disparate priors.

So a fallacy is that the uniform or the flat prior is unbiased.

There's so much to say about that.

I'm sure Ryan knows well.

But if you have one prior that's stacked up against zero and another prior that's stacked up against one,

And then they both converge like from kind of multiple sides.

That's a simple example.

But if different families of priors and models that are very pared down, ranging to ones that have very complex error models, if a lot of different layers of complexity of the model and densities that start stacked up versus one end versus another, then of course, that's a difficult thing to manage.

But that at least means that given the empirical data you have and the

task that you're modeling, you have a really predictive model.

So again, we're not realists.

We're not actually getting at the truth with these recursive and iterative and multi-perspectival models.

We're just fitting more and more of the variance explained in our empirical data.


SPEAKER_04:
Yeah.


SPEAKER_03:
Yeah.

So just to kind of give you guys another example, so you can see now for this other set of parameter estimates, you know, convergence took way less time, right?

It just took, you know, seven iterations and it converged really quickly.

And one reason to see why is that for this one, it didn't have to move values very far from priors because this was an agent

whose behavior was generated by parameters very close to priors.


SPEAKER_01:
One other point there, though, is even if it looks flat for 10 time steps, it could still be trapped, which is why things have to be seeded and have really good randomness, because there's no hard and fast rule for when you terminate.

For example, in a lot of the evolutionary simulations we would do, you discard the first 10,000 or more

time steps like burn in, you just discard them, because you think that actually, it's too much reflecting your prior estimates.

And then even if it looks like it's converging, or as one of my professors called it a fuzzy caterpillar, because it's kind of like the model was sampling from the fullest possible range of variables, and it was still coming back home.

That's the fuzzy caterpillar.

But even then, it could look like a fuzzy caterpillar for like a lot of time steps, and then just totally hit on a new combination break into a new route.

So it's really are in a challenge.


SPEAKER_03:
So just so people know that's actually not true in this case.

So variational Bayes is a, is a deterministic in the sense that, um, you, you don't do this kind of, you know, burn in, you don't, um, variability.

You just deterministically, like if I ran this over and over again, it would give me the exact same parameter estimates each time.

Um, so there is, there is actually no variability.

I mean, the kind of thing you're talking about is more like Monte Carlo.


SPEAKER_00:
um yeah sorry i didn't mean to say that there was a burn-in um i agree that was just yeah little analogy but thanks for clarifying so one advantage of monte carlo sampling methods is that you over given infinite time you're guaranteed to margin to approach the true posterior or to obtain the true posterior uh it's also extremely computationally expensive um variational base is really quick

You can run this stuff on your laptop.


SPEAKER_01:
The Monte Carlo is based upon sampling.

That was the different domain that I was referencing there.

You have to sample in cases where you don't know some of these distributions, don't have them defined so perfectly.

But when you have access to this level of specifying, then there's a whole new range of techniques, which is why it looks more like matrix multiplication

and this sort of like convergence to the variational free energy estimate, rather than just sampling endlessly from a landscape that's of unknown, you know, anything else.

But yeah, thanks for that.

Sorry.


SPEAKER_03:
So just to, I mean, show again, just to give you guys a sense.

So in this, so this kind of participant, you can see that it's pretty kind of, the distributions are a lot less precise.

And this is a person who has a much lower action precision value.

So these were generated by a lower action precision value agent

And the model is doing a pretty good job just finding a low action precision value that kind of spreads the distributions flat enough around it that it captures, it provides decent evidence for each action.

And so anyway, so just to, again, to give you guys an intuition for what's going on.

But so once this is done, then what will happen is we'll have these, the free energies of the winning team

model for each person, for each of the two models, the one with and the one without learning rate.

And so what we want to do is we want to do Bayesian model comparison, which is where you're going to compare the free energies for each model for each participant to find the model with the lowest free energy across participants.

And the winning model, so the little SPM function that we include here,

It'll spit out several things, but the one that you want to focus on, just to keep things simple, is the one that has the highest, what's called the protected exceedance probability.

And this is just the probability that each model is the most likely model across all subjects while taking into account the null possibility, the differences in model evidence are due to chance.

So like I said, it's just which is the best model when taking into account the null model.

um, as, as a possibility.

Um, and I'll, um, I'll show you that, um, in a second.

Let's see, is this thing still going?

It's, uh, yeah, okay.

Still going.

Um, hopefully it'll be done soon, but so I'll keep, I'll keep moving through here and then we can return to this once it's done.

It's actually gone pretty fast since I made it so few trials.

Um, but so, so that's, you know, what we're doing here.

Um, and, uh,

So now, you know, we've kind of already touched on this a bit, but so now is the point where we would confirm that the best model is identifiable, you know, or that the parameters are recoverable.

And this is, so I already mentioned this a little bit, but that, you know, it's clear that not all models are necessarily going to have unique parameter solutions, right?

So if you have a landscape like this, you might start out priors at very similar places and gradient descent would lead you to

different combinations of parameter values that are equally good at reproducing a participant's behavior.

So you always have to show that whatever model you're using and the priors that you're using will give you the same parameters that you fed in to generate the data to begin with.

And so here,

Again, we already talked about this a little bit, but step one is you would specify multiple sets of generative parameter values, which is what we're doing.

This is important as you should select values that are the same or similar to the actual parameter combinations that you got in your true participants.

Because it can be the case that parameter estimates are totally recoverable for certain parameter combinations,

but are not recoverable for other parameter combinations.

So you care about the ones you're actually getting for your participants.

So then you want to simulate behavior.

So generate simulated behavior using each of those parameter value combinations.

Run that simulated behavior through the estimation routine, just like you would for real data, and then check whether the generative parameters and the estimated parameters are highly correlated.

When I run this in advance, so without having to run it in real time like I'm doing right now, then for this particular case, and bear in mind, this is when I'm using 32 trials, not like the 16 or whatever I put in now, just to make things go faster.

This is what I get for alpha, for action precision for the two parameter model.

So you can see, even with only six people, the correlation between the generative

uh, action precision and the estimated action precision is pretty good.

It's 0.81.

So even with six people, right, it's significant.

Um, um, whereas for, uh, well, anyway, I have a, I have a bunch of them.

I can, I can, I can pull up, I could probably pull them up.


SPEAKER_04:
Um, let's see.

That's for a totally different thing.

Um,


SPEAKER_01:
Cool.


SPEAKER_04:
That was very tutorial.

Interesting.

Yeah, sorry.

I'm just trying to find the figures that I have.

There's probably here.

Well, no.

Okay.

Not positive where I put them, actually.

But if I can't find them here, then it will

it will spit them out.


SPEAKER_03:
Um, but, um, but point, point being, um, these are, uh, um, in this case, even with just six values, um, all of the parameters, um, tend to be really good, right.

In terms of recoverability, the, you know, the correlations between generative and estimated parameters tend to be, you know, between 0.7 and 1, um, 0.7 and 0.9, something or other.

Um, so they're, they're, they're good.

Um,

So then finally, the last thing that we wanna do is once we have parameter estimates for each person in a winning model that is recoverable, then we can take those values and we can put those parameters, we can use those parameters as individual difference measures between subjects.

And at that point, there's a number of things that you can do.

So the kind of simplest thing, if you wanna kind of fall back on

on frequentist statistics is you can do standard t-tests, ANOVAs, correlations, regressions, et cetera.

And we've done that before.

It's fine in some cases.

If you want to kind of stick with the more kind of general Bayesian theme of active inference, then you can also do things more like Bayes factor analyses.

And both of these are often kind of used

or even use together.

And just to give you a couple examples, sorry, I'm just going to triple check that this thing is not done.


SPEAKER_01:
Nope, still not done.

Just one note there is the SPM textbook has many points of contact between parametric and standard, very classical statistical approaches, and then very Bayesian approaches and mixed methods, and it will switch out or show it both ways.

So it really is pretty interesting how it comes together here.


SPEAKER_03:
So to kind of show you guys a couple examples of how you would use these kind of

you know, use parameters at the group level.

Um, you know, this is just, I'll give you a couple examples from, from our papers.

Um, so in this, in this one in journal of psychiatry and neuroscience, um, what we did is we had a, this simple kind of approach avoidance conflict task where more or less the, uh, participant had to kind of choose to move this little avatar guy closer or farther from one of these two ends of this, uh, little kind of runway thing.

Um, and they knew that, um,

the closer they were to one side, the higher the probability was that they would get an outcome associated with symbols on the left side.

Um, and same goes the closer they are to the right, the higher probability of getting the outcomes associated with the right side.

Um, and here, uh, rain cloud means they heard like a really aversive sound and saw like a really aversive picture, you know, like it's like hearing girls screaming and seeing a picture of her being like pulled into a van, you know, like really negative stuff.

Um,

And whereas the sun thing here meant you kind of saw this kind of like neutral, maybe slightly happy thing.

And you had this kind of red, this kind of bar thing on the side.

And the more filled the red bar was, the more points they would win.

So you have kind of clear cases, right, where it's just if it's just rain cloud versus sun, you should just go to the right.

If it's just sun and sun plus some reward, you should want to go to the right.

And then you have these conflict cases where it's negative stuff.

plus you get some reward, negative stuff, plus you get even more reward and negative stuff and even more reward than that.

And so that's how the task is structured.

And so you can make a pretty simple model of this where one state factor is beliefs about the trial type, right?

Whether it's this kind of trial, this kind of trial, this kind of trial, this kind of trial, or that kind of trial.

And then you can have a state factor corresponding to

beliefs about the runway position, right?

So beliefs about whether the avatar is in position one, two, three, four, five, et cetera.

That's it.

And then you just have, you set up likelihoods that generate the probability of each position generating what sorts of outcomes and what sorts of runway positions given beliefs about trial type.


SPEAKER_01:
Can I ask something here, Ryan?

So you have it phrased as there being five trial types, but would it be possible to have like a model where it was like a state estimate left and a state estimate right for the stimuli type and a state estimate left and state estimate right for the reward value?

Because this frames it in a very behavioral trial model.

centric way with doing estimate on which one of the five scenarios you're in.

But I'm just trying to think about cases where you might not know which scenario you're in or even what the total set of scenarios is.

So you're just doing stay estimate on reward and on stimuli.


SPEAKER_03:
Yeah, I mean, yes.

Well, I guess it kind of depends, right?

So, I mean, if you were to give somebody sort of uncertain cues about trial type, then, I mean, presumably you would just

you wouldn't need to, I mean, you would just still have, um, if they know what the different trial type possibilities are, then you could just have uncertain cues.

They just don't have a precise, uh, belief, a set precise prior belief over the different, over this state factor.

Um, but if you didn't, if there wasn't any set beliefs about trial types and there's just any possible combination of, you know, sun cloud, zero points, two, four, and six, then, um,

Um, I suppose you could do something where you just have a kind of non factorized state factor that has like just every possible combination, um, or something like that.

Um, you could do that.


SPEAKER_00:
I mean, it would, uh, um, there are reasons to actually build like, so there are obviously time saving and model building considerations that mean you should use factorized distributions.

But there are actual empirical considerations, which means that I think if you want to build a model that's kind of realistically how the brain works, you should tend towards, in a lot of cases, you should tend towards factorized distributions.

So, for example, there's good evidence that there's a factorized representation of task phase in PFC and MTL.

There's pretty good evidence.

So, I mean, the what versus where streams in vision, that's a factorized representation.

Um, so


SPEAKER_01:
Very, yeah.

So you're kind of using it in like a coarse graining sense to say sometimes you don't want to even allow for the all by all because it's more categorical how the task is being modeled anywhere, like fight or flight.

You wouldn't want to have all combinations of elbow and knee movements.

You're fitting something that's kind of at the wrong dimensionality.

And so for a lot of reasons, not the least of which it seems like that's what organisms would do would be at that high dimensional manifold or the factorized representation.


SPEAKER_03:
Yeah.

Well, and, and in this case, right.

I mean, just, we have experimental, the experimental design is such that they are, they do know ahead of time what the different possible combinations are.

Right.

So, I mean, they just, they just already know that there are these combinations.

Um, so it's, it's consistent with their beliefs about the generative model that we have given them via the instructions.

Um, so, um, so, so in this case, um, you know, we had a, a pretty

uh, typical, um, you know, sort of, sort of model graphical model, um, with two parameters, we had our, uh, beta here, which corresponds to the expected for energy precision, um, here, you know, just to be intuitive for the, uh, clinical audience that we were going for, we just called this decision uncertainty.

Um, and then we had a, and so that's just, this is, again, this is the rate prior, um, or rate parameter for a gamma distribution, uh, over, uh,

over this gamma term, which again is a thing that modulates the expected free energy estimate over policies.

So it modulates this G thing.

But then the other parameter we had was this emotion conflict parameter, which just said basically how much they dislike

the negative stimulus, how, how aversive they expect the negative stimulus to be.

Um, and so you can kind of show in simulation.

So each one of these, uh, vertical bars here corresponds to beliefs about, uh, the, what state you are in on the runway.

Um, so if beta equals one and, uh, the, um, and, uh, the emotion conflict equals one, then you should expect that if the good thing is on the, uh,

Okay, and this is a conflict plus two points trial.

So basically, the thing will approach the reward even if it's going to see something negative, if EC is one, and it will do it deterministically.

Whereas if EC is three, then it will fairly deterministically choose to go away from the negative stimulus, even if they would get reward.

Whereas if they have higher values, so more decision uncertainty,

then this distribution becomes, they become a lot less confident in this distribution and end up choosing these kinds of values that are more like in the middle.

And the likelihood is pretty clear.

It's just in each trial type, each of the different, each column here is a different runway position.

They will just generate the negative stimulus with an increasing probability as it goes left and the positive stimulus with an increasing probability as it goes right.

Um, et cetera.

Uh, the only confusing thing here is, is that white means a higher probability in these, whereas blocking means higher probability here.

But, um, but anyway, so that's, that's it.

And just kind of, et cetera, et cetera, for the five trial types.

So in this case, what we found was if you look at healthy controls versus people with depression and anxiety versus people with substance use disorders, um, the emotion conflict

is actually interestingly higher in healthy and lowest in substance use disorder, whereas decision uncertainty is highest in substance use disorder and kind of medium in depression and anxiety and low in healthy controls, and these are significantly different.

And that's true in a propensity-matched sample and in a larger sample.

So it's an interesting thing where it might be more clinically relevant as this kind of uncertainty over options as opposed to just being more sensitive to negative stimuli.


SPEAKER_01:
I'm just kind of curious if it's not known, but how would that shape treatment or conversation or approach in a given situation from whichever role makes sense?

Like to say, oh, it's not actually this psychological construct, but it's actually related to something like this.


SPEAKER_03:
I mean, in terms of like informing treatment or something like that?


SPEAKER_01:
Sure.

Like treatment via any modality.


SPEAKER_03:
Um, yeah, I mean, so, I mean, typically the, the main sorts of things that you might want to do is, you know, either talking about, um, so say, you know, at baseline before somebody starts treatment, what their beta value was, right.

It might be the case, you'd have to do a study to show this, but it might be the case that given different beta values at baseline, you know, like people with high beta values might respond well to CBT.

cognitive behavioral therapy, whereas people with low beta values might respond better to ACT or might respond better to an SSRI or, you know, like something like that.

So either it's something, I mean, that's kind of the ideal thing is you want to say, can I get this information?

And it will give me information about how to treat a person.

But there are lots of other things you might do besides that.

But that's like a primary kind of, you know, like ultimate goal example.


SPEAKER_01:
Yeah, it's interesting how there's probably a lot to be said and learned about the actual application of active inference clinically.

But even here on the beginning of applying it, we can use it as just a potential biomarker, just like a summary statistic related to a questionnaire or some other thing that's estimated from empirical clinical data.

fitting a different kind of underlying generative model.

So instead of doing a regression, oh, people who have higher on this end up doing better or worse in this kind of a program.

Well, now we can just do that same kind of parameter testing in the context of a different type of model.

It's an active inference generative model.

So I think that those are just some points of contact, but it's really interesting stuff.


SPEAKER_03:
Yeah, definitely.

I mean, you can do lots of other interesting things with these, right?

So this beta parameter, if you remember from previous sessions, is what's associated with...

It's proposed to be associated with dopamine dynamics in the brain.

And I'll show you briefly a study, an example study later, where Philip Shortenbeck, like I mentioned before, actually showed that the trial by trial updates in beta that are predicted by the model were correlated with a bold signal in fMRI

in the midbrain, in a midbrain region that's associated with dopamine.

So whereas the way that we're doing this here, we just have a stable beta estimate for each person, but you might look at, say, individual differences between contrast values in an fMRI analysis at the group level.

So people with higher beta values have higher, say, basal ganglia responses.

to reward versus no reward or something like that.

So you could do that kind of thing as well.

So both individual level and group level fMRI sorts of approaches, as well as much fancier things.

But those are just two kind of simple examples.

So as another example of doing this in the domain of kind of perception instead of decision making,

This is in the context of a task where a person is just told to push a button every time they feel their heartbeat.

And so in this model, we don't even have an explicit policy selection part.

All we do is they start with a precise belief that they're in this start state at time one, and then they have prior beliefs about whether or not

they're going to transition into a state of feeling their heartbeat versus not.

So probability of no heartbeat, probability of heartbeat.

And those priors are encoded in the B matrix here, the transition matrix, where sort of the higher the PHB is, the more they expect to feel a heartbeat, you know, more often, right, on each trial.

And then also there's a precision value here, which corresponds to beliefs about how precise the actual afferent signal is coming up from the heart.

And so we can estimate this IP parameters, interoceptive precision, and this prior over heartbeats.

I should say we also compared this to a model that included learning in this task.

And in this case, the model including learning didn't win.

And so what we found here is, and I should say also that they do this task three times.

They do it once when they're told they're allowed to guess, once when they're told they're not allowed to guess, and only to press it when they're sure they felt something.

And one where aside from, in addition to no guessing, they're told to hold their breath, which kind of makes it on average easier to feel your heartbeat.

And so it kind of amplifies the afferent signal, makes it more precise.

What we found was that in healthy people, the breath hold actually amplified interoceptive precision a lot.

Whereas in all the clinical groups, anxiety, depression, comorbid depression, anxiety, eating disorders, and substance use disorders, they just stayed flat, low,

interoceptive precision values.

So the actual changing, the actual precision of the afferent signal didn't have any effect on their beliefs about the precision of the signal.

So it's something like a rigidity in the way that the brain treats afferent interoceptive signals in psychiatric disorders transdiagnostically.

And so again, this is just a, again, kind of a standard like mixed model sort of analysis.

Whereas if you compare, say,

estimates for prior expectations, everybody showed higher prior values in the guessing condition than in the other two conditions, which you would expect.

But there were no differences between healthy and clinical groups.

So it's kind of interesting.

It says, hey, maybe this is more a precision issue than it is a prior expectation issue in terms of clinical significance.


SPEAKER_00:
Or another task has to be explored.

Christopher?

Put in a plug for how cool that paper was.

I think there's been a lot of conceptual work on interoception and active inference, and there's been a lot of discussion about whether it's priors or precision or anything like that.

And to my knowledge, I'm happy to be corrected about this by Ryan, but that was the first time that it's really been tested empirically, right?

Yes.


SPEAKER_03:
There's no other actual formal fitting

models to data studies that have tried that before.

Nice.

Epic work.

There are papers that have tested like more kind of like, quote unquote, qualitative or just kind of like go up versus down sorts of predictions that fall out of computational models, but not actually fitting a model.

Yes.

This is the first model based analysis of this kind of thing, right?

Yeah.

And we've actually replicated the results in healthy controls in a second sample now.

So it seems like the effect

at least the effect in healthies is pretty robust.

We haven't been able to replicate, we haven't tried to replicate the lack of an effect in clinical populations yet, but that's kind of in the works.

Cool.

So anyway, so this continues to go here, but I think it should be almost done.

pretty quick.

It only does six people.


SPEAKER_01:
Let me ask an interoception question and then one question from the chat.

So what other interoceptive methodologies might exist?

So you did a heart rate estimation task.

What other interoceptive modalities might be amenable to this kind of quantitative analysis?


SPEAKER_03:
So definitely cardiac interoception tasks are most common.

just because it's actually quite difficult.

The methodologies, it's pretty difficult to use.

So for instance, in vision, you can very tightly control, or in audition, you can really tightly control the timing and magnitude of the input signal, whereas it's hard to say precisely control changes in the signals that you're getting from the inside of your body.

so so it's um you know it's difficult at least the heart is a kind of like signal that has a rate and um and you kind of know precisely when it uh sent the signal upward you know and things like that so they're definitely the most common there's lots of different ways that people do cardiac there's lots of different cardiac um perception tasks um a lot of them have come under a lot of recent criticism recently um for various reasons um there's been some studies that show that

For instance, like standard, what are called heartbeat counting tasks, where people are just kind of asked to like over a period of a couple of minutes, just count every time they feel their heartbeat.

And you just kind of look at how close their counts are to the true number of heartbeats.

There's a number of papers, including our paper, actually, that show that this looks like it's primarily just tracking prior expectations.

It doesn't tell you a lot about, you know, what the way that they're actually treating individual signals is.

Um, and I mean, in ours, in ours, we actually showed this, that, um, that if you used a standard heartbeat counting measure instead of the heartbeat tapping, you know, which was our primary measure that we fed into the model, um, prior expectations in the model predicted the heartbeat counting, uh, like accuracy values, like 0.9 something.

So.

So it was like a nice confirmation that, yeah, this is primarily about priors.


SPEAKER_01:
It points towards having a generative model that can then be deployed and modified and corroborated across settings, because then you could say, well, what would be the task or what would be the trial and set structure?

How many participants would we need to resolve a parametric difference of such and such?

So statistical power down to a lot of other features would be influenced.

Okay, let me ask this question from the chat.

Well, hold on.


SPEAKER_03:
I need to finish that previous question.

But aside from cardiac stuff, there are a number of other methods.

So one that I currently do in my lab is something called using breathing resistances.

So basically you have these people wear this little kind of Darth Vader mask kind of thing, and you can change in really precise, subtle ways how much resistance there is when they try to breathe in.

Um, and you can, um, and you can get sort of individual differences in sensitivity, you know, where some people, you know, feel, uh, that change in like the, how hard their lungs are needing to work basically, um, at different, at different loads of different resistances than, than others.

Um, and you can also use it as kind of like an anxiety induction, which is how we use it to try to, you know, precisely induce uncomfortable, intercepted states at different, uh, intensities.

and see how that affects things.


SPEAKER_01:
What about muscular?

Sorry, what?

What about muscular?

Like how heavy is this or what angle is your arm at?


SPEAKER_03:
I mean, we don't consider that interoception.

That's more like proprioception or somatosensation.

But certainly, I mean, there's lots of tasks like that out there.

The kind of thing that we mean by interoception is things like feeling what's going on in your stomach, feeling what's going on in your

you know, your heart or your lungs or, you know, like various like effects of hormone levels, you know, like stuff going on inside.

Cool.

So, I mean, I should say I can't talk too much about it, but we have actually developed a method for precisely inducing, inducing with precise timing sensations in your stomach.

And we're currently just finishing up a paper on this using that method with EEG.

to test some predictions of the neural process theory for gastrointestinal interoception.

So that's another thing that's kind of common.


SPEAKER_01:
Looking forward to that.

Looking forward to that.

Let me ask the second question, if it's okay.

Yes.

What are the similarities and differences between the active inference model of emotional inference, is how it was phrased, and Lisa Feldman Barrett's approach to emotion construction?

So if you don't know, I'm just reading it off.

Or more generally, emotion construction.


SPEAKER_03:
That's probably a question for another time.

I mean, for people who are familiar with my actual

like simulation work in this area.

I've, you know, I and my group have published, um, multiple, uh, active inference simulation papers specifically about emotion inference.

Um, and, um, those, uh, models.

So the, so I should say it's not a straightforward question because you could build active inference models that are, uh, that would do more or less something constructionist, or you could build an active inference model that does something.

less constructionist.

So it's actually not as though active inference has something really unique to say about whether or not constructionist views of emotion are right.

But I should say that the most straightforward models that have to do with constructionist views, where what you're trying to do is learn and infer emotion concepts,

those uh the most straightforward ways of doing those are via the sort of multimodal um inference inference process um that definitely has a kind of when those mappings between um emotional concept like emotion state concepts and um lower level you know sorts of like uh observations like for instance like observing your arousal level you know observing how negative or positive you feel things like that treating those lower level things as observations um

when the likelihood between those and emotion concept states at a higher level, when the mapping and the likelihood is probabilistic, then you end up having to learn probabilistic mappings that look a lot like constructionist types of inference stories.

But what's kind of less clear is that constructionist views don't say very much about how the kind of states in your body are generated in the first place.

You know, say like, you know, if I see a predator or something and I have, I have this big change in my heart rate and like muscle tension and things like that.

Um, and then I feel that and I infer construct, right.

Some kind of belief about what emotion that corresponds to in this context.

Um, there's a, you also need a story and an active inference model for the mechanisms that generate those responses before you make sense of them.

Um, and you know, we've, we've talked about doing things like that with active inference models.

Um, and it's not so clear.

whether or not something like that would be consistent or inconsistent with constructionist views.

But yeah, anyway, so hopefully that helps.

The active inference doesn't take sides necessarily, I guess is my point.


SPEAKER_01:
Really interesting.

I didn't know about that theory.

So it sounds like something where we can use active inference to construct various kinds of models.

So all these orthogonal models and we're like looking always at the two by two on the active stream.

Is it going to work on this axis and this axis?

Which debates are relevant?

Which ones aren't for active inference in the free energy principle?

Do you want to just carry on a little bit?


SPEAKER_03:
Yeah, I mean, just to say, because this actually comes back very well to the actual topic of today's session, is that to really solve these questions, what you need to do is you need to construct an active inference model that looks very constructionist and an active inference model that doesn't look very constructionist and see which one fits empirical data better.

So it's more kind of a way of precisely testing

hypotheses of different models, as opposed to saying this model must be correct theoretically.


SPEAKER_00:
Yeah, I think it's kind of important to, when you're thinking about this kind of stuff, people often talk about active inference as a theory, and as we've said multiple times, there are multiple process theories that could fall out of this.

I think it's a very general framework under which you can build multiple, hopefully competing theories.

So, yeah, I mean, in addition with computational psychiatry stuff,

you could just use, you could think active inference is literally false, like as a theory, but it would still be, you could still also simultaneously think it's an awesome individual differences tool.

Like that would be an odd set of beliefs to hold, but there's no contradiction there.

Yeah.


SPEAKER_03:
Um, yeah, no, for sure.

Uh, yeah.

I mean, this comes back to the, you know, like, uh, whether you think of models, this kind of model is getting to like the ground truth versus just using it, you know, instrumentally.

Um,

But yes.

So, okay.

So last kind of thing here is that, you know, in addition to being able to do standard, you know, sorts of like, you know, again, like ANOVAs, regressions, you know, frequentist approaches, or even, you know, base factor analyses in place of or in addition to them.

One thing that's really nice about getting parameter estimates the way we've been talking about is you don't just get

point estimates.

You don't just get posterior means, which is, you know, what we've been talking about, you know, with like analyses like that, right?

Like this is just, you know, means of posterior means.

Whereas the active inference, so the parameter estimates and variational Bayes also correspond to posterior, also have posterior variances.

So it's not just the mean, but it's also the confidence that the estimation had around that mean.

And so that's actually additional information that's useful to incorporate when you're doing group level analyses.

And parametric empirical Bayes, which is something that, again, was initially developed and is primarily used for dynamic causal modeling, happens to work really well

with the setup for getting parameter estimates and for the way that we get parameter estimates.

Again, like I said before, already uses DCM scripts.

So you can actually do these parametric empirical Bayes approaches that are more or less general linear models, but that also use the posterior variances to get kind of probably the most principled way of doing like the fully Bayesian

group level stuff that, you know, kind of thing that you could do with these individual level parameter estimates, incorporating both the means and the variances.

Um, and so, uh, I think this, is this thing done?

Okay.

This has got to be the last one.

Um, but, uh, but, um, anyway, uh, I had, there is a thing that I can, I can, if it takes too long, I do have saved versions of this.

I just wanted to show you, uh,

I was hoping to be able to show you the actual model comparison part.

But so, you know, as an example of using parametric empirical Bayes in this paper on substance use, we did do this.

So in this model, in this task, it's pretty simple.

All people do is they just repeatedly choose option one, two, or three.

And on each one, either a green ball falls or a red ball falls.

Green ball means a win.

And in advance, they don't know which of the options has the highest payout probability.

So again, it's explore, exploit, right?

So you have to kind of try out different ones until they become confident which one's best, and then they kind of keep picking it.

And in this case, we were able to estimate a number of parameters.

We estimated action precision.

We estimated that risk-seeking parameter that I mentioned.

We estimated separate learning rates for

when they had a win versus when they had a loss.

And we're also able to estimate this kind of information sensitivity parameter.

It's probably too much to explain at the moment, but it's kind of like a belief rigidity.

It's like an initial how the higher it is, the less you should think you need to seek information, basically.

And what we were able to do with that is over here, we did standard kind of group differences.

And you could see that

action precision was a lot lower in substance users and, um, learning rates for, uh, losses were, uh, lower.

Um, but we also did the PAB version that the parametric empirical Bayes version over here, where we could get these group level, uh, effects that included the means and the variances.

And, um, and you could also see that these effects were, were there and you could actually sell like the posterior probabilities, um, and things like that, um, that were, uh,

Again, it's just a kind of nicer thing to do that will give you additional... You'll be able to incorporate additional information and keep things fully within a Bayesian framework.

And so if this thing is still... Are you done?


SPEAKER_01:
I have another question we can ask if you want to just let it run a couple minutes.

All right.

So...

Just curious, Christopher said that he'll be using this in his PhD.

So what is your PhD project?

I know it started in the PhD and everything, but what are you thinking about using it?

That's the first question, and then there'll be a follow-up part.


SPEAKER_00:
So I work in Alex Walgar's lab, who's a PI at the MRC Cognition Brain Sciences Unit, and her lab studies cognitive control.

So I'm planning on doing a lot of work on cognitive control at the moment.

So I don't really want to say too much about what projects I have.


SPEAKER_01:
Just what questions are you curious about or what do you think is exciting?


SPEAKER_00:
So mostly I'm really curious about using active inference to help us think about and formalize some otherwise kind of less formal hypotheses in the realm of cognitive control.

And then using those kind of generates some testable predictions.

Cool.

Although, yeah, one thing I should say is like, I don't think the active inference is always the best tool to answer all questions you might have.

Depends on what level of analysis you're working at.

I happen to be really interested in like algorithmic level questions where active inference is really good.

If you're interested in implantation level stuff, this might not be the best tool for you.


SPEAKER_01:
Great.

And so the second part of the question was, are there any FEP or active oriented supervisors at Cambridge?

But if there is yes or no to that one, but more generally, I'd like to hear both of your perspectives on like, let's say somebody were starting research as a graduate student in a lab that wasn't.

in this actual line of research already.

So what would you convey to a starting grad student or starting researcher who was looking for a PhD mentor or who wanted to do a research program that was like aligned with a lab that was studying something cool and interesting, but they wanted to take an active inference perspective on it?

Um, okay.


SPEAKER_03:
Just so, just so people know it is now done.


SPEAKER_00:
Yeah.


SPEAKER_03:
Yeah.

But, but go ahead if you want to answer that question quickly.


SPEAKER_00:
Um, I was just going to say, I have a couple of things to say.

I think it just depends on your personality, really.

Uh, are you the type of person who doesn't mind doing a lot of things alone and without much help?

In which case you can do something, um, that your other members of your group aren't doing.

If that doesn't bother you, then go ahead.

Um, this tutorial is a great resource.

Um, if you are working in a lab, I would genuinely just try and find a lab that studies what you're interested in.

That's probably the good advice.

I mean, so one thing to say is like Ryan is,

I haven't signed the paperwork yet, but he's going to be my associate supervisor.

So I do have someone in my supervision group who can actually help me with this stuff.

And so I would always say, make sure whatever you're doing, you actually have people who can help you.


SPEAKER_01:
Great point.

And I wonder if in the remote world, it will be easier to say, I remember it was kind of a big thing to have, I didn't even have this, but like somebody who's a committee member from another university.

So now it might be a little bit more possible to connect remotely.

Anyways, Ryan, I'd be curious to hear your thoughts on that.

And then we can go back to the model.

Um, sorry, I don't think I fully got the question.

A starting researcher, somebody who asked to be in your lab, but they were, you know, there's not room for them in your lab or they're in a different lab already, but they want to do something with research in this area.


SPEAKER_03:
Oh, I see.

I see.

Yeah.

I mean, so that's a very tricky question, I think.

I mean, when you're deciding where to go to grad school, um, I think it's really, really important that you find somebody and that you're confident you found somebody

where there's a good match between you and your supervisor.

I've seen cases where people start in labs and there's just not a good match between your supervisor's style and your own style as a student.

And a lot of times that's a recipe for either a really tough time getting through grad school or even deciding that maybe you don't want to be in science anymore.

I've seen that personally, like the case with other people.

Whereas if you find somebody who

you know, kind of matches your style, right?

In terms of like how much direct micromanagement and supervision you want versus how much independence you want, you know, like having a supervisor that really allows you to be creative and come up with your own projects versus someone who, you know, would prefer that you're working on a lot of stuff that's sort of directly, sort of more narrowly in line with what the lab's already doing.

You know, I mean, all sorts of supervisors will have, will have, you know,

types of grad students that work well with them, but finding the right matches is really, really important.

And part of that, you know, does also correspond to common interests.

I mean, if the question is, you know, I absolutely cannot find somebody who does what I want to do, that is really tough.

But I would say in that case, you know, what you were mentioning about, you know, kind of

remote or co-supervisor sorts of situations where as long as you can get your primary supervisor who's a decent match to you in general to be okay with you having some other supervisor or mentor of some sort that will do the sort of thing that you want to be doing methodologically and there's enough kind of overlap and interest between real you know primary mentor and co-mentor then I think that can work well you know in those cases but but

if at all possible, identifying a primary supervisor who does the, the methods and is, and has the same interests, um, is, is, you know, definitely the best option if you can.

Um, great advice.

But, um, okay.

So, all right.

So this is now done.

So you can actually see what the script does once it's done.

Um, so you can see, so what it will spit out, um,

I should say, this is also a new feature.

I just added this to the tutorial script, to this main tutorial script.

So if you go to the GitHub and download the newest version, then it will generate these scatter plots for you.

But so it will show you, for instance, so when we only did 16 trials, right, then recoverability for learning rate for the three-parameter model wasn't awesome, right?

The correlation between true and false

estimated parameters was only 0.58, whereas for risk-seeking, the correlation was 0.95, so super recoverable.

And for alpha, it was, I don't know, it's not showing up, but it was 0.42, so not great.

Whereas for the two-parameter model, risk-seeking was 0.95, and alpha was 0.94.

So I should say, depending on the values...

The RMP values might not show up on those graphs the way I have it set up, but it will print them out for you here.

So alpha recoverability 0.94 equals 0.046, et cetera.

So it will give you all of that.

It'll just spit that out for you.

It will also tell you the average log likelihood under the two parameter model and the average action probabilities of the two parameter model and the same thing for the three.

So the fits are actually very similar.

in terms of the log likelihoods and the action probabilities.

And so when you actually calculate the protected exceedance probability, which it also will do for you, PXP, it will show that in this case, the second model, so the model with learning, has a higher exceedance probability than the first model, but

this is actually not necessarily a very clear winner here.

I mean, what you're hoping for is something like one and zero or, you know, 0.8, 0.2, 0.9, 0.1, something like that.

So, and again, this is just an example.

This is not going to be reliable at all because I did 16 trials, right?

You know, if you did a real task, if you just use the 32 trials that I actually have in the real code on the GitHub, it will be better than this.

But,

you know, most actual behavioral tasks, you know, like for computational modeling studies are going to have, you know, like a hundred, you know, or more trials to really get kind of precise estimates and know how good these models are actually doing.

So, but so, so that's the kind of thing that you get here at the end.

So you'll get

So I should say this trajectory thing is not that meaningful.

It's just showing for the first and second parameter what the trajectory of the change in the parameters are as it converges toward the posterior values.

But when you have more than two parameters, it's not that informative.

So we have in the paper, we have this figure that more or less just kind of depicts an example of what this kind of thing looks like.

And more or less, as long as this kind of, you know, iterations converge and it has this kind of nice increasing slopey thing to it, then you're probably fine.

But if you're doing this and you find that through iterations, it kind of bounces up and then bounces way back down and then bounces up and it looks really kind of inconsistent, does have this nice kind of slope to convergence, that's usually a telltale sign that you're running into some local minima or, you know, something's going wrong.

Um, in which case you might want to like tweak your priors and see if it works better, or you might, that might tell you there's just something wrong with your model.

So it's a good kind of diagnostic.


SPEAKER_01:
I just would love to hear, sometimes it's hard to grasp, how can it be converging downhill?

We were talking earlier about the difference between sampling-based methods and the factorized methods.

So there's obviously a world of difference in the setup and in the computation.

But how is it that just doing something like minimizing free energy does take us to these acceptable parameter ranges, if there's any sense that you have of working so closely with it?

Sorry, can you say that one more time?

What is it about that accumulating bars that leads us inevitably, seemingly, at least reasonably to an acceptable set of parameters in a vast space?

What is actually happening there that somehow it doesn't converge into a local minima or which way you go, doesn't converge on a local solution set?


SPEAKER_03:
There's no guarantee that it doesn't do that.

This could easily converge into a local minimum that's not the global minimum.

I mean, literally all this is saying is that there's a kind of smooth locally convex gradient descent towards some stable value.

But no, I mean, that could totally be a local minimum.

There's nothing that stops it from being that.

It just means that there's a kind of clear slope.

It's not like a super kind of

like bumpy, weird parameter landscape.


SPEAKER_01:
But then we need to inject some layer of sampling with many starting positions in order to get good meta conversions, because it's not enough to look at one model trace and say like, yep, well, this one converged.

That model converged somewhere and it settled in given


SPEAKER_03:
a very complex free energy function but then we need to pull back another level right to get traces from many points in the landscape um well i mean that's just a that's just another approach that is a potential you know like not you know like trying to not testing out a bunch of different prior values and seeing if everything converges from a bunch of different prior values to the same spot um like yes it has a limitation because this could represent a local minimum but if it

if it stably converges on different reliable parameter estimates for each person, which corresponds to a range of places in a landscape, then those will be stable individual differences.

You'll probably get slightly different values if you start your priors at a different place, almost inevitably because the complexity cost will be less if you don't have to move as far to find accurate

parameters.

But so long as you're getting interesting variability and convergence for each person has a nice kind of slope like this, then it's not like everybody is getting stuck in some particular, you know, single well or attractor of some kind.

But again, the question isn't necessarily what the true one is per se.

It's, you know, like, can you find estimates that, you know, provide interesting individual differences given a set of priors?

Um, you know, I mean, it's, it's, it's important to realize too, and we mentioned this in the paper that, um, you know, typically the model, the model that will win the model, they'll have the best parameter estimates will typically be simpler than whatever the true underlying generative process was in the person that generated the data.

Cause typically there are simpler explanations, um, than the ones that generate the ones that actually generate, um, data in a complicated system like the brain.

You know, so you're not necessarily treating this as though it's the true one, but if it gives you nice convergence for each person and you're getting nice individual differences, then it still is a reasonable measure of, you know, of interesting individual differences.

And if you can simultaneously do things like show that parameter estimates correlate with other things, like, so just, you know, I happen to have it up.

So this is this EEG study that I'm in the process of putting a paper together before.

And we have parameter estimates here for interoceptive precision in that gastrointestinal task I mentioned.

Now, precision here correlates with reaction times, negative 0.74, despite the fact that the model is not fit to reaction times.

And it's a direct prediction of the neural process theory that higher interoceptive precision should mean faster evidence accumulation, which should mean faster convergence time, which should mean shorter reaction times.

So you can separately validate that your parameter estimates are tracking what you want them to be by seeing what they correlate with in a way that they ought to correlate with for construct validity for things that didn't get fed into the models.

Um, you know, so there's, there's, there's lots of ways of doing this.

Um, um, but, but you're right.

I mean, if you using other approaches that use multiple seeds, the kind of like the Monte Carlo stuff that you were talking about before, um, that is another approach and, um, it might be more likely to, um, you know, find like a global minimum.

Um, but, uh, but again, they're just, they're just different approaches.


SPEAKER_00:
Just a quick plug.

So I think the model that's kind of most closely related to active inference and is actually fitable, can be fitted to data, is probably the hierarchical Gaussian filter.

And I think they're actually, at the moment, it's estimated, parameters are estimated using variational Bayes.

But as I understand it, they're also, I don't know if they have developed it or are still developing, but they are also developing Monte Carlo Markov chain version of it.

Very interesting.

Yeah, this is all just.

Yeah.


SPEAKER_03:
So last thing here, just to wrap up everything, so at least people know what's available for them to expand on this in our example code that we've set up.

So down here, when we actually do the whereas,

this right here is the function for doing model comparison.

So SPM underscore BMS Bayesian model selection, you just feed in the free energies for each person.

So for instance, it will just store these.

So for instance,

So these are the negative free energies for my six simulated people for their best fit models, best fit parameters for the two parameter model, whereas that's it for the three parameter model.

And so Bayesian model comparison is just feeding both of those vectors into

column vectors into that function, and it will spit out the expected or predicted exceedance probability for you, as well as a number of other sort of diagnostic checks.

And you can read, there's good papers by Klaus Stefan, and anyway, we reference them, and we cite them in the paper that describe this in detail.

If you go into the actual function itself, it also will tell you exactly what the inputs are and exactly what the

outputs mean, right?

So alpha is just a vector of the model probabilities.

Um, again, XP is the exceedance probabilities before doing the protected part, um, and so forth.

Um, so, um, once that's all done, um, and I should say, I'm not going through every kind of line in this, but we commented, you know, as well as we could, and we hope it's, we hope it's clear as you go through, you know, what we're doing.

Um, but it also will spit out right cleanly into the

into, you know, the main terminal here, what, you know, what the outputs are.

So if you want to, you can kind of hopefully adapt this code for your own purposes to do recoverability and do model estimation and stuff like that.

But once you've done that, then you can do, I was just going to show you briefly the hierarchical base, the PEB stuff.

And for that,

We put things into these GCM structures, so GCM for Model 2, GCM for Model 3.

And you can just pick which one here that you want to use, whether you want to do PEB on the two-parameter model outputs or the three-parameter model outputs.

And this just sets a bunch of defaults in PEB.

You shouldn't have to mess with any of those.

But if you run it, so what I've done is in the GLM and the general linear model for peb, I've just set the mean value across subjects.

I've put a regressor for group.

So this is saying compare groups.

And then I've put in this sort of fake age variable that just with random numbers, basically.

So if I run this section on the three parameter model, then it'll do this kind of estimation thing.

And it will spit out a bunch of things.

And basically the way to read this is, ignore that.

So this one, just ignore when it spits it out.

This one will tell you, for instance, this is saying like for, so this one is probably the best one.

So these are the reduced models.

So the best model,

removing the parameters, parameter differences that didn't matter, didn't win in Bayesian model comparison.

So this is saying parameter two and parameter three were different between groups and remain different between groups in the best fit model.

And again, we explain this all in more detail on the paper.

You can really ignore these ones at the bottom.

In the figure, in the paper, we show you which ones matter to look at.

So it's just these, more or less, those top three.

So the estimated differences and the estimated differences that survive model comparison.

So pruning away the parameters that don't stick around in the best fit model.

So in this one, this is just- Say what the pink bars are, because to me, it wasn't immediate.

They're just the variances.

They're just the variances in the posterior estimates.

So, yeah.

So, I mean, for instance, so you would expect, right, this one has a really high variance.

You might expect this one wouldn't survive, you know, as a winner in the reduced model.

But even this isn't probably the most useful.

The most useful is the actual, like, PEB review parameter GUI.

Um, see, so this is the actual GLM.

So this is just saying the mean, the group difference.

So this gray, light gray versus dark gray, because the first three subjects are a group.

The second three are a group.

This third column is just the randomly generated age values.

Um, so you can just say, for instance, you know, which of these, which of the means are different?

Well, the means for one and two are different.

Um, you know, so there's a main effects essentially of those two.

Um, you could go to group.

And you can say, okay, these are the two that are different, but you can put a threshold on it, right?

So weak evidence, you know, posterior probability has to be above, you know, some value for positive evidence, right?

So if I make it, if I give it a higher, it has to, there has to be decent evidence for it, then only the second parameter here is the one that actually has good evidence for a group difference.

You know, for example, so you can,

But if I make it required to have very strong evidence, so a posterior probability greater than 0.99, then the whole thing gets removed.

So this is nice, and you can just do it in terms of free energy or just in terms of probability that a parameter is greater than zero.

So there's different ways to do it.

So this is actually probably the nicest thing for navigating the

the results of peb that will matter if you're doing group analyses using it.

So that is peb.

And yeah, I'm trying to think if there's anything else really.

Cause that's, I mean, that's primarily where things end.

So I mean, at that point you should be able to do,

everything.

Um, I guess the, yeah, I mean, I was going to briefly, I did skip over this, just the, this was the thing I mentioned that Philip, um, did where he was able to do kind of like within trial by trial, um, model predictions, as opposed to these just group differences and show the trial by trial, the beta updates, um, correlated with this midbrain dopamine area.

But, um, but yeah, I mean, really, I mean, that's more or less,

the steps you need to know, right?

So what we talked about was you have to have participants perform the task.

You build one or more models.

We did two.

Find the parameter values in each model that best reproduce the data, right?

The behavior, we did that.

We did model comparison.

We used the correlations between true and generative parameter or generative and estimated parameters to make sure the winning model was identifiable.

And then you could do either normal

group levels frequent to statistics or something like peb to test for between group or just, yeah, group level, you know, between subjects sorts of analyses on parameters at the end of the day.

So I think that, yeah, I think that really covers most of the, you know, kind of meat of it.

I mean, the only, oh, okay, sorry.

One other thing that I do need to show you is that,

to actually do the parameter estimation, that calls this estimate parameter script, which is one that we also included.

So if you open that, just right click it and say open, then this is where you set your prior means and variances.

So it says here we specify prior expectations for parameter means and variances, right?

So we say that.

So in this case, I just set the same prior variance for all the parameters, one over four.

You don't have to do that.

So here you can think about it as the smaller the value you put here, the greater the complexity penalty that you're adding.

So the smaller that is, the more you're gonna prevent overfitting.

But if you make it too tight, then the posterior estimates probably won't be that accurate.

So in this case,

what you can see that we've done is, so if you're estimating alpha, so action precision, then this is where you'd put the prior.

So we put a prior here of 16.

We log it here just so it's in log space.

And that just makes it so during estimation, it doesn't allow that number to ever become a negative value.

So it keeps it, it needs to stay a positive value.

during estimation.

If estimation ever tried to test out a negative value for that parameter, then the thing would break.

Same thing if we were doing beta.

So that would be, which we didn't try here, but we included if you want to.

So this would be the policy expected for energy precision.

So there the prior is one.

And again, we're just keeping the prior variance equal to one fourth up there for everybody.

This is the loss aversion, which we didn't include, but is in there if you want to.

That's the risk-seeking parameter, and we gave it a prior of five.

And then eta, the learning rate, this has to be not just any positive value.

It has to be a value between zero and one, which means that you have to put it into a logit space as opposed to log space.

And that just means that to set the prior, you have to make that whatever your prior is, and then also

put that same number here.

So in this case, we've given it a prior for learning rate of 0.5, which is just kind of halfway between zero and one.

So it's just kind of middle of the road, which would make sense in a lot of cases.

And then more or less this function down here just turns those back into their normal values.

So it exponentiates the log values and so forth.

Um, so it just brings them back, retransforms them.

Um, and, uh, this is just where we, um, for technical reasons, this is where we set what the, uh, what the, um, values are for risk seeking.

Um, and, uh, and then this is the actual log likelihood.

So it will just for each trial, it will add the log likelihood.

So the,

mdp.p here is the probability of each action that it will show that just is in the MDP structure, which again, we showed last time in other sessions and in the actual, we have a table in the table three, I think, in the tutorial that says what each MDP field is.

So it just takes those and just adds them, just logs them and it just adds them up across trials to get the total probability.

And then the less negative this is at the end, the better the

the better the fit is.

So it's important to know about this one just because this is where you would change your prior means and your prior variances.

So yeah, and then this, for instance, this DCM field thing, it's just set up so that you would just enter the name of whichever parameters you want to estimate for that model.

So here, if you put an alpha and RS, it'll estimate alpha and RS, but you could also add

you know, eta here, right?

If you wanted to, you could add that, then it would also estimate eta.

Anyway, I mean, I'm sure there's other little things in the code here that could be explained in more detail, but, you know, this hopefully will be enough to get people going.


SPEAKER_01:
Well, thanks so much, Ryan and Christopher.

This was really an awesome series.

It was our first model stream, and it was really just a great learning experience for all of us.

So I'm going to give some final comments from the chat.

Someone wrote, thanks for the author's continuous updating of the paper.

It will be great if these slides, as in their current version, are somehow made available at the end of the episode.

So maybe like you mentioned, doing a supplemental file or something for the paper.

So that was one thing.


SPEAKER_03:
for these, just for these slides, I'd like to put them.


SPEAKER_01:
Maybe so someone could say, oh, that was slide 87 at hour three of this thing.

And then we could just have this version so that people could know where that figure was.

Because I know that there are probably in other places are just copied over, but that would be one.


SPEAKER_03:
Is that like, I mean, I guess I can imagine, I can imagine putting together a PowerPoint that just kind of, you know, kind of concatenates all of the slides that we've gone through over these, um,

over these sessions, the vast majority of them will be just the same as the figures in the tutorial itself.

But the best I could do probably is to put that as a supplemental file for the supplementary materials on the SciArchive version.


SPEAKER_01:
That'd be cool.

That's probably the only thing we could imagine doing.

We could have a first page on a PDF or something that just says, here's a link to these streams as this version.

So that'd be one thing.

And then a related question was just that future people wanted future tutorials and model streams.

So...

Either both of you, it's an open invitation to just come on whenever the time is right and talk about any kind of modeling or all these other cool ideas we've been bringing up.


SPEAKER_03:
So, I mean, I'm happy to continue this, you know, in a more kind of, you know, on the fly, you know, way as people request things.

I mean, but at this point, I mean, I would probably need or we would probably need...

And again, requests, right?

Like, what is it that people want to cover?

Because I mean, now we've kind of gone through the tutorial start to finish, at least in broad strokes.

So I would need to know at that point, what, you know, from now forward, I need to know what else people would want us to cover, because, you know, we've gone through most of it.


SPEAKER_01:
Cool.

Well, this tutorial definitely took us to kind of the brink of all of these papers.

Maybe it'd be interesting to walk through a paper, like a tutorial of a paper, especially this interoceptive one.

Like, how would we adapt that for other... Do kind of a walkthrough.

That'd be one thought.

Or we could go into the formalism side.

We could invite a colleague or collaborator who came from a different perspective, more on the analytical, more from some other dimension.

that we're, that we are just also all learning about.


SPEAKER_03:
Yeah.

I mean, I should say, I mean, there's certainly a lot of areas of like the broader kind of free energy literature, free energy principle literature that we're totally not covering here intentionally.

Right.

Like our focus is, this is what you need to know to build models and use them experimentally.

Right.

So, you know, for instance, there's a lot of other stuff in, for instance, like the, the physics formulation for, for, you know, that also gets called right.

Part of active inference.

um, or the free energy principle more broadly that includes things like, for instance, like, um, Carl Friston's kind of variant on talking about Markov blankets, um, you know, or, I mean, there's a lot of stuff on like, uh, non-equilibrium steady states, right.

And how that, how that can be talked about in terms of, um, uh, minimizing variational free energy, but also a lot of the physics, um, you know, like thermodynamic, right.

Like free energy and things like that also come in.

And, um, that stuff's quite a bit more theoretical and, um, um,

It's certainly interesting, but it's not things that you need to know or all that directly relevant to actually build models and do this stuff in practice.

So, you know, there's other places to look to get that kind of stuff.

I guess this is what I'm pointing out as ours.

This is not completely comprehensive of what gets called the active inference or at least free energy principle.


SPEAKER_01:
So any other final thoughts from either of you?


SPEAKER_03:
no um this has been really fun thank you yay great well it was really fun same and yeah and like i said i mean if if people want to like you know like go through get walked through specific you know papers either like you know simulation either some of like our simulation papers or empirical papers in more detail um so they could get a sense of how they would do that in practice i'm happy to do that um but um i guess that's something that we can just kind of


SPEAKER_01:
determine at a later date.

Nice.

So we will wait for somebody to stimulate us to do a walkthrough on a certain topic or with a certain distribution of people.

We'll figure it out, but we'll figure it out then.

So really, thanks again, both.

I'm going to finish the live stream.

So peace out, everyone.

Thanks for watching ActiveInference.org.

Thanks so much.

Ryan Smith, Christopher White.
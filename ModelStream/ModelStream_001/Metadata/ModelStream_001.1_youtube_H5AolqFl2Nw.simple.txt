SPEAKER_03:
Hello, everyone.

Welcome to the Active Inference Lab.

This is the first Active Inference Model Stream, Active Inference Model Stream 1.0.

And I'm really excited for today's conversation.

I'm Daniel Friedman.

And just to introduce the other participants today, Ryan, go for it.


SPEAKER_02:
Yep.

Hi, I'm Ryan.

I'm from the Laurier Institute of Brain Research.


SPEAKER_06:
Hi, I'm Christopher.

I'm a PhD student at the MRC Cognition Brain Sciences Unit, which is based at the University of Cambridge.


SPEAKER_05:
Hi, I'm Max Murphy.

I just completed my PhD at University of Kansas in bioengineering with a focus on neural engineering.


SPEAKER_03:
awesome thank you everyone for participating and for ryan and christopher two of the authors of this awesome work we're going to be exploring so this is the first in a several part series that is going to be highlighting several perspectives and addressing questions related to the active inference tutorial paper of smith et al called a step-by-step tutorial on active inference and its applications to empirical data

so the idea here is for those who are working with empirical data to learn about active inference as a method and also for those in the active inference community to be learning about some of the methods that apply active inference if you're listening you're participating and if you have any questions during the live stream feel free to post it in the youtube live chat and we'll try to address it during or after this

presentation that we're about to get if you have questions after the live stream please feel free to leave it in a comment form and we'll try to address it and integrate your input in future sessions and to learn more and to participate check out active inference.org or any of the information in the video's description so that's all the information uh or metadata for this video

way this is going to work today is we're going to do some introductory questions uh just sort of like asking what in general is this work about what motivated the authors to write the paper the way that they did and then both ryan and christopher are going to share their screens for part of the presentation and they're going to show us a few different things about the work that they've done

And then we have a couple of questions prepared on our side.

But also, we're going to be looking at a live chat if anyone has questions.

So just post it whenever you feel like it in the live chat, and then we'll, again, try to address it.

So as I stated, the intro questions, and then we'll go to the presentations.

So first intro question to the authors is, what is this work?

What is exciting about it?

What motivated you to work on it?


SPEAKER_02:
OK.

So just to kind of reintroduce myself a little bit more.

So I'm Ryan Smith.

So I'm an investigator at the Laureate Institute for Brain Research in Tulsa, Oklahoma.

And the focus of our institute is primarily on neuroimaging and sort of neuroscience approaches to understanding psychology and psychiatry with a focus on sort of treating psychiatric disorders.

And so

for a while now, there has been the use of simpler computational models, primarily reinforcement learning models or drift diffusion models, things like that out there for researchers who are working with empirical data.

So there are good resources out there for people to learn those methods, to apply them to data in their own research.

However, at the moment, so active inference is a sort of much newer field, especially

sort of formulation in terms of partially observable Markov decision processes.

And there isn't really to date a really clear sort of combined place to learn the sort of practical methods to build these sorts of models and to then sort of apply them to task behavior in empirical studies.

So the kind of motivation for this paper, this tutorial,

was to allow somebody, so new students or somebody who's sort of like an early junior faculty, things like that, who wants to go in this direction, to give them the resources they need to do that as easily and as thoroughly as possible.

So even if you start out not really having any background in this stuff, the hope was that the first section of the paper

would allow you to kind of get enough of a background in theory such that you would understand how to then learn how to build these models specifically for empirical tasks and eventually to learn how to fit those models to data so that they could be used in your own sort of research, you know, even, you know, for across many different fields.

So I would say what's exciting or motivating is primarily just that,

if you're a new researcher and you're interested in this, but you don't know how to do it, the hope is it's gonna be like a one-stop shop where if you can get beginning to end, then you'll know how to do what you need to do to start using it in your own research.

So that's the major driving motivation.


SPEAKER_03:
Awesome.

I love that one stop shop for a researcher who wants to apply the methods but doesn't really know where to start.

So that's really a great idea.

And you have versioned the document many times to really address people's input.

So it's cool how it's an evolving document as well.

So Christopher, what would you say in response to those intro questions?


SPEAKER_06:
Um, so I think

there are two parts of my motivation for writing this.

The first was a selfish motivation in that I think if you want to understand something, the best way to do it is actually to write a tutorial on it.

And so in, when Ryan asked me to do this tutorial with him, I was jumped at the opportunity just because it would be an opportunity to kind of actually learn this stuff better myself and really get into some of the more technical details.

And so then more broadly speaking though, I think there are lots, if,

In principle, there's nothing kind of, applying these models in practice is no more complicated than doing like model-based reinforcement learning.

And yet it's incredibly more common to see researchers working with various more sophisticated RL schemes.

And I think if you are a graduate student, maybe starting in psychology or neuroscience, you're like, hey, I'm interested in whatever it is, decision-making under uncertainty.

And I mean, we have a finite time on this earth, right?

And a fine amount of time to actually dedicate into learning things.

And so I think it would be perfectly rational if you had someone to look at active inference research, like, wow, this is super technical.

I wish I could learn it, but here's this other thing where I can go on Neuromatch, I can read Sutton and Bartow and really totally get up to scratch and use it in my research fairly easily.

And there just wasn't something really accessible to help people actually apply it in their own research.

So that was kind of my, yeah.


SPEAKER_02:
Yeah, I mean, I should say, just as a little background, you know, when I wanted to learn this stuff,

originally at the end of my postdoc before I took my current faculty position.

I, the only way to do it, the only way that I could learn it was actually to go, you know, hang out at the Phil in London with Carl for about four months.

And, you know, just ask people a ton of questions and sit there.

And that was the only way, you know, so without having to, you know, physically go to one location in the world that was very far away from, you know, my current institution,

And without doing that, there really was no way for me to do it.

So, I mean, this is sort of out of empathy for my past self that, you know, this is the sort of thing I wish I would have had available to me so I could learn this stuff independently.


SPEAKER_03:
Cool.

So, Max, what is your background or what got you really excited to work through the paper and develop a lot of the examples out on your own computer and everything?

So what got you excited or where are you coming from today?

Sure.


SPEAKER_05:
Yeah.

So maybe I'm kind of your target demographic in some sense.

I don't know that I could really be considered a junior investigator, although I hope I could someday be that.

And I'm interested in I'm much more on the motor motor system side of neuroscience as opposed to the decision making.

So this.

Just the whole lexicon, it's a little different, but I am seeing so many commonalities when I'm coming from a signal processing background.

I think of things in terms of Kalman formulation and trying to integrate sensory and motor information.

I see a lot of similarities there.

And so for somebody like myself, then looking at some of Friston's work and seeing how he's described the anatomy of inference and could this apply also in motor systems?

I'm very interested to see how this might apply in my own future research and work.


SPEAKER_03:
Cool.

So let's just say that you were speaking, Ryan, first, and then Christopher, to that early investigator, researcher of any age, who's like, okay, I get it.

In reinforcement learning, it's about reward, or reward-centric learning is about reward.

What is active inference?

how is it taking a different perspective on what it is that organisms are doing than that kind of classical account of reward-based learning, for example, or whatever you'd like to contrast it with.

But what is the bridge from the kind of assumptions and implications that lead somebody to work on that classical framing versus what is the difference in thinking that is active inference and how is that manifested in the model?


SPEAKER_02:
So I think this is actually a really important thing to bring up right from the start is that, so there's a, there's a kind of big distinction between, um, when people talk about the free energy principle broadly or the philosophy of the free energy principle, there's a very kind of, uh, you know, somewhat wide chasm between that and what gets called active inference nowadays where active inference is a corollary of the free energy principle.

But, um,

active inference as formulated in terms of partially observable Markov decision processes is quite a bit narrower, right?

It's a very specific sort of discrete state space generative model that has particular sorts of elements to it.

And it doesn't actually require knowing a lot of the things that people talk about with respect to the free energy principle more broadly.

It appeals to

um, free energy and expected free energy as functions, right.

As more or less cost functions for figuring out what the best choice to make is.

Um, and it, but, but at the end of the day, I mean, when you apply these sorts of models to behavioral tasks that get used in studies, um, it's actually, there's quite a bit of similarity.

I mean, you can really see active inference models, um, or at least the MVP formulation,

as just a particular, it's kind of like what Chris said, as just a particular kind of flavor almost of model-based reinforcement learning.

You know, so instead of, I mean, there's some sort of technical differences, right, where the way that sort of Carl and them have set it up, what you're doing is kind of like, it allows for kind of a fully unified Bayesian way of doing reinforcement learning and decision-making processes.

And you do that by instead of calling something a reward per se, you define this probability distribution over observations that gets called a preference distribution.

And so reward then becomes a kind of probabilistic preference to observe some things over others.

You know, so if you call a particular observation a reward, then calling it a reward just amounts to having a precise

preference distribution that has a precise high value over whatever that rewarding outcome is.

And then the agent is simply driven to make decisions that it thinks is most likely to get it to observe the thing it prefers, right?

So this preference distribution is more or less a way of specifying what is rewarding to the agent, right?

What the agent is seeking.

One thing that is kind of nice about active inference that's a little different

from reinforcement learning per se, or there's a few things I should say.

One is that making decisions based on trying to minimize expected free energy, which Chris will cover, doesn't just try to maximize reward.

It also simultaneously tries to maximize information gain.

So what will happen is

Uh, you know, say, say an agent starts out in a dark room, you know, just to take a very kind of contentious, you know, philosophy of the FEP sort of example, right?

Say an agent starts out in a dark room, um, which means that it doesn't know where anything is, right?

So that means it has a lot of uncertainty over what state it's in.

Um, and let's say simultaneously in another room, there's a fridge with some food in it.

Um,

then there will be two different sorts of drives there, right?

There'll be this epistemic drive to do whatever it thinks is going to give it the most information gain, right?

Which will be go turn on the light, right?

So then it'll know what state it's in, right?

Whether it's in room A or room B, right?

Or whether there's a couch in the room or whether there's a TV in the room or whatever, right?

And that's not reward per se.

That's just trying to choose the action that's going to maximize the amount of information you get

Um, the other, the other chunk of it though, is to maximize observing preferred outcomes.

And so the agent will also be driven to just leave the room to go to the fridge, you know, to observe itself eating food.

Um, you know, so in practice, what will happen that typically won't happen, at least in a standard reinforcement learning setting is that the agent won't just try to maximize reward directly.

A lot of times it will first choose the action that will

help it figure out where it is so that it's more confident what to do to get the reward later.

So it's really not, again, in practice, these models are just a nice, fully Bayesian way to integrate perception, learning, and decision-making where decision-making is driven to both maximize information gain and maximize reward.


SPEAKER_06:
Christopher, thanks a lot.

Anything else to add there?

No, not particularly.

I think the only thing I would say, and I'm going to say this in a little bit because I've got another slide, is if you're interested in the technical details of what Ryan just said, there are two really good papers.

Maybe we can make these slides available afterwards as well.

One's by Norsajid, and I'm not sure if, I'm sorry if I butchered your name there.

The other one's Lancelot da Costa, and they have two excellent papers comparing it to reinforcement learning formulations.


SPEAKER_03:
Excellent.

Just to draw out a couple points, Ryan, from what you said.

So one is that the active inference framework and specifically the POMDP, the partially observable Markov decision process, instantiation of active inference is a corollary or like a derivative of the free energy principle.

And there's a lot of philosophy and contentiousness around different aspects of that.

We're going to read a paper in

active inference stream 14 by mel andrews about the math is not the territory about the philosophy and the status so yes if you want to go that way there's a whole rabbit hole there but this is kind of like a tool like a linear regression you don't get caught up in the weeds on number theory or on the information space that it's about where the calculation is useful and tractable it's going to be a tool so that's kind of where we're coming at at least today and in this series

Another interesting thing, and in contrast with reinforcement learning, which is kind of like reinforce what works and don't reinforce what doesn't work.

That neurons that fire together, wire together are positive reinforcement schemes.

There is a reward preference built into that kind of model.

But when there's a basin of low reward, it's often difficult for those models to latch on.

So we saw that in the paper of Alec Shantz et al in Active Inference Stream 8, Scaling Active Inference, that kind of showed how even up against these very, very large scale machine learning models like

Q reinforcement learning and things like that.

Other state-of-the-art deep learning models that the active inference trolley car and control theory robots were able to work really well because first they did what you just described, Ryan, which was they kind of went into an explore mode

before going into a more fine-tuning mode.

And so in doing so, they transcended the explore-exploit simple trade-off.

It's not just a knob in this model.

It's not a coefficient to balance explore versus exploit.

I hope we'll draw out an understanding that they're actually related in a different way because there's something that's model-based and generative that's happening.

and so really a ton of interesting stuff so at this point i would think we could go into the presentations if people have any questions that arise put them in the youtube chat and i'll be copying those out for addressing them later and um other than that let's just work through the presentations however ryan and christopher have set up and preferred cool okay go ahead and just share your screen chris you can just go ahead yeah just kidding


SPEAKER_06:
Okay, so I'm using a dual monitor setup.

We'll see how this goes.

Okay, sorry.

I now have to open system preferences, sorry, to allow Microsoft Teams to use it.

Sorry about that.

Okay.

Should be with you momentarily.


UNKNOWN:
Cool.


SPEAKER_03:
Good to digest.

It's really, it's interesting stuff and we're all learning by doing here.

So I just really appreciate and have just seen a lot of appreciation for this kind of work because in some ways it's like the missing piece between a lot of these

hypothetical or abstract or scaffold models or what people have heard about with respect to what active inference is and then how it's deployed how it's enacted so to take the inactivist insight seriously as practitioners of science and as communicators as well it means making this kind of work and showing us how you do it as we all are trying to like figure out every level from literally the tech to

the best way to make it rigorous and accessible, the best ways to talk about it.

So, yeah.

What do you think about that, Ryan?


SPEAKER_02:
Yeah, I mean, I totally agree.

Hold on.

So, Chris?

Yeah, actually, I've never used this.

So there should be a little thing in the upright corner.

It's like an arrow pointing into like a square.


SPEAKER_03:
Share content.


SPEAKER_06:
Are you guys both on PCs?


SPEAKER_01:
Yeah.


SPEAKER_06:
Yeah, okay.


SPEAKER_02:
So, yeah, so it should be upright corner, like, right next to the leave button.


SPEAKER_03:
Okay.

Or, Ryan, you could present first, or... Yeah.

I actually have a question.

I have a question.


SPEAKER_02:
It won't really work for me to present first, I think.

Sure.

You know, what I'm going to do.


SPEAKER_03:
Not going to be so comprehensible before Chris.

Got it.

I actually have a question that will take a couple of minutes.

Christopher, while you're figuring it out, is what tools would somebody need to follow along?

So what programming language or what interface do they need?

What background knowledge do they need?

Is there a software to download?

Is there a course?

What are the prerequisites for doing what we're about to basically jump into?


SPEAKER_02:
Um, yeah, so, I mean, I'll just say that, that, you know, a lot of this stuff was originally designed by Carl Friston.

Um, and if you know a lot about Carl, um, you know, before he proposed a lot of this active inference stuff, you know, he was, uh, sort of most, uh, you know, famous or his, his biggest contribution was putting together, uh, SP, SPM within MATLAB.

SPM is, um, like, uh, it's called statistical parametric mapping software, but it was,

more or less the original way of doing fMRI.

So like analyzing functional and like neuroimaging data.

And so a lot of the scripts and dynamic causal modeling, I should say, which is a particular sort of approach to doing neuroimaging.

And so a lot of basically all the standard resources, all the standard coding routines, everything for actually building active inference models right now is primarily in SPM, which runs within MATLAB.

um so and all the kind of supplementary uh tutorial code that we've provided we've provided i think six six or seven different um different supplementary scripts um where you can actually run the simulations and build these models yourself um they are they are all they're all in matlab um so if you so if you have matlab then you need to have downloaded spm um and with matlab and spm you um

you will have everything you need to do to open our supplementary code and to follow along.

So a lot of the tutorial is actually set up so it assumes that you're kind of have the paper open side by side with the code.

So you can actually click on, you know, different options to simulate and reproduce all the figures in the tutorial paper.

So it's, yeah, so the short answer is you need MATLAB and you need SPM.

And you need to have some minimal ability to work with MATLAB.


SPEAKER_05:
Just to piggyback onto that, for anybody's benefit watching this in the future, whenever you're watching this, once you get the SPM folder, you'll unzip that, you'll get that folder, you'll put that somewhere on your computer.

You'll want to make sure that you add path

from your MATLAB workspace, when you're in the supplementary code provided by Ryan and Christopher, when you're in that workspace, add path of that folder, wherever you've put it, add path of that folder slash toolbox slash D E M. And then you should have access to the additional SPM 12 scripts that you need in order to make use of the functions they've provided.


SPEAKER_02:
Yeah, no, thank you very much.

I mean, that's that,

for someone who's never used MATLAB, yes, there's a few steps like that, which are super important.

Yeah, because basically all these scripts are actually within the DEM toolbox of SPM.

And all the scripts they've provided call on sub-functions that are within SPM.

So yeah, it won't, unfortunately,

none of them will work without SPM at the moment.

Although there are certainly efforts to try to make some of these routines more sort of generally accessible and like free software.

Like Alex Chance, for example, is in the process of, I think, writing like a Python version of the main active inference MDP script, which is called SPM underscore MDP underscore VB underscore X.

And that just corresponds, that just MDP underscore VB underscore X corresponds to Markov decision process underscore variational base underscore X, which stands for factorized.

And I'll go over what factorization is and how you actually code that in once we get to my section.


SPEAKER_03:
Okay, I'm going to ask a follow up there.

So I have my desktop computer, I have MATLAB, and it's all working with referencing SPM, most updated version.

And I've run through the examples that we're going to be working through maybe today or the ones that are in the paper.

So I kind of hit enter and I got things to work.

Everything's all the scripts are working fine.

what information from my system do I need to know?

Like what measurements are gonna be relevant for me to bring to the table?

If I'm gonna do a t-test for whether two ant colonies are a different size, I know what kind of machinery I'm gonna need, two columns in the Excel spreadsheet or something.

But what kind of data or what attributes of the system are important to bring to the table for an investigator to utilize the examples that you're providing?


SPEAKER_02:
Yeah.

There's a couple of things.

The first goal is you take a particular behavioral task and you have to figure out what the regenerative model is for that task.

This is the thing that I'll give an example of.

I can actually show you multiple examples of this.

Once you have this regenerative model set up, then you can run simulations and those simulations will generate

observations and they'll generate expected actions.

So what you need then is you need data from an actual participant performing that task where you'll know on each trial what the person observed, right, what the task stimuli were, and you'll know what action they actually chose.

And so then what you'll do ultimately is you'll

do what's called parameter estimation, which is, and again, this will be a lot more comprehensible once I actually show you.

But basically what you try to do is you try to find the set of parameter values in the model that generates behavior that's most similar to what the participant actually did.

And so once you find the parameters, say one has a value of two and the other has a value of four, and that generates behavior that's identical to whatever a given participant did,

then those values of two and four, you can use those as individual difference estimates.

So if you have those values for a bunch of different people who behaved in different ways on the task, then you could say, hey, is there something different about the people that have a parameter value of four versus a parameter value of eight?

Can I use that to predict, for example, how well somebody is going to respond to a particular treatment, say, in computational psychiatry?

Um, I Chris twice.

Um, but so, but so, but so that's, that's kind of the idea.

Um, you know, and I mean, I can't, I can't share my screen in the meantime.

Um, I don't know.

Well, here I can, I can perhaps go through some of this.

Um, so I'll just share my screen until I'm good.

Yeah.


SPEAKER_06:
Sorry.

That was, this whole thing was, um,

Oh, you're good.

OK, I think I am at least.

Can you guys see my screen?


SPEAKER_03:
Yes, perfect.


SPEAKER_06:
OK, great.

Sorry about that.

There was some whole bunch of privacy issues that I had to pull the passwords there anyway.

OK, we are here, sorry.

OK, so you guys saying my present of you or are you saying the full thing?

We see the full slide.

Yeah, just the slides.

Okay.

So this is just part one.

After I've gone through this, Ryan's going to take over and go through some of the practical aspects of it.

So then just reiterate the scope and purpose.

Our target audience really is researchers in neuroscience and psychology that don't have a strong quantitative background in maths or ML, in particular, early career researchers.

We just really want to provide people with the requisite background.

to actually apply this in the context of their own research.

Okay, and then, so just to quickly highlight some really fantastic other resources, the first papers by Lancelot de Costa, which is an incredible technical review, just came out in Mathematical Psychology.

Norsedjit has a really other incredible paper, just came out in Neurocomputation.

And I think the comparison to dynamic programming in Bellman formulations is still in preprint, if that's right, Ryan.

But as well, there's some really phenomenal informal tutorials with Oleg Solopchuk.

I'm sorry if I butchered your name.

But anyway, they're fantastic, kind of informal tutorials on Medium.

And then lastly is kind of the closest thing to what we're doing today is these lectures by Philip Schwartenbeck in the Computational Psychiatry Summer School.

And these really are fantastic.

The only difference is that they work with

To my knowledge, I think they work with the unfactorized MDP scheme, which is a little less flexible than what we're working with.

Okay.

And so there are a lot of ways of kind of motivating active inference.

I think kind of the way that's most intuitive to a lot of people in cognitive science or backgrounds in cognitive science is to kind of start from the perspective of the Bayesian brain.

And so interrupting, if anything, is kind of moving too quickly or too slowly or anything like that.

But broadly speaking, the idea is that

The brain encodes a generative model of the environment where this generative model is just kind of a joint probability distribution.

And then outside or in the outside world, as it were, there are states and states give rise to observations.

And we use the generative model in combination with Bayesian updating to infer the hidden causes of our sensations from the observations.

and then we take actions based upon our kind of internal model of the world that couples us back to the generative process, if that makes sense.

And so there's this kind of perception-action loop that's always going on.


SPEAKER_02:
So Chris, I think you might want to tell people just what the PO comma S comma pi is.


SPEAKER_06:
Oh, sorry, okay, yeah, so, okay.

So that is a joint probability distribution over observations, states, and policies, where policies are just actions or sequences of actions.

that are available, all of the actions that are afforded to the agent essentially.

To illustrate this, we'll just give a really basic example.

You might imagine being presented with a shadowy shape like this.

What you want to do is infer the causes of that shape just based upon the observations.

But you don't just have observations, right?

You have some prior knowledge about the world.

And so we can specify that here.

So we might say there are two possible causes in this very limited example.

There is, we might say that the shadow is caused by convex surface or a concave surface, and they're fairly similar prior probabilities.

However, we also have the structural prior that we've kind of acquired through a lifetime of experience of just light emanating from above.

And under kind of a structural prior that light emanates from above, the likelihood of the observation of that particular shadow kind of conditional on being concave is much, much higher.

Okay.

And so then we can then combine the two, the prior and the likelihood, to give us a joint distribution where this is just kind of conditional on a specific observation.

We can then, and then we sum over

the states in that likelihood to give us our marginal likelihood and then divide our joint distribution or our generative model by our marginal likelihood.

And that's called model inversion or is often colloquially called model inversion.

And from there we get our posterior distribution.

This is the probability of states conditional on observations.

And so we started with a prior and a likelihood and got to a posterior through Bayes' rule.

That is formally speaking the optimal way to infer the probability of hidden causes given observations.

But the complication to all this is that the marginal likelihood is generally speaking computationally intractable.

In the case of these very simple discrete distributions, the number of sums that you have to perform scales exponentially with the size of your hypothesis space.

That's extremely impractical.

And kind of more realistically, when you're working in continuous state spaces, if you have non-Gaussian or non-linear signals, their distribution is analytically intractable.


SPEAKER_05:
So Chris, I think Max had a question.


SPEAKER_06:
Yeah, yeah, sorry.


SPEAKER_05:
Yeah, real quick.

Just to really explicitly touch on the idea of hidden states and hidden variables in this particular context, would it be fair to say that it's not only the convexity or concavity of

the potential thing that I'm looking at, but also I could think of the light source, the direction from where the light source is as being another hidden variable, and those things together, would it be fair to term those as under my Markov blanket, or is that getting a little too far out from where we want to be talking right now?


SPEAKER_02:
Yeah, so Markov blankets don't really, the Markov blanket concept here doesn't really come in in any really interesting way.

I mean, you can formulate it that way, but really the idea is just

you have this prior expectation.

So when you're looking at the little gray disc right there under observation, most people, when you look at it, even though it's actually just a flat shape with some, you know, it's just a flat two-dimensional thing that has a little bit of darkness in one place, a little bit of lightness in another, you know, if most people see that as being concave, even though it's a flat 2D thing, right?

So the idea is just that there's a reason, right, why people

typically see it as being concave, as it's kind of like popping in as opposed to out.

And that's specifically because there's this prior belief that light sources tend to come from above.

So the hidden states here are kind of, there is a joint in the likelihood, which just means that there's a, in the upright where it says likelihood, there's a concave comma light from above, right?

So those are two different hidden states.

What it's saying is

if the hidden states are concave and light from above, if the combination of those things is what's generating what I'm seeing with that gray disk thing, the probability of the shadow shape, the shadow pattern that you see, is 0.9.

Whereas if it was convex and light from above, if those were the two hidden states, then the probability you get that same shadow pattern is only 0.1.

In other words, it'd be really hard

to get that pattern of shading if the thing was popping out under the assumption that light's coming from above.

So the observation here is shadow and the hidden states are concave or convex and light from above and light from below.

Okay, thanks.


SPEAKER_03:
Yeah, just one other quick example there would be like the chessboard with a shadow and a dark square.

It's perceived under a deep cultural prior that chessboards are regular alternating grids.

It makes an ambiguous shaded square appear as something that's quite different than it is.

People can look that up.

And then another thing this reminds me of is like using a t-test

to test whether group A versus B is taller or something like that.

It assumes, but it can be an assumption that can be slightly bent, is it assumes that other than the variable being considered, other things are sort of fixed or not mattering.

Otherwise, your statistical test is basically misleading because it could be capturing some totally confounding variable in the framework you put it in.

And so whenever you're talking about a real biological organism, you're talking about conditioning on certain things.

And then given what is totally conditioned out of the picture.

So I kind of see where you're going with the blanket, but it's not specifically in this place, but a good question.

But it's like, depending on the irrelevancies that we can condition out, then we're looking at the conditional relationships between different observed states versus the hidden things.

So cool stuff.

And again, in the live chat, people can post any questions on it.

Continue, Christopher.

Yeah, thank you.


SPEAKER_06:
So yeah, thanks for covering that, Ryan.

OK, so then the idea is that the marginal likelihood is generally computationally intractable.

but kind of borrowing some ideas from statistical physics, what you can do is actually use some approximation techniques.

And instead of evaluating the marginal likelihood directly, we can evaluate something that's always provably greater than or equal to the marginal likelihood.

I'm going through that right now.

Okay.

And so we're just going to take the logs kind of for mathematical convenience.

And the reason we always work with logs or generally speaking is because log

Log algebra is just a lot easier just because it turns multiplications into arithmetic.

It's just easier to work with essentially.

We'll take the negative log of our marginal likelihood.

This is also sometimes called Bayesian model evidence.

When we have a negative log of a probability, it's also called surprise and information theory, which is the terminology that I'm going to use the rest of this presentation.

You can see on the right-hand side of the equality,

this is the sum rule of probability, we can get back our surprise by just summing over all of the states under our joint distribution of the states and outcomes.

Then we can do a little bit of a trick here.

So what I'm going to do now or show you is that we are going to multiply this joint distribution or generative model by some arbitrary distribution.

We multiply and divide by the same distribution.

So nothing has changed here, this quality still holds.

I could cancel these if we wanted to, but I don't want to.

And that's in particular because I want to take advantage of something called Jensen's inequality.

And that is that a result showing that the expectation of a logarithm is always less than or equal to the logarithm of an expectation.

And so the idea is, is that this sum here, where the logarithm is inside this summation, where, sorry, so we're taking, we're summing over

this kind of difference between a generative model and this approximate or target distribution, when the logarithm is inside that, it is always going to be, and we're in kind of negative territory, it's always going to be greater than this quantity on the left-hand side, where this left-hand quantity is equal to surprise.

Because the log is outside the sum, we can cancel these kind of approximate distributions, right, and end up back at surprise.

On the right hand side, we can't do that.

This right hand side will only be equal to surprise when our generative distribution perfectly matches our approximate distribution.

And so this quantity on the right hand side here, this is variational free energy.

And so the idea is, is that we find the probability distribute this approximate probability distribution, which we make simplifying assumptions about.

And so we can kind of evaluate analytically

We find the value of this distribution that best minimizes f, where f is variational free energy.

Does that kind of make sense?


SPEAKER_02:
Hey, Chris, just one thing.

Are you trying to move your mouse around to point to things?


SPEAKER_03:
No.


SPEAKER_02:
I'm actually putting my mouse.


SPEAKER_03:
I'm scribbling my mouse a little bit on the live stream.

They should be able to see that.

But Christopher, thank you for that really awesome example because it really clarified a few things.

Any other notes to add here?

Otherwise, this is cool to continue.


SPEAKER_06:
Yeah.

Okay.

Then just to give a really toy example of this.

We can define our approximate posterior arbitrarily as a flat distribution.

We'll have a true posterior.

Generally speaking, we don't know what this is, but for illustration sake, we're going to give it to you.

Then we can have a joint distribution and an observation.

What the observation will do is it's just going to select a column from this joint distribution.

And so then we can enter this into the equation we saw on the last slide.

And we'll slowly but surely in each update nudge our posterior distribution such that it minimizes f on each step.

And at the third step, what we should see is that when f is at a minimum, it is equal to surprise.

And when f is at a minimum as well, it's at a minimum because

the true posterior and our approximate posterior match.

Does that kind of make sense?

And so the idea is that by performing Bayesian inference and forcing some arbitrary distribution to approximate our posterior, we come up with an upper bound on surprise, or we come to an upper bound on surprise.


SPEAKER_03:
Let me kind of just walk through and just double check here.

So Q of S is our estimate of whether the coin is a fair coin.

50-50 means that it's 50-50 and then 80-20 is the reality on the table, so to speak.

So it's almost like first we click from 50-50, we see heads come up.

And that contributes a little bit of evidence that maybe heads is more likely.

So we go from 50-50 to 60-40.

And then something happens again.

And we click on the second update to 7.7.3.

We click eventually to 0.8.2.

And then if we click all the way to 0.9.1, so like a 90.10, all of a sudden we go back up from 0.693 back up.

So if we were to plot this final estimated F, we'd find that we were like going downhill, getting better, get to 0.693 in this discrete,

0.1 movement space, and then we pop up a little bit too far, we overshoot, and we get a little bit more surprised by the distributions that we're drawing.


SPEAKER_06:
Yeah, exactly.

That's perfect.


SPEAKER_02:
Well, the only thing that I would say that's, I mean, a little different, just to note, is that there's nothing in this example where you observe something over and over again.

This is just a single observation once, and you're trying to figure out what

what is the, you're trying to figure out what the true posterior is, trying to get as close to it as possible.

So all you're doing is saying, here's the single observation, but I'm going to try out a bunch of different values for Qs, a bunch of different values for that approximate distribution.

And I'm just going to find the one, you know, through this kind of iterative updating thing where I slowly move Qs, you know, to different values.

You just do that for a single observation and you update until you find

the minimum free energy value, which just tells you that your approximate posterior, the QS thing, is really close to the true posterior that you couldn't figure out on your own because the problem was too tough to solve with exact Bayesian inference.

So this kind of corresponds to what people might talk about as prediction error minimization, right?

So you just see this observation once, and the brain tries to minimize prediction error by minimizing F

by just moving beliefs until it finds the belief that minimizes F, which is the same thing as minimizing prediction error.


SPEAKER_03:
If that makes sense.

Let me clarify, because I think that really helped me understand it.

And at first I thought, wait, with one observation, because at first when I described it, I was thinking update was tied to a new observation from the coin.

But then I thought, well, if you're only getting one observation, the maximum likelihood model is a coin that only comes up heads because you only have one observation.

But it's not one shot parameter learning naive.

It's actually a tethered estimate that's tethered even loosely, but non-zero tethering to a prior.

0.5, 0.5, some people will say uninformative, but all priors are informative.

They're all what they are.

And then,

it's almost like because of the logs, even though you only observed the coin come up once, it's like, all right, it's a little too far to update to 99.1 just from seeing one coin flip.

I would need extraordinary evidence for extraordinary updates.

And so it's like that one heads observation updates you from 50.50 to near 80.20, which does happen to be close to the actual probability of the coin.

And it also happens to navigate this explore exploit in an interesting way because it updates it, but not all the way.

So it's just kind of showing how the Bayesian updating brings some of that wisdom of multi-observation learning, like slow updating of parameters and sequential updating to a little bit of a different context.


SPEAKER_02:
I do want to clarify here a little bit.

I mean, there's no actions here, so there can't really be explore-exploit.

You can't choose to look over here versus over there to gain information.

So explore-exploit is specifically in the realm of choosing actions that will minimize uncertainty versus maximize reward.

This is really more just with normal free energy.

We're not to expected free energy yet, which is the decision-making part.

With normal free energy, you can think the simplest way to think about free energy is just in terms of complexity minus accuracy, which is equivalently complexity plus prediction error.

And all that ultimately means is, so the complexity thing is basically how much you have to change your beliefs.

So it's basically saying, what's the minimum change in my beliefs

that will make my new beliefs as accurate as possible, right?

So I have to move my beliefs as little as possible while also minimizing prediction error, if that makes sense.

So what this generative model is saying is just that the probability that we're in state one is 0.8 if I were to get this observation, where the probability of being in state two is only 0.2 were I to get this observation, right?

So, I mean, probably a better example than the heads and tails thing would just be something like the concave-convex example we gave before.

Hidden state.

There is some possibility where light from below could cause the shading pattern, but it's just much less likely.

So you're just trying to find the belief that it's convex and light from above that is the one that most likely generated what you're seeing.

And it might take a bunch of updates like that given just what you see once to kind of arrive at the best fitting belief.


SPEAKER_03:
There's a related question in the live chat that I'm just going to ask because it's on the topic.

They asked, is this approximate Bayesian inference thing called something else in stats outside of active inference, or is this unique to active inference because this isn't sequential Bayesian updating, as you mentioned?

This isn't a standard Bayesian filter.

What is this called outside of the active inference field?


SPEAKER_06:
Yeah, it's just variational Bayes.


SPEAKER_03:
Variational Bayes, where does the partially observable Markov decision process come into play?


SPEAKER_06:
uh we'll cover that in a moment's time actually yeah it doesn't it doesn't yeah it doesn't come in yet so i think one thing to say is kind of writing this tutorial it's hard to please everyone right like in going through some of the feedback like we get we got some feedback where people were just like utterly confused and wanting clear explanations and then at the other end of the spectrum we wanted there were people wanting much more technical details like how does this relate to things like gradient descent because this is just kind of uh

very simplified cartoon example of something called a gradient descent scheme where you're doing a gradient descent on free energy.

And so I would just say kind of to flag all of these issues is one, I would say if something's unclear in this presentation, read the paper because we cover things in a lot more detail in the paper.

And if that isn't technical enough for you, actually go and just look at our code.

We supply a kind of standalone script where we

it's extreme and it's extremely well commented.

And from there, you should be able to figure out everything that's going on.

So all of this stuff in this presentation is kind of necessarily simplified because you don't really need those technical details to kind of start using the framework or start getting intuition for how things work.

But once you do have an intuition and you want more, just go and see the code, I would say.


SPEAKER_03:
Awesome.

It's like you can use the ANOVA package in R without going into the source code.

It's helpful.

It's a tool for scientists.

And then if you're curious about the underpinnings of statistics and perennial philosophical debates, there's a literature and a search bar.

But today it's about the applications of these methods, which is awesome.

So thanks everyone for the questions.

Keep them coming.

But this is great discussion.

I really appreciate it.


SPEAKER_06:
Okay, and so just kind of to recap there, I haven't covered action yet.

I'm going to cover action a little bit.

But just kind of to recap, the idea is that under active inference, organisms are kind of, we model them as if their phenotype is in their body and their brain kind of embodies a generative model of the environment.

And organisms kind of invert the generative model to arrive in an approximate posterior distribution over the hidden causes of sensory input.

And they do this by minimizing variational free energy.

That much should hopefully be clear by this point.

Then onto the generative model, and it is a very specific type of generative model.

Namely, it's a partially observable Markov decision process.

There's a graphical representation of this as a Bayesian network on the right-hand side.

I'm not going to give too much detail about this right now.

I will actually build it up step-by-step.

But just prelude,

The idea with POMDPs is that they describe transitions among hidden unobservable variables and the sensory data that's generated by the variables.

Whoops.

So those arrows mapping from states to observations kind of give information about the direction of influence or conditional influence.

So kind of the arrow between the purple node

O and the green node S is mediated by the A matrix, which is a likelihood.

And the transitions between states are mediated by a B matrix, which is a transition probability.

And so the goal of active inference with POMDPs is to infer states and action sequences or policies by minimizing various forms of variational free energy.

So let's start with a really, really simple example.

We have static inference.

The idea with static inference is this is just a graphical representation of Bayes' rule, essentially.

We have a prior, which is encoded in our D vector.

We have a likelihood encoded in our A matrix, and we end up with an update equation, which is a softmax function.

A softmax function is a normalized exponential.

And so I'm not going to kind of run through that explanation in detail.

I'm just going to kind of leave the slide here so people can figure it out for themselves and pause and go back and convince themselves that this is true.

But for this very simple example, the inference scheme that we're using here is formally equivalent.

It's just an exact inference scheme.

So moving into dynamic models, specifically so where states change over time, these are also called hidden Markov models.

This is when we have to start making approximate or when things are no longer equivalent to using active inference are no longer equivalent to exact inference.

And so here we have our transition probabilities encoded in a B matrix.

And this is just essentially like the probability of some state of T plus one conditioned on the previous state.

And so you can see over here in terms of the update equations, we're now in log space and this little sigma thing here is a softmax function, which just normalizes this equation.

And the idea is that this is kind of, sorry, my screen keeps flipping.

Anyway, the combination of D and B, so our two priors, in addition to our likelihoods,

will give us the approximate posterior.

And the reason we have this one half in front is just because in practice, the approximation scheme that's used by active inference, namely variational message passing, tends to overestimate kind of the value of the posterior.

And so this is kind of just a way of compensating for that.

And so anyone who's kind of interested in technical details there, see Thomas Parr has a really excellent paper out in Scientific Reports on neuronal message passing schemes under various approximations to free energy.

Okay, and so then the form of the free energy is just down here.

And so the idea is that by iteratively applying these updates, we minimize free energy.

And you iteratively kind of, you do a full round of updating every time you get a new observation.

Okay, but what about policy selection?

So the idea with policy selection is roughly speaking,

that policies are just state transitions that the agent has control over.

So imagine you are a psychophysics experimenter and you have just a very simple, boring example of someone kind of, say, estimating the orientation of some hard to see stimulus.

And so the policy space there is tiny.

It's two options.

It's left or right, let's say.

Or you could have an agent that you could be simulating something more interesting in psychophysics.

You could be simulating an agent navigating a maze.

And then the policy space is much larger.

They could move forwards, backwards, left, right, et cetera, et cetera.

And so the idea is that active inference agents, by definition, need to select policies that will minimize variational free energy.

But that relies upon observations that have yet to come.

So that's kind of a problem.

But the way around this is to treat observations as random variables.

And then what you do is you minimize an approximation not to surprise, which is variational for energy, but to expected surprise.

And that approximation to expected surprise is expected free energy.

And so this expected free energy has two key components.

The first is the expected cost.

And this is kind of minimize.

The idea here is to minimize expected cost.

you need to minimize the deviation between our predicted and our preferred outcomes.

And so this is kind of what we were talking about, Ryan was talking about before.

So this C vector encodes a distribution over an agent's preferences.

So for example, policies that minimize expected costs are policies that kind of bring about observations that the agent prefers, for example.

So if I prefer to kind of have my body temperature within a certain range,

The policy would be maybe staying inside because I live in England and it's like negative one outside, for example.

Now, expected ambiguity is a little bit... This is kind of the epistemic drive or the information gain term.

This is kind of the expected entropy of our likelihood distribution.

So the idea here is that to minimize ambiguity, you will have an A matrix, right?

An A matrix, if the idea is to minimize kind of ambiguity, you need to select...

observations in that A matrix that are maximally precise.

And so to return to or take actions even that will make those things, that A matrix, maximally precise.

And so if I'm in a dark room, to give Ryan an example before, the thing that will make or select kind of or policies that will make that distribution maximally precise are policies like turning light on.

And so then to minimise free energy as a whole, you have to minimise both of these things.

You can do this in terms of one-step policies, just looking one step ahead, or you can do it using deep policies, looking many, many steps ahead.

Hopefully, that's pretty clear.

Is there anything you'd like to add to that, Ryan or Daniel, anyone else?


SPEAKER_02:
Yeah, I guess I'll just say, just to make it as clear and explicit as possible for people who don't have a background in what this whole thing, what this equation means.

So the idea is just if you look at that term above expected cost, that's called a KL divergence.

And so basically it's just a value that encodes the difference between two distributions or the dissimilarity between two distributions.

And so that first one, that Q O given pi, that's just saying what observations do I expect given that I choose to do this versus that?

Then that second one, that P of O, that's the preferences.

So basically what it's trying to do is just minimize the difference between your preferred observations and the observations that you expect, given that you choose to do thing one versus thing two.

So it's really just choosing the thing that you think is going to get you as close to what you want as possible.

It's very just reward-seeking, more or less.

And that second one, the expected ambiguity, that H thing stands for entropy.

And basically entropy is just the higher the entropy, the flatter a distribution is.

So think of a distribution that's like one is like 0.5, 0.5 and the other one's like 0.8, 0.2.

Um, if you chose the state that would generate 0.5, 0.5 over observations, then it wouldn't tell you anything because either observation you've got, it tell you there's a 0.5 probability that you're in one state and a 0.5 probability you're in the other state.

Right.

Whereas the other state would generate a 0.8 or a 0.2, then that one will give you a lot more information because if you observe the thing that indicates 0.8, then you're really confident what state you're in.

Or if you observe 0.2, you're really confident you're not in that state, right?

So you're just seeking out the thing that you think will get you what you want as much as possible, but also moving to the states that are going to,

give you the observation that's going to tell you the most about where you are, if that makes sense.

So just, again, just for people who aren't necessarily as familiar with reading the kind of notation in these equations.


SPEAKER_03:
Thank you.

Max?


SPEAKER_05:
Yeah, just one point of clarification, and then I just wanted to tie it back to the example we had discussed previously.

So first I want to just make sure I'm understanding correctly that the D in that equation is the KL divergence.

So that's an operator.

It's not the same as the D that's in the figure in the block diagram.


SPEAKER_02:
No, that's the KL divergence.

That's kind of a standard way of representing it in other papers.

That's just kind of pasted in, but it's not very clear.

That should be the KL.

It has nothing to do with D.


SPEAKER_05:
And then the D that's in this graphic in the block diagram, would that be analogous in our previous example where we were talking when we illustrated step by step, that would be our 0.5, 0.5.

But in this context could be much more complicated than that.


SPEAKER_02:
Yeah.

In a simple example, yeah, it would just be your prior, right?

So if ahead of time you think that like ice cream is more likely than donuts, right?

then that could just be like 0.8, 0.2.

Or if you have no idea whether it's going to be ice cream or donuts, it could be 0.5, 0.5.


SPEAKER_05:
And for anyone who is wanting to get into optimization and how do we know whether or not this is going to converge when we do our message passing algorithm back and forth, there was a really good citation, and that's the one that was alluded to at the beginning of this on the slides, right?

The technical papers, is that correct?


SPEAKER_06:
So the citation to do with message passing in particular is a paper by Thomas Parr.

So that is in scientific reports.

Yeah, we can, I think, do you have show notes?

I assume we can.


SPEAKER_03:
Yes, please.

I have a couple other general questions from the live chat, but I think we'll take them at the end of your presentation as we turn towards some of the neurobiology beginnings and a few other aspects.

So continue, Christopher.


SPEAKER_06:
Yeah, cool.

Thank you.

Okay.

And so then kind of just to close or just to briefly recap, so there are multiple stages to this, to policy selection.

So under a generative model that has multiple policies, you need to minimize, you minimize your free energy with respect to each policy, your variational free energy.

And so you might think about this as say you're coming to a set of traffic lights and you have a whole bunch, you could turn left, right, or go straight ahead.

Those actions are possible, but you then get,

sensory input that says there's a no left turn sign.

So that would make that policy given an extremely high free energy value.

And so that would eliminate it from the plausible policies that you can kind of evaluate, right?

And so then expected free energy is what we just talked about.

And the posterior distribution is actually the softmax function over both of these.

And so this, because kind of these are both negatives,

policies that best minimize both variational free energy and expected free energy will have the highest posterior value.

Is that clear?

There are a couple of caveats to all of this, but just wanting to make sure.


SPEAKER_02:
Ryan, do you want to add anything here?

Well, I'll just say, just so anybody who doesn't know what a softmax function is, so all that means is that, for instance, when you take

minus F minus G, that's not going to give you something that's a true probability distribution, right?

It's not going to be a thing that sums to one, right?

Where probabilities altogether sum to one.

What it's going to do is give you this negative, you know, a bunch of negative numbers, right?

So what a softmax function does is it normalizes that, which just means that it takes the kind of relative values, you know, of each of those things and turns it into a probability distribution that sums to one.

So you end up with that bolded pie symbol is just a probability distribution that assigns a probability to each possible policy.

And a policy, by the way, is just a sequence of actions, right?

So one policy might be like turn left, turn right, and another policy might be turn right, turn left.

And so it's just saying that that minus F minus G thing

will just be turned into a probability distribution.

And that probability distribution over different policies is what the agent will sample from, right?

So if pi says this policy is 0.8 and this other policy is 0.2, then the agent will be a lot more likely to choose the policy that's 0.8.

Yeah.


SPEAKER_03:
Maybe go to the last caveat slide and then I have a few other questions where we'll just try to like have a question, simple answer, question, simple answer.

And then we'll move into the part two screen share.


SPEAKER_06:
Okay, so the two caveats are just, there are two extra components to the POMDP.

That is namely the E vector, which you can see is kind of pointing to, or there's that E block that's pointing to policies, and then there's our gamma as well that's also pointing to policies.

And so E is how, generally speaking, how one models habit formation, or one way of modeling habit formation.

And so I'm not going to talk about that now, I just kind of wanted to flag it.

And then when things come up in learning, you won't be surprised there's now this extra term in the policies.

And get what Gamma does.

Gamma is essentially weights the contribution of G to policy selection.

And so the idea is, roughly speaking, is that G is essentially a, you have your prior distribution of policies, which doesn't take in F at all.

And then you have your posterior distribution, which takes in F. And then you look at the difference between those.

And if there's a large difference between those, roughly speaking, then gamma will go down.

It will essentially downweight the contribution of expected free energy to your posterior over policies.

And this is kind of linked to, this is how we model phasic dopamine spiking, for example.

I'm not going to get into any more details there because I think it's actually much easier to just have like a fully worked example, but I did just want to flag all that stuff.

Okay, so I think that was my last.


SPEAKER_03:
So that's like this tightening of strategy while things are effective, but not overfitting, but a tightening while things are working.

And then during a mismatch, a period of high uncertainty or ambiguity, there's actually a movement towards more exploratory behavior.

Okay.

Let me try to go through a couple of these questions.


SPEAKER_02:
So we'll just try to have the sort of clippable... One further thing I just wanted to clarify about that, though, is that so gamma thing, really you should think about that as kind of like the precision estimate for expected free energy.

So all it's doing is just saying, hey, if things were super different than what I thought they were before policy-wise, then, I mean, in some cases, it's a little more complex than that.

But basically...

I can't trust my expected free energy estimate as much, right?

Right.

So I should downweight, I should sort of downweight how much my expected free energy contributes to what I choose to do because it's not as reliable, right?

So it's just saying how much should my habits and my current observations affect what I do versus what I expected ahead of time, how rewarding they were going to be.


SPEAKER_03:
Right, so if the clock has always been accurate, and then you look at it and you're like, wow, I didn't know it was 1pm, you're like, I believe the clock.

But if you know that the clock is inaccurate, then you think, I thought it was noon, I looked at it, it said 1, but you know, who knows with this clock these days.

So, that's about the confidence in the observation.

Okay, so, policy selection.

Two quick questions on that.

The first one is, what is the timescale of policy selection, and does this model assume any particular temporal scale?


SPEAKER_02:
um so i mean temporal temporal scales are kind of just whatever you want them to be right so so like you know in the context of a task right you might just say here's time point one which is where you know a participant gets some observation right and here's time point two which is where they choose an action right in which case that would be a two time step trial um

And I mean, it doesn't really matter, right?

I mean, it might, the stimulus might be on for five seconds or it might be on for one second in the first, you know, time step in the trial.

Um, and maybe their decision takes half a second, right?

So the time step two could be half a second or it could be like a minute, right?

I mean, like, so it's time steps are kind of whatever you define them to be relative to a task.

Um, you know, in another session, when we get to hierarchical models, you will have, um,

higher levels that operate on slower time scales than the lower levels, but that's a kind of more complex thing that we'll have to cover later.


SPEAKER_03:
Okay, another policy-related question is, can you comment on different forms of policies, such as plans from tau equals zero to big time T, so from the beginning till T, and then tau equals little t to big T, belief action policies, tree search followed by belief action policies, and then how is this related to different constructs, such as working memory and habits?

So maybe just...

First part of the question was, how are different kinds of policy estimation undertaken?

And the second part is, how is that related to maybe different constructs we have with regular vocabulary like working memory or habits?


SPEAKER_02:
Do you want me to take that, Chris, or do you want to?


SPEAKER_06:
Yeah, I'm happy to take it, I suppose.

So you can have shallow policies or you can have deep policies, where deep policies are just kind of

time point whatever time point one to t and you're just summing over basically all of you basically it's the path integral formally speaking just over expected free energy and that is updated at each time point um or you can have shallow policies where it's just one time step uh how does this relate to like things like tree search and all that stuff um well it depends so i i'm assuming that that question means that you're kind of familiar with like bellman formulations and that kind of thing um

So expected free energy is Bellman optimal for one-step policies, and it's not Bellman optimal for deep policies, but there are what's called a sophisticated inference scheme, which is, is this correct Ryan, that's formally equivalent to backward induction?


SPEAKER_02:
Yeah, yeah.

So yeah, sophisticated version of active inference, which is more sort of explicitly like a deep tree search, is equivalent to backward induction, which is Bellman optimal, yeah.


SPEAKER_06:
Exactly.

There are forms of this that do relate to tree search.

In regard to working memory, depends on how you want to set up the task.

There are models of, say, working memory where you have two-level models, where you model working memory as the thing that has stable maintenance, or you can model it with one-level models.

It just depends on how you want to set things up.

Generally speaking, I think of

working memory is being functionally defined.

There are lots of definitions, I won't put one out there right now, but like something to do with maintenance, right?

And then it kind of just depends on what you're trying to model about working memory, what type of model is appropriate.


SPEAKER_02:
Yeah, I mean, one additional thing to say that's just really kind of basic is that working memory is gonna come down to what your transition matrices look like, right?

So if you're put into state one,

and you have a really strong belief in your transition matrices that if you start in state one, you're going to stay in state one, then that more or less amounts to that state remaining active over several time steps.

And if you learn that at time step one, and then at time step four, believing you're in that same state tells you what action to do, then that's a type of working memory.


SPEAKER_06:
Thomas Parr has a really awesome paper, Prefrontal Computation as Active Inference, where he kind of thinks about transition precision in terms of excited tree recurrent connections in lateral PFC, essentially, which have been linked to things like maintenance.


SPEAKER_03:
Cool one last quick take before we go to Ryan sharing your screen is related to modeling a given behavioral task, what are the criteria for setting the factors, where the model will be embedded.

So how do we just sort of operationalize the kinds of things that we take into account in this model, I believe, is what the question is asking.


SPEAKER_02:
I mean, in a lot of tasks, there's probably not one unique way, right?

I mean, that's kind of where the kind of creativity problem-solving aspect of this comes in is because you kind of have to figure out, right, what factorization structure, what generative model structure, whether it's factorized or not, right, or what different sets of ways to set up a model that can generate

uh behavior that you would see in a task right i mean i'll give you i'll give you an example right when we build this task build this task model together in my part here um but you know general in a lot of cases you can make something factorized or you can make it not factorized um and you know sometimes you can set actions up so that uh it has to do with

Probability is in the B matrix in your transition beliefs.

In other cases, you can set it up so it's in the likelihood in the A matrix.

My point is there isn't one unique solution always for what generative model to use.

A lot of times you might try out multiple generative models and then do model comparison to figure out which one's best.


SPEAKER_03:
Yeah.

Maybe while you're setting up your screen, I'm going to ask one more question.

Somebody wrote, I'm still not exactly clear on why you would want to maximize information and minimize the difference between your desired outcomes and your expectation.

What advantage is afforded by maximizing information if it doesn't enhance the likelihood of receiving the desired outcome?


SPEAKER_02:
Well, it won't.

It won't in that case.

Like typically what will happen is if there's, so the agent,

I mean, it depends a little bit on the exact task set up, but what will usually happen is that if the agent knows what to do to get the reward, then that just means that that cost term or that kind of reward probability term will just be the one that dominates.

Like the value of that will dominate.

The thing will just select reward automatically.

The information seeking will typically only have a high weight.

if it's the case that the agent doesn't know what to do yet to get the reward.


SPEAKER_03:
You know, that reminds me a lot.

It reminds me a lot of the game theory strategy, which is like tit for tat, but then you start out playing nice.

It's like default to being cooperative, but then have a strategy.

And so it's sort of like a meta approach, which is if you're winning the game of Go or chess at whatever training level, then what is there to do?

Your strategy is working.

But then when there isn't a victory observation,

Then it entails an exploratory search that percolates through higher and higher abstractions of learning in the system.

So I hope that we can make that a little bit more tangible with this session right here.

So we have 40 minutes left.

And again, this is just part one of multiple.

So let's go for, you know, 20 to 30 minutes followed by 10 to 20 minutes of wrap up and final questions from the chat and a couple other ones I have stored up.

But thanks and take it away, Ryan.


SPEAKER_02:
Okay.

So I want to just ask real quick, can you guys see my like mouse if I like point to things?

Yes.

Yeah.

Okay, okay, because I need to point to things when I'm presenting this stuff.

So, okay, so just kind of, again, to get to scope here, I mean, the end goal of the tutorial is for people to be able to actually do research with these things, right?

So the end goal here is to learn how to build task models for empirical studies.

And the basic idea is that you're going to have participants perform a task.

And then what you want to do is find the parameter values in the model,

Um, say like values for precision or values for prior expectations or values for, um, habit distributions, right?

That E thing that, um, I'm not going to probably, I won't use either in this example, but, but just things like that, right?

Where different things in a model could take on different values.

And based on those values, the agent will act differently.

It'll make different choices.

And so what you want to do is take the behavior of an actual participant,

and then find the values for those parameters that best reproduce their behavior.

And then once you have those parameters that best reproduce a given person's behavior, and you do that for all the participants, then you can just use those values, those parameter values, as individual difference measures.

You might say, like, if one person has a higher precision, a higher

a matrix precision than another person, you know, does that predict something about, um, for instance, how well they're going to respond to a treatment like in computational psychiatry, or can that tell you something about, um, you know, some other cognitive function or, I mean, you know, et cetera, et cetera.

Um, and, um, so the goal again is to get people to that point where they can do that.

And like I said before, this is the kind of thing that people have been doing for a long time in reinforcement learning.

But it's new.

It's a novel, sort of not very common thing yet to do an active inference because not enough people know how to do it.

And so, you know, I can give you examples here.

If I can get my... What the... Oh.


SPEAKER_00:
It's...

I don't know why this is not letting me advance.


SPEAKER_01:
Okay.


SPEAKER_02:
So, you know, in these tasks and very, you know, in another session, we'll, you know, walk you through how to build like a perceptual task that's like widely used for like EEG research, as well as neuroimaging research more broadly, but called like an oddball task, which is primarily a perception task.

And there are also sorts of inferential, like prospective decision-making tasks that you can use that don't involve learning.

And then there are tasks that, you know, that use reinforcement learning or explore exploit dynamics.

And then you can also use it with neuroimaging, right?

You could say like trial by trial, where are the prediction errors, you know, in the brain, what parts of the brain look like they're doing the free energy minimization, you know, trial by trial.

So there's things like that that you can do.


SPEAKER_03:
okay um and so sorry what just that that really reminds me of just to pull back a level making measurements of a biological system like gene expression or neuroimaging and then saying what gene is upregulated in people who have this condition or

what brain region is activated in this condition.

And we're taking that beyond the simply descriptive, beyond the just measure and do a t-test.

And we're thinking about these observations, these behavioral phenotypes as being emitted

often across tasks potentially or across modalities emitted by a generative model that has these higher level parameters that shape the way that the person or the organism responds to stimuli under a certain specific framing of a time and task dependent model.

Just to sort of pull back a level and give that, but continue Ryan.


SPEAKER_02:
So, so I just want to say that, you know, there are, this is kind of a new

a new thing to do.

Not very many people have done it.

You know, my lab has kind of been trying to kind of get this approach out there more.

And so, I mean, here's some example papers that just do each of the things that I mentioned, right?

So this one on the right here, it's a recent class computational biology paper is using active inference primarily just for a perceptual model

And with that one, we were just looking at actually interoceptive processes, right?

So, for instance, every time people feel a heartbeat and then indicate by pushing a button that they feel their heartbeat, you know, who does better than that than others and why, right?

And some people might have stronger prior expectations that they are or are not going to feel a heartbeat.

And some people instead might just have different levels of sensory precision, right?

Like,

they treat the signal coming up from their body as more or less precise, right?

So that's like one example.

This paper in drug and alcohol dependence was a learning, an action, or a explore, exploit, and reinforcement learning task that was applied to people in substance use.

And then this one in the lower left, it's greater decision uncertainty paper, is an example of

purely just sort of planning, like decision-making inferential model without learning.

And then Philip Schwartenbeck's paper up here on the right is an example of using active inference to look for the neural correlates of particular model parameters.

So for instance, with the drug and alcohol dependence paper, it just used this simple explore-exploit task where people had three options.

They could push button one, two, or three,

And then you get either a green or a red ball would kind of fall.

And green meant they won and red meant they lost.

And initially they didn't know, you know, what the probability was of winning or losing for each button.

So the first few choices they had to kind of explore, right, to figure out which one's giving the most greens.

I mean, eventually they become confident that one of them is best and they kind of stick to that one.

And I won't go

through it, but you can specify a particular version of the graphical models that Chris showed that generate appropriate behavior like participants do on this task.

And again, I won't really go through this, but this is what the likelihood or A matrix would look like that they would wear over time, they would learn these A zero values that basically say, if I choose

slot machine one two or three what's the probability that that's going to generate uh this row the observation for this row which is a win or the observation for this row which is a loss and then the c vector here the one that encodes reward just says whatever the cr value is which would be a positive number just says i prefer winning versus losing by some amount um and then this is the equation for learning and then this is just showing um

So just showing the expected free energy equation.

And again, this isn't the example I'm going to work through, but what you could do, I'll skip that, is you could find that people with substance use showed lower action precision.

In other words, the behavior was a little more random.

They showed higher learning rates for wins and slower learning rates for losses compared to healthy people.

So you can tell this interesting story about, hey, if people with substance use

learn more slowly whenever a bad thing happens when they take the drug, but they learn more, you know, more quickly every time they feel good right after taking a drug, then, um, their behavior is going to have a much harder time stop stopping, uh, taking, taking a drug, um, despite the fact that it has negative consequences.

Um, you know, and so you can fit, uh, these are just group estimates just showing in a Bayesian way that learning rate for losses was, uh,

different um in people with uh substance use than healthy people um and uh the example for just uh planning and decision making without learning this was a uh an approach avoidance conflict task basically people just had to decide where to move this little uh avatar dude on uh along this little runway line and the closer they were to like the sun uh and uh

and a bar that had lots of red.

That meant they were going to see a nice, happy image, and they were going to win some points.

The more red in the rectangle thing, the more points they would win.

And a rainy cloud meant they were going to see a really terrible picture and hear a terrible sound.

So in some cases, they would need to say, OK, I'm willing to go through seeing this really negative, aversive thing to win a lot of points.

or not, right?

So in this model, it's just trying to say, you know, they start in the middle, for example, you know, how close do they decide to get to one side or the other, which controls the probability of getting what's on one side or the other.

So it's just planning where to move, right?

There's no learning because they already know exactly what the rewards and punishments are.

And so in that case, again, I won't go through this, but you can specify a generative model for this.

In a similar way, you can run simulations of what people will do under different parameter values.

This beta thing is the expected precision for expected free energy precision.

And EC here is just how much they like the reward, like the points relative to how much they dislike the negative images.

And we just called that emotion conflict.

That's just another way of

presenting the generative model.

I won't go through that.

But here, again, you can show, for example, that people with depression and people with substance use showed less emotion conflict than healthy people, and they showed greater decision uncertainty than healthy people.

They showed they had lower expected free energy precision, which corresponds to higher beta values.

So that's just another example.


SPEAKER_06:
I think.

One thing that, from the presenter view, you can't see questions, but now that I'm not in presenter view, I can see that Daniel had his hand up.


SPEAKER_03:
Oh, sorry.

Oh, okay.

Just, yes, this is great stuff.

I just want to highlight a few pieces here.

So with that same task, you could imagine a purely descriptive model and a paper being written on something like, people who have X diagnosis are more likely to approach when there's conflict or something.

So that would be a purely descriptive...

finding based upon the same exact data now what this tutorial is about is using the exact same behavioral data within and across participants and then modeling underlying parameters that have a very specific graphical layout and relatedness so here we're talking about a t-test basically being used on a summary statistic

so they're not just different in the descriptive outcomes like this group was more likely to gamble on red this is like saying this group had a higher hidden state variable that we're going to attach to what we're going to call like gambling propensity now that relates to a question from the chat which is um and a really important question how are the neurobiological interpretation of the agent's generative model parameter estimates

done with respect to the parameter model.

So how do we go from a summary statistic or a graphical model estimate to a neurobiological interpretation, let alone intervention, um, or anything like that, but just mainly the interpretation side.


SPEAKER_02:
Um, so, so we have a whole, a whole section in the tutorial on the neural process theory that's associated with active inference.

Um, that's something that we're going to, uh, that was, we plan to talk about in, uh, another session, um, probably in part two.

Um, so I'm probably gonna hold off on describing that in detail right now, but the, the kind of quick version, um, is that, uh, the, the process, the neural process theory just makes certain assumptions about how, uh, neurons can be connected up to do this kind of thing.

Um, you know, so for example,

Um, there's a, you can have different rates at which your beliefs change rates with which the, with your distribution over States, um, becomes more or less precise.

Um, and you know, the, the neural process theory says that the, for instance, the, the sorts of ERPs that you would measure, right.

The changes and the changes in, uh, the magnitude of change in the, uh, neural activation that you would see would correspond to the rate of change in, in those beliefs.

Essentially, you can think about it as how quickly prediction error is being minimized as well.

Whereas the updates to this beta thing, trial by trial, are modeled as the phasic dopamine spikes that are essentially updating how much the expected free energy controls action policy selection.

So there is just a neural process theory that

proposes one way in which the variational message passing that underlies these models, how that could be implemented in early and therefore what sorts of signals you ought to measure in the brain if that process theory was correct.

But it's really important to recognize that there are several different possible process theories that you could come up with, right?

So there's many different ways you could connect a bunch of neurons up together to implement these sorts of models.

So there's like a lot of different levels of study here, right?

There's what generative model best describes task behavior.

And then there's given that model, what of a bunch of different message passing algorithms is the one that is being used?

And then there's the question about given that it's this message passing algorithm, you know, there's a bunch of different possible ways the brain could implement that.

So which one is the right one?

So there's many different levels of empirical questions that you can ask at each of these different levels of description.


SPEAKER_03:
yep what i'll say to that is that the relationship between the free energy principle and active inference again as a process theory it means something uh within the philosophy of science literature so check out the alias 2018 interview with carl friston myself and marching forger or the recent paper of mel andrews because basically the process theory makes specific falsifiable testable hypotheses

So that's the kind of thing you could actually then say, dopamine, use this kind of tool and you should expect to see this.

And that's why this is a little bit more towards the data focused empirical end.

So continue, right?

Yeah.


SPEAKER_02:
But like I said, I mean, we'll go into it, you know, task models will make specific predictions about what kind of ERPs, you know, you would measure in EEG, you know, during certain tasks or same thing with like fMRI responses.

And so, but anyway, we'll get into that in another session.

So this is another example.

Again, I'll walk you through these really briefly, but that's another generative model we built for this heartbeat perception task.

And what we found here was that in a certain condition, a certain high arousal condition, healthy people showed higher precision, higher interoceptive precision, whereas a bunch of different clinical groups didn't show any change in precision.

Again, it's just showing differences here in the way the brain treats signals coming up from the body when in a high arousal state, or really a state where they're having to hold their breath for a long period of time.

You can also estimate prior expectations.

Those didn't differ by group.

Then finally, this is an example of using active inference for neuroimaging that Philip Schortenbeck did.

he was looking at the beta plus gamma updates, the expected free energy precision updates.

And he showed that trial by trial, those updates did correlate with these midbrain region, with this midbrain region that is where a chunk of that region is where a bunch of dopamine neurons are.

So sort of consistent with this kind of idea that phasic dopamine responses are the ones encoding these changes in expected free energy precision.

Anyway, so those are just a bunch of examples of the way that this kind of thing has been used.

And I should point out that this is all just within the last couple of years.

This is very recent.

So the task that I'm going to walk you through how to actually build it, and we'll see how much time we actually have to go through this, is a pretty simple task where the participant just starts in the start state and

Initially, they don't know anything about, they have to choose one of two of these slot machines, the one on the left or the one on the right.

And they don't know, I have no idea to start with on trial one, which one is more likely to give a reward.

And if you're in the left better context, then that means that choosing the left slot machine will win 80% of the time.

And if you're in the right better context,

then choosing the right slot machine will give you the reward, um, 80% of the time.

Um, and so crucially, if on the first time step, they just choose a slot machine, then, uh, they could, they'll win $4 if they're right.

Um, but the other things that'd be kind of like the reward seeking thing, you know, they want as much money as possible.

Um, but what they can also do is they can first ask for this hint.

If they choose to ask for the hint,

it will tell them what context they're in.

It will tell them whether it's left better or right better.

And then they can choose one of the two slot machines.

And again, they'll win 80% of the time if they choose the right one based on the hint.

But crucially, if they take the hint first, then they'll only win $2 if they get it right instead of four.

So in other words, taking the hint is costly.

So you can think of,

choosing one of the slot machines right away as being this kind of reward seeking thing, where it's also risk seeking because you don't know ahead of time which one's the right one.

Or you can do this information seeking thing where you choose the hint first, even though that'll get you less money, but you'll be more confident in which one's right.

So it's set up specifically to have this information gain component by choosing the hint and the reward seeking component by choosing one of the two slot machines.

The question is, how would you actually in practice build a model of this task?

There's going to be some basic steps to do this.

One is to define whatever your initial state priors are, and that's going to be that D thing.

We'll go into this more with the learning.

Big D, if you use both big D and little d, big D is kind of like the true generative process.

You know, the real thing out in the world that are generating the observations.

And little d, if you use it, is what's in the generative model.

So the agent's beliefs, right?

And those two can be... So big D and little d can be different.

If big D and little d are the same, that means that the agent's beliefs are accurate.

And so...

If you want the agent to learn or to have different prior expectations than what the true ones are, then you have to use this little d thing.

Same thing here.

We have to define the likelihood or the state outcome mapping, and that's going to be our big A and our little a if we want it to learn and have different beliefs than the true ones.

We have to define the preferences over outcomes, right?

So that's going to be the C thing.

then we have to define the possible transitions or actions.

And so if there's just one transition matrix for a factor, that just means the agent has no control over what it does.

But if there's a state factor that can have multiple possible transitions where each transition is like an action, then the agent can choose policies that correspond to different transition sequences.

So for instance,

Well, I'll just show you.

But then for V here, the last thing you have to define is policies, which is this thing V in the code.

And that's going to be specifying different sequences of B matrices, different sequences of possible transitions that could happen over the course of the trial, and that the agent could choose one of those transition sequences.

To just jump in?


SPEAKER_05:
really quickly for each of those.

Then if, if there's, uh, you know, if I'm talking about a different outcome modality, I would have, I would have to have one of those parameters for each outcome modality.

Is that correct?


SPEAKER_02:
Um, you would need to have, so there's going to be one, uh, uh, a matrix, um, for each outcome modality.

Um, and that's just going to say what, uh, what outcomes are going to most likely to be generated, um,

given each combination of states.

So given the value of each state factor.


SPEAKER_05:
And these are things that come up in experimental design, right?

So like the probability that a given machine would dispense a winning ticket.

That's something I should be thinking about as the empirical person designing my experiment, right?


SPEAKER_02:
Yeah, exactly.

So I'll show you here, but right, so

Typically, you can do it other ways, but typically, this likelihood thing is what would define the reward probabilities.

So you'd say, given that I'm in the state of having chosen the left slot machine, that will generate the reward observation with 0.8 and the not reward observation as 0.2, for example.

So I'm just saying the reward probability is 80%.

And the agent would, if you want them to learn those reward probabilities, then you'd specify one of these little a things, and then you'd have it learn it over repeated observations.


SPEAKER_03:
It's such a critical and really fascinating point that reward isn't absent from the model.

Through the preference vector c, it's baked into how policy is calculated.

And so it's like the agent is pursuing precision, all things being equal through natural selection, unsuccessful models are just not gonna exist.

And models that don't see themselves performing behavior are not gonna be active for long.

So under a model of a successful preference, then there's just a convergence towards that with whatever affordances are at hand.

So it's just a really interesting way to see how reinforcement

learning situations, um, can be adapted.


SPEAKER_02:
So the 18 minutes, some of the, some of the gets a little bit more into the kind of like free energy philosophy stuff.

But, but I mean, yes, like the assumption is that, uh, people inherit preferences that keep them alive.

Um, but, uh, but in, in kind of like an empirical task context, you just use the C vector to just define what counts as the reward.

Um,

You know, so it could be anything, right?

It could be winning money.

It could be winning points, even if it doesn't give you money, right?

It could be like seeing a positive image, you know, anything like that.

You just define what the preference distribution is.

And then usually you parameterize it.

So you fit that as a parameter to see essentially how much does the person like, you know, winning $2, for example.

And I'll show you that in a second.

But yeah, so...

You're going to need one A matrix and one C matrix for each output modality.

You're going to need one D vector and at least one B matrix for each state factor to answer the original question.

The first thing here is the way that I decided to build the model for this task is the first,

hidden state factor is the context, right?

So am I in the state where the left one is better or am I in the state where the right one is better?

And I started that out specifying this little D here thing.

And the brackets just say the number of the state factor.

So this is D for state factor, priors over states for state factor one.

And there's two different possible states, left better or right better.

And I just specified these as two really small numbers, which just says,

it's an even probability that it's going to be the left one or the right one is better.

But these are really small.

And so I'm not confident at all about whether that distribution is right.

And I'll explain more about that when we get to the learning.

Because this will be soft maxed during inference.

So this will become 0.5, 0.5 during inference.

But when you learn, you sort of

build up the numbers here so they become bigger numbers, which means the agent learns to be more confident in what the distribution is that it believes.

But then I can also specify big D, right, which is the generative process.

And I just put this as one and zero, which means that the true context is that the left one's better.

So that's what I would use to set up the priors for state factor one.

State factor two here,

If you can see at the bottom.

So, hold on.

Can you guys see this?

Or can you guys see the bottom?

Because in mine, I can see my bottom.


SPEAKER_03:
We only see one line of code.


SPEAKER_02:
Yeah.


SPEAKER_03:
The D2 line.


SPEAKER_02:
Yeah.

I'm just asking, do you see this search down here and all that stuff?


SPEAKER_03:
It's perfect.


SPEAKER_02:
It's perfect.

Okay.

There, now it's gone.

Okay.

So, then D2 is the second.

hidden state factor.

And that's the one that corresponds to your choices, right?

So this is a one here, which means it always starts out in the start state.

The first column here is the start state, and then it can either transition into the hint state, right?

Which is the second one here.

I just define that.

Um, and then the third one is the choose the left slot machine state.

And the fourth one is choose the right slot machine state.


SPEAKER_01:
Um,


SPEAKER_02:
So this just says at time one, my prior is with 100% certainty that I'm going to start in the start tape of the task.

So that's actually pretty simple, right?

So then the next thing you're going to do is you're going to have to specify the likelihood, so the A matrix.

And I'm not going to show you the complete one yet until we go into the code.

I'm just going to show you the parts that matter.

But so for the second state factor,

So for the second outcome modality, so the likelihood mapping for A2 is going to be this thing where the first row corresponds to null, which is just it hasn't won or lost yet.

The second row is the observation that it lost, and the third row is the observation that it won.

And each of the columns here correspond to the state factor values.

So the left column here corresponds to

the D one, you know, this thing, you know, uh, so the 0.25 here for left better for the left better state.

And the right one here is the one that corresponds to the light better state, um, which maps onto column two here.

So the way that you would read this matrix is that if you were in the right state, right, better state, then you would, uh, win with this probability.

and you would lose with this probability.

Or if you were in the left one, then you would lose with that probability and win with that probability.


SPEAKER_05:
Are columns of that soft maxed?

Are those distributions?


SPEAKER_02:
Yeah, the columns are soft maxed, yeah.

Again, if you're using, I mean, to get a little technical, if you're using little a, so these are Dirichlet distributions, then it will build up counts, right?

So these numbers could become like, you know,

like 50, 25, eight, you know, whatever, but those are that encode the confidence in those distributions, but yeah, they'll get softmax for inference.

Um, um, so then, so then you'd have to also specify the C matrix for that state factor, or I mean that outcome modality, sorry.

So C2 is going to correspond to A2.

Basically it's for the sec, they're both for the second outcome modality.

Um, and here again,

Rows are going to be observations, but here the, uh, the rows correspond to time points in the trial.

Um, so basically this is just saying that, um, at time zero or at time one, I have no preference for anything.

Um, at time two, I have a negative preference for losing and I have a positive preference for winning.

And this RS thing is just a parameter, right?

So we can say this LA thing.

So loss aversion is one.

So it doesn't want to lose with negative one, right?

And for RS, we say RS equals four, which is, you can think of as the $4, right?

And then so if it observes the win at time two, then it's going to prefer that with a value of four.

And if it gets it at time three, then it's going to be RS divided by two, which means at time three, if it gets it, it's only going to win $2, right?

Or whatever the relative values are for the person, right?

You could fit RS

for a given person to see how much they prefer $4 over two, for example.

And these also in the code are soft maxed and then logged.

So they become log probabilities.

So that's just saying how much you dislike losing and how much you want to win.

And again, you can set those as parameters.

And then for...

state out for outcome modality one which is the getting the hint right you can have no hint you can have the hint that the left machine is better and the hint that the right machine is better right so those are both observations and then you can say uh with this pha thing you can say basically how how informative is the hint right like if i observe the machine left hint does that tell me with certainty that the left one's better or does it just tell me with some probability that the left one's better

Um, and here you could just set this as a being, you know, like you could just set, uh, this is like ones and zeros, um, to just, to just tell you, uh, basically that would just say, uh, if PHA was one, then that would just say that the, uh, the thing is a hundred percent, uh, accurate, um, in telling you which one's better if you observe one hit versus another.

Um, and, um, and so.

then the next thing is you have to consider the possible actions or sequences of actions that are gonna correspond to policies.

And so here, the way to think about it is, so you're starting out in state one for state factor two, the action state factor, and you can either go from the start state immediately to choosing the right machine, or you can go immediately from the start to choosing the left machine, or you can choose to take the hint

and then go to the right one or take the hint and then go to the left one.

So those sequences of actions are the policies that are going to matter.

They're going to encode what the agent can do.

And so to do that, you have to set up different transition matrices, different B matrices that encode each of these actions.

Transitioning from one to four, transitioning from one to three, transitioning from one to two,

transitioning from two to four and two to three.

And so that's going to correspond to setting up these B matrices, which here, so B1, right, which is the state factor for which slot machine is better.

This is just going to be an identity matrix, which basically just says that the left better context is constant across the trial.

you know, it's not as if from time point one to time point two, the context is going to change or something like that, right?

So it just says the belief is, and it is true that each trial has a stable identity as being the left better one or the right better one.

But for state factor two, so B2 here, that's where you want the agent to have different possible actions, right?

So B2 colon colon one,

So the third dimension being the different possible action.

The first one would be like this, which basically just says columns are states at time t and rows are states at time t plus 1.

So this just says I can start in any state, so in any column, and choose to move to state 1.

This one says I could start in any column and move to state two, so taking the hint.

This one says I can start in any column, so any state, and move to state three, which is using the left machine.

Then same thing here for moving to the right machine.

So there are four actions, one, two, three, and four, that correspond to four different third dimensions in the B matrix for state factor two.

And so then, given those,

Given those, we specify V, which are the policies.

And here, the third dimension corresponds to which state factor?

So for this one, for state factor one, there are no actions, right?

It's just this one V matrix and its identity matrix.

And here the rows are the action and the columns are, or sorry, the rows are the time point and the columns are the action, or the policy, sorry.

And so

the row here just means action moving from time one to time two.

And this second row means the action from time two to time three.

And so that is useless, right?

For state factor one, because there's just only one action.

Whereas for state factor two, we have all the different possible action sequences, right?

So policy one would just be if the thing just decided to stay in the start state the whole time, right?

Like to just not do the trial.

Um, the other one would be action two, which is take the hint and then, and then take action three, which is moved to the left slot machine or take the hint it to the third column, take the hint and then move to four, which is the action of choosing the right slot machine.

Um, and then here, this is just kind of a little bit of a trick, but you can choose to, uh, at time from one time to one to time two, you can immediately choose the left slot machine and then move back to the start state.

So three and then one.

same thing choose the right slot machine and then again move back to state one and the only reason for that move back to state one thing is is that um if you let the thing stay in state three and state four then it's as if it won four dollars and then won two dollars after that so you have to kind of move it back out of the state where it would win um so so then you know then you've built everything really um so

and you throw all of it in this little MDP structure.

So I didn't mention this, but you have to specify T, which is the number of time points in the trial.

So in this case, it would be three.

So start, take the hint, choose left.

Start, take the hint, choose right, et cetera.

So there's three time points.

V is just V, which is what we defined.

We'll ignore U for now, but that's what you would use if you wanted to specify one-step policies instead, but the thing doesn't look ahead at all.

Um, you know, and then ABC and D are just the different matrices we specified and we want it to learn D. Um, so we specify the little D. Um, and then these other things are just different parameters that you can set.

Um, so ETA is the, as a learning rate, um, alpha is kind of like an action precision.

So it's basically an inverse temperature parameter.

It controls how random someone's choices are given the policy that they choose.

Um, beta is the expected free energy precision.

that we talked about.

And these other two parameters, I won't really go into detail, but they just have to do with specifying the time constant, so basically how quickly evidence accumulates after an observation.

And this ERP thing is basically it controls certain assumptions about what ought to happen if you make a new observation each time with respect to what sorts of neural responses you would get.

And this is all explained in the code.


SPEAKER_03:
I think that's a, it's a perfect pause point so that we can close within the hour and leave people with excitement for part two.

So what can they look forward to in part two and beyond in a minute?


SPEAKER_02:
Yeah.

So, I mean, I would just say that, you know, once, once you have this set up, then the next steps are to run this structure through this VBX underscore tutorial script, and that will actually run the model and, you know, simulate the behavior, the neural responses,

And then we'll show you how to use plotting scripts to display and show the behavior that was the outcome of the simulation.

And then we'll show you how the MDP structure works and then actually show you some simulation results and how they work and things like that.

And actually probably walk you through some of the actual code.


SPEAKER_03:
That was an amazing session.

Ryan, Christopher, Max, thanks so much for coming on.

Everybody who was watching live and in replay, also very appreciated.

So please leave a comment if you have a question or feedback for the authors or for anyone else.

And stay in touch with us because we're going to be making this a multi-part series where we're going to be going deeper into the technical aspects and also highlighting use cases, hearing about people who are just learning programming, learning to apply it, people who are experts in other fields.

So whatever your perspective is, you're in the right spot to be learning.

So thanks again, all of you for coming on and we will see you another time.
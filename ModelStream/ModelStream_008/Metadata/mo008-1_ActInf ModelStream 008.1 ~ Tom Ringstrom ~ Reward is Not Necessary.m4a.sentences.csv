start	end	sentNum	speaker	confidence	text
5290	6550	2	B	0.64799	Hello and welcome.
6700	9542	3	B	0.92964	It's January 26, 2023.
9676	14120	4	B	0.82684	We're here in active inference model stream number 8.1.
14490	28186	5	B	0.48	Today we're appreciative to have Thomas Ringstrom, who will be presenting on Reward is Not Necessary, a compositional theory of Self Preserving Agents with Empowerment gain maximization.
28378	31578	6	B	0.99995	There will be a presentation followed by a discussion.
31674	34074	7	B	0.97442	So Thomas, thank you for joining.
34122	35550	8	B	0.99997	Really looking forward to this.
35700	36800	9	B	0.99959	Off to you.
37810	39186	10	A	0.98609	Yeah, thank you very much.
39288	42660	11	A	0.99986	It's really nice to be here and talk to this group.
44310	49570	12	A	0.83893	I'm a computer science PhD student at the University of Minnesota.
50230	84270	13	A	0.99983	My interests are in what are the computational properties that we would need to have agents which flexibly plan in sort of high dimensional product spaces of variables, and also how do we get agents to perform complex tasks in an intrinsically motivated way, especially in high dimensional product spaces.
85650	103334	14	A	0.99921	So this presentation is going to argue that reward I know I'm talking to a sort of active inference crowd, but some of the same points apply to active inference perhaps too.
103372	110998	15	A	0.99879	But this presentation is mostly about how there's going to be some major problems.
111084	124282	16	A	1	I think using reward maximization objectives, and by moving to sort of reward free objective functions, we can get really nice factorizations that help us plan.
124416	131002	17	A	0.99559	So let's just start off with a sort of simple, picture simplified picture of an organism.
131066	132800	18	A	0.99913	So we have a honey badger here.
135650	140020	19	A	1	This honey badger has internal states.
140390	142830	20	A	0.99998	It gets hungry and it gets thirsty.
142990	148034	21	A	1	And there's also an external world that the agent lives in.
148232	158230	22	A	1	And in order to sort of modify internal state spaces, the agent might have to perform some complex tasks.
158890	167210	23	A	0.99372	It might have to get several items in order to eat an apple or things like that.
167280	180654	24	A	0.99989	So you could see that maybe some symbolic state space sort of mediates the connection between things you do in the world and transformations that you make on some internal state space.
180852	188674	25	A	1	And of course, real world organisms and humans live in a high dimensional world.
188712	209186	26	A	0.99665	So you could imagine that there's an encoder and a decoder and really there's a sort of high dimensional physiological or interoceptive domain that is these Y tilde with the dot or Y tilde, I should say, and Z tilde.
209298	211510	27	A	1	And then there's a high dimensional world.
211580	214870	28	A	0.99972	You could just say that's x tilde.
215030	232030	29	A	1	And really you just need to sort of map these down to some discrete state space in which these domains can interact here's like a simple encoder and decoder.
234210	251640	30	A	1	If an agent has a kind of simple ontology like this, you can imagine that it's sort of generatively entrained to the world that it lives in, which is a notion that's probably pretty familiar to sort of active inference people.
253130	262902	31	A	0.99748	So you have some encoder of these high dimensional states and then you have some operator PS, which is in the middle of the head here.
262956	265386	32	A	1	And that just sort of advances the latent states.
265488	273900	33	A	1	And then you decode and you'll get sort of expectations of the high dimensional world that the agent lives in.
277490	289886	34	A	1	The problem is that you can't really represent explicitly these latent space transition operators that would dictate the dynamics of all these variables.
289918	302046	35	A	0.99999	You can't represent it as an explicit object because the more state spaces that you keep track of in the world, the larger this object PS becomes.
302238	308438	36	A	0.98383	So in order to handle this, you'd have to represent it in a sort of factorized form.
308524	311522	37	A	0.99997	So you just would represent its factors.
311666	319290	38	A	0.99997	You wouldn't explicitly sort of enumerate all of the state vector transitions under this transition operator.
319870	350094	39	A	0.9	And so what we really need to think about is what are the sort of model based or the sort of bellman principles for decomposing hierarchical state spaces so that we can create the right representations that help us plan in a flexible way in a sort of time varying, time dependent world of lots of variables.
350222	369420	40	A	0.99782	So this just means what is a sort of objective function l calligraphic L applied to PS or its factorization that would give us some factors here ada and these omegas that could help us plan.
370030	386990	41	A	0.99	And also, given the fact that the state space, the effective product space that we're working in is so large and there's many possible state vectors that describe possible states of the world, how do you know what to do?
387140	401202	42	A	0.88	I mean, if you're an RL theorist, you would have to say, well, there exists some kind of reward function that tells you this state of the world is worth this much reward and this state of the world is worth this much reward.
401266	408226	43	A	0.99995	But it's not really clear how you should define reward functions on huge product spaces.
408338	414390	44	A	0.9999	So I will address this question from sort of reward free perspective.
414730	428474	45	A	1	And just to give you a hint of what it's going to be like, what I'm going to argue is that what we really need are agents that have a kind of structured core ontology that it needs to maintain.
428522	440740	46	A	1	That is coupled state spaces that really depend on each other, in which the policies that you use maintain the sort of internal integrity or controllability of the agent.
444150	446802	47	A	1	And that's where empowerment will eventually come in.
446856	449378	48	A	0.99984	So I'll talk about empowerment a little too.
449464	451510	49	A	0.99998	That's the controllability metric.
451930	469420	50	A	0.99901	So here I'm just saying what's a nice policy or what's an objective function f that takes in some intrinsic motivation function, which is this fancy V here, along with a nice factorization that allows you to plan in the world.
469870	470234	51	A	0.99913	Okay?
470272	478746	52	A	0.88649	So just to recap things that I already said, we're in a product space of state of variables.
478858	485194	53	A	0.99968	We have internal and external state spaces, hunger and hydration state spaces can be dynamic.
485242	488030	54	A	0.99985	As you do things in the world, you get hungrier.
489090	490510	55	A	0.99996	So how do we plan?
490660	492434	56	A	0.99996	How do we know what a good goal is?
492472	506518	57	A	0.62	And what I'm going to argue is that we should compute a specific representation called a state time feasibility function, which is going to be an abstraction that's going to map us from initial state times to final state times.
506604	514794	58	A	1	And this will have some really nice properties that allow us to sort of reason in this high dimensional state space.
514992	520554	59	A	1	And I'm going to talk about this from just a dynamic programming point of view.
520672	522220	60	A	1	There isn't going to be any learning.
523550	524106	61	A	0.99889	Okay?
524208	534538	62	A	0.99988	So let's just talk about transition operator composition, because I said, well, we have this large product space of state variables and it'd be nice if we just represented it as a factorization.
534714	536160	63	A	0.99992	So what does that mean?
537810	547118	64	A	0.99959	So imagine that you have PX, which is like a base state space to move around the world, and you have some internal state space or secondary state space PY.
547294	549762	65	A	1	And these are linked by some function f.
549816	551800	66	A	0.99768	So here's your base state space.
552170	555560	67	A	1	And here's the honey badger in the state space.
556010	563720	68	A	1	And here's the secondary state space, which is just like your internal hunger space.
565470	569338	69	A	1	And so F, what F is going to be is called an availability function.
569504	578506	70	A	0.82	And an availability function is going to say, oh, this honey badger is in state XAT at time T.
578608	589010	71	A	1	And given that it's there, what's the probability that this goal is available from this state time, state action time.
589160	595330	72	A	0.54	And so this goal is formally going to be an action on the higher level state space.
595480	608198	73	A	0.9998	So for instance, these green lines on the high level state space are going to map you to the most satiated state where you're not hungry anymore at all.
608284	619306	74	A	1	And then these black lines on the internal state space are just going to decrement you one state if you're not at the tree, so you eat apples at the tree and that's going to map you to the top.
619408	623820	75	A	1	And all other states without a tree, you're going to get hungrier over time.
624670	647314	76	A	0.98856	Okay, so if we wanted to write a product space operator PS y prime, x prime, we can just represent this as a composition where lambda P is our composition operator, and it's just going to be defined as the product of your two state spaces with F linking them.
647432	648642	77	A	1	And then we sum over G.
648696	655174	78	A	0.99986	So we're getting rid of the goal variable and so we can represent the product space this way.
655212	656950	79	A	1	And we can also just call the product space.
657020	667340	80	A	0.99998	We can say that bold S here is just the state vector for yx, or if we have more variables that can be incorporated into bold S.
667790	674442	81	A	0.99397	Okay, so what if we have more features of the world?
674496	678400	82	A	0.99999	We can drink water, we can get warm at the house.
679810	685018	83	A	0.99915	Well, we will need to have a bigger composition.
685194	701720	84	A	0.9978	So we can just create an operator called we'll just call PR, where bold R is just state vector of w y and Z, and w y and Z correspond to the hydration, the hunger, and the temperature space.
702250	708050	85	A	0.98	And so this is just a product of the individual ones, individual operators.
708210	715754	86	A	0.99	And so this influence graph is connecting PX to Pwpy and PZ through F.
715872	721500	87	A	1	And so we can just define the product space operator as a composition this way.
722110	736800	88	A	1	And if we do that, you start to realize, well, this is nice because the effective state space exponentiates essentially as you add on more state spaces that you can control.
737410	750618	89	A	1	And so this influence graph is just showing what is the sort of ontology of the agent, what constitutes it as its internal and external coupling.
750814	763734	90	A	1	And you could imagine that it gets more complicated, for instance, that if you hit W zero or Y zero or whatever, these skull and crossbones indicates that you die.
763782	768230	91	A	0.98597	So you can imagine being in a state has a bi directional influence.
768310	775366	92	A	0.6936	So Zeta here is conditioning the possible dynamics PX can produce.
775478	782030	93	A	0.99561	So you could imagine that once you're in one of those sort of defective bad states that it kills you and you can't move around.
782180	794046	94	A	0.99987	So you could imagine sort of more complicated structures like this, which essentially mean that you have to go out in the world and do things in order to keep the system alive.
794238	799140	95	A	0.99974	So you don't want to hit W zero or Z zero.
800390	802642	96	A	1	And you can imagine this gets even more complicated.
802706	819820	97	A	0.99984	You could compose sort of larger structures where P sigma is going to be some logical state space that keeps track of multiple conditional events that need to occur for say, eat an apple or something like that.
821550	825194	98	A	0.99788	So representing it in this form is very nice.
825312	836560	99	A	1	We just represent the factors and the links between the state spaces and that's a sort of memory efficient way of representing the space.
837650	848174	100	A	0.6199	Okay, so if we have a homeostatic task, I'll eventually get to the sort of Bellman equations and the model based formulas.
848302	858310	101	A	0.99746	But first I'm just going to sort of build up an intuition in the form of an example about how these state time feasibility functions are going to work.
858460	864780	102	A	1	The state time feasibility function being the representation that I'm arguing for in this talk.
865710	887200	103	A	0.99914	So if we have a hiker and it can go out into the world and drink and eat and get warm with these goal variables, well, there's also this goal variable, G epsilon, which is going to decrement by one state.
888390	898274	104	A	1	Then as the hiker moves around, you could represent this as a function.
898392	910534	105	A	0.99983	So imagine that the hiker starts at xg one, that's the house, and TS, which is the start time, and the hiker follows a policy pi two.
910732	920220	106	A	0.99998	So imagine that you have a policy that is like a goal condition policy pi G two that's going to take you to xg two.
920750	921114	107	A	0.54031	Okay?
921152	939620	108	A	0.99998	So it's a policy, it's like a shortest path policy or whatever, then that means that there's going to be some final state time xg two TF that you achieve this goal G drink, which remember is in action on the higher level space.
942550	950254	109	A	0.99953	So what this would look like is that you decrement two, right, because that takes two time steps to get to the lake.
950302	956006	110	A	0.75	And then once you get to the lake, you take the state action that's going to drink and it's going to bring you up.
956028	970540	111	A	0.99992	So there's three total steps in this process of inducing the drink goal variable and then the step after drinking, which would look like this.
970990	981470	112	A	1	And then you can imagine the other state spaces are not involved and so they will all decrement three because there's three time steps.
982210	1004754	113	A	1	And so one way to represent this is to realize that if all of these sort of goal variables on the way to the lake are sort of the null goal variables, meaning that the agent isn't affecting some other state space, then you can define that as a Markov chain.
1004802	1021542	114	A	0.99628	So if you set all of the goals to be the epsilon goal, the null goal, then this PY epsilon is going to be a Markov chain matrix.
1021686	1043838	115	A	0.98	And so that means that the time difference encoded in the state time feasibility function, the TF minus TS is going to be the power that you can take this Markov chain to in order to forward evolve all the other internal state spaces.
1043934	1052178	116	A	0.99875	So you can define an operator that does this in one step called Omega Y, which is just going to take some initial state.
1052264	1059686	117	A	0.99988	So this green dot or this red dot, and it's going to take the time difference.
1059788	1062994	118	A	0.99993	So this is going to be two to get from the house to the lake.
1063042	1064374	119	A	0.99994	It's two time steps.
1064502	1073020	120	A	1	And so these gray arcs are just an initial to final state map under this policy.
1075390	1088480	121	A	1	And so what this looks like, this looks sort of complicated, but you can build a jump operator that jumps you from your initial high dimensional state.
1088870	1090530	122	A	0.99	For instance YXT.
1091270	1093074	123	A	1	And we're just considering Y for now.
1093112	1103990	124	A	1	I don't have the other state spaces W and Z in here, but this Ada over here is mapping you on the X space.
1104140	1120022	125	A	1	The Omega Y is mapping you forward in the Y space and then the PY and the PX are just evolving by one step to update after you hit the goal.
1120086	1122938	126	A	0.99635	So after you hit the goal, you have to update by one step.
1123024	1131802	127	A	0.9879	So PY and PX do that for both state spaces and then of course you can do this for all of the state spaces.
1131866	1144420	128	A	0.99958	So a jump operator for all of the internal state spaces, r, where R is remember, is the vector of all your internal states w, Y and Z.
1145190	1155762	129	A	1	Then this is similarly defined where omega R is just the product of all of these other Omegas.
1155826	1161110	130	A	0.9996	So we defined Omega Y up here, but you can just define this for all of your other state spaces.
1163690	1171878	131	A	0.98016	So now that we've done that, we can continue our journey by going to the tree.
1172054	1173770	132	A	1	And so that's two steps away.
1173840	1177998	133	A	0.99997	So all of the other things decrement two and then an additional one.
1178164	1188670	134	A	1	And then if we go back to the house, that's four time steps away plus the additional time step of entering the house and getting warm.
1189110	1209298	135	A	0.84584	Okay, so here is another route you could take where the hiker goes to the tree to eat and then the hiker goes to the lake to drink and then goes back to the house to get warm.
1209394	1227040	136	A	0.99	And so this JS is a huge it's important to keep in mind that JS is a huge operator and we cannot explicitly form it in memory because it's unless we have a lot of memory on our computer.
1227890	1237754	137	A	0.99997	But having this factorization allows us to chain policies together and evolve a high dimensional state vector in these jumps.
1237882	1254680	138	A	0.99984	So we're jumping a state vector around in this object oriented fashion where each state vector is updated once you get to some key object of interest, such as the tree with the apple or the lake with the water.
1256570	1265900	139	A	0.93014	So also in this example, you can see that the agent went from the lake to the house.
1266270	1273770	140	A	1	And technically, if z, it occupies the lowest state, z zero.
1273920	1281066	141	A	1	And so technically it should die at the skull and crossbones on the map, but we let it finish his journey.
1281098	1289890	142	A	1	And we haven't talked about this sort of bi directional coupling yet, but we'll do that in the next slide to make that concrete.
1291590	1307378	143	A	1	All right, so if we have these defective states, we can define mode parameters e plus and E minus, which are just going to be variables that condition our dynamics.
1307474	1320502	144	A	0.56	And we can have a mode function that takes any vector r and R is any vector of blue, green and red squares in the space, and we can map it to a mode parameter.
1320646	1333518	145	A	0.99948	So we can define zeta to map to the good mode, the normal healthy mode, if it's not occupying any of the defective states.
1333604	1341918	146	A	0.99999	But if one or more defective states are occupied, then we can map it to E minus.
1342094	1355554	147	A	1	And so that means if we have some low level transition operator that's indexed by E, that means we can split it into a normal dynamics PE plus and a defective dynamics PE minus.
1355602	1361122	148	A	0.78	And you just notice that the defective dynamics is a sort of identity operation.
1361186	1369210	149	A	0.99991	It just sort of arrests your dynamics in the space so you can't move around, but the normal dynamics is just normal grid world movement.
1369950	1382186	150	A	0.97604	Okay, what we can do is we can compute a set of individual feasibility functions for each object of interest in the map.
1382378	1388642	151	A	0.98939	So in the previous example, we had two trees, two lakes and a house.
1388696	1392930	152	A	0.99708	So we have five state time feasibility functions.
1393670	1406440	153	A	0.99	And then this Ada hat is an aggregate function of all of these functions where the pi is indexing the corresponding pi for that function.
1407690	1411880	154	A	0.82375	Anyone can ask questions, by the way, if they have any questions.
1414990	1439948	155	A	0.85	All right, so the full dynamics is going to be just defined like we defined it before, where we have these two state spaces, the base state space coupled to the internal state space through F.
1440034	1445330	156	A	0.99998	But then we have this zeta R here on the conditioning side.
1445780	1462310	157	A	1	And so this is an operator that you have to achieve goals in order to keep yourself out of these sort of absorbing defective states.
1465640	1473400	158	A	0.97	I showed this influence diagram before, but this influence diagram is capturing this coupling.
1475340	1495440	159	A	0.66111	Okay, so many of you will probably be familiar with the standard Bellman equations which are formalized this way, where you have some reward function plus some discounted expectation of your future value.
1495590	1506500	160	A	0.99	And so the value function in a sort of standard Bellman equation says, how much reward am I going to get if I act here optimally over an infinite horizon?
1507240	1511904	161	A	1	And so the reward function is often thought to be a task.
1512032	1516630	162	A	0.92544	It has that sort of semantics in RL land.
1519340	1533148	163	A	1	And then the optimal policy is just going to return the action that corresponds to the best action that maximizes your value, your long run sum of rewards that you're going to get.
1533314	1559860	164	A	0.99427	So this is a recursive equation, but my thesis is that we need to rethink the sort of model based Bellman formalizations in order to compute these nice factorizations that move us around high dimensional space like we've been talking about on the previous slides.
1560280	1566420	165	A	0.92701	So we're going to formalize some new Bellman equations called operator Bellman equations.
1567100	1575210	166	A	0.99	And these are going to be non stationary Bellman equations or they're going to be functions of time.
1575660	1582316	167	A	0.99019	So instead of having a value function that says what's the accumulated reward that I get?
1582418	1586380	168	A	0.96469	I'm going to have a cumulative feasibility function kappa.
1587120	1597970	169	A	0.99	And this is going to represent what is the cumulative probability that I achieve a particular goal, where a goal, remember, is an action on a higher level space.
1599540	1605672	170	A	0.95138	So this has a very similar form to the infinite horizon Bellman equation that I just showed.
1605836	1610832	171	A	0.99843	But you'll notice that we have this availability function here for a specific goal.
1610896	1614004	172	A	0.99857	So this is a single goal G that we're picking out.
1614042	1618724	173	A	0.9996	So say the eaten apple goal or the drink the water goal.
1618772	1620170	174	A	0.99997	We're just choosing one.
1620620	1631900	175	A	1	And so F is returning the availability of G from any given state and time and action.
1633200	1646236	176	A	0.79	And then, so either the agent achieves a goal now, which is what this equation is saying, or which is the plus the agent does not achieve the goal.
1646268	1658628	177	A	0.99944	So one minus the probability of achieving the goal is the probability that you don't achieve the goal times the expectation that you achieve it in the future.
1658794	1666470	178	A	0.9951	So it has this similar recursive structure that the infinite horizon Bellman equation has.
1667500	1686350	179	A	0.9988	Okay, so then we have the policy, and this policy equation is a little different from the last one because you can imagine that you can maximize the cumulative that you achieve a goal, but you sort of do it at the last second.
1686960	1691944	180	A	0.9746	You have maximized certainty that you can get to the store right before it closes.
1692072	1701180	181	A	0.99974	So you know 100% that you can achieve the goal of getting an item from the store five minutes before it closes.
1701340	1712820	182	A	0.94182	Well, it might be if the store is open for some period of time, that essentially you might want to get it as soon as possible.
1712970	1727880	183	A	0.99997	So what this equation is saying is that we're going to pick the action which maximizes the cumulative, but from that set of equivalent actions that maximize the cumulative, which is a star.
1728030	1735084	184	A	0.99992	So we're collecting the actions that maximize the cumulative and we're going to pick the one that minimizes the time.
1735202	1755652	185	A	0.99028	So this is the sort of conditional optimization that says subject to the fact that we want to maximize the cumulative, we want to get there as fast as we can normally there's just with the infinite Horizon Belmont equation, there's the value function and the policy.
1755786	1763750	186	A	0.99999	Here we have a third function, which is the state time feasibility function which I've been talking about.
1764360	1780940	187	A	1	And the state time feasibility function says given that I start at X and T, what's the probability that I achieve a goal g at a particular state final state in time XF and TF?
1781520	1789260	188	A	1	And so you can compute this via dynamic programming as you're computing these other functions.
1789840	1808100	189	A	1	And there's a relationship between the cumulative feasibility and the state time feasibility, which is that if you can sum up the individual final states and times from your state time feasibility and that is the cumulative.
1808840	1816790	190	A	0.99104	So the cumulative feasibility is just summing over the individual probabilities of a given state final state time.
1818140	1848272	191	A	0.96	And the nice property is that when you use these operator bellman equations on hierarchical state spaces, such as the product space of the agent's core ontology, it has a nice decomposition property, which is that if you're just solving to go to a particular point in the world, then you can actually compute Ada separately from the high level space.
1848326	1860928	192	A	0.99498	So you just solve for Ada the state time feasibility function on the low level space, but then omega can be computed independently in the high level space and they can be combined.
1861104	1880990	193	A	0.99812	So this decomposition property is really nice because we never want to work in a product space, especially as we learn tons of dynamics about how the world works, we need to be able to compute representations in a factorized way which is going to help us move around in reason.
1881760	1903090	194	A	0.99633	So this means that when we compute a bunch of individual state time feasibility functions for each goal I don't know why this little hat is here, but then if we have five of these and we have an aggregate feasibility function.
1904020	1918730	195	A	1	Then we can use this aggregate feasibility function to move around from feature to feature in the low level world and update all of our internal and higher level states.
1919420	1927610	196	A	0.66	And so this is the object from the previous slides that sort of map us around the world.
1928460	1937224	197	A	0.78602	So you can also do, and I won't talk about this much, but you can also do logical tasks where instead of eating an apple, you might want to gather an apple.
1937352	1943416	198	A	0.9999	So if you go obtain an apple, then there could be a bit that corresponds to having an apple.
1943448	1954912	199	A	1	And you flip that bit to one once you go to the tree and then you go gather water at the lake and you flip that bit and then you go to the key and you obtain that key.
1954966	1964212	200	A	1	And that might be a task where you have to obtain these three items, perhaps because you wanted to compose this task with something else.
1964266	1968468	201	A	0.99998	You want to bring these items to another agent or whatever.
1968634	1979450	202	A	0.99996	So the point is that you can use these factorizations to move around the world, especially when things in the world have a sort of limited time period in which they are available.
1985120	1992860	203	A	1	I talked about how we can plan in these high dimensional state spaces using this factorization.
1993860	2003552	204	A	1	And now we get to the question, well, why should I plan to any particular high dimensional state?
2003686	2007030	205	A	0.99994	That's the intrinsic motivation question.
2007480	2018810	206	A	0.99963	Given that there's an exponential number of state vectors in a product space, why is one state vector of the world in the far off distance better than another?
2020220	2022984	207	A	0.99994	So that's where empowerment comes in.
2023182	2031340	208	A	0.99977	Empowerment is an intrinsic motivation metric which sort of represents controllability.
2031680	2037420	209	A	0.99823	So formally, empowerment is a function of a transition operator.
2038160	2048720	210	A	0.99979	It's an intrinsic measure of a transition operator and you can also condition it on the state that an agent is at.
2048790	2055716	211	A	0.99282	So it takes two arguments and it also has a horizon n, which we'll talk about in a second.
2055898	2066388	212	A	0.99017	So it's formally defined as the Shannon channel capacity between your actuators or sequences of actions that you take and the resulting states.
2066554	2078330	213	A	0.95136	So the channel capacity, so here open loop sequence of actions would just be go up right and up again, or up left and then down.
2080800	2083432	214	A	0.99988	There are a lot of possible sequences of actions.
2083496	2088456	215	A	0.99986	So big a here is a random variable for your sequences of actions.
2088648	2101852	216	A	0.99	And so for a horizon n, you can ask how much information can we transmit from our actuators to the resulting state at time tau.
2101996	2110384	217	A	0.99599	So tau minus T is our horizon n, which is this parameter on the empowerment.
2110512	2122120	218	A	0.99987	So it's just saying what's the agent's capacity to affect the future with certainty or varying amounts of certainty?
2122860	2152530	219	A	1	And so the channel capacity is formally defined as the maximum mutual information given distribution over these action sequences and the mutual information decomposes into the entropy of the final states minus the conditional entropy of the final state random variable given that, you know, given the actions and your starting state.
2153300	2162020	220	A	0.90429	So this means there are two sort of extremes to empowerment.
2162680	2183276	221	A	0.96	If PX is a deterministic operator, so anytime I'm at state X and I apply an action, I get a deterministic output x prime, then the conditional entropy, there's going to be no uncertainty over my future state.
2183378	2185596	222	A	0.99999	So the entropy is going to be zero.
2185698	2187208	223	A	0.89264	There's no uncertainty.
2187384	2192440	224	A	1	And so that means that the conditional entropy has to cancel out.
2192610	2201340	225	A	1	And so empowerment is really just the maximum possible entropy given this distribution.
2201500	2206916	226	A	1	And this just reduces down to the log of the number of possible reachable states.
2207018	2210550	227	A	0.85645	So how much can I actually reach in the world?
2213080	2219876	228	A	1	And so if my horizon is two, we can see that the empowerment here is just log of 13.
2219988	2227690	229	A	0.99998	So there are 13 states I can get to, and I have perfect control, so I can actually realize any of those 13 states if I want.
2229680	2259040	230	A	0.9	And the other extreme is, if PX is action independent, meaning that any sort of state that I'm in, I have an action, that maybe there's the same distribution given that action, say a uniform distribution, then my conditional entropy given the actions is just going to be the entropy.
2259200	2265830	231	A	1	And so the empowerment has to be zero because this term is going to cancel out.
2267160	2273540	232	A	1	And so there might be a lot of states you can technically reach because I can select actions and I'm just going to go random places.
2273620	2281950	233	A	0.99991	So Pink is all the states I could possibly reach, but I don't have any control over which state I'm going to end up in.
2282800	2290584	234	A	1	And so I can't influence my future in any way, even though there are a lot of possible futures.
2290632	2292880	235	A	0.99998	So empowerment in that extreme is zero.
2292950	2303516	236	A	1	And then there's an in between zone where you take actions and there's a bias in one direction or there's different distributions for each action.
2303628	2308420	237	A	0.94	And so there might be uncertainty, but you can sort of control how much information.
2308570	2312070	238	A	0.99988	You can sort of control what state that you want to end up at.
2313960	2319480	239	A	0.67316	Okay, so what I'm going to define is a function valence.
2321660	2331468	240	A	1	And it's important to note that there's two arguments, the states, and you can include the time too, and the conditioning side.
2331554	2340556	241	A	0.99814	So you have empowerment and you can define an empowerment difference.
2340658	2354284	242	A	0.99837	So say you start at S and T, and then you end up at a future state in time after you execute some sequence of policies.
2354412	2363590	243	A	0.99762	So Row here appended to the S and T is a sequence of policies, and S row and T row are the resulting states.
2364040	2366870	244	A	1	Then you can compute an empowerment difference.
2367980	2370810	245	A	0.91	And this will be our valence function.
2371820	2378456	246	A	0.99975	So Q here is a function that I'll talk about in just a second.
2378638	2389144	247	A	0.99998	But we can see that our jump operator that's moving around high dimensional space is defined as this factorization.
2389272	2390812	248	A	0.99998	So we don't have to represent it.
2390866	2392610	249	A	0.9999	Remember, that's an important part.
2393060	2393856	250	A	1	And so we.
2393878	2397612	251	A	0.99999	Can sample multiple policies from this or chains of policies.
2397676	2406870	252	A	0.99981	So if I have policy one, policy two, then I'm going to have some resulting state r double prime, x double prime, TF two.
2408840	2413744	253	A	1	And so Q here can just represent the output of chains of policies.
2413792	2426360	254	A	0.95378	So you can imagine a tree search of chains of policies, and Q is just summarizing the final state of those branches in a tree search of policies of chains of policies.
2428220	2432570	255	A	1	All right, so we can use this Q.
2433180	2437532	256	A	0.9998	So notice that this Q here is in the expectation over here.
2437586	2442464	257	A	0.64426	So it's linking our original state time to the final state time.
2442502	2445520	258	A	0.70484	So it's the empowerment after some chain of policies.
2447220	2461174	259	A	0.99814	Okay, so say we have a deterministic operator, and this should be P, then valence is an empowerment difference.
2461292	2468070	260	A	0.99998	We can have a simple sort of example here where we have two hikers that are considering two different plans.
2468510	2484590	261	A	0.52	And so if this tree here is like the space of two possible chains of policies, then these hikers are just executing one sort of path through this tree.
2489490	2497902	262	A	0.99776	They both have the same empowerment because they're starting at the same state and they're both two away from starving.
2497966	2518950	263	A	0.99988	So if we just consider an empowerment at the beginning, that's log of 13, and if we chain together two policies here, we can advance our internal state space and the agent will end up down at the lake.
2519310	2533118	264	A	0.82	And so given that it's at the lake, you'll notice that it's one state away from dying, so that there's an effective range of where it can go.
2533204	2542560	265	A	0.99973	So the final empowerment is log five, because there's five reachable states and we're assuming determinism to make this easy.
2543670	2555566	266	A	0.99701	So the valence here, and this should be PS here, but I have T here, but the valence is just log five minus log 13.
2555678	2559218	267	A	1	And that's the difference between the final and the initial.
2559314	2561430	268	A	1	And so that has a negative valence.
2563130	2570140	269	A	1	The yellow shirt hiker is clearly in a worse position that he started off at.
2571150	2585520	270	A	0.99999	But we can advance the other hiker with pi g three to the lake and then pi g four to the tree and update his internal states.
2586930	2590778	271	A	1	And we can see that he's three away from dying.
2590874	2592990	272	A	1	So there's a bigger effective range.
2594210	2597440	273	A	1	And so there are 25 states that he can get to.
2600550	2612102	274	A	0.99999	Log of 25 minus log of 13 is zero point 94, which means that he's better off than where he started.
2612156	2617320	275	A	0.53	And so clearly, the second hiker is executing a better plan.
2618090	2624380	276	A	0.99998	He has more freedom to engage with other tasks in the world.
2624910	2633742	277	A	1	And so if we searched over this entire tree of policy chains, we could pick the best one.
2633876	2644580	278	A	0.87	And in this example, we're just going to consider two of these branches and we'd pick policy pi g three, pi g four.
2646870	2657720	279	A	0.98	All right, so there's also another interesting so in this past example, we're just sort of changing the structure we're just changing the initial state.
2658970	2680730	280	A	0.99811	But since empowerment is also a function of a transition operator, and our operator Bellman equations are producing transition operators that map us from state time to state time, then the output of those Bellman equations, the operator Bellman equations, produce transition operators so we can compute their empowerment.
2682210	2688910	281	A	0.91325	So that's a deep connection between Bellman equations and these intrinsic motivation metrics.
2689570	2703380	282	A	0.97976	So we can do interesting things where we can say, well, if the structure of the world were different, then the feasibility functions that I could compute in different configurations of the world would be different.
2703910	2717110	283	A	0.98594	So if the Honey Badger gets a key and it opens a door in the mountain pass, then it could potentially get through to the other side.
2717180	2724410	284	A	0.99967	But it's important to note that this is changing the structure of the low level state space.
2724480	2733520	285	A	0.99645	So PE knot is what we'll call the original transition operator, in which we can't go through the mountain pass.
2734130	2752606	286	A	0.98	And that means that if we compute state time feasibility functions on this operator, that means PE here is going into the Bellman equation, then we're going to produce state time feasibility functions from those operator Bellman equations.
2752718	2760360	287	A	0.98035	That's this object here, which means that we can use it to construct j to move us around high dimensional space.
2760810	2764680	288	A	1	And so there's an empowerment for this j.
2765370	2778010	289	A	1	But then if we get the key and the key allows us to move through the door, it changes the structure of the world, then that's a different mode of dynamics.
2778510	2790494	290	A	0.95789	So that means that all of this applies on the other side, where we can compute feasibility functions off of a different mode of dynamics in which we can move to the mountain pass.
2790692	2813960	291	A	1	And so that means we can compute things, we can compute valence by asking is this configuration of the world more conducive to the agent's core ontology that is, the agent's internal external coupling that needs to be maintained as a sort of core object.
2815450	2827258	292	A	1	And so we can compute valence just by changing j the structure of the agent's abstractions that it moves to perform tasks in the world.
2827424	2832334	293	A	0.99734	So by computing empowerment on j, we're sort of computing it in task space.
2832532	2837262	294	A	0.99999	We don't have to consider all possible states of the product space.
2837316	2851570	295	A	0.99999	We can narrow it down to operators that move us around task space, that induce dynamics on key other state spaces that we care about maintaining, such as our physiological state spaces.
2853430	2856222	296	A	0.94442	Okay, so here's a simple example.
2856376	2868370	297	A	0.97791	Say the Honey Badger starts at the lake and has some initial empowerment just on the low level operator.
2868450	2876026	298	A	0.99999	So that's P 50 there's twelve states it can reach.
2876128	2889210	299	A	1	But it also has a task empowerment which is actually zero because there's only one task that it can engage in if you don't include getting the key as a task.
2889370	2896660	300	A	0.9983	So the circle here is saying that there's only one sort of task that can be done.
2897030	2906980	301	A	0.68	And so if the agent goes and gets the key, it's going to reduce its physiological states because it had to travel there.
2907350	2911938	302	A	1	And then it gets the key and it conditions a different mode of dynamics.
2912034	2918630	303	A	0.99995	So there's different feasibility functions associated with that mode which I just described on the last slide.
2919470	2925638	304	A	1	Then the door will open and it can travel back to the lake.
2925814	2933034	305	A	1	And so now that it's at the lake, it can go eat food on the other side, right?
2933152	2944830	306	A	0.99983	So before getting the key, if it couldn't get the key, if there were no key, then it would just starve because while it could drink water and stay hydrated, it couldn't eat from the apple tree.
2944910	2952706	307	A	0.99994	But now it can cycle back and forth between the apple tree and the lake for as long as it wants.
2952888	2970070	308	A	1	And so it has a higher empowerment just on the low level state spaces, but it also has a higher task empowerment because there are just over a horizon of three and we're just computing, we have to choose a horizon.
2970230	2980940	309	A	0.99989	So the empowerment in task space is eight because there's eight possible branches in resolving states from where it is.
2981250	2985550	310	A	0.79	And so that's very useful.
2986450	2997700	311	A	0.99992	So we can compute the valence, which is 0.5 just in the low level space, but also three in the task space.
2998390	3005138	312	A	1	And another interesting question which is very important is to say, well, what's the value of the key?
3005304	3006822	313	A	1	And you can compute this too.
3006876	3033630	314	A	0.99778	You can say, well, given if I just fix the state vector that I'm at, and I just alter the state phi that encodes the object of a key, if I just alter that state and switch it between having a key and not having a key, you can say, well, this key has this much value to the internal organization, the internal integrity or controllability of the agent.
3033700	3042026	315	A	0.99997	So it's a sort of agent centric judgment of how much something in the world is valuable.
3042138	3050930	316	A	1	And so the key can be not valuable if it doesn't do anything in the world that helps the agent control its core ontology.
3052150	3058440	317	A	0.99989	So this is a way in which you can sort of bootstrap value into the world.
3059770	3069722	318	A	0.99998	You can use the change in an agent's internal structure, its coupling between the internal state spaces and the external world.
3069856	3073866	319	A	0.63063	You can use the changes in that structure to assign value to things.
3073968	3075370	320	A	1	And that's very useful.
3077070	3080218	321	A	0.99934	Okay, I think we're approaching an hour.
3080304	3090762	322	A	1	And so I'll just conclude by saying intelligent systems operate in high dimensional product spaces, often with non stationary dynamics.
3090906	3108846	323	A	0.99998	This introduces a lot of problems, especially in artificial intelligence because people normally deal with structured tasks and non stationary by training recurrent neural networks and things like this, which contribute a lot to sample complexity.
3108878	3124810	324	A	0.98	And what I'm saying is know operator Bellman equations have this different form which produces transition operators, which helps you factorize your representations for moving around the world and predicting the resulting high dimensional state vector.
3126670	3133178	325	A	1	And these operators are composable, they compose with themselves, but they also compose with higher level structure.
3133274	3136958	326	A	0.99983	So you can remap different transition structures to them.
3137044	3157490	327	A	0.99965	It's very modular and these are very nice properties that you need if you don't want to recompute things and you want to have sort of modular structure come in and remap to your representations that you've already so forward sampling.
3158470	3160130	328	A	0.99993	Can you still hear me, Daniel?
3162710	3164162	329	A	0.43381	OK, just checking.
3164306	3170082	330	A	0.99193	So forward sampling is a good way of solving problems in high dimensional state spaces without representing the product space.
3170156	3177066	331	A	0.54379	We can't really solve, we can't do dynamic programming in a huge product space that's not going to work.
3177168	3183790	332	A	0.68071	We can't sample low level actions, that's not going to work, the tree is too big.
3183860	3195140	333	A	0.99986	But we can work at the level of sequences of policies and we can evaluate empowerment gain to justify our goal states.
3196790	3202702	334	A	1	And so Valence sort of unifies a lot of distinct drives.
3202766	3223240	335	A	0.9845	Like there's a different sort of subfield of RL called multi objective reinforcement learning, which says we'll have a bunch of different reward functions and then we'll have value functions for each of these reward functions for different tasks and that'll make like a high dimensional value function vector space.
3224890	3244546	336	A	1	And usually in multi objective RL, you have to pick a policy that that does well in that value function space, but normally you have to deal with the trade offs by some weighting function.
3244728	3269420	337	A	0.99974	So what I'm saying here is that because valence is just one number and it summarizes an entire sort of control architecture, that you don't have to introduce things like weighting functions or weighting coefficients to say, oh, this objective is more important now, or this objective is more important now.
3273070	3279686	338	A	0.68545	So yeah, many latent drives is not necessarily multi objective, it's multidimensional, it's multi goal.
3279718	3287850	339	A	1	But it doesn't have to be multi objective with empowerment.
3287930	3290510	340	A	0.99999	You don't have weighted combinations of empowerment.
3291010	3295950	341	A	1	And valence depends on the structure of the environment.
3296030	3304194	342	A	0.95679	So it's not just some static property of the world or a static property of an agent.
3304392	3308550	343	A	0.99739	It incorporates agent world coupling.
3309610	3324202	344	A	0.99	And I thought I'd just end with this quote from Terrence Deakin, who wrote a great book called Incomplete Nature, which I love and I read at the beginning of Grad school, which inspired me a lot.
3324336	3332410	345	A	0.98	And Terrence Deakin wrote a lot about teleology from a sort of thermodynamic perspective and it's really compelling.
3332910	3347758	346	A	0.99	And I just liked what he had to say about teleodynamics, the idea that an organism could be, or its behavior could be organized around realizing something which is sort of virtual.
3347854	3364962	347	A	0.94	And he says Teleodynamics is the dynamical realization of final causality in which a given dynamical organization exists because of the consequences of its own continuous continuance and therefore it can be described as being self generating.
3365106	3381490	348	A	0.85298	Specifically, it is the emergence of a distinctive realm of orthograde dynamics that is organized around a self realizing potential, or to be somewhat enigmatic, it is a consequence organized dynamic that is its own consequence.
3381670	3398750	349	A	1	And I think that's relevant to what I'm doing here because I think empowerment sort of on an internal sort of structured ontology, allows an agent to say there are multiple, there's a huge space of possible futures.
3398830	3419590	350	A	0.633	But I can evaluate a state of the world that's far off in the distant future, and I can organize all of my behavior around that because I can say I can give an explanation for why it benefits my sort of core ontology.
3419750	3425290	351	A	1	And so therefore, it makes me capable of acting that way in the future.
3425360	3429130	352	A	0.99965	It's a consequence organized dynamic that is its own consequence.
3429790	3436798	353	A	0.98764	So with that, I just want to thank the Active Inference Institute, and I will take questions.
3436964	3454334	354	A	0.99	And I'm very interested in what active inference theorists think about the sort of potential for a sort of integrated view of empowerment, because I think you all have a lot of experience thinking about generative models and things of that nature.
3454382	3456806	355	A	0.99995	So I'd be very curious to know what you think.
3456908	3457720	356	A	0.99986	Thank you.
3459850	3460658	357	B	0.81823	Excellent.
3460754	3461720	358	B	0.99987	Thanks a lot.
3462570	3464658	359	B	0.99997	Great presentation.
3464834	3467282	360	B	0.99989	So this will be a fun discussion.
3467346	3476620	361	B	0.99999	Those who are watching live, please feel free to add questions in the live chat and you can unshare your screen and we'll begin.
3478430	3490190	362	B	0.97	I guess I'll take an empowering deep breath and ask a general question, and then I have some scattered notes that I'll love to dive into.
3490260	3493920	363	B	0.98911	So how did you come to this area of research?
3494230	3501010	364	B	0.99955	What brought you to control theory modeling and to the empowerment perspective specifically?
3503670	3507880	365	A	0.99978	Hold on, I'm just bringing up the YouTube stream so I can see comments right now.
3510490	3513766	366	B	0.76361	What brought just a general question.
3513868	3514678	367	B	0.97529	Mute that one.
3514764	3515542	368	A	0.99986	There we go.
3515676	3518454	369	A	0.84929	Yeah, I'll mute that sounds good.
3518492	3519180	370	B	0.99998	Thank you.
3520030	3520538	371	A	0.99994	Yeah.
3520624	3523398	372	A	0.99994	So what brought me to empowerment?
3523494	3549778	373	A	0.9933	I've always been interested in how could animals interact in a world in a way that's so sample efficient, especially like, knowing that animals can, like a baby horse can get up and move around and interact with the world in a sort of fluid, flexible way.
3549944	3556370	374	A	0.99997	What are the sort of core representational capacities that are needed to do that?
3556440	3563190	375	A	1	And I didn't really see anything from the RL world, and this is before I knew about active inference.
3564090	3566870	376	A	0.99	And so that's always been in the back of my mind.
3567020	3593670	377	A	0.92	And another big influence was a guy named Nishith Srivastava who wrote an interesting paper about how you can basically have a sort of relativistic decision theory that allowed you to make judgments between different items without recourse to sort of hedonic utility theory maximization.
3593850	3614370	378	A	0.55	And so he sort of argued that if there was something like a latent acceptability function, that you could sort of remember a history of item acceptability and you could remember the context that you made decisions and that you could actually just do Bayesian inference over those memories.
3614530	3626634	379	A	1	And you could explain a lot of interesting things like preference reversal phenomena in decision theory where you introduce irrelevant alternatives and it changes the fundamental choice you make.
3626672	3642602	380	A	1	And I thought that sort of initiated a lot of thinking into, how could you bring those kinds of intuitions into sort of embodied planning?
3642666	3658150	381	A	0.93857	Like, how could it be that you have an agent assign value to things without them being sort of attributed as sort of static preferences or static utilities in the world?
3658220	3661910	382	A	0.96891	So I think that was also a big inspiration.
3664250	3665046	383	B	0.99972	Awesome.
3665228	3665718	384	B	0.69836	Okay.
3665804	3667670	385	B	0.77	And then one short general question.
3667740	3669110	386	B	0.99955	Why the honey badger?
3670590	3672010	387	A	0.75	The honey badger?
3673310	3683790	388	A	0.99974	My advisor showed me a YouTube video of a honey badger named Staffel, and in my paper, reward is not necessary.
3684450	3690670	389	A	1	The opening paragraph talks about Stauffel, and there's a link to this YouTube video.
3690740	3703394	390	A	0.96623	But Stauffel is a honey badger in Australia, and he's at some sort of animal care center, and he's really good at escaping from things.
3703512	3715142	391	A	0.93297	So the caretaker at this animal sanctuary constantly has to build elaborate structures to keep staffel in.
3715196	3718498	392	A	0.99284	So he has this sort of pen called badger alcatraz.
3718674	3726106	393	A	0.99	And staffel would do interesting things, know, find objects to lean against the wall and climb over.
3726288	3737342	394	A	1	And if you took those away, staffel would pack mud into balls and stack them into, like, a little pyramid against the wall to climb up and things like that.
3737476	3754946	395	A	0.98931	So, yeah, it got me thinking, what is a sort of good general intrinsic motivation function that doesn't just work on low level states, but also in a sort of more conceptual hierarchical space?
3755048	3761634	396	A	0.99166	Like there might be objects or mating opportunities or anything sort of outside badger alcatraz?
3761682	3773050	397	A	0.8824	What is the sort of way in which you could have an agent sort of think in a sort of abstract way in order to justify its motivations?
3774750	3779802	398	A	0.99982	So I encourage the listeners to look up that video.
3779856	3781210	399	A	0.99981	It's entertaining.
3782030	3787510	400	B	0.9999	It's like this general escape impulse yes.
3787680	3795842	401	B	0.99919	Extended into our open air context, where we also want to maintain the ability to move.
3795896	3805220	402	B	1	And for mobile creatures, that's quite a good proxy for what we might want to care about, like living.
3806330	3807080	403	A	0.82487	Right.
3808330	3808822	404	B	0.92	All right.
3808876	3813590	405	B	0.99981	I'll go to a question in the chat from Alex Kiefer.
3814250	3815366	406	B	0.98641	Fantastic work.
3815468	3828300	407	B	0.96593	Maybe a naive question, and I'm sure it's clear in the formalism, which I have only begun to look at, but the idea is that actual agent environment coupling figures in computing empowerment, right?
3828910	3837390	408	B	1	If so, is there a fully internal proxy that can be optimized given information available internally?
3840530	3844260	409	A	0.98214	Is there let's see.
3845510	3857910	410	A	0.99983	Is there a fully internal proxy that can be computed that can be optimized given info available internally and not part of the coupling?
3858330	3870202	411	A	0.99867	Does he mean, I suppose you could compute empowerment just on the internal state space, but I don't know.
3870336	3880700	412	A	0.69818	Actually, I kind of want to say no, just because you do need to use actions to move around and influence other state spaces and things like this.
3881330	3895700	413	A	1	I don't know how I would compute just a sort of internal intrinsic motivation function that isn't a part of some coupling to some broader system.
3896470	3902450	414	B	1	What about the desire to think freely and to move in cognitive spaces broadly?
3903670	3904820	415	A	0.99993	Yeah, I agree.
3905670	3908166	416	A	0.99923	Okay, so yeah, I definitely agree.
3908348	3919050	417	A	1	If you have all of your sort of physiological needs met and they aren't sort of imposing themselves on you, you're sort of freed up to do other things.
3919120	3919740	418	A	0.99992	Right?
3920990	3941582	419	A	0.64966	So yeah, I think that this could work generally into very abstract spaces, maybe even mathematical spaces and yeah, I think that there can be higher level dependency structures in abstract thought or mathematical thought or things like this.
3941636	3948400	420	A	0.98	I mean, you think about faulty proofs that sort of like destroy an entire field or something.
3949810	3968894	421	A	0.99988	There is a sort of dependency structure in which that if you're working on mathematics that assume some proof is true and it turns out to be false, then perhaps that's disempowering from a sort of abstract perspective.
3968962	3986622	422	A	1	I suppose so that would be yeah, good, I was just going to say but again, all that mathematics is being done by some system that has to perform computation which takes energy and stuff like that.
3986676	3996850	423	A	0.99983	So it's always sort of constrained by that, constrained by some kind of external internal coupling.
4000970	4008970	424	B	0.99998	Many ways to go let's swerve towards active inference and then see if we can come back to some other areas.
4011630	4019970	425	B	0.99999	You mentioned the generative model of active inference but you took a different approach there's, different model ontology.
4020150	4031162	426	B	0.69936	So just broadly, how would you structurally contrast the coupling of the agent and the environment in active inference and in what you've proposed?
4031306	4047986	427	B	0.99968	Because the representations that we see in active inference often feature the particular partitioning where a Markov blanket of a Bayesian graph is intermediating between internal and external states and then there's a mapping function between those internal and external.
4048018	4060582	428	B	1	States such that they can engage in an adaptive coupling, again mediated through the blanket, which is interpreted as providing incoming sensory observations and outgoing actions.
4060646	4069770	429	B	0.99992	So structurally, is that compatible, incompatible or some other secret third thing with what you proposed?
4071650	4085010	430	A	0.59343	Yeah, so I would start off by saying that I think the thing that makes my work different is that it's the structure of the latent sort of discrete state space.
4085080	4099154	431	A	0.96347	It's that structure that's under consideration and I think that in active inference usually encode things like homeostatic drives.
4099202	4099366	432	A	0.63942	Right?
4099388	4103766	433	A	0.92792	You encode them in a generative model right, correct.
4103868	4118582	434	B	0.99985	They are encoded as a preference over sensory observations so that the entity seeks out and selects ultimately policies that reduce or bound their surprise about those observations.
4118646	4122342	435	B	0.99822	Like I expect and prefer myself to be at a homeostatic temperature.
4122486	4128670	436	B	0.99997	I'm not surprised when I'm in that range and I'm going to undergo actions so that I find myself in that range.
4129730	4145960	437	A	0.99972	Yeah, so I would say that that is a major difference because the state space in my case has this sort of self undermining quality where it's like bad starvation states.
4147370	4155302	438	A	0.99867	It's not really a surprise, an expectation of receiving a particular signal or having a preference over some state of the world.
4155436	4163500	439	A	0.64363	It has a sort of self undermining quality that affects your ability to control everything else.
4163870	4178538	440	A	0.78286	So I think I would contrast it that way that usually the preferences or the quality of the states are sort of encoded in a generative model in the active inference setting.
4178714	4186770	441	A	1	And here I'm saying that there's a sort of structural coupling that's giving rise to these valence signals.
4188230	4189090	442	B	0.53717	MMM.
4192470	4205906	443	B	0.99687	You mentioned the key being obtained as inducing this change in the agent's ontology and one that was ultimately reflected by increase in empowerment, hence increase in valence.
4206018	4214810	444	B	0.99995	So how does it come to understand that this shiny object unlocks that door?
4215870	4232400	445	A	0.99969	Yeah, I think that throughout my career I will try to make steps towards actually figuring that out because a lot of this comes down to dynamics learning.
4232930	4241506	446	A	1	If an agent doesn't know what a key does, right, it's not going to know that it opens a door and therefore that it can move through the door and things like this.
4241608	4253862	447	A	0.96	I think there are a lot of sort of maybe like Dyna like algorithms in which you sort of alternate between learning things about dynamics or things like this.
4253916	4262730	448	A	0.99997	But that is a sort of outstanding question for me, is like, yeah, how would you take a key and learn exactly how it's changing the dynamics?
4264350	4276880	449	A	0.56559	But yeah, what I am saying is that given that you can do that, if you can do that, then you can really sort of make these value judgments to things in the world.
4277650	4281662	450	A	0.99996	So I think that's really important too, because consider money, right?
4281716	4285474	451	A	0.71744	Like, if I find a $20 bill on the ground, I'm just going to pick it up.
4285512	4297800	452	A	1	I don't think I'm doing a fancy computation, right, of like, oh, now my bank account is $20 greater and so therefore I have all this new capacity or things like this.
4298650	4314490	453	A	1	I think these sort of preferences for various objects like a key or a dollar bill or things like this can sort of be stored and maybe models with utility theory, who knows?
4316830	4320460	454	B	0.74452	Okay, jumping around to some different questions.
4321250	4333380	455	B	0.96	I hope I'm accurate in saying you described empowerment as a Shannon information theoretic channel capacity between the actuators and the realization of the state.
4334310	4344770	456	B	0.99853	We might be familiar with hearing Shannon channel capacity in the context of bandwidth of information transfer or upload and download, for example.
4344920	4348722	457	B	0.9991	But this is kind of an action oriented Shannon channel capacity.
4348786	4356440	458	B	0.96796	So what does it mean to get an intuition on that capacity between the actuators and the state?
4358110	4368634	459	A	0.99891	Yeah, so the channel capacity is the maximum possible mutual information between the actions you choose and the resulting states.
4368832	4382000	460	A	0.99957	So the channel capacity is sort of the maximum information that you can transmit from your actuators to possible states of the world.
4383190	4390910	461	A	0.94479	It's a form of optionality that says I can affect this many sort of possible futures.
4391070	4395220	462	A	0.99961	So, yeah, it's a sort of intrinsic property of an agent.
4396730	4403186	463	A	1	And so in a product space, this is going to be affected by a lot of different state spaces.
4403218	4409900	464	A	1	If they're interacting, like physiological, state spaces can kill you if they get too low.
4410270	4431482	465	A	0.99874	So I think the interesting thing about this is that it sort of encourages you to think about cognition in this sort of interrogative way because you're essentially trying to figure out what you can do.
4431536	4440990	466	A	0.9999	But there are a lot of, you know, different state spaces that are hindering that information transfer from your actuators to your state spaces.
4441150	4449414	467	A	0.99784	So I think it's very useful because it also sort of leads to explainable AI, right?
4449452	4457640	468	A	0.99844	You can sort of explain your intentions in terms of concrete state spaces which which have structure and explanation, things like that.
4458430	4467980	469	A	1	I think I got off topic from your question, but anything else on Shannon channel capacity that I should talk about?
4469310	4473920	470	B	1	I think, before we loop it back to potentially expected free energy.
4474770	4508310	471	B	0.99983	You mentioned the AI topic, and is there any risk of an imperative that features its own empowerment in terms of an AI being able to then select action policies that might not be what anyone else expects or prefers, may not even be concordant with their own encoded explainable AI priors, but rather something that takes an unbounding approach?
4510430	4513770	472	A	0.99998	Yeah, I do think that that is a fear.
4514110	4522670	473	A	1	I haven't thought that much about the alignment question, so I'd be very interested in what alignment researchers think of this perspective.
4523730	4545060	474	A	1	I think that there's a lot of interesting work to do on sort of multi agent empowerment, especially with these sort of abstract transition operators that work on long time scales and you can see that how sort of socialization matters and things like this.
4545610	4556754	475	A	1	If you're in a world with multiple agents, do you have to learn to respect all of the agents empowerment?
4556802	4559420	476	A	1	I mean, they can also act against you, right?
4560430	4561082	477	A	0.99934	So?
4561216	4561514	478	A	0.77241	Yeah.
4561552	4561994	479	A	1	I don't know.
4562032	4565530	480	A	1	I do think it's a fear to take seriously.
4566670	4571920	481	A	1	I don't know how I would do it though, because it's an outstanding research question to me.
4572850	4583940	482	B	0.99999	Coming from an ant colony background, you mentioned the socialization and I immediately thought, well, let's just say that the seeds take two or three nest mates to carry home.
4584630	4595566	483	B	0.9982	So in order to have one nest mate achieve the maximum empowerment, they must also engage in a pro social environment.
4595758	4618166	484	B	0.99999	Because if anyone else, even if their model is smaller and less empowered, if they just decide not to play, then that individual until it figures out how to carry the seed home alone is going to actually be kind of tethered to a social fabric that helps it actually obtain those goals.
4618278	4625230	485	B	0.99998	So it puts the social imperative as a screen in front of potentially any other imperative.
4626370	4627520	486	A	0.83864	Just a thought.
4628130	4630400	487	A	0.9997	Yeah, that's a great thought.
4631570	4637138	488	B	0.99996	You mentioned the decomposition of some function.
4637224	4655014	489	B	0.68704	I'll let you unpack exactly what function was being decomposed and you justified that by saying we never want to work in the product space from a computational complexity perspective or however makes sense.
4655212	4667820	490	B	0.99999	What are the dangers or what are the scaling features of that product space and then what is the decomposition that facilitates a more tractable form?
4668750	4670394	491	A	0.93357	Yeah, I'll share my screen again.
4670432	4672090	492	A	0.99927	We can go back to that slide.
4672590	4673500	493	A	0.99997	Thank you.
4679160	4705450	494	A	0.97197	Let's see, here it is.
4711170	4712334	495	A	0.96416	You can see my screen.
4712452	4712746	496	A	0.80989	Yep.
4712778	4723380	497	B	1	And maybe even a brief summary of what is a Bellman equation and how did you move from the standard formalization of Bellman equations into this operator space?
4725190	4726034	498	A	0.99742	Sure, yeah.
4726072	4735668	499	A	1	The standard Bellman equation, I don't know the button for one slide forward.
4735834	4745830	500	A	0.77142	Okay, so the standard Bellman equation just has this recursive form and it's just the value of a state that you're at.
4746520	4761768	501	A	1	The optimal value of that state is the maximum value that you can get by choosing an action that rewards you and takes you to a state from which you can act in the future to get more reward.
4761944	4773728	502	A	0.92587	So the Bellman equation can be solved by dynamic programming in order to maximize this function V.
4773894	4784080	503	A	0.99314	So it'll result in a policy that moves you around the world in a way that accumulates a reward that you'll find in your environment.
4785640	4787590	504	A	0.93022	So it has this recursive form.
4788120	4802968	505	A	0.99993	You can sort of unroll it into a sequence and then the operator Bellman equation has this similar recursive form where now you'll notice that there isn't a reward function.
4803134	4806936	506	A	0.99995	There's this availability function and it returns a number between zero and one.
4806958	4808200	507	A	0.99998	It returns a probability.
4808940	4815580	508	A	1	And so that's significant because it means that you can maximize the cumulative feasibility.
4816400	4820190	509	A	1	And so F here is just saying this goal is available.
4820800	4824300	510	A	0.99999	Either you achieve the goal now or you take an action.
4824380	4829330	511	A	0.58266	You don't achieve it now, but you take an action in which you'll achieve it in the future.
4830500	4842868	512	A	0.66424	So it has the same form where you can sort of think of an availability function as a reward, but it's maintaining a probabilistic form.
4842954	4864860	513	A	1	And that probabilistic form is important because it's what allows you to compute the state time feasibility as a transition operator, as an operator that maps you from where you are now, the state time you're at now, which is XT, to the final state time and goal that you achieve.
4866160	4886596	514	A	0.66018	So under the policy, so it says if I start at XT and I follow this policy and I'm choosing actions that move me through this state space, then I'm eventually going to get to the goal and I want to know the final state in time, the probability that I achieved this at any given state in time.
4886778	4891652	515	A	1	And so the state time feasibility function is here.
4891706	4896916	516	A	0.99999	As it's expressed is a transition operator with one action, which is the policy.
4897098	4900856	517	A	0.99999	But when we aggregate it I'll bring this up.
4900958	4926492	518	A	0.99997	When we aggregate it into multiple possible feasibility functions that are centered around multiple sort of objects in the world, then all of the policies associated with each one of those these are goal condition policies that are going to terminate on achieving the goal of going to one of these features and getting the apple, for instance.
4926636	4932492	519	A	0.63	Then each of those policies is an action for this transition operator.
4932636	4937956	520	A	0.77263	So there's five possible policies that are going to take you around the space.
4938058	4944736	521	A	0.98808	So these operator Bellman equations have this probabilistic form which retain this probabilistic structure.
4944928	4959412	522	A	0.99	And you can sort of compare this with I don't know if you're familiar, but there's this concept in RL called the successor representation which is often a hot topic in computational neuroscience.
4959556	4965400	523	A	0.98	And the successor representation is sort of talked about like it's this predictive operator.
4965560	4976284	524	A	0.99928	But really what it represents is expected state Occupancies under a policy and those expected state Occupancies are weighted by the discount factor.
4976412	4998340	525	A	0.98797	So a successor representation is really a sort of weighted statistic and it doesn't map from like an initial state to the state of inducing an event of achieving a goal where successor representations aren't compositional.
4998500	5003732	526	A	0.97876	You can't multiply two successor representations and get another successor representation.
5003876	5014008	527	A	0.92066	Well, you can multiply matrices that represent state time feasibility functions because they're mapping their probabilities of events.
5014104	5026224	528	A	0.99956	So you can combine them just by multiplying matrices that represent the state time feasibility function for a given policy with another one for a different policy.
5026422	5037940	529	A	0.9999	So that will retain the form of a probabilistic function and that's what makes them reusable composable, et cetera.
5038360	5045860	530	A	1	And I think you asked me about the decomposition and this decomposition.
5046600	5048344	531	A	0.98926	Yeah, I might have glossed over this.
5048382	5077520	532	A	0.99995	But the decomposition result is that if you were to compute a state time feasibility function in a product space which has lots of state vectors which are each states, then you don't want to do that because product spaces are very large and take a lot of memory to represent the operator.
5078180	5090804	533	A	0.9117	So if this was not PX and this was PS, and PS was the full product space operator that moves you around this high dimensional space, well, you can't really represent that and you don't really want to.
5090842	5113704	534	A	0.99983	But if you did and you computed a state time feasibility function in a product space, then you can under certain conditions and I can say what those conditions are, but under certain conditions you can decompose this into a prediction of all of the higher level state spaces computed independently.
5113832	5122940	535	A	0.99994	So you evolve the hunger space separately and you evolve the thirst space separately and you evolve the temperature space separately.
5123380	5135750	536	A	0.56577	You can do all of those computations locally on those spaces and you can combine them with a state time feasibility function that's only computed on the low level space.
5138120	5156996	537	A	0.75894	So this hierarchical state time feasibility function is an intractable object for most reasonably sized problems, but you can implicitly form it by this product of these things individually.
5157108	5181420	538	A	1	And so this works when your goal for the hierarchical state time feasibility function, when you just have a single goal at arriving at a particular feature of the world like a tree, that's one of the conditions in which this decomposition holds.
5181580	5186436	539	A	0.99977	So I think that answered your question about the burden of a product space.
5186618	5201528	540	A	0.82742	Well, you need to overcome it by doing local computations on individual state spaces in a sort of network of interconnected state spaces that implicitly form a product space.
5201614	5214460	541	A	0.85815	But you want to compute all of the representations separately in this network of state spaces so that you can sort of move around this high dimensional state space under successive policies.
5216000	5223650	542	A	1	And so that's what allows you to handle forward sampling in this high dimensional state space.
5224180	5238556	543	A	0.98	And that's important because I think just to recall the sort of presentation, I think it was aval A-V-E-L yes, the French.
5238748	5266270	544	A	0.80512	Gwen Carlo yeah, just to echo that sort of sentiment, that if you're creating new state spaces or you're composing, you're not computing policies in a fixed world, you're composing things together, that's of course, going to expand the product space implicitly of all the state vectors of the system.
5266880	5276144	545	A	1	And the act of composing or bringing new information in is expanding the implicit product space that you are in.
5276342	5285796	546	A	1	And so from an RL standpoint, it's not so clear what a reward function on that product space is even supposed to be.
5285898	5290724	547	A	1	I don't think that anyone will answer that question.
5290922	5298936	548	A	0.99997	But it's also not clear what a generative model should look like on that product space either.
5299118	5330816	549	A	1	And I think given that humans at least are so skilled at this sort of dialectical process of proposing theories and composing structure as hypotheses and interrogating what that means, I think that value comes from interrogating what it means for the structure of the world to be a certain way.
5330918	5339792	550	A	0.99993	So if I learn new dynamics of the world and I want to control dynamics on some new space, it might affect other state spaces.
5339936	5344150	551	A	0.99986	But from a normative perspective, it's not really clear.
5344840	5355272	552	A	1	Once you compose something and you're expanding the implicit product space, it's not so clear where any sort of source of normativity should come from.
5355406	5380060	553	A	0.96062	But I think the sort of flexible human reasoning that we sort of know humans to engage in, I think, is in this sort of regime of composition and interrogation where you're always sort of saying, oh, if the world were this way, then I could see how this state space affects these other state spaces in a way that I didn't anticipate.
5380220	5385600	554	A	1	And so I think normativity in a creative way has to come from controllability.
5386520	5388020	555	A	1	That would be my argument.
5391080	5393412	556	B	0.9996	There's a lot there.
5393546	5399816	557	B	0.99812	So a few directions first to our colleagues in reinforcement learning.
5399918	5407476	558	B	0.73016	RL, the paper is provocative in that it includes reward is not necessary.
5407668	5416190	559	B	0.97393	So is reward sufficient and or what is necessary for what?
5418640	5420716	560	A	0.76845	Yeah, it's a good question.
5420898	5422780	561	A	0.68584	Is reward sufficient?
5425140	5446912	562	A	0.63	I would argue this I think that the reward enough hypothesis, which just to remind some of the viewers is the hypothesis that reward maximization can account for all sort of artificial and sort of natural intelligence.
5447056	5462456	563	A	1	That all of the sort of features of intelligence, the sort of capabilities, the structure, learning and stuff can all sort of arise out of some need to some process of maximizing reward.
5462648	5481472	564	A	1	And from my standpoint that's a frustrating statement because one, it doesn't one it doesn't really address where your preferences for specific reward functions come from.
5481606	5509208	565	A	1	And in the paper they will say well, we acknowledge that there could be multiple sources of reward but the process of deciding on what reward you should attend to or care about that I think deciding what signals you should care about is an important part of intelligence itself and I think that reward is enough.
5509294	5522340	566	A	0.60678	Hypothesis as a hypothesis is sort of under constrained in that whatever that mechanism is to that perspective it's going to be maximizing reward under their paradigm.
5522520	5537780	567	A	1	And so therefore whatever shapes what that mechanism is that an agent should attend to this or an agent should stop attending to the things that's always cared about and attend to some new signal.
5540760	5560296	568	A	1	I think that that forces you to sort of take the position that maybe there's, like, a meta reward that directs this process because all sort of attendant processes of intelligent systems sort of underlie the process of reward.
5560328	5567596	569	A	0.94177	Maximization and also the reward is enough.
5567778	5576924	570	A	0.8772	Hypothesis is not being specific about it doesn't tell you what necessarily to compute.
5576972	5586320	571	A	0.99998	It just says that if you try to maximize reward, you will compute the right representations.
5586820	5595300	572	A	1	And so I think that there's just a lot of nuts and bolts about what it takes to be able to reason in a flexible human way.
5595450	5606760	573	A	0.97	And what I'm proposing here with the operator Bellman equations is to say hey, look, there's these reward free Bellman equations that help you deal with the complexity of the world, there's no reward in them.
5606910	5613848	574	A	1	And you could make the case, well, maybe you could just use these operator Bellman equations to occupy states that are rewarding.
5614024	5634416	575	A	0.99837	But I would argue that since the effective product space that we all live in is so vast and we reason about it in such a flexible way, I very much doubt that RL will rise to the challenge of being able to justify motivations in real time in a way that humans can.
5634598	5646020	576	A	0.99	And so to get to the question of is it sufficient, my hunch is no, I don't have a proof of reward is not sufficient.
5647660	5655320	577	A	0.99997	But I also think that the information that a reward function is supposed to carry about what is good.
5655470	5667676	578	A	1	I don't think that is noble or computable on the timescales that we understand human intelligence to work at, to work on.
5667858	5682240	579	A	1	And so, yeah, in order to answer your question about what is necessary, I don't know what is necessary, but I would just sort of make the point I already made of that.
5682310	5697620	580	A	1	I think that we have to get to a point in which we sort of acknowledge the problems of product spaces and sort of reasoning dialectically in a product space that we can't explicitly represent.
5698860	5708568	581	A	1	And so I don't know what is necessary, but I can say that it's not necessary for simple self preserving agents.
5708734	5710330	582	A	1	And that's the claim of the paper.
5712320	5713070	583	B	0.98843	Awesome.
5714000	5714668	584	B	1	All right.
5714754	5737124	585	B	0.8	In our closing segments, I'd like to take a journey to philosophy and then connect this back to potentially relationships between the models that you've presented here and active inference and maybe even walk to the edge of that cliff of the hybrid model.
5737242	5749770	586	B	0.86068	So Aristotle proposed four causes material cause, what something is made of the efficient cause, which is the source of change.
5750140	5758970	587	B	1	The formal cause is the essence, and the final cause is the teleology, the end goal of the object.
5759420	5771020	588	B	0.95	And your presentations TeLEOS was Deacon's analysis of these different forms of teleology.
5771600	5781360	589	B	0.99	And indeed, within the model proposed empowerment was that type of self referential teleology.
5783620	5797596	590	B	0.99916	When juxtaposing with active inference, and specifically the expected free energy functional, which has a lot of analogies with an operator, it's a function of other functions.
5797648	5813832	591	B	0.98	The expected free energy functional is predicated around helping the agent select policies that over expected futures, reduce their uncertainty the most about which sensations they receive.
5813976	5831068	592	B	1	And that's what ties active inference closely with perceptual control theory that that expected free energy is ultimately looking at a divergence between preferences over observations and incoming observations.
5831244	5834680	593	B	0.99987	So that's kind of the sense side of the coin.
5834860	5842180	594	B	0.99879	It's like, I want to stay in the game to be able to align observations with my preferences.
5842680	5849610	595	B	1	And I'm wondering if empowerment is the action side of the game.
5850300	5850920	596	A	0.95	I agree.
5850990	5859652	597	B	0.99328	Saying you'll be involved with, yes, repeatedly sensing yourself to not be starving, not be dying of thirst, not be dying of cold.
5859716	5870504	598	B	0.89929	You'll be in your preference vector by way of this single value, which is the empowerment.
5870632	5885520	599	B	0.53822	Whereas active inference kind of comes from the other side saying, you're going to end up having a lot of squares to move around in, but first you need to make sure that you're reducing divergence between your preferences and your expected observations.
5886900	5887760	600	A	0.37189	Mmhmm.
5891000	5892912	601	B	0.99994	What do you think about that map?
5892976	5898360	602	B	1	Or where would that take these intertwined models?
5899500	5905044	603	A	0.99997	Well, I think that there could be, like, an interface between the two concepts.
5905172	5920472	604	A	1	If we consider that models of how things work, like composed models of how things work, could induce particular generative models that you would want to use in a sort of active inference setting.
5920616	5934770	605	A	1	And that would be the sort of dual nature between the two that there's a cross talk between the proposal of some kind of generative model that would be conducive to the agent.
5935560	5946308	606	A	0.99	And if it is, then it could be a good state encoding which feeds back in on controllability or empowerment and things like this.
5946474	5969916	607	A	0.99406	So, I mean, there's a lot to think about on this topic, but I guess I would just put it that way, that we still need to justify where generative models come from in new situations, for new theories of how things work and things like that.
5970098	5983280	608	A	0.74	And there could be a dual process in which the action side, the internal controllability side is dictating what kinds of generative models that should be considered.
5987320	5989510	609	B	0.97923	Very interesting.
5989880	6003050	610	B	0.99	And the reason I brought up Aristotle's causes was because active inference as a process theory seems to be describing that efficient cause.
6003660	6012876	611	B	0.92881	It's just especially with a variational free energy, which is kind of the real time version of the expected free energy.
6013058	6018920	612	B	0.99852	It's like one step at a time ball going downhill.
6019080	6031440	613	B	1	And so variational inference is enabling incremental, unfolding optimization, again oriented around reducing that sensory preference and outcome.
6033300	6038964	614	B	0.99504	It does everything but specify a final cause.
6039162	6050280	615	B	1	In a sense, one might say that there's a local final cause within the active inference generative model, which is like to reduce the divergence between the preferences and the observations.
6051180	6062552	616	B	0.99993	But the generative model also from the action selection side, which is what makes active inference active inference.
6062696	6073810	617	B	0.94826	It also needs a final cause in that self referential teleodynamic way.
6074260	6094900	618	B	1	And so there could be some very interesting architectures where active inference picks up, where empowerment leads, like through a needle, because it's such a small representation with valence.
6095400	6118440	619	B	1	And then one other kind of connection or maybe mapping between them is we've seen models of valence in active inference, such as the affective inference work, where valence was associated broadly with whether things are going better or worse than expected.
6118600	6129120	620	B	1	In terms of statistical uncertainty, if you're reducing your uncertainty more than you expected, things are going better than expected and vice versa.
6130980	6154468	621	B	0.99974	So that is a very variance oriented valence concept where broader uncertainties are associated with inferior valence and tighter uncertainties are associated with positive valence.
6154564	6164648	622	B	1	And it's just interesting that that's kind of like an orthogonal valence concept from how much you can actually do that's the actionable valence.
6164824	6179090	623	B	0.99999	Would you rather have a high precision around not being able to do anything, or high uncertainty about being able to do a lot or a huge amount?
6179780	6197940	624	B	0.94	And so it almost seems like when we contrast those two, the direction that dominates is, in the final analysis, the ability to have empowerment, not necessarily to just have tight control over your observations.
6200140	6203464	625	A	0.98745	Yeah, I agree.
6203662	6207320	626	A	0.99985	Lots of interesting avenues for hybrid theories.
6212480	6216924	627	B	0.99932	Well, what a very interesting talk.
6217042	6231136	628	B	1	I guess one more question on the model and then we'll close, which is the time horizon is it an infinite time horizon, or what is the treatment of time?
6231318	6236240	629	B	1	And can time be continuous, or is time always discrete, and is it finite or infinite?
6238340	6243460	630	A	0.59	A so the operator Bellman equations are formalized as a finite horizon.
6243960	6251128	631	A	1	I suppose they could be extended to infinite horizon and infinite or continuous time.
6251214	6253256	632	A	1	I suppose that's possible too.
6253438	6260120	633	A	0.99979	But as they're formalized now, it's discrete state, discrete time, finite horizon.
6262400	6268350	634	A	0.94474	But yeah, I think that there's alternative forms that could be made.
6272240	6274568	635	A	0.99995	Was there another question too, in it?
6274754	6275650	636	A	0.99712	Can't remember.
6278590	6281846	637	B	1	Let us close with your time horizon.
6282038	6292780	638	B	0.99999	What are your next steps with the research, and how would you imagine an ecosystem of continuation of the work?
6294110	6319670	639	A	0.99991	Yeah, I'm interested in getting this work into computational neuroscience problem, because there's a lot of, I think, alternative models that need to be considered, especially given some of the themes that I've touched on in terms of justifying what to do in a product space, how do you represent control in a distributed system, et cetera.
6321130	6325538	640	A	1	And so I think that there's a lot to do there in computational neuroscience.
6325634	6329500	641	A	0.99996	On the AI side, I want to put this in a world model.
6330750	6349374	642	A	1	I want to get good auto encoders, kind of like how I had on the first slide, where you have a high dimensional agent in a high dimensional multimodal world, how do you have nice world models and put this in something like that?
6349412	6353982	643	A	0.9926	So that's the AI future direction, and I'd like to do both of them.
6354036	6357380	644	A	0.93461	So I have a lot of work to do.
6358470	6359026	645	B	0.99997	Awesome.
6359128	6368658	646	B	0.99965	Well, in closing, I'm just going to read some of the more statement like comments from the live chat, just so that they're included in the Active inference journal.
6368754	6386726	647	B	0.98874	So Dave Douglas wrote with regards to Deacon's consequence, galileo may have gotten in trouble less from insisting on a heliocentric universe than from insisting that purpose, value, and meaning be banished from science as connecting explanatory principles.
6386918	6407758	648	B	1	The Galileo tolerated remaining connecting principle, causation may have reached the limits of its explanatory power sometime between Newton's and Fourier's day, where all respectable principles of explanation must ultimately rest on invocation of either rigid rods or on elastic bands.
6407934	6422150	649	B	1	The Galilean program of Sola Casa attained its absurdity in Carnap's insistence that meaning, value, counterfactuals must be judged as strictly and literally as meaningless.
6422810	6434662	650	B	0.99977	Have we passed the point when the Galilean program must be simply abandoned, and meaning, value, and purpose must be restored to science as irreducible explanatory principles?
6434726	6448570	651	B	0.55489	Alongside Causation, I find the mysticisms of both Bohm and Heisenberg's quantum completeness and of Powelly and Jung's synchronicity to just be too fluffy to be very useful.
6448730	6458430	652	B	1	In a word, it has become a tradition of science as funded to insist that Causation, and Causation alone, must found our enterprise.
6458590	6462606	653	B	0.99995	This is a tradition of men, not a feature of ultimate reality.
6462718	6467080	654	B	0.54458	Meaning and value also have their place not reducible to cause.
6471370	6472134	655	A	1	I like it.
6472172	6472760	656	A	0.99173	Yeah.
6473770	6474402	657	B	0.99512	Excellent.
6474466	6477202	658	B	0.97922	Well, Thomas, thank you again for joining.
6477266	6482490	659	B	0.77329	You're always welcome back and really looking forward to seeing how this all continues.
6483470	6484026	660	A	1	Me too.
6484048	6485386	661	A	1	And thanks again for having me.
6485408	6491260	662	A	0.99797	I've been very impressed with how much work you do for the discord, and it's a great community.
6491710	6496346	663	A	0.96874	Everyone's very nice and enthusiastic, so I was excited to see it.
6496368	6498554	664	A	1	I just sort of randomly stumbled across it.
6498592	6502990	665	A	0.67933	So I'm glad I introduced myself to the discord.
6503070	6504100	666	A	0.99935	So thank you.
6504550	6505230	667	B	0.86211	Excellent.
6505310	6506658	668	B	1	All right, till next time.
6506744	6507540	669	B	0.99982	Thank you.
6508070	6508530	670	A	0.99947	Great.
6508600	6509460	671	A	0.99989	Thank you.
6512390	6512670	672	A	0.80261	Bye.

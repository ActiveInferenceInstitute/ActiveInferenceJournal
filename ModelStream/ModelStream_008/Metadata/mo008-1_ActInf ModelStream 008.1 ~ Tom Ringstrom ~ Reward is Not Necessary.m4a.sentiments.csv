start	end	speaker	sentiment	confidence	text
1170	1720	A	0.546256959438324	You.
5290	6550	B	0.872096598148346	Hello and welcome.
6700	9542	B	0.8811570405960083	It's January 26, 2023.
9676	14120	B	0.8068862557411194	We're here in active inference model stream number 8.1.
14490	28186	B	0.9536912441253662	Today we're appreciative to have Thomas Ringstrom, who will be presenting on Reward is Not Necessary, a compositional theory of Self Preserving Agents with Empowerment gain maximization.
28378	31578	B	0.9043112993240356	There will be a presentation followed by a discussion.
31674	34074	B	0.9448810815811157	So Thomas, thank you for joining.
34122	35550	B	0.9808918833732605	Really looking forward to this.
35700	36800	B	0.6221094727516174	Off to you.
37810	39186	A	0.9320470690727234	Yeah, thank you very much.
39288	42660	A	0.9912422895431519	It's really nice to be here and talk to this group.
44310	49570	A	0.9219417572021484	I'm a computer science PhD student at the University of Minnesota.
50230	84270	A	0.8290201425552368	My interests are in what are the computational properties that we would need to have agents which flexibly plan in sort of high dimensional product spaces of variables, and also how do we get agents to perform complex tasks in an intrinsically motivated way, especially in high dimensional product spaces.
85650	103334	A	0.8663976788520813	So this presentation is going to argue that reward I know I'm talking to a sort of active inference crowd, but some of the same points apply to active inference perhaps too.
103372	110998	A	0.7249200344085693	But this presentation is mostly about how there's going to be some major problems.
111084	124282	A	0.9136569499969482	I think using reward maximization objectives, and by moving to sort of reward free objective functions, we can get really nice factorizations that help us plan.
124416	131002	A	0.77873295545578	So let's just start off with a sort of simple, picture simplified picture of an organism.
131066	132800	A	0.7251734733581543	So we have a honey badger here.
135650	140020	A	0.7793698906898499	This honey badger has internal states.
140390	142830	A	0.5072311758995056	It gets hungry and it gets thirsty.
142990	148034	A	0.7857382893562317	And there's also an external world that the agent lives in.
148232	158230	A	0.821807324886322	And in order to sort of modify internal state spaces, the agent might have to perform some complex tasks.
158890	167210	A	0.9026538729667664	It might have to get several items in order to eat an apple or things like that.
167280	180654	A	0.8844472169876099	So you could see that maybe some symbolic state space sort of mediates the connection between things you do in the world and transformations that you make on some internal state space.
180852	188674	A	0.7433211207389832	And of course, real world organisms and humans live in a high dimensional world.
188712	209186	A	0.8728392124176025	So you could imagine that there's an encoder and a decoder and really there's a sort of high dimensional physiological or interoceptive domain that is these Y tilde with the dot or Y tilde, I should say, and Z tilde.
209298	211510	A	0.7499704360961914	And then there's a high dimensional world.
211580	214870	A	0.8483825325965881	You could just say that's x tilde.
215030	232030	A	0.8402628898620605	And really you just need to sort of map these down to some discrete state space in which these domains can interact here's like a simple encoder and decoder.
234210	251640	A	0.6725327372550964	If an agent has a kind of simple ontology like this, you can imagine that it's sort of generatively entrained to the world that it lives in, which is a notion that's probably pretty familiar to sort of active inference people.
253130	262902	A	0.8931704759597778	So you have some encoder of these high dimensional states and then you have some operator PS, which is in the middle of the head here.
262956	265386	A	0.7660940885543823	And that just sort of advances the latent states.
265488	273900	A	0.8369938731193542	And then you decode and you'll get sort of expectations of the high dimensional world that the agent lives in.
277490	289886	A	0.5872884392738342	The problem is that you can't really represent explicitly these latent space transition operators that would dictate the dynamics of all these variables.
289918	302046	A	0.5474581122398376	You can't represent it as an explicit object because the more state spaces that you keep track of in the world, the larger this object PS becomes.
302238	308438	A	0.791374683380127	So in order to handle this, you'd have to represent it in a sort of factorized form.
308524	311522	A	0.7588686943054199	So you just would represent its factors.
311666	319290	A	0.7170541286468506	You wouldn't explicitly sort of enumerate all of the state vector transitions under this transition operator.
319870	350094	A	0.8100422620773315	And so what we really need to think about is what are the sort of model based or the sort of bellman principles for decomposing hierarchical state spaces so that we can create the right representations that help us plan in a flexible way in a sort of time varying, time dependent world of lots of variables.
350222	369420	A	0.891145646572113	So this just means what is a sort of objective function l calligraphic L applied to PS or its factorization that would give us some factors here ada and these omegas that could help us plan.
370030	386990	A	0.729888379573822	And also, given the fact that the state space, the effective product space that we're working in is so large and there's many possible state vectors that describe possible states of the world, how do you know what to do?
387140	401202	A	0.5808089375495911	I mean, if you're an RL theorist, you would have to say, well, there exists some kind of reward function that tells you this state of the world is worth this much reward and this state of the world is worth this much reward.
401266	408226	A	0.5200531482696533	But it's not really clear how you should define reward functions on huge product spaces.
408338	414390	A	0.8527660965919495	So I will address this question from sort of reward free perspective.
414730	428474	A	0.8296546339988708	And just to give you a hint of what it's going to be like, what I'm going to argue is that what we really need are agents that have a kind of structured core ontology that it needs to maintain.
428522	440740	A	0.88121098279953	That is coupled state spaces that really depend on each other, in which the policies that you use maintain the sort of internal integrity or controllability of the agent.
444150	446802	A	0.6783345341682434	And that's where empowerment will eventually come in.
446856	449378	A	0.7813398241996765	So I'll talk about empowerment a little too.
449464	451510	A	0.8301859498023987	That's the controllability metric.
451930	469420	A	0.4945555627346039	So here I'm just saying what's a nice policy or what's an objective function f that takes in some intrinsic motivation function, which is this fancy V here, along with a nice factorization that allows you to plan in the world.
469870	470234	A	0.7123860716819763	Okay?
470272	478746	A	0.8419099450111389	So just to recap things that I already said, we're in a product space of state of variables.
478858	485194	A	0.8456419110298157	We have internal and external state spaces, hunger and hydration state spaces can be dynamic.
485242	488030	A	0.692171037197113	As you do things in the world, you get hungrier.
489090	490510	A	0.819979190826416	So how do we plan?
490660	492434	A	0.7556140422821045	How do we know what a good goal is?
492472	506518	A	0.8829227685928345	And what I'm going to argue is that we should compute a specific representation called a state time feasibility function, which is going to be an abstraction that's going to map us from initial state times to final state times.
506604	514794	A	0.9391334652900696	And this will have some really nice properties that allow us to sort of reason in this high dimensional state space.
514992	520554	A	0.8348533511161804	And I'm going to talk about this from just a dynamic programming point of view.
520672	522220	A	0.5950914025306702	There isn't going to be any learning.
523550	524106	A	0.7123860716819763	Okay?
524208	534538	A	0.5680360794067383	So let's just talk about transition operator composition, because I said, well, we have this large product space of state variables and it'd be nice if we just represented it as a factorization.
534714	536160	A	0.7886943221092224	So what does that mean?
537810	547118	A	0.8814212083816528	So imagine that you have PX, which is like a base state space to move around the world, and you have some internal state space or secondary state space PY.
547294	549762	A	0.8799934387207031	And these are linked by some function f.
549816	551800	A	0.8187853097915649	So here's your base state space.
552170	555560	A	0.6900458335876465	And here's the honey badger in the state space.
556010	563720	A	0.49265429377555847	And here's the secondary state space, which is just like your internal hunger space.
565470	569338	A	0.8506740927696228	And so F, what F is going to be is called an availability function.
569504	578506	A	0.8385438919067383	And an availability function is going to say, oh, this honey badger is in state XAT at time T.
578608	589010	A	0.8847933411598206	And given that it's there, what's the probability that this goal is available from this state time, state action time.
589160	595330	A	0.8337938189506531	And so this goal is formally going to be an action on the higher level state space.
595480	608198	A	0.5769888758659363	So for instance, these green lines on the high level state space are going to map you to the most satiated state where you're not hungry anymore at all.
608284	619306	A	0.6800826191902161	And then these black lines on the internal state space are just going to decrement you one state if you're not at the tree, so you eat apples at the tree and that's going to map you to the top.
619408	623820	A	0.528563380241394	And all other states without a tree, you're going to get hungrier over time.
624670	647314	A	0.8657240271568298	Okay, so if we wanted to write a product space operator PS y prime, x prime, we can just represent this as a composition where lambda P is our composition operator, and it's just going to be defined as the product of your two state spaces with F linking them.
647432	648642	A	0.7825995683670044	And then we sum over G.
648696	655174	A	0.8034604787826538	So we're getting rid of the goal variable and so we can represent the product space this way.
655212	656950	A	0.8373898267745972	And we can also just call the product space.
657020	667340	A	0.8954288959503174	We can say that bold S here is just the state vector for yx, or if we have more variables that can be incorporated into bold S.
667790	674442	A	0.8333302140235901	Okay, so what if we have more features of the world?
674496	678400	A	0.7195159196853638	We can drink water, we can get warm at the house.
679810	685018	A	0.7931886911392212	Well, we will need to have a bigger composition.
685194	701720	A	0.8901898860931396	So we can just create an operator called we'll just call PR, where bold R is just state vector of w y and Z, and w y and Z correspond to the hydration, the hunger, and the temperature space.
702250	708050	A	0.7133486866950989	And so this is just a product of the individual ones, individual operators.
708210	715754	A	0.9032789468765259	And so this influence graph is connecting PX to Pwpy and PZ through F.
715872	721500	A	0.8723565936088562	And so we can just define the product space operator as a composition this way.
722110	736800	A	0.8181371688842773	And if we do that, you start to realize, well, this is nice because the effective state space exponentiates essentially as you add on more state spaces that you can control.
737410	750618	A	0.835287868976593	And so this influence graph is just showing what is the sort of ontology of the agent, what constitutes it as its internal and external coupling.
750814	763734	A	0.7026655077934265	And you could imagine that it gets more complicated, for instance, that if you hit W zero or Y zero or whatever, these skull and crossbones indicates that you die.
763782	768230	A	0.7763063907623291	So you can imagine being in a state has a bi directional influence.
768310	775366	A	0.9117931127548218	So Zeta here is conditioning the possible dynamics PX can produce.
775478	782030	A	0.9611738324165344	So you could imagine that once you're in one of those sort of defective bad states that it kills you and you can't move around.
782180	794046	A	0.7536959648132324	So you could imagine sort of more complicated structures like this, which essentially mean that you have to go out in the world and do things in order to keep the system alive.
794238	799140	A	0.5225552916526794	So you don't want to hit W zero or Z zero.
800390	802642	A	0.5845284461975098	And you can imagine this gets even more complicated.
802706	819820	A	0.884366512298584	You could compose sort of larger structures where P sigma is going to be some logical state space that keeps track of multiple conditional events that need to occur for say, eat an apple or something like that.
821550	825194	A	0.9842421412467957	So representing it in this form is very nice.
825312	836560	A	0.8154042363166809	We just represent the factors and the links between the state spaces and that's a sort of memory efficient way of representing the space.
837650	848174	A	0.8924822211265564	Okay, so if we have a homeostatic task, I'll eventually get to the sort of Bellman equations and the model based formulas.
848302	858310	A	0.8885849714279175	But first I'm just going to sort of build up an intuition in the form of an example about how these state time feasibility functions are going to work.
858460	864780	A	0.9266068339347839	The state time feasibility function being the representation that I'm arguing for in this talk.
865710	887200	A	0.8225978016853333	So if we have a hiker and it can go out into the world and drink and eat and get warm with these goal variables, well, there's also this goal variable, G epsilon, which is going to decrement by one state.
888390	898274	A	0.9145330190658569	Then as the hiker moves around, you could represent this as a function.
898392	910534	A	0.9050135612487793	So imagine that the hiker starts at xg one, that's the house, and TS, which is the start time, and the hiker follows a policy pi two.
910732	920220	A	0.8909450769424438	So imagine that you have a policy that is like a goal condition policy pi G two that's going to take you to xg two.
920750	921114	A	0.7123860716819763	Okay?
921152	939620	A	0.8705278038978577	So it's a policy, it's like a shortest path policy or whatever, then that means that there's going to be some final state time xg two TF that you achieve this goal G drink, which remember is in action on the higher level space.
942550	950254	A	0.8749358654022217	So what this would look like is that you decrement two, right, because that takes two time steps to get to the lake.
950302	956006	A	0.8756466507911682	And then once you get to the lake, you take the state action that's going to drink and it's going to bring you up.
956028	970540	A	0.9112560153007507	So there's three total steps in this process of inducing the drink goal variable and then the step after drinking, which would look like this.
970990	981470	A	0.7218780517578125	And then you can imagine the other state spaces are not involved and so they will all decrement three because there's three time steps.
982210	1004754	A	0.8367887139320374	And so one way to represent this is to realize that if all of these sort of goal variables on the way to the lake are sort of the null goal variables, meaning that the agent isn't affecting some other state space, then you can define that as a Markov chain.
1004802	1021542	A	0.8704338073730469	So if you set all of the goals to be the epsilon goal, the null goal, then this PY epsilon is going to be a Markov chain matrix.
1021686	1043838	A	0.8348807692527771	And so that means that the time difference encoded in the state time feasibility function, the TF minus TS is going to be the power that you can take this Markov chain to in order to forward evolve all the other internal state spaces.
1043934	1052178	A	0.8976825475692749	So you can define an operator that does this in one step called Omega Y, which is just going to take some initial state.
1052264	1059686	A	0.8586755990982056	So this green dot or this red dot, and it's going to take the time difference.
1059788	1062994	A	0.8809798955917358	So this is going to be two to get from the house to the lake.
1063042	1064374	A	0.8286647200584412	It's two time steps.
1064502	1073020	A	0.5875173807144165	And so these gray arcs are just an initial to final state map under this policy.
1075390	1088480	A	0.6873709559440613	And so what this looks like, this looks sort of complicated, but you can build a jump operator that jumps you from your initial high dimensional state.
1088870	1090530	A	0.8150916695594788	For instance YXT.
1091270	1093074	A	0.8690563440322876	And we're just considering Y for now.
1093112	1103990	A	0.8732163906097412	I don't have the other state spaces W and Z in here, but this Ada over here is mapping you on the X space.
1104140	1120022	A	0.8308836817741394	The Omega Y is mapping you forward in the Y space and then the PY and the PX are just evolving by one step to update after you hit the goal.
1120086	1122938	A	0.8168246746063232	So after you hit the goal, you have to update by one step.
1123024	1131802	A	0.8432910442352295	So PY and PX do that for both state spaces and then of course you can do this for all of the state spaces.
1131866	1144420	A	0.8818151354789734	So a jump operator for all of the internal state spaces, r, where R is remember, is the vector of all your internal states w, Y and Z.
1145190	1155762	A	0.8844508528709412	Then this is similarly defined where omega R is just the product of all of these other Omegas.
1155826	1161110	A	0.8792228698730469	So we defined Omega Y up here, but you can just define this for all of your other state spaces.
1163690	1171878	A	0.5745079517364502	So now that we've done that, we can continue our journey by going to the tree.
1172054	1173770	A	0.5664944052696228	And so that's two steps away.
1173840	1177998	A	0.847805917263031	So all of the other things decrement two and then an additional one.
1178164	1188670	A	0.8516132235527039	And then if we go back to the house, that's four time steps away plus the additional time step of entering the house and getting warm.
1189110	1209298	A	0.8757883906364441	Okay, so here is another route you could take where the hiker goes to the tree to eat and then the hiker goes to the lake to drink and then goes back to the house to get warm.
1209394	1227040	A	0.5940178036689758	And so this JS is a huge it's important to keep in mind that JS is a huge operator and we cannot explicitly form it in memory because it's unless we have a lot of memory on our computer.
1227890	1237754	A	0.6327725648880005	But having this factorization allows us to chain policies together and evolve a high dimensional state vector in these jumps.
1237882	1254680	A	0.8754660487174988	So we're jumping a state vector around in this object oriented fashion where each state vector is updated once you get to some key object of interest, such as the tree with the apple or the lake with the water.
1256570	1265900	A	0.8928821086883545	So also in this example, you can see that the agent went from the lake to the house.
1266270	1273770	A	0.7522225975990295	And technically, if z, it occupies the lowest state, z zero.
1273920	1281066	A	0.6366140842437744	And so technically it should die at the skull and crossbones on the map, but we let it finish his journey.
1281098	1289890	A	0.882749080657959	And we haven't talked about this sort of bi directional coupling yet, but we'll do that in the next slide to make that concrete.
1291590	1307378	A	0.7328991293907166	All right, so if we have these defective states, we can define mode parameters e plus and E minus, which are just going to be variables that condition our dynamics.
1307474	1320502	A	0.7868121266365051	And we can have a mode function that takes any vector r and R is any vector of blue, green and red squares in the space, and we can map it to a mode parameter.
1320646	1333518	A	0.7723693251609802	So we can define zeta to map to the good mode, the normal healthy mode, if it's not occupying any of the defective states.
1333604	1341918	A	0.6058127284049988	But if one or more defective states are occupied, then we can map it to E minus.
1342094	1355554	A	0.8395195007324219	And so that means if we have some low level transition operator that's indexed by E, that means we can split it into a normal dynamics PE plus and a defective dynamics PE minus.
1355602	1361122	A	0.5487301349639893	And you just notice that the defective dynamics is a sort of identity operation.
1361186	1369210	A	0.5015478134155273	It just sort of arrests your dynamics in the space so you can't move around, but the normal dynamics is just normal grid world movement.
1369950	1382186	A	0.7090919613838196	Okay, what we can do is we can compute a set of individual feasibility functions for each object of interest in the map.
1382378	1388642	A	0.8686749339103699	So in the previous example, we had two trees, two lakes and a house.
1388696	1392930	A	0.856694757938385	So we have five state time feasibility functions.
1393670	1406440	A	0.8700888156890869	And then this Ada hat is an aggregate function of all of these functions where the pi is indexing the corresponding pi for that function.
1407690	1411880	A	0.8445553779602051	Anyone can ask questions, by the way, if they have any questions.
1414990	1439948	A	0.8790847063064575	All right, so the full dynamics is going to be just defined like we defined it before, where we have these two state spaces, the base state space coupled to the internal state space through F.
1440034	1445330	A	0.720615804195404	But then we have this zeta R here on the conditioning side.
1445780	1462310	A	0.6600916981697083	And so this is an operator that you have to achieve goals in order to keep yourself out of these sort of absorbing defective states.
1465640	1473400	A	0.8386259078979492	I showed this influence diagram before, but this influence diagram is capturing this coupling.
1475340	1495440	A	0.8576234579086304	Okay, so many of you will probably be familiar with the standard Bellman equations which are formalized this way, where you have some reward function plus some discounted expectation of your future value.
1495590	1506500	A	0.7283641695976257	And so the value function in a sort of standard Bellman equation says, how much reward am I going to get if I act here optimally over an infinite horizon?
1507240	1511904	A	0.7581411600112915	And so the reward function is often thought to be a task.
1512032	1516630	A	0.8186850547790527	It has that sort of semantics in RL land.
1519340	1533148	A	0.5229383111000061	And then the optimal policy is just going to return the action that corresponds to the best action that maximizes your value, your long run sum of rewards that you're going to get.
1533314	1559860	A	0.7682656645774841	So this is a recursive equation, but my thesis is that we need to rethink the sort of model based Bellman formalizations in order to compute these nice factorizations that move us around high dimensional space like we've been talking about on the previous slides.
1560280	1566420	A	0.9210842251777649	So we're going to formalize some new Bellman equations called operator Bellman equations.
1567100	1575210	A	0.8167229294776917	And these are going to be non stationary Bellman equations or they're going to be functions of time.
1575660	1582316	A	0.6549206972122192	So instead of having a value function that says what's the accumulated reward that I get?
1582418	1586380	A	0.8896018862724304	I'm going to have a cumulative feasibility function kappa.
1587120	1597970	A	0.764520525932312	And this is going to represent what is the cumulative probability that I achieve a particular goal, where a goal, remember, is an action on a higher level space.
1599540	1605672	A	0.874187707901001	So this has a very similar form to the infinite horizon Bellman equation that I just showed.
1605836	1610832	A	0.8509257435798645	But you'll notice that we have this availability function here for a specific goal.
1610896	1614004	A	0.9070744514465332	So this is a single goal G that we're picking out.
1614042	1618724	A	0.8639463186264038	So say the eaten apple goal or the drink the water goal.
1618772	1620170	A	0.730596125125885	We're just choosing one.
1620620	1631900	A	0.8816706538200378	And so F is returning the availability of G from any given state and time and action.
1633200	1646236	A	0.5108094215393066	And then, so either the agent achieves a goal now, which is what this equation is saying, or which is the plus the agent does not achieve the goal.
1646268	1658628	A	0.590724766254425	So one minus the probability of achieving the goal is the probability that you don't achieve the goal times the expectation that you achieve it in the future.
1658794	1666470	A	0.8888643980026245	So it has this similar recursive structure that the infinite horizon Bellman equation has.
1667500	1686350	A	0.7574159502983093	Okay, so then we have the policy, and this policy equation is a little different from the last one because you can imagine that you can maximize the cumulative that you achieve a goal, but you sort of do it at the last second.
1686960	1691944	A	0.6965412497520447	You have maximized certainty that you can get to the store right before it closes.
1692072	1701180	A	0.6980770826339722	So you know 100% that you can achieve the goal of getting an item from the store five minutes before it closes.
1701340	1712820	A	0.7629579901695251	Well, it might be if the store is open for some period of time, that essentially you might want to get it as soon as possible.
1712970	1727880	A	0.8286381959915161	So what this equation is saying is that we're going to pick the action which maximizes the cumulative, but from that set of equivalent actions that maximize the cumulative, which is a star.
1728030	1735084	A	0.7268413305282593	So we're collecting the actions that maximize the cumulative and we're going to pick the one that minimizes the time.
1735202	1755652	A	0.7620200514793396	So this is the sort of conditional optimization that says subject to the fact that we want to maximize the cumulative, we want to get there as fast as we can normally there's just with the infinite Horizon Belmont equation, there's the value function and the policy.
1755786	1763750	A	0.916181206703186	Here we have a third function, which is the state time feasibility function which I've been talking about.
1764360	1780940	A	0.8834574818611145	And the state time feasibility function says given that I start at X and T, what's the probability that I achieve a goal g at a particular state final state in time XF and TF?
1781520	1789260	A	0.8667477369308472	And so you can compute this via dynamic programming as you're computing these other functions.
1789840	1808100	A	0.8858081698417664	And there's a relationship between the cumulative feasibility and the state time feasibility, which is that if you can sum up the individual final states and times from your state time feasibility and that is the cumulative.
1808840	1816790	A	0.8732079863548279	So the cumulative feasibility is just summing over the individual probabilities of a given state final state time.
1818140	1848272	A	0.7805507183074951	And the nice property is that when you use these operator bellman equations on hierarchical state spaces, such as the product space of the agent's core ontology, it has a nice decomposition property, which is that if you're just solving to go to a particular point in the world, then you can actually compute Ada separately from the high level space.
1848326	1860928	A	0.864661455154419	So you just solve for Ada the state time feasibility function on the low level space, but then omega can be computed independently in the high level space and they can be combined.
1861104	1880990	A	0.6521799564361572	So this decomposition property is really nice because we never want to work in a product space, especially as we learn tons of dynamics about how the world works, we need to be able to compute representations in a factorized way which is going to help us move around in reason.
1881760	1903090	A	0.6151698231697083	So this means that when we compute a bunch of individual state time feasibility functions for each goal I don't know why this little hat is here, but then if we have five of these and we have an aggregate feasibility function.
1904020	1918730	A	0.7430704832077026	Then we can use this aggregate feasibility function to move around from feature to feature in the low level world and update all of our internal and higher level states.
1919420	1927610	A	0.88332599401474	And so this is the object from the previous slides that sort of map us around the world.
1928460	1937224	A	0.7482927441596985	So you can also do, and I won't talk about this much, but you can also do logical tasks where instead of eating an apple, you might want to gather an apple.
1937352	1943416	A	0.762022852897644	So if you go obtain an apple, then there could be a bit that corresponds to having an apple.
1943448	1954912	A	0.9018971920013428	And you flip that bit to one once you go to the tree and then you go gather water at the lake and you flip that bit and then you go to the key and you obtain that key.
1954966	1964212	A	0.8344681859016418	And that might be a task where you have to obtain these three items, perhaps because you wanted to compose this task with something else.
1964266	1968468	A	0.9072007536888123	You want to bring these items to another agent or whatever.
1968634	1979450	A	0.8101866841316223	So the point is that you can use these factorizations to move around the world, especially when things in the world have a sort of limited time period in which they are available.
1985120	1992860	A	0.8026887774467468	I talked about how we can plan in these high dimensional state spaces using this factorization.
1993860	2003552	A	0.670495331287384	And now we get to the question, well, why should I plan to any particular high dimensional state?
2003686	2007030	A	0.8003774285316467	That's the intrinsic motivation question.
2007480	2018810	A	0.6670325398445129	Given that there's an exponential number of state vectors in a product space, why is one state vector of the world in the far off distance better than another?
2020220	2022984	A	0.6684567928314209	So that's where empowerment comes in.
2023182	2031340	A	0.7962622046470642	Empowerment is an intrinsic motivation metric which sort of represents controllability.
2031680	2037420	A	0.8620719313621521	So formally, empowerment is a function of a transition operator.
2038160	2048720	A	0.8777490258216858	It's an intrinsic measure of a transition operator and you can also condition it on the state that an agent is at.
2048790	2055716	A	0.8865446448326111	So it takes two arguments and it also has a horizon n, which we'll talk about in a second.
2055898	2066388	A	0.9142239093780518	So it's formally defined as the Shannon channel capacity between your actuators or sequences of actions that you take and the resulting states.
2066554	2078330	A	0.8857083916664124	So the channel capacity, so here open loop sequence of actions would just be go up right and up again, or up left and then down.
2080800	2083432	A	0.8435616493225098	There are a lot of possible sequences of actions.
2083496	2088456	A	0.7715040445327759	So big a here is a random variable for your sequences of actions.
2088648	2101852	A	0.9073561429977417	And so for a horizon n, you can ask how much information can we transmit from our actuators to the resulting state at time tau.
2101996	2110384	A	0.8952471613883972	So tau minus T is our horizon n, which is this parameter on the empowerment.
2110512	2122120	A	0.8052234053611755	So it's just saying what's the agent's capacity to affect the future with certainty or varying amounts of certainty?
2122860	2152530	A	0.8915917873382568	And so the channel capacity is formally defined as the maximum mutual information given distribution over these action sequences and the mutual information decomposes into the entropy of the final states minus the conditional entropy of the final state random variable given that, you know, given the actions and your starting state.
2153300	2162020	A	0.6497427821159363	So this means there are two sort of extremes to empowerment.
2162680	2183276	A	0.7658259868621826	If PX is a deterministic operator, so anytime I'm at state X and I apply an action, I get a deterministic output x prime, then the conditional entropy, there's going to be no uncertainty over my future state.
2183378	2185596	A	0.5950208902359009	So the entropy is going to be zero.
2185698	2187208	A	0.7728108167648315	There's no uncertainty.
2187384	2192440	A	0.5321207046508789	And so that means that the conditional entropy has to cancel out.
2192610	2201340	A	0.6346480846405029	And so empowerment is really just the maximum possible entropy given this distribution.
2201500	2206916	A	0.5195708274841309	And this just reduces down to the log of the number of possible reachable states.
2207018	2210550	A	0.8324072360992432	So how much can I actually reach in the world?
2213080	2219876	A	0.8408589363098145	And so if my horizon is two, we can see that the empowerment here is just log of 13.
2219988	2227690	A	0.8274530172348022	So there are 13 states I can get to, and I have perfect control, so I can actually realize any of those 13 states if I want.
2229680	2259040	A	0.798567533493042	And the other extreme is, if PX is action independent, meaning that any sort of state that I'm in, I have an action, that maybe there's the same distribution given that action, say a uniform distribution, then my conditional entropy given the actions is just going to be the entropy.
2259200	2265830	A	0.636755108833313	And so the empowerment has to be zero because this term is going to cancel out.
2267160	2273540	A	0.8252735733985901	And so there might be a lot of states you can technically reach because I can select actions and I'm just going to go random places.
2273620	2281950	A	0.6371379494667053	So Pink is all the states I could possibly reach, but I don't have any control over which state I'm going to end up in.
2282800	2290584	A	0.5043361186981201	And so I can't influence my future in any way, even though there are a lot of possible futures.
2290632	2292880	A	0.6390009522438049	So empowerment in that extreme is zero.
2292950	2303516	A	0.808891236782074	And then there's an in between zone where you take actions and there's a bias in one direction or there's different distributions for each action.
2303628	2308420	A	0.7812185287475586	And so there might be uncertainty, but you can sort of control how much information.
2308570	2312070	A	0.8219761848449707	You can sort of control what state that you want to end up at.
2313960	2319480	A	0.8545331954956055	Okay, so what I'm going to define is a function valence.
2321660	2331468	A	0.8194575905799866	And it's important to note that there's two arguments, the states, and you can include the time too, and the conditioning side.
2331554	2340556	A	0.6959920525550842	So you have empowerment and you can define an empowerment difference.
2340658	2354284	A	0.9101282954216003	So say you start at S and T, and then you end up at a future state in time after you execute some sequence of policies.
2354412	2363590	A	0.8742235898971558	So Row here appended to the S and T is a sequence of policies, and S row and T row are the resulting states.
2364040	2366870	A	0.8462696075439453	Then you can compute an empowerment difference.
2367980	2370810	A	0.7866507768630981	And this will be our valence function.
2371820	2378456	A	0.8942855596542358	So Q here is a function that I'll talk about in just a second.
2378638	2389144	A	0.8790790438652039	But we can see that our jump operator that's moving around high dimensional space is defined as this factorization.
2389272	2390812	A	0.7111964225769043	So we don't have to represent it.
2390866	2392610	A	0.5031172037124634	Remember, that's an important part.
2393060	2393856	A	0.6595849990844727	And so we.
2393878	2397612	A	0.8911141157150269	Can sample multiple policies from this or chains of policies.
2397676	2406870	A	0.8985185623168945	So if I have policy one, policy two, then I'm going to have some resulting state r double prime, x double prime, TF two.
2408840	2413744	A	0.76777583360672	And so Q here can just represent the output of chains of policies.
2413792	2426360	A	0.8312850594520569	So you can imagine a tree search of chains of policies, and Q is just summarizing the final state of those branches in a tree search of policies of chains of policies.
2428220	2432570	A	0.7254433035850525	All right, so we can use this Q.
2433180	2437532	A	0.8144349455833435	So notice that this Q here is in the expectation over here.
2437586	2442464	A	0.8778631091117859	So it's linking our original state time to the final state time.
2442502	2445520	A	0.8791123628616333	So it's the empowerment after some chain of policies.
2447220	2461174	A	0.8472632169723511	Okay, so say we have a deterministic operator, and this should be P, then valence is an empowerment difference.
2461292	2468070	A	0.8618021607398987	We can have a simple sort of example here where we have two hikers that are considering two different plans.
2468510	2484590	A	0.8435226082801819	And so if this tree here is like the space of two possible chains of policies, then these hikers are just executing one sort of path through this tree.
2489490	2497902	A	0.5952611565589905	They both have the same empowerment because they're starting at the same state and they're both two away from starving.
2497966	2518950	A	0.8289584517478943	So if we just consider an empowerment at the beginning, that's log of 13, and if we chain together two policies here, we can advance our internal state space and the agent will end up down at the lake.
2519310	2533118	A	0.738918125629425	And so given that it's at the lake, you'll notice that it's one state away from dying, so that there's an effective range of where it can go.
2533204	2542560	A	0.518538773059845	So the final empowerment is log five, because there's five reachable states and we're assuming determinism to make this easy.
2543670	2555566	A	0.8404479622840881	So the valence here, and this should be PS here, but I have T here, but the valence is just log five minus log 13.
2555678	2559218	A	0.8338273763656616	And that's the difference between the final and the initial.
2559314	2561430	A	0.7563662528991699	And so that has a negative valence.
2563130	2570140	A	0.9284343719482422	The yellow shirt hiker is clearly in a worse position that he started off at.
2571150	2585520	A	0.8861027359962463	But we can advance the other hiker with pi g three to the lake and then pi g four to the tree and update his internal states.
2586930	2590778	A	0.6479817032814026	And we can see that he's three away from dying.
2590874	2592990	A	0.570475161075592	So there's a bigger effective range.
2594210	2597440	A	0.8306047320365906	And so there are 25 states that he can get to.
2600550	2612102	A	0.5326037406921387	Log of 25 minus log of 13 is zero point 94, which means that he's better off than where he started.
2612156	2617320	A	0.48844555020332336	And so clearly, the second hiker is executing a better plan.
2618090	2624380	A	0.4854965806007385	He has more freedom to engage with other tasks in the world.
2624910	2633742	A	0.5384452939033508	And so if we searched over this entire tree of policy chains, we could pick the best one.
2633876	2644580	A	0.896081805229187	And in this example, we're just going to consider two of these branches and we'd pick policy pi g three, pi g four.
2646870	2657720	A	0.5167921185493469	All right, so there's also another interesting so in this past example, we're just sort of changing the structure we're just changing the initial state.
2658970	2680730	A	0.8753398656845093	But since empowerment is also a function of a transition operator, and our operator Bellman equations are producing transition operators that map us from state time to state time, then the output of those Bellman equations, the operator Bellman equations, produce transition operators so we can compute their empowerment.
2682210	2688910	A	0.8107801079750061	So that's a deep connection between Bellman equations and these intrinsic motivation metrics.
2689570	2703380	A	0.5458914041519165	So we can do interesting things where we can say, well, if the structure of the world were different, then the feasibility functions that I could compute in different configurations of the world would be different.
2703910	2717110	A	0.6514966487884521	So if the Honey Badger gets a key and it opens a door in the mountain pass, then it could potentially get through to the other side.
2717180	2724410	A	0.7871928215026855	But it's important to note that this is changing the structure of the low level state space.
2724480	2733520	A	0.650100588798523	So PE knot is what we'll call the original transition operator, in which we can't go through the mountain pass.
2734130	2752606	A	0.8498175740242004	And that means that if we compute state time feasibility functions on this operator, that means PE here is going into the Bellman equation, then we're going to produce state time feasibility functions from those operator Bellman equations.
2752718	2760360	A	0.741493821144104	That's this object here, which means that we can use it to construct j to move us around high dimensional space.
2760810	2764680	A	0.6367476582527161	And so there's an empowerment for this j.
2765370	2778010	A	0.8545477390289307	But then if we get the key and the key allows us to move through the door, it changes the structure of the world, then that's a different mode of dynamics.
2778510	2790494	A	0.8485926389694214	So that means that all of this applies on the other side, where we can compute feasibility functions off of a different mode of dynamics in which we can move to the mountain pass.
2790692	2813960	A	0.7638630270957947	And so that means we can compute things, we can compute valence by asking is this configuration of the world more conducive to the agent's core ontology that is, the agent's internal external coupling that needs to be maintained as a sort of core object.
2815450	2827258	A	0.8581418991088867	And so we can compute valence just by changing j the structure of the agent's abstractions that it moves to perform tasks in the world.
2827424	2832334	A	0.8739386200904846	So by computing empowerment on j, we're sort of computing it in task space.
2832532	2837262	A	0.7282304763793945	We don't have to consider all possible states of the product space.
2837316	2851570	A	0.8785991072654724	We can narrow it down to operators that move us around task space, that induce dynamics on key other state spaces that we care about maintaining, such as our physiological state spaces.
2853430	2856222	A	0.757793128490448	Okay, so here's a simple example.
2856376	2868370	A	0.8909296989440918	Say the Honey Badger starts at the lake and has some initial empowerment just on the low level operator.
2868450	2876026	A	0.8778988718986511	So that's P 50 there's twelve states it can reach.
2876128	2889210	A	0.5795893669128418	But it also has a task empowerment which is actually zero because there's only one task that it can engage in if you don't include getting the key as a task.
2889370	2896660	A	0.692696213722229	So the circle here is saying that there's only one sort of task that can be done.
2897030	2906980	A	0.6936473250389099	And so if the agent goes and gets the key, it's going to reduce its physiological states because it had to travel there.
2907350	2911938	A	0.8413181304931641	And then it gets the key and it conditions a different mode of dynamics.
2912034	2918630	A	0.9182240962982178	So there's different feasibility functions associated with that mode which I just described on the last slide.
2919470	2925638	A	0.8113873600959778	Then the door will open and it can travel back to the lake.
2925814	2933034	A	0.8124410510063171	And so now that it's at the lake, it can go eat food on the other side, right?
2933152	2944830	A	0.583505392074585	So before getting the key, if it couldn't get the key, if there were no key, then it would just starve because while it could drink water and stay hydrated, it couldn't eat from the apple tree.
2944910	2952706	A	0.7240228056907654	But now it can cycle back and forth between the apple tree and the lake for as long as it wants.
2952888	2970070	A	0.5077243447303772	And so it has a higher empowerment just on the low level state spaces, but it also has a higher task empowerment because there are just over a horizon of three and we're just computing, we have to choose a horizon.
2970230	2980940	A	0.8726603388786316	So the empowerment in task space is eight because there's eight possible branches in resolving states from where it is.
2981250	2985550	A	0.8955916166305542	And so that's very useful.
2986450	2997700	A	0.8238562941551208	So we can compute the valence, which is 0.5 just in the low level space, but also three in the task space.
2998390	3005138	A	0.49608030915260315	And another interesting question which is very important is to say, well, what's the value of the key?
3005304	3006822	A	0.7465593218803406	And you can compute this too.
3006876	3033630	A	0.7310347557067871	You can say, well, given if I just fix the state vector that I'm at, and I just alter the state phi that encodes the object of a key, if I just alter that state and switch it between having a key and not having a key, you can say, well, this key has this much value to the internal organization, the internal integrity or controllability of the agent.
3033700	3042026	A	0.8294237852096558	So it's a sort of agent centric judgment of how much something in the world is valuable.
3042138	3050930	A	0.54648357629776	And so the key can be not valuable if it doesn't do anything in the world that helps the agent control its core ontology.
3052150	3058440	A	0.7826975584030151	So this is a way in which you can sort of bootstrap value into the world.
3059770	3069722	A	0.8896541595458984	You can use the change in an agent's internal structure, its coupling between the internal state spaces and the external world.
3069856	3073866	A	0.8622231483459473	You can use the changes in that structure to assign value to things.
3073968	3075370	A	0.8985545635223389	And that's very useful.
3077070	3080218	A	0.8817166090011597	Okay, I think we're approaching an hour.
3080304	3090762	A	0.7411890625953674	And so I'll just conclude by saying intelligent systems operate in high dimensional product spaces, often with non stationary dynamics.
3090906	3108846	A	0.7783087491989136	This introduces a lot of problems, especially in artificial intelligence because people normally deal with structured tasks and non stationary by training recurrent neural networks and things like this, which contribute a lot to sample complexity.
3108878	3124810	A	0.7840730547904968	And what I'm saying is know operator Bellman equations have this different form which produces transition operators, which helps you factorize your representations for moving around the world and predicting the resulting high dimensional state vector.
3126670	3133178	A	0.7084472179412842	And these operators are composable, they compose with themselves, but they also compose with higher level structure.
3133274	3136958	A	0.8465684056282043	So you can remap different transition structures to them.
3137044	3157490	A	0.8575156927108765	It's very modular and these are very nice properties that you need if you don't want to recompute things and you want to have sort of modular structure come in and remap to your representations that you've already so forward sampling.
3158470	3160130	A	0.8764153718948364	Can you still hear me, Daniel?
3162710	3164162	A	0.6885440945625305	OK, just checking.
3164306	3170082	A	0.7732284069061279	So forward sampling is a good way of solving problems in high dimensional state spaces without representing the product space.
3170156	3177066	A	0.84468674659729	We can't really solve, we can't do dynamic programming in a huge product space that's not going to work.
3177168	3183790	A	0.8501785397529602	We can't sample low level actions, that's not going to work, the tree is too big.
3183860	3195140	A	0.8047062754631042	But we can work at the level of sequences of policies and we can evaluate empowerment gain to justify our goal states.
3196790	3202702	A	0.7772772312164307	And so Valence sort of unifies a lot of distinct drives.
3202766	3223240	A	0.8519798517227173	Like there's a different sort of subfield of RL called multi objective reinforcement learning, which says we'll have a bunch of different reward functions and then we'll have value functions for each of these reward functions for different tasks and that'll make like a high dimensional value function vector space.
3224890	3244546	A	0.7843043804168701	And usually in multi objective RL, you have to pick a policy that that does well in that value function space, but normally you have to deal with the trade offs by some weighting function.
3244728	3269420	A	0.6922202110290527	So what I'm saying here is that because valence is just one number and it summarizes an entire sort of control architecture, that you don't have to introduce things like weighting functions or weighting coefficients to say, oh, this objective is more important now, or this objective is more important now.
3273070	3279686	A	0.7904955148696899	So yeah, many latent drives is not necessarily multi objective, it's multidimensional, it's multi goal.
3279718	3287850	A	0.8062759041786194	But it doesn't have to be multi objective with empowerment.
3287930	3290510	A	0.5828953385353088	You don't have weighted combinations of empowerment.
3291010	3295950	A	0.7797531485557556	And valence depends on the structure of the environment.
3296030	3304194	A	0.8046197891235352	So it's not just some static property of the world or a static property of an agent.
3304392	3308550	A	0.8232070207595825	It incorporates agent world coupling.
3309610	3324202	A	0.9855852723121643	And I thought I'd just end with this quote from Terrence Deakin, who wrote a great book called Incomplete Nature, which I love and I read at the beginning of Grad school, which inspired me a lot.
3324336	3332410	A	0.9363614916801453	And Terrence Deakin wrote a lot about teleology from a sort of thermodynamic perspective and it's really compelling.
3332910	3347758	A	0.7050663828849792	And I just liked what he had to say about teleodynamics, the idea that an organism could be, or its behavior could be organized around realizing something which is sort of virtual.
3347854	3364962	A	0.8761278390884399	And he says Teleodynamics is the dynamical realization of final causality in which a given dynamical organization exists because of the consequences of its own continuous continuance and therefore it can be described as being self generating.
3365106	3381490	A	0.8802830576896667	Specifically, it is the emergence of a distinctive realm of orthograde dynamics that is organized around a self realizing potential, or to be somewhat enigmatic, it is a consequence organized dynamic that is its own consequence.
3381670	3398750	A	0.674685001373291	And I think that's relevant to what I'm doing here because I think empowerment sort of on an internal sort of structured ontology, allows an agent to say there are multiple, there's a huge space of possible futures.
3398830	3419590	A	0.5785033702850342	But I can evaluate a state of the world that's far off in the distant future, and I can organize all of my behavior around that because I can say I can give an explanation for why it benefits my sort of core ontology.
3419750	3425290	A	0.7333351969718933	And so therefore, it makes me capable of acting that way in the future.
3425360	3429130	A	0.8268353343009949	It's a consequence organized dynamic that is its own consequence.
3429790	3436798	A	0.785447895526886	So with that, I just want to thank the Active Inference Institute, and I will take questions.
3436964	3454334	A	0.5358142256736755	And I'm very interested in what active inference theorists think about the sort of potential for a sort of integrated view of empowerment, because I think you all have a lot of experience thinking about generative models and things of that nature.
3454382	3456806	A	0.5705503821372986	So I'd be very curious to know what you think.
3456908	3457720	A	0.8529649972915649	Thank you.
3459850	3460658	B	0.5708940029144287	Excellent.
3460754	3461720	B	0.915550708770752	Thanks a lot.
3462570	3464658	B	0.9483615756034851	Great presentation.
3464834	3467282	B	0.9383494853973389	So this will be a fun discussion.
3467346	3476620	B	0.8051541447639465	Those who are watching live, please feel free to add questions in the live chat and you can unshare your screen and we'll begin.
3478430	3490190	B	0.864869236946106	I guess I'll take an empowering deep breath and ask a general question, and then I have some scattered notes that I'll love to dive into.
3490260	3493920	B	0.8715881705284119	So how did you come to this area of research?
3494230	3501010	B	0.9193155169487	What brought you to control theory modeling and to the empowerment perspective specifically?
3503670	3507880	A	0.8353433609008789	Hold on, I'm just bringing up the YouTube stream so I can see comments right now.
3510490	3513766	B	0.8079701066017151	What brought just a general question.
3513868	3514678	B	0.589512825012207	Mute that one.
3514764	3515542	A	0.5760965347290039	There we go.
3515676	3518454	A	0.7228895425796509	Yeah, I'll mute that sounds good.
3518492	3519180	B	0.8529649972915649	Thank you.
3520030	3520538	A	0.5491447448730469	Yeah.
3520624	3523398	A	0.8802416920661926	So what brought me to empowerment?
3523494	3549778	A	0.5980783104896545	I've always been interested in how could animals interact in a world in a way that's so sample efficient, especially like, knowing that animals can, like a baby horse can get up and move around and interact with the world in a sort of fluid, flexible way.
3549944	3556370	A	0.8826066851615906	What are the sort of core representational capacities that are needed to do that?
3556440	3563190	A	0.5249910354614258	And I didn't really see anything from the RL world, and this is before I knew about active inference.
3564090	3566870	A	0.7611204385757446	And so that's always been in the back of my mind.
3567020	3593670	A	0.7382124066352844	And another big influence was a guy named Nishith Srivastava who wrote an interesting paper about how you can basically have a sort of relativistic decision theory that allowed you to make judgments between different items without recourse to sort of hedonic utility theory maximization.
3593850	3614370	A	0.8728742003440857	And so he sort of argued that if there was something like a latent acceptability function, that you could sort of remember a history of item acceptability and you could remember the context that you made decisions and that you could actually just do Bayesian inference over those memories.
3614530	3626634	A	0.6594990491867065	And you could explain a lot of interesting things like preference reversal phenomena in decision theory where you introduce irrelevant alternatives and it changes the fundamental choice you make.
3626672	3642602	A	0.8280907273292542	And I thought that sort of initiated a lot of thinking into, how could you bring those kinds of intuitions into sort of embodied planning?
3642666	3658150	A	0.6193885207176208	Like, how could it be that you have an agent assign value to things without them being sort of attributed as sort of static preferences or static utilities in the world?
3658220	3661910	A	0.9104043841362	So I think that was also a big inspiration.
3664250	3665046	B	0.9184247851371765	Awesome.
3665228	3665718	B	0.584351658821106	Okay.
3665804	3667670	B	0.8032069802284241	And then one short general question.
3667740	3669110	B	0.6389870643615723	Why the honey badger?
3670590	3672010	A	0.8304879665374756	The honey badger?
3673310	3683790	A	0.573352575302124	My advisor showed me a YouTube video of a honey badger named Staffel, and in my paper, reward is not necessary.
3684450	3690670	A	0.8605614304542542	The opening paragraph talks about Stauffel, and there's a link to this YouTube video.
3690740	3703394	A	0.7060379385948181	But Stauffel is a honey badger in Australia, and he's at some sort of animal care center, and he's really good at escaping from things.
3703512	3715142	A	0.6672921180725098	So the caretaker at this animal sanctuary constantly has to build elaborate structures to keep staffel in.
3715196	3718498	A	0.8511881232261658	So he has this sort of pen called badger alcatraz.
3718674	3726106	A	0.6118298172950745	And staffel would do interesting things, know, find objects to lean against the wall and climb over.
3726288	3737342	A	0.7235004305839539	And if you took those away, staffel would pack mud into balls and stack them into, like, a little pyramid against the wall to climb up and things like that.
3737476	3754946	A	0.7181175947189331	So, yeah, it got me thinking, what is a sort of good general intrinsic motivation function that doesn't just work on low level states, but also in a sort of more conceptual hierarchical space?
3755048	3761634	A	0.8687785863876343	Like there might be objects or mating opportunities or anything sort of outside badger alcatraz?
3761682	3773050	A	0.8433244228363037	What is the sort of way in which you could have an agent sort of think in a sort of abstract way in order to justify its motivations?
3774750	3779802	A	0.6572033762931824	So I encourage the listeners to look up that video.
3779856	3781210	A	0.7920766472816467	It's entertaining.
3782030	3787510	B	0.7583171725273132	It's like this general escape impulse yes.
3787680	3795842	B	0.6129659414291382	Extended into our open air context, where we also want to maintain the ability to move.
3795896	3805220	B	0.6747032403945923	And for mobile creatures, that's quite a good proxy for what we might want to care about, like living.
3806330	3807080	A	0.5664746165275574	Right.
3808330	3808822	B	0.4896698594093323	All right.
3808876	3813590	B	0.9119596481323242	I'll go to a question in the chat from Alex Kiefer.
3814250	3815366	B	0.9151626825332642	Fantastic work.
3815468	3828300	B	0.761496365070343	Maybe a naive question, and I'm sure it's clear in the formalism, which I have only begun to look at, but the idea is that actual agent environment coupling figures in computing empowerment, right?
3828910	3837390	B	0.8988456130027771	If so, is there a fully internal proxy that can be optimized given information available internally?
3840530	3844260	A	0.6966869235038757	Is there let's see.
3845510	3857910	A	0.8850960731506348	Is there a fully internal proxy that can be computed that can be optimized given info available internally and not part of the coupling?
3858330	3870202	A	0.7249147891998291	Does he mean, I suppose you could compute empowerment just on the internal state space, but I don't know.
3870336	3880700	A	0.7165853381156921	Actually, I kind of want to say no, just because you do need to use actions to move around and influence other state spaces and things like this.
3881330	3895700	A	0.5043051242828369	I don't know how I would compute just a sort of internal intrinsic motivation function that isn't a part of some coupling to some broader system.
3896470	3902450	B	0.7652589678764343	What about the desire to think freely and to move in cognitive spaces broadly?
3903670	3904820	A	0.5375386476516724	Yeah, I agree.
3905670	3908166	A	0.8944734334945679	Okay, so yeah, I definitely agree.
3908348	3919050	A	0.7853594422340393	If you have all of your sort of physiological needs met and they aren't sort of imposing themselves on you, you're sort of freed up to do other things.
3919120	3919740	A	0.7360090017318726	Right?
3920990	3941582	A	0.6645618677139282	So yeah, I think that this could work generally into very abstract spaces, maybe even mathematical spaces and yeah, I think that there can be higher level dependency structures in abstract thought or mathematical thought or things like this.
3941636	3948400	A	0.8450615406036377	I mean, you think about faulty proofs that sort of like destroy an entire field or something.
3949810	3968894	A	0.6018072962760925	There is a sort of dependency structure in which that if you're working on mathematics that assume some proof is true and it turns out to be false, then perhaps that's disempowering from a sort of abstract perspective.
3968962	3986622	A	0.5445668697357178	I suppose so that would be yeah, good, I was just going to say but again, all that mathematics is being done by some system that has to perform computation which takes energy and stuff like that.
3986676	3996850	A	0.7482026815414429	So it's always sort of constrained by that, constrained by some kind of external internal coupling.
4000970	4008970	B	0.7314003109931946	Many ways to go let's swerve towards active inference and then see if we can come back to some other areas.
4011630	4019970	B	0.8492199182510376	You mentioned the generative model of active inference but you took a different approach there's, different model ontology.
4020150	4031162	B	0.9003452658653259	So just broadly, how would you structurally contrast the coupling of the agent and the environment in active inference and in what you've proposed?
4031306	4047986	B	0.8987855315208435	Because the representations that we see in active inference often feature the particular partitioning where a Markov blanket of a Bayesian graph is intermediating between internal and external states and then there's a mapping function between those internal and external.
4048018	4060582	B	0.8578792810440063	States such that they can engage in an adaptive coupling, again mediated through the blanket, which is interpreted as providing incoming sensory observations and outgoing actions.
4060646	4069770	B	0.7846372127532959	So structurally, is that compatible, incompatible or some other secret third thing with what you proposed?
4071650	4085010	A	0.7893067002296448	Yeah, so I would start off by saying that I think the thing that makes my work different is that it's the structure of the latent sort of discrete state space.
4085080	4099154	A	0.8757495880126953	It's that structure that's under consideration and I think that in active inference usually encode things like homeostatic drives.
4099202	4099366	A	0.7360090017318726	Right?
4099388	4103766	A	0.8219587206840515	You encode them in a generative model right, correct.
4103868	4118582	B	0.8057078719139099	They are encoded as a preference over sensory observations so that the entity seeks out and selects ultimately policies that reduce or bound their surprise about those observations.
4118646	4122342	B	0.7775692343711853	Like I expect and prefer myself to be at a homeostatic temperature.
4122486	4128670	B	0.6979811191558838	I'm not surprised when I'm in that range and I'm going to undergo actions so that I find myself in that range.
4129730	4145960	A	0.7230484485626221	Yeah, so I would say that that is a major difference because the state space in my case has this sort of self undermining quality where it's like bad starvation states.
4147370	4155302	A	0.7102887630462646	It's not really a surprise, an expectation of receiving a particular signal or having a preference over some state of the world.
4155436	4163500	A	0.8213834166526794	It has a sort of self undermining quality that affects your ability to control everything else.
4163870	4178538	A	0.8278535008430481	So I think I would contrast it that way that usually the preferences or the quality of the states are sort of encoded in a generative model in the active inference setting.
4178714	4186770	A	0.7820302844047546	And here I'm saying that there's a sort of structural coupling that's giving rise to these valence signals.
4188230	4189090	B	0.5746495127677917	MMM.
4192470	4205906	B	0.7715221643447876	You mentioned the key being obtained as inducing this change in the agent's ontology and one that was ultimately reflected by increase in empowerment, hence increase in valence.
4206018	4214810	B	0.8154674768447876	So how does it come to understand that this shiny object unlocks that door?
4215870	4232400	A	0.5518475770950317	Yeah, I think that throughout my career I will try to make steps towards actually figuring that out because a lot of this comes down to dynamics learning.
4232930	4241506	A	0.6503389477729797	If an agent doesn't know what a key does, right, it's not going to know that it opens a door and therefore that it can move through the door and things like this.
4241608	4253862	A	0.8799962997436523	I think there are a lot of sort of maybe like Dyna like algorithms in which you sort of alternate between learning things about dynamics or things like this.
4253916	4262730	A	0.7909026145935059	But that is a sort of outstanding question for me, is like, yeah, how would you take a key and learn exactly how it's changing the dynamics?
4264350	4276880	A	0.7671723365783691	But yeah, what I am saying is that given that you can do that, if you can do that, then you can really sort of make these value judgments to things in the world.
4277650	4281662	A	0.6218951344490051	So I think that's really important too, because consider money, right?
4281716	4285474	A	0.7315534949302673	Like, if I find a $20 bill on the ground, I'm just going to pick it up.
4285512	4297800	A	0.5549132227897644	I don't think I'm doing a fancy computation, right, of like, oh, now my bank account is $20 greater and so therefore I have all this new capacity or things like this.
4298650	4314490	A	0.9002211093902588	I think these sort of preferences for various objects like a key or a dollar bill or things like this can sort of be stored and maybe models with utility theory, who knows?
4316830	4320460	B	0.8248641490936279	Okay, jumping around to some different questions.
4321250	4333380	B	0.710342526435852	I hope I'm accurate in saying you described empowerment as a Shannon information theoretic channel capacity between the actuators and the realization of the state.
4334310	4344770	B	0.9049749970436096	We might be familiar with hearing Shannon channel capacity in the context of bandwidth of information transfer or upload and download, for example.
4344920	4348722	B	0.8998152613639832	But this is kind of an action oriented Shannon channel capacity.
4348786	4356440	B	0.8987986445426941	So what does it mean to get an intuition on that capacity between the actuators and the state?
4358110	4368634	A	0.8515002727508545	Yeah, so the channel capacity is the maximum possible mutual information between the actions you choose and the resulting states.
4368832	4382000	A	0.8972509503364563	So the channel capacity is sort of the maximum information that you can transmit from your actuators to possible states of the world.
4383190	4390910	A	0.778054416179657	It's a form of optionality that says I can affect this many sort of possible futures.
4391070	4395220	A	0.8520734906196594	So, yeah, it's a sort of intrinsic property of an agent.
4396730	4403186	A	0.7248677611351013	And so in a product space, this is going to be affected by a lot of different state spaces.
4403218	4409900	A	0.8280439972877502	If they're interacting, like physiological, state spaces can kill you if they get too low.
4410270	4431482	A	0.7395063042640686	So I think the interesting thing about this is that it sort of encourages you to think about cognition in this sort of interrogative way because you're essentially trying to figure out what you can do.
4431536	4440990	A	0.6213242411613464	But there are a lot of, you know, different state spaces that are hindering that information transfer from your actuators to your state spaces.
4441150	4449414	A	0.9203550219535828	So I think it's very useful because it also sort of leads to explainable AI, right?
4449452	4457640	A	0.8676151037216187	You can sort of explain your intentions in terms of concrete state spaces which which have structure and explanation, things like that.
4458430	4467980	A	0.7806223630905151	I think I got off topic from your question, but anything else on Shannon channel capacity that I should talk about?
4469310	4473920	B	0.8345168828964233	I think, before we loop it back to potentially expected free energy.
4474770	4508310	B	0.5392595529556274	You mentioned the AI topic, and is there any risk of an imperative that features its own empowerment in terms of an AI being able to then select action policies that might not be what anyone else expects or prefers, may not even be concordant with their own encoded explainable AI priors, but rather something that takes an unbounding approach?
4510430	4513770	A	0.5079947113990784	Yeah, I do think that that is a fear.
4514110	4522670	A	0.6684169769287109	I haven't thought that much about the alignment question, so I'd be very interested in what alignment researchers think of this perspective.
4523730	4545060	A	0.9174374938011169	I think that there's a lot of interesting work to do on sort of multi agent empowerment, especially with these sort of abstract transition operators that work on long time scales and you can see that how sort of socialization matters and things like this.
4545610	4556754	A	0.7854540348052979	If you're in a world with multiple agents, do you have to learn to respect all of the agents empowerment?
4556802	4559420	A	0.5281617641448975	I mean, they can also act against you, right?
4560430	4561082	A	0.6590757966041565	So?
4561216	4561514	A	0.5491447448730469	Yeah.
4561552	4561994	A	0.5666733384132385	I don't know.
4562032	4565530	A	0.6784894466400146	I do think it's a fear to take seriously.
4566670	4571920	A	0.529606282711029	I don't know how I would do it though, because it's an outstanding research question to me.
4572850	4583940	B	0.677890956401825	Coming from an ant colony background, you mentioned the socialization and I immediately thought, well, let's just say that the seeds take two or three nest mates to carry home.
4584630	4595566	B	0.5035766363143921	So in order to have one nest mate achieve the maximum empowerment, they must also engage in a pro social environment.
4595758	4618166	B	0.7621593475341797	Because if anyone else, even if their model is smaller and less empowered, if they just decide not to play, then that individual until it figures out how to carry the seed home alone is going to actually be kind of tethered to a social fabric that helps it actually obtain those goals.
4618278	4625230	B	0.7488739490509033	So it puts the social imperative as a screen in front of potentially any other imperative.
4626370	4627520	A	0.8010507822036743	Just a thought.
4628130	4630400	A	0.9462247490882874	Yeah, that's a great thought.
4631570	4637138	B	0.7871065139770508	You mentioned the decomposition of some function.
4637224	4655014	B	0.49634912610054016	I'll let you unpack exactly what function was being decomposed and you justified that by saying we never want to work in the product space from a computational complexity perspective or however makes sense.
4655212	4667820	B	0.792227566242218	What are the dangers or what are the scaling features of that product space and then what is the decomposition that facilitates a more tractable form?
4668750	4670394	A	0.5569052696228027	Yeah, I'll share my screen again.
4670432	4672090	A	0.7669821381568909	We can go back to that slide.
4672590	4673500	A	0.8529649972915649	Thank you.
4679160	4705450	A	0.7710094451904297	Let's see, here it is.
4711170	4712334	A	0.748557448387146	You can see my screen.
4712452	4712746	A	0.560655415058136	Yep.
4712778	4723380	B	0.9158793687820435	And maybe even a brief summary of what is a Bellman equation and how did you move from the standard formalization of Bellman equations into this operator space?
4725190	4726034	A	0.5765458345413208	Sure, yeah.
4726072	4735668	A	0.5434615015983582	The standard Bellman equation, I don't know the button for one slide forward.
4735834	4745830	A	0.8051537871360779	Okay, so the standard Bellman equation just has this recursive form and it's just the value of a state that you're at.
4746520	4761768	A	0.7660372853279114	The optimal value of that state is the maximum value that you can get by choosing an action that rewards you and takes you to a state from which you can act in the future to get more reward.
4761944	4773728	A	0.722032368183136	So the Bellman equation can be solved by dynamic programming in order to maximize this function V.
4773894	4784080	A	0.6365522146224976	So it'll result in a policy that moves you around the world in a way that accumulates a reward that you'll find in your environment.
4785640	4787590	A	0.7460551261901855	So it has this recursive form.
4788120	4802968	A	0.6596168875694275	You can sort of unroll it into a sequence and then the operator Bellman equation has this similar recursive form where now you'll notice that there isn't a reward function.
4803134	4806936	A	0.8083848357200623	There's this availability function and it returns a number between zero and one.
4806958	4808200	A	0.7755746245384216	It returns a probability.
4808940	4815580	A	0.8712872266769409	And so that's significant because it means that you can maximize the cumulative feasibility.
4816400	4820190	A	0.7527481913566589	And so F here is just saying this goal is available.
4820800	4824300	A	0.8434537649154663	Either you achieve the goal now or you take an action.
4824380	4829330	A	0.5130462646484375	You don't achieve it now, but you take an action in which you'll achieve it in the future.
4830500	4842868	A	0.878107488155365	So it has the same form where you can sort of think of an availability function as a reward, but it's maintaining a probabilistic form.
4842954	4864860	A	0.7018582820892334	And that probabilistic form is important because it's what allows you to compute the state time feasibility as a transition operator, as an operator that maps you from where you are now, the state time you're at now, which is XT, to the final state time and goal that you achieve.
4866160	4886596	A	0.8154987096786499	So under the policy, so it says if I start at XT and I follow this policy and I'm choosing actions that move me through this state space, then I'm eventually going to get to the goal and I want to know the final state in time, the probability that I achieved this at any given state in time.
4886778	4891652	A	0.8796181082725525	And so the state time feasibility function is here.
4891706	4896916	A	0.8465666770935059	As it's expressed is a transition operator with one action, which is the policy.
4897098	4900856	A	0.8507940173149109	But when we aggregate it I'll bring this up.
4900958	4926492	A	0.8452242612838745	When we aggregate it into multiple possible feasibility functions that are centered around multiple sort of objects in the world, then all of the policies associated with each one of those these are goal condition policies that are going to terminate on achieving the goal of going to one of these features and getting the apple, for instance.
4926636	4932492	A	0.8942567110061646	Then each of those policies is an action for this transition operator.
4932636	4937956	A	0.8917550444602966	So there's five possible policies that are going to take you around the space.
4938058	4944736	A	0.9015995860099792	So these operator Bellman equations have this probabilistic form which retain this probabilistic structure.
4944928	4959412	A	0.8135311007499695	And you can sort of compare this with I don't know if you're familiar, but there's this concept in RL called the successor representation which is often a hot topic in computational neuroscience.
4959556	4965400	A	0.8410912156105042	And the successor representation is sort of talked about like it's this predictive operator.
4965560	4976284	A	0.7781293392181396	But really what it represents is expected state Occupancies under a policy and those expected state Occupancies are weighted by the discount factor.
4976412	4998340	A	0.6126315593719482	So a successor representation is really a sort of weighted statistic and it doesn't map from like an initial state to the state of inducing an event of achieving a goal where successor representations aren't compositional.
4998500	5003732	A	0.5059090852737427	You can't multiply two successor representations and get another successor representation.
5003876	5014008	A	0.8354779481887817	Well, you can multiply matrices that represent state time feasibility functions because they're mapping their probabilities of events.
5014104	5026224	A	0.8949376940727234	So you can combine them just by multiplying matrices that represent the state time feasibility function for a given policy with another one for a different policy.
5026422	5037940	A	0.5561777353286743	So that will retain the form of a probabilistic function and that's what makes them reusable composable, et cetera.
5038360	5045860	A	0.8297630548477173	And I think you asked me about the decomposition and this decomposition.
5046600	5048344	A	0.5177786350250244	Yeah, I might have glossed over this.
5048382	5077520	A	0.5737650990486145	But the decomposition result is that if you were to compute a state time feasibility function in a product space which has lots of state vectors which are each states, then you don't want to do that because product spaces are very large and take a lot of memory to represent the operator.
5078180	5090804	A	0.5641279220581055	So if this was not PX and this was PS, and PS was the full product space operator that moves you around this high dimensional space, well, you can't really represent that and you don't really want to.
5090842	5113704	A	0.8654009103775024	But if you did and you computed a state time feasibility function in a product space, then you can under certain conditions and I can say what those conditions are, but under certain conditions you can decompose this into a prediction of all of the higher level state spaces computed independently.
5113832	5122940	A	0.8059857487678528	So you evolve the hunger space separately and you evolve the thirst space separately and you evolve the temperature space separately.
5123380	5135750	A	0.7766802906990051	You can do all of those computations locally on those spaces and you can combine them with a state time feasibility function that's only computed on the low level space.
5138120	5156996	A	0.7634608149528503	So this hierarchical state time feasibility function is an intractable object for most reasonably sized problems, but you can implicitly form it by this product of these things individually.
5157108	5181420	A	0.8020659685134888	And so this works when your goal for the hierarchical state time feasibility function, when you just have a single goal at arriving at a particular feature of the world like a tree, that's one of the conditions in which this decomposition holds.
5181580	5186436	A	0.8338557481765747	So I think that answered your question about the burden of a product space.
5186618	5201528	A	0.7883502244949341	Well, you need to overcome it by doing local computations on individual state spaces in a sort of network of interconnected state spaces that implicitly form a product space.
5201614	5214460	A	0.8780964612960815	But you want to compute all of the representations separately in this network of state spaces so that you can sort of move around this high dimensional state space under successive policies.
5216000	5223650	A	0.5608339905738831	And so that's what allows you to handle forward sampling in this high dimensional state space.
5224180	5238556	A	0.7694596648216248	And that's important because I think just to recall the sort of presentation, I think it was aval A-V-E-L yes, the French.
5238748	5266270	A	0.8214403390884399	Gwen Carlo yeah, just to echo that sort of sentiment, that if you're creating new state spaces or you're composing, you're not computing policies in a fixed world, you're composing things together, that's of course, going to expand the product space implicitly of all the state vectors of the system.
5266880	5276144	A	0.7699053287506104	And the act of composing or bringing new information in is expanding the implicit product space that you are in.
5276342	5285796	A	0.49283573031425476	And so from an RL standpoint, it's not so clear what a reward function on that product space is even supposed to be.
5285898	5290724	A	0.6513733863830566	I don't think that anyone will answer that question.
5290922	5298936	A	0.5401601195335388	But it's also not clear what a generative model should look like on that product space either.
5299118	5330816	A	0.6434560418128967	And I think given that humans at least are so skilled at this sort of dialectical process of proposing theories and composing structure as hypotheses and interrogating what that means, I think that value comes from interrogating what it means for the structure of the world to be a certain way.
5330918	5339792	A	0.8447310924530029	So if I learn new dynamics of the world and I want to control dynamics on some new space, it might affect other state spaces.
5339936	5344150	A	0.67335045337677	But from a normative perspective, it's not really clear.
5344840	5355272	A	0.5925501585006714	Once you compose something and you're expanding the implicit product space, it's not so clear where any sort of source of normativity should come from.
5355406	5380060	A	0.7000356316566467	But I think the sort of flexible human reasoning that we sort of know humans to engage in, I think, is in this sort of regime of composition and interrogation where you're always sort of saying, oh, if the world were this way, then I could see how this state space affects these other state spaces in a way that I didn't anticipate.
5380220	5385600	A	0.8101208806037903	And so I think normativity in a creative way has to come from controllability.
5386520	5388020	A	0.8071815371513367	That would be my argument.
5391080	5393412	B	0.7669020295143127	There's a lot there.
5393546	5399816	B	0.8329864144325256	So a few directions first to our colleagues in reinforcement learning.
5399918	5407476	B	0.7174900770187378	RL, the paper is provocative in that it includes reward is not necessary.
5407668	5416190	B	0.8348484635353088	So is reward sufficient and or what is necessary for what?
5418640	5420716	A	0.821924090385437	Yeah, it's a good question.
5420898	5422780	A	0.835134744644165	Is reward sufficient?
5425140	5446912	A	0.8244130611419678	I would argue this I think that the reward enough hypothesis, which just to remind some of the viewers is the hypothesis that reward maximization can account for all sort of artificial and sort of natural intelligence.
5447056	5462456	A	0.8054270148277283	That all of the sort of features of intelligence, the sort of capabilities, the structure, learning and stuff can all sort of arise out of some need to some process of maximizing reward.
5462648	5481472	A	0.854525625705719	And from my standpoint that's a frustrating statement because one, it doesn't one it doesn't really address where your preferences for specific reward functions come from.
5481606	5509208	A	0.589013934135437	And in the paper they will say well, we acknowledge that there could be multiple sources of reward but the process of deciding on what reward you should attend to or care about that I think deciding what signals you should care about is an important part of intelligence itself and I think that reward is enough.
5509294	5522340	A	0.6806662678718567	Hypothesis as a hypothesis is sort of under constrained in that whatever that mechanism is to that perspective it's going to be maximizing reward under their paradigm.
5522520	5537780	A	0.80863356590271	And so therefore whatever shapes what that mechanism is that an agent should attend to this or an agent should stop attending to the things that's always cared about and attend to some new signal.
5540760	5560296	A	0.8202493190765381	I think that that forces you to sort of take the position that maybe there's, like, a meta reward that directs this process because all sort of attendant processes of intelligent systems sort of underlie the process of reward.
5560328	5567596	A	0.4989999234676361	Maximization and also the reward is enough.
5567778	5576924	A	0.651680052280426	Hypothesis is not being specific about it doesn't tell you what necessarily to compute.
5576972	5586320	A	0.6196491122245789	It just says that if you try to maximize reward, you will compute the right representations.
5586820	5595300	A	0.6147416830062866	And so I think that there's just a lot of nuts and bolts about what it takes to be able to reason in a flexible human way.
5595450	5606760	A	0.6057612299919128	And what I'm proposing here with the operator Bellman equations is to say hey, look, there's these reward free Bellman equations that help you deal with the complexity of the world, there's no reward in them.
5606910	5613848	A	0.8016524314880371	And you could make the case, well, maybe you could just use these operator Bellman equations to occupy states that are rewarding.
5614024	5634416	A	0.5460108518600464	But I would argue that since the effective product space that we all live in is so vast and we reason about it in such a flexible way, I very much doubt that RL will rise to the challenge of being able to justify motivations in real time in a way that humans can.
5634598	5646020	A	0.5867130160331726	And so to get to the question of is it sufficient, my hunch is no, I don't have a proof of reward is not sufficient.
5647660	5655320	A	0.7017793655395508	But I also think that the information that a reward function is supposed to carry about what is good.
5655470	5667676	A	0.6477940678596497	I don't think that is noble or computable on the timescales that we understand human intelligence to work at, to work on.
5667858	5682240	A	0.7362732291221619	And so, yeah, in order to answer your question about what is necessary, I don't know what is necessary, but I would just sort of make the point I already made of that.
5682310	5697620	A	0.7099478244781494	I think that we have to get to a point in which we sort of acknowledge the problems of product spaces and sort of reasoning dialectically in a product space that we can't explicitly represent.
5698860	5708568	A	0.5137768387794495	And so I don't know what is necessary, but I can say that it's not necessary for simple self preserving agents.
5708734	5710330	A	0.7273939847946167	And that's the claim of the paper.
5712320	5713070	B	0.9184247851371765	Awesome.
5714000	5714668	B	0.4896698594093323	All right.
5714754	5737124	B	0.824347198009491	In our closing segments, I'd like to take a journey to philosophy and then connect this back to potentially relationships between the models that you've presented here and active inference and maybe even walk to the edge of that cliff of the hybrid model.
5737242	5749770	B	0.8651413917541504	So Aristotle proposed four causes material cause, what something is made of the efficient cause, which is the source of change.
5750140	5758970	B	0.770529568195343	The formal cause is the essence, and the final cause is the teleology, the end goal of the object.
5759420	5771020	B	0.8828844428062439	And your presentations TeLEOS was Deacon's analysis of these different forms of teleology.
5771600	5781360	B	0.8775405883789062	And indeed, within the model proposed empowerment was that type of self referential teleology.
5783620	5797596	B	0.8661447167396545	When juxtaposing with active inference, and specifically the expected free energy functional, which has a lot of analogies with an operator, it's a function of other functions.
5797648	5813832	B	0.808479368686676	The expected free energy functional is predicated around helping the agent select policies that over expected futures, reduce their uncertainty the most about which sensations they receive.
5813976	5831068	B	0.8512661457061768	And that's what ties active inference closely with perceptual control theory that that expected free energy is ultimately looking at a divergence between preferences over observations and incoming observations.
5831244	5834680	B	0.8265746831893921	So that's kind of the sense side of the coin.
5834860	5842180	B	0.7600693702697754	It's like, I want to stay in the game to be able to align observations with my preferences.
5842680	5849610	B	0.8038668632507324	And I'm wondering if empowerment is the action side of the game.
5850300	5850920	A	0.6408694982528687	I agree.
5850990	5859652	B	0.7509109377861023	Saying you'll be involved with, yes, repeatedly sensing yourself to not be starving, not be dying of thirst, not be dying of cold.
5859716	5870504	B	0.8286241292953491	You'll be in your preference vector by way of this single value, which is the empowerment.
5870632	5885520	B	0.793313205242157	Whereas active inference kind of comes from the other side saying, you're going to end up having a lot of squares to move around in, but first you need to make sure that you're reducing divergence between your preferences and your expected observations.
5886900	5887760	A	0.6514029502868652	Mmhmm.
5891000	5892912	B	0.8841739892959595	What do you think about that map?
5892976	5898360	B	0.8393746018409729	Or where would that take these intertwined models?
5899500	5905044	A	0.801304280757904	Well, I think that there could be, like, an interface between the two concepts.
5905172	5920472	A	0.8145415782928467	If we consider that models of how things work, like composed models of how things work, could induce particular generative models that you would want to use in a sort of active inference setting.
5920616	5934770	A	0.8830265402793884	And that would be the sort of dual nature between the two that there's a cross talk between the proposal of some kind of generative model that would be conducive to the agent.
5935560	5946308	A	0.6362716555595398	And if it is, then it could be a good state encoding which feeds back in on controllability or empowerment and things like this.
5946474	5969916	A	0.7758342027664185	So, I mean, there's a lot to think about on this topic, but I guess I would just put it that way, that we still need to justify where generative models come from in new situations, for new theories of how things work and things like that.
5970098	5983280	A	0.8798532485961914	And there could be a dual process in which the action side, the internal controllability side is dictating what kinds of generative models that should be considered.
5987320	5989510	B	0.9324204325675964	Very interesting.
5989880	6003050	B	0.8221331238746643	And the reason I brought up Aristotle's causes was because active inference as a process theory seems to be describing that efficient cause.
6003660	6012876	B	0.7905614972114563	It's just especially with a variational free energy, which is kind of the real time version of the expected free energy.
6013058	6018920	B	0.47874805331230164	It's like one step at a time ball going downhill.
6019080	6031440	B	0.7615869641304016	And so variational inference is enabling incremental, unfolding optimization, again oriented around reducing that sensory preference and outcome.
6033300	6038964	B	0.8103602528572083	It does everything but specify a final cause.
6039162	6050280	B	0.8508504033088684	In a sense, one might say that there's a local final cause within the active inference generative model, which is like to reduce the divergence between the preferences and the observations.
6051180	6062552	B	0.8163710832595825	But the generative model also from the action selection side, which is what makes active inference active inference.
6062696	6073810	B	0.8760029673576355	It also needs a final cause in that self referential teleodynamic way.
6074260	6094900	B	0.8200929760932922	And so there could be some very interesting architectures where active inference picks up, where empowerment leads, like through a needle, because it's such a small representation with valence.
6095400	6118440	B	0.7096076011657715	And then one other kind of connection or maybe mapping between them is we've seen models of valence in active inference, such as the affective inference work, where valence was associated broadly with whether things are going better or worse than expected.
6118600	6129120	B	0.6769641637802124	In terms of statistical uncertainty, if you're reducing your uncertainty more than you expected, things are going better than expected and vice versa.
6130980	6154468	B	0.6575649380683899	So that is a very variance oriented valence concept where broader uncertainties are associated with inferior valence and tighter uncertainties are associated with positive valence.
6154564	6164648	B	0.7100958228111267	And it's just interesting that that's kind of like an orthogonal valence concept from how much you can actually do that's the actionable valence.
6164824	6179090	B	0.6145303249359131	Would you rather have a high precision around not being able to do anything, or high uncertainty about being able to do a lot or a huge amount?
6179780	6197940	B	0.7566794753074646	And so it almost seems like when we contrast those two, the direction that dominates is, in the final analysis, the ability to have empowerment, not necessarily to just have tight control over your observations.
6200140	6203464	A	0.5375386476516724	Yeah, I agree.
6203662	6207320	A	0.8677412867546082	Lots of interesting avenues for hybrid theories.
6212480	6216924	B	0.9600375890731812	Well, what a very interesting talk.
6217042	6231136	B	0.8662562370300293	I guess one more question on the model and then we'll close, which is the time horizon is it an infinite time horizon, or what is the treatment of time?
6231318	6236240	B	0.8636069297790527	And can time be continuous, or is time always discrete, and is it finite or infinite?
6238340	6243460	A	0.8624060153961182	A so the operator Bellman equations are formalized as a finite horizon.
6243960	6251128	A	0.7477846145629883	I suppose they could be extended to infinite horizon and infinite or continuous time.
6251214	6253256	A	0.6264052391052246	I suppose that's possible too.
6253438	6260120	A	0.8607524633407593	But as they're formalized now, it's discrete state, discrete time, finite horizon.
6262400	6268350	A	0.7803798317909241	But yeah, I think that there's alternative forms that could be made.
6272240	6274568	A	0.8840952515602112	Was there another question too, in it?
6274754	6275650	A	0.4924672245979309	Can't remember.
6278590	6281846	B	0.8763808608055115	Let us close with your time horizon.
6282038	6292780	B	0.9043143391609192	What are your next steps with the research, and how would you imagine an ecosystem of continuation of the work?
6294110	6319670	A	0.6204825639724731	Yeah, I'm interested in getting this work into computational neuroscience problem, because there's a lot of, I think, alternative models that need to be considered, especially given some of the themes that I've touched on in terms of justifying what to do in a product space, how do you represent control in a distributed system, et cetera.
6321130	6325538	A	0.7150444984436035	And so I think that there's a lot to do there in computational neuroscience.
6325634	6329500	A	0.5163551568984985	On the AI side, I want to put this in a world model.
6330750	6349374	A	0.5444453954696655	I want to get good auto encoders, kind of like how I had on the first slide, where you have a high dimensional agent in a high dimensional multimodal world, how do you have nice world models and put this in something like that?
6349412	6353982	A	0.7488926649093628	So that's the AI future direction, and I'd like to do both of them.
6354036	6357380	A	0.5226561427116394	So I have a lot of work to do.
6358470	6359026	B	0.9184247851371765	Awesome.
6359128	6368658	B	0.8316968083381653	Well, in closing, I'm just going to read some of the more statement like comments from the live chat, just so that they're included in the Active inference journal.
6368754	6386726	B	0.5247988104820251	So Dave Douglas wrote with regards to Deacon's consequence, galileo may have gotten in trouble less from insisting on a heliocentric universe than from insisting that purpose, value, and meaning be banished from science as connecting explanatory principles.
6386918	6407758	B	0.7749009132385254	The Galileo tolerated remaining connecting principle, causation may have reached the limits of its explanatory power sometime between Newton's and Fourier's day, where all respectable principles of explanation must ultimately rest on invocation of either rigid rods or on elastic bands.
6407934	6422150	B	0.6930682063102722	The Galilean program of Sola Casa attained its absurdity in Carnap's insistence that meaning, value, counterfactuals must be judged as strictly and literally as meaningless.
6422810	6434662	B	0.6345893740653992	Have we passed the point when the Galilean program must be simply abandoned, and meaning, value, and purpose must be restored to science as irreducible explanatory principles?
6434726	6448570	B	0.571296751499176	Alongside Causation, I find the mysticisms of both Bohm and Heisenberg's quantum completeness and of Powelly and Jung's synchronicity to just be too fluffy to be very useful.
6448730	6458430	B	0.8682286143302917	In a word, it has become a tradition of science as funded to insist that Causation, and Causation alone, must found our enterprise.
6458590	6462606	B	0.5084948539733887	This is a tradition of men, not a feature of ultimate reality.
6462718	6467080	B	0.7953973412513733	Meaning and value also have their place not reducible to cause.
6471370	6472134	A	0.9081892371177673	I like it.
6472172	6472760	A	0.5491447448730469	Yeah.
6473770	6474402	B	0.5708940029144287	Excellent.
6474466	6477202	B	0.9751759767532349	Well, Thomas, thank you again for joining.
6477266	6482490	B	0.9884186387062073	You're always welcome back and really looking forward to seeing how this all continues.
6483470	6484026	A	0.6959903836250305	Me too.
6484048	6485386	A	0.9352246522903442	And thanks again for having me.
6485408	6491260	A	0.990742027759552	I've been very impressed with how much work you do for the discord, and it's a great community.
6491710	6496346	A	0.993252694606781	Everyone's very nice and enthusiastic, so I was excited to see it.
6496368	6498554	A	0.7618674039840698	I just sort of randomly stumbled across it.
6498592	6502990	A	0.9654912352561951	So I'm glad I introduced myself to the discord.
6503070	6504100	A	0.9191112518310547	So thank you.
6504550	6505230	B	0.5708940029144287	Excellent.
6505310	6506658	B	0.49069198966026306	All right, till next time.
6506744	6507540	B	0.8529649972915649	Thank you.
6508070	6508530	A	0.7671424746513367	Great.
6508600	6509460	A	0.8529649972915649	Thank you.
6512390	6512670	A	0.5137447118759155	Bye.

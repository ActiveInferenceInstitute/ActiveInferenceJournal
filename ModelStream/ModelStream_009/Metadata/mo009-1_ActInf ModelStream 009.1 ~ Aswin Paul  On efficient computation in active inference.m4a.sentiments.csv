start	end	speaker	sentiment	confidence	text
7210	8750	A	0.9103710651397705	Hello and welcome, everyone.
8900	12446	A	0.8560454249382019	It's July 15, 2023.
12628	18890	A	0.8475398421287537	We're here in active inference model stream number 9.1 with Aspen Paul.
19050	26154	A	0.6193874478340149	Today we're going to have a presentation and a discussion on efficient computation in active inference.
26202	31346	A	0.7056436538696289	So if you're watching along live, please feel free to add comments or questions in the chat.
31458	35510	A	0.976978600025177	Otherwise, Aswin, thanks so much for joining today.
35660	37720	A	0.983913242816925	Really looking forward to your talk.
38810	39666	B	0.8880262970924377	Thank you, Daniel.
39698	40662	B	0.9793016910552979	Thank you so much.
40796	44966	B	0.675969123840332	So, as mentioned today, I'm here to talk about efficient computation and active inference.
44998	46460	B	0.5686778426170349	And yeah, let's get started.
46830	52846	B	0.8717503547668457	So we are all familiar with this idea of the free energy principle, which is also known as active inference, right?
52948	61722	B	0.8658371567726135	So the central concept is that an agent minimizes entropy of its observation to maintain homeostasis or survive in its environment.
61866	66194	B	0.7450945377349854	And here the entropy is defined in the information theoretic sense, right?
66232	82102	B	0.7158332467079163	So if an observation is highly probabilistic, that is less entropic or less surprising because it was high probability and we were expecting it, that's the idea, then that's the base that we build this framework of active inference on.
82236	95622	B	0.8447645902633667	And this idea of Marco blanket, it gives us a systematic way of surprising or systematic way of separating an agent from its environment and model purposeful behavior.
95766	96266	B	0.7360090017318726	Right?
96368	99366	B	0.7499771118164062	So let's focus on this idea of minimizing entropy.
99478	105850	B	0.834094226360321	So how does an agent minimize entropy or know which observation is highly probabilistic and vice versa?
106010	108718	B	0.8145608305931091	So that is by maintaining a generative model.
108804	120174	B	0.7804516553878784	And the generative model is basically a toy model of the environment which the agent builds in its brain and that is built using only the observation that it gets from the environment.
120222	124654	B	0.517269492149353	So it has no access to the real states or the hidden states of the environment.
124702	126178	B	0.7455383539199829	It's building the toy model.
126344	136466	B	0.8147776126861572	And given this toy model, it has scope or ability to compute the probability of an observation and hence try to minimize the entropy.
136498	136982	B	0.7360090017318726	Right?
137116	139642	B	0.6967025399208069	So that's the idea.
139696	144678	B	0.7139674425125122	But it has a problem of cursive dimensionality.
144774	155594	B	0.6135287284851074	Like, given a generative model, it may not be possible always to calculate or marginalize the probability of observations out of it because the state space can quickly become intractable.
155722	161018	B	0.8349366188049316	But the idea is that you define an upper bound on the surprise using Jensen's inequality.
161114	170258	B	0.8914221525192261	And you may also define a new term called Q, which is the hidden belief or the belief about the hidden states.
170424	173666	B	0.8763313293457031	And this Q is going to be the focus of decision making, right?
173688	182274	B	0.8081422448158264	So if you have a noisy queue and you have no idea what's in the environment, then you can't make or hope to make decisions to control that environment.
182402	188034	B	0.719040036201477	And it is this belief about the hidden states that you use and that becomes useful to take decisions.
188082	191738	B	0.8305879831314087	And this whole quantity is of course called the free energy.
191904	197530	B	0.8209874629974365	And the variational free energy F can be interpreted in multiple ways.
197600	208334	B	0.7493218183517456	So the first or the most common is the machine learning way of how it is trying to minimize the complexity of the model at the same time trying to maximize the accuracy of it.
208372	211662	B	0.8465107083320618	So that's the machine learning interpretation of minimizing various free energy.
211796	223534	B	0.8462451100349426	You may also try to interpret free energy in the physics term where you at the same time try to minimize the energy of your model, but at the same time trying to maximize the entropy.
223582	233810	B	0.8688357472419739	But the focus of today or decision making is always on this belief that you have after you do the perception in active inference.
233970	236662	B	0.8312475085258484	So how do you do vanilla decision making?
236716	241446	B	0.9038098454475403	Or what is the most discussed idea of decision making in classical active inference?
241638	252686	B	0.8723047971725464	So, if you are in an environment and if you're an agent who's trying to make decisions, then you have a space of available actions, right?
252708	258240	B	0.8218607902526855	So in this toy model, you have three available actions run, jump or stay.
258690	270046	B	0.8209115862846375	And given these actions, you can hope to define a policy pi, small pi, which is a sequence of actions in time and capital T is the time horizon of planning.
270078	273700	B	0.7400153279304504	Or that's the length of your policy.
274070	278482	B	0.7811729907989502	And you have a policy space with many such policies, small pi.
278546	288674	B	0.8177523016929626	So given this bigger small C space, you can attempt to evaluate the expected free energy based on the beliefs that you accumulated.
288722	290698	B	0.662584125995636	So here you are not minimizing anything.
290784	302438	B	0.7966422438621521	You already have a belief from variation free energy and you are just calculating or evaluating the expected free energy corresponding to many small policies that you can define.
302614	308414	B	0.8359617590904236	And after you evaluate it for all policies, then you know which is the optimal policy to take.
308452	312366	B	0.8113131523132324	And that's the classical active inference idea of decision making, right?
312548	319262	B	0.8929458856582642	And this expected free energy is really useful in the sense that it is goal directed.
319326	326882	B	0.7788136601448059	So the risk term is goal directed and then you have this expected ambiguity term that forces you to explore as well.
327016	339570	B	0.6466220021247864	But there is a problem that this policy space can quickly turn interactable and that has always stayed as a bottleneck in scaling active inference to commonly seen environments.
339730	342422	B	0.5891984105110168	But let's see how it becomes quickly intractable.
342486	347466	B	0.8798739910125732	So how many policies can be defined, say, for a time horizon of 15, right?
347488	353142	B	0.8282817602157593	So if you are playing, say, supermago, you might want to plan at least say, ten times steps ahead.
353296	367038	B	0.8921259045600891	So the first policy could be the same action run, stacked for 15 timesteps and you can basically define several combinations of such actions.
367214	370894	B	0.8699444532394409	And the policy space is simply intractable.
370942	377874	B	0.5848548412322998	It becomes too huge for you to evaluate the expected free energy for all such policies.
378002	390082	B	0.4991023540496826	And in a stochastic problem setting where the environment itself is noisy, you don't really have a method to choose a subset of this policy space and do classical active inference.
390146	392602	B	0.822634756565094	And this is clearly a computational intractable problem.
392656	400390	B	0.7683476209640503	And that's why in literature we always see small grids or small environments when you discuss decision making in active inference.
400550	406906	B	0.8422543406486511	But recently, a new idea was proposed, which is called the sophisticated inference.
407018	410990	B	0.5802168250083923	And in sophisticated inference, it is not really the policy space.
411140	416290	B	0.6688380241394043	You are actually real time trying to think what to do.
416360	422674	B	0.8836920857429504	So if you have a belief, then you are trying to evaluate the actions based on that.
422712	428146	B	0.8068265318870544	So here we don't have a sequence of policies or things that becomes intractable.
428338	448122	B	0.893512487411499	Here we are doing basically tree search in the sense that you are trying to evaluate expected free energy of this joint distribution of actions and observations and to evaluate the expected free energy at some time small T, you will also need the expected free energy at the next time step.
448176	452426	B	0.8652377724647522	And to evaluate that, you will need the expected free energy of time T plus two.
452448	459290	B	0.8334357738494873	And this basically becomes a tree search that rolls out in time and it's a recursive relation.
459450	466474	B	0.6493793725967407	And here it's fundamentally different from the policy space that we saw in the last slide, right?
466612	472962	B	0.6726055145263672	And is it computationally better in the sense that if you have to plan, say, ten times steps ahead?
473096	474398	B	0.7675833106040955	Comprehensively?
474574	484946	B	0.8371530175209045	In classical active inference, we saw that the policy spaces, the cardinality of the action space raised to t, so that is the computational bottleneck.
484978	489086	B	0.7998781800270081	If you have to consider all possibilities.
489218	495958	B	0.8728958964347839	But in sophisticated inference, it's even worse, because you are considering combination of state and actions.
496054	498620	B	0.9211153984069824	So it is computationally worse in fact.
499070	513194	B	0.5625883936882019	But a solution was proposed in the sophisticated inference paper that we can do pruning of this research, that we can avoid some states and actions when you do this research and that becomes computationally very tractable.
513322	517246	B	0.8827110528945923	So let's see how pruning works in sophisticated inference.
517358	532342	B	0.7217400074005127	So this grid was discussed in the original sophisticated inference paper and say, for this grid, given that you have this kind of prior preference distribution, so this white square, which is the gold state, is the most preferred state.
532396	553050	B	0.8093588352203369	And you have a uniformly decreasing preference for states that is far away from that goal state, then basically if you observe yourself at some observation at Time T, then basically what you're doing is you're considering the consequence of available actions from that observations.
553210	565374	B	0.5982269644737244	And basically you can use the projection of your belief and set a threshold for those beliefs to kind of maybe ignore some actions and ignore some observations.
565502	577494	B	0.49537786841392517	And what you will find is that it's no longer a tree search, it's a subset of tree search and it's computationally way efficient than doing the whole tree search, right?
577532	583318	B	0.5235161781311035	So here you have avoided a lot of combinations and this becomes a computationally tractable problem.
583484	588434	B	0.6140664219856262	This is because you have decided to avoid some actions and observations.
588482	598986	B	0.6608825922012329	So you are essentially compromising on that possibilities which might give you a higher reward or which might be more optimal than the consequence of this partial research.
599168	600778	B	0.5758618116378784	But this works.
600864	605294	B	0.821708083152771	So this simulation was presented in the paper and it works.
605332	616100	B	0.8811970949172974	The agent kind of learns to do what needs to be done if it plans in this forward direction of planning in a pruned way.
619190	628106	B	0.555885374546051	And basically you can computationally show that even for a small search threshold, you can drastically decrease the computational complexity.
628158	637640	B	0.5243263840675354	So if you decide to do the whole research with the planning depth, the computational time is exponential and quickly you can't do much about it.
638010	645366	B	0.6052151918411255	But if you decide a search threshold which is even very small, you can see that this problem becomes computationally attractable.
645558	654240	B	0.6227855086326599	And this demo on sophisticated inferences available in my version of PMDP and it's going to be integrated to the original PMDP soon.
656690	674994	B	0.8007314801216125	But the key point is that the pruning of the tree search requires us a well informed prior preference like this in the sense that here the agent is aware of how desirable are neighboring states, it not only knows about the final goal state, it also knows about its neighboring state.
675112	686098	B	0.5965169668197632	And given such a prior preference, we can see that for a planning depth of three, the agent is getting stuck basically in this local maxima of prior preference.
686194	696218	B	0.6783549189567566	But with sufficient planning depth it is able to overcome this barrier and reach the goal state in in this great problem.
696384	701786	B	0.7629250288009644	And the question is that what if the agent only knows this final state?
701888	704846	B	0.639946460723877	It has no other knowledge about what to do.
704948	707550	B	0.7492899298667908	And in this case, what does the agent do?
707620	722770	B	0.6753676533699036	So this is the problem that we are trying to address and in the absence of a meaningful trip reference like this, the agent has basically no way of reaching the goal state other than by random exploration.
723110	730642	B	0.860001802444458	It has no way of planning because it cannot do planning eight times steps ahead, because it's computationally intractable.
730786	735160	B	0.8929520845413208	If it chooses to do the full tree search, right?
738970	756026	B	0.7843687534332275	For a grid like this, what do you do if you get a sparse trip reference which is not well informed and as highlighted in the previous slide, your research is now blind, you have no way of pruning the tree search, and you have to do the full tree search in this scenario.
756218	760462	B	0.801145076751709	So, as you might have thought, there are two solutions to this.
760596	776498	B	0.8642383217811584	Either you have to find out a way to do the full depth planning with this past prior preference or you have to learn a meaningful prior preference which will enable you to do this pruned research.
776664	783606	B	0.7455937266349792	So we are going to discuss these two solutions in this presentation for a given scenario like this.
783788	789314	B	0.7336844801902771	So, the first solution to do full research is basically using dynamic programming.
789442	797398	B	0.6238426566123962	And dynamic programming is a well known idea in operation research and industrial engineering and many branches of engineering.
797494	809886	B	0.7206634879112244	And the basic idea is that you solve the subparts of a bigger problem first and then later try to integrate the solutions of these sub problems to do optimal decision making.
810068	817502	B	0.8428270816802979	So in this scenario, imagine that you're trying to plan for the last action.
817566	824740	B	0.8861736059188843	So earlier we started from the first action, we started from the present and tried to predict what is happening in the future.
825110	828146	B	0.8767579793930054	So your direction of planning was basically forward in time.
828248	836902	B	0.8025956153869629	But imagine that you are trying to plan only for the last time step where you are just near the goal state and right going to that goal state.
836956	837126	B	0.5664746165275574	Right.
837148	856906	B	0.8852315545082092	So you are trying to make a decision for that last time step, which is capital T minus one, and your projections to the next time step or the last goal state can be done because you have access to this model of the world using this transient dynamics or the B matrix in active inference.
857018	870418	B	0.8516346216201782	So for this single time step, which is a sub problem, you are actually evaluating a table of expected free energy that tells you if you are in, say, this observation, what is to be done, which is for the last time step.
870584	875666	B	0.8754444122314453	And basically you can do this in state space or observation space.
875768	882822	B	0.8143805861473083	So this can be done using the A matrix and B matrix together, where you can do planning either way.
882876	883382	B	0.5664746165275574	Right.
883516	893770	B	0.7632855772972107	So the question is that if you know what to do in the last time steps, that you might be thinking, how do I know that I'm in the last time step?
893920	896250	B	0.6959555149078369	It is all about imagining.
909720	915080	A	0.5478458404541016	Sorry, last thing we heard for a second was it's all about imagining.
915580	916280	B	0.5491447448730469	Yeah.
916430	917624	A	0.6829121708869934	So just pick up from there.
917662	919080	A	0.6767829060554504	It's all about imagining.
919980	922088	B	0.5551509261131287	Okay, so there was a connection issue.
922174	923316	A	0.714516818523407	Yeah, just for a few seconds.
923348	924570	A	0.7533572316169739	It's all good then now.
925100	926730	B	0.7236750721931458	Oh, sorry about that.
928140	939096	B	0.8356073498725891	So what I'm trying to say is that you are trying to imagine what you will do if you are in time capital T minus one, which is the last time step for your planning horizon.
939208	941504	B	0.8424948453903198	And if you are in that time step, what do I do?
941542	950596	B	0.8463101983070374	So this table represents all such scenarios that if I am, say, at observation three, at time T minus one, what do I do?
950618	960292	B	0.854515016078949	And this quantity here, I'm only considering the risk term or the purposeful term, and this term represents that policy.
960426	965704	B	0.7500932812690735	And what if I do this backwards till time T minus one?
965742	974760	B	0.8780611157417297	So if I know what to do at time capital T minus one, then this table can inform what to do at capital T minus two.
974830	983544	B	0.5726391673088074	So rather than planning forward in time, what I'm doing is basically stacking many tables together just by fixing a capital T of planning.
983672	993408	B	0.7588467597961426	And given that I have all such stacked tables, then basically what I can do is use them to take decisions forward in time.
993494	1005180	B	0.5046101808547974	And what we have observed is that this idea works, that I can calculate the expected free energy backwards step by step, considering them as sub problems.
1005350	1019256	B	0.5430486798286438	And the fundamental difference is that in sophisticated inference to calculate the expected free energy at time small T, you don't know what is the expected free energy at time T plus one.
1019358	1026968	B	0.8657590746879578	So this becomes a research so that you have to calculate this first and to calculate for T plus one you need t plus two and so on.
1027054	1029676	B	0.6737411022186279	But here because you're calculating it backwards in time.
1029698	1033916	B	0.8541483283042908	You already know what is the expected free energy for T plus one.
1033938	1036584	B	0.6554574966430664	And it's basically the same equation.
1036632	1043356	B	0.5964082479476929	It's just that you're doing it backwards in time and pictorially in sophisticated inference.
1043388	1044832	B	0.7924026846885681	You are trying to do a tree search.
1044966	1050728	B	0.5232974290847778	But in the dynamic programming algorithm, you are doing your planning backwards, using tables.
1050844	1061300	B	0.5644193291664124	And given your planning horizon is sufficient enough for a problem, what we have seen is that the agent will be able to take optimal actions forward in time.
1061450	1078620	B	0.5085175037384033	So in the paper we are also proposing one algorithm for sequential palm DPS using this backward planning in time and we were able to scale up simulations for grid spaces which was previously intractable without neural networks.
1079040	1080792	B	0.757863461971283	So that was the first solution.
1080936	1099836	B	0.6952788233757019	So the second solution is that so in the first solution it was fixed that you only get a sparse prior preference, you don't get any other information, you only get your information about the final goal state and all you get is the model of the environment which you learn and you basically have to take decisions.
1099948	1112996	B	0.7926443815231323	But the second solution of course is that if you're allowed, you can attempt to learn a pride preference which is meaningful like the one we saw in the previous slides, which has the information about the other states also.
1113098	1114264	B	0.7241279482841492	But how do you learn it?
1114302	1114504	B	0.7360090017318726	Right?
1114542	1129020	B	0.7036324739456177	So there is a seminal work from optimal control literature that talks about efficient computation of optimal actions and in that work there is a quantity similar to our prior preference which is called the Desirability function.
1129090	1134216	B	0.7017233967781067	So for example, in this grid world here darker colors are more preferred.
1134248	1147440	B	0.7200635671615601	So if this crosses your final gold state, what the agent in this paper is trying to do or the said learning method in this paper is trying to do, is learn this Desirability function as optimally as possible.
1147590	1163616	B	0.8077375292778015	And what has been shown in this paper is that if you try to learn the Desirability function using a particular learning rule, it is computationally far more efficient than even Q learning which is a well known reinforcement learning algorithm.
1163648	1179804	B	0.5731008648872375	So Q learning is a well known computationally optimal algorithm but in this paper that particular learning rule for learning this kind of a Desirability function is way faster and this approximate error represents how different it is from the optimal Desirability function.
1179922	1191584	B	0.6713584661483765	So this Desirability function is nothing but our prior preference and as mentioned learning set is borders of magnitude efficient than learning the Q function in Q learning.
1191782	1212608	B	0.7494405508041382	So this is the particular learning rule depending upon the reward that it gets from the environment and in this particular grid world environment we are only giving it reward at the last step which is similar to the sparse parent preference, the agent basically gets no reward until it reaches that final goal state.
1212794	1221560	B	0.8675673007965088	And with this learning rule which has a parameter ETA that controls how fast or slow this learning happens.
1221710	1239384	B	0.7376750111579895	So basically we tried to study the effect of this learning parameter ETA, but what we observed is that it's very robust, it can learn the prior preference reliably even with variable values of this learning parameter.
1239512	1258112	B	0.6651008129119873	And what we see is that the agent is able to learn a meaningful prior preference over time very fast and using such a meaningful prior preference, then the agent don't have to plan a lot, it can manage optimal behavior or purposeful behavior with very low time horizons of planning.
1258256	1283800	B	0.5323486924171448	And given these two solutions, we could scale up active inference algorithm for decision making and talking about the computational efficiency, we saw that the dynamic programming method could plan 30 timesteps into the future with only say a computational complexity of thousand when compared to say ten to the power 68 for sophisticated inference.
1283880	1298592	B	0.6093966960906982	And the second method which learns the prior preference which we call active inference and it only needs to plan one time step ahead, so it is way more computationally efficient in that sense, but it has to learn.
1298646	1315464	B	0.6400153040885925	So in the Dpsv method we are not learning, we are using the sparse sprite preference and doing the whole depth of planning and in the other method we are letting the agent learn this prior preference but then we can save planning a lot because it knows a lot of what to do in time, right?
1315662	1332540	B	0.9125099182128906	So graphically we can see that the Dpfe method is really computationally efficient and when plotted against time in the AI t equal to one method, it is basically computationally way cheaper.
1333840	1343404	B	0.9639466404914856	So it scales very fascinatingly well with higher and higher complexity of the environment.
1343532	1359700	B	0.8782070279121399	So we tested these methods in very huge state spaces like say 900 states compared to state spaces which has dimension of say five or ten in the usually seen active inference literature.
1360200	1380856	B	0.7197500467300415	So I want to kind of emphasize that we are not using any neural networks here, we are using explainable active inference agents doing all necessary matrix multiplication so that we have access and explainability to every computation that is taking place in these algorithms.
1380968	1386012	B	0.6238594651222229	And when tested on these grids first, we validated this on a smaller grid with hundred states.
1386146	1396476	B	0.8628683090209961	And we observed is that when compared to benchmark reinforcement learning algorithms like Q learning and Dynaq, dynaq is a model based reinforcement learning algorithm.
1396588	1408096	B	0.3971956968307495	And we compared our newly proposed agents, which is Dpfe and AIF, and we saw really good performance and AIF agent is slightly worse.
1408128	1410644	B	0.7767184972763062	And that's because it's only planning one time step ahead, right?
1410682	1418660	B	0.6481245160102844	But the Dpfe agent who does full time planning, it's performing as good as benchmark reinforcement learning algorithms.
1418820	1422740	B	0.8270043730735779	And we tested it with bigger and bigger grids.
1422820	1435612	B	0.8761627078056335	And when we introduced Stochasticity in the goal state, in the sense that when the agent had to navigate to Stochastic goal state.
1435666	1438968	B	0.8574954867362976	So we changed the goal states at every ten episodes.
1439064	1446464	B	0.7075963020324707	And what we observed is that the Dynaq took more time than our Dpfe agent to kind of recover and do well.
1446502	1459716	B	0.878392219543457	So Dpfe agent in this Stochastic environment performed really well, even better than the Dynaq agent, and we could also see that the AIF agent is recovering faster but not as good as the other agents.
1459898	1464452	B	0.8710625767707825	So basically that was the result in the paper and the methods.
1464596	1468680	B	0.9280198216438293	So, yeah, thank you for listening, and I'm open for discussions.
1472210	1472878	A	0.4896698594093323	All right.
1472964	1473600	A	0.9184247851371765	Awesome.
1474770	1475520	A	0.7093686461448669	Wow.
1476390	1477460	A	0.9677404165267944	Very cool.
1478630	1479538	B	0.8529649972915649	Thank you.
1479704	1491270	A	0.894396185874939	Okay, well, just as we start the discussion, if anyone wants to add anything first, just how did you come to work on this project?
1491420	1501510	A	0.79347163438797	Were you studying active inference and you came to this question as being interesting, or were you working on the planning and came to active inference as a method?
1502750	1505450	B	0.8501497507095337	Yeah, so little bit background about myself.
1505520	1521866	B	0.5767068266868591	So I studied physics both in my undergrad and postgrad, and towards the end of my post graduation, I got interested in things like game theory and reinforcement learning, and I joined for a joint PhD with Professor Adela and Professor Manoj.
1521898	1526678	B	0.8936494588851929	And so Professor Manoj is a control theory person, and Adela is a neuroscientist.
1526874	1536418	B	0.5658182501792908	And in the beginning of my PhD, I started reading active inference literature, and I wanted to implement that in problems.
1536504	1546642	B	0.7658700346946716	And I was always fascinated with explainable active inference about this idea of expected free energy, trying to minimize risk as well as expected ambiguity.
1546706	1554182	B	0.8630043864250183	Just so not just making the agents work, but also being able to tell how they work is what fascinated me in active inference.
1554326	1565386	B	0.6378825902938843	And initially, I tried to implement them, and there is a conference paper which we published in which we compared it for a similar grid world task.
1565418	1572110	B	0.6936355829238892	And then I faced this problem of scaling active inference, and then I started working on sophisticated inference.
1572530	1578718	B	0.8691686987876892	Yeah, so basically these methods came out of the need that I wanted to scale the map up.
1578884	1592678	B	0.54306960105896	And I always had was skeptical in using deep active inference because I didn't want to use neural networks to do planning, because deep reinforcement learning in itself is a huge field.
1592764	1600280	B	0.8451843857765198	And if you're about just scaling active inference, then maybe just do deep reinforcement learning.
1600650	1602146	B	0.8091376423835754	That's what I thought.
1602268	1602554	B	0.5491447448730469	Yeah.
1602592	1607500	B	0.8623841404914856	So that's basically the background, and that's how it came about.
1608670	1616266	A	0.6437994837760925	All right, I'll go first to a question in the live chat, and we'll just probably discuss various aspects because there was a lot in your presentation.
1616378	1618314	A	0.6170855164527893	So ML.
1618362	1631700	A	0.8468910455703735	Don writes, I wonder, when it comes to computing the expected free energy over a time horizon, what kind of mean field approximation was used to factorize Q of S?
1633110	1633860	B	0.4753468334674835	Sure.
1639920	1650690	B	0.8683586120605469	So I assume that the question is about the expected free energy in Dpfe, and to calculate this, basically.
1652020	1663488	B	0.8807409405708313	So in my simulations, I use belief propagation for this belief cube and you can also use variation message passing or marginal message passing.
1663504	1664630	B	0.59861159324646	That's not a problem.
1668040	1671864	B	0.808164656162262	Once you have this belief queue, what do you do?
1671902	1672490	B	0.7360090017318726	Right?
1677100	1677512	B	0.5491447448730469	Yeah.
1677566	1687032	B	0.8509001135826111	So when I say you imagine Q for a time, then what I use mostly is one hot vector.
1687096	1694420	B	0.8915476202964783	So to take decisions, you use the belief that you get out of your perception step in active inference.
1694520	1699292	B	0.7291799187660217	But for imagining, these are hard tables where they are one hot vector.
1699356	1704450	B	0.8957300782203674	So given, say, in your generative model has ten states.
1705700	1711484	B	0.7992919087409973	The queues that you use are precise queues for planning.
1711612	1717972	B	0.8139606714248657	But for decision making you use the imprecise mean field approximation queue that you get from the perception step.
1718106	1729800	B	0.714381992816925	So I don't know if that answers the question, but maybe I also need to think about more about the approximation that may be there in steps.
1730860	1731368	A	0.84200119972229	Cool.
1731454	1734830	A	0.760854959487915	Yeah, they can write more if they want.
1735360	1739790	A	0.8150939345359802	Let's talk a little bit more generally about preference learning.
1740400	1751168	A	0.6008514761924744	So in the context of the active inference generative model, we have a mediating between observations and hidden states and learning makes a lot of sense.
1751254	1755024	A	0.7930096387863159	It's about learning the mapping between observations and hidden states of the world.
1755142	1761156	A	0.8253647685050964	And then we have B learning, learning the consequences of action and how things change through time.
1761338	1764870	A	0.7076330184936523	And preference learning is learning on C.
1765400	1773910	A	0.9255496263504028	And you've highlighted that this is a very interesting variable to learn.
1774280	1779930	A	0.7713848352432251	And I'm curious, how do we learn to learn the right thing?
1780300	1783900	A	0.8567624092102051	How do we know that we're learning in adaptive preference?
1784400	1790380	A	0.7402629852294922	And then how does that preference learning reduce cognitive overhead or computational complexity?
1790880	1792670	B	0.8782029747962952	Yeah, great.
1793200	1803772	B	0.8454059362411499	So if you are trying to learn a prior preference, then that assumes that there is something to chase or there is something to maximize, like a reward.
1803916	1811040	B	0.7749542593955994	So in a reinforcement learning setting, there is clear reward that's coming from the environment that you're trying to maximize.
1811200	1815830	B	0.8267286419868469	And say for this grid, you get that reward only in the last step.
1817160	1818692	B	0.9296877384185791	That's what makes this problem hard.
1818746	1828872	B	0.7039039134979248	So if you are getting rewards at every time step, then basically you know what to do that I just have to pursue that reward at every time step.
1828926	1829144	B	0.7360090017318726	Right?
1829182	1837788	B	0.6227104067802429	But here you might have to take say, 15 time steps ahead to get that one reward and that basically is hard to do.
1837954	1848232	B	0.7927587032318115	So here, because I'm trying to learn a prior preference, there should be a reward structure that exists in the environment.
1848376	1855884	B	0.6240107417106628	If the environment doesn't care about what I do or if I can't define what is good or bad, then there is no meaning in learning the prior preference.
1855932	1856096	B	0.7360090017318726	Right?
1856118	1860752	B	0.8736441731452942	So here the reward is what controls the learning of this prior preference.
1860896	1875128	B	0.8641704320907593	And the good thing is that even if only I get the reward at the last time step, I have my B matrix and I have experience of transitioning from different states, which I reached this final goal state.
1875294	1884924	B	0.8735838532447815	And this algorithm that we are using, or the learning rule that we are using, is precisely learning a similar thing in optimal control.
1885042	1888092	B	0.8753665685653687	That paper I introduced which is called the desirability function.
1888226	1899920	B	0.6730203628540039	And given that you have this desirability function so imagine that maybe I'm starting from this state, then I just have to look at my nearest neighbors to take a decision.
1900660	1912148	B	0.5130448341369629	If the state at state below the state is more preferred, then I only have to plan one times ahead and that's the optimal decision I have to take.
1912314	1921624	B	0.5187969207763672	So learning this prior preference reduces the cognitive load in the sense that I'm only now looking at nearest neighbors, I don't have to plan all the way up to this final goal state.
1921822	1935260	B	0.7524005770683289	And I will learn this as efficiently as possible because one, it is a guaranteed algorithm, we tested its robustness and it is also informed by the reward that I get from the environment.
1937040	1948080	B	0.743834912776947	If there is some way to kind of define what is preferred then you are guaranteed to learn a pride preference that's meaningful by this algorithm.
1948580	1961460	A	0.6265448331832886	So if only nearest neighbors and one time step inference is being used, then what prevents this kind of preference learning agent from being stuck in a local optima?
1963880	1967972	B	0.6573700904846191	Yeah, so there won't be local optimas here.
1968106	1969270	B	0.6341672539710999	That's the point.
1970220	1976360	B	0.5688283443450928	Why would there be a local optima if I am learning it using rewards?
1977500	1977864	B	0.5491447448730469	Yeah.
1977902	1982948	B	0.5992668271064758	So if there is local optima, then it will be getting stuck in the local optima.
1983044	1993176	B	0.7153914570808411	But there won't be one if you're learning us this way because you're learning it from your own experience and how you got the reward at time sometime.
1993288	2003200	B	0.6075361967086792	And what we observed is that it's very gradual and there is no glitches or local optimus when you learn such a practical.
2003860	2016340	A	0.7932447195053101	So it's kind of backfilling a preference so that there can be a smooth path to the distal goal.
2016840	2017300	B	0.46103888750076294	Yes.
2017370	2027624	B	0.7291123867034912	So the animation kind of makes it look like a backfilling but you're actually learning it from your experience forward in time.
2027662	2027864	B	0.5664746165275574	Right.
2027902	2032104	B	0.7892561554908752	So I observed a reward when I transition from this state to this state.
2032142	2033496	B	0.9768200516700745	So this must be good.
2033678	2039212	B	0.6547467708587646	Then in the next time step I say, okay, this state is responsible for taking me there.
2039266	2042892	B	0.48175838589668274	So this might be good also, but not as good as the other one.
2043026	2050530	B	0.7076514363288879	So it looks like it's learning backwards, but it's actually learning from the real t to T plus one experience.
2053540	2054290	A	0.584351658821106	Okay.
2055300	2064516	B	0.5811576843261719	So in that way you can't have local maximas because this is the learning rule that does that.
2064618	2067060	B	0.5511932373046875	I don't know how to answer it more systematically.
2068200	2068950	A	0.5491447448730469	Yeah.
2070120	2078650	A	0.8568721413612366	What real world situations or what real life situations do you kind of see this kind of preference learning happening in?
2079900	2082008	B	0.7677177786827087	Yeah, great question.
2082094	2090700	B	0.8475553393363953	So, so there is one paper that came out from our lab where neurons are laying learning the game of poem.
2092000	2095656	B	0.9191559553146362	So it's a paper is called Dish Brain and the device is called Dish.
2095688	2095988	B	0.6241250038146973	Brain.
2096024	2116928	B	0.64522784948349	And what they have accomplished is that they managed to culture neurons on a silicon chip and they gave it a good feedback signal when it successfully tackled the ball or played the game well and gave it a shock when it made mistakes.
2117024	2123720	B	0.7878791093826294	And what we saw is that in the paper, what we see is that it learns to play the game over time.
2123790	2124408	B	0.5664746165275574	Right.
2124574	2136924	B	0.6423386335372925	And in such a scenario, imagine that if I encounter, say, a positive thing in the world, then I might associate what is good with the states I observed in the past and so on.
2136962	2137164	B	0.5664746165275574	Right.
2137202	2144752	B	0.5185514092445374	So I learned step by step what is good or bad, and that's a reasonable hypothesis to make.
2144806	2155776	B	0.7543988227844238	So if I want to say, get fitter, then I might associate going to gym as a good thing.
2155878	2159364	B	0.8472737073898315	Then I might also associate walking to the gym as a good thing.
2159482	2162564	B	0.5092523694038391	Then I might associate, say, wearing my shoes as a good thing.
2162602	2168308	B	0.7225152254104614	So learning prior preference that way makes sense, I would say.
2168394	2175444	B	0.500654399394989	But yeah, this is a computational solution to the problem of cursive dimensionality.
2175492	2180024	B	0.6518123149871826	But testing this on the real world is definitely what we should do.
2180062	2184670	B	0.914944052696228	And yeah, I also look forward to doing that.
2185840	2191340	A	0.8716018199920654	Okay, let me try to run a real world situation by you and see if this connects.
2192240	2199600	A	0.7480460405349731	So we're rewarded by turning in an essay that gets a high grade.
2199940	2219460	A	0.6444093585014343	And so there's many steps between starting the idea of the essay and seeing that preferred sparse outcome, which is the grade, and we do all kinds of things, and then we learn, okay, well, this one where I did get a good grade, it was well formatted.
2219960	2228772	A	0.8381243348121643	And then now you sort of expand the umbrella of your preference out, and then you go, well, what brought me to getting it well formatted?
2228836	2230440	A	0.7415509223937988	Well, I did it on time.
2230590	2240860	A	0.847942590713501	And then you kind of work all the way to the ideation phase so that in the future you can do one step inference exactly.
2240930	2252560	A	0.7806158065795898	And just take it step by step as a skill, instead of needing to do a 15 time step inference over all the possible tree structures.
2252900	2260484	A	0.7665061950683594	So you've kind of used your embodied learnings to simplify the structure of the problem.
2260682	2262150	B	0.5801403522491455	Exactly, yeah.
2263560	2269430	A	0.789290189743042	And can we look at the computational complexities again of all them?
2271260	2275396	B	0.899807333946228	So here, CIF stands for the classical active inference.
2275508	2280324	B	0.9167729020118713	And to do the full horizon of planning, it's around ten to the Power 18.
2280372	2288040	B	0.9259865880012512	So this is for this particular grid example with hundred states and four available actions.
2288200	2293384	B	0.8203873038291931	So this is for a special case, and I just wanted to put numbers to put things in perspective.
2293512	2312580	B	0.8453022837638855	So for this particular example, if you are trying to do it with the policy space thing, you will have to do ten to the Power 18 computations for one step of planning or in one instance, in sophisticated inference, it's even worse because the state space also matters.
2313000	2315060	B	0.8653162717819214	And that won't work.
2315130	2326308	B	0.5869113802909851	So this third line was supposed to kind of show that even for a time horizon of two, it's really hard to do sophisticated inference if you have to do the full planning.
2326484	2337416	B	0.6369155049324036	But with dynamic programming, when you're planning backwards, you can attempt to do the full horizon of planning and that's only 1000 computations.
2337608	2343150	B	0.611824095249176	But with learning prior preference, it's even low when you do it just one time step ahead.
2344820	2349628	A	0.47808414697647095	Wow, this is quite stark.
2349804	2353904	A	0.8310509324073792	And in the branching time active inference work.
2354102	2359584	A	0.8809552192687988	In a previous model stream, we also saw some computational complexity estimates.
2359632	2361350	A	0.7149507999420166	But I think these are really clear.
2362520	2367700	A	0.5495926737785339	First thing it made me think of is no one said sophistication was cheap.
2369320	2387260	A	0.6435585618019104	I mean, it's astronomical how costly it is for maybe even only at two or three or four or five potentially starting to get up to so it's increasing the complexity of planning radically.
2388000	2409910	A	0.9212548136711121	So it's very interesting pedagogically as we think about like, as we imagine active inference, intelligence architectures, but this makes it pretty clear that it's not something that you can just enumerate over.
2410840	2440060	A	0.4593358337879181	And so I think it's very creative and important what you've done by connecting this to principled methods of computational complexity reduction rather than potentially effective but ad hoc methods of complexity reduction, like neural networks, which might work when they work, but then once those start to get bloated, now you have no principles and no efficacy.
2440640	2441772	B	0.5801403522491455	Exactly, yeah.
2441826	2453580	B	0.794755220413208	And sophisticated inference, I must note, that has its own benefits when compared to dynamic programming, in the sense that when you're planning forward, you are taking into consideration all the possibilities.
2453660	2454048	B	0.5664746165275574	Right.
2454134	2465844	B	0.7449989318847656	But when you're planning backwards, say, for example, like queue based exploration, like if you have to go somewhere, get a queue and then navigate to the goal state, then planning backwards might not work.
2465882	2472600	B	0.8445090651512146	So this was one feedback I got when I discussed the work with some people.
2472670	2476724	B	0.8392234444618225	So that's a thing to note about dynamic programming.
2476772	2486776	B	0.5984865427017212	But the other place where you learn prior preference, I believe it's not a problem, but yeah, dynamic programming is computationally cheap, but it also come with its own limitations, I must note.
2486808	2495372	A	0.6505254507064819	There, just to restate, that in dynamic programming, with a bellman optimality, we're solving backwards.
2495516	2507676	A	0.5678837299346924	And so it's kind of like the checkmate is what we want and so now we're working backwards to the present, but we end up not exploring counterfactual endpoints.
2507788	2512836	A	0.6867473721504211	We don't work back to the present to endpoints that we're not interested in.
2513018	2513412	B	0.5662814974784851	Exactly.
2513466	2515728	A	0.8196377754211426	So that's why it's so ruthless.
2515904	2523270	A	0.6496788859367371	But on the other hand, it's a much more constrained search.
2523800	2524550	B	0.46103888750076294	Yes.
2525080	2536670	B	0.8474281430244446	So what I'm thinking about these cases that we should also consider combinations of these two where I can afford to kind of ignore such counterfactual things.
2537280	2543544	B	0.5671112537384033	There I can do dynamic programming and maybe I can do as two steps of planning.
2543592	2553100	B	0.9001267552375793	So if it's a queue based exploration, say reaching Q is one task and from Q to the reward is the other task.
2553180	2564068	B	0.4956485629081726	So I can separate these two and use dynamic programming for them and that's computationally cheaper, but also kind of preserves this idea of having to do it.
2564154	2567430	B	0.7878153920173645	Yeah, but these are all future things I am thinking about.
2568120	2568532	B	0.84200119972229	Cool.
2568586	2573400	A	0.9083728790283203	I'll ask another question from the chat Alex wrote.
2574380	2587340	A	0.9013760089874268	Do you have ideas or developments on nested models where different scales could have different one time step ahead and speed of execution?
2587760	2591260	A	0.8733907341957092	So how does this model play out in nested models?
2591760	2598560	A	0.8654370903968811	And how do we think about this forwards and backwards in time in the speed of execution and nested models?
2599540	2602512	B	0.7429794073104858	Okay, so I might need a little more context here.
2602566	2607464	B	0.7248286008834839	Like when you say nested models, do you mean hierarchical models?
2607532	2608150	A	0.5491447448730469	Yeah.
2608840	2614592	B	0.848121702671051	So maybe something like there is an observation.
2614656	2630424	B	0.8845705389976501	I get on a fast pace and I do some inference on that, but then I use this inference to kind of do and basically this inference becomes the observation for the next state.
2630622	2633884	B	0.7942048907279968	That's what nested models mean, right?
2634002	2634332	B	0.46103888750076294	Yes.
2634386	2640076	B	0.8530433177947998	So I might have to actually think about that in terms of a task and then think about it.
2640098	2648080	B	0.5790660381317139	But honestly, I haven't thought how this might work in that context, but say, for a task.
2651300	2658144	B	0.8884785771369934	So in terms of navigation, if you think about it, you can think about, say, rooms environment.
2658192	2663364	B	0.8304511308670044	So that's one of the examples I can think about where you can apply this.
2663402	2674136	B	0.8274610638618469	So imagine maybe you have a collection of rooms and as an agent, you have to first figure out which room to go and then you have to navigate inside that room.
2674318	2680600	B	0.8421079516410828	And basically you can do inference in two stages or decision making in two stages.
2681100	2686172	B	0.8130614161491394	And you will have to separate your decisions in those stages inside the room.
2686226	2701280	B	0.6599722504615784	You can do, say, dynamic programming to navigate your optimal path, but you will always have to kind of have two stages of decision making and maybe different methods work better at different stages.
2701700	2713908	B	0.648067057132721	But this would be better, this discussion would be better to better in a well thought out task, I would say.
2713994	2718596	B	0.5569727420806885	I don't have an answer that might be fitting for everything.
2718698	2731112	A	0.8420579433441162	But yeah, we've seen almost exactly that kind of hierarchical simultaneous localization and mapping slam in a robotics case.
2731166	2733550	A	0.8847965002059937	There have been active inference models on that.
2734880	2749650	A	0.9124380946159363	Would it be possible to sort of do one of these methods at one level of the nested model and then have another computational method applied to another?
2750100	2759380	A	0.8436458706855774	Or if you wanted the advantages of one in one place, can you mix and match with these different methods even within one simulation?
2760200	2767430	B	0.7072746157646179	Yeah, I definitely think that's possible, but we'll have to maybe try.
2769080	2771844	B	0.8367227911949158	So all my methods are one stage.
2771892	2784940	B	0.7088270783424377	It's not a hierarchical model, but I strongly believe that it would also work in an estate model where you can have two methods of decision making working together in, say, for a room example.
2785010	2786030	B	0.7321493029594421	I just mentioned.
2786880	2787340	B	0.84200119972229	Cool.
2787410	2789870	A	0.9267987012863159	Could you go to the slide on Z learning?
2792480	2793230	B	0.5491447448730469	Yeah.
2795540	2796000	A	0.84200119972229	Cool.
2796070	2796256	A	0.5491447448730469	Yeah.
2796278	2802960	A	0.8260613083839417	So I noticed here and in the great paper that you had several totorov citations.
2803380	2804128	A	0.46103888750076294	Yes.
2804294	2810628	A	0.8078045845031738	And this introduction of the Z learning is a novelty with respect to the active inference field.
2810714	2826650	A	0.7642024159431458	So could you explain a little more what is the Z and what is it that enables such a rapid improvement in the Z with respect to the Q?
2827260	2827624	B	0.5491447448730469	Yeah.
2827662	2827912	B	0.584351658821106	Okay.
2827966	2841064	B	0.9211933016777039	So to give some context about this paper, it talks about a linear method of decision making in a particular class of MDP.
2841112	2848556	B	0.8389514684677124	So given that you are having a markout decision process where your actions can be based on states and not actions.
2848588	2856224	B	0.8682129979133606	So when you think about actions in a, say in, for example, a grid world task, you think about left right and north south right?
2856262	2861780	B	0.8740925192832947	So if I take a north, then that has a consequence on state space.
2861930	2869350	B	0.8872416615486145	But in this paper, they're introducing a class of MDP where decision is itself in terms of states.
2869880	2878152	B	0.8795558214187622	So if I am, say, in state s one, my decision will be depending upon the other states.
2878206	2881764	B	0.9045771360397339	So my decision will be based on the other states I want to be in the next time step.
2881822	2893420	B	0.7889500260353088	So it's a redefinition of decision making in terms of the state space rather than decisions being something else like left right and down up.
2893570	2910070	B	0.6563794016838074	So given that such an MDP exists where I can take decisions in terms of states, they have showed that computationally you can take decisions in linear complexity, however big the problem is.
2910760	2919248	B	0.7428666353225708	So for that to enable decision making in terms of states, you should have a sense of what is good and bad for the states.
2919434	2925752	B	0.8534472584724426	So in this grid word example, you have a desirability function which is C.
2925806	2931220	B	0.8653671741485596	So C is the desirability function which talks about how desirable is this tip?
2931380	2942216	B	0.7210816144943237	And if I have a C function, then what they have showed is that I can take decisions with linear computational complexity for this particular class of MDP.
2942248	2951488	B	0.7437343597412109	So if my MDP allows me to take decisions in terms of states, it's only linear complexity for that thing.
2951654	2960644	B	0.7123401165008545	So this graph is basically comparing how you can learn desirability better or faster, right?
2960682	2970952	B	0.8725755214691162	So Q learning, if you're familiar with Q learning, it's basically a table based method where you have desirability of actions given a state.
2971086	2972952	B	0.7626526355743408	So given a state, you know what to do.
2973006	2974756	B	0.809303879737854	That's basically the Q matrix.
2974868	2981290	B	0.8595858216285706	But in terms of C or the C learning method, it's only about states.
2981660	2983964	B	0.5367946028709412	You're only learning how desirable a state is.
2984002	2986430	B	0.5698933005332947	There is no concept of action states.
2986880	2997230	B	0.8027285933494568	And this is exactly what our prior preference is in active inference, where it is a distribution that quantifies how desirable or non desirable states are.
2998580	3017876	B	0.6765915155410767	So given that they have shown that you can learn the C matrix faster and that's optimal and it's way faster than even Q learning, then I thought, okay, why not try to learn C the same way as C is learned in this paper and using this.
3018058	3023448	B	0.9003220200538635	So there is a similar learning rule for learning C, which is called set learning in that paper.
3023614	3037900	B	0.7724260687828064	And when I attempted to learn C, what I've seen is that it learns really fast a useful prior preference that lets me take decisions or let the active inference agent take decisions only by say, one time step of planning.
3039360	3041150	B	0.7526845932006836	So basically that's a story.
3043440	3050610	B	0.7520098090171814	This idea of C being easily learnable is in that paper, the Tarot paper.
3052100	3058672	A	0.9193155169487	Okay, let me try to restate that since I think it's a very interesting augmentation of active inference.
3058736	3066256	A	0.831247091293335	So we're going to be learning C for all the reasons we discussed earlier.
3066448	3074650	A	0.7609409689903259	We're going to learn C analogously to how Todorov presented Z learning.
3075260	3098690	A	0.8294231295585632	And in the Z learning, instead of learning, for example, updated posterior probabilities on actions and then using actions to navigate amongst states that emit observations, we're going to kind of bake the action into the states.
3099380	3105440	A	0.8714247941970825	So that really we're learning transitions amongst states directly.
3111510	3125254	A	0.8145850300788879	And to connect this to the free energy principle and how it is playing out with active inference here, C is not just our desirability function, that's one way of thinking about it.
3125292	3126930	A	0.793980062007904	That's why we call it preference.
3127090	3129990	A	0.8039863109588623	But also C is our expectation.
3130410	3137754	A	0.550905704498291	And so that is what allows us to, on one hand use the language familiar to reward and preference learning.
3137872	3147094	A	0.8491178750991821	Like the agent is ending up where it likes to be, but also the expectation based definition of C.
3147152	3157402	A	0.8243550658226013	These are the same thing allows us to talk about that as a path of least action or as the most likely outcome or the least surprising outcome.
3157546	3178120	A	0.7250452637672424	And because we've defined it what we want as the least surprising outcome, then we can use variational free energy to bound surprise, whereas you can't use simply a variational method to bound or even necessarily approximate reward itself.
3178890	3203790	A	0.7503296136856079	But if you say I prefer what I expect and what I expect reduces my surprise and I'm going to bounce surprise, then you get both that sort of behavioral reward seeking couched in a surprise bounding surprise minimizing physics framework.
3207010	3208994	B	0.9447745680809021	Yeah, that's a beautiful way of saying it.
3209032	3209282	B	0.5491447448730469	Yeah.
3209336	3210100	B	0.8529649972915649	Thank you.
3211590	3220786	A	0.6569648385047913	Well, what are your next exciting steps or directions or what ways do you want to take this work?
3220968	3221846	B	0.8782029747962952	Yeah, great.
3221948	3233378	B	0.8072993755340576	So if you are talking about only this work, what I want to do next is think about queue based exploration tasks.
3233554	3236730	B	0.6943270564079285	First of all, address the limitations of dynamic programming.
3237310	3254720	B	0.6596423387527466	So if you have, say, a queue to explore in this grid first and that's more optimal, I want to see how the expected ambiguity term in the expected free energy is useful and should be made use in the dynamic programming strictly in that sense.
3255330	3261302	B	0.8544178605079651	But more generally, I'm also looking at other ways of making decisions in active instance.
3261466	3281046	B	0.8017012476921082	Like there are works from professor Isamura from CBS Reckon, he talks about how neural networks are doing active inference and basically decision making there is really more efficient in the sense that it's like Q learning.
3281228	3296042	B	0.48361143469810486	So there he's making clever use of the variation free energy to learn good state action mappings and it's a very drastic change to what we are used to in terms of expected free energy.
3296096	3300302	B	0.5863860845565796	So there is no concept of expected free energy in that work.
3300436	3304542	B	0.6199411153793335	It's all about learning what is good and bad directly from variation of free energy.
3304596	3306778	B	0.9506232142448425	So I find that also fascinating.
3306954	3317250	B	0.7525558471679688	I want to kind of explore that more and see how decision making in this way is better or worse or should be thought about.
3317400	3320242	B	0.7580629587173462	Should we even reconsider ways of decision making?
3320296	3324434	B	0.8206952214241028	Because active inference only talks about variation of free energy and that's the central tenet.
3324482	3327542	B	0.7230204343795776	Everything else is your interpretation of that.
3327596	3333560	B	0.6294142007827759	Right, that's another direction I want to work.
3335790	3337546	A	0.7358579635620117	Interesting way to say it.
3337648	3360900	A	0.7514169216156006	Definitely the variational free energy, which is a functional of our variational distribution Q and the data y, the variational free energy is kind of like the real time homeostasis, like how are things making sense given what I believe and the incoming data.
3361590	3370766	A	0.8573034405708313	And then to extend that kind of a sense making framework into decision making, we've seen a lot of different methods.
3370878	3388730	A	0.8326861262321472	Expected free energy is a common one, but for example, there's been free energy of the expected future and there's other constructions that have different methods.
3389550	3404714	A	0.899286687374115	And then you pointed also to Professor Samura's work with the sort of relationship between the variational free energy on the base graph and the loss function in a neural network and all those relationships.
3404762	3416450	A	0.983960747718811	That's very exciting work too, I guess kind of in closing, just as a last question or thought, you're coming close to the end of your PhD.
3416870	3440780	A	0.7629956603050232	So just in the time that you've been a PhD student, how have you seen active inference develop or what feels different to you today near the end than when you were fresh eyed and excited several years ago?
3441150	3447798	B	0.9888659119606018	Yeah, that's a really great question and I am also very excited of how the field has evolved.
3447974	3459614	B	0.49193963408470154	And frankly, I started from this reinforcement learning background and the physics background, and when I started reading, it was only, say, one or two papers and I did not quite understand much of it.
3459812	3469202	B	0.9182003736495972	It was only when I started implementing it using Carl's MATLAB scripts, I kind of understood, okay, this makes sense and I like it.
3469336	3480454	B	0.7646595239639282	But within, say, one or two years, I saw a lot of papers coming in from different directions and people also starting to use neural networks and all this scaling up came.
3480652	3491750	B	0.6805270910263062	And at some point I also questioned the need of active inference because if you have deep reinforcement learning, which can do many things, and why deep active inference?
3491910	3495210	B	0.7759556174278259	And that's the reason why I did not get into that.
3495280	3497382	B	0.8259001970291138	But I still find it fascinating.
3497446	3509474	B	0.6041712164878845	I want to kind of understand deep, active inference more than what I know now, but I've seen the field growing, like anything in two or three years, and many cohorts of people starting to work.
3509512	3520354	B	0.6054781675338745	And in no time, it was a seriously taken field other than a field with, say, two papers with nobody actually knowing what it is.
3520472	3521682	B	0.9870310425758362	So it's really exciting.
3521746	3521974	B	0.5491447448730469	Yeah.
3522012	3530920	B	0.943688690662384	So I really look forward how the field evolves in time and also what I can do after my PhD and so on.
3532250	3532950	A	0.84200119972229	Cool.
3533100	3537580	A	0.8167804479598999	Forward in time, backward in time, what we prefer, what we expect.
3538510	3539260	B	0.46103888750076294	Yes.
3542430	3546380	A	0.8809373378753662	Any other comments or anything else you want to add?
3547230	3553066	B	0.8458893895149231	Yeah, so please let me know what you think about the paper and what you think about these ideas.
3553178	3554734	B	0.7501773834228516	Feel free to let me know.
3554852	3558430	B	0.9647504091262817	I really look forward to the feedback and yeah.
3558580	3562080	B	0.9909862875938416	Thanks so much for this opportunity, Daniel, and thank you for your time.
3563330	3564190	A	0.9656825065612793	It was amazing.
3564260	3571630	A	0.5903642177581787	I hope people check out the paper and get in touch and replicate the code and take it their own direction.
3572250	3573110	A	0.8529649972915649	Thank you.
3573260	3574470	A	0.6020334959030151	See you next time.
3574620	3575414	B	0.6020334959030151	See you next time.
3575452	3576246	B	0.9793016910552979	Thank you so much.
3576348	3576818	A	0.5137447118759155	Bye.
3576914	3577654	B	0.974808931350708	Have a good day.
3577692	3577890	B	0.5137447118759155	Bye.

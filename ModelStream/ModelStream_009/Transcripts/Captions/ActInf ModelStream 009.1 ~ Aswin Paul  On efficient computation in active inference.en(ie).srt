1
00:00:03,120 --> 00:00:06,120
foreign

2
00:00:08,480 --> 00:00:12,660
it's July 15 2023

3
00:00:12,660 --> 00:00:15,900
we're here in active inference model

4
00:00:15,900 --> 00:00:19,080
stream number 9.1 with Aspen Paul

5
00:00:19,080 --> 00:00:21,060
today we're going to have a presentation

6
00:00:21,060 --> 00:00:24,180
and a discussion on efficient

7
00:00:24,180 --> 00:00:27,000
computation in active inference so if

8
00:00:27,000 --> 00:00:28,619
you're watching along live please feel

9
00:00:28,619 --> 00:00:30,840
free to add comments or questions in the

10
00:00:30,840 --> 00:00:33,059
chat otherwise

11
00:00:33,059 --> 00:00:36,120
thanks so much for joining today really

12
00:00:36,120 --> 00:00:38,820
looking forward to your talk

13
00:00:38,820 --> 00:00:41,640
thank you Daniel thank you so much so as

14
00:00:41,640 --> 00:00:42,960
mentioned today I'm going to talk about

15
00:00:42,960 --> 00:00:44,700
efficient computation and active

16
00:00:44,700 --> 00:00:46,920
inference and yeah let's get started

17
00:00:46,920 --> 00:00:49,500
so we're all familiar with this idea of

18
00:00:49,500 --> 00:00:51,420
the free energy principle which is also

19
00:00:51,420 --> 00:00:53,820
known as active inference right so the

20
00:00:53,820 --> 00:00:55,800
central concept is that an agent

21
00:00:55,800 --> 00:00:59,160
minimizes entropy of its observation to

22
00:00:59,160 --> 00:01:01,140
maintain homeostasis or survive in its

23
00:01:01,140 --> 00:01:03,719
environment and here the entropy is

24
00:01:03,719 --> 00:01:05,580
defined in the information theoretic

25
00:01:05,580 --> 00:01:08,520
sense right so if an observation is

26
00:01:08,520 --> 00:01:10,200
highly probabilistic that is less

27
00:01:10,200 --> 00:01:12,659
entropic or less surprising because it

28
00:01:12,659 --> 00:01:14,159
was high probability and we were

29
00:01:14,159 --> 00:01:16,560
expecting it and

30
00:01:16,560 --> 00:01:18,840
and and that that's the idea when that's

31
00:01:18,840 --> 00:01:20,939
the base that we build this framework of

32
00:01:20,939 --> 00:01:23,759
active inference on and this idea of

33
00:01:23,759 --> 00:01:26,520
Marco blanket uh it gives us a

34
00:01:26,520 --> 00:01:30,000
systematic way of surprising uh or the

35
00:01:30,000 --> 00:01:32,580
schematic way of separating uh an agent

36
00:01:32,580 --> 00:01:34,439
from its environment and model

37
00:01:34,439 --> 00:01:37,320
purposeful Behavior right so let's focus

38
00:01:37,320 --> 00:01:39,780
on this idea of minimizing entropy so

39
00:01:39,780 --> 00:01:41,700
how does an agent minimize entropy or

40
00:01:41,700 --> 00:01:43,740
know which observation is highly

41
00:01:43,740 --> 00:01:46,020
probabilistic and vice versa

42
00:01:46,020 --> 00:01:48,360
so that is by maintaining a generative

43
00:01:48,360 --> 00:01:50,399
model and the generative model is

44
00:01:50,399 --> 00:01:52,020
basically a toy model of the environment

45
00:01:52,020 --> 00:01:56,340
which the agent builds in its brain and

46
00:01:56,340 --> 00:01:58,860
that is built using only the observation

47
00:01:58,860 --> 00:02:00,540
that it gets from the environment so it

48
00:02:00,540 --> 00:02:03,420
has no access to the real uh States or

49
00:02:03,420 --> 00:02:04,740
the hidden states of the environment

50
00:02:04,740 --> 00:02:07,920
it's building the toy model and given

51
00:02:07,920 --> 00:02:10,318
this toy model it has scope or ability

52
00:02:10,318 --> 00:02:13,560
uh to compute the probability of an

53
00:02:13,560 --> 00:02:15,900
observation and hence try to minimize

54
00:02:15,900 --> 00:02:18,660
the entropy right so

55
00:02:18,660 --> 00:02:22,620
that's the idea but it has a problem of

56
00:02:22,620 --> 00:02:25,800
um cursive dimensionality like given a

57
00:02:25,800 --> 00:02:27,239
generative model it may not be possible

58
00:02:27,239 --> 00:02:30,120
always to calculate or marginalize the

59
00:02:30,120 --> 00:02:32,099
probability of observations out of it

60
00:02:32,099 --> 00:02:34,140
because the state space can quickly

61
00:02:34,140 --> 00:02:37,020
become intractable but the idea is that

62
00:02:37,020 --> 00:02:38,640
you define an upper bound on the

63
00:02:38,640 --> 00:02:41,580
surprise using Jensen's inequality and

64
00:02:41,580 --> 00:02:43,560
you may also

65
00:02:43,560 --> 00:02:46,800
Define a new term called Q which is the

66
00:02:46,800 --> 00:02:49,500
hidden belief or the belief about the

67
00:02:49,500 --> 00:02:52,080
hidden States and this queue is going to

68
00:02:52,080 --> 00:02:54,000
be the focus of decision making right so

69
00:02:54,000 --> 00:02:57,060
if you have a noisy queue and you have

70
00:02:57,060 --> 00:02:58,680
no idea what's in the environment then

71
00:02:58,680 --> 00:03:00,840
you can't make or hope to make decisions

72
00:03:00,840 --> 00:03:03,540
to control that environment and it is

73
00:03:03,540 --> 00:03:05,519
this belief about the hidden states that

74
00:03:05,519 --> 00:03:07,560
you use and that becomes useful to take

75
00:03:07,560 --> 00:03:10,379
decisions and this whole quantity is of

76
00:03:10,379 --> 00:03:13,019
course called the free energy and the

77
00:03:13,019 --> 00:03:14,420
variational free energy

78
00:03:14,420 --> 00:03:17,879
F can be interpreted in multiple ways so

79
00:03:17,879 --> 00:03:19,560
the first or the most common is the

80
00:03:19,560 --> 00:03:22,920
machine learning way of how it is

81
00:03:22,920 --> 00:03:25,200
um Trying to minimize the complexity of

82
00:03:25,200 --> 00:03:27,060
the model at the same time trying to

83
00:03:27,060 --> 00:03:28,739
maximize the accuracy of it so that's

84
00:03:28,739 --> 00:03:30,180
the machine learning interpretation of

85
00:03:30,180 --> 00:03:32,400
minimizing various free energy you may

86
00:03:32,400 --> 00:03:34,140
also try to

87
00:03:34,140 --> 00:03:34,739
um

88
00:03:34,739 --> 00:03:36,420
interpret free energy in the physics

89
00:03:36,420 --> 00:03:38,940
term where you at the same time try to

90
00:03:38,940 --> 00:03:41,760
minimize the energy of your model but at

91
00:03:41,760 --> 00:03:43,019
the same time trying to maximize the

92
00:03:43,019 --> 00:03:45,120
entropy but the focus of

93
00:03:45,120 --> 00:03:48,659
today or decision making is always on

94
00:03:48,659 --> 00:03:51,540
this belief that you have after you do

95
00:03:51,540 --> 00:03:54,780
the perception in active inference so

96
00:03:54,780 --> 00:03:57,120
how do you do vanilla decision making or

97
00:03:57,120 --> 00:03:58,920
what is the most discussed idea of

98
00:03:58,920 --> 00:04:00,659
decision making and classical active

99
00:04:00,659 --> 00:04:03,540
inference uh so if you are in an

100
00:04:03,540 --> 00:04:05,580
environment uh and if you're an agent

101
00:04:05,580 --> 00:04:08,819
who's trying to make decisions then

102
00:04:08,819 --> 00:04:10,620
you have a

103
00:04:10,620 --> 00:04:13,319
space of available actions right so in

104
00:04:13,319 --> 00:04:15,480
this toy model you have three available

105
00:04:15,480 --> 00:04:19,738
actions uh run jump or stay and

106
00:04:19,738 --> 00:04:22,199
given these actions you can hope to

107
00:04:22,199 --> 00:04:25,139
define a policy by small Pi which is a

108
00:04:25,139 --> 00:04:28,320
sequence of actions in time and capital

109
00:04:28,320 --> 00:04:30,300
t is the time Horizon of planning or

110
00:04:30,300 --> 00:04:32,660
that's the length of your

111
00:04:32,660 --> 00:04:36,540
policy and you have a policy space with

112
00:04:36,540 --> 00:04:39,419
many such policies small pie so given

113
00:04:39,419 --> 00:04:42,060
this bigger smallest space you can

114
00:04:42,060 --> 00:04:44,220
attempt to

115
00:04:44,220 --> 00:04:47,340
evaluate the expected free energy based

116
00:04:47,340 --> 00:04:48,720
on the beliefs that you have accumulated

117
00:04:48,720 --> 00:04:50,580
so here you're not minimizing anything

118
00:04:50,580 --> 00:04:52,860
you already have a belief from variation

119
00:04:52,860 --> 00:04:55,080
for energy and you you are just

120
00:04:55,080 --> 00:04:57,419
calculating or evaluating the expected

121
00:04:57,419 --> 00:04:59,520
free energy corresponding to many uh

122
00:04:59,520 --> 00:05:02,759
small policies that you can Define

123
00:05:02,759 --> 00:05:05,699
and after you evaluate it for all

124
00:05:05,699 --> 00:05:07,440
policies then you know which is the

125
00:05:07,440 --> 00:05:09,180
optimal policy to take and that's the

126
00:05:09,180 --> 00:05:11,160
classical active inference idea of

127
00:05:11,160 --> 00:05:13,800
decision making right and

128
00:05:13,800 --> 00:05:16,020
this expected free energy is really

129
00:05:16,020 --> 00:05:18,900
useful in the sense that it is goal

130
00:05:18,900 --> 00:05:21,720
directed so the risk term is goal

131
00:05:21,720 --> 00:05:23,520
directed and then you have this expected

132
00:05:23,520 --> 00:05:25,919
ambiguity term that forces you to

133
00:05:25,919 --> 00:05:27,960
explore as well but there is a problem

134
00:05:27,960 --> 00:05:30,539
that this policy space can quickly turn

135
00:05:30,539 --> 00:05:32,400
interactable and that has always stayed

136
00:05:32,400 --> 00:05:35,280
as a bottleneck in scaling active

137
00:05:35,280 --> 00:05:37,680
inference to

138
00:05:37,680 --> 00:05:40,500
um commonly seen environments but let's

139
00:05:40,500 --> 00:05:42,419
see how it becomes quickly interactable

140
00:05:42,419 --> 00:05:45,300
so how many policies can be defined say

141
00:05:45,300 --> 00:05:47,940
for a Time Horizon of 15 right so if you

142
00:05:47,940 --> 00:05:50,340
are playing say Super Mario uh you might

143
00:05:50,340 --> 00:05:52,440
might want to plan at least say 10 time

144
00:05:52,440 --> 00:05:56,460
step ahead so the first policy could be

145
00:05:56,460 --> 00:05:58,380
um the same action run

146
00:05:58,380 --> 00:06:02,600
um stacked for 15 time steps and you can

147
00:06:02,600 --> 00:06:05,460
basically Define several combinations of

148
00:06:05,460 --> 00:06:09,060
such actions and the policy space is

149
00:06:09,060 --> 00:06:10,139
simply

150
00:06:10,139 --> 00:06:13,860
um intractable it becomes uh too huge

151
00:06:13,860 --> 00:06:16,380
for you to evaluate the expected free

152
00:06:16,380 --> 00:06:19,440
energy for all such policies and in a

153
00:06:19,440 --> 00:06:21,900
stochastic problem setting where

154
00:06:21,900 --> 00:06:24,419
um the environment itself is noisy you

155
00:06:24,419 --> 00:06:27,120
don't really have a method to choose a

156
00:06:27,120 --> 00:06:28,860
subset of this policy space and do

157
00:06:28,860 --> 00:06:30,840
classical active inference and this is

158
00:06:30,840 --> 00:06:32,340
clearly a computational interactable

159
00:06:32,340 --> 00:06:34,380
problem and that's why in literature we

160
00:06:34,380 --> 00:06:36,360
always see small grids or small

161
00:06:36,360 --> 00:06:38,880
environments when you discuss decision

162
00:06:38,880 --> 00:06:41,520
making in active influence but recently

163
00:06:41,520 --> 00:06:43,860
uh a new idea was

164
00:06:43,860 --> 00:06:46,259
proposed which is called a sophisticated

165
00:06:46,259 --> 00:06:48,840
entrance and in sophisticated inference

166
00:06:48,840 --> 00:06:51,600
it is not really the policy space you

167
00:06:51,600 --> 00:06:52,800
are actually

168
00:06:52,800 --> 00:06:55,080
um real time trying to

169
00:06:55,080 --> 00:06:58,139
think what to do so if you have a belief

170
00:06:58,139 --> 00:07:00,000
then you are trying to

171
00:07:00,000 --> 00:07:02,880
um evaluate the actions based on that so

172
00:07:02,880 --> 00:07:04,800
here we don't have a sequence of

173
00:07:04,800 --> 00:07:07,199
policies or things that becomes

174
00:07:07,199 --> 00:07:10,020
interactable uh here we are doing

175
00:07:10,020 --> 00:07:12,600
basically research in the sense that you

176
00:07:12,600 --> 00:07:14,220
are trying to evaluate expected free

177
00:07:14,220 --> 00:07:16,500
energy of um

178
00:07:16,500 --> 00:07:19,560
this joint distribution of actions and

179
00:07:19,560 --> 00:07:21,900
observations and to evaluate the

180
00:07:21,900 --> 00:07:23,940
expected free energy at some time uh

181
00:07:23,940 --> 00:07:26,340
small T you will also need the expected

182
00:07:26,340 --> 00:07:29,039
free energy at the next time step and to

183
00:07:29,039 --> 00:07:31,259
evaluate that you will need the expected

184
00:07:31,259 --> 00:07:32,819
free energy of time t plus 2 and this

185
00:07:32,819 --> 00:07:35,039
basically becomes a research that rolls

186
00:07:35,039 --> 00:07:38,160
out in time and uh it's a it's a

187
00:07:38,160 --> 00:07:41,160
recursive relation and

188
00:07:41,160 --> 00:07:43,259
here it's fundamentally different from

189
00:07:43,259 --> 00:07:45,539
the policy space that we saw in the last

190
00:07:45,539 --> 00:07:47,759
slide right and

191
00:07:47,759 --> 00:07:49,680
is it computationally better in the

192
00:07:49,680 --> 00:07:51,780
sense that if you have to plan say 10

193
00:07:51,780 --> 00:07:55,139
times tips ahead comprehensively in

194
00:07:55,139 --> 00:07:56,759
classical active inference we saw that

195
00:07:56,759 --> 00:07:58,380
the policy spaces

196
00:07:58,380 --> 00:07:59,460
um

197
00:07:59,460 --> 00:08:01,940
the cardinality of the action space

198
00:08:01,940 --> 00:08:04,139
raised to T so that is the computational

199
00:08:04,139 --> 00:08:07,259
bottleneck if you have to uh consider

200
00:08:07,259 --> 00:08:10,199
all possibilities but in sophisticated

201
00:08:10,199 --> 00:08:12,780
inference it's even worse because uh it

202
00:08:12,780 --> 00:08:14,759
you are considering a combination of

203
00:08:14,759 --> 00:08:16,860
state and actions so it is

204
00:08:16,860 --> 00:08:20,400
computationally worse in fact but a

205
00:08:20,400 --> 00:08:21,660
solution was proposed in the

206
00:08:21,660 --> 00:08:23,520
sophisticated reference paper that we

207
00:08:23,520 --> 00:08:26,400
can do pruning of This research that we

208
00:08:26,400 --> 00:08:29,639
can avoid some states and actions uh

209
00:08:29,639 --> 00:08:31,080
when you do this research and that

210
00:08:31,080 --> 00:08:33,000
becomes computationally very tractable

211
00:08:33,000 --> 00:08:36,059
so let's see how pruning Works in

212
00:08:36,059 --> 00:08:38,940
sophisticated inference so this grid was

213
00:08:38,940 --> 00:08:40,440
discussed in the original sophisticated

214
00:08:40,440 --> 00:08:41,700
inference paper

215
00:08:41,700 --> 00:08:44,580
and say for this grid given that you

216
00:08:44,580 --> 00:08:47,100
have this kind of a prior preference

217
00:08:47,100 --> 00:08:50,399
distribution so this white square which

218
00:08:50,399 --> 00:08:52,140
is the goal status the most preferred

219
00:08:52,140 --> 00:08:54,300
State and you have a

220
00:08:54,300 --> 00:08:55,380
um

221
00:08:55,380 --> 00:08:58,140
uniformly decreasing preference for

222
00:08:58,140 --> 00:09:00,779
states that is far away from that gold

223
00:09:00,779 --> 00:09:03,240
state then basically if You observe

224
00:09:03,240 --> 00:09:05,480
yourself at some observation at time T

225
00:09:05,480 --> 00:09:07,560
then basically what you're doing is

226
00:09:07,560 --> 00:09:09,600
you're considering the consequence of

227
00:09:09,600 --> 00:09:12,779
available actions from that observations

228
00:09:12,779 --> 00:09:17,760
and basically you can use the projection

229
00:09:17,760 --> 00:09:20,700
of your belief and set a threshold for

230
00:09:20,700 --> 00:09:22,620
those beliefs to kind of maybe ignore

231
00:09:22,620 --> 00:09:24,600
some actions and ignore some

232
00:09:24,600 --> 00:09:27,420
observations and what you will find is

233
00:09:27,420 --> 00:09:30,120
that it it's it's no longer at research

234
00:09:30,120 --> 00:09:33,000
it's a subset of research and it's

235
00:09:33,000 --> 00:09:36,060
computationally way uh efficient than

236
00:09:36,060 --> 00:09:38,700
doing the whole research right so here

237
00:09:38,700 --> 00:09:40,680
you have avoided a lot of combinations

238
00:09:40,680 --> 00:09:42,120
and this becomes a computationally

239
00:09:42,120 --> 00:09:45,420
attractable problem this is because you

240
00:09:45,420 --> 00:09:47,880
have decided to avoid some actions and

241
00:09:47,880 --> 00:09:49,620
um observation so you say essentially

242
00:09:49,620 --> 00:09:52,740
compromising on that possibilities which

243
00:09:52,740 --> 00:09:54,600
might give you a higher reward or which

244
00:09:54,600 --> 00:09:56,580
might be more optimal than the

245
00:09:56,580 --> 00:09:59,700
consequence of this partial research but

246
00:09:59,700 --> 00:10:02,820
this works so this simulation

247
00:10:02,820 --> 00:10:05,459
was presented in the paper and it works

248
00:10:05,459 --> 00:10:09,959
the agent uh kind of learns to do uh

249
00:10:09,959 --> 00:10:12,300
what needs to be done if it plans in

250
00:10:12,300 --> 00:10:14,940
this forward direction of planning uh in

251
00:10:14,940 --> 00:10:17,880
in a pruned way but

252
00:10:17,880 --> 00:10:19,200
and

253
00:10:19,200 --> 00:10:20,820
and basically you can computationally

254
00:10:20,820 --> 00:10:23,519
show that even for a small search

255
00:10:23,519 --> 00:10:25,019
threshold

256
00:10:25,019 --> 00:10:27,180
um you can drastically decrease the

257
00:10:27,180 --> 00:10:29,100
computational complexity so if you

258
00:10:29,100 --> 00:10:31,500
decide to do the whole research

259
00:10:31,500 --> 00:10:33,540
um with the planning depth uh the

260
00:10:33,540 --> 00:10:35,100
computational time is exponential and

261
00:10:35,100 --> 00:10:38,040
quickly you can't do much about it then

262
00:10:38,040 --> 00:10:40,200
but if you decide at such threshold

263
00:10:40,200 --> 00:10:42,420
which is even very small you can see

264
00:10:42,420 --> 00:10:43,980
that this problem becomes

265
00:10:43,980 --> 00:10:46,620
computationally attractable and this

266
00:10:46,620 --> 00:10:49,200
demo uh on sophisticated instances are

267
00:10:49,200 --> 00:10:50,700
available in my version of my mdp and

268
00:10:50,700 --> 00:10:52,860
it's going to be integrated to the

269
00:10:52,860 --> 00:10:55,380
original by mdp soon

270
00:10:55,380 --> 00:10:56,820
um so

271
00:10:56,820 --> 00:10:59,160
but the key point is that the pruning of

272
00:10:59,160 --> 00:11:01,920
that research requires us a

273
00:11:01,920 --> 00:11:03,779
well-informed prime preference like this

274
00:11:03,779 --> 00:11:06,060
in the sense that here the agent is

275
00:11:06,060 --> 00:11:09,360
aware of how desirable our neighboring

276
00:11:09,360 --> 00:11:11,760
states it not only knows about the final

277
00:11:11,760 --> 00:11:13,860
goal set it also knows about its

278
00:11:13,860 --> 00:11:16,440
neighboring state and given such a prior

279
00:11:16,440 --> 00:11:18,420
preference we can see that for a

280
00:11:18,420 --> 00:11:20,399
planning depth of three the agent is

281
00:11:20,399 --> 00:11:23,820
getting stuck basically in this local

282
00:11:23,820 --> 00:11:27,060
Maxima of Prior preference but with

283
00:11:27,060 --> 00:11:30,480
sufficient planning depth it is it is

284
00:11:30,480 --> 00:11:33,240
able to overcome this barrier and reach

285
00:11:33,240 --> 00:11:35,579
the goal State uh in in this great

286
00:11:35,579 --> 00:11:37,800
problem and

287
00:11:37,800 --> 00:11:39,600
the question is that what if the agent

288
00:11:39,600 --> 00:11:42,720
only knows this final state it has no

289
00:11:42,720 --> 00:11:45,660
other knowledge about what to do and in

290
00:11:45,660 --> 00:11:48,240
this case what does the agent do so this

291
00:11:48,240 --> 00:11:50,459
is the problem that we are trying to

292
00:11:50,459 --> 00:11:53,399
address and in the absence of

293
00:11:53,399 --> 00:11:55,740
a meaningful prior preference like this

294
00:11:55,740 --> 00:11:58,200
the agent has basically no way of

295
00:11:58,200 --> 00:12:00,720
reaching the goal State

296
00:12:00,720 --> 00:12:03,300
um other than by random exploration it

297
00:12:03,300 --> 00:12:05,279
it has no way of planning because it it

298
00:12:05,279 --> 00:12:08,040
cannot do uh planning eight time steps

299
00:12:08,040 --> 00:12:09,779
ahead because it's computationally

300
00:12:09,779 --> 00:12:13,079
interactable uh if if it chooses to do

301
00:12:13,079 --> 00:12:17,880
the full research right and

302
00:12:17,880 --> 00:12:21,000
so if so for a grid like this what do

303
00:12:21,000 --> 00:12:22,500
you do if you get a sparse try

304
00:12:22,500 --> 00:12:24,240
preference which is not well informed

305
00:12:24,240 --> 00:12:28,079
and as highlighted in the previous slide

306
00:12:28,079 --> 00:12:30,839
your research is now blind uh you have

307
00:12:30,839 --> 00:12:33,000
no way of pruning that research and you

308
00:12:33,000 --> 00:12:35,100
have to do the full research in in this

309
00:12:35,100 --> 00:12:36,300
scenario

310
00:12:36,300 --> 00:12:38,880
so as you might have thought

311
00:12:38,880 --> 00:12:40,200
um there are two solutions to this

312
00:12:40,200 --> 00:12:42,959
either you have to find out a way to do

313
00:12:42,959 --> 00:12:47,359
the full depth planning uh

314
00:12:49,579 --> 00:12:52,920
or you have to learn a meaningful Pride

315
00:12:52,920 --> 00:12:54,600
preference which will enable you to do

316
00:12:54,600 --> 00:12:57,480
uh this pruned research so we are going

317
00:12:57,480 --> 00:12:59,760
to discuss these two solutions uh in

318
00:12:59,760 --> 00:13:02,399
this presentation uh for a given

319
00:13:02,399 --> 00:13:03,839
scenario like this

320
00:13:03,839 --> 00:13:06,420
so the first solution uh to do full

321
00:13:06,420 --> 00:13:08,639
research is basically using dynamic

322
00:13:08,639 --> 00:13:11,399
programming and dynamic programming is a

323
00:13:11,399 --> 00:13:13,800
well-known idea in operation research

324
00:13:13,800 --> 00:13:15,839
and industrial engineering and many in

325
00:13:15,839 --> 00:13:18,060
many branches of engineering and the

326
00:13:18,060 --> 00:13:19,920
basic idea is that you

327
00:13:19,920 --> 00:13:21,959
solve the subparts of a bigger problem

328
00:13:21,959 --> 00:13:24,000
first and then later try to integrate

329
00:13:24,000 --> 00:13:27,000
the solutions of these sub problems uh

330
00:13:27,000 --> 00:13:31,260
to do uh optimal decision making so

331
00:13:31,260 --> 00:13:33,779
in this scenario imagine that

332
00:13:33,779 --> 00:13:37,019
you're trying to plan for the Last

333
00:13:37,019 --> 00:13:39,540
Action so earlier we started from the

334
00:13:39,540 --> 00:13:41,760
first action we started from the present

335
00:13:41,760 --> 00:13:44,040
and try to predict what is happening in

336
00:13:44,040 --> 00:13:46,320
the future in in so your direction of

337
00:13:46,320 --> 00:13:47,940
planning was basically forward in time

338
00:13:47,940 --> 00:13:49,920
but imagine that you are trying to plan

339
00:13:49,920 --> 00:13:51,959
only for the last time step where you

340
00:13:51,959 --> 00:13:55,560
are just near the goal State and right

341
00:13:55,560 --> 00:13:57,600
going to that good state right so you

342
00:13:57,600 --> 00:13:59,519
are trying to make a decision for that

343
00:13:59,519 --> 00:14:01,740
last time step which is capital T minus

344
00:14:01,740 --> 00:14:05,459
1 and your projections uh to the next

345
00:14:05,459 --> 00:14:08,880
time step or the last goal state will

346
00:14:08,880 --> 00:14:09,839
um

347
00:14:09,839 --> 00:14:11,880
can be done because you have access to

348
00:14:11,880 --> 00:14:14,160
this model of the world uh using this

349
00:14:14,160 --> 00:14:16,079
transition Dynamics or the B Matrix in

350
00:14:16,079 --> 00:14:18,600
active inference so for this single time

351
00:14:18,600 --> 00:14:21,180
step which is a sub problem uh you are

352
00:14:21,180 --> 00:14:24,060
actually evaluating a table of expected

353
00:14:24,060 --> 00:14:26,100
free energy that tells you if you are in

354
00:14:26,100 --> 00:14:28,019
say this observation what is to be done

355
00:14:28,019 --> 00:14:31,740
which is for the last time step and

356
00:14:31,740 --> 00:14:34,440
basically uh you can do this in state

357
00:14:34,440 --> 00:14:36,540
space or observation space so this can

358
00:14:36,540 --> 00:14:39,120
be done uh using the a matrix and D

359
00:14:39,120 --> 00:14:41,699
Matrix together where you can do

360
00:14:41,699 --> 00:14:44,579
planning either way right so the

361
00:14:44,579 --> 00:14:46,260
question is that

362
00:14:46,260 --> 00:14:48,839
if you know what to do in the last time

363
00:14:48,839 --> 00:14:50,940
steps

364
00:14:50,940 --> 00:14:52,440
that you might be thinking how do I know

365
00:14:52,440 --> 00:14:54,600
that I'm in the last time step it is all

366
00:14:54,600 --> 00:14:58,699
about imagining you have

367
00:15:09,720 --> 00:15:12,839
sorry just last thing we heard for a

368
00:15:12,839 --> 00:15:15,540
second was it's all about imagining

369
00:15:15,540 --> 00:15:18,060
yeah so just pick up from there it's all

370
00:15:18,060 --> 00:15:19,860
about imagining

371
00:15:19,860 --> 00:15:22,139
okay so if there was a connection issue

372
00:15:22,139 --> 00:15:23,699
yeah just for a few seconds it's all

373
00:15:23,699 --> 00:15:25,199
good then now

374
00:15:25,199 --> 00:15:28,260
oh sorry about that but so

375
00:15:28,260 --> 00:15:30,899
um so what I'm trying to say is that you

376
00:15:30,899 --> 00:15:32,880
are trying to imagine what you will do

377
00:15:32,880 --> 00:15:35,519
if you are in time capital T minus one

378
00:15:35,519 --> 00:15:37,560
which is the last time step for your

379
00:15:37,560 --> 00:15:40,260
planning Horizon and if you are in that

380
00:15:40,260 --> 00:15:42,199
time step what do I do so this table

381
00:15:42,199 --> 00:15:45,480
represents all such scenarios that if I

382
00:15:45,480 --> 00:15:49,800
am say at observation 3 at time T minus

383
00:15:49,800 --> 00:15:52,680
one what do I do and this quantity here

384
00:15:52,680 --> 00:15:54,720
I'm only considering the risk term or

385
00:15:54,720 --> 00:15:56,279
the purposeful term

386
00:15:56,279 --> 00:15:57,720
uh and

387
00:15:57,720 --> 00:16:01,560
this term represents that policy and

388
00:16:01,560 --> 00:16:03,540
what if I do this

389
00:16:03,540 --> 00:16:06,180
um backwards till time T minus one so if

390
00:16:06,180 --> 00:16:08,699
I'm if I know what to do at time uh

391
00:16:08,699 --> 00:16:12,300
capital T minus 1 then this table can

392
00:16:12,300 --> 00:16:14,880
inform what to do at capital T minus 2.

393
00:16:14,880 --> 00:16:16,920
so rather than planning forward in time

394
00:16:16,920 --> 00:16:19,320
what I'm doing is basically stacking

395
00:16:19,320 --> 00:16:22,019
many tables together just by fixing a

396
00:16:22,019 --> 00:16:25,019
capital T of planning and

397
00:16:25,019 --> 00:16:29,279
given that I have all such stack tables

398
00:16:29,279 --> 00:16:31,800
then basically what I can do is use them

399
00:16:31,800 --> 00:16:33,899
to take decisions forward in time and

400
00:16:33,899 --> 00:16:35,940
what we have observed is that this idea

401
00:16:35,940 --> 00:16:38,519
works that

402
00:16:38,519 --> 00:16:40,259
um I can calculate the expected free

403
00:16:40,259 --> 00:16:43,019
energy backwards uh step by step

404
00:16:43,019 --> 00:16:46,920
considering them as sub problems and

405
00:16:46,920 --> 00:16:49,079
the fundamental difference is that in

406
00:16:49,079 --> 00:16:51,600
sophisticated inference uh to calculate

407
00:16:51,600 --> 00:16:54,959
the expected free energy at time small T

408
00:16:54,959 --> 00:16:57,540
you don't know what is the expected free

409
00:16:57,540 --> 00:16:59,880
energy at time team plus one so this

410
00:16:59,880 --> 00:17:01,920
becomes a three search so that you have

411
00:17:01,920 --> 00:17:04,319
to calculate this first and to calculate

412
00:17:04,319 --> 00:17:06,839
for t plus 1 you need t plus 2 and so on

413
00:17:06,839 --> 00:17:09,000
but here because you are calculating it

414
00:17:09,000 --> 00:17:11,339
backwards in time you already know what

415
00:17:11,339 --> 00:17:13,559
is uh the expected free energy for p

416
00:17:13,559 --> 00:17:16,020
plus 1 and you base it's basically the

417
00:17:16,020 --> 00:17:17,520
same equation it's just that you're

418
00:17:17,520 --> 00:17:20,900
doing it backwards in time and

419
00:17:20,900 --> 00:17:23,520
pictorially in sophisticated inference

420
00:17:23,520 --> 00:17:25,619
you are trying to do a research but in

421
00:17:25,619 --> 00:17:27,660
the dynamic programming algorithm you're

422
00:17:27,660 --> 00:17:30,120
doing your planning backwards uh using

423
00:17:30,120 --> 00:17:32,940
tables and given your planning Horizon

424
00:17:32,940 --> 00:17:35,520
is sufficient enough for a problem what

425
00:17:35,520 --> 00:17:38,160
we have seen is that the agent will be

426
00:17:38,160 --> 00:17:40,860
able to take optimal actions forward in

427
00:17:40,860 --> 00:17:44,340
time so in the paper we are also

428
00:17:44,340 --> 00:17:46,500
proposing uh one algorithm for

429
00:17:46,500 --> 00:17:49,020
sequential form DPS using uh this

430
00:17:49,020 --> 00:17:50,880
backward planning in time and we were

431
00:17:50,880 --> 00:17:54,059
able to scale up simulations for grid

432
00:17:54,059 --> 00:17:56,240
spaces which was previously interactable

433
00:17:56,240 --> 00:17:59,340
without neural networks

434
00:17:59,340 --> 00:18:01,080
um so that was the first solution

435
00:18:01,080 --> 00:18:03,840
so the second solution is that

436
00:18:03,840 --> 00:18:06,000
um so in the first solution it was fixed

437
00:18:06,000 --> 00:18:07,980
that you only get this past private

438
00:18:07,980 --> 00:18:09,539
reference you don't get any other

439
00:18:09,539 --> 00:18:10,919
information you only get your

440
00:18:10,919 --> 00:18:12,480
information about the final goal State

441
00:18:12,480 --> 00:18:15,120
and all you get is the model of the

442
00:18:15,120 --> 00:18:18,240
environment which you learned and you

443
00:18:18,240 --> 00:18:21,059
basically have to take decisions but the

444
00:18:21,059 --> 00:18:22,799
second solution of course is that if you

445
00:18:22,799 --> 00:18:24,600
are allowed you can attempt to learn a

446
00:18:24,600 --> 00:18:26,100
pride preference which is Meaningful

447
00:18:26,100 --> 00:18:28,620
like the one we saw in the previous

448
00:18:28,620 --> 00:18:31,740
slides which has the information about

449
00:18:31,740 --> 00:18:33,960
the other states also but how do you

450
00:18:33,960 --> 00:18:35,400
learn it right so

451
00:18:35,400 --> 00:18:38,039
there is a seminal work from optimal

452
00:18:38,039 --> 00:18:40,440
control literature that talks about

453
00:18:40,440 --> 00:18:43,380
efficient computation of optimal actions

454
00:18:43,380 --> 00:18:45,900
and in that work there is a quantity

455
00:18:45,900 --> 00:18:47,760
similar to our prior preference which is

456
00:18:47,760 --> 00:18:49,500
called the desirability function so for

457
00:18:49,500 --> 00:18:52,200
example in this grid World

458
00:18:52,200 --> 00:18:54,240
um here darker colors are more preferred

459
00:18:54,240 --> 00:18:56,400
so if this crosses your final gold state

460
00:18:56,400 --> 00:18:58,740
what the agent in this paper is trying

461
00:18:58,740 --> 00:19:01,980
to do uh or the said learning method in

462
00:19:01,980 --> 00:19:04,580
this paper is trying to do is learn this

463
00:19:04,580 --> 00:19:06,900
desirability function as optimally as

464
00:19:06,900 --> 00:19:09,059
possible and what has been shown in this

465
00:19:09,059 --> 00:19:12,660
paper is that if you try to learn the

466
00:19:12,660 --> 00:19:15,179
desirability function using a particular

467
00:19:15,179 --> 00:19:18,120
learning rule it is computationally far

468
00:19:18,120 --> 00:19:20,340
more efficient than even Q learning

469
00:19:20,340 --> 00:19:22,919
which is a well-known uh reinforcement

470
00:19:22,919 --> 00:19:24,960
learning algorithm so Q learning is a

471
00:19:24,960 --> 00:19:27,179
well-known computationally optimal

472
00:19:27,179 --> 00:19:29,520
algorithm but in this paper that

473
00:19:29,520 --> 00:19:31,140
particular learning rule for learning

474
00:19:31,140 --> 00:19:32,880
this kind of a desirability function is

475
00:19:32,880 --> 00:19:35,539
way faster and this approximate

476
00:19:35,539 --> 00:19:38,460
represents how different it is from the

477
00:19:38,460 --> 00:19:40,559
optimal desirability function so this

478
00:19:40,559 --> 00:19:42,480
desirability function is nothing but our

479
00:19:42,480 --> 00:19:45,179
prior preference and

480
00:19:45,179 --> 00:19:48,600
as mentioned learning said is borders of

481
00:19:48,600 --> 00:19:50,100
magnitude efficient than learning the Q

482
00:19:50,100 --> 00:19:53,039
function in Q learning so

483
00:19:53,039 --> 00:19:55,860
this is the particular learning rule

484
00:19:55,860 --> 00:19:57,600
depending upon the reward that it gets

485
00:19:57,600 --> 00:19:59,820
from the environment and in this

486
00:19:59,820 --> 00:20:01,860
particular grid World environment we are

487
00:20:01,860 --> 00:20:03,539
only giving it reward at the last step

488
00:20:03,539 --> 00:20:05,220
which is similar to the sparse friend

489
00:20:05,220 --> 00:20:08,580
preference the agent basically gets no

490
00:20:08,580 --> 00:20:11,400
reward uh until it reaches that final

491
00:20:11,400 --> 00:20:14,760
goal State and with this learning rule

492
00:20:14,760 --> 00:20:17,640
which which has a parameter ETA that

493
00:20:17,640 --> 00:20:20,100
controls how fast or slow

494
00:20:20,100 --> 00:20:22,980
this Learning Happens so basically we

495
00:20:22,980 --> 00:20:24,200
try to

496
00:20:24,200 --> 00:20:26,880
study the effect of this learning

497
00:20:26,880 --> 00:20:29,340
parameter Rita but what we observed is

498
00:20:29,340 --> 00:20:32,400
that it's very robust it can learn the

499
00:20:32,400 --> 00:20:35,059
prior preference reliably even with

500
00:20:35,059 --> 00:20:38,700
variable values of this uh learning

501
00:20:38,700 --> 00:20:41,580
parameter and what we see is that the

502
00:20:41,580 --> 00:20:43,620
agent is able to learn a meaningful

503
00:20:43,620 --> 00:20:45,539
prior performance over time very fast

504
00:20:45,539 --> 00:20:48,600
and using such a meaningful Prime

505
00:20:48,600 --> 00:20:50,700
preference then the agent don't have to

506
00:20:50,700 --> 00:20:53,580
plan a lot it can manage optimal

507
00:20:53,580 --> 00:20:56,160
Behavior or purposeful Behavior with

508
00:20:56,160 --> 00:20:59,700
very low time Horizons of planning and

509
00:20:59,700 --> 00:21:02,460
given these two solutions we could scale

510
00:21:02,460 --> 00:21:04,679
up active inference algorithm for

511
00:21:04,679 --> 00:21:07,919
decision making and talking about the

512
00:21:07,919 --> 00:21:11,400
computational efficiency uh we saw that

513
00:21:11,400 --> 00:21:13,320
the dynamic programming method could

514
00:21:13,320 --> 00:21:16,440
plant 30 time steps into the future uh

515
00:21:16,440 --> 00:21:18,360
with only say a computational complexity

516
00:21:18,360 --> 00:21:21,120
of thousand when compared to say 10 to

517
00:21:21,120 --> 00:21:23,700
the power 68 for sophisticated inference

518
00:21:23,700 --> 00:21:26,880
and the second method which learns the

519
00:21:26,880 --> 00:21:29,580
prior preference uh which we call Active

520
00:21:29,580 --> 00:21:32,340
inference and it only needs to plan one

521
00:21:32,340 --> 00:21:34,740
time step ahead so it is way more

522
00:21:34,740 --> 00:21:36,919
computationally efficient in that sense

523
00:21:36,919 --> 00:21:39,659
but it has to learn so in the DPFE

524
00:21:39,659 --> 00:21:41,640
method we are not learning we are using

525
00:21:41,640 --> 00:21:43,799
the spots by preference and doing the

526
00:21:43,799 --> 00:21:44,520
whole

527
00:21:44,520 --> 00:21:46,500
depth of planning and in the other

528
00:21:46,500 --> 00:21:48,240
method we are letting the agent learn

529
00:21:48,240 --> 00:21:50,280
this by preference but then we can save

530
00:21:50,280 --> 00:21:53,280
planning a lot because it knows a lot of

531
00:21:53,280 --> 00:21:57,299
what to do in time right so

532
00:21:57,299 --> 00:22:00,179
graphically we can see that um

533
00:22:00,179 --> 00:22:01,799
the DPFE method is really

534
00:22:01,799 --> 00:22:04,020
computationally efficient and when

535
00:22:04,020 --> 00:22:06,600
plotted Against Time uh in the AI 50

536
00:22:06,600 --> 00:22:08,520
equal to one method

537
00:22:08,520 --> 00:22:10,740
um it is basically computationally

538
00:22:10,740 --> 00:22:12,659
cheaper

539
00:22:12,659 --> 00:22:16,220
and yeah so it scales uh very

540
00:22:16,220 --> 00:22:19,860
fascinatingly well uh with Higher and

541
00:22:19,860 --> 00:22:21,720
Higher and

542
00:22:21,720 --> 00:22:24,120
complexity of the environment so we

543
00:22:24,120 --> 00:22:27,600
tested it uh this these methods in

544
00:22:27,600 --> 00:22:28,380
um

545
00:22:28,380 --> 00:22:31,020
very huge State spaces like say 900

546
00:22:31,020 --> 00:22:32,000
States

547
00:22:32,000 --> 00:22:33,840
compared to

548
00:22:33,840 --> 00:22:36,120
um State spaces which has dimension of

549
00:22:36,120 --> 00:22:38,640
say 5 or 10 in the usually seen active

550
00:22:38,640 --> 00:22:40,559
influence literature

551
00:22:40,559 --> 00:22:43,200
um so I want to kind of emphasize that

552
00:22:43,200 --> 00:22:44,760
we are not using any neural networks

553
00:22:44,760 --> 00:22:47,039
here we are using um

554
00:22:47,039 --> 00:22:49,740
explainable active inference agents are

555
00:22:49,740 --> 00:22:52,200
doing all necessary matrix

556
00:22:52,200 --> 00:22:53,820
multiplication so that we have access

557
00:22:53,820 --> 00:22:55,140
and

558
00:22:55,140 --> 00:22:56,220
um

559
00:22:56,220 --> 00:22:58,620
explainability to every computation that

560
00:22:58,620 --> 00:23:00,179
is taking place in this in these

561
00:23:00,179 --> 00:23:02,280
algorithms and when tested on these

562
00:23:02,280 --> 00:23:04,559
grids first we validated this on a

563
00:23:04,559 --> 00:23:06,780
smaller grid with understates and we

564
00:23:06,780 --> 00:23:08,580
observed is that

565
00:23:08,580 --> 00:23:11,220
when compared to Benchmark reinforcement

566
00:23:11,220 --> 00:23:12,780
learning algorithms like queue learning

567
00:23:12,780 --> 00:23:15,059
and dynaq dynacy is a model based

568
00:23:15,059 --> 00:23:17,760
reinforcement learning algorithm and we

569
00:23:17,760 --> 00:23:20,280
compared our newly proposed agents which

570
00:23:20,280 --> 00:23:24,419
is DPFE and aif and we saw really good

571
00:23:24,419 --> 00:23:27,900
performance and aif agent is slightly

572
00:23:27,900 --> 00:23:29,220
worse and that's because it's only

573
00:23:29,220 --> 00:23:31,080
planning one time step ahead right but

574
00:23:31,080 --> 00:23:34,500
the DPFE agent who does full time

575
00:23:34,500 --> 00:23:36,419
planning it's performing as good as

576
00:23:36,419 --> 00:23:37,919
benchmarking personal learning

577
00:23:37,919 --> 00:23:41,039
algorithms and we tested it with um

578
00:23:41,039 --> 00:23:44,640
bigger and weaker grids and when we

579
00:23:44,640 --> 00:23:47,100
introduced stochasticity in the gold

580
00:23:47,100 --> 00:23:49,620
state in the sense that when the agent

581
00:23:49,620 --> 00:23:53,039
had to navigate to

582
00:23:53,039 --> 00:23:54,480
um

583
00:23:54,480 --> 00:23:56,520
stochastic gold state so we changed the

584
00:23:56,520 --> 00:23:59,460
gold States at every 10 episodes and

585
00:23:59,460 --> 00:24:01,260
what we observed is that the Dyna queue

586
00:24:01,260 --> 00:24:04,140
took more time than our bpfe agent to

587
00:24:04,140 --> 00:24:07,200
kind of recover and do well so DPFE

588
00:24:07,200 --> 00:24:09,059
agent in this stochastic environment

589
00:24:09,059 --> 00:24:11,280
performed really well even better than

590
00:24:11,280 --> 00:24:13,620
the dynacy agents and you could also see

591
00:24:13,620 --> 00:24:16,500
that the AI phase and is recovering um

592
00:24:16,500 --> 00:24:19,140
faster but not as good as the other

593
00:24:19,140 --> 00:24:22,500
agents so basically that was the result

594
00:24:22,500 --> 00:24:25,200
in the paper and the methods So yeah

595
00:24:25,200 --> 00:24:27,960
thank you for listening and I'm open for

596
00:24:27,960 --> 00:24:30,380
discussions

597
00:24:31,640 --> 00:24:34,740
all right awesome

598
00:24:34,740 --> 00:24:36,360
wow

599
00:24:36,360 --> 00:24:38,520
very cool

600
00:24:38,520 --> 00:24:43,200
okay well just as we start the

601
00:24:43,200 --> 00:24:44,460
discussion

602
00:24:44,460 --> 00:24:47,280
um if anyone wants to add anything first

603
00:24:47,280 --> 00:24:50,220
just how did you come to work on this

604
00:24:50,220 --> 00:24:52,740
project uh were you studying active

605
00:24:52,740 --> 00:24:54,720
inference and you came to this question

606
00:24:54,720 --> 00:24:57,900
as being interesting or were you working

607
00:24:57,900 --> 00:25:00,179
on the planning and came to active

608
00:25:00,179 --> 00:25:03,059
inference as a method

609
00:25:03,059 --> 00:25:05,159
um yeah so little bit background about

610
00:25:05,159 --> 00:25:08,400
myself so I studied physics both in my

611
00:25:08,400 --> 00:25:11,340
undergrad and post that and towards the

612
00:25:11,340 --> 00:25:13,080
end of my post graduation I got

613
00:25:13,080 --> 00:25:15,120
interested in things like Game Theory

614
00:25:15,120 --> 00:25:17,880
and reinposable learning and I joined uh

615
00:25:17,880 --> 00:25:20,880
for a joint PhD uh with Professor Adil

616
00:25:20,880 --> 00:25:23,100
and Professor Manoj and so Professor

617
00:25:23,100 --> 00:25:25,200
Manoj is a control theory person and a

618
00:25:25,200 --> 00:25:28,140
deal is a neuroscientist and in the

619
00:25:28,140 --> 00:25:30,600
beginning of my PhD I started reading

620
00:25:30,600 --> 00:25:34,020
active inference literature and I wanted

621
00:25:34,020 --> 00:25:37,260
to implement that in problems and I was

622
00:25:37,260 --> 00:25:39,840
always fascinated with explainable

623
00:25:39,840 --> 00:25:42,720
active inference uh about this idea of

624
00:25:42,720 --> 00:25:44,700
expected free energy trying to minimize

625
00:25:44,700 --> 00:25:46,860
risk as well as expected ambiguity it's

626
00:25:46,860 --> 00:25:49,740
also not just making the agents work but

627
00:25:49,740 --> 00:25:52,020
also being able to tell how they work is

628
00:25:52,020 --> 00:25:54,419
what fascinated me in active influence

629
00:25:54,419 --> 00:25:56,820
and initially I try to implement them

630
00:25:56,820 --> 00:26:00,179
and there is a conference paper which we

631
00:26:00,179 --> 00:26:03,720
published in which we compare it for a

632
00:26:03,720 --> 00:26:06,419
similar gridual task and that then I

633
00:26:06,419 --> 00:26:08,100
faced this problem of scaling active

634
00:26:08,100 --> 00:26:10,620
inference and then I started working on

635
00:26:10,620 --> 00:26:12,659
sophisticated inference

636
00:26:12,659 --> 00:26:15,539
um yeah so basically these methods came

637
00:26:15,539 --> 00:26:17,580
out of the need that I wanted to scale

638
00:26:17,580 --> 00:26:21,539
the map up and I always had bus keptical

639
00:26:21,539 --> 00:26:22,919
in using

640
00:26:22,919 --> 00:26:25,559
um deep active inference because

641
00:26:25,559 --> 00:26:28,020
um I didn't want to use neural networks

642
00:26:28,020 --> 00:26:30,539
to do planning Because deep

643
00:26:30,539 --> 00:26:32,100
reinforcement learning in itself is a

644
00:26:32,100 --> 00:26:34,140
huge field and

645
00:26:34,140 --> 00:26:37,080
um if you if you are about just scaling

646
00:26:37,080 --> 00:26:39,240
active inference then maybe just do deep

647
00:26:39,240 --> 00:26:41,820
reinforcement learning and that's what I

648
00:26:41,820 --> 00:26:43,860
thought yeah so that's basically the

649
00:26:43,860 --> 00:26:45,659
background and yeah

650
00:26:45,659 --> 00:26:48,720
that's how it came about

651
00:26:48,720 --> 00:26:50,880
all right I'll go first to a question in

652
00:26:50,880 --> 00:26:52,559
the live chat and we'll just

653
00:26:52,559 --> 00:26:54,779
probably discuss various aspects because

654
00:26:54,779 --> 00:26:57,720
there was a lot in your presentation so

655
00:26:57,720 --> 00:27:01,980
NL Dawn writes I wonder when it comes to

656
00:27:01,980 --> 00:27:04,620
Computing the expected free energy over

657
00:27:04,620 --> 00:27:07,679
a Time Horizon what kind of mean field

658
00:27:07,679 --> 00:27:11,039
approximation was used to factorize Q of

659
00:27:11,039 --> 00:27:13,020
s

660
00:27:13,020 --> 00:27:16,100
store so

661
00:27:19,980 --> 00:27:23,040
so I assume that the question is about

662
00:27:23,040 --> 00:27:24,299
[Music]

663
00:27:24,299 --> 00:27:28,799
the expected free energy in DPFE and

664
00:27:28,799 --> 00:27:31,380
to calculate this basically

665
00:27:31,380 --> 00:27:32,159
um

666
00:27:32,159 --> 00:27:35,279
so in my simulations I use belief

667
00:27:35,279 --> 00:27:39,120
propagation for this belief queue and

668
00:27:39,120 --> 00:27:41,460
you can also also use variational

669
00:27:41,460 --> 00:27:43,140
message passing or marginal message

670
00:27:43,140 --> 00:27:47,279
passing that's not a problem but

671
00:27:47,400 --> 00:27:50,580
so once you have this belief queue

672
00:27:50,580 --> 00:27:52,740
um what do you do right

673
00:27:52,740 --> 00:27:55,340
so

674
00:27:56,940 --> 00:27:59,900
yeah so

675
00:27:59,940 --> 00:28:04,260
when I say you imagine Q for a time then

676
00:28:04,260 --> 00:28:07,520
what I use mostly is one hot Vector so

677
00:28:07,520 --> 00:28:11,220
to take decisions you use the belief

678
00:28:11,220 --> 00:28:13,260
that you get out of your perception step

679
00:28:13,260 --> 00:28:16,380
in active inference but for imagining

680
00:28:16,380 --> 00:28:18,720
these are hard tables where they are one

681
00:28:18,720 --> 00:28:22,020
hot Vector so given say you have in your

682
00:28:22,020 --> 00:28:25,380
generative model has 10 states uh your

683
00:28:25,380 --> 00:28:27,900
the queues that you use are

684
00:28:27,900 --> 00:28:29,100
um

685
00:28:29,100 --> 00:28:32,220
precise cues for planning but for

686
00:28:32,220 --> 00:28:33,840
decision making you use the impress

687
00:28:33,840 --> 00:28:36,299
imprecise mean field approximation queue

688
00:28:36,299 --> 00:28:38,460
that you get from the perception step so

689
00:28:38,460 --> 00:28:40,679
I don't know if that answers the

690
00:28:40,679 --> 00:28:42,059
question

691
00:28:42,059 --> 00:28:44,880
um but maybe I I also want to think I

692
00:28:44,880 --> 00:28:46,919
also need to think about more about the

693
00:28:46,919 --> 00:28:49,200
approximations that may be there in the

694
00:28:49,200 --> 00:28:50,880
steps

695
00:28:50,880 --> 00:28:53,700
cool yeah they can they can write more

696
00:28:53,700 --> 00:28:56,460
if they want um let's talk a little bit

697
00:28:56,460 --> 00:29:00,419
more generally about preference learning

698
00:29:00,419 --> 00:29:02,700
so in the context of the active

699
00:29:02,700 --> 00:29:05,940
inference generative model we have a

700
00:29:05,940 --> 00:29:08,279
mediating between observations in Hidden

701
00:29:08,279 --> 00:29:10,020
States and learning makes

702
00:29:10,020 --> 00:29:12,480
a lot of sense it's about learning the

703
00:29:12,480 --> 00:29:14,220
mapping between observations in Hidden

704
00:29:14,220 --> 00:29:15,779
states of the world and then we have

705
00:29:15,779 --> 00:29:18,240
blearning learning the consequences of

706
00:29:18,240 --> 00:29:20,399
action and how things change Through

707
00:29:20,399 --> 00:29:21,419
Time

708
00:29:21,419 --> 00:29:24,659
and preference learning is learning on c

709
00:29:24,659 --> 00:29:27,960
c and you've highlighted that that this

710
00:29:27,960 --> 00:29:29,700
is a very um

711
00:29:29,700 --> 00:29:31,340
you know

712
00:29:31,340 --> 00:29:34,320
interesting variable to learn

713
00:29:34,320 --> 00:29:38,100
and I'm curious how does how do we learn

714
00:29:38,100 --> 00:29:41,159
to learn the right thing how do we know

715
00:29:41,159 --> 00:29:42,960
that we're learning an Adaptive

716
00:29:42,960 --> 00:29:44,399
preference

717
00:29:44,399 --> 00:29:46,200
and then how does that preference

718
00:29:46,200 --> 00:29:48,720
learning reduce cognitive overhead or

719
00:29:48,720 --> 00:29:50,820
computational complexity

720
00:29:50,820 --> 00:29:53,460
yeah great

721
00:29:53,460 --> 00:29:54,480
um so

722
00:29:54,480 --> 00:29:57,179
if you are trying to learn a prayer

723
00:29:57,179 --> 00:29:59,940
preference then that assumes that there

724
00:29:59,940 --> 00:30:01,559
is something to Chase or there is

725
00:30:01,559 --> 00:30:04,500
something to maximize like a reward so

726
00:30:04,500 --> 00:30:06,779
in this say uh in a reinforcement

727
00:30:06,779 --> 00:30:08,340
learning setting there is clear reward

728
00:30:08,340 --> 00:30:09,840
that's coming from the environment that

729
00:30:09,840 --> 00:30:12,960
you're trying to maximize and say for

730
00:30:12,960 --> 00:30:15,000
this grid you get that reward only in

731
00:30:15,000 --> 00:30:17,220
the last step that which is that that's

732
00:30:17,220 --> 00:30:19,020
that's what makes this problem hard so

733
00:30:19,020 --> 00:30:22,500
if you are getting rewards at every time

734
00:30:22,500 --> 00:30:25,140
step then basically you know what to do

735
00:30:25,140 --> 00:30:28,020
that I just have to pursue that reward

736
00:30:28,020 --> 00:30:30,419
at every time step right but here you

737
00:30:30,419 --> 00:30:32,880
might have to take say 15 time steps

738
00:30:32,880 --> 00:30:36,059
ahead to get that one reward and that

739
00:30:36,059 --> 00:30:39,659
basically is hard to do so here um if

740
00:30:39,659 --> 00:30:41,520
you are trying because I'm trying to

741
00:30:41,520 --> 00:30:43,020
learn a prior preference

742
00:30:43,020 --> 00:30:44,820
um there should be a reward structure

743
00:30:44,820 --> 00:30:47,940
that uh that exists in the environment

744
00:30:47,940 --> 00:30:49,919
if the environment doesn't care about

745
00:30:49,919 --> 00:30:52,200
what I do or if I can't Define what is

746
00:30:52,200 --> 00:30:54,840
good or bad then there is no meaning in

747
00:30:54,840 --> 00:30:56,340
learning the prior preference right so

748
00:30:56,340 --> 00:30:57,840
here

749
00:30:57,840 --> 00:30:59,399
um the reward is what controls the

750
00:30:59,399 --> 00:31:01,559
learning of the Sprite difference and

751
00:31:01,559 --> 00:31:03,600
the good thing is that even if only I

752
00:31:03,600 --> 00:31:06,539
get the reward at the last time step

753
00:31:06,539 --> 00:31:09,240
um I have my B matrices and I have

754
00:31:09,240 --> 00:31:11,340
experience of transitioning from

755
00:31:11,340 --> 00:31:12,659
different states

756
00:31:12,659 --> 00:31:14,760
um which I reached this final gold state

757
00:31:14,760 --> 00:31:16,860
and

758
00:31:16,860 --> 00:31:19,440
this algorithm that we are using or the

759
00:31:19,440 --> 00:31:21,539
learning rule that we are using is

760
00:31:21,539 --> 00:31:23,820
precisely learning a similar thing in

761
00:31:23,820 --> 00:31:26,340
optimal control that paper I introduced

762
00:31:26,340 --> 00:31:27,539
which is called the desirability

763
00:31:27,539 --> 00:31:30,539
function and given that you have this

764
00:31:30,539 --> 00:31:32,760
desirability function so imagine that

765
00:31:32,760 --> 00:31:36,120
maybe I am starting from this state then

766
00:31:36,120 --> 00:31:37,860
I just have to look at my nearest

767
00:31:37,860 --> 00:31:40,399
neighbors to take a decision

768
00:31:40,399 --> 00:31:44,279
if the stated

769
00:31:44,279 --> 00:31:46,620
um State below the state is more

770
00:31:46,620 --> 00:31:48,899
preferred than I only have to plan one

771
00:31:48,899 --> 00:31:50,520
time instead ahead and that's the

772
00:31:50,520 --> 00:31:52,380
optimal decision I have to take

773
00:31:52,380 --> 00:31:54,059
so learning this Pride preference

774
00:31:54,059 --> 00:31:55,919
reduces the cognitive load in the sense

775
00:31:55,919 --> 00:31:57,419
that I'm only now looking at nearest

776
00:31:57,419 --> 00:31:59,760
neighbors I don't have to plan all the

777
00:31:59,760 --> 00:32:01,799
way up to this final good state

778
00:32:01,799 --> 00:32:04,799
and I will learn this as efficiently as

779
00:32:04,799 --> 00:32:07,260
possible because one it was it is a

780
00:32:07,260 --> 00:32:09,659
guaranteed algorithm we tested its

781
00:32:09,659 --> 00:32:13,320
robustness and it is also informed by

782
00:32:13,320 --> 00:32:14,580
the reward that I get from the

783
00:32:14,580 --> 00:32:15,659
environment and

784
00:32:15,659 --> 00:32:20,100
if yeah so if there is some way uh to

785
00:32:20,100 --> 00:32:22,860
kind of Define what is preferred uh then

786
00:32:22,860 --> 00:32:24,840
you are guaranteed to learn a private

787
00:32:24,840 --> 00:32:26,520
friends that's meaningful

788
00:32:26,520 --> 00:32:28,559
by this algorithm

789
00:32:28,559 --> 00:32:31,559
so if only nearest Neighbors

790
00:32:31,559 --> 00:32:34,260
and one time step inference is being

791
00:32:34,260 --> 00:32:37,320
used then what prevents this kind of

792
00:32:37,320 --> 00:32:39,360
preference learning agent from being

793
00:32:39,360 --> 00:32:43,158
stuck in a local Optima

794
00:32:43,320 --> 00:32:47,520
um yeah so there won't be local Optimas

795
00:32:47,520 --> 00:32:50,340
here that's the point like

796
00:32:50,340 --> 00:32:52,860
um why would there be a local Optima uh

797
00:32:52,860 --> 00:32:57,059
if I am learning it using rewards uh

798
00:32:57,059 --> 00:33:00,120
yeah so if if there is local Optima then

799
00:33:00,120 --> 00:33:02,159
it will be getting stuck in the local

800
00:33:02,159 --> 00:33:05,460
Optima but there won't be one uh if

801
00:33:05,460 --> 00:33:07,440
you're learning is this way because

802
00:33:07,440 --> 00:33:08,760
you're learning it from your own

803
00:33:08,760 --> 00:33:11,940
experience and how you got the reward at

804
00:33:11,940 --> 00:33:14,220
time uh sometime and what we have

805
00:33:14,220 --> 00:33:16,980
observed is that uh it's very

806
00:33:16,980 --> 00:33:17,760
um

807
00:33:17,760 --> 00:33:20,399
gradual and there is no glitches or

808
00:33:20,399 --> 00:33:23,880
local Optimas when you learn such as

809
00:33:23,880 --> 00:33:27,840
so it's kind of back filling a

810
00:33:27,840 --> 00:33:30,179
preference

811
00:33:30,179 --> 00:33:33,539
so that there can be a smooth path

812
00:33:33,539 --> 00:33:36,779
yeah to the distal goal

813
00:33:36,779 --> 00:33:39,480
yes so it in the animation kind of makes

814
00:33:39,480 --> 00:33:42,299
it look like a back feeling but

815
00:33:42,299 --> 00:33:44,279
um you're actually learning it from your

816
00:33:44,279 --> 00:33:45,360
experience

817
00:33:45,360 --> 00:33:46,620
um

818
00:33:46,620 --> 00:33:49,980
forward in time right so I observed a

819
00:33:49,980 --> 00:33:51,480
reward when I transitioned from this

820
00:33:51,480 --> 00:33:53,100
state to the state so this must be good

821
00:33:53,100 --> 00:33:55,860
uh then the in the in the next time step

822
00:33:55,860 --> 00:33:58,559
I say okay this date is responsible for

823
00:33:58,559 --> 00:34:00,419
taking me there so this might be good

824
00:34:00,419 --> 00:34:03,120
also but not as good as the other one

825
00:34:03,120 --> 00:34:05,100
so it looks like it's learning backwards

826
00:34:05,100 --> 00:34:07,799
but it's actually learning from the real

827
00:34:07,799 --> 00:34:12,000
um into t plus one experience

828
00:34:13,500 --> 00:34:15,119
okay

829
00:34:15,119 --> 00:34:16,980
you know so in that way

830
00:34:16,980 --> 00:34:20,339
um you can't have local Maximas because

831
00:34:20,339 --> 00:34:21,300
um

832
00:34:21,300 --> 00:34:24,119
yeah this is the learning rule that does

833
00:34:24,119 --> 00:34:25,859
that so I don't know how to answer it

834
00:34:25,859 --> 00:34:29,219
more systematically yeah yeah

835
00:34:29,219 --> 00:34:33,000
um what what real world situations or or

836
00:34:33,000 --> 00:34:35,399
what real life situations do you kind of

837
00:34:35,399 --> 00:34:37,560
see this kind of preference learning

838
00:34:37,560 --> 00:34:40,080
happening in

839
00:34:40,080 --> 00:34:41,159
yeah

840
00:34:41,159 --> 00:34:43,020
great question so

841
00:34:43,020 --> 00:34:46,080
so there is one paper that came out from

842
00:34:46,080 --> 00:34:48,659
our lab where neurons are laying

843
00:34:48,659 --> 00:34:51,359
learning the game of pong

844
00:34:51,359 --> 00:34:52,980
um so there was so if the paper is

845
00:34:52,980 --> 00:34:55,260
called dist brain uh and the device is

846
00:34:55,260 --> 00:34:57,000
called strain and what they have

847
00:34:57,000 --> 00:34:58,740
accomplished is that they manage to

848
00:34:58,740 --> 00:35:00,300
culture neurons

849
00:35:00,300 --> 00:35:03,420
on a silicon chip and they

850
00:35:03,420 --> 00:35:04,020
um

851
00:35:04,020 --> 00:35:08,520
gave it a good uh feedback signal when

852
00:35:08,520 --> 00:35:11,400
it successfully tackled the ball or

853
00:35:11,400 --> 00:35:13,880
played the game well and gave it a shock

854
00:35:13,880 --> 00:35:15,720
uh when

855
00:35:15,720 --> 00:35:18,119
um it made mistakes and what we saw is

856
00:35:18,119 --> 00:35:19,560
that

857
00:35:19,560 --> 00:35:22,260
in the paper what we see is that it

858
00:35:22,260 --> 00:35:24,140
learns to play the game over time right

859
00:35:24,140 --> 00:35:27,359
and in in such a scenario imagine that

860
00:35:27,359 --> 00:35:29,880
if I encounter say a positive thing in

861
00:35:29,880 --> 00:35:33,359
the world then I might associate

862
00:35:33,359 --> 00:35:35,520
um what is good with the states I

863
00:35:35,520 --> 00:35:37,380
observed in the past and so on right so

864
00:35:37,380 --> 00:35:38,880
I learned

865
00:35:38,880 --> 00:35:39,660
um

866
00:35:39,660 --> 00:35:42,119
step by step what is good or bad and

867
00:35:42,119 --> 00:35:44,700
that's a reasonable hypothesis to make

868
00:35:44,700 --> 00:35:46,619
so if

869
00:35:46,619 --> 00:35:47,220
um

870
00:35:47,220 --> 00:35:51,300
if I want to say get fitter then I might

871
00:35:51,300 --> 00:35:53,280
associate going to gym

872
00:35:53,280 --> 00:35:54,660
um

873
00:35:54,660 --> 00:35:57,000
as a good thing then I might also

874
00:35:57,000 --> 00:35:59,099
associate walking to the gym as a good

875
00:35:59,099 --> 00:36:01,260
thing then I might associate uh say

876
00:36:01,260 --> 00:36:03,900
wearing my shoes as a good thing so

877
00:36:03,900 --> 00:36:04,920
um

878
00:36:04,920 --> 00:36:06,960
learning prior preference that way makes

879
00:36:06,960 --> 00:36:10,079
sense I would say but yeah this is a

880
00:36:10,079 --> 00:36:11,640
computational

881
00:36:11,640 --> 00:36:12,480
um

882
00:36:12,480 --> 00:36:14,820
solution to the problem of curse of

883
00:36:14,820 --> 00:36:17,040
dimensionality but testing this on the

884
00:36:17,040 --> 00:36:19,619
real world is definitely uh what we

885
00:36:19,619 --> 00:36:21,599
should do and

886
00:36:21,599 --> 00:36:24,119
um yeah I also look forward to doing

887
00:36:24,119 --> 00:36:25,800
that

888
00:36:25,800 --> 00:36:27,660
okay let me let me try to run a real

889
00:36:27,660 --> 00:36:29,760
world situation by you and see if this

890
00:36:29,760 --> 00:36:32,040
if this connects

891
00:36:32,040 --> 00:36:35,099
um so we wanna we want to turn in we're

892
00:36:35,099 --> 00:36:38,280
rewarded by turning in an essay that

893
00:36:38,280 --> 00:36:39,900
gets a high grade

894
00:36:39,900 --> 00:36:42,599
and so there's many steps between

895
00:36:42,599 --> 00:36:45,540
starting the idea of the essay and

896
00:36:45,540 --> 00:36:48,839
seeing that preferred sparse outcome

897
00:36:48,839 --> 00:36:51,180
which is the grade

898
00:36:51,180 --> 00:36:53,520
um and we we do all kinds of things and

899
00:36:53,520 --> 00:36:55,260
then we learn okay well

900
00:36:55,260 --> 00:36:57,079
um this one where I did get a good grade

901
00:36:57,079 --> 00:36:59,940
it was well formatted

902
00:36:59,940 --> 00:37:02,880
and then that kind now you sort of

903
00:37:02,880 --> 00:37:05,460
expand the umbrella of your preference

904
00:37:05,460 --> 00:37:07,800
out and then what brought me to getting

905
00:37:07,800 --> 00:37:10,140
it well formatted well I did it on time

906
00:37:10,140 --> 00:37:12,900
and then you kind of work all the way to

907
00:37:12,900 --> 00:37:16,579
the ideation phase so that in the future

908
00:37:16,579 --> 00:37:20,640
you can do one step inference yes

909
00:37:20,640 --> 00:37:23,099
exactly and just take it step by step as

910
00:37:23,099 --> 00:37:24,180
a skill

911
00:37:24,180 --> 00:37:26,400
instead of needing to do

912
00:37:26,400 --> 00:37:28,740
a 15 time step

913
00:37:28,740 --> 00:37:31,740
inference over all the possible tree

914
00:37:31,740 --> 00:37:35,180
structures so you've kind of used your

915
00:37:35,180 --> 00:37:39,240
embodied learnings to simplify the

916
00:37:39,240 --> 00:37:43,680
structure of the problem exactly yeah

917
00:37:43,680 --> 00:37:45,540
and can we look at the computational

918
00:37:45,540 --> 00:37:47,220
complexities

919
00:37:47,220 --> 00:37:51,078
um again of the of all them

920
00:37:51,359 --> 00:37:54,300
so here CIF stands for the classical

921
00:37:54,300 --> 00:37:56,760
active inference and to do the full

922
00:37:56,760 --> 00:37:58,560
Horizon of planning

923
00:37:58,560 --> 00:38:00,660
um it's around 10 to the power 18. so

924
00:38:00,660 --> 00:38:03,180
this is for this particular grid example

925
00:38:03,180 --> 00:38:07,140
with 100 States and four available

926
00:38:07,140 --> 00:38:09,960
actions so this is for a special case

927
00:38:09,960 --> 00:38:12,119
and I just wanted to put numbers to put

928
00:38:12,119 --> 00:38:14,400
things in perspective so for this

929
00:38:14,400 --> 00:38:16,500
particular example if you are trying to

930
00:38:16,500 --> 00:38:18,960
do it with the policy space thing

931
00:38:18,960 --> 00:38:20,640
um you will have to do

932
00:38:20,640 --> 00:38:22,440
um 10 to the power 18 computations for

933
00:38:22,440 --> 00:38:25,380
one step of planning or in one

934
00:38:25,380 --> 00:38:28,380
um instance in sophisticated inference

935
00:38:28,380 --> 00:38:30,900
it's even worse because uh there you go

936
00:38:30,900 --> 00:38:34,560
the state space Also matters uh and that

937
00:38:34,560 --> 00:38:37,140
won't work so this third line was

938
00:38:37,140 --> 00:38:39,540
supposed to kind of show that even for a

939
00:38:39,540 --> 00:38:40,500
time

940
00:38:40,500 --> 00:38:42,599
um or rise enough two it's really hard

941
00:38:42,599 --> 00:38:44,280
to do sophisticated inference if you

942
00:38:44,280 --> 00:38:47,400
have to do the full planning but with

943
00:38:47,400 --> 00:38:48,960
dynamic programming when you're planning

944
00:38:48,960 --> 00:38:50,579
backwards

945
00:38:50,579 --> 00:38:52,500
um you can attempt to do the full

946
00:38:52,500 --> 00:38:54,420
Horizon of planning

947
00:38:54,420 --> 00:38:56,520
um and that's only a thousand

948
00:38:56,520 --> 00:38:58,800
computations but with learning

949
00:38:58,800 --> 00:39:01,380
preference it's even low uh when you do

950
00:39:01,380 --> 00:39:04,520
it just one time Step Ahead

951
00:39:04,520 --> 00:39:06,440
wow this is quite

952
00:39:06,440 --> 00:39:06,839
[Music]

953
00:39:06,839 --> 00:39:08,220
um

954
00:39:08,220 --> 00:39:12,000
quite Stark and in the branching time

955
00:39:12,000 --> 00:39:15,480
active inference work in a previous

956
00:39:15,480 --> 00:39:17,640
model stream we also saw some

957
00:39:17,640 --> 00:39:19,859
computational complexity estimates but I

958
00:39:19,859 --> 00:39:22,260
think these are really clear

959
00:39:22,260 --> 00:39:25,020
um first thing it made me think of is

960
00:39:25,020 --> 00:39:28,820
no one said sophistication was cheap

961
00:39:28,820 --> 00:39:32,940
I mean it's it's astronomical how costly

962
00:39:32,940 --> 00:39:35,099
it is for

963
00:39:35,099 --> 00:39:37,380
um maybe even only at two or three or

964
00:39:37,380 --> 00:39:38,880
four or five

965
00:39:38,880 --> 00:39:41,940
potentially starting to get up to so

966
00:39:41,940 --> 00:39:44,040
it's increasing the the complexity of

967
00:39:44,040 --> 00:39:45,720
planning

968
00:39:45,720 --> 00:39:50,040
um radically so it's very

969
00:39:50,040 --> 00:39:50,780
um

970
00:39:50,780 --> 00:39:52,460
interesting

971
00:39:52,460 --> 00:39:56,280
pedagogically as we think about like as

972
00:39:56,280 --> 00:39:57,660
we imagine

973
00:39:57,660 --> 00:39:59,940
active inference intelligence

974
00:39:59,940 --> 00:40:01,560
architectures

975
00:40:01,560 --> 00:40:04,500
but

976
00:40:04,500 --> 00:40:07,079
this makes it pretty clear that it's not

977
00:40:07,079 --> 00:40:09,060
something that you can just enumerate

978
00:40:09,060 --> 00:40:10,859
over

979
00:40:10,859 --> 00:40:13,380
and so I think it's very

980
00:40:13,380 --> 00:40:15,240
um very creative and important what

981
00:40:15,240 --> 00:40:19,520
you've done by connecting this to

982
00:40:19,520 --> 00:40:22,140
principled methods of computational

983
00:40:22,140 --> 00:40:25,440
complexity reduction rather than

984
00:40:25,440 --> 00:40:29,220
potentially effective but ad hoc methods

985
00:40:29,220 --> 00:40:31,320
of complexity reduction like neural

986
00:40:31,320 --> 00:40:33,359
networks which might work when they work

987
00:40:33,359 --> 00:40:35,280
but then once those start to get bloated

988
00:40:35,280 --> 00:40:38,940
now you have no principles and no

989
00:40:38,940 --> 00:40:40,680
efficacy

990
00:40:40,680 --> 00:40:43,200
exactly yeah and sophisticated inference

991
00:40:43,200 --> 00:40:45,240
I must note that has its own benefits

992
00:40:45,240 --> 00:40:47,579
when compared to dynamic programming uh

993
00:40:47,579 --> 00:40:49,500
in the sense that when you're planning

994
00:40:49,500 --> 00:40:51,839
forward you are taking into

995
00:40:51,839 --> 00:40:53,520
consideration all the possibilities

996
00:40:53,520 --> 00:40:55,859
right but when you're planning backwards

997
00:40:55,859 --> 00:40:58,380
say for example it's like cue based

998
00:40:58,380 --> 00:41:00,300
exploration

999
00:41:00,300 --> 00:41:02,280
um like if you have to go somewhere get

1000
00:41:02,280 --> 00:41:03,839
a queue and then navigate to the pool

1001
00:41:03,839 --> 00:41:05,640
State then planning backwards might not

1002
00:41:05,640 --> 00:41:06,960
work so this is what this was one

1003
00:41:06,960 --> 00:41:09,900
feedback I got and I discussed uh the

1004
00:41:09,900 --> 00:41:10,980
work with

1005
00:41:10,980 --> 00:41:13,920
um some people so

1006
00:41:13,920 --> 00:41:16,260
um that's a thing to note about dynamic

1007
00:41:16,260 --> 00:41:18,359
programming but the other place where

1008
00:41:18,359 --> 00:41:19,920
you learn prior to friends I believe

1009
00:41:19,920 --> 00:41:22,320
it's not a problem but yeah dynamic

1010
00:41:22,320 --> 00:41:23,700
programming is computationally cheap but

1011
00:41:23,700 --> 00:41:26,460
it also come with its own limitations I

1012
00:41:26,460 --> 00:41:28,099
must note here

1013
00:41:28,099 --> 00:41:31,200
just to restate that in dynamic

1014
00:41:31,200 --> 00:41:33,839
programming with a Bellman optimality

1015
00:41:33,839 --> 00:41:36,420
we're solving backwards and so it's kind

1016
00:41:36,420 --> 00:41:38,400
of like the Checkmate is what we want

1017
00:41:38,400 --> 00:41:40,440
and so now we're working backwards to

1018
00:41:40,440 --> 00:41:43,440
the present but we end up not exploring

1019
00:41:43,440 --> 00:41:46,440
counterfactual

1020
00:41:46,440 --> 00:41:49,200
end points we don't work back to the

1021
00:41:49,200 --> 00:41:51,540
present to end points that we're not

1022
00:41:51,540 --> 00:41:52,980
interested in

1023
00:41:52,980 --> 00:41:55,619
exactly so that's why it's so ruthless

1024
00:41:55,619 --> 00:41:58,320
but on the other hand it's a much more

1025
00:41:58,320 --> 00:42:00,420
constrained

1026
00:42:00,420 --> 00:42:02,280
um

1027
00:42:02,280 --> 00:42:06,960
search yes so yeah so what I'm thinking

1028
00:42:06,960 --> 00:42:08,460
about these days is that we should also

1029
00:42:08,460 --> 00:42:11,700
consider uh combinations of these two uh

1030
00:42:11,700 --> 00:42:14,160
where I can afford to kind of ignore

1031
00:42:14,160 --> 00:42:17,400
such counterfactual things

1032
00:42:17,400 --> 00:42:19,680
um there I can do dynamic programming uh

1033
00:42:19,680 --> 00:42:23,040
and maybe I can do as two steps uh of

1034
00:42:23,040 --> 00:42:25,380
planning so if it's a queue based

1035
00:42:25,380 --> 00:42:28,980
exploration uh say reaching queue uh is

1036
00:42:28,980 --> 00:42:32,220
one task and from queue to the reward is

1037
00:42:32,220 --> 00:42:34,619
the other task so I can separate these

1038
00:42:34,619 --> 00:42:37,460
two and use dynamic programming for them

1039
00:42:37,460 --> 00:42:40,440
and that's computationally cheaper but

1040
00:42:40,440 --> 00:42:43,079
also kind of preserves this idea of

1041
00:42:43,079 --> 00:42:45,480
um having to do it yeah but these are

1042
00:42:45,480 --> 00:42:48,119
all future things I am thinking about

1043
00:42:48,119 --> 00:42:50,700
cool or ask another question from the

1044
00:42:50,700 --> 00:42:54,359
chat Alex wrote

1045
00:42:54,359 --> 00:42:57,599
do you have ideas or developments on

1046
00:42:57,599 --> 00:43:00,180
nested models where different scales

1047
00:43:00,180 --> 00:43:02,330
could have different

1048
00:43:02,330 --> 00:43:02,640
[Music]

1049
00:43:02,640 --> 00:43:03,780
um

1050
00:43:03,780 --> 00:43:06,540
one time step ahead and speed of

1051
00:43:06,540 --> 00:43:07,800
execution

1052
00:43:07,800 --> 00:43:09,960
so how does this model play out in

1053
00:43:09,960 --> 00:43:11,579
nested models

1054
00:43:11,579 --> 00:43:13,859
and how how do we think about this

1055
00:43:13,859 --> 00:43:16,380
forwards and backwards in time in the

1056
00:43:16,380 --> 00:43:19,500
speed of execution and nested models

1057
00:43:19,500 --> 00:43:21,780
okay so I might need a little more

1058
00:43:21,780 --> 00:43:23,760
context here like

1059
00:43:23,760 --> 00:43:26,220
um when you say nested models um do you

1060
00:43:26,220 --> 00:43:28,440
mean hierarchical models yeah

1061
00:43:28,440 --> 00:43:30,780
yeah so maybe

1062
00:43:30,780 --> 00:43:34,619
um something like there is a observation

1063
00:43:34,619 --> 00:43:38,760
I get on a fast pace and I do some

1064
00:43:38,760 --> 00:43:41,160
inference on that but then I use this

1065
00:43:41,160 --> 00:43:45,180
inference uh to kind of do

1066
00:43:45,180 --> 00:43:48,000
or maybe uh and that basically this

1067
00:43:48,000 --> 00:43:49,500
inference becomes the observation for

1068
00:43:49,500 --> 00:43:52,800
the next state and that's what nested

1069
00:43:52,800 --> 00:43:55,980
models mean right yes so I might have to

1070
00:43:55,980 --> 00:43:58,140
actually think about that in terms of a

1071
00:43:58,140 --> 00:44:00,300
task and then think about it but

1072
00:44:00,300 --> 00:44:02,339
honestly I I haven't

1073
00:44:02,339 --> 00:44:04,800
um thought how this might work in that

1074
00:44:04,800 --> 00:44:08,760
context but say for a task

1075
00:44:08,760 --> 00:44:10,319
um

1076
00:44:10,319 --> 00:44:13,140
like uh so in terms of navigation if you

1077
00:44:13,140 --> 00:44:15,119
think about it you can think about

1078
00:44:15,119 --> 00:44:15,900
um

1079
00:44:15,900 --> 00:44:19,800
say rooms environment so that's where uh

1080
00:44:19,800 --> 00:44:21,480
that's one of the examples I can think

1081
00:44:21,480 --> 00:44:24,000
about where we can apply this so imagine

1082
00:44:24,000 --> 00:44:27,720
maybe you have a collection of rooms and

1083
00:44:27,720 --> 00:44:30,300
as an agent you have to first figure out

1084
00:44:30,300 --> 00:44:32,460
which room to go and then you have to

1085
00:44:32,460 --> 00:44:35,760
navigate inside that home and basically

1086
00:44:35,760 --> 00:44:39,060
you can do uh inference in two stages or

1087
00:44:39,060 --> 00:44:41,940
decision making in two stages uh and you

1088
00:44:41,940 --> 00:44:43,140
have you will have to separate your

1089
00:44:43,140 --> 00:44:45,240
decisions in in those stages

1090
00:44:45,240 --> 00:44:47,460
inside the room you can do say dynamic

1091
00:44:47,460 --> 00:44:49,740
programming to navigate your optimal uh

1092
00:44:49,740 --> 00:44:52,980
path but you will always have to kind of

1093
00:44:52,980 --> 00:44:54,960
have two stages of

1094
00:44:54,960 --> 00:44:57,300
um decision making and um

1095
00:44:57,300 --> 00:45:00,420
maybe different methods work better at

1096
00:45:00,420 --> 00:45:04,260
different stages but this would be uh

1097
00:45:04,260 --> 00:45:06,180
better in you know I mean this

1098
00:45:06,180 --> 00:45:08,460
discussion would be better to

1099
00:45:08,460 --> 00:45:09,420
um

1100
00:45:09,420 --> 00:45:12,060
better in in a well

1101
00:45:12,060 --> 00:45:14,400
thought out task I would say I I don't

1102
00:45:14,400 --> 00:45:18,240
have a answer that might be fitting for

1103
00:45:18,240 --> 00:45:19,740
everything but

1104
00:45:19,740 --> 00:45:21,119
yeah

1105
00:45:21,119 --> 00:45:24,359
yeah we've seen almost exactly that kind

1106
00:45:24,359 --> 00:45:27,300
of hierarchical simultaneous

1107
00:45:27,300 --> 00:45:30,180
localization and mapping Slam in a

1108
00:45:30,180 --> 00:45:32,040
robotics case there have been active

1109
00:45:32,040 --> 00:45:33,900
inference models on that

1110
00:45:33,900 --> 00:45:37,140
um yeah would it be possible to sort of

1111
00:45:37,140 --> 00:45:39,500
do

1112
00:45:40,079 --> 00:45:42,660
um one of these methods

1113
00:45:42,660 --> 00:45:45,420
at one level of the nested model and

1114
00:45:45,420 --> 00:45:48,359
then have another computational method

1115
00:45:48,359 --> 00:45:50,280
applied to another

1116
00:45:50,280 --> 00:45:51,960
um or is like if you wanted the

1117
00:45:51,960 --> 00:45:54,480
advantages of one in one place like can

1118
00:45:54,480 --> 00:45:56,579
you mix and match with these different

1119
00:45:56,579 --> 00:46:00,180
methods even within one simulation

1120
00:46:00,180 --> 00:46:03,540
yeah I I I definitely think that's

1121
00:46:03,540 --> 00:46:04,859
possible

1122
00:46:04,859 --> 00:46:09,140
um but yeah we'll have to maybe try

1123
00:46:09,180 --> 00:46:12,119
so all my methods are um one stage so

1124
00:46:12,119 --> 00:46:14,819
it's not a hierarchical model

1125
00:46:14,819 --> 00:46:17,220
and but I I strongly believe that it

1126
00:46:17,220 --> 00:46:19,260
would also work in an estate model where

1127
00:46:19,260 --> 00:46:20,520
you can have

1128
00:46:20,520 --> 00:46:22,200
um two methods of decision making

1129
00:46:22,200 --> 00:46:24,660
working together in say for a room

1130
00:46:24,660 --> 00:46:26,940
example I just mentioned

1131
00:46:26,940 --> 00:46:29,280
oh could you go to the slide on Z

1132
00:46:29,280 --> 00:46:31,819
learning

1133
00:46:35,640 --> 00:46:38,760
cool yeah so I noticed um here and in

1134
00:46:38,760 --> 00:46:41,220
the great paper that you had several

1135
00:46:41,220 --> 00:46:45,240
total of citations yes and so and this

1136
00:46:45,240 --> 00:46:48,060
introduction of the Z learning is is a

1137
00:46:48,060 --> 00:46:49,859
novelty with respect to the active

1138
00:46:49,859 --> 00:46:52,260
inference field so could you explain a

1139
00:46:52,260 --> 00:46:53,760
little more

1140
00:46:53,760 --> 00:46:55,920
what is the Z

1141
00:46:55,920 --> 00:47:00,660
and what is it that enables such a a

1142
00:47:00,660 --> 00:47:02,280
rapid

1143
00:47:02,280 --> 00:47:03,240
um

1144
00:47:03,240 --> 00:47:06,119
Improvement in the Z with respect to the

1145
00:47:06,119 --> 00:47:06,900
Q

1146
00:47:06,900 --> 00:47:10,619
yeah okay so uh to give some context

1147
00:47:10,619 --> 00:47:13,079
um context about this paper

1148
00:47:13,079 --> 00:47:13,920
um

1149
00:47:13,920 --> 00:47:16,260
it's it talks about a

1150
00:47:16,260 --> 00:47:19,800
linear method of decision making uh in a

1151
00:47:19,800 --> 00:47:21,720
particular class of mdp so given that

1152
00:47:21,720 --> 00:47:23,940
you are having a Markov decision process

1153
00:47:23,940 --> 00:47:27,240
where your actions can be based on

1154
00:47:27,240 --> 00:47:29,220
States and not accents so when you think

1155
00:47:29,220 --> 00:47:31,560
about actions in a say in for example a

1156
00:47:31,560 --> 00:47:32,760
gridual task

1157
00:47:32,760 --> 00:47:35,160
you think about left right and

1158
00:47:35,160 --> 00:47:37,200
um North South right so if I take a

1159
00:47:37,200 --> 00:47:40,200
north then that has a consequence of uh

1160
00:47:40,200 --> 00:47:42,720
consequence on state space but in this

1161
00:47:42,720 --> 00:47:43,680
paper

1162
00:47:43,680 --> 00:47:46,380
um they're introducing a class of mdp

1163
00:47:46,380 --> 00:47:48,839
where decision is itself in terms of

1164
00:47:48,839 --> 00:47:49,920
states

1165
00:47:49,920 --> 00:47:54,180
so if I am saying State S1 my decision

1166
00:47:54,180 --> 00:47:56,280
will be uh

1167
00:47:56,280 --> 00:47:58,740
depending upon the other state so my

1168
00:47:58,740 --> 00:48:00,000
decision will be based on the other

1169
00:48:00,000 --> 00:48:02,160
states I want to be in the next time so

1170
00:48:02,160 --> 00:48:04,020
uh

1171
00:48:04,020 --> 00:48:05,880
it's a really definition of decision

1172
00:48:05,880 --> 00:48:08,640
making uh in terms of the state space

1173
00:48:08,640 --> 00:48:10,800
rather than decisions being something

1174
00:48:10,800 --> 00:48:13,619
else like left right and down up

1175
00:48:13,619 --> 00:48:15,300
so given that

1176
00:48:15,300 --> 00:48:17,819
such an mdp exists where I can take

1177
00:48:17,819 --> 00:48:21,319
decisions in terms of states

1178
00:48:21,319 --> 00:48:23,520
they have showed that computationally

1179
00:48:23,520 --> 00:48:27,000
you can take uh decisions in linear

1180
00:48:27,000 --> 00:48:30,900
complexity however big the problem is

1181
00:48:30,900 --> 00:48:33,480
so for that to so to enable decision

1182
00:48:33,480 --> 00:48:35,339
making in terms of States you should

1183
00:48:35,339 --> 00:48:38,579
have a sense of what is good and bad for

1184
00:48:38,579 --> 00:48:39,540
the states

1185
00:48:39,540 --> 00:48:43,619
so in this gridward example uh you have

1186
00:48:43,619 --> 00:48:46,440
a desirability function which is C so C

1187
00:48:46,440 --> 00:48:48,240
is the desirability function

1188
00:48:48,240 --> 00:48:50,700
which talks about how desirable is a

1189
00:48:50,700 --> 00:48:54,180
state and if I have a c function then

1190
00:48:54,180 --> 00:48:56,579
what they have showed is that

1191
00:48:56,579 --> 00:48:57,359
um

1192
00:48:57,359 --> 00:48:59,280
I can take decisions with linear

1193
00:48:59,280 --> 00:49:01,079
computational complexity for this

1194
00:49:01,079 --> 00:49:03,540
particular class of mdp so if my mdp

1195
00:49:03,540 --> 00:49:05,640
allows me to take decisions in terms of

1196
00:49:05,640 --> 00:49:08,940
States it's linearly uh it's it's only

1197
00:49:08,940 --> 00:49:11,700
linear complexity for that thing

1198
00:49:11,700 --> 00:49:15,240
so this graph is basically comparing how

1199
00:49:15,240 --> 00:49:19,260
you can learn desirability uh better

1200
00:49:19,260 --> 00:49:22,200
or faster right so Q learning if you are

1201
00:49:22,200 --> 00:49:23,880
familiar with Q learning it's basically

1202
00:49:23,880 --> 00:49:28,319
a table based method uh where you have

1203
00:49:28,319 --> 00:49:30,780
um desirability of actions given a state

1204
00:49:30,780 --> 00:49:33,000
so given a state you know what to do

1205
00:49:33,000 --> 00:49:35,760
that's basically the queue Matrix but in

1206
00:49:35,760 --> 00:49:37,020
terms of C

1207
00:49:37,020 --> 00:49:40,380
uh or the C learning method it's only

1208
00:49:40,380 --> 00:49:43,020
about States uh you're only learning how

1209
00:49:43,020 --> 00:49:44,819
desirable a state is there is no concept

1210
00:49:44,819 --> 00:49:46,980
of actions

1211
00:49:46,980 --> 00:49:49,560
and this is exactly what our prior

1212
00:49:49,560 --> 00:49:51,839
preference uh is in active inference

1213
00:49:51,839 --> 00:49:54,000
where it is a distribution that

1214
00:49:54,000 --> 00:49:55,560
quantifies how desirable or

1215
00:49:55,560 --> 00:49:57,960
non-desirable states are

1216
00:49:57,960 --> 00:50:00,119
right so

1217
00:50:00,119 --> 00:50:02,579
given that they have shown that you have

1218
00:50:02,579 --> 00:50:05,099
you can learn the C Matrix faster and

1219
00:50:05,099 --> 00:50:07,980
that's optimal and it's way faster than

1220
00:50:07,980 --> 00:50:10,079
even Q learning then I thought okay why

1221
00:50:10,079 --> 00:50:13,319
not try to learn C the same way as C is

1222
00:50:13,319 --> 00:50:16,400
learned in this paper and

1223
00:50:16,400 --> 00:50:19,319
using this say so there is a similar

1224
00:50:19,319 --> 00:50:21,839
learning rule for learning C which is

1225
00:50:21,839 --> 00:50:24,119
called set learning in that paper and

1226
00:50:24,119 --> 00:50:25,800
when I attempted to learn see what I've

1227
00:50:25,800 --> 00:50:28,200
seen is that it learns really fast a

1228
00:50:28,200 --> 00:50:30,540
useful Pride preference that lets me

1229
00:50:30,540 --> 00:50:32,520
take decisions or let the active

1230
00:50:32,520 --> 00:50:34,619
influence agent take decisions

1231
00:50:34,619 --> 00:50:38,460
um only by say one time step of planning

1232
00:50:38,460 --> 00:50:40,859
and yeah so basically that took the

1233
00:50:40,859 --> 00:50:42,240
story so

1234
00:50:42,240 --> 00:50:43,440
um

1235
00:50:43,440 --> 00:50:47,460
this idea of C being easily learnable is

1236
00:50:47,460 --> 00:50:50,240
in that paper

1237
00:50:52,020 --> 00:50:54,780
okay let me try to um restate that since

1238
00:50:54,780 --> 00:50:56,420
I think it's a very interesting

1239
00:50:56,420 --> 00:50:59,099
augmentation of active inference so

1240
00:50:59,099 --> 00:51:02,280
we're going to be learning C

1241
00:51:02,280 --> 00:51:04,200
for all the

1242
00:51:04,200 --> 00:51:06,480
reasons we discussed earlier

1243
00:51:06,480 --> 00:51:09,619
we're going to learn C

1244
00:51:09,619 --> 00:51:13,740
analogously to how todorov presented Z

1245
00:51:13,740 --> 00:51:18,540
learning and in the Z learning

1246
00:51:18,540 --> 00:51:21,059
instead of learning

1247
00:51:21,059 --> 00:51:23,460
um for example updated posterior

1248
00:51:23,460 --> 00:51:26,160
probabilities on actions

1249
00:51:26,160 --> 00:51:28,380
and then using actions to navigate

1250
00:51:28,380 --> 00:51:31,339
amongst states that emit observations

1251
00:51:31,339 --> 00:51:35,760
yeah we're going to kind of bake the

1252
00:51:35,760 --> 00:51:37,319
action

1253
00:51:37,319 --> 00:51:39,300
into the states

1254
00:51:39,300 --> 00:51:41,420
so that really we're learning

1255
00:51:41,420 --> 00:51:44,280
transitions among states

1256
00:51:44,280 --> 00:51:48,200
directly yeah

1257
00:51:51,540 --> 00:51:53,880
and and to connect this to the free

1258
00:51:53,880 --> 00:51:56,040
energy principle and and how it is

1259
00:51:56,040 --> 00:51:57,660
playing out with with active inference

1260
00:51:57,660 --> 00:51:59,220
here

1261
00:51:59,220 --> 00:52:00,559
um

1262
00:52:00,559 --> 00:52:03,780
C is not just our desirability function

1263
00:52:03,780 --> 00:52:05,280
that's one way of thinking about it

1264
00:52:05,280 --> 00:52:07,559
that's why we call it preference but

1265
00:52:07,559 --> 00:52:11,640
also C is our expectation and so that is

1266
00:52:11,640 --> 00:52:14,400
what allows us to on one hand use the

1267
00:52:14,400 --> 00:52:16,800
language familiar to reward and

1268
00:52:16,800 --> 00:52:18,960
preference learning like the agent is

1269
00:52:18,960 --> 00:52:21,900
ending up where it likes to be but also

1270
00:52:21,900 --> 00:52:25,500
the expectation based

1271
00:52:25,500 --> 00:52:27,780
um definition of c these are the same

1272
00:52:27,780 --> 00:52:30,180
thing allows us to talk about that as a

1273
00:52:30,180 --> 00:52:32,760
path of least action or as the most

1274
00:52:32,760 --> 00:52:36,660
likely outcome or the least surprising

1275
00:52:36,660 --> 00:52:39,900
outcome and because we've defined it

1276
00:52:39,900 --> 00:52:42,960
what we want as the least surprising

1277
00:52:42,960 --> 00:52:47,040
outcome then we can use variational free

1278
00:52:47,040 --> 00:52:50,640
energy to bounce surprise whereas you

1279
00:52:50,640 --> 00:52:54,000
can't use simply a variational method to

1280
00:52:54,000 --> 00:52:56,579
bound or even necessarily approximate

1281
00:52:56,579 --> 00:52:58,859
reward itself

1282
00:52:58,859 --> 00:53:01,079
but if you say

1283
00:53:01,079 --> 00:53:04,980
I prefer what I expect

1284
00:53:04,980 --> 00:53:07,559
and what I expect reduces my surprise

1285
00:53:07,559 --> 00:53:09,859
and I'm going to balance surprise

1286
00:53:09,859 --> 00:53:13,520
then you get both that sort of

1287
00:53:13,520 --> 00:53:17,240
Behavioral reward seeking

1288
00:53:17,240 --> 00:53:21,420
couched in a surprise bounding surprise

1289
00:53:21,420 --> 00:53:25,579
minimizing physics framework

1290
00:53:27,059 --> 00:53:28,740
yeah yeah that's a beautiful way of

1291
00:53:28,740 --> 00:53:31,619
saying it yeah thank you

1292
00:53:31,619 --> 00:53:35,160
well what are your um next uh

1293
00:53:35,160 --> 00:53:36,720
exciting

1294
00:53:36,720 --> 00:53:39,420
steps or directions or what what ways do

1295
00:53:39,420 --> 00:53:41,640
you want to take this work yeah great

1296
00:53:41,640 --> 00:53:43,140
yeah so

1297
00:53:43,140 --> 00:53:44,940
um if you're talking about only this

1298
00:53:44,940 --> 00:53:46,140
work

1299
00:53:46,140 --> 00:53:50,640
um what I want to do next is uh

1300
00:53:50,640 --> 00:53:53,099
think about cue based exploration tasks

1301
00:53:53,099 --> 00:53:55,200
I first of all address the limitations

1302
00:53:55,200 --> 00:53:58,980
of dynamic programming so it's if you

1303
00:53:58,980 --> 00:54:01,020
have say a queue to explore in this grid

1304
00:54:01,020 --> 00:54:03,660
first and that's more optimal I want to

1305
00:54:03,660 --> 00:54:06,780
see how the uh expected ambiguity term

1306
00:54:06,780 --> 00:54:10,319
in the expected free energy is useful

1307
00:54:10,319 --> 00:54:12,240
and should be made use in the dynamic

1308
00:54:12,240 --> 00:54:13,260
programming

1309
00:54:13,260 --> 00:54:15,540
strictly in that sense

1310
00:54:15,540 --> 00:54:18,000
um but more generally I'm also looking

1311
00:54:18,000 --> 00:54:20,339
at other ways of making decisions in

1312
00:54:20,339 --> 00:54:23,040
active influence like there are works

1313
00:54:23,040 --> 00:54:24,900
from Professor

1314
00:54:24,900 --> 00:54:26,099
a summer

1315
00:54:26,099 --> 00:54:30,180
from CBS reckon he talks about how it's

1316
00:54:30,180 --> 00:54:33,059
How neural networks are doing active

1317
00:54:33,059 --> 00:54:35,099
inference and basically decision making

1318
00:54:35,099 --> 00:54:36,359
there is

1319
00:54:36,359 --> 00:54:39,540
um really uh more efficient in the sense

1320
00:54:39,540 --> 00:54:42,359
that it's like queue learning so in

1321
00:54:42,359 --> 00:54:44,940
there he's making clever use of the

1322
00:54:44,940 --> 00:54:47,520
variation free energy to learn good

1323
00:54:47,520 --> 00:54:50,760
State action mappings and it's it's a

1324
00:54:50,760 --> 00:54:52,200
very drastic

1325
00:54:52,200 --> 00:54:55,079
um change to what we are used to in

1326
00:54:55,079 --> 00:54:56,520
terms of expected free energy so there

1327
00:54:56,520 --> 00:54:59,220
is no concept of expected free energy uh

1328
00:54:59,220 --> 00:55:01,740
in in that work it's all about learning

1329
00:55:01,740 --> 00:55:03,599
what is good and bad directly from

1330
00:55:03,599 --> 00:55:05,700
variation for energy so I I find that

1331
00:55:05,700 --> 00:55:08,099
also fascinating I want to kind of

1332
00:55:08,099 --> 00:55:11,160
explore that more and see how decision

1333
00:55:11,160 --> 00:55:15,300
making uh in this way is better or worse

1334
00:55:15,300 --> 00:55:18,000
or should be thought about should we

1335
00:55:18,000 --> 00:55:20,220
even reconsider ways of decision making

1336
00:55:20,220 --> 00:55:22,200
because active inference only talks

1337
00:55:22,200 --> 00:55:23,700
about variational free energy and that's

1338
00:55:23,700 --> 00:55:25,520
the central tenet everything else is

1339
00:55:25,520 --> 00:55:28,440
your interpretation of that right

1340
00:55:28,440 --> 00:55:31,319
so yeah

1341
00:55:31,319 --> 00:55:35,180
that's another Direction I want to work

1342
00:55:35,640 --> 00:55:38,359
interesting way to say it um definitely

1343
00:55:38,359 --> 00:55:42,480
the variational free energy which is a

1344
00:55:42,480 --> 00:55:45,540
functional of our variational

1345
00:55:45,540 --> 00:55:48,660
um distribution q and the data y

1346
00:55:48,660 --> 00:55:51,119
the variational free energy is kind of

1347
00:55:51,119 --> 00:55:53,839
like the real time

1348
00:55:53,839 --> 00:55:57,359
homeostasis like yeah how are things

1349
00:55:57,359 --> 00:55:59,760
making sense given what I believe and

1350
00:55:59,760 --> 00:56:01,619
the incoming data

1351
00:56:01,619 --> 00:56:04,740
and then to extend that kind of a

1352
00:56:04,740 --> 00:56:07,020
sense-making framework into decision

1353
00:56:07,020 --> 00:56:10,020
making we've seen a lot of different

1354
00:56:10,020 --> 00:56:13,740
methods expected free energy is

1355
00:56:13,740 --> 00:56:16,260
um a common one but for example there's

1356
00:56:16,260 --> 00:56:19,400
been free energy of the expected future

1357
00:56:19,400 --> 00:56:25,200
as eef and there's other constructions

1358
00:56:25,200 --> 00:56:29,760
that have different methods

1359
00:56:29,760 --> 00:56:31,380
um and then you point it also to

1360
00:56:31,380 --> 00:56:33,599
Professor samura's work

1361
00:56:33,599 --> 00:56:37,020
with um the sort of uh

1362
00:56:37,020 --> 00:56:39,059
relationship between the variational

1363
00:56:39,059 --> 00:56:41,339
free energy on the base graph and the

1364
00:56:41,339 --> 00:56:43,680
loss function in a neural network and

1365
00:56:43,680 --> 00:56:45,300
all those relationships that's very

1366
00:56:45,300 --> 00:56:48,380
exciting work too

1367
00:56:49,079 --> 00:56:51,660
um I guess kind of in closing just as a

1368
00:56:51,660 --> 00:56:53,460
last question or thought

1369
00:56:53,460 --> 00:56:55,559
you're coming close to the end of your

1370
00:56:55,559 --> 00:56:56,880
PhD

1371
00:56:56,880 --> 00:57:00,540
so just in the time that you've been a

1372
00:57:00,540 --> 00:57:02,579
PhD student

1373
00:57:02,579 --> 00:57:06,300
how have you seen active inference

1374
00:57:06,300 --> 00:57:10,700
develop or what what

1375
00:57:11,339 --> 00:57:14,280
feels different to you today near the

1376
00:57:14,280 --> 00:57:17,400
end than when you were

1377
00:57:17,400 --> 00:57:19,980
yeah fresh-eyed and excited several

1378
00:57:19,980 --> 00:57:21,119
years ago

1379
00:57:21,119 --> 00:57:23,339
yeah that's that's a really great

1380
00:57:23,339 --> 00:57:25,920
question and uh I am also very excited

1381
00:57:25,920 --> 00:57:29,400
of how the field has evolved and frankly

1382
00:57:29,400 --> 00:57:30,900
I started from this reinforcement

1383
00:57:30,900 --> 00:57:32,400
learning background and the physics

1384
00:57:32,400 --> 00:57:34,680
background and when I started reading it

1385
00:57:34,680 --> 00:57:37,680
was only say one or two papers and I did

1386
00:57:37,680 --> 00:57:40,440
not quite understand much of it uh it

1387
00:57:40,440 --> 00:57:42,540
was only when I started uh implementing

1388
00:57:42,540 --> 00:57:44,700
it using calls

1389
00:57:44,700 --> 00:57:46,980
um Matlab scripts I kind of understood

1390
00:57:46,980 --> 00:57:49,559
okay this makes sense and I like it and

1391
00:57:49,559 --> 00:57:52,020
but within uh say one or two years I saw

1392
00:57:52,020 --> 00:57:54,780
a lot of papers coming in uh from

1393
00:57:54,780 --> 00:57:56,280
different directions and people also

1394
00:57:56,280 --> 00:57:58,740
starting to use neural networks and all

1395
00:57:58,740 --> 00:58:02,460
this scaling up came and I I at some

1396
00:58:02,460 --> 00:58:04,319
point I also questioned the need of

1397
00:58:04,319 --> 00:58:07,260
active inference uh because if you have

1398
00:58:07,260 --> 00:58:09,119
deep reinforcement learning can which

1399
00:58:09,119 --> 00:58:10,980
can do many things then why deep active

1400
00:58:10,980 --> 00:58:14,040
inference and that's the reason why I

1401
00:58:14,040 --> 00:58:16,319
did not get into that but I still find

1402
00:58:16,319 --> 00:58:18,359
it fascinating I want to kind of

1403
00:58:18,359 --> 00:58:20,280
understand deep active inference more

1404
00:58:20,280 --> 00:58:23,160
than what I know now but I've seen the

1405
00:58:23,160 --> 00:58:25,680
field growing like anything in in two or

1406
00:58:25,680 --> 00:58:28,619
three years and many cohorts of people

1407
00:58:28,619 --> 00:58:31,859
starting to work and in no time it was a

1408
00:58:31,859 --> 00:58:36,359
seriously taken field other than a field

1409
00:58:36,359 --> 00:58:39,119
with say two papers with nobody actually

1410
00:58:39,119 --> 00:58:41,280
knowing what it is so it's really

1411
00:58:41,280 --> 00:58:43,440
exciting yeah so I really look forward

1412
00:58:43,440 --> 00:58:47,040
how the field evolves in time and also

1413
00:58:47,040 --> 00:58:52,140
what I can do after my PhD and so on

1414
00:58:52,140 --> 00:58:55,319
cool forward in time backward in time

1415
00:58:55,319 --> 00:59:00,359
what we prefer what we expect yes

1416
00:59:02,339 --> 00:59:04,140
any other

1417
00:59:04,140 --> 00:59:05,819
comments or anything else you want to

1418
00:59:05,819 --> 00:59:07,200
add

1419
00:59:07,200 --> 00:59:09,839
yeah so please let me know what you

1420
00:59:09,839 --> 00:59:11,940
think about the paper and what you think

1421
00:59:11,940 --> 00:59:14,400
about these ideas feel free to let me

1422
00:59:14,400 --> 00:59:16,200
know I I really look forward to the

1423
00:59:16,200 --> 00:59:17,760
feedback and

1424
00:59:17,760 --> 00:59:20,160
yeah thanks so much for this opportunity

1425
00:59:20,160 --> 00:59:23,339
Daniel and thank you for your time

1426
00:59:23,339 --> 00:59:26,160
it was amazing I hope people um check

1427
00:59:26,160 --> 00:59:28,260
out the paper and and get in touch and

1428
00:59:28,260 --> 00:59:30,540
replicate the code and and take it their

1429
00:59:30,540 --> 00:59:33,359
own directions thank you

1430
00:59:33,359 --> 00:59:34,680
see you next time

1431
00:59:34,680 --> 00:59:36,839
see you next time thank you so much bye

1432
00:59:36,839 --> 00:59:40,040
have a good day bye


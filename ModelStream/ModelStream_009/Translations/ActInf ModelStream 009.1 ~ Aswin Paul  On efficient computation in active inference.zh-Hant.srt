1
00:00:03,120 --> 00:00:06,120
國外，

2
00:00:08,480 --> 00:00:12,660
2023 年 7 月 15 日，

3
00:00:12,660 --> 00:00:15,900
我們今天與 Aspen Paul 一起參加主動推理模型

4
00:00:15,900 --> 00:00:19,080
流 9.1，

5
00:00:19,080 --> 00:00:21,060
我們將就主動推理中的

6
00:00:21,060 --> 00:00:24,180
高效計算進行演示和討論，

7
00:00:24,180 --> 00:00:27,000
因此，如果

8
00:00:27,000 --> 00:00:28,619
您正在觀看直播，請

9
00:00:28,619 --> 00:00:30,840
隨時添加 聊天中的評論或問題，

10
00:00:30,840 --> 00:00:33,059
否則

11
00:00:33,059 --> 00:00:36,120
非常感謝您今天的加入，非常

12
00:00:36,120 --> 00:00:38,820
期待您的演講，

13
00:00:38,820 --> 00:00:41,640
謝謝丹尼爾，非常感謝您，正如

14
00:00:41,640 --> 00:00:42,960
今天提到的，我將討論

15
00:00:42,960 --> 00:00:44,700
高效計算和主動

16
00:00:44,700 --> 00:00:46,920
推理，是的，讓我們開始吧，

17
00:00:46,920 --> 00:00:49,500
所以我們' 我們都熟悉

18
00:00:49,500 --> 00:00:51,420
自由能原理的概念，也

19
00:00:51,420 --> 00:00:53,820
稱為主動推理權，因此

20
00:00:53,820 --> 00:00:55,800
中心概念是代理

21
00:00:55,800 --> 00:00:59,160
最小化其觀察到的熵以

22
00:00:59,160 --> 00:01:01,140
維持穩態或在其環境中生存

23
00:01:01,140 --> 00:01:03,719
，這裡的熵是

24
00:01:03,719 --> 00:01:05,580
在信息論中定義的

25
00:01:05,580 --> 00:01:08,520
感覺是對的，所以如果一個觀察結果是

26
00:01:08,520 --> 00:01:10,200
高概率的，

27
00:01:10,200 --> 00:01:12,659
那麼熵較小或不那麼令人驚訝，因為它

28
00:01:12,659 --> 00:01:14,159
是高概率的，我們

29
00:01:14,159 --> 00:01:16,560
期待著它，

30
00:01:16,560 --> 00:01:18,840


31
00:01:18,840 --> 00:01:20,939


32
00:01:20,939 --> 00:01:23,759
這就是我們構建主動推理框架的基礎的想法。

33
00:01:23,759 --> 00:01:26,520
Marco 毯子呃，它為我們提供了一種

34
00:01:26,520 --> 00:01:30,000
令人驚訝的系統方法，或者是一種將

35
00:01:30,000 --> 00:01:32,580
代理與其環境分離的示意性方法，

36
00:01:32,580 --> 00:01:34,439
並對有

37
00:01:34,439 --> 00:01:37,320
目的的行為進行建模，所以讓我們關注

38
00:01:37,320 --> 00:01:39,780
最小化熵的想法，那麼

39
00:01:39,780 --> 00:01:41,700
代理如何最小化熵或

40
00:01:41,700 --> 00:01:43,740
知道哪個觀察是 高

41
00:01:43,740 --> 00:01:46,020
概率性，反之亦然，

42
00:01:46,020 --> 00:01:48,360
因此這是通過維護生成

43
00:01:48,360 --> 00:01:50,399
模型而生成模型基本上是

44
00:01:50,399 --> 00:01:52,020


45
00:01:52,020 --> 00:01:56,340
代理在其大腦中構建的環境玩具模型，並且

46
00:01:56,340 --> 00:01:58,860
僅使用

47
00:01:58,860 --> 00:02:00,540
從環境中獲得的觀察來構建，因此它

48
00:02:00,540 --> 00:02:03,420
沒有 訪問真實的呃狀態或

49
00:02:03,420 --> 00:02:04,740
環境的隱藏狀態，

50
00:02:04,740 --> 00:02:07,920
它正在構建玩具模型，並且給定

51
00:02:07,920 --> 00:02:10,318
這個玩具模型，它具有計算觀察概率的範圍或能力，

52
00:02:10,318 --> 00:02:13,560


53
00:02:13,560 --> 00:02:15,900
因此嘗試最小化

54
00:02:15,900 --> 00:02:18,660
熵，

55
00:02:18,660 --> 00:02:22,620
這就是想法，但它 有

56
00:02:22,620 --> 00:02:25,800
一個草書維度的問題，就像給定一個

57
00:02:25,800 --> 00:02:27,239
生成模型一樣，它可能不可能

58
00:02:27,239 --> 00:02:30,120
總是計算或邊緣化

59
00:02:30,120 --> 00:02:32,099
觀察的概率，

60
00:02:32,099 --> 00:02:34,140
因為狀態空間很快就會

61
00:02:34,140 --> 00:02:37,020
變得棘手，但想法是

62
00:02:37,020 --> 00:02:38,640
你定義一個

63
00:02:38,640 --> 00:02:41,580
驚喜的上限 使用 Jensen 不等式，

64
00:02:41,580 --> 00:02:43,560
您還可以

65
00:02:43,560 --> 00:02:46,800
定義一個名為 Q 的新術語，它是

66
00:02:46,800 --> 00:02:49,500
隱藏信念或關於

67
00:02:49,500 --> 00:02:52,080
隱藏狀態的信念，並且該隊列將

68
00:02:52,080 --> 00:02:54,000
成為正確決策的焦點，因此

69
00:02:54,000 --> 00:02:57,060
如果您有一個嘈雜的隊列並且您

70
00:02:57,060 --> 00:02:58,680
不知道 環境中有什麼，那麼

71
00:02:58,680 --> 00:03:00,840
你就無法或希望做出決定

72
00:03:00,840 --> 00:03:03,540
來控制該環境，正是

73
00:03:03,540 --> 00:03:05,519
這種關於你使用的隱藏狀態的信念，對

74
00:03:05,519 --> 00:03:07,560
做出決定變得有用

75
00:03:07,560 --> 00:03:10,379
，這整個量

76
00:03:10,379 --> 00:03:13,019
當然稱為自由能，

77
00:03:13,019 --> 00:03:14,420
變分自由能

78
00:03:14,420 --> 00:03:17,879
F 可以用多種方式解釋，所以

79
00:03:17,879 --> 00:03:19,560
第一個或最常見的是

80
00:03:19,560 --> 00:03:22,920
機器學習方式，

81
00:03:22,920 --> 00:03:25,200
嘗試最小化模型的複雜性，

82
00:03:25,200 --> 00:03:27,060
同時嘗試

83
00:03:27,060 --> 00:03:28,739
最大化模型的準確性，這就是

84
00:03:28,739 --> 00:03:30,180


85
00:03:30,180 --> 00:03:32,400
最小化各種自由能的機器學習解釋您

86
00:03:32,400 --> 00:03:34,140
也可以嘗試用

87
00:03:34,140 --> 00:03:34,739


88
00:03:34,739 --> 00:03:36,420
物理術語解釋自由能，

89
00:03:36,420 --> 00:03:38,940
您同時嘗試

90
00:03:38,940 --> 00:03:41,760
最小化模型的能量，但

91
00:03:41,760 --> 00:03:43,019
同時嘗試最大化

92
00:03:43,019 --> 00:03:45,120
熵，但今天的焦點

93
00:03:45,120 --> 00:03:48,659
或者決策總是基於

94
00:03:48,659 --> 00:03:51,540
這樣的信念，

95
00:03:51,540 --> 00:03:54,780
即在進行主動推理中的感知之後，

96
00:03:54,780 --> 00:03:57,120
您如何進行普通決策，或者

97
00:03:57,120 --> 00:03:58,920


98
00:03:58,920 --> 00:04:00,659
決策和經典主動

99
00:04:00,659 --> 00:04:03,540
推理中討論最多的想法是什麼，所以如果您處於一個

100
00:04:03,540 --> 00:04:05,580
環境中，呃 如果你是一個試圖

101
00:04:05,580 --> 00:04:08,819
做出決定的代理，那麼

102
00:04:08,819 --> 00:04:10,620
你有一個

103
00:04:10,620 --> 00:04:13,319
可用的操作空間，所以在

104
00:04:13,319 --> 00:04:15,480
這個玩具模型中，你有三個可用的

105
00:04:15,480 --> 00:04:19,738
操作，呃，跑，跳躍或停留，

106
00:04:19,738 --> 00:04:22,199
考慮到這些操作，你可以希望

107
00:04:22,199 --> 00:04:25,139
通過小 Pi 來定義策略 這是

108
00:04:25,139 --> 00:04:28,320
時間和資本上的一系列行動

109
00:04:28,320 --> 00:04:30,300
t 是計劃的時間範圍，或者

110
00:04:30,300 --> 00:04:32,660
是你的政策的長度，

111
00:04:32,660 --> 00:04:36,540
你有一個政策空間，裡面有

112
00:04:36,540 --> 00:04:39,419
許多這樣的政策小餅，所以給定

113
00:04:39,419 --> 00:04:42,060
這個更大的最小空間，你可以

114
00:04:42,060 --> 00:04:44,220
嘗試

115
00:04:44,220 --> 00:04:47,340
評估預期的免費

116
00:04:47,340 --> 00:04:48,720
基於你積累的信念的能量，

117
00:04:48,720 --> 00:04:50,580
所以在這裡你不會最小化任何

118
00:04:50,580 --> 00:04:52,860
你已經擁有的來自能量變化的信念，你

119
00:04:52,860 --> 00:04:55,080
只是

120
00:04:55,080 --> 00:04:57,419
計算或評估與

121
00:04:57,419 --> 00:04:59,520
許多呃小政策相對應的預期自由能，

122
00:04:59,520 --> 00:05:02,759
你可以定義

123
00:05:02,759 --> 00:05:05,699
和之後 你對所有策略進行評估，

124
00:05:05,699 --> 00:05:07,440
然後你就知道採取哪個是

125
00:05:07,440 --> 00:05:09,180
最佳策略，這是

126
00:05:09,180 --> 00:05:11,160


127
00:05:11,160 --> 00:05:13,800
決策正確的經典主動推理思想，

128
00:05:13,800 --> 00:05:16,020
這種預期的自由能非常

129
00:05:16,020 --> 00:05:18,900
有用，因為它是目標

130
00:05:18,900 --> 00:05:21,720
導向的，因此風險項是目標

131
00:05:21,720 --> 00:05:23,520
導向的 然後你就有了這個預期的

132
00:05:23,520 --> 00:05:25,919
模糊性術語，它迫使你去

133
00:05:25,919 --> 00:05:27,960
探索，但有一個問題

134
00:05:27,960 --> 00:05:30,539
，這個策略空間可以很快變得

135
00:05:30,539 --> 00:05:32,400
可交互，並且一直是將

136
00:05:32,400 --> 00:05:35,280
主動推理擴展

137
00:05:35,280 --> 00:05:37,680
到

138
00:05:37,680 --> 00:05:40,500
常見環境的瓶頸，但讓我們

139
00:05:40,500 --> 00:05:42,419
看看它是如何實現的 變得可以快速交互，

140
00:05:42,419 --> 00:05:45,300
因此可以定義多少個策略，比如

141
00:05:45,300 --> 00:05:47,940
時間範圍為 15，所以如果你

142
00:05:47,940 --> 00:05:50,340
正在玩超級馬里奧，你

143
00:05:50,340 --> 00:05:52,440
可能需要提前計劃至少 10 個時間

144
00:05:52,440 --> 00:05:56,460
步長，這樣第一個策略可能是

145
00:05:56,460 --> 00:05:58,380
相同的操作 運行

146
00:05:58,380 --> 00:06:02,600
um stacked 15 個時間步長，你

147
00:06:02,600 --> 00:06:05,460
基本上可以定義這些動作的幾種組合，

148
00:06:05,460 --> 00:06:09,060
而策略空間

149
00:06:09,060 --> 00:06:10,139
只是

150
00:06:10,139 --> 00:06:13,860
um 棘手，它變得呃太大了，

151
00:06:13,860 --> 00:06:16,380
你無法評估

152
00:06:16,380 --> 00:06:19,440
所有這些策略的預期自由能，並且在

153
00:06:19,440 --> 00:06:21,900
隨機問題設置中

154
00:06:21,900 --> 00:06:24,419
um 環境本身很嘈雜，你

155
00:06:24,419 --> 00:06:27,120
實際上沒有一種方法來選擇

156
00:06:27,120 --> 00:06:28,860
這個策略空間的子集並進行

157
00:06:28,860 --> 00:06:30,840
經典的主動推理，這

158
00:06:30,840 --> 00:06:32,340
顯然是一個計算可交互的

159
00:06:32,340 --> 00:06:34,380
問題，這就是為什麼在文獻中，當你討論時我們

160
00:06:34,380 --> 00:06:36,360
總是看到小網格或小

161
00:06:36,360 --> 00:06:38,880
環境 決策

162
00:06:38,880 --> 00:06:41,520
具有積極的影響力，但最近

163
00:06:41,520 --> 00:06:43,860


164
00:06:43,860 --> 00:06:46,259
提出了一個新的想法，稱為複雜的

165
00:06:46,259 --> 00:06:48,840
入口，在復雜的推理中，

166
00:06:48,840 --> 00:06:51,600
它實際上並不是政策空間，你

167
00:06:51,600 --> 00:06:52,800
實際上是

168
00:06:52,800 --> 00:06:55,080
實時嘗試

169
00:06:55,080 --> 00:06:58,139
思考該怎麼做，如果你有一個信念，

170
00:06:58,139 --> 00:07:00,000
那麼你 試圖

171
00:07:00,000 --> 00:07:02,880
評估基於此的行動，所以

172
00:07:02,880 --> 00:07:04,800
這裡我們沒有一系列

173
00:07:04,800 --> 00:07:07,199


174
00:07:07,199 --> 00:07:10,020
可交互的政策或事物，呃這裡我們正在進行

175
00:07:10,020 --> 00:07:12,600
基本上的研究，從某種意義上說，你

176
00:07:12,600 --> 00:07:14,220
正在嘗試評估

177
00:07:14,220 --> 00:07:16,500


178
00:07:16,500 --> 00:07:19,560
這個聯合分佈的預期自由能 的行動和

179
00:07:19,560 --> 00:07:21,900
觀察，並評估

180
00:07:21,900 --> 00:07:23,940
某個時間 uh 小 T 的預期自由能，

181
00:07:23,940 --> 00:07:26,340
您還需要

182
00:07:26,340 --> 00:07:29,039
下一個時間步的預期自由能，並

183
00:07:29,039 --> 00:07:31,259
評估您將需要

184
00:07:31,259 --> 00:07:32,819
時間 t 加 2 的預期自由能，這

185
00:07:32,819 --> 00:07:35,039
基本上變成 一項

186
00:07:35,039 --> 00:07:38,160
及時推出的研究，呃，這是一個

187
00:07:38,160 --> 00:07:41,160
遞歸關係，

188
00:07:41,160 --> 00:07:43,259
這裡它與

189
00:07:43,259 --> 00:07:45,539
我們在上一張幻燈片中看到的政策空間有根本的不同，從

190
00:07:45,539 --> 00:07:47,759
某種意義上說，

191
00:07:47,759 --> 00:07:49,680
它在計算上更好，

192
00:07:49,680 --> 00:07:51,780
如果你必須計劃說 10

193
00:07:51,780 --> 00:07:55,139
次提示 在

194
00:07:55,139 --> 00:07:56,759
經典主動推理中，我們看到

195
00:07:56,759 --> 00:07:58,380
策略空間

196
00:07:58,380 --> 00:07:59,460
和

197
00:07:59,460 --> 00:08:01,940
動作空間的基數

198
00:08:01,940 --> 00:08:04,139
提升到 T，因此

199
00:08:04,139 --> 00:08:07,259
如果您必須考慮

200
00:08:07,259 --> 00:08:10,199
所有可能性，那麼這就是計算瓶頸，但在復雜的

201
00:08:10,199 --> 00:08:12,780
推理中，情況更糟，因為

202
00:08:12,780 --> 00:08:14,759
您正在考慮

203
00:08:14,759 --> 00:08:16,860
狀態和動作的組合，所以

204
00:08:16,860 --> 00:08:20,400
實際上在計算上更糟糕，但是

205
00:08:20,400 --> 00:08:21,660
在

206
00:08:21,660 --> 00:08:23,520
復雜的參考論文中提出了一個解決方案，我們

207
00:08:23,520 --> 00:08:26,400
可以對這項研究進行修剪，

208
00:08:26,400 --> 00:08:29,639


209
00:08:29,639 --> 00:08:31,080
當你進行這項研究時，我們可以避免一些狀態和動作，並且在

210
00:08:31,080 --> 00:08:33,000
計算上變得非常容易處理

211
00:08:33,000 --> 00:08:36,059
所以讓我們看看剪枝在

212
00:08:36,059 --> 00:08:38,940
復雜推理中是如何工作的，所以這個網格

213
00:08:38,940 --> 00:08:40,440
在原始復雜

214
00:08:40,440 --> 00:08:41,700
推理論文中進行了討論，

215
00:08:41,700 --> 00:08:44,580
並且對於這個網格來說，假設你

216
00:08:44,580 --> 00:08:47,100
有這種先驗偏好

217
00:08:47,100 --> 00:08:50,399
分佈，所以這個白色方塊

218
00:08:50,399 --> 00:08:52,140
是目標狀態最優選的狀態

219
00:08:52,140 --> 00:08:54,300
， 你對遠離黃金狀態的狀態有一個

220
00:08:54,300 --> 00:08:55,380


221
00:08:55,380 --> 00:08:58,140
均勻遞減的偏好，

222
00:08:58,140 --> 00:09:00,779


223
00:09:00,779 --> 00:09:03,240
那麼基本上如果你

224
00:09:03,240 --> 00:09:05,480
在時間 T 的某個觀察中觀察自己，

225
00:09:05,480 --> 00:09:07,560
那麼基本上你正在做的是

226
00:09:07,560 --> 00:09:09,600
你正在考慮

227
00:09:09,600 --> 00:09:12,779
該觀察中可用行動的結果，並且

228
00:09:12,779 --> 00:09:17,760
基本上，你可以使用

229
00:09:17,760 --> 00:09:20,700
你的信念的投影，並為這些信念設定一個閾值，

230
00:09:20,700 --> 00:09:22,620
以某種方式忽略

231
00:09:22,620 --> 00:09:24,600
某些行動並忽略某些

232
00:09:24,600 --> 00:09:27,420
觀察，你會發現

233
00:09:27,420 --> 00:09:30,120
它不再是研究，

234
00:09:30,120 --> 00:09:33,000
它是研究的一個子集，並且是

235
00:09:33,000 --> 00:09:36,060
計算方式 嗯，

236
00:09:36,060 --> 00:09:38,700
比做整個研究更有效率，所以在這裡

237
00:09:38,700 --> 00:09:40,680
你已經避免了很多組合

238
00:09:40,680 --> 00:09:42,120
，這成為一個計算上

239
00:09:42,120 --> 00:09:45,420
有吸引力的問題，這是因為你

240
00:09:45,420 --> 00:09:47,880
決定避免一些行動和

241
00:09:47,880 --> 00:09:49,620
嗯觀察，所以你說本質上是

242
00:09:49,620 --> 00:09:52,740
在這種可能性上妥協，這

243
00:09:52,740 --> 00:09:54,600
可能會給你一個 更高的獎勵，或者

244
00:09:54,600 --> 00:09:56,580
可能比

245
00:09:56,580 --> 00:09:59,700
部分研究的結果更優化，但是

246
00:09:59,700 --> 00:10:02,820
這是有效的，因此

247
00:10:02,820 --> 00:10:05,459
論文中提出了這個模擬，並且它有效，

248
00:10:05,459 --> 00:10:09,959


249
00:10:09,959 --> 00:10:12,300
如果代理朝著

250
00:10:12,300 --> 00:10:14,940
這個前進方向計劃，它會學習做需要做的事情 以

251
00:10:14,940 --> 00:10:17,880
修剪的方式規劃呃，但是

252
00:10:17,880 --> 00:10:19,200


253
00:10:19,200 --> 00:10:20,820
基本上你可以通過計算

254
00:10:20,820 --> 00:10:23,519
表明，即使對於一個小的搜索

255
00:10:23,519 --> 00:10:25,019
閾值，

256
00:10:25,019 --> 00:10:27,180
你也可以大大降低

257
00:10:27,180 --> 00:10:29,100
計算複雜度，所以如果你

258
00:10:29,100 --> 00:10:31,500
決定用規劃深度進行整個研究，

259
00:10:31,500 --> 00:10:33,540


260
00:10:33,540 --> 00:10:35,100
計算時間是 指數級的並且

261
00:10:35,100 --> 00:10:38,040
很快你就無法做太多事情，

262
00:10:38,040 --> 00:10:40,200
但是如果你決定在這樣的閾值

263
00:10:40,200 --> 00:10:42,420
（甚至非常小）你可以看到

264
00:10:42,420 --> 00:10:43,980
這個問題在計算上變得有

265
00:10:43,980 --> 00:10:46,620
吸引力，並且這個關於

266
00:10:46,620 --> 00:10:49,200
復雜實例的演示可以

267
00:10:49,200 --> 00:10:50,700
在我的 mdp 版本中找到，

268
00:10:50,700 --> 00:10:52,860
它是

269
00:10:52,860 --> 00:10:55,380
很快就會被 mdp 整合到原始版本中，

270
00:10:55,380 --> 00:10:56,820
所以

271
00:10:56,820 --> 00:10:59,160
但關鍵點是，

272
00:10:59,160 --> 00:11:01,920
該研究的修剪需要我們有一個

273
00:11:01,920 --> 00:11:03,779
像這樣消息靈通的主要偏好，

274
00:11:03,779 --> 00:11:06,060
因為這裡的代理

275
00:11:06,060 --> 00:11:09,360
知道我們的鄰居

276
00:11:09,360 --> 00:11:11,760
陳述它有多可取 只知道最終

277
00:11:11,760 --> 00:11:13,860
目標集，它還知道其

278
00:11:13,860 --> 00:11:16,440
鄰近狀態，並且考慮到這樣的先驗

279
00:11:16,440 --> 00:11:18,420
偏好，我們可以看到，對於

280
00:11:18,420 --> 00:11:20,399
三的規劃深度，智能體

281
00:11:20,399 --> 00:11:23,820
基本上陷入了

282
00:11:23,820 --> 00:11:27,060
先驗偏好的本地最大值，但具有

283
00:11:27,060 --> 00:11:30,480
足夠的規劃深度，它是 它

284
00:11:30,480 --> 00:11:33,240
能夠克服這個障礙並達到

285
00:11:33,240 --> 00:11:35,579
這個大問題中的目標狀態

286
00:11:35,579 --> 00:11:37,800
，

287
00:11:37,800 --> 00:11:39,600
問題是，如果代理

288
00:11:39,600 --> 00:11:42,720
只知道這個最終狀態，它

289
00:11:42,720 --> 00:11:45,660
不知道要做什麼，在

290
00:11:45,660 --> 00:11:48,240
這種情況下代理會做什麼 所以這就是

291
00:11:48,240 --> 00:11:50,459
我們正在努力

292
00:11:50,459 --> 00:11:53,399
解決的問題，在沒有

293
00:11:53,399 --> 00:11:55,740
像這樣的有意義的先驗偏好的情況下，

294
00:11:55,740 --> 00:11:58,200
代理基本上沒有辦法

295
00:11:58,200 --> 00:12:00,720
達到目標狀態，

296
00:12:00,720 --> 00:12:03,300
除了隨機探索之外，

297
00:12:03,300 --> 00:12:05,279
它沒有辦法進行計劃，因為它

298
00:12:05,279 --> 00:12:08,040
不能 呃，提前規劃八個時間步驟，

299
00:12:08,040 --> 00:12:09,779
因為它在計算上是可

300
00:12:09,779 --> 00:12:13,079
交互的，呃，如果它選擇

301
00:12:13,079 --> 00:12:17,880
正確地進行完整的研究，那麼如果是這樣的話，

302
00:12:17,880 --> 00:12:21,000
對於這樣的網格，

303
00:12:21,000 --> 00:12:22,500
如果你得到一個稀疏的嘗試

304
00:12:22,500 --> 00:12:24,240
偏好，並且信息不充分

305
00:12:24,240 --> 00:12:28,079
並且突出顯示，你會怎麼做 在上一張幻燈片中，

306
00:12:28,079 --> 00:12:30,839
你的研究現在是盲目的，呃，你

307
00:12:30,839 --> 00:12:33,000
無法修剪該研究，你

308
00:12:33,000 --> 00:12:35,100
必須在這種情況下進行全面的研究，

309
00:12:35,100 --> 00:12:36,300


310
00:12:36,300 --> 00:12:38,880
所以你可能會認為，

311
00:12:38,880 --> 00:12:40,200
嗯，有兩種解決方案，

312
00:12:40,200 --> 00:12:42,959
要么你必須找到一個 進行

313
00:12:42,959 --> 00:12:47,359
全面深度規劃的方法呃，

314
00:12:49,579 --> 00:12:52,920
或者你必須學習一個有意義的驕傲

315
00:12:52,920 --> 00:12:54,600
偏好，這將使你能夠進行

316
00:12:54,600 --> 00:12:57,480
這個經過修剪的研究，所以我們將

317
00:12:57,480 --> 00:12:59,760
在本演示文稿中討論這兩個解決方案

318
00:12:59,760 --> 00:13:02,399
呃對於這樣的給定

319
00:13:02,399 --> 00:13:03,839
場景，所以

320
00:13:03,839 --> 00:13:06,420
第一個 解決方案呃，做全面的

321
00:13:06,420 --> 00:13:08,639
研究基本上是使用動態

322
00:13:08,639 --> 00:13:11,399
規劃，動態規劃是

323
00:13:11,399 --> 00:13:13,800
運籌學

324
00:13:13,800 --> 00:13:15,839
和工業工程以及

325
00:13:15,839 --> 00:13:18,060
許多工程分支中的一個眾所周知的想法，

326
00:13:18,060 --> 00:13:19,920
基本思想是先

327
00:13:19,920 --> 00:13:21,959
解決一個更大問題的子部分，

328
00:13:21,959 --> 00:13:24,000
然後再解決 稍後嘗試整合

329
00:13:24,000 --> 00:13:27,000
這些子問題的解決方案，以

330
00:13:27,000 --> 00:13:31,260
做出最佳決策，因此

331
00:13:31,260 --> 00:13:33,779
在這種情況下，想像一下

332
00:13:33,779 --> 00:13:37,019
您正在嘗試計劃最後一個

333
00:13:37,019 --> 00:13:39,540
行動，所以早些時候，

334
00:13:39,540 --> 00:13:41,760
我們從現在開始的第一個行動開始，

335
00:13:41,760 --> 00:13:44,040
並嘗試預測 未來會發生什麼，

336
00:13:44,040 --> 00:13:46,320
所以你的

337
00:13:46,320 --> 00:13:47,940
計劃方向基本上是及時向前的，

338
00:13:47,940 --> 00:13:49,920
但想像一下，你試圖

339
00:13:49,920 --> 00:13:51,959
只為最後一個時間步驟進行計劃，此時你

340
00:13:51,959 --> 00:13:55,560
正接近目標狀態，並且正

341
00:13:55,560 --> 00:13:57,600
朝著那個良好的狀態前進，所以你

342
00:13:57,600 --> 00:13:59,519
正在嘗試為

343
00:13:59,519 --> 00:14:01,740
最後一個時間步長（大寫 T 減

344
00:14:01,740 --> 00:14:05,459
1）做出決定，並且您對下一個

345
00:14:05,459 --> 00:14:08,880
時間步長或最後一個目標狀態的預測

346
00:14:08,880 --> 00:14:09,839


347
00:14:09,839 --> 00:14:11,880
可以完成，因為您可以

348
00:14:11,880 --> 00:14:14,160
使用此

349
00:14:14,160 --> 00:14:16,079
轉換訪問這個世界模型 主動推理中的動力學或 B 矩陣，

350
00:14:16,079 --> 00:14:18,600
因此對於這個單一時間

351
00:14:18,600 --> 00:14:21,180
步驟，這是一個子問題，呃，您

352
00:14:21,180 --> 00:14:24,060
實際上正在評估一個預期

353
00:14:24,060 --> 00:14:26,100
自由能表，該表告訴您，如果您處於

354
00:14:26,100 --> 00:14:28,019
這個觀察中，則要做什麼，

355
00:14:28,019 --> 00:14:31,740
這是最後一個 時間步長，

356
00:14:31,740 --> 00:14:34,440
基本上呃，你可以在狀態

357
00:14:34,440 --> 00:14:36,540
空間或觀察空間中做到這一點，所以這可以通過

358
00:14:36,540 --> 00:14:39,120


359
00:14:39,120 --> 00:14:41,699
一起使用 a 矩陣和 D 矩陣來完成，你可以以

360
00:14:41,699 --> 00:14:44,579
正確的方式進行規劃，所以

361
00:14:44,579 --> 00:14:46,260
問題是，

362
00:14:46,260 --> 00:14:48,839
如果你知道在

363
00:14:48,839 --> 00:14:50,940


364
00:14:50,940 --> 00:14:52,440
你可能會想的最後一個步驟，我怎麼知道

365
00:14:52,440 --> 00:14:54,600
我處於最後一個時間步驟，這一切都是為了

366
00:14:54,600 --> 00:14:58,699
想像你已經

367
00:15:09,720 --> 00:15:12,839
抱歉，我們聽到的最後一件事是，

368
00:15:12,839 --> 00:15:15,540
這一切都是為了想像，

369
00:15:15,540 --> 00:15:18,060
是的，所以從那裡開始吧 所有

370
00:15:18,060 --> 00:15:19,860
關於想像，

371
00:15:19,860 --> 00:15:22,139
好吧，如果出現連接問題，

372
00:15:22,139 --> 00:15:23,699
是的，只要幾秒鐘，那麼

373
00:15:23,699 --> 00:15:25,199
現在一切都很好，哦，對此感到

374
00:15:25,199 --> 00:15:28,260
抱歉，但是所以

375
00:15:28,260 --> 00:15:30,899
嗯，所以我想說的是，你

376
00:15:30,899 --> 00:15:32,880
正在嘗試想像如果你會做什麼

377
00:15:32,880 --> 00:15:35,519
時間資本 T 減一，

378
00:15:35,519 --> 00:15:37,560
這是您規劃範圍的最後一個時間步驟

379
00:15:37,560 --> 00:15:40,260
，如果您處於該

380
00:15:40,260 --> 00:15:42,199
時間步驟，我該怎麼做，所以這張表

381
00:15:42,199 --> 00:15:45,480
代表了所有這樣的場景，如果我

382
00:15:45,480 --> 00:15:49,800
說在時間 T 的觀察 3 減

383
00:15:49,800 --> 00:15:52,680
一什麼 我這樣做，這裡的數量

384
00:15:52,680 --> 00:15:54,720
我只考慮風險項或

385
00:15:54,720 --> 00:15:56,279
有目的的項

386
00:15:56,279 --> 00:15:57,720
呃，

387
00:15:57,720 --> 00:16:01,560
這個項代表該政策，

388
00:16:01,560 --> 00:16:03,540
如果我向後做這個

389
00:16:03,540 --> 00:16:06,180
直到時間 T 減一，所以如果

390
00:16:06,180 --> 00:16:08,699
我知道該怎麼做 在時間呃

391
00:16:08,699 --> 00:16:12,300
大寫 T 負 1 那麼這個表可以

392
00:16:12,300 --> 00:16:14,880
告訴在大寫 T 負 2 時做什麼。

393
00:16:14,880 --> 00:16:16,920
所以我所做的不是及時計劃，而是

394
00:16:16,920 --> 00:16:19,320


395
00:16:19,320 --> 00:16:22,019
通過固定

396
00:16:22,019 --> 00:16:25,019
計劃的大寫 T 將許多表堆疊在一起，並

397
00:16:25,019 --> 00:16:29,279
考慮到 我有所有這樣的堆棧表，

398
00:16:29,279 --> 00:16:31,800
那麼基本上我能做的就是使用它們

399
00:16:31,800 --> 00:16:33,899
及時做出決策，

400
00:16:33,899 --> 00:16:35,940
我們觀察到這個想法是有效的，

401
00:16:35,940 --> 00:16:38,519


402
00:16:38,519 --> 00:16:40,259
嗯我可以逐步向後計算預期的

403
00:16:40,259 --> 00:16:43,019
自由能呃將

404
00:16:43,019 --> 00:16:46,920
它們視為子問題

405
00:16:46,920 --> 00:16:49,079
根本區別在於，在

406
00:16:49,079 --> 00:16:51,600
復雜的推理中，要計算

407
00:16:51,600 --> 00:16:54,959
小 T 時的預期自由能，

408
00:16:54,959 --> 00:16:57,540
您不知道團隊加一時的預期自由能是多少，

409
00:16:57,540 --> 00:16:59,880
因此這

410
00:16:59,880 --> 00:17:01,920
變成了三搜索，因此您必須

411
00:17:01,920 --> 00:17:04,319
首先計算它 要計算

412
00:17:04,319 --> 00:17:06,839
t 加 1，您需要 t 加 2 等等，

413
00:17:06,839 --> 00:17:09,000
但在這裡，因為您是

414
00:17:09,000 --> 00:17:11,339
在時間上向後計算它，所以您已經知道

415
00:17:11,339 --> 00:17:13,559
p 加 1 的預期自由能是多少，

416
00:17:13,559 --> 00:17:16,020
並且您的基礎是基本上

417
00:17:16,020 --> 00:17:17,520
相同的方程，只是這樣 你

418
00:17:17,520 --> 00:17:20,900
正在時間上倒退，並以

419
00:17:20,900 --> 00:17:23,520
復雜的推理方式進行圖像化，

420
00:17:23,520 --> 00:17:25,619
你正在嘗試進行一項研究，但在

421
00:17:25,619 --> 00:17:27,660
動態規划算法中，你正在

422
00:17:27,660 --> 00:17:30,120
使用表格向後進行規劃，

423
00:17:30,120 --> 00:17:32,940
並且考慮到你的規劃地平線

424
00:17:32,940 --> 00:17:35,520
足以解決

425
00:17:35,520 --> 00:17:38,160
我們所面臨的問題 可以看出，代理將能夠

426
00:17:38,160 --> 00:17:40,860
及時採取最佳行動，

427
00:17:40,860 --> 00:17:44,340
因此在本文中，我們還

428
00:17:44,340 --> 00:17:46,500
提出了一種

429
00:17:46,500 --> 00:17:49,020
使用這種向後規劃的順序形式 DPS 算法，

430
00:17:49,020 --> 00:17:50,880
並且我們

431
00:17:50,880 --> 00:17:54,059
能夠擴大對網格

432
00:17:54,059 --> 00:17:56,240
空間的模擬，即 以前可以在

433
00:17:56,240 --> 00:17:59,340
沒有神經網絡的情況下進行交互，

434
00:17:59,340 --> 00:18:01,080
嗯，所以這是第一個解決方案，所以

435
00:18:01,080 --> 00:18:03,840
第二個解決方案是，

436
00:18:03,840 --> 00:18:06,000
嗯，所以在第一個解決方案中，它被修復了，

437
00:18:06,000 --> 00:18:07,980
你只能得到這個過去的私人

438
00:18:07,980 --> 00:18:09,539
參考，你不會得到任何其他

439
00:18:09,539 --> 00:18:10,919
信息，你只能得到有關的

440
00:18:10,919 --> 00:18:12,480
信息 最終目標狀態

441
00:18:12,480 --> 00:18:15,120
和你得到的只是

442
00:18:15,120 --> 00:18:18,240
你學到的環境模型，你

443
00:18:18,240 --> 00:18:21,059
基本上必須做出決定，但

444
00:18:21,059 --> 00:18:22,799
第二個解決方案當然是，如果你被

445
00:18:22,799 --> 00:18:24,600
允許，你可以嘗試學習一種

446
00:18:24,600 --> 00:18:26,100
自豪偏好，這是有意義的，

447
00:18:26,100 --> 00:18:28,620
就像我們的那樣 在之前的幻燈片中看到，

448
00:18:28,620 --> 00:18:31,740
其中也包含有關

449
00:18:31,740 --> 00:18:33,960
其他狀態的信息，但是如何

450
00:18:33,960 --> 00:18:35,400
正確學習它，因此

451
00:18:35,400 --> 00:18:38,039
最優控製文獻中有一篇開創性的工作，

452
00:18:38,039 --> 00:18:40,440
討論了

453
00:18:40,440 --> 00:18:43,380
最佳動作的有效計算，

454
00:18:43,380 --> 00:18:45,900
並且在該工作中，有一個

455
00:18:45,900 --> 00:18:47,760
類似於我們的數量 先驗偏好，

456
00:18:47,760 --> 00:18:49,500
稱為意願函數，

457
00:18:49,500 --> 00:18:52,200
例如，在這個網格世界中，

458
00:18:52,200 --> 00:18:54,240
這裡較暗的顏色更受歡迎，

459
00:18:54,240 --> 00:18:56,400
所以如果這跨越了你的最終黃金狀態，

460
00:18:56,400 --> 00:18:58,740
本文中的代理正在嘗試

461
00:18:58,740 --> 00:19:01,980
做什麼，或者本文中的所述學習方法

462
00:19:01,980 --> 00:19:04,580
是 嘗試做的就是盡可能

463
00:19:04,580 --> 00:19:06,900
最佳地學習這個意願函數，

464
00:19:06,900 --> 00:19:09,059
本文已經表明，

465
00:19:09,059 --> 00:19:12,660
如果你嘗試

466
00:19:12,660 --> 00:19:15,179
使用特定的

467
00:19:15,179 --> 00:19:18,120
學習規則來學習意願函數，那麼它在計算上

468
00:19:18,120 --> 00:19:20,340
比 Q 學習要高效得多，Q 學習

469
00:19:20,340 --> 00:19:22,919
是一種很好的學習方法。 已知的強化

470
00:19:22,919 --> 00:19:24,960
學習算法，所以 Q 學習是一種

471
00:19:24,960 --> 00:19:27,179
眾所周知的計算最優

472
00:19:27,179 --> 00:19:29,520
算法，但在本文中，

473
00:19:29,520 --> 00:19:31,140
學習

474
00:19:31,140 --> 00:19:32,880
這種意願函數的特定學習規則要

475
00:19:32,880 --> 00:19:35,539
快得多，並且這種近似

476
00:19:35,539 --> 00:19:38,460
表示它與

477
00:19:38,460 --> 00:19:40,559
最優意願函數有多麼不同，所以這

478
00:19:40,559 --> 00:19:42,480
意願函數只不過是我們的先驗

479
00:19:42,480 --> 00:19:45,179
偏好，

480
00:19:45,179 --> 00:19:48,600
正如前面提到的，

481
00:19:48,600 --> 00:19:50,100
學習比

482
00:19:50,100 --> 00:19:53,039
在 Q 學習中學習 Q 函數效率更高，所以

483
00:19:53,039 --> 00:19:55,860
這是特定的學習規則，

484
00:19:55,860 --> 00:19:57,600
具體取決於它

485
00:19:57,600 --> 00:19:59,820
從環境和這個

486
00:19:59,820 --> 00:20:01,860
特定的網格世界環境中獲得的獎勵 我們

487
00:20:01,860 --> 00:20:03,539
只在最後一步給予它獎勵，這

488
00:20:03,539 --> 00:20:05,220
類似於稀疏的朋友

489
00:20:05,220 --> 00:20:08,580
偏好，在達到最終目標狀態之前，代理基本上不會獲得

490
00:20:08,580 --> 00:20:11,400
獎勵，

491
00:20:11,400 --> 00:20:14,760
並且使用這個學習規則，其中

492
00:20:14,760 --> 00:20:17,640
有一個參數 ETA

493
00:20:17,640 --> 00:20:20,100
控制這個速度的快慢

494
00:20:20,100 --> 00:20:22,980
學習發生所以基本上我們

495
00:20:22,980 --> 00:20:24,200
嘗試

496
00:20:24,200 --> 00:20:26,880
研究這個學習

497
00:20:26,880 --> 00:20:29,340
參數 Rita 的效果，但我們觀察

498
00:20:29,340 --> 00:20:32,400
到它非常強大，

499
00:20:32,400 --> 00:20:35,059
即使

500
00:20:35,059 --> 00:20:38,700
這個呃學習參數的值可變，它也可以可靠地學習先驗偏好

501
00:20:38,700 --> 00:20:41,580
，我們看到的是

502
00:20:41,580 --> 00:20:43,620
代理能夠

503
00:20:43,620 --> 00:20:45,539
隨著時間的推移，要非常快速地學習有意義的先驗表現，

504
00:20:45,539 --> 00:20:48,600
並使用如此有意義的 Prime

505
00:20:48,600 --> 00:20:50,700
偏好，那麼智能體就不必進行

506
00:20:50,700 --> 00:20:53,580
大量計劃，它可以用非常短的規劃時間範圍來管理最佳

507
00:20:53,580 --> 00:20:56,160
行為或有目的的行為，

508
00:20:56,160 --> 00:20:59,700
並且

509
00:20:59,700 --> 00:21:02,460
考慮到這兩種解決方案，我們可以

510
00:21:02,460 --> 00:21:04,679
擴大用於決策的主動推理算法

511
00:21:04,679 --> 00:21:07,919
並討論

512
00:21:07,919 --> 00:21:11,400
計算效率我們看到

513
00:21:11,400 --> 00:21:13,320
動態規劃方法可以將

514
00:21:13,320 --> 00:21:16,440
30 個時間步植入未來，而計算

515
00:21:16,440 --> 00:21:18,360
複雜度僅為 1000，而復雜的計算複雜度為

516
00:21:18,360 --> 00:21:21,120
10 的

517
00:21:21,120 --> 00:21:23,700
68 次方 推理

518
00:21:23,700 --> 00:21:26,880
和第二種學習先驗

519
00:21:26,880 --> 00:21:29,580
偏好的方法，我們稱之為主動

520
00:21:29,580 --> 00:21:32,340
推理，它只需要提前計劃一個

521
00:21:32,340 --> 00:21:34,740
時間步驟，因此

522
00:21:34,740 --> 00:21:36,919
從這個意義上說，它的計算效率更高，

523
00:21:36,919 --> 00:21:39,659
但它必須學習，所以在 DPFE

524
00:21:39,659 --> 00:21:41,640
方法中我們沒有學習 我們

525
00:21:41,640 --> 00:21:43,799
優先使用這些點並進行

526
00:21:43,799 --> 00:21:44,520
整個

527
00:21:44,520 --> 00:21:46,500
深度的規劃，在另一種

528
00:21:46,500 --> 00:21:48,240
方法中，我們讓智能體

529
00:21:48,240 --> 00:21:50,280
優先學習這一點，但這樣我們就可以節省

530
00:21:50,280 --> 00:21:53,280
很多規劃，因為它知道很多

531
00:21:53,280 --> 00:21:57,299
要做什麼，及時正確，如此

532
00:21:57,299 --> 00:22:00,179
圖形化 我們可以看到，嗯，

533
00:22:00,179 --> 00:22:01,799
DPFE 方法確實

534
00:22:01,799 --> 00:22:04,020
計算效率很高，並且當

535
00:22:04,020 --> 00:22:06,600
在 AI 50 中繪製相對於時間的 uh

536
00:22:06,600 --> 00:22:08,520
等於一種方法時，嗯，

537
00:22:08,520 --> 00:22:10,740
它基本上在計算上

538
00:22:10,740 --> 00:22:12,659
更便宜，是的，

539
00:22:12,659 --> 00:22:16,220
所以它的擴展性非常

540
00:22:16,220 --> 00:22:19,860
好，呃，隨著越來越

541
00:22:19,860 --> 00:22:21,720
高和

542
00:22:21,720 --> 00:22:24,120
復雜性 所以我們

543
00:22:24,120 --> 00:22:27,600
在

544
00:22:27,600 --> 00:22:28,380


545
00:22:28,380 --> 00:22:31,020
非常巨大的狀態空間（比如 900 個

546
00:22:31,020 --> 00:22:32,000
狀態）中測試了這些方法，而

547
00:22:32,000 --> 00:22:33,840


548
00:22:33,840 --> 00:22:36,120


549
00:22:36,120 --> 00:22:38,640
在通常看到的積極影響文獻中，狀態空間的維度為 5 或 10，

550
00:22:38,640 --> 00:22:40,559


551
00:22:40,559 --> 00:22:43,200
所以我想強調一下，

552
00:22:43,200 --> 00:22:44,760
我們是 這裡不使用任何神經網絡，

553
00:22:44,760 --> 00:22:47,039
我們使用

554
00:22:47,039 --> 00:22:49,740
可解釋的主動推理代理正在

555
00:22:49,740 --> 00:22:52,200
執行所有必要的矩陣

556
00:22:52,200 --> 00:22:53,820
乘法，以便我們可以訪問

557
00:22:53,820 --> 00:22:55,140
和

558
00:22:55,140 --> 00:22:56,220


559
00:22:56,220 --> 00:22:58,620
解釋

560
00:22:58,620 --> 00:23:00,179
這些算法中發生的每個計算，

561
00:23:00,179 --> 00:23:02,280
並且當首先在這些網格上進行測試時，

562
00:23:02,280 --> 00:23:04,559
我們驗證了這一點 在一個

563
00:23:04,559 --> 00:23:06,780
較小的網格上，我們

564
00:23:06,780 --> 00:23:08,580
觀察到，與

565
00:23:08,580 --> 00:23:11,220


566
00:23:11,220 --> 00:23:12,780
隊列學習

567
00:23:12,780 --> 00:23:15,059
和 dynaq 等基準強化學習算法相比，dynaq 是一種基於模型的

568
00:23:15,059 --> 00:23:17,760
強化學習算法，我們

569
00:23:17,760 --> 00:23:20,280
比較了我們新提出的代理（

570
00:23:20,280 --> 00:23:24,419
DPFE 和 aif），我們看到了非常好的

571
00:23:24,419 --> 00:23:27,900
性能和 aif 代理稍微

572
00:23:27,900 --> 00:23:29,220
差一些，這是因為它只

573
00:23:29,220 --> 00:23:31,080
提前規劃一步，但是進行

574
00:23:31,080 --> 00:23:34,500
全職規劃的 DPFE 代理的性能與

575
00:23:34,500 --> 00:23:36,419


576
00:23:36,419 --> 00:23:37,919
基準個人學習

577
00:23:37,919 --> 00:23:41,039
算法一樣好，我們使用

578
00:23:41,039 --> 00:23:44,640
更大和更弱的網格對其進行了測試，當我們

579
00:23:44,640 --> 00:23:47,100
在 黃金

580
00:23:47,100 --> 00:23:49,620
狀態是指當代理

581
00:23:49,620 --> 00:23:53,039
必須導航到

582
00:23:53,039 --> 00:23:54,480


583
00:23:54,480 --> 00:23:56,520
隨機黃金狀態時，我們

584
00:23:56,520 --> 00:23:59,460
每 10 集更改一次黃金狀態，

585
00:23:59,460 --> 00:24:01,260
我們觀察到 Dyna 隊列

586
00:24:01,260 --> 00:24:04,140
比我們的 bpfe 代理花費更多時間來

587
00:24:04,140 --> 00:24:07,200
恢復和執行操作 嗯，所以 DPFE

588
00:24:07,200 --> 00:24:09,059
代理在這種隨機環境中

589
00:24:09,059 --> 00:24:11,280
表現得非常好，甚至比

590
00:24:11,280 --> 00:24:13,620
動態代理更好，您還可以看到

591
00:24:13,620 --> 00:24:16,500
AI 階段恢復得

592
00:24:16,500 --> 00:24:19,140
更快，但不如其他代理那麼好，

593
00:24:19,140 --> 00:24:22,500
所以基本上這就是

594
00:24:22,500 --> 00:24:25,200
論文和 方法 所以是的，

595
00:24:25,200 --> 00:24:27,960
謝謝你的聆聽，我願意參與

596
00:24:27,960 --> 00:24:30,380
討論，

597
00:24:31,640 --> 00:24:34,740
好吧，

598
00:24:34,740 --> 00:24:36,360


599
00:24:36,360 --> 00:24:38,520
太棒了，非常酷，好吧，就在

600
00:24:38,520 --> 00:24:43,200
我們開始

601
00:24:43,200 --> 00:24:44,460
討論的時候，

602
00:24:44,460 --> 00:24:47,280
嗯，如果有人想先添加任何內容，那麼

603
00:24:47,280 --> 00:24:50,220
你是如何開始從事這個

604
00:24:50,220 --> 00:24:52,740
項目的，你在學習嗎？ 主動

605
00:24:52,740 --> 00:24:54,720
推理，你覺得這個問題

606
00:24:54,720 --> 00:24:57,900
很有趣，或者你正在做

607
00:24:57,900 --> 00:25:00,179
規劃，並把主動

608
00:25:00,179 --> 00:25:03,059
推理作為一種方法，

609
00:25:03,059 --> 00:25:05,159
嗯，是的，

610
00:25:05,159 --> 00:25:08,400
我對自己的背景了解很少，所以我在本科時學習了物理，

611
00:25:08,400 --> 00:25:11,340
並在接近

612
00:25:11,340 --> 00:25:13,080
結束時發布了它。 畢業後，我對

613
00:25:13,080 --> 00:25:15,120
博弈論

614
00:25:15,120 --> 00:25:17,880
和可重用學習之類的東西感興趣，我

615
00:25:17,880 --> 00:25:20,880
與 Adil 教授

616
00:25:20,880 --> 00:25:23,100
和 Manoj 教授聯合攻讀博士學位，所以

617
00:25:23,100 --> 00:25:25,200
Manoj 教授是一個控制理論專家，而 a

618
00:25:25,200 --> 00:25:28,140
deal 是一名神經科學家，在

619
00:25:28,140 --> 00:25:30,600
我的研究開始時 博士期間，我開始閱讀

620
00:25:30,600 --> 00:25:34,020
主動推理文獻，我想

621
00:25:34,020 --> 00:25:37,260
在問題中實現這一點，我

622
00:25:37,260 --> 00:25:39,840
總是著迷於可解釋的

623
00:25:39,840 --> 00:25:42,720
主動推理，呃關於

624
00:25:42,720 --> 00:25:44,700
預期自由能的想法試圖最小化

625
00:25:44,700 --> 00:25:46,860
風險以及預期的模糊性，它不僅

626
00:25:46,860 --> 00:25:49,740
使代理工作，而且

627
00:25:49,740 --> 00:25:52,020
能夠告訴我它們是如何工作的，這也是讓

628
00:25:52,020 --> 00:25:54,419
我著迷的積極影響力，

629
00:25:54,419 --> 00:25:56,820
最初我嘗試實現它們，

630
00:25:56,820 --> 00:26:00,179
我們發表了一篇會議論文，

631
00:26:00,179 --> 00:26:03,720
其中我們將其與

632
00:26:03,720 --> 00:26:06,419
類似的網格任​​務進行了比較，然後我

633
00:26:06,419 --> 00:26:08,100
遇到了擴展問題 主動

634
00:26:08,100 --> 00:26:10,620
推理，然後我開始研究

635
00:26:10,620 --> 00:26:12,659
複雜的推理，

636
00:26:12,659 --> 00:26:15,539
嗯，是的，所以基本上這些方法是

637
00:26:15,539 --> 00:26:17,580
出於我想要

638
00:26:17,580 --> 00:26:21,539
放大地圖的需要，並且我總是

639
00:26:21,539 --> 00:26:22,919
在使用

640
00:26:22,919 --> 00:26:25,559
嗯深主動推理時使用總線保持，因為嗯，

641
00:26:25,559 --> 00:26:28,020
我不想使用

642
00:26:28,020 --> 00:26:30,539
因為深度

643
00:26:30,539 --> 00:26:32,100
強化學習本身就是一個

644
00:26:32,100 --> 00:26:34,140
巨大的領域，

645
00:26:34,140 --> 00:26:37,080
如果你只是想擴展

646
00:26:37,080 --> 00:26:39,240
主動推理，那麼也許就做深度

647
00:26:39,240 --> 00:26:41,820
強化學習，這就是我的

648
00:26:41,820 --> 00:26:43,860
想法，是的，這基本上就是

649
00:26:43,860 --> 00:26:45,659
背景，是的，

650
00:26:45,659 --> 00:26:48,720
這就是它的方式

651
00:26:48,720 --> 00:26:50,880
一切順利，我將首先在實時聊天中提出一個問題，

652
00:26:50,880 --> 00:26:52,559
我們

653
00:26:52,559 --> 00:26:54,779
可能會討論各個方面，因為

654
00:26:54,779 --> 00:26:57,720
您的演示中有很多內容，因此

655
00:26:57,720 --> 00:27:01,980
NL Dawn 寫道，我想知道何時

656
00:27:01,980 --> 00:27:04,620
計算預期自由能

657
00:27:04,620 --> 00:27:07,679
時間範圍使用什麼樣的平均

658
00:27:07,679 --> 00:27:11,039
場近似來分解 s 存儲的 Q，

659
00:27:11,039 --> 00:27:13,020


660
00:27:13,020 --> 00:27:16,100


661
00:27:19,980 --> 00:27:23,040
所以我假設問題是關於

662
00:27:23,040 --> 00:27:24,299
[音樂]

663
00:27:24,299 --> 00:27:28,799
DPFE 中的預期自由能並

664
00:27:28,799 --> 00:27:31,380
基本上計算這個，

665
00:27:31,380 --> 00:27:32,159


666
00:27:32,159 --> 00:27:35,279
所以在我的模擬中我使用置信

667
00:27:35,279 --> 00:27:39,120
傳播來實現這一點 信念隊列，

668
00:27:39,120 --> 00:27:41,460
你也可以使用變分

669
00:27:41,460 --> 00:27:43,140
消息傳遞或邊際消息

670
00:27:43,140 --> 00:27:47,279
傳遞，這不是問題，

671
00:27:47,400 --> 00:27:50,580
但是一旦你有了這個信念隊列，

672
00:27:50,580 --> 00:27:52,740
你會做什麼，所以是的，

673
00:27:52,740 --> 00:27:55,340


674
00:27:56,940 --> 00:27:59,900
所以

675
00:27:59,940 --> 00:28:04,260
當我說你想像一下 Q 時，

676
00:28:04,260 --> 00:28:07,520
我主要使用什麼 是一個熱門向量，所以

677
00:28:07,520 --> 00:28:11,220
要做出決定，你需要相信

678
00:28:11,220 --> 00:28:13,260
你

679
00:28:13,260 --> 00:28:16,380
在主動推理中擺脫了感知步驟，但想像一下

680
00:28:16,380 --> 00:28:18,720
這些是硬表，它們是一個

681
00:28:18,720 --> 00:28:22,020
熱門向量，所以假設你的

682
00:28:22,020 --> 00:28:25,380
生成模型中有 10 個狀態，呃你的

683
00:28:25,380 --> 00:28:27,900
您使用的隊列是用於

684
00:28:27,900 --> 00:28:29,100


685
00:28:29,100 --> 00:28:32,220
規劃的精確提示，但對於

686
00:28:32,220 --> 00:28:33,840
決策，您使用

687
00:28:33,840 --> 00:28:36,299


688
00:28:36,299 --> 00:28:38,460
從感知步驟中獲得的令人印象深刻的不精確平均場近似隊列，所以

689
00:28:38,460 --> 00:28:40,679
我不知道這是否回答了

690
00:28:40,679 --> 00:28:42,059
問題，

691
00:28:42,059 --> 00:28:44,880
但也許我也想思考 我

692
00:28:44,880 --> 00:28:46,919
還需要更多地考慮

693
00:28:46,919 --> 00:28:49,200


694
00:28:49,200 --> 00:28:50,880
步驟中可能存在的近似值，

695
00:28:50,880 --> 00:28:53,700
很酷，是的，如果他們願意，他們可以寫更多內容，嗯，讓我們

696
00:28:53,700 --> 00:28:56,460
更籠統

697
00:28:56,460 --> 00:29:00,419
地談談偏好學習，

698
00:29:00,419 --> 00:29:02,700
所以在主動推理

699
00:29:02,700 --> 00:29:05,940
生成模型的背景下，我們 在

700
00:29:05,940 --> 00:29:08,279
隱藏狀態的觀察之間進行調解，

701
00:29:08,279 --> 00:29:10,020
學習

702
00:29:10,020 --> 00:29:12,480
很有意義，它是關於學習

703
00:29:12,480 --> 00:29:14,220


704
00:29:14,220 --> 00:29:15,779
世界隱藏狀態中的觀察之間的映射，然後我們

705
00:29:15,779 --> 00:29:18,240
通過學習學習行動的後果

706
00:29:18,240 --> 00:29:20,399
以及事物如何隨著時間而變化，

707
00:29:20,399 --> 00:29:21,419


708
00:29:21,419 --> 00:29:24,659
而偏好學習正在學習

709
00:29:24,659 --> 00:29:27,960
cc，你強調這

710
00:29:27,960 --> 00:29:29,700
是一個非常

711
00:29:29,700 --> 00:29:31,340


712
00:29:31,340 --> 00:29:34,320
有趣的變量，

713
00:29:34,320 --> 00:29:38,100
我很好奇我們如何

714
00:29:38,100 --> 00:29:41,159
學習正確的東西，我們如何知道

715
00:29:41,159 --> 00:29:42,960
我們正在學習適應性

716
00:29:42,960 --> 00:29:44,399
偏好，

717
00:29:44,399 --> 00:29:46,200
然後如何 偏好

718
00:29:46,200 --> 00:29:48,720
學習是否會減少認知開銷或

719
00:29:48,720 --> 00:29:50,820
計算複雜性是的，

720
00:29:50,820 --> 00:29:53,460
很好，

721
00:29:53,460 --> 00:29:54,480
所以

722
00:29:54,480 --> 00:29:57,179
如果你想學習祈禱

723
00:29:57,179 --> 00:29:59,940
偏好，那麼假設

724
00:29:59,940 --> 00:30:01,559
有一些東西可以追逐，或者有一些

725
00:30:01,559 --> 00:30:04,500
東西可以最大化，比如獎勵，所以

726
00:30:04,500 --> 00:30:06,779
在強化學習中說呃

727
00:30:06,779 --> 00:30:08,340
設置

728
00:30:08,340 --> 00:30:09,840
來自

729
00:30:09,840 --> 00:30:12,960
您試圖最大化的環境的明確獎勵，並說對於

730
00:30:12,960 --> 00:30:15,000
這個網格，您僅在最後一步中獲得該獎勵，這

731
00:30:15,000 --> 00:30:17,220
就是這

732
00:30:17,220 --> 00:30:19,020
使得這個問題變得困難，所以

733
00:30:19,020 --> 00:30:22,500
如果您每次都獲得獎勵 時間

734
00:30:22,500 --> 00:30:25,140
步那麼基本上你知道該怎麼做，

735
00:30:25,140 --> 00:30:28,020
我只需要

736
00:30:28,020 --> 00:30:30,419
在每個時間步都追求那個獎勵，但在這裡你

737
00:30:30,419 --> 00:30:32,880
可能需要提前 15 個時間步

738
00:30:32,880 --> 00:30:36,059
才能獲得那個獎勵，而這

739
00:30:36,059 --> 00:30:39,659
基本上很難在這裡做到，嗯，如果

740
00:30:39,659 --> 00:30:41,520
你正在嘗試，因為我正在嘗試

741
00:30:41,520 --> 00:30:43,020
學習先驗偏好，

742
00:30:43,020 --> 00:30:44,820
嗯，

743
00:30:44,820 --> 00:30:47,940


744
00:30:47,940 --> 00:30:49,919
如果環境不關心

745
00:30:49,919 --> 00:30:52,200
我所做的事情或者我無法定義什麼是

746
00:30:52,200 --> 00:30:54,840
好或壞，那麼環境中應該存在一個獎勵結構 那麼

747
00:30:54,840 --> 00:30:56,340
學習先驗偏好就沒有意義了，所以

748
00:30:56,340 --> 00:30:57,840
這裡

749
00:30:57,840 --> 00:30:59,399
獎勵是控制

750
00:30:59,399 --> 00:31:01,559
精靈差異學習的東西，

751
00:31:01,559 --> 00:31:03,600
好處是即使我只

752
00:31:03,600 --> 00:31:06,539
在最後一個時間步驟獲得獎勵，

753
00:31:06,539 --> 00:31:09,240
我也有 B 矩陣 我有

754
00:31:09,240 --> 00:31:11,340
從不同狀態過渡的經驗，

755
00:31:11,340 --> 00:31:12,659


756
00:31:12,659 --> 00:31:14,760
我達到了最終的黃金狀態，

757
00:31:14,760 --> 00:31:16,860


758
00:31:16,860 --> 00:31:19,440
我們正在使用的算法或

759
00:31:19,440 --> 00:31:21,539
我們正在使用的學習規則正是在

760
00:31:21,539 --> 00:31:23,820


761
00:31:23,820 --> 00:31:26,340
我介紹的論文中學習最優控制中類似的東西，

762
00:31:26,340 --> 00:31:27,539
稱為合意性

763
00:31:27,539 --> 00:31:30,539
函數 考慮到你有這個

764
00:31:30,539 --> 00:31:32,760
願望函數，所以想像一下，

765
00:31:32,760 --> 00:31:36,120
也許我從這個州開始，那麼

766
00:31:36,120 --> 00:31:37,860
我只需要看看我最近的

767
00:31:37,860 --> 00:31:40,399
鄰居就可以做出決定，

768
00:31:40,399 --> 00:31:44,279
如果

769
00:31:44,279 --> 00:31:46,620
該州下面的州

770
00:31:46,620 --> 00:31:48,899
比我只需要計劃一個州更受歡迎

771
00:31:48,899 --> 00:31:50,520
時間而不是提前，這是

772
00:31:50,520 --> 00:31:52,380
我必須做出的最佳決定，

773
00:31:52,380 --> 00:31:54,059
因此學習這種驕傲偏好會

774
00:31:54,059 --> 00:31:55,919
減少認知負擔，

775
00:31:55,919 --> 00:31:57,419
因為我現在只關注最近的

776
00:31:57,419 --> 00:31:59,760
鄰居，我不必一直計劃

777
00:31:59,760 --> 00:32:01,799
到最終的良好狀態

778
00:32:01,799 --> 00:32:04,799
我會盡可能有效地學習這一點，

779
00:32:04,799 --> 00:32:07,260
因為它是一種有

780
00:32:07,260 --> 00:32:09,659
保證的算法，我們測試了它的

781
00:32:09,659 --> 00:32:13,320
魯棒性，它也通過

782
00:32:13,320 --> 00:32:14,580
我從環境中獲得的獎勵來告知，

783
00:32:14,580 --> 00:32:15,659


784
00:32:15,659 --> 00:32:20,100
如果是的話，如果有某種方式可以

785
00:32:20,100 --> 00:32:22,860
定義 首選什麼呃那麼

786
00:32:22,860 --> 00:32:24,840
你一定會通過這個算法學習到一個

787
00:32:24,840 --> 00:32:26,520
有意義的私人朋友，

788
00:32:26,520 --> 00:32:28,559


789
00:32:28,559 --> 00:32:31,559
所以如果只使用最近鄰

790
00:32:31,559 --> 00:32:34,260
和一次推理，

791
00:32:34,260 --> 00:32:37,320
那麼什麼可以防止這種

792
00:32:37,320 --> 00:32:39,360
偏好學習代理

793
00:32:39,360 --> 00:32:43,158
陷入本地最優狀態，

794
00:32:43,320 --> 00:32:47,520
嗯，所以 這裡不會有本地 Optima，

795
00:32:47,520 --> 00:32:50,340
這就是問題所在，

796
00:32:50,340 --> 00:32:52,860
嗯，為什麼會有本地 Optima 呃，

797
00:32:52,860 --> 00:32:57,059
如果我使用獎勵來學習它，

798
00:32:57,059 --> 00:33:00,120
呃，是的，所以如果有本地 Optima，那麼

799
00:33:00,120 --> 00:33:02,159
它會卡在本地

800
00:33:02,159 --> 00:33:05,460
Optima 中，但贏了 如果

801
00:33:05,460 --> 00:33:07,440
你以這種方式學習，那麼就不是一個，因為

802
00:33:07,440 --> 00:33:08,760
你是從自己的

803
00:33:08,760 --> 00:33:11,940
經驗以及你如何在

804
00:33:11,940 --> 00:33:14,220
某個時候獲得獎勵來學習的，我們

805
00:33:14,220 --> 00:33:16,980
觀察到，這是非常

806
00:33:16,980 --> 00:33:17,760


807
00:33:17,760 --> 00:33:20,399
漸進的，沒有任何問題 或者

808
00:33:20,399 --> 00:33:23,880
當你學習時，

809
00:33:23,880 --> 00:33:27,840
它是一種回填

810
00:33:27,840 --> 00:33:30,179
偏好，

811
00:33:30,179 --> 00:33:33,539
這樣就可以有一條

812
00:33:33,539 --> 00:33:36,779
通往遠端目標的平滑路徑是的，

813
00:33:36,779 --> 00:33:39,480
所以它在動畫中

814
00:33:39,480 --> 00:33:42,299
看起來像是一種後退的感覺，但

815
00:33:42,299 --> 00:33:44,279
嗯，你實際上是 從你的經驗中學習，

816
00:33:44,279 --> 00:33:45,360


817
00:33:45,360 --> 00:33:46,620


818
00:33:46,620 --> 00:33:49,980
嗯，及時前進，所以

819
00:33:49,980 --> 00:33:51,480
當我從這個

820
00:33:51,480 --> 00:33:53,100
狀態過渡到這個狀態時，我觀察到了獎勵，所以這一定很好，

821
00:33:53,100 --> 00:33:55,860
呃，然後在下一個時間步驟中，

822
00:33:55,860 --> 00:33:58,559
我說好吧，這個日期負責

823
00:33:58,559 --> 00:34:00,419
帶我去那裡 所以這可能

824
00:34:00,419 --> 00:34:03,120
也不錯，但不如另一個好，

825
00:34:03,120 --> 00:34:05,100
所以它看起來像是向後學習，

826
00:34:05,100 --> 00:34:07,799
但實際上是從真實的

827
00:34:07,799 --> 00:34:12,000
嗯學習到加一種體驗

828
00:34:13,500 --> 00:34:15,119
好吧，

829
00:34:15,119 --> 00:34:16,980
你知道，這樣

830
00:34:16,980 --> 00:34:20,339
你就不能擁有本地馬克西瑪斯，因為

831
00:34:20,339 --> 00:34:21,300
嗯，

832
00:34:21,300 --> 00:34:24,119
是的，這就是這樣做的學習規則，

833
00:34:24,119 --> 00:34:25,859
所以我不知道如何

834
00:34:25,859 --> 00:34:29,219
更系統地回答它是的，是的，嗯，

835
00:34:29,219 --> 00:34:33,000
現實世界的情況或

836
00:34:33,000 --> 00:34:35,399
現實生活中的情況是什麼，你

837
00:34:35,399 --> 00:34:37,560
看到這種偏好學習

838
00:34:37,560 --> 00:34:40,080
發生在

839
00:34:40,080 --> 00:34:41,159
是的，

840
00:34:41,159 --> 00:34:43,020
太棒了 問題

841
00:34:43,020 --> 00:34:46,080
所以，我們實驗室發表了一篇論文，

842
00:34:46,080 --> 00:34:48,659
其中神經元正在

843
00:34:48,659 --> 00:34:51,359
學習乒乓球遊戲，

844
00:34:51,359 --> 00:34:52,980
所以如果這篇論文被

845
00:34:52,980 --> 00:34:55,260
稱為“dist Brain”呃，該設備被

846
00:34:55,260 --> 00:34:57,000
稱為“應變”，那麼他們所

847
00:34:57,000 --> 00:34:58,740
取得的成就就是他們管理 在

848
00:34:58,740 --> 00:35:00,300


849
00:35:00,300 --> 00:35:03,420
矽芯片上培養神經元，當它成功接球或打得很好

850
00:35:03,420 --> 00:35:04,020


851
00:35:04,020 --> 00:35:08,520
時，他們給它一個很好的反饋信號，當

852
00:35:08,520 --> 00:35:11,400


853
00:35:11,400 --> 00:35:13,880


854
00:35:13,880 --> 00:35:15,720


855
00:35:15,720 --> 00:35:18,119
它犯錯誤時，他們給它一個震驚，我們

856
00:35:18,119 --> 00:35:19,560


857
00:35:19,560 --> 00:35:22,260
在論文中看到的是我們 看到的是，

858
00:35:22,260 --> 00:35:24,140
隨著時間的推移，它學會了玩遊戲，

859
00:35:24,140 --> 00:35:27,359
在這種情況下，想像一下，

860
00:35:27,359 --> 00:35:29,880
如果我遇到說世界上積極的事情，

861
00:35:29,880 --> 00:35:33,359
那麼我可能會將

862
00:35:33,359 --> 00:35:35,520
嗯，什麼是好的與我

863
00:35:35,520 --> 00:35:37,380
過去觀察到的狀態等等聯繫起來，對吧 所以

864
00:35:37,380 --> 00:35:38,880
我

865
00:35:38,880 --> 00:35:39,660


866
00:35:39,660 --> 00:35:42,119
一步一步地了解到什麼是好什麼是壞，

867
00:35:42,119 --> 00:35:44,700
這是一個合理的假設，

868
00:35:44,700 --> 00:35:46,619
所以如果

869
00:35:46,619 --> 00:35:47,220
嗯，如果

870
00:35:47,220 --> 00:35:51,300
我想說變得更健康，那麼我可能

871
00:35:51,300 --> 00:35:53,280
會將去健身房

872
00:35:53,280 --> 00:35:54,660


873
00:35:54,660 --> 00:35:57,000
視為一件好事，那麼我也可能會將

874
00:35:57,000 --> 00:35:59,099
步行去健身房聯繫起來 作為一件

875
00:35:59,099 --> 00:36:01,260
好事，那麼我可能會將

876
00:36:01,260 --> 00:36:03,900
穿鞋視為一件好事，所以嗯，以這種

877
00:36:03,900 --> 00:36:04,920


878
00:36:04,920 --> 00:36:06,960
方式學習先驗偏好是

879
00:36:06,960 --> 00:36:10,079
有道理的，我會說，但是，是的，這是解決

880
00:36:10,079 --> 00:36:11,640


881
00:36:11,640 --> 00:36:12,480


882
00:36:12,480 --> 00:36:14,820


883
00:36:14,820 --> 00:36:17,040
維數詛咒問題的計算嗯解決方案，但要在

884
00:36:17,040 --> 00:36:19,619
真實的環境中進行測試 世界絕對是我們

885
00:36:19,619 --> 00:36:21,599
應該做的，

886
00:36:21,599 --> 00:36:24,119
嗯，是的，我也期待這樣做，

887
00:36:24,119 --> 00:36:25,800


888
00:36:25,800 --> 00:36:27,660
好吧，讓我嘗試由你來運行一個真實

889
00:36:27,660 --> 00:36:29,760
世界的情況，看看

890
00:36:29,760 --> 00:36:32,040
這是否連接起來，

891
00:36:32,040 --> 00:36:35,099
所以我們想要我們想要上交我們'

892
00:36:35,099 --> 00:36:38,280
通過提交一篇獲得高分的論文來獲得獎勵，

893
00:36:38,280 --> 00:36:39,900


894
00:36:39,900 --> 00:36:42,599
因此在

895
00:36:42,599 --> 00:36:45,540
開始論文的想法和

896
00:36:45,540 --> 00:36:48,839
看到首選的稀疏結果（即

897
00:36:48,839 --> 00:36:51,180
分數）之間有很多步驟，

898
00:36:51,180 --> 00:36:53,520
我們做了各種各樣的事情，

899
00:36:53,520 --> 00:36:55,260
然後我們學得很好

900
00:36:55,260 --> 00:36:57,079
嗯 這一次我確實得到了很好的成績，

901
00:36:57,079 --> 00:36:59,940
它的格式很好，

902
00:36:59,940 --> 00:37:02,880
然後就是這樣，現在你可以

903
00:37:02,880 --> 00:37:05,460
擴展你的偏好範圍，

904
00:37:05,460 --> 00:37:07,800
然後是什麼讓我得到

905
00:37:07,800 --> 00:37:10,140
很好的格式，我按時完成了，

906
00:37:10,140 --> 00:37:12,900
然後你就工作了 一直到

907
00:37:12,900 --> 00:37:16,579
構思階段，這樣將來

908
00:37:16,579 --> 00:37:20,640
您就可以準確地進行一步推理，並將

909
00:37:20,640 --> 00:37:23,099
其作為一項技能一步一步地進行，

910
00:37:23,099 --> 00:37:24,180


911
00:37:24,180 --> 00:37:26,400
而不需要

912
00:37:26,400 --> 00:37:28,740


913
00:37:28,740 --> 00:37:31,740
對所有可能的樹

914
00:37:31,740 --> 00:37:35,180
結構進行 15 個時間步的推理，這樣您就可以 有點用你的

915
00:37:35,180 --> 00:37:39,240
具體學習來簡化

916
00:37:39,240 --> 00:37:43,680
問題的結構，是的，

917
00:37:43,680 --> 00:37:45,540
我們可以再次看看所有這些的計算

918
00:37:45,540 --> 00:37:47,220
複雜性

919
00:37:47,220 --> 00:37:51,078
嗎？

920
00:37:51,359 --> 00:37:54,300
所以這裡 CIF 代表經典的

921
00:37:54,300 --> 00:37:56,760
主動推理，並進行全面的

922
00:37:56,760 --> 00:37:58,560
規劃，

923
00:37:58,560 --> 00:38:00,660
嗯，它是圍繞的 10 的 18 次方。所以

924
00:38:00,660 --> 00:38:03,180
這是這個特定的網格示例，

925
00:38:03,180 --> 00:38:07,140
有 100 個狀態和四個可用

926
00:38:07,140 --> 00:38:09,960
操作，所以這是一個特殊情況，

927
00:38:09,960 --> 00:38:12,119
我只是想用數字來正確地看待

928
00:38:12,119 --> 00:38:14,400
事物，所以對於這個

929
00:38:14,400 --> 00:38:16,500
特定的示例，如果您想

930
00:38:16,500 --> 00:38:18,960
用策略空間來做這件事，

931
00:38:18,960 --> 00:38:20,640
嗯，你將必須進行

932
00:38:20,640 --> 00:38:22,440
10 的 18 次方計算，以進行

933
00:38:22,440 --> 00:38:25,380
一步規劃，或者在

934
00:38:25,380 --> 00:38:28,380
復雜推理的一個實例中，

935
00:38:28,380 --> 00:38:30,900
情況更糟，因為呃，你去

936
00:38:30,900 --> 00:38:34,560
狀態空間也很重要，呃，那

937
00:38:34,560 --> 00:38:37,140
贏了 不起作用，所以第三行

938
00:38:37,140 --> 00:38:39,540
應該表明，即使有一段

939
00:38:39,540 --> 00:38:40,500
時間

940
00:38:40,500 --> 00:38:42,599
或者上升足夠多的時間，

941
00:38:42,599 --> 00:38:44,280
如果你

942
00:38:44,280 --> 00:38:47,400
必須進行完整的規劃，那麼也很難進行複雜的推理，但是

943
00:38:47,400 --> 00:38:48,960
當你向後規劃時，使用動態編程

944
00:38:48,960 --> 00:38:50,579


945
00:38:50,579 --> 00:38:52,500
嗯你 可以嘗試進行完整的

946
00:38:52,500 --> 00:38:54,420
規劃，

947
00:38:54,420 --> 00:38:56,520
嗯，這只有一千次

948
00:38:56,520 --> 00:38:58,800
計算，但隨著學習

949
00:38:58,800 --> 00:39:01,380
偏好，它甚至很低呃，當你只做

950
00:39:01,380 --> 00:39:04,520
一次時一步一步哇，

951
00:39:04,520 --> 00:39:06,440
這相當

952
00:39:06,440 --> 00:39:06,839
[音樂]

953
00:39:06,839 --> 00:39:08,220
嗯

954
00:39:08,220 --> 00:39:12,000
相當斯塔克，並且在分支時間

955
00:39:12,000 --> 00:39:15,480
主動推理 在之前的

956
00:39:15,480 --> 00:39:17,640
模型流中，我們還看到了一些

957
00:39:17,640 --> 00:39:19,859
計算複雜性估計，但我

958
00:39:19,859 --> 00:39:22,260
認為這些非常清楚，嗯，

959
00:39:22,260 --> 00:39:25,020
它讓我想到的第一件事是，

960
00:39:25,020 --> 00:39:28,820
沒有人說複雜性很便宜，

961
00:39:28,820 --> 00:39:32,940
我的意思是，它的成本是天文數字，嗯，

962
00:39:32,940 --> 00:39:35,099


963
00:39:35,099 --> 00:39:37,380
甚至可能只在 兩個或三個或

964
00:39:37,380 --> 00:39:38,880
四個或五個

965
00:39:38,880 --> 00:39:41,940
可能開始達到，所以

966
00:39:41,940 --> 00:39:44,040
它從根本上增加了規劃的複雜性，

967
00:39:44,040 --> 00:39:45,720


968
00:39:45,720 --> 00:39:50,040
所以從教學上來說這是非常

969
00:39:50,040 --> 00:39:50,780


970
00:39:50,780 --> 00:39:52,460
有趣的，因為

971
00:39:52,460 --> 00:39:56,280
我們像

972
00:39:56,280 --> 00:39:57,660
我們想像的

973
00:39:57,660 --> 00:39:59,940
主動推理智能

974
00:39:59,940 --> 00:40:01,560
架構一樣思考，

975
00:40:01,560 --> 00:40:04,500
但這

976
00:40:04,500 --> 00:40:07,079
很清楚地表明它不是

977
00:40:07,079 --> 00:40:09,060
你可以列舉一些東西，

978
00:40:09,060 --> 00:40:10,859


979
00:40:10,859 --> 00:40:13,380
所以我認為

980
00:40:13,380 --> 00:40:15,240


981
00:40:15,240 --> 00:40:19,520
通過將其與減少

982
00:40:19,520 --> 00:40:22,140
計算

983
00:40:22,140 --> 00:40:25,440
複雜性的原則方法聯繫起來，而不是

984
00:40:25,440 --> 00:40:29,220
潛在有效但臨時的

985
00:40:29,220 --> 00:40:31,320
複雜性減少方法（例如可能有效的神經

986
00:40:31,320 --> 00:40:33,359
網絡），你所做的事情非常非常有創意且重要 當它們起作用時，

987
00:40:33,359 --> 00:40:35,280
但是一旦它們開始變得臃腫，

988
00:40:35,280 --> 00:40:38,940
現在你就沒有原則，沒有

989
00:40:38,940 --> 00:40:40,680
功效了，是的，

990
00:40:40,680 --> 00:40:43,200
複雜的推理

991
00:40:43,200 --> 00:40:45,240
我必須指出，與動態編程相比，它有它自己的好處，

992
00:40:45,240 --> 00:40:47,579
呃，

993
00:40:47,579 --> 00:40:49,500
從某種意義上說，當你

994
00:40:49,500 --> 00:40:51,839
向前規劃時，你是 考慮到

995
00:40:51,839 --> 00:40:53,520
所有的可能性，

996
00:40:53,520 --> 00:40:55,859
但是當你向後規劃時，

997
00:40:55,859 --> 00:40:58,380
例如，這就像基於提示的

998
00:40:58,380 --> 00:41:00,300
探索，

999
00:41:00,300 --> 00:41:02,280
嗯，就像如果你必須去某個地方

1000
00:41:02,280 --> 00:41:03,839
排隊，然後導航到池

1001
00:41:03,839 --> 00:41:05,640
狀態，那麼向後規劃可能

1002
00:41:05,640 --> 00:41:06,960
不起作用，所以這就是什麼 這是

1003
00:41:06,960 --> 00:41:09,900
我得到的一個反饋，我和一些人討論了這項

1004
00:41:09,900 --> 00:41:10,980
工作，

1005
00:41:10,980 --> 00:41:13,920
所以

1006
00:41:13,920 --> 00:41:16,260
這是關於動態編程的一個值得注意的事情，

1007
00:41:16,260 --> 00:41:18,359
但是

1008
00:41:18,359 --> 00:41:19,920
你在朋友之前學習的另一個地方我相信

1009
00:41:19,920 --> 00:41:22,320
這不是問題，但是動態

1010
00:41:22,320 --> 00:41:23,700
編程的計算成本很低，但是

1011
00:41:23,700 --> 00:41:26,460
它也有其自身的局限性，我

1012
00:41:26,460 --> 00:41:28,099
必須在這裡指出，

1013
00:41:28,099 --> 00:41:31,200
只是為了重申一下，在具有

1014
00:41:31,200 --> 00:41:33,839
貝爾曼最優性的動態規劃中，

1015
00:41:33,839 --> 00:41:36,420
我們正在向後求解，所以這

1016
00:41:36,420 --> 00:41:38,400
有點像“將死”就是我們想要的，

1017
00:41:38,400 --> 00:41:40,440
所以現在我們正在向後工作到

1018
00:41:40,440 --> 00:41:43,440
現在 但我們最終沒有探索

1019
00:41:43,440 --> 00:41:46,440
反事實的

1020
00:41:46,440 --> 00:41:49,200
終點，我們不會回到

1021
00:41:49,200 --> 00:41:51,540
現在的終點，我們完全不

1022
00:41:51,540 --> 00:41:52,980
感興趣，這就是

1023
00:41:52,980 --> 00:41:55,619
為什麼它如此無情，

1024
00:41:55,619 --> 00:41:58,320
但另一方面，這是一個更加

1025
00:41:58,320 --> 00:42:00,420
受限的

1026
00:42:00,420 --> 00:42:02,280
嗯

1027
00:42:02,280 --> 00:42:06,960
搜索，是的，是的 所以我

1028
00:42:06,960 --> 00:42:08,460
這些天在想的是，我們還應該

1029
00:42:08,460 --> 00:42:11,700
考慮這兩個的組合，

1030
00:42:11,700 --> 00:42:14,160
我可以忽略

1031
00:42:14,160 --> 00:42:17,400
這些反事實的事情，

1032
00:42:17,400 --> 00:42:19,680
我可以做動態編程，

1033
00:42:19,680 --> 00:42:23,040
也許我可以做兩個步驟

1034
00:42:23,040 --> 00:42:25,380
因此，如果這是基於隊列的

1035
00:42:25,380 --> 00:42:28,980
探索，那麼說到達隊列是

1036
00:42:28,980 --> 00:42:32,220
一項任務，從隊列到獎勵是

1037
00:42:32,220 --> 00:42:34,619
另一項任務，所以我可以將這

1038
00:42:34,619 --> 00:42:37,460
兩個任務分開並為它們使用動態編程，

1039
00:42:37,460 --> 00:42:40,440
這在計算上更便宜，但

1040
00:42:40,440 --> 00:42:43,079
也保留了這種想法

1041
00:42:43,079 --> 00:42:45,480
嗯，必須這樣做，是的，但這些都是

1042
00:42:45,480 --> 00:42:48,119
未來的事情，我正在考慮

1043
00:42:48,119 --> 00:42:50,700
很酷，或者從

1044
00:42:50,700 --> 00:42:54,359
亞歷克斯寫的聊天中問另一個問題，

1045
00:42:54,359 --> 00:42:57,599
你對嵌套模型有想法或發展嗎，

1046
00:42:57,599 --> 00:43:00,180
其中不同的尺度

1047
00:43:00,180 --> 00:43:02,330
可能有不同的

1048
00:43:02,330 --> 00:43:02,640
[音樂]

1049
00:43:02,640 --> 00:43:03,780
嗯，

1050
00:43:03,780 --> 00:43:06,540
領先一步， 執行速度

1051
00:43:06,540 --> 00:43:07,800


1052
00:43:07,800 --> 00:43:09,960
那麼這個模型在嵌套模型中如何發揮作用

1053
00:43:09,960 --> 00:43:11,579


1054
00:43:11,579 --> 00:43:13,859
以及我們如何

1055
00:43:13,859 --> 00:43:16,380
在

1056
00:43:16,380 --> 00:43:19,500
執行速度和嵌套模型中及時考慮這個向前和向後的問題

1057
00:43:19,500 --> 00:43:21,780
好吧，所以我可能需要更多的

1058
00:43:21,780 --> 00:43:23,760
上下文，就像

1059
00:43:23,760 --> 00:43:26,220
嗯，當你說嵌套時 模型，嗯，你的

1060
00:43:26,220 --> 00:43:28,440
意思是層次模型是的，是的，

1061
00:43:28,440 --> 00:43:30,780
所以也許嗯，

1062
00:43:30,780 --> 00:43:34,619


1063
00:43:34,619 --> 00:43:38,760
我快速進行了一次觀察，我對此做了一些

1064
00:43:38,760 --> 00:43:41,160
推斷，但然後我使用這個

1065
00:43:41,160 --> 00:43:45,180
推斷呃來做

1066
00:43:45,180 --> 00:43:48,000
或者也許呃，基本上這個

1067
00:43:48,000 --> 00:43:49,500
推斷 成為

1068
00:43:49,500 --> 00:43:52,800
下一個狀態的觀察，這就是嵌套

1069
00:43:52,800 --> 00:43:55,980
模型的含義，是的，所以我可能必須

1070
00:43:55,980 --> 00:43:58,140
從任務的角度實際考慮這一點，

1071
00:43:58,140 --> 00:44:00,300
然後考慮它，但

1072
00:44:00,300 --> 00:44:02,339
老實說，我還沒有

1073
00:44:02,339 --> 00:44:04,800
想過這在這種情況下如何工作，

1074
00:44:04,800 --> 00:44:08,760
但是 比如說一個任務，

1075
00:44:08,760 --> 00:44:10,319


1076
00:44:10,319 --> 00:44:13,140
嗯，所以在導航方面，如果你

1077
00:44:13,140 --> 00:44:15,119
考慮一下，你可以考慮一下

1078
00:44:15,119 --> 00:44:15,900


1079
00:44:15,900 --> 00:44:19,800
房間環境，這就是

1080
00:44:19,800 --> 00:44:21,480
其中一個例子，我可以

1081
00:44:21,480 --> 00:44:24,000
考慮我們可以在哪裡應用這個，所以想像一下，

1082
00:44:24,000 --> 00:44:27,720
也許你有一個集合 作為

1083
00:44:27,720 --> 00:44:30,300
一個代理人，你必須首先弄清楚

1084
00:44:30,300 --> 00:44:32,460
要去哪個房間，然後你必須

1085
00:44:32,460 --> 00:44:35,760
在那個房子裡導航，基本上

1086
00:44:35,760 --> 00:44:39,060
你可以在兩個階段進行推理或

1087
00:44:39,060 --> 00:44:41,940
在兩個階段做出決策，然後

1088
00:44:41,940 --> 00:44:43,140
你就必須分開 你

1089
00:44:43,140 --> 00:44:45,240
在房間內這些階段的決定

1090
00:44:45,240 --> 00:44:47,460
你可以說動態

1091
00:44:47,460 --> 00:44:49,740
編程來導航你的最佳呃

1092
00:44:49,740 --> 00:44:52,980
路徑，但你總是必須

1093
00:44:52,980 --> 00:44:54,960
有兩個階段的

1094
00:44:54,960 --> 00:44:57,300
嗯決策，嗯

1095
00:44:57,300 --> 00:45:00,420
也許不同的方法在

1096
00:45:00,420 --> 00:45:04,260
不同的階段效果更好，但這將是 呃，

1097
00:45:04,260 --> 00:45:06,180
你知道更好，我的意思是，這個

1098
00:45:06,180 --> 00:45:08,460
討論會更好，嗯，

1099
00:45:08,460 --> 00:45:09,420


1100
00:45:09,420 --> 00:45:12,060
在一個經過

1101
00:45:12,060 --> 00:45:14,400
深思熟慮的任務中更好，我想說，我

1102
00:45:14,400 --> 00:45:18,240
沒有一個可能適合

1103
00:45:18,240 --> 00:45:19,740
所有事情的答案，但是

1104
00:45:19,740 --> 00:45:21,119
是

1105
00:45:21,119 --> 00:45:24,359
的，我們已經看到了幾乎完全一樣的那種

1106
00:45:24,359 --> 00:45:27,300


1107
00:45:27,300 --> 00:45:30,180
在機器人技術案例中，分層同步定位和映射 Slam

1108
00:45:30,180 --> 00:45:32,040
已經有了主動

1109
00:45:32,040 --> 00:45:33,900
推理模型，

1110
00:45:33,900 --> 00:45:37,140
嗯，是的，是否可以

1111
00:45:37,140 --> 00:45:39,500


1112
00:45:40,079 --> 00:45:42,660


1113
00:45:42,660 --> 00:45:45,420
在嵌套模型的一個級別上對其中一種方法進行排序，

1114
00:45:45,420 --> 00:45:48,359
然後將另一種計算方法

1115
00:45:48,359 --> 00:45:50,280
應用於另一個

1116
00:45:50,280 --> 00:45:51,960
嗯 或者就像如果你想要

1117
00:45:51,960 --> 00:45:54,480
一處一處的優勢，比如

1118
00:45:54,480 --> 00:45:56,579
你可以在一次模擬中混合和匹配這些不同的

1119
00:45:56,579 --> 00:46:00,180
方法是的我我我

1120
00:46:00,180 --> 00:46:03,540
絕對認為這是

1121
00:46:03,540 --> 00:46:04,859
可能的

1122
00:46:04,859 --> 00:46:09,140
嗯但是是的我們可能必須嘗試

1123
00:46:09,180 --> 00:46:12,119
所以我所有的方法都是嗯一種 所以

1124
00:46:12,119 --> 00:46:14,819
它不是一個分層模型

1125
00:46:14,819 --> 00:46:17,220
，但我堅信它

1126
00:46:17,220 --> 00:46:19,260
也適用於房地產模型，在這種模型中，

1127
00:46:19,260 --> 00:46:20,520
您可以讓

1128
00:46:20,520 --> 00:46:22,200
兩種決策方法

1129
00:46:22,200 --> 00:46:24,660
一起工作，例如

1130
00:46:24,660 --> 00:46:26,940
我剛才提到的房間示例哦，

1131
00:46:26,940 --> 00:46:29,280
您可以轉到幻燈片嗎 Z

1132
00:46:29,280 --> 00:46:31,819
學習

1133
00:46:35,640 --> 00:46:38,760
很酷，是的，所以我注意到，嗯，在

1134
00:46:38,760 --> 00:46:41,220
這篇偉大的論文中，您總共被引用了好幾次，

1135
00:46:41,220 --> 00:46:45,240
是的，所以

1136
00:46:45,240 --> 00:46:48,060
Z 學習的介紹

1137
00:46:48,060 --> 00:46:49,859
對於主動

1138
00:46:49,859 --> 00:46:52,260
推理領域來說是一個新穎的東西，所以您能再解釋一下

1139
00:46:52,260 --> 00:46:53,760


1140
00:46:53,760 --> 00:46:55,920
什麼嗎？  Z 是

1141
00:46:55,920 --> 00:47:00,660
什麼，是什麼使得

1142
00:47:00,660 --> 00:47:02,280


1143
00:47:02,280 --> 00:47:03,240


1144
00:47:03,240 --> 00:47:06,119
Z 相對於 Q 能夠如此快速地改進 是的，

1145
00:47:06,119 --> 00:47:06,900


1146
00:47:06,900 --> 00:47:10,619
好吧，所以呃，給一些關於

1147
00:47:10,619 --> 00:47:13,079
這篇論文的背景，

1148
00:47:13,079 --> 00:47:13,920
嗯，

1149
00:47:13,920 --> 00:47:16,260
它談論的是

1150
00:47:16,260 --> 00:47:19,800
在特定情況下的線性決策方法。

1151
00:47:19,800 --> 00:47:21,720
mdp 類，因此考慮到

1152
00:47:21,720 --> 00:47:23,940
您正在進行馬爾可夫決策過程，

1153
00:47:23,940 --> 00:47:27,240
其中您的行動可以基於

1154
00:47:27,240 --> 00:47:29,220
狀態而不是重音，因此當您

1155
00:47:29,220 --> 00:47:31,560
考慮例如網格任務中的行動時，

1156
00:47:31,560 --> 00:47:32,760


1157
00:47:32,760 --> 00:47:35,160
您會考慮左右和

1158
00:47:35,160 --> 00:47:37,200
嗯北南右，所以 如果我取

1159
00:47:37,200 --> 00:47:40,200
北，那麼這會對

1160
00:47:40,200 --> 00:47:42,720
狀態空間產生呃後果，但是在本文中，

1161
00:47:42,720 --> 00:47:43,680


1162
00:47:43,680 --> 00:47:46,380
嗯，他們引入了一類 mdp，

1163
00:47:46,380 --> 00:47:48,839
其中決策本身就是狀態，

1164
00:47:48,839 --> 00:47:49,920


1165
00:47:49,920 --> 00:47:54,180
所以如果我說狀態 S1，我的決定

1166
00:47:54,180 --> 00:47:56,280
將是呃

1167
00:47:56,280 --> 00:47:58,740
取決於 取決於另一個狀態，所以我的

1168
00:47:58,740 --> 00:48:00,000
決定將基於

1169
00:48:00,000 --> 00:48:02,160
我下次想要進入的其他狀態，所以

1170
00:48:02,160 --> 00:48:04,020
呃，

1171
00:48:04,020 --> 00:48:05,880
這是一個真正的決策定義，呃，

1172
00:48:05,880 --> 00:48:08,640
根據狀態空間而

1173
00:48:08,640 --> 00:48:10,800
不是

1174
00:48:10,800 --> 00:48:13,619
像左、右、下、上這樣的決策

1175
00:48:13,619 --> 00:48:15,300
因此，鑑於

1176
00:48:15,300 --> 00:48:17,819
存在這樣的 mdp，我可以

1177
00:48:17,819 --> 00:48:21,319
根據狀態做出決策，

1178
00:48:21,319 --> 00:48:23,520
他們已經表明，通過計算，

1179
00:48:23,520 --> 00:48:27,000
您可以以線性複雜度做出呃決策，

1180
00:48:27,000 --> 00:48:30,900
無論問題有多大，

1181
00:48:30,900 --> 00:48:33,480
以便能夠根據

1182
00:48:33,480 --> 00:48:35,339
您應該

1183
00:48:35,339 --> 00:48:38,579
擁有的狀態做出決策 對各州有利和不利的感覺，

1184
00:48:38,579 --> 00:48:39,540


1185
00:48:39,540 --> 00:48:43,619
所以在這個 gridward 示例中，您有

1186
00:48:43,619 --> 00:48:46,440
一個合意性函數，即 C，所以 C

1187
00:48:46,440 --> 00:48:48,240
是合意性函數，

1188
00:48:48,240 --> 00:48:50,700
它討論一個州的合意程度

1189
00:48:50,700 --> 00:48:54,180
，如果我有一個 c 函數，那麼

1190
00:48:54,180 --> 00:48:56,579
它們所顯示的內容 是，

1191
00:48:56,579 --> 00:48:57,359
嗯，

1192
00:48:57,359 --> 00:48:59,280
我可以針對

1193
00:48:59,280 --> 00:49:01,079
這一

1194
00:49:01,079 --> 00:49:03,540
特定類別的 mdp 以線性計算複雜度做出決策，所以如果我的 mdp

1195
00:49:03,540 --> 00:49:05,640
允許我根據狀態做出決策，那麼它

1196
00:49:05,640 --> 00:49:08,940
是線性的，呃，這只是

1197
00:49:08,940 --> 00:49:11,700
該事物的線性複雜度，

1198
00:49:11,700 --> 00:49:15,240
所以這張圖基本上是比較

1199
00:49:15,240 --> 00:49:19,260
你如何 學習意願呃更好

1200
00:49:19,260 --> 00:49:22,200
或更快，所以Q學習如果你

1201
00:49:22,200 --> 00:49:23,880
熟悉Q學習，它基本上是

1202
00:49:23,880 --> 00:49:28,319
一個基於表格的方法呃，你有給定

1203
00:49:28,319 --> 00:49:30,780
一個狀態的行動的意願，

1204
00:49:30,780 --> 00:49:33,000
所以給定一個狀態你知道該怎麼做，

1205
00:49:33,000 --> 00:49:35,760
這基本上是隊列矩陣，但在

1206
00:49:35,760 --> 00:49:37,020
就 C

1207
00:49:37,020 --> 00:49:40,380
uh 或 C 學習方法而言，它只與狀態

1208
00:49:40,380 --> 00:49:43,020
uh 有關，你只是學習

1209
00:49:43,020 --> 00:49:44,819
狀態有多理想，沒有

1210
00:49:44,819 --> 00:49:46,980
動作的概念，

1211
00:49:46,980 --> 00:49:49,560
這正是我們

1212
00:49:49,560 --> 00:49:51,839
在主動推理中的優先偏好 uh，

1213
00:49:51,839 --> 00:49:54,000
它是一種

1214
00:49:54,000 --> 00:49:55,560
量化的分佈 理想或

1215
00:49:55,560 --> 00:49:57,960
不理想的狀態有多

1216
00:49:57,960 --> 00:50:00,119
正確，所以

1217
00:50:00,119 --> 00:50:02,579
鑑於他們已經表明

1218
00:50:02,579 --> 00:50:05,099
你可以更快地學習 C 矩陣，

1219
00:50:05,099 --> 00:50:07,980
這是最佳的，它

1220
00:50:07,980 --> 00:50:10,079
甚至比 Q 學習還要快，那麼我想好吧，為什麼

1221
00:50:10,079 --> 00:50:13,319
不嘗試以與 C

1222
00:50:13,319 --> 00:50:16,400
在這篇論文中被學習，並

1223
00:50:16,400 --> 00:50:19,319
使用這個說法，所以有一個類似的

1224
00:50:19,319 --> 00:50:21,839
學習 C 的學習規則，

1225
00:50:21,839 --> 00:50:24,119
在那篇論文中被稱為集合學習，

1226
00:50:24,119 --> 00:50:25,800
當我嘗試學習時，我

1227
00:50:25,800 --> 00:50:28,200
看到的是它學得非常快，一個

1228
00:50:28,200 --> 00:50:30,540
有用的 Pride 偏好 這讓我可以

1229
00:50:30,540 --> 00:50:32,520
做出決定，或者讓積極的

1230
00:50:32,520 --> 00:50:34,619
影響力代理人做出決定，

1231
00:50:34,619 --> 00:50:38,460
嗯，只需要說一次計劃步驟，是的，

1232
00:50:38,460 --> 00:50:40,859
基本上就是

1233
00:50:40,859 --> 00:50:42,240


1234
00:50:42,240 --> 00:50:43,440


1235
00:50:43,440 --> 00:50:47,460
這個故事，所以嗯，C 很容易學習的想法

1236
00:50:47,460 --> 00:50:50,240
在那篇論文中，

1237
00:50:52,020 --> 00:50:54,780
好吧，讓我嘗試一下重述一下 因為

1238
00:50:54,780 --> 00:50:56,420
我認為這是

1239
00:50:56,420 --> 00:50:59,099
主動推理的一個非常有趣的增強，所以

1240
00:50:59,099 --> 00:51:02,280
我們將學習 C，

1241
00:51:02,280 --> 00:51:04,200
出於

1242
00:51:04,200 --> 00:51:06,480
我們之前討論的所有原因，

1243
00:51:06,480 --> 00:51:09,619
我們將學習 C，

1244
00:51:09,619 --> 00:51:13,740
類似於托多洛夫如何提出 Z

1245
00:51:13,740 --> 00:51:18,540
學習，並且在 Z 學習中而

1246
00:51:18,540 --> 00:51:21,059
不是學習

1247
00:51:21,059 --> 00:51:23,460
嗯，例如更新

1248
00:51:23,460 --> 00:51:26,160
動作的後驗概率，

1249
00:51:26,160 --> 00:51:28,380
然後使用動作在

1250
00:51:28,380 --> 00:51:31,339
發出觀察結果的狀態之間導航是的，

1251
00:51:31,339 --> 00:51:35,760
我們將把動作烘焙

1252
00:51:35,760 --> 00:51:37,319


1253
00:51:37,319 --> 00:51:39,300
到狀態中，

1254
00:51:39,300 --> 00:51:41,420
以便我們真正直接學習

1255
00:51:41,420 --> 00:51:44,280
狀態之間的轉換是的，

1256
00:51:44,280 --> 00:51:48,200


1257
00:51:51,540 --> 00:51:53,880
並將其連接起來 自由

1258
00:51:53,880 --> 00:51:56,040
能原理，以及它是如何

1259
00:51:56,040 --> 00:51:57,660
通過主動

1260
00:51:57,660 --> 00:51:59,220


1261
00:51:59,220 --> 00:52:00,559


1262
00:52:00,559 --> 00:52:03,780
推理髮揮作用的，嗯 C 不僅僅是我們的意願函數，這

1263
00:52:03,780 --> 00:52:05,280
是思考它的一種方式，這就是

1264
00:52:05,280 --> 00:52:07,559
為什麼我們稱之為偏好，而且

1265
00:52:07,559 --> 00:52:11,640
C 是我們的期望，所以這就是

1266
00:52:11,640 --> 00:52:14,400
允許 我們一方面使用

1267
00:52:14,400 --> 00:52:16,800
熟悉的語言來獎勵和

1268
00:52:16,800 --> 00:52:18,960
偏好學習，就像代理

1269
00:52:18,960 --> 00:52:21,900
最終到達牠喜歡的地方一樣，但也

1270
00:52:21,900 --> 00:52:25,500
基於期望的

1271
00:52:25,500 --> 00:52:27,780
um 定義 c 這些是相同的

1272
00:52:27,780 --> 00:52:30,180
事情，允許我們將其作為一條

1273
00:52:30,180 --> 00:52:32,760
最少的路徑來討論 行動或作為最

1274
00:52:32,760 --> 00:52:36,660
可能的結果或最不令人驚訝的

1275
00:52:36,660 --> 00:52:39,900
結果，因為我們已經將其定義為

1276
00:52:39,900 --> 00:52:42,960
我們想要的最不令人驚訝的

1277
00:52:42,960 --> 00:52:47,040
結果，那麼我們可以使用變分自由能

1278
00:52:47,040 --> 00:52:50,640
來反彈驚喜，而你

1279
00:52:50,640 --> 00:52:54,000
不能簡單地使用變分方法來

1280
00:52:54,000 --> 00:52:56,579
限製或 甚至必然近似

1281
00:52:56,579 --> 00:52:58,859
獎勵本身，

1282
00:52:58,859 --> 00:53:01,079
但如果你說

1283
00:53:01,079 --> 00:53:04,980
我更喜歡我的期望，

1284
00:53:04,980 --> 00:53:07,559
而我的期望減少了我的驚喜，

1285
00:53:07,559 --> 00:53:09,859
我將平衡驚喜，

1286
00:53:09,859 --> 00:53:13,520
那麼你就會得到這種行為

1287
00:53:13,520 --> 00:53:17,240
獎勵尋求，

1288
00:53:17,240 --> 00:53:21,420
以驚喜限制驚喜

1289
00:53:21,420 --> 00:53:25,579
最小化物理框架是的，

1290
00:53:27,059 --> 00:53:28,740
是的 這是一種美麗的

1291
00:53:28,740 --> 00:53:31,619
表達方式是的謝謝

1292
00:53:31,619 --> 00:53:35,160
你接下來的

1293
00:53:35,160 --> 00:53:36,720


1294
00:53:36,720 --> 00:53:39,420
步驟或方向是什麼令人興奮或者

1295
00:53:39,420 --> 00:53:41,640
你想以什麼方式完成這項工作是的很好是的

1296
00:53:41,640 --> 00:53:43,140
所以

1297
00:53:43,140 --> 00:53:44,940
嗯如果你只談論這項

1298
00:53:44,940 --> 00:53:46,140
工作

1299
00:53:46,140 --> 00:53:50,640
嗯我想要什麼 接下來要做的是呃

1300
00:53:50,640 --> 00:53:53,099
考慮基於提示的探索任務

1301
00:53:53,099 --> 00:53:55,200
我首先解決

1302
00:53:55,200 --> 00:53:58,980
動態編程的局限性所以如果你

1303
00:53:58,980 --> 00:54:01,020
說一個隊列首先在這個網格中探索

1304
00:54:01,020 --> 00:54:03,660
那是更優化的我想

1305
00:54:03,660 --> 00:54:06,780
看看呃預期的歧義項如何

1306
00:54:06,780 --> 00:54:10,319
預期的自由能是有用的

1307
00:54:10,319 --> 00:54:12,240
，應該嚴格在這個意義上在動態規劃中使用，

1308
00:54:12,240 --> 00:54:13,260


1309
00:54:13,260 --> 00:54:15,540


1310
00:54:15,540 --> 00:54:18,000
嗯，但更一般地說，我也在尋找

1311
00:54:18,000 --> 00:54:20,339
其他積極影響決策的方法，

1312
00:54:20,339 --> 00:54:23,040
就像

1313
00:54:23,040 --> 00:54:24,900


1314
00:54:24,900 --> 00:54:26,099


1315
00:54:26,099 --> 00:54:30,180
哥倫比亞廣播公司夏天教授的作品認為他在講話 關於

1316
00:54:30,180 --> 00:54:33,059
神經網絡是如何進行主動

1317
00:54:33,059 --> 00:54:35,099
推理和基本決策的，從某種意義上說，

1318
00:54:35,099 --> 00:54:36,359
它

1319
00:54:36,359 --> 00:54:39,540
確實更高效，

1320
00:54:39,540 --> 00:54:42,359
就像隊列學習一樣，因此

1321
00:54:42,359 --> 00:54:44,940
他巧妙地利用

1322
00:54:44,940 --> 00:54:47,520
變化自由能來學習良好的

1323
00:54:47,520 --> 00:54:50,760
狀態動作映射，這就是

1324
00:54:50,760 --> 00:54:52,200


1325
00:54:52,200 --> 00:54:55,079
我們習慣

1326
00:54:55,079 --> 00:54:56,520
的預期自由能發生了很大的變化，所以在那項工作中

1327
00:54:56,520 --> 00:54:59,220
沒有預期自由能的概念，

1328
00:54:59,220 --> 00:55:01,740
這都是

1329
00:55:01,740 --> 00:55:03,599
關於直接從能量變化中學習什麼是好什麼是壞，

1330
00:55:03,599 --> 00:55:05,700
所以我發現這

1331
00:55:05,700 --> 00:55:08,099
也是 令人著迷的是，我想

1332
00:55:08,099 --> 00:55:11,160
更多地探索這一點，看看以

1333
00:55:11,160 --> 00:55:15,300
這種方式做出的決策是更好還是更差，

1334
00:55:15,300 --> 00:55:18,000
或者應該考慮我們是否應該

1335
00:55:18,000 --> 00:55:20,220
重新考慮決策方式，

1336
00:55:20,220 --> 00:55:22,200
因為主動推理只討論

1337
00:55:22,200 --> 00:55:23,700
變分自由能，這是

1338
00:55:23,700 --> 00:55:25,520
一切的中心原則 否則是

1339
00:55:25,520 --> 00:55:28,440
你對這一點的解釋，

1340
00:55:28,440 --> 00:55:31,319
所以是的，

1341
00:55:31,319 --> 00:55:35,180
這是另一個方向，我想用

1342
00:55:35,640 --> 00:55:38,359
有趣的方式來表達，這絕對是

1343
00:55:38,359 --> 00:55:42,480
變分自由能，它是

1344
00:55:42,480 --> 00:55:45,540
我們變分

1345
00:55:45,540 --> 00:55:48,660
um分佈q和數據y的函數，

1346
00:55:48,660 --> 00:55:51,119
變分自由能有點像

1347
00:55:51,119 --> 00:55:53,839
實時

1348
00:55:53,839 --> 00:55:57,359
穩態，是的，

1349
00:55:57,359 --> 00:55:59,760
考慮到我所相信的和

1350
00:55:59,760 --> 00:56:01,619
輸入的數據，事情是如何有意義的，

1351
00:56:01,619 --> 00:56:04,740
然後將這種

1352
00:56:04,740 --> 00:56:07,020
有意義的框架擴展到決策

1353
00:56:07,020 --> 00:56:10,020
制定中，我們已經看到了很多不同的

1354
00:56:10,020 --> 00:56:13,740
方法，預期自由能是

1355
00:56:13,740 --> 00:56:16,260
一種常見的方法 但例如，

1356
00:56:16,260 --> 00:56:19,400
存在預期未來的自由能

1357
00:56:19,400 --> 00:56:25,200
eef，還有其他

1358
00:56:25,200 --> 00:56:29,760
具有不同方法的結構，

1359
00:56:29,760 --> 00:56:31,380
然後你也將其指向

1360
00:56:31,380 --> 00:56:33,599
samura 教授的工作，

1361
00:56:33,599 --> 00:56:37,020
即

1362
00:56:37,020 --> 00:56:39,059


1363
00:56:39,059 --> 00:56:41,339
基本圖上的變分自由能與

1364
00:56:41,339 --> 00:56:43,680
神經網絡中的損失函數以及

1365
00:56:43,680 --> 00:56:45,300
所有這些關係也是非常

1366
00:56:45,300 --> 00:56:48,380
令人興奮的工作，

1367
00:56:49,079 --> 00:56:51,660
嗯我想就像

1368
00:56:51,660 --> 00:56:53,460
最後一個問題一樣結束，或者認為

1369
00:56:53,460 --> 00:56:55,559
你即將完成

1370
00:56:55,559 --> 00:56:56,880
博士學位，

1371
00:56:56,880 --> 00:57:00,540
所以就在你已經完成的時候 作為一名

1372
00:57:00,540 --> 00:57:02,579
博士生，

1373
00:57:02,579 --> 00:57:06,300
您如何看待主動推理的

1374
00:57:06,300 --> 00:57:10,700
發展，或者

1375
00:57:11,339 --> 00:57:14,280
今天接近

1376
00:57:14,280 --> 00:57:17,400
尾聲時您的感覺與

1377
00:57:17,400 --> 00:57:19,980
幾年前您的新鮮感和興奮感有何不同，是的，

1378
00:57:19,980 --> 00:57:21,119


1379
00:57:21,119 --> 00:57:23,339
這是一個非常好的

1380
00:57:23,339 --> 00:57:25,920
問題，呃，我也很興奮

1381
00:57:25,920 --> 00:57:29,400
這個領域已經發展了，坦率地說，

1382
00:57:29,400 --> 00:57:30,900
我從強化

1383
00:57:30,900 --> 00:57:32,400
學習背景和物理

1384
00:57:32,400 --> 00:57:34,680
背景開始，當我開始閱讀時，

1385
00:57:34,680 --> 00:57:37,680
只說了一兩篇論文，我

1386
00:57:37,680 --> 00:57:40,440
不太理解其中的大部分內容，

1387
00:57:40,440 --> 00:57:42,540
直到我開始

1388
00:57:42,540 --> 00:57:44,700
使用它來實現它 調用

1389
00:57:44,700 --> 00:57:46,980
um Matlab 腳本 我有點理解，

1390
00:57:46,980 --> 00:57:49,559
好吧，這是有道理的，我喜歡它，

1391
00:57:49,559 --> 00:57:52,020
但是在一兩年內，我看到

1392
00:57:52,020 --> 00:57:54,780
很多來自

1393
00:57:54,780 --> 00:57:56,280
不同方向的論文，人們也

1394
00:57:56,280 --> 00:57:58,740
開始使用神經網絡和所有

1395
00:57:58,740 --> 00:58:02,460
這些擴展 我來了，我在某個

1396
00:58:02,460 --> 00:58:04,319
時候也質疑主動推理的必要性，

1397
00:58:04,319 --> 00:58:07,260
呃，因為如果你

1398
00:58:07,260 --> 00:58:09,119
有深度強化學習，它

1399
00:58:09,119 --> 00:58:10,980
可以做很多事情，那麼為什麼需要深度主動

1400
00:58:10,980 --> 00:58:14,040
推理，這就是為什麼我

1401
00:58:14,040 --> 00:58:16,319
沒有進入這一領域的原因，但我仍然覺得

1402
00:58:16,319 --> 00:58:18,359
它很有趣 我想比我現在所知道的

1403
00:58:18,359 --> 00:58:20,280
更多地理解深度主動推理，

1404
00:58:20,280 --> 00:58:23,160
但我看到這個

1405
00:58:23,160 --> 00:58:25,680
領域在兩三年內像任何事情一樣發展，

1406
00:58:25,680 --> 00:58:28,619
許多人

1407
00:58:28,619 --> 00:58:31,859
開始工作，很快它就成為一個

1408
00:58:31,859 --> 00:58:36,359
認真對待的領域 比一個只有

1409
00:58:36,359 --> 00:58:39,119
兩篇論文的領域沒有人真正

1410
00:58:39,119 --> 00:58:41,280
知道它是什麼所以這真的很

1411
00:58:41,280 --> 00:58:43,440
令人興奮是的所以我真的很期待

1412
00:58:43,440 --> 00:58:47,040
這個領域如何隨著時間的推移而發展以及

1413
00:58:47,040 --> 00:58:52,140
我在獲得博士學位後可以做什麼等等

1414
00:58:52,140 --> 00:58:55,319
在時間上向前冷卻

1415
00:58:55,319 --> 00:59:00,359
什麼 我們更喜歡我們所期望的 是的

1416
00:59:02,339 --> 00:59:04,140
任何其他

1417
00:59:04,140 --> 00:59:05,819
評論或您想添加的任何其他內容

1418
00:59:05,819 --> 00:59:07,200


1419
00:59:07,200 --> 00:59:09,839
是的，所以請讓我知道您

1420
00:59:09,839 --> 00:59:11,940
對這篇論文的看法以及您

1421
00:59:11,940 --> 00:59:14,400
對這些想法的看法 請隨時告訴我，

1422
00:59:14,400 --> 00:59:16,200
我真的很期待

1423
00:59:16,200 --> 00:59:17,760
反饋，

1424
00:59:17,760 --> 00:59:20,160
是的 非常感謝丹尼爾給我這個機會，

1425
00:59:20,160 --> 00:59:23,339
謝謝你的寶貴時間，

1426
00:59:23,339 --> 00:59:26,160
這太棒了，我希望人們

1427
00:59:26,160 --> 00:59:28,260
查看這篇論文，並聯繫並

1428
00:59:28,260 --> 00:59:30,540
複制代碼，並按照

1429
00:59:30,540 --> 00:59:33,359
自己的指示進行操作，謝謝

1430
00:59:33,359 --> 00:59:34,680
下次再見，

1431
00:59:34,680 --> 00:59:36,839
下次再見 非常感謝你，再見，祝你

1432
00:59:36,839 --> 00:59:40,040
有美好的一天，再見


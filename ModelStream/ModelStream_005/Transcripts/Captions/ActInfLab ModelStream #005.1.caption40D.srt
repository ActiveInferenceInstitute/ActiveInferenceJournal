1
00:00:08,041 --> 00:00:08,942
Okay.

2
00:00:09,142 --> 00:00:10,910
Hello and welcome everyone.

3
00:00:10,910 --> 00:00:12,712
It is January 28th.

4
00:00:12,712 --> 00:00:14,881
20, 22.

5
00:00:14,881 --> 00:00:18,952
We're here in Acton
Flat Model Stream number 5.1

6
00:00:19,352 --> 00:00:22,322
with Pietro Marseglia and Tim Verboten.

7
00:00:22,756 --> 00:00:26,059
So this is going to be a model stream
presentation

8
00:00:26,326 --> 00:00:30,830
and discussion on their recent work
Contrastive Active Inference.

9
00:00:31,297 --> 00:00:35,468
We're going to have a presentation
section and then discussion.

10
00:00:35,935 --> 00:00:38,605
So please feel free to ask any questions

11
00:00:38,605 --> 00:00:41,441
during the presentation
that we can address in the discussion.

12
00:00:41,875 --> 00:00:44,411
And Tim and Pietro, thanks a ton.

13
00:00:44,411 --> 00:00:47,414
We really appreciate you
joining to share your work.

14
00:00:47,414 --> 00:00:49,049
So please take it away.

15
00:00:49,049 --> 00:00:51,551
And thanks again.

16
00:00:52,452 --> 00:00:55,488
Yeah, thanks, Daniel,
for inviting us as well.

17
00:00:55,522 --> 00:01:00,794
So I'm going to be able to get the better
we are going to work on.

18
00:01:01,261 --> 00:01:03,563
Interesting of interest.

19
00:01:03,563 --> 00:01:05,732
So let me first

20
00:01:06,166 --> 00:01:06,933
set the scene.

21
00:01:06,933 --> 00:01:09,969
Why are we looking at active inference

22
00:01:10,737 --> 00:01:13,740
or basically our lab
wants to build convergence agents?

23
00:01:14,207 --> 00:01:17,010
And so from that perspective, the studio

24
00:01:17,043 --> 00:01:20,346
and if you want to build something
indulgent, it needs to be involved.

25
00:01:20,413 --> 00:01:23,750
It needs to be interacting
with its environments.

26
00:01:24,317 --> 00:01:27,787
And then most of those

27
00:01:27,787 --> 00:01:31,257
interactive
instruments are basically your agents to

28
00:01:32,192 --> 00:01:32,959
your environment.

29
00:01:32,959 --> 00:01:34,294
It's interacting with.

30
00:01:34,294 --> 00:01:37,363
And you need to build the model,
basically.

31
00:01:37,831 --> 00:01:40,500
So are first

32
00:01:40,800 --> 00:01:44,471
give an overview of active inference
and the way that the three are

33
00:01:45,872 --> 00:01:49,776
and a little
this material is also being covered

34
00:01:50,276 --> 00:01:53,213
in a previous middle stream,
I think number three.

35
00:01:53,580 --> 00:01:56,916
So if you want more details,
you can think of that one again.

36
00:01:58,084 --> 00:02:00,620
And so then afterwards

37
00:02:00,620 --> 00:02:04,090
gets over and he will go to the nitty

38
00:02:04,257 --> 00:02:08,361
gritty details
of the contrasting approach of inference.

39
00:02:08,862 --> 00:02:10,964
So let's get started.

40
00:02:11,131 --> 00:02:12,932
So basically active

41
00:02:12,932 --> 00:02:15,869
inference,
it's a process theory of the brain.

42
00:02:16,503 --> 00:02:19,706
And basically it says that your brain
or your agents

43
00:02:20,807 --> 00:02:24,244
build or
you build this model of the environment,

44
00:02:24,544 --> 00:02:29,349
which is basically the joins probability
distribution over that observation.

45
00:02:29,516 --> 00:02:32,385
So things that you can see or experience

46
00:02:33,520 --> 00:02:35,755
actions which we do know the state

47
00:02:35,755 --> 00:02:37,891
and then states or

48
00:02:40,994 --> 00:02:42,662
states of the universe.

49
00:02:42,662 --> 00:02:46,266
So basically you have your agents
that separate from the environment

50
00:02:46,933 --> 00:02:50,136
and its actions interact
with the environment.

51
00:02:50,537 --> 00:02:53,273
And this gives rise to new observations.

52
00:02:53,740 --> 00:02:57,810
And so the idea of the ball
is agents for out,

53
00:02:58,378 --> 00:03:00,513
which are kind of the hidden states.

54
00:03:01,080 --> 00:03:04,651
That's the change by actions

55
00:03:05,018 --> 00:03:07,820
and the different ways
to my observations.

56
00:03:08,154 --> 00:03:12,091
And if you can build such a model,
then basically this enables

57
00:03:12,091 --> 00:03:16,963
the agents to plan some actions
to to bring the agent

58
00:03:16,963 --> 00:03:20,800
to some preferred observations
or outcomes and so forth.

59
00:03:20,900 --> 00:03:23,937
But so the crucial bits is basically

60
00:03:23,937 --> 00:03:27,674
how do you get this model

61
00:03:27,674 --> 00:03:30,710
of what happens if I do my actions?

62
00:03:31,044 --> 00:03:35,281
How does this influence the states
and how does this influence the

63
00:03:35,715 --> 00:03:36,716
outcomes that I see?

64
00:03:38,284 --> 00:03:40,787
So the crucial for from experience
is twofold.

65
00:03:40,920 --> 00:03:46,025
First of all, the key is what the agent
does in a decision optimizing so-called

66
00:03:46,025 --> 00:03:49,729
free energy, which is a profound
surprise, a prediction error.

67
00:03:49,729 --> 00:03:51,397
So basically,

68
00:03:51,931 --> 00:03:56,102
if Joe, this model allows the agent
to predict the outcomes,

69
00:03:56,102 --> 00:03:58,805
that it will see that true witness and

70
00:03:59,839 --> 00:04:03,076
the better
this match your actual observations,

71
00:04:03,076 --> 00:04:05,411
the more happier you are as an agent.

72
00:04:07,146 --> 00:04:09,782
And crucially,
you will also select the actions

73
00:04:09,782 --> 00:04:13,653
that will minimize the frequency
you expect in the future.

74
00:04:13,987 --> 00:04:16,489
And so we think of this to the models

75
00:04:18,424 --> 00:04:20,593
just to to set the scene.

76
00:04:20,593 --> 00:04:23,196
On the one hand, the reason
why so that we all know for

77
00:04:23,229 --> 00:04:26,799
the system is for this also to then

78
00:04:27,567 --> 00:04:32,338
see the move the jet will make
from the let's see value

79
00:04:32,972 --> 00:04:37,043
act of influence presentation
thoughts more countries

80
00:04:37,043 --> 00:04:41,114
the formulation of the of
interest free energy object

81
00:04:42,982 --> 00:04:44,984
So we start off with

82
00:04:44,984 --> 00:04:47,053
setting to see what the genesis model so

83
00:04:47,487 --> 00:04:52,225
it's it's a bit
laid out the diagram of the agents

84
00:04:52,225 --> 00:04:54,360
and the environments
that was on the previous slide.

85
00:04:54,394 --> 00:04:57,063
So basically this unfolds over time.

86
00:04:57,397 --> 00:05:01,601
So you are in certain states
that gives rise to certain observation

87
00:05:02,068 --> 00:05:05,305
and then given an action
on your previous state and basically move

88
00:05:05,305 --> 00:05:07,974
ahead to the next stage

89
00:05:08,641 --> 00:05:11,344
and this process unfolds over time.

90
00:05:12,245 --> 00:05:14,714
And you can see some of the

91
00:05:16,215 --> 00:05:17,950
circles are colored gray.

92
00:05:17,950 --> 00:05:20,053
And these are basically
the things that you can observe.

93
00:05:20,653 --> 00:05:24,257
So, you know, the actions,
if you did that until now,

94
00:05:24,791 --> 00:05:28,828
as you know,
the observations that you saw until now

95
00:05:29,495 --> 00:05:32,065
and all the rest
is basically for you to infer.

96
00:05:32,432 --> 00:05:35,301
So you can only infer it in states
until now,

97
00:05:35,868 --> 00:05:39,706
but you can also try to infer
the hidden states of the future.

98
00:05:40,173 --> 00:05:44,410
Actions that you want to take or
d'observation that you will experience.

99
00:05:45,078 --> 00:05:46,079
And so basically that

100
00:05:47,814 --> 00:05:48,781
the so-called junk

101
00:05:48,781 --> 00:05:53,019
model joined distribution
over the sequence of observation states.

102
00:05:53,019 --> 00:05:55,988
And action is then
basically characterized as follows.

103
00:05:55,988 --> 00:05:58,424
So you have propriety over actions

104
00:06:00,159 --> 00:06:03,196
you have a license model. So

105
00:06:03,196 --> 00:06:04,864
so yeah. So your prior overreaction.

106
00:06:04,864 --> 00:06:07,266
So this basically versus
what is appropriate

107
00:06:07,266 --> 00:06:12,939
if you take certain actions or time
you have some transition probabilities.

108
00:06:12,939 --> 00:06:17,210
So what is the probability
that I will transition to this next stage

109
00:06:17,210 --> 00:06:22,148
given my previous state and this and
you have the accurate model, which basis?

110
00:06:22,815 --> 00:06:26,185
Yeah, given the state I am

111
00:06:26,352 --> 00:06:28,621
which observation, I will see.

112
00:06:28,621 --> 00:06:32,892
And so it is basically the versus
the so-called Markov assumption.

113
00:06:33,292 --> 00:06:36,729
That's your observation that you see
at this time

114
00:06:36,829 --> 00:06:39,632
that it only depends on your hidden state

115
00:06:39,766 --> 00:06:43,202
and it does not really directly
depend on anything else

116
00:06:44,003 --> 00:06:46,606
because if you know your
if your your current in state, then

117
00:06:46,606 --> 00:06:48,875
you know your observation, you will see

118
00:06:49,642 --> 00:06:52,612
so that basically what is reflected here.

119
00:06:53,880 --> 00:06:56,783
But of course, as an agent

120
00:06:56,783 --> 00:07:01,020
having this model
this allows you to still to to assess

121
00:07:01,020 --> 00:07:05,758
how likely is a sequence of observations,
for example, and allows you to predict

122
00:07:06,259 --> 00:07:09,629
given these actions
what will happen with one crucial

123
00:07:09,629 --> 00:07:12,398
but of course is still
the inverse of this model.

124
00:07:13,433 --> 00:07:15,601
Given that I saw these observations

125
00:07:15,601 --> 00:07:18,704
that are these actions,
which is my current state,

126
00:07:19,906 --> 00:07:23,142
and this is basically non-trivial.

127
00:07:23,776 --> 00:07:26,746
So even if you have the exact model

128
00:07:26,979 --> 00:07:29,849
that you refer to as
this is typically intractable.

129
00:07:30,750 --> 00:07:32,718
So that's why it makes of inference.

130
00:07:32,718 --> 00:07:36,322
You resort to variation inference
that you just say, okay,

131
00:07:36,856 --> 00:07:39,258
I just assume that I can build a model,

132
00:07:39,826 --> 00:07:41,861
the so-called for,
for example, your model.

133
00:07:42,428 --> 00:07:44,931
And this is the thing that will tell you,

134
00:07:44,931 --> 00:07:49,101
given certain observations,
what is my probability to begin a sort of

135
00:07:50,570 --> 00:07:51,938
this what's depicted here?

136
00:07:51,938 --> 00:07:53,473
So you do this.

137
00:07:53,473 --> 00:07:59,278
Q And Q is basically a variation
of your distribution contours

138
00:08:00,513 --> 00:08:02,582
and you just say, okay,

139
00:08:03,082 --> 00:08:04,650
given some observation

140
00:08:04,650 --> 00:08:08,421
I want to have the best estimates
for the state I am in.

141
00:08:09,489 --> 00:08:11,524
And the reality principle

142
00:08:11,524 --> 00:08:16,963
just sits, if that's what you want,
then this is and you just optimize

143
00:08:17,263 --> 00:08:21,467
the free energy, which is no pressure
and basically expectation over

144
00:08:22,368 --> 00:08:24,971
states and generated by your

145
00:08:26,105 --> 00:08:30,276
your expectation over to the difference
between the what an accurate

146
00:08:30,309 --> 00:08:33,913
or your approximate posture
and look like use false the drive model

147
00:08:34,413 --> 00:08:38,251
and if you can minimize deaths,
that basically means that you will have

148
00:08:40,052 --> 00:08:42,922
the best explanation for any observations
you see.

149
00:08:43,322 --> 00:08:46,692
But at the same time, you also have
the best approximate posterior

150
00:08:46,692 --> 00:08:49,762
for the true and you're not going to go

151
00:08:49,762 --> 00:08:54,166
to the whole derivation of this flow
but basically

152
00:08:54,800 --> 00:08:57,904
you can convert this
through to the second regression line.

153
00:08:57,904 --> 00:09:01,007
And this is the one that we use
most often

154
00:09:01,941 --> 00:09:04,577
in our models
which is basically the virgin between

155
00:09:05,778 --> 00:09:08,581
this and the approximate posterior.

156
00:09:08,915 --> 00:09:11,817
So basically what is the state
I am given your observations

157
00:09:11,817 --> 00:09:14,787
that so and

158
00:09:15,655 --> 00:09:18,925
described as basically just this is my,

159
00:09:19,091 --> 00:09:23,529
my guess that I am in the States
given the previous state elections.

160
00:09:23,529 --> 00:09:28,234
So I don't know the observation
but I want to test this

161
00:09:29,669 --> 00:09:30,870
without my observation.

162
00:09:30,870 --> 00:09:35,608
And if I see your observation, I don't
want my beliefs to completely switch

163
00:09:36,375 --> 00:09:38,978
because the probability
there's something wrong with my model.

164
00:09:39,645 --> 00:09:41,781
And then you have the second term,
which is the,

165
00:09:42,782 --> 00:09:46,886
the looks like this
basically the accuracy of your model

166
00:09:47,153 --> 00:09:50,823
or how good are you at
reconstructing the outcomes.

167
00:09:51,857 --> 00:09:52,792
And so

168
00:09:55,261 --> 00:09:57,930
this is all form to false

169
00:09:57,930 --> 00:10:02,201
basically or control your current events
that so you know, your observations

170
00:10:02,201 --> 00:10:06,238
do so now you can validate the energy

171
00:10:06,572 --> 00:10:10,009
and then you can be
a predictor model in order to

172
00:10:11,544 --> 00:10:12,812
minimize this thing.

173
00:10:12,812 --> 00:10:14,747
But of course
you also need to know your actions.

174
00:10:14,747 --> 00:10:16,682
You want to get to the future.

175
00:10:16,682 --> 00:10:21,454
And so if you look into the future,
then talk about the expected energy.

176
00:10:22,188 --> 00:10:25,891
So here we didn't we
we use by the shorthand

177
00:10:25,891 --> 00:10:31,130
for the sequence of actions
into the future that also switched from D

178
00:10:31,163 --> 00:10:35,468
to just to denote that we are talking
about future type steps basically.

179
00:10:35,468 --> 00:10:40,539
But the same with the important business
now that in the expectation

180
00:10:41,273 --> 00:10:44,644
now you don't have expectation
it's only over states

181
00:10:45,144 --> 00:10:49,248
with also over outcomes
because you couldn't sense

182
00:10:50,449 --> 00:10:51,751
your observations yet.

183
00:10:51,751 --> 00:10:54,754
So you don't know them so you can just

184
00:10:54,754 --> 00:10:57,456
do an expectation over
anything that could happen.

185
00:10:58,024 --> 00:11:00,926
And, and then the move

186
00:11:00,926 --> 00:11:05,464
that is made in
the conference is basically that's

187
00:11:07,333 --> 00:11:09,502
the on hand for the term

188
00:11:09,702 --> 00:11:15,408
which they call the instrumental value
or realizing preferences.

189
00:11:15,808 --> 00:11:20,713
And it's basically stating
that as an agents for future outcomes

190
00:11:21,681 --> 00:11:26,385
they have some prior
they think that they will regardless

191
00:11:26,485 --> 00:11:30,356
what happens that I think I will realize
it's kind of your your preferred

192
00:11:30,589 --> 00:11:34,493
outcomes let's say can also cross
this more like homeostasis.

193
00:11:34,493 --> 00:11:36,662
So my body temperature can be

194
00:11:37,730 --> 00:11:39,198
37 degrees Celsius.

195
00:11:39,198 --> 00:11:43,502
So my expectation before
knowing anything is that it will be

196
00:11:44,503 --> 00:11:48,674
37 degrees and hence
I will act in order to make it so.

197
00:11:48,674 --> 00:11:53,446
So that's a bit reflexive here
that disregards the

198
00:11:54,547 --> 00:11:56,849
the dependency on your reaction
that you just say,

199
00:11:56,849 --> 00:12:00,219
okay, my prayer
is that this is what I expected.

200
00:12:01,287 --> 00:12:03,322
So this becomes the instrumental value.

201
00:12:04,023 --> 00:12:06,092
And then the second,
the solution is basically that

202
00:12:06,492 --> 00:12:10,262
your approximate
just your model is basically very,

203
00:12:10,663 --> 00:12:13,899
very close to the studio
so that you have a good approximation.

204
00:12:13,899 --> 00:12:18,270
Then you can reside in the second term,

205
00:12:18,838 --> 00:12:23,142
which basically means
that you have on the one hand and

206
00:12:24,376 --> 00:12:26,779
the third that's says,

207
00:12:26,779 --> 00:12:28,981
this is my belief or the state

208
00:12:29,381 --> 00:12:32,785
given injections I will do.

209
00:12:33,219 --> 00:12:37,022
And the other thing
is also a belief about future states

210
00:12:37,490 --> 00:12:41,694
have an actions and some certain outcomes
that I expect to see.

211
00:12:42,061 --> 00:12:47,099
So basically it says
what would be the information that I gets

212
00:12:47,666 --> 00:12:51,403
from looking for certain outcomes.

213
00:12:51,971 --> 00:12:54,707
And it's it's kind of in a systemic value
or information

214
00:12:54,707 --> 00:12:57,910
gain which can drive you to explore.

215
00:12:57,910 --> 00:13:01,013
Basically,
you want to have if, if you don't know

216
00:13:01,013 --> 00:13:05,885
how to get to your preferred state,
at least you want to get to state as to

217
00:13:06,685 --> 00:13:09,688
states that give you more information
on where you are

218
00:13:10,055 --> 00:13:14,827
and go, first of all, often says
it's like the owl that needs fruits.

219
00:13:15,194 --> 00:13:18,597
So what do you do with the first order
you search for pre first.

220
00:13:18,597 --> 00:13:22,735
And so the epistemic value
is basically searching for prey.

221
00:13:22,735 --> 00:13:25,404
It's like where should I go to do

222
00:13:25,738 --> 00:13:30,176
to get more information on this
and then once you know where this

223
00:13:30,543 --> 00:13:35,281
then you can realize your preferences
and the thought is that

224
00:13:35,581 --> 00:13:36,715
there was a pretty risk

225
00:13:39,251 --> 00:13:43,155
so how does the action selection works
then?

226
00:13:43,622 --> 00:13:44,456
Well, basically

227
00:13:44,456 --> 00:13:47,760
you want to select the actions
that minimized your expected strategy.

228
00:13:47,760 --> 00:13:52,832
So at each times, the first thing you do
is you use your model to

229
00:13:53,632 --> 00:13:54,934
estimate your return.

230
00:13:54,934 --> 00:14:00,272
States like knowing which state
may now given my latest observation,

231
00:14:01,140 --> 00:14:04,543
then you can evaluate
the expected free energy for

232
00:14:05,344 --> 00:14:08,347
for each of also your plans

233
00:14:08,514 --> 00:14:12,852
for a future sequence of actions
you can evaluate

234
00:14:13,319 --> 00:14:16,088
and expect to be energy.

235
00:14:16,088 --> 00:14:19,892
And then this basically results
in a belief over policy.

236
00:14:19,892 --> 00:14:22,895
So you basically take the the mind,

237
00:14:23,262 --> 00:14:26,298
the negative energy, you're multiplied.

238
00:14:26,298 --> 00:14:30,302
It's this precision parameter
which just states how yeah.

239
00:14:31,503 --> 00:14:33,372
How how much confidence you have.

240
00:14:33,372 --> 00:14:35,875
The energy is correct basically.

241
00:14:36,208 --> 00:14:39,812
And then you use this
the suffix so basically it just

242
00:14:40,913 --> 00:14:42,581
it just says

243
00:14:43,382 --> 00:14:44,917
the the

244
00:14:44,917 --> 00:14:48,420
policies that have low expected
tree energy are the ones that are

245
00:14:48,420 --> 00:14:51,891
most likely basically that's that's
the only thing is from the source

246
00:14:52,358 --> 00:14:55,494
and then referred in this section,
according to this, you just select

247
00:14:55,861 --> 00:15:00,599
the next section for the sequence
that you think will really give you the

248
00:15:01,467 --> 00:15:04,603
the minimum energy
and that side of course.

249
00:15:04,603 --> 00:15:08,173
And then you take this action,
you get a new observation and the process

250
00:15:08,674 --> 00:15:13,746
is and so one crucial point
in all our work

251
00:15:13,746 --> 00:15:18,884
is that it all starts with this
just following is a flexible and

252
00:15:20,019 --> 00:15:21,287
typically

253
00:15:22,087 --> 00:15:26,158
you can and you have a certain problem
and you know how the problem

254
00:15:26,926 --> 00:15:30,396
looks like, what the observations are or
what the hidden state might be.

255
00:15:30,696 --> 00:15:35,167
And then you can really pinpoint
and write down the exact model

256
00:15:35,367 --> 00:15:37,002
and so optimizing.

257
00:15:37,002 --> 00:15:39,538
But in our case,
this is often the true issue.

258
00:15:39,538 --> 00:15:40,973
If you look at the robots

259
00:15:40,973 --> 00:15:44,343
that drives around
and gets them right inputs, for example.

260
00:15:44,710 --> 00:15:46,879
Yeah, well, these are
the states things that you need to

261
00:15:49,081 --> 00:15:49,815
track.

262
00:15:49,815 --> 00:15:53,485
How do you
convert these pixels to the state space?

263
00:15:53,752 --> 00:15:55,888
So all these things are there.

264
00:15:55,888 --> 00:15:56,822
They're just not there.

265
00:15:56,822 --> 00:16:00,592
You have the goal in a work
is, yeah, I can

266
00:16:01,226 --> 00:16:05,364
then completely start from scratch
and learn this and for listening

267
00:16:05,397 --> 00:16:09,768
the use of neural networks
to access, function, approximate

268
00:16:09,802 --> 00:16:12,471
or actually

269
00:16:12,838 --> 00:16:15,975
provide the smoothness, these models
optimize

270
00:16:15,975 --> 00:16:20,813
the parameters of these neural nets
also by minimizing the energy.

271
00:16:21,080 --> 00:16:23,682
That's that's great. That's it.

272
00:16:23,682 --> 00:16:25,951
So how does it look like all this?

273
00:16:26,819 --> 00:16:28,153
Not will.

274
00:16:28,153 --> 00:16:31,023
So we start off with observations

275
00:16:31,023 --> 00:16:34,126
and actions so this can be but

276
00:16:34,126 --> 00:16:38,864
pixels basically so
and and by matrix of numbers

277
00:16:38,864 --> 00:16:42,001
let's see actions
which could be any action

278
00:16:42,234 --> 00:16:45,738
could your whole city or whatever
your agent can do

279
00:16:46,839 --> 00:16:49,942
and these numbers are put into a neural
net which we told you in order

280
00:16:50,376 --> 00:16:53,345
and this basically reflects
this Brooks's posterior.

281
00:16:53,345 --> 00:16:59,818
This is just saying given my previous
action and my observation outputs

282
00:17:01,220 --> 00:17:02,688
probabilistic presentation

283
00:17:02,688 --> 00:17:07,292
which is basically the means
and the variances of the very patients.

284
00:17:08,160 --> 00:17:11,730
And then we have a second rule
that's rich called transition model.

285
00:17:12,097 --> 00:17:13,599
And this is then

286
00:17:16,668 --> 00:17:18,404
so yeah, well,

287
00:17:18,404 --> 00:17:21,774
what will happen if I do a certain action
out of my state evolves.

288
00:17:22,207 --> 00:17:26,812
If I say to a certain action Now
finally, we also have the decoder

289
00:17:26,812 --> 00:17:31,350
or the electric model, the then outputs
given the State's observation.

290
00:17:31,350 --> 00:17:35,287
So in case of an image, for example,
this will generate you a new image.

291
00:17:36,221 --> 00:17:38,791
And the goal is of course to

292
00:17:39,992 --> 00:17:42,628
have the best predictions possible.

293
00:17:42,928 --> 00:17:47,066
So if you look at the energy forms,
again, in this case

294
00:17:47,833 --> 00:17:50,969
it's again you have this like in the form
which basically just says

295
00:17:51,437 --> 00:17:54,206
that during the output of the decoder.

296
00:17:54,206 --> 00:17:56,075
So to generate image,

297
00:17:56,075 --> 00:17:59,611
I just want to have this close
to the actual image that you've been.

298
00:18:00,279 --> 00:18:05,517
See, so it's just a reconstruction
lost in terms of neural nets, let's say.

299
00:18:06,051 --> 00:18:07,019
And the second to

300
00:18:08,253 --> 00:18:10,823
sort thinking through on a scale
killer verges

301
00:18:11,123 --> 00:18:14,827
between the distribution
that you generate from the encoder

302
00:18:15,661 --> 00:18:18,163
and the distribution
that you generate from the transition.

303
00:18:18,163 --> 00:18:19,598
Well, it's

304
00:18:25,170 --> 00:18:27,439
so we apply this on a number of cases,

305
00:18:28,107 --> 00:18:30,776
which also

306
00:18:30,776 --> 00:18:32,678
we're seeing
in the previous model stream.

307
00:18:32,678 --> 00:18:36,048
So just to give you some intuitions,
the first thing was

308
00:18:36,715 --> 00:18:40,018
the multiple problems,
just which is the basic control problem.

309
00:18:40,018 --> 00:18:40,953
So here

310
00:18:41,587 --> 00:18:43,989
the sensory input,
that's the position you are

311
00:18:45,591 --> 00:18:48,393
you basically have to infer
not only the position

312
00:18:48,393 --> 00:18:50,996
you're in, but also the momentum
you have the most you have.

313
00:18:51,396 --> 00:18:53,866
And so you can see that

314
00:18:53,999 --> 00:18:56,702
on the right,
you can see the model predicting

315
00:18:56,702 --> 00:19:00,506
all likely trajectories
for going left or right.

316
00:19:00,906 --> 00:19:04,143
And you can see how in the beginning it's
not sure on the course.

317
00:19:04,276 --> 00:19:08,547
So it's it's very spread out
and and what it will predicts.

318
00:19:08,881 --> 00:19:11,783
But the more information that gets,
the more kind of collapses.

319
00:19:11,917 --> 00:19:14,820
Yeah, I'm pretty sure that
this is the behavior this will happen.

320
00:19:14,820 --> 00:19:18,390
And then you can use this tool
to drive the agents towards

321
00:19:19,391 --> 00:19:20,392
the preferred states.

322
00:19:20,392 --> 00:19:22,494
In this case, the, the flag

323
00:19:23,462 --> 00:19:25,831
the second one was using

324
00:19:25,831 --> 00:19:28,967
the quarries or environments or here
you get these

325
00:19:30,235 --> 00:19:34,473
observations that are now
just pixels from from this game

326
00:19:35,040 --> 00:19:38,477
and the preferred state of the car
will still be in the central metric.

327
00:19:39,211 --> 00:19:42,247
And so you can see how it actually infers
the actions that will

328
00:19:42,548 --> 00:19:45,050
bring it to the central district.

329
00:19:46,385 --> 00:19:49,321
And it might even cut corners
in order to the reach

330
00:19:49,321 --> 00:19:53,058
of preferred states
a bit faster And finally,

331
00:19:53,058 --> 00:19:57,129
we also have this fun on the robots
navigating our lab

332
00:19:58,330 --> 00:20:01,567
where encrypted
with the number of sensor mobility

333
00:20:01,567 --> 00:20:05,137
so you can see the camera,
but also from tracing

334
00:20:05,137 --> 00:20:08,207
lighter and also very dark range Doppler.

335
00:20:08,207 --> 00:20:10,909
So the radar range Doppler
basically gives you

336
00:20:11,276 --> 00:20:13,879
in the way access the range

337
00:20:14,379 --> 00:20:16,548
and in the x axis,

338
00:20:16,882 --> 00:20:20,619
the the velocity of the reflections,
basically the Doppler

339
00:20:21,186 --> 00:20:23,455
and here
you can see how in the beginning,

340
00:20:23,455 --> 00:20:27,893
if it has been a number of situations
and then basically that the model imagine

341
00:20:27,893 --> 00:20:28,927
what would happen.

342
00:20:28,927 --> 00:20:32,764
So these are real observations
and now it basically imagines

343
00:20:33,065 --> 00:20:37,102
what it will see if it turns around,
for example, and just yet actually learns

344
00:20:37,302 --> 00:20:40,072
basic dynamics, basic behavior

345
00:20:40,939 --> 00:20:43,008
of of all the sensor model.

346
00:20:43,141 --> 00:20:43,976
So that's pretty cool

347
00:20:47,079 --> 00:20:49,648
So what
are the limitations of this thing?

348
00:20:50,482 --> 00:20:53,485
Well, there are two core limitations
that we address

349
00:20:53,952 --> 00:20:55,988
in the first one is

350
00:20:56,488 --> 00:20:59,358
that we use this this six

351
00:20:59,391 --> 00:21:03,061
X reconstruction both to learn the model,

352
00:21:03,395 --> 00:21:06,798
but also to define
your preferred status like

353
00:21:07,165 --> 00:21:10,202
this is the image that you want to see
and try to make that happen.

354
00:21:10,869 --> 00:21:13,438
But the problem is that this great error
in pixel

355
00:21:15,407 --> 00:21:18,810
in terms of pixels
is not really the best metric.

356
00:21:19,044 --> 00:21:22,381
So, for example,
if you have the the left image

357
00:21:23,582 --> 00:21:25,951
and you look to assist output

358
00:21:25,951 --> 00:21:28,420
an image is similar to that one.

359
00:21:29,288 --> 00:21:31,290
We have two examples you on the right

360
00:21:31,290 --> 00:21:34,893
and you can see that's the same image
with some salt and pepper

361
00:21:34,893 --> 00:21:39,097
and those is actually scoring worse
in terms of squared error

362
00:21:39,431 --> 00:21:42,301
than an image where the two

363
00:21:42,534 --> 00:21:45,370
the joint arm is actually

364
00:21:46,438 --> 00:21:47,372
incorrect.

365
00:21:47,372 --> 00:21:52,377
So although in terms of behavior,
the left one is better

366
00:21:52,778 --> 00:21:56,181
in terms of great error
the right one is better.

367
00:21:56,181 --> 00:21:57,983
So that's of course problematic.

368
00:21:57,983 --> 00:22:00,752
If you want to control the arm,
the words to go

369
00:22:01,486 --> 00:22:05,324
and on seconds limitations
that if you need to evaluate

370
00:22:05,324 --> 00:22:09,528
the expected free energy for the huge
number of potential trajectories,

371
00:22:09,528 --> 00:22:11,263
potential actions you can do,

372
00:22:11,263 --> 00:22:14,599
then of course this becomes intractable
as the number increases.

373
00:22:15,200 --> 00:22:19,204
And so the the ways that we go
through this in the context of work

374
00:22:19,204 --> 00:22:23,241
is on the one hand, instead of
using a pixel wasting constriction

375
00:22:23,241 --> 00:22:27,245
or just compressive learning,
it sets out exactly

376
00:22:27,245 --> 00:22:30,682
this works with performance
and in the next few sites.

377
00:22:31,316 --> 00:22:33,685
And then the second thing is instead of

378
00:22:33,685 --> 00:22:37,622
evaluating
the expected the energy for all this,

379
00:22:38,590 --> 00:22:40,826
you basically
amortize the forces actions.

380
00:22:41,059 --> 00:22:45,630
So we also train neural nets outputs,
actions, different stages.

381
00:22:46,365 --> 00:22:47,733
And so with that,

382
00:22:48,266 --> 00:22:50,535
we can now shift to get through.

383
00:22:50,569 --> 00:22:54,806
Google talked about the contrastive
formulation of the expected

384
00:22:54,806 --> 00:22:56,241
benefits from.

385
00:23:02,381 --> 00:23:03,648
Thank you, Tim.

386
00:23:05,517 --> 00:23:06,251
All right.

387
00:23:06,251 --> 00:23:08,320
So thank you, Daniel, for having us.

388
00:23:08,320 --> 00:23:10,756
And thank you, Tim.

389
00:23:10,756 --> 00:23:13,558
You know, if you can hear me well,

390
00:23:15,761 --> 00:23:19,097
soon on

391
00:23:19,097 --> 00:23:20,832
my screen out

392
00:23:24,569 --> 00:23:26,004
Okay.

393
00:23:26,838 --> 00:23:27,539
All right.

394
00:23:27,572 --> 00:23:33,478
So now I would think about
our recent work contrasted in France.

395
00:23:34,312 --> 00:23:37,549
So this work was recently published
that noted

396
00:23:37,883 --> 00:23:40,051
that those in 31

397
00:23:40,786 --> 00:23:42,954
very recently

398
00:23:43,755 --> 00:23:45,223
nice months.

399
00:23:45,223 --> 00:23:47,359
Let's start delving into it.

400
00:23:48,560 --> 00:23:50,662
So the

401
00:23:50,762 --> 00:23:52,798
dissecting that we discuss

402
00:23:53,131 --> 00:23:56,568
in inductive inference is very similar
to the reinforcement learning.

403
00:23:56,568 --> 00:23:59,971
One with the difference
that in reinforcement learning,

404
00:24:00,806 --> 00:24:05,277
the old behavior
learning is driven by rewards.

405
00:24:05,577 --> 00:24:08,914
So the agent receive reward function

406
00:24:09,448 --> 00:24:12,083
and the positive rewards

407
00:24:12,250 --> 00:24:15,954
should reinforce positive behaviors
while negative.

408
00:24:15,954 --> 00:24:18,390
The rewards should penalize

409
00:24:18,990 --> 00:24:21,927
the agents
to avoid those states and actions.

410
00:24:22,661 --> 00:24:25,764
However, one of the problems that comes
with reinforcement learning is that

411
00:24:26,431 --> 00:24:30,202
in order to actually learn from rewards,
you need a reward function.

412
00:24:30,302 --> 00:24:34,539
And that's not always easy to have
For instance, as you mentioned,

413
00:24:35,040 --> 00:24:37,742
especially when the state
is not known in advance,

414
00:24:37,742 --> 00:24:41,112
so the agent don't exactly know
its status is difficult.

415
00:24:41,713 --> 00:24:42,380
In that case,

416
00:24:42,380 --> 00:24:46,384
the design, the reward function, because
you're not sure of what the agent knows

417
00:24:46,384 --> 00:24:49,921
and how it can assess its performance
compared to the environment.

418
00:24:51,590 --> 00:24:53,291
So we

419
00:24:53,291 --> 00:24:57,596
instead focus on active inference
in the inference the agents operates

420
00:24:57,596 --> 00:25:01,366
to that the principle of minimizing
free energy as we have

421
00:25:01,399 --> 00:25:05,370
just so that the principle

422
00:25:05,370 --> 00:25:08,707
of minimizing free energy
actually enables two things.

423
00:25:08,773 --> 00:25:11,776
The one thing is to learn a model.

424
00:25:11,877 --> 00:25:16,781
The work We call this an artificial work
model where work seems simple.

425
00:25:17,382 --> 00:25:20,352
And the other objective is to minimize

426
00:25:20,352 --> 00:25:22,787
the free energy future
by trying to achieve

427
00:25:23,889 --> 00:25:26,024
some preferred outcomes of the agent.

428
00:25:26,057 --> 00:25:30,161
So we assume that the agent
does some preferred outcome distribution

429
00:25:30,161 --> 00:25:33,465
that he wants to achieve
and that this goal will be in the future

430
00:25:33,465 --> 00:25:35,734
to actually achieve
these preferred outcomes.

431
00:25:37,969 --> 00:25:39,271
So the

432
00:25:39,271 --> 00:25:43,041
the environmental setting we discuss
is that one of the options

433
00:25:43,041 --> 00:25:45,710
piece of partially observable
market is in process.

434
00:25:46,444 --> 00:25:48,947
So just to recap the observation

435
00:25:49,915 --> 00:25:51,550
that the agent

436
00:25:51,550 --> 00:25:55,520
receives, just infer the internal state
of the environment which is not served.

437
00:25:55,854 --> 00:25:59,724
And then there's action
which are actually known for the past,

438
00:25:59,724 --> 00:26:04,195
but the agent should infer
or somehow choose

439
00:26:04,195 --> 00:26:07,132
among a set of possible action
in the future.

440
00:26:08,700 --> 00:26:10,635
So this is just a summary

441
00:26:10,635 --> 00:26:13,738
of what
the artificial word module looks like,

442
00:26:13,772 --> 00:26:17,342
or as we've seen in the previous lights,
we have an encoder that

443
00:26:18,176 --> 00:26:22,113
the information from observation
we focus on visual environments.

444
00:26:22,113 --> 00:26:26,918
So here we have again an image
which is basically owned by metrics,

445
00:26:26,985 --> 00:26:30,255
or the encoder could be, for instance,
a convolutional neural network

446
00:26:31,022 --> 00:26:34,826
In our case,
we have the the even state model

447
00:26:34,960 --> 00:26:38,396
with which takes the previous state
and the previous action

448
00:26:38,930 --> 00:26:40,732
and in particular no work.

449
00:26:40,732 --> 00:26:44,202
You use some some form of recurrent
neural network model

450
00:26:44,202 --> 00:26:47,839
in order to keep preserving
the history of the environment.

451
00:26:48,340 --> 00:26:50,842
And then we have the decoder that

452
00:26:50,842 --> 00:26:54,512
computes reconstruction
of the information on the current state.

453
00:26:54,512 --> 00:26:55,680
So it tries to encode

454
00:26:56,648 --> 00:26:57,482
inside that

455
00:26:57,482 --> 00:27:00,685
in the States as much information
not possible

456
00:27:01,453 --> 00:27:03,688
from what it comes
from the from the observation.

457
00:27:04,990 --> 00:27:06,791
So the problem with

458
00:27:06,958 --> 00:27:09,961
reconstruction is that computing then
especially in visual

459
00:27:09,961 --> 00:27:13,732
environments is is quite complicated
because you need a

460
00:27:15,133 --> 00:27:18,770
big models that there are very good
representational capacity.

461
00:27:18,770 --> 00:27:22,574
And also the models cannot be 100%
accurate.

462
00:27:22,574 --> 00:27:28,079
I think that low low dimensional settings
because for instance, predicting

463
00:27:28,680 --> 00:27:32,550
an image pixelbypixel
is practically very invisible.

464
00:27:32,550 --> 00:27:34,686
So it's very rarely happens.

465
00:27:35,387 --> 00:27:39,124
So let's call an example
a year or so, a few a few weeks ago

466
00:27:39,124 --> 00:27:43,428
I was a I was training to be like models.

467
00:27:43,428 --> 00:27:46,598
So like a model similar
to this one on the left.

468
00:27:46,598 --> 00:27:50,969
So where we have the this encoder
decoder architecture on that.

469
00:27:51,236 --> 00:27:53,972
Well, and I'm thinking the breakout game

470
00:27:54,506 --> 00:27:57,809
and try to to learn and even state to

471
00:28:00,211 --> 00:28:02,547
to learn action on top of the interstate.

472
00:28:02,614 --> 00:28:04,783
The problem is that

473
00:28:04,783 --> 00:28:09,087
the reconstruction of the V
so we're actually pretty bad

474
00:28:09,087 --> 00:28:12,323
in that they we're losing very important
information about the game.

475
00:28:12,323 --> 00:28:17,762
So for instance it was kind of able
so we some uncertainty to model

476
00:28:17,762 --> 00:28:20,832
where the model of the game is
but it wasn't able

477
00:28:21,533 --> 00:28:25,470
to model where the ball easy
which is actually one of the two

478
00:28:25,470 --> 00:28:29,574
most important details
in order to actually be able to play.

479
00:28:29,641 --> 00:28:34,012
So even adding the reward function
in this case so having the the

480
00:28:35,046 --> 00:28:38,817
the game score available
and the agent wasn't able

481
00:28:38,817 --> 00:28:43,021
to learn the task because of of the state
which was lacking

482
00:28:43,021 --> 00:28:46,925
the most important information
in order to to keep improving.

483
00:28:47,559 --> 00:28:51,696
So this is one issue
that we try to overcome in our work.

484
00:28:52,397 --> 00:28:55,500
And the second part of active interest

485
00:28:55,500 --> 00:28:58,970
involves
learning to pursue the preferred outcome.

486
00:28:59,671 --> 00:29:03,108
So in order to pursue
preferred outcomes active in agent,

487
00:29:03,742 --> 00:29:07,045
the two things, one tries to minimize a

488
00:29:08,880 --> 00:29:11,616
the distance
with respect to two disparate outcomes.

489
00:29:11,616 --> 00:29:12,417
But then the other way

490
00:29:12,417 --> 00:29:15,353
you also minimize the ambiguity
with respect to the environment

491
00:29:16,921 --> 00:29:19,924
So normally this is done
by trying to match the,

492
00:29:20,492 --> 00:29:23,695
the two distributions
or trying to match, as we saw,

493
00:29:23,695 --> 00:29:29,134
even if we've scaled divergence
to match the distribution of the match,

494
00:29:29,234 --> 00:29:32,737
the outcome with the preferred
outcomes, distribution, whatever.

495
00:29:32,737 --> 00:29:36,641
Again, in a dimensional setting,
this can be quite complex

496
00:29:36,641 --> 00:29:40,678
because how do you define a distribution
in an aid nation or here much

497
00:29:41,379 --> 00:29:45,817
could it be, for instance, just
the center go around the, the big self.

498
00:29:45,817 --> 00:29:47,318
So we've the

499
00:29:47,485 --> 00:29:51,256
with the mean being the pixel value
and then some fixed extend the deviation.

500
00:29:51,256 --> 00:29:56,094
But in that case we we get into troubles
because we have the same issue

501
00:29:56,127 --> 00:29:59,898
discussed before we for instance
have this kind of goal here

502
00:30:00,231 --> 00:30:03,835
and a noisy observation like this
which actually adds an eye.

503
00:30:03,835 --> 00:30:05,537
Another means square error

504
00:30:05,537 --> 00:30:08,673
compared to to an image
that is very distant from the goal.

505
00:30:09,274 --> 00:30:12,811
And this kind of situation,
especially when using reconstruction

506
00:30:12,811 --> 00:30:17,015
or in more realistic settings, are very,
very likely because for instance, you

507
00:30:17,015 --> 00:30:20,852
can you can see that the center image
is actually

508
00:30:20,852 --> 00:30:23,988
just a reconstruction of the model,
which is not under percent accurate.

509
00:30:23,988 --> 00:30:25,356
So that could be the case.

510
00:30:25,356 --> 00:30:29,794
And indeed, the agent could be confused
and you will think that is not achieving

511
00:30:29,794 --> 00:30:33,331
the goal compared to maybe,
for instance, a bus observation.

512
00:30:33,331 --> 00:30:36,167
But if it seemed that
it was actually actually closer

513
00:30:36,167 --> 00:30:39,971
to the goal or again, when there is some
some noise in the environment

514
00:30:39,971 --> 00:30:43,675
in real set up like the robotic,
we always have

515
00:30:43,675 --> 00:30:46,511
this noise in the observation
so it's our to

516
00:30:47,345 --> 00:30:51,649
to match our preferred outcome
in, you know, even national setting.

517
00:30:51,716 --> 00:30:54,853
So we we also try to
to overcome this issue here

518
00:30:55,687 --> 00:31:01,226
So what we do propose is to use contrast
learning a conservative learning is a

519
00:31:02,360 --> 00:31:05,530
is a mechanism
popular in the in unsupervised

520
00:31:05,530 --> 00:31:09,267
learning scene that we will discuss
more in depth in a few moments.

521
00:31:09,300 --> 00:31:13,638
So the the the objective that we want to
where we want where

522
00:31:13,805 --> 00:31:17,709
there are to avoid reconstruction
in learning the word model.

523
00:31:17,709 --> 00:31:21,312
So we don't have any more of the decoder
here as we see on the right.

524
00:31:22,046 --> 00:31:25,283
Then we want to be able to match
preferred outcomes in the lower

525
00:31:25,617 --> 00:31:28,786
dimensionality space
because we have seen that it's

526
00:31:29,120 --> 00:31:31,122
no dimensionality that's problematic.

527
00:31:31,322 --> 00:31:36,160
And also we we would like the
this low dimensional state to be somewhat

528
00:31:36,160 --> 00:31:40,365
representative of the task
so that when we match our goals

529
00:31:40,365 --> 00:31:45,003
in this low dimensional state,
we are actually doing something

530
00:31:45,003 --> 00:31:48,840
that actually brings us closer
to the actual preferred outcome that we

531
00:31:49,040 --> 00:31:51,209
we want to achieve
in the high dimensional setting.

532
00:31:53,211 --> 00:31:56,915
So let's let's try to compare

533
00:31:58,082 --> 00:32:00,919
to see what are the difference in between
using the likelihoods

534
00:32:01,319 --> 00:32:03,621
to the inference model.
And the contrasting models.

535
00:32:03,621 --> 00:32:06,824
So the idea in the likelihood
actually inference model

536
00:32:06,824 --> 00:32:10,495
is that we want to maximize
the accuracy of reconstruction.

537
00:32:10,495 --> 00:32:14,766
So basically this this means that we have
this decoder that maximizes this

538
00:32:16,134 --> 00:32:20,371
maximum likelihood of the observation
given the state.

539
00:32:20,638 --> 00:32:23,341
So we want the state to to

540
00:32:24,142 --> 00:32:28,513
maximize the information
that it contains about the observation.

541
00:32:28,513 --> 00:32:31,416
Basically, in contrast, learning

542
00:32:31,816 --> 00:32:35,753
we in contrast, deductive inference,
we do something different.

543
00:32:35,753 --> 00:32:40,558
So instead of trying to reconstruct
the current observation,

544
00:32:40,959 --> 00:32:44,062
we try to compress

545
00:32:44,262 --> 00:32:47,432
the encoder again, this observation

546
00:32:47,899 --> 00:32:50,168
and compare it to all of the other

547
00:32:51,869 --> 00:32:55,206
not all the other, as we will see
in a while, because that's visible,

548
00:32:55,206 --> 00:33:00,244
but in many, many other samples
that represent something different.

549
00:33:00,345 --> 00:33:03,815
So that we in the, in the late in space,
in this compressed space,

550
00:33:04,582 --> 00:33:07,819
we want our a state

551
00:33:07,819 --> 00:33:10,788
and the compressed
image to be very close.

552
00:33:11,322 --> 00:33:17,061
And while this our state should be very
distant from all the other hemorrhages.

553
00:33:17,061 --> 00:33:20,865
So we
we are indeed maximizing the similarity

554
00:33:20,865 --> 00:33:21,833
with this

555
00:33:21,833 --> 00:33:25,336
and with the corresponding sample that
this is here called the positive sample

556
00:33:25,336 --> 00:33:27,739
where we want to minimize
the similarities

557
00:33:27,739 --> 00:33:31,042
and maximize the distance
against all the other samples

558
00:33:31,042 --> 00:33:34,846
that are called negative samples
in contrasting learning.

559
00:33:34,879 --> 00:33:37,882
So as we'll see also in in a moment, this

560
00:33:38,249 --> 00:33:42,353
this mechanism here maximizes the lower
bound of the mutual information.

561
00:33:42,420 --> 00:33:46,257
So we are basically trying to maximize
the information in between

562
00:33:46,557 --> 00:33:50,061
corresponding observation in state while

563
00:33:50,561 --> 00:33:54,532
minimizing the information with respect
to the all the other negative spurts

564
00:33:56,667 --> 00:33:59,070
so as we seen

565
00:33:59,070 --> 00:34:04,275
before, the the free energy bus can be
summarized with this equation here.

566
00:34:04,275 --> 00:34:06,677
So here I'm just

567
00:34:07,178 --> 00:34:11,349
talking about one time,
one moment in this for ten steps.

568
00:34:11,382 --> 00:34:15,453
So instead the team presented
for all sequences by using the field

569
00:34:15,453 --> 00:34:15,987
the notation.

570
00:34:15,987 --> 00:34:20,291
Well, here we're just considering
one time steps the time.

571
00:34:20,525 --> 00:34:24,062
So as we as we have seen

572
00:34:24,162 --> 00:34:26,497
the free energy is basically are

573
00:34:27,532 --> 00:34:30,668
an upper bound
on the appraisal information

574
00:34:30,668 --> 00:34:32,637
that we would want to minimize.

575
00:34:32,637 --> 00:34:35,840
So we minimize free energy in order

576
00:34:35,840 --> 00:34:38,910
to minimize this appraisal of the agent.

577
00:34:39,177 --> 00:34:44,148
And the year is it's actually evident
that we have this evidence on.

578
00:34:44,148 --> 00:34:47,385
So the 80 scale diverged into these old
where there are equal to zero.

579
00:34:47,385 --> 00:34:53,257
We wanted to basically to to reach zero
hypothetically in order to minimize these

580
00:34:54,292 --> 00:34:56,427
this evidence bound.

581
00:34:56,427 --> 00:34:58,796
And this can also be repeated

582
00:34:59,597 --> 00:35:02,266
in a way
that is more practical implemented.

583
00:35:02,266 --> 00:35:07,338
So by having the the likelihood
of the observation given the state

584
00:35:07,338 --> 00:35:10,741
which actually means
the accuracy of our model

585
00:35:11,075 --> 00:35:13,578
and that again
the complexity of the model

586
00:35:13,578 --> 00:35:18,683
which is the close divergence between
our variation variational distribution.

587
00:35:18,683 --> 00:35:22,620
Q Which we, we use.

588
00:35:22,620 --> 00:35:26,324
Q Oh, that's given all
which is basically using the outgoing

589
00:35:26,324 --> 00:35:29,460
coding variation of relational

590
00:35:30,495 --> 00:35:31,329
posterior.

591
00:35:31,329 --> 00:35:33,498
So this is

592
00:35:33,564 --> 00:35:37,668
as typical
as a stunning in variational encoders.

593
00:35:37,668 --> 00:35:40,938
So when you
when you try to infer the parameters

594
00:35:40,938 --> 00:35:44,976
of your posterior distribution
by using the corresponding observation,

595
00:35:45,710 --> 00:35:50,381
and then we we want to minimize the close
divergence between this out coded

596
00:35:50,915 --> 00:35:53,618
posterior and the our prior

597
00:35:54,485 --> 00:35:58,623
about the about the, the,
the current or future states

598
00:35:58,623 --> 00:36:01,592
we can say given the boss
state and actions

599
00:36:02,126 --> 00:36:07,064
and the in our case in particular
we generally learn this this frame.

600
00:36:07,064 --> 00:36:08,432
So we don't just use

601
00:36:09,467 --> 00:36:10,368
a uniform

602
00:36:10,368 --> 00:36:14,739
prior overstate but
we will learn our years to be predictive

603
00:36:15,473 --> 00:36:18,242
of what the state is given the bus state.

604
00:36:18,276 --> 00:36:22,446
So it can basically seem like
if we're machine learning in particular

605
00:36:22,446 --> 00:36:25,683
as a, as a conditional variation
of encoder,

606
00:36:28,052 --> 00:36:31,222
we've, we've contrastive learning
what we try to do

607
00:36:31,222 --> 00:36:34,559
is as I said, to
maximize state similarity

608
00:36:35,226 --> 00:36:38,930
with the correct and corresponding
observations over

609
00:36:39,163 --> 00:36:41,499
could sample
while minimizing with the other

610
00:36:41,999 --> 00:36:44,769
and this means again

611
00:36:44,769 --> 00:36:47,972
that we want to maximize
the neutral information

612
00:36:47,972 --> 00:36:51,842
between the positive sample
and the corresponding state and minimize

613
00:36:52,577 --> 00:36:55,112
the information
with the negative samples.

614
00:36:55,112 --> 00:36:57,882
This can be written like this or

615
00:36:59,083 --> 00:37:02,687
the noise contrastive estimation.

616
00:37:02,687 --> 00:37:03,888
So noisy here.

617
00:37:03,888 --> 00:37:06,157
That's the abbreviation

618
00:37:07,391 --> 00:37:09,927
basically provides a again,

619
00:37:10,328 --> 00:37:13,497
a lower bound
and the neutral information.

620
00:37:14,332 --> 00:37:18,369
So where we where we see
that we basically have like a soft marks.

621
00:37:18,369 --> 00:37:21,405
So over the over all the the

622
00:37:22,807 --> 00:37:24,809
the observation
is state what does it mean?

623
00:37:24,809 --> 00:37:28,613
So for each pair of state
and observation,

624
00:37:29,347 --> 00:37:31,716
we run this volume.

625
00:37:31,716 --> 00:37:35,119
So the value of this critique
is that this critical function F

626
00:37:35,553 --> 00:37:39,223
to be as high as possible
or we want it to be

627
00:37:39,490 --> 00:37:44,195
a very low respect to the other
so that actually the exponential of these

628
00:37:44,762 --> 00:37:48,833
a compared to the sum
of all the other exponential is either

629
00:37:48,833 --> 00:37:52,236
with the corresponding dissipation
and very low with the rest.

630
00:37:52,803 --> 00:37:57,308
So this is basically saying that the
if you want matching bears

631
00:37:57,341 --> 00:38:00,278
to be to be very close
and distant bassoon

632
00:38:00,645 --> 00:38:03,314
to a very, very low value.

633
00:38:03,314 --> 00:38:08,786
And the the this is lower

634
00:38:08,786 --> 00:38:14,191
bound is a is an approximation
normally when we take a number of samples

635
00:38:14,191 --> 00:38:18,095
key from that
a joined distribution that we define

636
00:38:19,130 --> 00:38:20,264
between X

637
00:38:20,264 --> 00:38:23,868
and Y in particular in our case is X
and Y

638
00:38:23,868 --> 00:38:28,005
represent our observation
and our in the States.

639
00:38:29,040 --> 00:38:31,309
So we define a priori these

640
00:38:32,877 --> 00:38:35,313
disjoint distribution
to actually represent

641
00:38:35,613 --> 00:38:39,517
the fact that these state corresponds
to this observation

642
00:38:39,517 --> 00:38:42,353
and we want to maximize the information
in between them

643
00:38:43,220 --> 00:38:46,090
and the function F, x,

644
00:38:46,090 --> 00:38:49,660
y again
is so so-called critique function.

645
00:38:50,094 --> 00:38:54,432
So what does it mean is a function
that should approximate the log density

646
00:38:54,432 --> 00:38:58,736
ratio that we see here on the right
I won't go into the mathematical

647
00:38:58,736 --> 00:39:02,973
details of these,
but basically is a mapping of the

648
00:39:04,041 --> 00:39:06,444
of the the two inputs so basically

649
00:39:06,477 --> 00:39:09,714
the mapping of our observation
and our state

650
00:39:11,015 --> 00:39:14,518
and we want these again
these outputs to be I

651
00:39:14,819 --> 00:39:19,156
for corresponding bursts and the and load
for non corresponding bursts.

652
00:39:20,658 --> 00:39:22,560
So how do we

653
00:39:22,560 --> 00:39:25,296
transition
from the free energy of the path

654
00:39:26,697 --> 00:39:29,700
that we have seen to the
to our contrastive formulation

655
00:39:31,235 --> 00:39:34,805
still what the the first step that we do

656
00:39:35,039 --> 00:39:40,711
is adding to the to the free energy
functional as a term that we

657
00:39:40,978 --> 00:39:45,483
we assumed to be constant the despair,
the entropy of the observation.

658
00:39:45,783 --> 00:39:49,253
So how can we assume that the entropy
of the observation is constant?

659
00:39:49,887 --> 00:39:53,257
So in machine learning,
we generally have a data set

660
00:39:53,424 --> 00:39:56,660
from which we sampled
our observation about the past.

661
00:39:57,061 --> 00:40:00,931
So we assume when we train
that our data set

662
00:40:00,931 --> 00:40:04,068
so that the distribution
of the observation is fixed

663
00:40:05,169 --> 00:40:08,472
so the entropy of this distribution

664
00:40:08,472 --> 00:40:11,709
will always be a constant
because we can modify that.

665
00:40:13,210 --> 00:40:15,513
I suppose for instance, to the States
which

666
00:40:15,780 --> 00:40:19,350
which means that so the distribution
of our outcomes cannot be modified.

667
00:40:19,350 --> 00:40:21,452
And so its entropy is a constant.

668
00:40:21,452 --> 00:40:24,622
If we add disturbance
to the free energy functional,

669
00:40:25,322 --> 00:40:28,559
we can rewrite it
as a, as the coiled divergence

670
00:40:28,893 --> 00:40:31,429
minus this information

671
00:40:31,595 --> 00:40:36,367
gain or mutual information here
between the states and the observation.

672
00:40:37,768 --> 00:40:39,837
So given this, we can now

673
00:40:40,271 --> 00:40:43,874
apply the fact that we in

674
00:40:44,608 --> 00:40:46,877
contrast, keep learning

675
00:40:47,044 --> 00:40:52,149
a functional this is a lower
bound on the mutual information

676
00:40:52,283 --> 00:40:54,618
to actually derive the
the free energy of the past

677
00:40:55,686 --> 00:40:58,456
where expecting all the terms result

678
00:40:59,123 --> 00:41:01,892
according to the previous light nets.

679
00:41:02,493 --> 00:41:05,830
We basically have, again,
this coiled divergence there.

680
00:41:05,996 --> 00:41:07,731
And then we have

681
00:41:07,898 --> 00:41:12,870
this the value of F between all

682
00:41:12,870 --> 00:41:16,740
and this to maximize
and then to minimize all the

683
00:41:18,576 --> 00:41:21,111
again the

684
00:41:21,879 --> 00:41:24,114
the value of the function of with respect

685
00:41:24,114 --> 00:41:27,885
to all the other negative spurts

686
00:41:28,118 --> 00:41:31,355
So this brings those two

687
00:41:32,056 --> 00:41:35,526
to again
an upward bound on this appraisal term.

688
00:41:35,526 --> 00:41:40,998
We see that this upward bone
is actually even higher

689
00:41:40,998 --> 00:41:45,703
than the normal free energy,
upper bound bus as we'll see later.

690
00:41:45,703 --> 00:41:48,305
These are some nice property. Does it

691
00:41:49,807 --> 00:41:51,942
explicitly help us to get

692
00:41:52,009 --> 00:41:55,446
rid of the reconstruction and to learn
a different representational space?

693
00:41:55,446 --> 00:41:59,683
It does some advantages compared to the
to the likelihood

694
00:42:00,184 --> 00:42:02,286
based representation

695
00:42:04,355 --> 00:42:07,958
So let's now talk about how can we learn

696
00:42:08,993 --> 00:42:12,096
to behave using constructive inference.

697
00:42:12,129 --> 00:42:13,697
So in the likelihood based active

698
00:42:13,697 --> 00:42:17,401
inference model, what we were trying to
maximize was again,

699
00:42:17,468 --> 00:42:22,406
the likelihood of the future observation,
but under the preferred distribution.

700
00:42:22,740 --> 00:42:24,542
So we want to

701
00:42:25,109 --> 00:42:28,078
we want the imagined outcomes
to be as close

702
00:42:28,078 --> 00:42:31,615
as possible
to the two that outcomes that we prefer

703
00:42:32,883 --> 00:42:37,021
a So these for
these are environment in place that we

704
00:42:37,588 --> 00:42:40,891
we reconstruct
what the what we imagine will happen

705
00:42:40,891 --> 00:42:43,961
in the dimensional space.

706
00:42:43,961 --> 00:42:45,329
So that emerge basically.

707
00:42:45,329 --> 00:42:49,800
And then we compare it
to our preferred image, for instance.

708
00:42:50,401 --> 00:42:53,037
And as we see
before we can for instance, use

709
00:42:53,037 --> 00:42:57,241
a mean squared error distance
or we can just use like

710
00:42:58,208 --> 00:42:58,976
we can just go

711
00:42:58,976 --> 00:43:02,813
down and compute the likelihood
under the preferred the distribution

712
00:43:04,048 --> 00:43:05,516
or contested up to the inference.

713
00:43:05,516 --> 00:43:09,420
We again instead
use a contrasting mechanism when we want

714
00:43:09,453 --> 00:43:14,458
now the the future state
to be corresponding with our

715
00:43:15,593 --> 00:43:17,461
samples from the preferred distribution.

716
00:43:17,461 --> 00:43:18,963
So we want the,

717
00:43:18,963 --> 00:43:23,300
the outcomes that we prefer
to actually be a close to there

718
00:43:23,300 --> 00:43:27,538
to the state that we imagined so that now
we don't need anymore to reconstruct

719
00:43:27,938 --> 00:43:31,375
what the what
the outcome of our action will look like.

720
00:43:31,742 --> 00:43:34,378
But we can just say is the,

721
00:43:34,411 --> 00:43:36,914
the state that we match in matching with

722
00:43:38,182 --> 00:43:40,351
the preferred outcome
that they want to achieve.

723
00:43:40,751 --> 00:43:45,222
And the that's, that's what we,
we maximize similarity with.

724
00:43:45,389 --> 00:43:47,791
And again, we also have some

725
00:43:47,791 --> 00:43:51,562
some form of the ambiguity minimization
or epistemic value

726
00:43:51,562 --> 00:43:56,600
in trying to minimize
the similarity respect to other outcomes.

727
00:43:56,634 --> 00:43:58,869
So in this case, minimizing

728
00:43:59,403 --> 00:44:02,806
the similarity in respect to outcomes
that are not in the preferred

729
00:44:02,806 --> 00:44:05,943
distribution
basically means that the other want to go

730
00:44:05,943 --> 00:44:09,413
far from something
that you have already seen before

731
00:44:10,414 --> 00:44:11,382
in order to maybe

732
00:44:11,382 --> 00:44:14,184
get closer to the
the preferred the outcomes,

733
00:44:14,618 --> 00:44:18,022
or you either
just want to minimize your ambiguity.

734
00:44:18,022 --> 00:44:21,892
So you want to be as far as possible
from other outcomes in the schools

735
00:44:21,892 --> 00:44:26,463
as possible through that to the actual
preferred outcome view

736
00:44:27,431 --> 00:44:29,600
that you're opting for.

737
00:44:29,867 --> 00:44:31,835
So as we have seen

738
00:44:31,835 --> 00:44:35,305
before, the expected free energy
can be summarized like this.

739
00:44:35,305 --> 00:44:37,675
I'll first highlight
some difference in respect to

740
00:44:38,542 --> 00:44:41,545
the equation the team present that.

741
00:44:41,545 --> 00:44:45,149
So first of all year
we we take action to be part

742
00:44:45,282 --> 00:44:48,185
of the of their active entrance process.

743
00:44:48,185 --> 00:44:49,687
So they aim for the process.

744
00:44:49,687 --> 00:44:52,690
Well,
and before we have seen that, you can

745
00:44:53,223 --> 00:44:56,260
you can have a distribution
of our policies and then you can

746
00:44:56,527 --> 00:44:59,930
you can sample the action
from the policy and

747
00:45:00,130 --> 00:45:03,867
and compute
the free energy of the future a given

748
00:45:04,535 --> 00:45:07,204
a posterior on your

749
00:45:08,839 --> 00:45:10,674
on a policy when I give them policy

750
00:45:10,674 --> 00:45:14,078
instead here
we make the the actions part of the

751
00:45:14,078 --> 00:45:18,882
the generative models for the future
and we actually want the agent to insert

752
00:45:19,083 --> 00:45:23,053
the injection for the future
and not just compute them as a

753
00:45:23,620 --> 00:45:27,791
as a posterior of our over some
distribution of the over the policies.

754
00:45:28,459 --> 00:45:33,097
So we we have enough in the posterior
is this 18

755
00:45:33,831 --> 00:45:35,899
so we that we inferred both the

756
00:45:36,400 --> 00:45:39,002
the the future state
and the future election

757
00:45:39,503 --> 00:45:43,741
and also the the prior
other over the preferred outcomes

758
00:45:43,741 --> 00:45:47,077
that indicates we feel that
I have this not confusing before

759
00:45:47,077 --> 00:45:51,081
because before the deal
that was used to indicate sequences

760
00:45:51,081 --> 00:45:55,152
by in the paper I actually use it to
to indicate the preferred outcome

761
00:45:55,252 --> 00:45:59,156
so yes notation issues
but I hope there's not confusing.

762
00:45:59,156 --> 00:46:01,859
So that is basically to
say if this is the preferred

763
00:46:02,826 --> 00:46:04,595
distribution over

764
00:46:04,595 --> 00:46:07,965
observation state and explain
and so this is basically

765
00:46:08,165 --> 00:46:11,702
our target distribution what we what
we hope to achieve in the future

766
00:46:12,436 --> 00:46:17,708
and we can rewrite this as a
as the sum of three terms.

767
00:46:17,708 --> 00:46:19,209
So we have this.

768
00:46:19,576 --> 00:46:22,412
So first of all I'm assuming
that's the agent's us

769
00:46:23,180 --> 00:46:26,483
no prior preference in action
so that for him

770
00:46:27,351 --> 00:46:31,488
any action that will bring it
to the preferred outcomes it's fine

771
00:46:31,488 --> 00:46:36,960
so he has a uniform prior but action
so the action doesn't really matter

772
00:46:37,094 --> 00:46:39,863
it is as long as it brings to there
to the goal let's see.

773
00:46:40,531 --> 00:46:44,668
And the so this in this way
we we obtain an action and be

774
00:46:45,269 --> 00:46:48,472
and then the rest the intrinsic value is

775
00:46:48,472 --> 00:46:52,810
that is the same epistemic party
that we've seen before. And

776
00:46:53,977 --> 00:46:56,079
so the the one that the

777
00:46:56,079 --> 00:46:59,883
obligation
to to explore the environment more

778
00:46:59,883 --> 00:47:02,553
or either to reduce this ambiguity
about the environment.

779
00:47:02,953 --> 00:47:07,357
And then we have the extrinsic value
which is basically rewards

780
00:47:07,391 --> 00:47:09,426
or just

781
00:47:11,094 --> 00:47:14,865
just a way to get closer to the,
to the actual preferred outcomes

782
00:47:14,865 --> 00:47:19,336
so that the value to pursue
in order to minimize distance

783
00:47:19,770 --> 00:47:23,106
from the from the preferred outcomes

784
00:47:23,106 --> 00:47:26,343
in our competitive expected free energy,

785
00:47:27,010 --> 00:47:30,914
we we again to assume there moves
as we did from the past.

786
00:47:30,914 --> 00:47:35,385
So we assume that we are taking
interpretation over

787
00:47:35,419 --> 00:47:39,089
our preferred outcomes since we don't
imagine outcomes in the future,

788
00:47:39,089 --> 00:47:43,560
we just assume that the outcomes will
will be according to the

789
00:47:43,560 --> 00:47:46,864
so the preferred outcome situation
so that we can again sum

790
00:47:47,631 --> 00:47:52,069
the entropy over our fixed
preferred outcome distribution.

791
00:47:52,636 --> 00:47:54,404
And then the steps are the same.

792
00:47:54,404 --> 00:47:58,308
So we, we have again
this mutually information term

793
00:47:58,442 --> 00:48:02,479
between the preferred outcomes
and the, the state that we mentioned

794
00:48:02,479 --> 00:48:06,450
in the future and the, the, the action
and should be determined the

795
00:48:07,517 --> 00:48:09,486
scale divergence

796
00:48:09,486 --> 00:48:13,590
between the still other states
and the other states.

797
00:48:13,590 --> 00:48:16,593
So is

798
00:48:16,593 --> 00:48:18,562
a complex term

799
00:48:18,562 --> 00:48:21,598
I'd say,
because it basically should represent

800
00:48:22,065 --> 00:48:24,701
and the difference between what the, what

801
00:48:24,701 --> 00:48:27,170
the agent believes it will happen and,

802
00:48:27,804 --> 00:48:30,073
and what is supposed to happen
in the environment.

803
00:48:30,240 --> 00:48:32,609
So normally in active inference,
we assume

804
00:48:33,076 --> 00:48:36,213
for the future that the
the model of the world is correct

805
00:48:36,780 --> 00:48:40,918
so that the agent does not control
of the over word model.

806
00:48:40,918 --> 00:48:42,920
So it cannot change how the

807
00:48:43,387 --> 00:48:46,957
the environment dynamics will transition
from one state to another.

808
00:48:46,957 --> 00:48:49,860
So I, I'm assuming year that

809
00:48:50,294 --> 00:48:54,197
this is this scale divergence in terms
actually zero though

810
00:48:54,498 --> 00:48:57,634
I've seen that some work
this could also be built left

811
00:48:57,634 --> 00:49:01,238
there are being greater than zero
but then then it's basically

812
00:49:01,939 --> 00:49:07,177
adding the agent imagines that they can
then it can violates the the environment

813
00:49:07,210 --> 00:49:11,048
dynamics hoping for a better dynamics
that it will allow it to

814
00:49:11,915 --> 00:49:16,053
to be optimistic
and think yet the thing that I imagine

815
00:49:16,053 --> 00:49:19,957
it will happen
is actually going to happen so either we

816
00:49:19,957 --> 00:49:21,525
we don't allow the agent to

817
00:49:21,525 --> 00:49:25,796
to modify or the environment to move from
one state to another and we just assume

818
00:49:25,796 --> 00:49:29,599
yeah these are the dynamics
and betterment so our posterior over the

819
00:49:30,033 --> 00:49:32,803
in the transition
dynamics of the environment is correct.

820
00:49:33,003 --> 00:49:38,775
And so the scale diverging in zero
and then our objective again doing the

821
00:49:39,776 --> 00:49:42,145
applying the the contrastive the learning

822
00:49:42,946 --> 00:49:48,151
lower bound translates into this
when we we have these contrasting

823
00:49:48,151 --> 00:49:51,521
mutually information
between the preferred outcomes and the is

824
00:49:52,656 --> 00:49:54,224
this action entropy term.

825
00:49:54,224 --> 00:49:59,629
So if you write out explicitly we again
have this this to term

826
00:49:59,629 --> 00:50:02,899
which kind of reminds
the two extrinsic value

827
00:50:02,899 --> 00:50:06,403
and intrinsic value
for for the free energy.

828
00:50:06,436 --> 00:50:10,207
So we have this
the term that actually there should

829
00:50:11,875 --> 00:50:13,543
minimize the similarity

830
00:50:13,543 --> 00:50:17,748
with the negative sample
that is doing something similar

831
00:50:18,548 --> 00:50:22,986
to what the the intrinsic value in in
active inference should do.

832
00:50:22,986 --> 00:50:25,055
So basically are trying to

833
00:50:26,123 --> 00:50:30,794
to be distant from from previously seen
their outcomes.

834
00:50:30,794 --> 00:50:35,532
This is kind of similar to explore the
environment to minimize your ambiguity.

835
00:50:35,532 --> 00:50:39,536
So try to find something that gives
you more information, not something

836
00:50:39,536 --> 00:50:40,771
that you have already seen

837
00:50:42,572 --> 00:50:44,775
and so the, the,

838
00:50:45,108 --> 00:50:47,778
the word model can be summarized in

839
00:50:48,912 --> 00:50:51,381
these three main components
that we learn.

840
00:50:51,815 --> 00:50:54,618
We have our prior network that,

841
00:50:55,685 --> 00:50:57,754
as I said before, is learned.

842
00:50:58,388 --> 00:51:01,124
And Schiller should learn the,
the transition dynamics

843
00:51:01,124 --> 00:51:03,393
of the environment
so trying to predict future states,

844
00:51:04,161 --> 00:51:06,897
given the US states and actions.

845
00:51:07,631 --> 00:51:10,200
And then we have this jury yourself

846
00:51:10,200 --> 00:51:13,603
that is shared between the
the prior and the Posture Network.

847
00:51:13,603 --> 00:51:17,140
And this is what allows us
to brings our history both.

848
00:51:17,674 --> 00:51:19,876
So it just not stuck
to the previous state,

849
00:51:19,876 --> 00:51:22,779
but also to include some information
about previous states

850
00:51:23,547 --> 00:51:26,583
so that we have more, more information
available in order

851
00:51:26,583 --> 00:51:30,120
to infer
what the current state is actually is.

852
00:51:30,720 --> 00:51:34,491
Then we have our posterior network,
which also has access

853
00:51:34,791 --> 00:51:39,129
to the that observation
and the disclosure.

854
00:51:39,129 --> 00:51:43,333
CNN, as a as I mentioned,
is a convolutional neural network.

855
00:51:43,333 --> 00:51:46,703
And the yeah, here
we have the actual layered description

856
00:51:46,703 --> 00:51:50,273
for our environments, which are 64 by 64.

857
00:51:50,273 --> 00:51:52,209
But that's that's less important.

858
00:51:52,209 --> 00:51:54,978
The important thing is
that we have a convolutional model

859
00:51:54,978 --> 00:51:58,014
that compresses the formation
from the observations for us.

860
00:51:58,648 --> 00:52:02,886
And these same convolutional network
is also linked

861
00:52:02,886 --> 00:52:06,923
to the,
to the representation model that is this

862
00:52:08,058 --> 00:52:11,328
the, the critique of the contrastive
learning mechanism.

863
00:52:11,328 --> 00:52:16,867
So the function that is indeed
matching states and the observation

864
00:52:18,034 --> 00:52:20,737
in order to learn the good

865
00:52:21,071 --> 00:52:23,607
contrastive learning representation.

866
00:52:25,041 --> 00:52:27,477
So the, the function of that

867
00:52:27,477 --> 00:52:31,715
we minimize and respect
the past is a is our contrasted

868
00:52:31,715 --> 00:52:34,251
free energy of the past, some over

869
00:52:35,185 --> 00:52:37,854
an arbitrary number of, of these key

870
00:52:37,921 --> 00:52:40,957
time steps in the over
past the sequences.

871
00:52:41,958 --> 00:52:44,094
It is important to say that

872
00:52:44,094 --> 00:52:46,897
for the past there
the negative samples that we take

873
00:52:47,531 --> 00:52:49,900
are an observation

874
00:52:50,934 --> 00:52:53,203
of the of

875
00:52:53,203 --> 00:52:57,107
of the same sequence of the object
corresponding observation.

876
00:52:57,107 --> 00:52:59,943
So that said that we have an observation
in the States

877
00:53:00,243 --> 00:53:03,246
the negative samples will be

878
00:53:03,246 --> 00:53:07,350
on the other observation within the same
sequence that are not the same in time,

879
00:53:07,884 --> 00:53:11,988
but also observation
that come from, from other sequences.

880
00:53:12,022 --> 00:53:14,724
So, so that we are basically contrasting

881
00:53:15,725 --> 00:53:16,826
the, the

882
00:53:16,826 --> 00:53:20,363
current
state with the different time steps.

883
00:53:20,363 --> 00:53:24,167
So what happened in different
the moment of the

884
00:53:24,167 --> 00:53:28,471
of the same sequence of actions and the
what happened in different situations.

885
00:53:28,471 --> 00:53:34,010
So different sequences
and that's how we, we try to,

886
00:53:34,711 --> 00:53:36,913
to foster
our contrastive learning mechanism.

887
00:53:38,448 --> 00:53:39,950
Then we have the action models.

888
00:53:39,950 --> 00:53:43,186
So for our action model,
we have two networks.

889
00:53:43,320 --> 00:53:47,757
One is the so called action network,
which basically inverse

890
00:53:47,757 --> 00:53:51,962
to the, the action to take a given state.

891
00:53:51,962 --> 00:53:56,099
And then we have this expected
activity network and this helps

892
00:53:56,099 --> 00:53:59,903
us pursuing what the team anticipated

893
00:53:59,903 --> 00:54:02,539
so that the fact that we we are

894
00:54:04,174 --> 00:54:05,642
amortizing the action

895
00:54:05,642 --> 00:54:08,578
selection process for very

896
00:54:09,579 --> 00:54:11,815
long term sequences by using

897
00:54:13,149 --> 00:54:16,486
a network that should estimate

898
00:54:16,753 --> 00:54:19,956
what the the value of a certain
state is in the future.

899
00:54:21,291 --> 00:54:25,562
So I'll try to be more more clear here.

900
00:54:25,562 --> 00:54:28,365
So basically the action network to

901
00:54:30,267 --> 00:54:32,302
minimize these

902
00:54:32,402 --> 00:54:35,071
this G lambda

903
00:54:35,171 --> 00:54:38,575
a functional
that is basically an estimate

904
00:54:39,776 --> 00:54:40,910
that of

905
00:54:42,212 --> 00:54:45,148
of how much value is in a certain states.

906
00:54:46,116 --> 00:54:48,318
And although we get this estimate

907
00:54:48,885 --> 00:54:53,890
so the estimate is
is provided by this formula here.

908
00:54:54,324 --> 00:54:57,360
So basically at every step

909
00:54:57,360 --> 00:55:00,096
we provide the the actual expected

910
00:55:00,330 --> 00:55:05,468
a competitive free energy for that state
and then for the future

911
00:55:06,069 --> 00:55:08,805
the we we some

912
00:55:09,739 --> 00:55:13,710
we compromise in between an estimate
of what the what the network would

913
00:55:13,710 --> 00:55:17,881
predict is going to
is going to be the value in the future.

914
00:55:18,214 --> 00:55:23,553
And the, the, the value itself
that we are that we are computing

915
00:55:23,553 --> 00:55:27,557
we there with the functional
so that the baby steps we basically

916
00:55:28,825 --> 00:55:30,527
sum the value that we we

917
00:55:30,527 --> 00:55:34,731
expect in the step there
we bootstrap this.

918
00:55:34,798 --> 00:55:36,866
That's the way
we normally see reinforcement learning.

919
00:55:36,866 --> 00:55:41,971
So we we apply some form of dynamic
programing approach to

920
00:55:42,405 --> 00:55:46,109
to sum this value with what we expect
will happen in the future

921
00:55:46,543 --> 00:55:48,812
and the we use these

922
00:55:49,179 --> 00:55:52,382
as our target for learning the estimate.

923
00:55:52,382 --> 00:55:55,552
So we basically have the estimate

924
00:55:55,552 --> 00:55:59,089
and the estimate of the future
plus the current value.

925
00:55:59,522 --> 00:56:03,660
And we compare the two when we want the,
the actual estimates to be closer

926
00:56:03,893 --> 00:56:08,331
to what is actually happening
plus the future estimate.

927
00:56:09,165 --> 00:56:13,436
And this is this is actually what is
normally done in reinforcement learning.

928
00:56:13,436 --> 00:56:18,141
When you apply the the so-called
bellman equation in order to estimate

929
00:56:18,141 --> 00:56:21,945
what's going to happen in the future
by using what you actually know.

930
00:56:21,945 --> 00:56:27,384
So generally like the rewards
and in our case, the, the explicit

931
00:56:28,785 --> 00:56:30,820
free energy value and what you

932
00:56:31,154 --> 00:56:34,657
you can estimate for the future

933
00:56:35,692 --> 00:56:38,228
so you know, we're experiments

934
00:56:38,228 --> 00:56:41,865
we compare to
we compare four flavors that

935
00:56:43,500 --> 00:56:45,301
make for reinforcement

936
00:56:45,301 --> 00:56:47,971
learning using likelihood model.

937
00:56:48,438 --> 00:56:50,740
This is very measured
the dealer baseline.

938
00:56:50,740 --> 00:56:55,145
So the those are likely based
and they're the word model and

939
00:56:56,413 --> 00:56:59,249
it uses rewards for learning action.

940
00:56:59,249 --> 00:57:03,286
So the the river song
freeze is already given to the agent.

941
00:57:03,453 --> 00:57:05,955
And then we compare it
with contrastive drama.

942
00:57:06,055 --> 00:57:09,793
It is a modification of primary
using a contrastive

943
00:57:10,226 --> 00:57:12,862
a learning for this word
modeling sort of reconstructions

944
00:57:13,296 --> 00:57:16,199
and then we compare the two flavors
of active inference

945
00:57:16,566 --> 00:57:20,703
in the standard and see one
with the likelihood reconstruction model

946
00:57:21,037 --> 00:57:23,373
and our contrastive formulations

947
00:57:25,408 --> 00:57:27,610
so we use similar architecture

948
00:57:27,610 --> 00:57:31,448
and training routine
for all the different baselines in the

949
00:57:32,482 --> 00:57:33,516
training routine.

950
00:57:33,516 --> 00:57:36,986
Can be summarized
as we see here in so the code.

951
00:57:37,887 --> 00:57:39,823
So for certain

952
00:57:39,823 --> 00:57:44,027
and for a certain amount of number
of training steps that we if it's events,

953
00:57:44,060 --> 00:57:48,398
we are going to train our work model
on the previous experience.

954
00:57:48,398 --> 00:57:52,001
So now on a replay buffer
that basically represents

955
00:57:52,001 --> 00:57:56,673
our data sets of plus experiences,
then we are going to use

956
00:57:56,673 --> 00:57:59,943
the trained words model
to imagine some trajectories

957
00:57:59,943 --> 00:58:02,212
in the future by using

958
00:58:03,513 --> 00:58:07,050
our actually model
and the replay buffer as well,

959
00:58:07,050 --> 00:58:10,620
which is used because we need to take
the negative samples

960
00:58:11,221 --> 00:58:14,390
for the contrastive
pre energy functional.

961
00:58:14,390 --> 00:58:18,328
And then on the imagined trajectories,
we are going to train our

962
00:58:18,828 --> 00:58:21,931
our action model in order to actually

963
00:58:22,432 --> 00:58:24,968
try to pursue the the preferred outcomes

964
00:58:25,969 --> 00:58:26,803
better.

965
00:58:27,203 --> 00:58:31,174
And then we are going to go back to
the environment, collect a new trajectory

966
00:58:31,608 --> 00:58:35,378
using our word models to infer
what the what the even state of

967
00:58:35,378 --> 00:58:39,182
the of the environment is at
and everything step and using the action

968
00:58:39,182 --> 00:58:42,752
model to select the action
according to the state of tweaking first

969
00:58:43,353 --> 00:58:48,091
and then add to the just collected
trajectory to the data set.

970
00:58:48,458 --> 00:58:49,759
And we do this continuously.

971
00:58:49,759 --> 00:58:51,561
So train the word model.

972
00:58:51,561 --> 00:58:53,863
Imagine some trajectories

973
00:58:54,330 --> 00:58:58,401
and the train the action model and again
keep collecting so that we

974
00:58:58,635 --> 00:59:02,205
we continuously improve
both the data collection process

975
00:59:02,372 --> 00:59:06,409
because the the the word model
and the actual model gets better

976
00:59:06,409 --> 00:59:09,812
and also our modeling needs
so that we get closer to the goal

977
00:59:11,881 --> 00:59:14,117
So one important insight

978
00:59:14,117 --> 00:59:17,120
into the person to produce
before turning into an

979
00:59:17,387 --> 00:59:21,457
and vertical evaluation of the method
is the fact that the using control scheme

980
00:59:21,457 --> 00:59:26,095
learning strongly reduces the the
computational requirements of the model.

981
00:59:26,696 --> 00:59:29,899
So a year
high and comparing the number of

982
00:59:30,934 --> 00:59:31,801
multiple

983
00:59:31,801 --> 00:59:34,203
million
models can play a clinician operation.

984
00:59:34,671 --> 00:59:37,707
And for our models
and the number of parameters

985
00:59:37,707 --> 00:59:40,944
that we as we see these are much lower

986
00:59:41,044 --> 00:59:43,947
when we use a contrastive

987
00:59:43,947 --> 00:59:47,116
mechanism
compared to the using likelihood model.

988
00:59:47,984 --> 00:59:52,689
And the addition
is also reflected in terms where dinosaur

989
00:59:52,956 --> 00:59:57,060
our model is quite clustered
compared to the dreamer

990
00:59:57,827 --> 01:00:01,164
that the things things the likelihood

991
01:00:01,164 --> 01:00:04,701
based model for the world uses

992
01:00:05,101 --> 01:00:08,771
just rewards for learning action
and is much, much

993
01:00:08,771 --> 01:00:12,141
faster than the likelihood the active
inference model because they like

994
01:00:12,141 --> 01:00:16,479
you are not giving trends model
other than adding to to the

995
01:00:16,479 --> 01:00:19,983
the reconstruction during the world
model training also us to imagine

996
01:00:20,850 --> 01:00:24,420
the the eye
dimensional outcomes in the future.

997
01:00:24,721 --> 01:00:27,457
So in that case
you have even more computation

998
01:00:27,457 --> 01:00:31,194
because for every margin trajectory
you have to imagine all the possible

999
01:00:31,194 --> 01:00:34,964
images in our context that you are you in

1000
01:00:35,498 --> 01:00:38,635
you will get pursuing a certain
a certain policy.

1001
01:00:39,202 --> 01:00:42,472
So it's our model
is quite faster than that.

1002
01:00:43,806 --> 01:00:45,475
So the first star

1003
01:00:45,475 --> 01:00:49,078
that I will discuss
is the simple mini task.

1004
01:00:49,479 --> 01:00:52,882
So the agent represents the
the red arrow through

1005
01:00:52,915 --> 01:00:58,187
the navigate are black white
in order to reach Venus Square.

1006
01:00:58,187 --> 01:01:00,523
The display is
in one of the corner of the grids.

1007
01:01:01,124 --> 01:01:03,960
So the environment is partially observed

1008
01:01:04,127 --> 01:01:08,431
because the agent doesn't know
what's in every pixel of the grid.

1009
01:01:08,431 --> 01:01:12,368
So in order to find the gold
first issued the explorer,

1010
01:01:12,368 --> 01:01:16,105
the old grid and, and find the,
the Green Square or at least

1011
01:01:16,806 --> 01:01:19,108
be in a position that allows in to

1012
01:01:19,342 --> 01:01:21,911
to see the Green Square in front of them.

1013
01:01:22,612 --> 01:01:26,049
So for the further you what model
of course we have,

1014
01:01:27,183 --> 01:01:31,454
we have the highest reward and the in
correspondence of the of the gold states.

1015
01:01:31,454 --> 01:01:35,224
So when the when the agent
is actually on the on the gold square,

1016
01:01:35,224 --> 01:01:40,630
it will receive a reward of plus one
and this is a sparse reward task.

1017
01:01:40,630 --> 01:01:43,933
So for all the other states the edge
and that will just

1018
01:01:43,933 --> 01:01:45,902
receive volunteer rewards.

1019
01:01:45,902 --> 01:01:49,205
So it will be just
good still to reach the goal

1020
01:01:50,473 --> 01:01:52,508
was for active interest method.

1021
01:01:53,109 --> 01:01:55,178
The way that they chose to define
the preferred outcome

1022
01:01:55,178 --> 01:01:59,549
is to have an image of the agent
that is that sees himself on the goal.

1023
01:01:59,882 --> 01:02:03,019
So basically the agencies
in cells on the goal and says

1024
01:02:03,019 --> 01:02:05,822
this is the position
that they want to reach in the world

1025
01:02:07,090 --> 01:02:09,158
and let's see

1026
01:02:09,425 --> 01:02:12,528
what happens from our
from a qualitative level.

1027
01:02:12,528 --> 01:02:16,165
So as I said before,
the rewards for this task

1028
01:02:16,566 --> 01:02:20,103
is just a plus one in the right, a

1029
01:02:20,903 --> 01:02:23,372
in the right square in the goal square.

1030
01:02:23,372 --> 01:02:26,309
What happens for the
the active insurance model?

1031
01:02:26,309 --> 01:02:27,343
So what we'll do?

1032
01:02:27,343 --> 01:02:33,216
The active inference
model provides us a value of a certain

1033
01:02:33,216 --> 01:02:36,419
status of the agents in order to
to pursue the preferred outcome

1034
01:02:37,153 --> 01:02:40,857
so we see that the like
new active inference when

1035
01:02:41,924 --> 01:02:42,692
the is

1036
01:02:42,692 --> 01:02:46,763
imagining the outcome and comparing it to
the preferred images

1037
01:02:47,163 --> 01:02:50,299
is actually giving a very I value for the
for the right

1038
01:02:50,666 --> 01:02:54,403
the square saw this the you know

1039
01:02:55,171 --> 01:02:59,876
go from 01 we can say that it's the 10
sorry we can say that's the one

1040
01:02:59,876 --> 01:03:05,148
but other than that the the function
that it is providing is a bit confusing

1041
01:03:05,481 --> 01:03:09,519
because it's giving some other rewards
in the sentence

1042
01:03:10,253 --> 01:03:13,256
compared to the let's say the the

1043
01:03:14,123 --> 01:03:17,994
the the last row and column

1044
01:03:17,994 --> 01:03:20,930
that are the one that leads the
to the final goal.

1045
01:03:22,298 --> 01:03:26,369
The the the other corner
are not even close to the go one.

1046
01:03:26,369 --> 01:03:30,606
So it's just the,
it's just providing a perfect match.

1047
01:03:30,606 --> 01:03:34,310
And we have no, no control of what
the distance the goal is

1048
01:03:34,310 --> 01:03:36,579
because for the perfect match,
the goal that's,

1049
01:03:37,380 --> 01:03:40,316
that is indeed the, the correct part
of the biggest providing.

1050
01:03:40,316 --> 01:03:43,886
But other than that, we,
it's difficult to understand what the

1051
01:03:44,720 --> 01:03:47,156
the the likelihood
that the inference model is providing

1052
01:03:47,156 --> 01:03:51,961
with the contrastive up to the inference
we see that there is a different button.

1053
01:03:52,128 --> 01:03:56,933
So the agent is providing
very low value for the center.

1054
01:03:56,933 --> 01:04:02,205
So it's understanding that the center is
of course not what they want to see

1055
01:04:02,905 --> 01:04:06,175
but then is providing AI values
to all the corners

1056
01:04:06,542 --> 01:04:10,546
and in particular the highest that is
provided to the air, to the right corner.

1057
01:04:10,546 --> 01:04:14,483
So the one with the goal because is
of course the, the one that corresponds

1058
01:04:14,483 --> 01:04:18,020
to most with that
we want what we want to achieve.

1059
01:04:18,020 --> 01:04:22,792
But then on the other corner
also have a very high value and the

1060
01:04:23,626 --> 01:04:28,464
the fact is that from I will say
that the contrast back to inference

1061
01:04:28,464 --> 01:04:31,968
is especially capturing a more

1062
01:04:32,401 --> 01:04:34,904
I will see
semantic information about environment.

1063
01:04:35,037 --> 01:04:38,040
So in order to distinguish a corner

1064
01:04:38,875 --> 01:04:42,879
from from a central title
is actually a model in the fact

1065
01:04:42,879 --> 01:04:46,382
that there is a corner
in in a certain state of the environment.

1066
01:04:46,949 --> 01:04:52,121
And when you look at the preferred
outcome image, you actually says,

1067
01:04:52,121 --> 01:04:55,625
first of all, this is a corner
and that's the way distinguishes.

1068
01:04:55,625 --> 01:05:00,196
And then there is the green
as the green tile in the

1069
01:05:00,196 --> 01:05:04,333
in the in where the where the agent is so

1070
01:05:05,835 --> 01:05:08,070
much from a value perspective,
we can say that

1071
01:05:09,171 --> 01:05:11,841
a corner is closer in semantics,

1072
01:05:12,808 --> 01:05:15,511
respect
to to a central style to our goal.

1073
01:05:15,811 --> 01:05:18,915
And then of course,
when you also have the the green tile

1074
01:05:18,915 --> 01:05:21,117
which represents the goal, then your
are the closest.

1075
01:05:21,951 --> 01:05:25,087
So this is of course a bit risky because

1076
01:05:26,122 --> 01:05:29,592
it can also be due to suboptimal behavior
in some cases.

1077
01:05:29,592 --> 01:05:33,729
But with the good expression
of the environment that it will

1078
01:05:34,063 --> 01:05:38,000
of course, will lead the optimal behavior
because still the

1079
01:05:38,000 --> 01:05:41,070
the maximum value is still provided
correctly.

1080
01:05:41,070 --> 01:05:42,972
So it's still good
value for the right corner.

1081
01:05:42,972 --> 01:05:45,141
But if you didn't see
the other corner of the agent,

1082
01:05:45,174 --> 01:05:49,312
will we just go to another corner and
say, okay, this looks similar to the goal

1083
01:05:49,512 --> 01:05:53,849
so I'm trying to do something similar
to to what I would like to do, right?

1084
01:05:54,183 --> 01:05:56,118
If I didn't see anything else.

1085
01:05:56,118 --> 01:05:59,255
Disclosure, this is my the best I can do.

1086
01:05:59,722 --> 01:06:01,357
So this is a light.

1087
01:06:01,357 --> 01:06:05,227
So exploration is important
in order to to achieve

1088
01:06:06,295 --> 01:06:07,496
preferred outcomes.

1089
01:06:07,496 --> 01:06:11,801
And I think that's that also applies
to likely reductive inference as well

1090
01:06:12,134 --> 01:06:16,605
because even if it see the goal,
you just add some, some noise,

1091
01:06:17,139 --> 01:06:19,809
some noise, a signal in the center
so you wouldn't

1092
01:06:19,809 --> 01:06:23,679
be able to reach the goal as well.

1093
01:06:23,679 --> 01:06:26,782
And then yeah,
we quantified the performance.

1094
01:06:26,782 --> 01:06:29,719
We see that with the
with the likelihood active inference

1095
01:06:30,319 --> 01:06:33,990
that the agent struggles
to to reach the goal

1096
01:06:35,224 --> 01:06:36,092
consistently.

1097
01:06:36,092 --> 01:06:40,463
Well, our our methods leads
to consistent performance

1098
01:06:40,496 --> 01:06:44,066
that are in line with the reward base
baselines.

1099
01:06:44,100 --> 01:06:47,503
Of course, the reward base
baseline have an advantage because

1100
01:06:48,871 --> 01:06:51,640
during training
they always have a filtered objective.

1101
01:06:51,741 --> 01:06:55,277
So even if their model is not correct,
they always have this reward function

1102
01:06:55,277 --> 01:06:59,515
filter function that tells them, yes,
this is where you need to go.

1103
01:06:59,515 --> 01:07:02,952
Well,
our model can take a little bit more time

1104
01:07:02,952 --> 01:07:07,356
in order to first have a good model
and then being able to match. But

1105
01:07:08,324 --> 01:07:08,624
with the

1106
01:07:08,624 --> 01:07:13,262
contrastive mechanism,
this actually happens fast

1107
01:07:13,262 --> 01:07:17,199
and leads to a consistent performance
where we select the active inference.

1108
01:07:17,199 --> 01:07:22,138
We see that it will probably
take more time to converge or

1109
01:07:23,205 --> 01:07:26,675
it just leads to the suboptimal behavior.

1110
01:07:26,675 --> 01:07:29,879
So it's just inconsistent
according to our evaluation.

1111
01:07:30,646 --> 01:07:33,482
And yeah, these are two different

1112
01:07:34,083 --> 01:07:37,286
grid environments
where this one is smaller, one is bigger,

1113
01:07:37,286 --> 01:07:42,158
but yet the results are very similar
in terms of performance obtained.

1114
01:07:43,626 --> 01:07:48,264
Then the other task that we discuss
is our continuous control task.

1115
01:07:48,431 --> 01:07:52,535
We've are we've all
to the planner environment

1116
01:07:52,535 --> 01:07:56,639
when we're a robotic arm
to the penetrate.

1117
01:07:56,672 --> 01:08:00,609
Asked your goals for the the red sphere
and the

1118
01:08:00,843 --> 01:08:03,479
the this theory is bigger in the

1119
01:08:04,046 --> 01:08:07,083
the so called the return easy environment
which is the one

1120
01:08:07,283 --> 01:08:09,351
for which we see the preferred
outcome on the left

1121
01:08:09,752 --> 01:08:13,289
and it is smaller for the for
the second feature are the environment

1122
01:08:13,289 --> 01:08:16,225
that is the one that we will see
on the right.

1123
01:08:16,492 --> 01:08:20,963
So for every work based agent we have

1124
01:08:22,064 --> 01:08:25,167
the reward function that provides

1125
01:08:25,167 --> 01:08:27,136
the when the agent penetrates

1126
01:08:27,136 --> 01:08:30,272
to the sphere solar array was of one.

1127
01:08:30,272 --> 01:08:32,541
Otherwise when it's

1128
01:08:32,942 --> 01:08:36,278
of penetrating
or just partially penetrating the sphere,

1129
01:08:36,278 --> 01:08:38,047
it provides a dense reward.

1130
01:08:38,047 --> 01:08:41,350
That's a that tells you
you're getting closer to the goal.

1131
01:08:41,350 --> 01:08:45,554
So in this case, the reward function
is actually helping the agent

1132
01:08:45,554 --> 01:08:49,391
a bit more because it is telling him
that this is getting closer to the goal.

1133
01:08:49,391 --> 01:08:53,295
And he should just try it
in the neighbor area.

1134
01:08:53,796 --> 01:08:58,167
And while for the inference
we would just provide the the preferred

1135
01:08:59,401 --> 01:09:01,804
staging environment, which is the

1136
01:09:02,505 --> 01:09:06,876
the agent penetrating the gold sphere,
and let's see, let's see what happens.

1137
01:09:07,076 --> 01:09:10,246
So again, we have a similar

1138
01:09:10,246 --> 01:09:13,549
pattern, actually

1139
01:09:13,549 --> 01:09:16,519
similar but different
from the previous one or so in this case

1140
01:09:16,719 --> 01:09:21,123
where the the main square
there is the distance from the goal

1141
01:09:21,423 --> 01:09:25,027
is actually more confusing
as we have seen in previous example.

1142
01:09:25,561 --> 01:09:28,964
So the relative
with the active translation totally fails

1143
01:09:29,365 --> 01:09:33,369
to to reach the goal because it's
probably the order of the states

1144
01:09:33,369 --> 01:09:36,005
looking like an environment
because the background stays the same.

1145
01:09:36,338 --> 01:09:38,941
So the background is not moving
as it was happening.

1146
01:09:38,941 --> 01:09:41,610
For instance,
for the mini grid environment,

1147
01:09:41,610 --> 01:09:44,013
the goal is always in the same position.

1148
01:09:44,013 --> 01:09:48,117
So the difference between the two images
is just provide given by the

1149
01:09:48,751 --> 01:09:50,953
the few yellow pixels that moves around.

1150
01:09:51,253 --> 01:09:55,724
And then if the model is not, imagine
perfectly where this pixel go, it's, it's

1151
01:09:55,758 --> 01:09:57,059
very difficult.

1152
01:09:57,059 --> 01:10:01,964
It will provide some some informative
or objective for the stars instance.

1153
01:10:02,431 --> 01:10:06,869
Our contrastive active inference
agent is able to provide an informative

1154
01:10:06,902 --> 01:10:12,541
go and the appearance in the fact that
this providing some semantic information

1155
01:10:12,541 --> 01:10:16,111
about the task is actually helping it
to converge even faster than the

1156
01:10:16,679 --> 01:10:19,181
than the reward base at baseline

1157
01:10:19,181 --> 01:10:21,884
because the reserve based agents

1158
01:10:22,785 --> 01:10:25,854
have access to the just
when they are close to the to the goal

1159
01:10:25,854 --> 01:10:26,956
or the contrastive activity

1160
01:10:26,956 --> 01:10:30,025
first provide a reward function
everybody in the environment.

1161
01:10:30,392 --> 01:10:33,362
So when the
when we see that the army is very far

1162
01:10:33,862 --> 01:10:37,633
we have the mutual information term
that should basically take over

1163
01:10:37,633 --> 01:10:38,634
and don't you yeah.

1164
01:10:38,634 --> 01:10:43,439
You don't want to stay there go elsewhere
and the until we

1165
01:10:43,505 --> 01:10:48,310
we have only find the ghost here
and we come back to the correct behavior.

1166
01:10:48,744 --> 01:10:52,781
So the agent is actually converging
a bit faster than the other base lines

1167
01:10:52,781 --> 01:10:53,382
and then yeah.

1168
01:10:53,382 --> 01:10:57,119
The the contrastive dreamer
baseline is converging a bit faster

1169
01:10:57,119 --> 01:11:01,991
than the remote one because its model is
faster to learn because it's contrastive

1170
01:11:04,159 --> 01:11:06,395
so this is what actually happens.

1171
01:11:06,395 --> 01:11:07,663
These are a gift on the right.

1172
01:11:07,663 --> 01:11:10,966
At some point
they should be able to be said.

1173
01:11:10,966 --> 01:11:13,902
So basically we just see here
that the task is that

1174
01:11:14,403 --> 01:11:17,239
is correctly executed so that the agent

1175
01:11:17,940 --> 01:11:20,075
is, is able to match the,

1176
01:11:21,343 --> 01:11:22,945
the correct behavior.

1177
01:11:22,945 --> 01:11:26,282
So yeah, he's
taking a bit longer than expected.

1178
01:11:26,282 --> 01:11:29,785
But yeah, you can see that the
if for instance, in the in the atas,

1179
01:11:29,885 --> 01:11:33,122
the agent oscillates
around the current behavior

1180
01:11:33,122 --> 01:11:36,225
but it keeps saying,
okay, here we we see it.

1181
01:11:36,759 --> 01:11:39,728
So basically
the Indian environment is oscillating

1182
01:11:39,762 --> 01:11:43,499
in the position
that is a very close to the goal.

1183
01:11:43,499 --> 01:11:46,502
So it tries to stay as much as possible
to that

1184
01:11:46,802 --> 01:11:49,038
it's buoyant and not

1185
01:11:50,072 --> 01:11:52,174
not being driven

1186
01:11:52,174 --> 01:11:56,478
far from the goal,
but they never show the of the they are

1187
01:11:57,980 --> 01:11:58,580
then we

1188
01:11:58,580 --> 01:12:02,985
reanalyze qualitatively what's
what's happening in terms of the volume

1189
01:12:03,085 --> 01:12:06,689
is provided to the agents or what's
what is the objective

1190
01:12:06,755 --> 01:12:09,558
it is given to be to the agent
in order to learn more.

1191
01:12:10,326 --> 01:12:13,862
And as as I try to explain,

1192
01:12:13,862 --> 01:12:17,099
the reward is

1193
01:12:17,866 --> 01:12:20,736
is somewhere in the middle between zero
and one

1194
01:12:21,337 --> 01:12:23,739
when the agent
is partially penetrating the goal

1195
01:12:24,139 --> 01:12:27,376
and is totally one,
when the agent is fully penetrating,

1196
01:12:27,376 --> 01:12:30,112
the goal
in all the other situations is zero

1197
01:12:31,380 --> 01:12:33,916
for the active insurance
likelihood based model.

1198
01:12:34,283 --> 01:12:36,752
We see that the signal is very close

1199
01:12:37,219 --> 01:12:40,556
for all the states.

1200
01:12:41,056 --> 01:12:44,226
So the agent basically see things
that there is

1201
01:12:44,893 --> 01:12:48,630
very little difference
between being very close to goal

1202
01:12:49,164 --> 01:12:52,267
and being actually very far
off from the goal.

1203
01:12:52,768 --> 01:12:53,168
And that's

1204
01:12:54,570 --> 01:12:56,839
likely the reason why is not converging

1205
01:12:56,839 --> 01:13:00,809
to the to the optimal behavior
for contrastive active inference.

1206
01:13:00,809 --> 01:13:03,512
Instead,
we see that the agent is provided

1207
01:13:05,247 --> 01:13:06,815
an objective variance

1208
01:13:06,815 --> 01:13:11,019
some somewhat in between zero
and one when he's there,

1209
01:13:11,787 --> 01:13:14,656
when he's not closing the goal.

1210
01:13:14,857 --> 01:13:18,761
And something that is very close to one
when it's in the goal and in particular

1211
01:13:18,761 --> 01:13:22,831
when when it's closer to the goal,
the, the, the value is actually a bit

1212
01:13:22,831 --> 01:13:25,501
higher than when it's when it's far off

1213
01:13:26,268 --> 01:13:30,539
so we see that there is some again,
some semantic information provided,

1214
01:13:30,539 --> 01:13:36,211
which is the actual distance of the arm
from the from the right goal.

1215
01:13:36,211 --> 01:13:39,248
Or we can just see it as an objective

1216
01:13:40,048 --> 01:13:43,886
the contrastive learning mechanism
saying, okay, this, this part of the arm

1217
01:13:44,319 --> 01:13:47,756
is actually very different
from the one day, the day of today.

1218
01:13:47,890 --> 01:13:51,226
So let's try to move to a
place that is actually closer

1219
01:13:52,494 --> 01:13:56,331
and the we need of your values when the

1220
01:13:56,331 --> 01:14:01,003
when the pause is similar to the two,
the one that we we want to achieve.

1221
01:14:02,337 --> 01:14:06,241
So we exploit the fact
that some semantic information

1222
01:14:06,241 --> 01:14:08,944
is provided to the agent
by using contrastive

1223
01:14:09,511 --> 01:14:11,814
and learning to work on

1224
01:14:12,314 --> 01:14:15,984
the more difficult set up is the return
of the starting environment.

1225
01:14:16,652 --> 01:14:19,621
So in the visual dissecting task,

1226
01:14:20,222 --> 01:14:22,825
we have the same objective as before.

1227
01:14:22,825 --> 01:14:27,863
So we want the agent to reach the goal
by penetrating the red sphere.

1228
01:14:27,863 --> 01:14:30,999
But now we have our own backgrounds
and we have this structure

1229
01:14:30,999 --> 01:14:35,504
in the environment which could be just
altered colors on that camera.

1230
01:14:36,338 --> 01:14:41,510
And we still want to to achieve the agent
to still despite that.

1231
01:14:41,677 --> 01:14:44,246
So for the, for the reward based agents,

1232
01:14:45,180 --> 01:14:47,115
the reward is the same as before.

1233
01:14:47,115 --> 01:14:52,054
So they are being provided for the agent
penetrating the ghost here for active.

1234
01:14:52,054 --> 01:14:53,922
And for us the goal here is,

1235
01:14:53,922 --> 01:14:58,126
is actually a bit troublesome
to the design because given the fact

1236
01:14:58,126 --> 01:15:02,164
that the background is constantly
bringing across different episodes,

1237
01:15:02,731 --> 01:15:05,801
we cannot a priori define

1238
01:15:06,435 --> 01:15:10,405
what's the what
the the preferred outcome looks like.

1239
01:15:10,572 --> 01:15:15,477
So instead of doing that,
we attempted providing a more neutral

1240
01:15:15,978 --> 01:15:19,681
preferred outcome with the agent,
seeing itself achieving the goal.

1241
01:15:20,182 --> 01:15:23,552
But do we have
the standard task background?

1242
01:15:23,619 --> 01:15:26,021
So we have this the blue

1243
01:15:27,256 --> 01:15:32,361
just like the background
with with the arm penetrating the goal.

1244
01:15:32,361 --> 01:15:36,565
And we, we aim for this preferred outcome
to transfer

1245
01:15:37,533 --> 01:15:40,168
to the to the destructing set up.

1246
01:15:40,369 --> 01:15:41,336
This is of course,

1247
01:15:42,804 --> 01:15:45,807
pretty
much impossible for for the likelihood

1248
01:15:45,841 --> 01:15:50,112
the active inference model because
our source is trying to match this in the

1249
01:15:50,712 --> 01:15:51,613
we have a main square.

1250
01:15:51,613 --> 01:15:56,385
There are like functions
of course the signal providing

1251
01:15:56,385 --> 01:15:58,720
will be very confusing.

1252
01:15:58,820 --> 01:16:01,290
Very interestingly
we see yields that are so the dreamer

1253
01:16:01,323 --> 01:16:05,460
method fails because it's based
the on the likelihood based models

1254
01:16:05,460 --> 01:16:09,097
and the as we'll see
a reconstruction goal.

1255
01:16:09,097 --> 01:16:12,901
The the variations in the environment
is very difficult.

1256
01:16:13,368 --> 01:16:17,673
So the the word reconstruction
based work model struggles

1257
01:16:18,740 --> 01:16:21,209
to provide
informative states of the environment

1258
01:16:21,209 --> 01:16:25,480
while the contrastive learning
based model succeeded in particular.

1259
01:16:25,614 --> 01:16:30,519
It's very interesting that our model
was able to to actually achieve the goal

1260
01:16:30,786 --> 01:16:34,423
we see less consistently than before
so that there is another variance.

1261
01:16:34,423 --> 01:16:39,595
But still the agents is often able
to reach the correct position

1262
01:16:39,595 --> 01:16:42,631
despite of the difference
in the background and all the structure

1263
01:16:42,631 --> 01:16:43,832
present in the environment.

1264
01:16:43,832 --> 01:16:45,767
So we see that this actually

1265
01:16:47,369 --> 01:16:49,237
the representation

1266
01:16:49,237 --> 01:16:55,444
is actually learning
what the force of the robot should be.

1267
01:16:55,444 --> 01:16:57,946
And trying to match it in the future.

1268
01:16:59,281 --> 01:17:03,051
And then, yeah,
we see some, some videos with happening.

1269
01:17:03,051 --> 01:17:07,823
So we see indeed that the,
the arm is oscillating a bit more slowly.

1270
01:17:07,889 --> 01:17:10,892
It's actually a bit more difficult
for him to assess that he's doing

1271
01:17:11,660 --> 01:17:12,394
the right thing.

1272
01:17:12,394 --> 01:17:16,064
But still the, the behavior obtained
it's still

1273
01:17:17,466 --> 01:17:18,600
quite good, I would say.

1274
01:17:18,600 --> 01:17:22,337
So it's pretty much achieving the goal

1275
01:17:22,838 --> 01:17:25,073
and the dispose why

1276
01:17:26,141 --> 01:17:26,708
I like it.

1277
01:17:26,708 --> 01:17:28,844
The base
model will fail in this environment.

1278
01:17:28,844 --> 01:17:32,214
So here we compared the ground rules
in the different

1279
01:17:33,281 --> 01:17:35,283
range backgrounds and the

1280
01:17:35,484 --> 01:17:39,721
what the dreamers order them,
the likelihood

1281
01:17:39,721 --> 01:17:43,558
the active inference model
sees through the reconstruction.

1282
01:17:43,558 --> 01:17:47,729
So we see that either reconstructing
from there, from the procedure state

1283
01:17:47,729 --> 01:17:49,598
or from the prior state,

1284
01:17:49,598 --> 01:17:54,302
the agent cannot perfectly model
the important information

1285
01:17:54,302 --> 01:17:57,639
of the environment,
which in this case is the the arm.

1286
01:17:58,006 --> 01:18:01,710
So it sees where the the first
link of the the robot are missed

1287
01:18:01,710 --> 01:18:06,415
but he is not able to see it normally
with the second part of the arm.

1288
01:18:06,415 --> 01:18:08,684
He is
because is very uncertain about that.

1289
01:18:09,284 --> 01:18:14,456
And then and that leads the agent
to do not being able to actually assess

1290
01:18:14,556 --> 01:18:17,859
where it is in the environment
and to to provide the

1291
01:18:18,360 --> 01:18:22,130
the right value for our
for for what's going on.

1292
01:18:22,130 --> 01:18:27,836
So they're using reconstruction
this environment leads to the

1293
01:18:28,136 --> 01:18:30,872
to this kind of problem
where the agent is not

1294
01:18:32,040 --> 01:18:34,376
certain about the the internal states.

1295
01:18:34,376 --> 01:18:38,680
And so it's uncertain
what they should do next

1296
01:18:39,681 --> 01:18:43,652
because the signal of the state is
uncertain is computed.

1297
01:18:43,719 --> 01:18:46,822
And so it is also the

1298
01:18:46,822 --> 01:18:51,026
the value provided by the
but to the agent by the model,

1299
01:18:52,294 --> 01:18:54,096
and that's it.

1300
01:18:54,096 --> 01:18:57,132
So I'll just briefly summarize
what we have seen.

1301
01:18:57,499 --> 01:19:00,802
And so basically,
we use a constructive model

1302
01:19:01,269 --> 01:19:03,638
to reduce the computation

1303
01:19:03,939 --> 01:19:06,708
here of the objective inference

1304
01:19:06,708 --> 01:19:09,778
is also brought some advantages
and enforcement learning.

1305
01:19:09,778 --> 01:19:14,316
But yeah, we focus on the on the contract
and the active inference area

1306
01:19:14,316 --> 01:19:17,652
where this model brought to
to a twofold advantage

1307
01:19:17,652 --> 01:19:21,623
for some learning the the word
model cluster but also in imagining

1308
01:19:21,623 --> 01:19:24,526
future trajectories faster
because didn't have the reconstruction.

1309
01:19:26,061 --> 01:19:28,830
Then we saw that
the conceptual representational features

1310
01:19:28,830 --> 01:19:31,666
the better kept the relevant information
for the environment.

1311
01:19:31,900 --> 01:19:36,338
And this was key in solving
both the the research tasks

1312
01:19:36,538 --> 01:19:39,241
and especially in the research
disrupting task

1313
01:19:39,474 --> 01:19:44,079
where without this feature
we wouldn't be able to solve dusk.

1314
01:19:46,047 --> 01:19:48,750
And then we will show that to

1315
01:19:49,618 --> 01:19:52,721
we can we can use this method

1316
01:19:53,288 --> 01:19:57,292
to provide performance
that are similar to engineering rewards.

1317
01:19:58,093 --> 01:19:59,628
But in a much easier way.

1318
01:19:59,628 --> 01:20:03,398
So you can just say, okay, this is what
I want to achieve in the environment,

1319
01:20:03,398 --> 01:20:08,436
provide the, the observation
the agent engaging will find itself a way

1320
01:20:09,137 --> 01:20:12,474
to reach that that state without actually

1321
01:20:12,674 --> 01:20:17,078
having to provide a reward function
for every possible space.

1322
01:20:17,145 --> 01:20:21,183
The environment
which especially in realistic cases,

1323
01:20:21,216 --> 01:20:24,719
is usually unfeasible.

1324
01:20:24,786 --> 01:20:26,288
And finally, we

1325
01:20:26,288 --> 01:20:30,826
we have also seen that the exploration
is very key for our method to work

1326
01:20:30,959 --> 01:20:35,564
because we don't want the agent to
to convert to the suboptimal behavior

1327
01:20:35,564 --> 01:20:39,835
that looks like the
the right outcome, the preferred outcome.

1328
01:20:39,868 --> 01:20:43,071
So it's it's very important
to wisely explore

1329
01:20:43,071 --> 01:20:47,576
the environment
before actually delving into learning the

1330
01:20:48,710 --> 01:20:50,912
our preferred policy.

1331
01:20:50,912 --> 01:20:54,516
And we
aim to look more into this in the future.

1332
01:20:56,084 --> 01:20:59,187
So thank you very much.

1333
01:20:59,187 --> 01:21:00,822
That that was it.

1334
01:21:00,822 --> 01:21:03,525
I don't know if there's any question.

1335
01:21:06,728 --> 01:21:07,796
Thank you both.

1336
01:21:07,796 --> 01:21:10,298
Very interesting presentation.

1337
01:21:10,532 --> 01:21:13,735
So if anyone watching live
wants to ask a question

1338
01:21:14,736 --> 01:21:17,005
otherwise I have a few.

1339
01:21:17,772 --> 01:21:21,910
So you mentioned a critic model

1340
01:21:22,844 --> 01:21:25,113
when you were describing the architecture

1341
01:21:25,447 --> 01:21:27,916
and that reminded me of language
learning.

1342
01:21:27,916 --> 01:21:31,553
Like if someone says repeat after me
and then they give a sound, you

1343
01:21:31,553 --> 01:21:36,057
might be accurate or you might not be,
but if someone said no, it was not true.

1344
01:21:36,691 --> 01:21:39,060
You're a negative and a positive example.

1345
01:21:39,427 --> 01:21:44,132
So what does that speak
to perhaps the biological basis

1346
01:21:44,132 --> 01:21:48,003
of contrast of learning
or how these contrastive learning

1347
01:21:49,137 --> 01:21:53,341
settings act of inference or not
relate to the ways that organisms learn

1348
01:21:58,246 --> 01:21:59,381
Okay.

1349
01:22:00,181 --> 01:22:03,018
I will say that

1350
01:22:03,485 --> 01:22:05,754
the contrastive learning mechanism,

1351
01:22:05,754 --> 01:22:09,324
though it's not completely equal.

1352
01:22:09,557 --> 01:22:13,795
I was in Somalia in that resembles
the ability mechanism

1353
01:22:14,229 --> 01:22:16,564
where you when when when you have

1354
01:22:16,865 --> 01:22:19,467
corresponding bursts over.

1355
01:22:19,834 --> 01:22:24,539
So the things that should correspond
if you actually want to

1356
01:22:24,539 --> 01:22:27,976
to strengthen the link
and when you have stuff that shouldn't

1357
01:22:28,643 --> 01:22:32,113
be corresponding you actually you want to
to weaken the link.

1358
01:22:32,514 --> 01:22:35,450
So I think that biologically we call

1359
01:22:35,450 --> 01:22:38,520
that it could actually seeing this way.

1360
01:22:38,520 --> 01:22:42,123
So when you have something that you
you want to be

1361
01:22:43,725 --> 01:22:45,293
you want to link farther

1362
01:22:45,293 --> 01:22:48,697
in our case
to think that we want to to link further.

1363
01:22:48,730 --> 01:22:50,498
So to reinforce is the fact

1364
01:22:50,498 --> 01:22:53,535
that this a certain observation
correspond to a certain state

1365
01:22:54,569 --> 01:22:56,438
then you just strengthen this connection.

1366
01:22:56,438 --> 01:23:01,843
Well, when you where you want
you want to to be far and the square

1367
01:23:01,876 --> 01:23:02,677
where the contrast

1368
01:23:02,677 --> 01:23:06,514
between it differs maybe from this
that this biological perspective

1369
01:23:06,514 --> 01:23:10,952
we actually should be pushed farther
which is not always

1370
01:23:11,453 --> 01:23:14,222
the case for it being there
because normally you don't have this

1371
01:23:14,622 --> 01:23:16,358
a pushing part of the mechanism.

1372
01:23:16,358 --> 01:23:21,196
So I would say this could be
when one possible links

1373
01:23:22,130 --> 01:23:24,399
as you said
yeah the critique function is actually

1374
01:23:24,399 --> 01:23:27,569
doing something
very very similar to what you mentioned.

1375
01:23:27,569 --> 01:23:32,574
So you have a positive samples
and you reinforce the critics tells you

1376
01:23:32,574 --> 01:23:36,578
yeah this is this is correct and it sure
tells you that this is correct.

1377
01:23:36,745 --> 01:23:38,013
So it's strange to do that

1378
01:23:39,114 --> 01:23:41,583
with with with much lower me, but.

1379
01:23:41,583 --> 01:23:44,319
Well, if you have a good critique,
you could use that.

1380
01:23:44,319 --> 01:23:47,422
But yeah
it should tells you the right samples

1381
01:23:47,422 --> 01:23:51,359
while the for for non
corresponding states in the observation

1382
01:23:51,793 --> 01:23:54,896
they should go
yeah this is not what we want

1383
01:23:54,896 --> 01:23:57,332
in our representation
we want this further.

1384
01:23:58,933 --> 01:24:01,202
So maybe to add on Daniel

1385
01:24:01,202 --> 01:24:03,905
I think what you're hinting at that's

1386
01:24:03,905 --> 01:24:08,576
providing like it should be like
this is more like a way to

1387
01:24:09,244 --> 01:24:13,415
to define the preferred state
so so then if you translate it

1388
01:24:13,481 --> 01:24:16,751
to do what we are doing it's
basically saying

1389
01:24:18,086 --> 01:24:22,690
these observations are
what you should like basically.

1390
01:24:22,690 --> 01:24:25,126
So so they come into place for

1391
01:24:25,927 --> 01:24:29,397
creating the mobile like
how do they get to these observations.

1392
01:24:29,831 --> 01:24:32,167
The contrast off
defrauding for this more like

1393
01:24:34,335 --> 01:24:34,669
being

1394
01:24:34,669 --> 01:24:37,472
able to distinguish different things
basically

1395
01:24:38,706 --> 01:24:43,445
and it's more proof than the difference

1396
01:24:44,112 --> 01:24:46,948
but what what you
what you like to what you like

1397
01:24:47,649 --> 01:24:49,951
so the contrast different regarding
just learns to distinguish

1398
01:24:49,951 --> 01:24:53,855
all kinds of sounds
even all the bad ones and

1399
01:24:55,090 --> 01:24:56,891
you can just now say, okay,

1400
01:24:56,891 --> 01:25:00,728
but now I really don't have this sound,
so try to get there.

1401
01:25:00,795 --> 01:25:03,331
I think that's the difference here.

1402
01:25:05,934 --> 01:25:09,804
That kind of sounds like paying attention
to the right details,

1403
01:25:09,804 --> 01:25:13,441
which we saw with multiple times
like the breakout games.

1404
01:25:13,441 --> 01:25:14,843
Like how could you miss the ball

1405
01:25:14,843 --> 01:25:17,612
humans are watching that gif
and we're watching the ball,

1406
01:25:18,046 --> 01:25:22,951
but we also have a sense of how to pay
attention to the right details.

1407
01:25:23,351 --> 01:25:28,623
And then in terms of action and to have
curiosity about the right things.

1408
01:25:28,623 --> 01:25:29,691
So it definitely

1409
01:25:30,692 --> 01:25:31,092
starts to

1410
01:25:31,092 --> 01:25:33,561
bridge
into some very interesting behavior

1411
01:25:34,863 --> 01:25:38,333
Another question was about the action
entropy term

1412
01:25:38,800 --> 01:25:41,069
in the free energy calculations.

1413
01:25:41,402 --> 01:25:44,239
So maybe could you restate what

1414
01:25:44,239 --> 01:25:48,042
the action entropy term is since it's
one of the major contributions?

1415
01:25:48,376 --> 01:25:51,012
And also
what does that say about adding terms

1416
01:25:52,647 --> 01:25:54,916
to the free energy calculate action

1417
01:25:54,916 --> 01:25:57,485
like the action
entropy is always greater than zero.

1418
01:25:58,052 --> 01:26:00,088
Kind of like a scale divergence.

1419
01:26:00,421 --> 01:26:04,259
And so that you mentioned gives some
perhaps nice properties about the bounded

1420
01:26:04,259 --> 01:26:07,462
ness of F within a lower
and an upper bound.

1421
01:26:07,695 --> 01:26:10,865
So maybe just
what is the action entropy doing here?

1422
01:26:11,199 --> 01:26:14,702
Can we just add other terms
that are bounded zero to free energy

1423
01:26:14,702 --> 01:26:17,372
and use that in other ways?

1424
01:26:19,674 --> 01:26:20,441
Okay.

1425
01:26:20,441 --> 01:26:24,345
So I'll, I'll start with the
with a question about the,

1426
01:26:25,280 --> 01:26:26,314
the action entropy term.

1427
01:26:26,314 --> 01:26:30,585
And then I also delve into the,
the using different bounds for, for

1428
01:26:31,686 --> 01:26:33,955
the free energy term. So

1429
01:26:35,557 --> 01:26:38,226
everything is a in the way.

1430
01:26:38,226 --> 01:26:41,663
We just added the active inference
process for learning the actions.

1431
01:26:43,531 --> 01:26:45,567
The key part is that we

1432
01:26:46,034 --> 01:26:50,338
the actions are now part of the,
the future inference process.

1433
01:26:50,705 --> 01:26:54,409
So here I could,

1434
01:26:54,909 --> 01:26:57,579
I could also go back to the two biggest
like this necessary.

1435
01:26:57,579 --> 01:27:01,416
But the normally the way that we see
these objectives

1436
01:27:01,416 --> 01:27:05,420
without is a term here and there,
but instead

1437
01:27:05,653 --> 01:27:08,523
we have like a conditional

1438
01:27:09,023 --> 01:27:11,125
on a policy, on a certain policy.

1439
01:27:11,759 --> 01:27:16,798
So normally that means that you,
you have some set of policies already

1440
01:27:17,232 --> 01:27:21,636
and you're just trying to decide
which of them is better.

1441
01:27:22,537 --> 01:27:25,707
So this could be done like using a

1442
01:27:25,707 --> 01:27:28,943
no cameras or not cameras a first
and then

1443
01:27:30,545 --> 01:27:34,949
just assessing the the one that you think
are best or just assessing all of them.

1444
01:27:34,949 --> 01:27:36,851
But it's that's impossible.

1445
01:27:36,851 --> 01:27:39,420
For instance, in a continue section
set up

1446
01:27:39,954 --> 01:27:45,159
where you can add two sets of possible
policies because the objections

1447
01:27:45,159 --> 01:27:48,296
are continuous, a third number
is infinite for every dimension.

1448
01:27:48,329 --> 01:27:51,065
So that's made infinite by infinite
and so on.

1449
01:27:51,199 --> 01:27:56,504
So it's it's a huge
a is a huge dimensionality in space.

1450
01:27:57,238 --> 01:28:03,111
So instead here we, we make the action
part of the or the inference process.

1451
01:28:03,111 --> 01:28:06,948
So we want
we want to have a separate model

1452
01:28:06,948 --> 01:28:10,551
that tells you to tell us what's
what's the action

1453
01:28:10,551 --> 01:28:13,655
to to take at every step. And

1454
01:28:14,622 --> 01:28:16,157
the I

1455
01:28:16,157 --> 01:28:19,193
what I said
that we obtain an actual entropy term

1456
01:28:19,827 --> 01:28:23,464
and that's because we in, in choosing

1457
01:28:23,731 --> 01:28:26,801
the best action so in, in trying to,

1458
01:28:26,801 --> 01:28:31,572
to match our actions to the one that we,
we should actually prefer, we,

1459
01:28:32,440 --> 01:28:36,978
we think it like we don't have
a preference this over action.

1460
01:28:36,978 --> 01:28:38,246
So we would.

1461
01:28:38,479 --> 01:28:40,982
So for instance, I, if I want to reach

1462
01:28:42,684 --> 01:28:44,285
a certain state environment

1463
01:28:44,285 --> 01:28:47,555
so if I want to to go from this room
to to the kitchen,

1464
01:28:48,189 --> 01:28:51,492
maybe I don't care what's the,
what's the shortest word,

1465
01:28:51,826 --> 01:28:54,962
the shorter passes, I just care about
getting there at some point

1466
01:28:55,630 --> 01:29:00,168
or I don't care about going left
or going round right now.

1467
01:29:00,802 --> 01:29:03,604
When I when I get off of my chair,

1468
01:29:03,604 --> 01:29:06,974
I just want to to go where I need to go.

1469
01:29:07,208 --> 01:29:10,411
So we don't
we don't place a prior over the action.

1470
01:29:10,411 --> 01:29:14,949
We just say whatever action is fine
as long as it brings you

1471
01:29:14,949 --> 01:29:19,287
the fastest as possible to the goal
that the fastest thing

1472
01:29:19,287 --> 01:29:23,791
is not even by the action itself,
but but by minimizing the free energy

1473
01:29:23,791 --> 01:29:27,795
so we don't want a preference
over the actions we wanted.

1474
01:29:28,396 --> 01:29:31,132
We want the free energy
to be the first spot.

1475
01:29:31,833 --> 01:29:35,403
And so the actions are we
some a unit for distribution over them

1476
01:29:35,603 --> 01:29:38,573
and what remains is just an entropy.

1477
01:29:38,906 --> 01:29:41,542
So it will be a real divergence between

1478
01:29:42,643 --> 01:29:43,878
the queue

1479
01:29:43,878 --> 01:29:48,049
over the action given the states
and the this party,

1480
01:29:48,049 --> 01:29:51,953
which basically becomes an entropy value.

1481
01:29:51,953 --> 01:29:54,956
If you, if you you're some user constant
because it's just

1482
01:29:54,956 --> 01:29:58,259
subtracting a constant to that and the.

1483
01:29:58,259 --> 01:30:01,462
Yeah, the switching to the to the other

1484
01:30:02,764 --> 01:30:03,664
part of the equation.

1485
01:30:03,664 --> 01:30:08,903
So what does it mean to us
this discourse center here.

1486
01:30:09,237 --> 01:30:13,808
So is adding constant term useful.

1487
01:30:13,808 --> 01:30:20,014
So I don't know if there's any other
useful custom term that we could get.

1488
01:30:20,214 --> 01:30:24,819
So the mathematically speaking,
having a constant

1489
01:30:26,220 --> 01:30:29,223
having gone up from an upper bound
because of the constant

1490
01:30:29,457 --> 01:30:32,460
building beyond me, because you're
minimizing the same objective.

1491
01:30:32,593 --> 01:30:36,063
But then on top of that,
we apply the contrastive,

1492
01:30:37,365 --> 01:30:42,236
the contrastive approximation
and that leads to another

1493
01:30:42,670 --> 01:30:46,340
the upper bound and the as I said,
there are some implication of this.

1494
01:30:46,340 --> 01:30:49,410
We get to maybe
a better representation, but

1495
01:30:50,411 --> 01:30:53,448
are we getting farther from the
from the actual objectives?

1496
01:30:53,648 --> 01:30:56,818
So from my point of view,

1497
01:30:57,318 --> 01:31:01,722
as long as we achieve
something that is actually better,

1498
01:31:01,923 --> 01:31:06,360
it doesn't really matter how far are we
from the actual surprise and see now.

1499
01:31:06,961 --> 01:31:11,766
So in any case,
we will always get some kind of

1500
01:31:12,733 --> 01:31:15,536
amortization or approximation

1501
01:31:15,903 --> 01:31:20,341
and we will probably never get 100%

1502
01:31:20,341 --> 01:31:22,844
close to there to the surprise value.

1503
01:31:23,344 --> 01:31:26,714
So because we don't have a perfect model,
so a perfect model of the word

1504
01:31:26,714 --> 01:31:30,485
doesn't exist, it's it's impossible
to imagine that we can model every

1505
01:31:30,952 --> 01:31:33,855
details of the environment,
even if we are with a billion

1506
01:31:34,255 --> 01:31:35,389
machine learning parameter.

1507
01:31:35,389 --> 01:31:39,360
And it's impossible
to think that we will always act

1508
01:31:39,360 --> 01:31:43,664
perfectly and always get in the,
in the perfect route to go

1509
01:31:44,765 --> 01:31:47,702
using the always optimal action,

1510
01:31:47,702 --> 01:31:50,371
especially because there is always
some uncertainty in the environment

1511
01:31:50,972 --> 01:31:53,941
and there's a lot of things

1512
01:31:53,941 --> 01:31:57,912
that we, we normally want to ignore
in our everyday lives.

1513
01:31:57,912 --> 01:32:00,715
So a lot of things
there is not actually important

1514
01:32:00,715 --> 01:32:06,454
to the capturing the word model or action
y it's not always important

1515
01:32:06,487 --> 01:32:09,690
beyond just percentile
correcting over movement

1516
01:32:09,690 --> 01:32:12,627
and so in the action,
we think the important thing is that we,

1517
01:32:12,960 --> 01:32:13,961
we get close to the goal.

1518
01:32:13,961 --> 01:32:17,932
So I will say, yeah,
if there are any other term

1519
01:32:17,932 --> 01:32:19,800
that we at the other center

1520
01:32:19,800 --> 01:32:23,404
or just any other modification
that we could do to there

1521
01:32:23,404 --> 01:32:26,340
to the free energy function
or that actually leads to better results

1522
01:32:27,174 --> 01:32:30,344
without compromising the original goal
of minimizing the energy.

1523
01:32:30,344 --> 01:32:33,881
I think that's that would be a good way

1524
01:32:33,881 --> 01:32:36,984
to uh, to address

1525
01:32:37,285 --> 01:32:38,252
some of the issues

1526
01:32:38,252 --> 01:32:41,522
that we, that we currently have with,
we've actually been, for instance,

1527
01:32:42,089 --> 01:32:46,594
that could actually lead to,
to improving the performance of the,

1528
01:32:46,594 --> 01:32:48,930
the artificial implementation
of active inference

1529
01:32:49,931 --> 01:32:50,865
significantly.

1530
01:32:50,865 --> 01:32:53,701
So that's also why I think that's

1531
01:32:54,101 --> 01:32:57,104
taking advantage of some lessons that,

1532
01:32:57,338 --> 01:33:01,409
that we learned from reinforcement
learning is actually useful

1533
01:33:01,442 --> 01:33:05,479
in acting inference as well because the
there has been a ton of research

1534
01:33:05,479 --> 01:33:08,549
about the way to amortize this thing or

1535
01:33:08,549 --> 01:33:12,119
approximating something better or

1536
01:33:12,119 --> 01:33:16,624
train a better deep learning model for
some solving some very specific aspect.

1537
01:33:16,624 --> 01:33:20,962
And I think the objective inference
research there

1538
01:33:21,295 --> 01:33:24,298
to benefit from this,
to take inspiration from this.

1539
01:33:25,533 --> 01:33:27,234
And maybe if I make that's good.

1540
01:33:27,234 --> 01:33:31,005
Or can you go back to the slide
with the action

1541
01:33:31,005 --> 01:33:33,674
uh. It's one.

1542
01:33:34,809 --> 01:33:35,242
Yeah.

1543
01:33:35,309 --> 01:33:39,213
So, so maybe to make it easier here for

1544
01:33:40,114 --> 01:33:43,517
or maybe people
that are less familiar with reinforcement

1545
01:33:43,517 --> 01:33:47,355
learning but are coming
from a more active instance background.

1546
01:33:47,355 --> 01:33:49,357
So in, in terms of

1547
01:33:50,658 --> 01:33:51,692
active inference,

1548
01:33:51,692 --> 01:33:55,029
but on a scale
first thing, let's look at it.

1549
01:33:56,430 --> 01:33:58,833
This is basically something
that you should not do

1550
01:34:00,301 --> 01:34:00,768
source.

1551
01:34:01,035 --> 01:34:03,638
So so basically

1552
01:34:05,840 --> 01:34:09,377
what happens here
is that we see action inference

1553
01:34:09,377 --> 01:34:14,582
more like a habitual thing,
like a, a moment the state

1554
01:34:14,582 --> 01:34:17,551
or I think I'm in the state, so therefore

1555
01:34:17,818 --> 01:34:21,288
I can just infer my action results.

1556
01:34:21,589 --> 01:34:25,292
But it's kind of you become habituated.

1557
01:34:25,292 --> 01:34:29,230
So I explained this hundreds of times
and it's always this outcome.

1558
01:34:29,230 --> 01:34:32,099
So I just stop planning
and I just amortize

1559
01:34:32,366 --> 01:34:37,038
this action into in the more playful C
So that's basically the kind of

1560
01:34:37,038 --> 01:34:40,174
the mechanism
that that we apply here in order

1561
01:34:40,174 --> 01:34:43,277
to avoid planning all the time

1562
01:34:43,978 --> 01:34:46,847
because that's, that's the,
the tricky part.

1563
01:34:48,282 --> 01:34:51,118
We have too much options to plan
so we don't want to do this.

1564
01:34:51,352 --> 01:34:54,555
So we just say let's amortize it
from stuff,

1565
01:34:55,356 --> 01:35:00,361
which basically means that this
A, this or Q or a broken procedure.

1566
01:35:00,361 --> 01:35:02,863
It's not only over stays
but also over actions.

1567
01:35:03,330 --> 01:35:06,534
And then so the action entropy
just falls out

1568
01:35:06,901 --> 01:35:10,771
by introducing the,
the action in the user.

1569
01:35:11,038 --> 01:35:15,042
So it's not that we
magically add an action action and return

1570
01:35:15,376 --> 01:35:16,343
to the simulation.

1571
01:35:16,343 --> 01:35:19,947
It just comes out because of having

1572
01:35:20,581 --> 01:35:26,220
the actions esports over there
but so did the my, the, this also means

1573
01:35:26,220 --> 01:35:31,692
that we have an approximate there
over actual selection and this works

1574
01:35:32,893 --> 01:35:33,427
in at

1575
01:35:33,427 --> 01:35:36,597
least three or four string problems
because they're.

1576
01:35:37,531 --> 01:35:41,736
Yeah, your goal is always
the same, doesn't it shift.

1577
01:35:42,703 --> 01:35:45,606
It's not a, it's also not a

1578
01:35:47,241 --> 01:35:50,678
if you, if you think back on on
biological agents look like a complex

1579
01:35:50,678 --> 01:35:53,848
distribution to maintain homeostasis
basically just yet.

1580
01:35:54,081 --> 01:35:55,149
This is the reward.

1581
01:35:55,149 --> 01:35:57,451
This is very good. It's
it's always the same thing.

1582
01:35:57,451 --> 01:36:02,189
So this basically means that
your environments very enriched with this

1583
01:36:02,223 --> 01:36:06,761
this agency which also reinforcement
learning solutions to their agents

1584
01:36:07,027 --> 01:36:09,563
is exactly the environment is used for

1585
01:36:11,565 --> 01:36:14,401
I can amortize what what I have to do

1586
01:36:14,735 --> 01:36:17,905
because if I know my status
I know they have to do basically

1587
01:36:19,140 --> 01:36:23,277
things so change I guess
if you have another environment in which

1588
01:36:23,744 --> 01:36:26,814
this is not the case
where you could be in a certain state

1589
01:36:26,814 --> 01:36:31,418
and you would still have multiple options
to do and you and you can only

1590
01:36:32,887 --> 01:36:33,854
you can only

1591
01:36:33,854 --> 01:36:37,758
really know what to do
by planning ahead through by first.

1592
01:36:39,160 --> 01:36:42,696
So search for information
on what's happening

1593
01:36:42,997 --> 01:36:46,267
and in these kind of environments,
I, I think that's

1594
01:36:46,634 --> 01:36:49,503
what position treatments
will help you a lot.

1595
01:36:49,503 --> 01:36:52,139
Are you going to do it
just by amortizing.

1596
01:36:52,406 --> 01:36:55,009
So it's, it's a trick device to,

1597
01:36:55,276 --> 01:36:58,512
to allow it to work
in this kind of environment because you

1598
01:36:58,913 --> 01:37:01,649
you have to benchmark against some things

1599
01:37:01,649 --> 01:37:04,685
and you have to be on board there.

1600
01:37:05,119 --> 01:37:06,320
It's so

1601
01:37:07,388 --> 01:37:10,991
keep in mind it's not the silver bullet
that will always work.

1602
01:37:11,592 --> 01:37:14,595
We do deviate from vanilla extract

1603
01:37:14,595 --> 01:37:18,132
insurance here costs
section inference s like

1604
01:37:18,399 --> 01:37:20,968
we just want to learn habits
you don't want to plan.

1605
01:37:21,769 --> 01:37:25,472
But this also means that there might be
situations where it will not work

1606
01:37:26,273 --> 01:37:28,876
and then it's not due to active insurance

1607
01:37:28,876 --> 01:37:31,779
or for the energy principle
that's not working.

1608
01:37:31,912 --> 01:37:34,982
It's more like the
the crude approximation

1609
01:37:34,982 --> 01:37:38,185
here by which things might not working.

1610
01:37:38,185 --> 01:37:38,385
Well.

1611
01:37:40,588 --> 01:37:42,523
That's interesting

1612
01:37:42,523 --> 01:37:47,328
about which training environments favor,
what kind of algorithms and then how

1613
01:37:47,328 --> 01:37:52,800
that shapes the perception of different
algorithms like the navigation task.

1614
01:37:52,800 --> 01:37:55,469
What if there was a fuel tank
or there was a

1615
01:37:56,971 --> 01:37:59,406
larger space
that was going to require like multiple

1616
01:37:59,406 --> 01:38:02,443
foraging information,
foraging trips, for example.

1617
01:38:02,877 --> 01:38:06,080
And so then the sort of single minded

1618
01:38:06,247 --> 01:38:08,382
seeker is going to

1619
01:38:09,416 --> 01:38:11,986
just die fast,
but then something that's able

1620
01:38:11,986 --> 01:38:15,155
to actually engage in planning wouldn't.

1621
01:38:15,155 --> 01:38:19,526
So that was a little bit like to those
who are familiar with active inference.

1622
01:38:19,526 --> 01:38:22,062
And then here's a variance
on what we've seen before.

1623
01:38:22,663 --> 01:38:26,667
How about for those who are more familiar
with the dreamer architect

1624
01:38:26,800 --> 01:38:28,469
or reinforcement learning?

1625
01:38:28,469 --> 01:38:32,973
What makes active inference,
active inference, and how is it different

1626
01:38:38,312 --> 01:38:39,179
Well.

1627
01:38:39,613 --> 01:38:40,614
I think

1628
01:38:43,384 --> 01:38:44,485
they are

1629
01:38:44,919 --> 01:38:47,554
they're largely similar, let's say.

1630
01:38:47,655 --> 01:38:49,657
I think that would be the starting point
because

1631
01:38:50,424 --> 01:38:53,160
both awesome people think about
what's the difference.

1632
01:38:54,328 --> 01:38:57,331
But I think the the the main point
that we should

1633
01:38:57,831 --> 01:39:02,269
rather stress more as an active interest
community is that there are a lot

1634
01:39:02,269 --> 01:39:06,774
more similarities between reinforcement
learning and active inference.

1635
01:39:07,107 --> 01:39:07,975
I would say that

1636
01:39:07,975 --> 01:39:11,011
active inference is a bit more general
than reinforcement learning.

1637
01:39:11,278 --> 01:39:13,480
In the sense that on the one hand

1638
01:39:16,283 --> 01:39:18,953
we don't use the reward function per say,

1639
01:39:19,320 --> 01:39:24,058
but we relax that the bit as in
we just have a distribution over

1640
01:39:24,058 --> 01:39:27,061
preferred outcomes, which is a bit

1641
01:39:27,061 --> 01:39:29,630
more general, I would say.

1642
01:39:30,164 --> 01:39:32,900
And then the second thing
is that instead of

1643
01:39:34,001 --> 01:39:36,937
starting off from the free energy
principle as in

1644
01:39:37,471 --> 01:39:42,309
this is the objective that we want to
minimize you also gets the,

1645
01:39:43,344 --> 01:39:44,979
the extrinsic

1646
01:39:44,979 --> 01:39:48,482
value term here,
which is exactly the same thing

1647
01:39:48,482 --> 01:39:51,285
as for the reinforcement
learning agents optimize.

1648
01:39:51,585 --> 01:39:55,089
So if you only look at extrinsic value,

1649
01:39:55,489 --> 01:39:58,625
your agents will also do this.

1650
01:39:59,326 --> 01:40:02,363
But the, the,

1651
01:40:02,363 --> 01:40:07,001
the added value
I would say comes in the information

1652
01:40:07,001 --> 01:40:11,939
gain terms and this will only give you
an additional benefit

1653
01:40:12,573 --> 01:40:15,342
in environments
where there is information to date.

1654
01:40:16,343 --> 01:40:20,280
And this is not your typical
reinforcement learning environment.

1655
01:40:21,015 --> 01:40:23,250
But if you look at, for example,

1656
01:40:24,418 --> 01:40:28,922
the teammates mouths from golfers,
then these are typical environments

1657
01:40:28,922 --> 01:40:32,159
where you can actually show
that if you only go for extrinsic value

1658
01:40:32,793 --> 01:40:36,096
you won't you will be acting suboptimal.

1659
01:40:36,263 --> 01:40:40,834
So you can actually prove
almost that in some environments

1660
01:40:42,036 --> 01:40:44,071
over there
looking at the extrinsic value,

1661
01:40:45,305 --> 01:40:48,909
given the correct model
of the environment active inference rule.

1662
01:40:49,443 --> 01:40:50,444
I think the crucial

1663
01:40:51,512 --> 01:40:52,146
thing that

1664
01:40:52,146 --> 01:40:56,450
we need to research on is
how do you get to the of the cortex

1665
01:40:56,450 --> 01:40:59,219
or the optimal model of your results
enrich

1666
01:40:59,653 --> 01:41:03,123
by optimizing your expected actually

1667
01:41:03,724 --> 01:41:07,728
you're doing a sensible planning
and this is still largely unresolved

1668
01:41:08,762 --> 01:41:09,630
with our models.

1669
01:41:09,630 --> 01:41:12,332
We are taking steps in that direction.

1670
01:41:12,633 --> 01:41:15,702
But as you can see,
there are lots of issues

1671
01:41:16,270 --> 01:41:18,405
to just find the correct model

1672
01:41:19,006 --> 01:41:22,876
because if you just look at the models
the likelihood based model

1673
01:41:22,876 --> 01:41:24,445
should be perfectly fine,

1674
01:41:24,445 --> 01:41:26,947
but based by the way
you optimize in practice,

1675
01:41:26,947 --> 01:41:28,482
then you see all kinds of problems like,

1676
01:41:28,482 --> 01:41:32,486
okay, this, this, this little pixel
is actually the most important pixel.

1677
01:41:32,486 --> 01:41:34,188
Look over the thing.

1678
01:41:34,188 --> 01:41:37,658
And that does not appear
so in my lost function.

1679
01:41:37,658 --> 01:41:39,860
So that's why everything collapses.

1680
01:41:39,860 --> 01:41:44,131
So in theory, it should work,
but there are a lot of practical problems

1681
01:41:44,231 --> 01:41:47,534
to find the correct model
that pays attention

1682
01:41:47,534 --> 01:41:51,271
to the correct details
or to correct your observations.

1683
01:41:51,438 --> 01:41:54,575
And this is still that
this is something that is shared with

1684
01:41:55,075 --> 01:41:59,246
model based reinforcement
learning as well as active inference

1685
01:42:00,047 --> 01:42:04,384
and I think there is a
huge opportunity to find new

1686
01:42:05,419 --> 01:42:06,153
techniques that

1687
01:42:06,153 --> 01:42:09,356
can and put forward both both fields.

1688
01:42:10,023 --> 01:42:12,526
And we also showed this
like the contrastive drama

1689
01:42:12,793 --> 01:42:14,828
in the districting environment. Also

1690
01:42:17,164 --> 01:42:19,032
improves performance
on the normal dreams.

1691
01:42:19,032 --> 01:42:23,971
So by having a technique
that lets you build a better model,

1692
01:42:24,238 --> 01:42:27,975
any model based algorithm will work.

1693
01:42:28,475 --> 01:42:31,478
And effective inference
has this special notion

1694
01:42:31,478 --> 01:42:36,250
of also take into account information
gain in environments

1695
01:42:36,350 --> 01:42:40,354
where you might be denser
to ensure your status.

1696
01:42:41,421 --> 01:42:43,490
So that's that's where it can prevail.

1697
01:42:43,490 --> 01:42:47,294
But I think in most of the benchmark
environments

1698
01:42:47,361 --> 01:42:49,930
of similar days,
especially in machine learning,

1699
01:42:50,964 --> 01:42:52,299
you probably don't need these external.

1700
01:42:52,299 --> 01:42:56,270
So you probably get away
with just maximizing rewards

1701
01:42:56,970 --> 01:43:01,275
which is in fact
also an active inference agents.

1702
01:43:01,375 --> 01:43:06,013
And to some sense, if of course
if it's if it's I'm talking about a model

1703
01:43:06,013 --> 01:43:09,816
based like dream agents
in this case, of course, the model three

1704
01:43:10,884 --> 01:43:12,386
ones are

1705
01:43:12,886 --> 01:43:14,221
these are the differences.

1706
01:43:14,221 --> 01:43:16,356
They, they don't need to model the goal.

1707
01:43:16,623 --> 01:43:19,826
But at least for mobile based
reinforcement learning agents, I think

1708
01:43:20,294 --> 01:43:22,996
it's pretty similar
to what a negative influence to

1709
01:43:23,764 --> 01:43:25,632
in these environments

1710
01:43:28,902 --> 01:43:29,169
Thank you.

1711
01:43:29,169 --> 01:43:31,338
Tim Pietro, anything you'd add to

1712
01:43:34,174 --> 01:43:35,142
yeah.

1713
01:43:35,142 --> 01:43:38,512
I would like to

1714
01:43:39,713 --> 01:43:41,448
to discuss

1715
01:43:41,448 --> 01:43:45,018
one aspect of three merit
that's been a bit overlooked

1716
01:43:45,085 --> 01:43:48,488
is the fact that it makes this

1717
01:43:49,122 --> 01:43:52,993
amortization similar and it's also
similar to what we have done.

1718
01:43:52,993 --> 01:43:57,030
So yeah, this is this we learn a policy

1719
01:43:57,030 --> 01:44:00,267
basically we own an action network
that provides the correct state

1720
01:44:01,001 --> 01:44:03,537
in the correct action
for every state, but

1721
01:44:04,771 --> 01:44:06,773
it is the key step

1722
01:44:06,773 --> 01:44:11,078
to actually brings us closer to the acted
in principle relation is that we,

1723
01:44:11,878 --> 01:44:15,515
we imagine several steps in the future.

1724
01:44:15,515 --> 01:44:19,253
So it is true that we don't

1725
01:44:19,853 --> 01:44:23,190
evaluate and long policies

1726
01:44:23,190 --> 01:44:26,493
in the over
other times that we have these

1727
01:44:28,161 --> 01:44:29,229
prior

1728
01:44:29,229 --> 01:44:32,366
about
action it is given by our action network.

1729
01:44:33,233 --> 01:44:36,370
But it is also true
that given the fact that we

1730
01:44:37,804 --> 01:44:39,573
evaluate the state three

1731
01:44:39,573 --> 01:44:41,708
we expect to see and then from there

1732
01:44:42,776 --> 01:44:45,412
we re start doing the

1733
01:44:47,180 --> 01:44:48,849
the actual optimization process.

1734
01:44:48,849 --> 01:44:53,587
We actually get closer to the
optimization scheme of active importance.

1735
01:44:53,587 --> 01:44:55,922
In particular, there is a

1736
01:44:55,922 --> 01:44:59,626
paper called Sophisticated Inference
that discussed this when you,

1737
01:44:59,926 --> 01:45:04,298
when you actually take an action
and then you reimagine

1738
01:45:04,698 --> 01:45:07,434
from that step what's going to up
what's going to happen.

1739
01:45:07,434 --> 01:45:12,139
There are some implication of this,
but that we are not completely drifting

1740
01:45:12,873 --> 01:45:16,543
away from the
from the original activity story

1741
01:45:16,543 --> 01:45:21,815
because of this is just a different way
of doing the action selection process.

1742
01:45:21,815 --> 01:45:25,786
Similar in that indeed
the three results are very close to

1743
01:45:26,386 --> 01:45:27,721
the active inference itself.

1744
01:45:32,159 --> 01:45:34,161
Poole Thank you.

1745
01:45:34,161 --> 01:45:39,866
I wrote down, if you don't know where
you prefer to go, your lost drive fast.

1746
01:45:39,866 --> 01:45:42,502
If you know how to get there,
figure it out if you don't

1747
01:45:42,903 --> 01:45:45,872
and then re-assess continually.

1748
01:45:45,872 --> 01:45:49,142
And I hope that conveys
some of the similarities and differences

1749
01:45:50,243 --> 01:45:52,446
Do you have any final comments

1750
01:45:56,149 --> 01:45:57,651
This is a very

1751
01:45:57,651 --> 01:46:01,621
interesting line of research and
we really appreciate this model stream.

1752
01:46:02,422 --> 01:46:04,758
Hope to see you in the future.

1753
01:46:04,758 --> 01:46:07,060
Or should I say we expect and prefer it.

1754
01:46:07,060 --> 01:46:09,062
But thanks again, Pietro and Tim.

1755
01:46:09,129 --> 01:46:10,831
This is really awesome.

1756
01:46:11,765 --> 01:46:12,165
Welcome.

1757
01:46:12,165 --> 01:46:13,834
Thanks for having us.

1758
01:46:14,067 --> 01:46:15,268
Thank us.

1759
01:46:15,268 --> 01:46:16,770
Have a good day, everyone.

1760
01:46:16,770 --> 01:46:18,238
Have a good day, everyone.

1761
01:46:18,238 --> 01:46:18,739
Have a good day.

1762
01:46:18,739 --> 01:46:21,708
Everyone, have a good day.
Everyone, have a good day.

1763
01:46:21,708 --> 01:46:24,678
Everyone, have a good day.
Everyone have a good day.

1764
01:46:24,678 --> 01:46:27,647
Everyone have a good day.
Everyone have a good day.

1765
01:46:27,647 --> 01:46:29,116
Everyone have a good day.

1766
01:46:29,116 --> 01:46:30,951
Everyone, have a good day, everyone.

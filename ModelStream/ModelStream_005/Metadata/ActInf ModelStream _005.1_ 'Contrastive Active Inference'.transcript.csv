"Speaker Name","Start Time","End Time","Transcript"
"Speaker 1","00;00;08;01","00;00;41;12","Okay. Hello and welcome everyone. It is January 28th. 20, 22. We're here in Acton Flat Model Stream number 5.1 with Pietro Marseglia and Tim Verboten. So this is going to be a model stream presentation and discussion on their recent work Contrastive Active Inference. We're going to have a presentation section and then discussion. So please feel free to ask any questions during the presentation that we can address in the discussion."
"Speaker 1","00;00;41;25","00;00;50;07","And Tim and Pietro, thanks a ton. We really appreciate you joining to share your work. So please take it away. And thanks again."
"Speaker 2","00;00;52;12","00;01;20;10","Yeah, thanks, Daniel, for inviting us as well. So I'm going to be able to get the better we are going to work on. Interesting of interest. So let me first set the scene. Why are we looking at active inference or basically our lab wants to build convergence agents? And so from that perspective, the studio and if you want to build something indulgent, it needs to be involved."
"Speaker 2","00;01;20;12","00;01;53;04","It needs to be interacting with its environments. And then most of those interactive instruments are basically your agents to your environment. It's interacting with. And you need to build the model, basically. So are first give an overview of active inference and the way that the three are and a little this material is also being covered in a previous middle stream, I think number three."
"Speaker 2","00;01;53;15","00;02;29;10","So if you want more details, you can think of that one again. And so then afterwards gets over and he will go to the nitty gritty details of the contrasting approach of inference. So let's get started. So basically active inference, it's a process theory of the brain. And basically it says that your brain or your agents build or you build this model of the environment, which is basically the joins probability distribution over that observation."
"Speaker 2","00;02;29;15","00;03;00;16","So things that you can see or experience actions which we do know the state and then states or states of the universe. So basically you have your agents that separate from the environment and its actions interact with the environment. And this gives rise to new observations. And so the idea of the ball is agents for out, which are kind of the hidden states."
"Speaker 2","00;03;01;02","00;03;30;21","That's the change by actions and the different ways to my observations. And if you can build such a model, then basically this enables the agents to plan some actions to to bring the agent to some preferred observations or outcomes and so forth. But so the crucial bits is basically how do you get this model of what happens if I do my actions?"
"Speaker 2","00;03;31;01","00;04;05;13","How does this influence the states and how does this influence the outcomes that I see? So the crucial for from experience is twofold. First of all, the key is what the agent does in a decision optimizing so-called free energy, which is a profound surprise, a prediction error. So basically, if Joe, this model allows the agent to predict the outcomes, that it will see that true witness and the better this match your actual observations, the more happier you are as an agent."
"Speaker 2","00;04;07;05","00;04;54;10","And crucially, you will also select the actions that will minimize the frequency you expect in the future. And so we think of this to the models just to to set the scene. On the one hand, the reason why so that we all know for the system is for this also to then see the move the jet will make from the let's see value act of influence presentation thoughts more countries the formulation of the of interest free energy object So we start off with setting to see what the genesis model so it's it's a bit laid out the diagram of the agents and the environments that was on the previous slide."
"Speaker 2","00;04;54;11","00;05;20;02","So basically this unfolds over time. So you are in certain states that gives rise to certain observation and then given an action on your previous state and basically move ahead to the next stage and this process unfolds over time. And you can see some of the circles are colored gray. And these are basically the things that you can observe."
"Speaker 2","00;05;20;20","00;05;44;12","So, you know, the actions, if you did that until now, as you know, the observations that you saw until now and all the rest is basically for you to infer. So you can only infer it in states until now, but you can also try to infer the hidden states of the future. Actions that you want to take or d'observation that you will experience."
"Speaker 2","00;05;45;02","00;06;12;29","And so basically that the so-called junk model joined distribution over the sequence of observation states. And action is then basically characterized as follows. So you have propriety over actions you have a license model. So so yeah. So your prior overreaction. So this basically versus what is appropriate if you take certain actions or time you have some transition probabilities."
"Speaker 2","00;06;12;29","00;06;52;18","So what is the probability that I will transition to this next stage given my previous state and this and you have the accurate model, which basis? Yeah, given the state I am which observation, I will see. And so it is basically the versus the so-called Markov assumption. That's your observation that you see at this time that it only depends on your hidden state and it does not really directly depend on anything else because if you know your if your your current in state, then you know your observation, you will see so that basically what is reflected here."
"Speaker 2","00;06;53;25","00;07;23;05","But of course, as an agent having this model this allows you to still to to assess how likely is a sequence of observations, for example, and allows you to predict given these actions what will happen with one crucial but of course is still the inverse of this model. Given that I saw these observations that are these actions, which is my current state, and this is basically non-trivial."
"Speaker 2","00;07;23;24","00;07;51;28","So even if you have the exact model that you refer to as this is typically intractable. So that's why it makes of inference. You resort to variation inference that you just say, okay, I just assume that I can build a model, the so-called for, for example, your model. And this is the thing that will tell you, given certain observations, what is my probability to begin a sort of this what's depicted here?"
"Speaker 2","00;07;51;28","00;08;42;28","So you do this. Q And Q is basically a variation of your distribution contours and you just say, okay, given some observation I want to have the best estimates for the state I am in. And the reality principle just sits, if that's what you want, then this is and you just optimize the free energy, which is no pressure and basically expectation over states and generated by your your expectation over to the difference between the what an accurate or your approximate posture and look like use false the drive model and if you can minimize deaths, that basically means that you will have the best explanation for any observations you see."
"Speaker 2","00;08;43;09","00;09;08;18","But at the same time, you also have the best approximate posterior for the true and you're not going to go to the whole derivation of this flow but basically you can convert this through to the second regression line. And this is the one that we use most often in our models which is basically the virgin between this and the approximate posterior."
"Speaker 2","00;09;08;29","00;09;39;00","So basically what is the state I am given your observations that so and described as basically just this is my, my guess that I am in the States given the previous state elections. So I don't know the observation but I want to test this without my observation. And if I see your observation, I don't want my beliefs to completely switch because the probability there's something wrong with my model."
"Speaker 2","00;09;39;20","00;10;12;09","And then you have the second term, which is the, the looks like this basically the accuracy of your model or how good are you at reconstructing the outcomes. And so this is all form to false basically or control your current events that so you know, your observations do so now you can validate the energy and then you can be a predictor model in order to minimize this thing."
"Speaker 2","00;10;12;24","00;10;35;13","But of course you also need to know your actions. You want to get to the future. And so if you look into the future, then talk about the expected energy. So here we didn't we we use by the shorthand for the sequence of actions into the future that also switched from D to just to denote that we are talking about future type steps basically."
"Speaker 2","00;10;35;13","00;11;15;12","But the same with the important business now that in the expectation now you don't have expectation it's only over states with also over outcomes because you couldn't sense your observations yet. So you don't know them so you can just do an expectation over anything that could happen. And, and then the move that is made in the conference is basically that's the on hand for the term which they call the instrumental value or realizing preferences."
"Speaker 2","00;11;15;24","00;11;48;19","And it's basically stating that as an agents for future outcomes they have some prior they think that they will regardless what happens that I think I will realize it's kind of your your preferred outcomes let's say can also cross this more like homeostasis. So my body temperature can be 37 degrees Celsius. So my expectation before knowing anything is that it will be 37 degrees and hence I will act in order to make it so."
"Speaker 2","00;11;48;19","00;12;13;27","So that's a bit reflexive here that disregards the the dependency on your reaction that you just say, okay, my prayer is that this is what I expected. So this becomes the instrumental value. And then the second, the solution is basically that your approximate just your model is basically very, very close to the studio so that you have a good approximation."
"Speaker 2","00;12;13;27","00;12;51;11","Then you can reside in the second term, which basically means that you have on the one hand and the third that's says, this is my belief or the state given injections I will do. And the other thing is also a belief about future states have an actions and some certain outcomes that I expect to see. So basically it says what would be the information that I gets from looking for certain outcomes."
"Speaker 2","00;12;51;28","00;13;14;25","And it's it's kind of in a systemic value or information gain which can drive you to explore. Basically, you want to have if, if you don't know how to get to your preferred state, at least you want to get to state as to states that give you more information on where you are and go, first of all, often says it's like the owl that needs fruits."
"Speaker 2","00;13;15;06","00;13;43;04","So what do you do with the first order you search for pre first. And so the epistemic value is basically searching for prey. It's like where should I go to do to get more information on this and then once you know where this then you can realize your preferences and the thought is that there was a pretty risk so how does the action selection works then?"
"Speaker 2","00;13;43;17","00;14;14;25","Well, basically you want to select the actions that minimized your expected strategy. So at each times, the first thing you do is you use your model to estimate your return. States like knowing which state may now given my latest observation, then you can evaluate the expected free energy for for each of also your plans for a future sequence of actions you can evaluate and expect to be energy."
"Speaker 2","00;14;16;03","00;15;04;19","And then this basically results in a belief over policy. So you basically take the the mind, the negative energy, you're multiplied. It's this precision parameter which just states how yeah. How how much confidence you have. The energy is correct basically. And then you use this the suffix so basically it just it just says the the policies that have low expected tree energy are the ones that are most likely basically that's that's the only thing is from the source and then referred in this section, according to this, you just select the next section for the sequence that you think will really give you the the minimum energy and that side of course."
"Speaker 2","00;15;04;19","00;15;30;12","And then you take this action, you get a new observation and the process is and so one crucial point in all our work is that it all starts with this just following is a flexible and typically you can and you have a certain problem and you know how the problem looks like, what the observations are or what the hidden state might be."
"Speaker 2","00;15;30;21","00;15;53;14","And then you can really pinpoint and write down the exact model and so optimizing. But in our case, this is often the true issue. If you look at the robots that drives around and gets them right inputs, for example. Yeah, well, these are the states things that you need to track. How do you convert these pixels to the state space?"
"Speaker 2","00;15;53;22","00;16;22;03","So all these things are there. They're just not there. You have the goal in a work is, yeah, I can then completely start from scratch and learn this and for listening the use of neural networks to access, function, approximate or actually provide the smoothness, these models optimize the parameters of these neural nets also by minimizing the energy. That's that's great."
"Speaker 2","00;16;22;13","00;16;53;10","That's it. So how does it look like all this? Not will. So we start off with observations and actions so this can be but pixels basically so and and by matrix of numbers let's see actions which could be any action could your whole city or whatever your agent can do and these numbers are put into a neural net which we told you in order and this basically reflects this Brooks's posterior."
"Speaker 2","00;16;53;10","00;17;31;10","This is just saying given my previous action and my observation outputs probabilistic presentation which is basically the means and the variances of the very patients. And then we have a second rule that's rich called transition model. And this is then so yeah, well, what will happen if I do a certain action out of my state evolves. If I say to a certain action Now finally, we also have the decoder or the electric model, the then outputs given the State's observation."
"Speaker 2","00;17;31;10","00;17;59;18","So in case of an image, for example, this will generate you a new image. And the goal is of course to have the best predictions possible. So if you look at the energy forms, again, in this case it's again you have this like in the form which basically just says that during the output of the decoder. So to generate image, I just want to have this close to the actual image that you've been."
"Speaker 2","00;18;00;10","00;18;32;12","See, so it's just a reconstruction lost in terms of neural nets, let's say. And the second to sort thinking through on a scale killer verges between the distribution that you generate from the encoder and the distribution that you generate from the transition. Well, it's so we apply this on a number of cases, which also we're seeing in the previous model stream."
"Speaker 2","00;18;32;21","00;19;00;17","So just to give you some intuitions, the first thing was the multiple problems, just which is the basic control problem. So here the sensory input, that's the position you are you basically have to infer not only the position you're in, but also the momentum you have the most you have. And so you can see that on the right, you can see the model predicting all likely trajectories for going left or right."
"Speaker 2","00;19;00;29","00;19;20;13","And you can see how in the beginning it's not sure on the course. So it's it's very spread out and and what it will predicts. But the more information that gets, the more kind of collapses. Yeah, I'm pretty sure that this is the behavior this will happen. And then you can use this tool to drive the agents towards the preferred states."
"Speaker 2","00;19;20;13","00;19;45;02","In this case, the, the flag the second one was using the quarries or environments or here you get these observations that are now just pixels from from this game and the preferred state of the car will still be in the central metric. And so you can see how it actually infers the actions that will bring it to the central district."
"Speaker 2","00;19;46;12","00;20;28;15","And it might even cut corners in order to the reach of preferred states a bit faster And finally, we also have this fun on the robots navigating our lab where encrypted with the number of sensor mobility so you can see the camera, but also from tracing lighter and also very dark range Doppler. So the radar range Doppler basically gives you in the way access the range and in the x axis, the the velocity of the reflections, basically the Doppler and here you can see how in the beginning, if it has been a number of situations and then basically that the model imagine what would happen."
"Speaker 2","00;20;28;27","00;21;10;06","So these are real observations and now it basically imagines what it will see if it turns around, for example, and just yet actually learns basic dynamics, basic behavior of of all the sensor model. So that's pretty cool So what are the limitations of this thing? Well, there are two core limitations that we address in the first one is that we use this this six X reconstruction both to learn the model, but also to define your preferred status like this is the image that you want to see and try to make that happen."
"Speaker 2","00;21;10;26","00;21;46;26","But the problem is that this great error in pixel in terms of pixels is not really the best metric. So, for example, if you have the the left image and you look to assist output an image is similar to that one. We have two examples you on the right and you can see that's the same image with some salt and pepper and those is actually scoring worse in terms of squared error than an image where the two the joint arm is actually incorrect."
"Speaker 2","00;21;47;10","00;22;14;18","So although in terms of behavior, the left one is better in terms of great error the right one is better. So that's of course problematic. If you want to control the arm, the words to go and on seconds limitations that if you need to evaluate the expected free energy for the huge number of potential trajectories, potential actions you can do, then of course this becomes intractable as the number increases."
"Speaker 2","00;22;15;06","00;22;40;24","And so the the ways that we go through this in the context of work is on the one hand, instead of using a pixel wasting constriction or just compressive learning, it sets out exactly this works with performance and in the next few sites. And then the second thing is instead of evaluating the expected the energy for all this, you basically amortize the forces actions."
"Speaker 2","00;22;41;01","00;22;55;14","So we also train neural nets outputs, actions, different stages. And so with that, we can now shift to get through. Google talked about the contrastive formulation of the expected benefits from."
"Speaker 1","00;23;02;12","00;23;03;01","Thank you, Tim."
"Speaker 3","00;23;05;16","00;23;47;10","All right. So thank you, Daniel, for having us. And thank you, Tim. You know, if you can hear me well, soon on my screen out Okay. All right. So now I would think about our recent work contrasted in France. So this work was recently published that noted that those in 31 very recently nice months. Let's start delving into it."
"Speaker 3","00;23;48;16","00;24;30;06","So the dissecting that we discuss in inductive inference is very similar to the reinforcement learning. One with the difference that in reinforcement learning, the old behavior learning is driven by rewards. So the agent receive reward function and the positive rewards should reinforce positive behaviors while negative. The rewards should penalize the agents to avoid those states and actions. However, one of the problems that comes with reinforcement learning is that in order to actually learn from rewards, you need a reward function."
"Speaker 3","00;24;30;08","00;25;08;22","And that's not always easy to have For instance, as you mentioned, especially when the state is not known in advance, so the agent don't exactly know its status is difficult. In that case, the design, the reward function, because you're not sure of what the agent knows and how it can assess its performance compared to the environment. So we instead focus on active inference in the inference the agents operates to that the principle of minimizing free energy as we have just so that the principle of minimizing free energy actually enables two things."
"Speaker 3","00;25;08;24","00;25;35;22","The one thing is to learn a model. The work We call this an artificial work model where work seems simple. And the other objective is to minimize the free energy future by trying to achieve some preferred outcomes of the agent. So we assume that the agent does some preferred outcome distribution that he wants to achieve and that this goal will be in the future to actually achieve these preferred outcomes."
"Speaker 3","00;25;37;29","00;26;07;05","So the the environmental setting we discuss is that one of the options piece of partially observable market is in process. So just to recap the observation that the agent receives, just infer the internal state of the environment which is not served. And then there's action which are actually known for the past, but the agent should infer or somehow choose among a set of possible action in the future."
"Speaker 3","00;26;08;21","00;26;40;13","So this is just a summary of what the artificial word module looks like, or as we've seen in the previous lights, we have an encoder that the information from observation we focus on visual environments. So here we have again an image which is basically owned by metrics, or the encoder could be, for instance, a convolutional neural network In our case, we have the the even state model with which takes the previous state and the previous action and in particular no work."
"Speaker 3","00;26;40;22","00;27;18;24","You use some some form of recurrent neural network model in order to keep preserving the history of the environment. And then we have the decoder that computes reconstruction of the information on the current state. So it tries to encode inside that in the States as much information not possible from what it comes from the from the observation. So the problem with reconstruction is that computing then especially in visual environments is is quite complicated because you need a big models that there are very good representational capacity."
"Speaker 3","00;27;18;24","00;27;46;18","And also the models cannot be 100% accurate. I think that low low dimensional settings because for instance, predicting an image pixelbypixel is practically very invisible. So it's very rarely happens. So let's call an example a year or so, a few a few weeks ago I was a I was training to be like models. So like a model similar to this one on the left."
"Speaker 3","00;27;46;18","00;28;29;18","So where we have the this encoder decoder architecture on that. Well, and I'm thinking the breakout game and try to to learn and even state to to learn action on top of the interstate. The problem is that the reconstruction of the V so we're actually pretty bad in that they we're losing very important information about the game. So for instance it was kind of able so we some uncertainty to model where the model of the game is but it wasn't able to model where the ball easy which is actually one of the two most important details in order to actually be able to play."
"Speaker 3","00;28;29;19","00;28;58;29","So even adding the reward function in this case so having the the the game score available and the agent wasn't able to learn the task because of of the state which was lacking the most important information in order to to keep improving. So this is one issue that we try to overcome in our work. And the second part of active interest involves learning to pursue the preferred outcome."
"Speaker 3","00;28;59;19","00;29;32;23","So in order to pursue preferred outcomes active in agent, the two things, one tries to minimize a the distance with respect to two disparate outcomes. But then the other way you also minimize the ambiguity with respect to the environment So normally this is done by trying to match the, the two distributions or trying to match, as we saw, even if we've scaled divergence to match the distribution of the match, the outcome with the preferred outcomes, distribution, whatever."
"Speaker 3","00;29;32;23","00;30;03;25","Again, in a dimensional setting, this can be quite complex because how do you define a distribution in an aid nation or here much could it be, for instance, just the center go around the, the big self. So we've the with the mean being the pixel value and then some fixed extend the deviation. But in that case we we get into troubles because we have the same issue discussed before we for instance have this kind of goal here and a noisy observation like this which actually adds an eye."
"Speaker 3","00;30;03;25","00;30;25;10","Another means square error compared to to an image that is very distant from the goal. And this kind of situation, especially when using reconstruction or in more realistic settings, are very, very likely because for instance, you can you can see that the center image is actually just a reconstruction of the model, which is not under percent accurate. So that could be the case."
"Speaker 3","00;30;25;10","00;30;51;18","And indeed, the agent could be confused and you will think that is not achieving the goal compared to maybe, for instance, a bus observation. But if it seemed that it was actually actually closer to the goal or again, when there is some some noise in the environment in real set up like the robotic, we always have this noise in the observation so it's our to to match our preferred outcome in, you know, even national setting."
"Speaker 3","00;30;51;20","00;31;17;21","So we we also try to to overcome this issue here So what we do propose is to use contrast learning a conservative learning is a is a mechanism popular in the in unsupervised learning scene that we will discuss more in depth in a few moments. So the the the objective that we want to where we want where there are to avoid reconstruction in learning the word model."
"Speaker 3","00;31;17;21","00;31;51;05","So we don't have any more of the decoder here as we see on the right. Then we want to be able to match preferred outcomes in the lower dimensionality space because we have seen that it's no dimensionality that's problematic. And also we we would like the this low dimensional state to be somewhat representative of the task so that when we match our goals in this low dimensional state, we are actually doing something that actually brings us closer to the actual preferred outcome that we we want to achieve in the high dimensional setting."
"Speaker 3","00;31;53;05","00;32;20;11","So let's let's try to compare to see what are the difference in between using the likelihoods to the inference model. And the contrasting models. So the idea in the likelihood actually inference model is that we want to maximize the accuracy of reconstruction. So basically this this means that we have this decoder that maximizes this maximum likelihood of the observation given the state."
"Speaker 3","00;32;20;19","00;33;00;08","So we want the state to to maximize the information that it contains about the observation. Basically, in contrast, learning we in contrast, deductive inference, we do something different. So instead of trying to reconstruct the current observation, we try to compress the encoder again, this observation and compare it to all of the other not all the other, as we will see in a while, because that's visible, but in many, many other samples that represent something different."
"Speaker 3","00;33;00;11","00;33;34;25","So that we in the, in the late in space, in this compressed space, we want our a state and the compressed image to be very close. And while this our state should be very distant from all the other hemorrhages. So we we are indeed maximizing the similarity with this and with the corresponding sample that this is here called the positive sample where we want to minimize the similarities and maximize the distance against all the other samples that are called negative samples in contrasting learning."
"Speaker 3","00;33;34;26","00;34;04;09","So as we'll see also in in a moment, this this mechanism here maximizes the lower bound of the mutual information. So we are basically trying to maximize the information in between corresponding observation in state while minimizing the information with respect to the all the other negative spurts so as we seen before, the the free energy bus can be summarized with this equation here."
"Speaker 3","00;34;04;09","00;34;32;17","So here I'm just talking about one time, one moment in this for ten steps. So instead the team presented for all sequences by using the field the notation. Well, here we're just considering one time steps the time. So as we as we have seen the free energy is basically are an upper bound on the appraisal information that we would want to minimize."
"Speaker 3","00;34;32;19","00;35;02;09","So we minimize free energy in order to minimize this appraisal of the agent. And the year is it's actually evident that we have this evidence on. So the 80 scale diverged into these old where there are equal to zero. We wanted to basically to to reach zero hypothetically in order to minimize these this evidence bound. And this can also be repeated in a way that is more practical implemented."
"Speaker 3","00;35;02;09","00;35;37;20","So by having the the likelihood of the observation given the state which actually means the accuracy of our model and that again the complexity of the model which is the close divergence between our variation variational distribution. Q Which we, we use. Q Oh, that's given all which is basically using the outgoing coding variation of relational posterior. So this is as typical as a stunning in variational encoders."
"Speaker 3","00;35;37;20","00;36;07;03","So when you when you try to infer the parameters of your posterior distribution by using the corresponding observation, and then we we want to minimize the close divergence between this out coded posterior and the our prior about the about the, the, the current or future states we can say given the boss state and actions and the in our case in particular we generally learn this this frame."
"Speaker 3","00;36;07;03","00;36;54;16","So we don't just use a uniform prior overstate but we will learn our years to be predictive of what the state is given the bus state. So it can basically seem like if we're machine learning in particular as a, as a conditional variation of encoder, we've, we've contrastive learning what we try to do is as I said, to maximize state similarity with the correct and corresponding observations over could sample while minimizing with the other and this means again that we want to maximize the neutral information between the positive sample and the corresponding state and minimize the information with the negative samples."
"Speaker 3","00;36;55;03","00;37;31;22","This can be written like this or the noise contrastive estimation. So noisy here. That's the abbreviation basically provides a again, a lower bound and the neutral information. So where we where we see that we basically have like a soft marks. So over the over all the the the observation is state what does it mean? So for each pair of state and observation, we run this volume."
"Speaker 3","00;37;31;22","00;38;02;12","So the value of this critique is that this critical function F to be as high as possible or we want it to be a very low respect to the other so that actually the exponential of these a compared to the sum of all the other exponential is either with the corresponding dissipation and very low with the rest. So this is basically saying that the if you want matching bears to be to be very close and distant bassoon to a very, very low value."
"Speaker 3","00;38;03;11","00;38;49;20","And the the this is lower bound is a is an approximation normally when we take a number of samples key from that a joined distribution that we define between X and Y in particular in our case is X and Y represent our observation and our in the States. So we define a priori these disjoint distribution to actually represent the fact that these state corresponds to this observation and we want to maximize the information in between them and the function F, x, y again is so so-called critique function."
"Speaker 3","00;38;50;03","00;39;19;06","So what does it mean is a function that should approximate the log density ratio that we see here on the right I won't go into the mathematical details of these, but basically is a mapping of the of the the two inputs so basically the mapping of our observation and our state and we want these again these outputs to be I for corresponding bursts and the and load for non corresponding bursts."
"Speaker 3","00;39;20;21","00;39;49;08","So how do we transition from the free energy of the path that we have seen to the to our contrastive formulation still what the the first step that we do is adding to the to the free energy functional as a term that we we assumed to be constant the despair, the entropy of the observation. So how can we assume that the entropy of the observation is constant?"
"Speaker 3","00;39;49;27","00;40;19;10","So in machine learning, we generally have a data set from which we sampled our observation about the past. So we assume when we train that our data set so that the distribution of the observation is fixed so the entropy of this distribution will always be a constant because we can modify that. I suppose for instance, to the States which which means that so the distribution of our outcomes cannot be modified."
"Speaker 3","00;40;19;10","00;41;01;26","And so its entropy is a constant. If we add disturbance to the free energy functional, we can rewrite it as a, as the coiled divergence minus this information gain or mutual information here between the states and the observation. So given this, we can now apply the fact that we in contrast, keep learning a functional this is a lower bound on the mutual information to actually derive the the free energy of the past where expecting all the terms result according to the previous light nets."
"Speaker 3","00;41;02;14","00;41;45;20","We basically have, again, this coiled divergence there. And then we have this the value of F between all and this to maximize and then to minimize all the again the the value of the function of with respect to all the other negative spurts So this brings those two to again an upward bound on this appraisal term. We see that this upward bone is actually even higher than the normal free energy, upper bound bus as we'll see later."
"Speaker 3","00;41;45;20","00;42;22;12","These are some nice property. Does it explicitly help us to get rid of the reconstruction and to learn a different representational space? It does some advantages compared to the to the likelihood based representation So let's now talk about how can we learn to behave using constructive inference. So in the likelihood based active inference model, what we were trying to maximize was again, the likelihood of the future observation, but under the preferred distribution."
"Speaker 3","00;42;22;21","00;42;49;22","So we want to we want the imagined outcomes to be as close as possible to the two that outcomes that we prefer a So these for these are environment in place that we we reconstruct what the what we imagine will happen in the dimensional space. So that emerge basically. And then we compare it to our preferred image, for instance."
"Speaker 3","00;42;50;10","00;43;17;14","And as we see before we can for instance, use a mean squared error distance or we can just use like we can just go down and compute the likelihood under the preferred the distribution or contested up to the inference. We again instead use a contrasting mechanism when we want now the the future state to be corresponding with our samples from the preferred distribution."
"Speaker 3","00;43;17;14","00;43;40;09","So we want the, the outcomes that we prefer to actually be a close to there to the state that we imagined so that now we don't need anymore to reconstruct what the what the outcome of our action will look like. But we can just say is the, the state that we match in matching with the preferred outcome that they want to achieve."
"Speaker 3","00;43;40;22","00;44;18;01","And the that's, that's what we, we maximize similarity with. And again, we also have some some form of the ambiguity minimization or epistemic value in trying to minimize the similarity respect to other outcomes. So in this case, minimizing the similarity in respect to outcomes that are not in the preferred distribution basically means that the other want to go far from something that you have already seen before in order to maybe get closer to the the preferred the outcomes, or you either just want to minimize your ambiguity."
"Speaker 3","00;44;18;01","00;44;48;05","So you want to be as far as possible from other outcomes in the schools as possible through that to the actual preferred outcome view that you're opting for. So as we have seen before, the expected free energy can be summarized like this. I'll first highlight some difference in respect to the equation the team present that. So first of all year we we take action to be part of the of their active entrance process."
"Speaker 3","00;44;48;05","00;45;27;24","So they aim for the process. Well, and before we have seen that, you can you can have a distribution of our policies and then you can you can sample the action from the policy and and compute the free energy of the future a given a posterior on your on a policy when I give them policy instead here we make the the actions part of the the generative models for the future and we actually want the agent to insert the injection for the future and not just compute them as a as a posterior of our over some distribution of the over the policies."
"Speaker 3","00;45;28;14","00;45;59;04","So we we have enough in the posterior is this 18 so we that we inferred both the the the future state and the future election and also the the prior other over the preferred outcomes that indicates we feel that I have this not confusing before because before the deal that was used to indicate sequences by in the paper I actually use it to to indicate the preferred outcome so yes notation issues but I hope there's not confusing."
"Speaker 3","00;45;59;04","00;46;39;26","So that is basically to say if this is the preferred distribution over observation state and explain and so this is basically our target distribution what we what we hope to achieve in the future and we can rewrite this as a as the sum of three terms. So we have this. So first of all I'm assuming that's the agent's us no prior preference in action so that for him any action that will bring it to the preferred outcomes it's fine so he has a uniform prior but action so the action doesn't really matter it is as long as it brings to there to the goal let's see."
"Speaker 3","00;46;40;15","00;47;30;28","And the so this in this way we we obtain an action and be and then the rest the intrinsic value is that is the same epistemic party that we've seen before. And so the the one that the obligation to to explore the environment more or either to reduce this ambiguity about the environment. And then we have the extrinsic value which is basically rewards or just just a way to get closer to the, to the actual preferred outcomes so that the value to pursue in order to minimize distance from the from the preferred outcomes in our competitive expected free energy, we we again to assume there moves as we did from the past."
"Speaker 3","00;47;30;28","00;48;13;19","So we assume that we are taking interpretation over our preferred outcomes since we don't imagine outcomes in the future, we just assume that the outcomes will will be according to the so the preferred outcome situation so that we can again sum the entropy over our fixed preferred outcome distribution. And then the steps are the same. So we, we have again this mutually information term between the preferred outcomes and the, the state that we mentioned in the future and the, the, the action and should be determined the scale divergence between the still other states and the other states."
"Speaker 3","00;48;13;19","00;48;40;28","So is a complex term I'd say, because it basically should represent and the difference between what the, what the agent believes it will happen and, and what is supposed to happen in the environment. So normally in active inference, we assume for the future that the the model of the world is correct so that the agent does not control of the over word model."
"Speaker 3","00;48;40;28","00;49;25;05","So it cannot change how the the environment dynamics will transition from one state to another. So I, I'm assuming year that this is this scale divergence in terms actually zero though I've seen that some work this could also be built left there are being greater than zero but then then it's basically adding the agent imagines that they can then it can violates the the environment dynamics hoping for a better dynamics that it will allow it to to be optimistic and think yet the thing that I imagine it will happen is actually going to happen so either we we don't allow the agent to to modify or the environment to move from one state to another and"
"Speaker 3","00;49;25;05","00;49;54;02","we just assume yeah these are the dynamics and betterment so our posterior over the in the transition dynamics of the environment is correct. And so the scale diverging in zero and then our objective again doing the applying the the contrastive the learning lower bound translates into this when we we have these contrasting mutually information between the preferred outcomes and the is this action entropy term."
"Speaker 3","00;49;54;07","00;50;22;29","So if you write out explicitly we again have this this to term which kind of reminds the two extrinsic value and intrinsic value for for the free energy. So we have this the term that actually there should minimize the similarity with the negative sample that is doing something similar to what the the intrinsic value in in active inference should do."
"Speaker 3","00;50;22;29","00;50;51;10","So basically are trying to to be distant from from previously seen their outcomes. This is kind of similar to explore the environment to minimize your ambiguity. So try to find something that gives you more information, not something that you have already seen and so the, the, the word model can be summarized in these three main components that we learn."
"Speaker 3","00;50;51;23","00;51;17;03","We have our prior network that, as I said before, is learned. And Schiller should learn the, the transition dynamics of the environment so trying to predict future states, given the US states and actions. And then we have this jury yourself that is shared between the the prior and the Posture Network. And this is what allows us to brings our history both."
"Speaker 3","00;51;17;20","00;51;43;09","So it just not stuck to the previous state, but also to include some information about previous states so that we have more, more information available in order to infer what the current state is actually is. Then we have our posterior network, which also has access to the that observation and the disclosure. CNN, as a as I mentioned, is a convolutional neural network."
"Speaker 3","00;51;43;09","00;52;11;10","And the yeah, here we have the actual layered description for our environments, which are 64 by 64. But that's that's less important. The important thing is that we have a convolutional model that compresses the formation from the observations for us. And these same convolutional network is also linked to the, to the representation model that is this the, the critique of the contrastive learning mechanism."
"Speaker 3","00;52;11;10","00;52;57;02","So the function that is indeed matching states and the observation in order to learn the good contrastive learning representation. So the, the function of that we minimize and respect the past is a is our contrasted free energy of the past, some over an arbitrary number of, of these key time steps in the over past the sequences. It is important to say that for the past there the negative samples that we take are an observation of the of of the same sequence of the object corresponding observation."
"Speaker 3","00;52;57;02","00;53;28;14","So that said that we have an observation in the States the negative samples will be on the other observation within the same sequence that are not the same in time, but also observation that come from, from other sequences. So, so that we are basically contrasting the, the current state with the different time steps. So what happened in different the moment of the of the same sequence of actions and the what happened in different situations."
"Speaker 3","00;53;28;14","00;54;19;29","So different sequences and that's how we, we try to, to foster our contrastive learning mechanism. Then we have the action models. So for our action model, we have two networks. One is the so called action network, which basically inverse to the, the action to take a given state. And then we have this expected activity network and this helps us pursuing what the team anticipated so that the fact that we we are amortizing the action selection process for very long term sequences by using a network that should estimate what the the value of a certain state is in the future."
"Speaker 3","00;54;21;09","00;55;17;27","So I'll try to be more more clear here. So basically the action network to minimize these this G lambda a functional that is basically an estimate that of of how much value is in a certain states. And although we get this estimate so the estimate is is provided by this formula here. So basically at every step we provide the the actual expected a competitive free energy for that state and then for the future the we we some we compromise in between an estimate of what the what the network would predict is going to is going to be the value in the future."
"Speaker 3","00;55;18;07","00;55;52;11","And the, the, the value itself that we are that we are computing we there with the functional so that the baby steps we basically sum the value that we we expect in the step there we bootstrap this. That's the way we normally see reinforcement learning. So we we apply some form of dynamic programing approach to to sum this value with what we expect will happen in the future and the we use these as our target for learning the estimate."
"Speaker 3","00;55;52;11","00;56;21;29","So we basically have the estimate and the estimate of the future plus the current value. And we compare the two when we want the, the actual estimates to be closer to what is actually happening plus the future estimate. And this is this is actually what is normally done in reinforcement learning. When you apply the the so-called bellman equation in order to estimate what's going to happen in the future by using what you actually know."
"Speaker 3","00;56;21;29","00;56;59;07","So generally like the rewards and in our case, the, the explicit free energy value and what you you can estimate for the future so you know, we're experiments we compare to we compare four flavors that make for reinforcement learning using likelihood model. This is very measured the dealer baseline. So the those are likely based and they're the word model and it uses rewards for learning action."
"Speaker 3","00;56;59;07","00;57;33;12","So the the river song freeze is already given to the agent. And then we compare it with contrastive drama. It is a modification of primary using a contrastive a learning for this word modeling sort of reconstructions and then we compare the two flavors of active inference in the standard and see one with the likelihood reconstruction model and our contrastive formulations so we use similar architecture and training routine for all the different baselines in the training routine."
"Speaker 3","00;57;33;16","00;58;13;06","Can be summarized as we see here in so the code. So for certain and for a certain amount of number of training steps that we if it's events, we are going to train our work model on the previous experience. So now on a replay buffer that basically represents our data sets of plus experiences, then we are going to use the trained words model to imagine some trajectories in the future by using our actually model and the replay buffer as well, which is used because we need to take the negative samples for the contrastive pre energy functional."
"Speaker 3","00;58;14;13","00;58;48;03","And then on the imagined trajectories, we are going to train our our action model in order to actually try to pursue the the preferred outcomes better. And then we are going to go back to the environment, collect a new trajectory using our word models to infer what the what the even state of the of the environment is at and everything step and using the action model to select the action according to the state of tweaking first and then add to the just collected trajectory to the data set."
"Speaker 3","00;58;48;14","00;59;26;04","And we do this continuously. So train the word model. Imagine some trajectories and the train the action model and again keep collecting so that we we continuously improve both the data collection process because the the the word model and the actual model gets better and also our modeling needs so that we get closer to the goal So one important insight into the person to produce before turning into an and vertical evaluation of the method is the fact that the using control scheme learning strongly reduces the the computational requirements of the model."
"Speaker 3","00;59;26;22","01;00;19;29","So a year high and comparing the number of multiple million models can play a clinician operation. And for our models and the number of parameters that we as we see these are much lower when we use a contrastive mechanism compared to the using likelihood model. And the addition is also reflected in terms where dinosaur our model is quite clustered compared to the dreamer that the things things the likelihood based model for the world uses just rewards for learning action and is much, much faster than the likelihood the active inference model because they like you are not giving trends model other than adding to to the the reconstruction during the world model training also us to imagine"
"Speaker 3","01;00;20;25","01;00;49;01","the the eye dimensional outcomes in the future. So in that case you have even more computation because for every margin trajectory you have to imagine all the possible images in our context that you are you in you will get pursuing a certain a certain policy. So it's our model is quite faster than that. So the first star that I will discuss is the simple mini task."
"Speaker 3","01;00;49;13","01;01;21;27","So the agent represents the the red arrow through the navigate are black white in order to reach Venus Square. The display is in one of the corner of the grids. So the environment is partially observed because the agent doesn't know what's in every pixel of the grid. So in order to find the gold first issued the explorer, the old grid and, and find the, the Green Square or at least be in a position that allows in to to see the Green Square in front of them."
"Speaker 3","01;01;22;18","01;01;45;08","So for the further you what model of course we have, we have the highest reward and the in correspondence of the of the gold states. So when the when the agent is actually on the on the gold square, it will receive a reward of plus one and this is a sparse reward task. So for all the other states the edge and that will just receive volunteer rewards."
"Speaker 3","01;01;45;26","01;02;12;16","So it will be just good still to reach the goal was for active interest method. The way that they chose to define the preferred outcome is to have an image of the agent that is that sees himself on the goal. So basically the agencies in cells on the goal and says this is the position that they want to reach in the world and let's see what happens from our from a qualitative level."
"Speaker 3","01;02;12;16","01;03;00;08","So as I said before, the rewards for this task is just a plus one in the right, a in the right square in the goal square. What happens for the the active insurance model? So what we'll do? The active inference model provides us a value of a certain status of the agents in order to to pursue the preferred outcome so we see that the like new active inference when the is imagining the outcome and comparing it to the preferred images is actually giving a very I value for the for the right the square saw this the you know go from 01 we can say that it's the 10 sorry we can say that's the one but"
"Speaker 3","01;03;00;16","01;03;30;18","other than that the the function that it is providing is a bit confusing because it's giving some other rewards in the sentence compared to the let's say the the the the last row and column that are the one that leads the to the final goal. The the the other corner are not even close to the go one. So it's just the, it's just providing a perfect match."
"Speaker 3","01;03;30;18","01;03;51;28","And we have no, no control of what the distance the goal is because for the perfect match, the goal that's, that is indeed the, the correct part of the biggest providing. But other than that, we, it's difficult to understand what the the the likelihood that the inference model is providing with the contrastive up to the inference we see that there is a different button."
"Speaker 3","01;03;52;03","01;04;18;01","So the agent is providing very low value for the center. So it's understanding that the center is of course not what they want to see but then is providing AI values to all the corners and in particular the highest that is provided to the air, to the right corner. So the one with the goal because is of course the, the one that corresponds to most with that we want what we want to achieve."
"Speaker 3","01;04;18;01","01;04;46;11","But then on the other corner also have a very high value and the the fact is that from I will say that the contrast back to inference is especially capturing a more I will see semantic information about environment. So in order to distinguish a corner from from a central title is actually a model in the fact that there is a corner in in a certain state of the environment."
"Speaker 3","01;04;46;28","01;05;15;15","And when you look at the preferred outcome image, you actually says, first of all, this is a corner and that's the way distinguishes. And then there is the green as the green tile in the in the in where the where the agent is so much from a value perspective, we can say that a corner is closer in semantics, respect to to a central style to our goal."
"Speaker 3","01;05;15;25","01;05;41;02","And then of course, when you also have the the green tile which represents the goal, then your are the closest. So this is of course a bit risky because it can also be due to suboptimal behavior in some cases. But with the good expression of the environment that it will of course, will lead the optimal behavior because still the the maximum value is still provided correctly."
"Speaker 3","01;05;41;02","01;05;59;07","So it's still good value for the right corner. But if you didn't see the other corner of the agent, will we just go to another corner and say, okay, this looks similar to the goal so I'm trying to do something similar to to what I would like to do, right? If I didn't see anything else. Disclosure, this is my the best I can do."
"Speaker 3","01;05;59;21","01;06;26;24","So this is a light. So exploration is important in order to to achieve preferred outcomes. And I think that's that also applies to likely reductive inference as well because even if it see the goal, you just add some, some noise, some noise, a signal in the center so you wouldn't be able to reach the goal as well. And then yeah, we quantified the performance."
"Speaker 3","01;06;26;24","01;06;59;15","We see that with the with the likelihood active inference that the agent struggles to to reach the goal consistently. Well, our our methods leads to consistent performance that are in line with the reward base baselines. Of course, the reward base baseline have an advantage because during training they always have a filtered objective. So even if their model is not correct, they always have this reward function filter function that tells them, yes, this is where you need to go."
"Speaker 3","01;06;59;15","01;07;26;21","Well, our model can take a little bit more time in order to first have a good model and then being able to match. But with the contrastive mechanism, this actually happens fast and leads to a consistent performance where we select the active inference. We see that it will probably take more time to converge or it just leads to the suboptimal behavior."
"Speaker 3","01;07;26;21","01;07;56;19","So it's just inconsistent according to our evaluation. And yeah, these are two different grid environments where this one is smaller, one is bigger, but yet the results are very similar in terms of performance obtained. Then the other task that we discuss is our continuous control task. We've are we've all to the planner environment when we're a robotic arm to the penetrate."
"Speaker 3","01;07;56;20","01;08;16;08","Asked your goals for the the red sphere and the the this theory is bigger in the the so called the return easy environment which is the one for which we see the preferred outcome on the left and it is smaller for the for the second feature are the environment that is the one that we will see on the right."
"Speaker 3","01;08;16;16","01;08;49;12","So for every work based agent we have the reward function that provides the when the agent penetrates to the sphere solar array was of one. Otherwise when it's of penetrating or just partially penetrating the sphere, it provides a dense reward. That's a that tells you you're getting closer to the goal. So in this case, the reward function is actually helping the agent a bit more because it is telling him that this is getting closer to the goal."
"Speaker 3","01;08;49;12","01;09;25;02","And he should just try it in the neighbor area. And while for the inference we would just provide the the preferred staging environment, which is the the agent penetrating the gold sphere, and let's see, let's see what happens. So again, we have a similar pattern, actually similar but different from the previous one or so in this case where the the main square there is the distance from the goal is actually more confusing as we have seen in previous example."
"Speaker 3","01;09;25;18","01;09;50;29","So the relative with the active translation totally fails to to reach the goal because it's probably the order of the states looking like an environment because the background stays the same. So the background is not moving as it was happening. For instance, for the mini grid environment, the goal is always in the same position. So the difference between the two images is just provide given by the the few yellow pixels that moves around."
"Speaker 3","01;09;51;08","01;10;30;00","And then if the model is not, imagine perfectly where this pixel go, it's, it's very difficult. It will provide some some informative or objective for the stars instance. Our contrastive active inference agent is able to provide an informative go and the appearance in the fact that this providing some semantic information about the task is actually helping it to converge even faster than the than the reward base at baseline because the reserve based agents have access to the just when they are close to the to the goal or the contrastive activity first provide a reward function everybody in the environment."
"Speaker 3","01;10;30;11","01;10;53;09","So when the when we see that the army is very far we have the mutual information term that should basically take over and don't you yeah. You don't want to stay there go elsewhere and the until we we have only find the ghost here and we come back to the correct behavior. So the agent is actually converging a bit faster than the other base lines and then yeah."
"Speaker 3","01;10;53;10","01;11;22;14","The the contrastive dreamer baseline is converging a bit faster than the remote one because its model is faster to learn because it's contrastive so this is what actually happens. These are a gift on the right. At some point they should be able to be said. So basically we just see here that the task is that is correctly executed so that the agent is, is able to match the, the correct behavior."
"Speaker 3","01;11;22;28","01;12;08;16","So yeah, he's taking a bit longer than expected. But yeah, you can see that the if for instance, in the in the atas, the agent oscillates around the current behavior but it keeps saying, okay, here we we see it. So basically the Indian environment is oscillating in the position that is a very close to the goal. So it tries to stay as much as possible to that it's buoyant and not not being driven far from the goal, but they never show the of the they are then we reanalyze qualitatively what's what's happening in terms of the volume is provided to the agents or what's what is the objective it is given to be to the agent"
"Speaker 3","01;12;08;16","01;12;40;16","in order to learn more. And as as I try to explain, the reward is is somewhere in the middle between zero and one when the agent is partially penetrating the goal and is totally one, when the agent is fully penetrating, the goal in all the other situations is zero for the active insurance likelihood based model. We see that the signal is very close for all the states."
"Speaker 3","01;12;41;01","01;13;14;20","So the agent basically see things that there is very little difference between being very close to goal and being actually very far off from the goal. And that's likely the reason why is not converging to the to the optimal behavior for contrastive active inference. Instead, we see that the agent is provided an objective variance some somewhat in between zero and one when he's there, when he's not closing the goal."
"Speaker 3","01;13;14;26","01;13;36;06","And something that is very close to one when it's in the goal and in particular when when it's closer to the goal, the, the, the value is actually a bit higher than when it's when it's far off so we see that there is some again, some semantic information provided, which is the actual distance of the arm from the from the right goal."
"Speaker 3","01;13;36;06","01;14;01;01","Or we can just see it as an objective the contrastive learning mechanism saying, okay, this, this part of the arm is actually very different from the one day, the day of today. So let's try to move to a place that is actually closer and the we need of your values when the when the pause is similar to the two, the one that we we want to achieve."
"Speaker 3","01;14;02;11","01;14;27;26","So we exploit the fact that some semantic information is provided to the agent by using contrastive and learning to work on the more difficult set up is the return of the starting environment. So in the visual dissecting task, we have the same objective as before. So we want the agent to reach the goal by penetrating the red sphere."
"Speaker 3","01;14;27;26","01;14;52;01","But now we have our own backgrounds and we have this structure in the environment which could be just altered colors on that camera. And we still want to to achieve the agent to still despite that. So for the, for the reward based agents, the reward is the same as before. So they are being provided for the agent penetrating the ghost here for active."
"Speaker 3","01;14;52;01","01;15;19;20","And for us the goal here is, is actually a bit troublesome to the design because given the fact that the background is constantly bringing across different episodes, we cannot a priori define what's the what the the preferred outcome looks like. So instead of doing that, we attempted providing a more neutral preferred outcome with the agent, seeing itself achieving the goal."
"Speaker 3","01;15;20;06","01;15;51;18","But do we have the standard task background? So we have this the blue just like the background with with the arm penetrating the goal. And we, we aim for this preferred outcome to transfer to the to the destructing set up. This is of course, pretty much impossible for for the likelihood the active inference model because our source is trying to match this in the we have a main square."
"Speaker 3","01;15;51;18","01;16;25;15","There are like functions of course the signal providing will be very confusing. Very interestingly we see yields that are so the dreamer method fails because it's based the on the likelihood based models and the as we'll see a reconstruction goal. The the variations in the environment is very difficult. So the the word reconstruction based work model struggles to provide informative states of the environment while the contrastive learning based model succeeded in particular."
"Speaker 3","01;16;25;19","01;16;55;13","It's very interesting that our model was able to to actually achieve the goal we see less consistently than before so that there is another variance. But still the agents is often able to reach the correct position despite of the difference in the background and all the structure present in the environment. So we see that this actually the representation is actually learning what the force of the robot should be."
"Speaker 3","01;16;55;13","01;17;18;19","And trying to match it in the future. And then, yeah, we see some, some videos with happening. So we see indeed that the, the arm is oscillating a bit more slowly. It's actually a bit more difficult for him to assess that he's doing the right thing. But still the, the behavior obtained it's still quite good, I would say."
"Speaker 3","01;17;18;19","01;17;57;19","So it's pretty much achieving the goal and the dispose why I like it. The base model will fail in this environment. So here we compared the ground rules in the different range backgrounds and the what the dreamers order them, the likelihood the active inference model sees through the reconstruction. So we see that either reconstructing from there, from the procedure state or from the prior state, the agent cannot perfectly model the important information of the environment, which in this case is the the arm."
"Speaker 3","01;17;58;00","01;18;22;05","So it sees where the the first link of the the robot are missed but he is not able to see it normally with the second part of the arm. He is because is very uncertain about that. And then and that leads the agent to do not being able to actually assess where it is in the environment and to to provide the the right value for our for for what's going on."
"Speaker 3","01;18;22;05","01;18;54;02","So they're using reconstruction this environment leads to the to this kind of problem where the agent is not certain about the the internal states. And so it's uncertain what they should do next because the signal of the state is uncertain is computed. And so it is also the the value provided by the but to the agent by the model, and that's it."
"Speaker 3","01;18;54;02","01;19;24;17","So I'll just briefly summarize what we have seen. And so basically, we use a constructive model to reduce the computation here of the objective inference is also brought some advantages and enforcement learning. But yeah, we focus on the on the contract and the active inference area where this model brought to to a twofold advantage for some learning the the word model cluster but also in imagining future trajectories faster because didn't have the reconstruction."
"Speaker 3","01;19;26;03","01;19;57;09","Then we saw that the conceptual representational features the better kept the relevant information for the environment. And this was key in solving both the the research tasks and especially in the research disrupting task where without this feature we wouldn't be able to solve dusk. And then we will show that to we can we can use this method to provide performance that are similar to engineering rewards."
"Speaker 3","01;19;58;03","01;20;22;28","But in a much easier way. So you can just say, okay, this is what I want to achieve in the environment, provide the, the observation the agent engaging will find itself a way to reach that that state without actually having to provide a reward function for every possible space. The environment which especially in realistic cases, is usually unfeasible."
"Speaker 3","01;20;24;23","01;20;54;14","And finally, we we have also seen that the exploration is very key for our method to work because we don't want the agent to to convert to the suboptimal behavior that looks like the the right outcome, the preferred outcome. So it's it's very important to wisely explore the environment before actually delving into learning the our preferred policy. And we aim to look more into this in the future."
"Speaker 3","01;20;56;01","01;21;03;16","So thank you very much. That that was it. I don't know if there's any question."
"Speaker 1","01;21;06;22","01;21;36;01","Thank you both. Very interesting presentation. So if anyone watching live wants to ask a question otherwise I have a few. So you mentioned a critic model when you were describing the architecture and that reminded me of language learning. Like if someone says repeat after me and then they give a sound, you might be accurate or you might not be, but if someone said no, it was not true."
"Speaker 1","01;21;36;19","01;21;58;23","You're a negative and a positive example. So what does that speak to perhaps the biological basis of contrast of learning or how these contrastive learning settings act of inference or not relate to the ways that organisms learn Okay."
"Speaker 3","01;22;00;06","01;22;32;03","I will say that the contrastive learning mechanism, though it's not completely equal. I was in Somalia in that resembles the ability mechanism where you when when when you have corresponding bursts over. So the things that should correspond if you actually want to to strengthen the link and when you have stuff that shouldn't be corresponding you actually you want to to weaken the link."
"Speaker 3","01;22;32;15","01;22;56;12","So I think that biologically we call that it could actually seeing this way. So when you have something that you you want to be you want to link farther in our case to think that we want to to link further. So to reinforce is the fact that this a certain observation correspond to a certain state then you just strengthen this connection."
"Speaker 3","01;22;56;12","01;23;27;17","Well, when you where you want you want to to be far and the square where the contrast between it differs maybe from this that this biological perspective we actually should be pushed farther which is not always the case for it being there because normally you don't have this a pushing part of the mechanism. So I would say this could be when one possible links as you said yeah the critique function is actually doing something very very similar to what you mentioned."
"Speaker 3","01;23;27;17","01;23;57;08","So you have a positive samples and you reinforce the critics tells you yeah this is this is correct and it sure tells you that this is correct. So it's strange to do that with with with much lower me, but. Well, if you have a good critique, you could use that. But yeah it should tells you the right samples while the for for non corresponding states in the observation they should go yeah this is not what we want in our representation we want this further."
"Speaker 2","01;23;58;27","01;24;29;12","So maybe to add on Daniel I think what you're hinting at that's providing like it should be like this is more like a way to to define the preferred state so so then if you translate it to do what we are doing it's basically saying these observations are what you should like basically. So so they come into place for creating the mobile like how do they get to these observations."
"Speaker 2","01;24;29;25","01;25;00;23","The contrast off defrauding for this more like being able to distinguish different things basically and it's more proof than the difference but what what you what you like to what you like so the contrast different regarding just learns to distinguish all kinds of sounds even all the bad ones and you can just now say, okay, but now I really don't have this sound, so try to get there."
"Speaker 2","01;25;00;25","01;25;03;11","I think that's the difference here."
"Speaker 1","01;25;05;29","01;25;28;18","That kind of sounds like paying attention to the right details, which we saw with multiple times like the breakout games. Like how could you miss the ball humans are watching that gif and we're watching the ball, but we also have a sense of how to pay attention to the right details. And then in terms of action and to have curiosity about the right things."
"Speaker 1","01;25;28;18","01;25;57;14","So it definitely starts to bridge into some very interesting behavior Another question was about the action entropy term in the free energy calculations. So maybe could you restate what the action entropy term is since it's one of the major contributions? And also what does that say about adding terms to the free energy calculate action like the action entropy is always greater than zero."
"Speaker 1","01;25;58;00","01;26;16;02","Kind of like a scale divergence. And so that you mentioned gives some perhaps nice properties about the bounded ness of F within a lower and an upper bound. So maybe just what is the action entropy doing here? Can we just add other terms that are bounded zero to free energy and use that in other ways?"
"Speaker 3","01;26;19;21","01;26;50;10","Okay. So I'll, I'll start with the with a question about the, the action entropy term. And then I also delve into the, the using different bounds for, for the free energy term. So everything is a in the way. We just added the active inference process for learning the actions. The key part is that we the actions are now part of the, the future inference process."
"Speaker 3","01;26;50;21","01;27;21;20","So here I could, I could also go back to the two biggest like this necessary. But the normally the way that we see these objectives without is a term here and there, but instead we have like a conditional on a policy, on a certain policy. So normally that means that you, you have some set of policies already and you're just trying to decide which of them is better."
"Speaker 3","01;27;22;17","01;27;48;09","So this could be done like using a no cameras or not cameras a first and then just assessing the the one that you think are best or just assessing all of them. But it's that's impossible. For instance, in a continue section set up where you can add two sets of possible policies because the objections are continuous, a third number is infinite for every dimension."
"Speaker 3","01;27;48;09","01;28;12;16","So that's made infinite by infinite and so on. So it's it's a huge a is a huge dimensionality in space. So instead here we, we make the action part of the or the inference process. So we want we want to have a separate model that tells you to tell us what's what's the action to to take at every step."
"Speaker 3","01;28;13;11","01;29;00;07","And the I what I said that we obtain an actual entropy term and that's because we in, in choosing the best action so in, in trying to, to match our actions to the one that we, we should actually prefer, we, we think it like we don't have a preference this over action. So we would. So for instance, I, if I want to reach a certain state environment so if I want to to go from this room to to the kitchen, maybe I don't care what's the, what's the shortest word, the shorter passes, I just care about getting there at some point or I don't care about going left or going round right now."
"Speaker 3","01;29;00;26","01;29;27;25","When I when I get off of my chair, I just want to to go where I need to go. So we don't we don't place a prior over the action. We just say whatever action is fine as long as it brings you the fastest as possible to the goal that the fastest thing is not even by the action itself, but but by minimizing the free energy so we don't want a preference over the actions we wanted."
"Speaker 3","01;29;28;13","01;29;58;08","We want the free energy to be the first spot. And so the actions are we some a unit for distribution over them and what remains is just an entropy. So it will be a real divergence between the queue over the action given the states and the this party, which basically becomes an entropy value. If you, if you you're some user constant because it's just subtracting a constant to that and the."
"Speaker 3","01;29;58;08","01;30;32;13","Yeah, the switching to the to the other part of the equation. So what does it mean to us this discourse center here. So is adding constant term useful. So I don't know if there's any other useful custom term that we could get. So the mathematically speaking, having a constant having gone up from an upper bound because of the constant building beyond me, because you're minimizing the same objective."
"Speaker 3","01;30;32;17","01;31;06;11","But then on top of that, we apply the contrastive, the contrastive approximation and that leads to another the upper bound and the as I said, there are some implication of this. We get to maybe a better representation, but are we getting farther from the from the actual objectives? So from my point of view, as long as we achieve something that is actually better, it doesn't really matter how far are we from the actual surprise and see now."
"Speaker 3","01;31;06;28","01;31;35;11","So in any case, we will always get some kind of amortization or approximation and we will probably never get 100% close to there to the surprise value. So because we don't have a perfect model, so a perfect model of the word doesn't exist, it's it's impossible to imagine that we can model every details of the environment, even if we are with a billion machine learning parameter."
"Speaker 3","01;31;35;11","01;32;13;29","And it's impossible to think that we will always act perfectly and always get in the, in the perfect route to go using the always optimal action, especially because there is always some uncertainty in the environment and there's a lot of things that we, we normally want to ignore in our everyday lives. So a lot of things there is not actually important to the capturing the word model or action y it's not always important beyond just percentile correcting over movement and so in the action, we think the important thing is that we, we get close to the goal."
"Speaker 3","01;32;13;29","01;32;50;25","So I will say, yeah, if there are any other term that we at the other center or just any other modification that we could do to there to the free energy function or that actually leads to better results without compromising the original goal of minimizing the energy. I think that's that would be a good way to uh, to address some of the issues that we, that we currently have with, we've actually been, for instance, that could actually lead to, to improving the performance of the, the artificial implementation of active inference significantly."
"Speaker 3","01;32;50;25","01;33;16;19","So that's also why I think that's taking advantage of some lessons that, that we learned from reinforcement learning is actually useful in acting inference as well because the there has been a ton of research about the way to amortize this thing or approximating something better or train a better deep learning model for some solving some very specific aspect."
"Speaker 3","01;33;16;19","01;33;24;09","And I think the objective inference research there to benefit from this, to take inspiration from this."
"Speaker 2","01;33;25;15","01;33;31;10","And maybe if I make that's good. Or can you go back to the slide with the action uh."
"Speaker 3","01;33;33;09","01;33;33;20","It's one."
"Speaker 2","01;33;34;24","01;34;21;09","Yeah. So, so maybe to make it easier here for or maybe people that are less familiar with reinforcement learning but are coming from a more active instance background. So in, in terms of active inference, but on a scale first thing, let's look at it. This is basically something that you should not do source. So so basically what happens here is that we see action inference more like a habitual thing, like a, a moment the state or I think I'm in the state, so therefore I can just infer my action results."
"Speaker 2","01;34;21;18","01;34;46;25","But it's kind of you become habituated. So I explained this hundreds of times and it's always this outcome. So I just stop planning and I just amortize this action into in the more playful C So that's basically the kind of the mechanism that that we apply here in order to avoid planning all the time because that's, that's the, the tricky part."
"Speaker 2","01;34;48;08","01;35;10;24","We have too much options to plan so we don't want to do this. So we just say let's amortize it from stuff, which basically means that this A, this or Q or a broken procedure. It's not only over stays but also over actions. And then so the action entropy just falls out by introducing the, the action in the user."
"Speaker 2","01;35;11;02","01;35;41;22","So it's not that we magically add an action action and return to the simulation. It just comes out because of having the actions esports over there but so did the my, the, this also means that we have an approximate there over actual selection and this works in at least three or four string problems because they're. Yeah, your goal is always the same, doesn't it shift."
"Speaker 2","01;35;42;21","01;36;28;02","It's not a, it's also not a if you, if you think back on on biological agents look like a complex distribution to maintain homeostasis basically just yet. This is the reward. This is very good. It's it's always the same thing. So this basically means that your environments very enriched with this this agency which also reinforcement learning solutions to their agents is exactly the environment is used for I can amortize what what I have to do because if I know my status I know they have to do basically things so change I guess if you have another environment in which this is not the case where you could be in a certain state and you would still"
"Speaker 2","01;36;28;02","01;37;04;22","have multiple options to do and you and you can only you can only really know what to do by planning ahead through by first. So search for information on what's happening and in these kind of environments, I, I think that's what position treatments will help you a lot. Are you going to do it just by amortizing. So it's, it's a trick device to, to allow it to work in this kind of environment because you you have to benchmark against some things and you have to be on board there."
"Speaker 2","01;37;05;05","01;37;31;24","It's so keep in mind it's not the silver bullet that will always work. We do deviate from vanilla extract insurance here costs section inference s like we just want to learn habits you don't want to plan. But this also means that there might be situations where it will not work and then it's not due to active insurance or for the energy principle that's not working."
"Speaker 2","01;37;31;28","01;37;38;12","It's more like the the crude approximation here by which things might not working. Well."
"Speaker 1","01;37;40;18","01;38;15;06","That's interesting about which training environments favor, what kind of algorithms and then how that shapes the perception of different algorithms like the navigation task. What if there was a fuel tank or there was a larger space that was going to require like multiple foraging information, foraging trips, for example. And so then the sort of single minded seeker is going to just die fast, but then something that's able to actually engage in planning wouldn't."
"Speaker 1","01;38;15;06","01;38;38;23","So that was a little bit like to those who are familiar with active inference. And then here's a variance on what we've seen before. How about for those who are more familiar with the dreamer architect or reinforcement learning? What makes active inference, active inference, and how is it different Well."
"Speaker 2","01;38;39;19","01;39;11;01","I think they are they're largely similar, let's say. I think that would be the starting point because both awesome people think about what's the difference. But I think the the the main point that we should rather stress more as an active interest community is that there are a lot more similarities between reinforcement learning and active inference. I would say that active inference is a bit more general than reinforcement learning."
"Speaker 2","01;39;11;10","01;39;51;09","In the sense that on the one hand we don't use the reward function per say, but we relax that the bit as in we just have a distribution over preferred outcomes, which is a bit more general, I would say. And then the second thing is that instead of starting off from the free energy principle as in this is the objective that we want to minimize you also gets the, the extrinsic value term here, which is exactly the same thing as for the reinforcement learning agents optimize."
"Speaker 2","01;39;51;18","01;40;36;02","So if you only look at extrinsic value, your agents will also do this. But the, the, the added value I would say comes in the information gain terms and this will only give you an additional benefit in environments where there is information to date. And this is not your typical reinforcement learning environment. But if you look at, for example, the teammates mouths from golfers, then these are typical environments where you can actually show that if you only go for extrinsic value you won't you will be acting suboptimal."
"Speaker 2","01;40;36;07","01;41;09;19","So you can actually prove almost that in some environments over there looking at the extrinsic value, given the correct model of the environment active inference rule. I think the crucial thing that we need to research on is how do you get to the of the cortex or the optimal model of your results enrich by optimizing your expected actually you're doing a sensible planning and this is still largely unresolved with our models."
"Speaker 2","01;41;09;19","01;41;32;14","We are taking steps in that direction. But as you can see, there are lots of issues to just find the correct model because if you just look at the models the likelihood based model should be perfectly fine, but based by the way you optimize in practice, then you see all kinds of problems like, okay, this, this, this little pixel is actually the most important pixel."
"Speaker 2","01;41;32;14","01;42;09;11","Look over the thing. And that does not appear so in my lost function. So that's why everything collapses. So in theory, it should work, but there are a lot of practical problems to find the correct model that pays attention to the correct details or to correct your observations. And this is still that this is something that is shared with model based reinforcement learning as well as active inference and I think there is a huge opportunity to find new techniques that can and put forward both both fields."
"Speaker 2","01;42;10;01","01;42;40;10","And we also showed this like the contrastive drama in the districting environment. Also improves performance on the normal dreams. So by having a technique that lets you build a better model, any model based algorithm will work. And effective inference has this special notion of also take into account information gain in environments where you might be denser to ensure your status."
"Speaker 2","01;42;41;12","01;43;14;06","So that's that's where it can prevail. But I think in most of the benchmark environments of similar days, especially in machine learning, you probably don't need these external. So you probably get away with just maximizing rewards which is in fact also an active inference agents. And to some sense, if of course if it's if it's I'm talking about a model based like dream agents in this case, of course, the model three ones are these are the differences."
"Speaker 2","01;43;14;07","01;43;29;05","They, they don't need to model the goal. But at least for mobile based reinforcement learning agents, I think it's pretty similar to what a negative influence to in these environments Thank you."
"Speaker 1","01;43;29;05","01;43;34;20","Tim Pietro, anything you'd add to yeah."
"Speaker 3","01;43;35;04","01;44;15;16","I would like to to discuss one aspect of three merit that's been a bit overlooked is the fact that it makes this amortization similar and it's also similar to what we have done. So yeah, this is this we learn a policy basically we own an action network that provides the correct state in the correct action for every state, but it is the key step to actually brings us closer to the acted in principle relation is that we, we imagine several steps in the future."
"Speaker 3","01;44;15;16","01;44;48;25","So it is true that we don't evaluate and long policies in the over other times that we have these prior about action it is given by our action network. But it is also true that given the fact that we evaluate the state three we expect to see and then from there we re start doing the the actual optimization process."
"Speaker 3","01;44;48;25","01;45;21;25","We actually get closer to the optimization scheme of active importance. In particular, there is a paper called Sophisticated Inference that discussed this when you, when you actually take an action and then you reimagine from that step what's going to up what's going to happen. There are some implication of this, but that we are not completely drifting away from the from the original activity story because of this is just a different way of doing the action selection process."
"Speaker 3","01;45;21;25","01;45;27;21","Similar in that indeed the three results are very close to the active inference itself."
"Speaker 1","01;45;32;05","01;46;01;19","Poole Thank you. I wrote down, if you don't know where you prefer to go, your lost drive fast. If you know how to get there, figure it out if you don't and then re-assess continually. And I hope that conveys some of the similarities and differences Do you have any final comments This is a very interesting line of research and we really appreciate this model stream."
"Speaker 1","01;46;02;14","01;46;10;00","Hope to see you in the future. Or should I say we expect and prefer it. But thanks again, Pietro and Tim. This is really awesome."
"Speaker 2","01;46;11;24","01;46;13;01","Welcome. Thanks for having us."
"Speaker 1","01;46;14;03","01;46;30;29","Thank us. Have a good day, everyone. Have a good day, everyone. Have a good day. Everyone, have a good day. Everyone, have a good day. Everyone, have a good day. Everyone have a good day. Everyone have a good day. Everyone have a good day. Everyone have a good day. Everyone, have a good day, everyone."

SPEAKER_01:
All right, hello.

It is May 16th, 2024.

We're in the RxInfer JL project with some guests from Bias Lab and participants.

And Votzer will be sharing with us about the RxEnvironments.jl package.

So thanks all for joining.

Looking forward to this presentation and discussion.

Go for it.


SPEAKER_04:
right thank you daniel uh so my name is wouter i am a second year phd student in bias lab uh you may know bias lab because we are the main authors of the rx infer package a package for reactive bayesian inference and last year when i was working with rx infer and doing well reactive bayesian inference i found that there was a

lacking support for environments in the same reactive paradigm.

And we were relying on reinforcement learning packages such as Gymnasium or Reinforcement Learning JL, and I didn't really like it.

So I wrote rxenvironments.jl, a package for reactive environments, which aims to complement RxInfer in the reactive

um agent development uh ecosystem so if we look at rx environments it's a very lightweight boilerplate package it contains all boilerplate code to make something that i call a reactive environment a reactive environment is uh

an environment that consists of entities where an entity can be an agent but it can also be a component of an environment or the entire environment itself and what we have to describe is how entities

communicate with each other so every entity has its own markup blanket with which it can communicate with other entities in the environment so in that sense it's very related to active inference because well we have a very specific notion of the interior of an entity and the exterior of the entity and its markup blanket and i try to replicate that in rx environments

So to give a short demo, we can dive right into the code.

I have prepared the mountain car example in the documentation of our environments, but just to showcase to you that this gives you more flexibility to work with.

So we have on the left-hand side, I have my code because I'm going to manipulate this environment

uh in real time and on the right hand side i have the environment so i have my landscape i hope you guys can see my cursor by the way i have the landscape and in the middle i have one agent one car a dot representing a car what's nice about rx environments is that i can send um

actions to the environment so my landscape the environment is one entity the agent the car is a different entity and i can send a command that contains engine force on a scale from minus one to one so i can send minus one to the entity and you'll see that the car starts driving up the hill it doesn't have a lot of engine force and i can live send different commands and you'll see

that the car responds accordingly.

Now, what's nice about RX environments is that, as I said, you'll get native multi-agent environments out of the box.

So if I just add a different mountain car agent with some hyperparameters to the environment, you'll see that there is now a second dot at the bottom of the valley, a second car.

And similar to how this first car can be controlled

we can control the second car too.

Point one was maybe this.

I should have maybe given this car a bit less engine power because it's clearly flying off the screen.

But you get the gist that we can control both cars simultaneously.

And RX Environments handles all this communication natively, reactively under the hood.

So my idea is, or my next steps for this project are going to be to incorporate it better into the RxInfer ecosystem, because I think that building agents with RxEnvironments in this way, that both the Active Inference community as well as the RxInfer community can benefit from this.

So this is a very short overview of the things that Rx environments can do.

It can do a whole lot more.

But I think that this is a very short demo that gives you the best idea of why it's nice that we're going to go reactive, because you get stuff like this.

We can control in real time.

We can control multi-agent environments.

And it's very active inference-y in that sense.

So on that note, I'll stop sharing my screen.

Let's see.

And yeah, if you guys have any questions about this, which I, which I heard you have, then I'll be very happy to take them.


SPEAKER_01:
Awesome.

Thanks.

Very informative and hilarious mountain car example.


SPEAKER_04:
Yeah, you can play with this engine force of this mountain car, and I probably should have prepared that it doesn't fly off the screen.

But I mean, it gets the point across, right?


SPEAKER_01:
Yeah, okay.

I'll share screen and just type.

And so everyone else in the group, just go for asking any of the questions we'd already identified or any new questions arising.

So go for it.


SPEAKER_06:
I don't understand you very well.

Maybe there's, do you understand Cope as well?

Okay.

You're very, very faint.


UNKNOWN:
Thank you.


SPEAKER_06:
Yeah, okay.


SPEAKER_03:
I actually had a question, Valter, not on this particular page that Daniel's sharing right now.

I tried my hand at, let's say, plugging in the mountain car, the existing mountain car agent in the RX and third documentation into your mountain car environment.

I wasn't quite able to do that, but it appears that you have done that in that example just then.

and then maybe i was missing something because in the in the documentation the rx environments documentation from mounting car i didn't see that there was a an agent that had you know a instantiated generative model uh much in the same way that the rx infer documentation had and i thought oh okay this this is a wrapper or you know a nice api for me to specify an environment i i now have to go away and specify my agent also


SPEAKER_04:
but it seems is that what you did for that example just then or or did you just no no no i just for this example just now i just used my uh you know custom commands that i just uh sent myself um so this is something that i'm actively working on on combining the two um the

mountain active inference mountain car example on the arcs and fur website i believe is a bit um cheating in a sense because we give it all environment dynamics all functions of the environment dynamics and then we do some some approximation to the posterior of what we have to do but in a sense it doesn't really

discovered that it has to do anything because it knows all the environment dynamics so that's why i haven't really spent time on incorporating this specific example because i feel that if we're going to demo this package plus

an agent that it should be an agent that actually has capabilities of exploring the environment instead of knowing all the environment dynamics and just well basically doing model predictive control right okay that makes it a bit clearer okay i'll have a follow-up but i'll maybe let somebody else um thank you very much so to give a very short answer to your question we can we can sit together if we uh

on if you want to define an agent a generative model i know how to hook the two up i just don't have a working example yet right okay i don't want to use this


SPEAKER_03:
i don't want to use this agent from from the arcs and third documentation because indeed i feel that it's a bit cheating but it's my personal opinion right yeah yeah okay that actually clears a lot of things off me because i was kind of unclear like oh do i do i kind of plug that one in is that is that what i do yeah okay but that's not that functionality is exactly clear yeah we can um however


SPEAKER_04:
There is a subtlety there, and that is something that I find very interesting in my research, is if you open up this space or you open up your environment to be reactive, then you want your environment to be able to receive an action at any point in time.

so what we see in reinforcement learning is that you have a specific transition function the environment waits until you send an action it will move the environment forward a pre-specified amount of time and it will emit to you an observation and a reward however if you make this entire thing reactive then well our agent would be able to emit at any point in time and also it would be able to get an observation at any point in time now that is a very interesting

setting to work with because now our agents should kind of be robust to getting an observation at any point in time specifically what if our agent that lives on some embedded device and some drone whatsoever it has less compute to do to do inference however well at some point we're going to have to send an observation or an agent an action back to the environment we're going to have to do something

So this reactivity opens up a research avenue for, well, right now for me, because I find this very interesting, on how truly reactive agents would look like, if you see what I mean.


SPEAKER_03:
Yeah, I do.

That's exceptionally exciting stuff.

It's awesome.

Okay, but okay, just to wrap up my question, there is no, at the moment you kind of manually, you know, you put in the force limit and you say, okay, I'm going to kind of manually, you know, play around with


SPEAKER_04:
I have working examples with like Bayesian thermostat or very, very easy, which I probably, I don't know if I have them on my machine right now or if they live somewhere, like it works.

But the thing that I got to work is model predictive control and not active inference yet.


SPEAKER_06:
So that's why.

Awesome.

All right.

Thank you.

You're welcome.

Can you hear me a bit better now?


SPEAKER_05:
Yes.

Voter, I'm interested in the relationship between RxEnvironments and RxInfer.

For example, to put it simply, is it possible to use only RxEnvironments or is it supposed to be always used in collaboration with RxInfer?


SPEAKER_04:
yes the relation is that i wrote arcs environment specifically um as a complementary package to arcs infer so if you just want to do bayesian inference you use arcs infer but i think if you want to build agents that you're better off using the the two together so arcs environments is

a package that uses the same reactive backend as RxInfer.

So it's made to be compatible with RxInfer.

However, if you write agents just with an algorithm, you can use RxEnvironments by itself.

It's not mandatory to use RxInfer.

They are separate packages.

There's not a dependency on RxInfer.

However, since

um they share the same reactive backend so the same package for reactivity under the hood which is also from bias lab which is rocket um if you want to build agents i would advise to to use rx infer so they're they're disjoined but they're designed with each other in mind or arcs environments is designed with arcs and fur in mind so um


SPEAKER_05:
If we use this high level of abstraction given to us by RxInfer, do we lose anything?

Or can we at any time dive into the lower levels by means of RxInfer itself?


SPEAKER_04:
So I would say yes, because the inference process that happens in RxInfer

is disjoint from what's going on in Rx environments, similarly to what happens in real life.

So you can see Rx environments as a package in which you can make a digital twin of the environment, but then whatever happens in the inference process in your agents, well, that's still Rx Infer.

All Rx Infer diagnostic tools or debugging tools should be available there.


SPEAKER_05:
And a more practical question.

I know from the large papers and Professor De Vries as well, they like to divide what's going on into these six methods, the act, future, execute, observe, compute, and slide.

So do you see that as kind of superfluous in the Rx environment setting?

Is it fine to still use that approach?


SPEAKER_04:
Let me Google the definition for superfluous.

I'm going to tread very carefully in answering that question.

I don't think it is superfluous because in a sense that it's... If you want to design your agent such that it goes through those steps,

um then you're more than welcome to uh my personal opinion is that i'm i will i'm going to try and see if i can make an active inference agent that does inference in one step instead of having to act observe uh plan slide uh and so forth however for arc's environment so for your environment

it doesn't really matter how you will get to your uh to your action as long as well or not right you can you can send an action at any point in time and if you don't send an action the world will just continue that and arcs environments will just continue running there's no real constraints on that so that the design is to be as less constraining on users as possible while opening up this reactivity uh

um the reactivity aspect and also for um if you start with the package you could make what i call discrete environments they're they're a bit beating i would say because they're basically just the arcs environments equivalent of uh of a classical reinforcement learning environment where you have a pre-specified time step every time but i wanted to make that to make the transition easier

Because if you go full reactive right off the bat, I would say it's a very ambitious thing to do.

So I don't think it's a superfluous algorithm.


SPEAKER_06:
But I also don't think it's, well, RxEnvironments doesn't constrain you to use it, so I would say.


SPEAKER_05:
Nice.

Now, let's say one has a need to write one's own node in the factor graph.

Would Arc's environments support that kind of thing, or is it kind of an irrelevant question?


SPEAKER_04:
It's, in that sense, I would say irrelevant, because a custom node is something that lives in Arc's infer.

And Rx environments really is concerned with everything that goes on outside of your inference process.


SPEAKER_05:
And then another quick question.

While playing with the, I think it was a thermostat problem in Rx environments, I wanted to see the time behavior of some variables like observations, states, controls, free energy, and things like that.

It wasn't straightforward to me how to plot those in the RX environment settings.

Is there an example that you can point us to or a fragment of code that shows us how to do that?


SPEAKER_04:
I can make it.

because i i know how i can access so free energy is uh something that lives inside your agent so it's part of the inference process it's the it's the uh cost function if you will of of of the inference process so for free energy you would have to look at your uh would have to look into arcs and fur however everything that goes on in our environments we can extract it and we can plot it

So both environment states as well as emitted controls or observations, we can extract all those things.

I can show a code fragment that does this because it's one line.

However, I also understand that that is not something that

well, you probably remember after this meeting.

So I'll make sure that we can design an example that showcases this.

But in a nutshell, we can create a keep actor.

So this is, well, an entity that will keep all, everything that it gets.

uh we'll keep it inside memory and then we subscribe to the observations of the agent so we say subscribe to observations and with this ops keep actor we do this and if i then call ops values

this will contain all kinds of well arts environment observations which is basically a wrapper struct for let me call data on this entire array and you'll get your individual observations so we can basically tap anywhere in this process

Anywhere in this communication, we can tap whatever is being emitted.

So if I run this again, now it will contain 400 elements because, well, this environment is reactively running while we speak.

So the agent is getting more and more observations.

So you see this thing reactively growing every time I call or extract its contents.

So there is definitely functionality in which you can sort of inspect this process.

But yeah, I have to stress that it's really distinct from the inference process.

If you make an agent in RxInfer and you hook it up to RxEnvironments, then you have to set up a similar procedure to extract free energy and stuff like that from RxInfer.

And because I'm not the author of RxInfer, I don't know how easy that is.

But for RxEnvironments, yeah, this is one of the first things that I wanted to do, see what my agent was sending, see what my agent was observing.

So I made sure to build it.


SPEAKER_05:
Yeah, I'm not sure if what you just showed was recorded by us.

So if you could just make sure that that is included in your examples as well, that would be excellent.


SPEAKER_04:
Yes, that's my idea.

I just wanted to showcase for now such that you know that it exists, but it wasn't meant to be a big tutorial or anything.

I can just put it in the examples and then we can plot it.


SPEAKER_06:
That's not a problem.

Okay, thanks a lot.

You're welcome.

Okay.

Anyone else can go for a question or we have some other written ones or I can ask you a question.


SPEAKER_02:
I have a quick question.

I would just wonder once I don't know if you guys can hear me.

I hope so.

Yes.

Fantastic.

A lot of the examples that you have there and kind of the way I'm I'm probably naive in how I'm thinking about this.

The arts environment is modeling something in

the real world, or at least that's the example that we're thinking of right now.

Is art environments capable of essentially modeling more metaphysical spaces?

So if you think of your question in a way that you have a time series or a space that you're trying to occupy,

but it's not a physical space.

It's a more theoretical space dimensionality that you're trying to have an agent navigate.

Is that something that you see that this is usable for, or is this something that we need to kind of gear back towards the RxInfer package itself to kind of build the more metaphysical concept of the environment that you're putting together?

Does that make sense?


SPEAKER_04:
Yeah, if I understand your question correctly, yeah, my answer to that would be that there's no physics simulator or whatever in ARCS environments.

It's purely the boilerplate to make whatever you write act reactively and expose reactive streams that you can then use in ARCS.

So, for example, one of our colleagues has made a

an environment that returns the result of a stochastic differential equation.

So an agent could query with an action, I want the realization at time t, and the environment would respond with the actual value.

And with this interplay of querying a stochastic differential equation and

and getting a result, you can cast stochastic differential equations sort of a Bayesian state-based model.

I'm not an expert on this topic.

I just know that one of our colleagues did this.

So if I understand your question correctly, that would be the sort of environments you were after or you were questioning about.

So I would say, yes, you can.

So there's no, there's no physics simulator or anything in there because that's a whole different, a whole different topic.


SPEAKER_06:
My main thing was just to make whatever code you guys write to make it reactive.

Fantastic.

Thank you.


SPEAKER_02:
Now that makes a lot of sense.

I just wanted to clarify that since perfect.

not thinking so much in robotics or creating like physical environments and thinking more along the lines of data environments.

So that's why I was trying to understand how we could utilize this.


SPEAKER_04:
Yes.

Yeah, you can.

Yeah.

And, uh, you can push it further, right?

You can make, uh, uh, a combination of the two.

You can make a, uh, an environment that flies a drone,

uh your agent has to control the drone but it can also query the environment for hey what is going to be my position in five seconds so kind of this this well abstract action that's basically querying the environment well since we have this uh

this mechanism that we can depending on what action you send correspond or match this with an observation now you can do stuff like that there's nobody stopping you doing it whether or not it's useful for your agent well that's up to you i i was just yeah i'm trying to make your environments as expressive as you can or your the framework as expressive as i can and yeah you you can you can go wild with it right you can go crazy and that's perfect because the agent


SPEAKER_02:
that I'm wanting to put together would need to actually walk through time in a certain way and optimize its activities in a time series.


SPEAKER_04:
Yeah, I was already expecting you.

That's why I brought up this.

That's very cool.

I would be very excited to see, well, whether or not you can build your environment and our environments would be very cool.

Absolutely.


SPEAKER_06:
I'll keep you updated then.

Nice.

Thanks.


SPEAKER_05:
The observable and actor roles in Craig's experiments, can any entity take on either of these roles, or are there certain constraints?

For example, let's say the agent must always have the actor role or something like that.


SPEAKER_04:
No, they're completely unconstrained.

There is no real distinction under the hood between entities or between something that represents an environment or something that represents an agent.

uh they are both entities in arcs environments um the only thing is that you can make uh something which i call an active or a passive entity by the way all this terminology it's all up for discussion i've thought about this and most of the terms that i come up with there they don't really fit what i what i'm trying to say um

But an active entity is something that whenever it gets an observation or whenever it receives an action, it emits or at least considers emitting to its subscribers.

For example, the mountain car environment itself would be an active entity.

Whenever the agent sends an action to move the car, it will correspond or it will match this action with, well, moving the car.

Whereas the agent, whenever it gets an observation from the environment, well, it doesn't really have to do anything.

We have to, in a later stage, attach an inference process, but that is something that's not RxEnvironments, that is RxInfer.

So we make this entity passive in a sense that it's an observer that can also send actions, but those are not triggered by an actual algorithm that we write ourselves

uh in arcs environments but it's a result of some inference process it's the triggers are external to the the reactive ecosystem that we've built in arcs environments itself so that's the only distinction that i make but you can yeah something that represents an environment is something that represents an agent they're they're under the hood completely the same things they're just uh entities that that that can communicate with


SPEAKER_05:
with with external entities do you think you could um in addition add a discrete control space example for us as well because mountain car and the abrasion thermostat are both continuous examples

Even if it's just a simple discrete control space or categorical in particular, categorical control space, maybe a simple maze example or a grid example, you know, like the windy grid that's often used in reinforcement learning.

Even if it's just a simple grid.

Windy grid.


SPEAKER_06:
Windy grid.


SPEAKER_05:
Yeah.


SPEAKER_06:
Okay, I'll Google that.


SPEAKER_05:
Even if it's just a simple categorical control space, because that's what our problems currently center around.

Not so much continuous control spaces, but categorical control spaces.


SPEAKER_06:
Okay, that's, yeah, I can do that.

Okay, I see.

Windy crypt world.

Okay, I can do that.

Okay, excellent.

Thanks a lot.

Can I ask a sort of a very quick, broad question about the whole ontology bit?


SPEAKER_03:
When I first learned about this, my own naivete and just sort of looking through the docs, I thought, oh, okay, this is a wrapper around Rx and Fur for standardizing the creation of an environment that you will then use for whatever purposes.

But it sounds to me it's not at all that.

It's really, this is not even really a way of talking about an environment per se.


SPEAKER_04:
No, it's more of a...

It's a communication protocol for active inference agents.


SPEAKER_03:
This is a way to standardize the dyadic interaction between two abstract entities.

That could be between environment and an agent, or it could be something else entirely.

That's much broader than what I was conceptualizing.


SPEAKER_04:
So when I started this, indeed, I started as, OK, I want to standardize environment creation in our ecosystem, in our .

And I had a discussion with Magnus Koudal, who's also in this Discord I saw.

And, well, we had some very interesting talks.

And he said, well, actually, an environment and an agent, they're not so different, right?

they just hand or they just receive data from each other and handle them accordingly so yeah there is a certain uh symmetry there and then i thought well if there's a symmetry there then i can make an overarching uh programming concept and just kind of implement it that way and that's what i did and then it turned from just a way in which you can make an agent and an environment communicate to a package in which you can communicate this whole network of

uh of subscriptions and communications um because there's yeah there's no constraints anymore so for example we run an environment in in uh in our lab with marco actually that um has well in a non-trivial uh entity interaction where we have a hearing aid that uh

gets data from both the external world because it can well it has a microphone and it should register the external world but it can also get data from the user of the hearing aid saying hey unhappy with the hearing aid settings yes or no so now we have a three entity environment we have the user we have the hearing aid and we have the uh the external world that just emits noise

then we have a fourth entity which is our agent that should learn the preferences of the user that should get data from the hearing aid and should uh tweak the parameters inside of the hearing aid so now we have four entities that are communicating in this non-trivial way and well we built this in arcs environments and it all holds up like it all just

yeah it runs just because whenever we get data the hearing aid knows ah i get new data i get new noise i have to transform this noise and send it to the user and i have to do a free transform and send it to the agent for analysis uh and just because it does this reactively it's all tractable and all works nicely so yeah uh it it turned out to be a bit of a uh a bigger


SPEAKER_03:
well scope than i would have imagined but i'm yeah i i'm not making any claims on on how large it actually is that's uh yeah cool this this is also very cool and i'm very very thankful that you're working on this because i i had the thought like oh well what what what then is you know sort of predicated off the environment it's kind of like well we have the environment and the asian they're kind of both being equally treated here so


SPEAKER_04:
That's a very interesting question, right?

So what is the actual difference?

Some people say, well, an agent has some sort of inferential process on how it comes up with an action.

But in a sense, well, we are all just with our eyes and ears and muscles.

We have an interface to the environment.

yeah in in that sense we we are just all mark of blankets with internal states that that navigate this world uh and this well communication that we have with our outside world i i try to replicate that fantastic thanks


SPEAKER_00:
anyone else want to ask any questions yeah christoph or yeah question um i didn't quite get what you um what you said about the difference between the typical reinforcement learning setting and uh rx environments could you um could you could you explain that again please that was very interesting sure if if you want or if you make an environment in gym in gymnasium we can we can look at the


SPEAKER_04:
um at their documentation right so let's see gymnasium package so i go to gymnasium and i want to make my own custom environment i saw it here make your own custom environment i've visited this page before so we can make our environment blah blah and then you have this function step

The step method usually contains most of the logic for your environment.

It accepts an action, computes the state of the environment after applying that action, and returns a five-tuple observation reward terminated truncated info.

So we write a function step that takes only an action.

So this is the main design paradigm of GIM and most reinforcement learning packages, actually, where

i emit an action this action is incorporated into the world but the time that the environment is stimulated forward is constant so there's no notion of different time intervals whereas if i'm i'm a very big uh football aficionado for the americans that's soccer but it's actually football

but if i kick a ball i kick the ball the ball starts rolling well whether or not i do something the ball will just continue rolling like this this environment around me it doesn't wait for me to emit additional actions or i'm not consciously saying okay for the next second i do nothing and then the ball rolls okay for the next second i do nothing again and the ball rolls further no there is a

in the real world there is a difference right we do something and then the world just continues so we can do something at any point in time and the world around us will just well the world keeps on turning um and that's what i was trying to do with with rx environments is to um alleviate ourselves of this

um communication protocol because this specifies i get one observation i can do one action per observation

and that's it and in between different observations i have no power to do anything whatsoever also if i have an environment that instead of modeling one second forward i now do half a second well everything breaks down because now i have to emit actions at twice the speed or my observations don't correspond anymore with my internal model or there's all kinds of stuff that if you do

it's like this it turns kind of awkward and that's what i was trying to alleviate uh with with arc's environments is that i'm trying to make this communication between agent and environment as free as possible so if you have a hearing aid environment

Yeah, that's it.

Yeah, I'm rambling, sorry.


SPEAKER_00:
Good that you interrupted.

That's a good explanation.

And I think you also mentioned something about some difficulty there, because the question then becomes, if you have multiple agents interacting with the environment, then what exactly is the state of the world, right?


SPEAKER_04:
What exactly is the state of the world is not... Because you can just sort of...

you can have like a god entity that takes care of of this stuff that that kind of resolved all conflicts in that sense um the main question for me would be well we just took away your one moment to conduct an action in reinforcement learning you have one moment you can do an action and you'll get one observation we just took away this one moment and gave you the opportunity to act at any point in time well when are you going to do it


SPEAKER_00:
Right.

So, you know, the reason I'm asking about this is that it immediately got me thinking about Stephen Wolfram's work on observer theory.

So he's trying to formalize the notion of an observer, right?

And there is this whole field that he basically invented for the physics project related to what he calls multi-computation.

I'm not really deep into that yet, but it seems somehow relevant.

As an agent interacting with the environment, we effectively sample it precisely because it keeps rolling as we do the internal computation required to update our generative model in order to act.

So I think you stumbled onto something quite profound there.

And the question is about this God mode view, right?

That's an old question in physics, whether there exists the absolute reference frame or whether everything is relative.

And yeah, I just want to say that, you know, food for thought.

There's some interesting problems there.

Maybe we can talk about it sometime.

I would love to hear your thoughts about this.


SPEAKER_04:
Yeah, sure.

I'm very interested in this stuff.

So as a software developer, I would advise you in your environments, if you're not dealing with, you know, existential stuff that just make a God state that takes care of this, but you make a very good point, right?

So if we really want to make an accurate representation of the world, do we need this God state or because we now allow our environments to

to alleviate this right we can have multiple entities that together compose the environment and then there is some internal communication between them so um it does it does open up this door like it doesn't slam it wide open but at least you know there's a

In Dutch you would say open gear.


SPEAKER_00:
No Dutch people.

I think that's a potential incursion into effectively resolving some of the most interesting problems in the fundamental theories of physics.

Understanding precisely which one is the case, whether it is this absolute reference frame or whether you can just simply explain everything through these relative reference frames and agents sampling the environments.

And if you can effectively recover something that looks like physics from this interaction, which is what Wolfram is essentially working on, that would be a complete computational fundamental theory of physics.

And it's a big undertaking, but it's worth considering, not just merely from software engineering perspective, but from that angle, if you're interested in that kind of stuff.


SPEAKER_04:
I mean, yeah, it's very interesting.

There's something that, uh, you know, it's a very big deviation from what my supervisor wants me to work on.

So it's probably not something that I'm going to undertake, but if anyone wants to pick it up, right.


SPEAKER_06:
Who am I to stop them?

Thanks.

It's great work.


SPEAKER_01:
Cool.

Yeah.

Anyone else want to ask any questions?

How would you deal with the setting of multiple ant nest mates moving around and modifying the pheromones on the ground?

So in these kinds of simulations, it's common enough to have the pheromone pheromone state on the ground, and then nest mates that are reading and writing.

How do you deal with the spatial proximity and the collisions and interactions


SPEAKER_04:
So that is something very, very, very difficult.

I'm working on a project right now where, once again, we're modeling football, which is why I brought up this example just now.

And resolving collisions is something extremely difficult.

It's something that I don't have a standardized, advised way of doing in ARX environments just because game studios, they spend God knows how many developer hours on resolving collisions efficiently.

So that's not something that I think

well we we can figure out in an afternoon right um it's it's something that would be in the internal dynamics of your environment but not something that is part of your communication protocol per se just influences the the type of observations that your agent would get um so it's not something that i can give that i feel qualified to give advice on

But it is like making this environment with ants running around the floor.

That is something that's just a multi-agent environment that's fully covered.

The problem is the collisions.

That is a difficult thing.

It's not difficult in an ARCS environment perspective.

It's difficult in a computational geometry perspective that you want to resolve collisions.

Like in AAA games, you can still clip into walls somewhere.


SPEAKER_01:
That's a great and funny response.

Yeah, I would just highlight the real structural difference when you pointed to the step function in the gymnasium.

That is why in so often for these control problems, there's like a null action or like a weight action.

So you kind of get this...

bimodal action vector policy vector where there's like a do nothing option and then all the options because out of requirement something must be emitted at every click of the simulation and all of a sudden it's like we moved from walking on a staircase to like ice skating or flying with rx and fur because it has totally opened up a dimension of the control question when to act not just which with according to a metronome

And that's an entirely different approach that the communication protocols enable.

And then those communication protocols can have all these other functionalities, like they can have a sub ontology or they can have compliance features as well.

So it's super powerful and we're very excited to learn it.


SPEAKER_04:
That's yeah, that's the main point.

Yeah, it's two sides of the same coin.

It opens up this communication protocol, but then, yeah, you have to question when.

When?

It's something that I'm going to spend time on in my next project.

Can we make an iterative procedure, maybe with iterations and arcs and thirds?

We can send intermediate posteriors, and we always have an action that we supply.

I don't know yet.

I have no idea.


SPEAKER_06:
I have ideas, but I don't have anything working yet.


SPEAKER_05:
I think, to me, the underlying principle of the first suite versus traditional intercom is that instead of having synchronized communications, we now have asynchronous communications between everything.


SPEAKER_06:
Between everything, yes.


SPEAKER_01:
i'm glad i got that point across that's that's the point i wanted to make great well um if anyone else has a question otherwise we will continue learning and watching the github updates and just building out examples because in the coming months we know a lot of fun things will happen


SPEAKER_04:
All right.

Yeah.

You guys can always reach out to me on, I don't know, GitHub, Twitter.

I don't know how often I'll check Discord.

I'll try, you know, I'll try.

But you can always reach out to me on LinkedIn, Twitter, whatever.

That's all fine.


SPEAKER_03:
excellent i i do have a final question if that's all right um sure just just with respect to you know future directions and that kind of thing what's kind of the biggest problem set of problems what are you kind of looking to do i know that's a massive thing but is there anything relatively summative in that direction that you could perhaps speak to just for just a second so i'm very interested in this when to act uh problem so


SPEAKER_04:
This, to me, opens up the door to an agent that, because we, okay, so,

We can now act at any point in time, or at any point in time, we can send something to the environment.

A logical consequence for me is that at any point in time, we should have an action ready to send.

So when the next observation comes in, we don't know when that's going to be.

We should be able to act.

So we have a certain computational budget between one observation and the next.

We don't know how much it's going to be.

but we do have a budget to spend.

So I want to design indeed an iterative procedure that comes up with an action and then refined it and refined it and refined it until, well, the next observation comes in.

So we have to send whatever we had and we get the next observation.

this gives you if this all works right this gives you a very nice robust agent that if you give it a lot of computing power a lot of computational budget then in between observations well it's going to be able to come up with a pretty good um pretty good action that will solve the task accordingly like ideally however if you limit this uh this computational budget you put it on like a raspberry pi or an edge device

it will not have as much iterations to refine its actions so it will still solve the problem hopefully but it will be less efficient doing so it will be slightly dumber so this is a very to me it's a very interesting problem to think about because then you have a sort of a procedure for control that is robust to loss of computation or whatever and it's still adaptive and reactive and and all those nice things we get from from asian inference

um so yeah that's something that i that i would want to work on if i you know at 48 hours in the day it's very interesting so it sounds to me like the number of iterations you just referred to is the iteration field in the call of the infer function yes yes if i can do something like that so i run an indefinite amount of iterations and i just

stop the inference procedure whenever i have to do something and then i extract the latest posterior for the action and i i send that to the environment yeah that's something well that would be the first thing i would investigate yeah i certainly think it has the potential to make the behavior in general uh quite robust if you if you consider that approach

It would always have something to do, like it would always do something.

Whether or not the free energy would converge as a measure of whether or not this action is good enough, that is to be investigated.

But it would allow us to run the same models on edge devices as well and get slightly worse performance, but still

yeah yeah it would still do the job we and then you can investigate you know how many iterations do i actually need before i can solve the problem uh it's of course problem dependent but yeah it's a very yeah it's just something that that really interests me i really like this this way of thinking um and it's in part why why i wrote this package because i i don't like this this this synchronized communication protocol from jim i just i just don't like it yeah that's why i wrote wrote arts environments


SPEAKER_05:
I think the only alternative is to not be able to respond whenever it gets an observation.

In other words, to postpone your response.


SPEAKER_06:
Yes.


SPEAKER_05:
It nudges back into the synchronicity approach again.

So it breaks away from the reactiveness of the whole thing.


SPEAKER_04:
But then you run into the robustness issue, I would say.

If you have a self-driving car and it's driving into a wall because it thinks it's a green light, because those things are not that smart nowadays, and it figures out it's a wall, but it takes some time to adjust or it takes some time to come up with the idea, okay, now I have fully converged on the action that I have to steer away.

Well, by that time, we might have already crashed into the wall.

So...

this this queuing up of uh of observations and waiting until uh my computation is fully converged i think it takes away the robustness like if you if if you don't have enough computational budget to spend well yeah you might do the wrong thing it's the same as when you and i are in a car right if we're in a very um

a very busy situation so well i'm in the netherlands so we have cyclists right so if we have cyclists we have other cars there's a an ambulance coming whether it's very busy you might feel overwhelmed but you still kind of know the basic things to do to to to steer your car it's basically the same where you have limited computational budget there's all this stuff around you so to come up with a really thought through response or action

uh it's too much to ask because you don't have the computational budget but your brain kind of prioritizes and says well if we stay on the road and we let the ambulance pass and well we're fine for now we're this is the best we can do um so this feeling of being overwhelmed by what what goes out on around you inspired this idea of of robustness and uh uh of interruptability basically

yeah but then again how i'm going to do it well no idea yet or run rx infer iterations and see if i can uh can send intermediate posterior results or something or but it's just something yeah i find this very very very interesting awesome it's a good thing that free energy in general


SPEAKER_05:
decays predictably in a sense.

So, I mean, just fully using your budget, your computational budget for the time that you're looking for the next observation over this, I think it's fairly safe to just use the current value of the posterior.

That's the best you can do under the circumstances.


SPEAKER_06:
Yes, that would be the argument indeed.


SPEAKER_04:
Hopefully, this opens up the road to having actual robots driving around instead.

Because, well, there's no self-driving cars where I live.

There's no robots or drones flying around.

And I think one of the reasons is because they're not robust or interruptible.


SPEAKER_06:
Well, we agree.

Yeah.


SPEAKER_01:
All right.

Thank you all.

We'll look forward to staying updated, asynchronously even.


SPEAKER_04:
Asynchronously, reactantly.


SPEAKER_01:
See you all soon.

Bye.
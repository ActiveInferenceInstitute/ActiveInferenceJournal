SPEAKER_01:
Hello and welcome.

This is Active Inference Model Stream 16.1.

It's December 6th, 2024.

We're here with Eli Sinesh and Tomaso Salvatore discussing Divide and Conquer Predictive Coding.

So thank you both for joining.

Eli, to you for the presentation.


SPEAKER_02:
All right.

Hi, everyone.

I recall being here once before, presenting that old paper, Interoceptionist Modeling Allostasis as Control.

And I think at the time I even mentioned that there wasn't really an inference algorithm that I could just go and apply to all of these kinds of active inference and predictive coding problems.

And over roughly the last year, let's say, we've started work, this whole team of myself,

Hao Wu, who is unfortunately not on the stream due to having a 9 to 5, and Tommaso Salvatore, who's here with us.

We've actually been working on this.

How would we take these theories and this predictive coding idea and just turn it into a Bayesian inference algorithm you can use for stuff?

So as background, to really refresh ourselves,

This is the super short, you know, NeurIPS version of our talk.

You know, there's like basically theoretical neuroscience, neuro AI, or now, I don't know, Tommaso, like what keyword do you guys use it versus?

I should have asked you maybe.


SPEAKER_00:
We do use neuro AI actually, bioplausible deep learning.

That's a sentence I often use.

I think that dates back from even before my versus times, I guess.


SPEAKER_02:
Yeah, so the broad theory is that predictive coding sort of solves two tasks for the brain, and that's credit assignment and Bayesian inference.

Now, predictive coding is also a thing in machine learning now.

They do a lot of it at versus.

Tommaso's done a bunch with it.

But it doesn't always, for various reasons we can go into, it sometimes doesn't always scale as well as back prop, basically.

And so we basically started over again, I guess you could say, with the concept of predictive coding to build our divide and conquer PC algorithm.

And we basically find that you can use this at roughly the same scale you would with VAEs.

So of course, in order to test it out, we had this deep latent Gaussian model.

It's actually the same one from, I think, the Monte Carlo predictive coding paper out this year.

We really did this a lot, like, leaning on, you know, comparing to other predictive coding papers rather than to some arbitrary baseline, because, you know, we figured, if we pitch this as being about bioplausibility, then we can sort of maybe actually get results that are good enough to publish.

Anyway, yeah.

so you know we racked up our first win there you know we went into other algorithms that were related or inspired us really

actually interesting is you know basically we would say all right you have yeah oh um you know after you know demonstrating that this quote unquote wins like okay it works well enough the which

Oh, this entire slide.

Yes.

Okay.

Yeah.

So our DCPC, in addition to being a predictive coding algorithm, is a particle algorithm.

So it's training a particle cloud, so to speak, to approximate the posterior distribution in a Bayesian inference problem.

So as such, we compared it to this algorithm particle gradient descent, PGD,

which we were drawing pretty direct inspiration and then we also compared it to you know langevin predictive coding out at icml this year and so we compared on you know the fresh at inception distance that people use in deep learning rather than on you know the um variational bound to the surprise

You know, we didn't compare on log evidence because that's, you know, you can often get very bad visual samples from a model that has a very good log evidence, actually.

So in order to, you know, sort of say, like, okay, that, you know, log evidence is not the only thing that counts, we, you know, compared on this fresh at inception distance.

Over here, we've got some samples from our pseudo-posterior predictive distribution.

So it's a proper predictive distribution, but constructed from a pseudo-posterior.

Pseudo in the sense that we're fitting, I think, a small Gaussian mixture model to our big particle cloud of the actual posterior distribution.

And then having done that, we say, all right, now we'll draw samples from this GMM as a pseudoposterior, then pass those latents through the generative model again to generate samples.

And the actual neural architecture here is basically a one-level VAE.

You just chop off the encoder.

from a one-level VAE that would be used in these baseline papers, use the same neural architecture in PyTorch, and then basically just got slightly better results visually by just improving the inference algorithm and the training.

Okay, did you manage not to lose that?

Cool.

Okay, so sort of the interesting stuff about it, you know, especially compared to previous work, is how this actually works.

As I said, it's a particle algorithm, so we're actually drawing samples.

I think I said before we went live, you know, this is sort of the year of Monte Carlo predictive coding methods.

We're one of maybe, you know, three or four that are out this year.

So after you've initialized your graphical model by just doing ancestor sampling to get yourself some initial cloud, what you can do during every inference pass is start from the bottom up.

So that's the right over here would be the quote unquote bottom of the hierarchy.

can calculate you know your prediction errors which are the same kind of thing as in previous predictive coding work you know there's score functions with respect to the latent variable and the thing that we actually do differently from other work is that rather than just taking like a score function as a local gradient of the event of

let's say rather than taking a score function as like a direct gradient of the variational free energy we take score functions with respect to the log complete conditional distribution so like a coordinate update distribution if you have you know this variable z1

then its complete conditional is the distribution of this variable Z1 given its entire Markov blanket.

So given both the predictions coming in from above it and the observations coming in from below.

So that's why rather than one error unit here, we have an error unit that connects Z1 to the observation below it.

We also have an error unit that sort of recurrently tries to draw Z1 back towards its local prior until that prior is changed by an update at Z2.

But if you think about how Bayesian inference works, Bayesian inference is just what you get when you have one distribution and it has to be normalized.

but you're trying to fit it as closely as you can to both the likelihood and the prior at the same time and so when you try to do that locally within a big structured graphical model you know the logical posterior to go after is the complete conditional and that's what we actually did

And I want to shout out Hal here because he's not on the stream, but his previous work on amortized population Gibbs samplers is really what showed that this kind of sampling from the complete conditionals could work at scale and inspired this project pretty directly.

This is kind of follow-up work to his paper on that.

And the way that we sort of make this work in terms of constructing a free energy or in terms of making sure we target the correct joint distribution is that we can use Langevin proposals.

If you have a prediction error from E sub zero and another E down sub one, add them together.

you have the gradient of the log complete conditional.

Now it's the log unnormalized complete conditional, but that's fine because, you know, the gradient doesn't like, you know, when you take a gradient of a log density, the normalizing constant falls out.

And so you can target that with, you know, a Langevin dynamics proposal, which just says, take your previous prediction,

you know, multiply your prediction error by a learning rate, add the prediction error to the previous prediction, and then add some Gaussian noise.

You know, you have to balance like the, you have to balance the magnitude of the Gaussian noise with the learning rate in a certain way, but like that's a detail.

Really it's saying let's be in the latent space, then let's follow the gradient, and then let's inject a little bit of noise so we explore most of the time.

rather sorry so we explore some of the time while exploiting whenever the gradient signal is strong enough and then you know i note that we use sequential monte carlo here and what that's actually for is that sequential monte carlo algorithms give you a whole set of rules for how you can start from you know the proposal that you sample from then go to the local target

that you are actually trying to sample from, and then combine those in a neat way to get samples from the complete joint distribution.

So you would start here from the data on the right, do this predictive coding step for Z1,

be done with it, repeat the same predictive coding step for Z2, but now with a new prediction error from Z1 down here, repeat all the way up here, and then you have a bunch of samples from different complete conditionals, and you have to come up with some way to say, well, what's my free energy?

Or in SMC terms, what's my importance weight?

And that's really where

you know, that's really where like the SMC technique becomes helpful is for proving the whole thing is correct.

So if you think, you know, we named it divide and conquer because, you know, what we're essentially doing is we divide the graphical model into these complete conditional units.

Then we conquer each of them with, you know, basically a Monte Carlo predictive coding proposal.

And then we conquer the whole thing by recombining them.

So it really is like a divide and conquer algorithm in the classic sense.

Also, the reviewers wanted us to name it something clearer than the old name.

All right.

So then, you know, that's a summary of the talk, really.

And then we can go into like details of the paper and, you know, anything else we want to talk about.

But broadly, the idea here is that predictive coding as a theory enjoys biological plausibility for approximate Bayesian inference, which is actually this really hard task.

In deep learning world, people use all of these awkward neural network

arrangements to try and amortize inference and then the people who are real purists are just doing markov chain monte carlo because amortized inference isn't good enough for them and to be fair it's often not very good if you don't you know structure your amortized posterior like just so and more to the point in neuroscience you know like predictive coding theories

have often been you know built on like mean field assumptions about the approximate posterior so then you just have a coordinate wise representation of the approximate posterior and it's pretty well known that like this can't actually get you a very close approximation to a correlated structured posterior

So that was where the trouble was.

That was the problem that we took a first step on solving.

And we got it to work well enough that you can substitute it for neural networks in some of these cases where people use neural networks.

And we're going to be at the poster session next Friday at NeurIPS if anyone wants to come and talk to us.

You know, and of course, we have to mention the places that we work and our affiliations.


SPEAKER_01:
All right.

Awesome.

Thank you.

Thank you.


SPEAKER_02:
I'll stop the screen share.


SPEAKER_01:
Yeah, I'll ask some questions.

And if anyone in the live chat has questions, they can write.

Maybe just starting with the structured part.

What do you mean that

It's a structured problem.


SPEAKER_02:
Oh, yeah.

So, like, this is the experiment, you know, originally, I looked at this and said, well, how do we actually replicate how's results from amortized population Gibbs, and that was like a full on time series model.

in which there were you know like some local variables that were updated at each time step but also some global random variables that were standing you know at the hierarchical level above so you know that was really like I thought okay we need to tackle these kinds of structured problems because you know if you I mean go look at I'm trying to think like

You know, honestly, most of the time, most interesting generative models are going to be kind of structured.

If there's more than one latent variable, and it's not just IID sampling, so any kind of hierarchical Bayes, you know, time series structure, dynamical system structure, like all of this stuff that you find in cognitive science and neuroscience, so, so commonly,

And yet, which is often, you know, we've often sort of given up on doing anything better than particle filtering or amortized inference if you look in the machine learning world.


SPEAKER_01:
okay and the graphical structure you showed was a linear hierarchical model or it was going from side to side but that's just graphical convention so do you ever need to account for branching or any other kinds of structures or how do you divide and conquer when there's also splitting and merging so i mentioned about markov blankets right


SPEAKER_02:
As long as you're in a directed acyclic graphical model, then there's still a notion of a Markov blanket for every single random variable.

That's the thing that you divide the graphical model into, is variables with their Markov blankets.

And this is sort of intended to be a bit free energy principle.


SPEAKER_01:
perhaps bringing the notebook up would help

I'll ask the two questions, so return to them as you see fit.

The first is, how do you specify the model?

What does it look like before training and what does get updated during training?

And the second question is, how do you take that step as a modeler between the structure of a given problem, like the handwriting digits and adapting that

to what is that something more generic or does that graph need to already from some other source have structure that's amenable to the target


SPEAKER_02:
So the target density that we would try to sample from is defined by the graphical model structure.

And yeah, that has to come from somewhere.

So conventionally, if you're doing machine learning engineering, then that's sort of up to your choice as the user of our algorithm.

It depends on what domain knowledge you have about your application.

and if we were talking about brains then that structure of the generative model is something that evolved you know and there could be you know and I should mention you know the recent verses work like of course you can also have you know structure learning problems you know throughout like development but

throughout development and through certain behavioral tasks.

But that gets you a bit beyond the realm of just graphical models.

And so we basically aimed at just a given graphical model.

Take a graphical model as given and try to do inference in it.

You can work your way up to the harder stuff later.


SPEAKER_00:
In practice, we often use deep neural networks.

because we tried to replicate the experiments of other papers.

So we had multilayers and the convolutional models.


SPEAKER_01:
Yeah, maybe share the notebook and or the repo.


SPEAKER_02:
Working out, yeah.

Oh, the GitHub repo is like, yeah, we can share that pretty easily.

I'm going to put this first in the Zoom chat, then share my screen.

Yeah, so in the GitHub repo, we try, we attempt.

Just zoom in a little bit.

That's actually, sorry.


SPEAKER_01:
Yeah.


SPEAKER_02:
Yeah.

So in the GitHub repo minus a requirements file, we try to share enough that you could hypothetically download and run this like just from your command line.

You would need, you know, a Python environment that's got the necessary stuff installed, but all of that is standard stuff.

So if you wanted to just rerun the MNIST experiment, you would literally run this command on your command line.

And then you could also start your tensor board and watch it train.

Then we include evaluation notebooks that basically would help you to replicate the figures from the paper.

And yeah, we actually have

Yeah, we also share some pre-trained weights and particles.

I know that's probably a bit of a colliding name with weights and biases.

In this case, the weights include the biases.

Weights would mean neural network weights, and then particles would mean the frozen samples from the posterior distribution for random variables.

And then, of course, we just give, you know, the paper or sorry, the table of results.

And as a small bonus to everyone else who has to try and replicate our results, we include an implementation of the discretized Gaussian distribution, which is otherwise like buried in the diffusion models literature.

Thank you very much.

Yeah, and it's under MIT license, so you can pretty much go ahead and fork this, contribute that.

It's in the universal probabilistic programming language, Pyro, since that's basically the largest deep probabilistic programming language.

Let's take a look at some of the notebooks.

Let's see.

That one's going to look like garbage.

This one's going to look like not garbage.

Yeah.

So what this does is, you know, it's basically like PyTorch and PyTorch lightning, you know, so you're going to like read some JSON file and that's going to give you, you know, your data, your model and your trainer.

then you're going to remind the trainer to just load the weights and particles from a checkpoint.

The random variables are saved as part of the PyTorch light and checkpoints.

Yeah.

It loads them, blah, blah, blah, blah, blah.


SPEAKER_01:
Right there when it says 357,000 trainable parameters, what is that number coming from?


SPEAKER_02:
That is the number of neural network weight.

Like that's the number of float 64s in the neural network weights.

So that's actually a relatively small number because this was sort of a toy problem.

And normally it would be more in the millions.


SPEAKER_01:
And where were those combinatorics specified?


SPEAKER_02:
Right here in the architecture specification.


SPEAKER_01:
That's after it, though.


SPEAKER_02:
Yeah, because you have to load the thing to print the thing.


SPEAKER_01:
Okay, okay.

That's just printing it out.

But you specified it elsewhere, what the structure would actually be, loaded it in, printed it out.

Okay.


SPEAKER_02:
Yeah, the JSON file tells you which modules in the Python code you have to load and all of that.

So if we go look at what is the actual training file that we were loading here, it tells you load this model module, pass it these hyperparams, checkpoint this often, load this data set with this hyperparameter and batch size.

you know here's like learning rates and the number of particles you know the number of sweeps meaning like passes over the graphical model you know that you do per training step yeah this is like a relatively standard deep learning type stuff

Yeah, and when we've loaded the architecture, we actually print it out.

So it's a simple isotropic Gaussian prior.

And then we're running it through a linear layer, an activation function, a deconvolution layer.

some batch you know the then you know each layer includes batch normalization you know two more convolutional layers with their batch normalization and non-linearity two more convolutional layers again and then finally a deconvolutional layer that gives you the correct image size and channel number

Well, that's rude.

Anyway, yeah, so then we have this method we call load particles, which just tells you that's really for loading the particles into GPU memory, once you know which data items you're testing on.

again like relatively standard deep learning stuff, then here we say we're going to open a Python context handler by clamping the graphical model to the pre-trained particles that we loaded from disk.

So if you load your

posterior particle cloud from disk.

Then you have to say to the graphical model, I'm testing right now.

Don't do any additional inference steps.

Just use these samples as your distribution.

So then we run.

We average over particles to get our reconstructions of training images.

Now we're going to plot them.

here's the actual reconstructions so like they're you know kind of blurry like you expect from a VAE but it shows that you know things did actually work then you say okay I don't want to look at reconstructions of training images you know I want to know do I have a trained generative model so you clear all that stuff out and you're going to get

all of the training and validation particles from disk for fitting your pseudoposterior.

And you're going to predict some new images using the pseudoposterior.

Then lo and behold, you plot out the samples from the pseudoposterior.

kind of blurry and they're not as varied as you would sort of like them to be but that is what happens when you have a very simple generative model like you know if you go back to this neural architecture here this is nothing like a state-of-the-art you know diffusion model or whatever that you would use for image generation it's really just a bunch of neural layers stuck together mapping a latent variable to an observation

Yeah, and then we're gonna, you know, having generated those, we go through, we calculate, you know, some metrics across the validation data under a series of different random seeds.

We can also sample from the prior rather than the pseudo-posterior, and that gives us lower visual quality, but more variation, more realistic variation.

And then, you know, we calculate the statistics that we reported in the paper.

Plus some statistics we don't report in the paper now that I think of it.

Like the effective sample size.

That one's a good one for inference nerds.

You divide that by the number of particles you're using, and it gives you a sort of percentage that tells you how well are you actually fitting the true posterior distribution with this batch of samples.

And in our case, the answer is, yeah, about 92% effective samples.


SPEAKER_01:
Thanks.

Maybe could we walk through a similar logic for the MNIST handwriting?


SPEAKER_02:
Yeah, totally.

Let's see.


SPEAKER_00:
I think it would be almost equivalent.

It's just that we use a different model.


SPEAKER_02:
Yeah, it's pretty much the same, but with, you'll notice there's quote unquote three decoders here.

So each of these is actually performing a random Gaussian sampling step in between.

So this is a proper hierarchical Bayesian problem.

So you have a prior.

Then you have a forward conditional and a forward conditional.

So you have three levels of latent variables.

And then finally, you have your likelihood.


SPEAKER_01:
What leads you to use those here?

Like do you sweep or scan over two, three, four layers?

Do you explore out features equals other multipliers?

Or what led you to this specification for the Lightning DCPC?


SPEAKER_02:
So in this case, we took this exact specification from Monte Carlo predictive coding, which is over here.

So we wanted to compare inference algorithms head to head.

So we picked exactly the same generative model structure and replicated it.


SPEAKER_01:
Awesome.

Yeah, that makes a lot of sense and it helps with the continuity and isolating the inference algorithmic component.

Is there a...


SPEAKER_02:
in a methods or a procedural step anywhere other than these or or all that are needed these json configs and the notebooks like this so this is runnable like this should be runnable if you actually just download it and you know if you download it and follow the links to download you know the pre-trained weights put them in the path that the notebook expects here

then you should just be able to run this.

It should be point and click.

I say should because, you know, like I've tried it, but I've tried it once.


SPEAKER_01:
Cool.

Definitely opportunities for people to explore.

And then what about the training step?


SPEAKER_02:
So that would be done from the command line as the repo readme tells us.

That's this step.


SPEAKER_01:
And you watch on TensorBoard and make a manual call based on diagnostics?

Or how else do you determine?


SPEAKER_02:
I mean, I would generally say let it run, at least for these experiments, let it run to completion.

Like, you know, I know that if I'm stopping something partway in because of the diagnostics I see on TensorFlow, you know, like, for instance, a very noisy, staticky, like, loss curve, well, that's how I know when it's not working.

You know, the experiments that are mentioned in the paper are actually, you know, known to work.

Like, you can just leave that running until it's done,

and you will get, subject to differences in your random number generator, more or less the result we got in the paper.

I actually want to say this is one of the nice things about the culture of machine learning these days, is that everything is really expected to be not just replicable, but idiot-proof replicable.

As in, if you cannot just run this by rote and get the same result, then reviewers don't believe the result is real.

Which is very much for replicability of science the way it actually should be.


SPEAKER_01:
So we looked at faces and digits.

Let's say somebody were looking at another kind of image classification problem, cats and dogs.

So what would need to be adapted?

What kind of data would be in for the training step?

And then to what extent do these same lightning DCPC specs structures

work or to what extent do the first or the secondary layers need to be adapted to alternative cases?


SPEAKER_02:
So this thing that gets called a graph inside the Lightning DCPC, you'd have to write a new one of these classes for a new problem.

So let's say, for instance, that you want to do semi-supervised Bayesian learning.

My old advisor from my PhD had a paper about that.

So if you're doing, say, cats or dogs, then you're going to want to specify a new graphical structure that's rooted in a prior over your latent embedding.

And then it should have a pair of likelihoods.

essentially you know one being to the class label which is going to be it's semi-supervised so let's say you only get class labels on about half the samples and the other is going to be you know for the actual image which is going to be fully observed you know and then after writing what is it a um a high torch lightning data module for your data set

which like tells you you know when which sorry it handles figuring out which data points are semi are supervised versus unsupervised because it's a semi-supervised problem and then yeah you would put that together and run it more or less you know you would write another little json file and just run it and hopefully you would get

hopefully if your architecture for the generative model is rich enough, then your latent embedding space should begin to capture differences between the supervised classes and give you good prediction on classification.

And then if you wanted to do classification style testing or evaluation, you would present a whole new image as the observation

then just perform joint Bayesian inference on both the latent embedding and the class label and then check that your class labels are good so to speak you know for an unobserved class label you're not getting a one hot vector you're getting

you know, a series of logits or like an element on the simplex that specifies probabilities of class membership.

So, hence my saying good rather than absolutely correct.

You never get a completely absolute yes-no answer out of a classifier.


SPEAKER_00:
At test time, at least.

I think in general, you could also generalize it to different kinds of data types.

So it does not have to be, for example, continuous data as images.

You could generate discrete distribution, or you can also generate time series distribution.

This is something that has not been tested in this paper, but in theory, the theoretical framework would allow to do so.


SPEAKER_02:
Yeah, like we have an appendix in the paper that extends the math to discrete distributions.

That does take a mathematical extension, and then that would take some more coding.

So we don't make any promises there.


SPEAKER_01:
so in in past dreams and and journeys eli as you mentioned we discussed with dean and others the anticipatory allostasis and then tomaso we discussed the causal modeling so where would you triangulate this work or how do you see it as sort of

drawing on or complementing or or not but just interesting that it's it's not a homeostatic model often in the organismal ethological active inference there's a lot of focus on

organismal behavior in these kinds of topics homeostasis allostasis being critical there and then in the bayesian modeling space often there's a focus on the interpretability of smaller models and carving nature at the joints and all of that not this more machine learning sort of specify it in a sequence of convolutional layers

So how do you see all, just having worked on those different areas, how do each of you see, because clearly this is a relevant contribution and advance, so in what direction or how do you relate it with those prior pieces?


SPEAKER_00:
I think it depends in which direction you want to push this work a little bit forward.

So for example, if we're looking at the machine learning side,

and we want to scale it up to larger models, larger convolutional networks, you don't get interpretability because of this method.

I think the same interpretability you would have with the old algorithms, you have them with this one.

What you gain, apparently, what this work shows, is that you get better performance.

but then of course if you apply if you apply to for example uh directed graphical models or vision networks that have a specific causal structure that you can observe in that case you have the you're able to to stay you can do for example the the intervention kind uh queries that i that i presented here like a year ago or something like that and uh but i think i think that's not in the algorithm is more in the kind of model you

you pick up is the kind of task that you want to solve and whether you care about it or not.

So let's say maybe for generating faces for now, we did not care about having an interpretable model.


SPEAKER_01:
What would you say?


SPEAKER_02:
Yeah, Eli.

I would sort of compare it to something like a car engine.

You need to design a car engine of a certain size and power in order to put in a certain weight class of vehicle.

But that doesn't actually determine whether that vehicle is a Honda or a Jaguar.

If it's the same size and weight class, then you can actually use a very similar engine inside both of them.

And I think that's sort of, on the one hand,

the both the beauty and the pain in the ass of working with Bayesian methods is that you know the promise on the sticker is that here's Bayesian methods and you're going to solve a very broad range of problems with by reducing them to the same problem which is Bayesian inference and that's you know sort of what the free energy principle is all about is saying it's all really just reducible to variational inference right

now what you do find if you go look at like pi mdp or a lot of you know free energy type papers is that they'll design a model for the paper in order to model some piece of interesting organismic behavior but they'll include some real restrictions on the expressivity of that model which is that it has to be solvable by something like say variational message passing

So then there's the question of, well, how do you stop restricting the model class according to what inference algorithm you're going to use?

And that was really like the dirty secret of Bayesian methods that I learned in grad school was you're always restricting the model class to what you have a good inference algorithm for.

Or at absolutely last resort, running something like

plain old metropolis hastings with a gaussian random walk proposal and just waiting for like three weeks of compute and hoping so you know what we actually want to do here is to say we have a better engine like literally we have a better inference engine you don't have to you know now structure an entire neural network architecture of inference proposals

You don't have to play graduate student descent with your inference algorithm too much.

Assuming that brains have some kind of wondrous generic inference algorithm that solves so many different Bayesian problems all at once, let's try to model that and then just use it because it'll be more powerful and more efficient.

than you know customizing to the particular generative model you want and you know really it's like first baby steps but it is encouraging that you know if you i think we show in the table yes like we trained a variational auto encoder you know as a baseline

To say, well, what if you're doing variational inference by actually, you know, training a neural network to do it, to do it for you.

And it's encouraging to show that we can do a little bit better with like a generic inference algorithm that doesn't need a separate neural architecture.

You know, compared to like build a second neural network and train it jointly with the first one.


SPEAKER_00:
think something that is also exciting about this i mean at least for me is that we like we've been we've been working a lot with creative coding algorithms for machine learning in and i guess you can clearly separate them in like supervised learning and unsupervised learning and in supervised learning for example you train like large convolutional models on c410 c400 or tiny image net it seems that we've kind of like hit a momentary wall so it's

up to a certain point, it's becoming really hard to scale up those algorithms.

And I think like the old community and it's not it's not only predictive coding, like when I talk with, for example, the equilibrium propagation, three people with all the life by a plausible deep learning community, there's a there's a kind of similar

problem that is a when the model is more than seven eight layers deep it doesn't perform well and for example scaling it up to image net is a pain so a lot of people are taking a step back and say okay let's do a little bit more research to find out why and generalize and try to improve the performance in when using for example predictive coding for generative ai so to generate images to sample data points in an unsupervised way

the first problem that we had is that like in machine learning we had a couple of until like two years ago it was a mostly a deterministic algorithm so we were taking mostly raw and ballard's version which was the classification one there was no stochasticity so a bunch of people including me started publishing like associative memory papers it's like i give it a data set and i can by giving it a little bit of information you can regenerate exactly the same data points

And then in the last year and a half, as Eli said, some people started developing sampling algorithms.

And basically, the papers before ours, a little bit, either they do it on MNIST and Fashion MNIST, or they do it on colored images like C410, C400.

But they use some kind of gradient descent, so it's not a completely local message passing algorithm.

And what we did in our paper is that we have a local message passing algorithm and we compare against theirs by using basically exactly the same architectures, exactly the same number of epochs.

I think in some cases even exactly the same learning rate and things like that.


SPEAKER_02:
As a small correction, we aren't actually a message passing algorithm.

We're not technically any form of some product message passing.


SPEAKER_00:
that's a local algorithm let's say yes local algorithm yes and uh and and we do well like for example the images of faces you've seen may not be may not seem perfect but they are at least from my perspective better than expected and the thing is we haven't i don't i don't feel like we have reached a wall in terms of performance like we've tested only on the architectures that other people have used before and we do well

And for now, there's little knowledge on how much this can scale up.

Maybe you scale it up like two layers more and it doesn't work anymore.

That's something that we don't know yet.

But potentially, it would be interesting to, for example, something I was messaging with Eli earlier this week is, can we scale it up to larger decoder-only transformer models, for example?

Not like, of course, LLMs or things like that, but for example, something that is like a

four, five, six layers deep transformer model, that would already be quite an interesting result.

Definitely not an easy one.

But I think there is definitely room to improve the results of this paper.


SPEAKER_01:
Could you also maybe highlight, how do you see local update rules as superset, subset, what have you, of message passing?

What differentiates the local locality of connectivity from something that you would explicitly, Eli, call a message as passed?


SPEAKER_00:
That's an interesting question.

I guess there's a I think for message passing, you could call like all the belief propagation algorithms, or at least basically some kind of this class of algorithms where you clearly state which information has to be passed from from basically one latent variable to the other.

While local message passing, and they're in those kinds of algorithms, they're all local.

So it's a

Since you define them to be this way, they must have local information.

Well, probably if you generalize to local message passing in general, there are some proposals, maybe including this one, in which maybe things are not supposed to be local.

Like maybe you can, like since it's a structured learning algorithm, you may, in theory, get information from other, from latent variables that are more far away.


SPEAKER_02:
but is that but then when you actually do the computation they are local but that's that's an answer and giving now to the spot maybe like as a better thought on this so i would say that message passing algorithms differ from ours in both their means and their goals and really it's the goals actually that they're starting from so that family of algorithms

When people were working on it maybe, I don't know, 20 years ago, what they were really trying to do was to get a summary of a posterior marginal distribution.

So they just wanted to be able to pick out one latent variable for an inference query.

or a decision they needed to make say what is the posterior on this variable given the data not given any settings for all the other latent variables and that sort of

restricts you know the space of tasks that you can really address with message passing and it also you know does end up meaning that you know what you're basically doing is some form of some product or really integral product you know algorithm

And that gets you all the way out to, I think, probabilistic circuits are the most current and advanced form of some product-based exact inference algorithm.

And they only support tree-shaped generative models rather than those that are directed acyclic graphs in general.

So trees rather than forests.

And we said, well, let's try targeting, you know, the full joint distribution of all the latent variables together, partly because that's the most general form of evasion inference problem.

And partially because, you know, if you have something like a hierarchical time series model from the active inference literature, you might need several levels of the hierarchy.

You might need the latent variables across those levels in order to make an action decision.

several levels of hierarchy might actually parameterize your policy or the thing that defines the inference problem you care about solving as a modeler.


SPEAKER_01:
That's very interesting, again, to the sort of settings that a lot of active inference literature has tackled.

In these cases, it's like digit in, pixel intensities, et cetera, digit classification, generativity of digits out.

How would it be similar or different with something like

digit pixels in teammate's behavior out or robot arm out are you leveraging this uh identity or or at least uh

structural congruence with the recognition and the generativity?

Or like a VAE, can you have something different coming in and have the output be something that's categorically different than the type that was used as input?

Like action.


SPEAKER_02:
So I think one of the advantages of defining anything as a Bayesian inference problem is that it really changes your conception of what's an input versus an output variable.

For a Bayesian inference problem, you start with a graphical model.

You clamp several variables to be observed data.

Those observed data are now your inputs.

Your output is any aspect of the posterior distribution that you care to examine.

So we don't have to be outputting classification logits.

We could also be outputting functions.

You could have, say, an active inference time series module where your agent gets some data

at every step of computation, performs a full sweep over the entire history of latent variables, and then and only then actually makes a decision about what action it's going to take.


SPEAKER_01:
which really highlights the importance of being able to specify the full joint distribution and then clamp down with nuance on what you want to be taking as observable, but kind of clamping down from the full joint and then just specifying and then having an inference algorithm that will facilitate that locally operationally

rather than trying to, amidst the constraints of which inference procedures you can perform, work within subclasses of graphical models that abide by those constraints.

It's almost like it's a far cry in some ways from 2022 textbook figure 4.3, the kind of POMDPs and the matrix multiplication that it gets implemented in PyMDP, the sorts of pedagogical cases that do highlight certain key moves.

This, though, looks and feels so much more like a machine learning work.

And it's really interesting, though, what this enables.


SPEAKER_00:
I think you also shaped the work with respect to the venue you want to submit to.

Yes, I was just going to say, now that I've found... When we were discussing which experiments to run,

The first question is, are we going to submit a new REAPS?

Yes.

It's going to be OK.

Then it's going to be mostly image generation and also a little bit the venue and a little bit what did prior work do?


SPEAKER_02:
We picked out image generation because predictive coding networks had not yet been used to get this quality of result on unconditional image generation.


SPEAKER_01:
So other than the conference next week, what ways might you be taking it?


SPEAKER_00:
I mean, probably there are many future directions.

I guess from the perspective that interests me the most, I would be really curious about doing what I mentioned earlier about scaling it up.

So for example, how much can we push this kind of algorithm?

So I think at some point, if we're going to have enough compute and maybe a faster, this is like some fast training, but probably if it's implemented in a parallelizable library or things like that, you can run larger models, you can run a huge number of hyperparameter search, and you can see at some point where it scales up.

Because it's also true, we didn't do a huge hyperparameter search at the end.

We did some, but it's definitely not comparable to what's standard in a machine learning paper in which they try thousands of combination of hyperparameters.


SPEAKER_02:
No, we did not do grid sweeps of hyperparameters.


SPEAKER_00:
Yeah, yeah, exactly.

So I think there's room for pushing this a little bit more.

and may hopefully much more but this is I think scalability is something that is really hard to intuitively predict.

Like why some models work well on seven layers and then attend they collapse is you just just do it and notice that and then I guess you can do some interpretability afterwards and understand the reason.

But doing it before is always a tricky thing.

And so I guess trying it is probably the

If you have a method that allows you to try fast and fail fast is the best way of scaling it up, I guess.


SPEAKER_02:
Yeah, so I would also say, based on some of the not so publishable results that I've seen in, for instance, trying to replicate Howe's work with bouncing MNIST digits, how well this works often does seem to depend on exactly how much structure

the generative model provides you you know to give like prediction errors that point in the right direction rather than well in exactly the right direction rather than just vaguely the right direction right so you know what is sort of how smooth versus how peaked tightly peaked is the landscape of your like is your free energy landscape

And when it's more tightly peaked, this algorithm seems to work significantly better.

Versus when it's not so tightly peaked, it'll often sort of get into the region of valid solutions and then get very noisy as it becomes unable to decide which of the potentially valid solutions is really the best one to converge on.

I would say it's not so good yet at dealing with multimodal posterior distributions once a good generative model has been learned.

So they're scaling it up to that.

I'm also interested to see how far could we take this on a fairly standard machine learning problem by just saying every time we have more layers than would be biologically plausible to differentiate,

just insert a random variable, like parameterize a Gaussian right there, and then just say, okay, now, you know, the amount of gradient calculation you have to do between layers is biologically plausible.

And we'll use the predictive coding math to actually, you know, effectively solve like the, we'll have the random variables substitute for back propagation in a certain way.


SPEAKER_01:
What do you mean by that?


SPEAKER_02:
So if you have like a 12-layer, I think, Tommaso, you said like a 12-layer energy-based transformer or something?


SPEAKER_00:
So I think the general concept about biological plausibility is that you have a latent variable, another latent variable,

anyone, anyone that is basically computing the gradients of the parameters to depend on both latent variables, and not on other stuff that are in between.

So for example, if you consider the function that goes from one latent latent variable to the to the first from the first to the second, to be a three layer network,

the gradients of the weights of the intermediate layer or of the first they would depend on the last latent variable and this would make it basically make you break the locality assumption and biologically biological plausibility while if you consider like a function to be the function to be a single feed forward or convolutional layers

It means that for every weight, you have a latent variable before it.

So you have a presynaptic neuron and a postsynaptic neuron actually in practice.

And this will basically allow you to, once you compute the gradients of the free energy, to have everything that depends on presynaptic and postsynaptic neurons.

So this is the concept of biological plausibility.

The problem that, for example, I'm noticing when you do classification,

is that doing this allows you to propagate the error a little bit less regularly than backprop does because backprop is sequential so it shines a little bit for very deep models especially if you have the right activation function so as a so i guess that's what eli was mentioning when he was saying about yeah so if you have for instance 12


SPEAKER_02:
If you have, for instance, image, 12 layers, classification, then roughly speaking, getting a gradient for the activations of the middle layer of the neural network depends on all the way on the final observation.

Now, what we'd like to say is, well,

Ultimately, it's been proved that predictive coding in the limit implements backprop, just very slowly.

But what we'd like to do is to calculate gradients a bit more quickly.

And so what I was saying is that what you could do is take the middle layer and just say, well, this is going to be a random variable now.

Instead of a single deterministic activation that has to be exactly right,

in order to pass gradients through, this is now going to be a Gaussian distribution over activations.

And having done that, you can then use our predictive coding algorithm to say, I'll try and propagate through the latent space of possible weights.

Or actually, no, sorry.

I'll try to propagate through the latent space of layer activations.

find a relatively good solution, or rather a particle cloud of relatively good solutions for those, and then update the weights in order to make those activations more likely without depending on the final classification.

So without depending on the entire backwards computation tape of backcrop.

But at the cost of being a little bit random, let's say.


SPEAKER_01:
What's wrong with that?


SPEAKER_02:
Well, nothing say probabilistic machine learners and say free energy principle people and active inference people, but currently the machine learning world doesn't like it very much.

They say Bayes doesn't scale.

We're going to teach them they're wrong about that.


SPEAKER_01:
Well, it's super cool.

If you have any other things you want to add, otherwise, good luck with the conference and with proceeding.

It's super exciting, and I hope people watching this go to the repo

continue on some of the explorations that you've mentioned and explore it in their own ways too.


SPEAKER_02:
Yeah, I mean, feel free to send questions to our email addresses that are listed on the paper if you are not able to get the repo to work.


SPEAKER_00:
Or if you make the repo work and you have ideas for follow-ups.

Yes, that too.


SPEAKER_02:
Yeah, we've got a few ideas already, like...

You know, I think I was talking to Tommaso and saying, like, I've been meaning to write some code that will allow you to sort of pass a deterministic output between random variables so that we would, you know, not just be able to do like graphical models, but proper stochastic computation graphs.


SPEAKER_01:
Divide and contribute.


SPEAKER_00:
Exactly.

Yes.


SPEAKER_01:
Cool.


SPEAKER_00:
Thank you, fellas.


SPEAKER_01:
Until next time.


SPEAKER_00:
Thank you very much, Daniel, for organizing this.


SPEAKER_02:
Yeah, thanks for running these.

Like, every time a new one comes out, it goes on my Watch Later list, which, admittedly, I never work through, but they do go on it, and sometimes, sometimes, one or two videos.


SPEAKER_01:
Cool.

Cool.


SPEAKER_02:
when I get the opportunity to binge you are what I'm binging thank you see you guys bye have a nice day bye
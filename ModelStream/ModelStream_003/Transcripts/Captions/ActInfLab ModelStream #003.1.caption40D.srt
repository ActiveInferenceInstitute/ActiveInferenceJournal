1
00:00:09,743 --> 00:00:10,944
Hello, everyone.

2
00:00:10,944 --> 00:00:13,013
Welcome to Acting Flav.

3
00:00:13,013 --> 00:00:17,050
This is acting lab model stream
number 3.1.

4
00:00:17,550 --> 00:00:20,787
It's May 27th, 2021.

5
00:00:21,187 --> 00:00:25,592
And we're here with Tim
for Ballin and Ozone Kettle today.

6
00:00:25,592 --> 00:00:30,296
We're going to have a model stream
on some of their recent work on learning

7
00:00:30,296 --> 00:00:33,233
generative state space models
for active inference.

8
00:00:33,400 --> 00:00:37,604
We're going to have a presentation
followed by some time for question

9
00:00:37,604 --> 00:00:38,605
and answer.

10
00:00:38,605 --> 00:00:41,641
So please feel free
to write any questions

11
00:00:41,641 --> 00:00:45,111
in the chat and we will get to them
in the conversation.

12
00:00:45,111 --> 00:00:48,281
So thanks again to Tim
and Ozen for joining us today.

13
00:00:48,314 --> 00:00:49,349
We're really looking forward

14
00:00:49,349 --> 00:00:52,452
to what you have to share,
so please take it away and thanks again.

15
00:00:53,553 --> 00:00:55,622
For or so.

16
00:00:55,622 --> 00:00:58,691
Thank you, Daniel, for having us.

17
00:00:58,691 --> 00:01:03,530
So I'm Tim Burton and together
with the table, we will talk a bit

18
00:01:03,530 --> 00:01:06,032
on our paper on learning

19
00:01:06,566 --> 00:01:08,802
state space models for active inference.

20
00:01:09,335 --> 00:01:14,707
So we are both researchers
that I make university in Belgium.

21
00:01:15,875 --> 00:01:16,876
If you want to know more

22
00:01:16,876 --> 00:01:20,847
about what we do,
you can read about it on our blog

23
00:01:20,847 --> 00:01:25,318
or follow us on Twitter
on the rest over at the Smart Robots.

24
00:01:26,352 --> 00:01:29,823
So before we dove

25
00:01:29,823 --> 00:01:33,960
in, maybe a short introduction
on what we do, what our goal is.

26
00:01:34,394 --> 00:01:37,797
So basically we want to build
intelligent robots in our lab.

27
00:01:38,198 --> 00:01:43,470
Understand
we have a pretty big research base

28
00:01:43,470 --> 00:01:46,806
where we have some room
for robot manipulators on the one hand,

29
00:01:46,806 --> 00:01:50,577
but also driving and flying robots
on the other hand.

30
00:01:51,344 --> 00:01:55,081
We typically all kinds of sensors
on these things,

31
00:01:55,515 --> 00:02:00,620
and then we want to process
a source of information and infer

32
00:02:00,854 --> 00:02:05,892
some useful actions, for instance,
and of course effective inference.

33
00:02:05,892 --> 00:02:11,064
This agreement,
the biology to try also to

34
00:02:13,099 --> 00:02:16,302
further
investigate and it's in this context

35
00:02:16,302 --> 00:02:20,140
that basically this this work,
this is being done

36
00:02:22,075 --> 00:02:25,211
so why would you bother learning
state space?

37
00:02:25,211 --> 00:02:26,946
So if you look at

38
00:02:27,313 --> 00:02:29,349
active inference, the papers

39
00:02:29,349 --> 00:02:33,586
from the recent years then typically

40
00:02:33,586 --> 00:02:38,158
you will find a figure
like the ones that are on the slide.

41
00:02:38,591 --> 00:02:42,262
So you have this figure of
what is the kind of environments

42
00:02:43,163 --> 00:02:46,032
that you're building
that you were investigating

43
00:02:46,432 --> 00:02:50,570
and then a whole description
of how the model should look like.

44
00:02:50,570 --> 00:02:52,939
So you define a safe space.

45
00:02:52,939 --> 00:02:53,907
And then

46
00:02:54,541 --> 00:02:59,679
possibly in the case of the streets,
state space, as you defined

47
00:02:59,712 --> 00:03:02,882
the so-called A, B, C and D matrix,
that's

48
00:03:04,551 --> 00:03:06,219
the size of the model.

49
00:03:06,219 --> 00:03:09,489
So what is the likelihood
to see a certain observation

50
00:03:09,489 --> 00:03:13,092
when you're in a certain state
or other transition models?

51
00:03:13,626 --> 00:03:18,231
How do you transition from one state
to another and so forth?

52
00:03:19,332 --> 00:03:24,404
And recently, while we were doing this
research research,

53
00:03:24,404 --> 00:03:27,840
we saw some other people
also thinking about, you

54
00:03:30,310 --> 00:03:34,113
can be do more learning
using these these kind of multiple

55
00:03:34,147 --> 00:03:39,419
deploying techniques in these
in respect of inference methods.

56
00:03:39,419 --> 00:03:43,289
So, yes, for example, take also for
who said

57
00:03:44,457 --> 00:03:46,426
those two basically

58
00:03:46,426 --> 00:03:49,829
learn kind of a state space
for for the multicore.

59
00:03:50,163 --> 00:03:54,300
But then still
he needed to explicitly encode

60
00:03:54,367 --> 00:03:58,638
some of the of the information in state
after

61
00:03:59,906 --> 00:04:02,609
there was also some work from from
from there

62
00:04:02,609 --> 00:04:05,245
in Munich who basically

63
00:04:06,512 --> 00:04:08,114
that's learning.

64
00:04:08,114 --> 00:04:12,585
But more on learning the policy
rather than the state space.

65
00:04:12,585 --> 00:04:16,522
So this could use more of
this is your process

66
00:04:16,522 --> 00:04:21,027
with small state spaces where you would
basically just deal with the

67
00:04:21,427 --> 00:04:24,264
the raw state space the reservations

68
00:04:24,264 --> 00:04:28,368
that I gave you where
we're basically suited as a state space.

69
00:04:28,368 --> 00:04:31,137
I think would
then if you would then investigate

70
00:04:31,404 --> 00:04:34,741
how to learn the actions
given given these states.

71
00:04:36,142 --> 00:04:38,611
Our work is basically on

72
00:04:38,611 --> 00:04:43,349
what if you don't know the state space,
what if your observations are dimensional

73
00:04:43,349 --> 00:04:47,220
and you cannot really use these directly
as your state space?

74
00:04:47,220 --> 00:04:48,855
How do you define the model?

75
00:04:48,855 --> 00:04:50,356
How are you going to

76
00:04:51,324 --> 00:04:53,693
and maybe as a concrete example, suppose
you have

77
00:04:54,060 --> 00:04:56,029
one of our driving robots in the lab.

78
00:04:56,029 --> 00:04:59,365
So this this is the first person
view of the robots.

79
00:04:59,732 --> 00:05:03,269
This is the kind of observation that
you get is just the square of pixels.

80
00:05:03,670 --> 00:05:06,839
And then what's
what is your state's business space?

81
00:05:07,307 --> 00:05:10,643
It might be an x y position on the map.

82
00:05:10,643 --> 00:05:14,514
That might be something relevant
for robots as a state.

83
00:05:15,214 --> 00:05:17,350
It might also be what child?

84
00:05:17,350 --> 00:05:18,918
You're approaching a gable gooder.

85
00:05:18,918 --> 00:05:22,255
So there might be a
that might also be something useful.

86
00:05:23,289 --> 00:05:24,524
Maybe the items that are

87
00:05:24,524 --> 00:05:28,361
stored in direct order
or relevant for whatever task.

88
00:05:28,528 --> 00:05:30,663
So that might also be
part of state space.

89
00:05:31,364 --> 00:05:33,833
Maybe there are human workers
that that walk around

90
00:05:33,833 --> 00:05:35,068
that you also need to model these.

91
00:05:35,068 --> 00:05:39,238
So depending on your robots
and the use case, you have those

92
00:05:39,272 --> 00:05:43,676
get tense and what's going to happen
in the environments, it's becomes

93
00:05:44,277 --> 00:05:46,379
to to define kind of a

94
00:05:47,547 --> 00:05:50,149
precise set of states
that you want to break,

95
00:05:50,583 --> 00:05:53,586
let alone that you need some model
that's that

96
00:05:54,687 --> 00:05:58,891
that models this like how do you get
from pixels to your X by position

97
00:05:58,891 --> 00:06:01,094
so there are certain methods

98
00:06:01,928 --> 00:06:06,032
in robotics
that's to do part of these things,

99
00:06:06,499 --> 00:06:12,472
but none of them really just figure out
what is the complete state

100
00:06:12,472 --> 00:06:15,108
space that you need
for whatever you want to do,

101
00:06:15,675 --> 00:06:19,412
and especially not
in kind of an active inference

102
00:06:22,081 --> 00:06:23,850
so before we go into

103
00:06:23,850 --> 00:06:27,687
our methods, I'll briefly

104
00:06:27,820 --> 00:06:30,289
talk about active inference.

105
00:06:30,289 --> 00:06:33,426
Probably most of you
are already familiar with the topic,

106
00:06:33,726 --> 00:06:36,062
but I'll just rehearse
some of that stuff.

107
00:06:36,763 --> 00:06:41,134
Even if it's just to to get accustomed
through our way of, of,

108
00:06:41,534 --> 00:06:44,036
of notation, let's say,
because everybody has its own

109
00:06:45,471 --> 00:06:47,907
notation and it means

110
00:06:47,907 --> 00:06:51,811
it will put us all on the same page
for the next part.

111
00:06:52,578 --> 00:06:56,115
So it all starts with having an agent

112
00:06:56,115 --> 00:06:59,852
that needs to interact
with the environment and the agents.

113
00:07:00,119 --> 00:07:02,121
It's assumed to be kind of separate

114
00:07:02,121 --> 00:07:05,024
from the environments,
from the so-called blankets.

115
00:07:05,491 --> 00:07:09,295
So on the one hand,
the agents can come from actions.

116
00:07:10,029 --> 00:07:13,633
These actions, the,
these will impact young humans,

117
00:07:13,633 --> 00:07:16,369
which is kind of a change of process

118
00:07:17,170 --> 00:07:19,806
that has some, some, some hidden states

119
00:07:21,073 --> 00:07:21,874
and that provides

120
00:07:21,874 --> 00:07:25,678
you, given your action, it provides
you with some sensory observation.

121
00:07:25,678 --> 00:07:30,082
So in this case, an action
could be for the robots to drive around

122
00:07:30,783 --> 00:07:33,519
with some runs on the motors,

123
00:07:33,886 --> 00:07:37,023
and observation could be some pixels
from from the camera.

124
00:07:37,557 --> 00:07:40,126
And then the goal of the agents
is to build

125
00:07:40,426 --> 00:07:43,396
this model where he has his own

126
00:07:43,396 --> 00:07:46,232
events, his own state space,
basically the device.

127
00:07:46,232 --> 00:07:48,434
How actions

128
00:07:49,435 --> 00:07:51,537
have effects on on the states

129
00:07:51,537 --> 00:07:55,241
and how these states then
then generate these these certifications.

130
00:07:56,008 --> 00:07:59,212
So the question is how can you build this

131
00:07:59,212 --> 00:08:02,281
model
that actually comes up automatically?

132
00:08:02,348 --> 00:08:06,552
It's kind of a proper state space
to learn this model

133
00:08:08,588 --> 00:08:09,622
so basically

134
00:08:09,622 --> 00:08:12,725
the drive this model that looks like

135
00:08:12,725 --> 00:08:15,561
a partially observable
Markov decision process.

136
00:08:15,561 --> 00:08:19,465
So it just means that we assume that

137
00:08:19,565 --> 00:08:24,003
that's a state
that's a given time, that it only depends

138
00:08:24,036 --> 00:08:28,307
on the previous states and the action
that's that's you were doing.

139
00:08:28,708 --> 00:08:29,876
And each states

140
00:08:30,943 --> 00:08:34,480
gives or gives rise to certain sufficient

141
00:08:36,782 --> 00:08:41,621
and so then we end up
with the so-called free energy principle.

142
00:08:41,621 --> 00:08:45,358
That's basically what we want to do
is run to build the model

143
00:08:46,592 --> 00:08:48,594
that's that maximizes look evidence.

144
00:08:48,594 --> 00:08:51,597
So here you see the formula for energy

145
00:08:52,365 --> 00:08:55,401
and you can come back in several ways

146
00:08:55,801 --> 00:08:58,905
so the first rate is basically

147
00:08:58,905 --> 00:09:03,309
by base stating
that minimizing free energy is actually

148
00:09:04,210 --> 00:09:08,047
maximizing the, the, the everything.

149
00:09:08,047 --> 00:09:09,649
So balance, let's say.

150
00:09:09,649 --> 00:09:13,286
And if you're at this free energy
minimum,

151
00:09:13,519 --> 00:09:17,924
then basically you have your approximate
steward that is actually equals

152
00:09:17,924 --> 00:09:21,127
to your
or very close to your your triple steer.

153
00:09:21,894 --> 00:09:26,566
But then the the third line is basically
what we typically use for

154
00:09:26,899 --> 00:09:30,536
optimizing our model, which is basically
on the one hand complexity.

155
00:09:30,603 --> 00:09:31,971
Third, you want to

156
00:09:33,272 --> 00:09:34,507
have your

157
00:09:34,840 --> 00:09:39,645
your state's explain explained away
as simple as possible, whereas

158
00:09:39,645 --> 00:09:44,016
you want to have the highest accuracy
to predict whatever you're observing

159
00:09:44,517 --> 00:09:47,486
so in this case, for variation
or for example, steer

160
00:09:47,820 --> 00:09:51,924
is something that maps observations

161
00:09:51,924 --> 00:09:55,728
and actions
into inferring your preference state

162
00:09:58,030 --> 00:09:59,198
but what about the future?

163
00:09:59,198 --> 00:10:03,035
So in the past you basically knew
which actions you were doing

164
00:10:03,035 --> 00:10:06,572
and you saw the observations
that came in.

165
00:10:06,572 --> 00:10:08,674
So you kind of

166
00:10:08,808 --> 00:10:13,279
infer which states would have given rise
to that observations.

167
00:10:13,279 --> 00:10:15,815
So that's basically
the creation of the false.

168
00:10:16,248 --> 00:10:19,051
But then in the future
things change a bit

169
00:10:19,051 --> 00:10:21,721
because now
you don't know what you will observe.

170
00:10:22,088 --> 00:10:24,390
And also you have to select actions.

171
00:10:24,390 --> 00:10:28,260
You will do so
in your actions you will basically

172
00:10:29,996 --> 00:10:32,832
denote them as being generated
by so-called policy.

173
00:10:33,165 --> 00:10:36,135
But in this case,
the policy is basically just

174
00:10:36,736 --> 00:10:39,171
what is a certain sequence

175
00:10:39,772 --> 00:10:41,774
of actions basically.

176
00:10:42,575 --> 00:10:46,445
And you
now have to create some expectations

177
00:10:46,646 --> 00:10:51,017
not only on future states
but also on future observations.

178
00:10:51,017 --> 00:10:52,885
So what's which state?

179
00:10:52,885 --> 00:10:55,921
I think I will visit
by doing some actions, but also

180
00:10:55,921 --> 00:10:58,991
what kind of observations do
I think that I will see

181
00:10:58,991 --> 00:11:02,128
if I visit these states.

182
00:11:02,194 --> 00:11:06,165
So then the energy becomes
the so-called expected free energy

183
00:11:06,932 --> 00:11:10,803
and again, you can then take this thing,
this thing

184
00:11:13,139 --> 00:11:14,006
as follows.

185
00:11:14,006 --> 00:11:18,310
So basically what happens is again,
you have on the one hand, your

186
00:11:19,478 --> 00:11:22,281
probability of your steer model.

187
00:11:22,648 --> 00:11:26,485
Mine is low probability of your

188
00:11:26,752 --> 00:11:29,188
OK for this model.

189
00:11:29,188 --> 00:11:32,925
And now the difference
is that that you don't know

190
00:11:33,025 --> 00:11:37,863
yet the observations in the future
so yet you condition that on the policy.

191
00:11:37,863 --> 00:11:40,499
So everything will depend on the actions
you will do.

192
00:11:40,933 --> 00:11:44,003
And you also have to take the expectation

193
00:11:44,003 --> 00:11:47,039
over all kinds of of outcomes you expect.

194
00:11:48,207 --> 00:11:48,541
And then

195
00:11:48,541 --> 00:11:51,811
going to the second line, you basically

196
00:11:51,811 --> 00:11:54,246
make the move that after

197
00:11:54,914 --> 00:11:57,116
if you
if your model is sufficiently trained,

198
00:11:57,416 --> 00:11:59,652
then basically your

199
00:12:01,587 --> 00:12:04,290
your true

200
00:12:04,290 --> 00:12:08,160
your approximate posterior
will be very close to the true steer.

201
00:12:08,461 --> 00:12:12,465
And this then basically allows you to,
to rewrite the final term

202
00:12:13,666 --> 00:12:16,202
scene that I

203
00:12:16,602 --> 00:12:18,971
that I wrote it in a different way.

204
00:12:18,971 --> 00:12:23,442
But you can you can rewrite this address
and information gain on your

205
00:12:23,442 --> 00:12:27,379
your state's and

206
00:12:27,580 --> 00:12:29,648
moving towards different outcomes.

207
00:12:29,648 --> 00:12:33,352
You can also rewrite the view that here
where you basically

208
00:12:33,352 --> 00:12:37,089
state, OK, at some point in the future
I will visit some states.

209
00:12:37,089 --> 00:12:43,095
So this is this is the given the policy
and you basically replace this

210
00:12:43,095 --> 00:12:48,634
by by some prior belief that's regardless
which policy I'm actually choosing.

211
00:12:48,968 --> 00:12:53,072
I believe that's some point in the future
I will realize my preferences.

212
00:12:53,072 --> 00:12:56,876
So that's why you lose the
the conditioning on the policy

213
00:12:58,110 --> 00:13:00,146
in the last iteration.

214
00:13:00,146 --> 00:13:02,982
So this then basically becomes the key
other versions of this.

215
00:13:03,249 --> 00:13:07,853
OK, I, I want to find the policies
that actually bring me close

216
00:13:07,853 --> 00:13:11,190
to states that I,
that I prefer that I like to be into.

217
00:13:12,124 --> 00:13:15,561
And then on the other hand, you have this
ambiguity term that says, OK,

218
00:13:16,262 --> 00:13:20,199
what I don't like is to visit states
that can give me like any observation

219
00:13:20,199 --> 00:13:21,767
that I cannot predict.

220
00:13:21,767 --> 00:13:26,338
So I don't like these ambiguous states
where I don't really understand

221
00:13:26,438 --> 00:13:29,508
basically what's happening

222
00:13:30,810 --> 00:13:33,612
so that brings us to the

223
00:13:34,146 --> 00:13:37,049
the whole active instance scheme.

224
00:13:37,249 --> 00:13:40,252
So at each time step,
you basically evaluate

225
00:13:40,252 --> 00:13:43,522
your expected free energy
for each of your policies

226
00:13:44,924 --> 00:13:47,593
and you then basically assume

227
00:13:47,593 --> 00:13:52,231
that you will choose the policy
that will actually minimize your energy.

228
00:13:52,231 --> 00:13:55,568
So you, you take the max over

229
00:13:56,368 --> 00:14:00,873
minus G and you also rate this with this
gamma parameter just like the precision.

230
00:14:00,873 --> 00:14:04,610
So if you have a high precision,
then basically you will

231
00:14:04,844 --> 00:14:08,514
you will have a very big output
of the suffix function.

232
00:14:08,747 --> 00:14:11,517
So with the suffix,
it will basically become

233
00:14:13,018 --> 00:14:13,886
a mix.

234
00:14:13,886 --> 00:14:16,789
So you then definitely choose the policy

235
00:14:17,089 --> 00:14:21,327
with lowest expected tree energy
if you relax the, the,

236
00:14:21,427 --> 00:14:25,698
the precision, the bits, then
basically you allow for some

237
00:14:26,198 --> 00:14:29,134
or some more randomness in the system.

238
00:14:29,802 --> 00:14:33,939
And then basically you end up
inferring the next section according to

239
00:14:34,273 --> 00:14:39,245
what is the, the, the next action that
I should take according to this policy.

240
00:14:39,778 --> 00:14:43,482
So in the words follows the actual
the action

241
00:14:43,482 --> 00:14:47,853
selection, given the policy is basically
just deterministic method.

242
00:14:47,887 --> 00:14:51,957
So so the policies are just like
certain sequence of actions.

243
00:14:51,957 --> 00:14:55,861
So given once you choose the policy,
the actions are basically fixed.

244
00:14:55,861 --> 00:15:00,132
But in theory you could also make this
a probabilistic mapping.

245
00:15:01,967 --> 00:15:07,439
So let's now go to the really interesting
parts of the work.

246
00:15:07,439 --> 00:15:10,709
This is how do we learn state spaces

247
00:15:10,709 --> 00:15:14,747
with deep neural networks
in such an interesting

248
00:15:17,082 --> 00:15:18,951
so basically

249
00:15:20,085 --> 00:15:22,154
we have two components.

250
00:15:22,154 --> 00:15:23,088
One is the chair.

251
00:15:23,088 --> 00:15:28,060
This model
that I rehearse from the first slide

252
00:15:28,460 --> 00:15:32,898
and we have our approximate
posterior model and basically you can see

253
00:15:32,898 --> 00:15:37,303
that there are three parts
in these situations.

254
00:15:37,303 --> 00:15:40,272
And the first part
is basically the transition model.

255
00:15:40,639 --> 00:15:46,178
So its outputs,
the probability of birth states,

256
00:15:46,645 --> 00:15:49,648
given your previous states
and the action you did

257
00:15:49,682 --> 00:15:52,751
in the second part.

258
00:15:53,819 --> 00:15:56,121
And also the the initial state

259
00:15:56,121 --> 00:15:59,191
is basically also part of this model.

260
00:15:59,191 --> 00:16:02,194
But then you just provide us
with the zero action,

261
00:16:02,194 --> 00:16:06,031
for example, just to bootstrap it

262
00:16:06,065 --> 00:16:07,833
the second part is in the electric model.

263
00:16:07,833 --> 00:16:11,203
So given state, you want to have a model
that that predicts

264
00:16:11,203 --> 00:16:14,273
what kind of observation you will
you will see

265
00:16:15,841 --> 00:16:18,544
and then finally
you have the posterior model

266
00:16:18,544 --> 00:16:21,613
that's given the previous states action.

267
00:16:21,613 --> 00:16:24,717
And your current observation
also is to predict

268
00:16:25,351 --> 00:16:29,455
which state you're also assuming
for which state you were in.

269
00:16:29,688 --> 00:16:34,360
So basically the procedure model
is to come up with the same thing

270
00:16:34,360 --> 00:16:35,327
as transition model.

271
00:16:35,327 --> 00:16:38,797
But in addition,
it has access to your observation this

272
00:16:40,332 --> 00:16:42,501
and again similar to

273
00:16:42,501 --> 00:16:45,504
Transition Mobile to bootstrap this
for the initial summation,

274
00:16:45,504 --> 00:16:48,774
you don't have any action,
but it can be modeled by

275
00:16:49,008 --> 00:16:52,578
by the same thing
as to all this, these three components,

276
00:16:52,978 --> 00:16:55,647
we basically instance gives us deep

277
00:16:55,647 --> 00:16:58,817
neural nets where

278
00:16:58,817 --> 00:17:00,252
they have to

279
00:17:01,053 --> 00:17:02,588
basically be trained

280
00:17:02,588 --> 00:17:06,191
to come up with these three models

281
00:17:08,160 --> 00:17:13,699
so if we this in a schematic,
we basically have something like this.

282
00:17:13,699 --> 00:17:18,270
So given your state an action
from the previous time step

283
00:17:18,704 --> 00:17:21,907
and your observation
from the current time step, you on

284
00:17:21,907 --> 00:17:26,311
the one hand provides the previous state
and action to your transition model,

285
00:17:26,979 --> 00:17:28,981
which will then outputs

286
00:17:30,516 --> 00:17:33,452
the distribution over different states.

287
00:17:33,786 --> 00:17:37,389
And in this case,
the distribution is basically modeled

288
00:17:37,389 --> 00:17:39,958
as move to various custom distribution.

289
00:17:39,958 --> 00:17:42,661
So basically
the output of the neural nets

290
00:17:42,961 --> 00:17:47,833
will be the means
and the third deviations of regression.

291
00:17:48,300 --> 00:17:51,437
And so you basically use
this distribution to then

292
00:17:51,770 --> 00:17:53,872
generate samples for the next step.

293
00:17:54,473 --> 00:17:57,276
And so the posterior model gets also US

294
00:17:57,276 --> 00:18:00,746
inputs the state
and the action from previous step.

295
00:18:01,346 --> 00:18:04,383
But also in addition
the current observation,

296
00:18:04,583 --> 00:18:07,686
it also does a prediction on the

297
00:18:08,220 --> 00:18:11,957
the current time step and after sampling

298
00:18:12,858 --> 00:18:16,228
basically of the
model that can then generates

299
00:18:17,296 --> 00:18:19,465
the prediction of the outcome.

300
00:18:20,032 --> 00:18:24,303
And then this states
this puts to the next time step

301
00:18:25,304 --> 00:18:28,507
and the story repeats
so as I already mentioned,

302
00:18:28,640 --> 00:18:31,777
the output of each of our neural nets
is basically so

303
00:18:32,311 --> 00:18:35,914
and so variation of

304
00:18:36,515 --> 00:18:39,418
the aggression with

305
00:18:40,786 --> 00:18:44,356
and with the diagonal covariance matrix

306
00:18:44,857 --> 00:18:49,895
and we use the V from position trick
to generate samples.

307
00:18:49,895 --> 00:18:54,466
So basically we generate the sample
by having the the means

308
00:18:54,766 --> 00:19:00,305
and then at the standard deviation time,
some standard normal noise.

309
00:19:00,372 --> 00:19:03,342
And this basically allows you
to vector propagate

310
00:19:03,775 --> 00:19:05,777
regions all the way true.

311
00:19:06,345 --> 00:19:09,848
And this new and that's also if you
if you draw samples from

312
00:19:09,848 --> 00:19:11,483
from the models

313
00:19:13,952 --> 00:19:15,554
and then of course

314
00:19:15,554 --> 00:19:19,691
for the next step you just propagate
the sample and then the this

315
00:19:22,361 --> 00:19:25,931
so creating such a model is done
basically

316
00:19:26,231 --> 00:19:30,969
first collecting the data sets of action
observation sequences.

317
00:19:30,969 --> 00:19:34,072
So you take your, your,

318
00:19:34,506 --> 00:19:37,809
your agents, you either let it randomly

319
00:19:38,210 --> 00:19:42,314
generate sequences in the environments
or in case in real trouble.

320
00:19:42,381 --> 00:19:45,384
For example, you drive it around yourself
in the environment

321
00:19:45,851 --> 00:19:49,087
while you record the actions
and the observations

322
00:19:50,355 --> 00:19:52,958
and while the models have converged,

323
00:19:53,425 --> 00:19:58,096
you sample
some sequences from your data sets, you

324
00:19:58,830 --> 00:20:02,701
your dates, the states that are visited,

325
00:20:03,001 --> 00:20:05,370
and you reconstruct all the observations,

326
00:20:06,104 --> 00:20:09,141
and then you basically propagates
the free energy loss.

327
00:20:09,141 --> 00:20:13,745
So this is the the same formula
as the free energy starts.

328
00:20:13,779 --> 00:20:16,748
On the one hand,
you have the yield divergence

329
00:20:16,748 --> 00:20:21,753
between the output of your sphere,
your inlets and your prior units,

330
00:20:22,554 --> 00:20:26,291
and on the other on the other hand,
you have this reconstruction error

331
00:20:26,825 --> 00:20:30,062
that basically scores
how how good your reconstructing

332
00:20:30,062 --> 00:20:34,199
the actual traffic
and using this lost function.

333
00:20:34,199 --> 00:20:36,969
You just dates the parameters
of your units

334
00:20:37,869 --> 00:20:40,005
and you basically build

335
00:20:40,072 --> 00:20:43,141
a model that is able, on the one hand, to

336
00:20:45,344 --> 00:20:48,180
infer your
current state, given your observation

337
00:20:48,714 --> 00:20:52,951
that is able to generate new observations

338
00:20:53,085 --> 00:20:56,755
if you know which state you're in,
you can reconstruct these observations.

339
00:20:57,122 --> 00:21:00,325
But also you have a transition model
that you can then just use to

340
00:21:01,460 --> 00:21:05,964
in the state space plan ahead
what would happen if I

341
00:21:06,565 --> 00:21:09,301
if I were through this action
and it's really give you a distribution

342
00:21:09,301 --> 00:21:11,803
over over the next state, basically

343
00:21:14,006 --> 00:21:17,042
so that we come to the planning part.

344
00:21:17,643 --> 00:21:21,480
So once you have this model,
how can you use it to let your agent do

345
00:21:21,480 --> 00:21:26,885
useful, useful, useful things
so there we we will use Google sampling.

346
00:21:27,386 --> 00:21:32,624
So one of the things, one of
the limitations of these kind of pictures

347
00:21:33,058 --> 00:21:38,096
is that basically only approximates
the distribution for the next states.

348
00:21:38,096 --> 00:21:41,700
So given the samples
from from the previous states

349
00:21:41,700 --> 00:21:45,404
and given the action
the approximate distribution

350
00:21:45,570 --> 00:21:48,874
for the next stage of them, samples
you're creating as well.

351
00:21:48,874 --> 00:21:50,342
So we will never have like a

352
00:21:52,077 --> 00:21:53,011
grid distribution

353
00:21:53,011 --> 00:21:56,982
for let's say ten steps ahead

354
00:21:57,182 --> 00:22:00,919
so victim distributions
for for further in the future,

355
00:22:01,320 --> 00:22:04,156
we approximate these values
by doing multiple

356
00:22:05,524 --> 00:22:09,227
so for each of the policies we want to be
and we want to

357
00:22:10,562 --> 00:22:13,532
evaluate which sample and trajectories

358
00:22:13,899 --> 00:22:16,535
that's that all do the same actions.

359
00:22:16,535 --> 00:22:20,439
But the to the sampling
might give us different results

360
00:22:20,439 --> 00:22:24,943
in terms of future states and motivations
for each type stuff.

361
00:22:24,943 --> 00:22:28,613
We don't have a bunch of states
and a bunch of predictable observations.

362
00:22:29,481 --> 00:22:34,086
We don't fit regression distribution
using the sample means and variances

363
00:22:34,953 --> 00:22:38,957
and then we estimate
the expected energy as follows.

364
00:22:39,791 --> 00:22:44,963
So on the one hand,
we have this yellow divergence.

365
00:22:45,230 --> 00:22:47,866
If you remember that scores of goods,

366
00:22:48,100 --> 00:22:52,771
your distribution of states
is according to some some prior,

367
00:22:53,805 --> 00:22:56,041
some prior preferences of states.

368
00:22:56,041 --> 00:22:59,878
So you use this this is normal,
this discussion distribution

369
00:22:59,878 --> 00:23:03,014
to go through the divergence
with respect to disperse.

370
00:23:03,615 --> 00:23:06,551
And on the other hand,
you have this entropy term

371
00:23:06,918 --> 00:23:10,489
that scores
the ambiguity of these states.

372
00:23:10,756 --> 00:23:15,060
So you you basically use
then the entropy of the

373
00:23:15,427 --> 00:23:19,531
the observations
generated in our trajectories.

374
00:23:19,931 --> 00:23:22,501
And we also added the scale factor rule.

375
00:23:22,801 --> 00:23:25,470
You just kind of wait two thirds

376
00:23:26,671 --> 00:23:28,707
from one another,

377
00:23:28,740 --> 00:23:32,611
which allows you to make an agents

378
00:23:32,611 --> 00:23:34,379
more risky, let's say,

379
00:23:34,379 --> 00:23:38,884
if it's it's more
based on realizing references early red.

380
00:23:38,950 --> 00:23:42,988
Or you could also make a more cautious
agent let's say that's

381
00:23:43,388 --> 00:23:48,293
that's really does not want to end up
in some some ambiguous states

382
00:23:49,194 --> 00:23:51,496
and crucially here

383
00:23:51,496 --> 00:23:55,400
we also have kind of this
recursive notion in here. So

384
00:23:57,002 --> 00:23:59,638
we do this roll out for ten steps ahead

385
00:24:00,405 --> 00:24:02,774
and then for the future you could kind of

386
00:24:03,074 --> 00:24:05,043
recursively

387
00:24:06,011 --> 00:24:09,514
look further into the future
let's say aggregates

388
00:24:10,315 --> 00:24:13,518
also expected energies
from from that point

389
00:24:14,252 --> 00:24:17,155
and make it more clear and

390
00:24:18,924 --> 00:24:21,526
let's try to visualize this a bit better.

391
00:24:22,227 --> 00:24:27,232
So suppose at some given times
that you're in the states

392
00:24:27,666 --> 00:24:32,370
then basically what you do
is you use your transition model to say,

393
00:24:32,370 --> 00:24:35,240
OK, given that I fall or we'll see one

394
00:24:37,442 --> 00:24:42,013
what would be the next states and
you draw a sample from the distribution.

395
00:24:42,013 --> 00:24:45,951
So this is then test this one by forming
both C one

396
00:24:46,418 --> 00:24:49,154
and this thing you can repeat four times.

397
00:24:49,154 --> 00:24:53,358
So we re within this,
this K to just have some kind of

398
00:24:56,027 --> 00:24:57,796
box grading of actions.

399
00:24:57,796 --> 00:25:01,633
Say for example, in the case of A
for driving robots,

400
00:25:01,967 --> 00:25:05,670
it might not make sense to switch action
every

401
00:25:06,671 --> 00:25:08,039
ten milliseconds, let's say.

402
00:25:08,039 --> 00:25:10,442
So if you see
if you decide to drive forward,

403
00:25:10,442 --> 00:25:13,011
you want to keep on driving forward
towards this time.

404
00:25:13,011 --> 00:25:15,914
So that's basically what we're just
this comes from.

405
00:25:15,914 --> 00:25:20,352
So you follow the same policy
first four steps

406
00:25:21,386 --> 00:25:24,389
and you basically sampled this end times

407
00:25:24,389 --> 00:25:30,629
so you you repeats this
this process and times.

408
00:25:30,629 --> 00:25:36,701
So you have now at each time
step and samples for the kind of states

409
00:25:36,701 --> 00:25:41,239
you think you will visit
after following the first policy.

410
00:25:42,207 --> 00:25:44,042
And then if you have a second policy

411
00:25:44,042 --> 00:25:46,411
you can do the same thing for
this policy.

412
00:25:47,312 --> 00:25:48,213
So in this case,

413
00:25:49,648 --> 00:25:51,349
we only consider

414
00:25:51,349 --> 00:25:54,152
potential policies, for instance,

415
00:25:55,053 --> 00:25:58,323
go left or right, let's say.

416
00:25:58,390 --> 00:25:59,190
And then

417
00:26:00,058 --> 00:26:02,394
at this point, so it states that

418
00:26:03,628 --> 00:26:05,664
we can basically repeats

419
00:26:05,664 --> 00:26:07,666
this procedure and say, OK, what if

420
00:26:08,233 --> 00:26:11,803
what if off
time stops a switch policy, maybe,

421
00:26:12,437 --> 00:26:15,840
maybe a first left to go left
and and before turning right

422
00:26:17,943 --> 00:26:19,978
or maybe third place.

423
00:26:20,946 --> 00:26:23,748
So then basically you repeat this process

424
00:26:23,748 --> 00:26:26,251
and this you can basically

425
00:26:26,251 --> 00:26:28,587
keep from adding to the search tree

426
00:26:28,954 --> 00:26:33,191
for as long as your computation power
allows you.

427
00:26:33,224 --> 00:26:34,125
Let's see

428
00:26:37,228 --> 00:26:40,665
so then how do we calculate
the expected free energy from this?

429
00:26:40,665 --> 00:26:44,336
So we take our formula.

430
00:26:44,569 --> 00:26:49,608
So basically what we do is that each time
step, we first use likelihood model

431
00:26:49,608 --> 00:26:54,512
to also predict what are the observations
that are expected in states

432
00:26:55,780 --> 00:26:56,915
and you then

433
00:26:57,315 --> 00:27:00,285
look at all your state samples
from a certain time step.

434
00:27:00,652 --> 00:27:03,521
And this then gives you this this

435
00:27:03,521 --> 00:27:08,627
approximate version distribution
for both the states and the observations.

436
00:27:08,994 --> 00:27:13,999
So you plug this into the formula
and this then gives you kind of a number

437
00:27:13,999 --> 00:27:18,603
that says this is the expected
free energy, if I think I will,

438
00:27:18,937 --> 00:27:22,440
omitting states
plus K and I will follow along

439
00:27:22,440 --> 00:27:25,543
with policy one from thereafter.

440
00:27:26,645 --> 00:27:29,280
And then basically you can do this
for each,

441
00:27:29,481 --> 00:27:34,252
each of these branches and this is
then gravity recursion comes in.

442
00:27:34,686 --> 00:27:38,957
So then we basically states, well,
I assume that's

443
00:27:38,957 --> 00:27:44,462
my active inference agents
also at that times step will

444
00:27:44,996 --> 00:27:47,499
most likely choose the policy bits

445
00:27:47,599 --> 00:27:50,402
the the, the, the lowest tree energy.

446
00:27:50,769 --> 00:27:52,237
So basically you combine

447
00:27:53,672 --> 00:27:56,541
the, the free energy for pulse.

448
00:27:56,574 --> 00:28:00,111
You want obviously
to according to the weights

449
00:28:00,445 --> 00:28:04,816
that are provided using this
suffix function so if one of the

450
00:28:05,450 --> 00:28:10,321
if policy one has a very,
very low expected three energy

451
00:28:10,522 --> 00:28:15,827
compared to two, then basically only
the expected to be energy of, of policy.

452
00:28:15,827 --> 00:28:18,997
One will be accidents thereafter.

453
00:28:19,397 --> 00:28:22,801
If they are kind of the same
then they will both have kind of

454
00:28:24,169 --> 00:28:27,138
0.5 rate or if the other one is the key

455
00:28:27,739 --> 00:28:30,542
will mainly be the contribution of this.

456
00:28:31,042 --> 00:28:34,679
The so we basically combine these

457
00:28:35,013 --> 00:28:36,114
assuming that's

458
00:28:36,114 --> 00:28:39,484
your agent at this point also selects the

459
00:28:39,851 --> 00:28:43,054
the the possible to this region.

460
00:28:43,421 --> 00:28:47,559
So now we basically have an estimate of
what will be the expected free energy

461
00:28:47,992 --> 00:28:51,362
for the future given
that we are at the step interest rate.

462
00:28:52,530 --> 00:28:55,800
And then we can further go upstream.

463
00:28:56,234 --> 00:28:59,504
And so then for, for each branch again

464
00:28:59,604 --> 00:29:01,773
we will use the first part
of the formula to

465
00:29:02,807 --> 00:29:05,443
estimate these these versions

466
00:29:05,443 --> 00:29:09,547
calculate together versions and beta
and then add this together.

467
00:29:09,547 --> 00:29:13,418
It's what we already had
for the or the, the,

468
00:29:13,485 --> 00:29:16,054
the remaining part of three basically.

469
00:29:16,855 --> 00:29:17,922
And this is,

470
00:29:18,423 --> 00:29:20,458
this is very similar I think to what's

471
00:29:21,826 --> 00:29:23,762
called system proportions

472
00:29:23,762 --> 00:29:26,931
sophisticated inference paper.

473
00:29:27,132 --> 00:29:31,136
So basically it's at each step
in the future, you,

474
00:29:31,169 --> 00:29:37,242
you kind of only consider the free energy
of the branches that you think

475
00:29:37,809 --> 00:29:40,178
this could be the best months.

476
00:29:41,346 --> 00:29:44,516
So in the end,
you end up with the expected free energy

477
00:29:44,516 --> 00:29:47,485
for both of your policies
and you then select

478
00:29:49,654 --> 00:29:52,123
the best one and start acting accordingly

479
00:29:53,892 --> 00:29:56,161
so I'll now turn

480
00:29:56,161 --> 00:29:59,264
to also will give some more details
on all the various

481
00:29:59,531 --> 00:30:02,901
experiments that we did that
hopefully give some more insights

482
00:30:03,168 --> 00:30:06,638
on how this can work in practice
so they can reuse them.

483
00:30:08,206 --> 00:30:09,641
Yeah, I

484
00:30:11,342 --> 00:30:14,679
I have some echoing in

485
00:30:14,679 --> 00:30:16,948
my and maybe it's results now.

486
00:30:17,382 --> 00:30:18,149
Yeah, sounds fine.

487
00:30:18,149 --> 00:30:18,783
Thank you.

488
00:30:18,783 --> 00:30:24,322
OK, and so I'll go briefly
through our initial experiences

489
00:30:24,455 --> 00:30:27,859
experiments so that these are
the experiments we did in

490
00:30:29,260 --> 00:30:32,430
the past one or two years.

491
00:30:33,431 --> 00:30:36,868
And so one of the first experiments
we did and reported on

492
00:30:36,868 --> 00:30:42,240
was the mountain car, which is a
very well known benchmark, I think.

493
00:30:42,440 --> 00:30:46,444
So the goal is to have
this indirect rated car so it has not

494
00:30:46,477 --> 00:30:48,613
does not have enough power
to reach the top of

495
00:30:48,613 --> 00:30:51,349
the right hand mountain
but you want to reach it anyway.

496
00:30:52,150 --> 00:30:55,420
So ideally, your agent should learn here

497
00:30:56,487 --> 00:30:57,622
that it should

498
00:30:57,622 --> 00:31:01,092
first go back to the left
to gain momentum,

499
00:31:01,426 --> 00:31:05,263
to gain enough momentum so it can climb
the steeper mount on the right.

500
00:31:06,464 --> 00:31:09,567
And even though this is such a low
dimensional problem,

501
00:31:09,567 --> 00:31:13,104
it only has positions and velocities,
it is actually a very interesting

502
00:31:13,104 --> 00:31:17,642
benchmark to experiment upon
because there's no easy solution.

503
00:31:17,642 --> 00:31:19,010
So any agent that

504
00:31:19,010 --> 00:31:23,014
just wants to realize its preferences
immediately will fail at this

505
00:31:23,014 --> 00:31:25,216
because it will always fail
to drive up the mountain

506
00:31:27,252 --> 00:31:30,521
and now for our experiments,
we made it a bit more difficult.

507
00:31:32,056 --> 00:31:36,127
We made the fully observable
amount of car, partially observable

508
00:31:36,861 --> 00:31:39,597
so what we did was
we submitted the velocity information

509
00:31:40,632 --> 00:31:44,302
and only provided some noisy
estimates of the actual position.

510
00:31:44,702 --> 00:31:47,639
So the agents actually
had to learn two things.

511
00:31:47,672 --> 00:31:51,042
It had to learn, first of all,
what its precise position was,

512
00:31:51,242 --> 00:31:54,045
and then also how this position relates

513
00:31:54,479 --> 00:31:57,782
to the momentum by its velocity.

514
00:31:59,017 --> 00:32:01,686
And then
if we take if we start from the setup

515
00:32:01,686 --> 00:32:05,056
and we train the model
that explained a bit earlier

516
00:32:05,556 --> 00:32:08,359
and in its most basic form,

517
00:32:08,359 --> 00:32:13,064
you can actually learn a state space
that closely mimics

518
00:32:13,064 --> 00:32:16,167
these physical constraints of the system

519
00:32:17,869 --> 00:32:19,203
by minimizing energy.

520
00:32:19,203 --> 00:32:24,542
So if you look at the right most figure,
you'll see that

521
00:32:25,243 --> 00:32:27,512
and you see these sinusoidal

522
00:32:28,846 --> 00:32:31,149
state space dimensions

523
00:32:31,149 --> 00:32:33,651
and this goes closely mimic the actual

524
00:32:34,652 --> 00:32:38,556
observations of the real world
in the lower image.

525
00:32:38,556 --> 00:32:41,926
So as you can see in the lower image,
we have the ground truth in green,

526
00:32:42,226 --> 00:32:46,064
which is an actual trajectory
through its position of the cars

527
00:32:46,631 --> 00:32:49,200
and then also our state
transitional estimate.

528
00:32:49,200 --> 00:32:52,403
So our prior estimates on the position

529
00:32:54,138 --> 00:32:55,606
without observing

530
00:32:55,606 --> 00:33:00,812
and then also in the blue line,
which is a bit harder to see,

531
00:33:01,379 --> 00:33:04,682
you can see the same estimate,
but now correct it with the observations.

532
00:33:04,682 --> 00:33:07,118
So it's actually just
the posterior model outwards.

533
00:33:08,386 --> 00:33:08,586
And you

534
00:33:08,586 --> 00:33:11,322
can see these
shapes returning in the state space,

535
00:33:11,923 --> 00:33:15,159
which in itself does not say that much,
but it gives you

536
00:33:15,159 --> 00:33:18,663
at least a kind of vague idea
that at least

537
00:33:18,663 --> 00:33:21,632
the state space is sketching
something relevant for the problem.

538
00:33:22,066 --> 00:33:24,769
And I think there's one
more animation on this.

539
00:33:24,969 --> 00:33:27,205
Guess so?

540
00:33:27,205 --> 00:33:28,373
Yeah, it's the same conclusion.

541
00:33:28,373 --> 00:33:29,707
As I said, right now

542
00:33:29,707 --> 00:33:33,211
you can predict the future,
as you can see in the Orange Line

543
00:33:33,611 --> 00:33:38,049
and it appears to learn the velocity
in its state space.

544
00:33:38,783 --> 00:33:42,120
Now, if you can go to the next slides

545
00:33:43,388 --> 00:33:43,788
yeah.

546
00:33:44,555 --> 00:33:48,326
And now
we want to actually figure out how

547
00:33:49,327 --> 00:33:51,596
how we can use this model that we learned

548
00:33:52,630 --> 00:33:53,698
for active inference.

549
00:33:53,698 --> 00:33:57,802
And typically in RL
you would get some sparse reward

550
00:33:57,802 --> 00:33:59,670
for driving up the mountain.

551
00:33:59,670 --> 00:34:02,940
But in our initial experiments,
we were more interested in

552
00:34:03,674 --> 00:34:07,879
how an agent could learn from human
observations or demonstrations.

553
00:34:08,813 --> 00:34:14,085
So we recorded
seven, I think five or five

554
00:34:15,153 --> 00:34:18,022
human roll out in the environment,
just like driving around with the car.

555
00:34:18,056 --> 00:34:20,792
So you go first left and right

556
00:34:20,792 --> 00:34:22,226
and then we

557
00:34:22,760 --> 00:34:25,696
push these trajectories
through our models together

558
00:34:25,763 --> 00:34:29,667
to our through our posterior model
to get a preferred state solution.

559
00:34:30,168 --> 00:34:33,971
So here on this figure,
you can see the eight different

560
00:34:33,971 --> 00:34:39,043
dimensions, how these evolve through time
if you follow these trajectories.

561
00:34:39,544 --> 00:34:42,447
So in this thread, of course, give
you gives you the standard deviation

562
00:34:42,447 --> 00:34:46,350
at each time point
four for the X state value.

563
00:34:47,318 --> 00:34:48,419
And then

564
00:34:49,487 --> 00:34:51,355
if you use this

565
00:34:51,355 --> 00:34:53,825
these is preferred state distribution

566
00:34:54,625 --> 00:34:58,029
and you calculate the degree or this

567
00:34:58,429 --> 00:35:01,299
or even only to a divergence

568
00:35:01,299 --> 00:35:04,502
between your posterior and your

569
00:35:04,769 --> 00:35:08,072
preferred distribution
for different trajectories through time.

570
00:35:08,339 --> 00:35:11,008
So here every different color,

571
00:35:11,008 --> 00:35:14,512
every different trajectory is a different
roll out in the environment.

572
00:35:15,513 --> 00:35:17,715
Or imaginary
relative to the environments.

573
00:35:17,715 --> 00:35:19,617
You can see that
you can actually use this

574
00:35:19,617 --> 00:35:22,920
this preferred distribution
to rank trajectories according to

575
00:35:24,555 --> 00:35:26,724
lowest free energy.

576
00:35:26,724 --> 00:35:31,729
So you see that the blue curve
is the only one that reaches the top

577
00:35:31,863 --> 00:35:34,232
and is also
the one that the model seems to prefer

578
00:35:36,667 --> 00:35:38,936
and then in the next slide, yes,

579
00:35:41,005 --> 00:35:44,542
we experiment about what this means
in terms of the actual free energy.

580
00:35:44,542 --> 00:35:46,811
So also compensate it with the entropy

581
00:35:49,680 --> 00:35:51,716
and what we did here was we

582
00:35:51,716 --> 00:35:55,786
if that we had the agent
observe the world for one time step.

583
00:35:55,786 --> 00:36:01,459
So we give it an initial observation to
get a sense of bootstrap latent sample.

584
00:36:02,160 --> 00:36:05,429
And then we will only use the prior model
to imagine

585
00:36:06,197 --> 00:36:09,700
what will happen
if you follow certain policies.

586
00:36:10,601 --> 00:36:12,637
So here we considered three policies

587
00:36:12,904 --> 00:36:16,641
or like two possible policies
that you can switch at three time steps.

588
00:36:17,241 --> 00:36:21,512
So e to go left, right, right, left,
right, left, et cetera.

589
00:36:21,946 --> 00:36:23,080
Or you can start with right.

590
00:36:23,080 --> 00:36:24,682
And then these are the blue curves

591
00:36:26,551 --> 00:36:27,852
you see that if there is no

592
00:36:27,852 --> 00:36:30,021
initial velocity, then

593
00:36:31,923 --> 00:36:33,157
you see that

594
00:36:33,157 --> 00:36:37,261
agent imagines that the optimal policy,
which is left, right, right.

595
00:36:37,361 --> 00:36:41,532
So first go as far left
and then just blast your way to the top

596
00:36:42,233 --> 00:36:45,469
and give us actually
the least amount the spread.

597
00:36:45,469 --> 00:36:49,974
And also he believes that this will reach
the top earlier than any other policy.

598
00:36:51,275 --> 00:36:55,379
Inversely, if you look at the
the policy that the first goal rates

599
00:36:56,247 --> 00:36:58,249
and then you see that

600
00:36:59,283 --> 00:37:00,518
there is still a little

601
00:37:00,518 --> 00:37:04,055
and close to noise breath
due to the lack of initial velocity.

602
00:37:04,322 --> 00:37:08,559
But it's already knows that
it's impossible to reach the top.

603
00:37:09,860 --> 00:37:13,231
Now if you go to the next slides,

604
00:37:13,231 --> 00:37:13,965
the slides.

605
00:37:13,965 --> 00:37:18,703
So here we do the the same experiments,
but we add some random initial velocity.

606
00:37:19,303 --> 00:37:22,573
And again, we let the agent
observe the world for one time set

607
00:37:22,807 --> 00:37:26,010
and then plan
according to the same policies as before.

608
00:37:27,144 --> 00:37:27,712
First of all, you

609
00:37:27,712 --> 00:37:31,716
see that the agents
has lots of a much larger spread.

610
00:37:31,716 --> 00:37:33,618
Then it's believed outcomes.

611
00:37:33,618 --> 00:37:37,154
This is due to this external extra
uncertainty on

612
00:37:38,155 --> 00:37:39,991
what the initial velocity is.

613
00:37:39,991 --> 00:37:43,527
And also this initial velocity
might render

614
00:37:43,694 --> 00:37:48,266
previously infeasible policies
feasible as you can see at the bottom.

615
00:37:48,266 --> 00:37:52,069
Because now if the agent thinks, yes,
my initial velocity is high enough,

616
00:37:52,603 --> 00:37:56,907
even with the one of the more
suboptimal policies

617
00:37:57,308 --> 00:38:01,045
might still be possible
for me to reach the top

618
00:38:01,345 --> 00:38:04,949
and and if you go to the next slide,
please,

619
00:38:06,317 --> 00:38:08,185
here, we can then see this in action.

620
00:38:08,185 --> 00:38:13,724
So this animation we collapse
the imagined trajectories

621
00:38:13,724 --> 00:38:16,027
to the actual trajectory.

622
00:38:16,027 --> 00:38:18,029
And you see that

623
00:38:18,562 --> 00:38:20,998
in the beginning,
if we can play it again,

624
00:38:20,998 --> 00:38:25,136
in the beginning it believes that more
red policies will reach the top.

625
00:38:25,136 --> 00:38:27,571
And as it gains more momentum,

626
00:38:28,506 --> 00:38:31,609
all it will believe
that all blue policies will reach the top

627
00:38:31,609 --> 00:38:34,712
and even it will know that
if I now go left

628
00:38:34,712 --> 00:38:38,382
so I decelerates,
then I will less likely reach the top.

629
00:38:39,884 --> 00:38:41,786
So so I think here.

630
00:38:41,786 --> 00:38:44,822
So as you can see now,
he knows that you will not reach the top

631
00:38:46,624 --> 00:38:49,927
and yes,

632
00:38:50,361 --> 00:38:52,263
and then another experiment

633
00:38:52,263 --> 00:38:56,701
we did, building upon this was
what if we take this same approach?

634
00:38:56,701 --> 00:39:00,237
But now we move to the task,
the kind of problems

635
00:39:00,237 --> 00:39:02,373
we want to solve this
so the high dimensional

636
00:39:03,541 --> 00:39:06,444
observations
that you cannot model by hands.

637
00:39:07,311 --> 00:39:11,582
So we took another open
engine environment the the car racer.

638
00:39:12,450 --> 00:39:16,554
And here the goal is to drive
the red car, as you can see

639
00:39:16,554 --> 00:39:22,593
in the high resolution image on the left
and on the road for as long as you can.

640
00:39:24,061 --> 00:39:27,298
And then we trained our model
on a handful

641
00:39:27,298 --> 00:39:29,633
of human demonstrations of this

642
00:39:31,335 --> 00:39:33,871
and then you can see that
in the reconstructed image on the right,

643
00:39:34,071 --> 00:39:35,239
it's already it's

644
00:39:35,239 --> 00:39:39,543
even on a handful of observations
that can learn to predict this

645
00:39:40,945 --> 00:39:43,047
so we
did exactly the same thing as before.

646
00:39:43,481 --> 00:39:46,217
And I think this is an emotional.

647
00:39:46,217 --> 00:39:46,617
Yeah.

648
00:39:47,551 --> 00:39:49,553
So if you then use the exact

649
00:39:49,887 --> 00:39:54,024
the planning theory
that explained on this little cart,

650
00:39:54,024 --> 00:39:57,094
it will have learned to that
the the road.

651
00:39:57,094 --> 00:40:00,064
So the gray area is important
and it should stay on this.

652
00:40:01,165 --> 00:40:04,235
Now, this agent is a bit more greedy

653
00:40:04,268 --> 00:40:09,039
since it tries to shortcut corners
to get quicker on the gray path.

654
00:40:09,039 --> 00:40:10,841
As you can see here,

655
00:40:12,543 --> 00:40:17,548
OK, if we can go to the next slide,
maybe so now.

656
00:40:17,948 --> 00:40:23,087
And also an important point here
is that our active inference

657
00:40:23,621 --> 00:40:26,390
approach, which is model based,

658
00:40:26,390 --> 00:40:28,592
seems to be a lot

659
00:40:28,592 --> 00:40:30,428
more data efficient.

660
00:40:30,428 --> 00:40:34,131
That seems to be a lot more data
efficient than, for example, at

661
00:40:34,832 --> 00:40:36,000
a baseline age.

662
00:40:36,000 --> 00:40:39,870
And so we took decline
since this is also an of policy algorithm

663
00:40:40,471 --> 00:40:43,007
to be able to compare, it's

664
00:40:43,307 --> 00:40:47,745
kind of similar to our approach
and as you can see in the first graph,

665
00:40:47,912 --> 00:40:50,181
so for the mountain car and

666
00:40:51,115 --> 00:40:53,551
our model quickly and

667
00:40:54,885 --> 00:40:58,656
so our models are in is the orange

668
00:40:59,523 --> 00:41:01,692
our more quickly learns that

669
00:41:01,692 --> 00:41:05,229
at least some method to reach the top
and then afterwards

670
00:41:05,229 --> 00:41:08,732
it just improves upon this else
due to this part of the reports

671
00:41:09,366 --> 00:41:14,138
Ukraine is not able to learn as quickly
and even after having

672
00:41:14,672 --> 00:41:16,340
thousand times more thousand times

673
00:41:16,340 --> 00:41:19,343
more observations, it still isn't capable
of planning the month

674
00:41:20,177 --> 00:41:22,513
and even for the car race, it's
even worse.

675
00:41:22,513 --> 00:41:27,151
So here
we trained on seven or ten rollouts

676
00:41:27,818 --> 00:41:30,955
and immediately we're able
to get a reward of six hundreds

677
00:41:31,722 --> 00:41:34,058
while Deakin

678
00:41:34,592 --> 00:41:37,495
just fails
to get the same level of performance,

679
00:41:37,495 --> 00:41:40,498
even after thousands of rollouts
in the environments.

680
00:41:42,099 --> 00:41:43,601
So if we can go to the next slide.

681
00:41:43,601 --> 00:41:44,201
Yeah.

682
00:41:44,835 --> 00:41:47,972
And then a final experiment
I want to discuss today is the

683
00:41:49,406 --> 00:41:51,375
the robotic navigation or

684
00:41:51,375 --> 00:41:54,812
like maybe more accurately
robotic control.

685
00:41:55,479 --> 00:41:58,048
So here we took our Kuka platform

686
00:41:58,048 --> 00:42:00,451
and we mount some sensors on top of it

687
00:42:01,085 --> 00:42:03,587
and then also put it on a laptop

688
00:42:03,587 --> 00:42:06,657
for good measure to have some compute.

689
00:42:06,724 --> 00:42:08,359
And I don't think

690
00:42:08,359 --> 00:42:11,729
the sensors are really relevant
for what we're going to discuss now. So

691
00:42:12,897 --> 00:42:16,600
then we drove around to the robot
in our lab, as you can see on this movie,

692
00:42:16,600 --> 00:42:20,337
just like with the joystick
and captured a lot of data

693
00:42:21,305 --> 00:42:23,774
and we just drove up and down the ice

694
00:42:25,709 --> 00:42:27,077
with the robots.

695
00:42:27,144 --> 00:42:30,381
Now, this environment is also a bit
challenging for robots

696
00:42:30,381 --> 00:42:35,653
since all these aisles are super similar,
like for robots

697
00:42:36,153 --> 00:42:39,356
knowing if it's an aisle one or two,
it's nearly the same thing.

698
00:42:39,356 --> 00:42:41,792
It's very difficult
to comprehend for the machine.

699
00:42:42,960 --> 00:42:47,064
So then in this slide,
to see what a recording might look like.

700
00:42:47,097 --> 00:42:51,302
So you have right
lie there and read our feet some images.

701
00:42:52,436 --> 00:42:53,671
Yeah, sorry,

702
00:42:54,271 --> 00:42:56,206
this is not a great slide.

703
00:42:56,206 --> 00:43:01,245
And and then the goal is to and again
train

704
00:43:01,245 --> 00:43:03,781
a model that will be able to generate

705
00:43:04,415 --> 00:43:07,084
future observations for the robots.

706
00:43:07,084 --> 00:43:09,620
I don't know if there's an animation
on the slides. Yeah.

707
00:43:09,920 --> 00:43:13,490
So you saw the first, the high resolution
image was the real observation

708
00:43:13,824 --> 00:43:17,227
and this is
then what the model thinks will happen

709
00:43:17,761 --> 00:43:21,031
if the robot first turns right
and then continues to drive

710
00:43:22,399 --> 00:43:23,701
the little ghost artifact.

711
00:43:23,701 --> 00:43:26,770
You saw a bit a couple of frames ago,

712
00:43:27,838 --> 00:43:29,640
so we'll first look at it again.

713
00:43:29,640 --> 00:43:31,041
So we drive

714
00:43:31,475 --> 00:43:34,044
and then it'll turn
and suddenly you will see a ghost.

715
00:43:34,244 --> 00:43:34,912
Them appear.

716
00:43:36,480 --> 00:43:39,216
And this is

717
00:43:39,216 --> 00:43:43,320
because the model doesn't
actually know what will happen.

718
00:43:43,320 --> 00:43:46,957
It can only try to guess
based on its previously

719
00:43:47,458 --> 00:43:49,360
learned experiences in the model.

720
00:43:49,360 --> 00:43:51,862
And there's a lot of people
walking around in the data sets.

721
00:43:52,096 --> 00:43:55,833
So there was an assumption
that somebody would be walking there.

722
00:43:56,133 --> 00:43:59,470
So just might imagine
that there's somebody there.

723
00:44:00,471 --> 00:44:04,074
Then maybe if we go to the next slides

724
00:44:04,274 --> 00:44:06,343
and this is also an animation,

725
00:44:07,511 --> 00:44:11,348
so you can see, for example,
how these imaginary samples deviate.

726
00:44:12,016 --> 00:44:14,518
And this is also the reason
why we need the sampling

727
00:44:15,185 --> 00:44:19,256
and so the different sampling and
the buying tree for different outcomes.

728
00:44:19,256 --> 00:44:23,494
So you see that given the same
starting position and observation,

729
00:44:24,294 --> 00:44:27,665
the model learns that turning left
might have different outcomes

730
00:44:27,665 --> 00:44:29,933
depending where it is
and there in which out of this

731
00:44:29,933 --> 00:44:32,503
and we wear it in detail, for example,
the top right

732
00:44:33,270 --> 00:44:36,240
then the robot imagines that it is
at the end of the aisle

733
00:44:36,440 --> 00:44:39,943
whilst in the bottom to it
imagines that it is in the aisle.

734
00:44:39,943 --> 00:44:42,946
So just imagine some stuff on the X

735
00:44:45,516 --> 00:44:47,651
and here

736
00:44:47,651 --> 00:44:49,653
is and how we can evaluate policies.

737
00:44:50,454 --> 00:44:53,424
So we typically we provided
three possible policies

738
00:44:53,424 --> 00:44:55,092
turn left or center right.

739
00:44:55,092 --> 00:44:58,796
And then you can imagine what all
these things will do in the environments.

740
00:44:59,863 --> 00:45:02,700
And then similarly as before you can

741
00:45:05,402 --> 00:45:06,737
calculate the G

742
00:45:06,737 --> 00:45:10,174
and select
the one that will most likely bring you

743
00:45:10,174 --> 00:45:13,110
to your preferred sequences

744
00:45:15,446 --> 00:45:19,349
and yeah, right.

745
00:45:19,349 --> 00:45:22,252
So that's
also the nice thing of our model

746
00:45:22,853 --> 00:45:24,121
is that

747
00:45:25,255 --> 00:45:26,390
you can,

748
00:45:26,490 --> 00:45:31,161
you can because of its because it's
a neural network, you can put multiple

749
00:45:31,462 --> 00:45:36,066
types of observation into posterior
model, you can fuze them in various ways.

750
00:45:37,000 --> 00:45:40,070
So what you can see here
is it's similar to the camera feed.

751
00:45:41,405 --> 00:45:43,207
The robot will also learn the effect,

752
00:45:43,207 --> 00:45:46,009
for example, in the lighter scan

753
00:45:46,310 --> 00:45:50,581
and also it will learn the effects
on the velocity bins in a radar scan.

754
00:45:52,282 --> 00:45:55,586
And this gives you of
course extra robustness in your planning

755
00:45:55,586 --> 00:45:59,123
because you can
now reason on multiple modalities

756
00:46:00,591 --> 00:46:03,761
that maybe in the next slides

757
00:46:04,628 --> 00:46:07,397
there are still some limitations

758
00:46:07,397 --> 00:46:10,634
to using robotic to do the control
this way.

759
00:46:11,468 --> 00:46:13,904
First of all, our robot is extremely

760
00:46:15,239 --> 00:46:17,641
short sighted and

761
00:46:17,741 --> 00:46:21,145
in time it can only learn to predict

762
00:46:21,145 --> 00:46:25,449
as far as the length of the sequence,
the speed provided during training.

763
00:46:26,416 --> 00:46:29,520
And also the longer
you roll out, the larger

764
00:46:29,520 --> 00:46:32,389
your search tree
becomes, the more computational.

765
00:46:32,890 --> 00:46:36,460
But limits you will reach.

766
00:46:36,460 --> 00:46:38,662
So this is also an area of,

767
00:46:38,695 --> 00:46:42,232
you know, actively working on

768
00:46:42,766 --> 00:46:45,269
and then currently our models

769
00:46:45,269 --> 00:46:50,374
still require that we pre-record
the data set so that our models recovery,

770
00:46:50,374 --> 00:46:53,377
that we drive around ourselves
and then fit the model.

771
00:46:54,278 --> 00:46:57,514
So there's also a point
where currently at this moment working on

772
00:46:58,649 --> 00:47:01,118
and then tying into this, our models

773
00:47:01,552 --> 00:47:03,720
currently
do not really know how to explore.

774
00:47:03,720 --> 00:47:04,688
Whilst

775
00:47:05,422 --> 00:47:09,626
there's probably a sensible way to do
exploration based on the free energy

776
00:47:09,626 --> 00:47:12,830
principle, since your model uncertainty

777
00:47:12,830 --> 00:47:14,698
can be baked in

778
00:47:15,899 --> 00:47:19,203
I think I don't know
if you have any more slides actually.

779
00:47:19,203 --> 00:47:22,573
Now does it?

780
00:47:23,106 --> 00:47:26,476
Maybe it's time for questions.

781
00:47:26,476 --> 00:47:27,444
Awesome.

782
00:47:27,945 --> 00:47:32,049
You can share
and we can ask some questions.

783
00:47:32,549 --> 00:47:36,353
So I wrote down a bunch of stuff
and also anybody who's

784
00:47:36,787 --> 00:47:39,923
watching live, please ask some questions.

785
00:47:40,123 --> 00:47:42,926
So nice presentation though

786
00:47:42,926 --> 00:47:45,696
and awesome, very instructive videos

787
00:47:47,064 --> 00:47:49,766
hopefully made us think, made us laugh
a little bit with it.

788
00:47:49,766 --> 00:47:51,034
Cut corners.

789
00:47:51,034 --> 00:47:54,938
So maybe just a starting question
while people are writing their question,

790
00:47:56,206 --> 00:47:59,509
what brought you to be studying
this topic?

791
00:47:59,776 --> 00:48:01,345
In this way?

792
00:48:01,345 --> 00:48:02,179
Were you coming from

793
00:48:02,179 --> 00:48:05,849
active influence and saw robotics
as an interesting application,

794
00:48:06,183 --> 00:48:09,920
or were you in the area of robotics
or then found active inference to

795
00:48:09,920 --> 00:48:11,088
be a useful model?

796
00:48:13,190 --> 00:48:15,993
Yeah, so basically we were area of

797
00:48:17,194 --> 00:48:20,430
robotics and learning

798
00:48:20,430 --> 00:48:23,634
and we were working on building

799
00:48:24,902 --> 00:48:27,604
better,
more dimensional state representations

800
00:48:28,171 --> 00:48:32,309
to feet into that reinforcement
learning algorithm.

801
00:48:32,309 --> 00:48:34,745
Let's see the initial idea.

802
00:48:34,945 --> 00:48:37,781
So just building representations

803
00:48:38,382 --> 00:48:40,584
or or better for security.

804
00:48:40,951 --> 00:48:44,521
And then we stumbled upon
the active inference framework, which

805
00:48:44,922 --> 00:48:47,791
which basically not only gave us a way of

806
00:48:49,993 --> 00:48:52,629
bottom of how to

807
00:48:53,497 --> 00:48:56,833
to build models because basically we know

808
00:48:57,367 --> 00:49:01,104
we found ourselves
the, let's say, the energy of the balls.

809
00:49:01,772 --> 00:49:04,007
We were basically already doing that.

810
00:49:04,007 --> 00:49:08,679
But we said it's also gives us
in the same mathematical framework

811
00:49:08,679 --> 00:49:13,450
array of how to project these things
to the future and use this for planning,

812
00:49:13,951 --> 00:49:18,989
for resolving ambiguity,
and also for scoring the whole D

813
00:49:18,989 --> 00:49:23,660
and all these nice properties
that are basically lacking in oil.

814
00:49:24,061 --> 00:49:26,797
So that's why we basically started

815
00:49:26,797 --> 00:49:29,533
digging into this mathematical framework

816
00:49:29,766 --> 00:49:33,470
to go through all the papers of all
and see

817
00:49:33,470 --> 00:49:38,642
how all the ends or tied together
and see whether this would still work.

818
00:49:39,242 --> 00:49:43,146
If you if you don't have your
your state space

819
00:49:43,146 --> 00:49:46,149
defined problems,
but you just learn it from from data.

820
00:49:46,483 --> 00:49:48,118
So that's how we started this endeavor.

821
00:49:48,118 --> 00:49:50,854
And that's the mature area at this point.

822
00:49:52,122 --> 00:49:53,557
And still

823
00:49:53,557 --> 00:49:55,959
investigating it for

824
00:49:56,927 --> 00:49:57,361
awesome.

825
00:49:57,361 --> 00:50:02,099
That was going to be my second question
was like what differences or advantages

826
00:50:02,366 --> 00:50:05,302
would you describe for active inference
over reinforcement

827
00:50:05,302 --> 00:50:08,071
learning or other machine
learning frameworks?

828
00:50:08,372 --> 00:50:10,140
If you answered it
previously, that's great.

829
00:50:10,140 --> 00:50:13,744
Or do you want to add any other thoughts.

830
00:50:14,344 --> 00:50:15,679
I think

831
00:50:16,013 --> 00:50:21,251
I think the nice part is that you

832
00:50:21,385 --> 00:50:26,123
you automatically gets the,
the nice properties of

833
00:50:26,490 --> 00:50:29,760
of resolving ambiguity and of

834
00:50:29,760 --> 00:50:32,129
potential expiration if you also

835
00:50:33,263 --> 00:50:36,433
estimate positive distributions
over your benefits, for example.

836
00:50:36,700 --> 00:50:41,805
So these are very interesting properties
mathematically.

837
00:50:42,539 --> 00:50:45,742
However, it's still it's
there's still a gap

838
00:50:45,742 --> 00:50:48,979
to actually get this all of

839
00:50:50,514 --> 00:50:51,648
these revert cases.

840
00:50:51,648 --> 00:50:56,753
If you if you if you don't have the
the model redefines itself.

841
00:50:57,120 --> 00:50:59,923
So there are still some challenges
but the theory

842
00:50:59,923 --> 00:51:02,526
at least
this is it's very much a feeling.

843
00:51:03,026 --> 00:51:05,962
And I think if you look at

844
00:51:05,962 --> 00:51:09,299
what's going on at the URLs site
that reinforce three sites, then

845
00:51:10,367 --> 00:51:10,600
there's

846
00:51:10,600 --> 00:51:13,770
not that big of a gap between fields.

847
00:51:13,770 --> 00:51:17,207
I think because you look at all
the previous bonuses

848
00:51:17,207 --> 00:51:20,677
that they tried to come up with in
and your first learning

849
00:51:20,677 --> 00:51:25,382
and if you look at the model based stuff
like that is your ultimate approach,

850
00:51:25,615 --> 00:51:30,253
then everything is kind of similar
in converging to it's a good idea

851
00:51:30,253 --> 00:51:33,757
to build a model of your role,
get more simple efficiency,

852
00:51:34,057 --> 00:51:36,793
and it seems like a good idea
to have some planning in there

853
00:51:37,094 --> 00:51:41,064
and maybe there is some
there is some gaining curiosity.

854
00:51:41,064 --> 00:51:44,000
And so you see that a lot in a lot of

855
00:51:44,568 --> 00:51:47,170
different independent research tracks.

856
00:51:47,671 --> 00:51:49,773
Really converge along the same lines.

857
00:51:50,474 --> 00:51:52,876
And I think what's

858
00:51:52,909 --> 00:51:56,012
what is so appealing for active inference
is that basically

859
00:51:56,346 --> 00:52:00,283
it brings us all together from
from a single principle, which makes it

860
00:52:01,485 --> 00:52:04,020
a very nice framework to work with.

861
00:52:04,020 --> 00:52:04,621
I think.

862
00:52:06,022 --> 00:52:08,658
Anything to add on that or something.

863
00:52:08,725 --> 00:52:11,294
We had was before Tim mentioned
there was already thinking

864
00:52:11,294 --> 00:52:13,363
along the lines
of the work of the Ninja Helper.

865
00:52:13,396 --> 00:52:18,201
So I mean, if you look at his models
and I think that is

866
00:52:18,502 --> 00:52:21,538
currently almost state of the art,
the mobile based URL,

867
00:52:22,139 --> 00:52:26,376
then you will find that the models
they are building are very similar

868
00:52:26,376 --> 00:52:30,814
to the models we are building or other
active interest researchers are building.

869
00:52:31,781 --> 00:52:35,051
So yeah, I mean, it makes sense
that all these

870
00:52:36,153 --> 00:52:38,822
approaches converge on a single idea

871
00:52:38,822 --> 00:52:42,459
of the idea works yep.

872
00:52:42,859 --> 00:52:46,263
Very interesting that
like plan to dream and dream

873
00:52:46,263 --> 00:52:49,032
or imagine
so that you can sample appropriately.

874
00:52:49,432 --> 00:52:52,202
Why put that as a second
layer on the model

875
00:52:52,202 --> 00:52:55,605
or have to incentivize it
in a sort of ad hoc way?

876
00:52:55,872 --> 00:52:58,542
Why not have that
be the basis of the model?

877
00:52:58,542 --> 00:53:01,077
So that's a very nice point.

878
00:53:01,077 --> 00:53:03,880
So Dean in the chat asks,

879
00:53:04,314 --> 00:53:08,852
Have the authors heard of the Missionary
and Cannibals game slash problem,

880
00:53:09,019 --> 00:53:13,190
which is moving
two kinds of mutually incompatible

881
00:53:13,190 --> 00:53:15,892
agents back and forth
from two sides of a river?

882
00:53:16,459 --> 00:53:20,363
And if they have heard of this
or thought of any kind of analogies

883
00:53:20,397 --> 00:53:22,766
cases, do you see any applications

884
00:53:26,203 --> 00:53:26,736
right?

885
00:53:26,970 --> 00:53:29,306
Yeah, I haven't heard of the game before.

886
00:53:29,306 --> 00:53:31,374
I think it's probably similar to the

887
00:53:31,675 --> 00:53:35,212
like crossing the river with the chicken,
the fox and a goat or.

888
00:53:35,579 --> 00:53:36,379
Yes. Or was. It?

889
00:53:36,379 --> 00:53:37,914
Yeah, there's like animal versions.

890
00:53:37,914 --> 00:53:39,349
There's all kinds of versions
of this one,

891
00:53:39,349 --> 00:53:41,518
but going back and forth
with different kinds

892
00:53:41,518 --> 00:53:45,021
of incompatible agents
and you need to sort of go to the left

893
00:53:45,288 --> 00:53:48,692
before you can make it up the hill, but
it's a little bit of a different setting.

894
00:53:48,692 --> 00:53:51,228
So what does that make you think of.

895
00:53:51,228 --> 00:53:51,628
What.

896
00:53:52,662 --> 00:53:54,898
And actually really considered that

897
00:53:54,898 --> 00:53:57,834
for our models, but it might be

898
00:53:57,968 --> 00:54:00,036
if you find a way
to model it in an environment

899
00:54:00,036 --> 00:54:02,239
and then maybe collect some data on it.

900
00:54:03,473 --> 00:54:03,607
Yeah.

901
00:54:03,607 --> 00:54:09,045
I think this is a problem that this is
very nicely suited for doing.

902
00:54:09,045 --> 00:54:12,349
It's, let's say, the vanilla way
where you basically

903
00:54:12,349 --> 00:54:15,585
write out
all the different states that can happen

904
00:54:16,653 --> 00:54:19,189
of the outcomes that you can have.

905
00:54:19,189 --> 00:54:21,691
So I think you could

906
00:54:21,691 --> 00:54:27,264
actually formalize it in such a way
and then run an insurance

907
00:54:27,264 --> 00:54:31,268
simulation on, on that and see,
see what happens.

908
00:54:31,635 --> 00:54:37,641
So that's less of our interest
because in our case

909
00:54:37,674 --> 00:54:41,711
we are mainly interested in
what if your observations are so high

910
00:54:41,711 --> 00:54:44,648
dimensional that you kind of even start
thinking about

911
00:54:45,548 --> 00:54:47,651
writing out the model.

912
00:54:47,651 --> 00:54:48,551
That's it.

913
00:54:48,551 --> 00:54:52,088
And the only thing you can do is interact

914
00:54:52,088 --> 00:54:55,725
with the environment
and try to learn it from, from the data,

915
00:54:56,926 --> 00:54:58,128
which is

916
00:54:59,329 --> 00:55:00,163
slightly different.

917
00:55:00,163 --> 00:55:03,667
Take on the objective inference problem.

918
00:55:05,268 --> 00:55:06,036
Great.

919
00:55:06,036 --> 00:55:11,608
Another question is
you are all deploying these models

920
00:55:11,608 --> 00:55:16,112
sort of real time with physical agents,
embodied agents.

921
00:55:16,546 --> 00:55:19,716
So what surprised you
or what was interesting to note

922
00:55:20,150 --> 00:55:24,120
as far as going from the simulation
only where you can sort of

923
00:55:24,421 --> 00:55:27,123
put everything in a box and know
exactly what's going to influence

924
00:55:27,123 --> 00:55:30,627
what to the world of embodiment
where I don't know,

925
00:55:30,627 --> 00:55:34,364
some dust could get into the robot
or I saw a person walk by.

926
00:55:34,564 --> 00:55:38,335
So what comes into play when you actually

927
00:55:38,335 --> 00:55:41,705
deploy physically
and how does the model deal with that?

928
00:55:43,873 --> 00:55:44,240
For me,

929
00:55:44,240 --> 00:55:47,077
it's actually the thing I like first.

930
00:55:47,310 --> 00:55:49,946
I play in the round
a lot with the month ago in the car,

931
00:55:50,580 --> 00:55:52,982
but the first thing actually
was a real hurdle

932
00:55:52,982 --> 00:55:55,752
for me personally was deploying it
on a real robot.

933
00:55:56,086 --> 00:55:58,421
You suddenly have all these

934
00:55:58,421 --> 00:55:59,856
hardware constraints like you can.

935
00:55:59,856 --> 00:56:03,360
You don't have instant memory, you don't
have server, great compute anymore.

936
00:56:03,793 --> 00:56:06,363
And you all have to. You have to.

937
00:56:06,363 --> 00:56:07,831
Yeah, fit.

938
00:56:07,831 --> 00:56:11,334
It's for example,
I think Tim and I spent a lot of time

939
00:56:11,334 --> 00:56:14,971
to make a demo working
where we could roll this out real time

940
00:56:15,805 --> 00:56:19,542
and just the hardware constraints
of doing something as complex as active

941
00:56:19,542 --> 00:56:24,314
infants real time on a real power
constraint robot is another challenge.

942
00:56:24,514 --> 00:56:25,348
I don't know if it's self

943
00:56:27,450 --> 00:56:29,319
so. So I think

944
00:56:29,452 --> 00:56:32,155
if the question was

945
00:56:32,422 --> 00:56:35,592
what do you get from there,
from doing that on the real system?

946
00:56:35,592 --> 00:56:38,695
Well, a lot of frustration and pain,
I think is the answer.

947
00:56:39,129 --> 00:56:42,465
But likewise,
if you then if it works, then

948
00:56:43,967 --> 00:56:46,336
it's just so much higher.

949
00:56:46,403 --> 00:56:50,707
So I still remember older than me
cheering in the lab

950
00:56:50,707 --> 00:56:54,778
because we gave the robots
the preferred state of being like nicely

951
00:56:54,778 --> 00:56:56,246
in the center of Yale.

952
00:56:56,246 --> 00:56:58,782
And it was actually moving along the L.

953
00:56:58,782 --> 00:57:03,453
And then at the very end, they decided
this is not the center of the universe.

954
00:57:03,453 --> 00:57:06,689
And it just made a 360 degree
turn and started driving back.

955
00:57:06,890 --> 00:57:09,959
We were like, Oh, this is awesome.

956
00:57:09,959 --> 00:57:13,830
So I think the there's lots of feelings
of frustration to get it to work.

957
00:57:13,830 --> 00:57:17,400
But then once something comes out
and satisfaction

958
00:57:17,400 --> 00:57:21,471
so much higher than then, when you see
a mounting priority, which is also.

959
00:57:22,539 --> 00:57:25,608
Interesting,
you could set up a physical valley,

960
00:57:26,009 --> 00:57:29,879
maybe make a physical mountain car
because there's so much comparison

961
00:57:29,913 --> 00:57:32,982
of the software realization, but

962
00:57:33,349 --> 00:57:35,919
maybe that would be even taking it
to the next level

963
00:57:36,719 --> 00:57:42,292
so another thing that you talked about
repeatedly, maybe even in every example,

964
00:57:42,292 --> 00:57:47,931
was actually training the model
from just a handful of human cases.

965
00:57:47,931 --> 00:57:51,768
Like you drove the car, you had people
play the mountain car game.

966
00:57:52,202 --> 00:57:57,507
So what exactly is happening there
and how does the model like not overfit

967
00:57:57,507 --> 00:58:02,278
to the few trajectories you show or not
just say, Hey, you only gave me three?

968
00:58:02,312 --> 00:58:04,380
What's the deal with these three?

969
00:58:04,380 --> 00:58:07,951
Totally different trajectories,
like what exactly is being learned

970
00:58:07,951 --> 00:58:11,855
or updated in the model when you provide
just a handful of human examples.

971
00:58:16,726 --> 00:58:17,093
That.

972
00:58:17,560 --> 00:58:20,029
Then we're going to answer this one.

973
00:58:20,497 --> 00:58:23,633
So so basically

974
00:58:24,734 --> 00:58:26,736
yeah, I think overfitting is

975
00:58:26,736 --> 00:58:29,272
this is clearly an issue

976
00:58:31,975 --> 00:58:33,209
you're

977
00:58:33,209 --> 00:58:36,713
no matter what you do, your constraints
kind of through the data you provide

978
00:58:36,713 --> 00:58:41,150
to the models which cannot really learn
anything else than what is obvious.

979
00:58:41,384 --> 00:58:44,988
So that's also what
we pointed out at the loss.

980
00:58:44,988 --> 00:58:49,459
So that's a severe limitation of work
during grunt work activity.

981
00:58:49,792 --> 00:58:52,295
But there is

982
00:58:52,495 --> 00:58:54,097
there is no other way

983
00:58:54,097 --> 00:58:57,634
when there's a robot
involved to get started.

984
00:58:57,634 --> 00:58:59,502
So a lot of the easier ways
to get started.

985
00:58:59,502 --> 00:59:01,604
So that's mainly the, the driver.

986
00:59:02,005 --> 00:59:02,972
But at the moment

987
00:59:02,972 --> 00:59:06,342
we're actually working both in simulation
but also for real robots to see

988
00:59:06,776 --> 00:59:10,413
when we actually get these
systems to decide how to

989
00:59:11,447 --> 00:59:13,716
out of their own experience.

990
00:59:14,217 --> 00:59:18,922
What it's interesting to learn from them
because that's actually one of the two

991
00:59:19,556 --> 00:59:22,592
shortcut through
one of your previous questions about this

992
00:59:22,625 --> 00:59:27,463
X of entrance, if you will, that
it actually can deal with this and stuff.

993
00:59:27,497 --> 00:59:30,600
Things like my,
my Robert collects all his views

994
00:59:30,600 --> 00:59:34,571
but they are always the same
or both explorer or self.

995
00:59:34,571 --> 00:59:38,708
And so these are really connected
areas of research.

996
00:59:39,108 --> 00:59:41,911
But then so to come back
to the overfitting problem,

997
00:59:42,745 --> 00:59:45,014
so one thing that mitigates overfitting

998
00:59:45,014 --> 00:59:48,651
a bit is the fact that you have
all these stresses in the system

999
00:59:49,185 --> 00:59:52,088
in sample states
which makes it a bit more,

1000
00:59:53,156 --> 00:59:58,027
but it's not like a classifier
that that all fits the best of.

1001
00:59:58,061 --> 00:59:59,862
Let's say there's always some kind of

1002
01:00:00,997 --> 01:00:03,399
noise in there, but you're right

1003
01:00:04,801 --> 01:00:08,371
in this in some sense
or the data in the sense

1004
01:00:08,371 --> 01:00:14,410
that it's kind of predicts scenarios
that it really hasn't seen before.

1005
01:00:14,944 --> 01:00:17,447
But so one of the, one of the nice

1006
01:00:17,447 --> 01:00:20,416
things of having this

1007
01:00:20,516 --> 01:00:22,051
energy formulation is that you

1008
01:00:22,051 --> 01:00:24,187
actually finish

1009
01:00:25,288 --> 01:00:28,591
your planning to

1010
01:00:28,591 --> 01:00:31,327
read this entropy term,
which basically means that

1011
01:00:31,661 --> 01:00:36,599
if you plan ahead in some space,
that's the model wasn't rained on,

1012
01:00:36,966 --> 01:00:41,971
then typically you will have
some more variation, some higher entropy

1013
01:00:42,038 --> 01:00:45,008
or that area
because typically your your observation

1014
01:00:45,008 --> 01:00:48,311
becomes very bad or

1015
01:00:48,578 --> 01:00:50,647
very blurry or all of distribution.

1016
01:00:50,947 --> 01:00:55,818
And so so in some sense
the model is kind of robust against that.

1017
01:00:56,119 --> 01:00:59,889
And it will also
if you then deploy a policy

1018
01:01:00,657 --> 01:01:04,694
like that, do the planning,
it will kind of try to stay close

1019
01:01:04,694 --> 01:01:08,898
to where the model the regime
where the model was restrained.

1020
01:01:08,898 --> 01:01:12,068
OK, because there
you basically get the better predictions.

1021
01:01:12,535 --> 01:01:17,540
So in some sense
that also mitigates the problem.

1022
01:01:19,075 --> 01:01:21,411
So I want to add that

1023
01:01:22,178 --> 01:01:24,347
the by we

1024
01:01:24,881 --> 01:01:30,019
we could use as little of the rollouts
as we did because actually if you had

1025
01:01:30,019 --> 01:01:34,924
let a human do it roll out, you solved
the exploration problem for the agents.

1026
01:01:35,224 --> 01:01:41,197
You already get very good coverage
of the relevant, feasible state space.

1027
01:01:41,197 --> 01:01:43,266
You get like for the car racer.

1028
01:01:43,933 --> 01:01:46,736
If you let the random agent drive
around as exploration.

1029
01:01:47,103 --> 01:01:50,540
99% of your days will just be grass
and the robot will

1030
01:01:51,140 --> 01:01:53,309
learn nothing about robots.

1031
01:01:53,309 --> 01:01:57,380
So by having a human human drive it,
you know, this is the road.

1032
01:01:57,380 --> 01:02:01,017
And apparently this is important because
it's in every observation I've had.

1033
01:02:02,719 --> 01:02:07,123
That almost makes me think of two ways
that we're seeing these large models

1034
01:02:07,123 --> 01:02:11,360
be trained with a sort of mentor
like a human who says you know, here's

1035
01:02:11,360 --> 01:02:14,530
the first places you want to be sampling,
here's how you drive the first time.

1036
01:02:14,764 --> 01:02:17,567
That's the driving instructor
side by side.

1037
01:02:18,000 --> 01:02:21,370
And then there's the
the generative adversarial approach,

1038
01:02:21,704 --> 01:02:23,806
which is just the almost opposite.

1039
01:02:23,806 --> 01:02:29,011
Like we're going to be passing the model,
the most confusing possible data.

1040
01:02:29,512 --> 01:02:33,816
And so it's sort of like with this carrot
and the stick or the pushing the pole,

1041
01:02:34,250 --> 01:02:37,854
these models from both of them,
maybe one or maybe both,

1042
01:02:37,954 --> 01:02:41,357
they figure out
how to be on that razor's edge.

1043
01:02:41,657 --> 01:02:43,626
And then in the race car example,

1044
01:02:44,994 --> 01:02:46,562
it was cutting corners.

1045
01:02:46,562 --> 01:02:50,700
So that just made me wonder
about autonomous vehicles and you say,

1046
01:02:50,700 --> 01:02:54,103
OK, well, the goal is to stay on the road
and to get there fast,

1047
01:02:54,504 --> 01:02:58,741
but then sometimes get there
fast is going to take priority.

1048
01:02:58,741 --> 01:03:00,843
And then all of a sudden
you're way off the road

1049
01:03:00,843 --> 01:03:03,780
and maybe now your car is ruined
or something like that.

1050
01:03:04,046 --> 01:03:06,883
So if these were to be deployed, like

1051
01:03:06,883 --> 01:03:10,153
how will we even know
what kind of preferences

1052
01:03:11,487 --> 01:03:14,257
to instantiate the model with.

1053
01:03:16,893 --> 01:03:17,560
And that's it.

1054
01:03:17,560 --> 01:03:21,397
That's a very good point because

1055
01:03:21,531 --> 01:03:22,598
how nice

1056
01:03:22,598 --> 01:03:26,202
the inference might look in theory,
I think

1057
01:03:26,869 --> 01:03:30,206
it doesn't it's not a silver bullet
to autonomous agents

1058
01:03:30,206 --> 01:03:34,443
because a lot of the
a lot of uncertainty is still in.

1059
01:03:34,443 --> 01:03:40,116
How do you how do you provide it with
like the preferred prior distribution?

1060
01:03:40,483 --> 01:03:44,053
And that will be the commercial
in the real world system of

1061
01:03:45,822 --> 01:03:50,092
it's it's similar to do reward.

1062
01:03:50,092 --> 01:03:53,863
Basically, it's a bit more informative
than just to give a reward signal,

1063
01:03:53,863 --> 01:03:57,033
let's say
it suffers from the same issues as in

1064
01:03:57,400 --> 01:04:00,036
and if if it's right, that's

1065
01:04:00,036 --> 01:04:02,371
the way to do shortcuts.

1066
01:04:04,273 --> 01:04:06,709
You get this preferred states
that you didn't envision

1067
01:04:06,709 --> 01:04:10,279
before as a
as a designer of the experiment would say

1068
01:04:10,680 --> 01:04:13,850
it's basically has the same issues
as reinforcement learning.

1069
01:04:14,217 --> 01:04:16,953
So I don't see this as a silver bullet

1070
01:04:16,953 --> 01:04:19,021
of solving autonomy,

1071
01:04:19,989 --> 01:04:23,359
but at least it has some

1072
01:04:23,593 --> 01:04:25,628
it has some

1073
01:04:27,697 --> 01:04:29,065
dirty knobs that you can

1074
01:04:29,065 --> 01:04:32,635
you can,
you can use to at least avoid some cases

1075
01:04:32,635 --> 01:04:35,972
like avoid these ambiguous,
ambiguous states or

1076
01:04:37,173 --> 01:04:39,041
at least make it first,

1077
01:04:39,041 --> 01:04:41,244
learn the model properly.

1078
01:04:42,078 --> 01:04:44,547
So it has some nice properties,

1079
01:04:44,547 --> 01:04:48,150
but it's not a silver bullet
to solving autonomous systems.

1080
01:04:48,150 --> 01:04:48,484
I think

1081
01:04:51,087 --> 01:04:53,222
anything to add on that one.

1082
01:04:53,222 --> 01:04:55,057
I think Jim said it very well.

1083
01:04:55,057 --> 01:04:57,760
Yes, the model is not a silver bullet.

1084
01:04:57,760 --> 01:05:01,864
Definitely recognized
what areas might be.

1085
01:05:01,864 --> 01:05:03,366
Interesting.

1086
01:05:03,366 --> 01:05:07,837
Shooting ranges, first applications
where we can at least explore it.

1087
01:05:08,671 --> 01:05:12,408
Are those the same use cases
that people have been talking about

1088
01:05:12,942 --> 01:05:17,013
just more broadly
in terms of autonomous vehicles?

1089
01:05:17,013 --> 01:05:20,750
Or might there be sort of
division of labor where active inference

1090
01:05:20,750 --> 01:05:23,819
is going to specialize
like in those ambiguous scenarios?

1091
01:05:23,819 --> 01:05:27,657
Or I just was curious about that.

1092
01:05:35,364 --> 01:05:37,166
OK, so yeah,

1093
01:05:37,166 --> 01:05:40,836
I was also still thinking,
but like for us there, I think

1094
01:05:41,370 --> 01:05:46,375
the real next application worth
targeting is just still the constraints

1095
01:05:46,375 --> 01:05:48,744
navigation in that for example,
over at your house where

1096
01:05:49,445 --> 01:05:51,480
the impact of

1097
01:05:51,948 --> 01:05:54,583
mis planning is still fairly limited.

1098
01:05:54,984 --> 01:05:59,221
So the more realistic versions
of the situation we have in our lap,

1099
01:05:59,221 --> 01:06:04,560
maybe it's also that the trajectory
we're currently still on.

1100
01:06:05,294 --> 01:06:09,131
So I think we still believe
that there is some value in developing

1101
01:06:09,165 --> 01:06:14,704
autonomous active inference agents
for this more industry like settings

1102
01:06:14,704 --> 01:06:19,442
where you can at least separate
that from general public and more.

1103
01:06:19,742 --> 01:06:22,278
You have also some level of
control of the environments

1104
01:06:23,913 --> 01:06:24,613
hmm. Yeah.

1105
01:06:24,613 --> 01:06:30,553
I think they think it's similar to
to mitigating the same problem

1106
01:06:30,753 --> 01:06:33,422
of course is basically that you before,

1107
01:06:33,556 --> 01:06:37,727
before you just let the system randomly
pick actions or pick any action at once,

1108
01:06:38,027 --> 01:06:42,031
you kind of or an environment
where at least as a human you can

1109
01:06:42,698 --> 01:06:45,901
you can start to fix the system
and say, OK,

1110
01:06:46,268 --> 01:06:49,605
you want to drive forward into this
wreck, that's not a good idea.

1111
01:06:49,605 --> 01:06:53,175
So you have at least some
some ways of defining

1112
01:06:53,175 --> 01:06:56,212
some rules to keep it within some safety
range, let's say.

1113
01:06:56,512 --> 01:06:58,314
And within this boundary.

1114
01:06:58,314 --> 01:07:00,516
It's again, it's done, for example,

1115
01:07:01,650 --> 01:07:04,754
move autonomously,
but still then you might have

1116
01:07:04,754 --> 01:07:08,657
the nicer properties
of the active inference agent.

1117
01:07:09,058 --> 01:07:14,130
But it's if if you're driving around
in some ales and somebody

1118
01:07:14,130 --> 01:07:17,033
just dropped dropped off the box
in the middle of the EL,

1119
01:07:17,333 --> 01:07:19,902
it will not freak out because the the

1120
01:07:20,102 --> 01:07:23,305
slammer is is it's no longer

1121
01:07:24,273 --> 01:07:27,810
consistent with with how the robot
was programed, for example, before

1122
01:07:27,843 --> 01:07:32,515
just say, OK, this is another case
I didn't experience this before.

1123
01:07:32,581 --> 01:07:37,219
And it is it is really more an idea
how to cope with this part

1124
01:07:37,219 --> 01:07:40,823
will just be intrigued and start learning
about this new information.

1125
01:07:41,057 --> 01:07:44,693
So I think these are the nice properties
you have in these agents

1126
01:07:44,693 --> 01:07:47,663
and kind of by false the all

1127
01:07:49,365 --> 01:07:53,402
it's getting too greedy
to realize its preferences

1128
01:07:53,402 --> 01:07:58,074
because you kind of shortcut
these situations but by having this

1129
01:07:58,441 --> 01:08:01,944
this very rule based system in place

1130
01:08:01,944 --> 01:08:07,116
that kind of limits
the choice of eviction.

1131
01:08:07,116 --> 01:08:11,821
And thanks for the answer
here's a question from the chat.

1132
01:08:12,955 --> 01:08:16,025
As the code is not disclosed,
would you say

1133
01:08:16,025 --> 01:08:20,329
sticking to what you write in your paper
is sufficient to reproduce your results?

1134
01:08:20,329 --> 01:08:24,600
Or are there any further tricks you use
in designing and training the models.

1135
01:08:29,105 --> 01:08:30,272
I think that I.

1136
01:08:30,272 --> 01:08:32,942
Sorry, go ahead. Them first.

1137
01:08:32,942 --> 01:08:35,411
Yeah,
I think that together with your finishes,

1138
01:08:35,411 --> 01:08:38,347
it should be sufficient when you think.

1139
01:08:38,914 --> 01:08:40,983
Well, I think, well,
we had some experience, but

1140
01:08:41,550 --> 01:08:44,053
I try to help somebody out with trying

1141
01:08:44,053 --> 01:08:46,422
to replicate their results and

1142
01:08:47,857 --> 01:08:51,360
I think the
we have some tricks for the planning.

1143
01:08:51,660 --> 01:08:54,930
I think that isn't as straightforward
to replicate, but

1144
01:08:55,297 --> 01:08:58,300
the model prediction parts,

1145
01:08:58,300 --> 01:09:00,436
we explained the architectures

1146
01:09:00,436 --> 01:09:03,305
and I pretty detailed in the appendix.

1147
01:09:03,806 --> 01:09:07,443
So I think and we don't do
any exact tricks, for example,

1148
01:09:07,910 --> 01:09:11,580
on data processing or our long term.

1149
01:09:11,580 --> 01:09:16,018
So this would be easy to mimic,
but I think the planning is a bit

1150
01:09:16,018 --> 01:09:19,054
more involved as we
sometimes ourselves have difficulties

1151
01:09:19,054 --> 01:09:21,357
replicating. It's

1152
01:09:22,358 --> 01:09:23,893
planning is hard.

1153
01:09:23,893 --> 01:09:24,193
Yeah.

1154
01:09:26,896 --> 01:09:27,363
OK. If

1155
01:09:27,363 --> 01:09:30,566
anyone else has questions
in the live chat, they can type that.

1156
01:09:31,300 --> 01:09:35,204
Another piece that I thought
was really fascinating in the mountain

1157
01:09:35,204 --> 01:09:39,875
car example at least was
how you talked about the noise allowing

1158
01:09:40,242 --> 01:09:45,114
for the previously implausible policies
to become possible.

1159
01:09:45,114 --> 01:09:49,385
Like we saw a broader
spread of the of the trajectories

1160
01:09:49,652 --> 01:09:51,921
when there was noise.

1161
01:09:52,621 --> 01:09:56,392
How, how is that being like
integrated in real time?

1162
01:09:56,392 --> 01:10:00,362
Or how are the noise,
which are often very small?

1163
01:10:01,430 --> 01:10:05,668
How does that change the models
understanding of where it can go

1164
01:10:05,668 --> 01:10:07,203
and what it should do.

1165
01:10:11,440 --> 01:10:12,808
So in the multicore

1166
01:10:12,808 --> 01:10:15,811
example,
there are basically two sources of noise

1167
01:10:15,811 --> 01:10:20,249
that say one is on on the noise
on your observation that you get

1168
01:10:20,249 --> 01:10:22,751
so you get the versus the noise noisy
estimate of your position.

1169
01:10:23,519 --> 01:10:26,188
And so this is typically not

1170
01:10:26,722 --> 01:10:29,858
not so large because you need to have

1171
01:10:30,192 --> 01:10:33,629
the sufficient signal to noise ratio
just to learn anything.

1172
01:10:33,629 --> 01:10:34,063
Let's say

1173
01:10:36,465 --> 01:10:38,200
but the

1174
01:10:38,200 --> 01:10:41,470
second part was whether your agent starts

1175
01:10:41,470 --> 01:10:45,274
with a zero velocity
or it's a random velocity

1176
01:10:45,808 --> 01:10:50,079
and these are basically
two separate trains models.

1177
01:10:50,379 --> 01:10:55,718
So either you you have an agent
always starts with zero initial velocity

1178
01:10:56,118 --> 01:10:58,187
and then basically the model learns that

1179
01:10:59,054 --> 01:11:01,457
the initial observation

1180
01:11:01,457 --> 01:11:04,393
has zero velocity
and it basically knows how

1181
01:11:04,393 --> 01:11:08,197
to properly predicts
from from the first observation.

1182
01:11:08,330 --> 01:11:13,702
Let's see if you train the model
where the agent has a random velocity,

1183
01:11:14,103 --> 01:11:17,172
then it basically learns
that from the first observation.

1184
01:11:17,473 --> 01:11:21,143
So a lot of options can happen
depending on my velocity.

1185
01:11:21,143 --> 01:11:25,681
And the more observations
then you see how the model gains or

1186
01:11:26,615 --> 01:11:28,884
picks up, OK, this is now my philosophy

1187
01:11:28,884 --> 01:11:33,422
and from then on you
see how the wide range of options

1188
01:11:33,422 --> 01:11:36,859
collapses to the
most, the most likely means.

1189
01:11:39,461 --> 01:11:40,062
Anything on that?

1190
01:11:40,062 --> 01:11:40,729
Amazon.

1191
01:11:41,030 --> 01:11:45,034
Well, I was also thinking, for example,
in the robotic planning example

1192
01:11:45,034 --> 01:11:48,704
we gave there, you also see the spreads,

1193
01:11:49,271 --> 01:11:52,041
but then it's more as in

1194
01:11:52,041 --> 01:11:55,210
that the model learned
that it's different.

1195
01:11:55,244 --> 01:11:59,048
It's similar state values
different outcomes are possible.

1196
01:11:59,048 --> 01:12:02,785
So it will try to maybe make the gosh

1197
01:12:02,818 --> 01:12:05,888
and in the latest in a bit wider
so that if you sample from it's

1198
01:12:06,255 --> 01:12:10,426
you might get a slightly
different sample value and that will then

1199
01:12:10,626 --> 01:12:14,196
generate the different outcome you want
you want to visualize.

1200
01:12:14,797 --> 01:12:18,934
So that's also so even though
the standard normal you're sampling from

1201
01:12:18,934 --> 01:12:23,272
initially for every preference position
trick is very just standard normal.

1202
01:12:24,039 --> 01:12:27,943
The model can learn to inflate
or deflate this distribution

1203
01:12:29,511 --> 01:12:31,647
so that you get a wider coverage.

1204
01:12:33,182 --> 01:12:34,183
If for example, if

1205
01:12:34,183 --> 01:12:38,554
you also saw in the investigation example
is that basically the the model

1206
01:12:38,921 --> 01:12:43,625
since you train it on on really short
central step sequences

1207
01:12:43,625 --> 01:12:48,630
of like I think one or 2 seconds
and you kind of

1208
01:12:49,365 --> 01:12:54,203
so it's not able to cost
you guys a very consistent prediction

1209
01:12:54,203 --> 01:12:56,605
for or longer time

1210
01:12:57,573 --> 01:12:59,875
frames then that and also given the fact

1211
01:12:59,875 --> 01:13:02,711
that every El in the lab
looks very similar,

1212
01:13:03,278 --> 01:13:07,416
it learns the general structure
like there are wrecks left and right

1213
01:13:07,416 --> 01:13:11,887
and there might be boxes or,
or you saw the spine of the boxes

1214
01:13:11,887 --> 01:13:16,392
or the stuff in the end is actually
indirect, sustained, very blurry.

1215
01:13:16,392 --> 01:13:21,196
And it's kind of brownish blackish,
but there's no real

1216
01:13:22,297 --> 01:13:26,735
you can identify ones in the direct,
for example, and it's

1217
01:13:27,035 --> 01:13:31,740
so it has basically no
spatial awareness of rare in the

1218
01:13:32,975 --> 01:13:35,778
MDMA at the very end or beginning
or in the middle.

1219
01:13:36,044 --> 01:13:36,879
It has no idea.

1220
01:13:36,879 --> 01:13:40,048
And that's why you saw
if you don't see their rights

1221
01:13:40,516 --> 01:13:45,287
for a certain amount of time, then
it will either predict I'm in the middle.

1222
01:13:45,287 --> 01:13:50,359
So the next that if I turn 360, it will
still be ale after me or at the very end.

1223
01:13:50,359 --> 01:13:53,061
So if I turn around, I will see a wall.

1224
01:13:53,061 --> 01:13:56,064
So in this just no

1225
01:13:56,064 --> 01:13:58,834
systems knowledge of of this.

1226
01:13:59,168 --> 01:14:03,872
And so the this is basically model,
this kind of noise

1227
01:14:03,872 --> 01:14:08,811
in the institutions and if you
if you draw a different trajectory,

1228
01:14:09,178 --> 01:14:11,914
I think it's at the wall or

1229
01:14:11,914 --> 01:14:15,417
it's facing the other ale
or just a human passing by.

1230
01:14:15,417 --> 01:14:19,087
So I see some shady
people like Earth strictures.

1231
01:14:19,354 --> 01:14:21,957
So all of these kind of things are
then model.

1232
01:14:21,957 --> 01:14:23,525
This is kind of.

1233
01:14:25,060 --> 01:14:26,462
Yeah, noise

1234
01:14:26,462 --> 01:14:30,032
in your, in your distribution
that just appears to be happening there.

1235
01:14:30,899 --> 01:14:33,469
It makes sense

1236
01:14:33,469 --> 01:14:38,106
what might be helpful
or required for long term planning

1237
01:14:38,140 --> 01:14:41,577
because the, the tree
that you had with a multiple,

1238
01:14:43,245 --> 01:14:46,648
I guess Bifurcations or whatever
they represented,

1239
01:14:46,949 --> 01:14:50,185
that was very interesting how
you had a very fully fleshed out tree.

1240
01:14:50,486 --> 01:14:51,186
And then you showed

1241
01:14:51,186 --> 01:14:55,591
kind of how you recurse to prune
back down to make a policy selection.

1242
01:14:55,924 --> 01:15:01,363
So as you suggested, that
exponentially explodes the computation.

1243
01:15:02,297 --> 01:15:05,934
What might be helpful
or how can long term

1244
01:15:05,934 --> 01:15:08,337
planning
be achieved with reasonable hardware?

1245
01:15:10,539 --> 01:15:12,741
Yeah, so so the

1246
01:15:12,741 --> 01:15:15,310
the key thing here
is, is hierarchical models.

1247
01:15:15,744 --> 01:15:17,346
And that's what we're

1248
01:15:17,846 --> 01:15:19,681
we're pushing very hard

1249
01:15:19,681 --> 01:15:21,850
now is basically that you

1250
01:15:22,951 --> 01:15:25,921
given your given model
like retreat treat,

1251
01:15:25,921 --> 01:15:29,825
now you're basically the new model
on top of this

1252
01:15:30,526 --> 01:15:34,830
that now as observations
does not gets doesn't get pixels

1253
01:15:35,397 --> 01:15:39,668
but it gets state samples from this model
this and that.

1254
01:15:39,801 --> 01:15:43,906
The same step
now is not to predict the next step ahead

1255
01:15:43,939 --> 01:15:47,209
but like who give them sense of the type
or country

1256
01:15:47,242 --> 01:15:50,312
or whatever
or however you want to in this.

1257
01:15:51,413 --> 01:15:52,915
And once you're at that point,

1258
01:15:52,915 --> 01:15:55,884
then you have basically a system
that can then plan

1259
01:15:56,919 --> 01:15:59,888
if you plan templates
that you're actually planning over time

1260
01:15:59,888 --> 01:16:01,890
steps for the lower level model.

1261
01:16:01,890 --> 01:16:04,593
And so this way you can keep on course
grading that.

1262
01:16:05,494 --> 01:16:07,362
You only have to

1263
01:16:07,796 --> 01:16:10,432
explore if you policies at each level.

1264
01:16:10,933 --> 01:16:16,438
And then down below again, you only have
to predict for like one second.

1265
01:16:16,438 --> 01:16:19,775
That's because the other plan
was made for by the models.

1266
01:16:19,908 --> 01:16:23,545
And so this way you can, you can easily

1267
01:16:25,213 --> 01:16:27,316
bypass that and scale down

1268
01:16:27,316 --> 01:16:29,818
the complexity of the planning procedure.

1269
01:16:30,352 --> 01:16:33,755
So it reminds me a lot of driving

1270
01:16:33,755 --> 01:16:37,893
where it will be like,
OK, in five streets, take a right turn.

1271
01:16:38,193 --> 01:16:39,394
So you're not

1272
01:16:39,895 --> 01:16:42,197
pre staging the right turn.

1273
01:16:42,197 --> 01:16:44,700
OK, one, two, three, four.

1274
01:16:44,733 --> 01:16:46,034
All right. Now I should get ready.

1275
01:16:47,135 --> 01:16:49,204
What about symbolic information?

1276
01:16:49,504 --> 01:16:53,342
Like what if the aisle had a color
or gradients or if it had one

1277
01:16:53,342 --> 01:16:56,812
dot to dot three dot, could that be

1278
01:16:57,346 --> 01:16:59,681
learned in an unsupervised way?

1279
01:17:00,415 --> 01:17:03,051
A symbol in that pixel level model,

1280
01:17:03,251 --> 01:17:06,355
or is that where a hierarchical model
would come into play?

1281
01:17:08,490 --> 01:17:10,225
Well, I think the problem

1282
01:17:10,225 --> 01:17:12,894
currently is for us, architecture wise,

1283
01:17:13,729 --> 01:17:16,098
our model isn't capable of capturing

1284
01:17:16,098 --> 01:17:21,269
very like low level
details about the environments

1285
01:17:21,303 --> 01:17:24,840
just because
we are based on a V approach.

1286
01:17:24,840 --> 01:17:29,044
And then the means get objective
the use for reconstruction

1287
01:17:29,044 --> 01:17:31,947
and the way we sample
is already inhibiting,

1288
01:17:32,781 --> 01:17:35,651
for example,
recognizing dots in your inputs

1289
01:17:36,918 --> 01:17:40,756
and maybe I don't know what thing
then thinks about that.

1290
01:17:40,789 --> 01:17:44,826
I think color gradients
is something like the model might learn

1291
01:17:45,727 --> 01:17:47,963
if given enough data and incentive.

1292
01:17:49,531 --> 01:17:49,798
Yeah.

1293
01:17:49,798 --> 01:17:52,768
So I think there's a number of problems

1294
01:17:53,468 --> 01:17:58,807
with with the approach in the sense
that we're now doing prediction

1295
01:17:58,807 --> 01:18:01,610
and in, in pixel space it's safe

1296
01:18:02,177 --> 01:18:05,947
and the way you calculate the likelihoods

1297
01:18:06,915 --> 01:18:09,017
you evaluate the likelihood and you,

1298
01:18:09,017 --> 01:18:13,689
you go to the reconstruction area,
it basically means that

1299
01:18:14,222 --> 01:18:16,925
you want to have each pixel independently

1300
01:18:16,925 --> 01:18:20,762
predicted on average growth rates,
which means that

1301
01:18:20,762 --> 01:18:24,499
if you have very fine grained details, it
easily

1302
01:18:25,767 --> 01:18:27,369
ignores this.

1303
01:18:27,402 --> 01:18:29,671
You also have the,

1304
01:18:29,738 --> 01:18:33,375
the complexity term that basically says,
OK, I want to have the

1305
01:18:33,809 --> 01:18:38,914
the complex representation
for for reconstruction or reconstruction.

1306
01:18:38,914 --> 01:18:41,983
But this basically also means that

1307
01:18:42,284 --> 01:18:43,485
depending on

1308
01:18:43,485 --> 01:18:45,954
how much you you put pressure on

1309
01:18:46,955 --> 01:18:50,192
restricting the complexity,
the less information

1310
01:18:50,192 --> 01:18:53,829
you will actually and cohesion,
the more lower your base for your

1311
01:18:54,396 --> 01:18:59,101
your reconstruction will become it's
very similar to the base of the i.e.,

1312
01:18:59,668 --> 01:19:04,473
those where you have this better
parameter that the how much weight

1313
01:19:04,473 --> 01:19:08,043
you put on the yellow divergence
term versus the reconstruction.

1314
01:19:08,043 --> 01:19:08,343
Third,

1315
01:19:09,778 --> 01:19:11,113
and this also has

1316
01:19:11,113 --> 01:19:15,817
an impact on what the model will actually
within the state representation

1317
01:19:15,817 --> 01:19:19,287
and which details will
there be will be ignored.

1318
01:19:19,755 --> 01:19:23,959
And so yeah, I think
a lot of these things are very difficult

1319
01:19:23,959 --> 01:19:28,263
for the model of the street
now just because the way we rebuilt

1320
01:19:28,263 --> 01:19:31,466
and Bernard tries to like the model

1321
01:19:32,734 --> 01:19:35,971
very interesting.

1322
01:19:35,971 --> 01:19:38,406
How might somebody go about

1323
01:19:38,406 --> 01:19:42,310
learning or exploring this like
is there a textbook

1324
01:19:42,310 --> 01:19:46,381
or the citations
that are in your paper or hands on?

1325
01:19:46,381 --> 01:19:49,651
What would you encourage
somebody who is curious about this

1326
01:19:49,651 --> 01:19:52,287
and wanted to over the next
maybe few years

1327
01:19:52,654 --> 01:19:55,023
be following

1328
01:19:59,394 --> 01:19:59,628
yeah.

1329
01:19:59,628 --> 01:20:02,731
I mean, we had a lot of like
in the act of infant sports.

1330
01:20:02,731 --> 01:20:06,535
I think when we started out there
wasn't that much information

1331
01:20:06,535 --> 01:20:07,803
on how to do active inference.

1332
01:20:07,803 --> 01:20:12,274
So we had to figure it out the hard way
by trying a lot and failing a lot.

1333
01:20:13,008 --> 01:20:16,311
But now currently I think
even in this model stream, there was some

1334
01:20:17,045 --> 01:20:20,215
cool information and accessible
information on active inference.

1335
01:20:21,383 --> 01:20:23,118
So and

1336
01:20:23,118 --> 01:20:25,554
I mean specific implementation wise,

1337
01:20:25,554 --> 01:20:29,057
if you want to build a model

1338
01:20:29,057 --> 01:20:32,894
like this, I think the current state
of the art and URL and

1339
01:20:33,929 --> 01:20:35,997
active inference learned

1340
01:20:35,997 --> 01:20:38,133
active inference is or pretty similar

1341
01:20:40,435 --> 01:20:40,902
yeah.

1342
01:20:41,269 --> 01:20:43,738
I agree. So

1343
01:20:44,372 --> 01:20:46,174
a lot of things

1344
01:20:46,174 --> 01:20:50,679
have changed regarding our accessibility
information on active influence

1345
01:20:50,679 --> 01:20:55,083
has become if you look now
at the tutorial from MIT, for example,

1346
01:20:55,083 --> 01:20:59,154
which was also extensively covered
in one of your videos,

1347
01:20:59,554 --> 01:21:04,492
so I think that's really, really
those the, the, the entry bar

1348
01:21:04,492 --> 01:21:08,864
to just get to know the theory
and how everything works or should work

1349
01:21:08,864 --> 01:21:14,035
and to play around with some small play
examples and simulations

1350
01:21:14,035 --> 01:21:17,739
that you can get some insights on
and what does this thing do.

1351
01:21:17,772 --> 01:21:18,540
And obviously

1352
01:21:19,507 --> 01:21:23,044
the, the deep learning path, it's safe
to build these models.

1353
01:21:23,511 --> 01:21:26,448
Then probably the,
the resource to go through

1354
01:21:26,448 --> 01:21:30,418
are just like the photos and variation
with open borders.

1355
01:21:30,452 --> 01:21:36,491
I think these are the, the, the things
in learning that are most relevant for,

1356
01:21:36,491 --> 01:21:39,928
for or for the active inference work
we discussed here.

1357
01:21:40,262 --> 01:21:43,198
And if you can build the variation
also encoder

1358
01:21:43,398 --> 01:21:46,201
and you know how active inference work
and you push it

1359
01:21:46,201 --> 01:21:49,337
through together, together, generate
some details from our paper, for example.

1360
01:21:49,638 --> 01:21:51,907
So I think it should be
pretty straightforward

1361
01:21:52,540 --> 01:21:55,710
that to the first example.

1362
01:21:55,877 --> 01:21:59,447
Pool any closing thoughts

1363
01:21:59,447 --> 01:22:05,787
or even questions for our lab
or just to leave for people

1364
01:22:05,787 --> 01:22:09,124
to be thinking about
as they dream in preparation

1365
01:22:09,124 --> 01:22:10,225
for action.

1366
01:22:14,596 --> 01:22:15,563
Yeah.

1367
01:22:16,131 --> 01:22:17,265
Maybe I just want to

1368
01:22:18,800 --> 01:22:20,969
continue a bit on what them said
earlier that

1369
01:22:22,170 --> 01:22:24,406
for example,
if you want to build hierarchical models,

1370
01:22:26,007 --> 01:22:28,209
you don't have to build a negative
influence

1371
01:22:28,209 --> 01:22:30,145
model on top of an infinite model.

1372
01:22:30,145 --> 01:22:33,615
We experiment some
we did some preliminary experiments on,

1373
01:22:33,815 --> 01:22:36,918
for example,
just using our active infinite model as

1374
01:22:37,452 --> 01:22:39,988
dynamics, the model for a slam algorithm.

1375
01:22:39,988 --> 01:22:43,291
And then you also already get
hierarchical modeling

1376
01:22:44,092 --> 01:22:47,128
and some of the long term benefits.

1377
01:22:47,128 --> 01:22:50,098
So maybe that's also interesting like

1378
01:22:50,432 --> 01:22:53,501
to think about this,
how this active fits.

1379
01:22:53,501 --> 01:22:55,603
It's in an already existing techniques.

1380
01:22:56,705 --> 01:23:00,041
Yeah, so I think that

1381
01:23:00,075 --> 01:23:03,044
what goes on to say that

1382
01:23:03,044 --> 01:23:05,580
in this case
we use like these deep neural nets.

1383
01:23:05,580 --> 01:23:08,783
So the first rise, this transition

1384
01:23:08,783 --> 01:23:11,786
model like this model and

1385
01:23:11,786 --> 01:23:13,521
the super model, but

1386
01:23:14,522 --> 01:23:16,458
you shouldn't always

1387
01:23:16,458 --> 01:23:18,460
reverse automatically do this

1388
01:23:19,794 --> 01:23:21,796
kind of very popular right now.

1389
01:23:21,796 --> 01:23:23,631
Very, very cool.

1390
01:23:23,631 --> 01:23:26,267
But in some cases you might

1391
01:23:26,968 --> 01:23:29,804
it might be sufficient
if you know your environments

1392
01:23:29,804 --> 01:23:33,108
then, yeah, you shouldn't
bother learning the states be all this.

1393
01:23:33,108 --> 01:23:34,209
No, the states is little.

1394
01:23:34,209 --> 01:23:35,276
Just Yeah.

1395
01:23:35,276 --> 01:23:39,414
Use it or I know
you use the multi-core example here

1396
01:23:39,781 --> 01:23:42,851
this would be a good example of

1397
01:23:42,851 --> 01:23:46,354
why we shouldn't
just use the deep learning approach.

1398
01:23:46,354 --> 01:23:49,824
And so this is just like a proof
of principle work for us

1399
01:23:49,824 --> 01:23:53,294
to get some simple example working,
but it only pays off.

1400
01:23:53,294 --> 01:23:57,298
I think if you have like these real high
dimensional observations

1401
01:23:57,999 --> 01:24:00,702
and you don't,
you have no clue how to predict in space.

1402
01:24:00,702 --> 01:24:01,503
This will.

1403
01:24:01,503 --> 01:24:06,041
But it's not that this is like the,
the default way to go, let's say.

1404
01:24:06,341 --> 01:24:11,112
And we also like kind of evolving,
especially also

1405
01:24:11,746 --> 01:24:15,183
if you move through
these hierarchical models that we kind of

1406
01:24:15,316 --> 01:24:19,220
looking at, how can we make the
more kind of discrete style

1407
01:24:20,488 --> 01:24:23,024
bridge right states based model on top?

1408
01:24:23,558 --> 01:24:27,062
Very, very basically.

1409
01:24:27,228 --> 01:24:27,762
Right.

1410
01:24:28,029 --> 01:24:32,233
Good to the whole states phase
into discrete parts

1411
01:24:32,233 --> 01:24:34,302
and then have like a simple transition
model.

1412
01:24:34,302 --> 01:24:39,707
Like you give the example yourself if you
if you think about navigating yourself,

1413
01:24:40,075 --> 01:24:44,079
you're not thinking about predicting
all the pixels of all the houses

1414
01:24:44,579 --> 01:24:46,981
or but you just think about

1415
01:24:47,282 --> 01:24:50,952
I need to go forward now
and then the second street to the left.

1416
01:24:50,952 --> 01:24:55,323
So you basically think of the whole
continuously state space

1417
01:24:55,323 --> 01:24:57,992
and through some relevant parts.

1418
01:24:58,326 --> 01:25:01,863
And then your transition model
also becomes like the very simple

1419
01:25:02,664 --> 01:25:05,066
matrix with which position probabilities.

1420
01:25:05,500 --> 01:25:09,938
And so I think the future
is in the models

1421
01:25:10,305 --> 01:25:15,743
and the future is in a mixture
of some parts, but also some,

1422
01:25:16,478 --> 01:25:21,082
some very precise,
intuitively comprehensible parts.

1423
01:25:22,550 --> 01:25:24,953
Just two points on that.

1424
01:25:24,953 --> 01:25:28,923
One is something that's always drawn
me to active inference is that it's

1425
01:25:28,923 --> 01:25:34,162
inference conditioned and about action
so you're not going for that

1426
01:25:34,162 --> 01:25:38,366
for K Google Street View,
what does every house look like?

1427
01:25:38,733 --> 01:25:42,804
And again, when that's the input data,
even if you have a generative model,

1428
01:25:42,804 --> 01:25:44,439
that's the output data.

1429
01:25:44,439 --> 01:25:47,208
So active inference
is a really principled way to just

1430
01:25:47,208 --> 01:25:51,513
sort of reduce what you're predicting to
like which way should my elbow move

1431
01:25:51,813 --> 01:25:54,582
not what will
the pixels look like when my elbow moves,

1432
01:25:54,582 --> 01:25:57,452
which may be just
taking gigabytes of data,

1433
01:25:57,719 --> 01:26:02,490
but if it's just reduced to the state
space, then it's easier to learn.

1434
01:26:02,490 --> 01:26:04,792
And that's why it was such an interesting

1435
01:26:05,493 --> 01:26:09,631
contribution with your paper
to actually learn that state space

1436
01:26:09,864 --> 01:26:15,003
in the context of high resolution
and real time and heterogeneous sensors.

1437
01:26:15,336 --> 01:26:19,407
And then also it's something
that we've seen many perspectives on

1438
01:26:19,407 --> 01:26:24,179
in the lab and in these discussions
is there's the philosophical discussion

1439
01:26:24,212 --> 01:26:27,649
map and territory
who's really an active inference agent?

1440
01:26:27,649 --> 01:26:27,916
Is it

1441
01:26:27,916 --> 01:26:32,487
everything, a dust particle, a bacteria,
you know, is the world built this way?

1442
01:26:32,887 --> 01:26:36,391
And then there's
this sort of engineering approach

1443
01:26:36,391 --> 01:26:40,128
where you have your preferences
for how you want to see the robot work,

1444
01:26:40,395 --> 01:26:43,531
and then whatever
you can tinker and cobble together

1445
01:26:44,766 --> 01:26:46,367
that satisfies you.

1446
01:26:46,367 --> 01:26:46,601
And it

1447
01:26:46,601 --> 01:26:50,171
reduced your uncertainty about performing
the tasks you're trying to perform.

1448
01:26:50,438 --> 01:26:54,142
So it's sort of sidesteps,
but then it sidesteps

1449
01:26:54,142 --> 01:26:58,112
those questions in a way
that actually brings us to a higher level

1450
01:26:58,112 --> 01:27:01,449
of understanding,
because I know that many listeners

1451
01:27:02,050 --> 01:27:06,521
who are not as advanced in the machine
learning will be inspired

1452
01:27:06,521 --> 01:27:10,191
and have qualitative thoughts
based upon what you brought here today.

1453
01:27:10,792 --> 01:27:15,296
So thanks again for this
awesome presentation and conversation

1454
01:27:15,563 --> 01:27:19,267
and we'll always appreciate hearing
any follow up, whatever the time is right

1455
01:27:21,069 --> 01:27:21,302
yeah.

1456
01:27:21,302 --> 01:27:24,439
Thank you for having us this discussion.

1457
01:27:25,173 --> 01:27:26,808
OK, peace.

1458
01:27:26,808 --> 01:27:29,277
See you later. Bye bye.

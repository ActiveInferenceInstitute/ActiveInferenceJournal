1
00:00:02,940 --> 00:00:05,939
国外

2
00:00:17,359 --> 00:00:20,520
这是主动推理研究所

3
00:00:20,520 --> 00:00:22,400
，时间是 2022 年 11 月 15 日

4
00:00:22,400 --> 00:00:27,240
，我们在模型流 7.1 中，我们

5
00:00:27,240 --> 00:00:30,960
将讨论 Pi mdp 一个

6
00:00:30,960 --> 00:00:33,840
用于在离散状态空间中进行主动推理的 python 包，

7
00:00:33,840 --> 00:00:37,500
我们都会打个招呼，然后

8
00:00:37,500 --> 00:00:41,520
我们将传递给 Connor 对于演示

9
00:00:41,520 --> 00:00:43,860
之后的演示，我们将进行

10
00:00:43,860 --> 00:00:46,920
一些讨论 看一下 Pi mdp

11
00:00:46,920 --> 00:00:49,140
脚本 回答

12
00:00:49,140 --> 00:00:52,500
实时聊天中出现的任何问题 所以

13
00:00:52,500 --> 00:00:54,660
感谢作者今天的加入，也

14
00:00:54,660 --> 00:00:57,180
感谢 Jacob 我们先打个

15
00:00:57,180 --> 00:01:00,360
招呼 所以我是 Daniel，我是

16
00:01:00,360 --> 00:01:02,399
加利福尼亚的一名研究员，我很高兴能

17
00:01:02,399 --> 00:01:05,220
更多地了解 pi mdp 并

18
00:01:05,220 --> 00:01:07,560
了解如何应用主动推理

19
00:01:07,560 --> 00:01:11,460
，我会转告

20
00:01:11,460 --> 00:01:14,220
jakub 大家好，我是 yakub，我是 我

21
00:01:14,220 --> 00:01:18,020
是英国的一名学生，也很高兴听到

22
00:01:18,020 --> 00:01:22,200
更多关于父母的信息，并讨论最近的

23
00:01:22,200 --> 00:01:25,320
呃发展和未来

24
00:01:25,320 --> 00:01:30,500
发展的计划我会把它传递给达芙妮

25
00:01:30,500 --> 00:01:33,180
嗨，我是达芙妮，

26
00:01:33,180 --> 00:01:36,659
嗯，我在伦敦工作，我用过

27
00:01:36,659 --> 00:01:39,780
小学 p 很多我

28
00:01:39,780 --> 00:01:42,000
在我的 Mas 上和 Connor 一起做的工作 ter 的论文以及

29
00:01:42,000 --> 00:01:43,380
我们从那时起一直在做的工作

30
00:01:43,380 --> 00:01:44,939


31
00:01:44,939 --> 00:01:46,680
所以我非常喜欢认为它是一个

32
00:01:46,680 --> 00:01:50,159
非常非常棒的包而且我很高兴

33
00:01:50,159 --> 00:01:53,899
人们会开始使用它更

34
00:01:55,020 --> 00:01:57,540
优秀好吧康纳非常感谢你

35
00:01:57,540 --> 00:02:00,360
加入接受它

36
00:02:00,360 --> 00:02:02,700
太棒了，谢谢你，谢谢你的邀请，

37
00:02:02,700 --> 00:02:05,700
我很高兴，嗯，我们安排了这

38
00:02:05,700 --> 00:02:07,799
件事，这是一个很好的直播机会，

39
00:02:07,799 --> 00:02:10,020
我已经去过几次

40
00:02:10,020 --> 00:02:12,660
了，所以回来总是很好，所以

41
00:02:12,660 --> 00:02:14,640
我不 有一张幻灯片介绍我自己

42
00:02:14,640 --> 00:02:16,860
所以我只想简单地说一句

43
00:02:16,860 --> 00:02:19,920
关于我是谁所以我是 Connor 我是德国

44
00:02:19,920 --> 00:02:23,160
um the plug

45
00:02:23,160 --> 00:02:25,020
Institute for animal behavior

46
00:02:25,020 --> 00:02:28,020
and Constance 的生物学博士生，我的大部分工作

47
00:02:28,020 --> 00:02:30,120
是 关于

48
00:02:30,120 --> 00:02:33,360
在认知

49
00:02:33,360 --> 00:02:35,099
科学和复杂系统上应用主动推理和贝叶斯透镜，将

50
00:02:35,099 --> 00:02:37,379
其应用于集体行为，如

51
00:02:37,379 --> 00:02:39,959
集体动物行为，但今天

52
00:02:39,959 --> 00:02:41,940
我要谈论的是我的一个

53
00:02:41,940 --> 00:02:43,620
副博士项目，它一直在

54
00:02:43,620 --> 00:02:45,840
开发这个 Prime DP 包，它

55
00:02:45,840 --> 00:02:47,220
曾是

56
00:02:47,220 --> 00:02:49,739
与来自主动推理社区的一群人进行了非常多的协作小组努力，

57
00:02:49,739 --> 00:02:51,420


58
00:02:51,420 --> 00:02:52,200


59
00:02:52,200 --> 00:02:55,260
嗯，是的，让我们

60
00:02:55,260 --> 00:02:58,080
深入研究该软件包

61
00:02:58,080 --> 00:03:00,239
实际上已经发布了一段时间的论文，该

62
00:03:00,239 --> 00:03:02,519
论文于今年早些时候发表，

63
00:03:02,519 --> 00:03:04,800
嗯，它只是 我想首先

64
00:03:04,800 --> 00:03:06,599
强调 primevp 是一个非常重要的

65
00:03:06,599 --> 00:03:08,280
工作，尽管我们发布

66
00:03:08,280 --> 00:03:09,599
了一篇论文，它绝对像一个

67
00:03:09,599 --> 00:03:12,300
可用的独立包，它

68
00:03:12,300 --> 00:03:15,239
总是有大量的东西需要继续开发

69
00:03:15,239 --> 00:03:16,440
，在演示结束时我将

70
00:03:16,440 --> 00:03:18,000
谈论 一些正在进行的

71
00:03:18,000 --> 00:03:19,620
开发在我看来真的很令人兴奋

72
00:03:19,620 --> 00:03:21,959
，我们将真正

73
00:03:21,959 --> 00:03:23,700
开放软件包的使用

74
00:03:23,700 --> 00:03:26,459
和可扩展性

75
00:03:26,459 --> 00:03:27,900
谢谢

76
00:03:27,900 --> 00:03:30,360
所以基本上我的意思是这是一个活跃的

77
00:03:30,360 --> 00:03:33,000
推理研究所播客或

78
00:03:33,000 --> 00:03:35,640
直播所以我不 必须花太多

79
00:03:35,640 --> 00:03:37,379
时间来激励我不认为主动

80
00:03:37,379 --> 00:03:40,799
推理但简而言之，pi MVP

81
00:03:40,799 --> 00:03:43,379
包是一个 python 包，用于

82
00:03:43,379 --> 00:03:45,299
模拟

83
00:03:45,299 --> 00:03:46,980
um 并

84
00:03:46,980 --> 00:03:49,739
以离散方式运行主动推理过程 状态空间

85
00:03:49,739 --> 00:03:51,599
和离散状态空间案例是一个

86
00:03:51,599 --> 00:03:53,879
经过充分研究的案例，

87
00:03:53,879 --> 00:03:55,560
嗯，它的特征非常好，它

88
00:03:55,560 --> 00:03:57,360
在离散决策和规划等决策模型中非常流行，

89
00:03:57,360 --> 00:03:58,980


90
00:03:58,980 --> 00:04:01,620
并且已经

91
00:04:01,620 --> 00:04:03,659
在神经科学中作为模型得到了很多应用

92
00:04:03,659 --> 00:04:05,159


93
00:04:05,159 --> 00:04:07,019


94
00:04:07,019 --> 00:04:10,700
例如人类或其他动物的离散决策行为，

95
00:04:11,819 --> 00:04:14,580
所以我将简要介绍

96
00:04:14,580 --> 00:04:17,040
一下演示文稿，

97
00:04:17,040 --> 00:04:19,738
所以首先我将介绍

98
00:04:19,738 --> 00:04:21,839
从事 primevp 工作的团队，

99
00:04:21,839 --> 00:04:23,699
我们是 primevp 的作者 这篇论文，但

100
00:04:23,699 --> 00:04:26,699
实际有效的团队

101
00:04:26,699 --> 00:04:27,840
比这大得多，因为有很多

102
00:04:27,840 --> 00:04:29,460
人正在开发或

103
00:04:29,460 --> 00:04:31,020
使用它，并以他们自己的方式做出贡献，但

104
00:04:31,020 --> 00:04:32,940
他们实际上并不是论文的合著者

105
00:04:32,940 --> 00:04:34,380


106
00:04:34,380 --> 00:04:35,820
，然后我将讨论

107
00:04:35,820 --> 00:04:38,940
这个包的动机我将简要

108
00:04:38,940 --> 00:04:40,680
概述离散状态空间中的主动推理，

109
00:04:40,680 --> 00:04:44,580
但正如我所提到的，我

110
00:04:44,580 --> 00:04:47,340
认为这是一个非常好的

111
00:04:47,340 --> 00:04:48,960
讨论关于 Palm DP 的场所，因为 我

112
00:04:48,960 --> 00:04:50,639
认为我不需要深入

113
00:04:50,639 --> 00:04:53,220
讨论什么是主动推理，这样

114
00:04:53,220 --> 00:04:54,720
可以节省我们一些时间来

115
00:04:54,720 --> 00:04:58,080
更深入地了解池塘 DP

116
00:04:58,080 --> 00:04:59,400
嗯，我将讨论现有

117
00:04:59,400 --> 00:05:00,960
的模拟主动

118
00:05:00,960 --> 00:05:03,240
推理代理的方法，例如 即在 Matlab 中

119
00:05:03,240 --> 00:05:05,400
使用 SPM，只是将

120
00:05:05,400 --> 00:05:08,400
Prime DP 与 SPM 进行比较和对比，并帮助

121
00:05:08,400 --> 00:05:10,440
我们理解 Prime

122
00:05:10,440 --> 00:05:12,479
DB

123
00:05:12,479 --> 00:05:15,139
的来源，嗯，就其在 SPM 中的起源而言，

124
00:05:15,139 --> 00:05:17,160
所以我们将讨论一些

125
00:05:17,160 --> 00:05:18,840
功能 prime DP 及其通用

126
00:05:18,840 --> 00:05:20,820
包结构

127
00:05:20,820 --> 00:05:22,860
，然后我们将深入研究一些使用

128
00:05:22,860 --> 00:05:24,660
示例，其中我将展示一些

129
00:05:24,660 --> 00:05:27,600
模拟代理行为的类似输出，

130
00:05:27,600 --> 00:05:29,520
然后是附带的代码，只是

131
00:05:29,520 --> 00:05:31,440
为了演示

132
00:05:31,440 --> 00:05:35,400
Pi mdp 的一般流程 看起来像，然后

133
00:05:35,400 --> 00:05:36,960
在最后我将只讨论一些

134
00:05:36,960 --> 00:05:39,060
未来的方向和正在进行的

135
00:05:39,060 --> 00:05:42,060
um 活跃分支我猜 pine DP

136
00:05:42,060 --> 00:05:44,220
他们正在进行的开发工作，我

137
00:05:44,220 --> 00:05:47,400
认为这将使时间 EP 超级令人兴奋

138
00:05:47,400 --> 00:05:48,600
和可扩展的

139
00:05:48,600 --> 00:05:50,880
um 到各种 新的用例，所以我对此感到

140
00:05:50,880 --> 00:05:53,699
非常兴奋，现在，正如

141
00:05:53,699 --> 00:05:56,300
丹尼尔所说，如果有人想在任何时候

142
00:05:56,300 --> 00:05:59,400


143
00:05:59,400 --> 00:06:00,960
嗯，或者它有任何问题需要

144
00:06:00,960 --> 00:06:02,759
澄清，请随时告诉

145
00:06:02,759 --> 00:06:04,199
我，我们可以详细讨论一些

146
00:06:04,199 --> 00:06:05,940
要点 更长的时间，

147
00:06:05,940 --> 00:06:08,639
嗯，好吧，所以我将首先介绍

148
00:06:08,639 --> 00:06:11,039
团队，所以他们是

149
00:06:11,039 --> 00:06:13,440
我的论文版的合著

150
00:06:13,440 --> 00:06:15,900
者是米吉·达芙妮男爵让我们今天在这里，呃

151
00:06:15,900 --> 00:06:18,419
布伦南克莱因卡尔弗里斯顿伊恩堂兄

152
00:06:18,419 --> 00:06:21,360
和亚历山大机会所以卡尔 伊恩是

153
00:06:21,360 --> 00:06:24,360
我的博士生导师

154
00:06:24,360 --> 00:06:27,020
，嗯，卡尔是 Matlab 中主动推理

155
00:06:27,020 --> 00:06:30,660
的 mdp 或离散状态

156
00:06:30,660 --> 00:06:32,520
空间公式的原始祖先，

157
00:06:32,520 --> 00:06:34,080


158
00:06:34,080 --> 00:06:36,000
所以很高兴得到他

159
00:06:36,000 --> 00:06:38,280
对我们在这里的工作的认可

160
00:06:38,280 --> 00:06:41,220
，他有点 余弦，

161
00:06:41,220 --> 00:06:43,680
然后我认为重要的是要

162
00:06:43,680 --> 00:06:45,840
强调这里的每个人

163
00:06:45,840 --> 00:06:48,000
对包的重要性，这不仅仅是

164
00:06:48,000 --> 00:06:50,699
我做的，我猜大多数实际的

165
00:06:50,699 --> 00:06:52,080
软件开发，但

166
00:06:52,080 --> 00:06:53,840
prime DP 的早期阶段真的是

167
00:06:53,840 --> 00:06:56,639
之间的对话 我 Brennan 和

168
00:06:56,639 --> 00:07:00,000
Alec 早在 2019 年或什至更早的时候

169
00:07:00,000 --> 00:07:02,460
就需要一个进行主动推理的 python 包

170
00:07:02,460 --> 00:07:04,680
，我想

171
00:07:04,680 --> 00:07:05,819
当时其他人可能也有过

172
00:07:05,819 --> 00:07:07,440
类似的对话，

173
00:07:07,440 --> 00:07:10,080
但这是因为我们都走到了

174
00:07:10,080 --> 00:07:11,880
一起，所以我们 能够

175
00:07:11,880 --> 00:07:14,699
在不太长的时间内构建这个东西我的

176
00:07:14,699 --> 00:07:16,979
意思是超过两年而且

177
00:07:16,979 --> 00:07:18,660
如果我全职工作它可能会做得更快

178
00:07:18,660 --> 00:07:21,360
但它是嗯

179
00:07:21,360 --> 00:07:23,099
观看进展真的很有趣

180
00:07:23,099 --> 00:07:25,319
然后正如达芙妮

181
00:07:25,319 --> 00:07:27,599
所说，她实际上

182
00:07:27,599 --> 00:07:30,240
是第

183
00:07:30,240 --> 00:07:32,520
一批在她自己的硕士论文中真正使用 um active Pi mdp 的人之一

184
00:07:32,520 --> 00:07:34,380
，不仅如此，而且在一个非常

185
00:07:34,380 --> 00:07:35,880
雄心勃勃的应用程序中，就像

186
00:07:35,880 --> 00:07:38,340
多代理集体行为有

187
00:07:38,340 --> 00:07:40,380
一堆松树 DP 特工

188
00:07:40,380 --> 00:07:42,960
彼此互动以模拟

189
00:07:42,960 --> 00:07:45,060
Dynamics 和 Echo Chambers 以及

190
00:07:45,060 --> 00:07:46,680
社交网络的那种观点，这真的很令人兴奋，

191
00:07:46,680 --> 00:07:49,139
所以与她一起工作真的很令人欣慰

192
00:07:49,139 --> 00:07:51,660
，嗯，是的，所以这

193
00:07:51,660 --> 00:07:53,400
真的很不错 还要注意，Daphne 也是其中的一部分

194
00:07:53,400 --> 00:07:55,199
，因为它就像是

195
00:07:55,199 --> 00:07:57,960
pi mdp

196
00:07:57,960 --> 00:07:59,819
主动推理社区中先驱者的一个很好的例子，然后

197
00:07:59,819 --> 00:08:02,819
Baron 也是，呃，我，Alec 和 baron

198
00:08:02,819 --> 00:08:04,680
在开发一些

199
00:08:04,680 --> 00:08:06,360
更复杂的消息传递

200
00:08:06,360 --> 00:08:09,120
技术方面做了很多工作 inactive  Pym

201
00:08:09,120 --> 00:08:11,880
DP 和 uh Baron 中的推理

202
00:08:11,880 --> 00:08:15,120
在帮助我写论文方面也非常重要，

203
00:08:15,120 --> 00:08:16,860
嗯，他非常擅长写作和

204
00:08:16,860 --> 00:08:19,919
概念化，并且

205
00:08:19,919 --> 00:08:22,020
实际上还在他自己的一些

206
00:08:22,020 --> 00:08:24,360
关于后继表示和主动

207
00:08:24,360 --> 00:08:27,060
推理的工作中使用了 privp，这是一种 很酷，

208
00:08:27,060 --> 00:08:27,599
嗯，

209
00:08:27,599 --> 00:08:30,240
好吧，我很清楚在座

210
00:08:30,240 --> 00:08:32,159
的每个人最重要的事情

211
00:08:32,159 --> 00:08:35,159
之一是

212
00:08:35,159 --> 00:08:37,700


213
00:08:37,700 --> 00:08:40,380
，在

214
00:08:40,380 --> 00:08:42,000
过去 10 年，尤其是最近

215
00:08:42,000 --> 00:08:43,979
5 年里，人们对主动推理的流行和兴趣要大得多，

216
00:08:43,979 --> 00:08:46,140
所以这很明显 我们需要

217
00:08:46,140 --> 00:08:48,300
一些更通用的用户友好

218
00:08:48,300 --> 00:08:49,860
框架，让人们

219
00:08:49,860 --> 00:08:52,080


220
00:08:52,080 --> 00:08:53,700
首先从教学的角度

221
00:08:53,700 --> 00:08:54,959
以及 将其应用于他们自己的

222
00:08:54,959 --> 00:08:57,420
研究或工业应用，

223
00:08:57,420 --> 00:08:59,339
或者他们想用

224
00:08:59,339 --> 00:09:02,060
主动

225
00:09:02,100 --> 00:09:04,320
推理实际做

226
00:09:04,320 --> 00:09:05,880


227
00:09:05,880 --> 00:09:07,320
的任何

228
00:09:07,320 --> 00:09:08,880
事情

229
00:09:08,880 --> 00:09:10,680


230
00:09:10,680 --> 00:09:12,899


231
00:09:12,899 --> 00:09:14,580
软件工程师

232
00:09:14,580 --> 00:09:17,820
和许多感兴趣的领域，

233
00:09:17,820 --> 00:09:20,700
嗯，最主要的呃语言是

234
00:09:20,700 --> 00:09:23,880
python 所以当很多时候我

235
00:09:23,880 --> 00:09:25,200
听到的是人们开始学习

236
00:09:25,200 --> 00:09:26,880
主动推理并且他们有

237
00:09:26,880 --> 00:09:28,500
python 或 r 或其他东西的背景

238
00:09:28,500 --> 00:09:29,880
他们发现这一切都在

239
00:09:29,880 --> 00:09:31,680
Matlab 中，他们有一种

240
00:09:31,680 --> 00:09:34,560
进入障碍，因为他们要么

241
00:09:34,560 --> 00:09:37,740
不知道 Matlab，要么

242
00:09:37,740 --> 00:09:39,959
很难解析 Matlab 代码，特别是如果

243
00:09:39,959 --> 00:09:42,540
他们来自非数组编程

244
00:09:42,540 --> 00:09:44,339
框架，比如也许 他们是

245
00:09:44,339 --> 00:09:47,459
使用 uh JavaScript 之类的前端 Web 开发人员

246
00:09:47,459 --> 00:09:48,899
，他们

247
00:09:48,899 --> 00:09:50,220
实际上并不熟悉大型

248
00:09:50,220 --> 00:09:52,920
多维 数组编程

249
00:09:52,920 --> 00:09:55,200
嗯，这是

250
00:09:55,200 --> 00:09:57,180
专门制作一个执行主动推理的 python 包的另一个动机，

251
00:09:57,180 --> 00:10:01,019


252
00:10:01,019 --> 00:10:03,720
嗯，最后我们因为它在

253
00:10:03,720 --> 00:10:06,360
Python 中，这意味着我们现在正在

254
00:10:06,360 --> 00:10:08,940
创建一个可以与

255
00:10:08,940 --> 00:10:12,060
其他生态系统对话的生态系统，所以理想情况下

256
00:10:12,060 --> 00:10:14,459
primevp 不会 仅在

257
00:10:14,459 --> 00:10:16,860
仅使用 Pi mdp 的脚本中使用，它将

258
00:10:16,860 --> 00:10:19,500
与其他软件包一起使用，其中一些软件包

259
00:10:19,500 --> 00:10:20,760
与

260
00:10:20,760 --> 00:10:23,040
人工智能或网络科学或

261
00:10:23,040 --> 00:10:24,420
各种相关的框架有关

262
00:10:24,420 --> 00:10:26,640
，因此您现在可以使用插件和

263
00:10:26,640 --> 00:10:29,220
与 piondp 代理一起玩并将它们放在

264
00:10:29,220 --> 00:10:31,080
开放式 AI 体育馆等环境中以进行

265
00:10:31,080 --> 00:10:33,000
强化学习

266
00:10:33,000 --> 00:10:36,600
，呃，现在你可以

267
00:10:36,600 --> 00:10:39,000
在各种应用程序中使用主动推理，

268
00:10:39,000 --> 00:10:40,380
这对 python 来说真的很棒，因为

269
00:10:40,380 --> 00:10:43,019
python 的设计及其社区

270
00:10:43,019 --> 00:10:44,579
驱动的开发就是如此 许多

271
00:10:44,579 --> 00:10:46,860
不同的包都是

272
00:10:46,860 --> 00:10:49,140
为某些特定的东西构建得很好，所以现在

273
00:10:49,140 --> 00:10:51,540
它在 Python 中，你可以有点呃使用它

274
00:10:51,540 --> 00:10:54,240
与串联 w 与所有其他

275
00:10:54,240 --> 00:10:57,079
python 包一样，

276
00:10:58,079 --> 00:11:00,480
好的，现在简要介绍主动推理，主动

277
00:11:00,480 --> 00:11:02,519
推理的基本

278
00:11:02,519 --> 00:11:05,640
范式是你

279
00:11:05,640 --> 00:11:07,500
考虑一个嵌入其环境中的代理，

280
00:11:07,500 --> 00:11:08,700


281
00:11:08,700 --> 00:11:11,519
而不像传统的更

282
00:11:11,519 --> 00:11:15,540
被动的感知和

283
00:11:15,540 --> 00:11:18,540
行为方法，你有点 考虑

284
00:11:18,540 --> 00:11:20,880
环境为您提供信息，您进行

285
00:11:20,880 --> 00:11:22,560
一些感觉运动转换，

286
00:11:22,560 --> 00:11:24,300
然后您执行一个动作

287
00:11:24,300 --> 00:11:26,279
活跃的婴儿非常

288
00:11:26,279 --> 00:11:28,980
强调这样一个事实，即推理或

289
00:11:28,980 --> 00:11:31,320
处理不确定性的问题是

290
00:11:31,320 --> 00:11:33,959
感知或我们

291
00:11:33,959 --> 00:11:36,000
所说的状态估计或信念

292
00:11:36,000 --> 00:11:38,519
更新的特征 作为

293
00:11:38,519 --> 00:11:40,500
主动推理部分

294
00:11:40,500 --> 00:11:42,300
发挥作用的行动，因此代理人不仅

295
00:11:42,300 --> 00:11:44,640
更新了他们对世界状态

296
00:11:44,640 --> 00:11:46,320
的信念，还

297
00:11:46,320 --> 00:11:50,700
通过最小化这种

298
00:11:50,700 --> 00:11:52,620
被称为自由能的惊喜所束缚的隐藏状态，这

299
00:11:52,620 --> 00:11:55,380
就是这些 ham holsian

300
00:11:55,380 --> 00:11:58,500
感知作为推理的想法来自

301
00:11:58,500 --> 00:12:01,920
但你也在最小化惊喜

302
00:12:01,920 --> 00:12:05,279
或 一定会惊讶地选择你的

303
00:12:05,279 --> 00:12:09,120
行动，所以推断行动

304
00:12:09,120 --> 00:12:11,459
只是另一种推理

305
00:12:11,459 --> 00:12:13,740
问题，所以政策或

306
00:12:13,740 --> 00:12:15,839
行动序列被认为是潜在变量

307
00:12:15,839 --> 00:12:18,120
或隐藏状态，然后你也

308
00:12:18,120 --> 00:12:21,000
对这些进行推断并通过投下

309
00:12:21,000 --> 00:12:22,980
感知行动硬币的两面

310
00:12:22,980 --> 00:12:25,200
作为意外最小化的一个例子，

311
00:12:25,200 --> 00:12:27,899
你的事件是一种

312
00:12:27,899 --> 00:12:29,120
表现出

313
00:12:29,120 --> 00:12:32,940
有目的和好奇行为的代理人，

314
00:12:32,940 --> 00:12:35,519
并且非常像没有关于主动推理的任何东西

315
00:12:35,519 --> 00:12:37,320
，这意味着你必须

316
00:12:37,320 --> 00:12:39,660
使用离散状态空间或 Pi MVP

317
00:12:39,660 --> 00:12:42,720
，这只是一种特定类型或

318
00:12:42,720 --> 00:12:44,519
类别的生成 用于主动推理的模型，

319
00:12:44,519 --> 00:12:45,600


320
00:12:45,600 --> 00:12:49,320
但是嗯，Palm DP 离散状态

321
00:12:49,320 --> 00:12:51,720
空间生成模型真的很

322
00:12:51,720 --> 00:12:53,279
容易与主动保险一起使用，

323
00:12:53,279 --> 00:12:54,899
因为当你处理 palm DPS 时，

324
00:12:54,899 --> 00:12:57,600
我在这里展示的很多这些数量都非常容易

325
00:12:57,600 --> 00:12:59,040
计算

326
00:12:59,040 --> 00:13:00,660
或部分观察到的标记

327
00:13:00,660 --> 00:13:02,820
决策过程，因此我们将

328
00:13:02,820 --> 00:13:04,800
稍微了解所有数学，

329
00:13:04,800 --> 00:13:06,600
但是 这只是

330
00:13:06,600 --> 00:13:08,820
主动推理代理和环境的基本范式

331
00:13:08,820 --> 00:13:11,100
试图通过最小化意外来减少

332
00:13:11,100 --> 00:13:13,440
感知和行动的

333
00:13:13,440 --> 00:13:15,600
意外，

334
00:13:15,600 --> 00:13:17,100
嗯

335
00:13:17,100 --> 00:13:20,040
，只是为了

336
00:13:20,040 --> 00:13:22,079
对主动推理

337
00:13:22,079 --> 00:13:24,660
和离散状态空间进行更深入的数学审查，即使用

338
00:13:24,660 --> 00:13:26,399
这些部分观察到的马尔可夫决策

339
00:13:26,399 --> 00:13:28,860
过程 我建议阅读这篇

340
00:13:28,860 --> 00:13:31,620
论文，该论文在

341
00:13:31,620 --> 00:13:34,440
Lance Thomas

342
00:13:34,440 --> 00:13:38,040
Noor Sebastian victorita 和 Carl 撰写的医学心理学杂志中非常出色，

343
00:13:38,040 --> 00:13:40,680
它只是

344
00:13:40,680 --> 00:13:42,899
从形式基础上非常好的描述，说明我们如何

345
00:13:42,899 --> 00:13:44,459


346
00:13:44,459 --> 00:13:46,440
从最像形式的形式开始进行主动推理的更新方程式

347
00:13:46,440 --> 00:13:48,660
分类分布和狄利克雷

348
00:13:48,660 --> 00:13:50,639
分布的处理

349
00:13:50,639 --> 00:13:53,040
以及我们存档的 pine DP 论文

350
00:13:53,040 --> 00:13:55,380
也有一堆

351
00:13:55,380 --> 00:13:58,079
附录，这些附录做了很多类似

352
00:13:58,079 --> 00:14:00,779
的数学运算，所以我也会推荐

353
00:14:00,779 --> 00:14:02,720
人们参考那个

354
00:14:02,720 --> 00:14:05,220
好吧，现在让我们进入

355
00:14:05,220 --> 00:14:06,899
生成 在 Pym DP 中

356
00:14:06,899 --> 00:14:10,440
构成 uh 代理人大脑的面包和黄油的模型

357
00:14:10,440 --> 00:14:13,139
本质上如此 Centr  al to

358
00:14:13,139 --> 00:14:14,760
active inferences writing down a

359
00:14:14,760 --> 00:14:16,800
generative model which is just

360
00:14:16,800 --> 00:14:19,139
an specification of an agent

361
00:14:19,139 --> 00:14:21,180
how believe its World Works 它

362
00:14:21,180 --> 00:14:22,980
的环境是

363
00:14:22,980 --> 00:14:24,839
如何运作的，嗯，它是如何相信它的环境

364
00:14:24,839 --> 00:14:27,540
会像世界的动态一样影响自己，

365
00:14:27,540 --> 00:14:29,639
呃进步以及

366
00:14:29,639 --> 00:14:31,920
那些隐藏的如何 状态动力学也会

367
00:14:31,920 --> 00:14:34,139
产生观察结果，这些观察结果全部编码

368
00:14:34,139 --> 00:14:36,240
在我们称之为生成模型或

369
00:14:36,240 --> 00:14:38,519
世界模型中，有些人也称之为世界模型，

370
00:14:38,519 --> 00:14:39,839
因此

371
00:14:39,839 --> 00:14:43,199
在 Pine DP 中，我们只处理一种非常

372
00:14:43,199 --> 00:14:44,820
特定的性别模型，这些模型

373
00:14:44,820 --> 00:14:45,959
被称为这些部分观察到的

374
00:14:45,959 --> 00:14:48,600
马尔可夫决策过程 所以这些是

375
00:14:48,600 --> 00:14:50,279


376
00:14:50,279 --> 00:14:52,380
不确定情况下连续决策和规划的经典模型

377
00:14:52,380 --> 00:14:54,180
它们不是主动推理所独有的

378
00:14:54,180 --> 00:14:57,480
人们使用 Palm DP 模型进行经典

379
00:14:57,480 --> 00:14:59,160
强化学习和各种

380
00:14:59,160 --> 00:15:01,620
决策问题

381
00:15:01,620 --> 00:15:03,480
嗯 它们被称为马尔可夫马尔可夫

382
00:15:03,480 --> 00:15:05,279
决策过程 因为

383
00:15:05,279 --> 00:15:07,440
当前时间的状态仅取决于

384
00:15:07,440 --> 00:15:09,060
前一时间状态中的动作，

385
00:15:09,060 --> 00:15:10,740
因此这就是定义 在马尔可夫

386
00:15:10,740 --> 00:15:12,420
过程中，它们有点具有这种浅

387
00:15:12,420 --> 00:15:14,339
时间依赖性

388
00:15:14,339 --> 00:15:15,800
，例如，非马尔可夫

389
00:15:15,800 --> 00:15:18,120
过程不像

390
00:15:18,120 --> 00:15:20,279
具有长期或更深时间

391
00:15:20,279 --> 00:15:21,899
依赖性的过程那样延伸

392
00:15:21,899 --> 00:15:23,579
到过去，

393
00:15:23,579 --> 00:15:25,680
嗯，Palm DPS 通常 但并不总是

394
00:15:25,680 --> 00:15:27,720
制定离散状态空间和

395
00:15:27,720 --> 00:15:29,940
离散时间关于

396
00:15:29,940 --> 00:15:32,760
palmdp 这个词没有任何意义意味着它们必须是

397
00:15:32,760 --> 00:15:35,160
这些多项式或分类

398
00:15:35,160 --> 00:15:37,500
分布但是当我们

399
00:15:37,500 --> 00:15:39,240
谈论 Palm DPS 和主动影响时

400
00:15:39,240 --> 00:15:40,980
我们几乎总是在谈论这些

401
00:15:40,980 --> 00:15:43,760
离散的

402
00:15:43,800 --> 00:15:45,899
嗯 基本上你

403
00:15:45,899 --> 00:15:47,940
一次只能处于 K 个离散状态

404
00:15:47,940 --> 00:15:50,279
之一，下一次你只能进入

405
00:15:50,279 --> 00:15:52,260
K 个离散状态之一，所以

406
00:15:52,260 --> 00:15:53,699
一切都是离散的，但

407
00:15:53,699 --> 00:15:55,920
这些马尔可夫

408
00:15:55,920 --> 00:15:57,240
决策过程本质上没有什么必须是

409
00:15:57,240 --> 00:15:59,040
离散的 I 我认为这值得

410
00:15:59,040 --> 00:16:00,420
一提，因为这

411
00:16:00,420 --> 00:16:02,820
是人们

412
00:16:02,820 --> 00:16:04,139
在看到 Palm DP um 时经常做出的一种重要混合

413
00:16:04,139 --> 00:16:05,699


414
00:16:05,699 --> 00:16:08,279
我们决定使用这个

415
00:16:08,279 --> 00:16:11,040
uh 离散 Palm DP 生成模型进行

416
00:16:11,040 --> 00:16:12,899
主动推理的原因不仅仅是因为

417
00:16:12,899 --> 00:16:15,899
它在顺序决策和规划中的应用，

418
00:16:15,899 --> 00:16:18,060
而且

419
00:16:18,060 --> 00:16:20,339


420
00:16:20,339 --> 00:16:24,240
自 2010 年到 2011 年以来已有大量关于

421
00:16:24,240 --> 00:16:26,760
使用 pom DPS 作为 用于

422
00:16:26,760 --> 00:16:28,079
决策测试的生成模型，所以用这些模型

423
00:16:28,079 --> 00:16:30,540
进行主动推理的所有数学

424
00:16:30,540 --> 00:16:32,699
都已经完成，所以我们

425
00:16:32,699 --> 00:16:34,860
不必发明任何新的数学或

426
00:16:34,860 --> 00:16:36,720
理论来实际编写代码，

427
00:16:36,720 --> 00:16:38,339
因为其中很多已经写在

428
00:16:38,339 --> 00:16:41,040
论文中 就像 10 年这样让

429
00:16:41,040 --> 00:16:44,100
我们的生活更轻松开发它所以

430
00:16:44,100 --> 00:16:46,320
现在让我们深入

431
00:16:46,320 --> 00:16:48,899
了解这些 Palm DPS 的主要组件

432
00:16:48,899 --> 00:16:51,480
呃我在这里只列出四个主要

433
00:16:51,480 --> 00:16:54,360
组件但是

434
00:16:54,360 --> 00:16:55,980
如果有人感兴趣我们可以讨论其他组件

435
00:16:55,980 --> 00:16:58,560
但是 本质

436
00:16:58,560 --> 00:17:01,199
上，顶部的这一行是

437
00:17:01,199 --> 00:17:02,820
根据

438
00:17:02,820 --> 00:17:05,099


439
00:17:05,099 --> 00:17:08,640
Into the Fut 中隐藏状态 s 和观察值 o 的联合分布对生成模型的描述 是的

440
00:17:08,640 --> 00:17:11,939
，嗯，因为

441
00:17:11,939 --> 00:17:13,500
这个生成模型的马尔可夫性质，你可以

442
00:17:13,500 --> 00:17:15,000
写

443
00:17:15,000 --> 00:17:16,619
这个联合分布，这个

444
00:17:16,619 --> 00:17:20,339
分解的基本乘积是

445
00:17:20,339 --> 00:17:22,559
先验和

446
00:17:22,559 --> 00:17:24,179
跨时间因式分解的可能性，所以这就是为什么

447
00:17:24,179 --> 00:17:25,740
它们是随着时间的推移在顶部的那些产品，

448
00:17:25,740 --> 00:17:27,839
但它

449
00:17:27,839 --> 00:17:29,640
现在理解数学并不重要，但

450
00:17:29,640 --> 00:17:31,440
我们将

451
00:17:31,440 --> 00:17:34,320
在一秒钟内映射到示意图上的主要组成部分是代理

452
00:17:34,320 --> 00:17:36,360
关于隐藏状态如何

453
00:17:36,360 --> 00:17:38,940
引起观察的信念，我们将其编码在

454
00:17:38,940 --> 00:17:40,440
称为观察模型

455
00:17:40,440 --> 00:17:42,539
或似然映射的东西中 通常

456
00:17:42,539 --> 00:17:45,360
称为 a 矩阵或 a 数组，所以这是

457
00:17:45,360 --> 00:17:47,820


458
00:17:47,820 --> 00:17:50,460
当前隐藏状态如何影响

459
00:17:50,460 --> 00:17:52,559
或引起

460
00:17:52,559 --> 00:17:54,000
当前

461
00:17:54,000 --> 00:17:56,340
时间观察的概率表示，我们有转换或

462
00:17:56,340 --> 00:17:58,860
动态模型，这是另

463
00:17:58,860 --> 00:18:02,039
一种编码的可能性 agent 对

464
00:18:02,039 --> 00:18:03,960
某一

465
00:18:03,960 --> 00:18:05,640
时刻的隐藏状态如何与下一次的隐藏状态相关的信念，

466
00:18:05,640 --> 00:18:08,100
所以这个 Maps

467
00:18:08,100 --> 00:18:09,960
um 这就是 agent 使用的 对

468
00:18:09,960 --> 00:18:12,539


469
00:18:12,539 --> 00:18:14,940
如果发生这种情况或发生那种情况世界将如何演变做出前瞻性预测

470
00:18:14,940 --> 00:18:17,100
，并根据我昨天所在的位置携带过去的

471
00:18:17,100 --> 00:18:19,080
信息或经验先验，

472
00:18:19,080 --> 00:18:22,080


473
00:18:22,080 --> 00:18:24,720
现在我必须在哪里给出我

474
00:18:24,720 --> 00:18:26,400
对世界如何演变的信念 有时这

475
00:18:26,400 --> 00:18:28,860
一切都编码在 B 数组或 B

476
00:18:28,860 --> 00:18:31,679
矩阵中，然后这最后两个

477
00:18:31,679 --> 00:18:33,600
是先验

478
00:18:33,600 --> 00:18:35,700
的，其中第一个非常重要

479
00:18:35,700 --> 00:18:38,880
，称为 C 数组或 C

480
00:18:38,880 --> 00:18:41,340
向量，它编码代理人的先验

481
00:18:41,340 --> 00:18:43,679
信念可能是什么观察结果

482
00:18:43,679 --> 00:18:46,260
遇到，正如我们在一开始所说的那样

483
00:18:46,260 --> 00:18:47,520
，主动推理就是

484
00:18:47,520 --> 00:18:49,980
将行动和感知都

485
00:18:49,980 --> 00:18:51,660
视为一个推理问题，

486
00:18:51,660 --> 00:18:54,900
因此主动推理作为一种框架

487
00:18:54,900 --> 00:18:56,880
，

488
00:18:56,880 --> 00:18:58,500


489
00:18:58,500 --> 00:19:00,480
通过说而不是奖励来颠覆奖励函数和强化学习的经典范式

490
00:19:00,480 --> 00:19:02,700
function 只是让 agent 有

491
00:19:02,700 --> 00:19:04,559
一种乐观的信念，相信

492
00:19:04,559 --> 00:19:07,140
将来我会看到这种数据，

493
00:19:07,140 --> 00:19:09,120
然后通过 resp 进行推理

494
00:19:09,120 --> 00:19:10,980
对于具有先验信念的生成模型

495
00:19:10,980 --> 00:19:13,200
，代理人

496
00:19:13,200 --> 00:19:14,820
看起来像是在寻找

497
00:19:14,820 --> 00:19:16,919
符合其先验的观察结果，

498
00:19:16,919 --> 00:19:19,140
因此这符合这样的

499
00:19:19,140 --> 00:19:21,419
想法，即主动推理是

500
00:19:21,419 --> 00:19:23,340
代理人相信我的自我实现预言的过程

501
00:19:23,340 --> 00:19:25,919
我或多或少可能会看到

502
00:19:25,919 --> 00:19:27,840
某些观察结果，然后通过

503
00:19:27,840 --> 00:19:30,780
对具有这种

504
00:19:30,780 --> 00:19:32,580
偏向于生成模型的策略进行推理，代理人会

505
00:19:32,580 --> 00:19:34,740
有点使自己真正意识到

506
00:19:34,740 --> 00:19:37,799
其先验偏好或这些

507
00:19:37,799 --> 00:19:40,020
关于观察的先验信念，

508
00:19:40,020 --> 00:19:42,660
因此所有这些都编码在 C vector

509
00:19:42,660 --> 00:19:44,880
和 C Vector 你基本上可以

510
00:19:44,880 --> 00:19:48,000
认为是奖励函数的贝叶斯翻译

511
00:19:48,000 --> 00:19:49,340
，你实际上可以

512
00:19:49,340 --> 00:19:51,600
将 C Vector 的条目与

513
00:19:51,600 --> 00:19:53,220
两个奖励和强化

514
00:19:53,220 --> 00:19:55,080
学习联系起来，但我们不会，我们不必

515
00:19:55,080 --> 00:19:57,000
进入那个 现在，但

516
00:19:57,000 --> 00:20:00,000
如果有人感兴趣，我可以分享论文，

517
00:20:00,000 --> 00:20:01,919
最后是隐藏状态的先验，

518
00:20:01,919 --> 00:20:04,200
这只是代理人

519
00:20:04,200 --> 00:20:07,559
对先验 li 的信念

520
00:20:07,559 --> 00:20:09,539
每个隐藏状态在模拟的第一个

521
00:20:09,539 --> 00:20:11,820
时间步的可能性，所以这

522
00:20:11,820 --> 00:20:14,160
在主动推理中并不是真正必要的事情，

523
00:20:14,160 --> 00:20:15,900
但通常当我们

524
00:20:15,900 --> 00:20:18,299
模拟代理时，我们

525
00:20:18,299 --> 00:20:20,039
使用具有开始时间

526
00:20:20,039 --> 00:20:23,280
和结束时间的有限时间范围 所以如果你有一个

527
00:20:23,280 --> 00:20:25,200
有限的临时地平线，这意味着

528
00:20:25,200 --> 00:20:27,240
你必须基本上插入一个

529
00:20:27,240 --> 00:20:29,100
关于世界

530
00:20:29,100 --> 00:20:31,740
在时间步长是什么样子的先验信念，并且先验信念

531
00:20:31,740 --> 00:20:34,940
被编码在这个 d d 向量中

532
00:20:34,940 --> 00:20:38,460
，我们可以直观地勾勒

533
00:20:38,460 --> 00:20:41,039
出 Palm DP 生成模型作为

534
00:20:41,039 --> 00:20:43,799
贝叶斯图，其中如果节点

535
00:20:43,799 --> 00:20:45,720
彼此依赖，则节点通过这些箭头的边连接，

536
00:20:45,720 --> 00:20:47,280
因此这些红色节点

537
00:20:47,280 --> 00:20:48,660
表示

538
00:20:48,660 --> 00:20:51,720
随着时间的推移相互转换的隐藏状态

539
00:20:51,720 --> 00:20:53,700
，蓝色节点 O 节点

540
00:20:53,700 --> 00:20:56,340
表示代理的信念 关于如何

541
00:20:56,340 --> 00:20:58,320
从这些隐藏状态生成观察结果，

542
00:20:58,320 --> 00:21:00,299
所以这只是

543
00:21:00,299 --> 00:21:02,760
Palm DP 的图形表示

544
00:21:02,760 --> 00:21:05,580
和矩阵编码，正如我们所说的

545
00:21:05,580 --> 00:21:07,020
代理人对那些隐藏状态的信念 d

546
00:21:07,020 --> 00:21:09,120
节点随时产生蓝色节点，

547
00:21:09,120 --> 00:21:11,280
所以通常我们假设这个

548
00:21:11,280 --> 00:21:13,740
矩阵是时不变的，所以他们

549
00:21:13,740 --> 00:21:15,539
相信这个观察映射

550
00:21:15,539 --> 00:21:18,120
本身并不依赖于时间，这

551
00:21:18,120 --> 00:21:19,980
至少是大多数 Palm DPS 中所做的假设

552
00:21:19,980 --> 00:21:21,840


553
00:21:21,840 --> 00:21:25,860
，然后 还有代理人时

554
00:21:25,860 --> 00:21:27,299
不变的信念，

555
00:21:27,299 --> 00:21:30,179
关于世界的转换动态如何

556
00:21:30,179 --> 00:21:32,940
带来时间 T 减去时间 1 的两个状态

557
00:21:32,940 --> 00:21:35,159


558
00:21:35,159 --> 00:21:38,280
en 然后政策选择或

559
00:21:38,280 --> 00:21:41,760
行动通过将行动框架化为受控转变在非主动推理中发挥

560
00:21:41,760 --> 00:21:43,559
作用，

561
00:21:43,559 --> 00:21:46,980
所以 B  Matrix

562
00:21:46,980 --> 00:21:49,020
不只是说它应该

563
00:21:49,020 --> 00:21:50,520
如何考虑到昨天的世界现在应该如何看，

564
00:21:50,520 --> 00:21:52,919


565
00:21:52,919 --> 00:21:55,320
而且考虑到昨天的情况

566
00:21:55,320 --> 00:21:57,720
以及我在时间 T 减一时采取这个行动的事实，它现在应该如何看待，

567
00:21:57,720 --> 00:21:59,820
所以 B Matrix 不仅是

568
00:21:59,820 --> 00:22:01,559
以过去的状态为条件，也以

569
00:22:01,559 --> 00:22:04,320
以前的行为为条件，因此政策 Pi

570
00:22:04,320 --> 00:22:07,140
作为对这些行为的信念或分布发挥

571
00:22:07,140 --> 00:22:10,020
作用，因此这些行为

572
00:22:10,020 --> 00:22:11,880
直接影响转换，

573
00:22:11,880 --> 00:22:13,320
然后影响交易 状态会影响状态，

574
00:22:13,320 --> 00:22:14,520
所以这就是我们

575
00:22:14,520 --> 00:22:18,059
在许多 palm DP 场景和

576
00:22:18,059 --> 00:22:20,760
动作中考虑动作的方式，或者策略 Pi

577
00:22:20,760 --> 00:22:23,520
是动作序列或动作集合

578
00:22:23,520 --> 00:22:26,039
，然后我们有这个关键的目标

579
00:22:26,039 --> 00:22:28,380
函数，呃，称为预期自由

580
00:22:28,380 --> 00:22:31,080
能，它决定 哪些行动

581
00:22:31,080 --> 00:22:33,299
比其他行动更有可能，因此通过最小化

582
00:22:33,299 --> 00:22:35,820
预期的自由能，我们优化了

583
00:22:35,820 --> 00:22:37,860
对政策的信念，然后

584
00:22:37,860 --> 00:22:39,659
预期的自由能本身是

585
00:22:39,659 --> 00:22:41,280
你的生成模型的函数你

586
00:22:41,280 --> 00:22:43,500
对世界的信念，其中包括

587
00:22:43,500 --> 00:22:45,539
关于你期望自己观察什么的偏见先验信念

588
00:22:45,539 --> 00:22:47,580


589
00:22:47,580 --> 00:22:50,220
看到这种奖励函数

590
00:22:50,220 --> 00:22:52,919
通过这种预期的自由能进入政策选择

591
00:22:52,919 --> 00:22:54,240
，这就是为什么我们仍然能够

592
00:22:54,240 --> 00:22:57,000
将所有这些称为

593
00:22:57,000 --> 00:22:58,559
推理问题的一种形式，因为我们正在

594
00:22:58,559 --> 00:23:00,780
最小化这种预期的

595
00:23:00,780 --> 00:23:03,600
惊喜界限 这样做我们可以

596
00:23:03,600 --> 00:23:05,640
推断出政策的分布，

597
00:23:05,640 --> 00:23:08,400
然后我们从中抽样以实际产生

598
00:23:08,400 --> 00:23:10,500
行动并改变

599
00:23:10,500 --> 00:23:12,539
世界 d 然后就像我说的那样，D 向量

600
00:23:12,539 --> 00:23:14,340
基本上只是初始隐藏状态分布的先验，

601
00:23:14,340 --> 00:23:17,959


602
00:23:18,179 --> 00:23:19,260


603
00:23:19,260 --> 00:23:22,679
所以总而言之，要构建一个 palmdp

604
00:23:22,679 --> 00:23:24,120
主动推理模型，

605
00:23:24,120 --> 00:23:26,700
你必须写下或编码

606
00:23:26,700 --> 00:23:29,460
这些 a b c 和 D，还有一些

607
00:23:29,460 --> 00:23:30,900
其他先验 我们也可以

608
00:23:30,900 --> 00:23:33,000
像先验策略一样讨论

609
00:23:33,000 --> 00:23:34,980
称为 e Vector 但现在你

610
00:23:34,980 --> 00:23:37,200
可以认为

611
00:23:37,200 --> 00:23:39,780
主动推理中的大部分繁重工作包括

612
00:23:39,780 --> 00:23:41,039
写下这些东西实际上

613
00:23:41,039 --> 00:23:43,620
编码你的代理人

614
00:23:43,620 --> 00:23:46,320
对世界的看法 它存在于

615
00:23:46,320 --> 00:23:48,120
嗯，这些东西在这些分类离散分布的情况下，

616
00:23:48,120 --> 00:23:50,520
它们

617
00:23:50,520 --> 00:23:53,159
最终看起来像矩阵和向量，

618
00:23:53,159 --> 00:23:54,900
因为一切都是分类

619
00:23:54,900 --> 00:23:57,059
分布，你不是在

620
00:23:57,059 --> 00:23:59,100
处理连续的无限维

621
00:23:59,100 --> 00:24:01,200
空间，而是在处理

622
00:24:01,200 --> 00:24:02,940
具有离散数字的东西 诸如

623
00:24:02,940 --> 00:24:05,340
四乘四矩阵或五乘五

624
00:24:05,340 --> 00:24:07,260
矩阵之类的条目

625
00:24:07,260 --> 00:24:10,980
嗯是的我想在我们是之前所以

626
00:24:10,980 --> 00:24:12,299
下一部分我将讨论

627
00:24:12,299 --> 00:24:14,760
像 Matlab py  thon

628
00:24:14,760 --> 00:24:17,340
um dialectic before we move on that

629
00:24:17,340 --> 00:24:19,200
though we should may pause if there are

630
00:24:19,200 --> 00:24:21,780
any questions

631
00:24:21,780 --> 00:24:25,080
foreign yeah 非常感谢

632
00:24:25,080 --> 00:24:28,020
Daphne 或 Jakob 你想提供

633
00:24:28,020 --> 00:24:31,879
任何想法或思考吗？

634
00:24:35,000 --> 00:24:37,020
有一件事我一直

635
00:24:37,020 --> 00:24:38,340
有点困惑

636
00:24:38,340 --> 00:24:40,620
现在可能会很好地解决

637
00:24:40,620 --> 00:24:42,360
之前偏好的参数化，

638
00:24:42,360 --> 00:24:43,980


639
00:24:43,980 --> 00:24:47,640
你能解释一下

640
00:24:47,640 --> 00:24:49,980
为什么当你初始化它们时你只是

641
00:24:49,980 --> 00:24:52,760
用像

642
00:24:52,799 --> 00:24:55,679
um 整数一样大于 1

643
00:24:55,679 --> 00:24:57,900
大于或等于 1 但

644
00:24:57,900 --> 00:25:00,299
你喜欢 soft maximum 稍后当你

645
00:25:00,299 --> 00:25:02,039
实际使用像为什么

646
00:25:02,039 --> 00:25:04,559
我们不定义我们对这些概率的

647
00:25:04,559 --> 00:25:05,820


648
00:25:05,820 --> 00:25:08,460
偏好这样的推理时，这是一个很好的，这是一个很好的

649
00:25:08,460 --> 00:25:13,380
点，也是一件值得提及的好事，

650
00:25:13,620 --> 00:25:15,960
所以通常在 Matlab 和

651
00:25:15,960 --> 00:25:17,940
python 实现中，所以在构建时

652
00:25:17,940 --> 00:25:22,559
Pi mdp 我们决定在对

653
00:25:22,559 --> 00:25:24,659
c Vector 进行编码时做同样的事情，

654
00:25:24,659 --> 00:25:27,299
而不是将其编码为

655
00:25:27,299 --> 00:25:29,039
概率，它最终应该

656
00:25:29,039 --> 00:25:31,260
是你想要的 ally write down

657
00:25:31,260 --> 00:25:33,840
是 C Vector 的对数，所以你

658
00:25:33,840 --> 00:25:36,419
根据相对对数概率将其写下来

659
00:25:36,419 --> 00:25:40,080
，这可能是一个

660
00:25:40,080 --> 00:25:41,580


661
00:25:41,580 --> 00:25:44,340
更直观的参数化而

662
00:25:44,340 --> 00:25:46,080
不是直接根据概率写下来的

663
00:25:46,080 --> 00:25:49,620
原因是因为概率的对数

664
00:25:49,620 --> 00:25:51,840
是 更像是实际的

665
00:25:51,840 --> 00:25:54,299
奖励，所以如果我们回到这个

666
00:25:54,299 --> 00:25:56,220
生成模型，

667
00:25:56,220 --> 00:25:59,039
作为自由能的预期自由能

668
00:25:59,039 --> 00:26:01,559
实际上是

669
00:26:01,559 --> 00:26:02,220
um

670
00:26:02,220 --> 00:26:04,559
然后自由能是

671
00:26:04,559 --> 00:26:06,960
一种 KL 散度，

672
00:26:06,960 --> 00:26:08,820
它是用自然对数形式的比特编码的

673
00:26:08,820 --> 00:26:10,320
空间

674
00:26:10,320 --> 00:26:13,620
所以如果你在日志空间方面像之前那样写下来

675
00:26:13,620 --> 00:26:17,820
你

676
00:26:17,820 --> 00:26:21,059
知道如果我改变

677
00:26:21,059 --> 00:26:24,059
我之前的偏好中的日志单位数量我

678
00:26:24,059 --> 00:26:26,400
有它它与预期免费的变化更线性相关

679
00:26:26,400 --> 00:26:28,380


680
00:26:28,380 --> 00:26:30,080
导致该观察的策略的能量，

681
00:26:30,080 --> 00:26:32,220
而如果您

682
00:26:32,220 --> 00:26:35,760
根据概率空间写先验，那么

683
00:26:35,760 --> 00:26:37,980
ex 预期

684
00:26:37,980 --> 00:26:39,779
自由能因改变某些东西而产生的变化

685
00:26:39,779 --> 00:26:43,320
概率空间中的 g 不会是

686
00:26:43,320 --> 00:26:45,120
线性的，它会是非线性的

687
00:26:45,120 --> 00:26:45,900
变化，

688
00:26:45,900 --> 00:26:48,659
所以如果我们将奖励视为对数

689
00:26:48,659 --> 00:26:49,919
概率

690
00:26:49,919 --> 00:26:53,100
，就像一个额外的对数单位

691
00:26:53,100 --> 00:26:55,919
或一个自然对数，一个 Nat 或

692
00:26:55,919 --> 00:26:59,279
比另一个东西更有价值一点 这

693
00:26:59,279 --> 00:27:00,960
实际上会根据

694
00:27:00,960 --> 00:27:02,460


695
00:27:02,460 --> 00:27:04,799
看到事物一与事物二之间的预期自由能差异来反映自己，

696
00:27:04,799 --> 00:27:06,480
而如果我们

697
00:27:06,480 --> 00:27:08,340
根据概率对这两件事进行编码，那么

698
00:27:08,340 --> 00:27:10,020
预期的自由能差异将不会

699
00:27:10,020 --> 00:27:12,299
那么直观，因为它不是

700
00:27:12,299 --> 00:27:13,740
线性的

701
00:27:13,740 --> 00:27:16,080
嗯，这就是为什么

702
00:27:16,080 --> 00:27:18,659
我们决定在对数空间中编码事物的一个快速原因，

703
00:27:18,659 --> 00:27:21,179
但是在数学上没有

704
00:27:21,179 --> 00:27:23,039
任何必要的东西，你可以很容易地

705
00:27:23,039 --> 00:27:24,240


706
00:27:24,240 --> 00:27:26,580
直接用

707
00:27:26,580 --> 00:27:29,039
概率来写先验，它更像

708
00:27:29,039 --> 00:27:30,779
是一种方法 让它

709
00:27:30,779 --> 00:27:32,460
更像真正的奖励

710
00:27:32,460 --> 00:27:36,200
是的，这很有意义谢谢，

711
00:27:36,200 --> 00:27:39,860
谢谢雅各布

712
00:27:40,140 --> 00:27:43,260
是的，嗯，我想知道

713
00:27:43,260 --> 00:27:44,880


714
00:27:44,880 --> 00:27:49,860
amb 矩阵

715
00:27:49,860 --> 00:27:53,039
或张量是否有心脏要求是实际的 ly 编码为

716
00:27:53,039 --> 00:27:56,279
分类矩阵，或者因为

717
00:27:56,279 --> 00:27:58,020
考虑像 a 矩阵的函数

718
00:27:58,020 --> 00:28:00,840
以及代理使用它

719
00:28:00,840 --> 00:28:03,000
从观察中推断其隐藏状态的方式，我在

720
00:28:03,000 --> 00:28:05,340
考虑

721
00:28:05,340 --> 00:28:07,980
是否有可能将高

722
00:28:07,980 --> 00:28:09,720
维状态空间编码

723
00:28:09,720 --> 00:28:11,460
为神经网络 网络可以

724
00:28:11,460 --> 00:28:14,279
近似呃这些概率

725
00:28:14,279 --> 00:28:17,279
表示以及学习

726
00:28:17,279 --> 00:28:20,039
表示

727
00:28:20,039 --> 00:28:23,400
观察和隐藏状态之间关系的类型

728
00:28:23,400 --> 00:28:25,460


729
00:28:25,500 --> 00:28:29,460
嗯是的肯定是的所以基本上

730
00:28:29,460 --> 00:28:32,460
很抱歉可能会有类似的但

731
00:28:32,460 --> 00:28:34,080
基本上你是否认为这

732
00:28:34,080 --> 00:28:36,779
是可能的因为我我 感觉就像

733
00:28:36,779 --> 00:28:39,179
是 python 实现，我相信

734
00:28:39,179 --> 00:28:41,880
你会了解 python 相对于 Matlab 的优势，

735
00:28:41,880 --> 00:28:43,980
嗯，

736
00:28:43,980 --> 00:28:46,799
它还提供了与

737
00:28:46,799 --> 00:28:51,919
其他库和数据科学内部的互操作性，

738
00:28:51,919 --> 00:28:54,659
完全是的，这是一个很好的观点，

739
00:28:54,659 --> 00:28:56,880
所以我在

740
00:28:56,880 --> 00:28:59,159
一开始就提到了我们 谈到 Palm DPS

741
00:28:59,159 --> 00:29:01,020
没有关于 Palm DPS 的内容，这

742
00:29:01,020 --> 00:29:02,520
意味着所有这些东西都必须是

743
00:29:02,520 --> 00:29:05,279
矩阵 在 EP 上定义的主要内容

744
00:29:05,279 --> 00:29:06,600
是它是部分

745
00:29:06,600 --> 00:29:09,419
可观察的，因此代理只能

746
00:29:09,419 --> 00:29:11,820
看到蓝色节点，并且它是

747
00:29:11,820 --> 00:29:14,100
一个马尔可夫过程，因此隐藏状态

748
00:29:14,100 --> 00:29:16,080
的记忆中有一个暂时的浅度，

749
00:29:16,080 --> 00:29:18,000
但你可以替换

750
00:29:18,000 --> 00:29:21,240
a 和 b 对于任何类型的参数化，

751
00:29:21,240 --> 00:29:23,279
它都必须是一个适当的似然

752
00:29:23,279 --> 00:29:26,159
函数，所以它必须具有像

753
00:29:26,159 --> 00:29:28,080
嗯适当的可能性的属性，

754
00:29:28,080 --> 00:29:29,940
没有什么

755
00:29:29,940 --> 00:29:32,820
意味着它不能是多元高斯

756
00:29:32,820 --> 00:29:35,159
分布或科西分布或伯努利

757
00:29:35,159 --> 00:29:38,279
分布或呃 是的，任何一种

758
00:29:38,279 --> 00:29:41,220
指数族或

759
00:29:41,220 --> 00:29:43,440


760
00:29:43,440 --> 00:29:46,140
类似人们在强化学习中经常使用的参数化神经网络，

761
00:29:46,140 --> 00:29:47,399
就像他们会

762
00:29:47,399 --> 00:29:49,200
用一堆神经网络参数化动力学模型

763
00:29:49,200 --> 00:29:51,720
，这些神经网络说

764
00:29:51,720 --> 00:29:53,820
在时间 T 获取时间状态并将它们移动

765
00:29:53,820 --> 00:29:56,340
到状态 在时间 t 加 1 时，

766
00:29:56,340 --> 00:29:58,799
存在困难的原因仅仅

767
00:29:58,799 --> 00:30:00,899
是因为不清楚

768
00:30:00,899 --> 00:30:03,000
如何计算诸如一旦你站起来后的预期

769
00:30:03,000 --> 00:30:06,059
自由能之类的东西 rt 进入

770
00:30:06,059 --> 00:30:07,320
这些

771
00:30:07,320 --> 00:30:10,020
um 表现不佳的分布，

772
00:30:10,020 --> 00:30:12,539
所以这个

773
00:30:12,539 --> 00:30:14,580
框架是在离散

774
00:30:14,580 --> 00:30:17,220
分类状态版本中开发的很多原因不仅

775
00:30:17,220 --> 00:30:19,260
是因为对低维任务建模，

776
00:30:19,260 --> 00:30:21,299
比如有人玩老虎

777
00:30:21,299 --> 00:30:23,880
机或决定向左或

778
00:30:23,880 --> 00:30:27,240
向右 a 在 y 迷宫中，很容易使用这些

779
00:30:27,240 --> 00:30:29,279
离散分布来

780
00:30:29,279 --> 00:30:30,720
很好地描述这种行为，这是

781
00:30:30,720 --> 00:30:32,640
一回事，但实际上也是一种

782
00:30:32,640 --> 00:30:34,980
数学易处理性，原因

783
00:30:34,980 --> 00:30:37,080
是预期的自由能并不总是封闭形式的解决方案，

784
00:30:37,080 --> 00:30:38,820


785
00:30:38,820 --> 00:30:40,559
具体取决于那些那些

786
00:30:40,559 --> 00:30:42,899
分布看起来像所以有一些

787
00:30:42,899 --> 00:30:45,000
工作要做，例如 Magnus

788
00:30:45,000 --> 00:30:48,480
cudall 有一篇关于熵的论文，他们

789
00:30:48,480 --> 00:30:50,760
计算线性动力系统中的预期自由能，

790
00:30:50,760 --> 00:30:52,559
基本上

791
00:30:52,559 --> 00:30:54,360
所有这些 A 和 B 都由高斯表示，

792
00:30:54,360 --> 00:30:57,059
所以他们称之为

793
00:30:57,059 --> 00:30:59,940
线性可控 动力系统

794
00:30:59,940 --> 00:31:01,260
，他们还必须对

795
00:31:01,260 --> 00:31:03,120
这些行为如何

796
00:31:03,120 --> 00:31:05,039
影响 B 矩阵做出一些假设 它实际上不再是矩阵

797
00:31:05,039 --> 00:31:07,100
它更像是一个连续的

798
00:31:07,100 --> 00:31:10,080
高斯并且他们发现

799
00:31:10,080 --> 00:31:11,880
你得到的正常项与预期的

800
00:31:11,880 --> 00:31:13,559
自由能一样信息增益

801
00:31:13,559 --> 00:31:15,720
那些

802
00:31:15,720 --> 00:31:17,760
当我们使用分类分布时我们得到的很多有趣的东西那些

803
00:31:17,760 --> 00:31:19,740


804
00:31:19,740 --> 00:31:21,659
当我们使用高斯时，术语实际上消失了，所以你

805
00:31:21,659 --> 00:31:23,520
实际上不会得到像

806
00:31:23,520 --> 00:31:26,580
信息增益搜索或信息

807
00:31:26,580 --> 00:31:28,260
搜索术语这样的好东西，你通常

808
00:31:28,260 --> 00:31:30,720
会通过使用分类得到，但还有

809
00:31:30,720 --> 00:31:32,460
其他方法可以解决这个问题，比如如果

810
00:31:32,460 --> 00:31:34,080
你使用神经网络，你仍然可以

811
00:31:34,080 --> 00:31:36,600
使用 诸如

812
00:31:36,600 --> 00:31:39,240
计算预期自由能的采样方法之类的

813
00:31:39,240 --> 00:31:40,919
东西，你基本上喜欢对

814
00:31:40,919 --> 00:31:42,600
一堆可能的轨迹

815
00:31:42,600 --> 00:31:44,820
进行采样，然后你可以使用像蒙特卡罗风格这样的样本

816
00:31:44,820 --> 00:31:46,679
来计算预期的

817
00:31:46,679 --> 00:31:48,179
自由能，还有很多

818
00:31:48,179 --> 00:31:50,039
已经完成的

819
00:31:50,039 --> 00:31:52,620
小组，比如缩放 对深度

820
00:31:52,620 --> 00:31:54,539
神经网络的主动推理他们已经使用了

821
00:31:54,539 --> 00:31:56,340
这种方法，所以像 Alec chances paper

822
00:31:56,340 --> 00:31:57,960
Scout scaling active infer  ence 做

823
00:31:57,960 --> 00:31:59,820
了很多类似的事情

824
00:31:59,820 --> 00:32:02,940
，比如 Tim verbalance group

825
00:32:02,940 --> 00:32:04,620
嗯，我不想离开任何我认为

826
00:32:04,620 --> 00:32:07,080
喜欢的人，也没有做过很多

827
00:32:07,080 --> 00:32:08,700
North Vegeta 已经

828
00:32:08,700 --> 00:32:11,220
用深度神经网络做了很多主动推理 一种

829
00:32:11,220 --> 00:32:12,720
病毒发现我们，就像很多

830
00:32:12,720 --> 00:32:13,919
人实际上在尝试应用

831
00:32:13,919 --> 00:32:15,480
主动推理来保持神经网络一样，

832
00:32:15,480 --> 00:32:17,460
他们基本上必须想出一些方法

833
00:32:17,460 --> 00:32:18,960
来计算预期的自由能

834
00:32:18,960 --> 00:32:21,240
，这与

835
00:32:21,240 --> 00:32:23,279
在 pi MVP 中所做的不同，因为在 Pine BP 中 你

836
00:32:23,279 --> 00:32:25,200
可以准确地计算出预期的自由

837
00:32:25,200 --> 00:32:27,360
能，它最终只是

838
00:32:27,360 --> 00:32:29,159
一堆矩阵向量乘积，然后是

839
00:32:29,159 --> 00:32:30,179
求和，

840
00:32:30,179 --> 00:32:31,919
所以当你使用更复杂的分布时它会变得更加困难，

841
00:32:31,919 --> 00:32:34,020


842
00:32:34,020 --> 00:32:36,480
但这绝不是

843
00:32:36,480 --> 00:32:37,100


844
00:32:37,100 --> 00:32:39,539
不可能的，你必须得

845
00:32:39,539 --> 00:32:40,980
来 有一些近似值，

846
00:32:40,980 --> 00:32:43,380
但我认为你可以通过一些方法来

847
00:32:43,380 --> 00:32:45,899
解决这个问题，这涉及我所说的

848
00:32:45,899 --> 00:32:48,539
人们一直称之为混合

849
00:32:48,539 --> 00:32:50,159
模型的东西，你可能有深度神经

850
00:32:50,159 --> 00:32:53,100
网络 f 在

851
00:32:53,100 --> 00:32:54,960
一些更高的层进入

852
00:32:54,960 --> 00:32:56,640
palm DP，然后像离散分类空间一样进入 DP 空间，

853
00:32:56,640 --> 00:32:58,260
你仍然可以计算

854
00:32:58,260 --> 00:33:00,059
预期的自由能和自由能，

855
00:33:00,059 --> 00:33:03,000
但你仍然可以利用

856
00:33:03,000 --> 00:33:04,620
低层次的优势，如高维

857
00:33:04,620 --> 00:33:07,260
神经网络来投影你的数据

858
00:33:07,260 --> 00:33:09,720
进入这个低维空间，这样你

859
00:33:09,720 --> 00:33:11,820
就可以在这个

860
00:33:11,820 --> 00:33:13,620
低维的 Palm DP 空间中进行所有有效的主动推理，在这个空间中

861
00:33:13,620 --> 00:33:15,600
一切都是准确的，但你仍然

862
00:33:15,600 --> 00:33:16,740
可以利用神经网络良好的

863
00:33:16,740 --> 00:33:19,140
降维和特征

864
00:33:19,140 --> 00:33:20,880
提取特性

865
00:33:20,880 --> 00:33:22,500
来首先像预处理

866
00:33:22,500 --> 00:33:24,419
观察那样 这是我将

867
00:33:24,419 --> 00:33:26,340
在最后讨论的内容，因为我们在实现

868
00:33:26,340 --> 00:33:28,679
这一目标的第一步上取得了一些

869
00:33:28,679 --> 00:33:30,600
进展，这涉及

870
00:33:30,600 --> 00:33:32,580
基本上使所有 Pi mdp Auto 可

871
00:33:32,580 --> 00:33:34,080
微分，因此您可以喜欢训练

872
00:33:34,080 --> 00:33:36,120
连接到

873
00:33:36,120 --> 00:33:38,700
抽取素数 T 的神经网络 模型

874
00:33:38,700 --> 00:33:41,640
是的，这是一个很好的问题，虽然

875
00:33:41,640 --> 00:33:45,059
很棒我认为这就像

876
00:33:45,059 --> 00:33:48,779
生成模型的身体计划，也许是什么 它

877
00:33:48,779 --> 00:33:51,059
就像今天是明天的天线或

878
00:33:51,059 --> 00:33:53,580
延迟它变得更长或更厚

879
00:33:53,580 --> 00:33:56,159
有很多不同的方法可以换出和

880
00:33:56,159 --> 00:33:58,440
组成模型的不同部分所以

881
00:33:58,440 --> 00:34:00,600
谢谢继续

882
00:34:00,600 --> 00:34:01,860
酷是的

883
00:34:01,860 --> 00:34:03,059


884
00:34:03,059 --> 00:34:04,980
嗯所以是的现在让我们

885
00:34:04,980 --> 00:34:07,260
谈谈传统软件

886
00:34:07,260 --> 00:34:08,820
主动推理研究，我敢肯定

887
00:34:08,820 --> 00:34:10,020
现在通话中的每个人都

888
00:34:10,020 --> 00:34:11,820
熟悉它最初的

889
00:34:11,820 --> 00:34:15,540
完成方式基本上是卡尔弗里斯顿和

890
00:34:15,540 --> 00:34:17,760
其他一些人写了一堆 Matlab

891
00:34:17,760 --> 00:34:20,879
脚本，SPM 包的一部分

892
00:34:20,879 --> 00:34:22,440
最初只是为了

893
00:34:22,440 --> 00:34:24,780
神经成像 数据分析和

894
00:34:24,780 --> 00:34:26,940
统计测试

895
00:34:26,940 --> 00:34:29,159
，还有一个称为 Dem 或

896
00:34:29,159 --> 00:34:31,139
动态期望最大化的子包，

897
00:34:31,139 --> 00:34:33,000
它不仅用于主动推理，

898
00:34:33,000 --> 00:34:34,859
包括连续主动推理，

899
00:34:34,859 --> 00:34:36,599
而且就像广义过滤

900
00:34:36,599 --> 00:34:39,300
和从经验数据拟合非线性状态空间

901
00:34:39,300 --> 00:34:41,879
模型一样，

902
00:34:41,879 --> 00:34:43,679
所以我们在其中 有所有这种

903
00:34:43,679 --> 00:34:46,800
离散主动推理工具箱

904
00:34:46,800 --> 00:34:48,599
，其中大部分功能

905
00:34:48,599 --> 00:34:52,199
以 SPM 下划线 Mt 为前缀 mdp 所以基本上完​​成

906
00:34:52,199 --> 00:34:53,879


907
00:34:53,879 --> 00:34:56,399
Pine MVP 所做的大部分事情的主要

908
00:34:56,399 --> 00:34:58,740
功能是包含一个名为 SPM

909
00:34:58,740 --> 00:35:02,339
mdp vbx 的功能，并且有点

910
00:35:02,339 --> 00:35:04,080
像我们经常

911
00:35:04,080 --> 00:35:06,420
开玩笑说 Pine DP 只是一个

912
00:35:06,420 --> 00:35:07,980
包 实现了那个函数，

913
00:35:07,980 --> 00:35:09,599
因为那个函数本质上是

914
00:35:09,599 --> 00:35:12,420
做主动推理和学习的，

915
00:35:12,420 --> 00:35:14,460
所有的消息都在一次

916
00:35:14,460 --> 00:35:16,440
调用中传递，这在很多方面都非常好，

917
00:35:16,440 --> 00:35:18,839
因为你只需要传递一个 mdp

918
00:35:18,839 --> 00:35:21,000
给它，然后它以一种 Black  Box

919
00:35:21,000 --> 00:35:23,339
way 为您提供了所有信念更新

920
00:35:23,339 --> 00:35:25,140
和动作历史以及

921
00:35:25,140 --> 00:35:28,079
您需要的

922
00:35:28,079 --> 00:35:30,599
一切，尽管它很优雅，

923
00:35:30,599 --> 00:35:33,000
但它很有用，而且它实际上非常强大，

924
00:35:33,000 --> 00:35:35,760
因为您可以

925
00:35:35,760 --> 00:35:37,260
使用任何生成模型传递任何代理，而且

926
00:35:37,260 --> 00:35:38,579
它会很漂亮 很多

927
00:35:38,579 --> 00:35:40,859
工作问题是因为它只是一个单一的

928
00:35:40,859 --> 00:35:42,960
功能，它不是很模块化，所以

929
00:35:42,960 --> 00:35:45,300
很难灵活地

930
00:35:45,300 --> 00:35:47,220
组成一个主动推理

931
00:35:47,220 --> 00:35:49,200
过程的不同子计算，所以如果你想做一些

932
00:35:49,200 --> 00:35:50,700
定制

933
00:35:50,700 --> 00:35:53,280
主动推理应用程序，就像

934
00:35:53,280 --> 00:35:55,859
哦，在我进入并进行隐藏状态

935
00:35:55,859 --> 00:35:57,300
推理之前，我想更新

936
00:35:57,300 --> 00:35:58,859
矩阵的参数，我想以

937
00:35:58,859 --> 00:36:01,260
这种特殊的方式

938
00:36:01,260 --> 00:36:05,119
来做，在 SPM mdp vbx 中很难做到这一点，所以

939
00:36:05,119 --> 00:36:07,380
我经常看到 发生了，

940
00:36:07,380 --> 00:36:09,720
我在我的硕士研究中自己做了这件事，我

941
00:36:09,720 --> 00:36:11,220
最终得到

942
00:36:11,220 --> 00:36:13,140
了这个函数的五个不同版本，它们有一些

943
00:36:13,140 --> 00:36:15,960
像专家前缀的

944
00:36:15,960 --> 00:36:18,480
东西，用于我正在做的特定事情，然后

945
00:36:18,480 --> 00:36:21,119
最终就像呃，

946
00:36:21,119 --> 00:36:22,500
不是 高效，因为你正在

947
00:36:22,500 --> 00:36:24,060
创建一堆样板代码，

948
00:36:24,060 --> 00:36:26,400
然后你有一个函数可以执行你想在你的项目中探索的

949
00:36:26,400 --> 00:36:27,900
特定小版本的主动

950
00:36:27,900 --> 00:36:29,460
推理，

951
00:36:29,460 --> 00:36:32,220
所以

952
00:36:32,220 --> 00:36:34,680
即使在 Matlab 中，

953
00:36:34,680 --> 00:36:36,839
你也可以通过模块化这个函数来做到这一点，呃 基本上

954
00:36:36,839 --> 00:36:38,579
使事情变得更加灵活并

955
00:36:38,579 --> 00:36:41,099
节省大量复制和粘贴

956
00:36:41,099 --> 00:36:43,280
代码

957
00:36:43,500 --> 00:36:45,420
嗯另一个问题是推理和

958
00:36:45,420 --> 00:36:46,980
策略选择是固定的所以你

959
00:36:46,980 --> 00:36:48,839
不能比较 e 并对比不同的

960
00:36:48,839 --> 00:36:51,060
消息传递方法和操作

961
00:36:51,060 --> 00:36:52,740
选择例程，因此只有一种

962
00:36:52,740 --> 00:36:56,099
方法在 SPM vbx 中完成，称为

963
00:36:56,099 --> 00:36:58,440
边际消息传递

964
00:36:58,440 --> 00:37:01,079
嗯，所以很难比较它，

965
00:37:01,079 --> 00:37:02,339
就像如果你改变消息

966
00:37:02,339 --> 00:37:05,700
传递算法而在 uh Pi MVP 中一样

967
00:37:05,700 --> 00:37:07,800
我们现在有两个消息传递

968
00:37:07,800 --> 00:37:09,599
算法，你可以

969
00:37:09,599 --> 00:37:12,000
并排比较我的意思是我们希望在未来添加更多

970
00:37:12,000 --> 00:37:13,680
这很容易，因为

971
00:37:13,680 --> 00:37:16,920
它只是一个模块化的东西，然后

972
00:37:16,920 --> 00:37:18,119
因为我们在开始时谈论的内容

973
00:37:18,119 --> 00:37:21,000
仅仅因为

974
00:37:21,000 --> 00:37:22,859
它在 Matlab 中，就很难

975
00:37:22,859 --> 00:37:25,140
与其他框架进行合成，比如在

976
00:37:25,140 --> 00:37:27,839
强化学习开放 AI 健身房和

977
00:37:27,839 --> 00:37:29,099
深度神经网络，如

978
00:37:29,099 --> 00:37:32,339
intensorflow 流或 Pi torture Jacks，

979
00:37:32,339 --> 00:37:34,380
所以它在 Matlab 中的事实

980
00:37:34,380 --> 00:37:36,660
已经受到限制，当然人们 已经

981
00:37:36,660 --> 00:37:39,420
找到了像跨语言

982
00:37:39,420 --> 00:37:41,700
跨平台

983
00:37:41,700 --> 00:37:44,400
um 代码那样做的方法，比如从 Julia 到 Matlab

984
00:37:44,400 --> 00:37:46,980
再到 Python 再回来，但它只是

985
00:37:46,980 --> 00:37:49,440
要重得多 li 参与其中

986
00:37:49,440 --> 00:37:52,440
然后我将列出一些

987
00:37:52,440 --> 00:37:55,619
Matlab 和 Python 的优点和缺点所以

988
00:37:55,619 --> 00:37:57,420
我喜欢 Matlab 的一件事

989
00:37:57,420 --> 00:37:59,339
是它很容易开始

990
00:37:59,339 --> 00:38:01,680
数组编程有一个很好的编辑器

991
00:38:01,680 --> 00:38:04,200
有很多 使用和历史以及

992
00:38:04,200 --> 00:38:06,839
计算神经科学，就像去年

993
00:38:06,839 --> 00:38:09,720
我在计算精神病学课程上讲这个教程一样，

994
00:38:09,720 --> 00:38:11,400


995
00:38:11,400 --> 00:38:13,740
我认为大多数教程

996
00:38:13,740 --> 00:38:16,260
仍在使用 Matlab，所以仍然

997
00:38:16,260 --> 00:38:18,420
有充分的理由使用 Matlab，

998
00:38:18,420 --> 00:38:20,700
因为历史和

999
00:38:20,700 --> 00:38:22,440
非常适合做

1000
00:38:22,440 --> 00:38:24,780
神经科学和心理物理学的东西的包的数量所以

1001
00:38:24,780 --> 00:38:26,280
有这种优势就像

1002
00:38:26,280 --> 00:38:30,000
一种几乎势头或遗留优势

1003
00:38:30,000 --> 00:38:32,099
但当然有像专有的问题

1004
00:38:32,099 --> 00:38:33,480
我认为这是

1005
00:38:33,480 --> 00:38:35,520
你需要支付的

1006
00:38:35,520 --> 00:38:38,339
最大的一种方式或 另一个使用 Matlab

1007
00:38:38,339 --> 00:38:40,800
并且没有那么多社区驱动的

1008
00:38:40,800 --> 00:38:43,020
开发我的意思是有一些

1009
00:38:43,020 --> 00:38:44,880
像文件交换但它

1010
00:38:44,880 --> 00:38:47,400
与 Python 和 d 然后

1011
00:38:47,400 --> 00:38:49,740
python 可以与 Matlab 竞争，

1012
00:38:49,740 --> 00:38:51,420
因为它具有

1013
00:38:51,420 --> 00:38:53,700
numpy 形式的数组编程，当然它是开源的，

1014
00:38:53,700 --> 00:38:55,680
它已

1015
00:38:55,680 --> 00:38:57,660
被广泛采用，

1016
00:38:57,660 --> 00:39:00,960
不仅是学术领域，还有

1017
00:39:00,960 --> 00:39:02,940
许多商业应用，并且有

1018
00:39:02,940 --> 00:39:05,099
很多社区提供驱动

1019
00:39:05,099 --> 00:39:08,940
以软件包的形式进行开发，例如 Pi VP

1020
00:39:08,940 --> 00:39:11,280
um 但一个缺点是至少对我来说，

1021
00:39:11,280 --> 00:39:12,839
开始

1022
00:39:12,839 --> 00:39:14,520
使用 Matlab

1023
00:39:14,520 --> 00:39:17,460
um 不像使用 Matlab 那样容易，因为您

1024
00:39:17,460 --> 00:39:19,079
经常需要安装一堆

1025
00:39:19,079 --> 00:39:20,280
东西并学习 关于虚拟

1026
00:39:20,280 --> 00:39:21,540
环境，在开始使用 Python 之前，你必须喜欢

1027
00:39:21,540 --> 00:39:24,440
学习更多的编程知识

1028
00:39:24,440 --> 00:39:27,060
，

1029
00:39:27,060 --> 00:39:29,040
所以我认为这是

1030
00:39:29,040 --> 00:39:30,780
Matlab 实际上仍然作为一种

1031
00:39:30,780 --> 00:39:32,520
教学工具有用的原因之一，比如

1032
00:39:32,520 --> 00:39:33,960
从获得 Matlab 到实际

1033
00:39:33,960 --> 00:39:36,960
进行编程之间的时间 它很短

1034
00:39:36,960 --> 00:39:39,660
，很好，

1035
00:39:39,660 --> 00:39:43,440
所以是的，这个包被称为呃，它

1036
00:39:43,440 --> 00:39:46,020
在这个活跃的 GitHub

1037
00:39:46,020 --> 00:39:48,839
组织中，它被称为 Pi MVP，你

1038
00:39:48,839 --> 00:39:50,700
可以 pip insta 它会像在

1039
00:39:50,700 --> 00:39:52,380
虚拟环境中或仅在 python 的基本

1040
00:39:52,380 --> 00:39:55,140
安装中一样

1041
00:39:55,140 --> 00:39:57,540
，然后一旦安装了 piondp，

1042
00:39:57,540 --> 00:40:00,720
您就可以

1043
00:40:00,720 --> 00:40:03,599
以灵活的模块化方式导入和使用所有

1044
00:40:03,599 --> 00:40:05,640
不同的子包或子模块，

1045
00:40:05,640 --> 00:40:07,560
例如代理模块，基本上

1046
00:40:07,560 --> 00:40:09,660
只是实现代理类，然后

1047
00:40:09,660 --> 00:40:11,579
有不同的模块，例如推理

1048
00:40:11,579 --> 00:40:13,619
和控制以及学习，它们

1049
00:40:13,619 --> 00:40:15,119
实际上独立于主动

1050
00:40:15,119 --> 00:40:16,440
推理代理，您可以只使用

1051
00:40:16,440 --> 00:40:19,200
它们为隐藏状态推理进行消息传递

1052
00:40:19,200 --> 00:40:21,119
或计算

1053
00:40:21,119 --> 00:40:22,320


1054
00:40:22,320 --> 00:40:24,839
可能策略的预期自由能量或 计算

1055
00:40:24,839 --> 00:40:27,119
对参数的更新，这样所有这些

1056
00:40:27,119 --> 00:40:29,640
东西都可以组合起来，并且可以灵活

1057
00:40:29,640 --> 00:40:31,800
地使用你可以

1058
00:40:31,800 --> 00:40:33,839
通过以你想要的定制方式组合所有这些东西来制作一种弗兰肯斯坦主动推理代理

1059
00:40:33,839 --> 00:40:36,060


1060
00:40:36,060 --> 00:40:38,099


1061
00:40:38,099 --> 00:40:41,160
，其中很多是 pione P 的主要工作流程

1062
00:40:41,160 --> 00:40:43,260
细化到指定

1063
00:40:43,260 --> 00:40:45,060
生成模型和这些

1064
00:40:45,060 --> 00:40:47,640
离散数组的形式，然后将它们

1065
00:40:47,640 --> 00:40:49,680
插入代理的大脑中，

1066
00:40:49,680 --> 00:40:52,020
实例化一个代理，这样它

1067
00:40:52,020 --> 00:40:54,119
几乎被底部的这两行封装

1068
00:40:54,119 --> 00:40:57,000
从 Pine BP 导入代理

1069
00:40:57,000 --> 00:40:59,400
，然后你只需通过

1070
00:40:59,400 --> 00:41:02,220
插入 ABC 和 D 来构建一个代理对象，然后你就有了

1071
00:41:02,220 --> 00:41:04,619
这个代理对象，你可以用它来做

1072
00:41:04,619 --> 00:41:06,420
隐藏状态 通过

1073
00:41:06,420 --> 00:41:08,099
类似于第一个状态

1074
00:41:08,099 --> 00:41:10,560
推断策略的方法进行推理，这是代理在

1075
00:41:10,560 --> 00:41:12,240
内部计算其策略的预期自由

1076
00:41:12,240 --> 00:41:14,760
能，然后您

1077
00:41:14,760 --> 00:41:17,400
最终可以对动作进行采样，因此这

1078
00:41:17,400 --> 00:41:19,260
三行是

1079
00:41:19,260 --> 00:41:21,359
任何主动推理循环

1080
00:41:21,359 --> 00:41:24,660
状态推理策略推理的主要参与者的类型

1081
00:41:24,660 --> 00:41:26,820
然后是动作选择，你

1082
00:41:26,820 --> 00:41:30,320
只是把那些随着时间的推移

1083
00:41:30,359 --> 00:41:32,339
循环起来，以实例化一个活跃的推理

1084
00:41:32,339 --> 00:41:35,460
过程，以及

1085
00:41:35,460 --> 00:41:37,079
动作感知

1086
00:41:37,079 --> 00:41:39,300
循环的循环与代理的输入一起发挥

1087
00:41:39,300 --> 00:41:41,880
作用，这是它的观察 以及

1088
00:41:41,880 --> 00:41:43,500
它的动作选择的输出，

1089
00:41:43,500 --> 00:41:45,960
其中图表是它的动作，

1090
00:41:45,960 --> 00:41:48,000
当然你需要使用 Last Action

1091
00:41:48,000 --> 00:41:50,640
来获得新的观察结果，然后

1092
00:41:50,640 --> 00:41:53,520
你是不是通过将动作插入

1093
00:41:53,520 --> 00:41:55,980
某个环境来做到这一点，这在

1094
00:41:55,980 --> 00:41:57,480
主动推理文献中也经常

1095
00:41:57,480 --> 00:41:59,400
被称为生成过程，所以

1096
00:41:59,400 --> 00:42:00,960
外面的现实世界会

1097
00:42:00,960 --> 00:42:03,119
生成你的数据，所以这就是你将看到的

1098
00:42:03,119 --> 00:42:04,680
那种经典动作感知组

1099
00:42:04,680 --> 00:42:06,480
制定了任何主动推理

1100
00:42:06,480 --> 00:42:08,640
代理，这甚至不是

1101
00:42:08,640 --> 00:42:10,320
主动推理所独有的，这就是

1102
00:42:10,320 --> 00:42:12,119
强化学习问题的

1103
00:42:12,119 --> 00:42:14,880
一般框架，就像 Open AI Jam

1104
00:42:14,880 --> 00:42:17,040
使用非常相似的

1105
00:42:17,040 --> 00:42:20,339
控制流程，我们有点像

1106
00:42:20,339 --> 00:42:24,020
嗯是的感觉电机循环

1107
00:42:24,440 --> 00:42:27,359
所以是的 这是一个例子，

1108
00:42:27,359 --> 00:42:30,359
嗯，就像导入代理一样

1109
00:42:30,359 --> 00:42:31,980
设置生成模型，这是

1110
00:42:31,980 --> 00:42:33,720
最难的部分，所以我有点方便地

1111
00:42:33,720 --> 00:42:36,420
将省略号放在点之后，

1112
00:42:36,420 --> 00:42:37,500
但实际上大部分

1113
00:42:37,500 --> 00:42:38,640
代码将要发生的地方是构建

1114
00:42:38,640 --> 00:42:41,220
生成模型 构建代理你

1115
00:42:41,220 --> 00:42:43,020
构建一些环境，你

1116
00:42:43,020 --> 00:42:45,599
可以将其作为存储的 Pi mdp 环境之一导入，

1117
00:42:45,599 --> 00:42:47,820
或者你可以

1118
00:42:47,820 --> 00:42:50,040
从开放的 AI 健身房获取它，或者你可以 d 只是

1119
00:42:50,040 --> 00:42:51,660
创建你自己的环境，所以

1120
00:42:51,660 --> 00:42:53,940
这些将是你的代码，它实际上

1121
00:42:53,940 --> 00:42:55,920
描述了世界是如何运作

1122
00:42:55,920 --> 00:42:57,900
的，代理正在与之交互的世界

1123
00:42:57,900 --> 00:43:01,319
，然后你可以

1124
00:43:01,319 --> 00:43:02,940
用那几行代码实现主动推理的时间步骤

1125
00:43:02,940 --> 00:43:05,339
，并且

1126
00:43:05,339 --> 00:43:07,740
这些都是最后几行，比如 8

1127
00:43:07,740 --> 00:43:09,240
到 15 那些会

1128
00:43:09,240 --> 00:43:10,560


1129
00:43:10,560 --> 00:43:14,000
随着时间的推移被包裹在一个循环中

1130
00:43:14,640 --> 00:43:16,500
嗯是的这只是更多这样的

1131
00:43:16,500 --> 00:43:19,260
例子这将是一个快速的方法在

1132
00:43:19,260 --> 00:43:20,880
这个例子中我们甚至没有运行

1133
00:43:20,880 --> 00:43:22,920
主动 推理，但我们只是使用

1134
00:43:22,920 --> 00:43:25,079


1135
00:43:25,079 --> 00:43:28,380
Algos 子模块中的一种消息传递算法来进行隐藏

1136
00:43:28,380 --> 00:43:31,020
状态推理，所以在这种情况下，我只是

1137
00:43:31,020 --> 00:43:33,240
创建了一个随机矩阵，我创建了一个

1138
00:43:33,240 --> 00:43:36,660
随机观察，我给出

1139
00:43:36,660 --> 00:43:39,240
了一个随机先验和 然后我可以

1140
00:43:39,240 --> 00:43:42,420
做一个类似虚构更新的代理人

1141
00:43:42,420 --> 00:43:43,619
在隐藏状态推理期间可能正在做的事情

1142
00:43:43,619 --> 00:43:47,099
并优化 Qs 或

1143
00:43:47,099 --> 00:43:49,380
关于隐藏状态的信念所以这是一个

1144
00:43:49,380 --> 00:43:50,520
你如何实际使用

1145
00:43:50,520 --> 00:43:53,160
Pond DP 到 j 的例子 只需对隐马尔可夫模型进行通用推理，

1146
00:43:53,160 --> 00:43:55,380
您甚至不需要

1147
00:43:55,380 --> 00:43:58,020
在主动推理

1148
00:43:58,020 --> 00:44:00,900
循环中使用它，您可以使用它来使用我们

1149
00:44:00,900 --> 00:44:02,520


1150
00:44:02,520 --> 00:44:04,440
提供的特定算法对隐马尔可夫模型进行统计推理

1151
00:44:04,440 --> 00:44:06,720


1152
00:44:06,720 --> 00:44:08,280
，然后 当然，

1153
00:44:08,280 --> 00:44:10,560
我在 SPM 上谈到的优势之一是您

1154
00:44:10,560 --> 00:44:12,180
可以创建自定义的主动推理

1155
00:44:12,180 --> 00:44:14,400
过程，代理类有很多类似的额外

1156
00:44:14,400 --> 00:44:16,200
参数，您

1157
00:44:16,200 --> 00:44:18,300
可以在其中打开和关闭

1158
00:44:18,300 --> 00:44:21,000
奖励功能的不同部分或

1159
00:44:21,000 --> 00:44:23,819
预期的免费 agent 的能量，所以

1160
00:44:23,819 --> 00:44:26,339
例如在这个 agent 中，agent

1161
00:44:26,339 --> 00:44:28,619
没有包含 uh expected utility

1162
00:44:28,619 --> 00:44:30,540
，这是一种奖励

1163
00:44:30,540 --> 00:44:32,819
C Vector 驱动的动作选择组件，

1164
00:44:32,819 --> 00:44:35,240
但 agent 确实使用状态

1165
00:44:35,240 --> 00:44:37,800
信息增益和参数信息游戏

1166
00:44:37,800 --> 00:44:39,480
，这是

1167
00:44:39,480 --> 00:44:42,119
预期的自由能，所以有很多

1168
00:44:42,119 --> 00:44:43,500
方法可以创建一个

1169
00:44:43,500 --> 00:44:45,619
定制的主动推理代理，

1170
00:44:45,619 --> 00:44:47,940
它不会以不同的方式表现

1171
00:44:47,940 --> 00:44:49,800
取决于 在

1172
00:44:49,800 --> 00:44:53,300
你提供的这些呃关键字参数上，

1173
00:44:54,060 --> 00:44:55,500
嗯，

1174
00:44:55,500 --> 00:44:57,660
是的，这里只是更多的例子，比如在

1175
00:44:57,660 --> 00:44:58,920
这种情况下，我们使用主动推理

1176
00:44:58,920 --> 00:45:00,780
代理只是为了进行隐藏状态推理

1177
00:45:00,780 --> 00:45:03,060
，根本没有任何动作，所以代理

1178
00:45:03,060 --> 00:45:04,980
只是在推断它正在

1179
00:45:04,980 --> 00:45:07,200
更新的隐藏状态 它对 a 矩阵的

1180
00:45:07,200 --> 00:45:08,700
信念，它正在更新它对

1181
00:45:08,700 --> 00:45:11,099
D 向量或初始隐藏状态的信念

1182
00:45:11,099 --> 00:45:12,960
，所以你可以知道，只要灵活地

1183
00:45:12,960 --> 00:45:14,640
把所有这些线放在一起

1184
00:45:14,640 --> 00:45:16,380
，就可以创建一个代理，它可以做任何你想做的事

1185
00:45:16,380 --> 00:45:17,940
，甚至不需要行动 在

1186
00:45:17,940 --> 00:45:19,940
世界上技术上

1187
00:45:19,940 --> 00:45:22,140
还可以，所以现在我要展示几个

1188
00:45:22,140 --> 00:45:25,859
pine DP 的例子，

1189
00:45:25,859 --> 00:45:28,800
嗯，这是一篇经典的

1190
00:45:28,800 --> 00:45:31,020
主动推理论文，它是我

1191
00:45:31,020 --> 00:45:33,660
最喜欢的论文之一，嗯，我认为它

1192
00:45:33,660 --> 00:45:35,160
被称为场景构建和主动

1193
00:45:35,160 --> 00:45:37,319
推理 它描述了代理人必须完成的任务

1194
00:45:37,319 --> 00:45:39,839
，这被

1195
00:45:39,839 --> 00:45:41,640
用来在心理物理学测试中实际模拟人类数据，

1196
00:45:41,640 --> 00:45:44,160
代理人必须偶然地注视

1197
00:45:44,160 --> 00:45:45,960


1198
00:45:45,960 --> 00:45:48,780
发现四个象限中的两个

1199
00:45:48,780 --> 00:45:51,359
其中有特定的图像，

1200
00:45:51,359 --> 00:45:53,579
嗯，总共有四个象限，其中

1201
00:45:53,579 --> 00:45:56,579
两个有

1202
00:45:56,579 --> 00:45:59,460
感兴趣的图像，如果

1203
00:45:59,460 --> 00:46:02,640
在这种情况下的两个图像是一只鸟和一只猫的图像

1204
00:46:02,640 --> 00:46:05,579
，那是跳蚤场景的一个例子，

1205
00:46:05,579 --> 00:46:07,380
所以亚洲人基本上或 人类必须

1206
00:46:07,380 --> 00:46:09,960
对潜在场景进行分类，该场景

1207
00:46:09,960 --> 00:46:12,300
简单地由两个

1208
00:46:12,300 --> 00:46:15,540
图像的组合按顺序定义，然后对场景进行分类，

1209
00:46:15,540 --> 00:46:16,859
因此它基本上是一个

1210
00:46:16,859 --> 00:46:20,040
分类任务，但代理需要

1211
00:46:20,040 --> 00:46:22,740


1212
00:46:22,740 --> 00:46:24,599
在知道该类别之前偶然发现一系列线索

1213
00:46:24,599 --> 00:46:27,720
是或者那个场景是什么所以它

1214
00:46:27,720 --> 00:46:29,940
结合了

1215
00:46:29,940 --> 00:46:31,380
我们都知道和喜欢的关于主动

1216
00:46:31,380 --> 00:46:33,660
推理的认知成分试图

1217
00:46:33,660 --> 00:46:36,180
通过主动采样来揭示世界的隐藏状态

1218
00:46:36,180 --> 00:46:38,400
所以在这种情况下他们

1219
00:46:38,400 --> 00:46:39,720
通过将他们的眼睛移到不同的地方来采样世界

1220
00:46:39,720 --> 00:46:41,760
象限以揭示

1221
00:46:41,760 --> 00:46:44,760
它们背后的内容，然后

1222
00:46:44,760 --> 00:46:47,280
根据

1223
00:46:47,280 --> 00:46:49,020
它对世界的了解来实际选择真正的类别，这就是它所在

1224
00:46:49,020 --> 00:46:50,160
的位置 试图最大化

1225
00:46:50,160 --> 00:46:51,780
效用，因为有一些

1226
00:46:51,780 --> 00:46:55,140
与正确分类相关的奖励，

1227
00:46:55,140 --> 00:46:57,180
所以这是一个

1228
00:46:57,180 --> 00:46:58,740
最初在 Matlab 中完成的例子，我刚刚

1229
00:46:58,740 --> 00:47:01,020
在 pi mdp 中重新实现

1230
00:47:01,020 --> 00:47:03,300
，这是另一个例子，

1231
00:47:03,300 --> 00:47:05,400
现在场景是

1232
00:47:05,400 --> 00:47:07,800
um feed 场景所以

1233
00:47:07,800 --> 00:47:09,780
在此示例中，提示位于下两个象限，

1234
00:47:09,780 --> 00:47:12,359
因此代理必须环顾

1235
00:47:12,359 --> 00:47:14,099
不同的象限，它最终看到

1236
00:47:14,099 --> 00:47:15,839
右下角有一只鸟，

1237
00:47:15,839 --> 00:47:17,700
左下角的种子，所以这

1238
00:47:17,700 --> 00:47:20,880
一定是饲料场景，

1239
00:47:20,880 --> 00:47:22,980
嗯，只是 呃展示一个例子，

1240
00:47:22,980 --> 00:47:24,359
说明在主动推理中实际是什么样子的，比如

1241
00:47:24,359 --> 00:47:25,740
代码是什么

1242
00:47:25,740 --> 00:47:27,780
样子的，所以

1243
00:47:27,780 --> 00:47:29,160
你要做的第一件事

1244
00:47:29,160 --> 00:47:31,140
就是设置你的代理，然后再把它

1245
00:47:31,140 --> 00:47:33,720
扔进 a b c'  s 和 D 在这种

1246
00:47:33,720 --> 00:47:35,760
情况下，我使用了

1247
00:47:35,760 --> 00:47:36,960
一种称为边际消息

1248
00:47:36,960 --> 00:47:38,640
传递的特定消息传递算法，它与 Matlab 中使用的算法相同，

1249
00:47:38,640 --> 00:47:42,180
你设置了策略深度和

1250
00:47:42,180 --> 00:47:43,680
推理 Horizo​​n，有点

1251
00:47:43,680 --> 00:47:45,420
像 记忆，比如你考虑了多少过去的

1252
00:47:45,420 --> 00:47:48,119
观察

1253
00:47:48,119 --> 00:47:50,700
，然后你设置了一个

1254
00:47:50,700 --> 00:47:52,980
环境，就像

1255
00:47:52,980 --> 00:47:55,500
代理将与之交互的外部世界，

1256
00:47:55,500 --> 00:47:57,420
在这种情况下，我称之为场景

1257
00:47:57,420 --> 00:47:59,520
构建环境，

1258
00:47:59,520 --> 00:48:01,920
一旦 agent 将眼睛

1259
00:48:01,920 --> 00:48:03,300
移到某个地方，这

1260
00:48:03,300 --> 00:48:06,180
实际上是如何确定 agent

1261
00:48:06,180 --> 00:48:07,980
接下来看到的是什么，你

1262
00:48:07,980 --> 00:48:09,720
知道它决定看的象限后面的任何东西

1263
00:48:09,720 --> 00:48:13,200
，所以这是主要的

1264
00:48:13,200 --> 00:48:14,819
两件事，它们是动作的两个方面

1265
00:48:14,819 --> 00:48:16,619
perception 循环代理

1266
00:48:16,619 --> 00:48:18,780
和环境

1267
00:48:18,780 --> 00:48:21,900
，然后通常要做的是

1268
00:48:21,900 --> 00:48:24,300


1269
00:48:24,300 --> 00:48:26,880
通过重置环境来进行初始初步观察，这是

1270
00:48:26,880 --> 00:48:29,760
从 openai gym 借来的惯例，你

1271
00:48:29,760 --> 00:48:32,220
基本上做环境点重置，

1272
00:48:32,220 --> 00:48:33,960
就像吐出环境的方法

1273
00:48:33,960 --> 00:48:36,240
出最初的观察结果，

1274
00:48:36,240 --> 00:48:37,619
然后在这些事情

1275
00:48:37,619 --> 00:48:39,359
中创建

1276
00:48:39,359 --> 00:48:41,819
在观察索引之间具有映射关系的列表或字典通常很有用

1277
00:48:41,819 --> 00:48:43,800
这就像

1278
00:48:43,800 --> 00:48:46,319
零之间的整数，无论​​有多少

1279
00:48:46,319 --> 00:48:48,420
观察结果，然后是

1280
00:48:48,420 --> 00:48:49,619
那些实际在语义上对应的东西，

1281
00:48:49,619 --> 00:48:51,720
所以这只是一种非常

1282
00:48:51,720 --> 00:48:54,000
常见的方式，因为所有的 Palm

1283
00:48:54,000 --> 00:48:56,700
DP 和环境都会

1284
00:48:56,700 --> 00:48:59,220
吐出，就像两个和零和

1285
00:48:59,220 --> 00:49:00,780
所有这些都像离散索引，但是

1286
00:49:00,780 --> 00:49:02,700
拥有这些列表很有用，您可以使用这些列表

1287
00:49:02,700 --> 00:49:04,980
在语义上将

1288
00:49:04,980 --> 00:49:07,920
特定索引映射到

1289
00:49:07,920 --> 00:49:09,740
有意义的东西，例如

1290
00:49:09,740 --> 00:49:14,160
看到鸟类图像或选择

1291
00:49:14,160 --> 00:49:18,420
第一类与第二类

1292
00:49:18,420 --> 00:49:20,579
，然后一旦完成 你只是

1293
00:49:20,579 --> 00:49:23,400
随着时间的推移编写一个循环，你

1294
00:49:23,400 --> 00:49:25,260
基本上在执行主动推理

1295
00:49:25,260 --> 00:49:27,240
，其中包括隐藏状态

1296
00:49:27,240 --> 00:49:29,460
估计和策略推理

1297
00:49:29,460 --> 00:49:32,700
你呃对一个动作

1298
00:49:32,700 --> 00:49:34,680
进行采样，然后将其反馈到

1299
00:49:34,680 --> 00:49:37,079
环境中以产生另一个

1300
00:49:37,079 --> 00:49:38,819
观察结果，然后随着时间的推移发生这种情况，

1301
00:49:38,819 --> 00:49:39,900
所以这是 整个动作的

1302
00:49:39,900 --> 00:49:41,339
感知，

1303
00:49:41,339 --> 00:49:44,339
所以这是

1304
00:49:44,339 --> 00:49:45,720
嗯，是的

1305
00:49:45,720 --> 00:49:48,119
，

1306
00:49:48,119 --> 00:49:49,920
这看起来很简单，看起来很短，但我有点像 glossi

1307
00:49:49,920 --> 00:49:51,359
讨论我稍后要讨论的事情，

1308
00:49:51,359 --> 00:49:53,880
这是我之前提到的，

1309
00:49:53,880 --> 00:49:55,619
就像这看起来一样简单，最难的部分

1310
00:49:55,619 --> 00:49:57,720
实际上是在任何这种情况发生之前完成的

1311
00:49:57,720 --> 00:49:59,520
，写下 a b

1312
00:49:59,520 --> 00:50:02,760
c 和 D，这是迄今为止最多的时间

1313
00:50:02,760 --> 00:50:05,460


1314
00:50:05,460 --> 00:50:06,839
主动推理的密集和复杂部分实际上是

1315
00:50:06,839 --> 00:50:08,640
写下生成模型，一旦你

1316
00:50:08,640 --> 00:50:10,680
写下了生成模型，

1317
00:50:10,680 --> 00:50:12,780
那么剩下的基本上就像发条一样，你

1318
00:50:12,780 --> 00:50:15,060
只需要将代理与

1319
00:50:15,060 --> 00:50:17,160
环境联系起来，然后像

1320
00:50:17,160 --> 00:50:19,560
五六行一样运行 真正实现

1321
00:50:19,560 --> 00:50:20,819
这个东西，但最困难的部分是

1322
00:50:20,819 --> 00:50:22,560
在开始时写下那些 a b c 和 D

1323
00:50:22,560 --> 00:50:24,799


1324
00:50:25,260 --> 00:50:27,119
嗯我将展示另一个例子，我

1325
00:50:27,119 --> 00:50:28,079
有点

1326
00:50:28,079 --> 00:50:31,740
呃打电话给类固醇的队友所以在

1327
00:50:31,740 --> 00:50:33,960
经典的 teamaze 任务

1328
00:50:33,960 --> 00:50:35,579
中我忘记了原来的内容 论文

1329
00:50:35,579 --> 00:50:36,900
是，但它

1330
00:50:36,900 --> 00:50:37,980
在主动推理

1331
00:50:37,980 --> 00:50:39,599
文献中已经流行了一段时间，

1332
00:50:39,599 --> 00:50:41,460
呃，我认为 Carl 可能

1333
00:50:41,460 --> 00:50:44,700
在 2015 年或更早的时候想出，甚至你有一个

1334
00:50:44,700 --> 00:50:48,119
代理鼠标 必须访问其环境中

1335
00:50:48,119 --> 00:50:51,180
奖励或惩罚的两个潜在来源，

1336
00:50:51,180 --> 00:50:53,160
并且它

1337
00:50:53,160 --> 00:50:55,619
不知道该队友的哪只手臂

1338
00:50:55,619 --> 00:50:58,020
包含奖励，因此它必须先访问一个

1339
00:50:58,020 --> 00:51:00,300
提示，然后才能知道哪只手臂

1340
00:51:00,300 --> 00:51:02,700
有奖励，哪只手臂没有

1341
00:51:02,700 --> 00:51:04,559
奖励或像负面

1342
00:51:04,559 --> 00:51:07,200
刺激一样的震惊我只是在

1343
00:51:07,200 --> 00:51:09,660
空间上扩展了队友所以

1344
00:51:09,660 --> 00:51:12,119
代理现在必须访问一系列

1345
00:51:12,119 --> 00:51:14,819
线索，每个线索都揭示

1346
00:51:14,819 --> 00:51:17,099
了下一个立方体的位置，以便

1347
00:51:17,099 --> 00:51:19,559
找出最终的线索 只是

1348
00:51:19,559 --> 00:51:22,619
奶酪相对于冲击的位置

1349
00:51:22,619 --> 00:51:24,359
所以这是另一个例子，

1350
00:51:24,359 --> 00:51:26,520
代理首先去 q1 然后它

1351
00:51:26,520 --> 00:51:28,200
知道 Q2 在哪里然后它知道

1352
00:51:28,200 --> 00:51:30,300
奶酪在哪里所以我称之为认知

1353
00:51:30,300 --> 00:51:32,099
链因为代理

1354
00:51:32,099 --> 00:51:34,740
实际上没有 计划它的根

1355
00:51:34,740 --> 00:51:37,020
一直到奶酪的最终位置，

1356
00:51:37,020 --> 00:51:38,940
它所要做的就是到达

1357
00:51:38,940 --> 00:51:41,280
下一个 q，然后显示下一个

1358
00:51:41,280 --> 00:51:43,880
提示的位置，最终显示奖励的

1359
00:51:43,880 --> 00:51:47,099
隐藏位置在

1360
00:51:47,099 --> 00:51:49,440
哪里 您是在某种程度上使用

1361
00:51:49,440 --> 00:51:53,040
认知价值或好奇心来允许

1362
00:51:53,040 --> 00:51:56,160
其他时间上浅薄的动物

1363
00:51:56,160 --> 00:51:58,800
计划其方式来

1364
00:51:58,800 --> 00:52:00,839
获得远端奖励或

1365
00:52:00,839 --> 00:52:03,300
无法计划获得的

1366
00:52:03,300 --> 00:52:05,640
东西我们的先验

1367
00:52:05,640 --> 00:52:07,500
，这只是一个

1368
00:52:07,500 --> 00:52:09,059
例子 这看起来就像在 Pine DP

1369
00:52:09,059 --> 00:52:10,500
中，它基本上看起来完全

1370
00:52:10,500 --> 00:52:12,660
一样，只是环境

1371
00:52:12,660 --> 00:52:14,579
和生成模型不同，但

1372
00:52:14,579 --> 00:52:16,859
代码的一般流程总是

1373
00:52:16,859 --> 00:52:21,619
有这种经典的管道，

1374
00:52:21,839 --> 00:52:24,059
嗯，好吧，现在我会得到

1375
00:52:24,059 --> 00:52:25,619
对于最重要的部分，我认为

1376
00:52:25,619 --> 00:52:28,260


1377
00:52:28,260 --> 00:52:29,880
与主动推理混淆的最大来源是

1378
00:52:29,880 --> 00:52:31,500
最难的部分是生成模型

1379
00:52:31,500 --> 00:52:34,319
，所有的

1380
00:52:34,319 --> 00:52:36,599
复杂性都用于编码

1381
00:52:36,599 --> 00:52:38,280
代理对世界的信念，所以

1382
00:52:38,280 --> 00:52:40,079
我该如何写下

1383
00:52:40,079 --> 00:52:41,460


1384
00:52:41,460 --> 00:52:44,099
深度神经

1385
00:52:44,099 --> 00:52:47,960
网络或无监督学习等范式中的 B、C 和 D，

1386
00:52:47,960 --> 00:52:50,280
您不必写下现代

1387
00:52:50,280 --> 00:52:51,740


1388
00:52:51,740 --> 00:52:54,540
神经网络通过

1389
00:52:54,540 --> 00:52:57,000
观察负载和负载来学习模型 o  f 数据，所以

1390
00:52:57,000 --> 00:52:59,040
它的样本效率较低，但你不必一

1391
00:52:59,040 --> 00:53:01,319
开始就编码那么多，所以

1392
00:53:01,319 --> 00:53:03,119
这

1393
00:53:03,119 --> 00:53:06,359
有点符合

1394
00:53:06,359 --> 00:53:08,220
模型 3 和基于模型的

1395
00:53:08,220 --> 00:53:10,140
方法之间更大的鸿沟，

1396
00:53:10,140 --> 00:53:12,619
你是你是 有效地

1397
00:53:12,619 --> 00:53:15,420
向样本发布统计

1398
00:53:15,420 --> 00:53:17,220
复杂性，即必须

1399
00:53:17,220 --> 00:53:19,380
通过将

1400
00:53:19,380 --> 00:53:22,140
一堆非线性函数逼近器粘在一起来写下模型，

1401
00:53:22,140 --> 00:53:24,240
然后

1402
00:53:24,240 --> 00:53:25,619
通过

1403
00:53:25,619 --> 00:53:27,480
用数据轰炸它来学习代理对世界的信念，同样的事情也

1404
00:53:27,480 --> 00:53:29,339
适用 深度强化学习，

1405
00:53:29,339 --> 00:53:32,579
如主动推理中的 DQ 学习，

1406
00:53:32,579 --> 00:53:35,160
代理的样本效率更高

1407
00:53:35,160 --> 00:53:36,300
，因为它们不需要

1408
00:53:36,300 --> 00:53:40,740
训练数十亿个数据向量，但

1409
00:53:40,740 --> 00:53:42,240
另一方面，

1410
00:53:42,240 --> 00:53:44,040
作为建模者，你的投资更多，因为你

1411
00:53:44,040 --> 00:53:45,900
有 明确地写下

1412
00:53:45,900 --> 00:53:48,359
代理人对世界的信念是什么

1413
00:53:48,359 --> 00:53:49,920
你不只是用

1414
00:53:49,920 --> 00:53:53,220
像卷积层这样通用的

1415
00:53:53,220 --> 00:53:55,200
东西和一些评论和东西来装备它 然后让它

1416
00:53:55,200 --> 00:53:57,119
学习你实际上必须手动编写代码

1417
00:53:57,119 --> 00:53:59,400
，所以我认为这是

1418
00:53:59,400 --> 00:54:01,800
基于模型的强化学习之间最大的区别之一

1419
00:54:01,800 --> 00:54:03,059
，你

1420
00:54:03,059 --> 00:54:04,859
实际上编码了世界的贝叶斯一般

1421
00:54:04,859 --> 00:54:07,020
生成模型

1422
00:54:07,020 --> 00:54:08,940
和更多的无模型或数据驱动的

1423
00:54:08,940 --> 00:54:11,099
方法但是 并不是这样的

1424
00:54:11,099 --> 00:54:12,660
二分法，有一些方法可以

1425
00:54:12,660 --> 00:54:14,099
将两者结合起来，

1426
00:54:14,099 --> 00:54:16,980
但只是为了

1427
00:54:16,980 --> 00:54:18,720
非常具体地展示，例如

1428
00:54:18,720 --> 00:54:21,780
在这个场景构建演示中，我

1429
00:54:21,780 --> 00:54:23,460
在几张幻灯片前展示过，如果你

1430
00:54:23,460 --> 00:54:25,940
只看纯粹的线条 代码中

1431
00:54:25,940 --> 00:54:28,680
哪一个占用了更多代码，你可以

1432
00:54:28,680 --> 00:54:31,440
使用代码行数

1433
00:54:31,440 --> 00:54:34,559
作为统计复杂性或

1434
00:54:34,559 --> 00:54:36,660
包含多少信息的代理，因此

1435
00:54:36,660 --> 00:54:38,339
运行主动

1436
00:54:38,339 --> 00:54:40,140
推理循环的模拟本身就像 15 行代码

1437
00:54:40,140 --> 00:54:42,300
，就像和和 他们和代码

1438
00:54:42,300 --> 00:54:44,220
本身已经非常通用，并不

1439
00:54:44,220 --> 00:54:46,319
特定于场景构建演示

1440
00:54:46,319 --> 00:54:48,359
编写生成模型本身

1441
00:54:48,359 --> 00:54:49,920
，这是完成所有繁重工作的地方

1442
00:54:49,920 --> 00:54:51,119
这是所有

1443
00:54:51,119 --> 00:54:53,520
特定于该任务的信息都被

1444
00:54:53,520 --> 00:54:55,980
编码的地方，例如，我只是看看我是

1445
00:54:55,980 --> 00:54:58,260
如何创建矩阵的，

1446
00:54:58,260 --> 00:55:00,540
关于

1447
00:55:00,540 --> 00:55:02,520
场景构建演示的观察映射的信念，这已经是

1448
00:55:02,520 --> 00:55:05,460
比运行

1449
00:55:05,460 --> 00:55:07,859
整个主动推理更多的代码 模拟所以

1450
00:55:07,859 --> 00:55:09,540
就像通过大量

1451
00:55:09,540 --> 00:55:11,220
代码你已经可以告诉哦是

1452
00:55:11,220 --> 00:55:13,200
的有很多假设和

1453
00:55:13,200 --> 00:55:15,119
信息被烘焙到

1454
00:55:15,119 --> 00:55:16,559
生成模型中这就是

1455
00:55:16,559 --> 00:55:18,480
主动推理的大部分繁重工作

1456
00:55:18,480 --> 00:55:21,260
实际上来自

1457
00:55:21,300 --> 00:55:22,559
嗯是的所以我 只是认为

1458
00:55:22,559 --> 00:55:24,300
对此发表评论很重要，因为这是一件

1459
00:55:24,300 --> 00:55:26,640
非常关键的事情，我认为任何

1460
00:55:26,640 --> 00:55:28,440
想开始

1461
00:55:28,440 --> 00:55:30,599
在离散状态

1462
00:55:30,599 --> 00:55:32,220
空间中使用主动推理模型的人

1463
00:55:32,220 --> 00:55:34,559
都应该考虑一下，模型完成了

1464
00:55:34,559 --> 00:55:36,420
大部分工作 对你来说，预期自由

1465
00:55:36,420 --> 00:55:38,040
能是一个非常有趣的

1466
00:55:38,040 --> 00:55:39,660
目标函数，它有很多

1467
00:55:39,660 --> 00:55:41,880
优点，但

1468
00:55:41,880 --> 00:55:44,700
主动推理的大部分功能 开始写下

1469
00:55:44,700 --> 00:55:46,740
你的代理人对世界的信念

1470
00:55:46,740 --> 00:55:48,300
是什么，然后一旦你有了它，

1471
00:55:48,300 --> 00:55:50,520
那么剩下的一切都会

1472
00:55:50,520 --> 00:55:52,980
为你工作，因为 pi NDB 代码

1473
00:55:52,980 --> 00:55:55,440
非常通用，不通用的是

1474
00:55:55,440 --> 00:55:56,880
你如何对关于世界的信念进行编码

1475
00:55:56,880 --> 00:55:59,000
world

1476
00:55:59,400 --> 00:56:01,680
um okay so now I'm kind of finishing

1477
00:56:01,680 --> 00:56:03,960
maybe we should demember on that bit on

1478
00:56:03,960 --> 00:56:05,460
a second anyone have thoughts

1479
00:56:05,460 --> 00:56:08,579
or comments or questions about this

1480
00:56:08,579 --> 00:56:12,359
如果没有，我可以继续完成

1481
00:56:12,359 --> 00:56:16,460
达芙妮或雅各布，或者我会问 一个

1482
00:56:21,300 --> 00:56:23,520
我没有任何问题

1483
00:56:23,520 --> 00:56:25,680
好吧你已经强调

1484
00:56:25,680 --> 00:56:28,079
了代理生成模型的规范

1485
00:56:28,079 --> 00:56:29,099


1486
00:56:29,099 --> 00:56:31,680
以及硬币的另一面

1487
00:56:31,680 --> 00:56:34,740
如何我们如何指定生成过程

1488
00:56:34,740 --> 00:56:37,680
我们如何

1489
00:56:37,680 --> 00:56:40,399
为代理

1490
00:56:40,440 --> 00:56:41,880
指定真正的环境 好问题是的

1491
00:56:41,880 --> 00:56:44,520
基本上我所说的关于

1492
00:56:44,520 --> 00:56:46,680
生成

1493
00:56:46,680 --> 00:56:48,540
模型的所有内容都适用于生成

1494
00:56:48,540 --> 00:56:50,099
过程

1495
00:56:50,099 --> 00:56:53,520
除了代理的有趣行为

1496
00:56:53,520 --> 00:56:55,500
是的我的意思是你可以

1497
00:56:55,500 --> 00:56:57,059
想到生成过程 这也推动了

1498
00:56:57,059 --> 00:56:58,200
很多，

1499
00:56:58,200 --> 00:57:00,059
我猜瓶颈是

1500
00:57:00,059 --> 00:57:01,920
生成模型，因为如果你创建一个

1501
00:57:01,920 --> 00:57:04,140
非常复杂的生成过程，那么一个

1502
00:57:04,140 --> 00:57:05,819
非常复杂的环境具有

1503
00:57:05,819 --> 00:57:08,579
各种奇特的非线性动力学，

1504
00:57:08,579 --> 00:57:10,319
但代理的世界模型是超级

1505
00:57:10,319 --> 00:57:12,420
超级的 简单所以它只是相信

1506
00:57:12,420 --> 00:57:14,400
你知道有一个电灯开关

1507
00:57:14,400 --> 00:57:16,440
是开着还是关着

1508
00:57:16,440 --> 00:57:18,839
那么你可以

1509
00:57:18,839 --> 00:57:22,200
从这样一个简单的代理人那里得到的可能行为受到

1510
00:57:22,200 --> 00:57:23,880
其生成模型的复杂性的限制

1511
00:57:23,880 --> 00:57:28,020
所以一个非常复杂的生成器一个

1512
00:57:28,020 --> 00:57:29,760
非常简单的生成模型将 仍然

1513
00:57:29,760 --> 00:57:33,000
没有表现出非常有趣的行为，

1514
00:57:33,000 --> 00:57:34,619
即使它被嵌入到一个复杂的

1515
00:57:34,619 --> 00:57:37,079
生成过程中，但是

1516
00:57:37,079 --> 00:57:38,700
当你同时

1517
00:57:38,700 --> 00:57:40,500
拥有一个复杂的生成过程

1518
00:57:40,500 --> 00:57:44,040
和一个复杂的生成模型时，最丰富的动态显然会发生所以

1519
00:57:44,040 --> 00:57:46,920
在构建

1520
00:57:46,920 --> 00:57:48,660
生成模型时我会做的所有工作 说的是

1521
00:57:48,660 --> 00:57:50,760
这里的第一行也可以

1522
00:57:50,760 --> 00:57:52,319
通过

1523
00:57:52,319 --> 00:57:54,900
生成生成过程的大量工作来匹配，

1524
00:57:54,900 --> 00:57:56,760
在这种情况下是 是认知网格

1525
00:57:56,760 --> 00:57:58,319
世界环境，它只是一组

1526
00:57:58,319 --> 00:58:00,660
规则，说明当代理位于

1527
00:58:00,660 --> 00:58:03,000
队列位置时向他们显示队列

1528
00:58:03,000 --> 00:58:04,920
身份，就像这样相对

1529
00:58:04,920 --> 00:58:06,359
简单

1530
00:58:06,359 --> 00:58:08,760
但值得思考的一件有趣的事情

1531
00:58:08,760 --> 00:58:10,920
，我相信丹尼尔和你一样

1532
00:58:10,920 --> 00:58:13,260
想过 关于这个，当谈到

1533
00:58:13,260 --> 00:58:15,000
嗯，你知道你在主动

1534
00:58:15,000 --> 00:58:17,099
推理和集体行为方面的工作是

1535
00:58:17,099 --> 00:58:18,960
关于多代理行为的有趣的事情，

1536
00:58:18,960 --> 00:58:21,300
在这种情况下，生成

1537
00:58:21,300 --> 00:58:25,020
过程是其他代理的行为，

1538
00:58:25,020 --> 00:58:26,819
所以生成过程我的生成

1539
00:58:26,819 --> 00:58:28,980
过程实际上是

1540
00:58:28,980 --> 00:58:31,020
另一个主动推理代理的输出，所以这

1541
00:58:31,020 --> 00:58:32,819


1542
00:58:32,819 --> 00:58:34,500
是达芙妮和我在做的时候必须努力解决的最复杂的事情之一

1543
00:58:34,500 --> 00:58:36,720


1544
00:58:36,720 --> 00:58:38,339
毛泽东也是认知社区的第一作者

1545
00:58:38,339 --> 00:58:40,500
像

1546
00:58:40,500 --> 00:58:43,140
社交网络回声室一样工作 在

1547
00:58:43,140 --> 00:58:45,119
这种情况下，生成过程

1548
00:58:45,119 --> 00:58:47,880
有点困难，因为

1549
00:58:47,880 --> 00:58:49,980
该过程本身由

1550
00:58:49,980 --> 00:58:51,660
其他活跃的推理代理组成 我们

1551
00:58:51,660 --> 00:58:54,780
还采取行动，因此该代码的控制流

1552
00:58:54,780 --> 00:58:55,980
看起来会有点不同

1553
00:58:55,980 --> 00:58:57,420
，您将不得不循环遍历

1554
00:58:57,420 --> 00:59:00,599
所有代理，从中获取操作，

1555
00:59:00,599 --> 00:59:02,400
然后使用这些操作来参数

1556
00:59:02,400 --> 00:59:03,720
化所有其他代理的观察结果，

1557
00:59:03,720 --> 00:59:05,760
我的意思是这只是 一个

1558
00:59:05,760 --> 00:59:08,099
关于多智能体模拟

1559
00:59:08,099 --> 00:59:10,440
的一般性陈述，但是

1560
00:59:10,440 --> 00:59:12,240
当你考虑

1561
00:59:12,240 --> 00:59:14,400
智能体试图对其他智能体建模时，它特别有趣，

1562
00:59:14,400 --> 00:59:16,740
因为几乎每个活跃的

1563
00:59:16,740 --> 00:59:18,119
推理智能体都会有一个

1564
00:59:18,119 --> 00:59:20,400
关于世界如何运作的贫乏模型，

1565
00:59:20,400 --> 00:59:23,819
当世界如何运作时

1566
00:59:23,819 --> 00:59:25,500
是一堆交互的主动

1567
00:59:25,500 --> 00:59:27,000
推理代理，所以你将

1568
00:59:27,000 --> 00:59:29,819
不得不为每个代理

1569
00:59:29,819 --> 00:59:31,500
配备一个更简化的生成模型，

1570
00:59:31,500 --> 00:59:33,660
除非你希望它们都具有

1571
00:59:33,660 --> 00:59:35,460
无限递归深度并且能够

1572
00:59:35,460 --> 00:59:37,859
在它们自己的生成模型中模拟

1573
00:59:37,859 --> 00:59:39,480
每个其他

1574
00:59:39,480 --> 00:59:41,940
代理的生成模型所以是的，呃，我的意思是这

1575
00:59:41,940 --> 00:59:43,020
与

1576
00:59:43,020 --> 00:59:44,460
多代理案例有点切线，但我认为它' 考虑

1577
00:59:44,460 --> 00:59:46,619


1578
00:59:46,619 --> 00:59:49,140


1579
00:59:49,140 --> 00:59:51,119


1580
00:59:51,119 --> 00:59:52,680
生成模型复杂性和

1581
00:59:52,680 --> 00:59:54,480
生成过程复杂性之间的紧张关系，以及

1582
00:59:54,480 --> 00:59:56,579
它们如何相互约束彼此的

1583
00:59:56,579 --> 00:59:59,480
行为，这只是一个有趣的复杂问题，

1584
01:00:02,400 --> 01:00:04,799
好吧，我可以继续吗，嗯，是

1585
01:00:04,799 --> 01:00:06,540
的，最后两张幻灯片我 think 是一种

1586
01:00:06,540 --> 01:00:09,720
令人兴奋的东西，所以这里列出了

1587
01:00:09,720 --> 01:00:11,099
我们希望在未来使用 prime DP 做的事情，

1588
01:00:11,099 --> 01:00:13,500


1589
01:00:13,500 --> 01:00:15,480
嗯，我将通过它们，然后

1590
01:00:15,480 --> 01:00:16,980
我将详细介绍一些我认为

1591
01:00:16,980 --> 01:00:19,319
最重要的事情 很重要，所以一个是将 Pine DP

1592
01:00:19,319 --> 01:00:21,299
模型拟合到经验数据，所以我

1593
01:00:21,299 --> 01:00:23,160
与来自计算精神病学社区的人进行了很多互动，

1594
01:00:23,160 --> 01:00:24,960
他们

1595
01:00:24,960 --> 01:00:26,460
有兴趣实际创建

1596
01:00:26,460 --> 01:00:30,299
行为模型，通常是人类行为，

1597
01:00:30,299 --> 01:00:32,520
嗯，这是他们的 Palm DP 主动

1598
01:00:32,520 --> 01:00:34,740
推理模型和其中之一

1599
01:00:34,740 --> 01:00:37,020
我认为现在 pine DP 的最大局限性

1600
01:00:37,020 --> 01:00:40,319
是人们不能使用 pi mdp 来

1601
01:00:40,319 --> 01:00:42,900


1602
01:00:42,900 --> 01:00:44,819
推断正在执行某些任务的人类主体的主动推理参数，而

1603
01:00:44,819 --> 01:00:47,220
这正是您可以在 S 中执行的任务 现在是 PM，

1604
01:00:47,220 --> 01:00:48,540
但不幸的是，你不能

1605
01:00:48,540 --> 01:00:51,540
在 Pion VP 中这样做，所以这

1606
01:00:51,540 --> 01:00:53,339
在优先级列表中非常高，我认为

1607
01:00:53,339 --> 01:00:55,020
这将帮助主要 P 实际上

1608
01:00:55,020 --> 01:00:58,440
成为

1609
01:00:58,440 --> 01:00:59,640
有兴趣

1610
01:00:59,640 --> 01:01:02,640
安装 Pi mdp 模型的社区的 SPM 的竞争者 到数据，所以

1611
01:01:02,640 --> 01:01:03,780
这些就像计算精神病学等更经验主义的

1612
01:01:03,780 --> 01:01:05,040
科学学科，

1613
01:01:05,040 --> 01:01:08,040


1614
01:01:08,040 --> 01:01:10,440
我认为我们现在需要一个

1615
01:01:10,440 --> 01:01:12,059
更好的接口来实际

1616
01:01:12,059 --> 01:01:13,859
生成和构建生成

1617
01:01:13,859 --> 01:01:17,460
模型，所有

1618
01:01:17,460 --> 01:01:19,559
涉及构建 A 和 B 矩阵的代码真正

1619
01:01:19,559 --> 01:01:21,180
成为瓶颈 对于任何

1620
01:01:21,180 --> 01:01:23,280
试图进行主动推理的人来说，在很大程度上是

1621
01:01:23,280 --> 01:01:25,319
因为为

1622
01:01:25,319 --> 01:01:27,839
复杂的慷慨模型构建这些数组真的

1623
01:01:27,839 --> 01:01:29,280
很让人头疼，你必须做所有这些奇怪

1624
01:01:29,280 --> 01:01:31,680
的多维索引，因为

1625
01:01:31,680 --> 01:01:33,000
就像你在世界上有一堆不同的

1626
01:01:33,000 --> 01:01:34,920
相互作用变量一样 你

1627
01:01:34,920 --> 01:01:37,079
必须创建大量的多维

1628
01:01:37,079 --> 01:01:39,180
数组，这些数组具有不同数量的

1629
01:01:39,180 --> 01:01:41,160
额外维度，对应于 世界上所有

1630
01:01:41,160 --> 01:01:42,720
这些可能的意外事件

1631
01:01:42,720 --> 01:01:44,460
，它有点变成了一个巨大的

1632
01:01:44,460 --> 01:01:47,339
查找表，你必须对

1633
01:01:47,339 --> 01:01:49,680
变量之间的所有关系进行编码，

1634
01:01:49,680 --> 01:01:51,480
所以我认为这可能是一个

1635
01:01:51,480 --> 01:01:52,859
雄心勃勃的项目，但可能有一些

1636
01:01:52,859 --> 01:01:55,619
方法可以实际创建类似

1637
01:01:55,619 --> 01:01:58,440
用户的用户界面 帮助人们建立

1638
01:01:58,440 --> 01:02:00,540
生成模型的界面，比如问他们

1639
01:02:00,540 --> 01:02:02,579
一系列问题，例如

1640
01:02:02,579 --> 01:02:04,200
你想让这个变量影响那个

1641
01:02:04,200 --> 01:02:05,760
变量，然后根据他们的

1642
01:02:05,760 --> 01:02:08,339
回答，你可以

1643
01:02:08,339 --> 01:02:10,440
预先参数化矩阵的一部分

1644
01:02:10,440 --> 01:02:12,180
或其他东西，然后

1645
01:02:12,180 --> 01:02:13,740
矩阵草图窗口的实际结构

1646
01:02:13,740 --> 01:02:15,900
下降到一系列关于世界上不同突发事件的是的

1647
01:02:15,900 --> 01:02:17,160
问题

1648
01:02:17,160 --> 01:02:19,740


1649
01:02:19,740 --> 01:02:21,900
另一件事是与 openai

1650
01:02:21,900 --> 01:02:24,119
gym 的接口我们已经完成了

1651
01:02:24,119 --> 01:02:26,099
就像有几个例子我们已经

1652
01:02:26,099 --> 01:02:28,079
完成了这个 我还

1653
01:02:28,079 --> 01:02:31,020
没有在 GitHub 上积极地将这些放在基础设施上，

1654
01:02:31,020 --> 01:02:32,880
但这是一个开放的东西，

1655
01:02:32,880 --> 01:02:35,160
这是一件非常明显和容易

1656
01:02:35,160 --> 01:02:37,500
做的事情 因为就像我们编写

1657
01:02:37,500 --> 01:02:40,020
环境类一样，就

1658
01:02:40,020 --> 01:02:42,240
好像它是基于健身房环境一样，所以

1659
01:02:42,240 --> 01:02:43,859
一旦你这样做了，它就会打开

1660
01:02:43,859 --> 01:02:45,780
比较主动推理代理与层次模型上

1661
01:02:45,780 --> 01:02:46,920
各种强化学习

1662
01:02:46,920 --> 01:02:48,000
算法

1663
01:02:48,000 --> 01:02:51,780
的大问题，所以

1664
01:02:51,780 --> 01:02:54,140
嗯，基本上允许你在彼此之间堆叠

1665
01:02:54,140 --> 01:02:56,099
分层活跃的推理代理

1666
01:02:56,099 --> 01:02:58,559
，就像是的

1667
01:02:58,559 --> 01:03:00,119
，你

1668
01:03:00,119 --> 01:03:02,579
可以通过将活跃的推理代理堆叠

1669
01:03:02,579 --> 01:03:05,160
到分层的事物中来获得很多时间深度，比如

1670
01:03:05,160 --> 01:03:07,680
推理和计划的时间尺度

1671
01:03:07,680 --> 01:03:10,200
在一个较慢的时间尺度上发生 比 a 慢

1672
01:03:10,200 --> 01:03:12,420
一个 sub 更快的时间尺度，

1673
01:03:12,420 --> 01:03:14,520
我们需要更多演示

1674
01:03:14,520 --> 01:03:16,140
参数学习的演示，这样你就可以

1675
01:03:16,140 --> 01:03:19,020
更新 a b 和 d 数组我不

1676
01:03:19,020 --> 01:03:21,299
认为你到目前为止可以更新 C 我知道

1677
01:03:21,299 --> 01:03:22,740
这是丹尼尔你向我提到的东西

1678
01:03:22,740 --> 01:03:25,140
这是呃 你在

1679
01:03:25,140 --> 01:03:26,700
主动推理研究所的人通常

1680
01:03:26,700 --> 01:03:28,079
对

1681
01:03:28,079 --> 01:03:29,640
um 基本上更新关于

1682
01:03:29,640 --> 01:03:31,200
生成模型参数的信念感兴趣

1683
01:03:31,200 --> 01:03:32,460
然后有事情 就像

1684
01:03:32,460 --> 01:03:34,319
复杂的推理，这是

1685
01:03:34,319 --> 01:03:36,000
一种更新版本的

1686
01:03:36,000 --> 01:03:37,440
主动推理

1687
01:03:37,440 --> 01:03:40,200
下的规划，这很有趣，并且对它有一些

1688
01:03:40,200 --> 01:03:42,480
计算上的好处，然后

1689
01:03:42,480 --> 01:03:43,799
与复杂的推理携手并进

1690
01:03:43,799 --> 01:03:45,839
，人们

1691
01:03:45,839 --> 01:03:47,700
已经开发出必须在

1692
01:03:47,700 --> 01:03:49,140
深度强化中处理的东西 学习一段时间

1693
01:03:49,140 --> 01:03:51,720
，你如何驯服组合

1694
01:03:51,720 --> 01:03:53,460
爆炸性政策空间，所以当你

1695
01:03:53,460 --> 01:03:56,280
随着时间的推移进行深入规划时，呃

1696
01:03:56,280 --> 01:03:58,200
，政策的

1697
01:03:58,200 --> 01:04:01,020
数量与你计划的时间步数成指数关系，

1698
01:04:01,020 --> 01:04:03,720
所以有各种技术来

1699
01:04:03,720 --> 01:04:05,339
处理这个问题 就像蒙特卡洛树

1700
01:04:05,339 --> 01:04:07,200
搜索，我认为有些人像

1701
01:04:07,200 --> 01:04:09,380
teofio Champions 和其他人已经

1702
01:04:09,380 --> 01:04:11,640
尝试开始为 um 实施他们自己

1703
01:04:11,640 --> 01:04:14,700
的 palm DPS 实现

1704
01:04:14,700 --> 01:04:17,040
，所以沿着这些思路我

1705
01:04:17,040 --> 01:04:19,079
只想指出我们实际上非常

1706
01:04:19,079 --> 01:04:21,059
接近实现这个 现在工作，使

1707
01:04:21,059 --> 01:04:23,339
Pi mdp 模型与经验数据相匹配，

1708
01:04:23,339 --> 01:04:25,980
所以有一个分支

1709
01:04:25,980 --> 01:04:27,420
，我和 Dimitri markovic 一直在工作 继续

1710
01:04:27,420 --> 01:04:29,579
称为代理 Jax Branch，我们

1711
01:04:29,579 --> 01:04:31,680
基本上已经为 pione p 和 Jax 编写了一个后端

1712
01:04:31,680 --> 01:04:35,160
，它不仅让我们使用来自 numpyro 的

1713
01:04:35,160 --> 01:04:37,319
一堆统计概率

1714
01:04:37,319 --> 01:04:39,420
推理技术

1715
01:04:39,420 --> 01:04:42,480
来反转或

1716
01:04:42,480 --> 01:04:44,819
从类似数据推断 Pi mdp 代理的参数 实例

1717
01:04:44,819 --> 01:04:47,280
计算呃从人类参与者那里收集的，

1718
01:04:47,280 --> 01:04:49,920
但事实上它

1719
01:04:49,920 --> 01:04:52,440
也在 Jacks 中后端意味着 Pi mdp 现在是

1720
01:04:52,440 --> 01:04:54,299
完全自动可微的，

1721
01:04:54,299 --> 01:04:56,460
所以这意味着你可以在 pi VP 的矩阵层之前将深度

1722
01:04:56,460 --> 01:04:59,400
神经网络层堆叠到类似的层上

1723
01:04:59,400 --> 01:05:01,799


1724
01:05:01,799 --> 01:05:03,420
agent，然后你可以使用

1725
01:05:03,420 --> 01:05:05,460
诸如变分自由能或任何

1726
01:05:05,460 --> 01:05:07,500
其他目标函数之类的东西来

1727
01:05:07,500 --> 01:05:09,660
自动训练

1728
01:05:09,660 --> 01:05:11,880
链接到 pi VP agent 的神经网络的参数，

1729
01:05:11,880 --> 01:05:14,280
所以我认为这只是

1730
01:05:14,280 --> 01:05:16,260
在后端重新实现它，比如 Pi

1731
01:05:16,260 --> 01:05:18,720
torch 和 Jacks 这就像一个巨大的好处，

1732
01:05:18,720 --> 01:05:20,700
因为这将真正允许您

1733
01:05:20,700 --> 01:05:22,680


1734
01:05:22,680 --> 01:05:25,640
通过将

1735
01:05:25,640 --> 01:05:28,260
深度神经网络链接到各种

1736
01:05:28,260 --> 01:05:31,020
组件 o 将 pione P 扩展到更多的高维状态空间 f agent 的身体，正如你

1737
01:05:31,020 --> 01:05:33,119
所描述的那样，Daniel

1738
01:05:33,119 --> 01:05:34,619
嗯，所以我们最初这样做只是为了

1739
01:05:34,619 --> 01:05:36,359
让你可以拟合经验

1740
01:05:36,359 --> 01:05:38,700
数据，但它有一个附带的好处

1741
01:05:38,700 --> 01:05:41,280
，那就是允许你

1742
01:05:41,280 --> 01:05:43,619
像反向传播梯度一样区分和传递

1743
01:05:43,619 --> 01:05:45,559
你将用于 更新深度学习

1744
01:05:45,559 --> 01:05:48,119
模型，我认为这真的很令人兴奋，

1745
01:05:48,119 --> 01:05:51,299
所以这就像快完成了我的意思是，是的，

1746
01:05:51,299 --> 01:05:53,579
就像我们非常接近建立一个

1747
01:05:53,579 --> 01:05:55,559
笔记本，如果你

1748
01:05:55,559 --> 01:05:57,180
现在看代理 Jack's Branch，它实际上是这样做的，它

1749
01:05:57,180 --> 01:05:58,980
不是很有条理，但那东西

1750
01:05:58,980 --> 01:06:01,680
现在在那里并实施了

1751
01:06:01,680 --> 01:06:04,799
另一件事也不是 Sajid，我实际上已经

1752
01:06:04,799 --> 01:06:07,020


1753
01:06:07,020 --> 01:06:09,480
从她的论文中实施了一些环境主动

1754
01:06:09,480 --> 01:06:11,339
推理揭秘比较我们

1755
01:06:11,339 --> 01:06:15,180
实际上已经在 openai 健身房使用 pi MVP 做到了，

1756
01:06:15,180 --> 01:06:16,980
就像在冰冻的湖面环境中一样，这

1757
01:06:16,980 --> 01:06:19,380
是一个很受欢迎的环境 模拟 B 矩阵

1758
01:06:19,380 --> 01:06:20,640
学习

1759
01:06:20,640 --> 01:06:21,839
嗯，这

1760
01:06:21,839 --> 01:06:24,599
也是我们已经做到的事情，呃，我们

1761
01:06:24,599 --> 01:06:26,520
需要喜欢上传那个，否则我不知道

1762
01:06:26,520 --> 01:06:28,079
写一篇简短的论文做一些 事情

1763
01:06:28,079 --> 01:06:29,700
是这样的，因为有很多类似的

1764
01:06:29,700 --> 01:06:31,200
这些不同的卷须已经被

1765
01:06:31,200 --> 01:06:33,359
探索过，这只是一个

1766
01:06:33,359 --> 01:06:34,799
向前推进的问题，实际上把它们

1767
01:06:34,799 --> 01:06:37,500
放在松树 BP repo

1768
01:06:37,500 --> 01:06:39,359
嗯是的，然后这些其他的事情

1769
01:06:39,359 --> 01:06:42,000
我想找时间做但是 我只是

1770
01:06:42,000 --> 01:06:43,799
没有，但我的意思是，正如我在开始时所说的那样

1771
01:06:43,799 --> 01:06:44,940
，这是一种非常

1772
01:06:44,940 --> 01:06:47,039
协作的努力，所以我也不想

1773
01:06:47,039 --> 01:06:48,660


1774
01:06:48,660 --> 01:06:50,700
成为一个喜欢做

1775
01:06:50,700 --> 01:06:52,380
所有这些的人，因为我认为它也

1776
01:06:52,380 --> 01:06:54,180
更健康

1777
01:06:54,180 --> 01:06:55,680
如果不同的人

1778
01:06:55,680 --> 01:06:57,480
在不同的事情上带头并

1779
01:06:57,480 --> 01:06:58,980
以他们自己的方式开发它，那么包的开发

1780
01:06:58,980 --> 01:07:00,480
就是我通常也喜欢

1781
01:07:00,480 --> 01:07:03,059
鼓励的事情是让各种感兴趣的

1782
01:07:03,059 --> 01:07:04,140
人参与

1783
01:07:04,140 --> 01:07:05,520
开发

1784
01:07:05,520 --> 01:07:08,339
，我不 不认为 Brennan 在这里，嗯，但

1785
01:07:08,339 --> 01:07:11,000
Brennan Klein 也是东北大学网络科学研究所

1786
01:07:11,000 --> 01:07:12,839
的博士后

1787
01:07:12,839 --> 01:07:13,920
和研究科学家，

1788
01:07:13,920 --> 01:07:15,240


1789
01:07:15,240 --> 01:07:17,880
他开始了这些 Pi

1790
01:07:17,880 --> 01:07:20,339
mdp 奖学金，所以他得到了

1791
01:07:20,339 --> 01:07:21,960
N 的资助 ortheastern 我认为还有 Templeton

1792
01:07:21,960 --> 01:07:24,420
基金会资助人们从事

1793
01:07:24,420 --> 01:07:26,400
Pine MVP 开发或主要 DP

1794
01:07:26,400 --> 01:07:28,619
相邻项目我认为第一

1795
01:07:28,619 --> 01:07:30,839
轮申请已经结束，

1796
01:07:30,839 --> 01:07:32,099
但这只是一个很好

1797
01:07:32,099 --> 01:07:33,839
的宣传机会，我认为

1798
01:07:33,839 --> 01:07:35,339
将会有

1799
01:07:35,339 --> 01:07:38,000
夏天的另一个队列，所以这是一种

1800
01:07:38,000 --> 01:07:40,680
看似持续的资金来源，

1801
01:07:40,680 --> 01:07:42,359
所以很高兴看到其他人

1802
01:07:42,359 --> 01:07:45,180
正在努力推动 Pine VP

1803
01:07:45,180 --> 01:07:47,339
朝着他们自己的方向发展，所以这只是一个

1804
01:07:47,339 --> 01:07:48,839
令人鼓舞的发展，我

1805
01:07:48,839 --> 01:07:52,319
想让每个人都得到评价

1806
01:07:52,319 --> 01:07:54,299
嗯，哦，是的，然后我会

1807
01:07:54,299 --> 01:07:56,359


1808
01:07:57,240 --> 01:08:00,180
耐心地在阅读文档网站上结束，

1809
01:08:00,180 --> 01:08:01,740
这对于创建自动文档非常有用

1810
01:08:01,740 --> 01:08:03,960
，所以我们在

1811
01:08:03,960 --> 01:08:05,460
那里有很多演示，我们有不同的

1812
01:08:05,460 --> 01:08:07,440
教程，

1813
01:08:07,440 --> 01:08:09,539
嗯，我们有另一个新演示，它是

1814
01:08:09,539 --> 01:08:11,160
此处未列出，它只是

1815
01:08:11,160 --> 01:08:13,140
计算离散分类模型中自由能的变化，

1816
01:08:13,140 --> 01:08:15,480
该模型

1817
01:08:15,480 --> 01:08:17,939
基于 Ryan Smith 和

1818
01:08:17,939 --> 01:08:20,040
Christopher white 以及 Carl fursten 的

1819
01:08:20,040 --> 01:08:20,819
pap 的演示

1820
01:08:20,819 --> 01:08:23,698
嗯，呃，不像那篇关于主动推理的大教程论文，

1821
01:08:23,698 --> 01:08:26,100
所以我重新实现

1822
01:08:26,100 --> 01:08:28,319
了那篇论文中的一个演示，现在

1823
01:08:28,319 --> 01:08:31,020
它也在文档中，

1824
01:08:31,020 --> 01:08:33,060
嗯，是的，所以你可以在协作中打开所有这些演示

1825
01:08:33,060 --> 01:08:35,100
笔记本，然后逐步

1826
01:08:35,100 --> 01:08:36,960
完成它们，嗯，它们 真的，您

1827
01:08:36,960 --> 01:08:38,880
甚至不需要

1828
01:08:38,880 --> 01:08:40,198
在计算机上安装 python 就可以使用它们，您

1829
01:08:40,198 --> 01:08:42,600
只需打开协作中的链接并

1830
01:08:42,600 --> 01:08:44,460
逐步执行代码，就像构建您自己的

1831
01:08:44,460 --> 01:08:46,259
主动推理

1832
01:08:46,259 --> 01:08:48,238
代理一样，这对教学很有用，这

1833
01:08:48,238 --> 01:08:49,259
就是我提到的原因 如果你刚刚

1834
01:08:49,259 --> 01:08:50,880
开始，我肯定会建议

1835
01:08:50,880 --> 01:08:54,238
你去看文档，

1836
01:08:54,238 --> 01:08:56,698
所以是的，嗯，谢谢大家的倾听

1837
01:08:56,698 --> 01:08:59,640
，也谢谢你让我有

1838
01:08:59,640 --> 01:09:01,799
机会说话，一如既往地来到这里真是太好了

1839
01:09:01,799 --> 01:09:05,640
，我想 是的，

1840
01:09:05,640 --> 01:09:07,620
我在底部列出了下一个

1841
01:09:07,620 --> 01:09:08,939
直播流，我们可以浏览

1842
01:09:08,939 --> 01:09:10,738
一些演示笔记本，但

1843
01:09:10,738 --> 01:09:12,660
如果有时间，我们现在也可以浏览它们，也许

1844
01:09:12,660 --> 01:09:14,279
我们会先进行讨论，然后再

1845
01:09:14,279 --> 01:09:15,179
看看

1846
01:09:15,179 --> 01:09:17,880
有没有时间，

1847
01:09:17,880 --> 01:09:21,540
太棒了，谢谢你 嗯，让我们来解决

1848
01:09:21,540 --> 01:09:25,140
达芙妮和雅各布的一些问题，我会

1849
01:09:25,140 --> 01:09:28,140
在实时聊天中问一些问题，然后也许

1850
01:09:28,140 --> 01:09:31,738
你可以

1851
01:09:31,738 --> 01:09:34,920
在阅读文档时分享一个或几个例子

1852
01:09:34,920 --> 01:09:36,479
，我们可以从

1853
01:09:36,479 --> 01:09:40,140
结构上看一下解剖学和

1854
01:09:40,140 --> 01:09:42,899
生理学是什么 一个笔记本所以首先

1855
01:09:42,899 --> 01:09:46,080
达芙妮或雅各布有任何

1856
01:09:46,080 --> 01:09:49,339
想法或问题

1857
01:09:52,679 --> 01:09:56,120
是的是的去吧

1858
01:09:56,400 --> 01:09:58,620
嗯我想知道在 Jacks

1859
01:09:58,620 --> 01:10:00,300
实现上

1860
01:10:00,300 --> 01:10:03,660
是否有任何关于

1861
01:10:03,660 --> 01:10:07,400
定义生成过程的要求

1862
01:10:07,500 --> 01:10:11,160
或者它只是关于定义

1863
01:10:11,160 --> 01:10:13,140


1864
01:10:13,140 --> 01:10:14,820
然后我们将生成模型拟合到实验数据

1865
01:10:14,820 --> 01:10:16,199
，我想这也与

1866
01:10:16,199 --> 01:10:18,840
我在将这些模型扩展

1867
01:10:18,840 --> 01:10:21,780
到状态空间或一般过程时遇到的另一个问题有关

1868
01:10:21,780 --> 01:10:24,179
，我们作为建模者没有

1869
01:10:24,179 --> 01:10:27,000
自由来实际定义自己，但

1870
01:10:27,000 --> 01:10:28,699
我们想

1871
01:10:28,699 --> 01:10:31,159
部署和

1872
01:10:31,159 --> 01:10:34,739
在已经存在的一般流程中训练这些代理人，

1873
01:10:34,739 --> 01:10:37,080
就像在在线

1874
01:10:37,080 --> 01:10:39,780
环境中一样，您可以在其中获得分类或

1875
01:10:39,780 --> 01:10:42,500


1876
01:10:42,600 --> 01:10:45,360
离散数据，

1877
01:10:45,360 --> 01:10:46,920
是的，

1878
01:10:46,920 --> 01:10:48,960
完全是

1879
01:10:48,960 --> 01:10:51,360
这样 一个很好的问题，

1880
01:10:51,360 --> 01:10:54,659
嗯，我有一个计划 BP 代理，它

1881
01:10:54,659 --> 01:10:56,159
附加了一堆深度神经网络

1882
01:10:56,159 --> 01:10:59,580
，我想

1883
01:10:59,580 --> 01:11:01,320
在部署的环境中训练它，所以它就像在

1884
01:11:01,320 --> 01:11:03,179
那里，你知道，让我们说它是一个代理

1885
01:11:03,179 --> 01:11:04,739
那是在股票市场上交易，或者

1886
01:11:04,739 --> 01:11:06,300


1887
01:11:06,300 --> 01:11:09,060
像下注购买加密货币一样让我们

1888
01:11:09,060 --> 01:11:09,960
说

1889
01:11:09,960 --> 01:11:12,659
在这种情况下以同样的方式

1890
01:11:12,659 --> 01:11:14,280
在那种数据上训练深度神经网络

1891
01:11:14,280 --> 01:11:16,320
你不需要通过生成过程传递梯度

1892
01:11:16,320 --> 01:11:18,120
当然

1893
01:11:18,120 --> 01:11:20,159
你无法访问你

1894
01:11:20,159 --> 01:11:22,320
在股票市场上交易所以从这个意义上说

1895
01:11:22,320 --> 01:11:24,420
没有关于传递梯度的要求

1896
01:11:24,420 --> 01:11:27,300
或者编写一个

1897
01:11:27,300 --> 01:11:30,060
也是自动可微分的生成过程

1898
01:11:30,060 --> 01:11:32,820
有一种情况你想要

1899
01:11:32,820 --> 01:11:34,739
的是

1900
01:11:34,739 --> 01:11:37,980
通常在经验

1901
01:11:37,980 --> 01:11:40,800
上将 Pine DP 模型拟合到数据的情况下，通常

1902
01:11:40,800 --> 01:11:43,020
你想要做的一件事是你

1903
01:11:43,020 --> 01:11:45,000
有一堆就像你基本上有

1904
01:11:45,000 --> 01:11:47,219
一个人类参与者的行为和观察的历史，

1905
01:11:47,219 --> 01:11:49,679
你适合模型

1906
01:11:49,679 --> 01:11:52,140
t  pi VP 代理的参数

1907
01:11:52,140 --> 01:11:54,360
最能解释你的参与者观察到的行为

1908
01:11:54,360 --> 01:11:55,920
，你知道这些

1909
01:11:55,920 --> 01:11:57,120
观察结果，因为你是一个

1910
01:11:57,120 --> 01:11:59,219
实验者，喜欢决定这个

1911
01:11:59,219 --> 01:12:00,480
人将看到观察序列，

1912
01:12:00,480 --> 01:12:02,400
所以你可以在

1913
01:12:02,400 --> 01:12:04,380
没有可微分的情况下完成所有这些

1914
01:12:04,380 --> 01:12:07,380
生成过程或环境，但

1915
01:12:07,380 --> 01:12:09,000
贝叶斯推理中有一些东西

1916
01:12:09,000 --> 01:12:10,860
被称为后验

1917
01:12:10,860 --> 01:12:12,960
预测检查，你说好吧，

1918
01:12:12,960 --> 01:12:16,620
鉴于我对 uh

1919
01:12:16,620 --> 01:12:18,840
pione P 代理的参数的推断，然后我

1920
01:12:18,840 --> 01:12:20,520
想推出

1921
01:12:20,520 --> 01:12:23,100
这个代理的预期行为

1922
01:12:23,100 --> 01:12:25,560
给出我对这个

1923
01:12:25,560 --> 01:12:27,840
代理参数的最佳猜测，所以这被

1924
01:12:27,840 --> 01:12:29,640
称为后验预测密度

1925
01:12:29,640 --> 01:12:32,400
，你说给定我

1926
01:12:32,400 --> 01:12:34,500
对代理参数的后验 um 估计

1927
01:12:34,500 --> 01:12:37,080


1928
01:12:37,080 --> 01:12:39,480
在这些后验参数下它在未来会是什么样子

1929
01:12:39,480 --> 01:12:42,840
并在 使用 numpyro，它

1930
01:12:42,840 --> 01:12:45,420
是

1931
01:12:45,420 --> 01:12:47,699
使用 Jax 作为后端的概率推理框架，你会

1932
01:12:47,699 --> 01:12:49,800
想要一个生成器 过程

1933
01:12:49,800 --> 01:12:52,679
也是自动可微的，

1934
01:12:52,679 --> 01:12:54,659
但在那种情况下，我希望

1935
01:12:54,659 --> 01:12:56,400
编写这些生成过程会

1936
01:12:56,400 --> 01:12:58,020
很容易，因为这将是在将

1937
01:12:58,020 --> 01:13:01,199
人类行为与实验数据相匹配的情况

1938
01:13:01,199 --> 01:13:02,820
下，他们就像

1939
01:13:02,820 --> 01:13:05,159
在受控任务环境中一样，所以如果它

1940
01:13:05,159 --> 01:13:07,739
是 尝试拟合某人的

1941
01:13:07,739 --> 01:13:10,320
参数（例如他们的 C 向量的值）的情况，

1942
01:13:10,320 --> 01:13:12,420
并且他们正在执行

1943
01:13:12,420 --> 01:13:13,860
他们生病的场景构建任务，

1944
01:13:13,860 --> 01:13:15,300


1945
01:13:15,300 --> 01:13:17,040
您可以编写生成过程，

1946
01:13:17,040 --> 01:13:18,840
因为您作为实验或

1947
01:13:18,840 --> 01:13:21,480
开发了类似心理物理学的

1948
01:13:21,480 --> 01:13:22,739
任务 他们正在与您进行交互，

1949
01:13:22,739 --> 01:13:24,659
当您进行建模时，他们也可以在 Jax 中编写这些内容，

1950
01:13:24,659 --> 01:13:26,460
这样当您进行

1951
01:13:26,460 --> 01:13:28,620
这些后验预测检查时，您

1952
01:13:28,620 --> 01:13:30,300
知道这也是在 Jacks 中编写的，

1953
01:13:30,300 --> 01:13:31,620
并且您可以计算这些

1954
01:13:31,620 --> 01:13:33,900
数量，但在已部署的设置中

1955
01:13:33,900 --> 01:13:35,460
你甚至无法在未来进行

1956
01:13:35,460 --> 01:13:37,140
任何类型的后验预测检查

1957
01:13:37,140 --> 01:13:39,179
，因为你不

1958
01:13:39,179 --> 01:13:41,219
知道环境实际上是如何工作的 ht

1959
01:13:41,219 --> 01:13:43,080
所以你必须告诉

1960
01:13:43,080 --> 01:13:44,940
你，那甚至不是

1961
01:13:44,940 --> 01:13:47,520
你一开始就尝试做的事情，

1962
01:13:47,520 --> 01:13:48,540
嗯，但是是的，所以没有什么

1963
01:13:48,540 --> 01:13:52,020
内在地阻止你，

1964
01:13:52,020 --> 01:13:54,000
只要模型可以以

1965
01:13:54,000 --> 01:13:55,140
与它们相同的方式区分 深度神经

1966
01:13:55,140 --> 01:13:56,219
网络 没有什么能阻止你

1967
01:13:56,219 --> 01:13:57,659
将它们扔到一个

1968
01:13:57,659 --> 01:13:58,980
你不知道世界规则如何运作的环境中

1969
01:13:58,980 --> 01:14:01,940


1970
01:14:02,640 --> 01:14:04,199


1971
01:14:04,199 --> 01:14:06,900


1972
01:14:06,900 --> 01:14:10,620


1973
01:14:10,620 --> 01:14:13,080


1974
01:14:13,080 --> 01:14:15,239
在

1975
01:14:15,239 --> 01:14:16,020
那里

1976
01:14:16,020 --> 01:14:18,900
它要么会工作，要么不会

1977
01:14:18,900 --> 01:14:21,600
，如果它失败了，它就会失败，

1978
01:14:21,600 --> 01:14:23,940
计算环境允许我们

1979
01:14:23,940 --> 01:14:25,860
存在于这种灰色地带，在这个灰色地带

1980
01:14:25,860 --> 01:14:28,260
，计算代理可能

1981
01:14:28,260 --> 01:14:30,719
很难适应给定的部署

1982
01:14:30,719 --> 01:14:33,300
设置，但计算机程序

1983
01:14:33,300 --> 01:14:35,880
仍然会运行，但我们当然

1984
01:14:35,880 --> 01:14:38,300
对计算机程序运行

1985
01:14:38,300 --> 01:14:42,120
并且代理能够像我们一样发生某种

1986
01:14:42,120 --> 01:14:43,100


1987
01:14:43,100 --> 01:14:47,520
有意义甚至有用的行为的情况感兴趣

1988
01:14:47,520 --> 01:14:49,860
我们能不能

1989
01:14:49,860 --> 01:14:52,620
想象非常简单的 Prime DP 模型

1990
01:14:52,620 --> 01:14:54,420
在某些任务中会做得非常好

1991
01:14:54,420 --> 01:14:56,340
，就像一个具有两个隐藏状态的愚蠢模型一样

1992
01:14:56,340 --> 01:14:57,960
，它认为它们只是

1993
01:14:57,960 --> 01:14:59,580
随机地在彼此之间切换

1994
01:14:59,580 --> 01:15:02,340
，然后你给它一个任务，

1995
01:15:02,340 --> 01:15:05,520
呃，投资于 就像一个 10 只

1996
01:15:05,520 --> 01:15:07,679
股票的投资组合，当然它的

1997
01:15:07,679 --> 01:15:10,739
模型不适合，但是

1998
01:15:10,739 --> 01:15:13,800
将大的深度函数逼近

1999
01:15:13,800 --> 01:15:15,780
器应用于 Prime DP 代理的不同端的承诺

2000
01:15:15,780 --> 01:15:18,179
意味着希望你可以

2001
01:15:18,179 --> 01:15:20,760
学习一个好的生成模型，然后

2002
01:15:20,760 --> 01:15:22,739
仍然将它与一些较低的

2003
01:15:22,739 --> 01:15:24,719
维度生成模型

2004
01:15:24,719 --> 01:15:26,400
可以通过主动推理进行所有好的推理和

2005
01:15:26,400 --> 01:15:28,380
规划，但它

2006
01:15:28,380 --> 01:15:31,320
可以通过使用深度神经网络处理高维或丑陋的

2007
01:15:31,320 --> 01:15:33,420
难以驯服的观察和行动

2008
01:15:33,420 --> 01:15:36,360
空间，所以

2009
01:15:36,360 --> 01:15:39,000
我认为这真的是

2010
01:15:39,000 --> 01:15:40,800
在 同样，深度学习

2011
01:15:40,800 --> 01:15:42,659
在很多情况下都能发挥作用，

2012
01:15:42,659 --> 01:15:45,120
这就是使用主要

2013
01:15:45,120 --> 01:15:48,120
DP 模型以及

2014
01:15:48,120 --> 01:15:50,940
Daphne 任何评论的方式 或者我会

2015
01:15:50,940 --> 01:15:53,719
在聊天中问一些问题，

2016
01:15:54,540 --> 01:15:55,920
嗯，我真的没有任何问题，但

2017
01:15:55,920 --> 01:15:57,960
我确实认为这真的很有趣，

2018
01:15:57,960 --> 01:16:00,000
而且

2019
01:16:00,000 --> 01:16:02,040
我认为像你说的那样去

2020
01:16:02,040 --> 01:16:06,300
学习生成模型真的很令人兴奋

2021
01:16:06,300 --> 01:16:07,800
智能体学习它自己的生成

2022
01:16:07,800 --> 01:16:10,560
模型，就像一些真实世界的数据一样

2023
01:16:10,560 --> 01:16:12,480
，想弄清楚它们是什么，我想

2024
01:16:12,480 --> 01:16:14,040
就像你

2025
01:16:14,040 --> 01:16:15,600
所说的那样，比如函数

2026
01:16:15,600 --> 01:16:18,480
逼近和 numpyro 之类的

2027
01:16:18,480 --> 01:16:22,140
东西仍然像你们一样

2028
01:16:22,140 --> 01:16:24,239
仍然像从那个代码库一样工作

2029
01:16:24,239 --> 01:16:27,360
，然后最终转换那些或者

2030
01:16:27,360 --> 01:16:29,100
让它从头开始

2031
01:16:29,100 --> 01:16:31,320
是的，我们几乎

2032
01:16:31,320 --> 01:16:33,659
从头开始 pi DCM 的事情

2033
01:16:33,659 --> 01:16:35,400
我实际上不确定它的

2034
01:16:35,400 --> 01:16:37,260
IP 状态是什么，因为我们工作

2035
01:16:37,260 --> 01:16:39,060
在它上面作为嵌套我的一部分，所以

2036
01:16:39,060 --> 01:16:40,500
我无法再访问该代码，

2037
01:16:40,500 --> 01:16:43,260
但它更像是

2038
01:16:43,260 --> 01:16:46,739
变体 LaPlace 的实现，我们

2039
01:16:46,739 --> 01:16:48,900
在其中与变体 Applause 注入一起工作，

2040
01:16:48,900 --> 01:16:51,960
这是 wa 是的，

2041
01:16:51,960 --> 01:16:54,060


2042
01:16:54,060 --> 01:16:56,460
当你试图做 um 推理

2043
01:16:56,460 --> 01:16:59,640
我们现在正在做的事情而不是

2044
01:16:59,640 --> 01:17:02,400
我们说你能重写一个松树 DP

2045
01:17:02,400 --> 01:17:03,600
模型，

2046
01:17:03,600 --> 01:17:06,600
这样你就可以通过它传递梯度时，你可以对自由能进行梯度下降

2047
01:17:06,600 --> 01:17:09,420
Jacks like accelerated 梯度，

2048
01:17:09,420 --> 01:17:11,400
然后使用 numpyro 进行各种

2049
01:17:11,400 --> 01:17:13,020
拟合例程，不仅仅是变分

2050
01:17:13,020 --> 01:17:16,080
拉普拉斯，你可以使用 mcmc，你可以

2051
01:17:16,080 --> 01:17:18,540
像 numpower 一样使用，你知道有一个巨大的

2052
01:17:18,540 --> 01:17:21,440
库，包含不同的呃

2053
01:17:21,440 --> 01:17:24,179
概率近似贝叶斯

2054
01:17:24,179 --> 01:17:25,679
推理技术，所以你可以

2055
01:17:25,679 --> 01:17:28,380
把厨房的水槽扔掉

2056
01:17:28,380 --> 01:17:31,800
在主要 DP 模型中使用 numpyro 推理技术，

2057
01:17:31,800 --> 01:17:34,080
所以这里的挑战只是

2058
01:17:34,080 --> 01:17:36,239
重写主要 DP 模型，这样您就可以定义

2059
01:17:36,239 --> 01:17:38,280
一个似然函数，从

2060
01:17:38,280 --> 01:17:40,440
Palm DP 参数到

2061
01:17:40,440 --> 01:17:42,420
观察结果，在这种情况下将

2062
01:17:42,420 --> 01:17:45,300
是代理的动作 为了

2063
01:17:45,300 --> 01:17:47,040
以某种方式做到这一点，以便

2064
01:17:47,040 --> 01:17:48,780
与 Jax 友好地玩耍，我们只需要确保

2065
01:17:48,780 --> 01:17:51,300


2066
01:17:51,300 --> 01:17:53,340
pine DP 代理的所有内部功能都像推理一样

2067
01:17:53,340 --> 01:17:55,679
计划行动选择所有这些

2068
01:17:55,679 --> 01:17:58,199
都是用 Jacks 编写的，这样你就可以

2069
01:17:58,199 --> 01:18:00,000
在像似然梯度这样的计算时有效地传递梯度

2070
01:18:00,000 --> 01:18:03,199


2071
01:18:03,199 --> 01:18:05,699
是的，这很有意义

2072
01:18:05,699 --> 01:18:07,380
是的，

2073
01:18:07,380 --> 01:18:09,900
好吧，我会从实时聊天中问几个问题，

2074
01:18:09,900 --> 01:18:13,140
所以

2075
01:18:13,140 --> 01:18:15,360
首先 整个文献中对主动推理的描述

2076
01:18:15,360 --> 01:18:17,100
都是用矩阵来写的，

2077
01:18:17,100 --> 01:18:19,380
但是 Pi mdp 显然

2078
01:18:19,380 --> 01:18:20,880
适用于张量，

2079
01:18:20,880 --> 01:18:23,340
当你将方程从矩阵推广到张量时，你有一个很好的参考来说明

2080
01:18:23,340 --> 01:18:24,960
操作有何不同，

2081
01:18:24,960 --> 01:18:27,120


2082
01:18:27,120 --> 01:18:28,940


2083
01:18:28,940 --> 01:18:31,260
这是一个很好的观点，

2084
01:18:31,260 --> 01:18:32,100
嗯，这是其中之一

2085
01:18:32,100 --> 01:18:33,840
当我第一次学习主动推理时，真正让我很沮丧的

2086
01:18:33,840 --> 01:18:35,159


2087
01:18:35,159 --> 01:18:37,340
事情是我注意到这个人

2088
01:18:37,340 --> 01:18:40,260
问的是很多基本

2089
01:18:40,260 --> 01:18:42,000
操作

2090
01:18:42,000 --> 01:18:42,540
um

2091
01:18:42,540 --> 01:18:45,960
被写成好像只有一

2092
01:18:45,960 --> 01:18:47,520
维隐藏状态和一

2093
01:18:47,520 --> 01:18:50,580
维观察所以一切

2094
01:18:50,580 --> 01:18:53,159
都是 就像他们说的 Matrix Vector products

2095
01:18:53,159 --> 01:18:54,840
和 Matrix math 但我们真正

2096
01:18:54,840 --> 01:18:57,239
做的是张量乘法和

2097
01:18:57,239 --> 01:19:00,239
张量积所以就

2098
01:19:00,239 --> 01:19:03,060
其工作原理的参考而言，

2099
01:19:03,060 --> 01:19:05,940
嗯，是的，所以从本质上讲，我们在 pione P 中

2100
01:19:05,940 --> 01:19:07,800


2101
01:19:07,800 --> 01:19:10,260
所做的这些张量运算基本上没有什么超质的不同

2102
01:19:10,260 --> 01:19:12,600
，基本上只是

2103
01:19:12,600 --> 01:19:15,600
表达

2104
01:19:15,600 --> 01:19:17,880
矩阵乘法和的更奇特的方式，所以它

2105
01:19:17,880 --> 01:19:20,280
的数学是 所有仍然

2106
01:19:20,280 --> 01:19:22,380
标准的线性代数只是

2107
01:19:22,380 --> 01:19:24,060
我们表示这些高维

2108
01:19:24,060 --> 01:19:25,920
矩阵的方式，因为张量只是一种更

2109
01:19:25,920 --> 01:19:27,120
有效的表示方式，所以从

2110
01:19:27,120 --> 01:19:29,880
数学上讲，这并没有太疯狂

2111
01:19:29,880 --> 01:19:32,340
，我了解它是如何工作的方式

2112
01:19:32,340 --> 01:19:34,800
只是通过盯着

2113
01:19:34,800 --> 01:19:37,260
Matlab 中的函数看了一年直到 我只是

2114
01:19:37,260 --> 01:19:38,820
弄明白了，但这并不容易，

2115
01:19:38,820 --> 01:19:41,580
现在肯定有更好的选择，

2116
01:19:41,580 --> 01:19:42,900


2117
01:19:42,900 --> 01:19:45,600
嗯，嗯，我马上

2118
01:19:45,600 --> 01:19:48,719
推荐的一个参考资料是 um Pi mdp 的附录附录

2119
01:19:48,719 --> 01:19:50,460
，

2120
01:19:50,460 --> 01:19:52,920
嗯，存档文件

2121
01:19:52,920 --> 01:19:55,739
，这样就像 我认为附录 a

2122
01:19:55,739 --> 01:19:57,960
或所有这些附录基本上处理

2123
01:19:57,960 --> 01:20:01,199
的是完整的张量因式分解版本

2124
01:20:01,199 --> 01:20:03,179
，它不处理矩阵，而是

2125
01:20:03,179 --> 01:20:05,100
我们实际上是在索引更高

2126
01:20:05,100 --> 01:20:09,120
维度的另一个维度，我认为

2127
01:20:09,120 --> 01:20:11,460
它最初讨论的是张量

2128
01:20:11,460 --> 01:20:13,860
积，张量分解

2129
01:20:13,860 --> 01:20:16,500
在我认为主动

2130
01:20:16,500 --> 01:20:18,540
推理好奇心和洞察力的附录中，这

2131
01:20:18,540 --> 01:20:19,679
是他们第一次谈论的地方，比如

2132
01:20:19,679 --> 01:20:21,719
新颖性和参数信息增益

2133
01:20:21,719 --> 01:20:23,820
，这是一个 论文，

2134
01:20:23,820 --> 01:20:25,199
嗯，对不起，我不记得

2135
01:20:25,199 --> 01:20:27,900
它发表的年份是 2017 年还是 2018 年，但

2136
01:20:27,900 --> 01:20:30,420
我知道这篇论文的标题叫做主动

2137
01:20:30,420 --> 01:20:32,340
推理好奇心和洞察力，在

2138
01:20:32,340 --> 01:20:34,440
其中一个附录中，他们实际上做

2139
01:20:34,440 --> 01:20:38,580
了完整的基于张量的数学和

2140
01:20:38,580 --> 01:20:41,820
最后，另一个很好的参考是

2141
01:20:41,820 --> 01:20:44,640
最近的一篇论文，我认为它是由 teofio Champion 领导的，

2142
01:20:44,640 --> 01:20:48,780
我只是想尽快

2143
01:20:48,780 --> 01:20:50,760
找到它，因为我

2144
01:20:50,760 --> 01:20:52,739
不想忘记它，

2145
01:20:52,739 --> 01:20:55,679
也许我会停止共享我的屏幕

2146
01:20:55,679 --> 01:20:58,800
，我停止了吗？ 它停止了

2147
01:20:58,800 --> 01:20:59,580
嗯，

2148
01:20:59,580 --> 01:21:01,080
这是一个非常好的参考，它有

2149
01:21:01,080 --> 01:21:02,760
附录

2150
01:21:02,760 --> 01:21:04,500
关于

2151
01:21:04,500 --> 01:21:07,920
嗯为主动推理做张量数学

2152
01:21:07,920 --> 01:21:11,600
特别是

2153
01:21:13,260 --> 01:21:15,600
我们最近还了解了

2154
01:21:15,600 --> 01:21:18,360
分支时间主动推理 w 这

2155
01:21:18,360 --> 01:21:20,040
谈到了一些关于

2156
01:21:20,040 --> 01:21:24,480
计算复杂性的问题

2157
01:21:26,219 --> 01:21:27,840
，

2158
01:21:27,840 --> 01:21:29,699
好吧，是的，我应该提到

2159
01:21:29,699 --> 01:21:31,380
这可能是迄今为止最有前途的

2160
01:21:31,380 --> 01:21:33,960
方法，

2161
01:21:33,960 --> 01:21:37,080
关于嗯嗯，巧妙地处理

2162
01:21:37,080 --> 01:21:38,820
主动推理的计算复杂性，

2163
01:21:38,820 --> 01:21:41,400
所以是的，这是

2164
01:21:41,400 --> 01:21:44,580
teofio 的 Champions Mark gresh 我想

2165
01:21:44,580 --> 01:21:46,440
那是他的一位顾问，Howard

2166
01:21:46,440 --> 01:21:49,560
Bowman 是他的另一位顾问，这

2167
01:21:49,560 --> 01:21:51,600
就是我刚刚发布的多模态和多因素

2168
01:21:51,600 --> 01:21:53,400
分支时间主动推理，

2169
01:21:53,400 --> 01:21:55,620
所以我自己还没有读过，

2170
01:21:55,620 --> 01:21:57,000
但我从其他人那里听说

2171
01:21:57,000 --> 01:21:59,040
过 我认为 Alec chance 告诉

2172
01:21:59,040 --> 01:22:01,920
我附录对

2173
01:22:01,920 --> 01:22:03,780
技术非常有用主动推理的全张量泛化

2174
01:22:03,780 --> 01:22:05,940


2175
01:22:05,940 --> 01:22:08,699
好吧太棒了好吧我已经将所有

2176
01:22:08,699 --> 01:22:09,540


2177
01:22:09,540 --> 01:22:12,239
提到的引用添加到 YouTube

2178
01:22:12,239 --> 01:22:13,920
实时聊天中

2179
01:22:13,920 --> 01:22:16,500
太棒了谢谢你我要问

2180
01:22:16,500 --> 01:22:19,560
Fausto

2181
01:22:19,560 --> 01:22:22,739
使用 Jax 后端提出的以下问题使得

2182
01:22:22,739 --> 01:22:24,020
包装

2183
01:22:24,020 --> 01:22:28,800
pymc3 变得容易 围绕它应用 MC3 EG 让 pi

2184
01:22:28,800 --> 01:22:32,219
mdp 作为运算符在 pi MC3

2185
01:22:32,219 --> 01:22:35,580
模型中使用 有没有计划这样做我问，

2186
01:22:35,580 --> 01:22:37,320
因为

2187
01:22:37,320 --> 01:22:41,880
围绕 Pi MC3 的 Python 中有一个不断增长的贝叶斯社区，

2188
01:22:41,880 --> 01:22:43,380
嗯，这真的很有趣，我不

2189
01:22:43,380 --> 01:22:46,199
知道实际上 Prime G3 也

2190
01:22:46,199 --> 01:22:48,719
有一个 Jacks，所以

2191
01:22:48,719 --> 01:22:51,600
我不知道 老实说，

2192
01:22:51,600 --> 01:22:52,980
我首先要说我不知道​​，

2193
01:22:52,980 --> 01:22:55,080
因为我

2194
01:22:55,080 --> 01:22:57,840
是通过 DME Dimitri markovic 介绍 Python 中的概率建模的，

2195
01:22:57,840 --> 01:23:00,300
他

2196
01:23:00,300 --> 01:23:02,880
基本上向我推销了 numpyro numpyro

2197
01:23:02,880 --> 01:23:04,920
是未来的方式，我知道 numpyro

2198
01:23:04,920 --> 01:23:06,840
有一个 Jack 的后端，我认为

2199
01:23:06,840 --> 01:23:10,440
numpyro 和 Pi MC3 在 Python

2200
01:23:10,440 --> 01:23:13,199
的概率推理生态系统中占据相似的位置，

2201
01:23:13,199 --> 01:23:16,020


2202
01:23:16,020 --> 01:23:18,300
嗯，我不知道

2203
01:23:18,300 --> 01:23:22,020
pi MC3 中的模型是如何指定的，我假设

2204
01:23:22,020 --> 01:23:24,300
它与 numpyro 中的外观

2205
01:23:24,300 --> 01:23:26,520
并没有太大不同，因为 所有低级后端

2206
01:23:26,520 --> 01:23:29,880
现在都是用 Jax 编写的，我不能保证

2207
01:23:29,880 --> 01:23:31,860
这一点，但我假设你可以

2208
01:23:31,860 --> 01:23:34,140
只编写一个 pi MC3 模型，

2209
01:23:34,140 --> 01:23:36,480
就像我们编写一个 numpyro 模型一样

2210
01:23:36,480 --> 01:23:40,500
，它包装了 um Pi mdp 函数，但只有

2211
01:23:40,500 --> 01:23:43,620
pi MVP 实现 注入所以如果

2212
01:23:43,620 --> 01:23:46,020
Pine Z3 只依赖于

2213
01:23:46,020 --> 01:23:48,600
低级别的 Jax 那么是的它肯定可以工作

2214
01:23:48,600 --> 01:23:50,699
但是我不知道这里是否有其他

2215
01:23:50,699 --> 01:23:53,040
人有使用 pine G3 的经验

2216
01:23:53,040 --> 01:23:55,320
并且可能知道

2217
01:23:55,320 --> 01:23:56,820
因为我只是不太了解

2218
01:23:56,820 --> 01:23:58,560
它

2219
01:23:58,560 --> 01:24:01,320
我使用了一点 YMC 但我认为

2220
01:24:01,320 --> 01:24:03,239
它就像你说的那样

2221
01:24:03,239 --> 01:24:05,400
与 numpyro 非常相似我认为

2222
01:24:05,400 --> 01:24:07,560
你可能能够

2223
01:24:07,560 --> 01:24:10,080
做与 numpirers 相同的事情，

2224
01:24:10,080 --> 01:24:12,480
比如在

2225
01:24:12,480 --> 01:24:15,540
pyro 上集成 pi MC3 以及

2226
01:24:15,540 --> 01:24:18,360
啊酷所以它是主要的嗯

2227
01:24:18,360 --> 01:24:19,800
后端是 Jax

2228
01:24:19,800 --> 01:24:21,239
是

2229
01:24:21,239 --> 01:24:25,099
嗯我实际上没有那样做

2230
01:24:25,620 --> 01:24:28,460
嗯我不知道是

2231
01:24:28,460 --> 01:24:31,980
的与 Matlab 的矩阵

2232
01:24:31,980 --> 01:24:35,219
乘法相比是

2233
01:24:35,219 --> 01:24:37,380
什么让你对概率感到兴奋

2234
01:24:37,380 --> 01:24:40,980
编程方向以及

2235
01:24:40,980 --> 01:24:42,780


2236
01:24:42,780 --> 01:24:44,820
我们命名的所有这些包和方法，

2237
01:24:44,820 --> 01:24:48,179
概率编程与

2238
01:24:48,179 --> 01:24:51,420
仅仅写出矩阵并

2239
01:24:51,420 --> 01:24:54,000
在纸上计算它们有何不同，为什么

2240
01:24:54,000 --> 01:24:56,880
这对实现主动推理模型有一定的希望，

2241
01:24:56,880 --> 01:24:59,400


2242
01:24:59,400 --> 01:25:01,880
嗯，我觉得 k

2243
01:25:01,880 --> 01:25:03,780
概率

2244
01:25:03,780 --> 01:25:05,940
编程的最大优势不一定是

2245
01:25:05,940 --> 01:25:08,400
模拟主动推理代理来

2246
01:25:08,400 --> 01:25:10,020
模拟主动推理代理

2247
01:25:10,020 --> 01:25:12,659
确保矩阵乘法

2248
01:25:12,659 --> 01:25:15,840
足够在 Jax 中使其

2249
01:25:15,840 --> 01:25:18,480
更具可扩展性因此您可以使用

2250
01:25:18,480 --> 01:25:21,120
所有矢量化操作来运行

2251
01:25:21,120 --> 01:25:22,980
数十个 数以千计的活动推理

2252
01:25:22,980 --> 01:25:24,600
代理同时进行，因为您拥有

2253
01:25:24,600 --> 01:25:25,620
这些

2254
01:25:25,620 --> 01:25:28,440
高度优化的即时编译

2255
01:25:28,440 --> 01:25:30,840
函数和 Jack，它们允许

2256
01:25:30,840 --> 01:25:32,280
您基本上将速度提高一个

2257
01:25:32,280 --> 01:25:34,860
数量级，但概率

2258
01:25:34,860 --> 01:25:36,540
编程角度不如

2259
01:25:36,540 --> 01:25:39,060
模拟活动推理过程那么多

2260
01:25:39,060 --> 01:25:41,100
它用于对经验数据进行推理

2261
01:25:41,100 --> 01:25:43,620
或将主动推理代理的模型拟合，

2262
01:25:43,620 --> 01:25:46,860
所以说

2263
01:25:46,860 --> 01:25:49,460
观察动物或人

2264
01:25:49,460 --> 01:25:53,040
现在正在做我们可以做的事情，我们只能

2265
01:25:53,040 --> 01:25:55,440
做 SPM，但直到现在我们还不能在 Pine VP 中做

2266
01:25:55,440 --> 01:25:58,080
是我们可以采用

2267
01:25:58,080 --> 01:25:59,460
某人的一系列行为

2268
01:25:59,460 --> 01:26:02,699
，然后推断

2269
01:26:02,699 --> 01:26:04,260
出主动推理模型的最佳参数

2270
01:26:04,260 --> 01:26:06,239
这解释了他们的行为，所以

2271
01:26:06,239 --> 01:26:08,100
考虑到某人如何决定我可以说哦，

2272
01:26:08,100 --> 01:26:10,199
他们的矩阵必须看起来像这样或者

2273
01:26:10,199 --> 01:26:12,780
他们的 C 向量必须具有这个 um

2274
01:26:12,780 --> 01:26:14,880
Precision 就像你可以推断出

2275
01:26:14,880 --> 01:26:18,420
某人的风险敏感性或者他们的

2276
01:26:18,420 --> 01:26:21,239
风险厌恶给定他们的行为

2277
01:26:21,239 --> 01:26:22,500
和好事 关于使用

2278
01:26:22,500 --> 01:26:24,060
概率编程语言，在 Matlab 文献

2279
01:26:24,060 --> 01:26:26,520
中有很多不同的方法

2280
01:26:26,520 --> 01:26:29,159
还没有被很好地探索来

2281
01:26:29,159 --> 01:26:30,719
拟合主动推理模型，

2282
01:26:30,719 --> 01:26:31,860
因为几乎

2283
01:26:31,860 --> 01:26:34,679
每个人都使用这种 um 变分

2284
01:26:34,679 --> 01:26:36,420
贝叶斯方法，你基本上

2285
01:26:36,420 --> 01:26:38,940
最小化自由能，你使用高斯

2286
01:26:38,940 --> 01:26:40,800
近似 对于后验，它是一种

2287
01:26:40,800 --> 01:26:42,480
非常特殊

2288
01:26:42,480 --> 01:26:45,060
的变分推理类型，现在

2289
01:26:45,060 --> 01:26:48,480
它又出现在 numpyro 或 Pi mt3 中，

2290
01:26:48,480 --> 01:26:50,520
这将很快实现，它还没有完全

2291
01:26:50,520 --> 01:26:52,560
实现，但我们将能够抛出

2292
01:26:52,560 --> 01:26:54,480
各种具有不同优势的概率

2293
01:26:54,480 --> 01:26:55,620
推理技术

2294
01:26:55,620 --> 01:26:57,780
缺点

2295
01:26:57,780 --> 01:27:00,300
是概率

2296
01:27:00,300 --> 01:27:04,620
推理中的一件大事是 um mcmc Monte C  arlo Markov

2297
01:27:04,620 --> 01:27:06,540
chain Monte Carlo 推理它

2298
01:27:06,540 --> 01:27:09,840
应该给出更少的偏见呃呃

2299
01:27:09,840 --> 01:27:11,280
后验分布就像

2300
01:27:11,280 --> 01:27:13,500
使用 mcmc 优于

2301
01:27:13,500 --> 01:27:16,260
变分方法来近似

2302
01:27:16,260 --> 01:27:18,360
基础和推理的优势以及

2303
01:27:18,360 --> 01:27:19,739
我没有看到完成的一件事我很想

2304
01:27:19,739 --> 01:27:21,780
看到和很多

2305
01:27:21,780 --> 01:27:23,280
计算精神病学社区的人们

2306
01:27:23,280 --> 01:27:25,320
已经抱怨并告诉我他们

2307
01:27:25,320 --> 01:27:27,120
希望看到的是

2308
01:27:27,120 --> 01:27:30,440
对不同天数的并排比较，以推断

2309
01:27:30,440 --> 01:27:33,840
Pi mdp 参数或 Palm DP 参数

2310
01:27:33,840 --> 01:27:37,199
与类似 mcmc 方法，所以一旦你

2311
01:27:37,199 --> 01:27:38,520
拥有了一切 一个概率

2312
01:27:38,520 --> 01:27:40,139
程序框架，你可以

2313
01:27:40,139 --> 01:27:42,360
在所有

2314
01:27:42,360 --> 01:27:44,159
不同的推理技术之间进行并排比较，

2315
01:27:44,159 --> 01:27:46,020


2316
01:27:46,020 --> 01:27:47,400
如果你受限于

2317
01:27:47,400 --> 01:27:49,460
只实现一种或两种或三种

2318
01:27:49,460 --> 01:27:52,139
推理技术的语言，那么

2319
01:27:52,139 --> 01:27:53,639
它基本上只是利用

2320
01:27:53,639 --> 01:27:55,340
人们在

2321
01:27:55,340 --> 01:27:57,780
numpyro 上所做的所有工作中，已经完成了所有

2322
01:27:57,780 --> 01:27:59,880
这些不同类型的推理方法

2323
01:27:59,880 --> 01:28:01,440
ods

2324
01:28:01,440 --> 01:28:04,440
yeah awesome and fausta followed

2325
01:28:04,440 --> 01:28:05,960
primary back end

2326
01:28:05,960 --> 01:28:11,040
uh for YMC is a Sarah 但新

2327
01:28:11,040 --> 01:28:13,620
版本可以使用 Jax 一旦 Jax 版本的 Pi mdp 稳定，我可能会

2328
01:28:13,620 --> 01:28:16,139
开始编写 pi MC3 包装器

2329
01:28:16,139 --> 01:28:18,420
，这

2330
01:28:18,420 --> 01:28:19,739
听起来非常

2331
01:28:19,739 --> 01:28:21,659
可行，太棒了

2332
01:28:21,659 --> 01:28:23,639
如果你认为它很有可能

2333
01:28:23,639 --> 01:28:26,880
并且你有能力提供，那么只需

2334
01:28:26,880 --> 01:28:28,860
最小化你的自由能量

2335
01:28:28,860 --> 01:28:30,719
，当你绝对这样做时你不会感到惊讶

2336
01:28:30,719 --> 01:28:31,980


2337
01:28:31,980 --> 01:28:34,260
那太棒了这很有希望

2338
01:28:34,260 --> 01:28:35,940


2339
01:28:35,940 --> 01:28:38,760
嗯雅各布或达芙妮或者我现在可以问另一个

2340
01:28:38,760 --> 01:28:41,120
问题

2341
01:28:43,560 --> 01:28:45,600
对我来说这件事

2342
01:28:45,600 --> 01:28:47,460
好吧，你

2343
01:28:47,460 --> 01:28:51,780
在 Pi MGP 的上下文中多次提到了消息传递，

2344
01:28:51,780 --> 01:28:55,080
那么什么是消息传递

2345
01:28:55,080 --> 01:28:58,820
以及它是如何在 pi mdp 中使用的，

2346
01:28:58,820 --> 01:29:02,040
这是一个很好的问题，所以消息

2347
01:29:02,040 --> 01:29:04,199
传递通常描述为一

2348
01:29:04,199 --> 01:29:07,860
组算法，你可以使用它们来做一个

2349
01:29:07,860 --> 01:29:11,219
精确的 或近似贝叶斯推理

2350
01:29:11,219 --> 01:29:13,679
um 通常在对隐藏状态进行贝叶斯推理的背景下使其变得

2351
01:29:13,679 --> 01:29:15,420
非常具体，

2352
01:29:15,420 --> 01:29:17,340


2353
01:29:17,340 --> 01:29:19,860
因此

2354
01:29:19,860 --> 01:29:21,179
主动推理代理

2355
01:29:21,179 --> 01:29:22,920
在 面对一些观察，

2356
01:29:22,920 --> 01:29:25,800
他们必须组合消息，例如

2357
01:29:25,800 --> 01:29:28,500
一个消息对应于

2358
01:29:28,500 --> 01:29:30,900
感官信息，然后另一个

2359
01:29:30,900 --> 01:29:32,340
消息对应于他们

2360
01:29:32,340 --> 01:29:34,320
对世界的先前信念，他们使用

2361
01:29:34,320 --> 01:29:36,440
某种算法来组合这些消息

2362
01:29:36,440 --> 01:29:39,179
以优化对当前的信念

2363
01:29:39,179 --> 01:29:42,120
世界的隐藏状态，所以在 pione P 中，

2364
01:29:42,120 --> 01:29:43,679
我们有一种非常朴素的

2365
01:29:43,679 --> 01:29:45,600
计算高效的

2366
01:29:45,600 --> 01:29:48,420
做事方式，我们称之为朴素或

2367
01:29:48,420 --> 01:29:50,460
香草定点迭代，这

2368
01:29:50,460 --> 01:29:52,920
就像你能想到的最简单的消息传递

2369
01:29:52,920 --> 01:29:54,420
方案，这就是

2370
01:29:54,420 --> 01:29:56,580
我

2371
01:29:56,580 --> 01:29:58,860
我使用我过去的先验主动过滤隐藏状态，所以我说

2372
01:29:58,860 --> 01:30:01,380
给定我在最后一个时间步骤的

2373
01:30:01,380 --> 01:30:03,780
位置，现在我应该在哪里得到我的 B 矩阵

2374
01:30:03,780 --> 01:30:06,060
，然后我基本上将

2375
01:30:06,060 --> 01:30:08,300
它与我

2376
01:30:08,300 --> 01:30:10,739
传入的感官信息结合起来，这

2377
01:30:10,739 --> 01:30:13,020
只是观察 通过

2378
01:30:13,020 --> 01:30:15,239
似然矩阵 a 矩阵，然后

2379
01:30:15,239 --> 01:30:16,440
我将这两个消息

2380
01:30:16,440 --> 01:30:19,080
组合在一起，结果就是我的

2381
01:30:19,080 --> 01:30:21,540
后验分布 考虑一下我

2382
01:30:21,540 --> 01:30:24,000
对隐藏状态的后验或最佳信念，这

2383
01:30:24,000 --> 01:30:25,679
就像最简单的消息传递形式，

2384
01:30:25,679 --> 01:30:27,840
它具有这种暂时非常

2385
01:30:27,840 --> 01:30:28,920
浅薄的

2386
01:30:28,920 --> 01:30:31,920
当前证据，并结合之前

2387
01:30:31,920 --> 01:30:33,600
形成新的信念，它具有

2388
01:30:33,600 --> 01:30:35,639
这种贝叶斯风格

2389
01:30:35,639 --> 01:30:37,620
，而最佳后验只是

2390
01:30:37,620 --> 01:30:40,739
可能性和先验的产物，

2391
01:30:40,739 --> 01:30:41,520
嗯

2392
01:30:41,520 --> 01:30:43,139
，还有更高级的消息

2393
01:30:43,139 --> 01:30:45,480
传递技术，当你的信念本身更复杂时，就会使用这些技术，

2394
01:30:45,480 --> 01:30:46,980


2395
01:30:46,980 --> 01:30:48,480
所以

2396
01:30:48,480 --> 01:30:49,980


2397
01:30:49,980 --> 01:30:52,199
在 Matlab 版本中的主动推理的全面实施中，代理人

2398
01:30:52,199 --> 01:30:53,820
不只是相信什么

2399
01:30:53,820 --> 01:30:56,179
当前隐藏状态是他们对未来所有隐藏状态和过去所有隐藏状态具有完全

2400
01:30:56,179 --> 01:31:00,239
预测和后预测类型的

2401
01:31:00,239 --> 01:31:03,000
um 张量或信念立方体

2402
01:31:03,000 --> 01:31:04,860


2403
01:31:04,860 --> 01:31:07,020
进一步

2404
01:31:07,020 --> 01:31:09,179
条件我

2405
01:31:09,179 --> 01:31:11,219
可能采取或可能采取的所有政策

2406
01:31:11,219 --> 01:31:12,960
采取过去，所以你有这个巨大的

2407
01:31:12,960 --> 01:31:15,900
信念张量呃，它将延伸

2408
01:31:15,900 --> 01:31:17,940
到未来和过去的隐藏状态，

2409
01:31:17,940 --> 01:31:21,060
呃更远 r 由策略分解，当

2410
01:31:21,060 --> 01:31:22,800
您有那种需要更新的信念时，

2411
01:31:22,800 --> 01:31:24,600
您必须使用更

2412
01:31:24,600 --> 01:31:26,580
复杂的消息传递技术

2413
01:31:26,580 --> 01:31:28,920
，其中一种称为边际

2414
01:31:28,920 --> 01:31:30,659
消息传递，有一种叫做

2415
01:31:30,659 --> 01:31:32,880
变分消息传递的东西，所有

2416
01:31:32,880 --> 01:31:34,199
这些不同的消息传递

2417
01:31:34,199 --> 01:31:35,760
技术都只是

2418
01:31:35,760 --> 01:31:37,800


2419
01:31:37,800 --> 01:31:40,320
在时间上向前和向后传递消息以及在

2420
01:31:40,320 --> 01:31:42,600
表征

2421
01:31:42,600 --> 01:31:44,159
隐藏状态的不同变量之间基本一致，我们称之为隐藏状态

2422
01:31:44,159 --> 01:31:45,500
因素

2423
01:31:45,500 --> 01:31:48,060
，并且消息传递

2424
01:31:48,060 --> 01:31:49,739
算法基本上仍然

2425
01:31:49,739 --> 01:31:52,380
相当于将感官信息

2426
01:31:52,380 --> 01:31:55,199
与先验信念相结合，但它们只是

2427
01:31:55,199 --> 01:31:57,360
有点

2428
01:31:57,360 --> 01:31:59,280
在过去的未来信念空间中有更复杂的轨迹，

2429
01:31:59,280 --> 01:32:02,400
我有一些

2430
01:32:02,400 --> 01:32:03,960
人比我能更好地解释这一点

2431
01:32:03,960 --> 01:32:05,940
我已经在 pi mdp 中实现了其中的一些，

2432
01:32:05,940 --> 01:32:09,179
但我会推荐人们参考

2433
01:32:09,179 --> 01:32:12,060
有一篇非常好的 um 论文

2434
01:32:12,060 --> 01:32:14,280
我想你前几天可能转发了它，

2435
01:32:14,280 --> 01:32:16,440
Jacob，它是关于

2436
01:32:16,440 --> 01:32:16,980
嗯，

2437
01:32:16,980 --> 01:32:19,620
它叫做 mea  n field

2438
01:32:19,620 --> 01:32:21,719
oh yeah

2439
01:32:21,719 --> 01:32:24,360
um the paper comparing the mean field

2440
01:32:24,360 --> 01:32:27,179
and that they approximation and the best

2441
01:32:27,179 --> 01:32:28,679
air approximation neuronal message

2442
01:32:28,679 --> 01:32:30,719
passing using mean Fields Bethy and

2443
01:32:30,719 --> 01:32:32,400
marginal approximation

2444
01:32:32,400 --> 01:32:36,960
par Markovich kibel and friston 2019

2445
01:32:36,960 --> 01:32:38,820
um 这是一篇我们中的一些人

2446
01:32:38,820 --> 01:32:42,179
一直在看的论文

2447
01:32:42,179 --> 01:32:44,340
不同的自由能泛函

2448
01:32:44,340 --> 01:32:46,380
在不同的近似值下看起来有何不同

2449
01:32:46,380 --> 01:32:50,340
，它可能会成为 2023 年我们真正深入研究的焦点论文，

2450
01:32:50,340 --> 01:32:54,420


2451
01:32:54,420 --> 01:32:57,800
因为很多这些 um

2452
01:32:57,800 --> 01:33:03,020
vintage 让我们说 2011 年到 2019 年的论文

2453
01:33:03,020 --> 01:33:06,360
现在包装和发展方向，

2454
01:33:06,360 --> 01:33:08,400
例如 Pi  mdp

2455
01:33:08,400 --> 01:33:09,690
正在促进

2456
01:33:09,690 --> 01:33:11,880
[Music]

2457
01:33:11,880 --> 01:33:16,139
这些方法的实际使用，并且

2458
01:33:16,139 --> 01:33:18,600
存在大量

2459
01:33:18,600 --> 01:33:22,560
的概念可能性，它提出了

2460
01:33:22,560 --> 01:33:27,560


2461
01:33:27,840 --> 01:33:32,880
与其他类型的连接相关的启发式令人兴奋的用例，

2462
01:33:32,880 --> 01:33:33,480
嗯，

2463
01:33:33,480 --> 01:33:36,000
正如你之前提到的那样

2464
01:33:36,000 --> 01:33:40,139
，Matlab 非常有可能将这些

2465
01:33:40,139 --> 01:33:43,440
类型的连接带入 最后一英里

2466
01:33:43,440 --> 01:33:46,340
，尤其

2467
01:33:46,340 --> 01:33:50,480
是更细化或模块化的

2468
01:33:50,480 --> 01:33:52,440
开发

2469
01:33:52,440 --> 01:33:54,960
是

2470
01:33:54,960 --> 01:33:58,800
在 t 的保护伞下 他的 SPM

2471
01:33:58,800 --> 01:34:00,840
vbx 阻止了它们

2472
01:34:00,840 --> 01:34:02,400


2473
01:34:02,400 --> 01:34:04,080


2474
01:34:04,080 --> 01:34:07,739
以真正的分布式开放科学或去中心

2475
01:34:07,739 --> 01:34:10,080
化科学方式进行有意义的共享

2476
01:34:10,080 --> 01:34:12,060
，所以这就是为什么我们当然

2477
01:34:12,060 --> 01:34:14,580
很高兴与

2478
01:34:14,580 --> 01:34:17,460
Prime DP 合作并在其基础上构建并更多地了解它，因为

2479
01:34:17,460 --> 01:34:20,280
这正是

2480
01:34:20,280 --> 01:34:23,639
主动推理代理

2481
01:34:23,639 --> 01:34:26,219
及其不同实现的

2482
01:34:26,219 --> 01:34:28,380
可组合性，它们将能够以

2483
01:34:28,380 --> 01:34:30,840
大规模分布式方式进行处理，有人可能会

2484
01:34:30,840 --> 01:34:33,000
指定一个非常有趣的矩阵，

2485
01:34:33,000 --> 01:34:34,860
其他人可能会指定一个

2486
01:34:34,860 --> 01:34:37,320
有趣的 B，其他人将把它们

2487
01:34:37,320 --> 01:34:39,300
链接在一起 一种新的

2488
01:34:39,300 --> 01:34:40,320
代理人，

2489
01:34:40,320 --> 01:34:42,540
其他人可以以不同的方式实现它

2490
01:34:42,540 --> 01:34:46,139
，所以它带来了一种

2491
01:34:46,139 --> 01:34:49,139
自然而然的感觉，它分解

2492
01:34:49,139 --> 01:34:51,420
了开发这些算法的过程，

2493
01:34:51,420 --> 01:34:55,139
这些算法以前几乎

2494
01:34:55,139 --> 01:34:57,719


2495
01:34:57,719 --> 01:35:00,000
总是完全是 Matlab

2496
01:35:00,000 --> 01:35:03,239
或和/或定制的

2497
01:35:03,239 --> 01:35:06,179
并且非常定制

2498
01:35:06,179 --> 01:35:09,300
并且适合给定的论文

2499
01:35:09,300 --> 01:35:13,320
但是 不一定沿着

2500
01:35:13,320 --> 01:35:15,840


2501
01:35:15,840 --> 01:35:18,360
人们想要用于现代

2502
01:35:18,360 --> 01:35:22,260
特别是 pythonic 设置的相关轴进行调整

2503
01:35:22,260 --> 01:35:24,480
完全我可以同意更多我的意思是，这也是一种

2504
01:35:24,480 --> 01:35:25,800
很好的思考方式，

2505
01:35:25,800 --> 01:35:28,500
就像我正在制作模块化的灵活代码

2506
01:35:28,500 --> 01:35:30,600
，这些代码存在于其他包的生态系统中，

2507
01:35:30,600 --> 01:35:33,260
你本质上是在

2508
01:35:33,260 --> 01:35:35,580
分解手头任务的集体思想

2509
01:35:35,580 --> 01:35:38,760
表示，

2510
01:35:38,760 --> 01:35:40,679
然后不同

2511
01:35:40,679 --> 01:35:42,420
可以处理该表示的某些部分，而

2512
01:35:42,420 --> 01:35:45,840
无需传递消息或考虑

2513
01:35:45,840 --> 01:35:47,520


2514
01:35:47,520 --> 01:35:50,159
整个分布式工作人员网络中发生的事情，因此

2515
01:35:50,159 --> 01:35:52,739
就像有人可以编写自己的

2516
01:35:52,739 --> 01:35:54,239
消息传递算法一样，您知道更好的消息传递

2517
01:35:54,239 --> 01:35:56,520
算法，然后将其插入

2518
01:35:56,520 --> 01:35:59,219
使用 pi MVP 而不必

2519
01:35:59,219 --> 01:36:01,320
了解 prime DP 的每个小方面是如何

2520
01:36:01,320 --> 01:36:04,320
工作的你知道所以是的这是一个

2521
01:36:04,320 --> 01:36:06,239
非常重要的我认为

2522
01:36:06,239 --> 01:36:08,820
关于开放科学和模块化软件

2523
01:36:08,820 --> 01:36:10,940
开发的事情

2524
01:36:10,940 --> 01:36:14,639
在我们的最后几分钟

2525
01:36:14,639 --> 01:36:17,940
当然很好 Jacob 或 Daphne 任何评论或

2526
01:36:17,940 --> 01:36:23,219
问题 还有任何开胃菜

2527
01:36:23,219 --> 01:36:26,540
人们对什么样的模型感到兴奋

2528
01:36:26,540 --> 01:36:31,020
，或者我们在

2529
01:36:31,020 --> 01:36:33,900
以下直播中看到的可能意味着什么 在

2530
01:36:33,900 --> 01:36:36,360
2023 年 1 月的

2531
01:36:36,360 --> 01:36:39,920
模型流 7.2 中，

2532
01:36:41,699 --> 01:36:43,320
我只想说，我

2533
01:36:43,320 --> 01:36:45,719
认为笔记本真的非常有用，

2534
01:36:45,719 --> 01:36:48,420
所以对于

2535
01:36:48,420 --> 01:36:50,100


2536
01:36:50,100 --> 01:36:52,020
那些试图构建和建模并

2537
01:36:52,020 --> 01:36:53,580
理解

2538
01:36:53,580 --> 01:36:56,460
原生 IMDb 和我正在发生的事情的人来说，它是一个非常好的资源 认为

2539
01:36:56,460 --> 01:36:58,980


2540
01:36:58,980 --> 01:37:01,139
对那些还

2541
01:37:01,139 --> 01:37:02,360
讨论

2542
01:37:02,360 --> 01:37:05,340
学习 A 和 B 矩阵的持续时间参数的笔记本进行扩展真的很酷

2543
01:37:05,340 --> 01:37:08,100
我认为

2544
01:37:08,100 --> 01:37:11,659
那真的

2545
01:37:13,320 --> 01:37:15,360
很酷非常感谢 Connor 你

2546
01:37:15,360 --> 01:37:17,219
想谈谈

2547
01:37:17,219 --> 01:37:19,020
即将到来，这是一个

2548
01:37:19,020 --> 01:37:20,100
非常好的观点，这

2549
01:37:20,100 --> 01:37:22,199
是 Daniel

2550
01:37:22,199 --> 01:37:24,000
早些时候在电子邮件中也说过的，更新

2551
01:37:24,000 --> 01:37:26,639
A 和 B 就像它现在的记录非常少

2552
01:37:26,639 --> 01:37:28,199


2553
01:37:28,199 --> 01:37:30,840
，我认为那是因为这

2554
01:37:30,840 --> 01:37:32,639
是学习生成的一种形式

2555
01:37:32,639 --> 01:37:33,960
模型

2556
01:37:33,960 --> 01:37:36,239
嗯，现在我们不这样做，这不是

2557
01:37:36,239 --> 01:37:37,679
最复杂的方式，您仍然拥有

2558
01:37:37,679 --> 01:37:39,300
固定数量的行和列，因此您

2559
01:37:39,300 --> 01:37:41,460
可以做出一些假设，但这就像一种

2560
01:37:41,460 --> 01:37:42,960
灵活的方式，当 智能体

2561
01:37:42,960 --> 01:37:45,239
自己正在学习 b 和 a 矩阵，

2562
01:37:45,239 --> 01:37:46,679
所以

2563
01:37:46,679 --> 01:37:48,540
嗯，是的，我们绝对应该也许

2564
01:37:48,540 --> 01:37:50,280
这实际上是

2565
01:37:50,280 --> 01:37:52,260


2566
01:37:52,260 --> 01:37:54,719


2567
01:37:54,719 --> 01:37:57,300


2568
01:37:57,300 --> 01:37:59,639
在

2569
01:37:59,639 --> 01:38:01,920
一些 A 或 B 中学习并

2570
01:38:01,920 --> 01:38:03,780
展示它是如何工作的，

2571
01:38:03,780 --> 01:38:06,120
嗯，我会制作一个新的笔记本，最终将

2572
01:38:06,120 --> 01:38:08,360
它

2573
01:38:08,460 --> 01:38:12,600
用于教科书

2574
01:38:12,600 --> 01:38:15,659
和每篇论文

2575
01:38:15,659 --> 01:38:19,679
，如果能够看到

2576
01:38:19,679 --> 01:38:21,239


2577
01:38:21,239 --> 01:38:24,719
代码、分析表示和

2578
01:38:24,719 --> 01:38:27,900
图形表示，那将是惊人的 和不同的

2579
01:38:27,900 --> 01:38:30,780
自然语言表示，

2580
01:38:30,780 --> 01:38:34,260
因为它们都是正式的，嗯，正式

2581
01:38:34,260 --> 01:38:37,080
连接的，它们都可以

2582
01:38:37,080 --> 01:38:40,080
这样呈现，人们可能真的

2583
01:38:40,080 --> 01:38:42,239
希望增加

2584
01:38:42,239 --> 01:38:45,900
模型的可访问性和严谨性

2585
01:38:45,900 --> 01:38:48,719
，帮助我们跨

2586
01:38:48,719 --> 01:38:51,060
不同领域进行组合和连接，欢迎和

2587
01:38:51,060 --> 01:38:52,679
认识许多不同种类的

2588
01:38:52,679 --> 01:38:55,760
学习和

2589
01:38:56,040 --> 01:38:57,840


2590
01:38:57,840 --> 01:38:59,580


2591
01:38:59,580 --> 01:39:02,540


2592
01:39:02,760 --> 01:39:05,880


2593
01:39:05,880 --> 01:39:09,060
建模 非常感谢精彩的

2594
01:39:09,060 --> 01:39:11,699
演示，我

2595
01:39:11,699 --> 01:39:14,820
对所有新兴的

2596
01:39:14,820 --> 01:39:16,920
um

2597
01:39:16,920 --> 01:39:19,739
集成和用例感到非常兴奋，这些集成和用例

2598
01:39:19,739 --> 01:39:22,679
无疑会涌现，我们

2599
01:39:22,679 --> 01:39:26,159
一直在，嗯，我们也开始探索 uh numpyro

2600
01:39:26,159 --> 01:39:30,300
，并讨论如何

2601
01:39:30,300 --> 01:39:33,360
um 那可以 用于可扩展的主动

2602
01:39:33,360 --> 01:39:36,659
推理模型，我真的很兴奋

2603
01:39:36,659 --> 01:39:40,199
primevp 将如何

2604
01:39:40,199 --> 01:39:41,540


2605
01:39:41,540 --> 01:39:44,420
与所有这些不同的集成进行互操作，使用

2606
01:39:44,420 --> 01:39:49,219
它会非常

2607
01:39:50,580 --> 01:39:54,320
令人兴奋，

2608
01:39:56,159 --> 01:39:58,940
谢谢

2609
01:39:58,980 --> 01:40:03,239
Connor 任何倒数第二个词，

2610
01:40:03,239 --> 01:40:05,820
我想也许只是

2611
01:40:05,820 --> 01:40:09,060
本着什么精神 你是说我可以展示

2612
01:40:09,060 --> 01:40:11,699


2613
01:40:11,699 --> 01:40:14,340
下一次我们可以完成的事情的框架哦，太好了，我们看到了，

2614
01:40:14,340 --> 01:40:16,860
嗯，是的，好吧，它看起来

2615
01:40:16,860 --> 01:40:18,480
很棒，看起来很棒

2616
01:40:18,480 --> 01:40:20,760
所以

2617
01:40:20,760 --> 01:40:22,920
是的，基本上这是一个合作笔记本，

2618
01:40:22,920 --> 01:40:26,280
所以我鼓励任何人去 到

2619
01:40:26,280 --> 01:40:28,380
um Pine DP 教程网站，

2620
01:40:28,380 --> 01:40:30,179
像 Daphne 这样的每个笔记本都说

2621
01:40:30,179 --> 01:40:32,699
他们有他们的 I'm pretty useful and

2622
01:40:32,699 --> 01:40:34,860
they have collab links associated with

2623
01:40:34,860 --> 01:40:36,239
them

2624
01:40:36,239 --> 01:40:38,040
um and you can just open the link and

2625
01:40:38,040 --> 01:40:41,520
then ex 探索这个我认为是

2626
01:40:41,520 --> 01:40:43,139
代理

2627
01:40:43,139 --> 01:40:46,500
API 的一部分是的这是同一个除了我

2628
01:40:46,500 --> 01:40:48,060
最近在

2629
01:40:48,060 --> 01:40:49,679
计算精神病学课程上展示了这个所以我

2630
01:40:49,679 --> 01:40:51,060
更新了一点

2631
01:40:51,060 --> 01:40:53,340
所以我可以和

2632
01:40:53,340 --> 01:40:55,980
你丹尼尔分享这个然后你可以 要么

2633
01:40:55,980 --> 01:40:58,080
放在 Discord 上，要么放在这个

2634
01:40:58,080 --> 01:40:59,460
稍微更新一点的地方，但

2635
01:40:59,460 --> 01:41:01,739
这里的基本内容仍然

2636
01:41:01,739 --> 01:41:03,900
存在，您可以在此处访问，仍然会

2637
01:41:03,900 --> 01:41:04,980
显示相同的内容，

2638
01:41:04,980 --> 01:41:07,560
但无论如何，问题是您只需

2639
01:41:07,560 --> 01:41:09,179
打开协作，您需要一个 Google 帐户

2640
01:41:09,179 --> 01:41:12,060
这是使用这些的一个限制，

2641
01:41:12,060 --> 01:41:14,820
嗯，你可以在本地安装基础设施

2642
01:41:14,820 --> 01:41:17,219
Dash piondp

2643
01:41:17,219 --> 01:41:19,980
uh import numpy matplotlib 这里只有

2644
01:41:19,980 --> 01:41:22,080
一些进口，然后这整个

2645
01:41:22,080 --> 01:41:23,880
这本笔记本的精神

2646
01:41:23,880 --> 01:41:25,380
基本上只是通过

2647
01:41:25,380 --> 01:41:27,480
设置生成模型的所有步骤

2648
01:41:27,480 --> 01:41:29,820
所以 创建你的隐藏状态因子

2649
01:41:29,820 --> 01:41:31,020
，这是我们没有真正讨论的

2650
01:41:31,020 --> 01:41:33,360
事情，是在

2651
01:41:33,360 --> 01:41:35,460
Pi mdp 的上下文中分解表示，

2652
01:41:35,460 --> 01:41:36,480


2653
01:41:36,480 --> 01:41:40,139
构建 B 数组，它

2654
01:41:40,139 --> 01:41:41,940
不仅有

2655
01:41:41,940 --> 01:41:43,679
um 就像你可以 ju  st 自己做，但

2656
01:41:43,679 --> 01:41:45,060
也有这些隐藏的单元格，其中包含

2657
01:41:45,060 --> 01:41:47,580
对这些事情的解决方案，

2658
01:41:47,580 --> 01:41:49,139
然后在运行主动推理之前，本笔记本的主要冲击

2659
01:41:49,139 --> 01:41:51,119
只是单

2660
01:41:51,119 --> 01:41:52,800
步执行并实际

2661
01:41:52,800 --> 01:41:54,480
初始化

2662
01:41:54,480 --> 01:41:58,320
um a b c 和 d 数组的条目

2663
01:41:58,320 --> 01:42:00,659
，下次我有 一些幻灯片

2664
01:42:00,659 --> 01:42:02,040
与此一起进行所以我们基本上可以

2665
01:42:02,040 --> 01:42:03,840
在幻灯片

2666
01:42:03,840 --> 01:42:06,360
和实际代码之间切换

2667
01:42:06,360 --> 01:42:08,699
与数组相同的东西所以你喜欢

2668
01:42:08,699 --> 01:42:11,520
有一些表示你

2669
01:42:11,520 --> 01:42:13,440
想要的矩阵看起来像什么然后

2670
01:42:13,440 --> 01:42:15,119
你进入代码和 实际上构建

2671
01:42:15,119 --> 01:42:17,460
它，然后依次为生成模型的每个组件执行此操作，

2672
01:42:17,460 --> 01:42:19,020


2673
01:42:19,020 --> 01:42:20,820


2674
01:42:20,820 --> 01:42:21,540
嗯

2675
01:42:21,540 --> 01:42:23,460
，你正在沿途绘制它们，

2676
01:42:23,460 --> 01:42:25,920
这样你就可以在构建它之后看到它的样子

2677
01:42:25,920 --> 01:42:29,280
然后我们实际上

2678
01:42:29,280 --> 01:42:31,560
呃我们实际上实现了 在你

2679
01:42:31,560 --> 01:42:32,699
构建了通用模型之后，你

2680
01:42:32,699 --> 01:42:34,080
实际上将它插入到一个活跃的

2681
01:42:34,080 --> 01:42:35,340
推理代理中，

2682
01:42:35,340 --> 01:42:38,280
所以这第一件事就是

2683
01:42:38,280 --> 01:42:41,580
为一个通用的 um General 构建一个分解的 a 和 b

2684
01:42:41,580 --> 01:42:43,440
，我认为它只是一个 更

2685
01:42:43,440 --> 01:42:45,239
复杂的网格世界，但我们

2686
01:42:45,239 --> 01:42:47,100
实际上这是一种介绍性

2687
01:42:47,100 --> 01:42:48,600
任务，然后我们进入，我们实际上

2688
01:42:48,600 --> 01:42:50,940
为这个认知双臂带状任务构建 A、B 和 C，这

2689
01:42:50,940 --> 01:42:53,219


2690
01:42:53,219 --> 01:42:55,560
基本上就像队友一样

2691
01:42:55,560 --> 01:42:56,460


2692
01:42:56,460 --> 01:43:00,300
，然后你构建 你知道

2693
01:43:00,300 --> 01:43:02,520
你正在写

2694
01:43:02,520 --> 01:43:04,679
你的矩阵的所有小子矩阵，

2695
01:43:04,679 --> 01:43:06,420
这就是为什么有这么多单元格，就像我

2696
01:43:06,420 --> 01:43:07,739
说的那样，这是最长的部分，

2697
01:43:07,739 --> 01:43:09,950
它实际上是在构建东西

2698
01:43:09,950 --> 01:43:10,500
[音乐]

2699
01:43:10,500 --> 01:43:11,100


2700
01:43:11,100 --> 01:43:13,619
嗯，C 向量基本上是奖励

2701
01:43:13,619 --> 01:43:15,300
函数，它 Daphne 之前说过

2702
01:43:15,300 --> 01:43:16,800
你实际上是

2703
01:43:16,800 --> 01:43:20,100
根据相对对数概率进行编码

2704
01:43:20,100 --> 01:43:23,040
然后最后你基本上执行

2705
01:43:23,040 --> 01:43:25,320
了我们在演示期间讨论的那些步骤

2706
01:43:25,320 --> 01:43:27,980
你只需将你

2707
01:43:27,980 --> 01:43:30,360
精心生成的 A 和 B

2708
01:43:30,360 --> 01:43:32,639
插入你的

2709
01:43:32,639 --> 01:43:35,340
代理类中你希望不要太费力 生成两个武装

2710
01:43:35,340 --> 01:43:37,980
强盗认知到武装强盗呃

2711
01:43:37,980 --> 01:43:39,840
环境这只是

2712
01:43:39,840 --> 01:43:42,719
关于世界如何运作的规则给定

2713
01:43:42,719 --> 01:43:45,360
代理人的行为和 th 然后你实际上

2714
01:43:45,360 --> 01:43:48,480
运行了这个主动推理循环，

2715
01:43:48,480 --> 01:43:51,900
正如我们所讨论的那样，

2716
01:43:51,900 --> 01:43:53,639
随着时间的推移有效地运行一个循环，进行隐藏状态

2717
01:43:53,639 --> 01:43:55,739
影响策略推理动作

2718
01:43:55,739 --> 01:43:58,440
采样，然后逐步进入

2719
01:43:58,440 --> 01:44:00,480
环境以获得新的观察结果

2720
01:44:00,480 --> 01:44:02,159
，最后我只是这样写的

2721
01:44:02,159 --> 01:44:03,659
辅助函数，

2722
01:44:03,659 --> 01:44:05,699
基本上可以绘制出选择和信念的历史，

2723
01:44:05,699 --> 01:44:07,440


2724
01:44:07,440 --> 01:44:09,300
嗯，所以你在最后这

2725
01:44:09,300 --> 01:44:11,280
就像一个更有趣的实验部分，

2726
01:44:11,280 --> 01:44:14,280
你可以弄乱

2727
01:44:14,280 --> 01:44:16,739
呃环境的参数

2728
01:44:16,739 --> 01:44:18,420
以及代理模型的参数

2729
01:44:18,420 --> 01:44:20,760
，然后开始 看看它是如何改变

2730
01:44:20,760 --> 01:44:22,260
行为的，

2731
01:44:22,260 --> 01:44:24,000
嗯，只是通过一种迭代运行的

2732
01:44:24,000 --> 01:44:25,380
主动推理模拟，并

2733
01:44:25,380 --> 01:44:28,020
绘制出所产生的选择行为

2734
01:44:28,020 --> 01:44:29,699
和信念历史，

2735
01:44:29,699 --> 01:44:31,320
所以这就是一般情况，这是一个小

2736
01:44:31,320 --> 01:44:33,480
预览，我猜我们可以做什么，我们

2737
01:44:33,480 --> 01:44:35,040
也可以在其中有一个小子模块

2738
01:44:35,040 --> 01:44:36,780
在这里，我们实际上是让代理人

2739
01:44:36,780 --> 01:44:38,460
更新他们对 a 或 B 矩阵的信念，

2740
01:44:38,460 --> 01:44:42,500
这可能很酷，

2741
01:44:42,900 --> 01:44:46,260
很棒，看起来真的很令人兴奋

2742
01:44:46,260 --> 01:44:49,500


2743
01:44:49,500 --> 01:44:52,800
在 SPM 教科书和实验中的最终 SPM 笔记上，

2744
01:44:52,800 --> 01:44:54,840
有时会有这些令人难以置信的

2745
01:44:54,840 --> 01:44:59,159
灰度矩阵，它们总结

2746
01:44:59,159 --> 01:45:02,699
了 100 名参与者的多个实验因素

2747
01:45:02,699 --> 01:45:06,900
，呃所以

2748
01:45:06,900 --> 01:45:09,360
看到

2749
01:45:09,360 --> 01:45:13,500
你如何用黑白

2750
01:45:13,500 --> 01:45:18,300
或灰度矩阵表示来展示真的很有趣

2751
01:45:18,300 --> 01:45:22,800
以及这如何

2752
01:45:22,800 --> 01:45:25,739
为我们一直在讨论的其中一些主题提供视觉感受

2753
01:45:25,739 --> 01:45:27,960
，

2754
01:45:27,960 --> 01:45:30,600
当然，表示

2755
01:45:30,600 --> 01:45:33,420
形式与矩阵正式相关，但

2756
01:45:33,420 --> 01:45:36,060
有时只是说你有两个

2757
01:45:36,060 --> 01:45:37,800
选择，世界上有十​​个州

2758
01:45:37,800 --> 01:45:40,560
，可能性看起来 像这样，

2759
01:45:40,560 --> 01:45:42,119
而不是像在灰度中

2760
01:45:42,119 --> 01:45:45,300
看到带有数字的电子表格

2761
01:45:45,300 --> 01:45:49,040


2762
01:45:49,040 --> 01:45:52,560
那样，提供了一种感觉

2763
01:45:52,560 --> 01:45:55,199
，它看起来非常好，所以它看起来

2764
01:45:55,199 --> 01:45:56,639
像一个很棒的

2765
01:45:56,639 --> 01:45:59,100
会议，我们将为 DOT Two 举办

2766
01:45:59,100 --> 01:46:01,260
，是的，你带来的很有趣

2767
01:46:01,260 --> 01:46:03,360
就像我

2768
01:46:03,360 --> 01:46:05,580
一直在做的那样，我认为这在

2769
01:46:05,580 --> 01:46:07,679
很大程度上是因为

2770
01:46:07,679 --> 01:46:10,320
我从阅读中学到了所有这些 软管

2771
01:46:10,320 --> 01:46:13,619
主动推理和 SPM 论文

2772
01:46:13,619 --> 01:46:15,480
嗯所以我非常只是

2773
01:46:15,480 --> 01:46:18,659
从他们那里借用了可视化技术

2774
01:46:18,659 --> 01:46:20,400
嗯我有点认为这是理所当然的

2775
01:46:20,400 --> 01:46:22,139
但是很有趣它显然

2776
01:46:22,139 --> 01:46:24,600
不是唯一的方法但我总是

2777
01:46:24,600 --> 01:46:25,980
发现它非常直观 想想

2778
01:46:25,980 --> 01:46:28,560
概率，你可以给

2779
01:46:28,560 --> 01:46:30,420
它上色，因为数字

2780
01:46:30,420 --> 01:46:32,580
太具体了，它是颜色

2781
01:46:32,580 --> 01:46:34,199
，视觉方面真正喜欢的灰度，

2782
01:46:34,199 --> 01:46:35,940
只是表明这个东西

2783
01:46:35,940 --> 01:46:37,860
比这个东西更有可能

2784
01:46:37,860 --> 01:46:40,080
是的，

2785
01:46:40,080 --> 01:46:42,860
很酷，好吧，

2786
01:46:42,860 --> 01:46:46,020
达芙妮和康纳 非常感谢这次

2787
01:46:46,020 --> 01:46:48,179
精彩的会议，

2788
01:46:48,179 --> 01:46:51,480
我们将在一个多月后与您见面，

2789
01:46:51,480 --> 01:46:54,000
因为第二点

2790
01:46:54,000 --> 01:46:55,560
非常感谢丹尼尔，

2791
01:46:55,560 --> 01:46:57,060
谢谢大家

2792
01:46:57,060 --> 01:46:58,260
和平

2793
01:46:58,260 --> 01:47:01,820
保重再见谢谢大家


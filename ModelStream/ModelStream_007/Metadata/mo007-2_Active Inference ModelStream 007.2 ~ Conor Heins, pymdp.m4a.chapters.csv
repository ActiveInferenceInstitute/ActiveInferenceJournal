start	end	startTime	summary	headline	gist
5390	43214	00:05	We're back in our second session on pymdp. Welcome back, Conor. Thanks for joining again. And off to you for a presentation that you will weave together with some some code examples.	Daniel: Conor, welcome back for second session on pymdp	pymdp with Python code examples
43332	201870	00:43	This is the second model stream in a two part series where we'll be discussing implementing active inference with PMDP. Today we'll use it to code up an active inference agent performing a task. If anyone who's viewing wants to do it themselves, you can open the Co Lab notebook.	This is the second model stream in a two part series discussing active inference	How to code an Active Inference agent with pymdp
202530	493280	03:22	Under active inference, we typically have four main components. These components kind of represents the agent's beliefs about the world that it's operating in. We don't have to worry about writing out our own functions that do inference or do planning. All of that will be handled by the abstractions provided by prime VP.	Under active inference, we typically have these four main components	pymdp: Active Inference and generative models
495330	940910	08:15	For almost all the most interesting palmdp models you're going to want to build, it's going to be really crucial to factorize your system in some way. It's often useful to categorically separate observations into different modalities and hidden states into different factors.	Factorization in both observations and hidden states is crucial for most palmdp models	Factorized State Spaces
941060	1709440	15:41	PMDP is a powerful tool for creating multidimensional probability models. For every hidden state factor, there's also going to be a control state factor. Understanding how these arrays are structured is important to getting things working successfully.	So now we'll go over to Colab. So you've installed interactively PMDP, that's the whole name	Powered by pymdp
1710130	1859652	28:30	Just like for hidden states, we can factorize and we don't have to fully enumerate our observation space. So we could separate the grid GPS observations into an X and a Y GPS. Let's open it up for questions or comments if anything was unclear.	We can factorize and we don't have to fully enumerate our observation space	Multifactor multimodality factorized representations
1859716	2263604	30:59	I had a question on initializing the B matrix. Would there be any particular use case in the grid world where we would want toinitialize it with a sequence of policies? And is that needed if we want to do planning of different temporal depth?	I had a question on initializing the B matrix with a sequence of policies	Initializing the B matrix under pymdp
2263652	2460520	37:43	Another potential way would be to condense or kind of permute those two time steps into a single variable. The classical way to handle that is just to build a hierarchical model. anytime you're trying to imbue a system with more complex non Markovian memory, that's when it's the natural move.	Another potential way to do two times step model is to condense variables	Markovian 2-Step Algebra
2462060	2678830	41:02	The way the generative model is set up right now is like a bipartite graph that resembles a classic model for machine learning called like a restricted Bellman machine. In theory, but as currently implemented, there could be feedback loops between the hidden states. Is that separate from the Markovian assumption?	Conor: You suggest that there could be feedback loops between hidden states	Inference with a non-Markovian model
2679360	2864472	44:39	The breadth basically the number of conditional independent factors. To maximize the marginal likelihood of the evidence for your generative model, you have to carve the nature out there at its joints in the right kind of way. The right factorization will minimize the variation of free energy.	A graphical model can only have a number of different attributes	Generative models: How deep is it?
2864606	3227180	47:44	The first question about the policies, that was an interesting one. Pi is actually quite a complicated variable. There's no point in adding depth to a generative model unless you are mindful it is introducing a separation temporal time skills.	Adam Hohwy: First question about the policies is interesting	On the Nature of Generative Models
3228880	3474936	53:48	A multi factor generative model for a multi arm bandit or contextual multi armed bandit. Based on gambling task that was introduced in step by step tutorial and active inference. Agent can choose to play the left or the right machine or it can ask for a hint. Action selection comes down to choosing the state of the choice control factor.	Anthony G. Chen: The multi factor gambling task is based on reinforcement learning	The contextual 2-armed bandit
3475048	3591520	57:55	So now we're going to look at the different levels of the hidden states factors. The choice hidden state factor has four levels which correspond to the four possible choice states that the agent can be in. Action selection and planning boils down to choosing what level of you choice.	So now we're going to look at different levels of the hidden states factors	Choice Control State Factors and Hidden States Factors
3592340	4193200	59:52	The first modality is the hint modality which can either give the outcomes of null like I'm not at the hint state so I'm getting no sensory information from that channel. Then we have a reward modality that similarly has a null observation for when the agent isn't playing either slot machine and then two possible reward observations. Finally we have the choice modality, which just allows the agent to infer what it's doing.	Let's review the observation modalities and their different levels	Inferring a hint from the SPM
4194980	4820584	1:09:54	The context hidden states factor is stationary over time so they can't change that hidden state factor. We can encode the C vector, which is kind of like the reward function, the prior preferences or prior over observations directly in terms of relative log probabilities. This makes it easier to make one thing x times more rewarding or punish or aversive than another thing.	The hidden states factor is stationary over time so they can't change it	Reinforcement Learning 2, The B arrays
4820622	5272350	1:20:20	In time DP, there's also modules that are adjacent to the agent class. How we. Could make use of the different modules in the action perception loop. I have another demo that we can maybe do another time that's building a hierarchical model.	Could make use of the different modules in the action perception loop	Inferring with the Learning Module in pymdp
5276240	5323990	1:27:56	Okay, so that pretty much wraps up the main brunt of the entorhinal. I have additional material about learning parameters that we discussed last time. Let's do learning and then we'll have and closing questions at the end.	Let's do learning and then we'll have and closing questions at the end	Learning
5326760	6084356	1:28:46	Learning under active inference is cast as updating beliefs about the generative model parameters itself. Inference is saying, what is the best explanation in the sense of posterior over hidden states? And you get that by minimizing variational free energy.	Learning under active inference is cast as updating beliefs about generative model parameters	Inference under Active Inference and learning
6084378	6835344	1:41:24	In PMDP, you can say I only want to learn particular modalities. The learning rate, which is how much they increment their beliefs about the posterior or about the A matrix using this update rule. There's often a learning rate that's added here that basically scales how big this update is.	Sometimes we want certain contingencies to be unavailable to learning	Inference with Learning in pymdp
6835392	7017240	1:53:55	 Active inference is not in general, resolving Explore exploit. In many cases, it learns the wrong one. This really shows the space that we build in and what remains to be built is so open.	Pymdp and active inference are providing us a space for generative model construction	Enactive Inference and the Future of Probability
7021470	7461880	1:57:01	Yakub: What I found particularly interesting was how it could be modified with the action perception loop. And I would be interested to see how that also influences planning if you would do kind of pseudo update of your dirsh lay parameters when you're calculating your expected free energy. I think super interesting and really looking forward to hopefully another model stream.	Yakub: Conor's presentation on sophisticated planning was very interesting	Inverse inference and sophisticated planning
7462490	7652660	2:04:22	First area is structure learning on cognitive models from the outside. Second more experimental area is statistical power analysis. How long should these experiments be? There are interesting results with one shot learning.	Dot three is structure learning on cognitive models from the outside	A Dot-Three session?
7653350	7683750	2:07:33	End seven, two. And whenever you want to join, you and any colleagues are always welcome to share the next spiral in Pi MDP development. Thank you so much again for letting me come on and listening. I hope it was helpful.	End seven, two. Awesome. I hope it was helpful	Wrapping up pymdp 007.2

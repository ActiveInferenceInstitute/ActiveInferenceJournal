start	end	speaker	confidence	text
5390	34380	A	0.9584338461538461	Hello and welcome. This is model stream number 7.2. It's the 18 January 2023. We're back in our second session on Pi MDP. Welcome back, Conor. Thanks for joining again. And off to you for a presentation that you will weave together with some some code examples. And the notebook that we'll use is in the video description. So thanks again, Conor. Off to you.
35710	1847510	B	0.9444507890839813	Great. Thank you, Daniel. It's great to be back here again. I think I've been here maybe four or five times now. So it's always a pleasure to be with you all. So, yeah, as Daniel said, this is the second model stream in a two part series where we'll be discussing implementing active inference with PMDP. There is some background that I'm assuming you either know from your own research into active inference and palm DPS in particular, or because you've watched the first part of the series. So I'll just say that to start with. Like, if you want background, I would encourage you to go watch the first part where we talk about PMDP, the motivation for the package, what you can do with it in kind of sweeping terms. And then today we're actually going to go in and use it to code up an active inference agent performing a task, like a reinforcement learning style task that's very similar to classic active inference tasks that you can find in the literature. Yeah. So as Daniel said, I'm going to step through the slides. I have pauses for questions and stuff, but we also have pauses for code live coding or side by side coding. So I'll take a pause from the slides and I'll go over to the Collab notebook. The link is in the description of the YouTube video, and we'll kind of code up this agent based on things that we discuss in the slides. It's not that I'll be actually writing code. I'll just be running cell blocks. If anyone who's viewing wants to do it themselves, you can open the Co Lab notebook. Actually, let's start by doing that. So this should be the Collab notebook that you get brought to when you click on the link. And this is one caveat. You need to have a Google account to use this because co op is linked to your Google drive of your Google account. So if you're doing this on your own, this is a shared link. But it's not you can't edit it. You can't write to it. So what you have to do is go to file up here and then save a copy and drive. And then you'll get your own personal, private copy of the notebook. And I'll do that right now because I'll create a run copy where we can manipulate it all together in real time. So I'll just call it run. Copy. And I'll start by just installing Hinton, this colab environment. Pi MDP. So Colab is kind of like Jupiter's notebooks. It's an interactive cloud hosted notebook for running different kinds of code, but mainly Python code. So while that's installing, and I'll run that as well, we can start with the slides. Okay, so I'll just start with a quick review from last time, and again, go back to the video if you want to see the full picture of what we discussed. So the generative models, the kind of brains of the agents that will be constructing are these partially observed Markov decision processes or palm DPS. They have various parameters, and it's kind of up to you how to parameterize them. But under active inference, we typically have these four main components. There's fifth and 6th, depending on how complicated of a model you want to create. But these components kind of represents the agent's beliefs about the world that it's operating in. So these are usually hand designed or at least generally structured on a case by case basis, encoding to the behavioral task that you're trying to model. So we have the A matrix or A array, which encodes the agent's beliefs about how hidden states of the world that it's doing coherence about relate to the observations that it gets. You have the B array, which encoding beliefs about transitions or beliefs about dynamics. So how do hidden states relate to each other over time? You have the C vector or C array, which encodes agents prior preferences for seeing certain sorts of sensations. So this kind of plays the role of a quasi reward function, and it has the effect of biasing agents action selection such that it's more likely to visit states of affairs that a priority, expects to see I e. Goal directed behavior. It wants to achieve some goal. And then we have the D vector, which is kind of a baseline prior about what is the state of the world before I get any observations, like what do I believe the hidden state distribution is? And again, I'm glossing over this because I assume that you know about things like categorical distinctions. All the details of these different arrays are explained more in the first video. And just again, to show the graphical model, we have the A array relating hidden states in red to observations in Bleu. We have the B matrix or B array that relate hidden states to themselves over time. So how does the world chance? And then we have policies Pi, which are sequences or collections of actions that affect transitions and thereby change the state of the world. And then you have this critical quasi pseudovalue function to active inference in palmdp's. Active inference in general, which is the expected free energy. And the expected free energy is the thing you try to minimize as you optimize your beliefs about policies. So the best policies are those that minimize expected free energy and that has a bunch of interesting terms that lend active inference all of its interesting, curious, information seeking behavior. And then this prior over observations factors into the computation of that expected free energy. So if you have a particular prior preference to see some observations that will affect which policies get more or less expected free energy, and then you use that to actually choose a policy. And then of course, we have the D vector, which parameterizes the initial beliefs about his and states. So today we're going to actually dive in and write out our own ABCs and DS for particular generative model. We don't have to worry about writing out our own functions that do inference or do planning. All of that will be handled by the abstractions provided by prime VP, particularly in the agent class. So the agent class takes the components of the generative model. It gives you this very kind of black box API that you can just use to perform active inference. In the last video, we talked about different levels of abstraction. This is like the top level of abstraction in Pi MDP, where you're just passing a generative model to an agent and then kind of pressing go on the agent having to interact with the task environment. So typically this is another class, like an environment class or a task class. And then you just kind of string the two together in an action perception loop and you get a simulation of behavior under active inference. And this is a standard thing you'll see in like OpenAI gym or other reinforcement learning packages. You'll have usually like an environment class and an agent class, and they exchange actions and observations in a loop. So we designed the API of Pymdp, very much inspired by that. Okay, so before we start writing down the generative model so that A-B-C and D, I want to spend a little bit of time exploring the idea of factorized state spaces. And this is important because not only the model we're working today, it's important to understand factorization in both observations and hidden states, but for almost all the most interesting palmdp models you're going to want to build, it's going to be really crucial to factorize your system in some way. Not only because it becomes easier to reason about and work with when things are factorized, but also it handles a lot of computational explosions that happen if you don't factorize your state space. So it's often useful to categorically separate observations into different modalities and hidden states into different factors. So what this means is that when we get an observation, we're actually getting a collection of different observations, one coming from each distinct sensory modality or channel. So similarly for hidden states, at any given time, the hidden states are described by a distinct set of different features, or what we call hidden state factors. And so we refer to this as a factorized representation because the different modalities are independent of each other, and the different hidden states are also independent of each other. Their dynamics are so they evolve in time without affecting the other hidden state factors. So that's formally written down as follows. Like the modalities are conditionally independent of each other. Given the hidden states, that's what this factorization of the likelihood looks like. So you can think of the different modalities as different sensory channels like vision, audition, somata sensation, and they provide their own distinct type of information which are then integrated together during coherence. And factorizing the hidden states is like the idea that any given time the world can be described by independent features, like an object being described by both its location and space, as well as its identity and factorizing. The state's representations of the model in this way allows for a few different distinct advantages. One is computational efficiency, both in terms of memory and CPU. It simplifies the structure of generative model itself, so it lends it a bit of interpretability and transparency. So there's only certain sets of variables that represent the agent's beliefs about one feature of the environment, for instance. And then finally, some would argue it has a degree of neuronal or biological plausibility. And it's consistent with the idea of factorized what others would call modular representations in the brain. So this group of neuronal populations deals with representing where something is in space, and this group of neuronal populations deals with representing the identity or what the thing is. So that's kind of the reason we do this factorization. So one of the most classic examples of the early examples we do in Pinevp, and this can be found on the documentation and on the main page is this grid world example where the agent is kind of navigating in a 2D grid world space. In the notebooks online, we do like three by three grid world. So there's nine total locations. And in a fully unfactorized or what we call an enumerated state space representation, every location in grid world has its own particular identity or state. So we just arbitrary can label them like location zero all the way to location eight. And that covers all nine levels of gridworld. This is fine, but especially for a small state space like a three by three grid world. But it kind of taxes our memory and our interpretability a little bit because we have to remember exactly how our nine indices map to the different places in grid world. So if I'm in the middle of the grid, I need to have some kind of look up table to say, okay, location four is like zero or one comma one I e, the middle location in the grid. In a factorized representation, the grid could be represented as two orthogonal or independent hidden state factors. One is something like the x displacement or the horizontal displacement, which now just takes three values like which column of grid world are you in? And then vertical displacement or y position, which is something like which row of grid world M-I-N. And so now each hidden state is not a single nine dimensional vector, but it's actually a combination or a pair of two hidden states factors. So two, three vectors instead of a single nine vector. And then a given hidden states itself is picked out by a coincidence of these two hidden states factors. So if the x position is one and the y position is two, then it means I'm in one comma two. So like the middle bottom up the grid world. And then once you do this, so once we have a factorized hidden state representation, it has implication for the way we construct the generative model. So in a kind of simple generative model, you would just have one B array that in the original grid world example just is like nine possible hidden states, the nine possible locations of where I just was that maps to the nine possible subsequent hidden states. So the nine possible locations I could be going to. And then you'd have that nine by nine matrix conditioned on the different actions. So in the grid world demo, the actions are typically local like move up, move left, move down, move right. But once you factorize things, you're going to actually now have a collection of B matrices. And this is a consequence of the factorization across the transition models that we discussed a few slides ago. So you're going to have one B matrix or one set of B arrays for the X displacement. So how does the agent move in the X direction? These are what each of these X displacement B matrices look like, cognition on different actions. So this is what it looks like for moving left, this is what the B matrix looks like for moving right. And this is what it looks like for stay. And then similarly you have another B matrix that encodes the agent's abilities to move in the vertical direction or y displacement. And similarly they actually look identical because the world is like rotation and translation and variant and you'll have another smaller B array just for y movement. This is a very simple example. We're not going to actually do this in code because it's kind of trivial, but it's just an example of how instead of having a location fully enumerated representation, you can split that up into basically a y axis and an X axis. So now we're not going to simulate an active inference agent, but we're just going to play around with these data structures to get you used to the idea of factorized representations and how that manifests in the code. So now we'll go over to Colab. So if you've been running this on your own, disk should all be downloaded. So you've installed interactively PMDP, that's the whole name of the package that's now in your environment. And then you should also run this some basic imports like NumPy and pimedp, which is now in the environment. Okay? And so the basic variables that you'll see are things like there's a few variables that are almost always part of any time DP workflow. And this is very much mirrored on how it's done in SPM in the Dem tool box. So typically you'll have a list that specifies the dimensionalities or the number of different levels of each hidden state factor. So for our three by three grid world nine possible states, we have one x factor of three levels and one y factor of three levels. And then the length of that list tells us the total number of hidden state factors. So in this case, we have two hidden state factors. And then similarly you have hidden states are factorized, and then similarly you factorize your control states. So for every hidden state factor, there's also going to be a control state factor. And the fact that these are also three dimension each just basically means that the agent has three possible action in the x axis and three possible actions in the y axis. And then the number of control factors is also going to be two in that case. And so we have some nice utility functions that can quickly build a brain given the two things that are necessary, the numb states and NUM controls variables. So you just do initialize empty B NUM states and NUM controls, and then you'll have a B matrix that has the correct shapes and stuff. So three rows, three columns, and three possible actions for the x direction and similarly for the y direction. So this is just basic setting up your variables. And often we'll have these solution cells that are hidden underneath each little quasiexercise, so that's something you can do as well. Okay? And so that's just initializing and empty array, right? So we don't actually know what those contingencies are that we saw in the slides, but if we want to populate this thing, so this is what each factor specific V array looks like, then we have to go into these matrices and actually fill out the rows, columns and third order slices with numbers. So for instance, this loop where we loop over factors and we fill out the different entries of the B array. What this will do is just encode those local actions like move left, move right for the x factor, and then move up, move down for the y factor. And so once you've run that code, you've basically filled out all the numbers in that big empty B array that you started. And then you can use things like the plot likelihood function to plot these likelihood distributions, action conditioned likelihood distributions as matrices. So this is move left. For instance, if we're in the x hidden state factor, and if we change this to a one, we've changed the action index, then this is move right, and then similarly if we change this, this is move down if we're doing action one here. So darker colors, whether it's grayscale or red, or whatever, almost unanimously in prime DP, the darker colors represent more probability. So instead of having to display numbers, we'll just show darker numbers. Okay, so now let's go back to the slides and talk about the A arrays. So in our unfactorized, fully enumerated grid world example, and again, I'll refer you to the documentation of prime VP if you want to play around with that example. The hidden state factors, which are just these nine kind of Arbitrarily labeled locations map to an observation modality, which is like the agent's sensation of its own position, kind of appropriate GPS modality. You could think of it as like they're sensing where they are at any given time. And so there are various ways you could encode that. But let's just pretend that they have a very high fidelity GPS, so they're getting precise GPS readouts of where they are at any given time. So in a fully enumerated world, that a matrix, that likelihood modality would look like this. When we start factorizing things or splitting up the hidden state factors into an X factor and a Y factor, that choice not only manifests in the B arrays, but it also manifests in the structure of the arrays. And I think this is one of the most key things to understand, because when it comes to coding these things up yourself, understanding how these arrays are structured is really important to getting things working successfully, basically getting used to multidimensional indexing. So briefly, in summary, when you have multiple hidden state factors that manifests as extra Lagging dimensions or extra higher order dimensions in your, in your A array, so it's not really an A matrix anymore, it's an A tensor. So that means if you have two hidden state factors, the number of Lagging dimensions, ie. Excluding rows, that's what I mean when I say Lagging dimensions will be two. So you have columns and slices in addition to rows. So this is really important to understand when either using SPM or Pymdp to do palm DP based active inference. And another way of saying this is for each additional random variable that we're conditioning our observations on, we have an additional Lagging dimension in our array. So this is the same principle as with the B matrices because we condition the next state both on the past state and the past action. That's why the B arrays are also tensors. So we have two extra dimensions in the B array, one for each conditioning variable, past state and past action. So now in the X and Y factorized grid world, we have two Lagging dimensions where the first Lagging dimension, the columns in Bleu, index a particular X location, and the second Lagging dimension, the slices, if you will, index a particular Y location. So now what I'm going to do is unwrap these slices of the multi dimension A matrix. And so you can see that each slice of the A matrix, so that each purple indexed third order slice corresponds to conditioning on a particular setting of Y. So a particular conditional distribution over observations for the three settings of X for a fixed value of Y. So each slice is exactly this conditional distribution, where we're looking at the conditional distributions over observations for each setting of X. So those are the columns indexed by the Bleu numbers at the bottom, but we're fixing to one value of Y. So that's the conditional distribution over observations zero, one and two. So being in the top row, given that we're in the first row, y equals zero for the three possible settings of X. And then we move down to the next set of observations and the next slice of Y, and it looks like that and then finally like that. So this is the case where our GPS is noiseless and high fidelity, right? That's why we have like hyper dark cells and then everything else is white, is because the agent believes that its observations are high fidelity signals of the actual hidden state, namely the X comma y location. So then when we pat these back together, we see how this full a matrix or set of conditional distributions will be represented by a nine by three by three tensor, where again, each Lagrange dimension exists to encode the conditional dependencies between that particular hidden state factor indexed by the Lagging dimension and the observations for this modality. So when I index into the Ith column and the Jth slice of this a matrix or a tensor, I'm basically slicing out a particular conditional distribution over observations where I'm deciding what settings of the conditional distributions or the conditional random variables I'm interested in. Okay? So yeah, that's a kind of critical concept and to just make it more tangible and clear. And to be clear, this isn't the only way to encode conditional dependencies in categorical distributions, but it's just the way that was originally used in SPM to encode these dependencies. And then we just borrowed that convention when building Pine VP. And it's pretty useful because it makes generating things like conditional expectations pretty straightforward using linear algebra operations like higher order dot products and stuff like that. So we have this kind of unwrapped representation of the Ara here, and we can use this to refer back to when we're building our Ara. One thing I didn't mention that is also important is that in the same way we can factorize hidden states, we can factorize modalities. So we'll talk about that in a second. But just for now, let's assume we have one single GPS sensing modality of nine possible levels where I am in grid world. So it's just a list of one number nine, and then the number of modalities will trivially be one the length of that list. And then we can use similarly this utils initialized MTA array, which takes the number of observation modalities, their dimensions, and the number of hidden states and then it creates the A array with the proper dimensions and everything. So I've also created this combined list called a location dims, which is a combination of the number of observation modality dimensions and the number of hidden state factor dimensions. So that's, as we said, going to be nine, three, three. And so then we can populate the entry of our first modality in the A matrix, which is trivially here, the only modality with a matrix of zeros. This is actually what's happening under the hood in this thing. So we don't need to do it here. But then if we just run that, then we'll see that the shape of a zero is the same as a location dibs. Okay, so that's just initializing it. So we now have one A array that has one modality whose shape is nine by three by three. And then we just go and fill it out like we saw up here. So I'm filling it out under the assumption of this noiseless high, high quality GPS sensor. So you can see when I'm indexing into the Lagging dimension here, the the Y dimension, which is the third dimension of the tensor. I'm saying for this particular value of Y, which is when Y is zero, we're in the first row of grid world. Then the conditional distribution over the three settings of the GPS, given all the settings of X, is just an identity matrix. So I'm just coding that, that little chunk right there. And then we can go through each of the slices iteratively and do that. So, yeah, one of the things I think is a disadvantage currently of both Pine DP and the SPM is I think we need better ways to make people not have to do these multi dimension indexing operations to encode a particular conditional independency structure. And I've thought of ways to do that, but it's actually pretty hard to do that in a generic flexible way where people can just come to the table with their own semantics and then all this gets done for them. So for now, you still have to kind of do this by hand or write some algorithmic way to do it that's specific to your task. But yeah, that's just something to mention. It's not ideal that we have users doing this, but at the moment I can't really think of a better way to do it. And then once we've encoded that, we can look at our little conditional distributions given different settings of Y. And that's just one choice. We could also condition on X and look ant for all settings of Y, x equals zero. Like you can slice this up however you want, right? That's just a visualization choice. And these are our three collections of conditional distributions, one for each setting of Y. Okay, now let's go back to the slides. Yes. So one thing I didn't mention, or I said I would mention, is we just had a single observation modality. But just as for hidden states, we can factorize and we don't have to fully enumerate our observation space. So we could separate the grid GPS observations into an X and a Y GPS. So now you're observing a pair of different observations at any given time, one X observation and one Y. So just like we had a separate B array for each hidden state factor, similarly we now have a separate A array for each observation modality. So we would populate one big A object array with these different modality specific A arrays. So in this case, if we had X observations and Y observations, we would have two A sub arrays. Each one would have size three by three by three because three observations and then for the two possible states of the hidden states factors. So, yeah, that wraps up the slides I had on multifactor multimodality factorized representations and how that affects A and B arrays. So before we move on to the contextual multi arm bandit, let's open it up for questions or comments if anything was unclear and someone wants to contribute their own way of explaining something, happy to talk about that now.
1849720	1853910	A	0.8954222222222222	Awesome. Yakup. Ben, adam and Karl, if you'd like.
1856040	1890720	C	0.9461546666666667	Yeah, thanks a lot for the great overview. I had a question on initializing the B matrix. So in this case, we initialized it with the simplest policy, just all of the available actions the agent has. Would there be any particular use case in the grid world example where we would want to initialize it with a sequence of policies? And is that needed if we want to do planning of different temporal depth?
1892980	1897110	B	0.986666153846154	So when you say sequence of policies, can you give me an example.
1899240	1914440	C	0.9471738888888888	Instead of conditioning it on, just move up, you would move up and down fixed sequences of actions.
1915740	2180670	B	0.9418719003115266	Right, right. That's interesting. Okay, so that's so, yes, typically when we are building these cognition likelihoods, this is more of a consequence of the fact that it's a palm pom DP with an emphasis on the M, so it's Markov. So if you want to encode something like what is the predicted next hidden state given I did move up and move down in the last two time steps? If I'm understanding correctly, you're like saying condition my B matrix, my next hidden state on a sequence of two possible past actions. Then we're breaking the Markov property of the dynamics because we're saying the next hidden states not just depends on what I did at the last time step and the state, but it depends on also what I did two time steps ago. So now you have a higher order temporal dependence, like a semi Markov model or yeah, really, just not a Markov model, but a higher order model. And that's not something that's currently supported in Pamdp. The natural way you get around that is by building a hierarchical model. So at each individual. Level of the hierarchical you have a palm DP, but then when you look at a single layer, like the bottom layer of this hierarchical model, it is a semi Markov model or non Markovian. If you take into account what all the other layers are doing, but that's not currently supported at the level of a single Markov model. Like you can't Bull a B matrix that's dependent on more than what happened at the last time step. You can't have an extra lagging dimension that encodes also what were they doing two time steps ago? So that's one interpretation of when you say policy like up down, I think of two actions that happened in sequence. But maybe another thing, and correct me if I'm reading too deeply into your question, another thing would be can I have actions in a different control state factor affect the dynamics of a particular hidden states factor? So say I had one hidden state factor which is move up, and another factor which is Y displacement, another factor that's X displacement. Can I have the dynamics of the Y factor not just depend on my Y action, but also on my X action? So that's the idea of basically breaking this independence property. Where is it? Here, let me find it. Here. So if you allowed the lower right shows that the state of one factor over time only depends on the state of that factor ant the previous time step. But if you said I could also make this conditionally dependent on not just factor F, but say factor I or Q at T minus one. This is something that's also not currently supported, but I think is an interesting idea, which is you would actually have interactions between maybe pairs of hidden states factors in in hidden states space. This actually can be accommodated under PMDP. Like mathematically I haven't built in the functionality to do it, but I think there's nothing that stops you in principle from doing this. So this would be as if I can have actions from different control factors all influencing the next state of one hidden state factor. And that is something that will probably make the message passing a little bit more, maybe not more complicated, but maybe its convergence properties wouldn't be as guaranteed. I don't know what it looks because you're introducing weird loops in your graph, but in the factory graph that represents the generative model. But that's something in theory could be entertained. But I don't know if that's actually what you had in mind. Maybe you had more in mind of breaking the Markov property when you say conditioning on.
2181360	2203476	C	0.9446295121951219	Yeah, thank you for the answer. It was more of a clarification also on the mathematical notation because pi is interpreted as policy, which I guess can mean both individual actions and sequences of actions. So just wanted to clarify that.
2203658	2254100	B	0.9485163846153845	Yeah, that's a good point. So when I said pi up here, if pi was a sequence of ActInf lab. Any given time, the B matrix that you're indexing out is conditioned on the action that's entailed by the policy at that particular time. But the B matrix will never be entailed on what's going on in the full policy. But just by okay, at time T, I take out this slice of the B matrix because at time T, this is the action entailed by this policy of sequence length h or something. But yeah, that's a good point. So in a trivial case, policy length is one and it's just one action that's the policy is just an action that comes down to slice out the B matrix.
2255880	2259880	A	0.9256745454545455	Thanks yakub adam, anything you like to add? Go for it.
2260030	2302950	D	0.9369895762711862	Yeah, a couple of follow ups. Thank you for this very clear presentation, Conor. So first, just to follow on what Yakup just asked, it seems like another potential way if you wanted to do this sort of two times step model would be to actually condense or kind of permute those two time steps into a single variable. The same way that you could condense the row and column into a single nine dimension nine row vector, or you can break it down into two. Could you similarly Brea down the last two moves and sort of combine them into a permutation of all possibilities of the last two moves and have those be the hidden states?
2303560	2334050	B	0.948327875	Yeah, you could do that as well. I haven't seen that actually done in practice. The only thing is you'd actually have to somehow change the scheduling, I guess, of the message passing and the action to accommodate the fact that before I determine what my last hidden state was, I need to wait two. Time steps so I can get not only the proximal action I did, but also have this memory of the action two time steps ago.
2334740	2348470	D	0.9599269230769232	Yeah, I was imagining that you'd have some special case for the first move where it's like undefined and then the last move, then after that you'd have the last two moves, a separate matrix for those or something.
2349240	2436850	B	0.933480711297071	Yeah, like a moving window kind of so it's always remembering the last. Yeah, I could see that working. I think kind of the canonical active inference answer to that is instead of having to kind of manufacture this special way of basically baking memory into a Markovian system, the classical way to handle that is just to build a hierarchical model. Because the hierarchical model handles that memory easily by just having nested Markovian processes where if you were to look at a single process, it looks like it has memory, but that's just because its priors for its own Markovian process are evolving as a function of another palm DP that's operating at a slower timescale. So I could imagine doing that exact thing that you're talking about if you had a low level palm DP that was moving twice as fast as the top level palm DP. And so basically, what the top level palm DP is doing is setting an initial prior over one action that then allows this low level palm DP to condition whatever it does next on a belief that whatever I do next. I know that I just took this action two time steps ago, but you don't actually have to handcraft that. It's just by building one palm DP, like, within another palm DP. But I don't see that there's anything in principle going against what you're describing as well.
2438020	2439730	D	0.9557057142857143	I like your approach a lot better.
2440340	2460520	B	0.8921792156862745	Yeah. Essentially what I learned from working with Karl was that anytime you're trying to imbue a system with more complex non Markovian memory, full dynamics or higher order dependencies, that's when it's the natural move, is to go to a hierarchical model because it kind of handles it all naturally.
2462060	2489170	D	0.9296376056338025	So my next question is about what you were sort of tweaking in that formula just a couple of minutes ago, where you were suggesting the possibility that there could be feedback loops between the hidden states. In theory, but as currently implemented, if I understood correctly, that's not the case. The state of one of the hidden variables or the state of one of the hidden states depends only on.
2490980	2491296	B	0.98	The.
2491318	2510840	D	0.9610227272727273	Previous state of that hidden state as currently hidden factor. What I want to make sure I understand is that is that also something about the Markovian assumption, or is that just sort of an implementation or design decision completely separate from the Markov?
2511900	2657270	B	0.9358615549597862	Yeah, that's a good question. That's separate from the Markov separate from the Markovian assumption. So that's something having to do with kind of just how the graph that represents the generative model, which you could think of as like a Bayesian graph, like a bunch of nodes that affect a layer of observations. So you say a bunch of hidden states nodes, that all the observation nodes conditionally depend on the state of all the hidden states factor nodes. By constructing it this way, you basically have a graph that it's very easy to do coherence on that has like, stationary fixed points, so the inference does not depend on individual conditions of the posteriors and things like that. The way the generative model is set up right now is like a bipartite graph that resembles a classic model for machine learning called like a restricted Bellman machine. So this is where all the hidden states can all affect one observation modality, but there's no interactions between the observation layers, so there's no lines going between observations within one modality, and there's also no lines going between hidden state factors. So that's kind of one of the advantages of this assumption, is when you have the hidden state factors be only dependent on their own past state and their own past action within that control factor, then you're not introducing these loops in the graph that make inference a little bit harder. So you'll see this structure when people are using like restricted Bellman machines or other or even deep neural networks, it's very hard to get fixed point solutions for the posteriors when you have interactions between nodes in a single layer. So it's not quite the same thing as yeah, it's just not the same thing as the Markov property has more having to do with generating an acyclic graph so you can do a fast, efficient and fixed point coherence on it. That's my general answer to that. Because if you actually draw out the factor graph for these things, it looks like that. But there might be another reason this is done that I'm actually not aware of. Maybe that's something that Karl would know or someone else.
2661740	2669930	A	0.9442034782608697	If you want to add anything, Karl, otherwise and then after this we'll head into the script. Go for it, Karl. Thank you.
2671120	3139010	E	0.919973386524823	Yeah, I think everything that needs to be said has already been said very clearly and very usefully. But yes, Conor, Heins absolutely right. So I look at this in terms of the overall architecture of the graphical model you bring to the table and it can only have a number of different attributes. How deep is it? How many levels does it have? We're not in this presentation talking about deep MDP or Joy to models based upon hierarchically composed Markov decision processes. But generally you would think of any one level in the context of the level above and the level below, and that lends this attribute of depth to any given generative model. But probably more important, certainly from the point of view of the current discussion is the breadth. And the breadth basically the number of conditional independent factors. In physics, this is simply known as the mean field approximation. It's just a factorization of a joint distribution. In this instance, what we're aiming for is an approximate post state distribution. But it's a factorization into conditionally independent factors that incidentally, also induces certain Markov blanket and resolves all of the message passing and means that sort of variational iterations are more robust. But from the point of view of why do you do that? Well, you do that because to maximize the marginal likelihood of the evidence for your generative model, you have to carve the nature out there at its joints in the right kind of way. So if the world you're trying to navigate or exchange with or explain does have this factorization, these conditional independences, it is carved in this way, then your model has to comply in order to have the maximum evidence. So when learning that particular factorization or that structure, you would normally apply a process of basic model selection or structure learning to get the number of factors right, and that is getting the right kind of meanwhile approximation ant for this world and these data. And the right factorization will simply minimize the variation of free energy and therefore minimize the computational complexity and also the computational complexity expressed in terms of thermodynamics for example. So it's a really important sort of thing. Just a couple of little endorsement comments. Conditioning any one state with any one factor on states and other factors destroys that particular factorization. So you don't have to worry about introducing loops and things. All you're doing is saying that particular factorization has gone away, you've coarse grained and you now have to lump together all the elements of one factor and the other factor into one bigger factor. And now you've got a more coarse grained factorization and a more mathematically complex model because you haven't done as many carvings to leverage the conditional independences that you're trying to model. And the first question about the policies, that was an interesting one, and I suspect it may be an artifact of the way that we condition everything on pie without sort of forewarning people. Pi is actually quite a complicated variable. First of all, it's basically an index or a name for quite a complicated combination of things. So not only is it a sequence of actions upon which you condition each individual transition at any point in time, over time, so it's a sequence of action. So in answer to Yakub's question, the whole point of the expenditure energy is exploring lots and lots of different sequences as you roll out into the future, but also induced by the factorization. As Conor was saying, that pie actually has to now entail an action for every factor of a fixed combination. So if you change the combination of action, the two factors, you now have a different policy even at one time step, even for one step ahead. So the notion of a policy as a combination of actions is still in play when you have a factorized. Meanwhile, approximation of the hidden states, it's a subtle point, usually would resolve it by actually explicitly writing down, using the variable U, the thing that you're conditioning the state transitions on. And then U comes from a family of combinations of use over factors and over time. So you can have deep and shallow policies, but you've got very shallow policies one step ahead and one look ahead policies. There's still a subset of communication of actions. So it could be that you can either only move up or down sorry, up or to the left, but you can't move diagonally, for example, and that will be placed in terms of priors over pi, namely combinations of actions. So one final, just for Adam's benefit, that notion of putting semimarkovian dynamics and Collins absolutely right. As a physicist, the way you do that is by appealing to the depth of the hierarchical model, which always entails time. And that's an absolutely crucial observation. There's no point in adding depth to a generative model unless you are mindful it is introducing a separation temporal time skills. I think that's a profound sort of structural insight into the nature of generative models and has all sorts of implications. But then you're just speaking to this notion of having combinations or little histories or little caches of legacy states as if you like a superstate. Strictly speaking, that is exactly what is done effectively in continuous state space model with Kalman filters. So you actually have now two kinds of random variables the position and the velocity. So you've now got a sort of dual representation. And if you generalize that, you get to generalize quantum motion. And technically when you're doing variational message passing, you now use something called a chef, a free energy as opposed to which is a variant of variation free energy. However, when you move to discrete state spaces, those generalized coordinates now actually are replaced just by having states that unfold in time over the probability transition matrix. So that you shouldn't in principle ever need to do that. But you can now regard the little local history, the little packet or trajectory orbit the path that you're taking at this point at this temporal scale as sufficiently discrete and Clark brain with the different probability transition Atreides and then at the slower tank scale, then it's a succession of initial states. That's normally how we would think about that, but it's an interesting game to think about Hohwy what kinds of structures in these discrete states based models echo the equivalent moves would have to make in continuous state space modeling and thinking here, particularly of generalized filtering and Bayesian smoothing and basic filtering and the like.
3142340	3181820	B	0.9243415624999994	Yeah, thank you. It's really nice to have those afterthought because that clarifies, I think, and justifies a lot of the preceding slides, what Karl just said. Yeah. So you can kind of map the depths of the hierarchy. Think of the depths of the hierarchy as a discretion of what is smoothly handled with generalized filtering and generalized coordinates of motion. So states evolving at this palm DP are kind of like a quasi velocity variable because they are evolving slower than the position variable, which is like the low level, quicker clock palm DP.
3183060	3212090	A	0.9279316666666666	One just tiny footnote there is it relates to the continuum and differentiable free energy landscape and the inclination that is required on Modern digital hardware around sampling and the inclination problem on a continuous but hidden function and the approaches that we have to take to discretize the continuous nature of time especially.
3219180	4820584	B	0.9450431581332076	Okay, so I think we should proceed to the multi armed bandit. Okay. So now, having discussed these kind of factorized representations, we're in a position to build our multi factor generative model for what I'm calling a contextual two armed bandit or contextual multi armed bandit. This is coming from the reinforcement learning literature. This term counterfactuals, but it's based directly on a kind of gambling task that was introduced in Ryan Smith, Christopher White, Carls Friston's, step by step tutorial and active inference. So you basically have a multi arm bandit, two arm bandit. But you have to first find a clue or forage information to figure out which of the two slot machines or arms of the bandit is more favorable. And this is echoed as well in the original T Maze example where you have a rat that has to choose between two arms. One is more rewarding than the other, but you don't know which one is rewarding. So you have to visit an informative Q state first that tells you the left one is rewarding right now or the right one is rewarding. So I'm just kind of gathering all these things into the term contextual bandit because that's kind of the analogous term for this problem that you'll find in reinforcement learning world. Okay, so in the multi arm bandit or two armed bandit task, an agent has to play, choose to play one of two slot machines and they can only play one at a time. So one of the slot machines gives more reward than the other, but the agent doesn't know which is the better one. So Anthony G. Chen time the agent can choose to play the left or the right machine. That's what we'll call them. Or it can choose to ask for a hint. And if it chooses to ask for a hint, instead of playing the machine, it receives information about which of the two slot machines is better. So this already sets up the basic dilemma of tasks like this, the so called Explore explore dilemma. So you first forage for information that will ultimately lead to getting more reward or do you gamble immediately in order to get reward sooner, but at the risk of not having enough information to know which slot machine I should actually play. So to encode this task structure into a generative models for a Palm DP, we equip the agent with three observation modalities. First, we have this hint modality which is the sensory channel the agent uses to perceive the hint. So just what is the state of the hint? Then there's a reward modality which tells the agent whether it won or lost at the selected machine. And then finally there's a choice modality which is just the agent's proprioceptive conversation of its own choice. This is equivalent to the GPS modality that we were working with in gridworld. In terms of hidden state factors, we first have a context factor which is a binary variable encoding which slot machine is the more rewarding one. Either the left machine is better or the right machine is better. So those are the names of the two context variables. You could just arbitrary enable them like context A, context B or something like that. And then we have a choice hidden state factor which is simply encoding the choice state that the agent currently occupies. And finally, as Karl was just explaining, for each hidden state factor, you also have a control factor. So we'll have a context control factor, which as we'll see, is trivial since the agent can't control the context in this particular simulation and a choice control factor which is the control factor that allows the agent to model its own actions or choices. So even for a shallow policy horizon, as Karl was explaining, a policy will still be comprised of two actions. So there'll be a context action and a choice action. But as we'll see, the context action will always be to do nothing or basically to not be able to interfere in the dynamics of the context hidden state factors. So basically action selection comes down to just choosing the state of the choice control factor that you choice variable. So now we're going to look at the different levels of the hidden states factors. So as we said, the context one is a binary or you could think of it as like a Bernoulli variable. So it's either left better, right better, it's a distribution over these two states of the context and this is just the unchanging state of the world for a particular trial. Either left machine has a better payoff probability or the right one does. And then the choice hidden state factor has four levels which correspond to the four possible choice states that the agent can be in. So this can either be the neutral starting location, the hint state which is the location they occupy when they're acquiring the hint, or they can be playing the left machine or they can be playing the right machine. And so now for the control state factors. The control state factor for the context only has one level. You can call it do nothing or it's just a trivial one dimensional variable and the agent will always take this action with 100% probability. So in other words, the agent has no control over the state of the context and then the choice control state has four levels corresponding to the decision to change the S choice. The hidden states factor for the choice state in one of four different ways. Either it can move to the start location, it can move to the hint location, it can play the left machine or it can play the right machine. So what choice state the agent is in is kind of obviously under the agent's control which is encoded by the fact that this u choice variable has four levels. So optimizing a posterior distribution over this choice control state factor, that's what action selection and planning boils down to is basically choosing what level of you choice or inferring what the distribution over you choices and then sampling or taking the arg max of that distribution to actually make an action. Okay, so now let's review the observation modalities and their different levels. The first modality is the hint modality which can either give the outcomes of null like I'm not at the hint state so I'm getting no sensory information from that channel. A left is better hint hint left or a right is better hint and they're just named that because we'll see in the A matrix how seeing that observation relates to seeing the, to the state of the context. We have the no observation because as I said, if the agent isn't in the choice state of getting a hint, it still needs to receive some information from that modality. So that's why we often have in both the SPM and the Pine VP implementations we have this null observation that just encodes an observation that has no information about hidden states within some particular factor. And then we have a reward modality which similarly has a null observation for when the agent isn't playing either slot machine and then two possible reward observations, either a loss or a reward. But this is a choice that can depends on what you're trying to do. You could also make O reward have like ten different reward levels that have different magnitudes of reward. It's just a choice to call these like loss and reward. The point being that one is preferred relative to the other. And then finally we have the choice modality which just allows the agent to unambiguously infer what choice state it's in. So it just allows it to infer what it's doing. And this is important because remember active inference lab, everything has to be inferred. Everything is in the game of minimizing free energy and coming up with posteriors approximate beliefs about the state of the world, including your own choice state. So that's why we often equip agent with a proprietive sense of where I am like a GPS. That's the three observation by. We've done the hidden states and the controls. So now we're in a position to start building the arrays for this agent's generative model. So for each modality specific array we're going to have as many rows as there are levels of that modality and then two columns and four slices. And so why is it two columns, four slices? Because each column corresponds to a setting of the context hidden state factor and each third order slice corresponds to a setting of the choice state hidden state factor. And these lagging dimension of two columns, four slices will be the same for every single A matrix, not the values in them, but that shape. So the only thing that will change across modalities, the A modality specific A arrays is the number of rows, which is the number of observation levels. So for the hint modality, each slice of this A matrix tells us the mapping between the two possible context states and the observations in the hint modality for a fixed choice state that the agent could be in. So, for instance, if the agent is in the start choice state, what we're seeing now regardless of whether the left or the right mission is better, the agent will always get the null observation, which has zero information about the state of the context, as you can see, because given an observation of the null observation in the hint modality, you have no idea whether the left is better or the right is better. However, if the agent is in the visit a hint state or acquire a hint state, then the mapping between the concept state and the two hint observations will be informative. And when I say informative, I just mean that the columns of this matrix will be independent from each other and they will be low entropy. So another way of saying that is that they'll be low entropy in the observation conditioned rows of the matrix. So briefly what we're seeing here, where grayscale darker colors means more probability, if the slot less machine is better that's the context, then the agent is more likely to see the hint left observation. And if the right machine is better, they're more likely to see the hint to right observation. So this slice of the a matrix is what gives those observation levels their meaning. The very reason they're called hint left and hint right is just because of the kind of diagonal structure in this slice of the matrix. And then finally for the other states, if I'm playing the left or the right machine, I'm never getting any informative observations from this modality. Okay? And then we move on to the reward modality. So again, two columns and four slices like for all the arrays. But now we're looking at the mapping between the two context and the reward observation levels. So Bull lost reward for each possible choice state the agent could be in. So as we can see if the agent is in starting location or getting the hint, they're not getting any reward conversation, they get the trivial, meaningless null observation. However, if the agent is playing the left or the right machine, there's a probabilistic mapping between the context, the left machine or the right machine being better and the expected reward. So if the context is left is better, then the agent is more likely to see the reward if and only if they're playing the left machine. Likewise, if they're playing the right machine, this matrix is the same. So if you're playing right, then you're more likely to see reward. So this submatrix within this big a tensor is what determines the payoff structure of the task or the agent's beliefs about that payoff structure, I should say, because this is the array, it's not the actual rules of the game. So now we're finally up to the choice modality which again is just their observation of their own choice state which you can see only depends on the state of the choice that they're making. It doesn't depend on the context. So it just means that they'll always unambiguously infer whether they're playing the right machine or they're in the start state or whatnot. So this is just an example of going through those slices and really seeing how that looks. So now we can code this up in Colab. And again, we won't be doing it by hand, we'll just be running some predefined code. So one useful thing to do often with control factors and observation modalities is get some semantic labels like create a list of strings for instance. So the context names will have these two states left better, right, better choice names will have these four possible states. And then we can automatically create these lists of dimensions that are needed to create your A and B matrices just by looking at things like the length of this list of strings or the length of this list of choice names. So if you run this, then we'll have our NUM states variable, which will be 24 two possible contexts and four possible choice states. And then you'll have numbs, which will be three possible hints, three possible rewards and four proprioceptive location or choice state observations. And then we can use this function to initialize our empty A matrix and then the next few chunks of code basically just fill out these A matrices like we saw in the slides. So one thing we'll do is we'll have two parameters and this is again arbitrary. You can choose to do this however you want. We'll prioritize the A matrix with a probability of hint or a hint accuracy which will effectively just fill out that slice of the hint modality array that corresponds to how accurately the hint signal indicates the concept or lends evidence to the concept. So if the Hinton is 1.0, p hint is 1.0, it means whenever they visit the hint state, they immediately know, okay, the left arm is better, but this doesn't have to be 100% right? The hint itself could have some noise or uncertainty associated with it. So we'll parameterize that just with this probability. So this is the probability of the two hint types for the two game states. And you see the null has zero probability and then hint left has more probably when the context is left indicated by the column. And then hint right has more probability when the context is right. And then we'll just go through and fill out this chance stuff. Oh, the reward modality will have a similar thing of P reward. So this will determine the payoff structure of both bandit arms. So if P reward is high, it means if the context is left is better and you're playing the left slot machine, you'll have an 80% chance of getting the reward. And this thing itself can be changed to determine basically how how rewarding the bandits are. So this is the payoff structure if you're playing the left arm, the payoff structure for the two contexts. And there's an inverseness that I didn't discuss. So if I'm playing the left arm but the right arm is better, then I'm also more likely symmetrically to get punishment or negative reward with the same probability that I would get positive reward if I was playing the right on so that's also a choice but just to make this a single parameter. That's how we did it here. And then finally the choice observation by that is just filling out that GPS high fidelity GPS sensor. If you slice it along any context hidden states factor, since it doesn't depend on context, this could be a zero or a one. Then we'll get an identity mapping which just allows them to infer their choice state and then we wrap this all into one function. So we just have two parameters p Hinton, p reward and then we can see once we start doing simulations how manipulating those things messes with their behavior. So now we'll move on to the B arrays. Oh yeah, so let me go here and talk through some slides. This will be quick. The brain are pretty simple so we're going to mess with this later. But we're going to start by saying that the context hidden states factor is stationary over time so they can't change that hidden state factor and even in the absence of their ambiguity to intervene, it stays in the same state over time. So we parameterize this with like a p change probability that just fills out the diagonal of this kind of trivial transition matrix with 100% probability. But later on we can mess with that parameter and decrease it. So they believe that the concept can actually change with some stochasticity over time. But for now we just assume they don't believe the context changes and then this is their ability to so unlike Grid World where you have local actions, they can only move up, down, left, right here. They can move from any other state to any other state. So this is like a signature or thing if you want a controllable hidden state space. So the agent can go anywhere from any other place that will manifest as these dark bands in your b matrix or bands of 100% probability. So this is another way of saying that the transition graph is fully connected, that you can get anywhere from anywhere. But that's also not sure we could have made it such that once they leave the start state they can't return there. So that would correspond to having the absence of one of these. Like basically the start state is a source but it can't also be a sync or there's no in arrows going to it so that would affect the structure of this. Okay and then we can quickly code that up. So b zero is the concept hidden states which as we said is just going to be stationarity and then the choice hidden states is fully controllable and then we will parameterize this all as a create B function which will for now have this p change thing. So we can make the agent belief about the stationary of the context different. Like if the context is likely to change, but for now, by setting it to zero, it just means that they believe the context is fixed for a given trial or set of time points. And then finally, very basic. Stuff. This is just how we establish the semantics of the reward modality. So the reward modality is, as we said, the null, the loss and the reward. And we can encode the C vector, which is kind of like the reward function, the prior preferences or prior over observations directly in terms of relative log probabilities. So we can encode punishment by having the prior expectation about loss be much lower in relative log in natural log units. Much. Lower than the reward observation within that modality. And then the other ones are just going to be uniform distribution, so you can leave them empty. And this is just what allows them to want to see reward as opposed to loss or punishment. I think we discussed this last time. Daphne had a question about that. So why do we encode things in relative log probabilities rather than sheer probabilities? It's just more analogous to the reward construct from reinforcement learning that's one benefit and they're unbounded. They don't have to be bounded between zero and one. So if you just encode them that way, it becomes easier to kind of make one thing x times more rewarding or punish or aversive than another thing. And that directly relates to how kind of reward is calculated in our expected utility of the expected free energy is because it's always kind of an expected it's kind of like an entropy or a cross entropy. So its units are in natural log space. Okay. And then the D vector. Again, they can have prior beliefs about which context is better. So that will also be a collection of vectors, one over the context factor and one over the initial state of where they are, which we can make it start, but it doesn't really matter because they have precise observations. And then that's very easy to do in prime DP. We'll just create some functions that parameterize how rewarding the reward observation is and how punishing the punishment observation is. Then we can plot those and this could be ten. Then the reward is much more is much better than the punishment, or the punishment could be very bad, in which case the punishment is much, much lower than both the null and the reward. And here I'm showing the prior directly in terms of probabilities. So when you convert the log probabilities to probabilities, these things will become bounded between zero and one. Okay? And then the D vector simulator will just create a quick function that parameterizes how much they believe the left is better. Context is true at the beginning of the simulation. And if you just set that to 0.5, it means they have flat, unbiased, uninformed prior beliefs about the concept. Yes. Okay. All right, so now I think we are good to actually do this after all the build up. So, like 95% of the work is encoding the generative model. And this is something Karl will tell you as well. SPM most of the work is not actually running active inference. Maybe computation wise it is, but in terms of the time and the intellectual energy spent, it all comes down to the generative model. That's where all the information is. The rest is just optimization, basically. So we've written down our ABCs and DS, then we just basically plug them into this agent API from Pinedp. And then we create an environment class which could be as something as ad hoc as I'm getting a list of observations from some API that's talking to the Internet or from a robot sensors. Or you can actually write down an environment class that generates observations like another agent, for instance. And then you plug the observations from the environment into the infer states function you do infer policies. And then you sample an action according to the expected free energy stuff that we talked about last time. Okay, so now we're actually going to do this. So we have our nice nifty functions for creating A-B-C and D. We can just run those with desired levels of P hint and P reward and then create our agent. And this is something we discussed last time, but an important distinctions is the generative model versus the generative process or the environment. So the things we're putting in A and B right now and C are just the agent's beliefs about, for instance, the hint accuracy or the payoff matrix of reward that can be arbitrarily different from the actual structure of the environment. And we'll see how later we'll see how you can adjust. The agent can actually learn the correct reward statistics through a process of learning. So even if they start out with the wrong generative model, they can adapt their generative model online so that they match the generative process better. Okay, and then we'll define this quick class, the two arm bandit, which will be the generative process, the real world for our agent. We're not going to step through the code, but the link is available. So if anyone wants to look at this, this is basically just encoding the rules of the game. So when the agent is in the left arm, they'll get reward or Costa, depending on the P reward and P hint values. And as we said, these values can be different than the agent's beliefs about them, which will be in their A matrix. So this is the generative process, the actual reward, and then the generative models are the parameters you give to the A Constructor. And then we'll just have a function here that runs the active inference loop. It looks a little bit more complicated than those three lines I showed on the slide, but that's just because I'm storing things like the history of their choices, the history of their beliefs. I'm having optional plotting of beliefs things like that and plotting of actions. But this function basically the key things are you get an observation from the you start with an observation, which it will be from the generative process. You do hidden state inference using that observation to get your posterior over hidden states. And this you can return if you want to plot it, but you don't have to. And then you'll do inference about policy. So this is optimization of a posterior over policies, which then gets translated into marginal posterior over control states or action that you sample from and you sample from them if this next line with sample action. So the main three lines of any active coherence process are inference about states, coherence about policies, and then action selection, which can be either deterministic or stochastic depending on your application. And then the rest is just basically code that converts observations from the generative process into the terms that the agent can understand and vice versa. And then we have this helper function that plots the history of choices and beliefs and the true context over time. So, yeah, I just blaze through that quickly because it's not very important, but if anyone has questions, we can go back and dissect that code a little more. So now all we have to do is define the yako. You have your hand up, you'll ask.
4820622	4838220	C	0.8902020689655172	Something, yeah, maybe this is better suited for the end, but in time DP, there's also modules that are adjacent to the agent class. So I was just wondering.
4841220	4841728	B	0.9540200000000001	How we.
4841734	4860620	C	0.9060963157894735	Could make use of the different modules in the action perception loop, because I haven't seen many examples of using, for instance, the inference module or the learning module and how that would augment the run active infrastructure.
4860640	5300550	B	0.9396702550571682	Yeah, that's a great question. So I have another demo that we can maybe do another time that's building a hierarchical model in PMDP. So building a two layer model. It's a visual foraging hierarchical model based on a paper that we did back in 2018, 2019. And there you're in this inner loop over time, you're making use of these sub module functions. So say you wanted to do some special kind of inference that's not just the standard infer states. You could actually say, okay, I want to do this particular weird thing that's specific to this inference process. And you would just add lines here like, I want to take a function from the inference module and run like, I don't know, a different kind of message passing on it, like run MMP or something inference. Or there's an Algos module too that has different message passing algorithms. And then you could optimize your hidden state belief, updating that message passing algorithm, and then set them equal to the special thing you did just with that. So, yeah, there's no examples here, but I haven't done that here, but it's definitely something that can be done. Or yeah, there's another example I did, like simulating an active inference equivalent of an evidence accumulation drift diffusion style task where the policy selection was done in a particular way, where you're only using certain components of the expected free energy. And yes, so basically there's other demos where you can, like, zoom in and not use the Agent class and use sub modules, but I haven't written those in a publicly accessible way yet, but the hierarchical one I can just easily make public. I just haven't had time to put it on the dock yet. But yeah, that's a good question because you're right, most of the stuff right now is just the highest level implementation with the Agent. Okay? And now that to just run the active inference process, I've made a distinction between generative process parameters, like how accurate the hint is, how the payoff structure is, and then generative model parameters which are used when creating the a matrix. So the p hint end is the true hint accuracy p reward end is the true payoff. And then just choose some time horizon and run the active inference soup with that time horizon and then plot the history of beliefs. Okay, so here's an example. The top plot shows the Agent's behavior. So white squares indicate what it was doing at any given time. So time is on the x axis. So you can see it start in the start state. Then it gathered information at the hint. And the reason this wasn't immediate is because its beliefs about accuracy were only 0.7. So it needs to actually do some evidence accumulation before it's sure what active states is. Then it played right. But as soon as it played right, because that's what the hint was telling it. As soon as it played right, it probably had a loss observation, which then messed up its inference and made it go back to the hint to accumulate evidence again before being okay, the hint is really suggesting that I should go, right? So then it goes back, revisits the right arm, and then over time, you see here, the red dot is the actual true context, and the gray scale indicates the Agent's beliefs about the hidden states. So here, the hidden states beliefs are now converging to believing that the right arm is better. And I know I said hit the beginning that all probability would be darker means higher probability, but it's actually the opposite here, so sorry, that's confusing. So yeah, right. Better is higher probability, and their beliefs get more and more competent as they play. So this is the exact same thing you'll see with the teammates, right? They first forage for information that's driven by this epistemic value component to the expected free energy. As their posterior over hidden states becomes more precise, epistemic value goes down, and then the expected utility component goes up, which is bolstered by the fact that they now have confident beliefs about the context. And then that drives them to forge information. And then so the kind of exercise that I would have people do if this was like in a classroom context is start messing with the bandit probabilities and also mess with the agent's sensitivity to punishment. So up here, we made them have plus two relative nats in their reward function and minus four for punishment. But if you made this even lower, let's say you made it negative six, what would happen? So now, because they're more risk Andersen the risk of getting a loss observation, if I was to play, immediately becomes higher. So now the agent basically requires more confidence before it's willing to actually play the left arm, because the risk of getting it wrong and getting a loss is too great because their reward function is now shaped more risk aversely. And then if I decrease this even more down to negative eight, you can see that they never even go for playing. Because even once they're very certain that the left arm is better, it's still too risky, given their beliefs about the payoff structure because their beliefs about the payoff structure are such that they believe that there's still a 20% chance that they'll get. Costa. But if we change this and said what if their p reward was 1.0 or, say, zero point 99? Then they're willing, even despite the great risk of being punished because the expected reward given, you know, the context is high enough, they will still end up risking it and they acquire the hint for some time and then they start risking it. But the observations will, because peer is so high, their beliefs about the payoff structure, their beliefs will kind of oscillate as the observations actually change, because the true observations are generated much more stochastically, because this is the P reward of the bandit. So you can mess around with this. And I would encourage, if anyone listening now or afterwards wants to mess with these parameters, I would encourage you to just go crazy and mess with parameters and see what different kinds of behavior you can get. Okay, so that pretty much wraps up the main brunt of the entorhinal. Let's pause there for questions because I have additional material about learning parameters that we discussed last time would be good to get into because that's something that's really underdocumented right now with PMDP. So we can move into that. But I think we should first break for some questions.
5303080	5310810	A	0.8827822222222221	Awesome. Thank you. Jacob if you ant to ask or Karl. Otherwise, I'm happy to hear about learning.
5312380	5313130	B	0.91063	Cool.
5318020	5322370	A	0.9648284615384616	Let's do learning and then we'll have and closing questions at the end.
5323240	6841480	B	0.9408503438754973	Perfect. Okay, I'll have some quick slides. So learning under active inference is cast as updating beliefs about the generative model parameters itself. So inference is one thing. Inference is saying, given my generative models, so my beliefs about the way the world works, what is the best explanation in the sense of posterior over hidden states? And policies that explains my actual data. And you get that by minimizing variational free energy. Learning becomes much more complex but also more interesting in the sense that if the agent's generative model itself is wrong, they can change the generative model to also optimize variational free energy. So learning and inference can kind of cooperate or sometimes interfere with each other as we'll see to minimize variational free energy. So the way we have to start by doing this is now treating the parameters themselves. So the parameters of the A array which are categorical likelihood parameters, b array, D array, CRA those themselves now become random variables over which we can have priors and of course variational posteriors. So we can go back to our original A or palm DP representation. So now what we see is in this third column next to A-B-C and D we have parameters that are priors over the categorical parameters of the A-B-C and D. So a natural prior for these sorts of categorical variables is called the deer schley distribution which is basically a conjugate prior for a categorical likelihood distribution. That's why it's very nice and also the values of its parameters have a very nice intuitive feeling and interpretability to them. So now the agents will have priors which are updated by this p of Phi at the top. And phi is just a collection of all these deer schleich hyper parameters. They're also called or prior parameters and these are parameters or priors over random variables that themselves are the likelihood distinctions and priors of the generative model. So we're just going to do a few instances of learning today. I don't know if we'll have time to do B or D learning. We'll start with a matrix learning but the principle applies the same. So here's an example of the deer slate prior distribution over some categorical parameters. So let's say our a matrix or A array, let's just say it's a matrix. So now the random variable itself are the entries of the A matrix. A prior over those entries is something called a deer schley distribution which is just a vector of positive real numbers. And here we've reshaped it so that it has the shape of an array. So let's say this array represents like the payoff matrix in our bandit task. So the two columns or two possible hidden states are context one, context two and then the two observations are punishment and reward. So if this is our prior which is measured by these deer schley parameters, the likelihood distribution that they parameterisation also you can express this as the expected value of the deer sleigh distribution is a categorical with these values. So what you can see is that the deer Shlay counts like the kind of scalar magnitude of the deer slate parameters kind of represent a prior coherence about the probability of of the different contingencies encoded in the A matrix. So I made these, these very simple here like the the deer slate priors are nine counts for seeing punishment given context and one count for seeing reward given context one. But you can change those to very high numbers. So for instance, if that nine on the left was changed to a 10,000, then on the right the probabilities would become like 0.99 to .00,001. So the scale of the deer schley, also known as pseudo count parameters, encodes something like a belief about how many times that particular coincidence of observation and hidden state has been observed, which you can also think of as like a prior confidence about that contingency and the expectation to create your A matrix via this. Going from a deer slate to an A array is very simple because you just take each deer slate count and divide it by the column wise sum, which is this a not variable on the lower right. So it's just that particular value of the A array and divided by the sum of the counts. So that's how you go from a deer slate prior over a categorical likelihood distribution through the expectation and then inference. And I'm not going to guide into the variational posterior, but it's very simple. So when you're doing learning and you're trying to actually update these Deer parameters as a function of observations, what essentially you have is a new posterior that's now not over hidden states of the world, but you have a variational posterior over the parameters the categorical or rather a variational posterior over the deer distribution, the deer slay distinctions that parameterizes your a matrix. So that's what's representing the lower right. So let's assume that our beliefs about the deer shay parameters were as it is now. So your beliefs about the A matrix and then we get an observation and let's say that we saw the conversation of punishment and that can be represented by this one hot vector. Now we want to update our beliefs about the A matrix given the observation, but to do that, what you also need is a hidden state or a belief about hidden states. And let's say that the agent was very confident that the hidden state was concept one. What ends up happening to the resulting posterior over the A matrix is a very simple associative, quasi associative learning rule where the update to the deer schley represented by this bold a SUBQ becomes the prior over the deer sleigh parameters plus the outer product between the observation vector and the hidden state belief vector. So this is kind of like a form of coincidence detection where you just see what parts of the hidden states line up with what parts of the observations and you increment your beliefs about the A matrix accordingly. So in this case, if I really believe the concept was one and I saw a punishment, then my belief about that particular contingency seeing punishment under context one would just be incremented by a plus one. And that's what this outer product at the lower right calculates. It computes a matrix that is the increment to your beliefs about the A matrix. So in this case, the matrix would be have one in the upper left and then zeros in the other three entries. And you use that to kind of increment your A matrix. Technically you're incrementing a deer schley kind of conjugate prior over your A matrix, but it's really a variational deer schley posterior. But you can imagine if your hidden states are also contaminated with uncertainty. So say you had 50%, you were 50 50, whether it was concept one or context two. Then similarly with this a matrix update, the update would then be spread over the two possible contingencies. So if you saw an observation but you weren't sure what the hidden state was, then both just the overall probability of seeing punishment under your generative model would go up because you would increment that entire row of the A matrix within this case 0.5. So that's just an important thing to note is that uncertainty in your own hidden state beliefs will bleed into the updates to the deer sleigh parameters and oftentimes alongside, I mean optimally alongside a matrix learning or parameter learning. You're also going to be augmenting your expected free energy because your generative models now different. So you have these new priors and posteriors with a so called parameter information gain term or a novelty term. So while you're pursuing learning, it also makes sense to choose your policies such that you'll maximize the information you get based on the consequences of your policies about the parameters of the generative model. So this expected KL divergence exactly quantifies how much a given policy will lead to a good Bayesian update of your parameter beliefs. So intuitively this quantity might be described as how much do I expect the consequences of my actions will update my beliefs about parameters. So consequences of actions is represented by that Q of O given pi, the thing that the KL divergence is taken under expectation of. And then the actual KL divergence is saying how much surprise will I get given these observations relative to my current beliefs about the parameters of the generative model? So this is what's called in the literature of the novelty term and it's very easy to experimentally turn off or on this novelty term using simple flags in the agent class. And this goes for the other components of the variation or the expected free energy as well. You can kind of turn on flags that say do I want to use this novelty or parameter information gain? Do I want to use the state info gain which is the same as the epistemic value or the Bayesian surprise and then do I want to use the utility? So for the learning simulations, it makes sense if you're doing learning to make sure that this parameter is turned on. Okay, so. That's the kind of slides on learning. And now we can actually get into implementing this in Pinevp. So we first will create an A matrix and we'll let the agent have very slight positive beliefs about the reward contingencies. And I'll explain later why this helps. It helps with learning if they don't have total ignorance about the reward probabilities but they have some bias in some direction. It doesn't actually have to be zero point 51, it could be the other way. And then there's utility functions that allow you to create deer schley variables that have the same shape as some base A array. So here this PA variable is basically the prior that bold a SUBP variable that represents the agent's deer schleich prior over the A matrix. So this is now a new variable that we're going to pass in to the agent Constructor. So if our A matrix is like let's look at the reward contingency, reward modality. So that's a one. So this is the agent's current beliefs about the payoff structure, right? So under choose left and choose right. Those are the two contingencies. Given the two hidden states, PA will have the same structure except that it'll have those contingencies encoded in terms of deer sleigh pseudo counts. So if I made them made the pseudo count scale ten, then they're very relatively confident that the payoff probabilities look like this. But again, these are not probabilities, they're deer slate parameters that parameterize a categorical likelihood. So you can use the normalized distribution function to actually then visualize the deer slate distribution in terms of actual categorical parameters. So taking the expectation I e. The normalization of the deer slay prior will give you exactly this. So if I say are these two things the same? They should be the same. Okay, maybe down to numerical differences, they're not. Okay? So yeah, that's how you parameterize that. So we'll create this is an important point that's also used in SPM. Often we want certain contingencies to be unavailable to learning. So we assume these are contingencies that are baked into the agent's beliefs about the world and thus not adaptable. So one way, and this is just barred from the SPM way of doing it, to encode that is to bake in a kind of really precise confidence or very high confidence that certain contingencies are the case. And you can just do that by adjusting the scale parameter for particular indices. For instance, by doing this I'm just telling the agent that it essentially doesn't learn the contingencies related to the null modality. So for instance, if you're in the start state it believes with really high confidence that you'll always get the null observation. And we just operationalize that by creating very high pseudo count or deer sleigh priors over that particular slice of the A matrix. So that's an important point to do. And then we'll just write all this stuff. So this prior count about the Bull observations and then the scale that determines their general confidence outside of this null thing. We'll write that all into a function so that we can then parameterize our PA. And then we now write a new active inference with learning loop where instead of just storing the history of choices and beliefs, we also store the history of their beliefs about the Dean Shlay parameters. And an important thing that you can also do in PMDP, which is nice, is you can say I only want to learn particular modalities. So when you're creating the agent class, there's a bunch of arguments you can pass in, but one of them is I only want to be learning the reward payoff modality right now. And this can change. You could have them learn the hint accuracy as well. But by passing in a list of which of the modality indices that you want them to learn, you turn off learning on all the other modalities, which normally in SPM or what we used to have to do in pi MDP is. You just have to turn up those deer slay pseudo counts super high on all the modalities that you don't want to learn. But if there are other modalities that you don't want to learn now you can just pass in this list of modalities that you want to learn and it'll only focus learning onto that. So now I'm setting up an agent that has these particular beliefs about the world via A and now has a prior that is going to update through doing variational inference or learning. And then so we'll create the generative model here, including this new prior over A and then we'll run the active inference loop with learning which involves creating an agent that can only do learning on the payoff structure modality. So we want the agent to now learn which arm is best basically because it doesn't know the payoffs and then also to use parameter information gain to motivate its decision. And there's another thing I forgot to mention, which is the learning rate, which is how much they increment their beliefs about the posterior or about the A matrix using this update rule. So there's something I didn't mention. There's often a learning rate that's added here that basically scales how big this update is so that you can experiment. It depends on the application, what the scale of that is going to be, but the default is one. Okay, so in this example, the agent gets the hint for one time step. It's got the standard like negative four punishment and then it goes straight to the right because it believes the hint is accurate. So it gets a reward is right, then it goes and starts playing the right arm and you see it's posterior beliefs instantly go to right. And now since what we've also spit out of this active coherence group is the beliefs about the A matrix in this QA posterior over A history we can plot its beliefs about the A matrix probability over the contingency of seeing reward given that I was in the right arm. And what you see is that even though they start with basically 50 50 beliefs as they gather observations, their beliefs about that posterior probability over that particular energy of the A matrix get bigger and bigger. An interesting consequence of this is their beliefs about the left arm contingencies don't change because they don't ever experience that state of the world. So that's like an interesting, what you might call in machine learning. It's not really a bad bootstrap but it's the idea of selective sampling. So you only learn those contingencies that are selective to the part of the world that you're sampling so the agent doesn't know what that other slice of the A matrix looks like because it's never experienced what it's like to be in the left is better context and playing the left arm. So it's posterior beliefs about that part of the A matrix remain the same but you can see that over time its beliefs start to converge to zero eight which is what we set in the generative model. This is just basic statistical learning, right? It's just getting reward observations over time and as it gets a sequence of reward and punishment through this very associative basic mechanism, it's just incrementing its deer slay beliefs. And what I'm showing is the normalized IED. Expectation of those beliefs over time. So you get something that's 0.8 but the actual deer slay parameters themselves will be growing like linearly in time effectively depending on the observations they get. Yeah, so that is just the basic learning. And then there's another thing you can do which is take advantage of this change probability. So now we allow the context to change and this is one way to get them to actually be able to explore and learn more about both reward probabilities in the landscape is allow them to have not only to make the environment change, but they also entertain beliefs in their B matrix that the environment can change. So you could use this to do B matrix learning as well by defining like a PB variable but here we're just going to have a matrix learning as we were doing before. I've changed a little bit the parameters to make things a little more stable because sometimes you get weird behavior. I mean that's something we can play around with too. So now they allow the environment itself can change and they also in their B matrix they think the environment can change. And then I'll plot the history of beliefs and choices. So now they instantly go for playing the arms. They kind of risk it. The parameter information gain actually outweighs the epistemic or state information gain. They start playing, they gather observations but they're not very confident about what the state of the world is because they only are getting information about the hidden states from the actual sequence of rewards and losses. And then eventually they go to the hint because probably the utility becomes low because they don't know what their reward probabilities are and they're building their own beliefs. Then they get the hint. Then the hint is accurate. So they have very precise beliefs about the hidden states of the world and then the world actually switches. So you see the red dot is the actual switching statistics of the bandit. And now they've kind of explored both bandit arms while also updating their beliefs about the bandits. And you see that their beliefs about the left arm are kind of bad because it seems like they've probably mostly gotten generative data while they're in the left arm and they didn't explore it very much. Or no, I guess at the end they're exploring it quite a bit, but their beliefs about to go down because they have, I guess, a bad experience with it. Yeah, that's the left arm, that's the right arm. But you can keep trying this and mess around with different parameters and you'll get different sorts of beliefs. So here in this example, there's some switching going on. Their posteriors aren't tracking very well until they actually visit the hint and then their posteriors start getting much more, much more precise. And then at that point they can start making more precise beliefs. In this case, they ended up kind of doing poorly on learning on both of them because the true reward probabilities are P eight. Yeah. So everything I've talked about so far is an example where learning can often lead to suboptimal outcomes because it's done in an online fashion. So this is an interesting point of departure between SPM and PMDP, but you can do all the same things in SPM that you can do in Pimdp. So here what I'm doing is I'm updating the A matrix every time I get an observation. So they do hidden state coherence, they select policies, they select actions, and then given the last observation and the last hidden state belief, they do one step of parameter inference and then they use the next A matrix. That's what's handled in this update, a function of the agent class. They update their beliefs about the A matrix and they use that for inference at the next time step. Another way you could do this is you could do an entire trial and then update your A matrix at the very end of all the time steps of action selection and observation sampling and then use that A matrix for another trial. So this is kind of the difference between a more em separation of timescales em or expectation maximization approach where given a set of observations, then I update my model and then I go back and I do more trials without updating my model but using the same fixed one. And then you can kind of do that in epochs and that's typically how it's done in SPM because you only do parameter learning basically this part outside this time loop. So you do it like down here, but then what you do is you run multiple trials. And the nice thing about that is it kind of makes the A matrix updates less likely to in the moment bias your action selection. So for a given set of time steps, you're locked into what your A matrix beliefs are and you don't do the update until a set of time steps has elapsed. For the purposes of this entorhinal, I just did this online learning thing. But as you can see, that can lead to actually weirdly suboptimal behavior where they don't learn the true reward statistics very well. And it depends on a bunch of things too, like the learning rate. I haven't explored this demo as much as I would like, but there's a lot of interesting kind of side effects. Like here they're actually learning very weird statistics about the bandit landscape. They start kind of learning it well, but then as their observations keep going, they don't actually learn very well here. They just avoid playing because they have a bad early experience. So this is an example of like a bad bootstrap. And then they go and just play the hint because they're too scared of anything else, even though their hidden states inference is perfect because the hint perfectly tracks the hidden state. So, yeah, this is just kind of meant to be an example of what you can do with learning. But there's going to be examples where learning actually causes suboptimal behavior, but in an interesting way. So here's an example where they actually kind of are learning the correct reward statistics. Things are starting to converge to .8 for both bandit arms, but it's very stochastic, as you can see. And I also chose action selection to be stochastic with deterministic. I wonder what you would get. Yeah, you're there just arc maxing the expected free energy or Marco mining. So hint is always the most safest option given their history of observations.
6846740	6922024	A	0.948707417218543	If I can make one remark on that kind of diversity of behaviors that we're seeing, pymdp and active inference are providing us a space and and approach to composable generative model construction that we can then sift through and coevolve with to find different strategies and behaviors. Active inference lab or even any specified generative model. It's not an answer or a solution. For example, to the explore explore dilemma. We see this like empirically right now. Active inference is not in general, resolving Explore exploit. Any GM in general is not resolving Explore exploit any parameterization is not resolving Explore exploit even for one environment. It just is equivalent to saying like, well, we have a linear model of healthcare data, so we've resolved the health care issue. And this really shows the space that we build in and what remains to be built is so open yeah, absolutely.
6922142	7017240	B	0.9440649097472925	Yeah. Because optimality is not really a function of what the generative model is or what the algorithm is. It's more guaranteed to be optimal based on how analogous the generative model and the generative process are. So if the generative model is a perfect model of the generative process, then doing Bayesian inference with respect to that generative model, that is optimal for everything. But how you learn the generative model, that's what we're seeing here. If you learn the generative model in an online fashion, it's not guaranteed to learn the right generative model. In many cases, it learns the wrong one. And this is something that a lot of people in active inference world have explore. For instance, it's North Sajid, who's also a student of Carl's, has explored a lot the limitations and the boundaries of parameter learning and things like preference learning. Like what if you were learning the C vector itself? How can that be learned online in an adaptive way? Yeah, and also Alec Chance has that paper action Oriented models where they're learning the B matrix. And in that thing they're showing how the agent learns kind of suboptimal strategies depending on its exploration of parameter space. But that also actually is kind of one of the benefits of active inference that paper shows. Because it shows if you have an agent that does have epistemic value, the model it learns of the world is like better than agents that don't have epistemic value. But again, the models are always action oriented. They're always based on what parts of the full state space is the agent driven to sample.
7021470	7027420	A	0.8465394444444443	Awesome. Yakup. Any closing comments and then all the closing comment and then Conor with the last word.
7029330	7119130	C	0.9023510958904107	Yeah, well, thank you very much for the great overview. What particularly I found particularly interesting was both how you touched on how it could be modified with the action perception loop, could be modified with the other modules, even though, as you mentioned, there would be more opportunities to go deeper in that, but also on the learning the dershleigh parameters. And I would be interested to see how that also influences planning if you would do kind of pseudo update of your dirsh lay parameters when you're calculating your expected free energy. As in what would I'm calculating my expected free energy whilst also taking into account that my B would change on each step and how that changes like the optimality of the temporal depths that we do planning with. I think super interesting and really looking forward to hopefully another model stream.
7125950	7127386	B	0.9628114285714285	Is it okay if I quickly respond?
7127418	7128160	E	0.724675	Yeah, please.
7129250	7269320	B	0.9246414484679667	Yeah. That's a very interesting point that you just made about this fictive or imagined update. Whereas this slide so that term, that novelty term in theory that does culture exactly what you're saying, yakub, is that this thing says how would my Dean sleep parameters update if I was to take this policy pile? And then I use that term to actually choose where to explore next. So if I turn on the parameter information gain term in the agent class, they'll much more quickly skip the hint and go directly to sampling the bandits because they're driven by novelty. I want to know what the reward probabilities are, so I'm going to go explore. However, what it doesn't do and what I think you're intimating with your comment is that if I did like multi step planning two time steps in the future, will I be able to do my planning given how I think I updated my parameters at the first time step? So that's what the equivalent of sophisticated inference, sophisticated inference is saying, how would I plan ant time step three given how I think my beliefs would update up to time step two? And that's done right now for hidden states. But I haven't seen that done for parameters. And I think there's someone like a student of Ryan Smith's is working on that now is like this propagation of counterfactual beliefs about how my parameter beliefs would change in the future. And that's like a really cutting edge, I think, active inference. It's like not only novelty but how would my perspective sophistication about how your own parameter beliefs will have evolved by time point T and then using that to do planning for time point T plus one, that's like really sophisticated stuff. I think getting just even bare Ronen sophisticated inference with hidden state counterfactual stuff in pinevp, that would be a huge accomplishment too. And then of course incorporating it to be more with the parameter sophistication as well. And then another quick thing is I can quickly show you an example of one of those other let's see.
7271290	7320330	A	0.925512	Just while you're finding it, that last discussion on sophisticated planning. It's something that at the semantic level we engage with every day. What courses should I take this quarter so that I can learn what I don't know today so that next quarter I'll be able to make a better plan for which classes to take so that in three years when I graduate DAG DA DA dot. But it's already phrased at the semantic granularity that these models are rapidly converging towards and they're not converging towards it by scaling is all you need. They're converging towards it with a factorized actually semantic approach which is very exciting.
7320910	7375740	B	0.928163726708074	Yeah, that's a great analogy. If you were a naive active inference agent without sophistication you would never plan with, like, a ten time step planning horizon. You would never plan to take, or let's say, a three semester plans horizon. You never plan to take multivariate calculus in your third semester because you would not have anticipated by the second semester I now have enough linear algebra to take I know that by the time I finish the second semester, I will now know linear algebra, so I'll be well suited to take multivariable calculus. Whereas what we as humans do is we do have that parameter. Sophistication I can plan as a freshman, as a first year to take in my third or fourth semester some high advanced physics because I know by that time I will have the requisite multivariable calculus or whatever. So that's a really nice analogy that I never I think that's a good example.
7376670	7378540	A	0.6395575	Welcome to active you.
7379250	7459400	B	0.931534502369668	Yeah, exactly. So here I just wanted to show you yakub, here's an example of a hierarchical hierarchical active inference demo. I can share this too. It's another collab notebook where we're exactly doing we're composing two palm DPS. So there's like a high level palm DP and then there's a low level palm DP. And for example, when you're doing a step of updating the empirical priors at the high level before you pass them down to be empirical priors at the low level, we use the control module, for example, to do get expected states. We use the high level beliefs, the high level B matrix and then the high level chosen action to propagate forward the next postered beliefs at the high level. And then those things themselves parameterize a low level empirical prior for this faster palm VP that's going on at the low level. So this is an example where you're composing like agent class calls like this, but you're composing them with functions that are built from sub modules and adjacent modules of PMDP. So that's an example of the thing you were talking about. But yeah, that's very brief. We can get into that later on. Ant a different stream or something like that.
7460330	7620460	A	0.9422688169014087	Awesome. Yes. Dot three, whenever the time is right, I'll just give two closing areas. Again, really appreciate coming back on and sharing this development in progress. Already it feels more powerful and documented than when we were in the dot one just a few weeks or months ago. I think two areas that are going to be really exciting to discuss and see how they're implemented and also the plurality of ways that they're implemented. The first area is structure learning on cognitive models from the outside. So as an ethologist, as a behavioral researcher, how do we do structure learning on cognitive models for systems that we actually know about their cognitive architecture or not, but also the view from the inside in terms of structure learning and metacognition. Like how should I change the dimensionality of my B matrix or should I turn on that flag to engage in this kind of sophisticated inference? So structure learning on cognitive models, view from the outside, view from the inside is one exciting area and the second more experimental area is statistical power analysis. Pre and post hoc statistical power analysis along the lines of the design matrix in SPM so that you put 15 time steps and zero 8.2 and punishment is four. And then you could sweep across parameters and learn about how well it did. But if there was certain interfaces, analytical or numerical approaches to be like, yes, with 25 time steps, we have this much of an expectation of convergence, or this differentiate in rewards should be resolvable by an adaptive agent over this long. And just understand, how long should these experiments be? Because there are such interesting results with one shot learning and with being able to generate someone's voice from just 10 seconds of them talking, or make a video of somebody as a deep fake with just a still image. And so it seems like it's possible to learn a lot from a little. And if we can learn a lot from a little and have a semantic cognitive model, that would be quite great.
7622510	7649410	B	0.9469807042253521	Absolutely. Yeah. It's almost like hyper parameter optimization on the landscape of active inference models. Like, how do I choose the parameters of an active inference model? In a smart way? Yeah, there's several methods for doing that that we could definitely explore. Data efficiency is the main thing, like you said, making it so you don't have to train it on a trillion images, like with a deep neural network.
7651910	7667450	A	0.9236076923076925	Perfect. End seven, two. And whenever you want to join, you and any colleagues are always welcome to share the next spiral in Pi MDP development.
7668430	7681020	B	0.9653302564102564	Awesome. Thank you so much again for letting me come on and listening. I hope it was helpful. Yeah, and I'm glad that it's recorded. It's really an amazing resource that you guys are developing here. So thanks again.
7681950	7683750	A	0.9308683333333333	Thank you. Till next time. Bye.

SPEAKER_04:
Hello and welcome everyone.

It is September 1st, 2022 and we are here in model stream number 6.1.

We're going to be discussing branching time, active inference, the theory and its generality.

going to have a presentation followed by a discussion so thank you ali and jacob for joining and anyone else to be adding their questions in the live chat without further ado over to tiafili champion and thanks so much for joining really appreciate it


SPEAKER_02:
hello thank you very much for the kind introduction uh and thank you for inviting me to present today i'm very glad i had this opportunity so today i will be speaking about branching time active in france um basically three different versions of the of the approach the first using biological message passing the second using bayesian filtering and the third one is using

belief propagation and allows the model to contain several observations and several ident states.

This work has been realized in collaboration with Lancelot Acosta, Marek Jesch and Howard Bromath.

So first of all, I want to speak a bit about the action perception cycle, which is a core idea in active inference.

Basically, there are two entities here.

The first is the environment, and the second is the agent, which is over there.

The environment provides an observation to the agent, for example, an image of the environment.

And then the agent takes this input and performs inference on it.

And the goal of the inference process is to extract high-level ident states, such as the position of Pac-Man in x and y,

or the position of the ghost, or whatever information may be relevant.

Then, based on those states, we can perform planning and action selection.

And action selection outputs the action to perform, maybe the action going up, which is fed into the environment, which produces another observation, and this cycle continues until the trail ends.

Now that we have the core idea of active inference, which is the action perception cycle, I will be speaking about active inference in a bit more depth.

Basically, active inference is about an agent which is equipped of a model.

This agent makes, as I said, observations, which are represented here at the bottom of the screen.

And those observations depend on the ident states through the A matrix.

So basically the A matrix provides a distribution of the observations for each possible latent state.

We also have the D-vector, which contains the parameter of the prior over the initial ident state.

as well as the B matrix, which explains how the transition of the environment works.

So basically, it explains how, given a state and an action, we get the new state at time t plus one.

we, as I said, have an action, or here it's a policy variable, and this action variable, or policy, depends on the precision parameter, which is called gamma, and as we will see, influence how stochastic or deterministic the policy of the agent will be.

So here we see how the prior overaction is being defined.

So it depends, as I said, on the gain parameter, and it is defined as the softmax function of minus the gamma, the precision parameter, times the expected free energy.

And the expected free energy for a particular policy is basically a sum over all future time steps, so from t plus 1, which is the first time step into the future, to uppercase t, which is the time horizon.

And for each time step, the expected free energy is defined as the expected cost plus the ambiguity.

The expected cost is the KL divergence between the predictive posterior over future observation

and the prior preferences.

The prior preferences defines which observation the agent wants to observe.

And this, the predictive posterior, defines how likely each observation is.

And so what we want to do is to minimize the divergence between those two distributions so that we actually observe what we like.

Okay, and the second term is about the entropy of the likelihood mapping expected under the variational posterior overstate.

So I'm speaking about variational posterior.

I will explain what this is in a minute, but that's the definition of the expected free energy, risk plus ambiguity, or also called expected cost plus ambiguity.

So I presented the model.

Here's a more formal definition.

Here we have a joint over all the variables in the model.

We saw that the policy depends upon the precision parameter.

We have a gamma distribution for the precision, like the gamma parameter.

We have Dirichlet priors for each of the tensor of parameters, so A, B, and D. And we see that S indeed depends on D.

and that the observation depends on the state through the A matrix.

And same thing for the transition mapping, we have a state which depends on the previous states and the B matrix, as well as the action being performed in the environment.

Okay, so now we have the model, but what we want to do is, given some observation, we want to be able to compute posterior beliefs over the latent variables.

In probability theory, we call that computing the posterior distribution, and we do that through a process which is called inference.

We can, for example, use exact inference, which is based on Bayes' theorem, so that the posterior is equal to the likelihood times the prior, and then we normalize using the evidence.

And basically, the evidence is just obtained from the numerators by summing out all the latent variable x. The problem is that when x is a continuous random variable, this summation turns into an integral, and we may not have an analytical solution for this integral.

So this method, the exact inference, can become intractable because of this.

What we do instead, when using variational inference, is that maybe this true posterior is very complex, but we are going to approximate it using this Q, this variational distribution, and try to make the divergence between those two distributions as close as possible.

So here we have the true posterior in red, and here we have an example of how the variational posterior may be fit to the true posterior in red.

In the context of active inference, the variational distribution is defined as follows.

So it is a joint distribution over all the latent variables of the model, and we are doing what we call the mean field approximation, which means that all the variables within this distribution are assumed to be independent.

Thus, there is no more dependency for the pi, a, b, d, and gamma parameters.

And we do a slight exception where the state still depends on the policy pi.

Okay, so that's the definition of the variational distribution.

And now that we have the variational distribution and the generative model, we can define the variational free energy.

So the goal of the variational free energy is to make sure that the

approximate posterior so that our variational distribution remain as close as possible to the true posterity and it is defined as the KL divergence between the approximate posterior and the generative model this variational free energy is also called like the negative evidence lower bound or ELBO in machine learning and it decomposes into

two intuitive terms.

So this is the variational free energy, and it decomposes into the KL divergence between the approximate and the true posterior.

This is the term that will make the approximate posterior be as close as possible to the true posterior.

And here we have as evidence, but which is a constant with respect to the variational distribution, which we are optimizing for.

So yeah, really the variational free energy is a proxy for the first term over there.

Okay, so what is variational message passing?

Variational message passing is an inference algorithm.

Basically, it is based on what is called the Markov blanket.

So let's suppose we want to compute the variational posterior for one specific node in a graphical model.

What variational message passing is about is saying that this node A only depends on its Markov blanket.

More specifically, it depends on the child of A, so here D and C. It depends on the parent of A, here F and G, and also in the co-parent of A, for example, E and B in this picture.

And what this Markov blanket says is that, really, we only need to know the values of the variable inside the Markov blanket to be able to perform inference over A.

Here is a bit more formal view on this question.

So here we have the optimal variational distribution for one random variable.

So this could be, for example, A. And the equation on the right gives us basically the analytical solution for this posterior.

And we see that it's only depending on the node itself, its parents, its children, and the co-parents.

And that's all we need to know.

Okay, so why a variational message passing?

The message bit comes from the decomposition of this analytical solution into messages, which are added together to form the variational posterior.

Here we can see the first message, which basically comes from the parents.

Here we can see one message for each chap.

And what we do is that all those messages will be added to form the parameter of the approximate posterity.

Here's a graphical example.

Basically, we are trying to perform inference over the random variable y. So we want to compute the parameter of the distribution, like the posterior distribution over y. And the way we go about doing this is that we send messages from the parents all the way through the random variable y. And same thing for the child and the co-parents.

And each time we reach a factor node, we combine the input messages and forward the result, forward y. When we have received all the messages, we just add them together and this provides us with the parameter of the variational distribution over y. So this was variational message passing, which is basically the algorithm that we use to perform inference.

Now I will be speaking about Monte Carlo research, which is a planning algorithm that we use to look forward into the future and estimate the quality of each policy.

So before to do that, I want just to introduce the notion of multi-index.

So here we can see the roots of the tree, which is the state at the present time step.

And then from there, we see that we have nodes in the future which are indexed by sequences of indices.

For example, here's a sequence of size one, which contains just the action or the index one.

So this is a state reached after taking action one.

And then here,

we have the states rich when performing action one followed by action two and this those indices are called multi-index because they are composed of several indices which here represent actions

OK, so now that we have this in mind, I can discuss Monte Carlo's research.

So the way Monte Carlo's research is structured is in four steps.

First, we have the selection step, where we start at the root node and compute the UCT criteria.

So this is just a real value, like a number, for each of the child.

And then we select the node which actually has the highest UCT value.

For example, maybe it is S1, so the state we reach after taking action 1.

Then from this, we can compute the UCT of each children, and maybe this node will be the highest value.

Okay, once we have reached a lift node, we can move towards the second phase, which is about expanding the child run of this node.

And for each child, what we are going to do is just simulating some rollouts from this node and computing the average expected frequency for this node.

Once we have this, there is a fourth phase, which is about updating the expected synergy of the ancestor of the node we just expanded, as well as the number of visits, because we want to explore part of the tree which have not been explored a lot in the past.

So we need to keep track of how many times each branch has been explored.

Okay, so that was the algorithm for planning.

And I now have introduced all the background required to discuss the first approach, which is branching time active inference with variational message passing.

So first, we need to define the model.

It is basically split into two parts.

The first one represents the past and the present.

And it is basically a partially observable Markov decision process, which is just a fancy word to say that we have observations, which depends on state.

This is with the A matrix.

We have action variables over there, and we have states, as I said.

So the likelihood mapping, as I said, is parameterized by the A matrix.

The transition, as usual, is parameterized by the B matrix.

And we also have here the D vector, which defines the prior over the initial island states.

So this is coming from standard active inference.

And then the novelty is that we are expanding basically the future

Each time we have this model, then we perform multi-carriage research.

Each time we want to expand a node, what we do is that we are going to add some parts, some bits to the generative model.

For example, if we want to add this part of the generative model, we are going to add one transition mapping.

This will add a random variable, here it's like S12.

And then we will add the associated observation using the likelihood map.

This is how we expand the generative model as we go.

Mathematically, this is defined like this.

So we see here a generative model over all the variables in the model.

We have Dirichlet prior over the parameters of the model, so the A, B, and D matrices.

The state at time-step zero depends on D. Each observation depends on the state associated to it and the A matrix.

And then we have prior distribution over actions, which depends on some theta parameters for which we have a Dirichlet prior.

Okay, and then the state, as usual, depends on the previous state and the previous actions, as well as on the B matrix, which defines the transition probabilities.

So this is a POMDP version, like the POMDP part of the model.

And this is a tree-like structure that expands as planning is going on.

So IT is a set of all multi-indexes which has been expanded in the model.

For example, if I come back here, we see that there are three multi-indexes that have been expanded.

so i is equal to the multi-index is one the multi-index is two and the multi-index uh one one okay and now for each of those multi-index we add to the generative model a transition and likelihood mapping that's exactly what we are doing for each index in the set of multi-index which have been expanded we had

the likelihood mapping and then transition for this specific node in the future.

Okay, so now we need to test this approach in an environment.

And for now, we'll be testing it inside a maze environment where we have here the exit of the maze.

here the starting position and then the prior preferences of the agents will be that the closer we are from the exits the more the happier uh the agents will be

So here we have a distance of 0, 1, 1, 2, 3, 4.

One feature which is really important is that the prior preferences fly across both.

So here we have 3, and here we have 4 in terms of the distance.

We don't have like 5, 6, 7, 8.

We have 4.

And what this produces

is a blue cell and what this blue says blue cell actually is is a local minimum in which the agent can be stuck if it is not careful and the way to avoid this local minimum will be to be able to plan final into the future to see that the rewarding path is actually this one and not the one leading to the local minimum so this is basically a challenge that we are adding uh to the task

And here what we can see, so here is basically an illustration of the tree which is being expanded.

And here we have the table of results.

We see that as we increase the number of planning iterations, we go from the agent being stuck inside the local minimum, so never reaching the exit, to an agent which is basically behaving properly and reaches the exit 100% of the time.

Next, we need to compare our potential branching time active inference to the previous state of the art, which is active inference.

And basically, we designed an environment in which for the agent to be able to solve it, it has to plan three, five, and then eight time steps into the future.

For ActiveInference, what happened is that it was able to properly solve the two first tasks.

But then, what happened is that because the number of policy tasks we evaluate into the future grows exponentially with the number of times that it has to plan for, the last one, which is the biggest, crashed.

With our approach, we did the exact same thing where we increased once again the number of planning iterations.

We see that the agent becomes able to solve tasks all the time and does not crash because it is able to explore in a clever fashion the space of all possible policies using multi-carriage research.

This was a bit of an empirical comparison between BTEI and active inference.

Now what I want to do in this slide is to compare them in terms of complexity classes, which is a lot more theoretical.

So basically here, each circle corresponds to a categorical distribution of a state, which means that to store one of those circles, we need to store the number of states.

So a number of parameters, which is equal to the number of state values.

So if we have a state which takes three values, we need to store three parameters.

then for each time step into the future we need to store one more parameters one more categorical when it comes to active inference and we also need to store one more categorical for each possible policy so the total complexity class is equal to the number of policies times the number of time step until the time horizon times the number of parameters we need to store for each categorical distribution

Now in the worst case scenario, the first thing to say is that BTEI does not store every single possible combination.

It's using the tree structure of the generative model to store only one distribution for the past and present.

for each time step in the past and present and then it's if we expand all the tree then it's going to x to store uh the number of action to the power of the time horizon minus t so this is still exponential in terms of course because of this change over there

But in practice, we never expand the entirety of the tree.

So what we do is maybe we will expand this, this, this, and this, but not the two others.

So in practice, the real complexity class of this algorithm is linear in the number of expansions that we are making.

Okay, so that was to show that branching time active inference does not require as much storage space as standard active inference.

now we want i want to speak about the second approach which is based on bayesian filtering so first what is bayesian filtering well basically it is an inference algorithm which starts with just a simple generative model with a state and an observation we actually know which observation we are making and we have we also know what is the prior of a state and the likelihood is and then we can for example use

by a sterine to compute the posterior overstate, given some observations.

And we just compute it like this.

Once we have a posterior overstate, we can use it as an empirical prior.

So here is our empirical prior.

And we also know the transition probability that leads us to the next time stop.

So we can use this information as well as the action that we are actually performing in the environment to compute the predictive posterior over the state at time step one, given the action that we just made, which is U0 and the observation that we made before.

And the way this is done is just by performing Bayesian prediction through the transition mapping.

So averaging out the dimension of S0.

And then comes another observation, and we can just use our predictive posterior that we got from the previous prediction step to now compute a posterior over S1 according to this new observation.

And those two steps of integrating evidence and then prediction through the transition mapping are going to be iterated as many times as we need to.

So this leads us to the second approach that I want to present today, which is branching time active inference with Bayesian filtering.

So first thing, we are not storing anymore the past, the past observation on state, because all the information we need is stored within the beliefs over the initial state, like the current time state.

The state as a current time state.

Okay, so we have an observation at time t. We can perform the integration of evidence step to get a belief about this state at time step t. And then we can perform forward prediction for each of the children that we want to expand.

For example, maybe we'll compute this one, and then this one, and so on and so forth.

And if Monte Carroll's research tells us to expand this child, then we will just perform forward prediction as well for this one and expand its associated observations.

So that's the main idea.

So really, the only difference here is that we don't even have any more partially observable Markov decision process for the past.

We only have the current timestep, and we are also changing the inference algorithm from variational message passing to Bayesian catchment.

Here is a more formal definition of the generative model.

So we can see here the likelihood and prior overstate for the initial timestep.

Here is the current timestep t. And then for each likelihood,

For each future state and observation, for each multi-indices that we already have expanded, we have the likelihood mapping and the transition mapping associated to it.

In terms of performance, we compare here branching time, active inference, using Bayesian filtering, to the same algorithm, so BTEI, but with via Einstein message passing.

And we see that for the same task, one of them is performing with an order of magnitude which is about minute scale, so around four minutes, between four and seven minutes, while the other performs a lot faster, between two and 11 seconds.

So this speed up in performance is basically made possible by the change of inference algorithm.

But now what we want to do is to be able to define more than one observation and state at each time point.

So the way we are going to do this is by changing once again the inference algorithm from Bayesian filtering to belief propagation.

So what is belief propagation?

Basically, belief propagation is an algorithm that takes as input a function over some state variables.

And this function, we know that it factorizes into a set of n factors, which we call fi.

And the question is, how can we compute the marginal distribution, like the marginal over the marginal of this function, when we marginalize out all the other random variables except for one, sm.

Okay, so we want to marginalize the distribution over all but sm.

And the way I believe propagation is solving this task is basically by passing messages through the computational graph.

So in the graph, we have two kinds of nodes.

We have factor nodes, which represent factors of the distribution, for example, F1.

And then we have random variable, maybe X1.

And we may have several of them, so F2 and then X2.

Okay, and maybe we have a transition mapping between the two.

this is the setting and now what we want to do is to pass some messages through the graph so when it comes to a message from a node x to a factual so let's suppose we have one more here

What we are going to do to compute the output message is just multiply the input messages that come from the other arrows that go towards this node.

And we just multiply all of the input messages and output the result.

When it comes to a message for PowerFacture,

We basically are going to take the factors associated to this factor node and multiply it by all the incoming messages.

So we take all the incoming messages and multiply them by the factor associated to this factor node.

And then we marginalize out all the input dimensions so that the message has the exact same shape as the output, as the target random variables.

So this is the marginalization I'm speaking about.

We do that for every single messages we can inside of the phytograph.

And then we use those messages to compute the marginal that was the goal of this algorithm.

And the way we do that is that we just take all the input messages and multiply them together.

And this gives us the marginal distribution over the specific state we wanted to.

So this leads us to multimodal on multifactorial branching time activity trends, which is the last approach that we have been developing.

Before to be able to speak a bit more about this approach, I need to introduce the notion of temporal slice.

So a temporal slice is just a set of states and observations.

We have S states and O observations.

So this is a plate notation which just duplicates the variable O time or S time.

And then we have those dashed lines.

What those dashed lines are doing is just connecting the observation to a subset of the states over there.

So for example, maybe we have an observation 1, which depends on state 1 and state 2.

And then maybe we have observation 2.

But this observation 2 only depends on state 2.

So the reason for which it's a dashed line is because we can have a sparse mapping between the state and observation.

We don't have to have all the possible connections.

Okay, and two slides, two temporal slides, can be connected through the transition mapping, which is these arrows.

And what these arrows mean is exactly the same, but between two timesteps.

So, for example, this, the state over there, can depend on the state at the previous timestep in an arbitrary fashion, exactly like the observation depends on an arbitrary subset of the state.

But this representation is a bit unpractical when it comes to presenting the entire generative model.

So what we do is that we just represent this temporal slice.

as a square, which is called TST.

So the temporal slice at time T. And the background here is gray because the observations within the temporal slice are provided.

They are actually observed.

While in the future, the background will be white because the observations are just not observed.

Okay, so now that we have this more compact presentation, we can present the generative model.

So here we see the initial time step.

And then from there, we can expand some new temporal size, exactly when multi-carot research asks us to.

So we start there, maybe we complete the UCT criterion.

This is the node with the highest UCT criterion, so we are basically asked to expand those children.

we can do so by just using the forward prediction for computing the state here and then from the state we use forward prediction once again to predict the future observation associated to this temporal slice okay more formally

this generative model is a joint over all the random variable once again and it is the product of the probability of the temporal size at time t multiplied by all the future time temporal size okay so all the temporal size we have already been expanding during Monte Carlo research

Each observation depends on a subset of the state within this temporal slice.

And because we are at the top of the tree, the state at time step t does not depend on anything.

After that, for each temporal slice in the future, we still have the dependency

that we still have the fact that the observation depends on the state within this temporal slice, but we also have the fact that the state in this temporal slice depends on the state in the parent temporal slice.

So, for example, the state within this temporal slice has parents inside this temporal slice.

Okay, so this is the way the generative model is defined.

Now, the way we perform inference is using what we call the IP algorithm.

So I stand for inference and P will stand for prediction.

So this slide is about the inference step, and the goal is to compute the posterior over the state within the initial, like the current temporal slice, given the observations.

I'm not going to go through this derivation, but you can pause the video if you are watching it on YouTube.

But yeah, so basically we do some derivation and then we obtain this solution.

which just tells us to take the product over all the likelihood mapping, with all the priors, and then use the belief propagation to actually marginalize out this function.

And that's exactly what we are going to do.

We use belief propagation within the current time step.

So if I go back here, we have some observation here, and we use belief propagation to compute the state within the initial temporal phase.

So this is the E-step, the I-step, which is the inference step.

Then we need to perform the P-step each time the multicolored research tells us to expand a part of the generative model.

So once again, I'm not going to go through this derivation, but basically the idea is to compute the posterior over the state in the next temporal slice, given the observation that we made in the current temporal slice.

so here we see that basically once again we have some kind of summation over all the parents of the states in the temporal size that we want to conclude the posterior form and this is the transition mapping that we know and this is a posterior distribution from the previous temporal size so basically what we are doing is just doing forward prediction in this case taking the expectation of the transition mapping

We can do this for the state and for the observations.

So to come back to the main picture, we first use the i-step to compute the posterior over the state at the initial temporal slice.

So this is the I-step.

And then we can use this posterior distribution to compute the posterior over the state in the future.

So maybe the state in this top wire slice.

And then we can use, once again, the P-step to compute the distribution over future observations that correspond to this top wire slice.

And we are going to do that for all top wire slides we want to explain in the future.

Okay, and the next thing that we need to define, and last for this approach, is the expected fringe.

So basically, the way we are going to define this is by first grouping all the observations into distinct subsets.

So basically, O is the set of all observations in the temporal slice index by the multi-index I. Okay, and now we are going to split

Once we have grouped those observations into subsets, the way we define the expected free energy is just as a sum over all possible groups of random variables.


SPEAKER_01:
So each of those, we will iterate over those, and then include the K-diversions between the prior and the predictive posterior for those observations.


SPEAKER_02:
So this is the risk term, the total divergence between what will happen and what we want to happen.

And then we will compute the ambiguity for each of the observations, which is as usual defined as the expected entropy of the likelihood mapping.

So this vert equation is probably new for people that have not read the paper.

But we can look at one specific case, which makes it more intuitive.

And basically, this specific case is just when each subset corresponds to one observation in the temporal phase.

And in this case, the expected free energy for a specific policy, so for one specific multi-index,

is just the risk for the specific observation plus the ambiguity for the specific observation so we still have ambiguity plus uh risk and now we need um to present like to test those approaches together so compare branching time access inference with various time message passing by the same filtering and the last approach which is based on belief propagation

way we are going to do that is by using a variant of the described data sets basically we represent the environment as a bride and so we okay in this environment we have three different shapes we have entities

hertz that need to be pulled towards the bottom right corner of the image and we have squares and the squares need to be pulled on the bottom left corner of the image and because there are way too many positions in x and y what we do is that we do some form of state aggregation and

For the 8th first position in the upper left corner, if the shape is in one of those 8 positions, we are going to aggregate them into one state with index 0.

And for this, it will be index 1, and so on and so forth, up to 19 for the squares.

And if it's a Hertz, we will just allocate those states, the indices, between 20 and 39.

something for ellipses so what we are doing is basically reducing the state space so that some of those approaches can still do something because they are not powerful enough to solve the entire state space so here are the results where we compare violation passing but by then filtering and the last one which is based on belief propagation

In variational message passing, we had to use a granularity of 4, which meant that the size of the square is like a 4x4 square.

So the cells are the size of 4x4.

And with this setting, we were able to solve 96% of the task.

And the average time for one trial is around 5 seconds.

With the Bayesian filtering approach, we were able to go down to a granularity of 2.

So this time the cells are the size of 2 by 2.

And with this granularity, the agent becomes able to solve the task 98% of the time.

But the thing is that because we reduce the size of the granularity, we also increase the size of the state space.

And this produces an increase in computational times, which means that each trial now requires around 17 seconds to be executed.

For the last approach, we use the fact that we now know the factorization of the likelihood and transition mapping.

And this allows us to go down all the way to only one, a range of one, basically.

So we are now able to differentiate between every single x and y position inside the image.

And with this granularity, we can solve the task perfectly.

And because we can take advantage of the factorization of the distribution, we go a lot faster than all of the previous approaches.

And we can solve the task in around 2.5 seconds.

just i would just want to make one thing uh very clear here because this approach basically was able to model every y position every x position every shapes inside like your shapes uh of the despite environments all the orientation like possible orientation of the shape and all the scale um so basically each

its shape can have different sizes and this scale dimension uh represent that so basically this approach was able to to deal with around 700 000 configuration of the state space now we have presented the results and show that this approach approaches um very performance like can be a very good performance but now how do we create that uh well here i've got a very small uh

code example, where basically I'm retrieving from the environment the A, B, C and D matrices.

The C matrix corresponds to the prior preferences of the agent, and as I already said several times, A corresponds to the likelihood, B corresponds to the transition, and D is the prior of the initial states.

And then the way we go about creating the BTEI 3MF agents is just by creating a temporary slice builder, telling him that we have one action, which is called A underscore zero, then giving it the number of value that this action can take.

then we just add one state for every single state of our system so the x position y position shape scale and orientation and we provide as a second parameter the like is the parameter of the trial over the state so now within our generative model we have the state of the system then we need to add the observation

For each of the states, we add one observation, which depends on the state through an A matrix.

So we provide an A matrix and the list of parents over there.

And this basically will add an observation for each state in the generative model.

The before last step is just to add transition.

So basically for each state in the system, we say what is the B matrix that need to be used and what are the parents for this state.

So for example, the position in X of the agent depends on the position in X at the previous time step and the action which is being performed.

And this will basically add transition probabilities within the model.

the last step before to build the temporal size is just to define our prior preferences and because in the display data set we need to have prior preferences about the x y and the shape of um of a blob what we do that we we say here is one factor this is one of the x subsets of observations and we provide the c matrix associated to that

Then we call the function build, which just returns the temporal slice.

And we create a BTEI3MapAgent, which uses this temporal slice.

And we provide the number of planning iterations we want the algorithm to use, as well as the exploration constants that trade off exploration versus exploitation.

Exploration versus exploitation.

And this is a graphical user interface that we have been developing.

Basically, here we can see the initial temporal slice.

If we were in the software, we can click on the temporal slice and see the different posterior over all the state variable.

We can also see different information about those as the temporal slice.

So the number of times have been visited on all the states.

And then we can use the button on the right, on the left, to basically perform a step-by-step multi-carriage research.

So what will happen in the interface is just we are going to add the children, and then we will be able to click on those children to explore those parts of the trees.

And we will have information about the temporal slice in the future.

So that's a tool that you can use to analyze both planning and the beliefs of the agents.

Okay, so now I'm done with presenting the different approaches.

It's now time for me to conclude this presentation.

So we have seen three different approaches.

The first is based on Bayerstein-Mistache passing, and obviously using active inference and multicolored research.

The second is based on Bayesian filtering.

And the last one is based on the IP algorithm, which is a mixture of belief propagation and forward prediction.

In terms of performance, the first, so BTEI-VMP, was basically slower, like less performance than the second one, which is less performance than the third one.

But even with this increase in terms of performance ability on one approach to the next, there are still some tasks that we cannot solve.

For example, how do we solve image-based problems?

it is not clear how we can learn the structure of the generative model.

For now, the modeler has to provide the description of the model, but it would be very nice if we could learn it from the data.

And also,

For now, we have been providing the agent with useful sequences of actions.

For example, if we go back to the display environment, we could imagine a task where each time we go right, it's only pushing the shape one position on the right, and then one more, and then one more.

But what we have been doing to make planning tractable is just by chunking all those actions together and maybe executing like eight actions altogether or maybe four actions altogether.

But this has its limitation and it will be really nice if we could run sequences of action like this automatically.

But that's all I wanted to say.

Here are a few references if you want to look them up after the presentation.

And yeah, thank you.


SPEAKER_04:
Thank you for the presentation.

Indeed use our reactions.

Well,

Very interesting, a lot of material and things for us to discuss.

I'll start with just one general question and then Jakob and Ali looking forward to your questions.

Just for some context, how did this team and you come to be working on this problem?

Were you working on active inference and interested in extensions or were you working in a different adjacent area and came to this algorithm?


SPEAKER_02:
Okay, so maybe one thing that I should have said is that I started a PhD at the University of Kent and

Basically, Howard and Marek, which are two of my collaborators on this project, are my supervisors.

So this is how we came up to work together just through my PhD.

And Nostra Lacosta is a collaborator that I've been working with because I've been doing some presentation at the FILM, which is the Institute of Neuroscience, where Carl Finston is.

And likely through a presentation, like during a presentation, Lancelot told me that he was interested in working with me.

So this is how I started working with Lancelot da Costa.

And so in terms of my background, I'm coming from a very computer science-like school.

pull up into coding, and then I arrived at the University of Kent, where I started to study about machine learning.

And this is where I started to gain some experience into reinforcement learning or even active inference.

and uh basically Howard Bowman was initially and my suggestion as well was uh where two of my teachers and Howard Bowman was already interested in uh active inference and one one day basically after those classes i came and see him and say well i would be interested to a side project and that's how everything started and i ended up doing a PhD after that so

That's it.

So basically, a bit of both.

I come from machine learning, my address as well.

Low slow comes from pure mathematics.

And what comes from, like neuroscience is a guy which bringing the activity from subject to the table.


SPEAKER_04:
Excellent.

Thank you.

So I have many more questions.

But how about Ali first with a question?


SPEAKER_00:
Yeah, well, first of all, thanks a lot for your amazing presentation.

I really learned a lot.

Well, I'd like to make a couple of comments, if I may, and maybe a bunch of questions.

Well, as you're well aware, active inference slash FVP research has been going on recently in basically two distinct paths.

The theoretical work geared

mostly toward developing the underlying foundational principles.

For example, the work of Dalton, Sack, DeVita, Bell, Maxwell, Ramstead and colleagues comes to mind.

And the more application-oriented research, perhaps not unlike the distinction between theoretical and experimental physics line of researchers.

But I'm not sure if you agree with me on this, but it seems to me that sadly the application-oriented research and especially the work on the various algorithmic implementations of active inference has not gained as much recognition as it should, at least as compared to the research and theoretical side.

I mean, the amount of

related published literature is pretty scarce comparatively.

So in light of this context,

Your line of research seems to me a lot more daring and, again, much more significant.

So I wanted to congratulate on that.

But you see, I'm a big fan of unification in science and technology.

So my first question is about your opinion on the possibility of unifying

the different algorithmic implementations of active inference.

As an example, you just mentioned the automatic learning of the structure of the generative model as a possible subject for future research.

I'm not sure if you've seen Wuthier et al.

's very recent paper from a couple of weeks ago, namely Learning Generative Models for Active Inference Using Tensor Networks, if I'm correct.

which outlines an interesting physics-inspired approach for that task.

But it doesn't include any citations to any of the branching time active inference papers, probably because they weren't aware of your work at the time of their writing.

But this work looks to me as a pretty good candidate for a potential integration with BTAI in order to overcome some of the limitations you just mentioned.

Or another recent example would probably be Senech et al.

's paper deriving time-averaged active inference from control principles, which is an attempt to derive an infinite horizon average surprise formulation of active inference.

So I really liked your comparative overview of the different variants of

branching time active inference, especially the benchmark analyses.

And I know you described a sophisticated active inference in your work as a subset of branching time active inference.

But regardless of these specific examples I just mentioned, I wanted to ask, how do you see the future of BTAI in terms of its possible unification with the other variants of active inference

implementations, each with its own pros and cons, in order to overcome some of their limitations without compromising the advantages of each.

I mean, do you see it as a possibility that branching time active inference will one day subsume all the other approaches somehow in a truly integrated kind of framework?


SPEAKER_02:
So to be honest, I don't exactly know.

The only part of the literature that I've been exploring is the connection with project-time active inference as being active inference and sophisticated inference.

I won't be able to speak about this in the presentation, but I can quickly give the idea of what has been the conclusion of this

And basically, it is really about how we backpropagate what I call the local expected free energy, which is basically the expected free energy associated with one node in the future.

And so if you backpropagate those upwards in the tree following

like multi-carriage research which basically comes from Bellman's equations and all this kind of literature and reinforcement learning you will fall into you will end up with an approach which is very close to sophisticated inference basically because sophisticated inference is also taking some inspiration into the Bellman equations just applying it to the expected free energy instead of just having reward

uh if you go downward so if you back propagate those local local costs towards the future then what you are effectively doing is just computing like the path integral of the expected free energy and so this will be active inference just taking the sum of all future parent stat of

of the expected for energy basically.

So this was sophisticated inference and this would be active inference.

Now concerning the other approaches that you mentioned, I haven't been reading those papers, so I can't really state on that.

But yeah, I believe that branching time-active inference is a fairly general framework.

So it may be the case that some of them will be related, but more research is needed on this.


SPEAKER_04:
Thank you so much.

Great comments.

Jakob, do you have any question?


SPEAKER_03:
Yeah, once again, thanks a lot for the awesome presentation.

It definitely explained a lot of things that I didn't understand on my reading through the original paper.

I'm wondering, again, going back to the question of learning the structure of the different components of the generative model.

In your paper, you mentioned using deep neural networks as general function approximators for learning the states-based representation.

And I'm wondering whether you...

have given some thought of into how neural networks might fit into this factor graph representation of the generative model.

And I guess I guess there are perhaps also two ways to look at learning the structure of these different components.

One is just the initial step of in your slide where you showed the kind of initialization of the model, getting the

a and b tensors the the prior the preferences and and um the prior beliefs kind of replacing that step with uh with deep neural networks to learn the representations but perhaps there's also another uh another side to um where you could dynamically change

the dimensions of these different components as in perhaps the agent receives an observation that wasn't captured in the likelihood mapping of the a tensor or perhaps it's a multi-agent setting where one agent has affordances the other agent doesn't and a new transition mapping needs to be learned through observations uh so i'm uh wondering what your thoughts are on this and


SPEAKER_02:
how you think this might be compatible with branching time active inference okay so the deep learning um area is i think really interesting and should be enabling active inference to scale to a more complicated task and more recently so this paper is not yet out but i'm working on a deep learning version of active inference so for now there is no branching time inside the picture so there is no multi-carat research

And the reason is because it's already surprisingly difficult to make it work just for active inference.

And basically, I've been reviewing some of the paper in the literature, and then I provide my own implementation of deep active inference.

But for example, I spoke about this prior dataset, and I was not able to make deep active inference work on these approaches, on these environments.

so yeah and i've been doing a presentation as a fear about this uh but basically some of the implementation of the internets contain mistakes um and yeah some of the paper also contains some like i mean i'm not sure if for the paper it's like mistake but that's that i don't understand and um for now the authors have not been able to to answer my questions um so basically what i'm trying to say that i've been trying to implement deep active inference and

surprisingly it's quite difficult to make it work at least on the despite environment so there will be a first paper about and that is analyzing what the deep neural networks are actually learning and why it is failing on these environments

And then what I wanted to do is to try to apply this implementation that I hope is correct to different tasks, more especially like the target games and stuff like this, and try to find out whether there are some tasks for which the expected free energy and this implementation of deep active inference can actually solve the task.

And the preliminary result that I have on this

is that there seems to be some task of which uh deep active inference um is actually performing better than for example dqn which is a benchmark from the reinforcement learning literature and uh yeah but it's not as um as straightforward basically uh as it seems um for now it's quite challenging to implement that so that was more for like the

deep learning aspects i think we still need to work quite a lot before to to make something very robust and that can beat more standard reinforcement learning for like benchmarks and yeah the other approaches to structural learning i have not been able like i have not researched it for now so we need more time to to think about a more robust answer to your question that's yeah that's basically what i had to say


SPEAKER_04:
Thank you.

One really striking aspect of the presentation was the analysis of the computational complexity.

So maybe we could return to this because it's something that we've wondered about and discussed on a few occasions.

You presented the theoretical complexity classes with a big O notation and then also discussed some of the practical aspects of the actual clock time on a given hardware.

Wasn't exactly sure what language or hardware you ran it on, but provided the theoretical complexity class as well as some runtime provisioning.

So I was curious to hear some...

Thoughts on how does this big O computational complexity analysis shine light on different variants of active inference as well as branching time active inference and what real computational resources were taxed

in the analysis was this a ram overflow that caused the crash that you referenced earlier is it a cpu throttling is it paralyzable does it require temp files like what in theory is happening with the computational complexity and the exponential blow up and then what in practice is going to facilitate this kind of analysis to scale


SPEAKER_02:
Okay, so first thing, this complexity analysis was done in terms of the space which is required to store all the parameters of the distribution of a state.

Okay, so here we are really interested in how much space do I need in order to store all the distribution of a state, like all the posterior distribution of a state.

and that if what happens in standard active inference is that the number of policies that will be available to the agent so let's suppose we have two actions we have one here one here okay action zero action one here at the first time step there are two actions at the second time step there will be four of them like four policies basically that the agent can

can actually perform.

It can go for 00, 01, 10, and 11.

And this number of policies will basically be multiplied by two each time step further down the road, right?

Because each time we can now pick each of the action again for each of the previous policies.

And this exponential growth is quite problematic for, for example, the prior of a policy.

So if you remember,

this definition for the prior of the policies.

We see that in order to define the prior over the policy, we need to compute the expected free energy for each of those policies.

But we need to do that for an exponential number of them as the time horizon of planning increases.

So this is the first problem.

Also,

this exponentially exponential explosion is not limited to the number of policy because we still remember okay so maybe for this one i need to go and look at the variational distribution which is used in variational inference but we see that the number like the variational posterior of a state depends on the policy so for each of the policy we need to store a distribution of a state

And this is, once again, a problem because there is an exponential number of policy, which means that there is an exponential number of of a that we need to store.

So this is the kind of problem that happens within standard active inference.

The number of policy is growing exponentially.

And we also need to store the distribution of a state for each time step.

now where branching time active inference becomes useful is that it uses a structure like the graph structure to avoid to have to store every single single possible combination of uh you know time step versus policy and so

this, those two, we see that it's growing linearly because we just keep in memory one distribution for each state in the past and present.

And only when it comes to the current time step do we start imagining what's going to happen in the future.

And this growth is still exponential.

Like if we had to explore every single possible policy in the future, we will still have an exponential growth.

but because we are using multi-carriage research basically we are going to only explore a small amount of the tree and this is where the complexity move from exponential to linear into the with respect to the number of expansion of the model so each time we expand a new branch in this model we need to store one more categorical for this uh future time step basically and

So this is how we can move from an exponential complexity class into a linear one with respect to the number of expansion of the generative model.

So that was for the complexity class.

In terms of hardware, it was basically just on my own computer.

So no graphical GPU used, nothing like this, just CPU, basically.


SPEAKER_04:
Very interesting and on the hardware or on the implementation side, where do you see

packages in Python or Julia like Forney Lab and the reactive message passing paradigm being developed?

Or do you see GPUs as being relevant?

Like this is the storage consideration.

What kinds of scaling relationships or in theory and practice, how are the operational aspects of the computing rather than the space requirements computed?


SPEAKER_02:
So, first thing to say is that the space complexity is also linked to the time and computational complexity.

Because, for example, as I said when I was speaking about the

the prior of our policy if we have an exponential number of policy we need to compute an exponential number of expected free energy for each of those policies so and same thing for when it comes to the posterior when we have to store the variational posterior and that there is an exponential number of them then we also need to compute them so this will also become intractable in the long term

And in terms of implementation, I know that some people have been developing like Phonelab in Julia.

I've been providing my own implementation in Python.

So yeah, those are possibilities.

In terms of GPUs, I guess their usage will be really useful only if the graphical model allows for parallelization.

example one case where the gpus are very useful is for images because each position in the image can generally be processed in parallel so if we had like a generative model where we had i don't know likelihood mapping purchase four pixel next to each others like a patch in the image and we had to compute the all the posterior for all the image then there is a very large potential for parallelization

But if, for example, a message has a dependency on a previous message, then there will be just a part of the GPU which is just waiting for the input message to arrive.

So there is also a particular limitation on that, because some of the analytical solutions require some other messages.

So there are some dependencies throughout the dependency, like the graphical model, by the way.

So using GPU, yes, but probably in specific generative model for which it's useful, such as image-based generative model.

So yeah, this kind of generative model.

If we have something which is very simple, then I think it's not going to benefit a lot from GPU's computation.


SPEAKER_04:
Excellent.

Thank you.

Ali again, or Jakob, if you'd like another question, or I can ask one.


SPEAKER_00:
Yeah, I also wanted to ask about the different possible future implementations of branching time active inference because

Daniel and Jakob know that I'm a big Julia fan.

So I wanted to know if there's a plan to have a Julia implementation of branching time active inference, because I think we already have C++ and Python, if I'm correct.

The first branching time active inference was implemented in C++.

the multimodal one in Python.

So what are the future plans for the other forms of implementation of BTAI?


SPEAKER_02:
So for now, I was just planning to just use Python.

But I guess it should not be too hard to port it in Julia.

It's just for now I don't have the usage for it.

But yeah, and also for future possibility when it comes to branching time active inference, I've been starting to work on trying to implement a SLAM algorithm.

um so this uh is a simultaneously location and mapping so that if we create a map of the environment as we navigate for it so this is also like a possibility but within this context

basically was going to grow exponentially that we can have observation that depends on a very large number of states and therefore what we need to have is more like a conditional probability table which is stored as a tree so it's basically you can encode rules within the conditional probability table for example you have let's put that you have okay probability of c given a and b

then maybe if A is equal to one, you want to have a branching on B, and then we have zero one.

And maybe this is the whole point, okay, nine.

Okay, so what this means is that if A is equal to 1 and B is equal to 0, then the probability distribution of a C is going to be 0.1 for the first value and 0.9 for the second value.

And so basically the idea is to try to not represent the entire table, but choose a tree structure to encode rules about the dynamic of the world and the likelihood function as well.

And then the challenge is to be able to perform forward prediction from this tree and inference also from this tree.

So this is another feature which may be integrated inside BTEI in the future.


SPEAKER_04:
Definitely a theme that runs through a lot of these discussions is representing objects as trees and then taking the tree turn or the forest turn seriously because the tree structure allows us to avoid redundancies and enable some new types of analyses.

Jakob, do you have a question or I can ask one?


SPEAKER_03:
Yeah, maybe.

Continuing on the SLAM thread, I'm wondering whether you're considering the application of SLAM to the image classification problem, and perhaps how the image classification problem needs to be reframed to even fit this, well, first of all, branching time active inference scheme, but overall active inference scheme, because

It seems that active inference overall is much better suited to these kind of continuously evolving problems where the generative process changes whenever the agent takes actions, whereas an image classification problem

seems to be way more static, which at least in the machine learning scheme, it's just input and output, and then perhaps some error that gets back-propagated through the network.

So I'm wondering how you are thinking about image classification with active inference, and overall, just how


SPEAKER_02:
images can act um as input in in um dynamical environments so yeah thank you for the question once again um this is so basically the thing with like image classification is that we don't have this temporal structure like you just mentioned which makes it quite difficult for an active inference agent to be applied to this in some sense it's a bit like the transition mapping is

is like an identity function in some sense.

So it's difficult to think about how you can bring it.

But yeah, because each timestamp could be one image, for example, in the classification.

But I think active inference is just not really well suited to something like this for classification.

I think there are just classification models, like whatever, ResNet or whatever, which are much better suited.

Basically, if you had to apply active inference, you would have to change the structure of the model to remove the temporal transition and just to have observation.

Otherwise, you will need some kind of dynamic environments, like the Atari games, Pac-Man, or this kind of thing, or the display environments.

And in this case, you can model the temporal dynamic of the environments.

and so here active inference really helps because you can you know think about action and how they impact the transition and you can have so basically an encoder that will compress the image so you will have technically an image here you will have an encoder networks a bit like in a variation autoencoder that will produce a parameter so the mean and the variance

of a distribution of a state and then we will have the decoder over there which produce another image from the latent variable basically and then we will have here like a transition networks which is also a deep neural networks which outputs as a minimum variance of a distribution over the state at the next time step

and yeah and then here you will have another encoder for the future image and another decoder for the future image as well and these transition networks will have to take into account basically the action as well as the states to predict the next state so that's the kind of architecture you will need to create a deep active inference agent

And I think it's better suited to dynamic environments, like a target game, than it is for static environments.

This is indeed a lot more complicated to apply it to.


SPEAKER_04:
If I could give a few thoughts on ImageSlam.

Very fascinating point about static analyses and dynamic analyses.

And so,

what are some ways that we could pseudo-dynamicize the image classification task?

So a few options.

One of them is navigation amongst large databases of images.

So potentially choosing informative examples for training in large empirical image databases or frames from video, or in a dynamic feedback with prompt engineering for AI generated images.

so then it makes it into a dynamic question response task that would be using dynamics at the level across images but still taking in the whole image and one other approach could be building on some of the oculomotor active inference models of attention

only taking in a small amount of the image, potentially reducing the state space or the computational complexity vastly, and then making the dynamics of some lower level entity related to policy selections on eye movements or attention.

and then treat that as like the lower level of the slam and the classification what kind of image am i looking at as a higher level of a slam but the policy is being enacted at the level of which parts of the image are being scanned yeah that's indeed a very nice setting of ideas which require modification of the task uh but is indeed pretty much more like much better suited to active inference um so very interesting

I wanted to also ask about two modules or functions that various other active proposals have had, which are hierarchical nesting of models and learning.

So how do nesting and learning influence the theoretical and the realized computational complexity?


SPEAKER_02:
So I think having a hierarchy can really help reduce the computational complexity of an active inference agent.

For example, one idea would be to... So I was speaking about generative model over images.

imagine you had like an image and then so an image can be like it's millions of possible combination rights like not even more than that but probably more than atoms in the universe but what you could do is like a bit

by imitating the structure of a convolutional neural networks.

Maybe you can create, over a patch of pixels, a generative world that will extract, for example, different line patterns.

So maybe diagonal lines or horizontal lines.

And so you will have a first level of the hierarchy, which will extract those informations.

And then you will have pattern of patterns and so on and so forth.

I think you can create this.

know as a category called distribution but in a year model basically at the very beginning you have pixels and then you have uh small edges and stuff like this and combination of pages and all the way up basically to to having objects um

but this is very complicated like um in terms of implementation we will require to like probably use gpu for the inference process because there will be a very large number of patches uh so we will need to speed up the training

But still, I think this is a very good way to reduce the state space of the agents.

Because if you try to basically put an image as input of a standard active inference agent, you will have to have more possible images on the board of atoms in the universe, even for relatively small images.

So the hierarchy can really help on that.

So you had another question, right?


SPEAKER_04:
The second aspect was about learning.

For example, what if we update our priors as the tree continues, or we want to consider policies on priors or other types of updating of our different parameters that might be fixed in other settings?


SPEAKER_02:
Yeah, so one thing to say about having learnable priors is that

some cases this could go very wrong so if you don't like okay imagine you have observations and you have states so those are the observations uh or sorry this is the states and this would be the observations so three possible states two possible observations and if you start with the gvhk prior which is just fully uniform so maybe the parameters are all one everywhere

Well, what's going to happen is that if you make one of the observations, it's just going to give you as much as the inference process within a uniform distribution of the states.

Because the weights within the matrix are basically all one.

So for each observation, there is no real state which will be more likely to basically generate it.

mean that the inference process will have a uniform distribution of a state and basically this is a problem because what you will end up having is like a matrix where maybe some states are more likely but each state is not more likely to generate different observations and so basically there is a failure of learning because

it's just not able to... So if you remember the way we update the parameters of the Dirichlet, it's just by counting the number of state observation pairs that we have been observing.

And if the state which is observed is always like one point, like one third, for example,

like if it's a uniform distribution, then it's going to count one third of the observation for all the states at the same time.

And we are not able to identify which state has been able to generate this observation because they are all as likely to generate these observations.

And so what is going to happen is that you obviously just cannot learn which states generate which observations just counting the number of times a state has appeared, but not with respect to observations.

So this is a degenerate case, which shows that having matrices, for example, with Dirichlet priors and stuff like this, can fail to ruin the dynamic of the environment and the likelihood of the environment as well.

so maybe having deep neural networks can you know avoid this problem uh but yeah it seems that's a real challenge like learning the parameters within an active inference model seems to require like a human to first give it a first draft where the likelihood like like the prior is not uniform when it comes to the Jewish layer if the model is to be able to learn

So this is one of the challenges for learning in active inference.


SPEAKER_04:
It's something we've come across in specifying the state space and what policies are possible.

And it's an interesting conversation because it brings us as modelers into engagement with the model.

and helps clarify where are we setting scaffolds and constraints what information are we what manifolds are we placing that agent into that set it up oh it's rolling downhill

within some super local context, even if that local context is still enormous in its state space, it may still be just the tip of the iceberg in terms of the total model structures.

And that's not even to say we need to explore the total model structure in practice, but in theory, it's quite important, or we might just be looking where the light is and putting the rabbit in the hat, making these models that play out a certain way, maybe even deterministically,

because they've been kind of told a secret in the beginning.

I have one more question, and then I'll leave it to Jakob.

You juxtaposed and contrasted three different approaches, which were variational message passing, Bayesian filtering, and belief propagation.

And whether for didactic or pragmatic use, where do you see these different approaches as being useful or specialized?


SPEAKER_02:
where are they better or where is one a generalization or a special case of another so for example by national message testing so the way we structured the model in via massage passing that we keep track of the past and each time we get new observation into inside into the future so let me maybe just go here um oh no

yeah so okay so in variational message passing for example we keep the past and this is quite interesting because when you have variational message passing you can also have backward messages between that as you get a new observation you will have an echo like the state associated to it

And what is going to happen is that you will have a message like this, and you will also have messages that go backward in time.

And those messages will enable you to refine your understanding of what happened in the past.

So this ability to revisit or update your understanding about the past is something which is quite specific to the Weierstein message passing algorithm and does not appear, for example, in Bayesian filtering, in the Bayesian filtering setting, because we only keep a belief state

of uh over the current random viable and when we expand like when we get a new observation um and a new state associated to it we are just going to perform prediction uh to get the posterior and then we are going to get rid of that so we cannot have those backboard messages um to get posterior beliefs over past states so we can't really have this kind of counterfactual abilities

Now with the belief propagation algorithm, basically it's very similar in ID to what is done in the belief propagation settings.

It is just a more scalable approach, which enables one to have different item states and different observation, because this BTIBF approach was only restricted to one observation and one state.

And if you have, for example, the X position, the position of the text as a disparate environment, then you will need to create one random variable that corresponds to all the combinations of those X and Y positions.

So maybe it will be

random variable describing the position and if we had two value for each of other you know for the x position and the y position then all the combination will be like four uh each of the value for one uh times all the other

and and this goes exponentially with the number of variables so let's suppose we have now a scale variable available and that this kind of variable can take two additional values then the total number of combinations of those three random variables will be like eight eight possible combinations basically all the x and y position for each of the two scale a possibility maybe scale one and scale two

And this exponential growth becomes problematic if you don't have this ability to have several observations and several states, because you will have an exponential growth in the number of states and observations you try to model.

And this is where the other approach, multi-factor and multi-modality, is really useful to scale to a more complicated approach with more states and observations.


SPEAKER_04:
Excellent.

Ali or Jakob, any closing questions or thoughts?


SPEAKER_00:
Well, you see, I came across your work a few months ago, and it got me truly excited so much that I read all five of your papers.

Is the number five right?

I mean,

You've published five papers up to now.

Because you see, more often than not, people see active inference and the free energy principle as basically speculative thinking and endeavor without so much pragmatic value in the real applications.

So in my opinion, your work,

As I mentioned, there's a very welcome addition to this nascent yet exponentially growing field.

And I hope to see more exciting developments in the future for branching time active inference or possibly the other variants you might come up with in the future.

And I'll definitely keep following your work from now on.

So thanks so much for your joining us today.


SPEAKER_02:
No problem.

And thank you for inviting me.

I'm really glad I could present here.

So thank you for the invitation.


SPEAKER_04:
Jakob, any final thoughts?


SPEAKER_03:
Yeah.

Well, this has been a really great presentation and discussion.

Ali also linked your work a couple of months ago and also got me very excited for the

future of active inference modeling.

And it's a topic that we're discussing quite a lot in the Institute.

And I think that this approach to reducing the computational cost of performing active inference in more and more complex state spaces is

probably the best way to go to really reach adoption of these models in different domains.

So yeah, thank you very much for

joining today and uh i look forward to reading that paper on on the deep active inference deep branching time active inference sure thank you well you're welcome back anytime and we will um certainly be observing thank you sure thank you bye bye thanks bye


SPEAKER_01:
Perfect.

Yeah, great.

Brilliant.
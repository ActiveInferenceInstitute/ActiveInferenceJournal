1
00:00:07,741 --> 00:00:08,641
Hello and

2
00:00:08,641 --> 00:00:11,778
welcome
everyone to the Active Inference Lab.

3
00:00:12,078 --> 00:00:17,217
This is the model stream
number 2.1 on April 16th.

4
00:00:17,217 --> 00:00:18,952
20, 21.

5
00:00:18,952 --> 00:00:21,421
And today is going to be an awesome
model stream.

6
00:00:21,654 --> 00:00:25,025
We're just going to briefly go around
and introduce ourselves

7
00:00:25,358 --> 00:00:28,962
and then I'll just mention
how the session will be run today,

8
00:00:28,962 --> 00:00:31,431
and then we'll pass it
to Noor for presentation.

9
00:00:31,798 --> 00:00:35,668
So I'm Daniel and I'm
a post-doctoral researcher in California.

10
00:00:35,902 --> 00:00:38,171
I'll pass to Philip.

11
00:00:40,073 --> 00:00:44,277
I am currently a student
at Oxford in my second year.

12
00:00:44,544 --> 00:00:49,249
And yeah, I guess this is a work
I did kind of force my Ph.D.

13
00:00:49,916 --> 00:00:50,784
once. I'd know.

14
00:00:50,784 --> 00:00:55,655
And I'm currently focusing on data
efficiency within specifically

15
00:00:55,655 --> 00:00:56,556
reinforcement learning.

16
00:00:56,556 --> 00:00:59,893
But I guess on that topic,
like the knowledge of active inference

17
00:01:00,927 --> 00:01:03,897
is obviously useful
for approaching such research.

18
00:01:06,599 --> 00:01:07,500
And or

19
00:01:08,902 --> 00:01:10,170
I I'm

20
00:01:10,170 --> 00:01:13,740
I'm a third year patient
in a theoretical neurobiology group

21
00:01:13,740 --> 00:01:17,243
at the Welcome Center for Human
Imaging at UCL.

22
00:01:18,111 --> 00:01:20,313
So that's University College London.

23
00:01:20,313 --> 00:01:23,616
So my Ph.D., supervised by calls
focused on these ideas

24
00:01:23,983 --> 00:01:27,554
pertaining to adaptation, one of which
I'll be focusing on today, which is

25
00:01:27,787 --> 00:01:31,758
behavioral adaptation and station
environments using active inference.

26
00:01:32,058 --> 00:01:33,993
So thank you. Awesome.

27
00:01:33,993 --> 00:01:37,764
Thanks both for joining
and for this presentation.

28
00:01:38,031 --> 00:01:42,302
We're going to be hearing a who knows
how long a presentation from Noor

29
00:01:42,702 --> 00:01:45,705
and then I'm going to be
compiling questions from the chat.

30
00:01:45,705 --> 00:01:46,473
So please

31
00:01:46,473 --> 00:01:50,677
just type questions as they come to you
and then we'll address them at the end.

32
00:01:51,144 --> 00:01:52,112
So thanks again.

33
00:01:52,112 --> 00:01:55,381
And Noor, please take it away.

34
00:01:55,515 --> 00:01:56,916
Perfect. Thank you.

35
00:01:56,916 --> 00:02:01,154
So today I'll be presenting
some what that I did in collaboration

36
00:02:01,154 --> 00:02:04,557
with Philip, who you've just had
from the most part uncle for.

37
00:02:06,359 --> 00:02:09,829
So it's titled Accident Friends
Demystified.

38
00:02:09,829 --> 00:02:11,731
And compared

39
00:02:13,600 --> 00:02:14,200
Okay.

40
00:02:14,200 --> 00:02:15,168
Okay. Perfect.

41
00:02:15,168 --> 00:02:17,470
So the presentation
structured as follows.

42
00:02:17,470 --> 00:02:20,306
Can you show some screens?

43
00:02:20,540 --> 00:02:22,008
I think I was sharing it. Was it?

44
00:02:22,008 --> 00:02:23,109
You're not able to see it.

45
00:02:23,109 --> 00:02:26,012
I'm not seeing it.
Could you just share it?

46
00:02:26,012 --> 00:02:27,380
Yeah, sure.

47
00:02:30,850 --> 00:02:33,419
And technology, I tell you.

48
00:02:34,587 --> 00:02:36,089
There we go. And I'll crop it.

49
00:02:36,089 --> 00:02:39,259
So go for it. Thanks. And perfect.

50
00:02:39,526 --> 00:02:39,959
Thank you.

51
00:02:39,959 --> 00:02:42,262
So the presentation
structured as follows.

52
00:02:42,262 --> 00:02:45,765
That's our briefly motivate the program
setting and provide details

53
00:02:45,765 --> 00:02:49,302
of a particular active inference
instantiation under consideration

54
00:02:49,302 --> 00:02:52,038
today, which is the discrete state space
setting.

55
00:02:52,338 --> 00:02:54,207
And the second half of the presentation

56
00:02:54,207 --> 00:02:57,210
is going to be focused
on some particular examples, comparing

57
00:02:57,477 --> 00:03:01,381
that to the inference formulation
with reinforcement learning, specifically

58
00:03:01,381 --> 00:03:05,518
Q learning and Bayesian
model based algorithm.

59
00:03:05,852 --> 00:03:09,822
And then what I'm going to do is provide
some face validity of particular aspects

60
00:03:09,822 --> 00:03:12,125
of why you would want
to even use active inference

61
00:03:14,160 --> 00:03:14,827
Okay.

62
00:03:14,827 --> 00:03:17,564
And so what is active inference

63
00:03:18,131 --> 00:03:22,835
and is a first principle account
of how biological or artificial agents

64
00:03:22,835 --> 00:03:25,738
may operate in dynamic
non stationary settings?

65
00:03:26,206 --> 00:03:30,543
It stipulates that these agents,
in order to maintain homeostasis, reside

66
00:03:30,543 --> 00:03:34,113
in attraction states that minimize
their entropy or their surprise.

67
00:03:34,647 --> 00:03:38,284
So if you take this particular example
that we're seeing of this pass middle

68
00:03:38,518 --> 00:03:42,288
hungry agent, I think the fridge,
the way it would work is

69
00:03:42,555 --> 00:03:45,158
you would need to
why would you open a French fry?

70
00:03:45,191 --> 00:03:49,062
So you want to make a particular choice
between eating at home or outside.

71
00:03:49,462 --> 00:03:52,031
And in order to do that,
you have to decide

72
00:03:52,031 --> 00:03:55,468
what is the optimal action
that would allow you to resolve your own

73
00:03:55,468 --> 00:03:57,870
uncertainty
about the current stage of the set.

74
00:03:58,538 --> 00:04:01,274
So and that would help you then decide

75
00:04:01,274 --> 00:04:04,177
whether you want to cook at home
or you want to walk to the restaurant.

76
00:04:04,444 --> 00:04:07,647
And in this particular instance,
this has led to the agent

77
00:04:07,647 --> 00:04:10,650
opening the fridge to check
whether it even has food at home.

78
00:04:11,851 --> 00:04:14,754
And what's nice about active inference
is that it allows you

79
00:04:14,754 --> 00:04:17,890
to think about these problem settings
in a more formal way

80
00:04:18,258 --> 00:04:21,394
by specifying that optimal behavior
is determined

81
00:04:21,394 --> 00:04:25,398
by evaluating the evidence that is,
the sensory input under the agents

82
00:04:25,431 --> 00:04:29,235
gentle model of observations
that it is being exposed to.

83
00:04:30,003 --> 00:04:33,373
And in this particular presentation,
what we'll do is focus on

84
00:04:33,373 --> 00:04:34,607
just the process theory

85
00:04:34,607 --> 00:04:38,244
that underwrites active inference
and not talk through the biological

86
00:04:38,244 --> 00:04:41,681
or the neural plausibility
of the active inference

87
00:04:41,681 --> 00:04:43,816
message passing game.

88
00:04:44,684 --> 00:04:48,421
But to properly motivate
why we would want to even use

89
00:04:48,821 --> 00:04:52,592
active inference in comparison to generic
reinforcement learning algorithms.

90
00:04:52,859 --> 00:04:56,062
We need to first start off with this
understanding that with an active

91
00:04:56,062 --> 00:04:59,065
inference, this is commitment
to a pure belief base scheme,

92
00:04:59,599 --> 00:05:02,402
which means that reward functions
are not always

93
00:05:02,402 --> 00:05:06,906
necessarily unnecessary
because any policy that you would have

94
00:05:07,340 --> 00:05:11,110
has a epistemic value,
even in the absence of preferences

95
00:05:11,644 --> 00:05:16,082
Additionally, active inference agents can
also learn their own reward functions.

96
00:05:16,416 --> 00:05:21,287
And this helps the agent describe
the type of behavioral that it expects

97
00:05:21,287 --> 00:05:26,526
to see itself, as opposed to something
that you would get from the environment.

98
00:05:27,360 --> 00:05:30,296
And these two particular points
are important in contrast

99
00:05:30,296 --> 00:05:34,634
to reinforcement learning,
because understanding RL settings,

100
00:05:34,634 --> 00:05:39,005
the reward function would define
how the agent interacts or behaves

101
00:05:39,372 --> 00:05:41,040
within a particular environment. Setting.

102
00:05:42,175 --> 00:05:43,576
But defining that reward

103
00:05:43,576 --> 00:05:46,813
function in the first place
is quite difficult because it assumes

104
00:05:46,813 --> 00:05:50,850
that there is a specific signal
being given from the environment

105
00:05:51,217 --> 00:05:54,487
that can be unanimously good
or bad for the agent,

106
00:05:54,787 --> 00:06:00,893
which wouldn't necessarily hold true
in a real setting where these environment

107
00:06:00,893 --> 00:06:03,563
signals can change
depending on the setting, for example.

108
00:06:03,896 --> 00:06:07,233
Eating ice cream is not always
going to be rewarding, if you will.

109
00:06:07,934 --> 00:06:11,804
And it might make you less
And that's why constructing

110
00:06:11,804 --> 00:06:14,807
these reward functions in the first place
is extremely difficult.

111
00:06:15,208 --> 00:06:16,809
And if you're not constructing them

112
00:06:16,809 --> 00:06:19,112
in the appropriate way,
even with a natural setting,

113
00:06:19,412 --> 00:06:22,682
it can result in suboptimal behavior
for your agents.

114
00:06:24,450 --> 00:06:26,519
An activity, for instance,
where you good in the sense

115
00:06:26,519 --> 00:06:30,690
because we are actually replacing
or bypassing the traditional reward

116
00:06:30,690 --> 00:06:34,427
function that you would have
in the RL setting with prior beliefs

117
00:06:34,427 --> 00:06:38,598
about bad outcomes that are sort of
desired states of the face

118
00:06:38,598 --> 00:06:41,000
that you want to see yourself being.

119
00:06:41,601 --> 00:06:45,138
And this becomes important in settings

120
00:06:45,138 --> 00:06:49,075
where there's no reward
or this imprecise understanding

121
00:06:49,075 --> 00:06:52,278
of a reward or preference
setting should look like.

122
00:06:52,712 --> 00:06:56,182
And that in this scenario,
within the sum of natural inference,

123
00:06:56,416 --> 00:07:00,286
discrete state formulation,
what we can do is not empirical prior

124
00:07:00,319 --> 00:07:02,822
distribution
over these preferred outcomes.

125
00:07:03,823 --> 00:07:05,691
And intrinsic as

126
00:07:05,691 --> 00:07:08,127
the internal reward
function of that agent.

127
00:07:08,661 --> 00:07:11,664
And this sort of brings me
to like the first distinct

128
00:07:12,932 --> 00:07:17,003
conceptualization between oral
setting and active inference,

129
00:07:17,003 --> 00:07:21,574
because within active inference,
rewards are nothing distinct.

130
00:07:21,574 --> 00:07:24,911
They are just a standard observation
that the agent is getting

131
00:07:24,911 --> 00:07:28,881
from the environment, whereas in our role
they're quite necessary to

132
00:07:30,216 --> 00:07:32,618
have appropriate action

133
00:07:32,618 --> 00:07:35,321
that the agent is going to that

134
00:07:36,255 --> 00:07:38,591
The second point we want to make.

135
00:07:39,058 --> 00:07:43,129
So we wanted to make was that active
inference provides a principal account

136
00:07:43,129 --> 00:07:47,700
of epistemic exploration and intrinsic
motivation as minimizing uncertainty.

137
00:07:48,267 --> 00:07:52,104
And again, within the oral setting,
this is quite crucial because the whole

138
00:07:52,638 --> 00:07:56,075
premise of like lots of new algorithms
that we see in our in our role

139
00:07:56,075 --> 00:08:00,513
is to try and find the right tradeoff
for the balance between exploration

140
00:08:00,513 --> 00:08:01,514
and exploration.

141
00:08:01,514 --> 00:08:04,917
So what are the rights of actions
that the agent should make

142
00:08:04,917 --> 00:08:06,385
at a given point in time?

143
00:08:06,385 --> 00:08:09,655
Should it carry on choosing
all the different ice cream flavors

144
00:08:09,655 --> 00:08:14,260
that it's been never been exposed
to, like mustard, et cetera?

145
00:08:14,527 --> 00:08:18,664
Or should it always use the oh, sorry,
always have the same ice cream flavor

146
00:08:18,664 --> 00:08:21,934
that's been exposed in the past
that it really likes it

147
00:08:22,201 --> 00:08:24,737
because I'm Caucasian on a tether
for instance.

148
00:08:25,571 --> 00:08:30,810
So this is it's an outstanding problem
with an oral and under a patient

149
00:08:30,810 --> 00:08:33,613
framework, active inference deals with it
naturally.

150
00:08:34,380 --> 00:08:38,384
Using the expected free energy formulation that I'll come to in a moment.

151
00:08:40,152 --> 00:08:41,187
And the last bit

152
00:08:41,187 --> 00:08:44,924
that you can see within the active
inference framework is that it

153
00:08:44,957 --> 00:08:49,195
naturally accounts for uncertainty
as part of the belief updating process.

154
00:08:50,429 --> 00:08:51,063
Okay.

155
00:08:51,964 --> 00:08:56,702
So now that's sort of laid out
the three things that that's super

156
00:08:56,702 --> 00:08:59,839
interesting about the active inference
scheme in comparison to IRL.

157
00:08:59,872 --> 00:09:02,909
I'm going to provide some intuitions
as to why

158
00:09:04,277 --> 00:09:06,312
you can assert

159
00:09:06,479 --> 00:09:09,348
some motivations
as to why we can even formulate

160
00:09:09,348 --> 00:09:11,584
that of inference formulation
the way we do

161
00:09:13,085 --> 00:09:15,254
OK, so I'm

162
00:09:17,223 --> 00:09:19,025
sorry, I just realized I like skipped

163
00:09:19,025 --> 00:09:20,226
ahead.

164
00:09:22,061 --> 00:09:22,862
Sorry.

165
00:09:22,862 --> 00:09:25,731
Great presentation. Yep.
Thank you so much.

166
00:09:25,831 --> 00:09:28,801
And let me just scroll down to

167
00:09:32,004 --> 00:09:32,305
okay.

168
00:09:32,305 --> 00:09:35,908
So I previously stated that within
active inference,

169
00:09:36,375 --> 00:09:40,012
it stipulates that agents are maintaining
their hemostasis

170
00:09:40,346 --> 00:09:43,082
by residing in attracting states
that minimize surprise.

171
00:09:43,482 --> 00:09:45,418
So you must have been thinking
what is surprise?

172
00:09:45,418 --> 00:09:50,056
Well, how we define surprise
as a negative low probability of outcomes

173
00:09:50,089 --> 00:09:54,393
And for this, we introduce
one run variable, which is that

174
00:09:54,393 --> 00:09:57,964
corresponds to a particular outcome
that's received by an agent.

175
00:09:58,064 --> 00:10:03,235
And this exists within a finite set of
all possible outcomes, and that's here.

176
00:10:03,603 --> 00:10:08,140
So first equation that we have
just formally states that out

177
00:10:08,174 --> 00:10:12,311
and here denotes the probability
distribution over outcomes

178
00:10:15,881 --> 00:10:19,552
And so in active inference,

179
00:10:19,552 --> 00:10:23,656
the way the agent will actually minimize
the surprise quantity that we just walk

180
00:10:23,656 --> 00:10:26,325
through is by maintaining
a gentle model throughout.

181
00:10:27,526 --> 00:10:30,429
And this is important
because at any given point in time,

182
00:10:30,429 --> 00:10:33,633
the agent wouldn't necessarily
have access to the true measurements

183
00:10:33,633 --> 00:10:34,967
of the current state of the world.

184
00:10:34,967 --> 00:10:37,269
So in this particular graphic
that you see here,

185
00:10:37,637 --> 00:10:40,539
you've got the environment
and the agent interacting

186
00:10:40,539 --> 00:10:44,310
with the environment in the way
it's being exposed to the sensory signal,

187
00:10:44,477 --> 00:10:49,115
but it doesn't know what the outcome was
generated by.

188
00:10:49,615 --> 00:10:54,253
So many perceive itself and the world
around it through only the eye.

189
00:10:54,320 --> 00:10:57,556
And needs to make inferences
about what type of states or

190
00:10:58,691 --> 00:11:02,662
the true causes were responsible

191
00:11:02,662 --> 00:11:05,364
for the particular sensory
input that is being exposed to.

192
00:11:06,065 --> 00:11:06,866
And this is why

193
00:11:07,867 --> 00:11:10,102
an act
of inference when we form the problem,

194
00:11:10,102 --> 00:11:13,239
we formulate as a party observe
what make up decision process.

195
00:11:13,572 --> 00:11:17,910
Because in this way
we are able to formulate a gentle model

196
00:11:17,910 --> 00:11:22,615
that defines this internal distribution
over the internal states

197
00:11:22,615 --> 00:11:26,485
that the agent would use
in order to impact the outcome

198
00:11:26,485 --> 00:11:28,487
so they doesn't have access
to the true state.

199
00:11:28,487 --> 00:11:32,024
But it can make hypotheses or beliefs
about the states that could have given

200
00:11:32,658 --> 00:11:35,861
rise to a particular sense outcome space

201
00:11:35,861 --> 00:11:38,798
that it's being exposed to.

202
00:11:39,098 --> 00:11:40,399
And using this, the agent

203
00:11:40,399 --> 00:11:42,735
will make inferences about the true state

204
00:11:43,502 --> 00:11:47,673
using a process of reverse mapping,
specifically Bayesian model inversion.

205
00:11:48,107 --> 00:11:51,143
And to make this a little bit
more concrete, what you can do

206
00:11:51,143 --> 00:11:55,147
is think about the hidden states
as locations or color.

207
00:11:55,781 --> 00:11:59,852
For example, the observation states
that the agent would be exposed to

208
00:11:59,852 --> 00:12:03,389
would be for example, that the velocity

209
00:12:03,389 --> 00:12:05,991
of the movement or a particular reward

210
00:12:06,692 --> 00:12:09,762
or like a happy face
that they're being exposed to

211
00:12:11,630 --> 00:12:12,565
Okay.

212
00:12:12,965 --> 00:12:15,968
So if we were to think about this
in this example formally,

213
00:12:15,968 --> 00:12:17,737
so what is the to model?

214
00:12:17,737 --> 00:12:20,072
So as we

215
00:12:20,072 --> 00:12:23,542
described before, a gentle model
is a partially observable MDP.

216
00:12:23,576 --> 00:12:28,514
And within this active inference
formulation, which rests on a simplified

217
00:12:28,514 --> 00:12:31,951
setting that we're considering here
where we only have to run the variables.

218
00:12:32,284 --> 00:12:34,920
The first one is that we've discussed.

219
00:12:35,154 --> 00:12:38,591
And the second one is S where S denotes

220
00:12:39,325 --> 00:12:44,063
a random variable representing hidden
or latent states exists within a finite

221
00:12:44,063 --> 00:12:48,701
set of all possible hidden states,
which is denoted by capital S here.

222
00:12:49,301 --> 00:12:51,837
And this join probability
that we get over

223
00:12:51,871 --> 00:12:56,075
O and S can be factored
into the likelihood function which is

224
00:12:57,109 --> 00:12:58,410
given s.

225
00:12:58,410 --> 00:13:02,381
And then you have the prior either
the internal states, which is P. S.

226
00:13:02,748 --> 00:13:05,451
So this gives you a very nice formulation

227
00:13:06,318 --> 00:13:10,623
that we are going to use
in these next couple of slides.

228
00:13:11,290 --> 00:13:15,661
I just wanted to ask, are you able to see
my mouse when I highlight or when I.

229
00:13:15,961 --> 00:13:17,263
Or is that not that.

230
00:13:18,330 --> 00:13:20,099
I can see it? Yes.

231
00:13:20,099 --> 00:13:21,567
In a perfect

232
00:13:22,868 --> 00:13:25,171
so we know that for an agent to minimize
that surprise,

233
00:13:25,171 --> 00:13:26,372
we would need to marginalize out

234
00:13:26,372 --> 00:13:29,508
all the possible hidden states that
could have led to a different outcome.

235
00:13:29,775 --> 00:13:33,379
And this can be achieved by using
the factorization that I just mentioned,

236
00:13:33,379 --> 00:13:36,415
the likelihood of the problem.

237
00:13:36,415 --> 00:13:38,818
But the the problem is that

238
00:13:39,485 --> 00:13:43,455
this is not a trivial task because
the dimensionality of the hidden states

239
00:13:43,923 --> 00:13:45,090
can be extremely large.

240
00:13:45,090 --> 00:13:47,359
And if you're considering
additional random variables, though,

241
00:13:47,526 --> 00:13:49,595
we're going to introduce in a bit.

242
00:13:49,595 --> 00:13:51,630
This becomes even more problematic.

243
00:13:51,630 --> 00:13:54,133
And that's why we use

244
00:13:54,133 --> 00:13:57,203
another quantity, a variation
or approximation of this quantity,

245
00:13:58,204 --> 00:14:00,472
which is more tractable and allows us

246
00:14:00,472 --> 00:14:03,809
to estimate the quantities of interest

247
00:14:03,809 --> 00:14:06,512
And so so this will

248
00:14:07,580 --> 00:14:10,182
be a natural step
to talk about the variation for energy,

249
00:14:10,182 --> 00:14:13,252
which is this very general approximation
of the quantity of interest.

250
00:14:14,286 --> 00:14:15,888
So what is variation for energy?

251
00:14:15,888 --> 00:14:19,525
So variation for energy is defined
as the upper bound one surprise

252
00:14:19,525 --> 00:14:20,693
to the first definition.

253
00:14:20,693 --> 00:14:22,862
Sorry, first definition
that we considered

254
00:14:22,862 --> 00:14:26,498
as derived using Jensen's
inequality is commonly known

255
00:14:26,498 --> 00:14:30,302
as negative evidence
bound in the variation in literature.

256
00:14:30,769 --> 00:14:35,374
So we get this from the equation for that
we just saw by introducing

257
00:14:35,908 --> 00:14:39,745
negative lock on both sides
and then multiplying

258
00:14:39,745 --> 00:14:43,215
this term by one,
which is essentially Q of S.

259
00:14:43,582 --> 00:14:47,486
So we're assuming that Q
if S cannot be equal to zero.

260
00:14:48,020 --> 00:14:51,957
And with that
we then apply Jensen's inequality

261
00:14:51,957 --> 00:14:56,362
and we move the log inside the function
and we end up with our expectation

262
00:14:56,362 --> 00:14:57,429
with respect to Cube.

263
00:14:57,429 --> 00:15:01,133
As for the joint over the approximate
or the

264
00:15:01,634 --> 00:15:04,169
on quantity of interest here.

265
00:15:04,503 --> 00:15:09,008
And then we take the negative inside
and looking around and we get our

266
00:15:09,975 --> 00:15:11,477
first nice

267
00:15:11,477 --> 00:15:15,881
quantity of interest here
where we get the balance

268
00:15:15,881 --> 00:15:20,352
that we're interested in
in terms of scale between the approximate

269
00:15:20,719 --> 00:15:24,023
and the joint that we have

270
00:15:24,456 --> 00:15:27,259
So to make this a little bit
more concrete, what we can do

271
00:15:27,259 --> 00:15:30,195
now is to further manipulate
the variation for energy summons

272
00:15:31,230 --> 00:15:38,304
into the scale between the approximate
and the true posterior.

273
00:15:38,637 --> 00:15:42,808
Minus the log evidence that we
the model evidence that we had

274
00:15:43,142 --> 00:15:45,911
and we can rearrange the last equation

275
00:15:45,911 --> 00:15:49,648
to really hone in on the connection
between surprises.

276
00:15:49,682 --> 00:15:51,817
Variational Free Energy.

277
00:15:51,817 --> 00:15:55,054
So if you remember
that Cal is a divergence,

278
00:15:55,054 --> 00:16:00,192
which means that it cannot be
less than zero, so is always strictly

279
00:16:00,926 --> 00:16:05,297
greater than or equal to zero,
which means that when our approximate

280
00:16:05,431 --> 00:16:09,601
is equal to the true posterior,
we end up with the variation free energy,

281
00:16:09,601 --> 00:16:13,872
equal to the model evidence,
which means that minimizing

282
00:16:13,872 --> 00:16:18,010
free energy is essentially equivalent
to maximizing the gentle model evidence.

283
00:16:19,979 --> 00:16:25,551
OK, And

284
00:16:26,118 --> 00:16:30,856
we can rewrite the previous equation
that we had equation ten

285
00:16:31,023 --> 00:16:32,124
to express the variation

286
00:16:32,124 --> 00:16:35,928
for energy as a function of plus
to replace in multiple different forms.

287
00:16:36,195 --> 00:16:39,932
So I'm just going to focus on Equation
12 here, which is the complexity

288
00:16:39,999 --> 00:16:40,966
minus accuracy.

289
00:16:40,966 --> 00:16:44,136
So this is a tradeoff
that's normally used within

290
00:16:45,437 --> 00:16:49,241
the papers
that essentially says the complexity

291
00:16:50,209 --> 00:16:53,078
or complexity cost is essentially

292
00:16:53,579 --> 00:16:57,249
your tail between your approximate cis

293
00:16:57,783 --> 00:17:00,285
s given pi with respect

294
00:17:00,285 --> 00:17:02,654
to your p of s given

295
00:17:03,322 --> 00:17:06,692
pi and here PI is just your policies

296
00:17:06,692 --> 00:17:12,097
and these can be regardless of hypothesis
of how the agent is going to act.

297
00:17:12,364 --> 00:17:14,800
But I'll come back
to what policies really entail.

298
00:17:14,800 --> 00:17:18,670
But for now, just consider them as a term
that allows us to condition

299
00:17:18,670 --> 00:17:22,908
the free energy on a sequence
of trajectories of interest.

300
00:17:23,709 --> 00:17:28,914
And the second term that we have
is the log probability of a given s.

301
00:17:28,914 --> 00:17:29,882
So the likelihood

302
00:17:31,250 --> 00:17:32,451
with a

303
00:17:32,484 --> 00:17:35,954
with respect to the key of s
that gives you the accuracy.

304
00:17:35,988 --> 00:17:40,159
So a simple way to think about it
is that this is just

305
00:17:41,560 --> 00:17:44,630
how accurate the model is
and this is some regularization time.

306
00:17:44,730 --> 00:17:48,500
That's a penalty term to make sure
that it's not diverging too far away

307
00:17:48,500 --> 00:17:52,771
from our initial priors. OK.

308
00:17:54,206 --> 00:17:56,742
So this particular quantity variation
for energy.

309
00:17:57,109 --> 00:17:59,945
Any questions at this point
about the potential for energy

310
00:18:01,046 --> 00:18:03,082
Not yet. Thank you. Yep.

311
00:18:03,449 --> 00:18:06,418
OK, so the variation for energy
is giving us this way of perceiving

312
00:18:06,418 --> 00:18:10,155
the environment and address is one part
of the active inference formulation

313
00:18:10,222 --> 00:18:13,225
which is making inferences
about getting wild

314
00:18:13,525 --> 00:18:16,428
that the agent is interacting
with at a given point in time.

315
00:18:16,929 --> 00:18:21,733
However, we have not actually accounted
for the active part,

316
00:18:21,733 --> 00:18:25,838
whereas, like this particular agent that
we have under the active in principle

317
00:18:25,838 --> 00:18:30,142
relation can take series of actions
or interact with the environment

318
00:18:30,142 --> 00:18:33,445
in such a way that it affects
that environment in the future.

319
00:18:34,947 --> 00:18:38,250
So to motivate this a little bit further,
what we can think about

320
00:18:38,250 --> 00:18:41,053
is that not only do we want to minimize
operational free energy,

321
00:18:41,053 --> 00:18:44,156
we also want to minimize
the quantity called expected for energy,

322
00:18:44,356 --> 00:18:48,827
which depends on anticipated observations
about the future in the future

323
00:18:48,827 --> 00:18:53,198
or about the future
and minimization of this particular times

324
00:18:53,198 --> 00:18:57,436
allows the agent to influence the future
by taking particular actions

325
00:18:57,436 --> 00:19:00,806
in the present which are selected
from a set of policies.

326
00:19:01,273 --> 00:19:03,475
So I mentioned policies a few times now.

327
00:19:03,475 --> 00:19:06,044
So what are they?

328
00:19:06,111 --> 00:19:09,748
So policies can be defined
as a sequence of actions, a timetable

329
00:19:10,249 --> 00:19:13,585
that enable an agent to transition
between hidden states.

330
00:19:13,752 --> 00:19:15,888
And here is essentially

331
00:19:16,588 --> 00:19:18,924
a sequence of trajectories up

332
00:19:18,924 --> 00:19:21,627
to a particular horizon cap,

333
00:19:22,060 --> 00:19:24,663
which is considering the total number

334
00:19:24,663 --> 00:19:28,734
of time steps that you are considering
in a particular setup.

335
00:19:29,234 --> 00:19:32,771
And for us to properly define policy,

336
00:19:32,771 --> 00:19:35,274
we need to introduce
two additional random variables.

337
00:19:35,507 --> 00:19:39,945
So the first one is an action
that's conditional on Tao,

338
00:19:40,145 --> 00:19:42,548
which is denoted by Utah here.

339
00:19:43,015 --> 00:19:45,851
And this exists within a finite set

340
00:19:45,851 --> 00:19:48,687
of all possible actions
that the agents can take

341
00:19:49,221 --> 00:19:52,124
And the second random variable
that we introduce is

342
00:19:52,524 --> 00:19:55,060
policy,
which is the PI that we've discussed.

343
00:19:55,928 --> 00:20:00,132
And this exists within a finite set
of all possible policies

344
00:20:00,666 --> 00:20:03,001
or sequence of actions in a sense of like

345
00:20:03,535 --> 00:20:06,705
the sequential policy optimization
that we're interested in here.

346
00:20:07,139 --> 00:20:10,876
So to make it a little bit more concrete
perhaps the random variable

347
00:20:10,876 --> 00:20:15,080
can be decomposed
into a series of actions

348
00:20:15,080 --> 00:20:19,484
over a particular time and time
horizon. Tao.

349
00:20:19,785 --> 00:20:23,155
So you want you to when going up to Utah

350
00:20:23,322 --> 00:20:28,427
to see action from action
at times, one at a time we do and so on.

351
00:20:28,794 --> 00:20:32,331
And the link is explicit
when you consider that

352
00:20:32,331 --> 00:20:36,702
if you consider policy
at a particular time point.

353
00:20:36,702 --> 00:20:40,739
So Tao then the action that
you get would be that action.

354
00:20:41,907 --> 00:20:44,443
OK, cool.

355
00:20:44,443 --> 00:20:48,347
And so I also wanted to highlight
that this definition of policy

356
00:20:48,347 --> 00:20:53,552
is actually quite different to
or distinct to how it's considered in RL,

357
00:20:53,552 --> 00:20:57,656
which is when they say policy
to mean state action policies.

358
00:20:58,056 --> 00:21:01,727
So as I just mentioned,
an act of inference, a policy is simply

359
00:21:01,727 --> 00:21:05,664
a sequence of choices for actions, a time
that is a sequential policy.

360
00:21:06,164 --> 00:21:09,768
And this is different
to a state action policy in reinforcement

361
00:21:09,768 --> 00:21:12,671
learning,
which is mapping of states to actions.

362
00:21:13,071 --> 00:21:16,708
So your RL policy,
which takes into account

363
00:21:16,708 --> 00:21:21,246
the action of the state,
is probability of your action

364
00:21:21,246 --> 00:21:26,952
given the state
and under a common formulation

365
00:21:27,986 --> 00:21:31,423
the definition of action,
a sorry definition of policies

366
00:21:31,423 --> 00:21:35,260
and oral and active inference
become exactly the same when we consider

367
00:21:35,260 --> 00:21:39,231
the setting where Tao is equal to one
so you're only considering one step ahead

368
00:21:43,268 --> 00:21:44,636
OK, so

369
00:21:44,636 --> 00:21:49,374
I'm going to move a little bit
and consider the constant of interest,

370
00:21:49,374 --> 00:21:53,011
the expense of energy which is to
how do we even derive it?

371
00:21:53,011 --> 00:21:56,915
So in order to derive that,
we first need to extend the variation

372
00:21:56,915 --> 00:21:59,051
for energy definition that we had before.

373
00:21:59,051 --> 00:22:03,722
So a few slides ago
and now make it dependent about time.

374
00:22:03,855 --> 00:22:05,957
So towel and policy.

375
00:22:05,957 --> 00:22:09,661
And what we're doing
essentially is taking that same equation

376
00:22:09,661 --> 00:22:13,765
and just decomposing it for previous,
so previous and current

377
00:22:13,765 --> 00:22:16,034
time step under a particular policy
so that what

378
00:22:16,134 --> 00:22:19,838
this is why we have the conditioning
and then we're decomposing it

379
00:22:19,838 --> 00:22:25,077
in a specific way in equation 15
and then writing out the matrix

380
00:22:25,077 --> 00:22:30,349
formulation in equation 16
so we can come back to this 30 questions.

381
00:22:31,149 --> 00:22:34,186
But the key thing
to take away from the slides is that now

382
00:22:34,186 --> 00:22:36,688
we're including
a functional dependency in time

383
00:22:37,989 --> 00:22:40,759
for the variation for energy

384
00:22:40,859 --> 00:22:42,361
and this is allowing us to now

385
00:22:42,361 --> 00:22:44,796
move to the expected year
for energy formulation.

386
00:22:45,831 --> 00:22:49,234
But the key thing to note here is
that we're only considering time points

387
00:22:49,668 --> 00:22:53,538
the previous time point and the present,
not the future

388
00:22:53,538 --> 00:22:54,272
until

389
00:22:58,009 --> 00:23:00,345
so using the free energy equation for

390
00:23:00,379 --> 00:23:03,482
we can derive the expected free energy

391
00:23:04,516 --> 00:23:07,018
and what is the expected
free energy then.

392
00:23:07,018 --> 00:23:11,223
So expect to free energy
is the free energy function of future

393
00:23:11,256 --> 00:23:15,861
trajectories and it effectively rallies
the evidence for a plausible policies

394
00:23:16,395 --> 00:23:18,864
based on outcomes
that have not been observed yet.

395
00:23:18,864 --> 00:23:19,631
So that's the key thing.

396
00:23:19,631 --> 00:23:22,467
So you're making inferences
about the pulse,

397
00:23:22,868 --> 00:23:26,471
the set of features or directories
that you have observed,

398
00:23:28,073 --> 00:23:31,676
and there's a two restricts
that are introduced here

399
00:23:31,676 --> 00:23:36,615
in order to get to the G formulation
that we see in Equation 70.

400
00:23:37,983 --> 00:23:38,917
The first

401
00:23:38,917 --> 00:23:43,622
one is to include beliefs
about future outcomes in the expectation.

402
00:23:43,622 --> 00:23:47,826
That is where supplementing excitation
under the approximate posterior

403
00:23:47,826 --> 00:23:49,995
with the likelihood here

404
00:23:49,995 --> 00:23:52,364
which results
in a predictive distribution given

405
00:23:52,364 --> 00:23:56,468
by these two terms here
and the second one is

406
00:23:56,468 --> 00:24:00,238
that we're implicitly
or explicitly conditional

407
00:24:00,439 --> 00:24:04,876
on the joint probabilities of states
and observations in the gentle model

408
00:24:06,044 --> 00:24:08,947
or in the gentle model dependent

409
00:24:08,947 --> 00:24:14,553
on the desired state of affairs,
as opposed to a particular policy now.

410
00:24:14,553 --> 00:24:18,089
So this constrains the type
of preferences that the agent would have.

411
00:24:18,890 --> 00:24:22,527
And what's helpful with these two moves
that we're making is that

412
00:24:22,527 --> 00:24:26,965
we can now evaluate this quantity
before actually having the observations.

413
00:24:27,199 --> 00:24:30,202
And the second one
is that the minimization of GEE

414
00:24:30,202 --> 00:24:34,406
would actually encourage polity policies
to be consistent

415
00:24:34,406 --> 00:24:39,010
with the desired state of affairs
that the agent expects itself to be in.

416
00:24:39,277 --> 00:24:43,248
And I'm just going to briefly mention
that this is not the only way

417
00:24:43,248 --> 00:24:45,217
to derive expected for energy.

418
00:24:45,217 --> 00:24:48,587
And there's been some work
that's looked at other formulations,

419
00:24:49,321 --> 00:24:50,822
including what I call

420
00:24:52,290 --> 00:24:53,058
the formulation of

421
00:24:53,058 --> 00:24:56,862
expected free energy can be decomposed
into different structures.

422
00:24:56,962 --> 00:25:00,031
So if anyone's interested in that,
we can go through it

423
00:25:00,031 --> 00:25:02,801
later on.

424
00:25:03,134 --> 00:25:06,671
But this free, I expect you for energy
that I just introduced me

425
00:25:06,671 --> 00:25:08,673
to compose in certain ways.

426
00:25:08,673 --> 00:25:12,777
So equation 20
and 21 and two different decompositions

427
00:25:13,111 --> 00:25:16,248
the first one being that the stomach
and the extrinsic value trade

428
00:25:16,481 --> 00:25:19,251
and the second one being the expected

429
00:25:19,251 --> 00:25:21,786
cost and ambiguity time

430
00:25:21,786 --> 00:25:24,990
so if we just consider
the extent of the first equation,

431
00:25:25,257 --> 00:25:29,528
we can say that if we're minimizing
this equation then we're capturing

432
00:25:29,528 --> 00:25:33,532
this imperative to maximize
the information gain that you would have

433
00:25:33,798 --> 00:25:37,769
from observing the environment
about particular hidden states

434
00:25:38,837 --> 00:25:41,873
while maximizing the expected value

435
00:25:41,873 --> 00:25:45,877
which is scored by the low preferences
or the extrinsic value here.

436
00:25:46,211 --> 00:25:49,881
So this particular formulation
actually gives us a very clear tradeoff

437
00:25:49,881 --> 00:25:50,615
between the

438
00:25:51,816 --> 00:25:54,853
the first component, which
is the epistemic value that promotes

439
00:25:54,853 --> 00:25:55,887
curious behavior.

440
00:25:55,887 --> 00:26:00,425
So that's what you want with exploration
encouraged as the agent seeks out

441
00:26:00,859 --> 00:26:04,529
these new states that minimize
uncertainty about the environment.

442
00:26:05,030 --> 00:26:09,868
And the latter bit is more pragmatic
and it encourages exploitative behavior

443
00:26:09,868 --> 00:26:14,339
through this understanding
of the type of policies

444
00:26:14,339 --> 00:26:17,342
that the agent would prefer to reach.

445
00:26:17,342 --> 00:26:20,245
In other words, like this expected
free energy formulation

446
00:26:20,745 --> 00:26:25,383
that we've seen in Equation
20 is essentially treating exploration

447
00:26:25,383 --> 00:26:28,687
and exploitation as two different ways
of tackling the same problem.

448
00:26:28,920 --> 00:26:31,890
So minimizing uncertainty
that I mentioned at the start

449
00:26:31,890 --> 00:26:33,959
of the presentation

450
00:26:34,759 --> 00:26:35,527
table.

451
00:26:35,760 --> 00:26:38,463
We can also think about the second
equation here,

452
00:26:38,730 --> 00:26:42,901
which is just offering us
an alternative perspective on

453
00:26:43,134 --> 00:26:46,671
if we expect to find you,
which is an agent who wishes to minimize

454
00:26:46,671 --> 00:26:49,674
the ambiguity
and the degree to which outcomes

455
00:26:49,674 --> 00:26:53,178
under a particular policy
deviate from prior preferences.

456
00:26:53,612 --> 00:26:57,415
Thus, ambiguity here is expectation
of the conditional entropy

457
00:26:57,415 --> 00:27:01,119
or the uncertainty about outcomes
under the current policy.

458
00:27:01,486 --> 00:27:04,623
In this particular setting,
low entropy would suggest the outcomes

459
00:27:04,623 --> 00:27:07,993
are quite salient and uniquely
informative about the states.

460
00:27:08,360 --> 00:27:11,029
So for example,
the visual cues that you might see

461
00:27:11,029 --> 00:27:14,799
if the room is actually pretty tough in
comparison to is quite dark,

462
00:27:14,799 --> 00:27:16,968
you're not going to encounter
anything important from that.

463
00:27:18,069 --> 00:27:19,437
In addition,

464
00:27:19,437 --> 00:27:22,107
you might pursue policy
dependent outcomes

465
00:27:22,240 --> 00:27:26,878
that resemble is preferred outcomes
so that's the denoted by C here.

466
00:27:26,878 --> 00:27:29,314
And this is achieved
when the or divergence

467
00:27:29,314 --> 00:27:33,451
between the predicted
and the preferred outcomes is minimized.

468
00:27:34,386 --> 00:27:38,123
By a particular policy set.

469
00:27:39,524 --> 00:27:41,526
And these beliefs about the future

470
00:27:41,526 --> 00:27:44,262
outcomes equip
the agent with goal directed behavior,

471
00:27:44,996 --> 00:27:47,666
which is one of the,
I guess the instances

472
00:27:47,732 --> 00:27:50,335
that is very important
in active inference

473
00:27:52,804 --> 00:27:53,438
OK, so

474
00:27:53,438 --> 00:27:58,710
once we have expected your fantasy,
we can derive the the policies.

475
00:27:59,110 --> 00:28:04,015
So and this is realized
by deriving the probability of any policy

476
00:28:04,015 --> 00:28:07,585
by applying a soft max function
over the expected for energy.

477
00:28:08,486 --> 00:28:12,223
And this sort of illustrates the self
evidencing behavior of active inference

478
00:28:12,590 --> 00:28:16,127
because any sort of policy
or action sequence that results in lower

479
00:28:16,127 --> 00:28:18,263
expected free energy are more likely.

480
00:28:18,263 --> 00:28:19,731
And intuitively this would

481
00:28:20,732 --> 00:28:22,967
make sense because

482
00:28:22,967 --> 00:28:26,071
the X factor for energy
is sort of encapsulating

483
00:28:26,071 --> 00:28:28,740
all the types of things
that you want to include

484
00:28:28,740 --> 00:28:30,709
or consider
when you're interacting with the well.

485
00:28:30,709 --> 00:28:31,576
So you want to explore,

486
00:28:31,576 --> 00:28:34,012
you want to exploit,
but you want to have a balance of that.

487
00:28:35,280 --> 00:28:37,749
And then when you're selecting
your policy,

488
00:28:38,149 --> 00:28:40,819
it's just a matter of determining
the set of actions

489
00:28:41,086 --> 00:28:43,855
which get you closest
to this particular goal.

490
00:28:44,355 --> 00:28:47,492
And this can be defined
by an attractive search that is defined

491
00:28:47,492 --> 00:28:50,528
by you see

492
00:28:50,528 --> 00:28:53,531
matrix that we described before.

493
00:28:54,532 --> 00:28:57,702
And if you don't have that,
then it's just random exploration

494
00:28:57,702 --> 00:28:59,370
that you would get.

495
00:28:59,971 --> 00:29:03,341
Sometimes you can also include
a temperature parameter beta here.

496
00:29:04,142 --> 00:29:06,878
And by having a hyper prime on this,
you introduce

497
00:29:07,512 --> 00:29:11,850
additional complexity customs
and I expect you for energy formulation

498
00:29:12,250 --> 00:29:15,253
which allows you to account

499
00:29:15,253 --> 00:29:18,256
for how flat or

500
00:29:19,557 --> 00:29:21,659
how confident

501
00:29:21,659 --> 00:29:25,130
or precise you want your preferences
to be over

502
00:29:25,130 --> 00:29:28,133
the policy space.

503
00:29:28,433 --> 00:29:31,269
But the key thing to note is that

504
00:29:31,269 --> 00:29:34,372
for sake of simplicity,
I'm not going to go through a lot of the

505
00:29:34,939 --> 00:29:37,942
the details
about how these are optimized.

506
00:29:37,942 --> 00:29:40,745
But you can do that
in multiple different ways.

507
00:29:40,745 --> 00:29:44,516
For example, in active inference,
we can optimize expectation

508
00:29:44,516 --> 00:29:47,652
about the hidden states of interests,
the policies, the precision

509
00:29:47,719 --> 00:29:51,322
through inference, and then we can also
optimize the model parameters through

510
00:29:51,756 --> 00:29:54,492
through the planning procedures involved.

511
00:29:55,326 --> 00:29:58,630
But those sort of differ
depending on the setup.

512
00:29:58,630 --> 00:30:00,031
You're looking at for example,

513
00:30:00,031 --> 00:30:02,934
if you're using very full face,
you would just iterate this.

514
00:30:03,368 --> 00:30:08,072
These functions are objective functions
until convergence or an act of inference,

515
00:30:08,573 --> 00:30:12,477
you do a gradient descent to find
the sufficient statistics of interest.

516
00:30:12,744 --> 00:30:16,648
Again, this depends on exactly which
formulation and setup you're looking at.

517
00:30:18,183 --> 00:30:18,616
But the key

518
00:30:18,616 --> 00:30:23,788
thing to note here is that there's three
particular aspects to the active

519
00:30:23,788 --> 00:30:28,226
inference algorithm that are useful
and can be taken from this particular

520
00:30:28,893 --> 00:30:32,697
framework and apply to other settings.

521
00:30:33,198 --> 00:30:35,800
So I'm just going to reiterate
summarize them briefly.

522
00:30:36,100 --> 00:30:39,504
So we first have the to model
that's crucial so in order for an agent

523
00:30:39,504 --> 00:30:43,107
to interact and minimize a surprise,
it needs a gentleman of the world.

524
00:30:43,508 --> 00:30:47,111
And that's described as simply I'm
not I'm

525
00:30:47,312 --> 00:30:49,113
not including any of the model parameters

526
00:30:49,113 --> 00:30:52,717
here, but you can have your outcomes,
your states and your policies.

527
00:30:53,084 --> 00:30:56,287
And this these are decomposed into.

528
00:30:56,955 --> 00:30:59,791
Yes, sorry,
there's an error with the brackets here,

529
00:30:59,791 --> 00:31:03,895
but these are decomposed into your prior
your likelihood

530
00:31:03,895 --> 00:31:05,763
of your transition function.

531
00:31:05,763 --> 00:31:09,100
And then you set once you have this done
to model the objective,

532
00:31:09,100 --> 00:31:12,170
the agent is to set the model
to sample observations

533
00:31:12,170 --> 00:31:16,407
to reduce the price, and that is through
variation, energy optimization.

534
00:31:16,407 --> 00:31:17,775
So this particular tradeoff

535
00:31:17,775 --> 00:31:21,212
that we have between the complexity
and the accuracy cost.

536
00:31:21,746 --> 00:31:23,248
And then the second sorry,

537
00:31:23,248 --> 00:31:27,018
the last part of this algorithm
is to plan to select actions

538
00:31:27,018 --> 00:31:31,956
that minimize uncertainty,
that that is the expected strategy.

539
00:31:31,956 --> 00:31:34,926
And the way you do
that is by having a soft max over that

540
00:31:34,959 --> 00:31:38,830
this g negative of G, the quantity
we have here

541
00:31:39,130 --> 00:31:43,534
and then sampling for that in order
to select the next best, best action.

542
00:31:44,702 --> 00:31:46,804
OK, so that's a

543
00:31:47,138 --> 00:31:52,176
quick and deep dove into a massive amount
of active inference literature.

544
00:31:52,243 --> 00:31:53,177
I just wanted to highlight

545
00:31:53,177 --> 00:31:56,948
that those are the three core ingredients
that if you are interested

546
00:31:57,081 --> 00:31:59,250
in implementing
these algorithms yourself.

547
00:31:59,250 --> 00:32:02,186
OK, so now I'm just going to switch gears
a little bit

548
00:32:02,186 --> 00:32:05,256
and walk through the comparisons
with reinforcement learning.

549
00:32:05,657 --> 00:32:09,160
So in our what we considered
a modified version

550
00:32:09,160 --> 00:32:11,896
of the Open
Jim's Frozen Lake environment.

551
00:32:12,263 --> 00:32:16,334
So Frozen Lake has a grid like structure
with four distinct catches.

552
00:32:16,501 --> 00:32:20,605
It has a starting point,
which is s so we can see it.

553
00:32:20,705 --> 00:32:24,676
Yes, apologies,
but it's super tiny S's here

554
00:32:25,176 --> 00:32:28,112
and you've got the frozen surface,
which is F.

555
00:32:28,112 --> 00:32:32,383
So again, I don't think I can differentiate because I'm just moving my mouse.

556
00:32:33,785 --> 00:32:35,687
I'm zooming in, they can see it.

557
00:32:35,687 --> 00:32:36,654
I get perfect.

558
00:32:36,654 --> 00:32:39,290
So you've got the frozen s
and then you've got the hole.

559
00:32:39,757 --> 00:32:41,659
And then lastly, you have the goal.

560
00:32:41,659 --> 00:32:44,829
So you have with a first base located

561
00:32:45,296 --> 00:32:49,400
an oil patches in this particular
setup are safe, except for a hole

562
00:32:49,400 --> 00:32:53,538
where if the agent goes to H,
it gets a negative reward,

563
00:32:54,706 --> 00:32:56,574
the agent starts each episode

564
00:32:56,574 --> 00:33:00,078
at the first position,
which is the starting position.

565
00:33:00,211 --> 00:33:03,614
And from there
it needs to reach the first location in

566
00:33:05,283 --> 00:33:07,952
in the
in the least amount of steps possible.

567
00:33:07,952 --> 00:33:10,488
And the way it can do
that is by performing

568
00:33:10,788 --> 00:33:14,459
four different types of actions
either going left, right down or up

569
00:33:15,293 --> 00:33:17,462
and the agent is allowed to carry

570
00:33:17,462 --> 00:33:20,698
on moving through the frozen lake
with multiple revisit.

571
00:33:20,698 --> 00:33:22,433
So settings can go back to the starting
position.

572
00:33:22,433 --> 00:33:26,337
Having gone in other places
but each episode will end

573
00:33:26,337 --> 00:33:29,807
when either
it reaches the hole or the goal location.

574
00:33:30,842 --> 00:33:32,810
And these locations

575
00:33:32,810 --> 00:33:37,081
differ depending on the set up
that we have in our simulations.

576
00:33:37,382 --> 00:33:43,054
So in one set up, the position of
the hole is eight and the goal is six.

577
00:33:43,321 --> 00:33:48,026
And another set up
the position of the hole is six and

578
00:33:49,327 --> 00:33:50,895
the goal is eight.

579
00:33:50,895 --> 00:33:54,832
And the objective is,
as I said, to reach the goal and it's few

580
00:33:54,832 --> 00:33:58,603
steps as possible while avoiding the hole
because that would end the episode.

581
00:33:59,470 --> 00:34:01,773
If it reaches the goal,
it gets the positive reward.

582
00:34:01,806 --> 00:34:04,675
100 and negative otherwise.

583
00:34:05,710 --> 00:34:08,880
The key thing to note here
is that the scoring metric actually

584
00:34:08,880 --> 00:34:12,750
allows us some way to compare the active
inference algorithms to a reinforcement

585
00:34:12,750 --> 00:34:13,851
learning algorithms

586
00:34:13,851 --> 00:34:18,389
but it is not really important
for the active inference algorithm to

587
00:34:19,791 --> 00:34:22,026
have the reward function as a get go

588
00:34:22,026 --> 00:34:26,664
because it can still move around using
just the information gain time.

589
00:34:26,664 --> 00:34:29,600
So not having the extrinsic value
component.

590
00:34:30,668 --> 00:34:35,006
And that's quite interesting
because we'll see the ramifications

591
00:34:35,006 --> 00:34:38,743
of that in our simulations
for this particular set up.

592
00:34:38,743 --> 00:34:41,979
We limit to the maximum number of time
steps for each episode

593
00:34:42,280 --> 00:34:44,749
to 15 OK,

594
00:34:45,783 --> 00:34:48,753
so what I'm going to do
is first talk through the gentle model

595
00:34:48,753 --> 00:34:50,855
that we use for the active inference
formulation.

596
00:34:51,823 --> 00:34:54,392
So here what you're seeing on
the slide is a graphical

597
00:34:54,392 --> 00:34:57,061
representation of the active inference
gentle model.

598
00:34:57,562 --> 00:34:59,730
So this model contains
four action states.

599
00:34:59,931 --> 00:35:02,400
So right down up to the left

600
00:35:03,434 --> 00:35:06,404
and these control
the ability to transition

601
00:35:06,404 --> 00:35:09,474
between
hidden states in the case, in fact.

602
00:35:09,774 --> 00:35:12,510
So for example,
if you are in a position where

603
00:35:13,044 --> 00:35:17,815
and you take the action right,
then you end up in position two

604
00:35:18,316 --> 00:35:22,520
or if you are in location five
and you take the other action

605
00:35:22,520 --> 00:35:24,789
you will end up in position two as well.

606
00:35:27,892 --> 00:35:31,062
In this
particular setting, both positions

607
00:35:31,062 --> 00:35:34,031
six and eight are absolving states
because if you remember,

608
00:35:34,332 --> 00:35:37,969
once the agent goes to that location,
they're not able to move out.

609
00:35:37,969 --> 00:35:40,605
So that's when the episode ends.

610
00:35:40,605 --> 00:35:44,375
And if an agent makes an improper move
in this particular case, for example,

611
00:35:44,375 --> 00:35:46,611
if it tries to go from position
one to left

612
00:35:46,978 --> 00:35:49,013
and with a state in that location,
it won't move

613
00:35:50,047 --> 00:35:52,216
in this particular gentle model.

614
00:35:52,216 --> 00:35:54,218
So I'm just looking
at the hidden states now.

615
00:35:54,619 --> 00:35:58,122
We have a to transfer product
between the two factors.

616
00:35:58,122 --> 00:36:00,291
So we have location and context here.

617
00:36:00,725 --> 00:36:04,862
The context cannot be changed
by the agent that we have

618
00:36:05,196 --> 00:36:07,965
because this is something that's
determined by the environment in which

619
00:36:08,199 --> 00:36:11,235
this determines
what are the going to hold locations are,

620
00:36:11,869 --> 00:36:14,805
whereas the location is something
that the agent has control over.

621
00:36:14,805 --> 00:36:17,542
And that's why
we have the action states over this.

622
00:36:19,410 --> 00:36:21,512
So with
the context, we have two contexts.

623
00:36:21,946 --> 00:36:24,482
The first one is where the goal is in

624
00:36:25,616 --> 00:36:28,953
your location eight
and the is in the patient.

625
00:36:28,986 --> 00:36:33,691
Six And the second context
is where the goal is application six

626
00:36:33,891 --> 00:36:36,194
and the hole is in the case eight

627
00:36:38,362 --> 00:36:39,697
at each time point,

628
00:36:39,697 --> 00:36:43,601
the agent will observe two outcomes.

629
00:36:43,701 --> 00:36:46,404
One would be its own position
in this particular maze.

630
00:36:46,771 --> 00:36:50,007
And the second would be the school
that the agent would get

631
00:36:51,008 --> 00:36:51,876
the likelihood for.

632
00:36:51,876 --> 00:36:55,580
The great position is entirely determined
by the location of the agent

633
00:36:56,047 --> 00:36:59,684
and the school is determined by

634
00:37:01,519 --> 00:37:04,222
both the location of the context in play.

635
00:37:04,222 --> 00:37:08,392
So if the agent is in location six

636
00:37:08,960 --> 00:37:12,964
and eight is in context to,
then I will receive a positive reward.

637
00:37:13,130 --> 00:37:17,501
Otherwise it will receive a negative or
neutral reward depending on where it is

638
00:37:20,238 --> 00:37:20,805
based

639
00:37:20,805 --> 00:37:24,175
on trying to make that comparison
with reinforcement learning.

640
00:37:24,175 --> 00:37:27,144
What we're doing here
is we are introducing

641
00:37:27,578 --> 00:37:31,015
price preferences
where the agent has plus school

642
00:37:32,016 --> 00:37:35,486
for positive reward, negative for school,

643
00:37:36,387 --> 00:37:39,357
sorry, minus
false negative reward and otherwise.

644
00:37:39,357 --> 00:37:44,962
And at the first stage it expects itself
to be in the first location

645
00:37:47,531 --> 00:37:51,435
so we compared this particular
Gem's model

646
00:37:51,435 --> 00:37:55,539
and the active inference agent
to two reinforcement learning algorithms.

647
00:37:55,539 --> 00:37:59,944
So the first one was the Q learning
using epsilon greedy exploration.

648
00:38:00,444 --> 00:38:03,414
And the second one was the Bayesian
model based reinforcement

649
00:38:03,414 --> 00:38:06,150
learning algorithm
using standard sports and sampling

650
00:38:06,717 --> 00:38:10,154
and some sum sampling is appropriate
procedure higher because it entails

651
00:38:10,154 --> 00:38:15,459
the optimization of few objectives,
reward maximization and information gain

652
00:38:15,960 --> 00:38:18,195
and this is achieved
by having this distribution

653
00:38:18,195 --> 00:38:21,465
over a particular function
that we promote twice by a prior,

654
00:38:21,465 --> 00:38:24,969
just by having a prior distribution
over it that we sample from

655
00:38:27,204 --> 00:38:33,110
K so for

656
00:38:33,144 --> 00:38:38,716
the 22 learning algorithms,
we have to explain greedy parameters.

657
00:38:39,016 --> 00:38:43,721
So one where it's fixed exploration
set to 0.1, and then another one

658
00:38:43,721 --> 00:38:47,191
where we have the king exploration
that starts from one and decays

659
00:38:47,191 --> 00:38:48,859
down to zero

660
00:38:50,961 --> 00:38:53,631
so first we assessed this

661
00:38:53,631 --> 00:38:56,801
how the agents interacted
in a stationary setting where the reward

662
00:38:56,801 --> 00:38:58,502
was not changing

663
00:38:58,502 --> 00:39:01,639
such that the goal location was always at
six and the whole location was

664
00:39:01,772 --> 00:39:03,107
always at eight.

665
00:39:03,107 --> 00:39:05,343
And then we evaluate
the performance of the agents.

666
00:39:05,710 --> 00:39:08,979
The key thing to take away from here
is that both the Bayesian general

667
00:39:08,979 --> 00:39:12,683
and the active inference agents
are able to quickly learn

668
00:39:13,451 --> 00:39:16,687
whether reward location is
and just maximize it out.

669
00:39:16,687 --> 00:39:20,725
And this is this is performance
is consistent, denoted by the body

670
00:39:20,758 --> 00:39:24,462
type confidence problems
that we see in comparison.

671
00:39:24,462 --> 00:39:26,764
The Q learning agents are

672
00:39:27,698 --> 00:39:29,900
so for the one where we have

673
00:39:30,201 --> 00:39:32,470
six exploration, it's fairly

674
00:39:32,970 --> 00:39:37,074
fairly good and is able to learn
where the reward is allocated.

675
00:39:37,074 --> 00:39:42,613
But there is some deviation generated by
that 10% of selected more random action,

676
00:39:43,914 --> 00:39:44,515
whereas the

677
00:39:44,515 --> 00:39:48,252
Q learning where we have
Epsilon is equal to 1280.

678
00:39:49,320 --> 00:39:51,455
The performance isn't the greatest.

679
00:39:51,455 --> 00:39:55,259
And for the No model, the active
inference where there is no reward here,

680
00:39:55,559 --> 00:39:58,396
the agent randomly goes to the hole
and randomly goes

681
00:39:58,396 --> 00:40:00,398
to the goal 50% of the time.

682
00:40:01,499 --> 00:40:03,968
But the key thing to note
is that apart from the non model,

683
00:40:04,201 --> 00:40:07,371
all the models are doing fairly
well and seem

684
00:40:07,371 --> 00:40:10,241
to be performing OK
within this stationary setting.

685
00:40:11,041 --> 00:40:11,842
So the next thing.

686
00:40:11,842 --> 00:40:14,311
Nor can I just make a point
about these experiment as well

687
00:40:14,311 --> 00:40:18,182
just because they also do bear away
slightly from the traditional light,

688
00:40:18,983 --> 00:40:21,252
which is yeah, great.

689
00:40:21,252 --> 00:40:24,355
So basically what usually happens is
you kind of have

690
00:40:24,355 --> 00:40:27,458
this ambiguous between like training time
and test time performance.

691
00:40:27,458 --> 00:40:28,793
And you usually especially in a

692
00:40:28,793 --> 00:40:31,662
Q learning, like you just take
the max over the cue function.

693
00:40:31,662 --> 00:40:34,665
We have a policy
that tries to do like some search

694
00:40:34,665 --> 00:40:38,102
over the cue function
and takes that max like given a state.

695
00:40:39,537 --> 00:40:41,338
But that's
kind of an artificial distinction,

696
00:40:41,338 --> 00:40:45,810
which is like obviously as you're
acquiring data, you're making mistakes

697
00:40:45,810 --> 00:40:47,611
in the environment, you're interacting
with the real environment.

698
00:40:47,611 --> 00:40:48,179
So in order

699
00:40:48,179 --> 00:40:51,849
to kind of make that fair comparison
between here and active inference

700
00:40:52,249 --> 00:40:53,551
where you don't really have
this distinction

701
00:40:53,551 --> 00:40:56,787
between like training time and test
time, it's all just interaction.

702
00:40:57,388 --> 00:41:00,224
The that's the reason
why the Q learning agent, especially when

703
00:41:00,224 --> 00:41:04,094
Epsilon is fixed to say 0.1
never achieves the optimal policy simply

704
00:41:04,094 --> 00:41:07,765
because we're also with 0.1 probability
taking a random action.

705
00:41:08,065 --> 00:41:11,368
So we're not making this distinction
like you sometimes would with normal rule

706
00:41:11,368 --> 00:41:12,570
between train and test.

707
00:41:12,570 --> 00:41:15,473
Like we're saying, train
and test some of the same thing.

708
00:41:15,473 --> 00:41:16,373
So however

709
00:41:16,373 --> 00:41:19,710
you're choosing to interact with
the world is how you should be assessed.

710
00:41:19,877 --> 00:41:21,846
So that's that's why

711
00:41:21,846 --> 00:41:23,881
I'm actually looking at this, because
you might be like, hang on, my why isn't.

712
00:41:23,881 --> 00:41:24,882
Q letting solving this?

713
00:41:24,882 --> 00:41:28,352
But yeah, that's just a clear
something up if a more familiar

714
00:41:28,352 --> 00:41:32,189
with, say, somebody like
de pro experimental procedure

715
00:41:34,658 --> 00:41:35,092
perfect.

716
00:41:35,092 --> 00:41:36,360
Thank you.

717
00:41:36,727 --> 00:41:39,363
OK, and

718
00:41:39,363 --> 00:41:43,434
then just following on from that,
so we change the environment a little bit

719
00:41:43,434 --> 00:41:47,505
to make it a little bit more difficult
to see whether the Bayesian

720
00:41:47,505 --> 00:41:50,941
and the active inference agents
might struggle when we stopped

721
00:41:50,941 --> 00:41:54,311
having the reward occasion change
after every few episodes.

722
00:41:54,678 --> 00:41:58,282
So specifically we swapped the goal
and the whole location at

723
00:41:59,483 --> 00:42:03,020
a time point 21 a time, 120,

724
00:42:03,020 --> 00:42:06,991
141, 251 and 451.

725
00:42:07,258 --> 00:42:11,061
So you can see that in these figures
where the line, the gray lines are shown

726
00:42:11,161 --> 00:42:13,464
that these points,
they're all location flipped.

727
00:42:14,198 --> 00:42:17,735
So like the stationary setting
for the first 20 trials

728
00:42:17,735 --> 00:42:22,006
or the agents seem to be doing
as you would expect for both the Bayesian

729
00:42:22,439 --> 00:42:25,009
RL and the action
inference agents are doing fairly.

730
00:42:25,009 --> 00:42:29,713
OK, the Q landing with the fixed
explorations set

731
00:42:29,713 --> 00:42:34,752
to 0.1 is doing fairly
OK as we saw before and the Q learning

732
00:42:34,952 --> 00:42:39,290
with the decay exploration is
is doing as it was doing before.

733
00:42:39,590 --> 00:42:42,526
But when you flip it around such

734
00:42:42,526 --> 00:42:44,762
that the goal locations change,

735
00:42:45,663 --> 00:42:50,568
what you notice is that with the Bayesian
RL agent, the

736
00:42:51,035 --> 00:42:54,672
the amount of reward or the score
it gets is quite low.

737
00:42:54,805 --> 00:42:59,443
And then we see a phase
where it then transitions and becomes

738
00:42:59,910 --> 00:43:05,015
to the optimal policy again in comparison
to the active inference agent

739
00:43:05,015 --> 00:43:10,020
where it instantly after the first trial
of doing the incorrect is able

740
00:43:10,020 --> 00:43:14,325
to switch over to the active
and sorry the appropriate policy.

741
00:43:14,658 --> 00:43:19,563
And the reason for that is for these oral
settings where we're considering Yes.

742
00:43:19,563 --> 00:43:20,965
And learning problem.

743
00:43:20,965 --> 00:43:24,835
You need to first do professor
learning of where the reward location is

744
00:43:24,969 --> 00:43:27,271
and then the new reward location.

745
00:43:27,271 --> 00:43:28,706
So you're seeing that for.

746
00:43:28,706 --> 00:43:33,043
Q learning about the different epsilon
greedy dramatizations

747
00:43:33,043 --> 00:43:34,445
and also for the patient zero.

748
00:43:34,445 --> 00:43:38,215
And we're seeing that consistently
whereas with the active inference agent,

749
00:43:38,215 --> 00:43:40,618
because we're treating it
as a timing inference problem

750
00:43:40,618 --> 00:43:44,321
where the plots two years from the
previous date are moved over to the prize

751
00:43:44,655 --> 00:43:49,159
and we do the rest prize,
the agent is able to instantly

752
00:43:49,693 --> 00:43:54,331
realize that the current policy
that was following the less time step

753
00:43:54,331 --> 00:43:58,769
is an appropriate and switches its policy
to the next to the, the other one.

754
00:44:00,004 --> 00:44:00,871
Again,

755
00:44:00,871 --> 00:44:03,841
the non model as expected
doesn't really do much because it's

756
00:44:03,841 --> 00:44:08,379
just exploring doesn't really count where
the reward or the whole location are.

757
00:44:09,947 --> 00:44:12,182
OK, and so do you want to add something

758
00:44:12,182 --> 00:44:13,684
to this or.

759
00:44:15,452 --> 00:44:17,821
No, I think the same point applies
to this one.

760
00:44:18,422 --> 00:44:20,924
Yep, definitely. OK,

761
00:44:22,259 --> 00:44:25,195
so just to wrap up
with these two comparisons and

762
00:44:26,263 --> 00:44:29,333
with the non well the stationary setting,

763
00:44:29,900 --> 00:44:33,003
all types of agents
and including the patient oral

764
00:44:33,003 --> 00:44:35,506
and the Q letting would be reasonable
frameworks. Yes.

765
00:44:35,806 --> 00:44:39,076
Whereas with the with a non state
forensic

766
00:44:39,243 --> 00:44:41,612
stochastic
setting having an active inference

767
00:44:41,612 --> 00:44:44,782
agents might be appropriate way
of having change in dynamics.

768
00:44:45,949 --> 00:44:48,986
The key caveat with that
is that you can introduce

769
00:44:48,986 --> 00:44:53,590
a lot more additional complexity
into the patient RL or the Q learning

770
00:44:53,624 --> 00:44:58,729
or the RL framework in general
to allow for way to handle uncertainty,

771
00:44:58,729 --> 00:45:01,799
but it wouldn't be
a natural way of adding it.

772
00:45:01,932 --> 00:45:03,000
You would have to sort of

773
00:45:03,967 --> 00:45:04,968
augment the

774
00:45:04,968 --> 00:45:08,138
function or the algorithm
in particular ways to justify

775
00:45:10,107 --> 00:45:11,241
OK, so,

776
00:45:11,408 --> 00:45:13,711
so once we have done this comparison,
we were interested in,

777
00:45:15,045 --> 00:45:17,648
why would you even want to use
active and friends

778
00:45:19,083 --> 00:45:23,153
when you don't have
an understanding about the world or

779
00:45:23,153 --> 00:45:27,257
you don't have sort of a preference over
the type of things that could be done?

780
00:45:27,257 --> 00:45:30,761
Because as we saw the active inference
model the model is just exploring.

781
00:45:30,761 --> 00:45:32,763
It's not really doing much.

782
00:45:32,763 --> 00:45:36,166
And that's brings us to
one of the initial points that introduce

783
00:45:36,500 --> 00:45:39,803
to start the presentation, which is that
within an active inference line,

784
00:45:39,870 --> 00:45:43,340
what we don't really care about having
a reward function that we can learn.

785
00:45:43,907 --> 00:45:48,912
We can learn that based on
some interaction with the environment and

786
00:45:49,913 --> 00:45:52,116
so for that

787
00:45:52,182 --> 00:45:55,352
what we did was we carried out
a few different simulations

788
00:45:55,352 --> 00:45:58,622
to see how the active inference agents
can select different types

789
00:45:58,622 --> 00:46:01,191
of policies
in the absence of prior preferences.

790
00:46:02,126 --> 00:46:05,195
And for this, we
we did three different experiments

791
00:46:05,696 --> 00:46:08,665
where we allowed either or the likelihood

792
00:46:09,032 --> 00:46:13,103
and all the outcome preferences
to be large over time.

793
00:46:13,103 --> 00:46:17,808
And so how the agent interacted
when this learning over

794
00:46:17,808 --> 00:46:21,345
the different preferences took place

795
00:46:21,812 --> 00:46:26,450
and the way we did
it was using the conjugate models where

796
00:46:26,817 --> 00:46:28,285
because these are discrete state models

797
00:46:28,285 --> 00:46:31,488
we have categorical distributions
over our model parameters

798
00:46:31,855 --> 00:46:36,193
and we're introducing dirichlet
distributions on top as our hyper price.

799
00:46:36,193 --> 00:46:39,496
And that is a type of prize

800
00:46:39,596 --> 00:46:42,366
we can go into the details of this
if anyone has any questions.

801
00:46:42,366 --> 00:46:47,171
But just for simplicity,
just assume that all the Jewish life

802
00:46:47,171 --> 00:46:52,609
distributions were set to set
completely flat and the agents were all

803
00:46:53,210 --> 00:46:56,580
given an opportunity to land
based on different interactions they had.

804
00:46:57,381 --> 00:47:00,484
So the first experiment,
like the simulations that we ran,

805
00:47:00,484 --> 00:47:03,954
were to understand how the agent reduces

806
00:47:03,954 --> 00:47:06,957
uncertainty about the environment
that is interaction.

807
00:47:07,090 --> 00:47:10,260
So if it doesn't know that the, the, the,

808
00:47:10,928 --> 00:47:15,065
the frozen lake, how would it interact
or explore that lake?

809
00:47:16,233 --> 00:47:18,936
And what we see is that

810
00:47:18,936 --> 00:47:22,973
in this particular instance,
the agent was just interested

811
00:47:22,973 --> 00:47:26,376
in exploring and didn't
particularly care about the

812
00:47:26,910 --> 00:47:29,446
where the goal or the whole location was.

813
00:47:29,446 --> 00:47:32,616
And this is highlighted in a series

814
00:47:32,616 --> 00:47:36,253
of different
exploration trajectories that we see.

815
00:47:36,253 --> 00:47:39,356
So the first one was
where the agent falls in the hole,

816
00:47:40,224 --> 00:47:44,995
and then another one where it goes around
and ends up at the second.

817
00:47:44,995 --> 00:47:47,965
And in the second episode
at the goal location.

818
00:47:48,198 --> 00:47:52,502
And then somewhere it just ends
the episode by just going back and forth.

819
00:47:52,502 --> 00:47:56,106
So this is just for exploration,
nothing more here.

820
00:47:58,008 --> 00:47:59,276
The next

821
00:47:59,743 --> 00:48:04,181
set of analysis or simulations
that we ran was to see what would happen

822
00:48:04,181 --> 00:48:06,450
if the agent knew about the world
but had no

823
00:48:06,450 --> 00:48:10,187
preferences of the type of outcomes
they expected itself to be in.

824
00:48:10,654 --> 00:48:14,691
And for this, we ran multiple different

825
00:48:15,058 --> 00:48:19,062
simulations and so that in the absence of

826
00:48:20,364 --> 00:48:20,964
any sort

827
00:48:20,964 --> 00:48:24,001
of preferences,
the hole could actually become

828
00:48:24,001 --> 00:48:27,271
really attractive
if it's encountered first.

829
00:48:27,604 --> 00:48:30,173
So we see that in the first figure

830
00:48:30,307 --> 00:48:33,076
that the agent learns
to prefer to hide in holes.

831
00:48:34,011 --> 00:48:36,413
And in the second type of trial,
that we saw

832
00:48:36,413 --> 00:48:38,749
was where the agent

833
00:48:39,483 --> 00:48:42,953
exhibited the preference
to actually go to the goal location.

834
00:48:42,953 --> 00:48:46,189
So this is entirely dependent
on the incentive option

835
00:48:46,223 --> 00:48:49,626
or the type of stimulus
that the agent is exposed to initially

836
00:48:49,860 --> 00:48:56,600
that determines the type of preferences
it would like to have OK.

837
00:48:56,700 --> 00:49:01,972
And then the last set of simulations
that we ran was just to check

838
00:49:01,972 --> 00:49:05,342
what would happen
when we interacted with the epistemic

839
00:49:05,742 --> 00:49:09,313
to such a result, uncertainty
about the environment that the agent

840
00:49:09,313 --> 00:49:10,447
was interacting with,

841
00:49:10,447 --> 00:49:14,084
specifically the likelihood mapping
between the outcomes given the states

842
00:49:14,084 --> 00:49:15,352
and the uncertainty

843
00:49:15,352 --> 00:49:19,389
about the desired state of affairs
that the agent expected herself to be in.

844
00:49:19,856 --> 00:49:23,694
And what we saw was that if we allowed

845
00:49:24,161 --> 00:49:28,532
a sufficient number of cons to cost
the agent in this particular setting land

846
00:49:28,532 --> 00:49:34,438
to prefer to ride us or hide in holes
after X number of episodes.

847
00:49:34,438 --> 00:49:35,672
But it had a very distinct

848
00:49:36,640 --> 00:49:37,741
preference over

849
00:49:37,741 --> 00:49:42,179
the type of outcomes it expected itself
to be depending on the particular

850
00:49:42,813 --> 00:49:44,915
trajectory as a particular time point.

851
00:49:46,416 --> 00:49:50,821
And so that brings me
to the last simulation.

852
00:49:50,821 --> 00:49:53,790
So I'm just going to wrap up

853
00:49:53,790 --> 00:49:56,660
which is in active inference,

854
00:49:57,294 --> 00:50:00,330
I saw over presentation
what saying it's a particular algorithm

855
00:50:00,330 --> 00:50:03,967
that gives us very nice things
to consider when we're

856
00:50:04,568 --> 00:50:07,437
operating in Bayesian or belief

857
00:50:07,437 --> 00:50:09,573
based setting, which is firstly

858
00:50:10,474 --> 00:50:14,011
the principle account of epistemic
exploration and intrinsic motivation

859
00:50:14,011 --> 00:50:17,848
that we get from the particular
X free energy decomposition

860
00:50:17,848 --> 00:50:19,349
that we went through.

861
00:50:19,349 --> 00:50:22,586
The second thing that I wanted to
highlight was that

862
00:50:23,353 --> 00:50:26,523
under an active inference setting,
we don't have to explicitly specify

863
00:50:26,523 --> 00:50:28,992
a reward function,
which we saw in the last

864
00:50:29,793 --> 00:50:31,862
the second set of simulations

865
00:50:31,862 --> 00:50:35,032
where the agent can also
learn its own reward and prefer to become

866
00:50:35,032 --> 00:50:38,635
something that is quite counterintuitive
from a RL setting

867
00:50:38,635 --> 00:50:41,805
where the signal from environment
to saying something is bad,

868
00:50:41,805 --> 00:50:45,842
but the agents in tandem
occupations of preference allow it to

869
00:50:45,842 --> 00:50:50,447
then do something that's quite at odds
with what the environment is expecting it

870
00:50:50,447 --> 00:50:51,281
to do.

871
00:50:51,348 --> 00:50:54,017
And lastly,
because of this belief, Bayesian

872
00:50:54,017 --> 00:50:56,987
setting uncertainty
is a natural part of the beliefs.

873
00:50:57,054 --> 00:50:57,821
Updating

874
00:50:59,623 --> 00:51:02,025
so within the stationary settings, active

875
00:51:02,025 --> 00:51:04,861
inference agents perform
as well as reinforcement learning agents.

876
00:51:05,195 --> 00:51:08,899
However, enunciation settings
that perform due to their ability

877
00:51:08,899 --> 00:51:12,903
to carry out this planning inference
and that's I think that holds.

878
00:51:13,703 --> 00:51:16,173
This is not a conclusive statement,
but is is a

879
00:51:16,373 --> 00:51:18,675
nice way to start thinking about how

880
00:51:19,576 --> 00:51:22,512
if you're scaling up
active inference agents to interact

881
00:51:22,512 --> 00:51:26,316
on the same type of environments
as reinforcement learning agents,

882
00:51:26,750 --> 00:51:29,853
that might be a little bit hard
to resolve

883
00:51:29,853 --> 00:51:31,922
because there is some nonspeech
narrative,

884
00:51:31,922 --> 00:51:34,424
some weird fluctuations
happening in the assignment,

885
00:51:34,724 --> 00:51:38,028
and that's an inference agents
could potentially perform quite nicely

886
00:51:38,028 --> 00:51:40,063
if we have this planning as an incentive

887
00:51:43,066 --> 00:51:45,502
and that brings me
to the end of the presentation.

888
00:51:45,502 --> 00:51:48,271
So I just want to thank everyone
who was involved with this work.

889
00:51:48,271 --> 00:51:51,842
So I call Tom and Phil
and everyone who's helped me

890
00:51:51,842 --> 00:51:55,011
think through these interesting ideas
that I presented.

891
00:51:55,645 --> 00:51:57,914
So and everyone also for listening.

892
00:51:57,914 --> 00:51:58,381
Thank you.

893
00:51:59,950 --> 00:52:00,984
Thank you.

894
00:52:01,384 --> 00:52:03,687
Awesome talk.

895
00:52:04,387 --> 00:52:07,991
So you can maybe unshared
and we can ask a few questions

896
00:52:07,991 --> 00:52:10,827
and if anyone who's watching
live would like to ask questions, there

897
00:52:11,394 --> 00:52:12,963
more than welcome.

898
00:52:12,963 --> 00:52:16,166
So maybe I'll start with just

899
00:52:16,166 --> 00:52:20,504
a general point that it was awesome
to see how clearly you differentiated

900
00:52:20,504 --> 00:52:24,374
between the reinforcement learning
and the active inference paradigm.

901
00:52:24,741 --> 00:52:29,279
And just one question I had was
you mentioned that in the case of a time

902
00:52:29,279 --> 00:52:33,450
step of one, a time horizon of one,
there was like an equivalence between

903
00:52:33,450 --> 00:52:36,319
the reinforcement learning approach
and the active inference approach.

904
00:52:36,720 --> 00:52:40,023
Now people are using reinforcement
learning to do

905
00:52:40,323 --> 00:52:43,126
planning for the future
amidst uncertainty.

906
00:52:43,560 --> 00:52:47,430
So how are they accomplishing
those kinds of planning

907
00:52:47,797 --> 00:52:51,268
and where are some situations
where active inference

908
00:52:51,468 --> 00:52:55,672
might be able to step into those settings
and potentially do better

909
00:52:57,207 --> 00:52:59,209
now? I
think that's a really good question.

910
00:52:59,209 --> 00:53:01,478
And so frame for you to jump in
if I missed something.

911
00:53:01,478 --> 00:53:05,248
But so within the oral setting,
from what I know,

912
00:53:05,248 --> 00:53:09,119
if they are considering temporal
horizon is greater than one,

913
00:53:09,119 --> 00:53:12,189
then they have hierarchical RL
or they have options

914
00:53:12,622 --> 00:53:16,626
where they're considering trajectories
that are greater than one

915
00:53:16,626 --> 00:53:20,130
that allow them
to consider a whole sequence of play

916
00:53:21,631 --> 00:53:24,067
before they like that's rolled out.

917
00:53:24,334 --> 00:53:28,872
If they select a particular policy
instead of like that one single action

918
00:53:28,872 --> 00:53:30,907
to statement thing.

919
00:53:32,776 --> 00:53:36,746
But I'm not I haven't really worked
with options that much though.

920
00:53:36,780 --> 00:53:37,714
Do you know?

921
00:53:38,381 --> 00:53:41,218
Yes. So options are like one way to

922
00:53:41,218 --> 00:53:44,521
achieve this kind of multistep sort of

923
00:53:45,555 --> 00:53:48,892
like I guess oversize,
almost like this inductive bias, right?

924
00:53:48,892 --> 00:53:53,396
That you will like your choices
will be kind of contiguous blocks of

925
00:53:53,396 --> 00:53:56,366
like more than one step,
whereas a kind of policy kind of it's

926
00:53:56,366 --> 00:53:59,069
just like I'm going to take one step
and then I'll have this kind of

927
00:53:59,402 --> 00:54:00,537
next stay resistance.

928
00:54:02,038 --> 00:54:02,706
But I think

929
00:54:02,706 --> 00:54:05,875
what's important to disambiguate
is actually you can do sort of planning

930
00:54:05,875 --> 00:54:06,910
in the model style things

931
00:54:06,910 --> 00:54:10,347
because you can view active inference
as a sort of model based like

932
00:54:10,947 --> 00:54:13,083
in the kind of RL paradigm.

933
00:54:13,416 --> 00:54:17,354
And indeed, like within model based,
you can have

934
00:54:18,989 --> 00:54:22,292
like planning methods similar to
what is done in active inference.

935
00:54:22,292 --> 00:54:26,896
However, I guess like the key distinction
here is when you do planning in model

936
00:54:26,896 --> 00:54:29,299
based on real,
you simply just trying to type.

937
00:54:29,299 --> 00:54:32,402
You tend to take some sort of suit
like kind of mini

938
00:54:32,402 --> 00:54:35,205
evolutionary method, usually
something like the cross entropy method.

939
00:54:35,672 --> 00:54:36,206
And what you're doing

940
00:54:36,206 --> 00:54:40,010
is this kind of search overall actions
that you take at a certain point in time

941
00:54:40,310 --> 00:54:44,481
that maximizes your kind of reward over
say a horizon of say 20 steps

942
00:54:45,282 --> 00:54:48,251
and you can do some clever stuff, you can
terminate with a Q value function.

943
00:54:49,219 --> 00:54:50,920
But fundamentally, like

944
00:54:50,920 --> 00:54:55,892
it doesn't take you away from this sort
of definitional difference, which is in

945
00:54:56,026 --> 00:54:59,029
active inference, you're doing this
sort of planning over actions,

946
00:54:59,029 --> 00:55:02,499
even though they might seem superficially
similar, like kind of basically

947
00:55:02,499 --> 00:55:06,036
like drawing lots of actions and seeing
which ones maximize some sort of utility.

948
00:55:06,036 --> 00:55:10,774
In the case of active inference,
or the useful kind of information

949
00:55:10,774 --> 00:55:13,910
and the desiderata about like exploration

950
00:55:14,311 --> 00:55:18,682
and exploitation
are wrapped up in that maximization.

951
00:55:18,682 --> 00:55:21,451
Whereas in all it's
kind of it's a bit harder to understand

952
00:55:22,485 --> 00:55:25,922
like in the classical sense,
we're just trying to maximize reward.

953
00:55:26,323 --> 00:55:29,125
But you can have heuristics
where you say, oh, but maybe

954
00:55:29,125 --> 00:55:32,295
I want to also maximize
some notion of model uncertainty.

955
00:55:32,295 --> 00:55:34,064
And it kind of gets a bit

956
00:55:35,732 --> 00:55:36,533
more difficult to

957
00:55:36,533 --> 00:55:39,202
naturally integrate
all these approaches into the same thing

958
00:55:40,303 --> 00:55:43,173
because like in the example of factoring,
because everything's a distribution,

959
00:55:43,406 --> 00:55:44,407
you just get a higher priority.

960
00:55:44,407 --> 00:55:47,510
Or if you want to integrate it out,
if you don't want to deal with

961
00:55:47,744 --> 00:55:48,812
kind of tuning it.

962
00:55:48,812 --> 00:55:51,981
So yeah, that's kind of where I feel like
there is this kind of difference

963
00:55:53,983 --> 00:55:54,584
cool.

964
00:55:54,584 --> 00:55:59,656
And what kinds of settings
do you think that those types of, of

965
00:56:00,557 --> 00:56:04,894
action as inference and planning,
as inference could be utilized,

966
00:56:04,894 --> 00:56:09,799
like what kind of data sets or questions
or contexts are people currently

967
00:56:09,799 --> 00:56:11,201
using another type of method?

968
00:56:11,201 --> 00:56:14,037
But then you're excited
to see active inference play a role.

969
00:56:18,808 --> 00:56:20,310
I think most of the open

970
00:56:20,310 --> 00:56:24,080
ended problems where you don't really
have a reward function, I think

971
00:56:25,782 --> 00:56:26,816
because the

972
00:56:26,816 --> 00:56:30,620
way the the oral agent is learning
how to interact with

973
00:56:30,620 --> 00:56:32,956
the environment
is through a reward function.

974
00:56:34,491 --> 00:56:36,626
So anything
where you don't really have that,

975
00:56:36,626 --> 00:56:39,863
where you have an environment
that is changing.

976
00:56:41,598 --> 00:56:42,899
But I know that there's like

977
00:56:42,899 --> 00:56:46,770
a whole host of people
within the oral community

978
00:56:46,770 --> 00:56:51,007
working on intrinsic motivation
or internal motivation.

979
00:56:51,541 --> 00:56:55,145
So those sort of things do overlap
with the activities

980
00:56:55,178 --> 00:56:56,946
discrimination

981
00:56:58,948 --> 00:57:00,750
but particular paradigms.

982
00:57:00,750 --> 00:57:04,921
I think for me the more interesting
aspect of active inference

983
00:57:04,921 --> 00:57:07,557
comes from the fact
when you start thinking about

984
00:57:09,426 --> 00:57:13,129
biological agents
and if you're modeling a patient

985
00:57:13,296 --> 00:57:17,667
or someone with schizophrenia,
you can with this Bayesian framework

986
00:57:17,667 --> 00:57:20,370
you can change the priors

987
00:57:20,703 --> 00:57:24,707
to try and see
how the person is interacting,

988
00:57:24,941 --> 00:57:27,210
where I guess it's

989
00:57:27,210 --> 00:57:29,913
the it could be that their policy,
like the way they're following

990
00:57:29,913 --> 00:57:35,084
the policies, are broken or just could be
are the parts that are different.

991
00:57:35,318 --> 00:57:40,623
But I think within the oral setting,
if you are stepping outside the

992
00:57:43,493 --> 00:57:45,862
the standard the game,

993
00:57:45,862 --> 00:57:49,399
right, so like Majic or Jim environments
and start getting into ones

994
00:57:49,399 --> 00:57:52,202
which are more open ended
and you don't have any rewards, then

995
00:57:52,502 --> 00:57:55,305
active inference
could be potentially useful, but

996
00:57:56,272 --> 00:58:00,210
I'm a little bit hesitant
to just say whether it will be better

997
00:58:00,710 --> 00:58:04,147
because if you start augmenting
Bayesian zero

998
00:58:04,147 --> 00:58:07,250
with all sorts of interesting
components, then

999
00:58:08,418 --> 00:58:10,587
to a certain extent it will be active

1000
00:58:10,587 --> 00:58:12,689
inference scaled up

1001
00:58:13,656 --> 00:58:16,192
which
which is potentially a contentious point.

1002
00:58:17,927 --> 00:58:20,330
But I think it depends on

1003
00:58:20,330 --> 00:58:23,700
exactly what you are incorporating
and that's why for this particular

1004
00:58:23,900 --> 00:58:27,537
presentation and our work,
we were quite careful in defining what

1005
00:58:27,604 --> 00:58:29,272
reinforcement learning meant,

1006
00:58:29,272 --> 00:58:32,842
which is that you have to have
this reward function in play

1007
00:58:32,842 --> 00:58:35,044
and you want to maximize
this reward function.

1008
00:58:35,044 --> 00:58:39,482
And then if you include into that,
our framework has to have that.

1009
00:58:40,416 --> 00:58:43,286
But like the objective has to be that.

1010
00:58:43,286 --> 00:58:44,020
But if you

1011
00:58:44,020 --> 00:58:45,255
if you sort of bypass

1012
00:58:45,255 --> 00:58:48,091
them, say, OK, I'm just going to add
in all sorts of interesting components

1013
00:58:48,525 --> 00:58:53,830
to make the algorithm or the way
the agent interacts with the environment.

1014
00:58:54,898 --> 00:58:58,167
I I guess
as similar to an active inference

1015
00:58:58,434 --> 00:59:00,970
framework or perhaps even better,

1016
00:59:01,304 --> 00:59:04,941
then you get that fine distinction
between

1017
00:59:04,941 --> 00:59:08,945
what RL is better
or where active inference is better.

1018
00:59:08,945 --> 00:59:11,381
Sort of it's not really bad for me

1019
00:59:13,616 --> 00:59:17,120
because I think both communities
from my perspective are working

1020
00:59:17,253 --> 00:59:19,923
similar things,
which is sequential decision making.

1021
00:59:21,090 --> 00:59:23,660
And with our work, it's mostly

1022
00:59:23,660 --> 00:59:27,330
with sequential decision
making in the face of uncertainty,

1023
00:59:27,597 --> 00:59:31,734
whereas some of our work might our work
might not be focused on that.

1024
00:59:31,734 --> 00:59:34,971
So I think it's
when you start drawing the boundaries,

1025
00:59:34,971 --> 00:59:38,541
it becomes a little bit hazy
where like things are separate or not.

1026
00:59:39,208 --> 00:59:41,844
And I think I went off
on a little tangent

1027
00:59:43,012 --> 00:59:46,115
but so do you want to add anything

1028
00:59:46,115 --> 00:59:50,219
to that in terms of the environments
and the paradigms that might be useful?

1029
00:59:50,853 --> 00:59:51,721
Yeah, I think

1030
00:59:52,822 --> 00:59:56,025
if one of your aims is that you're given
an environment

1031
00:59:56,025 --> 00:59:59,896
that you just have no prior knowledge
about how you ought to behave,

1032
01:00:00,630 --> 01:00:03,600
you could argue that you could deploy
an oral agent that uses

1033
01:00:03,600 --> 01:00:07,203
some sort of curiosity or epistemic
uncertainty reduction mechanism.

1034
01:00:07,670 --> 01:00:12,375
But I mean, I know there is like
a tiny bit of work about learning priors

1035
01:00:12,375 --> 01:00:16,546
over reward functions and learning those,
but I'm not hugely aware.

1036
01:00:16,546 --> 01:00:21,117
But I think what's important
to understand is in the limit of like

1037
01:00:21,150 --> 01:00:22,952
exploring the entire environment,

1038
01:00:22,952 --> 01:00:25,221
your epistemic uncertainty
is going to go to zero, right?

1039
01:00:25,254 --> 01:00:27,690
Like you, you will observe it everything.

1040
01:00:27,690 --> 01:00:29,292
And then it's unclear
what your relationship

1041
01:00:29,292 --> 01:00:32,195
is going to do at that point, especially.
So you have a deep neural network.

1042
01:00:32,195 --> 01:00:35,264
That parameter dramatizes
how you take actions.

1043
01:00:35,765 --> 01:00:39,268
Whereas in the case, and I think what
this paper was really interesting

1044
01:00:39,268 --> 01:00:43,906
for me to see when we run these
experiments was actually has put a prior

1045
01:00:43,906 --> 01:00:47,644
over your preferences and eventually
you learn a mode of behavior.

1046
01:00:47,644 --> 01:00:51,881
It may not be optimal,
but your agent eventually learns to adopt

1047
01:00:51,881 --> 01:00:55,718
the behavior that is self-fulfilling
because it reduces epistemic uncertainty.

1048
01:00:56,019 --> 01:00:58,054
And then all that's left is says, well,

1049
01:00:58,221 --> 01:01:00,423
I think these are kind
of useful behaviors,

1050
01:01:00,423 --> 01:01:03,059
or at least these are behaviors
for me to do in the world.

1051
01:01:03,059 --> 01:01:06,295
And eventually you get
quite repetitive behavior and it might be

1052
01:01:06,295 --> 01:01:09,799
a more kind of accurate simulation
of in the absence of any information

1053
01:01:10,033 --> 01:01:13,069
like how something intelligent
might actually behave in a world.

1054
01:01:13,069 --> 01:01:15,638
Whereas in the oral paradigm,
it's less clear.

1055
01:01:15,638 --> 01:01:17,473
You know, once
you reduce all that epistemic uncertainty

1056
01:01:17,473 --> 01:01:19,475
about where the value is
and you haven't found any

1057
01:01:19,709 --> 01:01:22,345
like what is your agent
really doing at that point?

1058
01:01:22,345 --> 01:01:26,683
But I think my problem at the moment
with like the actual formulation

1059
01:01:26,683 --> 01:01:30,420
is like this works on
like a discrete state formulation, right?

1060
01:01:30,453 --> 01:01:34,190
We saw really nice results
in that setting.

1061
01:01:34,691 --> 01:01:36,559
But I think if you scale it up,

1062
01:01:36,559 --> 01:01:39,295
act of inference,
agents are going to have the same issues

1063
01:01:39,295 --> 01:01:42,198
if you're using like advertise inference
or something to actually

1064
01:01:43,032 --> 01:01:46,602
approximate your likelihood
or transition functions, which means that

1065
01:01:47,303 --> 01:01:49,872
you might not get these nice properties
that we're seeing

1066
01:01:49,872 --> 01:01:52,275
at the small scale.

1067
01:01:52,942 --> 01:01:53,976
So scaling it up, this

1068
01:01:55,244 --> 01:01:58,047
is an interesting and open ended problem
at the moment.

1069
01:01:58,081 --> 01:01:59,182
Scaling up in the right way

1070
01:01:59,182 --> 01:02:02,118
that you can include
like these confusing models that we have

1071
01:02:02,885 --> 01:02:05,254
in the discrete state formulation.

1072
01:02:05,254 --> 01:02:08,257
And that's how we do
the last set of simulations

1073
01:02:08,257 --> 01:02:12,161
where we introduce the conjugate, tries
to do the learning over there.

1074
01:02:12,495 --> 01:02:15,164
The prior preference is,
but also the likelihood

1075
01:02:16,365 --> 01:02:20,236
so becomes a little bit hazy
how to learn like I have a hyper prior.

1076
01:02:20,870 --> 01:02:23,973
I've read in time your network
if you're scaling it up that way.

1077
01:02:24,907 --> 01:02:26,943
So that must be

1078
01:02:26,943 --> 01:02:29,011
a lot of work
needs to be done in that area.

1079
01:02:29,011 --> 01:02:32,482
And in order to actually show
those appropriate

1080
01:02:33,649 --> 01:02:36,285
that you can have
these interesting components

1081
01:02:36,285 --> 01:02:41,891
that you need to start thinking about
how you would include these hyper priors

1082
01:02:42,759 --> 01:02:45,795
because it is not reasonable to say
that you're going to have a hyper prime

1083
01:02:45,828 --> 01:02:50,733
over the the parameters space
such like the beat up hype

1084
01:02:50,967 --> 01:02:54,737
so that hyper pi over the gamma that that
would be sufficient in those settings,

1085
01:02:56,339 --> 01:02:57,106
not hyper

1086
01:02:57,106 --> 01:02:59,609
prior, but the way the agent,
the selecting is actions.

1087
01:03:00,343 --> 01:03:02,678
It has to be over that model parameters.

1088
01:03:02,678 --> 01:03:07,283
And if scaling up means
that you're losing that nice way

1089
01:03:07,283 --> 01:03:11,154
to disentangle that particular model
parameters, then becomes very uncertain

1090
01:03:13,256 --> 01:03:13,990
I don't know.

1091
01:03:13,990 --> 01:03:16,926
It's an open ended problem for me.

1092
01:03:16,926 --> 01:03:17,193
Yeah.

1093
01:03:17,193 --> 01:03:20,563
I think high dimensional problems
still represent

1094
01:03:20,563 --> 01:03:24,133
something that is relatively difficult
especially just due to the kind of

1095
01:03:25,768 --> 01:03:29,505
you know, the further we stray from base,
like the less principled it becomes.

1096
01:03:29,672 --> 01:03:33,109
And, you know,
it's like it's, it's quite fine line.

1097
01:03:34,944 --> 01:03:36,312
Very interesting

1098
01:03:36,312 --> 01:03:40,950
about how whether within the oral
or active inference paradigm, there's

1099
01:03:40,950 --> 01:03:45,021
sort of the sparse skeleton,
the bare bones at the core.

1100
01:03:45,488 --> 01:03:49,258
And then sometimes these other layers or

1101
01:03:49,292 --> 01:03:52,261
tweaks are needed and

1102
01:03:53,529 --> 01:03:55,364
pretty interesting to learn about.

1103
01:03:55,364 --> 01:03:58,835
And also what you had said earlier, nor
about how the challenge

1104
01:03:58,835 --> 01:04:01,270
is planning for sequential action

1105
01:04:01,838 --> 01:04:05,308
when you're in feedback, whether
just by moving around in environments

1106
01:04:05,308 --> 01:04:08,578
or your local environment changes
or you're playing a game with a board

1107
01:04:08,578 --> 01:04:11,981
game is going to change or you're
trading on a market sequential action

1108
01:04:12,114 --> 01:04:14,550
you can't just plan steps one through 100

1109
01:04:15,318 --> 01:04:18,020
without at least thinking about some
what ifs

1110
01:04:18,721 --> 01:04:21,991
and then only having access to limited

1111
01:04:22,191 --> 01:04:26,295
observational data and planning amidst
fundamental uncertainty.

1112
01:04:26,629 --> 01:04:30,132
So I think a lot of those points
of contact with the motivations

1113
01:04:30,132 --> 01:04:34,003
of reinforcement
learning and machine learning will maybe

1114
01:04:34,003 --> 01:04:36,372
bring some more light
into active inference and

1115
01:04:38,241 --> 01:04:40,309
push some of those frontiers
you just mentioned.

1116
01:04:41,010 --> 01:04:43,880
So I have a question and anyone else in
the chat can ask a question to

1117
01:04:45,147 --> 01:04:46,449
let's say somebody is

1118
01:04:46,449 --> 01:04:51,854
wanting to learn about this
and they're actually in a lucky beginners

1119
01:04:51,854 --> 01:04:56,125
mind perspective
because they might not have been enticed

1120
01:04:56,125 --> 01:04:58,961
by learning reinforcement learning,
but they've gotten curious

1121
01:04:58,961 --> 01:05:02,765
in active inference
and excited by your presentation.

1122
01:05:03,299 --> 01:05:06,435
And so what
what kinds of computer languages

1123
01:05:06,435 --> 01:05:10,840
or skills might they want to learn
or what kinds of approaches or mindsets

1124
01:05:10,840 --> 01:05:15,811
would be helpful if somebody, let's say,
weren't coming from a classical machine

1125
01:05:15,811 --> 01:05:17,780
learning perspective
and learning active inference,

1126
01:05:17,780 --> 01:05:20,816
but rather
kind of upskilling into active inference.

1127
01:05:21,617 --> 01:05:25,087
What would you recommend
either of you to that?

1128
01:05:25,454 --> 01:05:27,123
Do you want to go first with that?

1129
01:05:27,123 --> 01:05:30,459
Yeah, I mean,
I think it's an interesting way to view

1130
01:05:30,459 --> 01:05:31,961
a kind of this kind of potential person

1131
01:05:31,961 --> 01:05:34,931
because I kind of felt
like I was somewhat like this person

1132
01:05:35,364 --> 01:05:37,900
like way back
when, like when I was starting

1133
01:05:37,900 --> 01:05:40,336
to have conversations
about active inference some

1134
01:05:41,370 --> 01:05:43,406
and I like.

1135
01:05:43,406 --> 01:05:46,776
The. This paper started off
basically as a tutorial I was writing

1136
01:05:46,776 --> 01:05:50,513
and having spent two or three months
kind of in the evenings reading about

1137
01:05:50,880 --> 01:05:55,084
trying to like sift through the active
inference literature and I think,

1138
01:05:56,252 --> 01:05:58,354
you know, I think it's fair to say that
at times it's

1139
01:05:58,354 --> 01:06:01,424
unapologetically dense
and quite difficult to read.

1140
01:06:01,424 --> 01:06:02,525
So, you know,

1141
01:06:02,525 --> 01:06:06,595
without trying to self endorse
but I do think reading this manuscript

1142
01:06:06,595 --> 01:06:09,966
in particular, like the whole aim
philosophically when like when we started

1143
01:06:09,966 --> 01:06:15,204
writing this was to really understand
like what is what is happening here?

1144
01:06:15,204 --> 01:06:17,440
Like what is this kind of expected
free energy quantity?

1145
01:06:17,440 --> 01:06:18,841
Like why do we care about it?

1146
01:06:18,841 --> 01:06:21,978
So I know
from a kind of theoretical perspective,

1147
01:06:21,978 --> 01:06:25,915
I think this is a very lucid
presentation of the concept.

1148
01:06:25,915 --> 01:06:28,851
So at least you can get some sort
of intuition as to what's happening.

1149
01:06:29,518 --> 01:06:32,388
As for the kind of coding side,
I can't really speak to that,

1150
01:06:32,388 --> 01:06:36,258
but I'm sure I'm sure more kind of has
has worked it works with it quite a bit.

1151
01:06:36,959 --> 01:06:40,262
And I was going to say it
sort of depends on what

1152
01:06:40,262 --> 01:06:42,164
the primary objective of the person is.

1153
01:06:42,164 --> 01:06:46,969
Is it for a sort of
get an understanding of the

1154
01:06:47,403 --> 01:06:51,140
the high level conceptual ideas
or treat it as an algorithm?

1155
01:06:51,140 --> 01:06:54,310
Because if you're coming from free energy
principle, to active inference

1156
01:06:54,310 --> 01:06:55,611
is a different story.

1157
01:06:55,611 --> 01:06:59,281
Or if you if you're taking
two inferences, a siloed algorithm,

1158
01:07:00,049 --> 01:07:04,053
specific kind of sequential decision
making scheme, right?

1159
01:07:05,254 --> 01:07:07,923
So depending on that is sort of differs.

1160
01:07:08,724 --> 01:07:13,329
But I still will say I think this paper
definitely is really nice in the sense

1161
01:07:13,329 --> 01:07:18,100
like it does try and define
all the different concepts and goes

1162
01:07:18,100 --> 01:07:23,072
through the different combinations,
maybe not in as much detail

1163
01:07:23,072 --> 01:07:26,942
so with the assumptions in play,
it gives you the

1164
01:07:27,710 --> 01:07:30,413
the layout
of how you might be able to derive it,

1165
01:07:31,447 --> 01:07:35,684
but certain things like what,
what the approximate

1166
01:07:36,018 --> 01:07:38,621
density, even reading entails,
those are very

1167
01:07:39,822 --> 01:07:42,458
difficult questions
where you would have to like dove

1168
01:07:42,458 --> 01:07:45,461
into variational inference
literature to understand.

1169
01:07:45,461 --> 01:07:49,365
So I guess from that perspective,
someone who's coming into the field

1170
01:07:49,865 --> 01:07:52,568
should spend some time
thinking about racial inference

1171
01:07:52,568 --> 01:07:56,372
and how that ties
back to the active inference formulation,

1172
01:07:56,939 --> 01:08:02,011
because the perception part of active
inference is in most instances exactly

1173
01:08:02,011 --> 01:08:06,615
the same as variational inference,
which is on optimizing

1174
01:08:07,550 --> 01:08:12,721
the model evidence
or the maximizing the evidence around

1175
01:08:14,090 --> 01:08:14,423
so the

1176
01:08:14,423 --> 01:08:16,859
second thing that I was going to add is

1177
01:08:18,561 --> 01:08:22,865
that the paper with Lancelot de
Costa is really good for someone

1178
01:08:22,865 --> 01:08:27,136
who wants to drill down
into deriving everything themselves.

1179
01:08:27,136 --> 01:08:30,506
And sometimes it's super technical.

1180
01:08:30,506 --> 01:08:33,676
So this are the paper
that I walk through today.

1181
01:08:34,110 --> 01:08:35,978
The one with Phil and Tilman Co

1182
01:08:35,978 --> 01:08:40,082
is, is a nice introduction for someone
who is not familiar with the mathematics.

1183
01:08:40,082 --> 01:08:42,551
I'm just wants to get a layman's summary

1184
01:08:43,152 --> 01:08:46,622
whereas this the paper,
the paper with Lance's

1185
01:08:46,622 --> 01:08:50,292
first
author gives like the detail, derivations

1186
01:08:50,292 --> 01:08:54,230
and a lot of assumptions in place
so that's from like understanding

1187
01:08:54,230 --> 01:08:55,831
the theory part.

1188
01:08:55,898 --> 01:08:59,969
From the coding perspective, it entirely
depends what the end objective is.

1189
01:08:59,969 --> 01:09:04,440
So if someone wants to work
with discrete state formulations,

1190
01:09:04,440 --> 01:09:07,643
then the MATLAB code call has written

1191
01:09:07,643 --> 01:09:10,713
and it's yes, work with lots of

1192
01:09:11,747 --> 01:09:14,283
nice simulations
and examples that you can use.

1193
01:09:14,283 --> 01:09:18,254
And also our code is online
and you can access it.

1194
01:09:18,554 --> 01:09:21,323
So there's a link in the paper

1195
01:09:21,323 --> 01:09:23,792
in the software

1196
01:09:23,792 --> 01:09:26,295
section
that gives where exactly the code is

1197
01:09:26,295 --> 01:09:29,298
and you can look through that
and see how the simulations were done.

1198
01:09:29,865 --> 01:09:33,002
If someone is interested in

1199
01:09:33,002 --> 01:09:35,671
more, I guess,

1200
01:09:35,738 --> 01:09:38,641
high dimensional simulations
of active inference that

1201
01:09:39,808 --> 01:09:42,745
there's some recent work with

1202
01:09:42,745 --> 01:09:44,513
the software Zephyrus.

1203
01:09:44,513 --> 01:09:47,383
So it's fantastic as first author

1204
01:09:48,150 --> 01:09:52,021
where we've got a nice again,
we get ready before that work as well,

1205
01:09:52,021 --> 01:09:55,124
which gives a breakdown

1206
01:09:55,124 --> 01:09:59,028
of how you would implement
the simple act of inference agent using

1207
01:09:59,361 --> 01:10:02,464
very short 25 fighters
and a simple transition network.

1208
01:10:02,765 --> 01:10:06,335
So there's lots of different areas,
but for someone starting out,

1209
01:10:06,335 --> 01:10:07,836
they sort of
have to understand what they want

1210
01:10:07,836 --> 01:10:09,638
to focus on the theoretical side

1211
01:10:09,638 --> 01:10:11,874
or whether they want to focus
on the implementation side.

1212
01:10:11,941 --> 01:10:13,142
The radical side

1213
01:10:13,142 --> 01:10:16,679
would be drilling down into the variation
inference in the maps behind it,

1214
01:10:16,679 --> 01:10:19,148
and if they want to focus
on the coding aspect

1215
01:10:19,148 --> 01:10:22,484
and they want them to figure out
whether it's continuous or discrete state

1216
01:10:22,484 --> 01:10:27,389
limitations that interest interested in
and then sort of break into

1217
01:10:27,389 --> 01:10:32,094
is going to use them,
most of it would either be writing the

1218
01:10:33,062 --> 01:10:34,129
coordinates of motions

1219
01:10:34,129 --> 01:10:37,266
themselves or they would have to use
like some sort of like

1220
01:10:39,935 --> 01:10:40,502
neuro network

1221
01:10:40,502 --> 01:10:43,205
to approximate
that continuous distribution of interest

1222
01:10:43,672 --> 01:10:46,342
or the discrete state
which calls written out.

1223
01:10:47,042 --> 01:10:50,412
And if if they have questions
about the discrete state,

1224
01:10:51,714 --> 01:10:54,650
I'm happy to take emails
as well as anyone

1225
01:10:54,650 --> 01:10:59,121
because all the continuous states
that state space as well.

1226
01:11:00,856 --> 01:11:02,291
Thanks for the distinction.

1227
01:11:02,291 --> 01:11:07,263
And it's such a large difference
between the MATLAB code,

1228
01:11:07,263 --> 01:11:11,567
which we got to walk through with Ryan
Smith and Christopher White,

1229
01:11:11,900 --> 01:11:14,870
which is doing matrix
multiplication and such.

1230
01:11:15,204 --> 01:11:19,675
And then here comes the neural networks
and it's offering

1231
01:11:19,875 --> 01:11:23,112
sounds like new opportunities
with high dimensionality

1232
01:11:23,112 --> 01:11:27,583
and continuous variables,
but also a lot of new challenges.

1233
01:11:27,583 --> 01:11:32,388
So what is the essence that's shared

1234
01:11:33,422 --> 01:11:36,392
by the matrix form and by this more

1235
01:11:37,426 --> 01:11:37,893
machine?

1236
01:11:37,893 --> 01:11:40,496
Learning style?

1237
01:11:40,496 --> 01:11:42,564
Because for some people

1238
01:11:42,564 --> 01:11:44,833
it might be splitting a hair,
quite literally.

1239
01:11:44,833 --> 01:11:46,402
The difference between

1240
01:11:46,402 --> 01:11:49,571
two computer languages when they're
thinking about active inference

1241
01:11:50,239 --> 01:11:53,342
from a ecological psychology

1242
01:11:53,375 --> 01:11:57,579
or in an active philosophical
or an embodied performance

1243
01:11:57,579 --> 01:12:02,017
perspective, of all backgrounds
that converge on active inference.

1244
01:12:02,017 --> 01:12:05,821
And so to somebody outside the
the strand of hair,

1245
01:12:06,588 --> 01:12:11,060
what is it that we can really distill
that is core active inference?

1246
01:12:11,060 --> 01:12:14,163
And I wrote down a few things
so that you had said,

1247
01:12:14,163 --> 01:12:18,067
what are those core pieces that allow us
to dove into the matrix mode?

1248
01:12:18,067 --> 01:12:21,070
With MATLAB or into the neural network
mode with like Python.

1249
01:12:22,938 --> 01:12:26,642
And so I think it comes down to my
summary slide the way I think about it.

1250
01:12:26,642 --> 01:12:29,278
So the active inference,
the core ingredients are formulating

1251
01:12:29,311 --> 01:12:30,846
the gentle model

1252
01:12:30,979 --> 01:12:34,149
and here simulating the gentle model
is a privatization of the gentle model.

1253
01:12:34,149 --> 01:12:37,786
So you can use the discrete state
categorical distributions

1254
01:12:37,786 --> 01:12:40,489
or you can use a more continuous state
formulation.

1255
01:12:40,489 --> 01:12:41,323
And again,

1256
01:12:41,690 --> 01:12:46,328
the the neural networks simulation
is a specific instantiation of that.

1257
01:12:46,328 --> 01:12:50,833
So that would be one way of sort
of differentiating that.

1258
01:12:51,166 --> 01:12:56,772
The second one is the optimization
of the objective functions in play.

1259
01:12:57,406 --> 01:13:01,543
So in the the MATLAB code

1260
01:13:01,844 --> 01:13:06,081
we're doing gradient descent using mean
field message crossing algorithm.

1261
01:13:06,081 --> 01:13:09,451
So specific formulation that's been
introduced in a couple of cases.

1262
01:13:09,718 --> 01:13:15,090
And I specifically didn't
walk through that or you're doing back

1263
01:13:15,090 --> 01:13:20,362
propagation to actually calculate
the or lambda distributions.

1264
01:13:20,362 --> 01:13:24,199
And then we're just solving
those distributions that you have.

1265
01:13:24,533 --> 01:13:28,203
So it sort of depends
on which formulation your

1266
01:13:30,339 --> 01:13:33,175
and it depends on how you optimize
those objectives.

1267
01:13:33,175 --> 01:13:35,411
Either
you're taking the implicit forward model

1268
01:13:35,677 --> 01:13:38,013
or you're taking an explicit,
gentler model

1269
01:13:39,181 --> 01:13:44,186
and one thing I forgot to mention
is that Alex Chavez and Conor Hynes,

1270
01:13:44,186 --> 01:13:47,956
they've been working
on the discrete state formulation of

1271
01:13:48,757 --> 01:13:52,928
active inference in Python,
which might be of interest

1272
01:13:52,928 --> 01:13:55,764
for people who want to focus on
one specific language

1273
01:13:56,298 --> 01:14:00,235
that can do like the more high end
or high dimensions stuff

1274
01:14:00,602 --> 01:14:03,505
and the discrete state formulation
that Karl has.

1275
01:14:03,972 --> 01:14:07,976
I know they're also looking for people
to work on the code base,

1276
01:14:07,976 --> 01:14:09,211
if anyone's interested.

1277
01:14:09,211 --> 01:14:11,313
So it's called Infer Activity.

1278
01:14:11,313 --> 01:14:12,247
I think

1279
01:14:12,848 --> 01:14:15,851
that's on GitHub as well,
if anyone's interested

1280
01:14:17,853 --> 01:14:18,821
cool.

1281
01:14:19,655 --> 01:14:23,258
Any sort of last thoughts
or comments from either of you

1282
01:14:27,396 --> 01:14:30,899
and. No, I think I'm OK now.

1283
01:14:30,933 --> 01:14:32,034
What do you reckon.

1284
01:14:38,607 --> 01:14:41,877
I think we're kind of noticing a bit of a

1285
01:14:44,279 --> 01:14:46,114
an increase in interest
in active inference.

1286
01:14:46,114 --> 01:14:49,451
Like just to give you an example,
like maybe areas where

1287
01:14:49,952 --> 01:14:51,954
even active inference
would have been considered before,

1288
01:14:52,488 --> 01:14:55,924
like robotics you're now beginning
to see more and more of it.

1289
01:14:55,924 --> 01:14:59,461
So I think if you do want to get involved
in it, now is a particularly good time.

1290
01:15:01,163 --> 01:15:03,799
Great call up and I'll

1291
01:15:04,700 --> 01:15:08,337
just re recommend the excellent paper
that we're discussing.

1292
01:15:08,337 --> 01:15:10,739
It's in the description of this video.

1293
01:15:10,739 --> 01:15:13,575
I really appreciate both of you
for joining.

1294
01:15:13,575 --> 01:15:15,077
You're always welcome to

1295
01:15:15,077 --> 01:15:18,747
come on to be speaking about a paper
you've authored or not.

1296
01:15:19,147 --> 01:15:22,050
But again,
thanks a lot to Nora and Philip

1297
01:15:22,284 --> 01:15:25,621
and I hope to see you again
on a future active inference stream.

1298
01:15:27,122 --> 01:15:27,723
Thank you.

1299
01:15:27,723 --> 01:15:29,358
Thank you for inviting us.

1300
01:15:29,358 --> 01:15:32,027
So I see I've done your piece by.

1301
01:15:35,097 --> 01:15:36,164
Awesome

1302
01:15:38,033 --> 01:15:38,700
stop stream.

1303
01:15:38,700 --> 01:15:40,002
Great conversation.

1304
01:15:40,002 --> 01:15:42,504
Thanks a lot to Philip and nor really,
but to Philip.

1305
01:15:42,504 --> 01:15:44,806
And nor really,
but to Philip. And nor really.

1306
01:15:44,806 --> 01:15:46,308
But to Philip. And nor really.

1307
01:15:46,308 --> 01:15:47,776
But to Philip. And nor really.

1308
01:15:47,776 --> 01:15:49,311
But to philip. And nor really.

1309
01:15:49,311 --> 01:15:52,147
But to philip. And nor really.
But to philip. And nor really.

1
00:00:07,839 --> 00:00:08,400
hello

2
00:00:08,400 --> 00:00:11,120
and welcome everyone to the active

3
00:00:11,120 --> 00:00:12,160
inference lab

4
00:00:12,160 --> 00:00:15,759
this is the model stream number 2.1

5
00:00:15,759 --> 00:00:18,960
on april 16 2021

6
00:00:18,960 --> 00:00:20,720
and today is going to be an awesome

7
00:00:20,720 --> 00:00:22,400
model stream we're just going to

8
00:00:22,400 --> 00:00:24,400
briefly go around and introduce

9
00:00:24,400 --> 00:00:25,920
ourselves and then

10
00:00:25,920 --> 00:00:27,920
i'll just mention how the session will

11
00:00:27,920 --> 00:00:30,000
be run today and then we'll pass it to

12
00:00:30,000 --> 00:00:33,200
noor for a presentation so i'm daniel

13
00:00:33,200 --> 00:00:35,120
and i'm a postdoctoral researcher in

14
00:00:35,120 --> 00:00:36,719
california i'll pass to

15
00:00:36,719 --> 00:00:39,040
philip

16
00:00:40,000 --> 00:00:42,800
hi yeah i'm currently a phd student at

17
00:00:42,800 --> 00:00:43,280
oxford

18
00:00:43,280 --> 00:00:46,719
in my second year and yeah i guess this

19
00:00:46,719 --> 00:00:48,320
is work i did kind of before i started

20
00:00:48,320 --> 00:00:49,440
my phd

21
00:00:49,440 --> 00:00:52,640
um alongside nor and i've currently more

22
00:00:52,640 --> 00:00:53,600
focusing on

23
00:00:53,600 --> 00:00:55,600
data efficiency within specifically

24
00:00:55,600 --> 00:00:57,280
reinforcement learning but

25
00:00:57,280 --> 00:00:58,960
i guess on that topic like a knowledge

26
00:00:58,960 --> 00:01:00,879
of active inference

27
00:01:00,879 --> 00:01:03,520
is obviously useful for approaching such

28
00:01:03,520 --> 00:01:06,239
research problems

29
00:01:06,240 --> 00:01:09,360
who and or hi

30
00:01:09,360 --> 00:01:12,000
hi i'm noah i'm a third year phd student

31
00:01:12,000 --> 00:01:13,840
in the theoretical neurobiology group at

32
00:01:13,840 --> 00:01:15,200
the welcome center for human

33
00:01:15,200 --> 00:01:16,159
neuroimaging

34
00:01:16,159 --> 00:01:18,799
um at ucl um so that's university

35
00:01:18,799 --> 00:01:19,920
college london

36
00:01:19,920 --> 00:01:22,720
um so my phd supervised by cars focused

37
00:01:22,720 --> 00:01:23,920
on these ideas

38
00:01:23,920 --> 00:01:26,000
pertaining to adaptation one of which

39
00:01:26,000 --> 00:01:27,759
i'll be focusing on today which is

40
00:01:27,759 --> 00:01:29,759
behavioral adaptation in non-stationary

41
00:01:29,759 --> 00:01:32,000
environments using active inference

42
00:01:32,000 --> 00:01:35,119
so thank you awesome thanks both for

43
00:01:35,119 --> 00:01:38,159
joining and for this presentation we're

44
00:01:38,159 --> 00:01:38,720
going to be

45
00:01:38,720 --> 00:01:41,280
hearing a who knows how long

46
00:01:41,280 --> 00:01:42,720
presentation from nor

47
00:01:42,720 --> 00:01:44,399
and then i'm gonna be compiling

48
00:01:44,399 --> 00:01:45,759
questions from the chat

49
00:01:45,759 --> 00:01:48,240
so please just type questions as they

50
00:01:48,240 --> 00:01:48,880
come to you

51
00:01:48,880 --> 00:01:51,119
and then we'll address them at the end

52
00:01:51,119 --> 00:01:53,200
so thanks again and nor please

53
00:01:53,200 --> 00:01:56,479
take it away perfect thank you

54
00:01:56,479 --> 00:01:58,880
um so today i'll be presenting some work

55
00:01:58,880 --> 00:01:59,520
um

56
00:01:59,520 --> 00:02:01,600
that i did in collaboration with philip

57
00:02:01,600 --> 00:02:03,119
who you've just heard from

58
00:02:03,119 --> 00:02:06,399
thomas parr and carl fristan um

59
00:02:06,399 --> 00:02:08,800
so it's titled active inference uh

60
00:02:08,800 --> 00:02:12,720
demystified and compared

61
00:02:13,599 --> 00:02:15,440
okay okay perfect okay so the

62
00:02:15,440 --> 00:02:17,440
presentation structured as follows

63
00:02:17,440 --> 00:02:20,480
can you hear the screens

64
00:02:20,480 --> 00:02:22,160
i think it's sharing or was it are you

65
00:02:22,160 --> 00:02:23,840
not able to see it i'm not seeing it

66
00:02:23,840 --> 00:02:26,000
could you just re-share it

67
00:02:26,000 --> 00:02:28,640
yep sure

68
00:02:29,920 --> 00:02:34,480
um technology i tell you

69
00:02:34,480 --> 00:02:36,800
there we go and i'll crop it so go for

70
00:02:36,800 --> 00:02:38,720
it thanks

71
00:02:38,720 --> 00:02:40,879
perfect thank you so the presentation

72
00:02:40,879 --> 00:02:42,239
structured as well as

73
00:02:42,239 --> 00:02:44,080
first i'll briefly motivate the problem

74
00:02:44,080 --> 00:02:45,280
setting and provide

75
00:02:45,280 --> 00:02:47,360
details of a particular active inference

76
00:02:47,360 --> 00:02:49,920
instantiation under consideration today

77
00:02:49,920 --> 00:02:51,519
which is the discrete state space

78
00:02:51,519 --> 00:02:52,319
setting

79
00:02:52,319 --> 00:02:54,160
and the second half of the presentation

80
00:02:54,160 --> 00:02:55,599
is going to be focused on some

81
00:02:55,599 --> 00:02:57,440
particular examples comparing

82
00:02:57,440 --> 00:02:59,360
the active influence formulation with

83
00:02:59,360 --> 00:03:00,720
reinforcement learning

84
00:03:00,720 --> 00:03:03,599
specifically q learning and bayesian

85
00:03:03,599 --> 00:03:04,959
model based

86
00:03:04,959 --> 00:03:07,519
algorithm and then what i'm going to do

87
00:03:07,519 --> 00:03:08,959
is provide some face validity of

88
00:03:08,959 --> 00:03:10,640
particular aspects of why you would want

89
00:03:10,640 --> 00:03:14,080
to even use active inference

90
00:03:14,080 --> 00:03:18,080
okay um so what is active inference

91
00:03:18,080 --> 00:03:20,480
um it's a first principles account of

92
00:03:20,480 --> 00:03:21,599
how biological

93
00:03:21,599 --> 00:03:23,920
or artificial agents may operate in

94
00:03:23,920 --> 00:03:26,239
dynamic non-stationary settings

95
00:03:26,239 --> 00:03:28,560
it stipulates that these agents in order

96
00:03:28,560 --> 00:03:30,000
to maintain homeostasis

97
00:03:30,000 --> 00:03:31,680
reside in attracting states that

98
00:03:31,680 --> 00:03:34,560
minimize their entropy or their surprise

99
00:03:34,560 --> 00:03:36,239
so if you take this particular example

100
00:03:36,239 --> 00:03:38,000
that we're seeing of this past

101
00:03:38,000 --> 00:03:41,280
little hungry agent opening the fridge

102
00:03:41,280 --> 00:03:42,879
the way it would work is like you would

103
00:03:42,879 --> 00:03:44,959
need to why would you open the fridge

104
00:03:44,959 --> 00:03:46,480
right so you want to make a particular

105
00:03:46,480 --> 00:03:47,280
choice between

106
00:03:47,280 --> 00:03:50,080
eating at home or outside and in order

107
00:03:50,080 --> 00:03:51,120
to do that you

108
00:03:51,120 --> 00:03:53,200
have to decide what is the optimal

109
00:03:53,200 --> 00:03:55,040
action that would allow you to resolve

110
00:03:55,040 --> 00:03:56,959
your own uncertainty about the current

111
00:03:56,959 --> 00:03:58,560
stage of affairs

112
00:03:58,560 --> 00:04:00,799
so um and that would help you then

113
00:04:00,799 --> 00:04:02,480
decide whether you want to cook at home

114
00:04:02,480 --> 00:04:04,400
or you want to walk to the restaurant

115
00:04:04,400 --> 00:04:06,720
and this particular instance this has

116
00:04:06,720 --> 00:04:08,560
led to the agent opening the fridge to

117
00:04:08,560 --> 00:04:11,760
check whether it even has food at home

118
00:04:11,760 --> 00:04:13,519
and what's nice about active inference

119
00:04:13,519 --> 00:04:15,519
is that it allows you to

120
00:04:15,519 --> 00:04:16,959
think about these problem settings in a

121
00:04:16,959 --> 00:04:19,358
more formal way by specifying that

122
00:04:19,358 --> 00:04:20,639
optimal behavior

123
00:04:20,639 --> 00:04:23,199
is determined by evaluating the evidence

124
00:04:23,199 --> 00:04:24,639
that is the sensory input

125
00:04:24,639 --> 00:04:26,639
under the agent's gender model of

126
00:04:26,639 --> 00:04:28,080
observations that it

127
00:04:28,080 --> 00:04:30,400
is being exposed to and in this

128
00:04:30,400 --> 00:04:31,520
particular

129
00:04:31,520 --> 00:04:33,360
presentation what we'll do is focus on

130
00:04:33,360 --> 00:04:35,280
just the process theory that underwrites

131
00:04:35,280 --> 00:04:37,199
active inference and not talk through

132
00:04:37,199 --> 00:04:38,240
the biological

133
00:04:38,240 --> 00:04:41,199
on the neural plausibility of the active

134
00:04:41,199 --> 00:04:44,240
inference message passing scheme

135
00:04:44,240 --> 00:04:47,280
um but to properly uh motivate why we

136
00:04:47,280 --> 00:04:48,479
would want to even use

137
00:04:48,479 --> 00:04:50,560
um active inference in comparison to

138
00:04:50,560 --> 00:04:52,000
generic reinforcement learning

139
00:04:52,000 --> 00:04:52,800
algorithms

140
00:04:52,800 --> 00:04:54,720
we need to first start off with this

141
00:04:54,720 --> 00:04:56,080
understanding that with an active

142
00:04:56,080 --> 00:04:57,759
inference there's this commitment to a

143
00:04:57,759 --> 00:04:59,600
pure belief based scheme

144
00:04:59,600 --> 00:05:01,600
which means that reward functions are

145
00:05:01,600 --> 00:05:03,520
not always necessarily um

146
00:05:03,520 --> 00:05:06,000
sorry unnecessary because any policy

147
00:05:06,000 --> 00:05:07,360
that you would have

148
00:05:07,360 --> 00:05:10,240
has epistemic value even in the absence

149
00:05:10,240 --> 00:05:11,680
of preferences

150
00:05:11,680 --> 00:05:14,000
additionally active inference agents can

151
00:05:14,000 --> 00:05:16,400
also learn their own reward functions

152
00:05:16,400 --> 00:05:19,120
and this helps the agent describe the

153
00:05:19,120 --> 00:05:20,400
type of behavioral

154
00:05:20,400 --> 00:05:23,199
that it expects to see it himself as

155
00:05:23,199 --> 00:05:25,120
opposed to something that it would get

156
00:05:25,120 --> 00:05:28,000
uh from the environment and these two

157
00:05:28,000 --> 00:05:29,600
particular points are really important

158
00:05:29,600 --> 00:05:30,800
in contrast to

159
00:05:30,800 --> 00:05:33,039
reinforcement learning because under

160
00:05:33,039 --> 00:05:35,680
standard rl settings the reward function

161
00:05:35,680 --> 00:05:38,160
uh would define how the agent interacts

162
00:05:38,160 --> 00:05:39,360
or behaves

163
00:05:39,360 --> 00:05:41,520
within a particular environment setting

164
00:05:41,520 --> 00:05:42,560
um but

165
00:05:42,560 --> 00:05:44,080
defining that reward function in the

166
00:05:44,080 --> 00:05:46,000
first place is quite difficult because

167
00:05:46,000 --> 00:05:48,160
it assumes that there is a specific

168
00:05:48,160 --> 00:05:49,520
signal that's being

169
00:05:49,520 --> 00:05:51,919
given from the environment that can be

170
00:05:51,919 --> 00:05:53,600
unanimously good or bad

171
00:05:53,600 --> 00:05:56,160
for the agent which wouldn't necessarily

172
00:05:56,160 --> 00:05:56,800
hold true

173
00:05:56,800 --> 00:06:00,400
in a real setting where these um

174
00:06:00,400 --> 00:06:02,319
environment signals can change depending

175
00:06:02,319 --> 00:06:03,919
on the setting for example

176
00:06:03,919 --> 00:06:05,680
eating ice cream is not always going to

177
00:06:05,680 --> 00:06:07,280
be rewarding if you're

178
00:06:07,280 --> 00:06:09,919
ill and it might make you worse um and

179
00:06:09,919 --> 00:06:11,199
that's why

180
00:06:11,199 --> 00:06:12,800
constructing these reward functions in

181
00:06:12,800 --> 00:06:15,199
the first place is extremely difficult

182
00:06:15,199 --> 00:06:16,880
and if you're not constructing them in

183
00:06:16,880 --> 00:06:19,360
appropriate way even with an rl setting

184
00:06:19,360 --> 00:06:21,680
it can result in sub-optimal behavior

185
00:06:21,680 --> 00:06:24,080
for your agents

186
00:06:24,080 --> 00:06:26,000
um an active inference is really good in

187
00:06:26,000 --> 00:06:27,520
the sense because we are actually

188
00:06:27,520 --> 00:06:29,680
um replacing or bypassing the

189
00:06:29,680 --> 00:06:31,360
traditional reward function that you

190
00:06:31,360 --> 00:06:31,919
would have

191
00:06:31,919 --> 00:06:34,400
in the rl setting with prior beliefs

192
00:06:34,400 --> 00:06:34,960
about

193
00:06:34,960 --> 00:06:36,880
preferred outcomes so the sort of

194
00:06:36,880 --> 00:06:39,039
desired states of affairs that you want

195
00:06:39,039 --> 00:06:40,560
to see yourself

196
00:06:40,560 --> 00:06:43,840
be in and this becomes important

197
00:06:43,840 --> 00:06:47,039
in settings where there's no reward or

198
00:06:47,039 --> 00:06:48,400
there's really imprecise

199
00:06:48,400 --> 00:06:50,479
understanding of what a reward or

200
00:06:50,479 --> 00:06:52,639
preference setting should look like

201
00:06:52,639 --> 00:06:55,360
and in this scenario within the standard

202
00:06:55,360 --> 00:06:56,319
active influence

203
00:06:56,319 --> 00:06:58,160
discrete state formulation what we can

204
00:06:58,160 --> 00:07:00,319
do is learn the empirical prior

205
00:07:00,319 --> 00:07:01,599
distribution over these

206
00:07:01,599 --> 00:07:05,280
preferred outcomes um and intrinsic uh

207
00:07:05,280 --> 00:07:07,199
sorry the internal reward function of

208
00:07:07,199 --> 00:07:07,599
that

209
00:07:07,599 --> 00:07:10,400
agent and this sort of brings me to like

210
00:07:10,400 --> 00:07:11,120
the first

211
00:07:11,120 --> 00:07:14,479
distinct um conceptualization between

212
00:07:14,479 --> 00:07:17,919
rl setting and active inference because

213
00:07:17,919 --> 00:07:20,639
within active inference rewards are

214
00:07:20,639 --> 00:07:22,560
nothing distinct they're just a standard

215
00:07:22,560 --> 00:07:24,560
observation that the agent is

216
00:07:24,560 --> 00:07:26,800
getting from the environment whereas in

217
00:07:26,800 --> 00:07:27,840
rl they're quite

218
00:07:27,840 --> 00:07:32,160
necessary to have appropriate action

219
00:07:32,160 --> 00:07:35,759
um that the agent is going to learn

220
00:07:35,759 --> 00:07:38,960
um the second point i we want to make

221
00:07:38,960 --> 00:07:41,280
sorry we wanted to make was that active

222
00:07:41,280 --> 00:07:43,039
influence provides a principal account

223
00:07:43,039 --> 00:07:44,080
of epistemic

224
00:07:44,080 --> 00:07:46,160
exploration and intrinsic motivation as

225
00:07:46,160 --> 00:07:48,160
minimizing uncertainty

226
00:07:48,160 --> 00:07:50,080
and again within the rl setting this is

227
00:07:50,080 --> 00:07:51,440
quite crucial because

228
00:07:51,440 --> 00:07:53,759
the whole uh premise of like lots of new

229
00:07:53,759 --> 00:07:54,479
algorithms that

230
00:07:54,479 --> 00:07:57,759
we see in our nrl is to try and find the

231
00:07:57,759 --> 00:07:58,000
right

232
00:07:58,000 --> 00:07:59,680
trade-off for the balance between

233
00:07:59,680 --> 00:08:02,000
exploration and exploration so

234
00:08:02,000 --> 00:08:03,919
what are the right set of actions that

235
00:08:03,919 --> 00:08:05,599
the agent should make at a given point

236
00:08:05,599 --> 00:08:06,400
in time

237
00:08:06,400 --> 00:08:08,240
should i carry on choosing all the

238
00:08:08,240 --> 00:08:10,160
different ice cream flavors that it's

239
00:08:10,160 --> 00:08:10,800
been

240
00:08:10,800 --> 00:08:13,520
never been exposed to um like mustard

241
00:08:13,520 --> 00:08:14,479
etc

242
00:08:14,479 --> 00:08:16,879
or should it always use this oh sorry

243
00:08:16,879 --> 00:08:17,840
always have the same

244
00:08:17,840 --> 00:08:20,000
ice cream flavor that's been exposed to

245
00:08:20,000 --> 00:08:21,120
in the past that it

246
00:08:21,120 --> 00:08:23,199
really likes like for example hazelnut

247
00:08:23,199 --> 00:08:25,199
or nutella for instance

248
00:08:25,199 --> 00:08:28,080
um so this um is it's an outstanding

249
00:08:28,080 --> 00:08:28,560
problem

250
00:08:28,560 --> 00:08:30,800
with an rl and under a bayesian

251
00:08:30,800 --> 00:08:32,958
framework active inference deals with it

252
00:08:32,958 --> 00:08:36,159
naturally um using um the expected free

253
00:08:36,159 --> 00:08:37,760
energy formulation that i'll come to in

254
00:08:37,760 --> 00:08:39,760
a moment

255
00:08:39,760 --> 00:08:42,799
um and the last bit that you can see

256
00:08:42,799 --> 00:08:44,560
within the active inference framework is

257
00:08:44,560 --> 00:08:46,080
that it naturally accounts for

258
00:08:46,080 --> 00:08:47,920
uncertainty as part of the belief

259
00:08:47,920 --> 00:08:48,560
updating

260
00:08:48,560 --> 00:08:51,839
process okay

261
00:08:51,839 --> 00:08:54,959
so now that i've sort of um laid out the

262
00:08:54,959 --> 00:08:57,200
three things that are super interesting

263
00:08:57,200 --> 00:08:58,640
about the active influence scheme in

264
00:08:58,640 --> 00:09:00,640
comparison to rl i'm going to

265
00:09:00,640 --> 00:09:04,480
provide some intuitions as to why um

266
00:09:04,480 --> 00:09:08,000
you can sorry some motivations as to

267
00:09:08,000 --> 00:09:09,760
why we can even formulate the active

268
00:09:09,760 --> 00:09:13,120
influence formulation the way we do

269
00:09:13,120 --> 00:09:17,040
okay so um

270
00:09:17,200 --> 00:09:18,959
sorry i just realized i like skipped

271
00:09:18,959 --> 00:09:20,480
ahead um

272
00:09:20,480 --> 00:09:24,399
no no sorry great presentation

273
00:09:24,399 --> 00:09:27,519
yep thank you so much and let me just

274
00:09:27,519 --> 00:09:32,000
scroll down to um

275
00:09:32,000 --> 00:09:35,040
okay so i previously stated that with an

276
00:09:35,040 --> 00:09:36,399
active inference

277
00:09:36,399 --> 00:09:38,480
it stipulates that agents are

278
00:09:38,480 --> 00:09:40,320
maintaining their homostasis

279
00:09:40,320 --> 00:09:42,080
by residing in attracting states that

280
00:09:42,080 --> 00:09:43,440
minimize surprise

281
00:09:43,440 --> 00:09:44,880
so you must have been thinking what is

282
00:09:44,880 --> 00:09:46,560
surprise well um

283
00:09:46,560 --> 00:09:48,800
how we define surprise as a negative log

284
00:09:48,800 --> 00:09:50,080
probability of outcomes

285
00:09:50,080 --> 00:09:52,080
and for this we introduce one random

286
00:09:52,080 --> 00:09:53,760
variable which is

287
00:09:53,760 --> 00:09:55,760
o that corresponds to a particular

288
00:09:55,760 --> 00:09:57,440
outcome that's received by an

289
00:09:57,440 --> 00:10:00,480
agent and this um exists within a finite

290
00:10:00,480 --> 00:10:01,040
set of

291
00:10:01,040 --> 00:10:03,600
all possible outcomes and that's o here

292
00:10:03,600 --> 00:10:04,560
so um

293
00:10:04,560 --> 00:10:06,720
first equation that we have just um

294
00:10:06,720 --> 00:10:08,160
formally states that out

295
00:10:08,160 --> 00:10:10,240
and here p denotes the probability

296
00:10:10,240 --> 00:10:11,680
distribution over

297
00:10:11,680 --> 00:10:14,239
outcomes

298
00:10:15,839 --> 00:10:19,920
um okay so in active inference the way

299
00:10:19,920 --> 00:10:21,200
the agent will actually

300
00:10:21,200 --> 00:10:23,200
minimize this surprise quantity that we

301
00:10:23,200 --> 00:10:24,959
just walked through is by maintaining a

302
00:10:24,959 --> 00:10:27,120
gender model of the world

303
00:10:27,120 --> 00:10:29,360
um and this is important because at any

304
00:10:29,360 --> 00:10:31,040
given point in time the agent wouldn't

305
00:10:31,040 --> 00:10:32,640
necessarily have access to

306
00:10:32,640 --> 00:10:34,000
the true measurements of the current

307
00:10:34,000 --> 00:10:36,000
state of the world so in this particular

308
00:10:36,000 --> 00:10:37,600
graphic that you see here

309
00:10:37,600 --> 00:10:39,600
you've got the environment and the agent

310
00:10:39,600 --> 00:10:41,200
um interacting with the environment in a

311
00:10:41,200 --> 00:10:42,320
particular way

312
00:10:42,320 --> 00:10:44,480
it's being exposed to the sensory signal

313
00:10:44,480 --> 00:10:46,480
but it doesn't know what the

314
00:10:46,480 --> 00:10:49,600
outcome of the o was regenerated by

315
00:10:49,600 --> 00:10:51,760
so it can only perceive itself and the

316
00:10:51,760 --> 00:10:52,959
world around it

317
00:10:52,959 --> 00:10:55,120
through only the o and it needs to make

318
00:10:55,120 --> 00:10:57,279
inferences about what type of states

319
00:10:57,279 --> 00:11:00,959
or um the true causes um

320
00:11:00,959 --> 00:11:03,279
were um responsible for the particular

321
00:11:03,279 --> 00:11:06,000
sensory input that it's being exposed to

322
00:11:06,000 --> 00:11:08,959
and this is why um in active inference

323
00:11:08,959 --> 00:11:10,240
when we formulate the problem we

324
00:11:10,240 --> 00:11:11,839
formulate it as a partially observable

325
00:11:11,839 --> 00:11:13,519
markov decision process

326
00:11:13,519 --> 00:11:16,560
because um in this way we are able to

327
00:11:16,560 --> 00:11:18,959
formulate a gender model that it defines

328
00:11:18,959 --> 00:11:19,279
this

329
00:11:19,279 --> 00:11:22,160
internal distribution over the internal

330
00:11:22,160 --> 00:11:23,920
states that the agent

331
00:11:23,920 --> 00:11:26,399
would use in order to infer the outcome

332
00:11:26,399 --> 00:11:27,920
so he doesn't have access to the true

333
00:11:27,920 --> 00:11:28,399
state

334
00:11:28,399 --> 00:11:30,320
but it can make hypothesis or beliefs

335
00:11:30,320 --> 00:11:32,000
about the states that could have given

336
00:11:32,000 --> 00:11:32,640
ris

337
00:11:32,640 --> 00:11:35,519
rise to a particular sense uh outcome

338
00:11:35,519 --> 00:11:38,320
space that is being exposed to

339
00:11:38,320 --> 00:11:40,800
um and using this the agent will make

340
00:11:40,800 --> 00:11:43,600
inferences about the true state

341
00:11:43,600 --> 00:11:45,839
using a process of reverse mapping

342
00:11:45,839 --> 00:11:48,079
specifically bayesian model inversion

343
00:11:48,079 --> 00:11:49,680
and to make this a little bit more

344
00:11:49,680 --> 00:11:52,000
concrete what you can do is think about

345
00:11:52,000 --> 00:11:55,680
the hidden states as locations or color

346
00:11:55,680 --> 00:11:58,240
for example and the observation space

347
00:11:58,240 --> 00:12:00,079
that the agent would be exposed to would

348
00:12:00,079 --> 00:12:00,560
be

349
00:12:00,560 --> 00:12:03,680
um for example the the velocity of the

350
00:12:03,680 --> 00:12:04,480
movement

351
00:12:04,480 --> 00:12:07,440
or a particular reward uh or like a

352
00:12:07,440 --> 00:12:10,959
happy face that they're being exposed to

353
00:12:10,959 --> 00:12:14,720
um okay so if we were to think about

354
00:12:14,720 --> 00:12:16,320
this a little bit more formally so

355
00:12:16,320 --> 00:12:19,920
what is the gentle model so as we um

356
00:12:19,920 --> 00:12:21,760
described before a gentleman is a

357
00:12:21,760 --> 00:12:23,040
partially observable

358
00:12:23,040 --> 00:12:25,200
mdp within this active and friends

359
00:12:25,200 --> 00:12:26,160
formulation

360
00:12:26,160 --> 00:12:29,040
which rests on a simplified setting that

361
00:12:29,040 --> 00:12:30,639
we're considering here where we only

362
00:12:30,639 --> 00:12:32,240
have two random variables

363
00:12:32,240 --> 00:12:35,120
the first one is o that we've discussed

364
00:12:35,120 --> 00:12:36,320
and the second one

365
00:12:36,320 --> 00:12:39,680
is s where s denotes um a random

366
00:12:39,680 --> 00:12:41,839
variable representing hidden or latent

367
00:12:41,839 --> 00:12:44,079
states and they exist within a finite

368
00:12:44,079 --> 00:12:45,760
set of all possible

369
00:12:45,760 --> 00:12:47,519
hidden states which is denoted by

370
00:12:47,519 --> 00:12:49,279
capital s here

371
00:12:49,279 --> 00:12:51,440
and this joint probability that we get

372
00:12:51,440 --> 00:12:52,399
over o and

373
00:12:52,399 --> 00:12:54,880
s can be factorized into the likelihood

374
00:12:54,880 --> 00:12:55,440
function

375
00:12:55,440 --> 00:12:59,120
which is p of o given s and then you

376
00:12:59,120 --> 00:13:01,360
have the prior over the internal states

377
00:13:01,360 --> 00:13:02,720
which is pss

378
00:13:02,720 --> 00:13:04,839
so this gives you a very nice

379
00:13:04,839 --> 00:13:06,240
formulation um

380
00:13:06,240 --> 00:13:09,519
that we're going to use in a few

381
00:13:09,519 --> 00:13:11,839
next couple of slides and i just wanted

382
00:13:11,839 --> 00:13:12,720
to ask

383
00:13:12,720 --> 00:13:14,480
are you able to see my mouse when i

384
00:13:14,480 --> 00:13:15,920
highlight or when i

385
00:13:15,920 --> 00:13:20,079
or is that not there i can see it yes oh

386
00:13:20,079 --> 00:13:22,719
yeah perfect

387
00:13:22,800 --> 00:13:24,560
so we know that for an agent to minimize

388
00:13:24,560 --> 00:13:25,600
its surprise we would need to

389
00:13:25,600 --> 00:13:27,360
marginalize out all the possible hidden

390
00:13:27,360 --> 00:13:29,040
states that could have led to a given

391
00:13:29,040 --> 00:13:29,760
outcome

392
00:13:29,760 --> 00:13:31,440
and this can be achieved by using the

393
00:13:31,440 --> 00:13:33,440
factorization that i just mentioned the

394
00:13:33,440 --> 00:13:34,160
likelihood

395
00:13:34,160 --> 00:13:37,360
and the prior um but the

396
00:13:37,360 --> 00:13:40,320
the problem is that um this is not a

397
00:13:40,320 --> 00:13:42,399
trivial task because the dimensionality

398
00:13:42,399 --> 00:13:43,519
of the hidden states

399
00:13:43,519 --> 00:13:45,360
um can be extremely large and if you're

400
00:13:45,360 --> 00:13:47,120
considering additional random variables

401
00:13:47,120 --> 00:13:48,480
that we're going to

402
00:13:48,480 --> 00:13:50,240
introduce in a bit this becomes even

403
00:13:50,240 --> 00:13:51,600
more problematic

404
00:13:51,600 --> 00:13:54,480
um and that's why we use uh another

405
00:13:54,480 --> 00:13:56,240
quantity a variational approximation of

406
00:13:56,240 --> 00:13:57,199
this quantity

407
00:13:57,199 --> 00:13:59,920
uh p of i which is more attractable and

408
00:13:59,920 --> 00:14:01,920
allows us to estimate the quantities of

409
00:14:01,920 --> 00:14:05,839
interest um so

410
00:14:05,839 --> 00:14:08,720
so this will um be a natural step to

411
00:14:08,720 --> 00:14:10,160
talk about the variation free energy

412
00:14:10,160 --> 00:14:11,760
which is this variational approximation

413
00:14:11,760 --> 00:14:12,800
of the quantity of

414
00:14:12,800 --> 00:14:15,519
interest um so what is variational free

415
00:14:15,519 --> 00:14:17,199
energy so variation free energy is

416
00:14:17,199 --> 00:14:19,440
defined as the upper bound on surprise

417
00:14:19,440 --> 00:14:20,560
so the first definition

418
00:14:20,560 --> 00:14:21,920
sorry first definition that we

419
00:14:21,920 --> 00:14:24,240
considered is derived using jensen's

420
00:14:24,240 --> 00:14:25,519
inequality and

421
00:14:25,519 --> 00:14:27,600
is commonly known as negative evidence

422
00:14:27,600 --> 00:14:29,760
lower bound in the variational inference

423
00:14:29,760 --> 00:14:32,480
literature so we get this from the

424
00:14:32,480 --> 00:14:34,240
equation 4 that we just saw

425
00:14:34,240 --> 00:14:37,199
by introducing negative log on both

426
00:14:37,199 --> 00:14:37,760
sides

427
00:14:37,760 --> 00:14:41,040
and then um multiplying this term

428
00:14:41,040 --> 00:14:43,680
by 1 which is essentially q of s over q

429
00:14:43,680 --> 00:14:45,360
of s so we're assuming that

430
00:14:45,360 --> 00:14:48,560
q of s cannot be equal to zero and

431
00:14:48,560 --> 00:14:51,199
with that um we then apply jensen's

432
00:14:51,199 --> 00:14:53,199
inequality and we move the log inside

433
00:14:53,199 --> 00:14:54,000
the function

434
00:14:54,000 --> 00:14:56,560
and we end up with our expectation with

435
00:14:56,560 --> 00:14:58,000
respect to t of s

436
00:14:58,000 --> 00:15:00,079
for log of the joint over the

437
00:15:00,079 --> 00:15:01,920
approximate or the variational

438
00:15:01,920 --> 00:15:05,040
uh quantity of interest here and then we

439
00:15:05,040 --> 00:15:07,120
take the negative inside and we can flip

440
00:15:07,120 --> 00:15:07,839
it around

441
00:15:07,839 --> 00:15:11,120
and we get our first

442
00:15:11,120 --> 00:15:14,160
nice quantity of interest here where we

443
00:15:14,160 --> 00:15:14,560
get

444
00:15:14,560 --> 00:15:17,199
the the bound that we're interested in

445
00:15:17,199 --> 00:15:17,600
and

446
00:15:17,600 --> 00:15:20,720
in terms of tail between the approximate

447
00:15:20,720 --> 00:15:24,240
and the joint that we have

448
00:15:24,480 --> 00:15:25,920
so to make this a little bit more

449
00:15:25,920 --> 00:15:27,760
concrete what we can do now is to

450
00:15:27,760 --> 00:15:29,279
further manipulate the variational free

451
00:15:29,279 --> 00:15:29,759
energy

452
00:15:29,759 --> 00:15:34,959
summons um into uh the kill between the

453
00:15:34,959 --> 00:15:38,720
approximate and the true posterior

454
00:15:38,720 --> 00:15:41,360
minus the the log evidence that we the

455
00:15:41,360 --> 00:15:43,120
model evidence that we had

456
00:15:43,120 --> 00:15:45,920
and we can re arrange the last equation

457
00:15:45,920 --> 00:15:47,519
to really hone in on the connection

458
00:15:47,519 --> 00:15:48,959
between

459
00:15:48,959 --> 00:15:51,759
surprise and variational free energy um

460
00:15:51,759 --> 00:15:52,320
so

461
00:15:52,320 --> 00:15:55,040
if you remember that kl is a divergence

462
00:15:55,040 --> 00:15:56,720
which means that it cannot be less than

463
00:15:56,720 --> 00:15:58,079
zero so

464
00:15:58,079 --> 00:16:01,519
um it's always strictly uh greater than

465
00:16:01,519 --> 00:16:03,440
or equal to zero which means that

466
00:16:03,440 --> 00:16:06,399
when our approximate is equal to the

467
00:16:06,399 --> 00:16:07,519
true posterior

468
00:16:07,519 --> 00:16:09,600
we end up with the variation free energy

469
00:16:09,600 --> 00:16:10,720
equal to the model

470
00:16:10,720 --> 00:16:13,839
evidence which means that minimizing

471
00:16:13,839 --> 00:16:15,839
free energy is essentially equivalent to

472
00:16:15,839 --> 00:16:18,320
maximizing the gender model evidence

473
00:16:18,320 --> 00:16:23,839
um okay

474
00:16:25,279 --> 00:16:28,240
um we can rewrite the the previous

475
00:16:28,240 --> 00:16:29,600
equation that we had

476
00:16:29,600 --> 00:16:31,680
um equation 10 to express the

477
00:16:31,680 --> 00:16:33,040
variational free energy

478
00:16:33,040 --> 00:16:34,800
as a function of posterior beliefs in

479
00:16:34,800 --> 00:16:36,079
multiple different forms

480
00:16:36,079 --> 00:16:37,680
so i'm just going to focus on equation

481
00:16:37,680 --> 00:16:40,000
12 here which is the complexity

482
00:16:40,000 --> 00:16:42,800
minus accuracy so this is a trade-off uh

483
00:16:42,800 --> 00:16:45,440
that's normally used when

484
00:16:45,440 --> 00:16:48,839
the papers that essentially says um the

485
00:16:48,839 --> 00:16:50,160
complexity tam

486
00:16:50,160 --> 00:16:53,600
or complexity cost is essentially um

487
00:16:53,600 --> 00:16:57,120
your kale between your approximate um

488
00:16:57,120 --> 00:17:01,120
of s given pi with respect to

489
00:17:01,120 --> 00:17:05,199
uh your p of s given um pi and here pi

490
00:17:05,199 --> 00:17:07,280
is just your policies and these can be

491
00:17:07,280 --> 00:17:09,280
regardless of hypothesis of

492
00:17:09,280 --> 00:17:12,319
how uh the agent is going to act

493
00:17:12,319 --> 00:17:14,079
but i'll come back to what policies

494
00:17:14,079 --> 00:17:15,679
really entail but for now

495
00:17:15,679 --> 00:17:17,599
just consider them as a term that allows

496
00:17:17,599 --> 00:17:19,919
us to condition the free energy

497
00:17:19,919 --> 00:17:22,319
on a sequence of trajectories of

498
00:17:22,319 --> 00:17:23,679
interest

499
00:17:23,679 --> 00:17:26,319
and the second term that we have is the

500
00:17:26,319 --> 00:17:27,599
log probability

501
00:17:27,599 --> 00:17:31,200
of o given s so the likelihood uh

502
00:17:31,200 --> 00:17:34,559
with a with respect to the key of s

503
00:17:34,559 --> 00:17:37,200
that gives you the accuracy so a simple

504
00:17:37,200 --> 00:17:39,120
way to think about it is that

505
00:17:39,120 --> 00:17:42,240
um this is just um um how accurate the

506
00:17:42,240 --> 00:17:43,440
model is and this is some

507
00:17:43,440 --> 00:17:45,919
regularization term that's a penalty

508
00:17:45,919 --> 00:17:47,200
term to make sure that it's not

509
00:17:47,200 --> 00:17:49,280
diverging too far away from our

510
00:17:49,280 --> 00:17:52,320
initial priors

511
00:17:52,320 --> 00:17:55,679
okay so this particular quantity

512
00:17:55,679 --> 00:17:56,960
variational free energy

513
00:17:56,960 --> 00:17:58,480
are there any questions at this point

514
00:17:58,480 --> 00:18:01,039
about the variation for energy

515
00:18:01,039 --> 00:18:03,760
not yet thank you okay perfect okay so

516
00:18:03,760 --> 00:18:05,360
the variation for energy is giving us

517
00:18:05,360 --> 00:18:07,360
this way of perceiving the environment

518
00:18:07,360 --> 00:18:09,200
and addresses one part of the active

519
00:18:09,200 --> 00:18:11,039
influence formulation which is making

520
00:18:11,039 --> 00:18:11,760
inferences

521
00:18:11,760 --> 00:18:14,240
about the given world that the agent is

522
00:18:14,240 --> 00:18:16,080
interacting with at a given point in

523
00:18:16,080 --> 00:18:16,880
time

524
00:18:16,880 --> 00:18:19,440
however we have not actually accounted

525
00:18:19,440 --> 00:18:20,320
for the

526
00:18:20,320 --> 00:18:22,960
active part whereas like this particular

527
00:18:22,960 --> 00:18:23,600
agent

528
00:18:23,600 --> 00:18:25,679
that we have under the active inference

529
00:18:25,679 --> 00:18:26,880
formulation can

530
00:18:26,880 --> 00:18:29,520
take series of actions or interact with

531
00:18:29,520 --> 00:18:31,360
the environment in such a way that it

532
00:18:31,360 --> 00:18:34,240
affects that environment in the future

533
00:18:34,240 --> 00:18:34,960
um

534
00:18:34,960 --> 00:18:37,280
so to motivate this a little bit further

535
00:18:37,280 --> 00:18:38,880
what we can think about is that not only

536
00:18:38,880 --> 00:18:40,480
do we want to minimize our variation

537
00:18:40,480 --> 00:18:42,240
free energy we also want to minimize a

538
00:18:42,240 --> 00:18:44,320
quantity called expected free energy

539
00:18:44,320 --> 00:18:46,320
which depends on anticipated

540
00:18:46,320 --> 00:18:48,240
observations about the future

541
00:18:48,240 --> 00:18:50,799
in the future or about the future and

542
00:18:50,799 --> 00:18:52,080
minimization of this

543
00:18:52,080 --> 00:18:54,480
particular terms allows the agent to

544
00:18:54,480 --> 00:18:56,400
influence the future by taking

545
00:18:56,400 --> 00:18:58,400
particular actions in the present

546
00:18:58,400 --> 00:19:00,080
which are selected from a set of

547
00:19:00,080 --> 00:19:02,720
policies so i mentioned policies a few

548
00:19:02,720 --> 00:19:03,520
times now

549
00:19:03,520 --> 00:19:06,960
so what are they um so policies can be

550
00:19:06,960 --> 00:19:09,280
defined as a sequence of actions at time

551
00:19:09,280 --> 00:19:10,240
tau

552
00:19:10,240 --> 00:19:12,240
that enable an agent to transition

553
00:19:12,240 --> 00:19:13,760
between hidden states

554
00:19:13,760 --> 00:19:17,520
and tau here is essentially as a

555
00:19:17,520 --> 00:19:19,120
sequence of trajectories up to a

556
00:19:19,120 --> 00:19:21,039
particular horizon

557
00:19:21,039 --> 00:19:24,320
uh cap which is um considering the total

558
00:19:24,320 --> 00:19:25,679
number of

559
00:19:25,679 --> 00:19:27,760
time steps that you are considering in a

560
00:19:27,760 --> 00:19:29,280
particular setup

561
00:19:29,280 --> 00:19:32,320
and for us to properly define uh

562
00:19:32,320 --> 00:19:33,919
policy we need to introduce two

563
00:19:33,919 --> 00:19:35,440
additional random variables

564
00:19:35,440 --> 00:19:38,160
so the first one is an action that's

565
00:19:38,160 --> 00:19:38,960
conditioned

566
00:19:38,960 --> 00:19:43,039
on tau which is denoted by utau here

567
00:19:43,039 --> 00:19:46,160
and this exists within a finite set of

568
00:19:46,160 --> 00:19:48,320
all possible actions that the agents can

569
00:19:48,320 --> 00:19:49,200
take

570
00:19:49,200 --> 00:19:50,960
and the second random variable that we

571
00:19:50,960 --> 00:19:52,640
introduce is uh

572
00:19:52,640 --> 00:19:54,559
policy which is the pi that we've

573
00:19:54,559 --> 00:19:55,919
discussed

574
00:19:55,919 --> 00:19:58,000
and this exists within a finite set of

575
00:19:58,000 --> 00:19:58,960
all possible

576
00:19:58,960 --> 00:20:02,000
uh policies um or sequence of actions

577
00:20:02,000 --> 00:20:04,640
in a sense of like the sequential policy

578
00:20:04,640 --> 00:20:06,240
optimization that we're interested

579
00:20:06,240 --> 00:20:08,240
in here so to make it a little bit more

580
00:20:08,240 --> 00:20:09,520
concrete chi

581
00:20:09,520 --> 00:20:11,200
hair the random variable can be

582
00:20:11,200 --> 00:20:13,440
decomposed into

583
00:20:13,440 --> 00:20:16,559
a series of actions over a particular

584
00:20:16,559 --> 00:20:19,760
time time horizon tau

585
00:20:19,760 --> 00:20:23,280
so u1 u2 and um going up to utah would

586
00:20:23,280 --> 00:20:25,120
denote the action from

587
00:20:25,120 --> 00:20:26,960
uh action at time point one action of

588
00:20:26,960 --> 00:20:28,799
time point two and so on

589
00:20:28,799 --> 00:20:31,360
and the link is explicit when you

590
00:20:31,360 --> 00:20:32,320
consider that

591
00:20:32,320 --> 00:20:35,679
if you consider a policy at a particular

592
00:20:35,679 --> 00:20:38,880
time point so tau then the action that

593
00:20:38,880 --> 00:20:41,840
you get would be that action

594
00:20:41,840 --> 00:20:45,280
okay cool um so

595
00:20:45,280 --> 00:20:47,360
i also wanted to highlight that this

596
00:20:47,360 --> 00:20:49,120
definition of policy is actually quite

597
00:20:49,120 --> 00:20:50,559
different to

598
00:20:50,559 --> 00:20:53,520
or distinct to how it's considered in rl

599
00:20:53,520 --> 00:20:54,240
which is

600
00:20:54,240 --> 00:20:56,640
um when they say policy that means state

601
00:20:56,640 --> 00:20:58,000
action policies

602
00:20:58,000 --> 00:20:59,840
so as i just mentioned an act of

603
00:20:59,840 --> 00:21:02,240
inference a policy is simply a sequence

604
00:21:02,240 --> 00:21:04,159
of choices for actions through time that

605
00:21:04,159 --> 00:21:06,159
is a sequential policy

606
00:21:06,159 --> 00:21:08,320
and this is different to a state action

607
00:21:08,320 --> 00:21:10,400
policy in reinforcement learning which

608
00:21:10,400 --> 00:21:12,080
is mapping of states to

609
00:21:12,080 --> 00:21:16,080
actions so your rl policy which takes in

610
00:21:16,080 --> 00:21:16,720
account

611
00:21:16,720 --> 00:21:19,919
the action and the state is probability

612
00:21:19,919 --> 00:21:20,799
of your

613
00:21:20,799 --> 00:21:24,159
action given the state and under

614
00:21:24,159 --> 00:21:27,919
um our mdp formulation

615
00:21:27,919 --> 00:21:30,159
the definition of action oh sorry

616
00:21:30,159 --> 00:21:31,760
definition of policies in

617
00:21:31,760 --> 00:21:33,760
rl and active inference become exactly

618
00:21:33,760 --> 00:21:35,679
the same when we consider the setting

619
00:21:35,679 --> 00:21:37,679
where tau is equal to one so you're only

620
00:21:37,679 --> 00:21:41,120
considering one step ahead

621
00:21:41,120 --> 00:21:45,120
okay okay so um i'm going to move a

622
00:21:45,120 --> 00:21:45,760
little bit

623
00:21:45,760 --> 00:21:49,039
and consider uh the quantitative

624
00:21:49,039 --> 00:21:50,960
interest they expected free energy which

625
00:21:50,960 --> 00:21:54,080
is how do we even derive it so in order

626
00:21:54,080 --> 00:21:56,240
to derive it we first need to extend

627
00:21:56,240 --> 00:21:57,840
the variational free energy definition

628
00:21:57,840 --> 00:22:00,000
that we had before so

629
00:22:00,000 --> 00:22:02,240
a few slides ago and now make it

630
00:22:02,240 --> 00:22:03,360
dependent on both

631
00:22:03,360 --> 00:22:06,799
time so tau and policy and what we're

632
00:22:06,799 --> 00:22:07,919
doing essentially

633
00:22:07,919 --> 00:22:10,320
is taking that same equation and just

634
00:22:10,320 --> 00:22:12,320
decomposing it for previous

635
00:22:12,320 --> 00:22:14,320
sorry previous and current time step

636
00:22:14,320 --> 00:22:16,080
under a particular policy so that way

637
00:22:16,080 --> 00:22:18,080
this is why we have the conditioning and

638
00:22:18,080 --> 00:22:20,480
then we're decomposing it in a

639
00:22:20,480 --> 00:22:23,520
specific way um in equation 15

640
00:22:23,520 --> 00:22:25,039
and then writing out the matrix

641
00:22:25,039 --> 00:22:27,360
formulation in equation 16.

642
00:22:27,360 --> 00:22:28,960
so we can come back to this if there are

643
00:22:28,960 --> 00:22:31,039
any questions

644
00:22:31,039 --> 00:22:32,880
but the key thing to take away from the

645
00:22:32,880 --> 00:22:35,039
slide is that now we're including a

646
00:22:35,039 --> 00:22:37,120
functional dependency on time

647
00:22:37,120 --> 00:22:40,799
um for the variation for energy um

648
00:22:40,799 --> 00:22:42,799
and this is allowing us to now move to

649
00:22:42,799 --> 00:22:45,120
the expected free energy formulation

650
00:22:45,120 --> 00:22:47,280
um but the key thing to note here is

651
00:22:47,280 --> 00:22:49,360
that we're only considering time points

652
00:22:49,360 --> 00:22:52,320
um the previous time point and the

653
00:22:52,320 --> 00:22:52,799
present

654
00:22:52,799 --> 00:22:55,840
not the future at all

655
00:22:58,000 --> 00:23:00,320
so using the free energy equation before

656
00:23:00,320 --> 00:23:01,440
we can derive

657
00:23:01,440 --> 00:23:04,559
uh the expected free energy um

658
00:23:04,559 --> 00:23:06,720
and what is the expected free energy

659
00:23:06,720 --> 00:23:08,320
then so expected free energy

660
00:23:08,320 --> 00:23:11,200
is um the free energy function of future

661
00:23:11,200 --> 00:23:12,159
trajectories

662
00:23:12,159 --> 00:23:14,559
g and it effectively values the evidence

663
00:23:14,559 --> 00:23:16,400
for plausible policies

664
00:23:16,400 --> 00:23:18,240
based on outcomes that have not been

665
00:23:18,240 --> 00:23:19,760
observed yet so that's the key thing so

666
00:23:19,760 --> 00:23:20,640
you're making

667
00:23:20,640 --> 00:23:24,240
inferences about the polls the set of

668
00:23:24,240 --> 00:23:26,000
future trajectories that you haven't

669
00:23:26,000 --> 00:23:28,000
observed um

670
00:23:28,000 --> 00:23:30,640
and there's a two heuristics that are

671
00:23:30,640 --> 00:23:31,679
introduced here

672
00:23:31,679 --> 00:23:34,159
in order to get to the g formulation

673
00:23:34,159 --> 00:23:35,120
that we see

674
00:23:35,120 --> 00:23:38,960
in equation 17. the first

675
00:23:38,960 --> 00:23:41,440
one is to include beliefs about future

676
00:23:41,440 --> 00:23:42,320
outcomes

677
00:23:42,320 --> 00:23:44,159
in the expectation that is we're

678
00:23:44,159 --> 00:23:46,000
supplementing expectation

679
00:23:46,000 --> 00:23:48,159
under the approximate posterior with the

680
00:23:48,159 --> 00:23:50,000
likelihood here

681
00:23:50,000 --> 00:23:51,440
which results in a predictive

682
00:23:51,440 --> 00:23:53,279
distribution given by these first two

683
00:23:53,279 --> 00:23:54,880
terms here

684
00:23:54,880 --> 00:23:57,600
and the second one is that we're

685
00:23:57,600 --> 00:23:58,559
implicitly or

686
00:23:58,559 --> 00:24:00,799
i guess explicitly conditioning on the

687
00:24:00,799 --> 00:24:02,799
joint probabilities of states

688
00:24:02,799 --> 00:24:06,080
and observations in the gender model um

689
00:24:06,080 --> 00:24:08,960
sorry in the gender model um dependent

690
00:24:08,960 --> 00:24:10,159
on

691
00:24:10,159 --> 00:24:13,200
the desired state of affairs as opposed

692
00:24:13,200 --> 00:24:14,880
to a particular policy now so this

693
00:24:14,880 --> 00:24:16,799
constrains the type of preferences that

694
00:24:16,799 --> 00:24:17,440
the agent

695
00:24:17,440 --> 00:24:20,640
would have and what's helpful with these

696
00:24:20,640 --> 00:24:21,200
two

697
00:24:21,200 --> 00:24:22,720
moves that we're making is that we can

698
00:24:22,720 --> 00:24:24,480
now evaluate this quantity

699
00:24:24,480 --> 00:24:27,200
before actually having the observations

700
00:24:27,200 --> 00:24:28,240
and the second one

701
00:24:28,240 --> 00:24:30,400
is that the minimization of g would

702
00:24:30,400 --> 00:24:32,080
actually encourage polity

703
00:24:32,080 --> 00:24:35,440
policies to be consistent with uh the

704
00:24:35,440 --> 00:24:37,520
desired state of affairs that the agent

705
00:24:37,520 --> 00:24:39,760
expects itself to be in

706
00:24:39,760 --> 00:24:41,600
um i'm just going to briefly mention

707
00:24:41,600 --> 00:24:44,240
that this is not the only way to derive

708
00:24:44,240 --> 00:24:45,679
expected free energy and there's been

709
00:24:45,679 --> 00:24:47,760
some work um that's looked at other

710
00:24:47,760 --> 00:24:48,960
formulations

711
00:24:48,960 --> 00:24:52,240
uh including work by carl um where

712
00:24:52,240 --> 00:24:53,600
the formulation of the expected free

713
00:24:53,600 --> 00:24:55,919
energy can be decomposed into different

714
00:24:55,919 --> 00:24:58,799
um structures so if anyone's interested

715
00:24:58,799 --> 00:25:02,480
in that we can go through it later

716
00:25:02,640 --> 00:25:04,799
um but this free energy expected free

717
00:25:04,799 --> 00:25:05,679
energy that

718
00:25:05,679 --> 00:25:07,520
i just introduced can be decomposed in

719
00:25:07,520 --> 00:25:10,320
certain ways um so equation 20 and 21

720
00:25:10,320 --> 00:25:10,799
give

721
00:25:10,799 --> 00:25:13,200
two different decompositions of the

722
00:25:13,200 --> 00:25:14,960
first one being the epistemic and the

723
00:25:14,960 --> 00:25:17,039
extrinsic value trader and the second

724
00:25:17,039 --> 00:25:18,960
one being the expected and the

725
00:25:18,960 --> 00:25:22,320
uh cost and ambiguity term so if we just

726
00:25:22,320 --> 00:25:23,360
consider the

727
00:25:23,360 --> 00:25:26,640
um the first equation we can say that if

728
00:25:26,640 --> 00:25:28,080
we're minimizing this equation

729
00:25:28,080 --> 00:25:30,559
then we're capturing this imperative to

730
00:25:30,559 --> 00:25:31,360
maximize

731
00:25:31,360 --> 00:25:33,760
the information gain that you would have

732
00:25:33,760 --> 00:25:35,679
from observing the environment

733
00:25:35,679 --> 00:25:38,880
um about particular hidden states

734
00:25:38,880 --> 00:25:41,840
while maximizing the expected value

735
00:25:41,840 --> 00:25:43,360
which is scored by the

736
00:25:43,360 --> 00:25:45,520
log preferences or the extrinsic value

737
00:25:45,520 --> 00:25:46,559
here so

738
00:25:46,559 --> 00:25:48,159
this particular formulation actually

739
00:25:48,159 --> 00:25:50,320
gives us a very clear trade-off between

740
00:25:50,320 --> 00:25:50,880
the

741
00:25:50,880 --> 00:25:53,200
um the first component which is the

742
00:25:53,200 --> 00:25:54,960
epistemic value that promotes

743
00:25:54,960 --> 00:25:57,120
curious behavior so that's what you want

744
00:25:57,120 --> 00:25:59,760
with exploration encouraged as the agent

745
00:25:59,760 --> 00:26:00,799
seeks out

746
00:26:00,799 --> 00:26:02,400
these new states that minimize

747
00:26:02,400 --> 00:26:04,960
uncertainty about the environment

748
00:26:04,960 --> 00:26:07,520
and the latter bit is more pragmatic and

749
00:26:07,520 --> 00:26:08,400
it encourages

750
00:26:08,400 --> 00:26:11,360
exploitative behavior through um this

751
00:26:11,360 --> 00:26:13,360
understanding of the type of

752
00:26:13,360 --> 00:26:15,679
uh policies that the agent would prefer

753
00:26:15,679 --> 00:26:16,720
to reach

754
00:26:16,720 --> 00:26:18,960
um in other words like this expected

755
00:26:18,960 --> 00:26:20,480
free energy formulation

756
00:26:20,480 --> 00:26:23,200
and that we're seeing in equation 20 is

757
00:26:23,200 --> 00:26:24,559
essentially treating

758
00:26:24,559 --> 00:26:26,640
exploration and exploitation as two

759
00:26:26,640 --> 00:26:28,240
different ways of tackling the same

760
00:26:28,240 --> 00:26:28,799
problem

761
00:26:28,799 --> 00:26:31,440
so minimizing uncertainty that mentioned

762
00:26:31,440 --> 00:26:34,720
at the start of the presentation

763
00:26:34,720 --> 00:26:37,120
okay um we can also think about the

764
00:26:37,120 --> 00:26:38,159
second equation

765
00:26:38,159 --> 00:26:41,279
here which is just offering us an

766
00:26:41,279 --> 00:26:43,200
alternative perspective on the

767
00:26:43,200 --> 00:26:45,360
free expected frequency which is an

768
00:26:45,360 --> 00:26:47,919
agent wishes to minimize the ambiguity

769
00:26:47,919 --> 00:26:50,000
and the degree to which outcomes under a

770
00:26:50,000 --> 00:26:52,000
particular policy deviate from

771
00:26:52,000 --> 00:26:55,039
prior preferences thus ambiguity here is

772
00:26:55,039 --> 00:26:56,880
the expectation of the conditional

773
00:26:56,880 --> 00:26:58,880
entropy or the uncertainty about

774
00:26:58,880 --> 00:27:01,600
outcomes under the current policy in

775
00:27:01,600 --> 00:27:02,799
this particular setting

776
00:27:02,799 --> 00:27:04,559
low entropy would suggest that outcomes

777
00:27:04,559 --> 00:27:06,240
are quite salient and uniquely

778
00:27:06,240 --> 00:27:08,320
informative about the hidden states

779
00:27:08,320 --> 00:27:10,320
so for example the visual cues that you

780
00:27:10,320 --> 00:27:11,760
might see if the room is actually

781
00:27:11,760 --> 00:27:13,039
completely tough

782
00:27:13,039 --> 00:27:14,720
in comparison to if it's quite dark

783
00:27:14,720 --> 00:27:16,080
you're not going to make out anything

784
00:27:16,080 --> 00:27:19,440
important from that in addition um the

785
00:27:19,440 --> 00:27:20,880
agent will like to pursue policy

786
00:27:20,880 --> 00:27:21,520
dependent

787
00:27:21,520 --> 00:27:23,760
outcomes that resemble its preferred

788
00:27:23,760 --> 00:27:25,039
outcomes

789
00:27:25,039 --> 00:27:27,120
so it's the donated by c here and this

790
00:27:27,120 --> 00:27:29,279
is achieved when the care divergence

791
00:27:29,279 --> 00:27:31,039
between the predicted

792
00:27:31,039 --> 00:27:34,320
and the preferred outcomes is minimized

793
00:27:34,320 --> 00:27:37,679
by a particular policy

794
00:27:37,679 --> 00:27:40,880
okay um and these prior beliefs about

795
00:27:40,880 --> 00:27:41,520
the future

796
00:27:41,520 --> 00:27:42,960
outcomes equip the agent with

797
00:27:42,960 --> 00:27:44,960
goal-directed behavior

798
00:27:44,960 --> 00:27:46,960
which is one of the i guess the

799
00:27:46,960 --> 00:27:48,320
instances that

800
00:27:48,320 --> 00:27:51,840
is really important in active influence

801
00:27:51,840 --> 00:27:54,640
um okay so once we have the expected

802
00:27:54,640 --> 00:27:56,240
free energy we can derive

803
00:27:56,240 --> 00:28:00,159
um the the policies um so

804
00:28:00,159 --> 00:28:02,240
um and this is realized by deriving the

805
00:28:02,240 --> 00:28:04,080
probability of any policy

806
00:28:04,080 --> 00:28:06,399
by applying a soft max function over the

807
00:28:06,399 --> 00:28:08,480
expected free energy

808
00:28:08,480 --> 00:28:09,919
and this sort of illustrates the

809
00:28:09,919 --> 00:28:11,760
self-evidencing behavior of active

810
00:28:11,760 --> 00:28:12,559
inference

811
00:28:12,559 --> 00:28:14,559
because any sort of policy or action

812
00:28:14,559 --> 00:28:16,480
sequence that results in lower expected

813
00:28:16,480 --> 00:28:18,240
free energy are more likely

814
00:28:18,240 --> 00:28:21,520
and intuitively this would uh make

815
00:28:21,520 --> 00:28:24,159
sense because the expected free energy

816
00:28:24,159 --> 00:28:25,039
is sort of

817
00:28:25,039 --> 00:28:26,960
encapsulating all the types of things

818
00:28:26,960 --> 00:28:28,640
that you want to include

819
00:28:28,640 --> 00:28:30,399
or consider when you're interacting with

820
00:28:30,399 --> 00:28:31,600
the world so you want to explore you

821
00:28:31,600 --> 00:28:32,960
want to exploit but you want to have

822
00:28:32,960 --> 00:28:36,000
a balance of that um and then

823
00:28:36,000 --> 00:28:38,159
when you're selecting your policy um

824
00:28:38,159 --> 00:28:39,600
it's just a matter of determining the

825
00:28:39,600 --> 00:28:40,159
set of

826
00:28:40,159 --> 00:28:42,960
actions which get you closest to this

827
00:28:42,960 --> 00:28:44,000
particular core

828
00:28:44,000 --> 00:28:45,760
um and this can be defined by an

829
00:28:45,760 --> 00:28:48,960
attractive state that's defined by your

830
00:28:48,960 --> 00:28:52,480
c matrix that we

831
00:28:52,480 --> 00:28:55,200
described before um and if you don't

832
00:28:55,200 --> 00:28:56,159
have that then

833
00:28:56,159 --> 00:28:57,919
it's just random exploration that you

834
00:28:57,919 --> 00:28:59,600
would get

835
00:28:59,600 --> 00:29:01,600
um sometimes you can also include a

836
00:29:01,600 --> 00:29:03,760
temperature parameter beta here

837
00:29:03,760 --> 00:29:06,080
um and by having a hyper prime on this

838
00:29:06,080 --> 00:29:07,520
you introduce an

839
00:29:07,520 --> 00:29:09,440
additional complexity cost into the

840
00:29:09,440 --> 00:29:10,799
expected free energy

841
00:29:10,799 --> 00:29:14,720
uh formulation which allows you to

842
00:29:14,720 --> 00:29:17,840
account for um how flat

843
00:29:17,840 --> 00:29:20,880
or how

844
00:29:20,880 --> 00:29:23,279
confident or precise you want your

845
00:29:23,279 --> 00:29:24,159
preferences

846
00:29:24,159 --> 00:29:27,919
to be over the policy space

847
00:29:27,919 --> 00:29:31,279
um the key thing to note is that um

848
00:29:31,279 --> 00:29:33,039
for a sake of simplicity i'm not going

849
00:29:33,039 --> 00:29:34,880
to go through a lot of the

850
00:29:34,880 --> 00:29:37,200
the details about how these are

851
00:29:37,200 --> 00:29:37,919
optimized

852
00:29:37,919 --> 00:29:40,159
but you can do that in multiple

853
00:29:40,159 --> 00:29:41,919
different ways for example in active

854
00:29:41,919 --> 00:29:42,720
inference

855
00:29:42,720 --> 00:29:45,039
we can optimize expectation about the

856
00:29:45,039 --> 00:29:47,039
hidden states of interest the policies

857
00:29:47,039 --> 00:29:48,640
the precisions that are inferenced

858
00:29:48,640 --> 00:29:50,320
and then we can also optimize the model

859
00:29:50,320 --> 00:29:51,679
parameters through

860
00:29:51,679 --> 00:29:53,919
um through the learning procedures

861
00:29:53,919 --> 00:29:54,960
involved

862
00:29:54,960 --> 00:29:58,080
um but those sort of differ depending on

863
00:29:58,080 --> 00:30:00,000
the setup you're looking at for example

864
00:30:00,000 --> 00:30:01,600
if you're using variation base you would

865
00:30:01,600 --> 00:30:02,559
just iterate

866
00:30:02,559 --> 00:30:04,799
this uh these functions or objective

867
00:30:04,799 --> 00:30:06,559
functions until convergence

868
00:30:06,559 --> 00:30:08,960
or an active inference um you do a

869
00:30:08,960 --> 00:30:10,080
gradient ascend

870
00:30:10,080 --> 00:30:12,000
um to find the sufficient statistics of

871
00:30:12,000 --> 00:30:13,919
interest again this depends on

872
00:30:13,919 --> 00:30:15,840
exactly which formulation and setup

873
00:30:15,840 --> 00:30:18,080
you're looking at

874
00:30:18,080 --> 00:30:20,240
but the key thing to note here is that

875
00:30:20,240 --> 00:30:21,679
um that there's

876
00:30:21,679 --> 00:30:23,760
three particular aspects to the active

877
00:30:23,760 --> 00:30:25,360
inference algorithm that are useful and

878
00:30:25,360 --> 00:30:26,159
can be

879
00:30:26,159 --> 00:30:28,960
uh taken from this particular uh

880
00:30:28,960 --> 00:30:29,679
framework

881
00:30:29,679 --> 00:30:32,799
and applied to uh other settings

882
00:30:32,799 --> 00:30:34,559
um so i'm just gonna reiterate and

883
00:30:34,559 --> 00:30:36,080
summarize them briefly

884
00:30:36,080 --> 00:30:38,000
so we first have the gender model that's

885
00:30:38,000 --> 00:30:39,679
crucial so in order for an agent to

886
00:30:39,679 --> 00:30:41,440
interact and minimize its surprise it

887
00:30:41,440 --> 00:30:43,520
needs a gentle model of the world

888
00:30:43,520 --> 00:30:46,880
and that's described as simply i'm not

889
00:30:46,880 --> 00:30:48,640
i'm not including any of the model

890
00:30:48,640 --> 00:30:50,240
parameters here but you can have

891
00:30:50,240 --> 00:30:52,159
your outcomes your states and your

892
00:30:52,159 --> 00:30:53,679
policies and this

893
00:30:53,679 --> 00:30:56,960
um these are decomposed into um

894
00:30:56,960 --> 00:30:59,120
yes sorry there's a arrow with the

895
00:30:59,120 --> 00:31:00,000
brackets here but

896
00:31:00,000 --> 00:31:03,120
these are decomposed into your prior

897
00:31:03,120 --> 00:31:04,720
your likelihood and your transition

898
00:31:04,720 --> 00:31:05,760
function

899
00:31:05,760 --> 00:31:07,840
and then you set once you have this

900
00:31:07,840 --> 00:31:10,000
genital model the objective the agent

901
00:31:10,000 --> 00:31:11,519
is to fit the model to sample

902
00:31:11,519 --> 00:31:13,360
observations to reduce the price

903
00:31:13,360 --> 00:31:14,960
and that is through variational free

904
00:31:14,960 --> 00:31:17,279
energy optimization so this particular

905
00:31:17,279 --> 00:31:19,120
trade-off that we have between

906
00:31:19,120 --> 00:31:21,840
the complexity and the accuracy cost and

907
00:31:21,840 --> 00:31:22,240
then

908
00:31:22,240 --> 00:31:24,000
the second oh sorry the last part of

909
00:31:24,000 --> 00:31:25,760
this algorithm is to plan

910
00:31:25,760 --> 00:31:27,760
so select actions that minimize

911
00:31:27,760 --> 00:31:29,200
uncertainty

912
00:31:29,200 --> 00:31:32,000
that um that is the expected free energy

913
00:31:32,000 --> 00:31:33,760
and the way you do that is by having a

914
00:31:33,760 --> 00:31:34,480
soft max

915
00:31:34,480 --> 00:31:37,600
over that this uh g negative of g the

916
00:31:37,600 --> 00:31:39,120
quantity we have here

917
00:31:39,120 --> 00:31:41,360
and then sampling from that in order to

918
00:31:41,360 --> 00:31:42,720
select the next specs

919
00:31:42,720 --> 00:31:46,080
best action okay

920
00:31:46,080 --> 00:31:49,600
so that's a quick and deep dive into

921
00:31:49,600 --> 00:31:51,760
a massive amount of active inference

922
00:31:51,760 --> 00:31:52,799
literature but i just wanted to

923
00:31:52,799 --> 00:31:53,600
highlight that

924
00:31:53,600 --> 00:31:55,120
those are the three core ingredients

925
00:31:55,120 --> 00:31:57,120
that if you are interested in

926
00:31:57,120 --> 00:31:59,279
implementing these algorithms yourself

927
00:31:59,279 --> 00:32:01,440
okay so now i'm just going to switch

928
00:32:01,440 --> 00:32:02,720
gears a little bit and walk through

929
00:32:02,720 --> 00:32:04,080
comparisons with

930
00:32:04,080 --> 00:32:07,679
reinforcement learning so in our work we

931
00:32:07,679 --> 00:32:09,360
considered a modified version of the

932
00:32:09,360 --> 00:32:10,320
open ai

933
00:32:10,320 --> 00:32:12,880
gyms frozen lake environment so frozen

934
00:32:12,880 --> 00:32:14,720
lake has a grid-like structure

935
00:32:14,720 --> 00:32:17,120
with four distinct patches so it has a

936
00:32:17,120 --> 00:32:18,399
starting point which is

937
00:32:18,399 --> 00:32:22,000
s um so we can see it here um apologies

938
00:32:22,000 --> 00:32:22,799
but it's super

939
00:32:22,799 --> 00:32:25,679
tiny so s is here and you've got the

940
00:32:25,679 --> 00:32:26,640
frozen

941
00:32:26,640 --> 00:32:29,600
surface which is f so again i don't

942
00:32:29,600 --> 00:32:31,120
think i can differentiate because i'm

943
00:32:31,120 --> 00:32:32,399
just moving my mouth

944
00:32:32,399 --> 00:32:36,080
um i'm zooming in they can see it

945
00:32:36,080 --> 00:32:38,080
perfect so you've got the frozen f and

946
00:32:38,080 --> 00:32:39,760
then you've got the whole

947
00:32:39,760 --> 00:32:42,240
and then lastly you have the goal so g

948
00:32:42,240 --> 00:32:43,360
here

949
00:32:43,360 --> 00:32:45,760
where the first b is located and all

950
00:32:45,760 --> 00:32:48,000
patches in this particular setup

951
00:32:48,000 --> 00:32:50,320
are safe except for hall where if the

952
00:32:50,320 --> 00:32:51,279
agent goes

953
00:32:51,279 --> 00:32:54,640
to h it gets a negative reward um

954
00:32:54,640 --> 00:32:57,039
the agent starts each episode at the

955
00:32:57,039 --> 00:32:57,679
first

956
00:32:57,679 --> 00:32:59,519
um position which is the starting

957
00:32:59,519 --> 00:33:01,519
position and from there it needs to

958
00:33:01,519 --> 00:33:03,200
reach the frisbee location

959
00:33:03,200 --> 00:33:07,279
in um in the least amount of steps

960
00:33:07,279 --> 00:33:08,000
possible

961
00:33:08,000 --> 00:33:09,840
and the way you can do that is by

962
00:33:09,840 --> 00:33:11,679
performing uh four different types of

963
00:33:11,679 --> 00:33:12,799
actions so either going

964
00:33:12,799 --> 00:33:15,840
left right down or up and

965
00:33:15,840 --> 00:33:17,840
the agent is allowed to carry on moving

966
00:33:17,840 --> 00:33:18,960
through the frozen lake

967
00:33:18,960 --> 00:33:21,279
uh with multiple revisits so you can go

968
00:33:21,279 --> 00:33:22,720
back to the starting position having

969
00:33:22,720 --> 00:33:24,480
like on in other places

970
00:33:24,480 --> 00:33:26,960
but each episode will end when either

971
00:33:26,960 --> 00:33:27,919
reaches the

972
00:33:27,919 --> 00:33:31,600
hall or the goal location and

973
00:33:31,600 --> 00:33:34,000
these locations differ depending on the

974
00:33:34,000 --> 00:33:35,120
setup that we'll

975
00:33:35,120 --> 00:33:38,320
um have in our simulations so

976
00:33:38,320 --> 00:33:40,720
in one set up um the position of the

977
00:33:40,720 --> 00:33:41,440
whole is

978
00:33:41,440 --> 00:33:43,840
eight and the goal is six and another

979
00:33:43,840 --> 00:33:45,679
set up the position of the whole

980
00:33:45,679 --> 00:33:49,279
is six and um

981
00:33:49,279 --> 00:33:51,919
the goal is eight and the objective is

982
00:33:51,919 --> 00:33:53,679
as i said to reach the goal

983
00:33:53,679 --> 00:33:56,000
in ideally as few steps as possible

984
00:33:56,000 --> 00:33:57,440
while avoiding the whole because that

985
00:33:57,440 --> 00:33:59,120
would end the episode

986
00:33:59,120 --> 00:34:00,880
uh if it reaches the goal it gets a

987
00:34:00,880 --> 00:34:02,559
positive reward of 100

988
00:34:02,559 --> 00:34:05,600
and negative otherwise um

989
00:34:05,600 --> 00:34:07,840
the key thing to note here is that this

990
00:34:07,840 --> 00:34:09,760
query metric actually allows us a way to

991
00:34:09,760 --> 00:34:11,280
compare the active inference

992
00:34:11,280 --> 00:34:12,960
algorithms to reinforcement learning

993
00:34:12,960 --> 00:34:14,879
algorithms but it's not really

994
00:34:14,879 --> 00:34:16,639
important for the active inference

995
00:34:16,639 --> 00:34:18,839
algorithm to

996
00:34:18,839 --> 00:34:22,000
um have the reward function as a get-go

997
00:34:22,000 --> 00:34:24,000
because it can still move around

998
00:34:24,000 --> 00:34:26,639
and using just the information game tab

999
00:34:26,639 --> 00:34:28,879
so not having the extrinsic value

1000
00:34:28,879 --> 00:34:30,239
component

1001
00:34:30,239 --> 00:34:33,199
um and that that's quite interesting

1002
00:34:33,199 --> 00:34:35,040
because we'll see the ramifications of

1003
00:34:35,040 --> 00:34:36,960
that in our assimilations

1004
00:34:36,960 --> 00:34:39,599
and for this particular setup we limited

1005
00:34:39,599 --> 00:34:41,040
the maximum number of time steps for

1006
00:34:41,040 --> 00:34:42,320
each episode

1007
00:34:42,320 --> 00:34:45,679
to 15. okay

1008
00:34:45,679 --> 00:34:47,918
so what i'm going to do is first talk

1009
00:34:47,918 --> 00:34:49,199
through the genital model that we use

1010
00:34:49,199 --> 00:34:51,359
for the active inference formulation

1011
00:34:51,359 --> 00:34:53,440
um so here what you're seeing on the

1012
00:34:53,440 --> 00:34:55,359
slide is a graphical representation of

1013
00:34:55,359 --> 00:34:57,520
the active and for instance model

1014
00:34:57,520 --> 00:34:59,280
so this model contains four action

1015
00:34:59,280 --> 00:35:00,800
states so right

1016
00:35:00,800 --> 00:35:03,920
down up and left and

1017
00:35:03,920 --> 00:35:06,400
these control the ability to transition

1018
00:35:06,400 --> 00:35:08,240
between hidden states

1019
00:35:08,240 --> 00:35:11,359
uh location fact so for example if you

1020
00:35:11,359 --> 00:35:12,240
are in position

1021
00:35:12,240 --> 00:35:14,960
one and you take the action right then

1022
00:35:14,960 --> 00:35:15,760
you'll end up

1023
00:35:15,760 --> 00:35:20,000
in position two or if you're in location

1024
00:35:20,000 --> 00:35:22,480
five and you take um the up action

1025
00:35:22,480 --> 00:35:24,240
you'll end up in position two

1026
00:35:24,240 --> 00:35:27,920
as well um

1027
00:35:27,920 --> 00:35:30,560
in this particular setting both

1028
00:35:30,560 --> 00:35:31,200
positions

1029
00:35:31,200 --> 00:35:33,040
six and eight are absorbing states

1030
00:35:33,040 --> 00:35:34,320
because if you'll remember

1031
00:35:34,320 --> 00:35:36,480
once the agent goes to that location

1032
00:35:36,480 --> 00:35:37,680
they're not able to move

1033
00:35:37,680 --> 00:35:40,640
out so that's when the episode ends um

1034
00:35:40,640 --> 00:35:42,720
and if an agent makes an improbable move

1035
00:35:42,720 --> 00:35:44,480
in this particular maze for example if

1036
00:35:44,480 --> 00:35:45,040
it tries

1037
00:35:45,040 --> 00:35:47,200
to go from position one to left it will

1038
00:35:47,200 --> 00:35:50,000
just stay in that location it won't move

1039
00:35:50,000 --> 00:35:52,320
in this particular um gender model so

1040
00:35:52,320 --> 00:35:53,920
i'm just looking at the hidden states

1041
00:35:53,920 --> 00:35:54,640
now

1042
00:35:54,640 --> 00:35:56,960
we have a chronic um transfer product

1043
00:35:56,960 --> 00:35:58,640
between the two factors so we have

1044
00:35:58,640 --> 00:36:00,720
location and context here um

1045
00:36:00,720 --> 00:36:03,359
the context cannot be changed by the

1046
00:36:03,359 --> 00:36:04,000
agent

1047
00:36:04,000 --> 00:36:05,680
that we have um because this is

1048
00:36:05,680 --> 00:36:07,040
something that's determined by the

1049
00:36:07,040 --> 00:36:08,160
environment and

1050
00:36:08,160 --> 00:36:10,079
this determines where the goal of the

1051
00:36:10,079 --> 00:36:12,800
whole locations are whereas the location

1052
00:36:12,800 --> 00:36:14,480
is something that the agent has control

1053
00:36:14,480 --> 00:36:16,160
over and that's where we have the action

1054
00:36:16,160 --> 00:36:16,800
states

1055
00:36:16,800 --> 00:36:20,720
over this um so with the context we have

1056
00:36:20,720 --> 00:36:21,839
two contexts

1057
00:36:21,839 --> 00:36:25,599
the first one is where the goal is in

1058
00:36:25,599 --> 00:36:28,560
your location eight and the hallways in

1059
00:36:28,560 --> 00:36:29,839
location six

1060
00:36:29,839 --> 00:36:32,079
and the second context is where the goal

1061
00:36:32,079 --> 00:36:33,280
is in location

1062
00:36:33,280 --> 00:36:37,119
six and the hall is in location eight

1063
00:36:37,119 --> 00:36:41,119
um at each time point the agent will

1064
00:36:41,119 --> 00:36:44,560
observe uh two outcomes one would be its

1065
00:36:44,560 --> 00:36:46,800
own position in this particular maze

1066
00:36:46,800 --> 00:36:48,960
and the second would be the score that

1067
00:36:48,960 --> 00:36:50,400
the agent would get

1068
00:36:50,400 --> 00:36:52,560
um the likelihood for the grid position

1069
00:36:52,560 --> 00:36:54,720
is entirely determined by the location

1070
00:36:54,720 --> 00:36:56,079
of the agent

1071
00:36:56,079 --> 00:37:01,440
and the score um is determined by

1072
00:37:01,599 --> 00:37:03,920
both the location and the context in

1073
00:37:03,920 --> 00:37:04,960
place so

1074
00:37:04,960 --> 00:37:08,960
if the agent is in location six

1075
00:37:08,960 --> 00:37:11,680
and it's in context two then it will

1076
00:37:11,680 --> 00:37:13,119
receive a positive reward

1077
00:37:13,119 --> 00:37:15,119
otherwise it will re receive a negative

1078
00:37:15,119 --> 00:37:16,320
or mutual reward

1079
00:37:16,320 --> 00:37:20,320
depending on where it is um

1080
00:37:20,320 --> 00:37:22,640
based on um trying to make that

1081
00:37:22,640 --> 00:37:24,079
comparison with reinforcement learning

1082
00:37:24,079 --> 00:37:25,839
what we're doing here is we

1083
00:37:25,839 --> 00:37:28,560
are introducing uh prime preferences

1084
00:37:28,560 --> 00:37:29,119
where

1085
00:37:29,119 --> 00:37:32,640
the agent has plus four um for

1086
00:37:32,640 --> 00:37:36,320
um positive reward negative four four

1087
00:37:36,320 --> 00:37:38,720
sorry minus four for negative reward and

1088
00:37:38,720 --> 00:37:39,760
otherwise and it

1089
00:37:39,760 --> 00:37:42,240
at the first stage it expects itself to

1090
00:37:42,240 --> 00:37:43,839
be in the

1091
00:37:43,839 --> 00:37:46,880
first location

1092
00:37:47,440 --> 00:37:50,560
so we compared um this particular

1093
00:37:50,560 --> 00:37:52,480
gender model and the active inference

1094
00:37:52,480 --> 00:37:53,760
agent to

1095
00:37:53,760 --> 00:37:55,760
two reinforcement learning algorithms so

1096
00:37:55,760 --> 00:37:58,320
the first one was the q learning using

1097
00:37:58,320 --> 00:38:00,400
epsilon greedy exploration

1098
00:38:00,400 --> 00:38:02,079
and the second one was the bayesian

1099
00:38:02,079 --> 00:38:03,839
model based reinforcement learning

1100
00:38:03,839 --> 00:38:05,599
algorithm using standard thompson

1101
00:38:05,599 --> 00:38:06,720
sampling

1102
00:38:06,720 --> 00:38:08,560
and thompson sampling is appropriate

1103
00:38:08,560 --> 00:38:10,320
procedure here because it entails the

1104
00:38:10,320 --> 00:38:12,560
optimization of dual objectives

1105
00:38:12,560 --> 00:38:15,920
reward maximization and information gain

1106
00:38:15,920 --> 00:38:17,440
and this is achieved by having this

1107
00:38:17,440 --> 00:38:19,440
distribution over a particular function

1108
00:38:19,440 --> 00:38:20,960
that we primarize by

1109
00:38:20,960 --> 00:38:22,720
a prior just by having a prior

1110
00:38:22,720 --> 00:38:26,879
distribution over it that we sample from

1111
00:38:27,119 --> 00:38:32,320
okay um

1112
00:38:32,320 --> 00:38:35,760
so for the two q lining algorithms we um

1113
00:38:35,760 --> 00:38:38,960
have two epsilon greedy uh parameters

1114
00:38:38,960 --> 00:38:41,680
so one where it's fixed exploration set

1115
00:38:41,680 --> 00:38:42,880
to 0.1

1116
00:38:42,880 --> 00:38:44,240
and then another one where we have

1117
00:38:44,240 --> 00:38:46,400
decaying exploration that starts from

1118
00:38:46,400 --> 00:38:49,920
one and decays down to zero

1119
00:38:50,880 --> 00:38:54,000
so first we assessed um this um how the

1120
00:38:54,000 --> 00:38:55,440
agents interacted in a stationary

1121
00:38:55,440 --> 00:38:56,320
setting where the

1122
00:38:56,320 --> 00:38:58,960
reward wasn't changing uh such that the

1123
00:38:58,960 --> 00:39:00,800
goal location was always at six and the

1124
00:39:00,800 --> 00:39:01,760
whole location was

1125
00:39:01,760 --> 00:39:04,240
always that age and then we evaluate the

1126
00:39:04,240 --> 00:39:06,000
performance of the agents um the key

1127
00:39:06,000 --> 00:39:07,599
thing to take away from here is that

1128
00:39:07,599 --> 00:39:08,240
both the

1129
00:39:08,240 --> 00:39:09,920
bayesian rl and the active inference

1130
00:39:09,920 --> 00:39:11,680
agents are able to

1131
00:39:11,680 --> 00:39:14,400
quickly learn um where the reward

1132
00:39:14,400 --> 00:39:16,720
location is and just maximize it out

1133
00:39:16,720 --> 00:39:19,119
and this is um this is performance is

1134
00:39:19,119 --> 00:39:21,119
consistent denoted by the really tight

1135
00:39:21,119 --> 00:39:23,440
confidence bounds that we see

1136
00:39:23,440 --> 00:39:26,960
in comparison the q learning agents are

1137
00:39:26,960 --> 00:39:30,480
um so for the one where we have uh fixed

1138
00:39:30,480 --> 00:39:31,680
exploration

1139
00:39:31,680 --> 00:39:34,640
it's fairly um fairly good and is able

1140
00:39:34,640 --> 00:39:36,240
to land where the reward

1141
00:39:36,240 --> 00:39:38,480
is located but there is um some

1142
00:39:38,480 --> 00:39:40,320
deviation denoted by that

1143
00:39:40,320 --> 00:39:43,359
ten percent of selecting a random action

1144
00:39:43,359 --> 00:39:46,320
um whereas the q learning where we have

1145
00:39:46,320 --> 00:39:48,640
epsilon is equal to one to gain to zero

1146
00:39:48,640 --> 00:39:51,440
um the performance isn't the greatest

1147
00:39:51,440 --> 00:39:53,119
and for the null model the active

1148
00:39:53,119 --> 00:39:55,520
inference where there is no reward here

1149
00:39:55,520 --> 00:39:57,680
the agent randomly goes to the hall and

1150
00:39:57,680 --> 00:39:59,040
randomly goes to the goal

1151
00:39:59,040 --> 00:40:02,560
50 of the time but the key thing to note

1152
00:40:02,560 --> 00:40:04,160
is that apart from the non-model

1153
00:40:04,160 --> 00:40:07,040
all the models are doing fairly well and

1154
00:40:07,040 --> 00:40:07,760
seem to be

1155
00:40:07,760 --> 00:40:09,760
performing okay within the stationary

1156
00:40:09,760 --> 00:40:10,960
setting

1157
00:40:10,960 --> 00:40:12,720
so the next thing no could i just make a

1158
00:40:12,720 --> 00:40:14,319
point about these experiments as well

1159
00:40:14,319 --> 00:40:14,960
just because

1160
00:40:14,960 --> 00:40:16,640
they also do veer away slightly from

1161
00:40:16,640 --> 00:40:19,280
like the traditional rl like

1162
00:40:19,280 --> 00:40:22,480
which is yeah great so basically

1163
00:40:22,480 --> 00:40:24,000
like usually in ro what happens is you

1164
00:40:24,000 --> 00:40:25,760
kind of have this ambiguation between

1165
00:40:25,760 --> 00:40:27,040
like training time and test time

1166
00:40:27,040 --> 00:40:28,400
performance and you usually especially

1167
00:40:28,400 --> 00:40:29,440
in something like q learning

1168
00:40:29,440 --> 00:40:31,280
like you just hit the max over the q

1169
00:40:31,280 --> 00:40:32,640
function or you have a policy

1170
00:40:32,640 --> 00:40:34,800
that tries to do like some search over

1171
00:40:34,800 --> 00:40:35,680
the q function

1172
00:40:35,680 --> 00:40:39,440
and takes that max like given a state um

1173
00:40:39,440 --> 00:40:40,720
but there's kind of an artificial

1174
00:40:40,720 --> 00:40:42,480
distinction which is

1175
00:40:42,480 --> 00:40:44,960
like obviously as you're acquiring data

1176
00:40:44,960 --> 00:40:45,920
you're making mistakes in the

1177
00:40:45,920 --> 00:40:46,960
environment you're interacting with the

1178
00:40:46,960 --> 00:40:48,640
real environment so in order to kind of

1179
00:40:48,640 --> 00:40:49,280
make that

1180
00:40:49,280 --> 00:40:51,280
fair comparison between here and active

1181
00:40:51,280 --> 00:40:52,960
inference where you don't really have

1182
00:40:52,960 --> 00:40:54,319
this distinction between like training

1183
00:40:54,319 --> 00:40:55,760
time and test time it's all

1184
00:40:55,760 --> 00:40:58,240
just interaction the that's the reason

1185
00:40:58,240 --> 00:40:59,599
why the q learning agent

1186
00:40:59,599 --> 00:41:01,520
especially when epsilon is fixed to say

1187
00:41:01,520 --> 00:41:03,760
0.1 never achieves the optimal policy

1188
00:41:03,760 --> 00:41:04,720
simply because

1189
00:41:04,720 --> 00:41:07,040
we're also with 0.1 probability taking a

1190
00:41:07,040 --> 00:41:08,000
random action

1191
00:41:08,000 --> 00:41:09,280
so we're not making this distinction

1192
00:41:09,280 --> 00:41:11,359
like you sometimes would with normal rl

1193
00:41:11,359 --> 00:41:11,839
between

1194
00:41:11,839 --> 00:41:13,520
train and test time we're all saying

1195
00:41:13,520 --> 00:41:14,960
train and test some are basically the

1196
00:41:14,960 --> 00:41:15,520
same thing

1197
00:41:15,520 --> 00:41:17,520
so however you're choosing to interact

1198
00:41:17,520 --> 00:41:19,280
with the world is how you should be

1199
00:41:19,280 --> 00:41:20,000
assessed

1200
00:41:20,000 --> 00:41:22,319
so that's that's why initially looking

1201
00:41:22,319 --> 00:41:23,359
at these girls you might be like hang on

1202
00:41:23,359 --> 00:41:24,880
like why isn't q learning solving this

1203
00:41:24,880 --> 00:41:25,359
but um

1204
00:41:25,359 --> 00:41:27,599
yeah that's just to clear something up

1205
00:41:27,599 --> 00:41:28,800
if you're more familiar with say some of

1206
00:41:28,800 --> 00:41:30,400
the like dprl

1207
00:41:30,400 --> 00:41:34,640
uh experimental procedure

1208
00:41:34,640 --> 00:41:38,640
perfect thank you okay

1209
00:41:38,640 --> 00:41:41,200
um and then um just following on from

1210
00:41:41,200 --> 00:41:41,920
that so we

1211
00:41:41,920 --> 00:41:43,520
changed the environment a little bit to

1212
00:41:43,520 --> 00:41:45,200
make it a little bit more difficult to

1213
00:41:45,200 --> 00:41:46,480
see whether

1214
00:41:46,480 --> 00:41:48,480
um the bayesian and the active inference

1215
00:41:48,480 --> 00:41:49,839
agents might struggle

1216
00:41:49,839 --> 00:41:52,240
when we start having the relocation

1217
00:41:52,240 --> 00:41:53,680
change after every few

1218
00:41:53,680 --> 00:41:56,400
episodes so specifically we swapped the

1219
00:41:56,400 --> 00:41:58,079
goal and the whole location

1220
00:41:58,079 --> 00:42:01,119
at um at time point 21

1221
00:42:01,119 --> 00:42:04,319
at time 121 141

1222
00:42:04,319 --> 00:42:08,319
251 and 451 so you can see that

1223
00:42:08,319 --> 00:42:10,319
in these figures where the line the gray

1224
00:42:10,319 --> 00:42:12,000
lines are shown so these points

1225
00:42:12,000 --> 00:42:15,200
they're all location flipped so like the

1226
00:42:15,200 --> 00:42:16,720
stationary setting for the first

1227
00:42:16,720 --> 00:42:19,440
20 trials all the asians seem to be

1228
00:42:19,440 --> 00:42:20,000
doing

1229
00:42:20,000 --> 00:42:22,480
as you would expect so both the bayesian

1230
00:42:22,480 --> 00:42:24,240
rl and the active inference agents are

1231
00:42:24,240 --> 00:42:24,640
doing

1232
00:42:24,640 --> 00:42:27,760
fairly okay uh the q landing with

1233
00:42:27,760 --> 00:42:30,800
um the fixed exploration sector 0.1 is

1234
00:42:30,800 --> 00:42:32,000
doing fairly okay

1235
00:42:32,000 --> 00:42:34,960
as we saw before and the acute learning

1236
00:42:34,960 --> 00:42:35,680
with the

1237
00:42:35,680 --> 00:42:38,400
decaying exploration is is doing as it

1238
00:42:38,400 --> 00:42:39,520
was doing before

1239
00:42:39,520 --> 00:42:42,640
but when you flip it around um such that

1240
00:42:42,640 --> 00:42:44,240
the goal locations

1241
00:42:44,240 --> 00:42:47,520
change um what you notice is that with

1242
00:42:47,520 --> 00:42:48,640
the

1243
00:42:48,640 --> 00:42:52,079
bayesian rl agent the the amount of

1244
00:42:52,079 --> 00:42:53,760
reward or the score it gets

1245
00:42:53,760 --> 00:42:56,800
is quite low and then we we see a phase

1246
00:42:56,800 --> 00:42:57,119
where

1247
00:42:57,119 --> 00:43:00,000
it then transitions and becomes um to

1248
00:43:00,000 --> 00:43:02,160
the optimal policy again

1249
00:43:02,160 --> 00:43:04,560
in comparison to the active inference

1250
00:43:04,560 --> 00:43:05,599
agent where

1251
00:43:05,599 --> 00:43:08,720
it instantly after the first trial of

1252
00:43:08,720 --> 00:43:10,560
doing the incorrect is able to switch

1253
00:43:10,560 --> 00:43:12,960
over to the active and sorry the

1254
00:43:12,960 --> 00:43:14,640
appropriate policy

1255
00:43:14,640 --> 00:43:17,680
and the reason for that is for these uh

1256
00:43:17,680 --> 00:43:19,440
rl settings where we're considering it

1257
00:43:19,440 --> 00:43:20,880
as a learning problem

1258
00:43:20,880 --> 00:43:22,480
you need to first do reverse and

1259
00:43:22,480 --> 00:43:24,480
learning of where the reward location

1260
00:43:24,480 --> 00:43:26,720
is and then we learn the new reward

1261
00:43:26,720 --> 00:43:28,720
location so you're seeing that for

1262
00:43:28,720 --> 00:43:31,280
q learning uh for both the different

1263
00:43:31,280 --> 00:43:33,200
epsilon greedy parameterizations and

1264
00:43:33,200 --> 00:43:34,720
also for the bayesian rl and we're

1265
00:43:34,720 --> 00:43:36,160
seeing that consistently

1266
00:43:36,160 --> 00:43:38,160
whereas for the active inference agent

1267
00:43:38,160 --> 00:43:39,599
because we're treating it as a planning

1268
00:43:39,599 --> 00:43:41,599
as an inference problem where the

1269
00:43:41,599 --> 00:43:42,960
posteriors from the previous state are

1270
00:43:42,960 --> 00:43:45,359
moved over to the prize and moved over

1271
00:43:45,359 --> 00:43:46,160
as priors

1272
00:43:46,160 --> 00:43:49,200
and the agent is able to instantly

1273
00:43:49,200 --> 00:43:52,160
um realize that the current policy that

1274
00:43:52,160 --> 00:43:52,960
was following

1275
00:43:52,960 --> 00:43:55,200
the this time step is an appropriateness

1276
00:43:55,200 --> 00:43:57,040
which is its policy to the next

1277
00:43:57,040 --> 00:44:00,480
um to the the other one again

1278
00:44:00,480 --> 00:44:02,480
um the null model as expected doesn't

1279
00:44:02,480 --> 00:44:04,000
really do much because it's just

1280
00:44:04,000 --> 00:44:05,599
exploring doesn't really care

1281
00:44:05,599 --> 00:44:08,160
where the reward or the whole location

1282
00:44:08,160 --> 00:44:09,839
are

1283
00:44:09,839 --> 00:44:11,839
okay um phil do you want to add

1284
00:44:11,839 --> 00:44:14,799
something to this or

1285
00:44:15,440 --> 00:44:17,440
no i think the same point applies to

1286
00:44:17,440 --> 00:44:18,480
this one

1287
00:44:18,480 --> 00:44:22,160
yep definitely okay um

1288
00:44:22,160 --> 00:44:23,760
so just to wrap up with these two

1289
00:44:23,760 --> 00:44:25,760
comparisons and

1290
00:44:25,760 --> 00:44:29,440
i with the non welder stationary setting

1291
00:44:29,440 --> 00:44:32,319
um all types of agents um including the

1292
00:44:32,319 --> 00:44:34,160
bayesian rl and the q-learning would be

1293
00:44:34,160 --> 00:44:36,400
reasonable frameworks to use whereas

1294
00:44:36,400 --> 00:44:39,280
with the uh with a non-stationary

1295
00:44:39,280 --> 00:44:40,480
stochastic setting

1296
00:44:40,480 --> 00:44:42,240
having an active inference agent might

1297
00:44:42,240 --> 00:44:44,079
be appropriate way of handling changing

1298
00:44:44,079 --> 00:44:45,200
dynamics

1299
00:44:45,200 --> 00:44:47,359
um but the key caveat with that is that

1300
00:44:47,359 --> 00:44:48,480
you can

1301
00:44:48,480 --> 00:44:50,480
introduce a lot more additional

1302
00:44:50,480 --> 00:44:53,119
complexity into the bayesian rl or the q

1303
00:44:53,119 --> 00:44:55,599
learning or the rl framework in general

1304
00:44:55,599 --> 00:44:56,560
to allow

1305
00:44:56,560 --> 00:44:58,800
uh for a way to handle uncertainty but

1306
00:44:58,800 --> 00:44:59,760
it wouldn't be

1307
00:44:59,760 --> 00:45:02,240
a natural way of adding it you would

1308
00:45:02,240 --> 00:45:04,960
have to sort of uh augment the

1309
00:45:04,960 --> 00:45:06,880
function or the algorithm in particular

1310
00:45:06,880 --> 00:45:09,359
ways to justify it

1311
00:45:09,359 --> 00:45:12,319
um okay so so once we have done this

1312
00:45:12,319 --> 00:45:15,040
comparison we were interested in is um

1313
00:45:15,040 --> 00:45:16,960
why would you even want to use active

1314
00:45:16,960 --> 00:45:19,200
inference um

1315
00:45:19,200 --> 00:45:22,079
when um you don't have an understanding

1316
00:45:22,079 --> 00:45:22,880
about the world

1317
00:45:22,880 --> 00:45:25,440
or you don't have sort of a preference

1318
00:45:25,440 --> 00:45:27,040
over the type of things that could be

1319
00:45:27,040 --> 00:45:28,640
done because as we saw the active

1320
00:45:28,640 --> 00:45:30,240
inference model the null model is just

1321
00:45:30,240 --> 00:45:32,319
exploring it's not really doing much

1322
00:45:32,319 --> 00:45:34,720
um and that's brings us to one of the

1323
00:45:34,720 --> 00:45:36,319
initial points that i introduced

1324
00:45:36,319 --> 00:45:38,160
at the start of the presentation which

1325
00:45:38,160 --> 00:45:39,599
is that within an active influence

1326
00:45:39,599 --> 00:45:41,200
framework we don't really care about

1327
00:45:41,200 --> 00:45:42,560
having a reward function

1328
00:45:42,560 --> 00:45:45,920
we can learn we can learn that based on

1329
00:45:45,920 --> 00:45:48,640
some interaction with the environment

1330
00:45:48,640 --> 00:45:52,240
um so for that um

1331
00:45:52,240 --> 00:45:54,480
what we did was we carried out a few

1332
00:45:54,480 --> 00:45:56,160
different simulations to see how the

1333
00:45:56,160 --> 00:45:57,920
active inference agent can select

1334
00:45:57,920 --> 00:45:59,520
different types of policies in the

1335
00:45:59,520 --> 00:46:02,079
absence of prior preferences

1336
00:46:02,079 --> 00:46:04,560
and for this we we did three different

1337
00:46:04,560 --> 00:46:05,280
experiments

1338
00:46:05,280 --> 00:46:08,160
um where we allowed either or the

1339
00:46:08,160 --> 00:46:09,040
likelihood

1340
00:46:09,040 --> 00:46:11,359
and all the outcome preferences to be

1341
00:46:11,359 --> 00:46:12,240
learned

1342
00:46:12,240 --> 00:46:15,440
over time and saw how the agent

1343
00:46:15,440 --> 00:46:16,560
interacted when

1344
00:46:16,560 --> 00:46:19,680
this learning over the different um

1345
00:46:19,680 --> 00:46:22,640
preferences uh took place and the the

1346
00:46:22,640 --> 00:46:24,160
way we did it was using

1347
00:46:24,160 --> 00:46:26,960
uh the conjugacy models where uh because

1348
00:46:26,960 --> 00:46:28,800
these are discrete state models we have

1349
00:46:28,800 --> 00:46:30,800
categorical distributions over our model

1350
00:46:30,800 --> 00:46:31,839
parameters

1351
00:46:31,839 --> 00:46:33,760
and we're introducing directional

1352
00:46:33,760 --> 00:46:34,880
distributions on top

1353
00:46:34,880 --> 00:46:36,960
as a hybrid price and learning those

1354
00:46:36,960 --> 00:46:38,319
hyper price

1355
00:46:38,319 --> 00:46:41,119
um we can go into the details of this if

1356
00:46:41,119 --> 00:46:42,800
anyone has any questions but

1357
00:46:42,800 --> 00:46:46,400
just for simplicity um just assume that

1358
00:46:46,400 --> 00:46:48,800
all the dirichlet distributions uh were

1359
00:46:48,800 --> 00:46:50,880
set to flat completely flat and

1360
00:46:50,880 --> 00:46:54,079
the agents were all given an opportunity

1361
00:46:54,079 --> 00:46:54,800
to land

1362
00:46:54,800 --> 00:46:57,359
based on different interactions they had

1363
00:46:57,359 --> 00:46:58,400
so the first set of

1364
00:46:58,400 --> 00:47:00,160
experiments like the simulations that we

1365
00:47:00,160 --> 00:47:02,319
ran were to understand how

1366
00:47:02,319 --> 00:47:04,640
um the agent would reduces uncertainty

1367
00:47:04,640 --> 00:47:05,520
about the

1368
00:47:05,520 --> 00:47:07,280
environment that is interacting with so

1369
00:47:07,280 --> 00:47:09,040
if it doesn't know the

1370
00:47:09,040 --> 00:47:12,319
the the the frozen lake

1371
00:47:12,319 --> 00:47:14,720
how would it interact or explore that

1372
00:47:14,720 --> 00:47:15,599
lake

1373
00:47:15,599 --> 00:47:18,880
um and what we see is that um

1374
00:47:18,880 --> 00:47:21,920
in this particular instance the agent um

1375
00:47:21,920 --> 00:47:23,680
was just interested in exploring it

1376
00:47:23,680 --> 00:47:25,920
didn't particularly care about

1377
00:47:25,920 --> 00:47:28,559
the where the goal or the whole location

1378
00:47:28,559 --> 00:47:29,040
was

1379
00:47:29,040 --> 00:47:32,079
um and this is highlighted in

1380
00:47:32,079 --> 00:47:35,119
a series of different exploration um

1381
00:47:35,119 --> 00:47:36,800
trajectories that we see so the first

1382
00:47:36,800 --> 00:47:38,960
one was where the agent falls in the

1383
00:47:38,960 --> 00:47:39,359
hall

1384
00:47:39,359 --> 00:47:42,400
um and then another one where it goes

1385
00:47:42,400 --> 00:47:43,359
around

1386
00:47:43,359 --> 00:47:46,079
and ends up at the second in the second

1387
00:47:46,079 --> 00:47:48,160
episode at the goal location

1388
00:47:48,160 --> 00:47:50,079
and in somewhere it just ends the

1389
00:47:50,079 --> 00:47:51,200
episode by

1390
00:47:51,200 --> 00:47:52,960
just going back and forth so this is

1391
00:47:52,960 --> 00:47:54,800
just floor exploration

1392
00:47:54,800 --> 00:47:58,000
nothing more to hear um

1393
00:47:58,000 --> 00:48:01,440
the next um set of analysis or

1394
00:48:01,440 --> 00:48:03,520
simulations that we ran was to see what

1395
00:48:03,520 --> 00:48:05,440
would happen if the agent knew about the

1396
00:48:05,440 --> 00:48:07,599
world but had no preferences of the type

1397
00:48:07,599 --> 00:48:08,480
of outcomes they

1398
00:48:08,480 --> 00:48:12,160
expected itself to be in and for this um

1399
00:48:12,160 --> 00:48:15,920
we ran multiple different simulations

1400
00:48:15,920 --> 00:48:19,200
and saw that in the absence of

1401
00:48:19,200 --> 00:48:22,240
um any sort of preferences

1402
00:48:22,240 --> 00:48:24,079
um the whole could actually become

1403
00:48:24,079 --> 00:48:25,599
really attractive

1404
00:48:25,599 --> 00:48:28,480
if it's encountered first so we see that

1405
00:48:28,480 --> 00:48:29,440
in the

1406
00:48:29,440 --> 00:48:31,680
first figure where the agent learns to

1407
00:48:31,680 --> 00:48:34,000
prefer to hide in halls

1408
00:48:34,000 --> 00:48:36,160
and in the second type of trial that we

1409
00:48:36,160 --> 00:48:38,000
saw was where the agent

1410
00:48:38,000 --> 00:48:41,520
uh exhibited the preference to actually

1411
00:48:41,520 --> 00:48:43,359
go to the goal location so this is

1412
00:48:43,359 --> 00:48:45,200
entirely dependent on the

1413
00:48:45,200 --> 00:48:47,680
instantiation or the type of stimulus

1414
00:48:47,680 --> 00:48:49,119
that the agent is exposed to

1415
00:48:49,119 --> 00:48:51,119
initially that determines the type of

1416
00:48:51,119 --> 00:48:52,319
preferences it would

1417
00:48:52,319 --> 00:48:55,200
learn to have

1418
00:48:56,240 --> 00:48:59,520
okay and then the last set of

1419
00:48:59,520 --> 00:49:01,520
simulations that we ran was just to

1420
00:49:01,520 --> 00:49:03,119
check what would happen when we

1421
00:49:03,119 --> 00:49:04,640
interacted with the

1422
00:49:04,640 --> 00:49:06,640
epistemic imperatives actually resolve

1423
00:49:06,640 --> 00:49:08,000
uncertainty about the

1424
00:49:08,000 --> 00:49:09,440
environment that the agent was

1425
00:49:09,440 --> 00:49:11,200
interacting with specifically the

1426
00:49:11,200 --> 00:49:13,200
likelihood mapping between the outcomes

1427
00:49:13,200 --> 00:49:14,480
given the states and

1428
00:49:14,480 --> 00:49:16,880
the uncertainty about the desired state

1429
00:49:16,880 --> 00:49:18,559
of affairs that the agent expected

1430
00:49:18,559 --> 00:49:19,839
herself to be in

1431
00:49:19,839 --> 00:49:22,960
and what we saw was that um if we

1432
00:49:22,960 --> 00:49:25,520
allowed um a sufficient number of trials

1433
00:49:25,520 --> 00:49:27,040
to pass the agent

1434
00:49:27,040 --> 00:49:28,640
in this particular setting learn to

1435
00:49:28,640 --> 00:49:31,119
prefer to ride sorry hiding holes

1436
00:49:31,119 --> 00:49:34,800
after x number of episodes but it had a

1437
00:49:34,800 --> 00:49:36,000
very distinct

1438
00:49:36,000 --> 00:49:38,960
um preference over the type of outcomes

1439
00:49:38,960 --> 00:49:39,280
it

1440
00:49:39,280 --> 00:49:41,119
expected itself to be in depending on

1441
00:49:41,119 --> 00:49:42,480
the particular

1442
00:49:42,480 --> 00:49:46,400
uh a sorry particular time point

1443
00:49:46,400 --> 00:49:50,079
um so that brings me to the last

1444
00:49:50,079 --> 00:49:53,839
simulation so i'm just going to wrap up

1445
00:49:53,839 --> 00:49:57,200
which is um in active inference

1446
00:49:57,200 --> 00:49:59,040
i started the presentation was saying

1447
00:49:59,040 --> 00:50:00,640
it's a particular algorithm that gives

1448
00:50:00,640 --> 00:50:01,200
us

1449
00:50:01,200 --> 00:50:03,599
very nice um things to consider when

1450
00:50:03,599 --> 00:50:04,559
we're

1451
00:50:04,559 --> 00:50:07,760
operating in a bayesian or belief based

1452
00:50:07,760 --> 00:50:08,960
setting which is

1453
00:50:08,960 --> 00:50:11,680
firstly the principal account of

1454
00:50:11,680 --> 00:50:13,359
epistemic exploration and intrinsic

1455
00:50:13,359 --> 00:50:15,119
motivation that we get from the

1456
00:50:15,119 --> 00:50:17,040
particular expected free energy

1457
00:50:17,040 --> 00:50:19,440
decomposition that we went through the

1458
00:50:19,440 --> 00:50:20,400
second thing

1459
00:50:20,400 --> 00:50:23,359
that i wanted to highlight was that um

1460
00:50:23,359 --> 00:50:25,040
under an active inference setting we

1461
00:50:25,040 --> 00:50:26,000
don't have to explicitly

1462
00:50:26,000 --> 00:50:28,240
specify a reward function which we saw

1463
00:50:28,240 --> 00:50:29,119
in the last

1464
00:50:29,119 --> 00:50:32,079
uh the second set of simulations where

1465
00:50:32,079 --> 00:50:34,000
the agent can also learn its own reward

1466
00:50:34,000 --> 00:50:35,680
and prefer to become something that is

1467
00:50:35,680 --> 00:50:37,359
quite counter-intuitive

1468
00:50:37,359 --> 00:50:39,839
from an rl setting where the signal from

1469
00:50:39,839 --> 00:50:41,440
the environment is saying something is

1470
00:50:41,440 --> 00:50:42,319
bad but the

1471
00:50:42,319 --> 00:50:44,160
agent's internal motivations of

1472
00:50:44,160 --> 00:50:46,240
preference allow it to then

1473
00:50:46,240 --> 00:50:48,559
do something that's quite uh at odds

1474
00:50:48,559 --> 00:50:50,319
with what the environment is expecting

1475
00:50:50,319 --> 00:50:51,359
it to do

1476
00:50:51,359 --> 00:50:53,599
and lastly because of this belief um

1477
00:50:53,599 --> 00:50:55,280
invasion setting uncertainty is a

1478
00:50:55,280 --> 00:50:57,119
natural part of the belief

1479
00:50:57,119 --> 00:51:00,559
updating um so within the

1480
00:51:00,559 --> 00:51:02,319
stationary settings active inference

1481
00:51:02,319 --> 00:51:04,000
agents perform as well as reinforcement

1482
00:51:04,000 --> 00:51:05,200
learning agents

1483
00:51:05,200 --> 00:51:07,359
however in non-stationary settings they

1484
00:51:07,359 --> 00:51:08,960
outperformed due to their ability to

1485
00:51:08,960 --> 00:51:10,480
carry out this planning as

1486
00:51:10,480 --> 00:51:13,280
inference and that's i think that holds

1487
00:51:13,280 --> 00:51:15,280
um this is not a conclusive statement

1488
00:51:15,280 --> 00:51:15,760
but

1489
00:51:15,760 --> 00:51:18,240
it's a nice way to start thinking about

1490
00:51:18,240 --> 00:51:19,040
how

1491
00:51:19,040 --> 00:51:21,280
um if you're scaling up active inference

1492
00:51:21,280 --> 00:51:23,440
agents to interact on the same type of

1493
00:51:23,440 --> 00:51:24,160
environments

1494
00:51:24,160 --> 00:51:26,960
as reinforcement learning agents that

1495
00:51:26,960 --> 00:51:27,760
might be

1496
00:51:27,760 --> 00:51:30,079
a little bit hard to resolve because

1497
00:51:30,079 --> 00:51:30,839
there is some

1498
00:51:30,839 --> 00:51:32,640
non-stationarity or some weird

1499
00:51:32,640 --> 00:51:33,920
fluctuations happening in the

1500
00:51:33,920 --> 00:51:34,640
environment

1501
00:51:34,640 --> 00:51:36,240
and active inference agents could

1502
00:51:36,240 --> 00:51:38,240
potentially perform quite nicely if we

1503
00:51:38,240 --> 00:51:42,000
have this planning as inference set up

1504
00:51:42,079 --> 00:51:44,800
um and that brings me to the end of the

1505
00:51:44,800 --> 00:51:46,480
presentation so i just want to thank

1506
00:51:46,480 --> 00:51:48,240
everyone who was involved with this work

1507
00:51:48,240 --> 00:51:48,559
so

1508
00:51:48,559 --> 00:51:51,520
called tom and phil and everyone who's

1509
00:51:51,520 --> 00:51:52,640
helped me think through these

1510
00:51:52,640 --> 00:51:55,920
interesting ideas that i presented

1511
00:51:55,920 --> 00:51:58,079
so and everyone also for listening thank

1512
00:51:58,079 --> 00:51:59,920
you

1513
00:51:59,920 --> 00:52:04,400
thank you awesome talk

1514
00:52:04,400 --> 00:52:07,119
so you can maybe unshare and we can ask

1515
00:52:07,119 --> 00:52:08,000
a few questions

1516
00:52:08,000 --> 00:52:09,599
and if anyone who's watching live would

1517
00:52:09,599 --> 00:52:11,359
like to ask questions they're

1518
00:52:11,359 --> 00:52:14,400
more than welcome so maybe i'll start

1519
00:52:14,400 --> 00:52:16,160
with just um

1520
00:52:16,160 --> 00:52:18,000
a general point that it was awesome to

1521
00:52:18,000 --> 00:52:20,480
see how clearly you differentiated

1522
00:52:20,480 --> 00:52:21,280
between

1523
00:52:21,280 --> 00:52:22,880
the reinforcement learning and the

1524
00:52:22,880 --> 00:52:24,640
active inference paradigm

1525
00:52:24,640 --> 00:52:27,040
and just one question i had was you

1526
00:52:27,040 --> 00:52:27,760
mentioned that

1527
00:52:27,760 --> 00:52:30,319
in the case of a time step of one a time

1528
00:52:30,319 --> 00:52:31,599
horizon of one

1529
00:52:31,599 --> 00:52:33,359
there was like an equivalence between

1530
00:52:33,359 --> 00:52:34,960
the reinforcement learning approach and

1531
00:52:34,960 --> 00:52:36,640
the active inference approach

1532
00:52:36,640 --> 00:52:38,640
now people are using reinforcement

1533
00:52:38,640 --> 00:52:40,400
learning to do

1534
00:52:40,400 --> 00:52:42,400
planning for the future amidst

1535
00:52:42,400 --> 00:52:43,599
uncertainty

1536
00:52:43,599 --> 00:52:46,240
so how are they accomplishing those

1537
00:52:46,240 --> 00:52:47,839
kinds of planning

1538
00:52:47,839 --> 00:52:50,400
and where are some situations where

1539
00:52:50,400 --> 00:52:51,440
active inference

1540
00:52:51,440 --> 00:52:53,920
might be able to step into those um

1541
00:52:53,920 --> 00:52:54,800
settings and

1542
00:52:54,800 --> 00:52:57,760
potentially do better i think that's

1543
00:52:57,760 --> 00:52:58,480
already a question

1544
00:52:58,480 --> 00:53:00,400
a good question and feel free for you to

1545
00:53:00,400 --> 00:53:02,720
jump in if i miss something but um

1546
00:53:02,720 --> 00:53:04,720
so within the url setting from what i

1547
00:53:04,720 --> 00:53:07,040
know if they are considering

1548
00:53:07,040 --> 00:53:09,280
temporal horizons greater than one then

1549
00:53:09,280 --> 00:53:10,960
they have hierarchical rl

1550
00:53:10,960 --> 00:53:13,280
or they have options where they're

1551
00:53:13,280 --> 00:53:14,800
considering trajectories that are

1552
00:53:14,800 --> 00:53:16,240
greater than

1553
00:53:16,240 --> 00:53:18,800
one that allow them to consider a whole

1554
00:53:18,800 --> 00:53:20,720
sequence of play

1555
00:53:20,720 --> 00:53:24,319
um before they like um that's rolled out

1556
00:53:24,319 --> 00:53:26,000
if they select a particular policy

1557
00:53:26,000 --> 00:53:27,680
instead of like that actually

1558
00:53:27,680 --> 00:53:32,800
one single action to state mapping um

1559
00:53:32,800 --> 00:53:34,880
but i i'm not i haven't really worked

1560
00:53:34,880 --> 00:53:36,559
with options that much maybe

1561
00:53:36,559 --> 00:53:39,119
phil do you know um yeah so options are

1562
00:53:39,119 --> 00:53:41,280
like one way to

1563
00:53:41,280 --> 00:53:45,119
achieve this kind of multi-step sort of

1564
00:53:45,119 --> 00:53:47,680
um like i guess oversized almost like

1565
00:53:47,680 --> 00:53:49,359
this inductive bias right that

1566
00:53:49,359 --> 00:53:52,000
you will like your choices will be kind

1567
00:53:52,000 --> 00:53:54,079
of contiguous blocks of like

1568
00:53:54,079 --> 00:53:55,520
more than one step whereas a kind of

1569
00:53:55,520 --> 00:53:57,280
policy kind of it's just like i'm going

1570
00:53:57,280 --> 00:53:58,720
to take one step and then i'll have this

1571
00:53:58,720 --> 00:53:59,359
kind of

1572
00:53:59,359 --> 00:54:02,720
next state i exist in um but i think

1573
00:54:02,720 --> 00:54:04,240
what's important to disambiguate is

1574
00:54:04,240 --> 00:54:06,000
actually you can do sort of planning the

1575
00:54:06,000 --> 00:54:07,440
model style things because

1576
00:54:07,440 --> 00:54:09,119
you can view active inference as a sort

1577
00:54:09,119 --> 00:54:10,880
of model based like

1578
00:54:10,880 --> 00:54:13,680
in the kind of you know rl paradigm and

1579
00:54:13,680 --> 00:54:15,280
indeed like within model based

1580
00:54:15,280 --> 00:54:18,400
like you can have

1581
00:54:18,400 --> 00:54:20,800
um like planning methods similar to what

1582
00:54:20,800 --> 00:54:21,440
is done

1583
00:54:21,440 --> 00:54:23,040
in active inference however i guess like

1584
00:54:23,040 --> 00:54:24,480
the key distinction here is

1585
00:54:24,480 --> 00:54:27,440
when you do planning in model based rl

1586
00:54:27,440 --> 00:54:27,760
you're

1587
00:54:27,760 --> 00:54:29,920
simply just trying to take you tend to

1588
00:54:29,920 --> 00:54:30,800
take some sort of

1589
00:54:30,800 --> 00:54:32,880
suit like kind of mini evolutionary

1590
00:54:32,880 --> 00:54:34,079
method usually something like

1591
00:54:34,079 --> 00:54:35,920
the cross entropy method and what you're

1592
00:54:35,920 --> 00:54:37,440
doing is this kind of search over all

1593
00:54:37,440 --> 00:54:38,640
actions that you take

1594
00:54:38,640 --> 00:54:40,480
at a certain point in time that

1595
00:54:40,480 --> 00:54:42,319
maximizes your kind of reward

1596
00:54:42,319 --> 00:54:45,200
over say a horizon of say 20 steps um

1597
00:54:45,200 --> 00:54:46,559
and you can do some clever stuff you can

1598
00:54:46,559 --> 00:54:48,880
terminate with a q value function

1599
00:54:48,880 --> 00:54:51,280
um but fundamentally like it doesn't

1600
00:54:51,280 --> 00:54:52,799
take you away from this sort of

1601
00:54:52,799 --> 00:54:55,359
definitional difference which is

1602
00:54:55,359 --> 00:54:57,760
in active inference you're doing this

1603
00:54:57,760 --> 00:54:59,119
sort of planning over actions even

1604
00:54:59,119 --> 00:55:00,480
though they might seem superficially

1605
00:55:00,480 --> 00:55:02,000
similar like kind of

1606
00:55:02,000 --> 00:55:03,680
basically like drawing lots of actions

1607
00:55:03,680 --> 00:55:05,280
and seeing which ones maximize some sort

1608
00:55:05,280 --> 00:55:06,000
of utility

1609
00:55:06,000 --> 00:55:09,119
in the case of active inference all the

1610
00:55:09,119 --> 00:55:10,880
useful kind of information and the

1611
00:55:10,880 --> 00:55:12,559
desired rata about um

1612
00:55:12,559 --> 00:55:15,760
like exploration and it's and um

1613
00:55:15,760 --> 00:55:17,839
exploitation are wrapped up in that

1614
00:55:17,839 --> 00:55:20,000
maximization whereas in rl it's kind of

1615
00:55:20,000 --> 00:55:23,440
it's a bit harder to understand um like

1616
00:55:23,440 --> 00:55:24,799
in the classical sense we're just trying

1617
00:55:24,799 --> 00:55:26,960
to maximize reward but um

1618
00:55:26,960 --> 00:55:28,720
you can have heuristics where you say oh

1619
00:55:28,720 --> 00:55:30,240
but maybe i want to also maximize some

1620
00:55:30,240 --> 00:55:31,200
notion of

1621
00:55:31,200 --> 00:55:33,359
model uncertainty and you know it kind

1622
00:55:33,359 --> 00:55:34,640
of gets a bit

1623
00:55:34,640 --> 00:55:37,280
um more difficult to naturally integrate

1624
00:55:37,280 --> 00:55:39,680
all these approaches into the same thing

1625
00:55:39,680 --> 00:55:41,680
um because like in the example of active

1626
00:55:41,680 --> 00:55:42,400
influence because everything's a

1627
00:55:42,400 --> 00:55:43,280
distribution

1628
00:55:43,280 --> 00:55:44,720
you just put a hyper prior over if you

1629
00:55:44,720 --> 00:55:46,480
want to you just integrate it out if you

1630
00:55:46,480 --> 00:55:47,760
don't want to deal with

1631
00:55:47,760 --> 00:55:49,920
kind of tuning it so yeah that's kind of

1632
00:55:49,920 --> 00:55:50,880
where i feel like

1633
00:55:50,880 --> 00:55:53,920
there is this kind of difference

1634
00:55:54,000 --> 00:55:57,280
cool and what what kinds of settings

1635
00:55:57,280 --> 00:56:00,640
do you think that those types of of uh

1636
00:56:00,640 --> 00:56:03,200
action as inference and planning as

1637
00:56:03,200 --> 00:56:04,240
inference could be

1638
00:56:04,240 --> 00:56:06,960
utilized like what kind of data sets or

1639
00:56:06,960 --> 00:56:08,880
questions or contexts

1640
00:56:08,880 --> 00:56:10,799
are people currently using another type

1641
00:56:10,799 --> 00:56:12,720
of method but then you're excited to see

1642
00:56:12,720 --> 00:56:17,839
active inference play a role

1643
00:56:18,720 --> 00:56:21,280
i think mostly open-ended problems where

1644
00:56:21,280 --> 00:56:23,520
you don't really have a reward function

1645
00:56:23,520 --> 00:56:26,640
i think because

1646
00:56:26,640 --> 00:56:30,079
the way the rl agent is learning how to

1647
00:56:30,079 --> 00:56:31,839
interact with the environment is through

1648
00:56:31,839 --> 00:56:33,839
a reward function

1649
00:56:33,839 --> 00:56:36,079
um so anything where you don't really

1650
00:56:36,079 --> 00:56:37,280
have that or you have

1651
00:56:37,280 --> 00:56:40,640
um an environment that that is changing

1652
00:56:40,640 --> 00:56:43,440
um but i know that there's like a whole

1653
00:56:43,440 --> 00:56:44,160
host of

1654
00:56:44,160 --> 00:56:46,720
uh people within the rl community

1655
00:56:46,720 --> 00:56:47,599
working on like

1656
00:56:47,599 --> 00:56:50,160
intrinsic motivation or internal

1657
00:56:50,160 --> 00:56:51,520
motivation

1658
00:56:51,520 --> 00:56:54,160
so those sort of things do overlap with

1659
00:56:54,160 --> 00:56:54,559
the

1660
00:56:54,559 --> 00:56:58,960
active influence formulation um

1661
00:56:58,960 --> 00:57:02,400
but particular paradigms i think for me

1662
00:57:02,400 --> 00:57:04,640
the more interesting aspect of active

1663
00:57:04,640 --> 00:57:06,079
influence comes from the fact

1664
00:57:06,079 --> 00:57:09,440
when you start thinking about um

1665
00:57:09,440 --> 00:57:12,240
biological agents and if you're modeling

1666
00:57:12,240 --> 00:57:13,280
a patient

1667
00:57:13,280 --> 00:57:16,240
or someone with schizophrenia you can

1668
00:57:16,240 --> 00:57:18,079
with this bayesian framework you can

1669
00:57:18,079 --> 00:57:21,760
change um the priors to try and see

1670
00:57:21,760 --> 00:57:24,960
how the person is interacting

1671
00:57:24,960 --> 00:57:27,920
where i guess it's it could be that

1672
00:57:27,920 --> 00:57:28,799
their policy

1673
00:57:28,799 --> 00:57:30,079
like the way they're valuing their

1674
00:57:30,079 --> 00:57:32,319
policies are broken or it just could be

1675
00:57:32,319 --> 00:57:35,599
other parts that are different but

1676
00:57:35,599 --> 00:57:38,720
i think within the rl setting if you

1677
00:57:38,720 --> 00:57:43,440
are stepping outside the um

1678
00:57:43,440 --> 00:57:46,480
the standard the game mode right so like

1679
00:57:46,480 --> 00:57:47,359
magic or

1680
00:57:47,359 --> 00:57:49,040
gym environments and start going into

1681
00:57:49,040 --> 00:57:50,799
ones which are more open-ended and you

1682
00:57:50,799 --> 00:57:52,480
don't have any rewards then

1683
00:57:52,480 --> 00:57:54,160
active influence could be potentially

1684
00:57:54,160 --> 00:57:56,240
useful but

1685
00:57:56,240 --> 00:57:58,640
i i'm a little bit hesitant to say

1686
00:57:58,640 --> 00:57:59,599
whether it will be

1687
00:57:59,599 --> 00:58:02,640
better because if you start

1688
00:58:02,640 --> 00:58:05,040
augmenting bayesian rl with all sorts of

1689
00:58:05,040 --> 00:58:06,839
interesting components

1690
00:58:06,839 --> 00:58:10,160
then to a certain extent it will be

1691
00:58:10,160 --> 00:58:13,680
active in front scaled up um

1692
00:58:13,680 --> 00:58:15,760
which which is potentially a contentious

1693
00:58:15,760 --> 00:58:16,880
point

1694
00:58:16,880 --> 00:58:20,319
um but i i think it it depends on

1695
00:58:20,319 --> 00:58:22,079
exactly what you were incorporating and

1696
00:58:22,079 --> 00:58:23,920
that's why for this particular

1697
00:58:23,920 --> 00:58:26,000
presentation and our work we were quite

1698
00:58:26,000 --> 00:58:27,599
careful in defining what

1699
00:58:27,599 --> 00:58:29,599
reinforcement learning meant which is

1700
00:58:29,599 --> 00:58:31,760
that you have to have this reward

1701
00:58:31,760 --> 00:58:33,280
function in play and you want to

1702
00:58:33,280 --> 00:58:35,040
maximize this reward function

1703
00:58:35,040 --> 00:58:37,280
and anything you include into the rl

1704
00:58:37,280 --> 00:58:39,040
framework has to have

1705
00:58:39,040 --> 00:58:42,000
that uh that like the object that

1706
00:58:42,000 --> 00:58:44,000
objective has to be there but if you

1707
00:58:44,000 --> 00:58:45,839
if you sort of bypass them say okay i'm

1708
00:58:45,839 --> 00:58:47,040
just going to add in all sorts of

1709
00:58:47,040 --> 00:58:48,480
interesting components

1710
00:58:48,480 --> 00:58:51,200
to make the algorithm or the way um the

1711
00:58:51,200 --> 00:58:52,400
agent

1712
00:58:52,400 --> 00:58:55,599
interacts with the environment um

1713
00:58:55,599 --> 00:58:57,760
i i guess as similar to a active

1714
00:58:57,760 --> 00:58:59,119
inference framework

1715
00:58:59,119 --> 00:59:02,000
or even perhaps even better then you

1716
00:59:02,000 --> 00:59:03,040
don't

1717
00:59:03,040 --> 00:59:06,319
that fine distinction between where rl

1718
00:59:06,319 --> 00:59:08,400
is better or where active influence

1719
00:59:08,400 --> 00:59:10,960
is better sort of it's not really there

1720
00:59:10,960 --> 00:59:12,559
for me

1721
00:59:12,559 --> 00:59:15,680
um because i think both communities from

1722
00:59:15,680 --> 00:59:16,079
my

1723
00:59:16,079 --> 00:59:17,599
perspective are working from similar

1724
00:59:17,599 --> 00:59:19,440
things which is sequential decision

1725
00:59:19,440 --> 00:59:20,400
making

1726
00:59:20,400 --> 00:59:23,760
um and with our work is mostly

1727
00:59:23,760 --> 00:59:26,000
with uh sequential decision making in

1728
00:59:26,000 --> 00:59:27,520
the face of uncertainty

1729
00:59:27,520 --> 00:59:29,920
whereas some of our work my rl work

1730
00:59:29,920 --> 00:59:30,720
might not be

1731
00:59:30,720 --> 00:59:33,440
focused on that so i think it's when you

1732
00:59:33,440 --> 00:59:33,920
start

1733
00:59:33,920 --> 00:59:35,359
drawing the boundaries it becomes a

1734
00:59:35,359 --> 00:59:37,119
little bit hazy where like

1735
00:59:37,119 --> 00:59:40,079
things are separate or not and i think i

1736
00:59:40,079 --> 00:59:41,920
went up on a little tangent

1737
00:59:41,920 --> 00:59:45,760
um but phil do you want to add

1738
00:59:45,760 --> 00:59:47,119
anything to that in terms of the

1739
00:59:47,119 --> 00:59:49,280
environments and the paradigms that

1740
00:59:49,280 --> 00:59:49,839
might be

1741
00:59:49,839 --> 00:59:53,520
useful yeah i think if

1742
00:59:53,520 --> 00:59:55,359
one of your aims is say you're given an

1743
00:59:55,359 --> 00:59:57,440
environment that you just have no prior

1744
00:59:57,440 --> 00:59:58,640
knowledge about

1745
00:59:58,640 --> 01:00:01,599
how you ought to behave you could argue

1746
01:00:01,599 --> 01:00:03,359
that you could deploy an rl agent that

1747
01:00:03,359 --> 01:00:05,200
uses some sort of curiosity or epistemic

1748
01:00:05,200 --> 01:00:08,720
uncertainty reduction mechanism but

1749
01:00:08,720 --> 01:00:10,559
i mean i know i know there is like a

1750
01:00:10,559 --> 01:00:12,319
tiny bit of work about learning priors

1751
01:00:12,319 --> 01:00:14,160
over reward functions and learning those

1752
01:00:14,160 --> 01:00:14,640
but

1753
01:00:14,640 --> 01:00:17,920
i'm not hugely like aware but um

1754
01:00:17,920 --> 01:00:19,280
i think what's important to understand

1755
01:00:19,280 --> 01:00:21,839
is in the limit of like exploring the

1756
01:00:21,839 --> 01:00:22,880
entire environment

1757
01:00:22,880 --> 01:00:24,319
your epistemic uncertainty is going to

1758
01:00:24,319 --> 01:00:26,319
go to zero right like you um

1759
01:00:26,319 --> 01:00:27,760
you will have observed everything and

1760
01:00:27,760 --> 01:00:29,359
then it's unclear what your rl agent is

1761
01:00:29,359 --> 01:00:30,480
going to do at that point especially you

1762
01:00:30,480 --> 01:00:31,280
say you have a

1763
01:00:31,280 --> 01:00:33,680
deep neural network that parameterizes

1764
01:00:33,680 --> 01:00:35,839
um how you take actions

1765
01:00:35,839 --> 01:00:37,440
whereas in the case and i think what

1766
01:00:37,440 --> 01:00:39,280
this paper was really interesting

1767
01:00:39,280 --> 01:00:41,119
for me to see like when we ran these

1768
01:00:41,119 --> 01:00:42,480
experiments was

1769
01:00:42,480 --> 01:00:44,319
actually you just put a prior over your

1770
01:00:44,319 --> 01:00:46,079
prior preferences and eventually you

1771
01:00:46,079 --> 01:00:46,480
learn

1772
01:00:46,480 --> 01:00:48,960
a mode of behavior it may not be optimal

1773
01:00:48,960 --> 01:00:50,960
but your agent eventually

1774
01:00:50,960 --> 01:00:53,040
learns to adopt a behavior that is

1775
01:00:53,040 --> 01:00:54,480
self-fulfilling because it reduces

1776
01:00:54,480 --> 01:00:56,000
epidemic uncertainty

1777
01:00:56,000 --> 01:00:58,400
and then all that's left is says well i

1778
01:00:58,400 --> 01:00:59,520
think these are kind of

1779
01:00:59,520 --> 01:01:00,960
useful behaviors or at least these are

1780
01:01:00,960 --> 01:01:03,200
behaviors for me to do in the world and

1781
01:01:03,200 --> 01:01:04,000
eventually

1782
01:01:04,000 --> 01:01:05,680
you get quite a repetitive behavior and

1783
01:01:05,680 --> 01:01:07,119
it might be a more kind of accurate

1784
01:01:07,119 --> 01:01:08,160
simulation of

1785
01:01:08,160 --> 01:01:10,400
in the absence of any information like

1786
01:01:10,400 --> 01:01:12,079
how something intelligent might actually

1787
01:01:12,079 --> 01:01:12,559
behave

1788
01:01:12,559 --> 01:01:15,040
in a world whereas in the rl paradigm

1789
01:01:15,040 --> 01:01:16,480
it's less clear you know once you reduce

1790
01:01:16,480 --> 01:01:17,599
all that epistemic and certainly about

1791
01:01:17,599 --> 01:01:18,480
where the value is

1792
01:01:18,480 --> 01:01:20,160
and you haven't found any like what is

1793
01:01:20,160 --> 01:01:22,319
your agent really doing at that point

1794
01:01:22,319 --> 01:01:24,240
but i think my problem at the moment

1795
01:01:24,240 --> 01:01:26,079
with like the acting from

1796
01:01:26,079 --> 01:01:28,960
formulation is that this works on like a

1797
01:01:28,960 --> 01:01:31,040
discrete state formulation right we saw

1798
01:01:31,040 --> 01:01:34,640
really nice results in that setting

1799
01:01:34,640 --> 01:01:36,880
but i think if you scale it up active

1800
01:01:36,880 --> 01:01:38,240
inference agents are going to have the

1801
01:01:38,240 --> 01:01:38,640
same

1802
01:01:38,640 --> 01:01:40,559
issues if you're using like amortized

1803
01:01:40,559 --> 01:01:42,640
inference or something to actually

1804
01:01:42,640 --> 01:01:44,640
uh approximate your likelihood

1805
01:01:44,640 --> 01:01:47,359
transition functions which means that

1806
01:01:47,359 --> 01:01:49,280
you might not get these nice properties

1807
01:01:49,280 --> 01:01:51,119
that we're seeing at the

1808
01:01:51,119 --> 01:01:53,920
this small scale um so scaling it up is

1809
01:01:53,920 --> 01:01:55,200
like a

1810
01:01:55,200 --> 01:01:57,119
it's a interesting and open-ended

1811
01:01:57,119 --> 01:01:58,720
problem at the moment scaling up in the

1812
01:01:58,720 --> 01:01:59,760
right way that you can

1813
01:01:59,760 --> 01:02:01,440
include like these conjugating models

1814
01:02:01,440 --> 01:02:02,880
that we have

1815
01:02:02,880 --> 01:02:05,280
in um the discrete state formulation and

1816
01:02:05,280 --> 01:02:06,000
that's how we're

1817
01:02:06,000 --> 01:02:08,240
like do the last set of simulations

1818
01:02:08,240 --> 01:02:10,079
where we introduce the conjugate priors

1819
01:02:10,079 --> 01:02:10,880
to do the

1820
01:02:10,880 --> 01:02:13,839
learning over the the prior preferences

1821
01:02:13,839 --> 01:02:15,440
but also the likelihood

1822
01:02:15,440 --> 01:02:17,839
um so it becomes a little bit hazy how

1823
01:02:17,839 --> 01:02:18,720
to learn

1824
01:02:18,720 --> 01:02:21,680
like i have a hyper prior over an entire

1825
01:02:21,680 --> 01:02:23,280
neural network if you're scaling it up

1826
01:02:23,280 --> 01:02:24,400
that way

1827
01:02:24,400 --> 01:02:27,760
um so they must like a little work needs

1828
01:02:27,760 --> 01:02:29,039
to be done that area

1829
01:02:29,039 --> 01:02:31,520
and in order to actually show that it's

1830
01:02:31,520 --> 01:02:32,960
appropriate

1831
01:02:32,960 --> 01:02:35,680
um that you can have these interesting

1832
01:02:35,680 --> 01:02:37,520
components then you need to

1833
01:02:37,520 --> 01:02:40,079
start thinking about how you would

1834
01:02:40,079 --> 01:02:42,319
include these hyper priors

1835
01:02:42,319 --> 01:02:44,240
uh because it is not reasonable to say

1836
01:02:44,240 --> 01:02:45,920
that you're gonna have a hybrid prior

1837
01:02:45,920 --> 01:02:49,119
over the the parameter space such

1838
01:02:49,119 --> 01:02:51,760
like the the beta hyper so that hyper

1839
01:02:51,760 --> 01:02:53,359
prior over the gamma that that would be

1840
01:02:53,359 --> 01:02:55,359
sufficient in those settings

1841
01:02:55,359 --> 01:02:58,319
um hyper prior for the way the agent is

1842
01:02:58,319 --> 01:03:00,319
selecting his actions

1843
01:03:00,319 --> 01:03:02,640
it has to be over the model parameters

1844
01:03:02,640 --> 01:03:03,440
and if

1845
01:03:03,440 --> 01:03:06,559
scaling up means that you're losing that

1846
01:03:06,559 --> 01:03:08,559
nice way to disentangle the particular

1847
01:03:08,559 --> 01:03:10,400
model parameters then it becomes very

1848
01:03:10,400 --> 01:03:13,440
uncertain i

1849
01:03:13,440 --> 01:03:15,039
i don't know it's it's an open-ended

1850
01:03:15,039 --> 01:03:16,559
problem for me

1851
01:03:16,559 --> 01:03:18,640
cool yeah i think high high-dimensional

1852
01:03:18,640 --> 01:03:19,760
problems

1853
01:03:19,760 --> 01:03:21,599
still represent something that um is

1854
01:03:21,599 --> 01:03:23,119
relatively difficult especially

1855
01:03:23,119 --> 01:03:25,760
just due to the kind of you know you

1856
01:03:25,760 --> 01:03:27,359
know the further we stray from bays

1857
01:03:27,359 --> 01:03:29,680
like um the less principle it becomes

1858
01:03:29,680 --> 01:03:30,640
and um you know

1859
01:03:30,640 --> 01:03:34,960
it's like it's it's quite a fine line

1860
01:03:34,960 --> 01:03:37,520
very interesting about how whether

1861
01:03:37,520 --> 01:03:38,720
within the rl

1862
01:03:38,720 --> 01:03:40,960
or active inference paradigm there's

1863
01:03:40,960 --> 01:03:41,839
sort of the

1864
01:03:41,839 --> 01:03:44,559
sparse skeleton the bare bones at the

1865
01:03:44,559 --> 01:03:45,520
core

1866
01:03:45,520 --> 01:03:48,799
and then sometimes these other layers

1867
01:03:48,799 --> 01:03:51,839
or tweaks are needed

1868
01:03:51,839 --> 01:03:55,359
and pretty interesting to learn about

1869
01:03:55,359 --> 01:03:57,200
and also what you had said earlier

1870
01:03:57,200 --> 01:04:00,079
about how the challenge is planning for

1871
01:04:00,079 --> 01:04:01,839
sequential action

1872
01:04:01,839 --> 01:04:03,920
when you're in feedback whether just by

1873
01:04:03,920 --> 01:04:05,440
moving around an environment so your

1874
01:04:05,440 --> 01:04:06,720
local environment changes or you're

1875
01:04:06,720 --> 01:04:07,839
playing a game

1876
01:04:07,839 --> 01:04:09,440
with a board game is going to change or

1877
01:04:09,440 --> 01:04:10,880
you're trading on a market

1878
01:04:10,880 --> 01:04:13,359
sequential action you can't just plan

1879
01:04:13,359 --> 01:04:15,280
steps one through 100

1880
01:04:15,280 --> 01:04:17,440
without at least thinking about some

1881
01:04:17,440 --> 01:04:18,720
what-ifs

1882
01:04:18,720 --> 01:04:22,240
and then only having access to limited

1883
01:04:22,240 --> 01:04:24,559
observational data and planning amidst

1884
01:04:24,559 --> 01:04:25,520
fundamental

1885
01:04:25,520 --> 01:04:27,680
uncertainty so i think a lot of those

1886
01:04:27,680 --> 01:04:30,160
points of contact with the motivations

1887
01:04:30,160 --> 01:04:32,559
of reinforcement learning and machine

1888
01:04:32,559 --> 01:04:33,680
learning will

1889
01:04:33,680 --> 01:04:35,599
maybe bring some more light into active

1890
01:04:35,599 --> 01:04:37,440
inference and

1891
01:04:37,440 --> 01:04:39,920
um push some of those frontiers you just

1892
01:04:39,920 --> 01:04:40,960
mentioned

1893
01:04:40,960 --> 01:04:42,720
so i have a question and anyone else in

1894
01:04:42,720 --> 01:04:45,119
the chat can ask a question too um

1895
01:04:45,119 --> 01:04:47,680
let's say somebody is wanting to learn

1896
01:04:47,680 --> 01:04:48,160
about

1897
01:04:48,160 --> 01:04:51,280
this and they're actually in a lucky

1898
01:04:51,280 --> 01:04:52,960
beginner's mind perspective

1899
01:04:52,960 --> 01:04:56,079
because they might not have been enticed

1900
01:04:56,079 --> 01:04:57,760
by learning reinforcement learning but

1901
01:04:57,760 --> 01:04:58,960
they've gotten curious

1902
01:04:58,960 --> 01:05:02,000
in active inference and excited by your

1903
01:05:02,000 --> 01:05:03,280
presentation

1904
01:05:03,280 --> 01:05:05,599
and so what what kinds of computer

1905
01:05:05,599 --> 01:05:06,720
languages or

1906
01:05:06,720 --> 01:05:08,880
skills might they want to learn or what

1907
01:05:08,880 --> 01:05:11,119
kinds of approaches or mindsets would be

1908
01:05:11,119 --> 01:05:12,000
helpful

1909
01:05:12,000 --> 01:05:13,680
if somebody let's say weren't coming

1910
01:05:13,680 --> 01:05:15,039
from a

1911
01:05:15,039 --> 01:05:16,559
classical machine learning perspective

1912
01:05:16,559 --> 01:05:18,400
and learning active inference but rather

1913
01:05:18,400 --> 01:05:21,520
kind of upskilling into active inference

1914
01:05:21,520 --> 01:05:23,039
what would you recommend either of you

1915
01:05:23,039 --> 01:05:25,119
to them

1916
01:05:25,119 --> 01:05:27,119
bill do you want to go first with that

1917
01:05:27,119 --> 01:05:28,480
yeah i mean i think

1918
01:05:28,480 --> 01:05:30,640
it's an interesting way to view a kind

1919
01:05:30,640 --> 01:05:32,079
of this kind of potential person because

1920
01:05:32,079 --> 01:05:33,280
i kind of felt like

1921
01:05:33,280 --> 01:05:35,920
i was somewhat like this person like way

1922
01:05:35,920 --> 01:05:36,799
back when

1923
01:05:36,799 --> 01:05:38,079
like nora and i was starting to have

1924
01:05:38,079 --> 01:05:39,520
these conversations about active

1925
01:05:39,520 --> 01:05:41,440
inference um

1926
01:05:41,440 --> 01:05:44,640
and i like the the this paper started

1927
01:05:44,640 --> 01:05:45,680
off basically as a

1928
01:05:45,680 --> 01:05:47,680
tutorial i was writing having spent two

1929
01:05:47,680 --> 01:05:48,960
three months kind of

1930
01:05:48,960 --> 01:05:51,359
in the evenings reading about trying to

1931
01:05:51,359 --> 01:05:53,280
like sift through

1932
01:05:53,280 --> 01:05:54,799
the act of inference literature and i

1933
01:05:54,799 --> 01:05:57,440
think um you know i think it's fair to

1934
01:05:57,440 --> 01:05:59,280
say that at times it's unapologetically

1935
01:05:59,280 --> 01:05:59,920
dense

1936
01:05:59,920 --> 01:06:02,480
and quite difficult to read so you know

1937
01:06:02,480 --> 01:06:03,760
without trying to you know

1938
01:06:03,760 --> 01:06:06,079
self-endorse but i do think reading this

1939
01:06:06,079 --> 01:06:07,520
manuscript in particular like the whole

1940
01:06:07,520 --> 01:06:08,079
aim

1941
01:06:08,079 --> 01:06:10,000
philosophy when we like when we started

1942
01:06:10,000 --> 01:06:11,440
writing this was to

1943
01:06:11,440 --> 01:06:14,640
really understand like what is what is

1944
01:06:14,640 --> 01:06:16,160
happening here like what is this kind of

1945
01:06:16,160 --> 01:06:17,760
expected free energy quantity like why

1946
01:06:17,760 --> 01:06:18,880
do we care about it

1947
01:06:18,880 --> 01:06:21,359
so i know from a kind of theoretical

1948
01:06:21,359 --> 01:06:23,359
perspective i think this is a very

1949
01:06:23,359 --> 01:06:26,160
lucid presentation of the concept so at

1950
01:06:26,160 --> 01:06:26,640
least

1951
01:06:26,640 --> 01:06:28,240
you can get some sort of intuition as to

1952
01:06:28,240 --> 01:06:30,319
what's happening um as for the

1953
01:06:30,319 --> 01:06:32,160
kind of coding side i can't really speak

1954
01:06:32,160 --> 01:06:33,760
to that i'm sure i'm sure nor

1955
01:06:33,760 --> 01:06:35,760
kind of has has worked it works with it

1956
01:06:35,760 --> 01:06:36,960
quite a bit

1957
01:06:36,960 --> 01:06:38,960
um i i was going to say it sort of

1958
01:06:38,960 --> 01:06:40,000
depends on

1959
01:06:40,000 --> 01:06:41,920
what the primary objective of the person

1960
01:06:41,920 --> 01:06:43,440
is is it for a

1961
01:06:43,440 --> 01:06:46,559
sort of get an understanding of

1962
01:06:46,559 --> 01:06:49,760
the the high-level conceptual ideas or

1963
01:06:49,760 --> 01:06:51,359
treat it as an algorithm because if

1964
01:06:51,359 --> 01:06:52,799
you're going coming from a free energy

1965
01:06:52,799 --> 01:06:54,799
principle too active inference is a

1966
01:06:54,799 --> 01:06:56,400
different story or if you if you're

1967
01:06:56,400 --> 01:06:59,440
taking active inferences of siloed

1968
01:06:59,440 --> 01:07:01,839
a specific kind of sequential

1969
01:07:01,839 --> 01:07:02,799
decision-making

1970
01:07:02,799 --> 01:07:06,559
uh scheme right um so depending on that

1971
01:07:06,559 --> 01:07:06,880
is

1972
01:07:06,880 --> 01:07:09,599
sort of differs uh but as phil was

1973
01:07:09,599 --> 01:07:11,440
saying i think this paper definitely

1974
01:07:11,440 --> 01:07:15,359
is really nice in the sense like it does

1975
01:07:15,359 --> 01:07:16,960
try and define all the different

1976
01:07:16,960 --> 01:07:18,720
concepts and goes through

1977
01:07:18,720 --> 01:07:21,920
the different formulations maybe not in

1978
01:07:21,920 --> 01:07:25,440
as much detail um with the assumptions

1979
01:07:25,440 --> 01:07:26,559
in play it gives you

1980
01:07:26,559 --> 01:07:29,520
the the layout of how you might be able

1981
01:07:29,520 --> 01:07:30,880
to derive it

1982
01:07:30,880 --> 01:07:34,079
um but certain things like what

1983
01:07:34,079 --> 01:07:37,200
what the approximate density even really

1984
01:07:37,200 --> 01:07:40,319
entails those are very uh difficult

1985
01:07:40,319 --> 01:07:41,200
questions

1986
01:07:41,200 --> 01:07:43,440
where you would have to like dive into

1987
01:07:43,440 --> 01:07:44,880
variational inference literature to

1988
01:07:44,880 --> 01:07:46,000
understand so

1989
01:07:46,000 --> 01:07:47,839
i guess from that perspective someone

1990
01:07:47,839 --> 01:07:49,920
who's coming into the field

1991
01:07:49,920 --> 01:07:51,440
should spend some time thinking about

1992
01:07:51,440 --> 01:07:53,280
variational inference and how that

1993
01:07:53,280 --> 01:07:53,760
really

1994
01:07:53,760 --> 01:07:55,839
ties back to the act of influence

1995
01:07:55,839 --> 01:07:56,960
formulation

1996
01:07:56,960 --> 01:07:59,119
because the perception part of active

1997
01:07:59,119 --> 01:08:00,559
inference is

1998
01:08:00,559 --> 01:08:03,359
in most instances exactly the same as

1999
01:08:03,359 --> 01:08:04,160
variational

2000
01:08:04,160 --> 01:08:07,119
uh inference literature and optimizing

2001
01:08:07,119 --> 01:08:08,640
uh the model evidence

2002
01:08:08,640 --> 01:08:12,240
uh or the maximizing the evidence law

2003
01:08:12,240 --> 01:08:13,200
abound

2004
01:08:13,200 --> 01:08:15,599
um this is the second thing that i was

2005
01:08:15,599 --> 01:08:16,399
going to add

2006
01:08:16,399 --> 01:08:19,679
is um that the paper

2007
01:08:19,679 --> 01:08:22,319
with lancelot de costa is really good

2008
01:08:22,319 --> 01:08:24,158
for someone who wants to drill down

2009
01:08:24,158 --> 01:08:27,198
into deriving everything themselves

2010
01:08:27,198 --> 01:08:30,719
uh sometimes it's super technical so

2011
01:08:30,719 --> 01:08:31,198
this

2012
01:08:31,198 --> 01:08:33,279
art that the paper that i walked through

2013
01:08:33,279 --> 01:08:35,359
today uh the one with phil and tom and

2014
01:08:35,359 --> 01:08:36,080
carl

2015
01:08:36,080 --> 01:08:38,238
is it's a nice introduction for someone

2016
01:08:38,238 --> 01:08:40,080
who's not familiar with the mathematics

2017
01:08:40,080 --> 01:08:41,198
and just wants to

2018
01:08:41,198 --> 01:08:44,399
get a layman's summary uh whereas this

2019
01:08:44,399 --> 01:08:46,560
the paper the the paper where lance's

2020
01:08:46,560 --> 01:08:48,080
first author gives like the

2021
01:08:48,080 --> 01:08:50,960
detailed derivations and a lot of the

2022
01:08:50,960 --> 01:08:51,759
assumptions

2023
01:08:51,759 --> 01:08:53,679
in place so that's from like

2024
01:08:53,679 --> 01:08:55,439
understanding the theory part

2025
01:08:55,439 --> 01:08:57,759
um from the coding perspective it

2026
01:08:57,759 --> 01:08:59,759
entirely depends what the end objective

2027
01:08:59,759 --> 01:09:00,640
is so if they want

2028
01:09:00,640 --> 01:09:03,198
someone wants to work with discrete

2029
01:09:03,198 --> 01:09:04,479
state formulations

2030
01:09:04,479 --> 01:09:07,679
then the matlab code carl has written

2031
01:09:07,679 --> 01:09:10,719
and it's years of work with lots of

2032
01:09:10,719 --> 01:09:13,600
um nice simulations and examples that

2033
01:09:13,600 --> 01:09:14,319
you can use

2034
01:09:14,319 --> 01:09:17,439
and also our code is online and you can

2035
01:09:17,439 --> 01:09:18,479
access it

2036
01:09:18,479 --> 01:09:21,679
so there's a link in the paper um in

2037
01:09:21,679 --> 01:09:24,880
the software um section that gives where

2038
01:09:24,880 --> 01:09:25,679
exactly

2039
01:09:25,679 --> 01:09:27,040
the code is and you can look through

2040
01:09:27,040 --> 01:09:29,040
that and see how the simulations were

2041
01:09:29,040 --> 01:09:29,920
done

2042
01:09:29,920 --> 01:09:33,120
if someone is interested in um

2043
01:09:33,120 --> 01:09:36,719
more i guess um high-dimensional

2044
01:09:36,719 --> 01:09:39,120
formulations of active inference that

2045
01:09:39,120 --> 01:09:42,799
um there's some recent work with uh

2046
01:09:42,799 --> 01:09:46,399
zephyr zephyrus um so zeff fontes

2047
01:09:46,399 --> 01:09:48,799
as first author uh where we've got a

2048
01:09:48,799 --> 01:09:50,080
nice again we have a

2049
01:09:50,080 --> 01:09:52,479
git reaper for that work as well which

2050
01:09:52,479 --> 01:09:53,198
uh

2051
01:09:53,198 --> 01:09:55,920
gives uh a breakdown of how you would

2052
01:09:55,920 --> 01:09:57,360
implement a simple

2053
01:09:57,360 --> 01:09:59,600
active inference agent using uh very

2054
01:09:59,600 --> 01:10:01,280
short encoders and a simple

2055
01:10:01,280 --> 01:10:03,679
transition network so there's lots of

2056
01:10:03,679 --> 01:10:04,719
like different

2057
01:10:04,719 --> 01:10:06,400
areas but for someone starting out they

2058
01:10:06,400 --> 01:10:07,679
sort of have to understand whether they

2059
01:10:07,679 --> 01:10:08,800
want to focus on the

2060
01:10:08,800 --> 01:10:10,159
theoretical side or whether they want to

2061
01:10:10,159 --> 01:10:11,840
focus on the implementation side

2062
01:10:11,840 --> 01:10:13,199
theoretical side

2063
01:10:13,199 --> 01:10:14,640
would be drilling down into the

2064
01:10:14,640 --> 01:10:16,000
variational influence and the maths

2065
01:10:16,000 --> 01:10:17,920
behind it and if they want to focus on

2066
01:10:17,920 --> 01:10:18,239
the

2067
01:10:18,239 --> 01:10:19,600
the coding aspect then they want to

2068
01:10:19,600 --> 01:10:21,760
figure out whether it's continuous or

2069
01:10:21,760 --> 01:10:23,199
discrete state formulations are

2070
01:10:23,199 --> 01:10:25,199
interested interested in and then

2071
01:10:25,199 --> 01:10:27,679
sort of break down into if it's

2072
01:10:27,679 --> 01:10:29,040
continuous then

2073
01:10:29,040 --> 01:10:32,880
most of it would either be writing the

2074
01:10:32,880 --> 01:10:34,800
the coordinates of motions themselves or

2075
01:10:34,800 --> 01:10:36,960
they would have to use like some sort of

2076
01:10:36,960 --> 01:10:40,640
like um um a neural network to

2077
01:10:40,640 --> 01:10:42,480
approximate that continuous distribution

2078
01:10:42,480 --> 01:10:43,280
of interest

2079
01:10:43,280 --> 01:10:45,520
out or the discrete state which carl's

2080
01:10:45,520 --> 01:10:46,400
written out

2081
01:10:46,400 --> 01:10:49,199
um and if if they have questions about

2082
01:10:49,199 --> 01:10:50,480
the discrete state

2083
01:10:50,480 --> 01:10:53,679
um i'm i'm happy to take emails as well

2084
01:10:53,679 --> 01:10:54,080
so

2085
01:10:54,080 --> 01:10:57,040
if anyone has or the the continuous

2086
01:10:57,040 --> 01:10:57,840
state state

2087
01:10:57,840 --> 01:11:01,679
um state space as well thanks for the

2088
01:11:01,679 --> 01:11:04,960
distinction and um it's such a large

2089
01:11:04,960 --> 01:11:06,159
difference between the

2090
01:11:06,159 --> 01:11:09,040
matlab code which we got to walk through

2091
01:11:09,040 --> 01:11:09,679
with

2092
01:11:09,679 --> 01:11:12,159
ryan smith and christopher white which

2093
01:11:12,159 --> 01:11:13,120
is doing

2094
01:11:13,120 --> 01:11:16,000
matrix multiplication and such and then

2095
01:11:16,000 --> 01:11:17,920
here comes the neural networks

2096
01:11:17,920 --> 01:11:20,880
and it's offering sounds like new

2097
01:11:20,880 --> 01:11:23,120
opportunities with high dimensionality

2098
01:11:23,120 --> 01:11:26,159
and continuous variables but also a lot

2099
01:11:26,159 --> 01:11:26,880
of new

2100
01:11:26,880 --> 01:11:30,320
challenges so what is

2101
01:11:30,320 --> 01:11:33,360
the essence that's shared

2102
01:11:33,360 --> 01:11:37,440
by the matrix form and by this more

2103
01:11:37,440 --> 01:11:40,480
machine learning style

2104
01:11:40,480 --> 01:11:43,040
because for some people um it might be

2105
01:11:43,040 --> 01:11:43,920
splitting a hair

2106
01:11:43,920 --> 01:11:46,480
quite literally the difference between

2107
01:11:46,480 --> 01:11:47,840
two computer languages

2108
01:11:47,840 --> 01:11:49,120
when they're thinking about active

2109
01:11:49,120 --> 01:11:51,760
inference from a

2110
01:11:51,760 --> 01:11:54,800
ecological psychology or in an active

2111
01:11:54,800 --> 01:11:57,600
philosophical or an embodied performance

2112
01:11:57,600 --> 01:11:58,320
perspective

2113
01:11:58,320 --> 01:12:01,600
all backgrounds that converge on active

2114
01:12:01,600 --> 01:12:02,880
inference and so

2115
01:12:02,880 --> 01:12:05,440
to somebody outside the the strand of

2116
01:12:05,440 --> 01:12:06,800
hair

2117
01:12:06,800 --> 01:12:09,199
what is it that we can really distill

2118
01:12:09,199 --> 01:12:09,920
that is

2119
01:12:09,920 --> 01:12:11,679
core active inference and i wrote down a

2120
01:12:11,679 --> 01:12:13,440
few things so

2121
01:12:13,440 --> 01:12:14,800
that you had said what are those core

2122
01:12:14,800 --> 01:12:16,480
pieces that allow us to

2123
01:12:16,480 --> 01:12:19,120
dive into the matrix mode with matlab or

2124
01:12:19,120 --> 01:12:20,840
into the neural network mode with like

2125
01:12:20,840 --> 01:12:22,960
python

2126
01:12:22,960 --> 01:12:24,640
um so i think it comes down to my

2127
01:12:24,640 --> 01:12:26,640
summary slide the way i think about it

2128
01:12:26,640 --> 01:12:28,159
so the active inference the core

2129
01:12:28,159 --> 01:12:29,679
ingredients are formulating the gender

2130
01:12:29,679 --> 01:12:30,960
model

2131
01:12:30,960 --> 01:12:32,480
and here formulating the general model

2132
01:12:32,480 --> 01:12:33,840
is the parameterization of the general

2133
01:12:33,840 --> 01:12:35,520
model so either you can use the

2134
01:12:35,520 --> 01:12:37,760
discrete stage categorical distributions

2135
01:12:37,760 --> 01:12:38,719
or you can use

2136
01:12:38,719 --> 01:12:40,560
a more continuous state formulation and

2137
01:12:40,560 --> 01:12:42,719
again

2138
01:12:42,719 --> 01:12:44,560
the neural network formulation is a

2139
01:12:44,560 --> 01:12:46,400
specific instantiation of that

2140
01:12:46,400 --> 01:12:49,679
so that would be one way of sort of

2141
01:12:49,679 --> 01:12:50,480
differentiating

2142
01:12:50,480 --> 01:12:53,679
that the second one is the

2143
01:12:53,679 --> 01:12:56,239
optimization of the objective functions

2144
01:12:56,239 --> 01:12:57,440
in play

2145
01:12:57,440 --> 01:13:01,840
so in the the matlab code

2146
01:13:01,840 --> 01:13:04,239
we're doing gradients to send using mean

2147
01:13:04,239 --> 01:13:06,400
field message passing algorithm so

2148
01:13:06,400 --> 01:13:07,760
specific formulation that's been

2149
01:13:07,760 --> 01:13:09,679
introduced in a couple of papers

2150
01:13:09,679 --> 01:13:11,440
and i specifically didn't walk through

2151
01:13:11,440 --> 01:13:12,880
that or

2152
01:13:12,880 --> 01:13:16,000
uh you're doing back propagation to

2153
01:13:16,000 --> 01:13:17,280
actually

2154
01:13:17,280 --> 01:13:20,320
calculate the or lambda distributions

2155
01:13:20,320 --> 01:13:21,760
and then you're just

2156
01:13:21,760 --> 01:13:23,840
solving those distributions that you

2157
01:13:23,840 --> 01:13:26,560
have so it sort of depends on

2158
01:13:26,560 --> 01:13:29,840
which formulation you're um

2159
01:13:29,840 --> 01:13:32,400
like um it depends on how you optimize

2160
01:13:32,400 --> 01:13:33,840
those objectives either you're like

2161
01:13:33,840 --> 01:13:35,600
taking the implicit forward model

2162
01:13:35,600 --> 01:13:37,600
or you're taking an explicit gender

2163
01:13:37,600 --> 01:13:39,199
model um

2164
01:13:39,199 --> 01:13:41,360
and one thing i forgot to mention is

2165
01:13:41,360 --> 01:13:42,480
that um

2166
01:13:42,480 --> 01:13:44,480
alex shams and connor hines they've been

2167
01:13:44,480 --> 01:13:47,520
working on a discrete state formulation

2168
01:13:47,520 --> 01:13:50,880
of um active inference in python

2169
01:13:50,880 --> 01:13:53,360
which might be in of interest for people

2170
01:13:53,360 --> 01:13:55,280
who want to like focus on one specific

2171
01:13:55,280 --> 01:13:56,239
language

2172
01:13:56,239 --> 01:13:58,960
that can do like the more high end or

2173
01:13:58,960 --> 01:14:00,640
high dimensional stuff

2174
01:14:00,640 --> 01:14:02,719
and the discrete state formulation that

2175
01:14:02,719 --> 01:14:03,920
carl has um

2176
01:14:03,920 --> 01:14:05,760
i i know they're also looking for people

2177
01:14:05,760 --> 01:14:08,719
to work on the code base if anyone's

2178
01:14:08,719 --> 01:14:09,840
interested so it's called

2179
01:14:09,840 --> 01:14:13,520
infer activity i think um but that's on

2180
01:14:13,520 --> 01:14:17,760
github as well if anyone's interested

2181
01:14:17,840 --> 01:14:21,760
cool um any sort of last

2182
01:14:21,760 --> 01:14:25,280
thoughts or comments from either of you

2183
01:14:27,440 --> 01:14:30,640
um no i think i'm okay

2184
01:14:30,640 --> 01:14:33,440
so what do you reckon

2185
01:14:34,880 --> 01:14:37,199
um

2186
01:14:38,560 --> 01:14:41,360
i think we're kind of noticing a bit of

2187
01:14:41,360 --> 01:14:42,239
a

2188
01:14:42,239 --> 01:14:45,679
um an increase in interest in active

2189
01:14:45,679 --> 01:14:46,640
inference like

2190
01:14:46,640 --> 01:14:48,560
just to give you an example like maybe

2191
01:14:48,560 --> 01:14:49,840
areas where

2192
01:14:49,840 --> 01:14:51,040
even active inference would have been

2193
01:14:51,040 --> 01:14:53,840
considered before like robotics

2194
01:14:53,840 --> 01:14:55,360
um you're now beginning to see more and

2195
01:14:55,360 --> 01:14:57,360
more of it so i think if you do want to

2196
01:14:57,360 --> 01:14:58,960
get involved in it now is a particularly

2197
01:14:58,960 --> 01:15:01,199
good time

2198
01:15:01,199 --> 01:15:04,400
great call philip and i'll

2199
01:15:04,400 --> 01:15:07,199
um just re-recommend the excellent paper

2200
01:15:07,199 --> 01:15:08,880
that we're discussing it's in the

2201
01:15:08,880 --> 01:15:11,120
description of this video really

2202
01:15:11,120 --> 01:15:12,719
appreciate both of you

2203
01:15:12,719 --> 01:15:15,040
for joining you're always welcome to

2204
01:15:15,040 --> 01:15:16,480
come on to be

2205
01:15:16,480 --> 01:15:18,239
speaking about a paper you've authored

2206
01:15:18,239 --> 01:15:19,520
or not but

2207
01:15:19,520 --> 01:15:21,679
again thanks a lot uh to nora and

2208
01:15:21,679 --> 01:15:23,679
phillip and i hope to see you again on a

2209
01:15:23,679 --> 01:15:24,239
future

2210
01:15:24,239 --> 01:15:27,679
active inference stream thank you

2211
01:15:27,679 --> 01:15:30,159
thank you for inviting us bye-bye see

2212
01:15:30,159 --> 01:15:31,440
you daniel

2213
01:15:31,440 --> 01:15:34,000
peace bye

2214
01:15:36,840 --> 01:15:39,280
awesome stop the stream great

2215
01:15:39,280 --> 01:15:41,040
conversation thanks a lot to philip and

2216
01:15:41,040 --> 01:15:43,840
noor really


SPEAKER_01:
Hello and welcome everyone to the Active Inference Lab.

This is the model stream number 2.1 on April 16th, 2021.

And today's going to be an awesome model stream.

We're just going to briefly go around and introduce ourselves and then I'll just mention how the session will be run today and then we'll pass it to Noor for a presentation.

So I'm Daniel and I'm a postdoctoral researcher in California.

I'll pass to Philip.


SPEAKER_02:
Hi, yeah.

I'm currently a PhD student at Oxford in my second year.

And yeah, I guess this is a work I did before I started my PhD alongside Noor.

And I'm currently more focusing on data efficiency within specifically reinforcement learning.

But I guess on that topic, the knowledge of active inference is obviously useful for approaching such research problems.


SPEAKER_01:
Cool.

And Noor?


SPEAKER_03:
Hi.

Hi, I'm Nora.

I'm a third year PhD student in a theoretical neurobiology group at the Wellcome Centre for Human Neuroimaging at UCL.

So that's University College London.

So my PhD supervised by CARS focused on these ideas pertaining to adaptation, one of which I'll be focusing on today, which is behavioural adaptation in non-stationary environments using active inference.

So thank you.


SPEAKER_01:
Awesome.

Thanks both for joining.

And for this presentation, we're going to be hearing a who knows how long presentation from Noor.

And then I'm going to be compiling questions from the chat.

So please just type questions as they come to you, and then we'll address them at the end.

So thanks again.

And Noor, please take it away.


SPEAKER_03:
Perfect.

Thank you.

So today I'll be presenting some work that I did in collaboration with Philip, who you've just heard from

Thomas Parr and Carl Friston.

So it's titled Active Inference, Demystified and Compared.

Okay, okay, perfect.

Okay, so the presentation is structured as follows.


SPEAKER_00:
Can you share the screens?


SPEAKER_03:
I think it's sharing.

Are you not able to see it?


SPEAKER_01:
I'm not seeing it.

Could you just re-share it?


SPEAKER_03:
Yep, sure.

technology, I tell you.


SPEAKER_01:
There we go.

And I'll crop it.

So go for it.

Thanks.


SPEAKER_03:
Perfect.

Thank you.

So the presentations Richard as well as that's our briefly motivate the problem setting and provide details of a particular active inference instantiation under consideration today, which is the discrete state space setting.

And the second half of the presentation is going to be focused on some particular examples comparing

the active inference formulation with reinforcement learning, specifically Q-learning and Bayesian model-based algorithm.

And then what I'm going to do is provide some face validity of particular aspects of why you would want to even use active inference.

So what is active inference?

It's a first principle account of how biological or artificial agents may operate in dynamic non-stationary settings.

It stipulates that these agents, in order to maintain homeostasis, reside in attracting states that minimise their entropy or their surprise.

So if you take this particular example that we're seeing of this past little hungry agent opening the fridge, the way it would work is like you would need to, why would you open the fridge?

Right?

So you want to make a particular choice between eating at home or outside.

And in order to do that, you have to decide what is the optimal action that would allow you to resolve your own uncertainty about the current stage of affairs.

So, and that would help you then decide whether you want to cook at home or you want to walk to the restaurant.

And this particular instance, this has led to the agent opening the fridge to check whether it even has food at home.

And what's nice about active inference is that it allows you to think about these problem settings in a more formal way by specifying that optimal behavior is determined by evaluating the evidence that is the sensory input under the agent's gender model of observations that it is being exposed to.

And in this particular presentation, what we'll do is focus on just the process theory that underwrites active inference and not talk through the biological or the neural plausibility of the active inference message passing scheme.

But to properly motivate why we would want to even use active inference in comparison to generic reinforcement learning algorithms, we need to first start off with this understanding that with an active inference, there's this commitment to a pure belief-based scheme.

which means that reward functions are not always unnecessary because any policy that you would have has epistemic value even in the absence of preferences.

Additionally, active inference agents can also learn their own reward functions and this helps the agent describe the type of behaviour that it expects to see in itself as opposed to something that it would get from the environment.

And these two particular points are really important in contrast to reinforcement learning, because under standard RL settings, the reward function would define how the agent interacts or behaves within a particular environment setting.

But defining that reward function in the first place is quite difficult because it assumes that there is a specific signal that's being given from the environment

that can be unanimously good or bad for the agent, which wouldn't necessarily hold true in a real setting where these environment signals can change depending on the setting.

For example, eating ice cream is not always going to be rewarding if you're ill and it might make you worse.

And that's why constructing these reward functions in the first place is extremely difficult.

And if you're not constructing them in an appropriate way, even with an RL setting,

it can result in suboptimal behaviour for your agents.

And active inference is really good in the sense because we are actually replacing or bypassing the traditional reward function that you would have in the RL setting with prior beliefs about preferred outcomes.

So the sort of desire states of affairs that you want to see yourself be in.

And this becomes important in settings where there's no reward or there's really imprecise understanding of what a reward or preference setting should look like.

And in this scenario, within the standard action inference discrete state formulation, what we can do is learn the empirical prior distribution over these preferred outcomes.

and intrinsic, sorry, the internal reward function of that agent.

And this sort of brings me to like the first distinct conceptualization between RL setting and active inference, because within active inference, rewards are nothing distinct.

They're just a standard observation that the agent is getting from the environment, whereas in RL, they're quite necessary to have appropriate action

that the agent is going to learn.

The second point we wanted to make was that active inference provides a principal account of epistemic exploration and intrinsic motivation as minimizing uncertainty.

And again, within the RL setting, this is quite crucial because the whole premise of like lots of new algorithms that we see in RL is to try and

find the right trade-off or the balance between exploration and exploration.

So what are the right set of actions that the agent should make at a given point in time?

Should it carry on choosing all the different ice cream flavors that it's never been exposed to, like mustard, et cetera?

Or should it always have the same ice cream flavor that it's been exposed to in the past that it really likes?

For example, hazelnut on a teller, for instance.

So this is an outstanding problem within RL and under a Bayesian framework, active inference deals with it naturally using the expected free energy formulation that I'll come to in a moment.

And the last bit that you can see within the active inference framework is that it naturally accounts for uncertainty as part of the belief updating process.

Okay.

So now that I've sort of laid out the three things that are super interesting about the active inference scheme in comparison to RL, I'm going to provide some intuitions as to why you can, sorry, some motivations as to why we can even formulate the active inference formulation the way we do.

Okay, so...

Sorry, I just realized I like skipped ahead.

Um, no, no, sorry.


SPEAKER_01:
Great presentation.


SPEAKER_03:
Yeah.

Thank you so much.

Um, let me just scroll down to, um, okay.

So I previously stated that with an active inference, it stipulates that agents are maintaining their homeostasis.

by residing in attracting states that minimize surprise.

So you must have been thinking, what is surprise?

Well, here we define surprise as a negative log probability of outcomes.

And for this, we introduce one random variable, which is O, that corresponds to a particular outcome that's received by an agent.

And this exists within a finite set of all possible outcomes.

And that's O here.

So first equation that we have just formally states that out.

And here P denotes the probability distribution over outcomes.

So in active inference, the way the agent will actually minimize the surprise quantity that we just walked through is by maintaining a gender model of the world.

And this is important because at any given point in time, the agent wouldn't necessarily have access to the true measurements of the current state of the world.

So in this particular graphic that you see here,

You've got the environment and the agent interacting with the environment in a particular way.

It's being exposed to the sensory signal, but it doesn't know what the outcome of the O was really generated by.

So it can only perceive itself and the world around it through only the O, and it needs to make inferences about what type of states or the true causes were responsible for the particular sensory input that it's being exposed to.

And this is why

In active inference, when we formulate the problem, we formulate it as a partially observable Markov decision process, because in this way, we are able to formulate a gender model that defines this internal distribution over the internal states that the agent would use in order to infer the outcomes.

So it doesn't have access to the true state, but it can make hypothesis of beliefs about the states that could have given rise to a particular sense outcome space that it's being exposed to.

And using this, the agent will make inferences about the true state using a process of reverse mapping, specifically Bayesian model inversion.

And to make this a little bit more concrete, what you can do is think about the hidden states as locations or color, for example, and the observation space that the agent would be exposed to would be, for example, the velocity of the movement or a particular reward

or like a happy face that they're being exposed to.

Okay.

So if we were to think about this a little bit more formally, so what is the gender model?

So as we described before, a gender model is a partially observable MDP within this active inference formulation, which rests on a simplified setting that we're considering here where we only have two random variables.

The first one is O that we've discussed.

And the second one is S, where S denotes a random variable representing hidden or latent states.

And they exist within a finite set of all possible hidden states, which is denoted by capital S here.

And this joint probability that we get over O and S can be factorized into the likelihood function, which is P of O given S. And then you have the prior over the internal states, which is P of S.

So this gives you a very nice formulation that we're going to use in the next couple of slides.

I just wanted to ask, are you able to see my mouse when I highlight or is that not there?


SPEAKER_01:
I can see it, yes.


SPEAKER_03:
Oh yeah, perfect.

So we know that for an agent to minimize its surprise, we would need to marginalize out all the possible hidden states that could have led to a given outcome.

And this can be achieved by using the factorization that I just mentioned, the likelihood and the prior.

But the problem is that this is not a trivial task because the dimensionality of the hidden states can be extremely large.

And if you're considering additional random variables that we're going to introduce in a bit, this becomes even more problematic.

And that's why we use

another quantity, a variational approximation of this quantity P , which is more tractable and allows us to estimate the quantities of interest.

So this will be a natural step to talk about the variation for energy, which is this variational approximation of the quantity of interest.

So what is variational free energy?

So variational free energy is defined as the upper bound on surprise.

So the first definition that we considered is derived using Jensen's inequality and is commonly known as negative evidence lower bound in the variational inference literature.

So we get this from the equation four that we just saw by introducing negative log on both sides and then multiplying this term by one, which is essentially Q of S over Q of S. So we're assuming that Q of S cannot be equal to zero.

and with that we then apply Jensen's inequality and we move the log inside the function and we end up with our expectation with respect to Q for log of the joint over the approximate or the variational quantity of interest here and then we take the negative inside and we can flip it around and we get our

first nice quantity of interest here, where we get the bound that we're interested in, in terms of kale between the approximate and the joint that we have.

So to make this a little bit more concrete, what we can do now is to further manipulate the variational free energy summons into the kale between the approximate and the

true posterior minus the log evidence that we, the model evidence that we had.

And we can rearrange the last equation to really hone in on the connection between

surprise in the variational free energy.

So if you remember that KL is a divergence, which means that it cannot be less than zero.

So it's always strictly greater than or equal to zero, which means that when our approximate is equal to the true posterior, we end up with a variational free energy equal to the model evidence, which means that minimizing free energy is essentially equivalent to maximizing the gelatin model evidence.

We can rewrite the previous equation that we had, so equation 10, to express the variation of free energy as a function of posterior beliefs in multiple different forms.

So I'm just going to focus on equation 12 here, which is the complexity minus accuracy.

So this is a trade-off that's normally used when

the papers that essentially says the complexity TAM or complexity cost is essentially your kale between your approximate S given pi with respect to your P of S given pi.

And here pi is just your policies.

And these can be regardless of hypothesis of how the agent is going to act.

But I'll come back to what policies really entail.

But for now,

just consider them as a term that allows us to condition the free energy on a sequence of trajectories of interest.

And the second term that we have is the log probability of O given S, so the likelihood with respect to the Q of S that gives you the accuracy.

So a simple way to think about it is that this is just how accurate the model is, and this is some regularization term

penalty term to make sure that it's not diverging too far away from our initial priors.

Okay, so this particular quantity, variational free energy, are there any questions at this point about the variational free energy?


SPEAKER_01:
Not yet, thank you.


SPEAKER_03:
Okay, perfect.

Okay, so the variational free energy is giving us this way of perceiving the environment and addresses one part of the active inference formulation, which is making inferences about the given world.

that the agent is interacting with at a given point in time.

However, we have not actually accounted for the active part, whereas like this particular agent that we have under the active inference formulation can take series of actions or interact with the environment in such a way that it affects that environment in the future.

So to motivate this a little bit further, what we can think about is that not only do we want to minimise our variation of free energy, we also want to minimise a quantity called expected free energy, which depends on anticipated observations in the future or about the future.

And minimisation of this particular term allows the agent to influence the future

by taking particular actions in the present, which are selected from a set of policies.

So I mentioned policies a few times now.

So what are they?

So policies can be defined as a sequence of actions at time tau that enable an agent to transition between hidden states.

And tau here is essentially a sequence of trajectories up to a particular horizon, which is considering the total number

time steps that you are considering in a particular setup.

And for us to properly define policy, we need to introduce two additional random variables.

So the first one is an action that's conditioned on tau, which is denoted by u tau here.

And this exists within a finite set of all possible actions that the agents can take.

And the second random variable that we introduce is policy, which is the pi that we've discussed.

And this exists within a finite set of all possible policies or sequence of actions in a sense of like the sequential policy optimization that we're interested in here.

So to make it a little bit more concrete, pi here, the random variable can be decomposed into a series of actions over a particular time period.

time horizon tau.

So U1, U2, and going up to U tau would denote the action from action at time point one, action at time point two, and so on.

And the link is explicit when you consider that.

If you consider policy at a particular time point, so tau, then the action that you get would be that action.

Okay.

So I also wanted to highlight that this definition of policy is actually quite different to or distinct to how it's considered in RL, which is when they say policy, they mean state action policies.

So as I just mentioned, an active inference policy is simply a sequence of choices for actions through time.

That is a sequential policy.

And this is different to a state action policy in reinforcement learning, which is mapping of states to actions.

So your RL policy, which takes in account the action and the state is probability of your action given the state.

And under our POMMDP formulation, the definition of action, sorry, definition of policies in RL and active inference become exactly the same.

when we consider the setting where tau is equal to one.

So you're only considering one step ahead.

Okay, so I'm going to move a little bit and consider the quantitative interest they expected for energy, which is how do we even derive it?

So in order to derive it, we first need to extend the variational free energy definition that we had before.

So a few slides ago, and now make it dependent on both time, so tau and policy.

And what we're doing essentially is taking that same equation and just decomposing it for previous, sorry, previous and current time step under a particular policy.

So that way, this is why we have the conditioning and then we're decomposing it in a specific way in equation 15 and then writing out the matrix formulation in equation 16.

So we can come back to this if there are any questions.

But the key thing to take away from this slide is that now we're including a functional dependency on time for the variation of free energy.

And this is allowing us to now move to the expected free energy formulation.

But the key thing to note here is that we're only considering time points, the previous time point at the present, not the future at all.

So using the free energy equation before we can derive the expected free energy.

And what is the expected free energy then?

So expected free energy is the free energy function of future trajectories G and it effectively values the evidence for plausible policies based on outcomes that have not been observed yet.

So that's the key thing.

So you're making inferences about the set of future trajectories that you haven't observed

And there's two heuristics that are introduced here in order to get to the G formulation that we see in equation 17.

The first one is to include beliefs about future outcomes in the expectation.

That is, we're supplementing expectation under the approximate posterior with the likelihood here, which results in a predictive distribution given by these first two terms here.

And the second one is that we're implicitly or, I guess, explicitly conditioning on the joint probabilities of states and observations in the gender model.

So in the gender model, dependent on the desired state of affairs, as opposed to a particular policy now.

So this constrains the type of preferences that the agent would have.

And what's helpful with these two moves that we're making is that we can now evaluate this quantity before actually having the observations.

And the second one is that the minimization of G would actually encourage policies to be consistent with the desired state of affairs that the agent expects itself to be in.

I'm just going to briefly mention that this is not the only way to derive expected free energy.

And there's been some work that's looked at other formulations

including work by Carl, where the formulation of the expected free energy can be decomposed into different structures.

So if anyone's interested in that, we can go through it later.

But this free energy, expected free energy that I just introduced can be decomposed in certain ways.

So equation 20 and 21 give

two different decompositions, the first one being the epistemic and the extrinsic value trade-off, and the second one being the expected and the cost and ambiguity term.

So if we just consider the first equation, we can say that if we're minimising this equation, then we're capturing this imperative to maximise the information gain that you would have from observing the environment about particular hidden states.

while maximizing the expected value, which is scored by the log preferences or the extrinsic value here.

So this particular formulation actually gives us a very clear trade-off between the first component, which is the epistemic value that promotes curious behavior.

So that's what you want with exploration encouraged as the agent seeks out these new states that minimize uncertainty about the environment.

And the latter bit is more pragmatic and encourages exploitative behavior through this understanding of the type of policies that the agent would prefer to reach.

In other words, like this expected free energy formulation that we're seeing in equation 20 is essentially treating exploration and exploitation as two different ways of tackling the same problem.

So minimizing uncertainty that mentioned at the start of the presentation.

Okay, we can also think about the second equation here, which is just offering us an alternative perspective on the expected free energy, which is an agent wishes to minimise the ambiguity

and the degree to which outcomes under a particular policy deviate from prior preferences.

Thus, ambiguity here is the expectation of the conditional entropy or the uncertainty about outcomes under the current policy.

In this particular setting, low entropy would suggest that outcomes are quite salient and uniquely informative about the hidden states.

So, for example, the visual cues that you might see if the room is actually probably lit up in comparison to if it's quite dark, you're not going to make out anything important from that.

In addition, the agent would like to pursue policy dependent outcomes that resemble its preferred outcomes.

So C denoted by C here, and this is achieved when the KL divergence between the predicted and the preferred outcomes is minimized by a particular policy.

Okay.

And these prior beliefs about the future outcomes equip the agent with goal directed behavior, which is one of the

I guess the instances that is really important in active inference.

Okay, so once we have the expected free energy, we can derive the policies.

So, and this is realized by deriving the probability of any policy by applying a softmax function over the expected free energy.

And this sort of illustrates the self-evidencing behavior of active inference, because any sort of policy or action sequence that results in lower expected free energy are more likely.

And intuitively, this would make sense because the expected free energy is sort of encapsulating all the types of things that you want to include or consider when you're interacting with the world.

So you want to explore, you want to exploit, but you want to have a balance of that.

And then when you're selecting your policy, it's just a matter of determining the set of actions which get you closest to this particular goal.

And this can be defined by an attractor set that's defined by your C matrix that we described before.

And if you don't have that, then it's just random exploration that you would get.

Sometimes you can also include a temperature parameter beta here.

And by having a hyper prior on this, you introduce an additional complexity cost into the expected free energy formulation, which allows you to account for how flat or how confident or precise you want your preferences to be over the policy space.

The key thing to note is that

For the sake of simplicity, I'm not going to go through a lot of the details about how these are optimized, but you can do that in multiple different ways.

For example, in active inference, we can optimize the expectation about the hidden states of interest, the policies, the precisions through inference, and then we can also optimize the model parameters through the learning procedures involved.

But those sort of differ depending on the setup you're looking at.

For example, if you're using variational phase, you would just iterate these functions or objective functions until convergence.

Or in active inference, you do a gradient descent to find the sufficient statistics of interest.

Again, this depends on exactly which formulation and setup you're looking at.

But the key thing to note here is that there's three particular aspects to the active inference algorithm that are useful and can be taken from this particular

framework and apply to other settings.

So I'm just going to reiterate and summarise them briefly.

So we first have the gender model that's crucial.

So in order for an agent to interact and minimise a surprise, it needs a gender model of the world.

And that's described as simply, I'm not including any of the model parameters here, but you can have your outcomes, your states and your policies.

And these are decomposed into, sorry, there's a

error with the brackets here, but these are decomposed into your prior, your likelihood and your transition function.

And then once you have this general model, the objective of the agent is to fit the model to sample observations to reduce surprise, and that is through variational free energy optimisation.

So this particular trade-off that we have between the complexity and the accuracy cost

And then the last part of this algorithm is to plan.

So select actions that minimize uncertainty.

That is the expected free energy.

And the way you do that is by having a softmax over this negative of g, the quantity we have here, and then sampling from that in order to select the next best action.

Okay.

So that's it.

quick deep dive into a massive amount of active inference literature, but just wanted to highlight that those are the three core ingredients that if you are interested in implementing these algorithms yourself.

Okay.

So now I'm just going to switch gears a little bit and walk through comparisons with reinforcement learning.

So in our work, we considered a modified version of the OpenAI GIMS frozen lake environment.

So Frozen Lake has a grid-like structure with four distinct patches.

It has a starting point, which is S. So we can see it here.

Apologies, but it's super tiny.

So S is here.

And you've got the frozen surface, which is F. So again, I don't think I can differentiate because I'm just moving my mouse.


SPEAKER_01:
I'm zooming in.

They can see it.


SPEAKER_03:
Perfect.

So you've got the frozen F, and then you've got the hole.

And then lastly, you have the goal, so G here, where the frisbee is located.

And all patches in this particular setup are safe except for hole, where if the agent goes to H, it gets a negative reward.

The agent starts each episode at the first position, which is the starting position.

And from there, it needs to reach the frisbee location

in the least amount of steps possible.

And the way it can do that is by performing four different types of actions, either going left, right, down or up.

And the agent is allowed to carry on moving through the frozen lake with multiple revisits.

So you can go back to the starting position having gone in other places, but each episode will end when it either reaches the hole or the goal location.

And these locations differ depending on the setup that we have in our simulations.

So in one setup, the position of the hole is eight and the goal is six.

And another setup, the position of the hole is six and

the goal is eight.

And the objective is, as I said, to reach the goal in ideally as few steps as possible while avoiding the hole, because that would end the episode.

If it reaches the goal, it gets a positive reward of 100 and negative otherwise.

The key thing to note here is that the scoring metric actually allows us a way to compare the active inference algorithms to reinforcement learning algorithms.

But it's not really important for the active inference algorithm to

have the reward function as a get-go because it can still move around using just the information game term, so not having the extrinsic value component.

And that's quite interesting because we'll see the ramifications of that in our simulations.

For this particular setup, we limited the maximum number of time steps for each episode to 15.

Okay.

So what I'm going to do is first talk through the gender model that we use for the active inference formulation.

So here, what you're seeing on the slide is a graphical representation of the active inference gender model.

So this model contains four action states, so right, down, up, and left.

And these control the ability to transition between hidden states, location fact.

So for example, if you are in position one and you take the action right, then you'll end up in position two.

Or if you are in location five and you take the up action, you'll end up in position two as well.

In this particular setting, both

Position six and eight are absorbing states because if you remember, once the agent goes to that location, they're not able to move out.

So that's when the episode ends.

And if an agent makes an improbable move in this particular maze, for example, if it tries to go from position one to left, it will just stay in that location.

It won't move.

In this particular gender model, so I'm just looking at the hidden states now, we have a chronic transfer product between the two factors.

So we have location and context here.

The context cannot be changed by the agent that we have because this is something that's determined by the environment and this determines where the goal and the whole locations are.

Whereas the location is something that the agent has control over and that's why we have the action states over this.

So with the context, we have two contexts.

The first one is where the goal is in your location eight and the hole is in location six.

And the second context is where the goal is in location six and the hole is in location eight.

At each time point, the agent will observe two outcomes.

One would be its own position in this particular maze, and the second would be the score that the agent would get.

The likelihood for the grid position is entirely determined by the location of the agent, and the score is determined by both the location and the context in play.

The agent is in location six and it's in context two, then it will receive a positive reward.

Otherwise it will receive a negative or neutral reward depending on where it is.

Based on trying to make that comparison with reinforcement learning, what we're doing here is we are introducing prior preferences where the agent has plus four for positive reward, negative four for sorry, minus four for negative reward and otherwise.

And at the first stage, it expects itself to be in the first location.

So we compared this particular GEN2 model and the active inference agent to reinforcement learning algorithms.

So the first one was the Q-learning using epsilon greedy exploration.

And the second one was the Bayesian model-based reinforcement learning algorithm using standard Thomson sampling.

And Thomson sampling is appropriate procedure here because it entails the optimization of dual objectives

reward maximization, and information gain.

And this is achieved by having this distribution over a particular function that we parameterize by having a prior distribution over it that we sample from.

Okay.

So for the two Q-lining algorithms, we have two epsilon greedy parameters.

So one where it's fixed exploration set to 0.1, and then another one where we have decaying exploration that starts from one and decays down to zero.

So first we assessed how the agents interact in a stationary setting where the reward wasn't changing such that the goal location was always at six and the whole location was always at eight.

And then we evaluate the performance of the agents.

The key thing to take away from here is that both the Bayesian RL and the active inference agents are able to

quickly learn where the reward location is and just maximize it out.

And this performance is consistent, denoted by the really tight confidence bounds that we see.

In comparison, the Q-learning agents are, so for the one where we have fixed exploration, it's fairly good and is able to learn where the reward

is located, but there is some deviation denoted by that 10% of selecting a random action.

Whereas the Q learning where we have epsilon is equal to one decaying to zero, the performance isn't the greatest.

And for the null model, the active inference where there is no reward here, the agent randomly goes to the hole and randomly goes to the goal 50% of the time.

But the key thing to note is that apart from the non-model, all the models are doing fairly well and seem to be performing okay within this stationary setting.


SPEAKER_02:
So the next thing... Noor, could I just make a point about these experiments as well, just because they also do VRA study from the traditional RL assessment, which is...

Yeah, great.

So basically, like usually in RL, what happens is you kind of have this ambiguation between like training time and test time performance.

And you usually, especially in something like queue learning, like you just hit the max over the queue function, or you have a policy that tries to do like some search over the queue function and takes that max, like given a state.

But there's kind of an artificial distinction, which is

Like, obviously, as you're acquiring data, you're making mistakes in the environment, you're interacting with the real environment.

So in order to kind of make that fair comparison between here and active inference, where you don't really have this distinction between like training time and test time, it's all just interaction.

That's the reason why the Q learning agent, like especially when Epsilon is fixed to say 0.1 never achieves the optimal policy simply because

we're also with 0.1 probability taking a random action.

So we're not making this distinction like you sometimes would with normal RL between train and test time.

We're all saying train and test time are basically the same thing.

So however you're choosing to interact with the world is how you should be assessed.

So that's why initially looking at these codes, you might be like, hang on, why isn't Q-learning solving this?

But yeah, that's just to clear something up if you're more familiar with, say, some of the deep RL experimental procedure.


SPEAKER_03:
Perfect.

Thank you.

Okay.

And then just following on from that, so we changed the environment a little bit to make it a little bit more difficult to see whether the Bayesian and the active inference agents might struggle when we start having the reward location change after every few episodes.

So specifically, we swapped the goal and the whole location at time point 21, at time point

121, 141, 251, and 451.

So you can see that in these figures where the gray lines are shown.

So at these points, the rule location flipped.

So like the stationary setting for the first 20 trials, all the agents seem to be doing as you would expect.

So both the Bayesian RL and the active inference agents are doing

fairly okay.

The queue learning with the fixed exploration set to 0.1 is doing fairly okay, as we saw before.

And the queue learning with the decaying exploration is doing as it was doing before.

But when you flip it around, such that the goal locations change, what you notice is that with the Bayesian RL agent, the

the amount of reward or the score it gets is quite low.

And then we see a phase where it then transitions and becomes to the optimal policy again.

In comparison to the active inference agent, where it instantly after the first trial of doing the incorrect is able to switch over to the active inference, sorry, the

appropriate policy and the reason for that is for these uh RL settings where we're considering it as a learning problem you need to first do reverse and learning of where the reward location is and then we learn the new reward location so you're seeing that for Q-learning for both the different epsilon greedy parameterizations and also for the Bayesian RL and we're seeing that consistently whereas for the active inference agent because we're treating it as a planning as an inference problem where the

posteriors from the previous state are moved over to the priors.

The agent is able to instantly realise that the current policy that I was following, this time step is inappropriate and switches its policy to the other one.

Again, the non-model, as expected, doesn't really do much because it's just exploring, doesn't really care where the reward or the whole location are.

OK, Phil, do you want to add something to this or?


SPEAKER_02:
No, I think the same point applies to this one.


SPEAKER_03:
Yeah, definitely.

OK.

So just to wrap up with these two comparisons, with the stationary setting, all types of agents, including the Bayesian RL and the Q-learning, would be reasonable frameworks to use.

Whereas with the non-stationary stochastic setting, having an active inference agent might be an appropriate way of having a change in dynamics.

The key caveat with that is that you can introduce a lot more additional complexity into the Bayesian RL or the Q-learning or the RL framework in general to allow for a way to handle uncertainty, but it wouldn't be a natural way of adding it.

You would have to sort of augment the function or the algorithm in particular ways to justify it.

Okay, so once we had done this comparison, we were interested in, I guess,

Why would you even want to use active inference when you don't have an understanding about the world or you don't have a preference over the type of things that could be done?

Because as we saw, the active inference model, the null model, is just exploring.

It's not really doing much.

And that brings us to one of the initial points that I introduced at the start of the presentation, which is that within an active inference framework, we don't really care about having a reward function that we can learn all

we can learn that based on some interaction with the environment.

So for that, what we did was we carried out a few different simulations to see how the active inference agent can select different types of policies in the absence of prior preferences.

And for this, we did three different experiments where we allowed either or the likelihood and all the outcome preferences to be learned.

over time and saw how the agent interacted when this learning over the different preferences took place and the way we did it was using the conjugacy models where because these are discrete state models we have categorical distributions over our model parameters and we're introducing durational distributions on top as our hyperpriors and learning those hyperpriors

We can go into the details of this if anyone has any questions, but just for simplicity, just assume that all the Dirichlet distributions were set to flat, completely flat, and the agents were all given an opportunity to learn based on different interactions they had.

So the first set of simulations that we ran were to understand how the agent would reduce its uncertainty about the

the environment that it's interacting with.

So if it doesn't know the frozen lake, how would it interact or explore that lake?

And what we see is that in this particular instance, the agent was just interested in exploring.

It didn't particularly care about where the goal or the whole location was.

And this is highlighted in a series of different exploration

trajectories that we see.

So the first one was where the agent falls in the hole, and then another one where it goes around and ends up in the second episode at the goal location, and in some where it just ends the episode by just going back and forth.

So this is just exploration, nothing more to hear.

The next set of analysis or simulations that we ran was to see what would happen if the agent knew about the world, but had no preferences of the type of outcomes they expected itself to be in.

And for this, we ran multiple different simulations and saw that in the absence of

any sort of preferences, the hole could actually become really attractive if it's encountered first.

So we see that in the first figure where the agent learns to prefer to hide in holes.

And in the second type of trial that we saw was where the agent exhibited the preference to actually go to the goal location.

So this is entirely dependent on the instantiation or the type of stimulus that the agent is exposed to initially.

determines the type of preferences it would learn to have.

Okay and then the last set of simulations that we ran was just to check what would happen when we interacted with the epistemic imperative search to resolve uncertainty about the environment that the agent was interacting with, specifically the likelihood mapping between the outcomes given the states and the uncertainty about the

desired state of affairs that the agent expected itself to be in.

What we saw was that if we allowed a sufficient number of trials to pass, the agent in this particular setting learned to prefer to hide in holes after X number of episodes, but it had a very distinct preference over the type of outcomes it expected itself to be in depending on the particular time point.

So that brings me to the last simulation.

So I'm just going to wrap up, which is in active inference, I started the presentation with saying it's a particular algorithm that gives us very nice things to consider when we're operating in a Bayesian or a

belief-based setting, which is firstly the principal account of epistemic exploration and intrinsic motivation that we get from the particular expected free energy decomposition that we went through.

The second thing that I wanted to highlight was that under an active inference setting, we don't have to explicitly specify a reward function, which we saw in the second set of simulations, where the agent can also learn its own reward and prefer to become something that is quite counterintuitive.

from an RL setting where the signal from the environment is saying something is bad, but the agent's internal motivations of preference allow it to then do something that's quite at odds with what the environment is expecting it to do.

And lastly, because of this belief, Bayesian setting uncertainty is a natural part of the belief updating.

So within the stationary settings, active inference agents perform as well as reinforcement learning agents.

However, in non-stationary settings, they outperform due to their ability to carry out this planning as inference.

And I think that holds, this is not a conclusive statement, but it's a nice way to start thinking about how

If you're scaling up active inference agents to interact on the same type of environments as reinforcement learning agents, that might be a little bit hard to resolve because there is some non-stationarity or some weird fluctuations happening in the environment and active inference agents could potentially perform quite nicely if we have this planning as inference set up.

And that brings me to the end of the presentation.

So I just want to thank everyone who was involved with this work, so Carl, Tom, and Phil, and everyone who's helped me think through these interesting ideas that I presented.

And everyone also for listening.

Thank you.


SPEAKER_01:
Thank you.

Awesome talk.

So you can maybe unshare, and we can ask a few questions.

And if anyone who's watching live would like to ask questions, they're more than welcome.

So maybe I'll start with just a general point that it was awesome to see how clearly you differentiated between the reinforcement learning and the active inference paradigm.

And just one question I had was, you mentioned that in the case of a time step of one, a time horizon of one,

there was like an equivalence between the reinforcement learning approach and the active inference approach.

Now people are using reinforcement learning to do planning for the future amidst uncertainty.

So how are they accomplishing those kinds of planning and where are some situations where active inference might be able to step into those settings and potentially do better?


SPEAKER_03:
I think that's already a good question and feel free to jump in if I missed something.

So within the RL setting, from what I know, if they are considering temporal horizons greater than one, then they have hierarchical RL or they have options where they're considering trajectories that are greater than one that allow them to consider a whole sequence of play

um before they like um that's rolled out if they select a particular policy instead of like that actually one single action to state mapping um but i i'm not i haven't really worked with options that much maybe phil do you know um yeah so options are like one way to achieve this kind of multi-step sort of um like


SPEAKER_02:
I guess oversize almost like this inductive bias, right?

That you will like your choices will be kind of contiguous blocks of like more than one step.

Whereas the kind of policy kind of, it's just like, I'm going to take one step and then I'll have this kind of next state I exist in.

Um, but I think what's important to disambiguate is actually you can do sort of planning the model style things because you can view active inferences as sort of model based, like in the kind of, you know, RL paradigm and indeed like within model based, like you can,

have planning methods similar to what is done in active inference.

However, I guess the key distinction here is when you do planning in model-based RL, you're simply just trying to take some sort of mini evolutionary method, usually something like the cross-entropy method.

And what you're doing is this kind of search over all actions that you take at a certain point in time that maximizes your kind of reward over, say, a horizon of, say, 20 steps.

And you can do some clever stuff.

You can terminate with a Q value function.

But fundamentally, like, it doesn't take you away from this sort of definitional difference, which is in active inference, you're doing this sort of planning over actions, even though they might seem superficially similar, like kind of

basically drawing lots of actions and seeing which ones maximize some sort of utility.

In the case of Active Inference, all the useful information and the desiderata about exploration and exploitation are wrapped up in that maximization, whereas in RL, it's a bit harder to understand.

In the classical sense, we're just trying to maximize reward.

but um you can have heuristics where you say oh but maybe i want to also maximize some notion of model uncertainty and you know it kind of gets a bit um more difficult to naturally integrate all these approaches into the same thing um because like in the example of active inference because everything's a distribution you just put a hyper prior over if you want to you just integrate it out if you don't want to deal with kind of tuning it so yeah that's kind of where i feel like there is this kind of difference


SPEAKER_01:
Cool.

And what kinds of settings do you think that those types of action as inference and planning as inference could be utilized?

Like what kind of data sets or questions or contexts are people currently using another type of method, but then you're excited to see active inference play a role?


SPEAKER_03:
I think mostly open-ended problems where you don't really have a reward function.

I think because the way the RL agent is learning how to interact with the environment is through a reward function.

So anything where you don't really have that or you have an environment that is changing.

But I know that there's a whole host of people within the RL community working on intrinsic motivation or internal motivation.

So those sort of things do overlap with the active inference formulation.

But particular paradigms, I think, for me, the more interesting aspect of active inference comes from the fact that when you start thinking about

biological agents and if you're modeling a patient or someone with schizophrenia you can with this Bayesian framework you can change the priors to try and see how the person is interacting where I guess it's that it could be that their policy like the way they're valuing their policies are broken or it just could be other parts that are different but I think within the RL setting if you

are stepping outside the, um, the standard, the game mode, right?

So like magic core gym environments and start going into ones which are more open-ended and you don't have any rewards, then active inference could be potentially useful.

But I'm a little bit hesitant to just say whether it will be better because if you start augmenting Bayesian RL with all sorts of interesting components, then

to a certain extent, it will be active inference scaled up, which is potentially a contentious point.

But I think it depends on exactly what you are incorporating.

And that's why for this particular presentation and our work, we were quite careful in defining what reinforcement learning meant, which is that you have to have this reward function in play and you want to maximise this reward function.

and anything you include into the RL framework has to have that, that objective has to be there.

But if you sort of bypass them, say, okay, I'm just going to add in all sorts of interesting components to make the algorithm or the way the agent interacts with the environment, I guess, as similar to an active inference framework, or perhaps even better, then you don't, that framework

find distinction between where RL is better or where active inference is better, sort of, it's not really there for me.

Because I think both communities, from my perspective of working for similar things, which is sequential decision making.

And with our work, it's mostly with sequential decision making in the face of uncertainty, whereas some of our work might, RL work might not be focused on that.

So I think it's when you start

drawing the boundaries, it becomes a little bit hazy where things are separate or not.

I think I went off on a little tangent, but Phil, do you want to add anything to that in terms of the environments and the paradigms that might be useful?


SPEAKER_02:
Yeah, I think if one of your aims is, say you're given an environment that you just have no prior knowledge about how you ought to behave,

you could argue that you could deploy an rl agent that uses some sort of curiosity or epistemic uncertainty reduction mechanism but i mean i know there is like a tiny bit of work about learning priors over reward functions and learning those but i'm not hugely like aware but

I think what's important to understand is in the limit of like exploring the entire environment, your epistemic uncertainty is going to go to zero, right?

Like you will have observed everything and then it's unclear what your R relation is going to do at that point, especially you say you have a deep neural network that parameterizes how you take actions.

Whereas in the case, and I think what this paper was really interesting for me to see when we ran these experiments was actually you just put a prior over your prior preferences and eventually you learn a mode of behavior.

It may not be optimal, but your agent eventually learns to adopt a behavior that is self-fulfilling because it reduces epistemic uncertainty.

and then all that's left is says well i think these are kind of useful behaviors or at least these are behaviors for me to do in the world and eventually you get quite a repetitive behavior and it might be a more kind of accurate simulation of in the absence of any information like how something intelligent might actually behave in a world whereas in the rl paradigm it's less clear you know once you reduce all that epistemic uncertainty about where the value is and you haven't found any like what is your agent really doing at that point but i think my problem at the moment with like the active information


SPEAKER_03:
formulation is that this works on like a discrete state formulation, right?

We saw really nice results in that setting.

But I think if you scale it up, active inference agents are going to have the same issues if you're using like amortized inference or something to actually approximate your likelihood or transition functions, which means that you might not get these nice properties that we're seeing at this small scale.

So scaling it up is like a

It's an interesting and open-ended problem at the moment, scaling up in the right way that you can include these conjugacy models that we have in the discrete state formulation.

And that's how we do the last set of simulations where we introduce the conjugate priors to do the learning over the prior preferences, but also the likelihood.

So it becomes a little bit hazy how to learn, but I can have a hyper prior over an entire neural network if you're scaling it up that way.

Um, so they must like a lot of work needs to be done that area.

And in order to actually show that it's appropriate, um, that you can have these interesting components, then you need to start thinking about how you would include these hyper priors.

Uh, because it is, it's not reasonable to say that you're going to have a hyper prior over the, the parameter space, such like the, the beta

hyper prior over the gamma, that won't be sufficient in those settings.

Or hyper prior over the way the agent is selecting its actions.

It has to be over the model parameters.

And if scaling up means that you're losing that nice way to disentangle the particular model parameters, then it becomes very uncertain.

I don't know.

It's an open-ended problem for me.


SPEAKER_02:
Yeah, I think high-dimensional problems

still represent something that is relatively difficult, especially just due to the further we stray from Bayes, the less principled it becomes.

And it's quite a fine line.


SPEAKER_01:
very interesting about how whether within the rl or active inference paradigm there's sort of the sparse skeleton the bare bones at the core and then sometimes these other layers or tweaks are needed and um

Pretty interesting to learn about.

And also what you had said earlier, Noor, about how the challenge is planning for sequential action when you're in feedback, whether just by moving around an environment, so your local environment changes, or you're playing a game where the board game's gonna change, or you're trading on a market, sequential action, you can't just plan steps one through 100 without at least thinking about some what ifs.

And then only having access to limited observational data and planning amidst fundamental uncertainty.

So I think a lot of those points of contact with the motivations of reinforcement learning and machine learning will maybe bring some more light into active inference and

push some of those frontiers you just mentioned.

So I have a question and anyone else in the chat can ask a question too.

Let's say somebody is wanting to learn about this and they're actually in a lucky beginner's mind perspective because they might not have been

enticed by learning reinforcement learning but they've gotten curious in active inference and excited by your presentation and so what what kinds of computer languages or skills might they want to learn or what kinds of approaches or mindsets would be helpful if somebody let's say weren't coming from a classical machine learning perspective and learning active inference but rather kind of upskilling into active inference what would you recommend either of you to them


SPEAKER_04:
Phil, do you want to go first with that?


SPEAKER_02:
Yeah, I mean, I think it's an interesting way to view a kind of this kind of potential person, because I kind of felt like I was somewhat like this person, like way back when, like, Nora and I were starting to have these conversations about active inference.

And I, like, this paper started off basically as a tutorial I was writing, having spent two, three months kind of in the evenings reading about trying to like sift through

the active inference literature.

And I think it's fair to say that at times it's unapologetically dense and quite difficult to read.

So without trying to self-endorse, but I do think reading this manuscript in particular, the whole aim philosophically when we started writing this was to really understand what is happening here.

What is this kind of expected free energy quantity?

Why do we care about it?

know from a kind of theoretical perspective i think this is a very lucid presentation of the concept so at least you can get some sort of intuition as to what's happening um as for the kind of coding side i can't really speak to that but i'm sure i'm sure nor kind of has has worked it works with it quite a bit um i i was going to say it sort of depends on what the primary objective of the person is is it for a sort of get an understanding of the


SPEAKER_03:
the high-level conceptual ideas or treat it as an algorithm.

Because if you're going, coming from free energy principle to active inference is a different story, or if you're taking active inference as a siloed algorithm for a specific kind of sequential decision-making scheme, right?

So depending on that, it sort of differs.

But as Phil was saying, I think this paper definitely is really nice in the sense like,

it does try and define all the different concepts and goes through the different formulations, maybe not in as much detail with the assumptions in play.

It gives you the layout of how you might be able to derive it.

But certain things like what the approximate density even really entails, those are very difficult questions where you would have to dive into

variational inference literature to understand.

So I guess from that perspective, someone who's coming into the field should spend some time thinking about variational inference and how that really ties back to the active inference formulation.

Because the perception part of active inference is, in most instances, exactly the same as variational inference literature and optimizing the model evidence or the

maximising the evidence law bound.

So the second thing that I was going to add is that the paper with Lancelot da Costa is really good for someone who wants to drill down into deriving everything themselves.

Sometimes it's super technical.

So the paper that I walked through today, the one with Phil and Tom and Carl,

is it's a nice introduction for someone who's not familiar with the mathematics and just wants to get a layman's summary.

Whereas the paper with Lance's first author gives the detailed derivations and a lot of the assumptions in place.

So that's from understanding the theory part.

From the coding perspective, it entirely depends what the end objective is.

So if someone wants to work with

discrete state formulations then the MATLAB code Carl has written and it's years of work with lots of nice simulations and examples that you can use and also our code is online and you can access it so there's a link in the paper in the software

section that gives where exactly the code is.

And you can look through that and see how the simulations were done.

If someone is interested in more, I guess, high dimensional formulations of active inference, there's some recent work with Zephyrus.

So Zephyrus as first author, where we've got a nice, again, we have a Git reaper for that work as well, which

gives a breakdown of how you would implement a simple active inference agent using very short encoders and a simple transition network.

So there's lots of like different areas, but for someone starting out, they sort of have to understand whether they want to focus on the theoretical side or whether they want to focus on the implementation side, theoretical side,

would be drilling down into the variational influence and the maths behind it and if they want to focus on the coding aspect then they want to figure out whether it's continuous or discrete state formulations they're interested in and then sort of break down into if it's continuous then most of it would either be writing the coordinates of motions themselves or they would have to use like some sort of like

a neural network to approximate that continuous distribution of interest or the discrete state which Carl's written out.

And if they have questions about the discrete state, I'm happy to take emails as well.

So if anyone has, or the continuous state space as well.


SPEAKER_01:
Thanks for that distinction.

And it's such a large difference between the MATLAB code, which we got to walk through with Ryan Smith and Christopher White, which is doing matrix multiplication and such.

And then here comes the neural networks.

And it's offering sounds like new opportunities with high dimensionality and continuous variables, but also a lot of new challenges.

what is the essence that's shared by the matrix form and by this more um machine learning style

because for some people it might be splitting a hair quite literally the difference between two computer languages when they're thinking about active inference from a ecological psychology or in an active philosophical or an embodied performance perspective, all backgrounds that converge on active inference.

And so to somebody outside the strand of hair,

What is it that we can really distill that is core active inference?

And I wrote down a few things that you had said.

What are those core pieces that allow us to dive into the matrix mode with MATLAB or into the neural network mode with Python?


SPEAKER_03:
So I think it comes down to my summary slide, the way I think about it.

So the active inference, the core ingredients are formulating the gender model.

And here, formulating the gender model is a parameterization of the gender model.

So either you can use the discrete state categorical distributions, or you can use a more continuous state formulation.

And again, the neural network formulation is a specific instantiation of that.

That would be one way of sort of differentiating that.

The second one is the optimization of the objective functions in play.

So in the MATLAB code, we're doing gradients of send using mean field message passing algorithm.

So a specific formulation that's been introduced in a couple of papers.

And I specifically didn't walk through that.

Or you're doing back propagation to actually

calculate or learn the distributions and then you're just solving those distributions that you have.

So it sort of depends on which formulation you're... It depends on how you optimize those objectives.

Either you're like taking the implicit forward model or you're taking an explicit gender model.

And one thing I forgot to mention is that Alex Shams and Connor Hines, they've been working on a

discrete state formulation of active inference in Python, which might be of interest for people who want to focus on one specific language that can do the more high-end or high-dimensional stuff, and the discrete state formulation that Carl has.

I know they're also looking for people to work on the code base if anyone's interested.

So it's called InferActivity, I think, but that's on

GitHub as well if anyone's interested.


SPEAKER_00:
Cool.

Any sort of last thoughts or comments from either of you?


SPEAKER_03:
No, I think I'm okay.

Phil, what do you reckon?


SPEAKER_02:
I think we're kind of noticing a bit of a...

An increase in interest in active inference, like just to give you an example, like maybe areas where even active inference would have been considered before, like robotics, you're now beginning to see more and more of it.

So I think if you do want to get involved in it now is a particularly good time.


SPEAKER_01:
great call, Philip, and I'll just re-recommend the excellent paper that we're discussing.

It's in the description of this video.

Really appreciate both of you for joining.

You're always welcome to come on to be speaking about a paper you've authored or not.

But again, thanks a lot to Nora and Philip, and I hope to see you again on a future Active Inference stream.


SPEAKER_03:
Thank you.

Thank you for inviting us.

Bye-bye.


SPEAKER_01:
See you.

Peace.

Bye-bye.

Awesome.

Stop the stream.

Great conversation.

Thanks a lot to Philip and Noor, really.
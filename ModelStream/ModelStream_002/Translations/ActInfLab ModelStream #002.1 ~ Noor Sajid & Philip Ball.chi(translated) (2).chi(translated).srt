1
00:00:07,839 --> 00:00:08,400


2
00:00:08,400 --> 00:00:11,120
大家好，歡迎大家來到活動

3
00:00:11,120 --> 00:00:12,160
推理實驗室，

4
00:00:12,160 --> 00:00:15,759
這是

5
00:00:15,759 --> 00:00:18,960
2021 年 4 月 16 日的模型流編號 2.1

6
00:00:18,960 --> 00:00:20,720
，今天將是一個很棒的

7
00:00:20,720 --> 00:00:22,400
模型流，我們將

8
00:00:22,400 --> 00:00:24,400
簡要介紹一下

9
00:00:24,400 --> 00:00:25,920
自己，然後

10
00:00:25,920 --> 00:00:27,920
我會提到如何 會議

11
00:00:27,920 --> 00:00:30,000
將在今天舉行，然後我們會將其傳遞給

12
00:00:30,000 --> 00:00:33,200
noor 進行演示，所以我是 daniel

13
00:00:33,200 --> 00:00:35,120
，我是加利福尼亞的博士後研究員，

14
00:00:35,120 --> 00:00:36,719
我將傳遞給

15
00:00:36,719 --> 00:00:39,040
philip

16
00:00:40,000 --> 00:00:42,800
嗨，是的，我目前是牛津大學的博士生

17
00:00:42,800 --> 00:00:43,280


18
00:00:43,280 --> 00:00:46,719
我的第二年，是的，我想這

19
00:00:46,719 --> 00:00:48,320
是我在開始

20
00:00:48,320 --> 00:00:49,440
攻讀博士學位之前

21
00:00:49,440 --> 00:00:52,640


22
00:00:52,640 --> 00:00:53,600
所做

23
00:00:53,600 --> 00:00:55,600


24
00:00:55,600 --> 00:00:57,280


25
00:00:57,280 --> 00:00:58,960


26
00:00:58,960 --> 00:01:00,879


27
00:01:00,879 --> 00:01:03,520
的工作 對解決這類

28
00:01:03,520 --> 00:01:06,239
研究

29
00:01:06,240 --> 00:01:09,360


30
00:01:09,360 --> 00:01:12,000


31
00:01:12,000 --> 00:01:13,840
問題很有用

32
00:01:13,840 --> 00:01:15,200


33
00:01:15,200 --> 00:01:16,159


34
00:01:16,159 --> 00:01:18,799


35
00:01:18,799 --> 00:01:19,920


36
00:01:19,920 --> 00:01:22,720
嗯，所以我在汽車監督下的博士專注

37
00:01:22,720 --> 00:01:23,920
於

38
00:01:23,920 --> 00:01:26,000
與適應有關的這些想法，

39
00:01:26,000 --> 00:01:27,759
我今天將重點關注其中之一，即使用主動推理

40
00:01:27,759 --> 00:01:29,759
在非平穩環境中進行行為適應，

41
00:01:29,759 --> 00:01:32,000


42
00:01:32,000 --> 00:01:35,119
非常感謝您的

43
00:01:35,119 --> 00:01:38,159
加入和我們的演示文稿 我將

44
00:01:38,159 --> 00:01:38,720


45
00:01:38,720 --> 00:01:41,280
聽到一個誰知道

46
00:01:41,280 --> 00:01:42,720
nor 的演示文稿

47
00:01:42,720 --> 00:01:44,399
，然後我

48
00:01:44,399 --> 00:01:45,759
將從聊天中編譯問題，

49
00:01:45,759 --> 00:01:48,240
所以請在他們

50
00:01:48,240 --> 00:01:48,880
來找你時輸入問題

51
00:01:48,880 --> 00:01:51,119
，然後我們將在最後解決它們，

52
00:01:51,119 --> 00:01:53,200
再次感謝和 nor 請

53
00:01:53,200 --> 00:01:56,479
把它拿走完美謝謝你，

54
00:01:56,479 --> 00:01:58,880
嗯，所以今天我將展示一些

55
00:01:58,880 --> 00:01:59,520


56
00:01:59,520 --> 00:02:01,600
我與菲利普合作

57
00:02:01,600 --> 00:02:03,119


58
00:02:03,119 --> 00:02:06,399


59
00:02:06,399 --> 00:02:08,800


60
00:02:08,800 --> 00:02:12,720


61
00:02:13,599 --> 00:02:15,440
完成的工作 好的，所以

62
00:02:15,440 --> 00:02:17,440
演示文稿的結構如下

63
00:02:17,440 --> 00:02:20,480
，你

64
00:02:20,480 --> 00:02:22,160


65
00:02:22,160 --> 00:02:23,840


66
00:02:23,840 --> 00:02:26,000
能聽到屏幕嗎？

67
00:02:26,000 --> 00:02:28,640


68
00:02:29,920 --> 00:02:34,480
我告訴

69
00:02:34,480 --> 00:02:36,800
你我們走了，我會剪掉它，所以去吧，

70
00:02:36,800 --> 00:02:38,720
謝謝

71
00:02:38,720 --> 00:02:40,879
完美謝謝你，所以演示文稿的

72
00:02:40,879 --> 00:02:42,239
結構以及

73
00:02:42,239 --> 00:02:44,080
首先我將簡要介紹問題

74
00:02:44,080 --> 00:02:45,280
設置並提供今天正在考慮

75
00:02:45,280 --> 00:02:47,360
的特定主動推理

76
00:02:47,360 --> 00:02:49,920
實例化的詳細信息

77
00:02:49,920 --> 00:02:51,519
，即 離散狀態空間

78
00:02:51,519 --> 00:02:52,319
設置

79
00:02:52,319 --> 00:02:54,160
和演示文稿的後半部分

80
00:02:54,160 --> 00:02:55,599
將集中在一些

81
00:02:55,599 --> 00:02:57,440
特定示例上，

82
00:02:57,440 --> 00:02:59,360
將主動影響公式與

83
00:02:59,360 --> 00:03:00,720
強化學習進行比較，

84
00:03:00,720 --> 00:03:03,599
特別是 q 學習和

85
00:03:03,599 --> 00:03:04,959
基於貝葉斯模型的

86
00:03:04,959 --> 00:03:07,519
算法，然後我要做的

87
00:03:07,519 --> 00:03:08,959
是提供一些面子

88
00:03:08,959 --> 00:03:10,640
為什麼你甚至想要使用主動推理的特定方面的有效性

89
00:03:10,640 --> 00:03:14,080


90
00:03:14,080 --> 00:03:18,080
好吧，嗯，什麼是主動推理，

91
00:03:18,080 --> 00:03:20,480
嗯，這是

92
00:03:20,480 --> 00:03:21,599
關於生物

93
00:03:21,599 --> 00:03:23,920
或人工代理如何在

94
00:03:23,920 --> 00:03:26,239
動態非靜止環境中運行的首要原則，

95
00:03:26,239 --> 00:03:28,560
它規定這些代理

96
00:03:28,560 --> 00:03:30,000
為了維持 體內平衡

97
00:03:30,000 --> 00:03:31,680
存在於吸引狀態，

98
00:03:31,680 --> 00:03:34,560
這些狀態最小化它們的熵或驚喜，

99
00:03:34,560 --> 00:03:36,239
所以如果你採取 這個特殊的例子

100
00:03:36,239 --> 00:03:38,000
，我們看到這個過去的

101
00:03:38,000 --> 00:03:41,280
小飢餓代理打開冰箱

102
00:03:41,280 --> 00:03:42,879
的工作方式就像你

103
00:03:42,879 --> 00:03:44,959
需要知道為什麼要正確打開冰箱，

104
00:03:44,959 --> 00:03:46,480
所以你想

105
00:03:46,480 --> 00:03:47,280


106
00:03:47,280 --> 00:03:50,080
在家里或外面吃飯之間做出一個特別的選擇，

107
00:03:50,080 --> 00:03:51,120
為了做到這一點，您

108
00:03:51,120 --> 00:03:53,200
必須決定什麼是最佳

109
00:03:53,200 --> 00:03:55,040
行動，可以讓您解決

110
00:03:55,040 --> 00:03:56,959
自己對當前

111
00:03:56,959 --> 00:03:58,560
事務階段的不確定性

112
00:03:58,560 --> 00:04:00,799
，嗯，這將幫助您

113
00:04:00,799 --> 00:04:02,480
決定是要在家做飯

114
00:04:02,480 --> 00:04:04,400
還是要走路 到餐廳

115
00:04:04,400 --> 00:04:06,720
和這個特定的實例，這

116
00:04:06,720 --> 00:04:08,560
導致代理打開冰箱以

117
00:04:08,560 --> 00:04:11,760
檢查家裡是否有食物，

118
00:04:11,760 --> 00:04:13,519
而主動推理的好處

119
00:04:13,519 --> 00:04:15,519
在於它允許您

120
00:04:15,519 --> 00:04:16,959


121
00:04:16,959 --> 00:04:19,358
通過指定以下方式以更正式的方式思考這些問題設置

122
00:04:19,358 --> 00:04:20,639
最佳行為

123
00:04:20,639 --> 00:04:23,199
是通過評估證據

124
00:04:23,199 --> 00:04:24,639
來

125
00:04:24,639 --> 00:04:26,639


126
00:04:26,639 --> 00:04:28,080


127
00:04:28,080 --> 00:04:30,400


128
00:04:30,400 --> 00:04:31,520


129
00:04:31,520 --> 00:04:33,360
確定的 我們要做的只是專注於支持

130
00:04:33,360 --> 00:04:35,280
主動推理的過程理論，

131
00:04:35,280 --> 00:04:37,199
而不是

132
00:04:37,199 --> 00:04:38,240
通過生物學來

133
00:04:38,240 --> 00:04:41,199
討論主動

134
00:04:41,199 --> 00:04:44,240
推理消息傳遞方案的

135
00:04:44,240 --> 00:04:47,280
神經合理性

136
00:04:47,280 --> 00:04:48,479


137
00:04:48,479 --> 00:04:50,560
與

138
00:04:50,560 --> 00:04:52,000
一般的強化學習

139
00:04:52,000 --> 00:04:52,800
算法相比，

140
00:04:52,800 --> 00:04:54,720
我們首先需要

141
00:04:54,720 --> 00:04:56,080
理解的是，通過主動

142
00:04:56,080 --> 00:04:57,759
推理，有對

143
00:04:57,759 --> 00:04:59,600
純信念方案的承諾，

144
00:04:59,600 --> 00:05:01,600
這意味著獎勵函數

145
00:05:01,600 --> 00:05:03,520
不一定總是

146
00:05:03,520 --> 00:05:06,000
不必要的，

147
00:05:06,000 --> 00:05:07,360
因為您將擁有的任何策略

148
00:05:07,360 --> 00:05:10,240
都有 認知價值即使在沒有偏好的情況下

149
00:05:10,240 --> 00:05:11,680
，

150
00:05:11,680 --> 00:05:14,000
額外的主動推理代理

151
00:05:14,000 --> 00:05:16,400
也可以學習自己的獎勵功能

152
00:05:16,400 --> 00:05:19,120
，這有助於代理描述

153
00:05:19,120 --> 00:05:20,400


154
00:05:20,400 --> 00:05:23,199
它期望自己看到的行為類型，

155
00:05:23,199 --> 00:05:25,120
而不是它會

156
00:05:25,120 --> 00:05:28,000
從環境中得到的東西，這些

157
00:05:28,000 --> 00:05:29,600


158
00:05:29,600 --> 00:05:30,800
與強化學習相比，有

159
00:05:30,800 --> 00:05:33,039
兩點非常重要 rning，因為在

160
00:05:33,039 --> 00:05:35,680
標準 rl 設置下，獎勵函數

161
00:05:35,680 --> 00:05:38,160
uh 將定義代理

162
00:05:38,160 --> 00:05:39,360


163
00:05:39,360 --> 00:05:41,520
在特定環境設置中如何交互或行為，

164
00:05:41,520 --> 00:05:42,560
但是

165
00:05:42,560 --> 00:05:44,080
首先定義獎勵

166
00:05:44,080 --> 00:05:46,000
函數非常困難，因為

167
00:05:46,000 --> 00:05:48,160
它假設有一個特定的

168
00:05:48,160 --> 00:05:49,520
信號

169
00:05:49,520 --> 00:05:51,919
來自 環境對代理來說可能是

170
00:05:51,919 --> 00:05:53,600
一致的好或壞

171
00:05:53,600 --> 00:05:56,160
，

172
00:05:56,160 --> 00:05:56,800


173
00:05:56,800 --> 00:06:00,400
在真實環境中不一定會成立，這些

174
00:06:00,400 --> 00:06:02,319
環境信號可能會根據環境而變化

175
00:06:02,319 --> 00:06:03,919


176
00:06:03,919 --> 00:06:05,680


177
00:06:05,680 --> 00:06:07,280


178
00:06:07,280 --> 00:06:09,919
這可能會讓你變得更糟，

179
00:06:09,919 --> 00:06:11,199
這就是為什麼首先

180
00:06:11,199 --> 00:06:12,800
構建這些獎勵

181
00:06:12,800 --> 00:06:15,199
函數非常困難

182
00:06:15,199 --> 00:06:16,880
，如果你沒有以

183
00:06:16,880 --> 00:06:19,360
適當的方式構建它們，即使使用 rl 設置，

184
00:06:19,360 --> 00:06:21,680
它可能會導致

185
00:06:21,680 --> 00:06:24,080
你的代理的次

186
00:06:24,080 --> 00:06:26,000
優行為

187
00:06:26,000 --> 00:06:27,520
從某種意義上說，主動推理確實很好，因為我們實際上是在

188
00:06:27,520 --> 00:06:29,680
替換或繞過

189
00:06:29,680 --> 00:06:31,360
傳統的獎勵函數 你

190
00:06:31,360 --> 00:06:31,919
會

191
00:06:31,919 --> 00:06:34,400
在 rl 環境中對首選結果有先驗信念

192
00:06:34,400 --> 00:06:34,960
，

193
00:06:34,960 --> 00:06:36,880
所以

194
00:06:36,880 --> 00:06:39,039
你希望看到自己所處的那種期望的事態

195
00:06:39,039 --> 00:06:40,560


196
00:06:40,560 --> 00:06:43,840
，這

197
00:06:43,840 --> 00:06:47,039
在沒有獎勵或

198
00:06:47,039 --> 00:06:48,400


199
00:06:48,400 --> 00:06:50,479
對獎勵或

200
00:06:50,479 --> 00:06:52,639
偏好的理解真的不准確的環境中變得很重要 設置應該看起來像

201
00:06:52,639 --> 00:06:55,360
，在這種情況下，在標準的

202
00:06:55,360 --> 00:06:56,319
主動影響

203
00:06:56,319 --> 00:06:58,160
離散狀態公式中，我們可以

204
00:06:58,160 --> 00:07:00,319
做的是學習這些首選結果的經驗先驗

205
00:07:00,319 --> 00:07:01,599
分佈

206
00:07:01,599 --> 00:07:05,280
嗯和內在的呃

207
00:07:05,280 --> 00:07:07,199
對不起那個代理的內部獎勵函數

208
00:07:07,199 --> 00:07:07,599


209
00:07:07,599 --> 00:07:10,400
，這種讓我喜歡

210
00:07:10,400 --> 00:07:11,120


211
00:07:11,120 --> 00:07:14,479


212
00:07:14,479 --> 00:07:17,919
rl 設置和主動推理之間的第一個不同的 um 概念化，因為

213
00:07:17,919 --> 00:07:20,639
在主動推理中獎勵

214
00:07:20,639 --> 00:07:22,560
沒有什麼不同，它們只是

215
00:07:22,560 --> 00:07:24,560
代理

216
00:07:24,560 --> 00:07:26,800
從環境中獲得的標準觀察，而在

217
00:07:26,800 --> 00:07:27,840
rl 中，它們非常

218
00:07:27,840 --> 00:07:32,160
有必要採取適當的行動

219
00:07:32,160 --> 00:07:35,759
um 我要

220
00:07:35,759 --> 00:07:38,960
學習第二點，我們想要

221
00:07:38,960 --> 00:07:41,280
道歉，我們想要 要做的是，積極

222
00:07:41,280 --> 00:07:43,039
影響提供

223
00:07:43,039 --> 00:07:44,080
了認知

224
00:07:44,080 --> 00:07:46,160
探索和內在動機的主要說明，以

225
00:07:46,160 --> 00:07:48,160
最小化不確定性，

226
00:07:48,160 --> 00:07:50,080
並且在 rl 設置中這也是

227
00:07:50,080 --> 00:07:51,440
非常關鍵的，因為

228
00:07:51,440 --> 00:07:53,759


229
00:07:53,759 --> 00:07:54,479


230
00:07:54,479 --> 00:07:57,759
我們在 nrl 中看到的許多新算法的整個前提是嘗試和

231
00:07:57,759 --> 00:07:58,000


232
00:07:58,000 --> 00:07:59,680
為探索和探索之間的平衡找到正確的權衡，

233
00:07:59,680 --> 00:08:02,000
那麼

234
00:08:02,000 --> 00:08:03,919


235
00:08:03,919 --> 00:08:05,599


236
00:08:05,599 --> 00:08:06,400


237
00:08:06,400 --> 00:08:08,240
如果我繼續選擇

238
00:08:08,240 --> 00:08:10,160
它從未接觸過的所有不同的冰淇淋口味，代理應該在

239
00:08:10,160 --> 00:08:10,800


240
00:08:10,800 --> 00:08:13,520
給定的時間點採取哪些正確的行動 嗯，像芥末

241
00:08:13,520 --> 00:08:14,479
等，

242
00:08:14,479 --> 00:08:16,879
還是應該一直用這個哦，抱歉，

243
00:08:16,879 --> 00:08:17,840


244
00:08:17,840 --> 00:08:20,000
冰淇淋的味道總是

245
00:08:20,000 --> 00:08:21,120
和過去一樣，它

246
00:08:21,120 --> 00:08:23,199
真的很喜歡，比如榛子

247
00:08:23,199 --> 00:08:25,199
或花生醬，

248
00:08:25,199 --> 00:08:28,080
所以這個是 rl 的一個突出

249
00:08:28,080 --> 00:08:28,560
問題

250
00:08:28,560 --> 00:08:30,800
在貝葉斯

251
00:08:30,800 --> 00:08:32,958
框架下，主動推理自然地處理它

252
00:08:32,958 --> 00:08:36,159


253
00:08:36,159 --> 00:08:37,760


254
00:08:37,760 --> 00:08:39,760


255
00:08:39,760 --> 00:08:42,799
m，你可以

256
00:08:42,799 --> 00:08:44,560
在主動推理框架中看到的最後一點是

257
00:08:44,560 --> 00:08:46,080
，它自然地將

258
00:08:46,080 --> 00:08:47,920
不確定性作為信念

259
00:08:47,920 --> 00:08:48,560
更新

260
00:08:48,560 --> 00:08:51,839
過程的一部分，好吧，

261
00:08:51,839 --> 00:08:54,959
所以現在我已經列出了關於主動推理的

262
00:08:54,959 --> 00:08:57,200
三件事

263
00:08:57,200 --> 00:08:58,640
影響力計劃

264
00:08:58,640 --> 00:09:00,640
與 rl 相比，我將

265
00:09:00,640 --> 00:09:04,480
提供一些直覺，為什麼

266
00:09:04,480 --> 00:09:08,000
你可以原諒一些動機，

267
00:09:08,000 --> 00:09:09,760
為什麼我們甚至可以

268
00:09:09,760 --> 00:09:13,120
按照我們的方式製定積極影響力公式，

269
00:09:13,120 --> 00:09:17,040
所以，

270
00:09:17,200 --> 00:09:18,959
對不起，我剛剛意識到我喜歡跳過

271
00:09:18,959 --> 00:09:20,480
，嗯

272
00:09:20,480 --> 00:09:24,399
不 不 抱歉 很棒的介紹

273
00:09:24,399 --> 00:09:27,519
是的 非常感謝，讓我

274
00:09:27,519 --> 00:09:32,000
向下滾動到 嗯，

275
00:09:32,000 --> 00:09:35,040
好吧，所以我之前說過，通過

276
00:09:35,040 --> 00:09:36,399
積極的推理，

277
00:09:36,399 --> 00:09:38,480
它規定代理

278
00:09:38,480 --> 00:09:40,320


279
00:09:40,320 --> 00:09:42,080
通過居住在最小化驚喜的吸引狀態來維持它們的穩態，

280
00:09:42,080 --> 00:09:43,440


281
00:09:43,440 --> 00:09:44,880
所以你一定一直在想 什麼是

282
00:09:44,880 --> 00:09:46,560
驚喜，嗯

283
00:09:46,560 --> 00:09:48,800
，我們如何將驚喜定義為結果的負對數

284
00:09:48,800 --> 00:09:50,080
概率

285
00:09:50,080 --> 00:09:52,080
，為此我們引入一個隨機

286
00:09:52,080 --> 00:09:53,760
變量，它

287
00:09:53,760 --> 00:09:55,760
是對應的 ds 到代理

288
00:09:55,760 --> 00:09:57,440
收到的特定結果，

289
00:09:57,440 --> 00:10:00,480
並且這個 um 存在於所有可能結果的有限

290
00:10:00,480 --> 00:10:01,040
集合中

291
00:10:01,040 --> 00:10:03,600
，這就是 o，

292
00:10:03,600 --> 00:10:04,560


293
00:10:04,560 --> 00:10:06,720
所以我們剛剛的第一個等式

294
00:10:06,720 --> 00:10:08,160
正式說明了

295
00:10:08,160 --> 00:10:10,240
，這裡 p 表示結果的概率

296
00:10:10,240 --> 00:10:11,680
分佈，

297
00:10:11,680 --> 00:10:14,239


298
00:10:15,839 --> 00:10:19,920
嗯，好吧 因此，在主動推理

299
00:10:19,920 --> 00:10:21,200
中，代理實際上

300
00:10:21,200 --> 00:10:23,200
將我們剛剛經歷的這個意外數量最小化的方式

301
00:10:23,200 --> 00:10:24,959
是通過維護

302
00:10:24,959 --> 00:10:27,120
世界的性別模型，

303
00:10:27,120 --> 00:10:29,360
嗯，這很重要，因為在任何

304
00:10:29,360 --> 00:10:31,040
給定的時間點，代理

305
00:10:31,040 --> 00:10:32,640
不一定可以訪問

306
00:10:32,640 --> 00:10:34,000


307
00:10:34,000 --> 00:10:36,000
對世界當前狀態的真實測量，因此在

308
00:10:36,000 --> 00:10:37,600
您在這裡看到的這個特定圖形中，

309
00:10:37,600 --> 00:10:39,600
您已經獲得了環境和代理

310
00:10:39,600 --> 00:10:41,200
um 以特定方式與環境交互，

311
00:10:41,200 --> 00:10:42,320


312
00:10:42,320 --> 00:10:44,480
它被暴露在感官信號中，

313
00:10:44,480 --> 00:10:46,480
但它不知道是什麼

314
00:10:46,480 --> 00:10:49,600
o 的結果是由 o 再生的，

315
00:10:49,600 --> 00:10:51,760
所以它只能通過 o 感知自己和

316
00:10:51,760 --> 00:10:52,959
周圍的世界

317
00:10:52,959 --> 00:10:55,120
，它需要

318
00:10:55,120 --> 00:10:57,279
推斷什麼類型 狀態

319
00:10:57,279 --> 00:11:00,959
或 um 真正的原因 um

320
00:11:00,959 --> 00:11:03,279


321
00:11:03,279 --> 00:11:06,000
對它所暴露的特定感覺輸入負責

322
00:11:06,000 --> 00:11:08,959
，這就是為什麼在主動推理中，

323
00:11:08,959 --> 00:11:10,240
當我們制定問題時，我們將

324
00:11:10,240 --> 00:11:11,839
其表述為部分可觀察的

325
00:11:11,839 --> 00:11:13,519
馬爾可夫決策過程，

326
00:11:13,519 --> 00:11:16,560
因為 um 以這種方式我們 能夠

327
00:11:16,560 --> 00:11:18,959
制定一個性別模型，它定義

328
00:11:18,959 --> 00:11:19,279


329
00:11:19,279 --> 00:11:22,160


330
00:11:22,160 --> 00:11:23,920
了代理

331
00:11:23,920 --> 00:11:26,399
將用來推斷結果的內部狀態的內部分佈，

332
00:11:26,399 --> 00:11:27,920
因此他無法訪問真實

333
00:11:27,920 --> 00:11:28,399
狀態，

334
00:11:28,399 --> 00:11:30,320
但它可以

335
00:11:30,320 --> 00:11:32,000
對以下狀態做出假設或信念 可能會

336
00:11:32,000 --> 00:11:32,640


337
00:11:32,640 --> 00:11:35,519
產生一種特殊的感覺，呃結果

338
00:11:35,519 --> 00:11:38,320
空間正在暴露給

339
00:11:38,320 --> 00:11:40,800
嗯，並且使用它，代理將

340
00:11:40,800 --> 00:11:43,600


341
00:11:43,600 --> 00:11:45,839
使用反向映射過程，

342
00:11:45,839 --> 00:11:48,079
特別是貝葉斯模型

343
00:11:48,079 --> 00:11:49,680
反演來推斷真實狀態，並使這更

344
00:11:49,680 --> 00:11:52,000
具體一點 您可以做的是

345
00:11:52,000 --> 00:11:55,680
將隱藏狀態視為位置或顏色

346
00:11:55,680 --> 00:11:58,240
，例如

347
00:11:58,240 --> 00:12:00,079
，代理將暴露於的觀察空間將

348
00:12:00,079 --> 00:12:00,560
是

349
00:12:00,560 --> 00:12:03,680
嗯，例如運動的速度

350
00:12:03,680 --> 00:12:04,480


351
00:12:04,480 --> 00:12:07,440
或特定的獎勵，嗯，或者像

352
00:12:07,440 --> 00:12:10,959
他們正在暴露的一張快樂的臉，

353
00:12:10,959 --> 00:12:14,720
嗯，好吧，如果我們要

354
00:12:14,720 --> 00:12:16,320
更正式地考慮這個問題，

355
00:12:16,320 --> 00:12:19,920
那麼溫和模型是什麼？ 嗯，

356
00:12:19,920 --> 00:12:21,760
之前描述的一位紳士是

357
00:12:21,760 --> 00:12:23,040


358
00:12:23,040 --> 00:12:25,200
這個積極和朋友

359
00:12:25,200 --> 00:12:26,160
公式

360
00:12:26,160 --> 00:12:29,040
中的部分可觀察 mdp，它基於

361
00:12:29,040 --> 00:12:30,639
我們在這裡考慮的一個簡化設置，其中我們

362
00:12:30,639 --> 00:12:32,240
只有兩個隨機

363
00:12:32,240 --> 00:12:35,120
變量，第一個是我們已經討論過的 o

364
00:12:35,120 --> 00:12:36,320
，第二個

365
00:12:36,320 --> 00:12:39,680
是 s 其中 s 表示 um 一個

366
00:12:39,680 --> 00:12:41,839
代表隱藏或潛在

367
00:12:41,839 --> 00:12:44,079
狀態的隨機變量，它們存在於

368
00:12:44,079 --> 00:12:45,760
所有可能

369
00:12:45,760 --> 00:12:47,519
隱藏狀態的有限集合中，這裡用

370
00:12:47,519 --> 00:12:49,279
大寫 s 表示

371
00:12:49,279 --> 00:12:51,440
，我們

372
00:12:51,440 --> 00:12:52,399
超過 o 和

373
00:12:52,399 --> 00:12:54,880
s 的聯合概率可以分解為似然

374
00:12:54,880 --> 00:12:55,440
函數

375
00:12:55,440 --> 00:12:59,120
這是 o 給定 s 的 p，然後你

376
00:12:59,120 --> 00:13:01,360
有內部狀態的先驗

377
00:13:01,360 --> 00:13:02,720
，即 pss，

378
00:13:02,720 --> 00:13:04,839
所以這給了你一個非常好的

379
00:13:04,839 --> 00:13:06,240
公式

380
00:13:06,240 --> 00:13:09,519
，我們將在

381
00:13:09,519 --> 00:13:11,839
接下來的幾張幻燈片中使用它 我只是

382
00:13:11,839 --> 00:13:12,720
想問一下

383
00:13:12,720 --> 00:13:14,480
，當我

384
00:13:14,480 --> 00:13:15,920
突出顯示或當我

385
00:13:15,920 --> 00:13:20,079
或不在那裡時，你能看到我的鼠標嗎？

386
00:13:20,079 --> 00:13:22,719


387
00:13:22,800 --> 00:13:24,560


388
00:13:24,560 --> 00:13:25,600


389
00:13:25,600 --> 00:13:27,360


390
00:13:27,360 --> 00:13:29,040
可能導致給定

391
00:13:29,040 --> 00:13:29,760
結果的隱藏狀態

392
00:13:29,760 --> 00:13:31,440
，這可以通過使用

393
00:13:31,440 --> 00:13:33,440
我剛剛提到的

394
00:13:33,440 --> 00:13:34,160
可能性

395
00:13:34,160 --> 00:13:37,360
和先驗

396
00:13:37,360 --> 00:13:40,320
um 的分解來實現，但問題是這不是一個

397
00:13:40,320 --> 00:13:42,399
簡單的任務，因為

398
00:13:42,399 --> 00:13:43,519
隱藏的維度 狀態

399
00:13:43,519 --> 00:13:45,360
um 可能非常大，如果您正在

400
00:13:45,360 --> 00:13:47,120
考慮

401
00:13:47,120 --> 00:13:48,480
我們將

402
00:13:48,480 --> 00:13:50,240
在稍後引入的其他隨機變量，這將變得

403
00:13:50,240 --> 00:13:51,600
更加成問題

404
00:13:51,600 --> 00:13:54,480


405
00:13:54,480 --> 00:13:56,240


406
00:13:56,240 --> 00:13:57,199


407
00:13:57,199 --> 00:13:59,920
更吸引人，並且

408
00:13:59,920 --> 00:14:01,920
允許我們估計感興趣的數量，

409
00:14:01,920 --> 00:14:05,839


410
00:14:05,839 --> 00:14:08,720
所以這將是一個自然的步驟來

411
00:14:08,720 --> 00:14:10,160
討論變分自由能

412
00:14:10,160 --> 00:14:11,760
，這是 qua 的變分近似

413
00:14:11,760 --> 00:14:12,800


414
00:14:12,800 --> 00:14:15,519
感興趣的實體嗯所以什麼是變分自由

415
00:14:15,519 --> 00:14:17,199
能所以變分自由能被

416
00:14:17,199 --> 00:14:19,440
定義為驚喜的上限，

417
00:14:19,440 --> 00:14:20,560
所以我們考慮的第一個定義

418
00:14:20,560 --> 00:14:21,920
抱歉第一個定義

419
00:14:21,920 --> 00:14:24,240
是使用詹森不等式推導出來的

420
00:14:24,240 --> 00:14:25,519
，

421
00:14:25,519 --> 00:14:27,600
並且通常被稱為

422
00:14:27,600 --> 00:14:29,760
變分推理中的負面證據下限

423
00:14:29,760 --> 00:14:32,480
所以我們從

424
00:14:32,480 --> 00:14:34,240
方程 4 中得到這個，我們剛剛

425
00:14:34,240 --> 00:14:37,199
通過在兩邊引入負對數

426
00:14:37,199 --> 00:14:37,760


427
00:14:37,760 --> 00:14:41,040
然後將該項

428
00:14:41,040 --> 00:14:43,680
乘以 1，這本質上是 s 的 q 超過 s 的 q

429
00:14:43,680 --> 00:14:45,360
所以我們假設

430
00:14:45,360 --> 00:14:48,560
s 的 q 不能相等 到零，

431
00:14:48,560 --> 00:14:51,199
然後我們應用 jensen

432
00:14:51,199 --> 00:14:53,199
不等式，我們將 log 移動

433
00:14:53,199 --> 00:14:54,000
到函數中

434
00:14:54,000 --> 00:14:56,560
，我們最終得到我們

435
00:14:56,560 --> 00:14:58,000
對 t of s 的期望，

436
00:14:58,000 --> 00:15:00,079
對於關節的 log 在

437
00:15:00,079 --> 00:15:01,920


438
00:15:01,920 --> 00:15:05,040
這里和然後 我們

439
00:15:05,040 --> 00:15:07,120
把負數放在裡面，我們可以

440
00:15:07,120 --> 00:15:07,839
把它翻轉過來

441
00:15:07,839 --> 00:15:11,120
，我們在這裡得到了我們第一個

442
00:15:11,120 --> 00:15:14,160
很好的興趣量，我們

443
00:15:14,160 --> 00:15:14,560
得到

444
00:15:14,560 --> 00:15:17,199
了我們感興趣的界限

445
00:15:17,199 --> 00:15:17,600


446
00:15:17,600 --> 00:15:20,720
和術語

447
00:15:20,720 --> 00:15:24,240


448
00:15:24,480 --> 00:15:25,920
為了使這一點更

449
00:15:25,920 --> 00:15:27,760
具體一點，我們現在可以做的是

450
00:15:27,760 --> 00:15:29,279
進一步操縱變分自由

451
00:15:29,279 --> 00:15:29,759
能將

452
00:15:29,759 --> 00:15:34,959
um 召喚到 uh

453
00:15:34,959 --> 00:15:38,720
近似和真實後驗之間的殺戮

454
00:15:38,720 --> 00:15:41,360
減去對數

455
00:15:41,360 --> 00:15:43,120
證明我們的模型證據

456
00:15:43,120 --> 00:15:45,920
，我們可以重新安排最後一個方程，

457
00:15:45,920 --> 00:15:47,519
以真正磨練出

458
00:15:47,519 --> 00:15:48,959


459
00:15:48,959 --> 00:15:51,759
意外和變分自由能 um 之間的聯繫，

460
00:15:51,759 --> 00:15:52,320
所以

461
00:15:52,320 --> 00:15:55,040
如果你記得 kl 是一個散度

462
00:15:55,040 --> 00:15:56,720
，這意味著它不能小於

463
00:15:56,720 --> 00:15:58,079
零，所以

464
00:15:58,079 --> 00:16:01,519
嗯，它總是嚴格地大於

465
00:16:01,519 --> 00:16:03,440
或等於零，這意味著

466
00:16:03,440 --> 00:16:06,399
當我們的近似值等於

467
00:16:06,399 --> 00:16:07,519
真實後驗時，

468
00:16:07,519 --> 00:16:09,600
我們最終得到的變化自由能

469
00:16:09,600 --> 00:16:10,720
等於模型

470
00:16:10,720 --> 00:16:13,839
證據，這意味著最小化

471
00:16:13,839 --> 00:16:15,839
自由能本質上等同於

472
00:16:15,839 --> 00:16:18,320
最大化性別模型 證據

473
00:16:18,320 --> 00:16:23,839
嗯 好的

474
00:16:25,279 --> 00:16:28,240
嗯 我們可以重寫之前的

475
00:16:28,240 --> 00:16:29,600
方程

476
00:16:29,600 --> 00:16:31,680
嗯 方程 10 將

477
00:16:31,680 --> 00:16:33,040
變分自由能表示

478
00:16:33,040 --> 00:16:34,800
為 po 的函數

479
00:16:34,800 --> 00:16:36,079
多種不同形式的先驗信念，

480
00:16:36,079 --> 00:16:37,680
所以我只關注公式

481
00:16:37,680 --> 00:16:40,000
12，這裡是複雜性

482
00:16:40,000 --> 00:16:42,800
減去準確性，所以這是一個權衡，呃

483
00:16:42,800 --> 00:16:45,440
，通常

484
00:16:45,440 --> 00:16:48,839
在論文基本上說

485
00:16:48,839 --> 00:16:50,160
複雜性 tam

486
00:16:50,160 --> 00:16:53,600
或複雜性成本本質上是時使用 嗯，

487
00:16:53,600 --> 00:16:57,120
你的羽衣甘藍在你

488
00:16:57,120 --> 00:17:01,120
給定 pi 的大概 um 與

489
00:17:01,120 --> 00:17:05,199
uh 你給定 um pi 的 p 之間，這裡 pi

490
00:17:05,199 --> 00:17:07,280
只是你的政策，這些可能

491
00:17:07,280 --> 00:17:09,280
與

492
00:17:09,280 --> 00:17:12,319
呃代理人將如何行動的假設無關，

493
00:17:12,319 --> 00:17:14,079
但我會來 回到政策

494
00:17:14,079 --> 00:17:15,679
真正需要的東西，但現在

495
00:17:15,679 --> 00:17:17,599
只將它們視為一個術語，它允許

496
00:17:17,599 --> 00:17:19,919
我們將自由能限制

497
00:17:19,919 --> 00:17:22,319
在一系列感興趣的軌跡上

498
00:17:22,319 --> 00:17:23,679


499
00:17:23,679 --> 00:17:26,319
，我們擁有的第二個術語

500
00:17:26,319 --> 00:17:27,599


501
00:17:27,599 --> 00:17:31,200
是 o 給定 s 的對數概率，所以可能性 uh

502
00:17:31,200 --> 00:17:34,559
與 a 關於 s 的鍵，

503
00:17:34,559 --> 00:17:37,200
它可以為您提供準確度，所以一個簡單

504
00:17:37,200 --> 00:17:39,120
的思考方式是，

505
00:17:39,120 --> 00:17:42,240
嗯，這只是 um

506
00:17:42,240 --> 00:17:43,440
模型的準確度，這是一些

507
00:17:43,440 --> 00:17:45,919
正則化項，它是 s 的懲罰

508
00:17:45,919 --> 00:17:47,200
項 確保它不會

509
00:17:47,200 --> 00:17:49,280
與我們最初的先驗相差太遠，

510
00:17:49,280 --> 00:17:52,320


511
00:17:52,320 --> 00:17:55,679
好吧，所以這個特定數量的

512
00:17:55,679 --> 00:17:56,960
變分自由能

513
00:17:56,960 --> 00:17:58,480
在這一點上是否有任何

514
00:17:58,480 --> 00:18:01,039
關於能量變化

515
00:18:01,039 --> 00:18:03,760


516
00:18:03,760 --> 00:18:05,360
的問題

517
00:18:05,360 --> 00:18:07,360
呢？ 感知環境

518
00:18:07,360 --> 00:18:09,200
並解決主動

519
00:18:09,200 --> 00:18:11,039
影響公式的一部分，該公式正在

520
00:18:11,039 --> 00:18:11,760


521
00:18:11,760 --> 00:18:14,240
對代理在給定時間點與之交互的給定世界進行推斷，

522
00:18:14,240 --> 00:18:16,080


523
00:18:16,080 --> 00:18:16,880


524
00:18:16,880 --> 00:18:19,440
但是我們實際上並沒有

525
00:18:19,440 --> 00:18:20,320
考慮到

526
00:18:20,320 --> 00:18:22,960
主動部分，而就像

527
00:18:22,960 --> 00:18:23,600


528
00:18:23,600 --> 00:18:25,679
我們擁有的這個特定代理一樣 在主動推理公式下，

529
00:18:25,679 --> 00:18:26,880
可以

530
00:18:26,880 --> 00:18:29,520
採取一系列行動或

531
00:18:29,520 --> 00:18:31,360
與環境交互，從而

532
00:18:31,360 --> 00:18:34,240
影響未來的環境

533
00:18:34,240 --> 00:18:34,960


534
00:18:34,960 --> 00:18:37,280
嗯，為了進一步激發這一點

535
00:18:37,280 --> 00:18:38,880
，我們可以考慮的是，

536
00:18:38,880 --> 00:18:40,480
我們不僅要最小化 我們的變化

537
00:18:40,480 --> 00:18:42,240
自由能我們還希望最小化一個

538
00:18:42,240 --> 00:18:44,320
稱為預期自由能的量

539
00:18:44,320 --> 00:18:46,320
，它取決於預期

540
00:18:46,320 --> 00:18:48,240
對未來或未來的觀察

541
00:18:48,240 --> 00:18:50,799
以及

542
00:18:50,799 --> 00:18:52,080
對這個

543
00:18:52,080 --> 00:18:54,480
特定術語的最小化允許代理

544
00:18:54,480 --> 00:18:56,400
通過在當前採取特定行動來影響未來，這些

545
00:18:56,400 --> 00:18:58,400
行動

546
00:18:58,400 --> 00:19:00,080
是從一組政策中選擇的，

547
00:19:00,080 --> 00:19:02,720
所以我現在提到了幾次政策，

548
00:19:02,720 --> 00:19:03,520


549
00:19:03,520 --> 00:19:06,960
所以什麼是 他們嗯，所以策略可以

550
00:19:06,960 --> 00:19:09,280
定義為時間 tau 的一系列動作

551
00:19:09,280 --> 00:19:10,240


552
00:19:10,240 --> 00:19:12,240
，使代理能夠

553
00:19:12,240 --> 00:19:13,760
在隱藏狀態

554
00:19:13,760 --> 00:19:17,520
和 tau 之間轉換，這裡本質上是

555
00:19:17,520 --> 00:19:19,120
一系列軌跡，直到

556
00:19:19,120 --> 00:19:21,039
特定的地平線，

557
00:19:21,039 --> 00:19:24,320
呃上限，考慮到總

558
00:19:24,320 --> 00:19:25,679


559
00:19:25,679 --> 00:19:27,760
時間 您在特定設置中考慮的步驟

560
00:19:27,760 --> 00:19:29,280


561
00:19:29,280 --> 00:19:32,320
以及為了我們正確定義 uh

562
00:19:32,320 --> 00:19:33,919
策略，我們需要引入兩個

563
00:19:33,919 --> 00:19:35,440
額外的隨機變量，

564
00:19:35,440 --> 00:19:38,160
因此第一個是一個

565
00:19:38,160 --> 00:19:38,960


566
00:19:38,960 --> 00:19:43,039
以 tau 為條件的動作，此處由 utau 表示，

567
00:19:43,039 --> 00:19:46,160
並且存在於有限的集合中

568
00:19:46,160 --> 00:19:48,320
代理可以採取的所有可能的行動

569
00:19:48,320 --> 00:19:49,200


570
00:19:49,200 --> 00:19:50,960
以及我們引入的第二個隨機變量

571
00:19:50,960 --> 00:19:52,640
是呃

572
00:19:52,640 --> 00:19:54,559
策略，它是我們討論的

573
00:19:54,559 --> 00:19:55,919
pi  ssed

574
00:19:55,919 --> 00:19:58,000
並且這存在於

575
00:19:58,000 --> 00:19:58,960
所有可能的

576
00:19:58,960 --> 00:20:02,000
uh 策略 um 或動作序列的有限集合中

577
00:20:02,000 --> 00:20:04,640
，就像我們在這裡感興趣的順序策略

578
00:20:04,640 --> 00:20:06,240
優化一樣，

579
00:20:06,240 --> 00:20:08,240
所以為了讓它更具體一點

580
00:20:08,240 --> 00:20:09,520


581
00:20:09,520 --> 00:20:11,200
，隨機變量可以是

582
00:20:11,200 --> 00:20:13,440


583
00:20:13,440 --> 00:20:16,559
在特定

584
00:20:16,559 --> 00:20:19,760
時間範圍內分解為一系列動作 tau

585
00:20:19,760 --> 00:20:23,280
因此 u1 u2 和 um 上升到 utah 將

586
00:20:23,280 --> 00:20:25,120
表示從

587
00:20:25,120 --> 00:20:26,960
uh 動作在時間點一的動作到

588
00:20:26,960 --> 00:20:28,799
時間點二的動作，依此類推

589
00:20:28,799 --> 00:20:31,360
，當您考慮時鏈接是明確的

590
00:20:31,360 --> 00:20:32,320


591
00:20:32,320 --> 00:20:35,679
如果您在特定時間點考慮一項政策，

592
00:20:35,679 --> 00:20:38,880
那麼 tau 那麼

593
00:20:38,880 --> 00:20:41,840
您得到的行動將是那個行動，

594
00:20:41,840 --> 00:20:45,280
好吧，很酷，嗯，所以

595
00:20:45,280 --> 00:20:47,360
我還想強調一下，這個

596
00:20:47,360 --> 00:20:49,120
政策的定義實際上與

597
00:20:49,120 --> 00:20:50,559


598
00:20:50,559 --> 00:20:53,520
rl 中的考慮方式完全不同或不同

599
00:20:53,520 --> 00:20:54,240


600
00:20:54,240 --> 00:20:56,640
嗯，當他們說政策意味著國家

601
00:20:56,640 --> 00:20:58,000
行動政策

602
00:20:58,000 --> 00:20:59,840
時，正如我剛剛提到的推理行為，

603
00:20:59,840 --> 00:21:02,240
政策只是

604
00:21:02,240 --> 00:21:04,159
隨著時間的推移對行動的一系列選擇，這

605
00:21:04,159 --> 00:21:06,159
是一個順序政策，

606
00:21:06,159 --> 00:21:08,320
並且 這與強化學習中的狀態動作策略不同，

607
00:21:08,320 --> 00:21:10,400
後者

608
00:21:10,400 --> 00:21:12,080
是將狀態映射到

609
00:21:12,080 --> 00:21:16,080
動作，因此您的 rl 策略考慮

610
00:21:16,080 --> 00:21:16,720


611
00:21:16,720 --> 00:21:19,919
了動作和狀態，是

612
00:21:19,919 --> 00:21:20,799
在

613
00:21:20,799 --> 00:21:24,159
給定狀態的情況下您的動作的概率，並且根據

614
00:21:24,159 --> 00:21:27,919
我們的 mdp 公式

615
00:21:27,919 --> 00:21:30,159
，動作的定義哦 抱歉

616
00:21:30,159 --> 00:21:31,760
，當我們考慮 tau 等於 1 的設置時，rl 中的策略定義

617
00:21:31,760 --> 00:21:33,760
和主動推理變得

618
00:21:33,760 --> 00:21:35,679
完全相同，

619
00:21:35,679 --> 00:21:37,679
所以你只

620
00:21:37,679 --> 00:21:41,120
考慮領先一步

621
00:21:41,120 --> 00:21:45,120


622
00:21:45,120 --> 00:21:45,760


623
00:21:45,760 --> 00:21:49,039


624
00:21:49,039 --> 00:21:50,960
他們期望自由能的定量興趣，這

625
00:21:50,960 --> 00:21:54,080
就是我們如何推導它，所以

626
00:21:54,080 --> 00:21:56,240
為了推導它，我們首先需要擴展

627
00:21:56,240 --> 00:21:57,840


628
00:21:57,840 --> 00:22:00,000
我們在

629
00:22:00,000 --> 00:22:02,240
幾張幻燈片之前擁有的變分自由能定義，現在讓它

630
00:22:02,240 --> 00:22:03,360
依賴於

631
00:22:03,360 --> 00:22:06,799
時間 so tau 和 政策，我們

632
00:22:06,799 --> 00:22:07,919
所做的本質上

633
00:22:07,919 --> 00:22:10,320
是採用相同的等式，並

634
00:22:10,320 --> 00:22:12,320


635
00:22:12,320 --> 00:22:14,320


636
00:22:14,320 --> 00:22:16,080
在特定政策下將其分解為先前遺憾的先前和當前時間步長，

637
00:22:16,080 --> 00:22:18,080
這樣就可以 這就是為什麼我們有條件，

638
00:22:18,080 --> 00:22:20,480
然後我們

639
00:22:20,480 --> 00:22:23,520
在等式 15 中以特定方式分解它

640
00:22:23,520 --> 00:22:25,039
，然後

641
00:22:25,039 --> 00:22:27,360
在等式 16 中寫出矩陣公式。

642
00:22:27,360 --> 00:22:28,960
所以如果有任何問題，我們可以回到這個

643
00:22:28,960 --> 00:22:31,039
問題，

644
00:22:31,039 --> 00:22:32,880
但要採取的關鍵事項 遠離

645
00:22:32,880 --> 00:22:35,039
幻燈片的是，現在我們包括

646
00:22:35,039 --> 00:22:37,120
對時間 um 的函數依賴性，

647
00:22:37,120 --> 00:22:40,799
用於能量 um 的變化

648
00:22:40,799 --> 00:22:42,799
，這使我們現在可以

649
00:22:42,799 --> 00:22:45,120
轉向預期的自由能公式

650
00:22:45,120 --> 00:22:47,280
um，但這裡要注意的關鍵是

651
00:22:47,280 --> 00:22:49,360
我們 只考慮時間點

652
00:22:49,360 --> 00:22:52,320
um 之前的時間點和

653
00:22:52,320 --> 00:22:52,799
現在，

654
00:22:52,799 --> 00:22:55,840
而不是未來，

655
00:22:58,000 --> 00:23:00,320
所以在

656
00:23:00,320 --> 00:23:01,440
我們可以推導出

657
00:23:01,440 --> 00:23:04,559
uh 預期的自由能 um 和預期的自由能之前使用自由能方程，

658
00:23:04,559 --> 00:23:06,720


659
00:23:06,720 --> 00:23:08,320
那麼預期的自由能

660
00:23:08,320 --> 00:23:11,200
是自由能 未來

661
00:23:11,200 --> 00:23:12,159
軌跡

662
00:23:12,159 --> 00:23:14,559
g 的函數，它有效地

663
00:23:14,559 --> 00:23:16,400


664
00:23:16,400 --> 00:23:18,240
根據尚未觀察到的結果評估合理政策的證據，

665
00:23:18,240 --> 00:23:19,760
因此這是關鍵，因此

666
00:23:19,760 --> 00:23:20,640
您正在

667
00:23:20,640 --> 00:23:24,240
對民意調查進行推斷

668
00:23:24,240 --> 00:23:26,000
你還沒有

669
00:23:26,000 --> 00:23:28,000
觀察到的未來軌跡

670
00:23:28,000 --> 00:23:30,640
，這裡介紹了兩種啟發式方法

671
00:23:30,640 --> 00:23:31,679


672
00:23:31,679 --> 00:23:34,159
，以便得到

673
00:23:34,159 --> 00:23:35,120
我們

674
00:23:35,120 --> 00:23:38,960
在等式 17 中看到的 g 公式。第

675
00:23:38,960 --> 00:23:41,440
一個是將關於未來結果的信念包含

676
00:23:41,440 --> 00:23:42,320


677
00:23:42,320 --> 00:23:44,159
在我們的期望中

678
00:23:44,159 --> 00:23:46,000


679
00:23:46,000 --> 00:23:48,159
用這裡的可能性補充近似後驗下的期望，

680
00:23:48,159 --> 00:23:50,000


681
00:23:50,000 --> 00:23:51,440
這導致了這裡

682
00:23:51,440 --> 00:23:53,279
前兩個項給出的預測分佈

683
00:23:53,279 --> 00:23:54,880


684
00:23:54,880 --> 00:23:57,600
，第二個是我們

685
00:23:57,600 --> 00:23:58,559
隱含地或者

686
00:23:58,559 --> 00:24:00,799
我想明確地以狀態的

687
00:24:00,799 --> 00:24:02,799
聯合概率

688
00:24:02,799 --> 00:24:06,080
和性別觀察為條件 模型 嗯，

689
00:24:06,080 --> 00:24:08,960
對不起，性別模型 嗯，

690
00:24:08,960 --> 00:24:10,159


691
00:24:10,159 --> 00:24:13,200
取決於期望的事態，而

692
00:24:13,200 --> 00:24:14,880
不是現在的特定政策，因此

693
00:24:14,880 --> 00:24:16,799
這限制了代理將擁有的偏好類型，

694
00:24:16,799 --> 00:24:17,440


695
00:24:17,440 --> 00:24:20,640


696
00:24:20,640 --> 00:24:21,200


697
00:24:21,200 --> 00:24:22,720
而我們正在採取的這兩個舉措的幫助在於我們可以

698
00:24:22,720 --> 00:24:24,480
現在

699
00:24:24,480 --> 00:24:27,200
在實際進行觀察之前評估這個數量

700
00:24:27,200 --> 00:24:28,240
，第二個

701
00:24:28,240 --> 00:24:30,400
是 g 的最小化

702
00:24:30,400 --> 00:24:32,080
實際上會遇到 憤怒的政體

703
00:24:32,080 --> 00:24:35,440
政策要與呃代理人期望自己處於的理想狀態保持一致，嗯，

704
00:24:35,440 --> 00:24:37,520


705
00:24:37,520 --> 00:24:39,760


706
00:24:39,760 --> 00:24:41,600
我只想簡單提一下

707
00:24:41,600 --> 00:24:44,240
，這不是獲得預期自由能的唯一方法，

708
00:24:44,240 --> 00:24:45,679
而且還有

709
00:24:45,679 --> 00:24:47,760
一些工作，嗯，看起來 在其他

710
00:24:47,760 --> 00:24:48,960
公式中，

711
00:24:48,960 --> 00:24:52,240
包括 carl um 的工作，其中

712
00:24:52,240 --> 00:24:53,600
預期自由能的公式

713
00:24:53,600 --> 00:24:55,919
可以分解成不同的

714
00:24:55,919 --> 00:24:58,799
um 結構，所以如果有人對此

715
00:24:58,799 --> 00:25:02,480
感興趣，我們可以稍後再討論，

716
00:25:02,640 --> 00:25:04,799
但是我剛剛介紹的這個自由能預期自由

717
00:25:04,799 --> 00:25:05,679
能

718
00:25:05,679 --> 00:25:07,520
可以是 以

719
00:25:07,520 --> 00:25:10,320
某種方式分解 嗯，所以方程 20 和 21

720
00:25:10,320 --> 00:25:10,799
給出了

721
00:25:10,799 --> 00:25:13,200
兩種不同的分解，

722
00:25:13,200 --> 00:25:14,960
第一個是認知和

723
00:25:14,960 --> 00:25:17,039
外在價值交易者，第二

724
00:25:17,039 --> 00:25:18,960
個是預期的，

725
00:25:18,960 --> 00:25:22,320
呃成本和歧義項，所以如果我們只

726
00:25:22,320 --> 00:25:23,360
考慮

727
00:25:23,360 --> 00:25:26,640
um，第一個方程 我們可以說，如果

728
00:25:26,640 --> 00:25:28,080
我們最小化這個等式，

729
00:25:28,080 --> 00:25:30,559
那麼我們就抓住了這個必要性，以

730
00:25:30,559 --> 00:25:31,360
最大化

731
00:25:31,360 --> 00:25:33,760
你從觀察中獲得的信息增益

732
00:25:33,760 --> 00:25:35,679


733
00:25:35,679 --> 00:25:38,880
關於特定隱藏狀態的環境 um，

734
00:25:38,880 --> 00:25:41,840
同時最大化

735
00:25:41,840 --> 00:25:43,360
由

736
00:25:43,360 --> 00:25:45,520
日誌偏好或外部值評分的期望值

737
00:25:45,520 --> 00:25:46,559
，因此

738
00:25:46,559 --> 00:25:48,159
這個特定的公式實際上

739
00:25:48,159 --> 00:25:50,320
給了我們一個非常明確的權衡，

740
00:25:50,320 --> 00:25:50,880
即

741
00:25:50,880 --> 00:25:53,200
um 第一個組件，它

742
00:25:53,200 --> 00:25:54,960
是促進的認知值

743
00:25:54,960 --> 00:25:57,120
好奇的行為，所以這就是你想要

744
00:25:57,120 --> 00:25:59,760
的，鼓勵探索，因為代理會

745
00:25:59,760 --> 00:26:00,799
尋找

746
00:26:00,799 --> 00:26:02,400
這些新的狀態，以最大限度地減少

747
00:26:02,400 --> 00:26:04,960
環境的不確定性

748
00:26:04,960 --> 00:26:07,520
，而後者更務實，

749
00:26:07,520 --> 00:26:08,400
它

750
00:26:08,400 --> 00:26:11,360
通過

751
00:26:11,360 --> 00:26:13,360


752
00:26:13,360 --> 00:26:15,679
對代理的政策類型的理解來鼓勵剝削行為 更喜歡

753
00:26:15,679 --> 00:26:16,720
達到

754
00:26:16,720 --> 00:26:18,960
um 換句話說，就像這個預期的

755
00:26:18,960 --> 00:26:20,480
自由能公式

756
00:26:20,480 --> 00:26:23,200
，我們在等式 20 中看到的

757
00:26:23,200 --> 00:26:24,559
本質上是將

758
00:26:24,559 --> 00:26:26,640
探索和開發視為

759
00:26:26,640 --> 00:26:28,240
解決同一問題的兩種不同方式，

760
00:26:28,240 --> 00:26:28,799


761
00:26:28,799 --> 00:26:31,440
從而最大限度地減少演示文稿開頭提到的不確定性

762
00:26:31,440 --> 00:26:34,720


763
00:26:34,720 --> 00:26:37,120
我們也可以考慮這裡的

764
00:26:37,120 --> 00:26:38,159
第二個方程

765
00:26:38,159 --> 00:26:41,279
wh  ich 只是為我們提供了

766
00:26:41,279 --> 00:26:43,200
關於

767
00:26:43,200 --> 00:26:45,360
自由預期頻率的另一種觀點，即

768
00:26:45,360 --> 00:26:47,919
代理人希望最小化模糊性

769
00:26:47,919 --> 00:26:50,000
以及特定政策下的結果

770
00:26:50,000 --> 00:26:52,000
偏離

771
00:26:52,000 --> 00:26:55,039
先前偏好的程度，因此這裡的模糊性

772
00:26:55,039 --> 00:26:56,880
是條件熵的期望

773
00:26:56,880 --> 00:26:58,880
或不確定性

774
00:26:58,880 --> 00:27:01,600
在這種特定環境下，當前政策下的結果

775
00:27:01,600 --> 00:27:02,799


776
00:27:02,799 --> 00:27:04,559
低熵表明結果

777
00:27:04,559 --> 00:27:06,240
非常顯著，並且

778
00:27:06,240 --> 00:27:08,320
對於隱藏狀態具有獨特的信息

779
00:27:08,320 --> 00:27:10,320
，例如，

780
00:27:10,320 --> 00:27:11,760
與房間

781
00:27:11,760 --> 00:27:13,039


782
00:27:13,039 --> 00:27:14,720
是否非常黑暗相比，

783
00:27:14,720 --> 00:27:16,080
您可能會看到的視覺提示 '不會從中做出任何

784
00:27:16,080 --> 00:27:19,440
重要的事情，此外，

785
00:27:19,440 --> 00:27:20,880
代理人希望追求

786
00:27:20,880 --> 00:27:21,520


787
00:27:21,520 --> 00:27:23,760
類似於其首選結果的政策依賴結果，

788
00:27:23,760 --> 00:27:25,039


789
00:27:25,039 --> 00:27:27,120
所以這是由 c 捐贈的，這

790
00:27:27,120 --> 00:27:29,279
是

791
00:27:29,279 --> 00:27:31,039
在預測結果和首選結果之間的護理差異時實現的

792
00:27:31,039 --> 00:27:34,320


793
00:27:34,320 --> 00:27:37,679
被一個特定的政策最小化，

794
00:27:37,679 --> 00:27:40,880
好吧，嗯，這些先前的信念

795
00:27:40,880 --> 00:27:41,520
但是，未來的

796
00:27:41,520 --> 00:27:42,960
結果使代理具有

797
00:27:42,960 --> 00:27:44,960
目標導向的行為

798
00:27:44,960 --> 00:27:46,960
，這是我認為

799
00:27:46,960 --> 00:27:48,320


800
00:27:48,320 --> 00:27:51,840
在積極影響中真正重要的實例之一，

801
00:27:51,840 --> 00:27:54,640
嗯，好吧，所以一旦我們有了預期的

802
00:27:54,640 --> 00:27:56,240
自由能量，我們就可以推導

803
00:27:56,240 --> 00:28:00,159
出政策，嗯，

804
00:28:00,159 --> 00:28:02,240
嗯，這樣 是通過在預期自由能上應用軟最大函數來推導任何策略的概率來實現的，

805
00:28:02,240 --> 00:28:04,080


806
00:28:04,080 --> 00:28:06,399


807
00:28:06,399 --> 00:28:08,480


808
00:28:08,480 --> 00:28:09,919
並且這種說明了主動推理的不

809
00:28:09,919 --> 00:28:11,760
言自明的行為，

810
00:28:11,760 --> 00:28:12,559


811
00:28:12,559 --> 00:28:14,559
因為導致較低預期自由能的任何類型的策略或動作

812
00:28:14,559 --> 00:28:16,480
序列

813
00:28:16,480 --> 00:28:18,240
更多 可能

814
00:28:18,240 --> 00:28:21,520
且直觀地說，這是

815
00:28:21,520 --> 00:28:24,159
有道理的，因為預期的自由能

816
00:28:24,159 --> 00:28:25,039
在某種程度上

817
00:28:25,039 --> 00:28:26,960
封裝了

818
00:28:26,960 --> 00:28:28,640


819
00:28:28,640 --> 00:28:30,399
您在與世界互動時想要包括或考慮的所有類型的事物，

820
00:28:30,399 --> 00:28:31,600
因此您想要探索

821
00:28:31,600 --> 00:28:32,960
想要利用但想要利用 有

822
00:28:32,960 --> 00:28:36,000
一個平衡，然後

823
00:28:36,000 --> 00:28:38,159
當你選擇你的政策時，

824
00:28:38,159 --> 00:28:39,600
這只是確定

825
00:28:39,600 --> 00:28:40,159


826
00:28:40,159 --> 00:28:42,960
讓你最接近這一點的一組行動的問題

827
00:28:42,960 --> 00:28:44,000
特定的核心

828
00:28:44,000 --> 00:28:45,760
um 這可以由一個

829
00:28:45,760 --> 00:28:48,960
有吸引力的狀態定義，該狀態由

830
00:28:48,960 --> 00:28:52,480
我們之前描述的 c 矩陣定義

831
00:28:52,480 --> 00:28:55,200


832
00:28:55,200 --> 00:28:56,159


833
00:28:56,159 --> 00:28:57,919


834
00:28:57,919 --> 00:28:59,600


835
00:28:59,600 --> 00:29:01,600


836
00:29:01,600 --> 00:29:03,760
在這裡，

837
00:29:03,760 --> 00:29:06,080
嗯，通過在這上面設置一個超素數，

838
00:29:06,080 --> 00:29:07,520
你

839
00:29:07,520 --> 00:29:09,440
在

840
00:29:09,440 --> 00:29:10,799
預期的自由能

841
00:29:10,799 --> 00:29:14,720
uh 公式中引入了額外的複雜性成本，這讓你可以

842
00:29:14,720 --> 00:29:17,840
解釋

843
00:29:17,840 --> 00:29:20,880


844
00:29:20,880 --> 00:29:23,279
你希望你的

845
00:29:23,279 --> 00:29:24,159


846
00:29:24,159 --> 00:29:27,919
偏好在政策空間上有多平坦或有多自信或精確，

847
00:29:27,919 --> 00:29:31,279
嗯，關鍵 需要注意的是，嗯

848
00:29:31,279 --> 00:29:33,039
，為了簡單起見，我不會

849
00:29:33,039 --> 00:29:34,880


850
00:29:34,880 --> 00:29:37,200
詳細介紹如何優化這些細節，

851
00:29:37,200 --> 00:29:37,919


852
00:29:37,919 --> 00:29:40,159
但是您可以通過多種不同的方式來做到這一點，

853
00:29:40,159 --> 00:29:41,919
例如在主動

854
00:29:41,919 --> 00:29:42,720
推理中，

855
00:29:42,720 --> 00:29:45,039
我們可以優化對

856
00:29:45,039 --> 00:29:47,039
隱藏的感興趣狀態 策略

857
00:29:47,039 --> 00:29:48,640
推斷的精度

858
00:29:48,640 --> 00:29:50,320
然後我們還可以

859
00:29:50,320 --> 00:29:51,679


860
00:29:51,679 --> 00:29:53,919
通過所涉及的學習過程通過 um 優化模型參數

861
00:29:53,919 --> 00:29:54,960


862
00:29:54,960 --> 00:29:58,080
嗯，但是這些取決於

863
00:29:58,080 --> 00:30:00,000
您正在查看的設置，例如，

864
00:30:00,000 --> 00:30:01,600
如果您使用變體基礎，您

865
00:30:01,600 --> 00:30:02,559
只需迭代

866
00:30:02,559 --> 00:30:04,799
這些函數或目標

867
00:30:04,799 --> 00:30:06,559
函數，直到收斂

868
00:30:06,559 --> 00:30:08,960
或主動推理，嗯，您進行

869
00:30:08,960 --> 00:30:10,080
梯度上升，

870
00:30:10,080 --> 00:30:12,000
嗯，以找到 再次感興趣的足夠統計數據，

871
00:30:12,000 --> 00:30:13,919
這取決於

872
00:30:13,919 --> 00:30:15,840


873
00:30:15,840 --> 00:30:18,080
您正在查看的公式和設置，

874
00:30:18,080 --> 00:30:20,240
但這裡要注意的關鍵是，

875
00:30:20,240 --> 00:30:21,679


876
00:30:21,679 --> 00:30:23,760
主動

877
00:30:23,760 --> 00:30:25,360
推理算法有三個特定方面是有用的，

878
00:30:25,360 --> 00:30:26,159
可以

879
00:30:26,159 --> 00:30:28,960
從這個特定的 呃

880
00:30:28,960 --> 00:30:29,679
框架

881
00:30:29,679 --> 00:30:32,799
並應用於呃其他設置，

882
00:30:32,799 --> 00:30:34,559
嗯，所以我只是要重申和

883
00:30:34,559 --> 00:30:36,080
簡要總結它們，

884
00:30:36,080 --> 00:30:38,000
所以我們首先有至關重要的性別模型，

885
00:30:38,000 --> 00:30:39,679
所以為了讓代理進行

886
00:30:39,679 --> 00:30:41,440
交互並最大限度地減少它的驚喜，它

887
00:30:41,440 --> 00:30:43,520
需要一個溫和的世界模型

888
00:30:43,520 --> 00:30:46,880
和 這被描述為簡單我不是

889
00:30:46,880 --> 00:30:48,640
我不包括任何模型

890
00:30:48,640 --> 00:30:50,240
參數但是你可以有

891
00:30:50,240 --> 00:30:52,159
你的結果你的狀態和你的

892
00:30:52,159 --> 00:30:53,679
政策和這個

893
00:30:53,679 --> 00:30:56,960
嗯這些 被分解成嗯

894
00:30:56,960 --> 00:30:59,120
是的對不起這裡有一個帶括號的箭頭

895
00:30:59,120 --> 00:31:00,000
但是

896
00:31:00,000 --> 00:31:03,120
這些被分解成你之前

897
00:31:03,120 --> 00:31:04,720
的可能性和你的轉換

898
00:31:04,720 --> 00:31:05,760
函數

899
00:31:05,760 --> 00:31:07,840
然後你設置一旦你有了這個

900
00:31:07,840 --> 00:31:10,000
生殖器模型代理的目標

901
00:31:10,000 --> 00:31:11,519
是讓模型適應樣本

902
00:31:11,519 --> 00:31:13,360
觀察以減少 價格

903
00:31:13,360 --> 00:31:14,960
，那是通過變分自由

904
00:31:14,960 --> 00:31:17,279
能優化，

905
00:31:17,279 --> 00:31:19,120
所以我們

906
00:31:19,120 --> 00:31:21,840
在復雜性和準確性成本之間進行了這種特殊的權衡，

907
00:31:21,840 --> 00:31:22,240
然後

908
00:31:22,240 --> 00:31:24,000
是第二個哦，對不起，這個算法的最後一部分

909
00:31:24,000 --> 00:31:25,760
是計劃

910
00:31:25,760 --> 00:31:27,760
所以選擇最小化

911
00:31:27,760 --> 00:31:29,200
不確定性的行動

912
00:31:29,200 --> 00:31:32,000
，嗯 是預期的自由能

913
00:31:32,000 --> 00:31:33,760
，你這樣做的方式是通過一個

914
00:31:33,760 --> 00:31:34,480
軟最大值

915
00:31:34,480 --> 00:31:37,600
超過這個 uh g 負 g

916
00:31:37,600 --> 00:31:39,120
我們在這裡的數量

917
00:31:39,120 --> 00:31:41,360
，然後從中採樣，以便

918
00:31:41,360 --> 00:31:42,720
選擇下一個規格

919
00:31:42,720 --> 00:31:46,080
最好的動作，

920
00:31:46,080 --> 00:31:49,600
好吧，這是一個快速和 深入

921
00:31:49,600 --> 00:31:51,760
研究大量活躍的推理

922
00:31:51,760 --> 00:31:52,799
文獻，但我只是想

923
00:31:52,799 --> 00:31:53,600
強調一下，如果您有興趣，

924
00:31:53,600 --> 00:31:55,120
這些是三個核心要素

925
00:31:55,120 --> 00:31:57,120


926
00:31:57,120 --> 00:31:59,279
自己實現這些算法

927
00:31:59,279 --> 00:32:01,440
沒問題，所以現在我要

928
00:32:01,440 --> 00:32:02,720
稍微切換一下，並通過

929
00:32:02,720 --> 00:32:04,080
與強化學習的比較進行比較，

930
00:32:04,080 --> 00:32:07,679
所以在我們的工作中，我們

931
00:32:07,679 --> 00:32:09,360
考慮了

932
00:32:09,360 --> 00:32:10,320
開放 AI

933
00:32:10,320 --> 00:32:12,880
健身房冰凍湖環境的修改版本，所以冰凍

934
00:32:12,880 --> 00:32:14,720
湖有一個網格- 就像

935
00:32:14,720 --> 00:32:17,120
具有四個不同補丁的結構，所以它的

936
00:32:17,120 --> 00:32:18,399
起點

937
00:32:18,399 --> 00:32:22,000
是 um，所以我們可以在這裡

938
00:32:22,000 --> 00:32:22,799


939
00:32:22,799 --> 00:32:25,679


940
00:32:25,679 --> 00:32:26,640


941
00:32:26,640 --> 00:32:29,600


942
00:32:29,600 --> 00:32:31,120
看到 我可以區分，因為我

943
00:32:31,120 --> 00:32:32,399
只是在移動我的嘴，

944
00:32:32,399 --> 00:32:36,080
嗯，我在放大，他們可以看到它

945
00:32:36,080 --> 00:32:38,080
完美，所以你有凍結的 f，

946
00:32:38,080 --> 00:32:39,760
然後你有整體

947
00:32:39,760 --> 00:32:42,240
，最後你有目標，所以 g

948
00:32:42,240 --> 00:32:43,360
在

949
00:32:43,360 --> 00:32:45,760
這裡 第一個 b 被定位，並且

950
00:32:45,760 --> 00:32:48,000
這個特定設置

951
00:32:48,000 --> 00:32:50,320
中的所有補丁都是安全的，除了大廳，如果

952
00:32:50,320 --> 00:32:51,279
代理

953
00:32:51,279 --> 00:32:54,640
去 h，它會得到負獎勵

954
00:32:54,640 --> 00:32:57,039


955
00:32:57,039 --> 00:32:57,679


956
00:32:57,679 --> 00:32:59,519


957
00:32:59,519 --> 00:33:01,519


958
00:33:01,519 --> 00:33:03,200
到達fr  isbee

959
00:33:03,200 --> 00:33:07,279
以盡可能少的步驟在 um 中定位

960
00:33:07,279 --> 00:33:08,000


961
00:33:08,000 --> 00:33:09,840
，你可以通過

962
00:33:09,840 --> 00:33:11,679
執行 uh 四種不同類型的

963
00:33:11,679 --> 00:33:12,799
動作，所以要么

964
00:33:12,799 --> 00:33:15,840
從左向右向下或向上，

965
00:33:15,840 --> 00:33:17,840
並且允許代理繼續

966
00:33:17,840 --> 00:33:18,960
在冰凍的湖中移動

967
00:33:18,960 --> 00:33:21,279
多次重訪，這樣你就可以

968
00:33:21,279 --> 00:33:22,720
回到開始位置，

969
00:33:22,720 --> 00:33:24,480
在其他地方也有類似的情況，

970
00:33:24,480 --> 00:33:26,960
但每一集都會在

971
00:33:26,960 --> 00:33:27,919
到達

972
00:33:27,919 --> 00:33:31,600
大廳或目標位置時結束，

973
00:33:31,600 --> 00:33:34,000
這些位置會根據我們在模擬中的設置而有所不同，

974
00:33:34,000 --> 00:33:35,120


975
00:33:35,120 --> 00:33:38,320
所以

976
00:33:38,320 --> 00:33:40,720
在 一個設置，嗯整體的位置是

977
00:33:40,720 --> 00:33:41,440


978
00:33:41,440 --> 00:33:43,840
八，目標是六，另一個

979
00:33:43,840 --> 00:33:45,679
設置整體的位置

980
00:33:45,679 --> 00:33:49,279
是六，

981
00:33:49,279 --> 00:33:51,919
嗯，目標是八，目標

982
00:33:51,919 --> 00:33:53,679
就像我說的

983
00:33:53,679 --> 00:33:56,000
那樣理想地以盡可能少的步驟達到目標 可能

984
00:33:56,000 --> 00:33:57,440
同時避免整體，因為這

985
00:33:57,440 --> 00:33:59,120
將結束這一集，

986
00:33:59,120 --> 00:34:00,880
呃，如果它達到目標，它會獲得

987
00:34:00,880 --> 00:34:02,559
100 的正獎勵，

988
00:34:02,559 --> 00:34:05,600
否則

989
00:34:05,600 --> 00:34:07,840
是負的，這裡要注意的關鍵是這個

990
00:34:07,840 --> 00:34:09,760
查詢指標實際上允許我們 y

991
00:34:09,760 --> 00:34:11,280
將主動推理

992
00:34:11,280 --> 00:34:12,960
算法與強化學習

993
00:34:12,960 --> 00:34:14,879
算法進行比較，但

994
00:34:14,879 --> 00:34:16,639
對於主動推理

995
00:34:16,639 --> 00:34:18,839
算法來說，

996
00:34:18,839 --> 00:34:22,000
將獎勵功能作為開始並不重要，

997
00:34:22,000 --> 00:34:24,000
因為它仍然可以四處移動

998
00:34:24,000 --> 00:34:26,639
並僅使用信息遊戲選項卡，

999
00:34:26,639 --> 00:34:28,879
因此沒有外在 價值

1000
00:34:28,879 --> 00:34:30,239
組件

1001
00:34:30,239 --> 00:34:33,199
嗯，這很有趣，

1002
00:34:33,199 --> 00:34:35,040
因為我們將在同化中看到它的後果

1003
00:34:35,040 --> 00:34:36,960


1004
00:34:36,960 --> 00:34:39,599
，對於這個特定的設置，我們

1005
00:34:39,599 --> 00:34:41,040
將每集的最大時間步數限制

1006
00:34:41,040 --> 00:34:42,320


1007
00:34:42,320 --> 00:34:45,679
為 15。

1008
00:34:45,679 --> 00:34:47,918
好吧，我要做的是首先 討論

1009
00:34:47,918 --> 00:34:49,199
我們用於主動推理公式的生殖器模型

1010
00:34:49,199 --> 00:34:51,359


1011
00:34:51,359 --> 00:34:53,440
嗯，所以您在幻燈片上看到的

1012
00:34:53,440 --> 00:34:55,359


1013
00:34:55,359 --> 00:34:57,520
是主動模型和示例模型的圖形表示，

1014
00:34:57,520 --> 00:34:59,280
因此該模型包含四個動作

1015
00:34:59,280 --> 00:35:00,800
狀態，

1016
00:35:00,800 --> 00:35:03,920
上下左右以及

1017
00:35:03,920 --> 00:35:06,400
這些 控制在隱藏狀態之間轉換的能力

1018
00:35:06,400 --> 00:35:08,240


1019
00:35:08,240 --> 00:35:11,359
uh location fact 例如，如果

1020
00:35:11,359 --> 00:35:12,240
你在位置

1021
00:35:12,240 --> 00:35:14,960
一併且你採取了正確的行動，那麼你

1022
00:35:14,960 --> 00:35:15,760
會 我最終

1023
00:35:15,760 --> 00:35:20,000
在位置二，或者如果你在位置

1024
00:35:20,000 --> 00:35:22,480
五並且你採取了向上的動作，

1025
00:35:22,480 --> 00:35:24,240
你最終也會在位置

1026
00:35:24,240 --> 00:35:27,920
二，

1027
00:35:27,920 --> 00:35:30,560
在這個特定的設置中，

1028
00:35:30,560 --> 00:35:31,200
位置

1029
00:35:31,200 --> 00:35:33,040
六和八都是吸收狀態，

1030
00:35:33,040 --> 00:35:34,320
因為如果你記得的話

1031
00:35:34,320 --> 00:35:36,480
一旦特工到達那個位置，

1032
00:35:36,480 --> 00:35:37,680
他們就無法

1033
00:35:37,680 --> 00:35:40,640
離開，所以那是情節結束的時候

1034
00:35:40,640 --> 00:35:42,720
，如果特工

1035
00:35:42,720 --> 00:35:44,480
在這個特定的迷宮中做出了不太可能的移動，例如，如果

1036
00:35:44,480 --> 00:35:45,040
它

1037
00:35:45,040 --> 00:35:47,200
試圖從位置一到左，它

1038
00:35:47,200 --> 00:35:50,000
就會停留 在那個位置，它不會

1039
00:35:50,000 --> 00:35:52,320
在這個特定的 um 性別模型中移動，所以

1040
00:35:52,320 --> 00:35:53,920
我只是在查看隱藏狀態，

1041
00:35:53,920 --> 00:35:54,640
現在

1042
00:35:54,640 --> 00:35:56,960
我們在這兩個因素之間有一個長期的 um 轉移產物

1043
00:35:56,960 --> 00:35:58,640
，所以我們

1044
00:35:58,640 --> 00:36:00,720
在這裡有位置和上下文，

1045
00:36:00,720 --> 00:36:03,359
嗯，上下文不能被改變

1046
00:36:03,359 --> 00:36:04,000


1047
00:36:04,000 --> 00:36:05,680
我們擁有的代理 嗯，因為

1048
00:36:05,680 --> 00:36:07,040
這是由環境決定的

1049
00:36:07,040 --> 00:36:08,160
，

1050
00:36:08,160 --> 00:36:10,079
這決定了整個位置的目標在哪裡，

1051
00:36:10,079 --> 00:36:12,800
而位置

1052
00:36:12,800 --> 00:36:14,480
是代理可以控制的東西

1053
00:36:14,480 --> 00:36:16,160
，這就是我們可以採取行動的地方

1054
00:36:16,160 --> 00:36:16,800


1055
00:36:16,800 --> 00:36:20,720
關於這個 um 的狀態，所以我們有

1056
00:36:20,720 --> 00:36:21,839
兩個

1057
00:36:21,839 --> 00:36:25,599
上下文，第一個是目標在

1058
00:36:25,599 --> 00:36:28,560
您的位置 8 和走廊在

1059
00:36:28,560 --> 00:36:29,839
位置 6

1060
00:36:29,839 --> 00:36:32,079
的位置，第二個上下文是

1061
00:36:32,079 --> 00:36:33,280
目標在位置

1062
00:36:33,280 --> 00:36:37,119
6 的位置，大廳在位置 8

1063
00:36:37,119 --> 00:36:41,119
嗯 在每個時間點，智能體將

1064
00:36:41,119 --> 00:36:44,560
觀察到兩個結果，一個是它

1065
00:36:44,560 --> 00:36:46,800
自己在這個特定迷宮中的位置，第二個是智能體

1066
00:36:46,800 --> 00:36:48,960
將獲得的分數，

1067
00:36:48,960 --> 00:36:50,400


1068
00:36:50,400 --> 00:36:52,560
網格位置的

1069
00:36:52,560 --> 00:36:54,720
可能性完全由智能體的位置決定

1070
00:36:54,720 --> 00:36:56,079


1071
00:36:56,079 --> 00:37:01,440
， 分數 um

1072
00:37:01,599 --> 00:37:03,920
由位置和上下文決定，

1073
00:37:03,920 --> 00:37:04,960
因此

1074
00:37:04,960 --> 00:37:08,960
如果代理在位置 6

1075
00:37:08,960 --> 00:37:11,680
並且它在上下文 2 中，那麼它將

1076
00:37:11,680 --> 00:37:13,119
獲得正獎勵，

1077
00:37:13,119 --> 00:37:15,119
否則它將重新獲得負

1078
00:37:15,119 --> 00:37:16,320
獎勵或相互獎勵，

1079
00:37:16,320 --> 00:37:20,320
具體取決於它在哪裡 um

1080
00:37:20,320 --> 00:37:22,640
基於 um 試圖

1081
00:37:22,640 --> 00:37:24,079
與強化學習進行比較，

1082
00:37:24,079 --> 00:37:25,839
我們在這裡所做的是，我們

1083
00:37:25,839 --> 00:37:28,560
正在引入 uh 主要偏好

1084
00:37:28,560 --> 00:37:29,119
，

1085
00:37:29,119 --> 00:37:32,640
其中代理具有加上四個 um 為

1086
00:37:32,640 --> 00:37:36,320
um 正獎勵 neg  ative 四四

1087
00:37:36,320 --> 00:37:38,720
抱歉 減去四 負獎勵

1088
00:37:38,720 --> 00:37:39,760
否則它

1089
00:37:39,760 --> 00:37:42,240
在第一階段它期望自己

1090
00:37:42,240 --> 00:37:43,839
處於

1091
00:37:43,839 --> 00:37:46,880
第一個位置

1092
00:37:47,440 --> 00:37:50,560
所以我們將這個特定的

1093
00:37:50,560 --> 00:37:52,480
性別模型和主動推理

1094
00:37:52,480 --> 00:37:53,760
代理與

1095
00:37:53,760 --> 00:37:55,760
兩個強化學習算法進行比較

1096
00:37:55,760 --> 00:37:58,320
所以第一個是 q 使用

1097
00:37:58,320 --> 00:38:00,400
epsilon 貪婪探索進行學習

1098
00:38:00,400 --> 00:38:02,079
，第二個是

1099
00:38:02,079 --> 00:38:03,839
基於貝葉斯模型的強化學習

1100
00:38:03,839 --> 00:38:05,599
算法，使用標準湯普森

1101
00:38:05,599 --> 00:38:06,720
採樣

1102
00:38:06,720 --> 00:38:08,560
和湯普森採樣在這裡是合適的

1103
00:38:08,560 --> 00:38:10,320
程序，因為它需要

1104
00:38:10,320 --> 00:38:12,560
優化雙目標

1105
00:38:12,560 --> 00:38:15,920
獎勵最大化和信息增益

1106
00:38:15,920 --> 00:38:17,440
，這是通過具有此分佈來實現的

1107
00:38:17,440 --> 00:38:19,440
在一個特定的函數

1108
00:38:19,440 --> 00:38:20,960
上，我們

1109
00:38:20,960 --> 00:38:22,720
只通過先驗對其進行先驗

1110
00:38:22,720 --> 00:38:26,879
分佈，我們從

1111
00:38:27,119 --> 00:38:32,320
好的 um 中採樣，

1112
00:38:32,320 --> 00:38:35,760
所以對於兩個 q 襯裡算法，我們

1113
00:38:35,760 --> 00:38:38,960
有兩個 epsilon 貪婪 uh 參數

1114
00:38:38,960 --> 00:38:41,680
，所以一個固定探索設置

1115
00:38:41,680 --> 00:38:42,880
為 0.1

1116
00:38:42,880 --> 00:38:44,240
，然後 另一個我們

1117
00:38:44,240 --> 00:38:46,400
從一個開始的衰敗探索

1118
00:38:46,400 --> 00:38:49,920
並且衰減到零，

1119
00:38:50,880 --> 00:38:54,000
所以首先我們評估了嗯，這個，嗯，

1120
00:38:54,000 --> 00:38:55,440
代理如何在獎勵不變的靜止環境中交互，

1121
00:38:55,440 --> 00:38:56,320


1122
00:38:56,320 --> 00:38:58,960
嗯，

1123
00:38:58,960 --> 00:39:00,800
目標位置總是在 6，

1124
00:39:00,800 --> 00:39:01,760
整個位置

1125
00:39:01,760 --> 00:39:04,240
總是那個年齡，然後我們評估

1126
00:39:04,240 --> 00:39:06,000
代理的性能 嗯

1127
00:39:06,000 --> 00:39:07,599
，從這裡要帶走的關鍵是

1128
00:39:07,599 --> 00:39:08,240


1129
00:39:08,240 --> 00:39:09,920
貝葉斯 rl 和主動推理

1130
00:39:09,920 --> 00:39:11,680
代理都能夠

1131
00:39:11,680 --> 00:39:14,400
快速了解獎勵

1132
00:39:14,400 --> 00:39:16,720
位置的位置，並最大化它

1133
00:39:16,720 --> 00:39:19,119
，這就是嗯，這就是性能是

1134
00:39:19,119 --> 00:39:21,119
一致的 通過比較我們看到的非常嚴格的

1135
00:39:21,119 --> 00:39:23,440
置信界限，

1136
00:39:23,440 --> 00:39:26,960
q 個學習代理是

1137
00:39:26,960 --> 00:39:30,480
嗯，所以對於我們有固定

1138
00:39:30,480 --> 00:39:31,680
探索的

1139
00:39:31,680 --> 00:39:34,640
那個來說，它相當不錯，並且

1140
00:39:34,640 --> 00:39:36,240
能夠降落在獎勵所在的

1141
00:39:36,240 --> 00:39:38,480
位置，但是有一些

1142
00:39:38,480 --> 00:39:40,320
偏差，表示為

1143
00:39:40,320 --> 00:39:43,359
選擇一個隨機動作的百分之十，

1144
00:39:43,359 --> 00:39:46,320
而我們有 epsilon 的 q 學習

1145
00:39:46,320 --> 00:39:48,640
等於 1 以獲得零

1146
00:39:48,640 --> 00:39:51,440
嗯，性能不是最好的

1147
00:39:51,440 --> 00:39:53,119
，對於空模型

1148
00:39:53,119 --> 00:39:55,520
這裡沒有獎勵的主動推理

1149
00:39:55,520 --> 00:39:57,680
代理隨機進入大廳並

1150
00:39:57,680 --> 00:39:59,040
隨機進入目標

1151
00:39:59,040 --> 00:40:02,560
50 但要注意的關鍵

1152
00:40:02,560 --> 00:40:04,160
是，除了非模型之外，

1153
00:40:04,160 --> 00:40:07,040
所有模型都做得相當好並且

1154
00:40:07,040 --> 00:40:07,760
似乎

1155
00:40:07,760 --> 00:40:09,760
在靜止的環境中表現還不錯，

1156
00:40:09,760 --> 00:40:10,960


1157
00:40:10,960 --> 00:40:12,720
所以接下來我不能

1158
00:40:12,720 --> 00:40:14,319
就這些實驗說明

1159
00:40:14,319 --> 00:40:14,960
一點，因為

1160
00:40:14,960 --> 00:40:16,640
它們也確實與傳統的 rl 有點不同，

1161
00:40:16,640 --> 00:40:19,280


1162
00:40:19,280 --> 00:40:22,480
這很好，所以基本上

1163
00:40:22,480 --> 00:40:24,000
就像通常在 ro 中發生的事情是 您

1164
00:40:24,000 --> 00:40:25,760
在

1165
00:40:25,760 --> 00:40:27,040
訓練時間和測試時間

1166
00:40:27,040 --> 00:40:28,400
性能之間存在這種歧義，並且您通常

1167
00:40:28,400 --> 00:40:29,440
尤其是在諸如 q 學習之類的事情中，例如

1168
00:40:29,440 --> 00:40:31,280
您只是在 q 函數上達到了最大值，

1169
00:40:31,280 --> 00:40:32,640
或者您有一個

1170
00:40:32,640 --> 00:40:34,800
策略試圖對 q 函數進行一些搜索，

1171
00:40:34,800 --> 00:40:35,680


1172
00:40:35,680 --> 00:40:39,440
並且 就像給定一個狀態一樣接受那個最大值，

1173
00:40:39,440 --> 00:40:40,720
但是有一種人為的

1174
00:40:40,720 --> 00:40:42,480
區別，

1175
00:40:42,480 --> 00:40:44,960
很明顯，當你獲取數據時，

1176
00:40:44,960 --> 00:40:45,920
你在你所處的

1177
00:40:45,920 --> 00:40:46,960
環境中犯了錯誤 與

1178
00:40:46,960 --> 00:40:48,640
真實環境相結合，因此為了

1179
00:40:48,640 --> 00:40:49,280


1180
00:40:49,280 --> 00:40:51,280
在此處和主動推理之間進行公平比較，在這種情況

1181
00:40:51,280 --> 00:40:52,960
下，您

1182
00:40:52,960 --> 00:40:54,319
在訓練

1183
00:40:54,319 --> 00:40:55,760
時間和測試時間之間沒有真正的區別，這一切都

1184
00:40:55,760 --> 00:40:58,240
只是交互，這就是

1185
00:40:58,240 --> 00:40:59,599
為什麼 q 學習代理

1186
00:40:59,599 --> 00:41:01,520
特別 當 epsilon 被固定為說

1187
00:41:01,520 --> 00:41:03,760
0.1 永遠不會達到最佳策略時，

1188
00:41:03,760 --> 00:41:04,720
僅僅是因為

1189
00:41:04,720 --> 00:41:07,040
我們也有 0.1 的概率採取

1190
00:41:07,040 --> 00:41:08,000
隨機行動，

1191
00:41:08,000 --> 00:41:09,280
所以我們不會

1192
00:41:09,280 --> 00:41:11,359
像你有時在訓練和測試時間之間使用正常的 rl 那樣做出這種區分

1193
00:41:11,359 --> 00:41:11,839


1194
00:41:11,839 --> 00:41:13,520
，我們都在說

1195
00:41:13,520 --> 00:41:14,960
訓練和測試一些基本上是

1196
00:41:14,960 --> 00:41:15,520
一回事，

1197
00:41:15,520 --> 00:41:17,520
所以不管你選擇

1198
00:41:17,520 --> 00:41:19,280
與世界互動，你應該如何被

1199
00:41:19,280 --> 00:41:20,000


1200
00:41:20,000 --> 00:41:22,319
評估，這就是為什麼最初

1201
00:41:22,319 --> 00:41:23,359
看著這些女孩你可能會堅持

1202
00:41:23,359 --> 00:41:24,880
下去，為什麼 q learning 不能解決這個問題

1203
00:41:24,880 --> 00:41:25,359
但是，

1204
00:41:25,359 --> 00:41:27,599
嗯，是的，這只是為了澄清一些事情，

1205
00:41:27,599 --> 00:41:28,800
如果你更熟悉說

1206
00:41:28,800 --> 00:41:30,400
一些像 dprl

1207
00:41:30,400 --> 00:41:34,640
呃實驗程序

1208
00:41:34,640 --> 00:41:38,640
完美謝謝你好吧

1209
00:41:38,640 --> 00:41:41,200
從那開始

1210
00:41:41,200 --> 00:41:41,920
，所以我們

1211
00:41:41,920 --> 00:41:43,520
稍微改變了環境，以便

1212
00:41:43,520 --> 00:41:45,200
更難

1213
00:41:45,200 --> 00:41:46,480
看出當我們在每幾集之後開始進行重定位時

1214
00:41:46,480 --> 00:41:48,480
，貝葉斯和主動推理

1215
00:41:48,480 --> 00:41:49,839
代理是否可能會遇到困難，

1216
00:41:49,839 --> 00:41:52,240


1217
00:41:52,240 --> 00:41:53,680


1218
00:41:53,680 --> 00:41:56,400
所以特別是我們交換了

1219
00:41:56,400 --> 00:41:58,079
目標 以及

1220
00:41:58,079 --> 00:42:01,119
在時間點 21 的時間點

1221
00:42:01,119 --> 00:42:04,319
121 141

1222
00:42:04,319 --> 00:42:08,319
251 和 451 的整個位置，因此您可以看到，

1223
00:42:08,319 --> 00:42:10,319
在這些圖中顯示了灰

1224
00:42:10,319 --> 00:42:12,000
線的線，因此這些點的

1225
00:42:12,000 --> 00:42:15,200
位置都被翻轉了，就像第一個的

1226
00:42:15,200 --> 00:42:16,720
固定設置一樣

1227
00:42:16,720 --> 00:42:19,440
20 次試驗所有亞洲人似乎都

1228
00:42:19,440 --> 00:42:20,000
在

1229
00:42:20,000 --> 00:42:22,480
按照您的預期進行，所以貝葉斯

1230
00:42:22,480 --> 00:42:24,240
rl 和主動推理代理都

1231
00:42:24,240 --> 00:42:24,640
做得

1232
00:42:24,640 --> 00:42:27,760
很好

1233
00:42:27,760 --> 00:42:30,800


1234
00:42:30,800 --> 00:42:32,000


1235
00:42:32,000 --> 00:42:34,960


1236
00:42:34,960 --> 00:42:35,680
隨著

1237
00:42:35,680 --> 00:42:38,400
衰變的探索正在像

1238
00:42:38,400 --> 00:42:39,520
以前一樣做，

1239
00:42:39,520 --> 00:42:42,640
但是當你把它翻轉過來時

1240
00:42:42,640 --> 00:42:44,240
，目標位置會

1241
00:42:44,240 --> 00:42:47,520
改變，你注意到的是

1242
00:42:47,520 --> 00:42:48,640


1243
00:42:48,640 --> 00:42:52,079
貝葉斯rl a  gent 獎勵的數量

1244
00:42:52,079 --> 00:42:53,760
或它獲得的分數

1245
00:42:53,760 --> 00:42:56,800
非常低，然後我們看到一個階段

1246
00:42:56,800 --> 00:42:57,119
，

1247
00:42:57,119 --> 00:43:00,000
它然後轉換並

1248
00:43:00,000 --> 00:43:02,160
再次成為最佳策略

1249
00:43:02,160 --> 00:43:04,560
，與主動推理

1250
00:43:04,560 --> 00:43:05,599
代理相比，

1251
00:43:05,599 --> 00:43:08,720
它在第一次嘗試後立即

1252
00:43:08,720 --> 00:43:10,560
執行 不正確能夠

1253
00:43:10,560 --> 00:43:12,960
切換到積極和抱歉

1254
00:43:12,960 --> 00:43:14,640
適當的政策

1255
00:43:14,640 --> 00:43:17,680
，其原因是對於這些呃

1256
00:43:17,680 --> 00:43:19,440
rl 設置，我們將其

1257
00:43:19,440 --> 00:43:20,880
視為一個學習問題，

1258
00:43:20,880 --> 00:43:22,480
您需要首先進行反向並

1259
00:43:22,480 --> 00:43:24,480
了解獎勵位置的位置

1260
00:43:24,480 --> 00:43:26,720
和 然後我們學習新的獎勵

1261
00:43:26,720 --> 00:43:28,720
位置，所以你會看到

1262
00:43:28,720 --> 00:43:31,280
q 學習 uh 對於不同的

1263
00:43:31,280 --> 00:43:33,200
epsilon 貪心參數化

1264
00:43:33,200 --> 00:43:34,720
和貝葉斯 rl，我們

1265
00:43:34,720 --> 00:43:36,160
看到的是一致的，

1266
00:43:36,160 --> 00:43:38,160
而對於主動推理代理，

1267
00:43:38,160 --> 00:43:39,599
因為我們將其視為

1268
00:43:39,599 --> 00:43:41,599
作為推理問題的規劃，其中

1269
00:43:41,599 --> 00:43:42,960
來自先前狀態的後驗被

1270
00:43:42,960 --> 00:43:45,359
移動到獎品並

1271
00:43:45,359 --> 00:43:46,160
作為先驗移動

1272
00:43:46,160 --> 00:43:49,200
並且代理能夠立即

1273
00:43:49,200 --> 00:43:52,160
嗯意識到

1274
00:43:52,160 --> 00:43:52,960


1275
00:43:52,960 --> 00:43:55,200
遵循這個時間步驟的當前策略是一種適當性

1276
00:43:55,200 --> 00:43:57,040
，它是它對下

1277
00:43:57,040 --> 00:44:00,480
一個 um 到另一個的策略

1278
00:44:00,480 --> 00:44:02,480


1279
00:44:02,480 --> 00:44:04,000


1280
00:44:04,000 --> 00:44:05,599


1281
00:44:05,599 --> 00:44:08,160
或者整個位置

1282
00:44:08,160 --> 00:44:09,839
都

1283
00:44:09,839 --> 00:44:11,839
還可以，嗯，菲爾，你想添加

1284
00:44:11,839 --> 00:44:14,799
一些東西還是

1285
00:44:15,440 --> 00:44:17,440
不，我認為同樣的一點也適用於

1286
00:44:17,440 --> 00:44:18,480
這個

1287
00:44:18,480 --> 00:44:22,160
是肯定的，嗯，

1288
00:44:22,160 --> 00:44:23,760
所以只是總結這兩個

1289
00:44:23,760 --> 00:44:25,760
比較，

1290
00:44:25,760 --> 00:44:29,440
我用非焊機固定設置

1291
00:44:29,440 --> 00:44:32,319
嗯所有 包括

1292
00:44:32,319 --> 00:44:34,160
貝葉斯 rl 和 q-learning 在內的代理類型 um 將是

1293
00:44:34,160 --> 00:44:36,400
合理的使用框架，而

1294
00:44:36,400 --> 00:44:39,280
具有非平穩隨機設置的 uh

1295
00:44:39,280 --> 00:44:40,480


1296
00:44:40,480 --> 00:44:42,240
具有主動推理代理可能

1297
00:44:42,240 --> 00:44:44,079
是處理不斷變化的

1298
00:44:44,079 --> 00:44:45,200
動態

1299
00:44:45,200 --> 00:44:47,359
um 的適當方式，但關鍵警告是 是

1300
00:44:47,359 --> 00:44:48,480
你可以

1301
00:44:48,480 --> 00:44:50,480


1302
00:44:50,480 --> 00:44:53,119
在貝葉斯 rl 或 q

1303
00:44:53,119 --> 00:44:55,599
學習或 rl 框架中引入更多額外的複雜性，

1304
00:44:55,599 --> 00:44:56,560
以允許

1305
00:44:56,560 --> 00:44:58,800
呃有一種處理不確定性的方法，但

1306
00:44:58,800 --> 00:44:59,760
它不會 這不是

1307
00:44:59,760 --> 00:45:02,240
一種自然的添加方式，你

1308
00:45:02,240 --> 00:45:04,960
必須以特定的方式增加

1309
00:45:04,960 --> 00:45:06,880
函數或算法

1310
00:45:06,880 --> 00:45:09,359
來證明它是正確的

1311
00:45:09,359 --> 00:45:12,319


1312
00:45:12,319 --> 00:45:15,040


1313
00:45:15,040 --> 00:45:16,960
使用主動

1314
00:45:16,960 --> 00:45:19,200
推理，嗯，

1315
00:45:19,200 --> 00:45:22,079
當你

1316
00:45:22,079 --> 00:45:22,880
不了解世界，

1317
00:45:22,880 --> 00:45:25,440
或者你對

1318
00:45:25,440 --> 00:45:27,040
可以做的事情類型沒有偏好時，

1319
00:45:27,040 --> 00:45:28,640
因為正如我們看到的主動

1320
00:45:28,640 --> 00:45:30,240
推理模型，空模型只是在

1321
00:45:30,240 --> 00:45:32,319
探索 它並沒有真正做

1322
00:45:32,319 --> 00:45:34,720
太多，這讓我們

1323
00:45:34,720 --> 00:45:36,319
想到了我

1324
00:45:36,319 --> 00:45:38,160
在演示開始時介紹的初始點之一，那

1325
00:45:38,160 --> 00:45:39,599
就是在一個積極的影響

1326
00:45:39,599 --> 00:45:41,200
框架內，我們並不真正關心

1327
00:45:41,200 --> 00:45:42,560


1328
00:45:42,560 --> 00:45:45,920
我們可以學習的獎勵功能 我們可以學習 基於

1329
00:45:45,920 --> 00:45:48,640
與環境的一些交互

1330
00:45:48,640 --> 00:45:52,240
，嗯

1331
00:45:52,240 --> 00:45:54,480
，我們所做的是我們進行了一些

1332
00:45:54,480 --> 00:45:56,160
不同的模擬，以查看

1333
00:45:56,160 --> 00:45:57,920
主動推理代理如何

1334
00:45:57,920 --> 00:45:59,520
在

1335
00:45:59,520 --> 00:46:02,079
沒有先驗 p 的情況下選擇不同類型的策略 參考資料

1336
00:46:02,079 --> 00:46:04,560
，為此我們做了三個不同的

1337
00:46:04,560 --> 00:46:05,280
實驗

1338
00:46:05,280 --> 00:46:08,160
，我們允許隨著時間的推移學習其中一個或

1339
00:46:08,160 --> 00:46:09,040
可能性

1340
00:46:09,040 --> 00:46:11,359
和所有結果偏好，

1341
00:46:11,359 --> 00:46:12,240


1342
00:46:12,240 --> 00:46:15,440
並觀察

1343
00:46:15,440 --> 00:46:16,560
當

1344
00:46:16,560 --> 00:46:19,680
這種學習對不同的

1345
00:46:19,680 --> 00:46:22,640
嗯偏好發生時代理如何交互，以及

1346
00:46:22,640 --> 00:46:24,160
我們的方式 是不是使用

1347
00:46:24,160 --> 00:46:26,960
了共軛模型，因為

1348
00:46:26,960 --> 00:46:28,800
這些是離散狀態模型，我們

1349
00:46:28,800 --> 00:46:30,800
在模型參數上有分類分佈，我們

1350
00:46:30,800 --> 00:46:31,839


1351
00:46:31,839 --> 00:46:33,760


1352
00:46:33,760 --> 00:46:34,880
在頂部引入方向分佈

1353
00:46:34,880 --> 00:46:36,960
作為混合價格並學習那些

1354
00:46:36,960 --> 00:46:38,319
超價格，

1355
00:46:38,319 --> 00:46:41,119
我們可以深入了解 如果

1356
00:46:41,119 --> 00:46:42,800
有人有任何問題，但

1357
00:46:42,800 --> 00:46:46,400
只是為了簡單起見，嗯，假設

1358
00:46:46,400 --> 00:46:48,800
所有的狄利克雷分佈都

1359
00:46:48,800 --> 00:46:50,880
設置為完全平坦，

1360
00:46:50,880 --> 00:46:54,079
並且代理都有機會

1361
00:46:54,079 --> 00:46:54,800


1362
00:46:54,800 --> 00:46:57,359
根據他們進行的不同交互進行著陸，

1363
00:46:57,359 --> 00:46:58,400
所以第一組

1364
00:46:58,400 --> 00:47:00,160
實驗就像模擬 我們

1365
00:47:00,160 --> 00:47:02,319
運行的目的是了解

1366
00:47:02,319 --> 00:47:04,640
嗯代理將如何減少

1367
00:47:04,640 --> 00:47:05,520
對所處

1368
00:47:05,520 --> 00:47:07,280
環境的不確定性 互動，所以

1369
00:47:07,280 --> 00:47:09,040
如果它不

1370
00:47:09,040 --> 00:47:12,319
知道結冰的湖

1371
00:47:12,319 --> 00:47:14,720
，它將如何交互或探索那個

1372
00:47:14,720 --> 00:47:15,599
湖，

1373
00:47:15,599 --> 00:47:18,880
嗯，我們看到的是，

1374
00:47:18,880 --> 00:47:21,920
在這個特定的例子中，代理

1375
00:47:21,920 --> 00:47:23,680
只是對探索感興趣，

1376
00:47:23,680 --> 00:47:25,920
並不特別關心

1377
00:47:25,920 --> 00:47:28,559
目標或整個位置在哪裡

1378
00:47:28,559 --> 00:47:29,040


1379
00:47:29,040 --> 00:47:32,079
，這在我們看到

1380
00:47:32,079 --> 00:47:35,119
的一系列不同的探索 um

1381
00:47:35,119 --> 00:47:36,800
軌跡中突出顯示，所以第

1382
00:47:36,800 --> 00:47:38,960
一個是代理落在

1383
00:47:38,960 --> 00:47:39,359
大廳

1384
00:47:39,359 --> 00:47:42,400
um 的地方，然後是另一個它

1385
00:47:42,400 --> 00:47:43,359
四處走動

1386
00:47:43,359 --> 00:47:46,079
並最終到達的地方

1387
00:47:46,079 --> 00:47:48,160
第二集的第二集在目標位置

1388
00:47:48,160 --> 00:47:50,079
，在某個地方，它只是通過來回來回結束這一

1389
00:47:50,079 --> 00:47:51,200
集，

1390
00:47:51,200 --> 00:47:52,960
所以這

1391
00:47:52,960 --> 00:47:54,800
只是地板探索，

1392
00:47:54,800 --> 00:47:58,000
沒有更多的東西可以聽到，我們運行

1393
00:47:58,000 --> 00:48:01,440
的下一組分析或

1394
00:48:01,440 --> 00:48:03,520
模擬是看看什麼

1395
00:48:03,520 --> 00:48:05,440
如果代理了解這個

1396
00:48:05,440 --> 00:48:07,599
世界，但對他們期望自己所處的結果類型沒有偏好，那麼就會發生這種情況

1397
00:48:07,599 --> 00:48:08,480


1398
00:48:08,480 --> 00:48:12,160
，為此，

1399
00:48:12,160 --> 00:48:15,920
我們運行了多個不同的模擬，

1400
00:48:15,920 --> 00:48:19,200
並看到在沒有

1401
00:48:19,200 --> 00:48:22,240
任何 某種偏好，

1402
00:48:22,240 --> 00:48:24,079
嗯，如果首先遇到它，整體實際上會變得

1403
00:48:24,079 --> 00:48:25,599
非常有吸引力

1404
00:48:25,599 --> 00:48:28,480
，所以我們看到，

1405
00:48:28,480 --> 00:48:29,440
在

1406
00:48:29,440 --> 00:48:31,680
第一個圖中，代理學會了

1407
00:48:31,680 --> 00:48:34,000
更喜歡躲在大廳裡，

1408
00:48:34,000 --> 00:48:36,160
而在我們看到的第二種類型的試驗

1409
00:48:36,160 --> 00:48:38,000
中，代理

1410
00:48:38,000 --> 00:48:41,520
呃展示了 實際

1411
00:48:41,520 --> 00:48:43,359
去目標位置的偏好，所以這

1412
00:48:43,359 --> 00:48:45,200
完全取決於代理最初暴露的

1413
00:48:45,200 --> 00:48:47,680
實例化或刺激類型，

1414
00:48:47,680 --> 00:48:49,119


1415
00:48:49,119 --> 00:48:51,119
這決定了

1416
00:48:51,119 --> 00:48:52,319
它將

1417
00:48:52,319 --> 00:48:55,200
學習的

1418
00:48:56,240 --> 00:48:59,520
偏好類型，然後是我們運行的最後一組

1419
00:48:59,520 --> 00:49:01,520
模擬 只是為了

1420
00:49:01,520 --> 00:49:03,119
檢查當我們與認知命令交互時會發生什麼

1421
00:49:03,119 --> 00:49:04,640
，

1422
00:49:04,640 --> 00:49:06,640
實際上解決

1423
00:49:06,640 --> 00:49:08,000


1424
00:49:08,000 --> 00:49:09,440
了代理正在

1425
00:49:09,440 --> 00:49:11,200
與之交互的環境的不確定性，特別是給定狀態

1426
00:49:11,200 --> 00:49:13,200
的結果之間的可能性映射

1427
00:49:13,200 --> 00:49:14,480


1428
00:49:14,480 --> 00:49:16,880


1429
00:49:16,880 --> 00:49:18,559
與代理期望自己期望的事務狀態的不確定性

1430
00:49:18,559 --> 00:49:19,839
進入

1431
00:49:19,839 --> 00:49:22,960
，我們看到的是，如果我們

1432
00:49:22,960 --> 00:49:25,520
允許足夠數量的

1433
00:49:25,520 --> 00:49:27,040
試驗通過 s

1434
00:49:27,040 --> 00:49:28,640
在這個特定設置中的代理學會了

1435
00:49:28,640 --> 00:49:31,119


1436
00:49:31,119 --> 00:49:34,800
在 x 集之後更喜歡騎抱歉的隱藏洞，但是它對它期望自己的結果類型有一個

1437
00:49:34,800 --> 00:49:36,000
非常明顯的

1438
00:49:36,000 --> 00:49:38,960
偏好

1439
00:49:38,960 --> 00:49:39,280


1440
00:49:39,280 --> 00:49:41,119
，這

1441
00:49:41,119 --> 00:49:42,480
取決於特定的

1442
00:49:42,480 --> 00:49:46,400
呃一個抱歉的特定時間點

1443
00:49:46,400 --> 00:49:50,079
所以這把我帶到了最後一個

1444
00:49:50,079 --> 00:49:53,839
模擬，所以我要總結一下

1445
00:49:53,839 --> 00:49:57,200
，嗯，在主動推理中，

1446
00:49:57,200 --> 00:49:59,040
我開始演示時

1447
00:49:59,040 --> 00:50:00,640
說它是一種特殊的算法，當我們在貝葉斯中操作時，它給了

1448
00:50:00,640 --> 00:50:01,200
我們

1449
00:50:01,200 --> 00:50:03,599
非常好的東西來考慮

1450
00:50:03,599 --> 00:50:04,559


1451
00:50:04,559 --> 00:50:07,760
或基於信念的

1452
00:50:07,760 --> 00:50:08,960
設置，這

1453
00:50:08,960 --> 00:50:11,680
首先

1454
00:50:11,680 --> 00:50:13,359


1455
00:50:13,359 --> 00:50:15,119
是我們從

1456
00:50:15,119 --> 00:50:17,040
特定的預期自由能

1457
00:50:17,040 --> 00:50:19,440
分解中獲得的認知探索和內在動機的主要說明，我們經歷了

1458
00:50:19,440 --> 00:50:20,400
第二件事

1459
00:50:20,400 --> 00:50:23,359
，我想強調的是，

1460
00:50:23,359 --> 00:50:25,040
在積極的推理設置下，我們

1461
00:50:25,040 --> 00:50:26,000
沒有 必須明確

1462
00:50:26,000 --> 00:50:28,240
指定一個獎勵函數，我們

1463
00:50:28,240 --> 00:50:29,119
在最後，

1464
00:50:29,119 --> 00:50:32,079
呃第二組模擬

1465
00:50:32,079 --> 00:50:34,000
中看到代理也可以學習自己的

1466
00:50:34,000 --> 00:50:35,680
獎勵 並且更願意從 rl 設置中變成

1467
00:50:35,680 --> 00:50:37,359
非常違反直覺的東西，

1468
00:50:37,359 --> 00:50:39,839
其中

1469
00:50:39,839 --> 00:50:41,440
來自環境的信號說某事

1470
00:50:41,440 --> 00:50:42,319
不好，但

1471
00:50:42,319 --> 00:50:44,160
代理的內部

1472
00:50:44,160 --> 00:50:46,240
偏好動機允許它然後

1473
00:50:46,240 --> 00:50:48,559
做一些

1474
00:50:48,559 --> 00:50:50,319
與環境期望完全不一致的事情

1475
00:50:50,319 --> 00:50:51,359


1476
00:50:51,359 --> 00:50:53,599
最後因為這種信念 um

1477
00:50:53,599 --> 00:50:55,280
入侵設置不確定性

1478
00:50:55,280 --> 00:50:57,119
是信念更新 um 的自然組成部分，

1479
00:50:57,119 --> 00:51:00,559
因此在

1480
00:51:00,559 --> 00:51:02,319
靜態設置中，主動推理

1481
00:51:02,319 --> 00:51:04,000
代理的性能與強化學習代理一樣好，

1482
00:51:04,000 --> 00:51:05,200


1483
00:51:05,200 --> 00:51:07,359
但是在非靜態設置中，它們的

1484
00:51:07,359 --> 00:51:08,960
表現優於它們的能力

1485
00:51:08,960 --> 00:51:10,480
執行這個計劃作為

1486
00:51:10,480 --> 00:51:13,280
推理，我

1487
00:51:13,280 --> 00:51:15,280
認為這不是一個結論性的陳述，

1488
00:51:15,280 --> 00:51:15,760
但

1489
00:51:15,760 --> 00:51:18,240
它是一個很好的方式來開始

1490
00:51:18,240 --> 00:51:19,040


1491
00:51:19,040 --> 00:51:21,280
思考如果你正在擴展主動推理

1492
00:51:21,280 --> 00:51:23,440
代理以在與強化學習相同類型的環境中進行交互

1493
00:51:23,440 --> 00:51:24,160


1494
00:51:24,160 --> 00:51:26,960


1495
00:51:26,960 --> 00:51:27,760
可能

1496
00:51:27,760 --> 00:51:30,079
有點難以解決的代理，因為

1497
00:51:30,079 --> 00:51:30,839
存在一些

1498
00:51:30,839 --> 00:51:32,640
非平穩性或一些奇怪的東西

1499
00:51:32,640 --> 00:51:33,920


1500
00:51:33,920 --> 00:51:34,640
環境中發生的波動

1501
00:51:34,640 --> 00:51:36,240
和主動推理代理

1502
00:51:36,240 --> 00:51:38,240
可能會表現得非常好，如果我們

1503
00:51:38,240 --> 00:51:42,000
有這個計劃作為推理設置

1504
00:51:42,079 --> 00:51:44,800
嗯，這讓我到了演示的結尾，

1505
00:51:44,800 --> 00:51:46,480
所以我只想感謝

1506
00:51:46,480 --> 00:51:48,240
參與這項工作的每個人

1507
00:51:48,240 --> 00:51:48,559
，

1508
00:51:48,559 --> 00:51:51,520
所謂的湯姆 菲爾和所有

1509
00:51:51,520 --> 00:51:52,640
幫助我思考

1510
00:51:52,640 --> 00:51:55,920
我提出的有趣想法的人，

1511
00:51:55,920 --> 00:51:58,079
以及每個人的聆聽，謝謝

1512
00:51:58,079 --> 00:51:59,920
，

1513
00:51:59,920 --> 00:52:04,400
謝謝你精彩的演講，

1514
00:52:04,400 --> 00:52:07,119
所以你可以取消分享，我們可以

1515
00:52:07,119 --> 00:52:08,000
問一些問題

1516
00:52:08,000 --> 00:52:09,599
，如果有人在看直播

1517
00:52:09,599 --> 00:52:11,359
想問問題 他們

1518
00:52:11,359 --> 00:52:14,400
非常受歡迎，所以也許我會從

1519
00:52:14,400 --> 00:52:16,160


1520
00:52:16,160 --> 00:52:18,000
一個一般性的觀點開始，

1521
00:52:18,000 --> 00:52:20,480
看到你

1522
00:52:20,480 --> 00:52:21,280
在

1523
00:52:21,280 --> 00:52:22,880
強化學習和

1524
00:52:22,880 --> 00:52:24,640
主動推理範式

1525
00:52:24,640 --> 00:52:27,040
之間有多麼清晰的區別真是太棒了，我只有一個問題是你

1526
00:52:27,040 --> 00:52:27,760


1527
00:52:27,760 --> 00:52:30,319
在案例中提到的 一個時間步長 一個時間

1528
00:52:30,319 --> 00:52:31,599
範圍

1529
00:52:31,599 --> 00:52:33,359


1530
00:52:33,359 --> 00:52:34,960
強化學習方法和主動 infe 之間是等價的

1531
00:52:34,960 --> 00:52:36,640


1532
00:52:36,640 --> 00:52:38,640
現在人們正在使用強化

1533
00:52:38,640 --> 00:52:40,400
學習

1534
00:52:40,400 --> 00:52:42,400
在不確定的情況下對未來進行規劃，

1535
00:52:42,400 --> 00:52:43,599


1536
00:52:43,599 --> 00:52:46,240
那麼他們如何完成

1537
00:52:46,240 --> 00:52:47,839
這些規劃

1538
00:52:47,839 --> 00:52:50,400
以及在哪些情況下

1539
00:52:50,400 --> 00:52:51,440
主動推理

1540
00:52:51,440 --> 00:52:53,920
可能能夠進入這些 um

1541
00:52:53,920 --> 00:52:54,800
設置並

1542
00:52:54,800 --> 00:52:57,760
可能做得更好，我認為這

1543
00:52:57,760 --> 00:52:58,480
已經 一個問題，

1544
00:52:58,480 --> 00:53:00,400
一個好問題，

1545
00:53:00,400 --> 00:53:02,720
如果我錯過了什麼，你可以隨意加入，但是

1546
00:53:02,720 --> 00:53:04,720
在 url 設置中，據我

1547
00:53:04,720 --> 00:53:07,040
所知，如果他們正在考慮

1548
00:53:07,040 --> 00:53:09,280
大於 1 的時間範圍，那麼

1549
00:53:09,280 --> 00:53:10,960
他們有分層 rl

1550
00:53:10,960 --> 00:53:13,280
或者他們有選擇的地方

1551
00:53:13,280 --> 00:53:14,800
考慮

1552
00:53:14,800 --> 00:53:16,240
大於

1553
00:53:16,240 --> 00:53:18,800
一個的軌跡，允許他們

1554
00:53:18,800 --> 00:53:20,720


1555
00:53:20,720 --> 00:53:24,319
在他們喜歡之前考慮

1556
00:53:24,319 --> 00:53:26,000


1557
00:53:26,000 --> 00:53:27,680


1558
00:53:27,680 --> 00:53:32,800
整個遊戲

1559
00:53:32,800 --> 00:53:34,880
序列 還沒有真正

1560
00:53:34,880 --> 00:53:36,559
與選項一起工作過，也許

1561
00:53:36,559 --> 00:53:39,119
菲爾你知道嗎，嗯，是的，所以選項

1562
00:53:39,119 --> 00:53:41,280
就像

1563
00:53:41,280 --> 00:53:45,119
實現這種多步驟的一種方法 ort of

1564
00:53:45,119 --> 00:53:47,680
um like I guess oversized 幾乎就像

1565
00:53:47,680 --> 00:53:49,359
這種歸納偏見一樣，

1566
00:53:49,359 --> 00:53:52,000
你會喜歡你的選擇將是

1567
00:53:52,000 --> 00:53:54,079
一種連續的塊，比如

1568
00:53:54,079 --> 00:53:55,520
不止一個步驟，而

1569
00:53:55,520 --> 00:53:57,280
一種政策就像我

1570
00:53:57,280 --> 00:53:58,720
要邁出一步一樣 然後我將擁有

1571
00:53:58,720 --> 00:53:59,359


1572
00:53:59,359 --> 00:54:02,720
我存在的這種下一個狀態，但是我

1573
00:54:02,720 --> 00:54:04,240
認為消除歧義的重要之處

1574
00:54:04,240 --> 00:54:06,000
實際上是您可以對模型樣式進行某種規劃，

1575
00:54:06,000 --> 00:54:07,440
因為

1576
00:54:07,440 --> 00:54:09,119
您可以將主動推理視為一種

1577
00:54:09,119 --> 00:54:10,880
基於模型的模型

1578
00:54:10,880 --> 00:54:13,680
您知道 rl 範式，並且

1579
00:54:13,680 --> 00:54:15,280
確實像在基於模型的情況下

1580
00:54:15,280 --> 00:54:18,400
一樣，您可以擁有

1581
00:54:18,400 --> 00:54:20,800
類似於在主動推理中所做的規劃方法，

1582
00:54:20,800 --> 00:54:21,440


1583
00:54:21,440 --> 00:54:23,040
但是我想

1584
00:54:23,040 --> 00:54:24,480
這裡的關鍵區別是

1585
00:54:24,480 --> 00:54:27,440
當您在基於模型的 rl 中進行規劃時，

1586
00:54:27,440 --> 00:54:27,760
您

1587
00:54:27,760 --> 00:54:29,920
只是在嘗試 帶你傾向於

1588
00:54:29,920 --> 00:54:30,800
採取某種

1589
00:54:30,800 --> 00:54:32,880
西裝，比如一種迷你進化

1590
00:54:32,880 --> 00:54:34,079
方法，通常

1591
00:54:34,079 --> 00:54:35,920
是交叉熵方法，你正在

1592
00:54:35,920 --> 00:54:37,440
做的是

1593
00:54:37,440 --> 00:54:38,640
對你在某個時刻採取的所有行動的這種搜索

1594
00:54:38,640 --> 00:54:40,480


1595
00:54:40,480 --> 00:54:42,319


1596
00:54:42,319 --> 00:54:45,200
在假設 20 步的範圍內最大化你的獎勵的時間，嗯

1597
00:54:45,200 --> 00:54:46,559
，你可以做一些聰明的事情，你可以

1598
00:54:46,559 --> 00:54:48,880
用 q 值函數 um 終止，

1599
00:54:48,880 --> 00:54:51,280
但從根本上來說，它不會

1600
00:54:51,280 --> 00:54:52,799
讓你遠離這種

1601
00:54:52,799 --> 00:54:55,359
定義差異，即

1602
00:54:55,359 --> 00:54:57,760
在主動推理中，您正在

1603
00:54:57,760 --> 00:54:59,119
對動作進行這種計劃，

1604
00:54:59,119 --> 00:55:00,480
即使它們看起來表面上

1605
00:55:00,480 --> 00:55:02,000
相似，

1606
00:55:02,000 --> 00:55:03,680
基本上就像繪製大量動作

1607
00:55:03,680 --> 00:55:05,280
並查看

1608
00:55:05,280 --> 00:55:06,000


1609
00:55:06,000 --> 00:55:09,119
在主動推理的情況下哪些動作最大化某種效用 所有

1610
00:55:09,119 --> 00:55:10,880
有用的信息 以及

1611
00:55:10,880 --> 00:55:12,559
關於 um 的期望值，

1612
00:55:12,559 --> 00:55:15,760
比如探索，它和

1613
00:55:15,760 --> 00:55:17,839
um 開發都包含在最大化中，

1614
00:55:17,839 --> 00:55:20,000
而在 rl 中，

1615
00:55:20,000 --> 00:55:23,440
有點難以理解，就像

1616
00:55:23,440 --> 00:55:24,799
在經典意義上，我們只是

1617
00:55:24,799 --> 00:55:26,960
試圖最大化獎勵，但是

1618
00:55:26,960 --> 00:55:28,720
你可以擁有 啟發式你說哦，

1619
00:55:28,720 --> 00:55:30,240
但也許我也想最大化

1620
00:55:30,240 --> 00:55:31,200


1621
00:55:31,200 --> 00:55:33,359
模型不確定性的一些概念，你知道它

1622
00:55:33,359 --> 00:55:34,640


1623
00:55:34,640 --> 00:55:37,280
有點難以自然整合 將

1624
00:55:37,280 --> 00:55:39,680
所有這些方法評價為同一件事，

1625
00:55:39,680 --> 00:55:41,680
嗯，因為就像在積極影響的例子中一樣，

1626
00:55:41,680 --> 00:55:42,400
因為一切都是一個

1627
00:55:42,400 --> 00:55:43,280
分佈

1628
00:55:43,280 --> 00:55:44,720
，如果你願意，你只需將它放在一個超優先級之上，

1629
00:55:44,720 --> 00:55:46,480
如果你不想處理它，你只需將

1630
00:55:46,480 --> 00:55:47,760


1631
00:55:47,760 --> 00:55:49,920
其整合出來 所以是的，這

1632
00:55:49,920 --> 00:55:50,880
就是我覺得

1633
00:55:50,880 --> 00:55:53,920
有這種差異

1634
00:55:54,000 --> 00:55:57,280
很酷的地方，你認為什麼樣的設置

1635
00:55:57,280 --> 00:56:00,640
可以像推理那樣使用那些類型的呃

1636
00:56:00,640 --> 00:56:03,200
動作和計劃作為

1637
00:56:03,200 --> 00:56:04,240
推理，

1638
00:56:04,240 --> 00:56:06,960
比如什麼樣的數據集或

1639
00:56:06,960 --> 00:56:08,880
問題或 上下文

1640
00:56:08,880 --> 00:56:10,799
是人們目前正在使用另一種類型

1641
00:56:10,799 --> 00:56:12,720
的方法，但是您很高興看到

1642
00:56:12,720 --> 00:56:17,839
主動推理髮揮作用

1643
00:56:18,720 --> 00:56:21,280


1644
00:56:21,280 --> 00:56:23,520


1645
00:56:23,520 --> 00:56:26,640


1646
00:56:26,640 --> 00:56:30,079


1647
00:56:30,079 --> 00:56:31,839
與環境互動是

1648
00:56:31,839 --> 00:56:33,839
通過獎勵功能，

1649
00:56:33,839 --> 00:56:36,079
嗯，所以任何你沒有真正

1650
00:56:36,079 --> 00:56:37,280
擁有的地方，或者你有

1651
00:56:37,280 --> 00:56:40,640
一個正在改變的環境，

1652
00:56:40,640 --> 00:56:43,440
但我知道這就像一個整體

1653
00:56:43,440 --> 00:56:44,160


1654
00:56:44,160 --> 00:56:46,720
rl 社區中的很多人都

1655
00:56:46,720 --> 00:56:47,599
在研究

1656
00:56:47,599 --> 00:56:50,160
內在動機或內部

1657
00:56:50,160 --> 00:56:51,520
動機，

1658
00:56:51,520 --> 00:56:54,160
所以這些事情確實與

1659
00:56:54,160 --> 00:56:54,559


1660
00:56:54,559 --> 00:56:58,960
積極影響公式重疊，

1661
00:56:58,960 --> 00:57:02,400
但是我認為對我來說

1662
00:57:02,400 --> 00:57:04,640
，積極影響更有趣的方面

1663
00:57:04,640 --> 00:57:06,079
來自於

1664
00:57:06,079 --> 00:57:09,440
你開始時的事實 考慮一下

1665
00:57:09,440 --> 00:57:12,240
生物製劑，如果你正在

1666
00:57:12,240 --> 00:57:13,280
為

1667
00:57:13,280 --> 00:57:16,240
患者或精神分裂症患者建模，你可以

1668
00:57:16,240 --> 00:57:18,079
使用這個貝葉斯框架，你可以

1669
00:57:18,079 --> 00:57:21,760
改變先驗，

1670
00:57:21,760 --> 00:57:24,960
看看這個人是如何互動的

1671
00:57:24,960 --> 00:57:27,920
，我想這可能是

1672
00:57:27,920 --> 00:57:28,799
他們的政策

1673
00:57:28,799 --> 00:57:30,079
喜歡 他們評估

1674
00:57:30,079 --> 00:57:32,319
政策的方式被打破了，或者可能只是

1675
00:57:32,319 --> 00:57:35,599
其他部分不同，但

1676
00:57:35,599 --> 00:57:38,720
我認為在 rl 設置中，如果

1677
00:57:38,720 --> 00:57:43,440
你走出

1678
00:57:43,440 --> 00:57:46,480
um 標準遊戲模式，就像

1679
00:57:46,480 --> 00:57:47,359
魔術或

1680
00:57:47,359 --> 00:57:49,040
健身房環境一樣，開始進入

1681
00:57:49,040 --> 00:57:50,799
那些 哪個更開放，你

1682
00:57:50,799 --> 00:57:52,480
沒有任何獎勵，那麼

1683
00:57:52,480 --> 00:57:54,160
積極的影響可能會

1684
00:57:54,160 --> 00:57:56,240
很有用，但

1685
00:57:56,240 --> 00:57:58,640
我有點猶豫 說它

1686
00:57:58,640 --> 00:57:59,599
是否會

1687
00:57:59,599 --> 00:58:02,640
更好，因為如果你開始

1688
00:58:02,640 --> 00:58:05,040
用各種有趣的組件來增強貝葉斯 rl，

1689
00:58:05,040 --> 00:58:06,839


1690
00:58:06,839 --> 00:58:10,160
那麼在一定程度上它會

1691
00:58:10,160 --> 00:58:13,680
在放大的前面活躍

1692
00:58:13,680 --> 00:58:15,760
，這可能是一個有爭議的

1693
00:58:15,760 --> 00:58:16,880
點，

1694
00:58:16,880 --> 00:58:20,319
但我認為這取決於

1695
00:58:20,319 --> 00:58:22,079
究竟是什麼 你正在整合，

1696
00:58:22,079 --> 00:58:23,920
這就是為什麼對於這個特定的

1697
00:58:23,920 --> 00:58:26,000
演示和我們的工作，我們非常

1698
00:58:26,000 --> 00:58:27,599
謹慎地定義

1699
00:58:27,599 --> 00:58:29,599
強化學習的含義，

1700
00:58:29,599 --> 00:58:31,760
即你必須有這個獎勵

1701
00:58:31,760 --> 00:58:33,280
函數，你想

1702
00:58:33,280 --> 00:58:35,040
最大化這個獎勵函數

1703
00:58:35,040 --> 00:58:37,280
以及你包含在 rl 中的任何東西

1704
00:58:37,280 --> 00:58:39,040


1705
00:58:39,040 --> 00:58:42,000
框架必須像目標必須存在的對像一樣，

1706
00:58:42,000 --> 00:58:44,000
但是

1707
00:58:44,000 --> 00:58:45,839
如果您繞過它們說好的，

1708
00:58:45,839 --> 00:58:47,040
我將添加各種

1709
00:58:47,040 --> 00:58:48,480
有趣的組件

1710
00:58:48,480 --> 00:58:51,200
來製作算法或代理方式

1711
00:58:51,200 --> 00:58:52,400


1712
00:58:52,400 --> 00:58:55,599
與環境交互 嗯，

1713
00:58:55,599 --> 00:58:57,760
我想它類似於主動

1714
00:58:57,760 --> 00:58:59,119
推理框架，

1715
00:58:59,119 --> 00:59:02,000
或者甚至可能更好，然後你

1716
00:59:02,000 --> 00:59:03,040


1717
00:59:03,040 --> 00:59:06,319
沒有區分 whe  re

1718
00:59:06,319 --> 00:59:08,400
rl 更好，或者積極

1719
00:59:08,400 --> 00:59:10,960
影響更好，這

1720
00:59:10,960 --> 00:59:12,559
對我來說並不是

1721
00:59:12,559 --> 00:59:15,680
真的，因為我認為從

1722
00:59:15,680 --> 00:59:16,079
我的

1723
00:59:16,079 --> 00:59:17,599
角度來看，兩個社區都在從類似的事情上工作，

1724
00:59:17,599 --> 00:59:19,440
這是順序

1725
00:59:19,440 --> 00:59:20,400
決策，

1726
00:59:20,400 --> 00:59:23,760
嗯，我們的工作主要

1727
00:59:23,760 --> 00:59:26,000
是呃順序決策

1728
00:59:26,000 --> 00:59:27,520
面對不確定性，

1729
00:59:27,520 --> 00:59:29,920
而我們的一些工作我的 rl 工作

1730
00:59:29,920 --> 00:59:30,720
可能不

1731
00:59:30,720 --> 00:59:33,440
專注於此，所以我認為當你

1732
00:59:33,440 --> 00:59:33,920
開始

1733
00:59:33,920 --> 00:59:35,359
劃定界限時，它會變得

1734
00:59:35,359 --> 00:59:37,119
有點朦朧，就像

1735
00:59:37,119 --> 00:59:40,079
事情是分開的，我想我

1736
00:59:40,079 --> 00:59:41,920
上升了一點 切線

1737
00:59:41,920 --> 00:59:45,760
嗯，但是 phil 你

1738
00:59:45,760 --> 00:59:47,119
想在

1739
00:59:47,119 --> 00:59:49,280
環境和範式方面添加任何

1740
00:59:49,280 --> 00:59:49,839
可能

1741
00:59:49,839 --> 00:59:53,520
有用的東西

1742
00:59:53,520 --> 00:59:55,359
嗎是的

1743
00:59:55,359 --> 00:59:57,440


1744
00:59:57,440 --> 00:59:58,640


1745
00:59:58,640 --> 01:00:01,599
應該表現你可以爭辯

1746
01:00:01,599 --> 01:00:03,359
說你可以部署一個

1747
01:00:03,359 --> 01:00:05,200
使用某種好奇心或認知

1748
01:00:05,200 --> 01:00:08,720
不確定性減少機制的 rl 代理，但

1749
01:00:08,720 --> 01:00:10,559
我的意思是我知道我知道有一

1750
01:00:10,559 --> 01:00:12,319
點點 關於學習先驗

1751
01:00:12,319 --> 01:00:14,160
而不是獎勵函數並學習這些的工作，

1752
01:00:14,160 --> 01:00:14,640
但

1753
01:00:14,640 --> 01:00:17,920
我不太了解，但

1754
01:00:17,920 --> 01:00:19,280
我認為重要的是要

1755
01:00:19,280 --> 01:00:21,839
理解就像探索

1756
01:00:21,839 --> 01:00:22,880
整個環境一樣，

1757
01:00:22,880 --> 01:00:24,319
你的認知不確定性會

1758
01:00:24,319 --> 01:00:26,319
像你一樣

1759
01:00:26,319 --> 01:00:27,760
歸零 會觀察到一切，

1760
01:00:27,760 --> 01:00:29,359
然後不清楚你的 rl 代理

1761
01:00:29,359 --> 01:00:30,480
在這一點上會做什麼，特別是你

1762
01:00:30,480 --> 01:00:31,280
說你有一個

1763
01:00:31,280 --> 01:00:33,680
深度神經網絡，可以參數化

1764
01:00:33,680 --> 01:00:35,839
你如何採取行動，

1765
01:00:35,839 --> 01:00:37,440
而在這種情況下，我認為

1766
01:00:37,440 --> 01:00:39,280
這篇論文對我來說真的很有趣

1767
01:00:39,280 --> 01:00:41,119
看看當我們進行這些

1768
01:00:41,119 --> 01:00:42,480
實驗時，

1769
01:00:42,480 --> 01:00:44,319
實際上你只是將先驗置於你

1770
01:00:44,319 --> 01:00:46,079
之前的偏好之上，最終你

1771
01:00:46,079 --> 01:00:46,480
學習

1772
01:00:46,480 --> 01:00:48,960
了一種行為模式，它可能不是最優的，

1773
01:00:48,960 --> 01:00:50,960
但你的代理最終

1774
01:00:50,960 --> 01:00:53,040
學會了採用一種自我實現的行為，

1775
01:00:53,040 --> 01:00:54,480
因為它減少了

1776
01:00:54,480 --> 01:00:56,000
流行病 不確定性

1777
01:00:56,000 --> 01:00:58,400
，然後剩下的就是說得好我

1778
01:00:58,400 --> 01:00:59,520
認為這些是

1779
01:00:59,520 --> 01:01:00,960
有用的行為，或者至少這些

1780
01:01:00,960 --> 01:01:03,200
是我在這個世界上要做的行為 最終

1781
01:01:03,200 --> 01:01:04,000


1782
01:01:04,000 --> 01:01:05,680
你會得到一個相當重複的行為，

1783
01:01:05,680 --> 01:01:07,119
它可能是一種更準確的

1784
01:01:07,119 --> 01:01:08,160
模擬，

1785
01:01:08,160 --> 01:01:10,400
在沒有任何信息的情況下，

1786
01:01:10,400 --> 01:01:12,079
比如智能事物

1787
01:01:12,079 --> 01:01:12,559


1788
01:01:12,559 --> 01:01:15,040
在世界中的實際行為，而在 rl 範式中

1789
01:01:15,040 --> 01:01:16,480
，一旦你減少所有這些，你就不太清楚

1790
01:01:16,480 --> 01:01:17,599
了 認知的，當然關於

1791
01:01:17,599 --> 01:01:18,480
價值在哪裡

1792
01:01:18,480 --> 01:01:20,160
，你還沒有發現

1793
01:01:20,160 --> 01:01:22,319
你的代理人在那個時候真正在做什麼，

1794
01:01:22,319 --> 01:01:24,240
但我認為我目前的問題是

1795
01:01:24,240 --> 01:01:26,079
，就像從公式化的行為一樣

1796
01:01:26,079 --> 01:01:28,960
，這就像

1797
01:01:28,960 --> 01:01:31,040
離散狀態公式一樣起作用 我們

1798
01:01:31,040 --> 01:01:34,640
在該設置中看到了非常好的結果，

1799
01:01:34,640 --> 01:01:36,880
但我認為如果你擴大它，

1800
01:01:36,880 --> 01:01:38,240


1801
01:01:38,240 --> 01:01:38,640


1802
01:01:38,640 --> 01:01:40,559
如果你使用類似攤銷

1803
01:01:40,559 --> 01:01:42,640
推理或其他東西來實際

1804
01:01:42,640 --> 01:01:44,640
近似你的似然

1805
01:01:44,640 --> 01:01:47,359
轉換函數，那麼主動推理代理將會遇到同樣的問題，這意味著

1806
01:01:47,359 --> 01:01:49,280
你可能不會 獲得

1807
01:01:49,280 --> 01:01:51,119
我們在

1808
01:01:51,119 --> 01:01:53,920
這個小規模上看到的這些不錯的屬性，所以擴大規模就像

1809
01:01:53,920 --> 01:01:55,200


1810
01:01:55,200 --> 01:01:57,119
是一個有趣且開放式的

1811
01:01:57,119 --> 01:01:58,720
問題 時刻以

1812
01:01:58,720 --> 01:01:59,760
正確的方式擴大規模，您可以

1813
01:01:59,760 --> 01:02:01,440
包括這些共軛模型

1814
01:02:01,440 --> 01:02:02,880
，我們

1815
01:02:02,880 --> 01:02:05,280
在 um 中擁有離散狀態公式，

1816
01:02:05,280 --> 01:02:06,000
這

1817
01:02:06,000 --> 01:02:08,240
就是我們在最後一組模擬

1818
01:02:08,240 --> 01:02:10,079
中引入共軛先驗

1819
01:02:10,079 --> 01:02:10,880
來進行

1820
01:02:10,880 --> 01:02:13,839
學習的方式 先驗偏好

1821
01:02:13,839 --> 01:02:15,440
還有可能性

1822
01:02:15,440 --> 01:02:17,839
嗯，所以如果你以這種方式擴大它，那麼

1823
01:02:17,839 --> 01:02:18,720
如何學習

1824
01:02:18,720 --> 01:02:21,680
就像我對整個神經網絡有一個超先驗一樣變得有點模糊

1825
01:02:21,680 --> 01:02:23,280


1826
01:02:23,280 --> 01:02:24,400
，

1827
01:02:24,400 --> 01:02:27,760
嗯，所以他們必須喜歡需要做一些工作

1828
01:02:27,760 --> 01:02:29,039
那個區域

1829
01:02:29,039 --> 01:02:31,520
，為了真正

1830
01:02:31,520 --> 01:02:32,960


1831
01:02:32,960 --> 01:02:35,680
表明你可以擁有這些有趣的

1832
01:02:35,680 --> 01:02:37,520
組件是合適的，那麼

1833
01:02:37,520 --> 01:02:40,079
你需要開始考慮如何

1834
01:02:40,079 --> 01:02:42,319
包含這些超先驗，

1835
01:02:42,319 --> 01:02:44,240
嗯，因為

1836
01:02:44,240 --> 01:02:45,920
說你將擁有一個混合先驗是不合理的

1837
01:02:45,920 --> 01:02:49,119
在參數空間上，

1838
01:02:49,119 --> 01:02:51,760
例如 beta 超，因此超

1839
01:02:51,760 --> 01:02:53,359
先於 gamma，這

1840
01:02:53,359 --> 01:02:55,359
在這些設置中就足夠了，

1841
01:02:55,359 --> 01:02:58,319
超先於代理

1842
01:02:58,319 --> 01:03:00,319
選擇他的動作的方式

1843
01:03:00,319 --> 01:03:02,640
它必須超過模型參數

1844
01:03:02,640 --> 01:03:03,440
，如果

1845
01:03:03,440 --> 01:03:06,559
擴大規模意味著你失去

1846
01:03:06,559 --> 01:03:08,559
了解開特定

1847
01:03:08,559 --> 01:03:10,400
模型參數的好方法，那麼它變得非常

1848
01:03:10,400 --> 01:03:13,440
不確定

1849
01:03:13,440 --> 01:03:15,039
我不知道這對我來說是一個開放式

1850
01:03:15,039 --> 01:03:16,559
問題

1851
01:03:16,559 --> 01:03:18,640
很酷是的我想 高維

1852
01:03:18,640 --> 01:03:19,760
問題

1853
01:03:19,760 --> 01:03:21,599
仍然代表著一些

1854
01:03:21,599 --> 01:03:23,119
相對困難的東西，尤其

1855
01:03:23,119 --> 01:03:25,760
是因為你知道你

1856
01:03:25,760 --> 01:03:27,359
知道的那種我們離海灣

1857
01:03:27,359 --> 01:03:29,680
越遠，它變得

1858
01:03:29,680 --> 01:03:30,640
越少，你知道

1859
01:03:30,640 --> 01:03:34,960
它就像它是一條非常細的線

1860
01:03:34,960 --> 01:03:37,520
有趣的是，無論是

1861
01:03:37,520 --> 01:03:38,720
在 rl

1862
01:03:38,720 --> 01:03:40,960
還是主動推理範式中，都有

1863
01:03:40,960 --> 01:03:41,839
一種

1864
01:03:41,839 --> 01:03:44,559
稀疏的骨架，核心是裸露的骨骼

1865
01:03:44,559 --> 01:03:45,520


1866
01:03:45,520 --> 01:03:48,799
，然後有時需要這些其他層

1867
01:03:48,799 --> 01:03:51,839
或調整，

1868
01:03:51,839 --> 01:03:55,359
而且學習起來非常有趣

1869
01:03:55,359 --> 01:03:57,200
，還有你之前所說的

1870
01:03:57,200 --> 01:04:00,079
關於挑戰的內容 正在計劃

1871
01:04:00,079 --> 01:04:01,839


1872
01:04:01,839 --> 01:04:03,920
在您獲得反饋時採取順序行動，無論是通過

1873
01:04:03,920 --> 01:04:05,440
在環境中移動以改變您的

1874
01:04:05,440 --> 01:04:06,720
本地環境還是您正在

1875
01:04:06,720 --> 01:04:07,839
玩遊戲 我

1876
01:04:07,839 --> 01:04:09,440
的棋盤遊戲將要發生變化，或者

1877
01:04:09,440 --> 01:04:10,880
您正在根據市場

1878
01:04:10,880 --> 01:04:13,359
順序行動進行交易，您不能只計劃

1879
01:04:13,359 --> 01:04:15,280
步驟 1 到 100

1880
01:04:15,280 --> 01:04:17,440
，至少不考慮一些

1881
01:04:17,440 --> 01:04:18,720
假設

1882
01:04:18,720 --> 01:04:22,240
，然後只能訪問有限的

1883
01:04:22,240 --> 01:04:24,559
觀察數據並在其中進行計劃

1884
01:04:24,559 --> 01:04:25,520
基本的

1885
01:04:25,520 --> 01:04:27,680
不確定性，所以我認為很多與

1886
01:04:27,680 --> 01:04:30,160


1887
01:04:30,160 --> 01:04:32,559
強化學習和機器學習動機的聯繫點

1888
01:04:32,559 --> 01:04:33,680


1889
01:04:33,680 --> 01:04:35,599
可能會給主動推理帶來更多

1890
01:04:35,599 --> 01:04:37,440
啟示，並且

1891
01:04:37,440 --> 01:04:39,920
推動你剛才提到的一些前沿，

1892
01:04:39,920 --> 01:04:40,960


1893
01:04:40,960 --> 01:04:42,720
所以我有一個問題，還有其他人

1894
01:04:42,720 --> 01:04:45,119
在 聊天也可以問一個問題，嗯，

1895
01:04:45,119 --> 01:04:47,680
假設有人想了解

1896
01:04:47,680 --> 01:04:48,160


1897
01:04:48,160 --> 01:04:51,280
這個，他們實際上是幸運的

1898
01:04:51,280 --> 01:04:52,960
初學者，

1899
01:04:52,960 --> 01:04:56,079
因為他們可能沒有

1900
01:04:56,079 --> 01:04:57,760
被學習強化學習所吸引，但

1901
01:04:57,760 --> 01:04:58,960
他們

1902
01:04:58,960 --> 01:05:02,000
對主動推理感到好奇並興奮 你的

1903
01:05:02,000 --> 01:05:03,280
演講

1904
01:05:03,280 --> 01:05:05,599
，他們想學習什麼樣的計算機

1905
01:05:05,599 --> 01:05:06,720
語言或

1906
01:05:06,720 --> 01:05:08,880
技能，或者他們想學習什麼

1907
01:05:08,880 --> 01:05:11,119
樣的方法或心態

1908
01:05:11,119 --> 01:05:12,000


1909
01:05:12,000 --> 01:05:13,680
如果有人說不是來自

1910
01:05:13,680 --> 01:05:15,039


1911
01:05:15,039 --> 01:05:16,559
經典機器學習的角度

1912
01:05:16,559 --> 01:05:18,400
和學習主動推理，

1913
01:05:18,400 --> 01:05:21,520
而是將技能提升到主動推理

1914
01:05:21,520 --> 01:05:23,039
，你會向他們推薦你們中的任何一個

1915
01:05:23,039 --> 01:05:25,119


1916
01:05:25,119 --> 01:05:27,119


1917
01:05:27,119 --> 01:05:28,480
嗎？ 認為

1918
01:05:28,480 --> 01:05:30,640
這是一種有趣的方式來看待

1919
01:05:30,640 --> 01:05:32,079
這種潛在的人，因為

1920
01:05:32,079 --> 01:05:33,280
我覺得

1921
01:05:33,280 --> 01:05:35,920
我有點像這個人，就像

1922
01:05:35,920 --> 01:05:36,799
回到

1923
01:05:36,799 --> 01:05:38,079
過去的時候，就像諾拉一樣，我開始有

1924
01:05:38,079 --> 01:05:39,520
這些關於主動

1925
01:05:39,520 --> 01:05:41,440
推理的對話，嗯

1926
01:05:41,440 --> 01:05:44,640
，我喜歡 這篇論文

1927
01:05:44,640 --> 01:05:45,680
基本上是作為一個

1928
01:05:45,680 --> 01:05:47,680
教程開始的

1929
01:05:47,680 --> 01:05:48,960


1930
01:05:48,960 --> 01:05:51,359


1931
01:05:51,359 --> 01:05:53,280


1932
01:05:53,280 --> 01:05:54,799


1933
01:05:54,799 --> 01:05:57,440


1934
01:05:57,440 --> 01:05:59,280


1935
01:05:59,280 --> 01:05:59,920
密集

1936
01:05:59,920 --> 01:06:02,480
且很難閱讀，因此您

1937
01:06:02,480 --> 01:06:03,760
無需嘗試就知道

1938
01:06:03,760 --> 01:06:06,079
自我認可，但我確實認為閱讀這份

1939
01:06:06,079 --> 01:06:07,520
手稿特別像整個

1940
01:06:07,520 --> 01:06:08,079
目標

1941
01:06:08,079 --> 01:06:10,000
哲學當我們喜歡當我們開始

1942
01:06:10,000 --> 01:06:11,440
寫作的時候，這是為了

1943
01:06:11,440 --> 01:06:14,640
真正理解

1944
01:06:14,640 --> 01:06:16,160
這裡發生了什麼，比如這種

1945
01:06:16,160 --> 01:06:17,760
預期的自由能是什麼，比如

1946
01:06:17,760 --> 01:06:18,880
我們為什麼關心它，

1947
01:06:18,880 --> 01:06:21,359
所以我從一種理論的角度知道，

1948
01:06:21,359 --> 01:06:23,359
我認為這是 一個非常

1949
01:06:23,359 --> 01:06:26,160
清晰的概念介紹，所以至少

1950
01:06:26,160 --> 01:06:26,640


1951
01:06:26,640 --> 01:06:28,240
你可以

1952
01:06:28,240 --> 01:06:30,319
對正在發生的事情有

1953
01:06:30,319 --> 01:06:32,160
某種直覺

1954
01:06:32,160 --> 01:06:33,760


1955
01:06:33,760 --> 01:06:35,760
工作了

1956
01:06:35,760 --> 01:06:36,960
很多，

1957
01:06:36,960 --> 01:06:38,960
嗯，我想說這有點

1958
01:06:38,960 --> 01:06:40,000
取決於

1959
01:06:40,000 --> 01:06:41,920
這個人的主要目標

1960
01:06:41,920 --> 01:06:43,440
是什麼，是為了

1961
01:06:43,440 --> 01:06:46,559


1962
01:06:46,559 --> 01:06:49,760
理解高級概念思想還是

1963
01:06:49,760 --> 01:06:51,359
將其視為算法 因為如果

1964
01:06:51,359 --> 01:06:52,799
你來自自由能

1965
01:06:52,799 --> 01:06:54,799
原理，那麼積極的推理是

1966
01:06:54,799 --> 01:06:56,400
另一回事，或者如果你正在

1967
01:06:56,400 --> 01:06:59,440
採取孤立的積極推理，那麼

1968
01:06:59,440 --> 01:07:01,839
一種特定的順序

1969
01:07:01,839 --> 01:07:02,799
決策，

1970
01:07:02,799 --> 01:07:06,559
嗯，計劃對，嗯，所以取決於那

1971
01:07:06,559 --> 01:07:06,880


1972
01:07:06,880 --> 01:07:09,599
是 不同的 rs 呃，但正如 phil

1973
01:07:09,599 --> 01:07:11,440
所說，我認為這篇論文

1974
01:07:11,440 --> 01:07:15,359
在某種意義上確實非常好，就像它確實

1975
01:07:15,359 --> 01:07:16,960
嘗試定義所有不同的

1976
01:07:16,960 --> 01:07:18,720
概念並

1977
01:07:18,720 --> 01:07:21,920
通過不同的表述可能沒有

1978
01:07:21,920 --> 01:07:25,440
那麼詳細，嗯，

1979
01:07:25,440 --> 01:07:26,559
它給你

1980
01:07:26,559 --> 01:07:29,520
的假設 您可能

1981
01:07:29,520 --> 01:07:30,880
如何推導出它的

1982
01:07:30,880 --> 01:07:34,079
佈局，但是某些事情，例如

1983
01:07:34,079 --> 01:07:37,200
近似密度甚至真正需要什麼，

1984
01:07:37,200 --> 01:07:40,319
這些都是非常困難的

1985
01:07:40,319 --> 01:07:41,200
問題

1986
01:07:41,200 --> 01:07:43,440
，您必須深入研究

1987
01:07:43,440 --> 01:07:44,880
變分推理文獻才能

1988
01:07:44,880 --> 01:07:46,000
理解，所以

1989
01:07:46,000 --> 01:07:47,839
我想從這個角度來看有人

1990
01:07:47,839 --> 01:07:49,920
誰進入該領域

1991
01:07:49,920 --> 01:07:51,440
應該花一些時間思考

1992
01:07:51,440 --> 01:07:53,280
變分推理以及它如何

1993
01:07:53,280 --> 01:07:53,760
真正

1994
01:07:53,760 --> 01:07:55,839
與影響制定行為聯繫起來，

1995
01:07:55,839 --> 01:07:56,960


1996
01:07:56,960 --> 01:07:59,119
因為主動推理的感知部分

1997
01:07:59,119 --> 01:08:00,559


1998
01:08:00,559 --> 01:08:03,359
在大多數情況下與

1999
01:08:03,359 --> 01:08:04,160
變分

2000
01:08:04,160 --> 01:08:07,119
呃推理文獻和優化

2001
01:08:07,119 --> 01:08:08,640
呃模型證據完全相同

2002
01:08:08,640 --> 01:08:12,240
嗯，或者最大化證據法

2003
01:08:12,240 --> 01:08:13,200
比比皆是，

2004
01:08:13,200 --> 01:08:15,599
嗯，這是我要做的第二件事

2005
01:08:15,599 --> 01:08:16,399
補充

2006
01:08:16,399 --> 01:08:19,679


2007
01:08:19,679 --> 01:08:22,319
一下 lancelot de costa 的論文

2008
01:08:22,319 --> 01:08:24,158
對於想要深入研究

2009
01:08:24,158 --> 01:08:27,198
自己推導一切的人來說

2010
01:08:27,198 --> 01:08:30,719


2011
01:08:30,719 --> 01:08:31,198


2012
01:08:31,198 --> 01:08:33,279


2013
01:08:33,279 --> 01:08:35,359


2014
01:08:35,359 --> 01:08:36,080


2015
01:08:36,080 --> 01:08:38,238
非常有用 對於

2016
01:08:38,238 --> 01:08:40,080
不熟悉數學

2017
01:08:40,080 --> 01:08:41,198
並且只想

2018
01:08:41,198 --> 01:08:44,399
獲得外行摘要的人來說，這是

2019
01:08:44,399 --> 01:08:46,560


2020
01:08:46,560 --> 01:08:48,080


2021
01:08:48,080 --> 01:08:50,960
一個很好的介紹

2022
01:08:50,960 --> 01:08:51,759


2023
01:08:51,759 --> 01:08:53,679


2024
01:08:53,679 --> 01:08:55,439
理論部分

2025
01:08:55,439 --> 01:08:57,759
嗯，從編碼的角度來看，它

2026
01:08:57,759 --> 01:08:59,759
完全取決於最終目標是什麼，

2027
01:08:59,759 --> 01:09:00,640
所以如果他們想要

2028
01:09:00,640 --> 01:09:03,198
有人想要使用離散

2029
01:09:03,198 --> 01:09:04,479
狀態公式，

2030
01:09:04,479 --> 01:09:07,679
那麼 matlab 代碼 carl 已經

2031
01:09:07,679 --> 01:09:10,719
編寫了多年的工作，有很多

2032
01:09:10,719 --> 01:09:13,600
很好的模擬和例子，

2033
01:09:13,600 --> 01:09:14,319
你 可以使用

2034
01:09:14,319 --> 01:09:17,439
，而且我們的代碼是在線的，您可以

2035
01:09:17,439 --> 01:09:18,479
訪問它，

2036
01:09:18,479 --> 01:09:21,679
所以在軟件 um 部分的論文中有一個鏈接，

2037
01:09:21,679 --> 01:09:24,880
它給出了

2038
01:09:24,880 --> 01:09:25,679


2039
01:09:25,679 --> 01:09:27,040
代碼到底在哪裡，你可以看

2040
01:09:27,040 --> 01:09:29,040
一下，看看模擬是如何

2041
01:09:29,040 --> 01:09:29,920
完成

2042
01:09:29,920 --> 01:09:33,120


2043
01:09:33,120 --> 01:09:36,719


2044
01:09:36,719 --> 01:09:39,120


2045
01:09:39,120 --> 01:09:42,799


2046
01:09:42,799 --> 01:09:46,399


2047
01:09:46,399 --> 01:09:48,799
的 第一作者 呃，我們又得到了一個

2048
01:09:48,799 --> 01:09:50,080
不錯的結果，我們也有一個

2049
01:09:50,080 --> 01:09:52,479
git reaper 來完成這項工作

2050
01:09:52,479 --> 01:09:53,198


2051
01:09:53,198 --> 01:09:55,920


2052
01:09:55,920 --> 01:09:57,360


2053
01:09:57,360 --> 01:09:59,600


2054
01:09:59,600 --> 01:10:01,280


2055
01:10:01,280 --> 01:10:03,679


2056
01:10:03,679 --> 01:10:04,719
喜歡不同的

2057
01:10:04,719 --> 01:10:06,400
領域，但對於剛開始的人來說，他們

2058
01:10:06,400 --> 01:10:07,679
必須了解他們是

2059
01:10:07,679 --> 01:10:08,800
要專注於

2060
01:10:08,800 --> 01:10:10,159
理論方面還是要

2061
01:10:10,159 --> 01:10:11,840
專注於實施方面

2062
01:10:11,840 --> 01:10:13,199
理論方面

2063
01:10:13,199 --> 01:10:14,640
將深入研究

2064
01:10:14,640 --> 01:10:16,000
變分影響及其背後的數學

2065
01:10:16,000 --> 01:10:17,920
， 如果他們想專注

2066
01:10:17,920 --> 01:10:18,239


2067
01:10:18,239 --> 01:10:19,600
於編碼方面，那麼他們想

2068
01:10:19,600 --> 01:10:21,760
弄清楚是對連續

2069
01:10:21,760 --> 01:10:23,199
狀態還是離散狀態公式

2070
01:10:23,199 --> 01:10:25,199
感興趣 輸入然後

2071
01:10:25,199 --> 01:10:27,679
分解成如果它是

2072
01:10:27,679 --> 01:10:29,040
連續的，那麼

2073
01:10:29,040 --> 01:10:32,880
大部分要么是寫

2074
01:10:32,880 --> 01:10:34,800
運動本身的坐標，要么

2075
01:10:34,800 --> 01:10:36,960
他們必須使用

2076
01:10:36,960 --> 01:10:40,640
某種類似的神經網絡來

2077
01:10:40,640 --> 01:10:42,480
逼近興趣的連續分佈

2078
01:10:42,480 --> 01:10:43,280


2079
01:10:43,280 --> 01:10:45,520
或者是 carl 寫出的離散狀態

2080
01:10:45,520 --> 01:10:46,400


2081
01:10:46,400 --> 01:10:49,199
，如果他們對離散狀態有疑問，

2082
01:10:49,199 --> 01:10:50,480


2083
01:10:50,480 --> 01:10:53,679
嗯，我很樂意接受電子郵件，

2084
01:10:53,679 --> 01:10:54,080
所以

2085
01:10:54,080 --> 01:10:57,040
如果有人有或連續

2086
01:10:57,040 --> 01:10:57,840
狀態狀態，

2087
01:10:57,840 --> 01:11:01,679
嗯，狀態空間以及感謝

2088
01:11:01,679 --> 01:11:04,960
區別，嗯

2089
01:11:04,960 --> 01:11:06,159


2090
01:11:06,159 --> 01:11:09,040
，我們與

2091
01:11:09,040 --> 01:11:09,679


2092
01:11:09,679 --> 01:11:12,159
ryan smith 和 christopher

2093
01:11:12,159 --> 01:11:13,120
white 進行

2094
01:11:13,120 --> 01:11:16,000
矩陣乘法等的 matlab 代碼之間存在如此大的差異，

2095
01:11:16,000 --> 01:11:17,920
然後神經網絡出現了

2096
01:11:17,920 --> 01:11:20,880
，它提供了聽起來像是

2097
01:11:20,880 --> 01:11:23,120
高維

2098
01:11:23,120 --> 01:11:26,159
和連續的新機會 變量，但也有

2099
01:11:26,159 --> 01:11:26,880
很多新的

2100
01:11:26,880 --> 01:11:30,320
挑戰，所以

2101
01:11:30,320 --> 01:11:33,360


2102
01:11:33,360 --> 01:11:37,440
矩陣形式和這種更

2103
01:11:37,440 --> 01:11:40,480
機器學習的風格所

2104
01:11:40,480 --> 01:11:43,040
共有的本質是什麼，因為 e 對於某些人來說，嗯，

2105
01:11:43,040 --> 01:11:43,920


2106
01:11:43,920 --> 01:11:46,480


2107
01:11:46,480 --> 01:11:47,840


2108
01:11:47,840 --> 01:11:49,120
當他們

2109
01:11:49,120 --> 01:11:51,760
從

2110
01:11:51,760 --> 01:11:54,800
生態心理學或從積極的

2111
01:11:54,800 --> 01:11:57,600
哲學或體現的性能

2112
01:11:57,600 --> 01:11:58,320
角度考慮主動推理時，這可能會毫不誇張地說兩種計算機語言之間的差異，

2113
01:11:58,320 --> 01:12:01,600
所有背景都集中在主動

2114
01:12:01,600 --> 01:12:02,880
推理上，等等

2115
01:12:02,880 --> 01:12:05,440
對於那些頭髮之外的人

2116
01:12:05,440 --> 01:12:06,800


2117
01:12:06,800 --> 01:12:09,199
，我們真正可以提煉

2118
01:12:09,199 --> 01:12:09,920
出的

2119
01:12:09,920 --> 01:12:11,679
核心主動推理是什麼，我寫了

2120
01:12:11,679 --> 01:12:13,440
一些東西，

2121
01:12:13,440 --> 01:12:14,800
以便您說過

2122
01:12:14,800 --> 01:12:16,480
那些允許我們

2123
01:12:16,480 --> 01:12:19,120
使用 matlab 或深入矩陣模式的核心部分是什麼

2124
01:12:19,120 --> 01:12:20,840
像 python um 一樣進入神經網絡模式，

2125
01:12:20,840 --> 01:12:22,960


2126
01:12:22,960 --> 01:12:24,640
所以我認為它歸結為我的

2127
01:12:24,640 --> 01:12:26,640
總結幻燈片，我認為它的方式

2128
01:12:26,640 --> 01:12:28,159
是主動推理，核心

2129
01:12:28,159 --> 01:12:29,679
成分正在製定性別

2130
01:12:29,679 --> 01:12:30,960
模型

2131
01:12:30,960 --> 01:12:32,480
，這裡制定一般模型

2132
01:12:32,480 --> 01:12:33,840
是一般模型的參數化

2133
01:12:33,840 --> 01:12:35,520
因此，您可以使用

2134
01:12:35,520 --> 01:12:37,760
離散階段分類分佈

2135
01:12:37,760 --> 01:12:38,719
，也可以

2136
01:12:38,719 --> 01:12:40,560
使用更連續的狀態公式並

2137
01:12:40,560 --> 01:12:42,719
再次使用

2138
01:12:42,719 --> 01:12:44,560
神經網絡公式是它的一個

2139
01:12:44,560 --> 01:12:46,400
特定實例，

2140
01:12:46,400 --> 01:12:49,679
因此這將是一種

2141
01:12:49,679 --> 01:12:50,480
區分方式

2142
01:12:50,480 --> 01:12:53,679
，第二種方式是優化正在運行

2143
01:12:53,679 --> 01:12:56,239
的目標函數

2144
01:12:56,239 --> 01:12:57,440
，

2145
01:12:57,440 --> 01:13:01,840
因此在 matlab 代碼中，

2146
01:13:01,840 --> 01:13:04,239
我們正在使用平均場消息進行梯度發送

2147
01:13:04,239 --> 01:13:06,400
傳遞算法如此

2148
01:13:06,400 --> 01:13:07,760
具體的公式，已經

2149
01:13:07,760 --> 01:13:09,679
在幾篇論文中介紹過

2150
01:13:09,679 --> 01:13:11,440
，我特別沒有走過

2151
01:13:11,440 --> 01:13:12,880
那個，

2152
01:13:12,880 --> 01:13:16,000
或者你正在做反向傳播來

2153
01:13:16,000 --> 01:13:17,280
實際

2154
01:13:17,280 --> 01:13:20,320
計算或 lambda 分佈

2155
01:13:20,320 --> 01:13:21,760
，然後你只是

2156
01:13:21,760 --> 01:13:23,840
解決你所

2157
01:13:23,840 --> 01:13:26,560
擁有的那些分佈 這有點取決於

2158
01:13:26,560 --> 01:13:29,840
您喜歡哪種公式，

2159
01:13:29,840 --> 01:13:32,400
這取決於您如何優化

2160
01:13:32,400 --> 01:13:33,840
這些目標，或者您喜歡

2161
01:13:33,840 --> 01:13:35,600
採用隱式前向模型，

2162
01:13:35,600 --> 01:13:37,600
還是採用顯式性別

2163
01:13:37,600 --> 01:13:39,199
模型，嗯

2164
01:13:39,199 --> 01:13:41,360
，我忘記提及的一件事

2165
01:13:41,360 --> 01:13:42,480
是 嗯，

2166
01:13:42,480 --> 01:13:44,480
alex shams 和 connor hines，他們一直

2167
01:13:44,480 --> 01:13:47,520


2168
01:13:47,520 --> 01:13:50,880
在研究 Python 中 um 主動推理的離散狀態公式，

2169
01:13:50,880 --> 01:13:53,360
這可能對

2170
01:13:53,360 --> 01:13:55,280
那些 想要專注於一種特定的

2171
01:13:55,280 --> 01:13:56,239
語言

2172
01:13:56,239 --> 01:13:58,960
，它可以做更高端或

2173
01:13:58,960 --> 01:14:00,640
高維的東西，

2174
01:14:00,640 --> 01:14:02,719
以及 carl 擁有的離散狀態公式，

2175
01:14:02,719 --> 01:14:03,920


2176
01:14:03,920 --> 01:14:05,760
我知道如果有人感興趣，他們也在尋找人

2177
01:14:05,760 --> 01:14:08,719
在代碼庫上工作，

2178
01:14:08,719 --> 01:14:09,840
所以它被稱為

2179
01:14:09,840 --> 01:14:13,520
推斷活動，我認為嗯，但那也在

2180
01:14:13,520 --> 01:14:17,760
github 上，如果有人感興趣的話，

2181
01:14:17,840 --> 01:14:21,760
很酷，嗯

2182
01:14:21,760 --> 01:14:25,280
，你們中的任何一個最後的想法或評論，

2183
01:14:27,440 --> 01:14:30,640
嗯，不，我認為我還好，

2184
01:14:30,640 --> 01:14:33,440
所以你認為

2185
01:14:34,880 --> 01:14:37,199
嗯，

2186
01:14:38,560 --> 01:14:41,360
我認為我們有點注意到 有點

2187
01:14:41,360 --> 01:14:42,239


2188
01:14:42,239 --> 01:14:45,679
嗯，對主動推理的興趣增加了，

2189
01:14:45,679 --> 01:14:46,640


2190
01:14:46,640 --> 01:14:48,560
就像給你一個例子，比如可能

2191
01:14:48,560 --> 01:14:49,840
以前

2192
01:14:49,840 --> 01:14:51,040
甚至會考慮主動推理的領域，

2193
01:14:51,040 --> 01:14:53,840
比如機器人，

2194
01:14:53,840 --> 01:14:55,360
你現在開始看到越來越

2195
01:14:55,360 --> 01:14:57,360
多的東西，所以我想如果你 現在確實

2196
01:14:57,360 --> 01:14:58,960
想參與其中是一個特別

2197
01:14:58,960 --> 01:15:01,199
好的時間

2198
01:15:01,199 --> 01:15:04,400
偉大的電話菲利普我會

2199
01:15:04,400 --> 01:15:07,199
重新

2200
01:15:07,199 --> 01:15:08,880
推薦我們正在討論的優秀論文在

2201
01:15:08,880 --> 01:15:11,120
這個視頻的描述中非常

2202
01:15:11,120 --> 01:15:12,719
感謝你們倆

2203
01:15:12,719 --> 01:15:15,040
的加入 總是歡迎你來

2204
01:15:15,040 --> 01:15:16,480


2205
01:15:16,480 --> 01:15:18,239
談論你是否寫過一篇論文，

2206
01:15:18,239 --> 01:15:19,520
但

2207
01:15:19,520 --> 01:15:21,679
再次感謝諾拉和

2208
01:15:21,679 --> 01:15:23,679
菲利普，我希望在

2209
01:15:23,679 --> 01:15:24,239
未來的

2210
01:15:24,239 --> 01:15:27,679
活躍推理流中再次見到你謝謝你

2211
01:15:27,679 --> 01:15:30,159
感謝你邀請我們 再見，

2212
01:15:30,159 --> 01:15:31,440
再見，丹尼爾·

2213
01:15:31,440 --> 01:15:34,000
和平，再見，

2214
01:15:36,840 --> 01:15:39,280
很棒，停止直播，很棒的

2215
01:15:39,280 --> 01:15:41,040
談話，非常感謝菲利普和

2216
01:15:41,040 --> 01:15:43,840
諾爾，真的


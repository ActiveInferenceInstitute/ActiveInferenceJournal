1
00:00:07,839 --> 00:00:08,400
hola

2
00:00:08,400 --> 00:00:11,120
y bienvenidos a todos al

3
00:00:11,120 --> 00:00:12,160
laboratorio de inferencia activo,

4
00:00:12,160 --> 00:00:15,759
esta es la transmisión de modelo número 2.1

5
00:00:15,759 --> 00:00:18,960
el 16 de abril de 2021

6
00:00:18,960 --> 00:00:20,720
y hoy será una

7
00:00:20,720 --> 00:00:22,400
transmisión de modelo increíble, solo vamos a dar una

8
00:00:22,400 --> 00:00:24,400
breve vuelta y

9
00:00:24,400 --> 00:00:25,920
presentarnos y

10
00:00:25,920 --> 00:00:27,920
luego solo mencionaré cómo  la sesión se

11
00:00:27,920 --> 00:00:30,000
llevará a cabo hoy y luego se la pasaremos a

12
00:00:30,000 --> 00:00:33,200
noor para una presentación, así que soy daniel

13
00:00:33,200 --> 00:00:35,120
y soy un investigador postdoctoral en

14
00:00:35,120 --> 00:00:36,719
california le pasaré a

15
00:00:36,719 --> 00:00:39,040
philip

16
00:00:40,000 --> 00:00:42,800
hola, sí, actualmente soy estudiante de doctorado en

17
00:00:42,800 --> 00:00:43,280
oxford

18
00:00:43,280 --> 00:00:46,719
en  mi segundo año y sí, supongo que este

19
00:00:46,719 --> 00:00:48,320
es un trabajo que hice antes de comenzar

20
00:00:48,320 --> 00:00:49,440
mi doctorado

21
00:00:49,440 --> 00:00:52,640
um junto con nor y actualmente me estoy

22
00:00:52,640 --> 00:00:53,600
enfocando más en

23
00:00:53,600 --> 00:00:55,600
la eficiencia de los datos dentro del

24
00:00:55,600 --> 00:00:57,280
aprendizaje de refuerzo específico,

25
00:00:57,280 --> 00:00:58,960
pero supongo que en ese tema como un conocimiento

26
00:00:58,960 --> 00:01:00,879
de la inferencia activa

27
00:01:00,879 --> 00:01:03,520
es obviamente  útil para abordar tales

28
00:01:03,520 --> 00:01:06,239
problemas de investigación

29
00:01:06,240 --> 00:01:09,360
quién y o hola

30
00:01:09,360 --> 00:01:12,000
hola soy noah soy un estudiante de doctorado de tercer año

31
00:01:12,000 --> 00:01:13,840
en el grupo de neurobiología teórica en

32
00:01:13,840 --> 00:01:15,200
el centro de bienvenida para neuroimagen humana

33
00:01:15,200 --> 00:01:16,159


34
00:01:16,159 --> 00:01:18,799
um at ucl um entonces eso es university

35
00:01:18,799 --> 00:01:19,920
college london

36
00:01:19,920 --> 00:01:22,720
um, mi doctorado supervisado por autos se centró

37
00:01:22,720 --> 00:01:23,920
en estas ideas

38
00:01:23,920 --> 00:01:26,000
relacionadas con la adaptación, una de las cuales

39
00:01:26,000 --> 00:01:27,759
me enfocaré hoy, que es

40
00:01:27,759 --> 00:01:29,759
la adaptación del comportamiento en entornos no estacionarios

41
00:01:29,759 --> 00:01:32,000
usando inferencia activa,

42
00:01:32,000 --> 00:01:35,119
así que gracias, gracias por

43
00:01:35,119 --> 00:01:38,159
unirse y por esta presentación.

44
00:01:38,159 --> 00:01:38,720
voy a

45
00:01:38,720 --> 00:01:41,280
escuchar una presentación de quién sabe cuánto tiempo

46
00:01:41,280 --> 00:01:42,720
de nor

47
00:01:42,720 --> 00:01:44,399
y luego voy a recopilar

48
00:01:44,399 --> 00:01:45,759
preguntas del chat

49
00:01:45,759 --> 00:01:48,240
, así que escriba las preguntas a medida que le

50
00:01:48,240 --> 00:01:48,880
lleguen

51
00:01:48,880 --> 00:01:51,119
y luego las abordaremos al final,

52
00:01:51,119 --> 00:01:53,200
así que gracias de nuevo y ni  por favor

53
00:01:53,200 --> 00:01:56,479
quítenlo perfecto gracias

54
00:01:56,479 --> 00:01:58,880
um así que hoy presentaré un trabajo

55
00:01:58,880 --> 00:01:59,520
um

56
00:01:59,520 --> 00:02:01,600
que hice en colaboración con philip a

57
00:02:01,600 --> 00:02:03,119
quien acaban de escuchar de

58
00:02:03,119 --> 00:02:06,399
thomas parr y carl fristan um

59
00:02:06,399 --> 00:02:08,800
así que se titula inferencia activa uh

60
00:02:08,800 --> 00:02:12,720
desmitificado y comparado

61
00:02:13,599 --> 00:02:15,440
ok ok perfecto  De acuerdo, la

62
00:02:15,440 --> 00:02:17,440
presentación está estructurada de la siguiente manera.

63
00:02:17,440 --> 00:02:20,480
¿Puedes escuchar las pantallas?

64
00:02:20,480 --> 00:02:22,160
Creo que se está compartiendo o no puedes

65
00:02:22,160 --> 00:02:23,840
verlas. No las veo.

66
00:02:23,840 --> 00:02:26,000


67
00:02:26,000 --> 00:02:28,640


68
00:02:29,920 --> 00:02:34,480
y les digo que

69
00:02:34,480 --> 00:02:36,800
ahí vamos y lo recortaré, así que

70
00:02:36,800 --> 00:02:38,720
adelante, gracias

71
00:02:38,720 --> 00:02:40,879
perfecto, gracias, así que la presentación se

72
00:02:40,879 --> 00:02:42,239
estructuró tan bien como

73
00:02:42,239 --> 00:02:44,080
primero, motivaré brevemente la configuración del problema

74
00:02:44,080 --> 00:02:45,280
y proporcionaré

75
00:02:45,280 --> 00:02:47,360
detalles de una instanciación de inferencia activa en particular

76
00:02:47,360 --> 00:02:49,920


77
00:02:49,920 --> 00:02:51,519
que se está considerando hoy, que es  la configuración del espacio de estado discreto

78
00:02:51,519 --> 00:02:52,319


79
00:02:52,319 --> 00:02:54,160
y la segunda mitad de la presentación

80
00:02:54,160 --> 00:02:55,599
se centrará en algunos

81
00:02:55,599 --> 00:02:57,440
ejemplos particulares que comparan

82
00:02:57,440 --> 00:02:59,360
la formulación de influencia activa con el

83
00:02:59,360 --> 00:03:00,720
aprendizaje por refuerzo,

84
00:03:00,720 --> 00:03:03,599
específicamente el aprendizaje q y el algoritmo

85
00:03:03,599 --> 00:03:04,959
basado en el modelo bayesiano

86
00:03:04,959 --> 00:03:07,519
y luego lo que voy a hacer

87
00:03:07,519 --> 00:03:08,959
es proporcionar algo de cara  validez de

88
00:03:08,959 --> 00:03:10,640
aspectos particulares de por qué

89
00:03:10,640 --> 00:03:14,080
querría incluso usar la inferencia activa,

90
00:03:14,080 --> 00:03:18,080
está bien, entonces, ¿qué es la inferencia activa?

91
00:03:18,080 --> 00:03:20,480


92
00:03:20,480 --> 00:03:21,599


93
00:03:21,599 --> 00:03:23,920


94
00:03:23,920 --> 00:03:26,239


95
00:03:26,239 --> 00:03:28,560


96
00:03:28,560 --> 00:03:30,000
la homeostasis

97
00:03:30,000 --> 00:03:31,680
reside en atraer estados que

98
00:03:31,680 --> 00:03:34,560
minimizan su entropía o su sorpresa,

99
00:03:34,560 --> 00:03:36,239
por lo que si toma  este ejemplo particular

100
00:03:36,239 --> 00:03:38,000
que estamos viendo de este

101
00:03:38,000 --> 00:03:41,280
pequeño agente hambriento del pasado que abre el refrigerador de

102
00:03:41,280 --> 00:03:42,879
la manera en que funcionaría es como si

103
00:03:42,879 --> 00:03:44,959
necesitaras ¿por qué abrirías el refrigerador

104
00:03:44,959 --> 00:03:46,480
correctamente para que quieras hacer una

105
00:03:46,480 --> 00:03:47,280
elección particular entre

106
00:03:47,280 --> 00:03:50,080
comer en casa o afuera y

107
00:03:50,080 --> 00:03:51,120
para hacer eso,

108
00:03:51,120 --> 00:03:53,200
debe decidir cuál es la

109
00:03:53,200 --> 00:03:55,040
acción óptima que le permitiría resolver

110
00:03:55,040 --> 00:03:56,959
su propia incertidumbre sobre la

111
00:03:56,959 --> 00:03:58,560
etapa actual de los asuntos

112
00:03:58,560 --> 00:04:00,799
y eso lo ayudaría a

113
00:04:00,799 --> 00:04:02,480
decidir si quiere cocinar en casa

114
00:04:02,480 --> 00:04:04,400
o caminar.  al restaurante

115
00:04:04,400 --> 00:04:06,720
y esta instancia en particular, esto ha

116
00:04:06,720 --> 00:04:08,560
llevado al agente a abrir el refrigerador para

117
00:04:08,560 --> 00:04:11,760
verificar si tiene comida en casa

118
00:04:11,760 --> 00:04:13,519
y lo bueno de la inferencia activa

119
00:04:13,519 --> 00:04:15,519
es que le permite

120
00:04:15,519 --> 00:04:16,959
pensar en estos problemas de configuración de una

121
00:04:16,959 --> 00:04:19,358
manera más formal al especificar que

122
00:04:19,358 --> 00:04:20,639
el comportamiento óptimo

123
00:04:20,639 --> 00:04:23,199
se determina evaluando la evidencia

124
00:04:23,199 --> 00:04:24,639
que es la entrada sensorial

125
00:04:24,639 --> 00:04:26,639
bajo el modelo de género de las observaciones del agente al

126
00:04:26,639 --> 00:04:28,080
que

127
00:04:28,080 --> 00:04:30,400
está siendo expuesto y en esta

128
00:04:30,400 --> 00:04:31,520


129
00:04:31,520 --> 00:04:33,360
presentación particular.  ion lo que haremos es centrarnos

130
00:04:33,360 --> 00:04:35,280
solo en la teoría del proceso que respalda

131
00:04:35,280 --> 00:04:37,199
la inferencia activa y no hablar a través de

132
00:04:37,199 --> 00:04:38,240
lo biológico

133
00:04:38,240 --> 00:04:41,199
sobre la plausibilidad neuronal del

134
00:04:41,199 --> 00:04:44,240
esquema de paso de mensajes de inferencia activa

135
00:04:44,240 --> 00:04:47,280
um sino para motivar adecuadamente por qué

136
00:04:47,280 --> 00:04:48,479
querríamos incluso usar

137
00:04:48,479 --> 00:04:50,560
um inferencia activa  en comparación con

138
00:04:50,560 --> 00:04:52,000
los algoritmos genéricos de aprendizaje por refuerzo

139
00:04:52,000 --> 00:04:52,800


140
00:04:52,800 --> 00:04:54,720
, primero debemos comenzar con este

141
00:04:54,720 --> 00:04:56,080
entendimiento de que con una

142
00:04:56,080 --> 00:04:57,759
inferencia activa existe este compromiso con un

143
00:04:57,759 --> 00:04:59,600
esquema basado en creencias puras, lo

144
00:04:59,600 --> 00:05:01,600
que significa que las funciones de recompensa

145
00:05:01,600 --> 00:05:03,520
no siempre son necesariamente, lo

146
00:05:03,520 --> 00:05:06,000
siento, innecesarias porque cualquier política

147
00:05:06,000 --> 00:05:07,360
que tenga

148
00:05:07,360 --> 00:05:10,240
tiene  valor epistémico incluso en ausencia

149
00:05:10,240 --> 00:05:11,680
de preferencias,

150
00:05:11,680 --> 00:05:14,000
además, los agentes de inferencia activos

151
00:05:14,000 --> 00:05:16,400
también pueden aprender sus propias funciones de recompensa

152
00:05:16,400 --> 00:05:19,120
y esto ayuda al agente a describir el

153
00:05:19,120 --> 00:05:20,400
tipo de comportamiento

154
00:05:20,400 --> 00:05:23,199
que espera ver por sí mismo en

155
00:05:23,199 --> 00:05:25,120
oposición a algo que

156
00:05:25,120 --> 00:05:28,000
obtendría del entorno y estos  dos

157
00:05:28,000 --> 00:05:29,600
puntos particulares son realmente importantes

158
00:05:29,600 --> 00:05:30,800
en contraste con el

159
00:05:30,800 --> 00:05:33,039
refuerzo lea  rning porque bajo

160
00:05:33,039 --> 00:05:35,680
la configuración estándar de rl, la función de recompensa

161
00:05:35,680 --> 00:05:38,160
definiría cómo interactúa

162
00:05:38,160 --> 00:05:39,360
o se comporta el agente

163
00:05:39,360 --> 00:05:41,520
dentro de un entorno particular,

164
00:05:41,520 --> 00:05:42,560
pero

165
00:05:42,560 --> 00:05:44,080
definir esa función de recompensa en

166
00:05:44,080 --> 00:05:46,000
primer lugar es bastante difícil

167
00:05:46,000 --> 00:05:48,160
porque asume que hay una

168
00:05:48,160 --> 00:05:49,520
señal específica que se está

169
00:05:49,520 --> 00:05:51,919
dando desde el  entorno que puede ser

170
00:05:51,919 --> 00:05:53,600
unánimemente bueno o malo

171
00:05:53,600 --> 00:05:56,160
para el agente, lo que no necesariamente sería

172
00:05:56,160 --> 00:05:56,800
cierto

173
00:05:56,800 --> 00:06:00,400
en un entorno real donde estas

174
00:06:00,400 --> 00:06:02,319
señales ambientales pueden cambiar según

175
00:06:02,319 --> 00:06:03,919
el entorno, por ejemplo,

176
00:06:03,919 --> 00:06:05,680
comer helado no siempre

177
00:06:05,680 --> 00:06:07,280
será gratificante si está

178
00:06:07,280 --> 00:06:09,919
enfermo  y podría empeorarlo, um, y

179
00:06:09,919 --> 00:06:11,199
es por eso que

180
00:06:11,199 --> 00:06:12,800
construir estas funciones de recompensa

181
00:06:12,800 --> 00:06:15,199
en primer lugar es extremadamente difícil

182
00:06:15,199 --> 00:06:16,880
y si no las construye de

183
00:06:16,880 --> 00:06:19,360
manera adecuada, incluso con una configuración rl

184
00:06:19,360 --> 00:06:21,680
, puede resultar en un comportamiento subóptimo

185
00:06:21,680 --> 00:06:24,080
para sus agentes,

186
00:06:24,080 --> 00:06:26,000
um, un  la inferencia activa es realmente buena en

187
00:06:26,000 --> 00:06:27,520
el sentido de que en realidad

188
00:06:27,520 --> 00:06:29,680
estamos reemplazando o pasando por alto la

189
00:06:29,680 --> 00:06:31,360
función de recompensa tradicional que usted  usted

190
00:06:31,360 --> 00:06:31,919
tendría

191
00:06:31,919 --> 00:06:34,400
en el entorno rl con creencias previas

192
00:06:34,400 --> 00:06:34,960
sobre

193
00:06:34,960 --> 00:06:36,880
los resultados preferidos, por lo que el tipo de

194
00:06:36,880 --> 00:06:39,039
estado de cosas deseado en el que desea

195
00:06:39,039 --> 00:06:40,560


196
00:06:40,560 --> 00:06:43,840
verse y esto se vuelve importante

197
00:06:43,840 --> 00:06:47,039
en entornos donde no hay recompensa o

198
00:06:47,039 --> 00:06:48,400
hay una comprensión realmente imprecisa

199
00:06:48,400 --> 00:06:50,479
de lo que es una recompensa o

200
00:06:50,479 --> 00:06:52,639
preferencia  la configuración debería verse

201
00:06:52,639 --> 00:06:55,360
y en este escenario dentro de la

202
00:06:55,360 --> 00:06:56,319


203
00:06:56,319 --> 00:06:58,160
formulación de estado discreto de influencia activa estándar, lo que podemos

204
00:06:58,160 --> 00:07:00,319
hacer es aprender la distribución previa empírica

205
00:07:00,319 --> 00:07:01,599
sobre estos

206
00:07:01,599 --> 00:07:05,280
resultados preferidos um e intrínseco uh, lo

207
00:07:05,280 --> 00:07:07,199
siento, la función de recompensa interna de

208
00:07:07,199 --> 00:07:07,599
ese

209
00:07:07,599 --> 00:07:10,400
agente y esto me hace gustar

210
00:07:10,400 --> 00:07:11,120
la primera

211
00:07:11,120 --> 00:07:14,479
conceptualización um distinta entre la

212
00:07:14,479 --> 00:07:17,919
configuración de rl y la inferencia activa porque

213
00:07:17,919 --> 00:07:20,639
dentro de la inferencia activa las recompensas

214
00:07:20,639 --> 00:07:22,560
no son nada distintas, son solo una

215
00:07:22,560 --> 00:07:24,560
observación estándar que el agente

216
00:07:24,560 --> 00:07:26,800
obtiene del entorno, mientras que en

217
00:07:26,800 --> 00:07:27,840
rl son bastante

218
00:07:27,840 --> 00:07:32,160
necesarias para tener la acción apropiada

219
00:07:32,160 --> 00:07:35,759
um que el agente  va a aprender

220
00:07:35,759 --> 00:07:38,960
um el segundo punto que queremos

221
00:07:38,960 --> 00:07:41,280
disculparnos queríamos  hacer fue que la

222
00:07:41,280 --> 00:07:43,039
influencia activa proporciona una explicación principal

223
00:07:43,039 --> 00:07:44,080
de la

224
00:07:44,080 --> 00:07:46,160
exploración epistémica y la motivación intrínseca como

225
00:07:46,160 --> 00:07:48,160
minimización de la incertidumbre

226
00:07:48,160 --> 00:07:50,080
y nuevamente dentro del entorno rl esto es

227
00:07:50,080 --> 00:07:51,440
bastante crucial

228
00:07:51,440 --> 00:07:53,759
porque toda la premisa de muchos

229
00:07:53,759 --> 00:07:54,479
algoritmos nuevos que

230
00:07:54,479 --> 00:07:57,759
vemos en nuestro nrl es intentar y  encuentre el

231
00:07:57,759 --> 00:07:58,000


232
00:07:58,000 --> 00:07:59,680
equilibrio correcto para el equilibrio entre

233
00:07:59,680 --> 00:08:02,000
exploración y exploración, entonces,

234
00:08:02,000 --> 00:08:03,919
¿cuál es el conjunto correcto de acciones que

235
00:08:03,919 --> 00:08:05,599
el agente debe realizar en un momento dado

236
00:08:05,599 --> 00:08:06,400


237
00:08:06,400 --> 00:08:08,240
si sigo eligiendo todos los

238
00:08:08,240 --> 00:08:10,160
diferentes sabores de helado a los que

239
00:08:10,160 --> 00:08:10,800


240
00:08:10,800 --> 00:08:13,520
nunca ha estado expuesto?  um, como mostaza,

241
00:08:13,520 --> 00:08:14,479
etc.

242
00:08:14,479 --> 00:08:16,879
o debería usar siempre esto, oh, lo siento,

243
00:08:16,879 --> 00:08:17,840
siempre tiene el mismo

244
00:08:17,840 --> 00:08:20,000
sabor de helado al que ha estado expuesto

245
00:08:20,000 --> 00:08:21,120
en el pasado que

246
00:08:21,120 --> 00:08:23,199
realmente le gusta, por ejemplo, avellana

247
00:08:23,199 --> 00:08:25,199
o nutella, por ejemplo,

248
00:08:25,199 --> 00:08:28,080
um, entonces este um es un problema excepcional

249
00:08:28,080 --> 00:08:28,560


250
00:08:28,560 --> 00:08:30,800
con un rl  y bajo un marco bayesiano,

251
00:08:30,800 --> 00:08:32,958
la inferencia activa lo trata de forma

252
00:08:32,958 --> 00:08:36,159
natural um usando um la

253
00:08:36,159 --> 00:08:37,760
formulación de energía libre esperada a la que llegaré en

254
00:08:37,760 --> 00:08:39,760
un momento

255
00:08:39,760 --> 00:08:42,799
u  m y lo último que puede ver

256
00:08:42,799 --> 00:08:44,560
dentro del marco de inferencia activa es

257
00:08:44,560 --> 00:08:46,080
que, naturalmente, da cuenta de la

258
00:08:46,080 --> 00:08:47,920
incertidumbre como parte del proceso de actualización de creencias.

259
00:08:47,920 --> 00:08:48,560


260
00:08:48,560 --> 00:08:51,839


261
00:08:51,839 --> 00:08:54,959


262
00:08:54,959 --> 00:08:57,200


263
00:08:57,200 --> 00:08:58,640
esquema de influencia en

264
00:08:58,640 --> 00:09:00,640
comparación con rl. Voy a

265
00:09:00,640 --> 00:09:04,480
proporcionar algunas intuiciones sobre por qué,

266
00:09:04,480 --> 00:09:08,000
um, puede disculparse, algunas motivaciones sobre

267
00:09:08,000 --> 00:09:09,760
por qué incluso podemos formular la

268
00:09:09,760 --> 00:09:13,120
formulación de influencia activa de la manera en que lo hacemos.

269
00:09:13,120 --> 00:09:17,040


270
00:09:17,200 --> 00:09:18,959


271
00:09:18,959 --> 00:09:20,480


272
00:09:20,480 --> 00:09:24,399
no, no, lo siento, gran presentación,

273
00:09:24,399 --> 00:09:27,519
sí, muchas gracias y déjame

274
00:09:27,519 --> 00:09:32,000
desplazarme hacia abajo hasta um,

275
00:09:32,000 --> 00:09:35,040
está bien, así que dije anteriormente que con una

276
00:09:35,040 --> 00:09:36,399
inferencia activa

277
00:09:36,399 --> 00:09:38,480
estipula que los agentes

278
00:09:38,480 --> 00:09:40,320
mantienen su homostasis

279
00:09:40,320 --> 00:09:42,080
al residir en estados de atracción que

280
00:09:42,080 --> 00:09:43,440
minimizan la sorpresa,

281
00:09:43,440 --> 00:09:44,880
por lo que debe haber estado pensando.  qué es la

282
00:09:44,880 --> 00:09:46,560
sorpresa, bueno,

283
00:09:46,560 --> 00:09:48,800
cómo definimos la sorpresa como una probabilidad logarítmica negativa

284
00:09:48,800 --> 00:09:50,080
de los resultados

285
00:09:50,080 --> 00:09:52,080
y para esto introducimos una

286
00:09:52,080 --> 00:09:53,760
variable aleatoria que es

287
00:09:53,760 --> 00:09:55,760
o que corresponde  ds a un

288
00:09:55,760 --> 00:09:57,440
resultado particular que es recibido por un

289
00:09:57,440 --> 00:10:00,480
agente y este um existe dentro de un

290
00:10:00,480 --> 00:10:01,040
conjunto finito de

291
00:10:01,040 --> 00:10:03,600
todos los resultados posibles y eso es

292
00:10:03,600 --> 00:10:04,560
así que um

293
00:10:04,560 --> 00:10:06,720
primera ecuación que tenemos solo um

294
00:10:06,720 --> 00:10:08,160
establece formalmente que out

295
00:10:08,160 --> 00:10:10,240
y aquí p denota la

296
00:10:10,240 --> 00:10:11,680
distribución de probabilidad sobre los

297
00:10:11,680 --> 00:10:14,239
resultados

298
00:10:15,839 --> 00:10:19,920
um bien  Entonces, en la inferencia activa, la forma en que

299
00:10:19,920 --> 00:10:21,200
el agente realmente

300
00:10:21,200 --> 00:10:23,200
minimizará esta cantidad sorpresa que

301
00:10:23,200 --> 00:10:24,959
acabamos de ver es manteniendo un

302
00:10:24,959 --> 00:10:27,120
modelo de género del mundo,

303
00:10:27,120 --> 00:10:29,360
um, y esto es importante porque en un

304
00:10:29,360 --> 00:10:31,040
momento dado, el agente no

305
00:10:31,040 --> 00:10:32,640
necesariamente tendría acceso a

306
00:10:32,640 --> 00:10:34,000
la  medidas reales del estado actual

307
00:10:34,000 --> 00:10:36,000
del mundo, así que en este

308
00:10:36,000 --> 00:10:37,600
gráfico particular que ves aquí

309
00:10:37,600 --> 00:10:39,600
tienes el medio ambiente y el agente

310
00:10:39,600 --> 00:10:41,200
interactuando con el medio ambiente de una

311
00:10:41,200 --> 00:10:42,320
manera particular

312
00:10:42,320 --> 00:10:44,480
, está expuesto a la señal sensorial

313
00:10:44,480 --> 00:10:46,480
pero no sabe qué  el

314
00:10:46,480 --> 00:10:49,600
resultado de o fue regenerado por, por

315
00:10:49,600 --> 00:10:51,760
lo que solo puede percibirse a sí mismo y al

316
00:10:51,760 --> 00:10:52,959
mundo que lo rodea solo a

317
00:10:52,959 --> 00:10:55,120
través de o y necesita hacer

318
00:10:55,120 --> 00:10:57,279
inferencias sobre qué tipo  de estados

319
00:10:57,279 --> 00:11:00,959
o um las verdaderas causas um

320
00:11:00,959 --> 00:11:03,279
fueron um responsables de la

321
00:11:03,279 --> 00:11:06,000
entrada sensorial particular a la que está siendo expuesto

322
00:11:06,000 --> 00:11:08,959
y es por eso que um en la inferencia activa

323
00:11:08,959 --> 00:11:10,240
cuando formulamos el problema lo

324
00:11:10,240 --> 00:11:11,839
formulamos como un

325
00:11:11,839 --> 00:11:13,519
proceso de decisión de Markov parcialmente observable

326
00:11:13,519 --> 00:11:16,560
porque um de esta manera  son capaces de

327
00:11:16,560 --> 00:11:18,959
formular un modelo de género que define

328
00:11:18,959 --> 00:11:19,279
esta

329
00:11:19,279 --> 00:11:22,160
distribución interna sobre los

330
00:11:22,160 --> 00:11:23,920
estados internos que el agente

331
00:11:23,920 --> 00:11:26,399
usaría para inferir el resultado

332
00:11:26,399 --> 00:11:27,920
por lo que no tiene acceso al estado verdadero

333
00:11:27,920 --> 00:11:28,399


334
00:11:28,399 --> 00:11:30,320
pero puede hacer hipótesis o creencias

335
00:11:30,320 --> 00:11:32,000
sobre los estados que  podría haber dado

336
00:11:32,000 --> 00:11:32,640


337
00:11:32,640 --> 00:11:35,519
lugar a un sentido particular uh

338
00:11:35,519 --> 00:11:38,320
espacio de resultado que está siendo expuesto a

339
00:11:38,320 --> 00:11:40,800
um y al usar esto, el agente hará

340
00:11:40,800 --> 00:11:43,600
inferencias sobre el estado real

341
00:11:43,600 --> 00:11:45,839
usando un proceso de mapeo inverso

342
00:11:45,839 --> 00:11:48,079
específicamente inversión del modelo bayesiano

343
00:11:48,079 --> 00:11:49,680
y para hacer esto un poco más

344
00:11:49,680 --> 00:11:52,000
concreto qué  lo que puede hacer es pensar en

345
00:11:52,000 --> 00:11:55,680
los estados ocultos como ubicaciones o colores,

346
00:11:55,680 --> 00:11:58,240
por ejemplo, y el espacio de observación al

347
00:11:58,240 --> 00:12:00,079
que estaría expuesto el agente sería

348
00:12:00,079 --> 00:12:00,560


349
00:12:00,560 --> 00:12:03,680
um, por ejemplo, la velocidad del

350
00:12:03,680 --> 00:12:04,480
movimiento

351
00:12:04,480 --> 00:12:07,440
o una recompensa en particular, eh, o como una

352
00:12:07,440 --> 00:12:10,959
cara feliz a la que están expuestos,

353
00:12:10,959 --> 00:12:14,720
um, está bien, si tuviéramos que pensar en

354
00:12:14,720 --> 00:12:16,320
esto un poco más formalmente,

355
00:12:16,320 --> 00:12:19,920
¿cuál es el modelo suave para nosotros?  um

356
00:12:19,920 --> 00:12:21,760
descrito antes de un caballero es un

357
00:12:21,760 --> 00:12:23,040


358
00:12:23,040 --> 00:12:25,200
mdp parcialmente observable dentro de esta formulación activa y amigos

359
00:12:25,200 --> 00:12:26,160


360
00:12:26,160 --> 00:12:29,040
que se basa en una configuración simplificada que

361
00:12:29,040 --> 00:12:30,639
estamos considerando aquí donde solo

362
00:12:30,639 --> 00:12:32,240
tenemos dos variables aleatorias,

363
00:12:32,240 --> 00:12:35,120
la primera es o que hemos discutido

364
00:12:35,120 --> 00:12:36,320
y la segunda

365
00:12:36,320 --> 00:12:39,680
es  s donde s denota um una

366
00:12:39,680 --> 00:12:41,839
variable aleatoria que representa estados ocultos o latentes

367
00:12:41,839 --> 00:12:44,079
y existen dentro de un

368
00:12:44,079 --> 00:12:45,760
conjunto finito de todos

369
00:12:45,760 --> 00:12:47,519
los estados ocultos posibles que se denota con

370
00:12:47,519 --> 00:12:49,279
s mayúscula aquí

371
00:12:49,279 --> 00:12:51,440
y esta probabilidad conjunta de que superemos

372
00:12:51,440 --> 00:12:52,399
o y

373
00:12:52,399 --> 00:12:54,880
s puede factorizarse en la

374
00:12:54,880 --> 00:12:55,440
función de probabilidad

375
00:12:55,440 --> 00:12:59,120
que es p de o dado s y luego

376
00:12:59,120 --> 00:13:01,360
tienes el anterior sobre los estados internos

377
00:13:01,360 --> 00:13:02,720
que es pss,

378
00:13:02,720 --> 00:13:04,839
así que esto te da una formulación muy buena,

379
00:13:04,839 --> 00:13:06,240
um,

380
00:13:06,240 --> 00:13:09,519
que usaremos en las

381
00:13:09,519 --> 00:13:11,839
siguientes diapositivas.  y solo quería

382
00:13:11,839 --> 00:13:12,720


383
00:13:12,720 --> 00:13:14,480
preguntar si puede ver mi mouse cuando

384
00:13:14,480 --> 00:13:15,920
resalto

385
00:13:15,920 --> 00:13:20,079
o cuando no está ahí puedo verlo sí, oh,

386
00:13:20,079 --> 00:13:22,719
sí, perfecto,

387
00:13:22,800 --> 00:13:24,560
así que sabemos que para que un agente minimice

388
00:13:24,560 --> 00:13:25,600
su sorpresa, tendríamos que

389
00:13:25,600 --> 00:13:27,360
marginarlos a todos.  los posibles

390
00:13:27,360 --> 00:13:29,040
estados ocultos que podrían haber llevado a un resultado dado

391
00:13:29,040 --> 00:13:29,760


392
00:13:29,760 --> 00:13:31,440
y esto se puede lograr mediante el uso de la

393
00:13:31,440 --> 00:13:33,440
factorización que acabo de mencionar la

394
00:13:33,440 --> 00:13:34,160
probabilidad

395
00:13:34,160 --> 00:13:37,360
y el anterior um pero

396
00:13:37,360 --> 00:13:40,320
el problema es que um esto no es una

397
00:13:40,320 --> 00:13:42,399
tarea trivial debido a la dimensionalidad

398
00:13:42,399 --> 00:13:43,519
de lo oculto  estados

399
00:13:43,519 --> 00:13:45,360
um puede ser extremadamente grande y si está

400
00:13:45,360 --> 00:13:47,120
considerando variables aleatorias adicionales

401
00:13:47,120 --> 00:13:48,480
que vamos a

402
00:13:48,480 --> 00:13:50,240
introducir en un momento esto se vuelve aún

403
00:13:50,240 --> 00:13:51,600
más problemático

404
00:13:51,600 --> 00:13:54,480
um y es por eso que usamos uh otra

405
00:13:54,480 --> 00:13:56,240
cantidad una aproximación variacional de

406
00:13:56,240 --> 00:13:57,199
esta cantidad

407
00:13:57,199 --> 00:13:59,920
uh p de i que  es más atrayente y

408
00:13:59,920 --> 00:14:01,920
nos permite estimar las cantidades de

409
00:14:01,920 --> 00:14:05,839
interés, por

410
00:14:05,839 --> 00:14:08,720
lo que este será un paso natural para

411
00:14:08,720 --> 00:14:10,160
hablar sobre la energía libre de variación,

412
00:14:10,160 --> 00:14:11,760
que es esta aproximación variacional

413
00:14:11,760 --> 00:14:12,800
de la qua  entidad de

414
00:14:12,800 --> 00:14:15,519
interés, um, entonces, ¿qué es la

415
00:14:15,519 --> 00:14:17,199
energía libre variacional? Entonces, la energía libre variacional se

416
00:14:17,199 --> 00:14:19,440
define como el límite superior de la sorpresa,

417
00:14:19,440 --> 00:14:20,560
por lo que la primera definición, lo

418
00:14:20,560 --> 00:14:21,920
siento, la primera definición que

419
00:14:21,920 --> 00:14:24,240
consideramos se deriva usando la desigualdad de Jensen

420
00:14:24,240 --> 00:14:25,519
y

421
00:14:25,519 --> 00:14:27,600
se conoce comúnmente como

422
00:14:27,600 --> 00:14:29,760
límite inferior de evidencia negativa en la inferencia variacional.

423
00:14:29,760 --> 00:14:32,480
literatura así que obtenemos esto de la

424
00:14:32,480 --> 00:14:34,240
ecuación 4 que acabamos de

425
00:14:34,240 --> 00:14:37,199
ver introduciendo logaritmo negativo en ambos

426
00:14:37,199 --> 00:14:37,760
lados

427
00:14:37,760 --> 00:14:41,040
y luego multiplicando este término

428
00:14:41,040 --> 00:14:43,680
por 1 que es esencialmente q de s sobre q

429
00:14:43,680 --> 00:14:45,360
de s así que estamos asumiendo que

430
00:14:45,360 --> 00:14:48,560
q de s no puede ser igual  a cero y

431
00:14:48,560 --> 00:14:51,199
con eso um, luego aplicamos la desigualdad de Jensen

432
00:14:51,199 --> 00:14:53,199
y movemos el logaritmo dentro de

433
00:14:53,199 --> 00:14:54,000
la función

434
00:14:54,000 --> 00:14:56,560
y terminamos con nuestra expectativa con

435
00:14:56,560 --> 00:14:58,000
respecto a t de s

436
00:14:58,000 --> 00:15:00,079
para el logaritmo de la articulación sobre la cantidad

437
00:15:00,079 --> 00:15:01,920
aproximada o variacional

438
00:15:01,920 --> 00:15:05,040
uh de interés aquí y luego

439
00:15:05,040 --> 00:15:07,120
tomamos el negativo adentro y podemos

440
00:15:07,120 --> 00:15:07,839
darle la vuelta

441
00:15:07,839 --> 00:15:11,120
y obtenemos nuestra primera

442
00:15:11,120 --> 00:15:14,160
buena cantidad de interés aquí donde obtenemos

443
00:15:14,160 --> 00:15:14,560


444
00:15:14,560 --> 00:15:17,199
el límite que nos interesa

445
00:15:17,199 --> 00:15:17,600
y

446
00:15:17,600 --> 00:15:20,720
en términos  de cola entre el aproximado

447
00:15:20,720 --> 00:15:24,240
y la articulación que tenemos,

448
00:15:24,480 --> 00:15:25,920
así que para hacer esto un poco más

449
00:15:25,920 --> 00:15:27,760
concreto, lo que podemos hacer ahora es

450
00:15:27,760 --> 00:15:29,279
manipular aún más la

451
00:15:29,279 --> 00:15:29,759


452
00:15:29,759 --> 00:15:34,959
invocación de energía libre variacional um en uh la muerte entre el

453
00:15:34,959 --> 00:15:38,720
aproximado y el verdadero posterior

454
00:15:38,720 --> 00:15:41,360
menos el registro  evidencia de que tenemos el

455
00:15:41,360 --> 00:15:43,120
modelo evidencia de que teníamos

456
00:15:43,120 --> 00:15:45,920
y podemos reorganizar la última ecuación

457
00:15:45,920 --> 00:15:47,519
para realmente afinar la conexión

458
00:15:47,519 --> 00:15:48,959
entre la

459
00:15:48,959 --> 00:15:51,759
sorpresa y la energía libre variacional, um,

460
00:15:51,759 --> 00:15:52,320
así que

461
00:15:52,320 --> 00:15:55,040
si recuerdas que kl es una divergencia, lo

462
00:15:55,040 --> 00:15:56,720
que significa que no puede ser menor que

463
00:15:56,720 --> 00:15:58,079
cero, entonces

464
00:15:58,079 --> 00:16:01,519
um, siempre es estrictamente mayor

465
00:16:01,519 --> 00:16:03,440
o igual a cero, lo que significa que

466
00:16:03,440 --> 00:16:06,399
cuando nuestro aproximado es igual al

467
00:16:06,399 --> 00:16:07,519
verdadero posterior

468
00:16:07,519 --> 00:16:09,600
, terminamos con la variación de energía libre

469
00:16:09,600 --> 00:16:10,720
igual a la evidencia del modelo,

470
00:16:10,720 --> 00:16:13,839
lo que significa que minimizar

471
00:16:13,839 --> 00:16:15,839
la energía libre es esencialmente equivalente a

472
00:16:15,839 --> 00:16:18,320
maximizar el modelo de género  evidencia

473
00:16:18,320 --> 00:16:23,839
um bien

474
00:16:25,279 --> 00:16:28,240
um podemos reescribir la

475
00:16:28,240 --> 00:16:29,600
ecuación anterior que teníamos

476
00:16:29,600 --> 00:16:31,680
um ecuación 10 para expresar la

477
00:16:31,680 --> 00:16:33,040
energía libre variacional

478
00:16:33,040 --> 00:16:34,800
como una función de po  creencias anteriores en

479
00:16:34,800 --> 00:16:36,079
múltiples formas diferentes,

480
00:16:36,079 --> 00:16:37,680
así que solo me centraré en la ecuación

481
00:16:37,680 --> 00:16:40,000
12 aquí, que es la complejidad

482
00:16:40,000 --> 00:16:42,800
menos la precisión, por lo que esta es una compensación, uh,

483
00:16:42,800 --> 00:16:45,440
que normalmente se usa cuando

484
00:16:45,440 --> 00:16:48,839
los documentos que esencialmente dicen um, la

485
00:16:48,839 --> 00:16:50,160
complejidad tam

486
00:16:50,160 --> 00:16:53,600
o el costo de la complejidad es esencialmente  um,

487
00:16:53,600 --> 00:16:57,120
su col rizada entre su um aproximado

488
00:16:57,120 --> 00:17:01,120
de s dado pi con respecto a

489
00:17:01,120 --> 00:17:05,199
uh su p de s dado um pi y aquí pi

490
00:17:05,199 --> 00:17:07,280
son solo sus políticas y estas pueden ser

491
00:17:07,280 --> 00:17:09,280
independientemente de la hipótesis de

492
00:17:09,280 --> 00:17:12,319
cómo va a actuar el agente,

493
00:17:12,319 --> 00:17:14,079
pero iré  Volvamos a lo que

494
00:17:14,079 --> 00:17:15,679
realmente implican las políticas, pero por ahora

495
00:17:15,679 --> 00:17:17,599
solo considérelas como un término que

496
00:17:17,599 --> 00:17:19,919
nos permite condicionar la energía libre

497
00:17:19,919 --> 00:17:22,319
en una secuencia de trayectorias de

498
00:17:22,319 --> 00:17:23,679
interés

499
00:17:23,679 --> 00:17:26,319
y el segundo término que tenemos es la

500
00:17:26,319 --> 00:17:27,599
probabilidad logarítmica

501
00:17:27,599 --> 00:17:31,200
de o dado s, por lo que la probabilidad eh

502
00:17:31,200 --> 00:17:34,559
con  a con respecto a la clave de s

503
00:17:34,559 --> 00:17:37,200
que le da la precisión, por lo que una

504
00:17:37,200 --> 00:17:39,120
forma simple de pensar es que

505
00:17:39,120 --> 00:17:42,240
um, esto es solo um, qué tan preciso es el

506
00:17:42,240 --> 00:17:43,440
modelo y este es un

507
00:17:43,440 --> 00:17:45,919
término de regularización que es un

508
00:17:45,919 --> 00:17:47,200
término de penalización para hacer s  Estoy seguro de que no se está

509
00:17:47,200 --> 00:17:49,280
alejando demasiado de nuestros

510
00:17:49,280 --> 00:17:52,320
antecedentes iniciales,

511
00:17:52,320 --> 00:17:55,679
está bien, entonces esta cantidad particular de

512
00:17:55,679 --> 00:17:56,960
energía libre variacional,

513
00:17:56,960 --> 00:17:58,480
¿hay alguna pregunta en este punto

514
00:17:58,480 --> 00:18:01,039
sobre la variación de la energía

515
00:18:01,039 --> 00:18:03,760


516
00:18:03,760 --> 00:18:05,360


517
00:18:05,360 --> 00:18:07,360
?  percibir el entorno

518
00:18:07,360 --> 00:18:09,200
y aborda una parte de la

519
00:18:09,200 --> 00:18:11,039
formulación de influencia activa que hace

520
00:18:11,039 --> 00:18:11,760
inferencias

521
00:18:11,760 --> 00:18:14,240
sobre el mundo dado con el que el agente está

522
00:18:14,240 --> 00:18:16,080
interactuando en un momento dado;

523
00:18:16,080 --> 00:18:16,880


524
00:18:16,880 --> 00:18:19,440
sin embargo, en realidad no hemos tenido en

525
00:18:19,440 --> 00:18:20,320
cuenta la

526
00:18:20,320 --> 00:18:22,960
parte activa mientras que, como este

527
00:18:22,960 --> 00:18:23,600
agente en particular

528
00:18:23,600 --> 00:18:25,679
que tenemos  bajo la

529
00:18:25,679 --> 00:18:26,880
formulación de inferencia activa puede

530
00:18:26,880 --> 00:18:29,520
tomar una serie de acciones o interactuar con

531
00:18:29,520 --> 00:18:31,360
el entorno de tal manera que

532
00:18:31,360 --> 00:18:34,240
afecte ese entorno en el futuro,

533
00:18:34,240 --> 00:18:34,960
um

534
00:18:34,960 --> 00:18:37,280
para motivar esto un poco más,

535
00:18:37,280 --> 00:18:38,880
lo que podemos pensar es que no solo

536
00:18:38,880 --> 00:18:40,480
queremos minimizar  nuestra

537
00:18:40,480 --> 00:18:42,240
energía libre de variación también queremos minimizar una

538
00:18:42,240 --> 00:18:44,320
cantidad llamada energía libre esperada

539
00:18:44,320 --> 00:18:46,320
que depende de la energía anticipada

540
00:18:46,320 --> 00:18:48,240
las observaciones sobre el futuro

541
00:18:48,240 --> 00:18:50,799
en el futuro o sobre el futuro y la

542
00:18:50,799 --> 00:18:52,080
minimización de estos

543
00:18:52,080 --> 00:18:54,480
términos particulares le permiten al agente

544
00:18:54,480 --> 00:18:56,400
influir en el futuro al tomar

545
00:18:56,400 --> 00:18:58,400
acciones particulares en el presente

546
00:18:58,400 --> 00:19:00,080
que se seleccionan de un conjunto de

547
00:19:00,080 --> 00:19:02,720
políticas, por lo que mencioné políticas algunas

548
00:19:02,720 --> 00:19:03,520
veces,

549
00:19:03,520 --> 00:19:06,960
¿cuáles son?  ellos, por lo tanto, las políticas se pueden

550
00:19:06,960 --> 00:19:09,280
definir como una secuencia de acciones en el tiempo

551
00:19:09,280 --> 00:19:10,240
tau

552
00:19:10,240 --> 00:19:12,240
que permiten a un agente hacer la transición

553
00:19:12,240 --> 00:19:13,760
entre estados ocultos

554
00:19:13,760 --> 00:19:17,520
y tau aquí es esencialmente como una

555
00:19:17,520 --> 00:19:19,120
secuencia de trayectorias hasta un

556
00:19:19,120 --> 00:19:21,039
horizonte particular,

557
00:19:21,039 --> 00:19:24,320
uh cap, que es, um, considerando el

558
00:19:24,320 --> 00:19:25,679
número total de

559
00:19:25,679 --> 00:19:27,760
tiempo.  pasos que está considerando en una

560
00:19:27,760 --> 00:19:29,280
configuración particular

561
00:19:29,280 --> 00:19:32,320
y para que definamos correctamente la

562
00:19:32,320 --> 00:19:33,919
política uh, necesitamos introducir dos

563
00:19:33,919 --> 00:19:35,440
variables aleatorias adicionales,

564
00:19:35,440 --> 00:19:38,160
por lo que la primera es una acción que está

565
00:19:38,160 --> 00:19:38,960
condicionada

566
00:19:38,960 --> 00:19:43,039
a tau, que se denota aquí con utau

567
00:19:43,039 --> 00:19:46,160
y existe dentro de un conjunto finito de

568
00:19:46,160 --> 00:19:48,320
todas las acciones posibles que los agentes pueden

569
00:19:48,320 --> 00:19:49,200
tomar

570
00:19:49,200 --> 00:19:50,960
y la segunda variable aleatoria que

571
00:19:50,960 --> 00:19:52,640
introducimos es la

572
00:19:52,640 --> 00:19:54,559
política, que es el pi que hemos

573
00:19:54,559 --> 00:19:55,919
discu  ssed

574
00:19:55,919 --> 00:19:58,000
y esto existe dentro de un conjunto finito de

575
00:19:58,000 --> 00:19:58,960
todas las

576
00:19:58,960 --> 00:20:02,000
uh políticas um o secuencia de acciones posibles

577
00:20:02,000 --> 00:20:04,640
en un sentido similar a la

578
00:20:04,640 --> 00:20:06,240
optimización de políticas secuenciales que nos

579
00:20:06,240 --> 00:20:08,240
interesa aquí, así que para hacerlo un poco más

580
00:20:08,240 --> 00:20:09,520
concreto

581
00:20:09,520 --> 00:20:11,200
, la variable aleatoria puede ser

582
00:20:11,200 --> 00:20:13,440
descompuesto en

583
00:20:13,440 --> 00:20:16,559
una serie de acciones durante un

584
00:20:16,559 --> 00:20:19,760
horizonte de tiempo particular tau,

585
00:20:19,760 --> 00:20:23,280
por lo que u1 u2 y um subiendo a utah

586
00:20:23,280 --> 00:20:25,120
denotarían la acción de

587
00:20:25,120 --> 00:20:26,960
uh acción en el punto de tiempo uno acción del

588
00:20:26,960 --> 00:20:28,799
punto de tiempo dos y así sucesivamente

589
00:20:28,799 --> 00:20:31,360
y el vínculo es explícito cuando se

590
00:20:31,360 --> 00:20:32,320
considera que

591
00:20:32,320 --> 00:20:35,679
si considera una política en un

592
00:20:35,679 --> 00:20:38,880
punto de tiempo particular tan tau, entonces la acción

593
00:20:38,880 --> 00:20:41,840
que obtiene sería esa acción,

594
00:20:41,840 --> 00:20:45,280
está bien, genial, um, así

595
00:20:45,280 --> 00:20:47,360
que también quería resaltar que esta

596
00:20:47,360 --> 00:20:49,120
definición de política es en realidad bastante

597
00:20:49,120 --> 00:20:50,559
diferente

598
00:20:50,559 --> 00:20:53,520
o distinta de cómo se considera en rl

599
00:20:53,520 --> 00:20:54,240
que  es

600
00:20:54,240 --> 00:20:56,640
um cuando dicen política eso significa

601
00:20:56,640 --> 00:20:58,000
políticas de acción estatal,

602
00:20:58,000 --> 00:20:59,840
así que como acabo de mencionar un acto de

603
00:20:59,840 --> 00:21:02,240
inferencia, una política es simplemente una secuencia

604
00:21:02,240 --> 00:21:04,159
de opciones para acciones a través del tiempo que

605
00:21:04,159 --> 00:21:06,159
es una política secuencial

606
00:21:06,159 --> 00:21:08,320
y  esto es diferente a una

607
00:21:08,320 --> 00:21:10,400
política de acción estatal en el aprendizaje por refuerzo, que

608
00:21:10,400 --> 00:21:12,080
es el mapeo de estados a

609
00:21:12,080 --> 00:21:16,080
acciones, por lo que su política rl que tiene en

610
00:21:16,080 --> 00:21:16,720
cuenta

611
00:21:16,720 --> 00:21:19,919
la acción y el estado es la probabilidad

612
00:21:19,919 --> 00:21:20,799
de su

613
00:21:20,799 --> 00:21:24,159
acción dado el estado y bajo

614
00:21:24,159 --> 00:21:27,919
um nuestra formulación mdp

615
00:21:27,919 --> 00:21:30,159
la definición de acción oh  lo siento, la

616
00:21:30,159 --> 00:21:31,760
definición de políticas en

617
00:21:31,760 --> 00:21:33,760
rl y la inferencia activa se vuelven

618
00:21:33,760 --> 00:21:35,679
exactamente iguales cuando consideramos la configuración

619
00:21:35,679 --> 00:21:37,679
donde tau es igual a uno, por lo que solo está

620
00:21:37,679 --> 00:21:41,120
considerando un paso adelante,

621
00:21:41,120 --> 00:21:45,120
está bien, está bien, entonces me voy a mover un

622
00:21:45,120 --> 00:21:45,760
poco

623
00:21:45,760 --> 00:21:49,039
y considerar, eh, el

624
00:21:49,039 --> 00:21:50,960
interés cuantitativo, esperaban energía libre, que

625
00:21:50,960 --> 00:21:54,080
es cómo la derivamos, así que

626
00:21:54,080 --> 00:21:56,240
para derivarla primero debemos extender

627
00:21:56,240 --> 00:21:57,840
la definición de energía libre variacional

628
00:21:57,840 --> 00:22:00,000
que teníamos antes,

629
00:22:00,000 --> 00:22:02,240
hace unas diapositivas, y ahora hacer que

630
00:22:02,240 --> 00:22:03,360
dependa tanto del

631
00:22:03,360 --> 00:22:06,799
tiempo, así que tau y  política y lo que estamos

632
00:22:06,799 --> 00:22:07,919
haciendo esencialmente

633
00:22:07,919 --> 00:22:10,320
es tomar la misma ecuación y simplemente

634
00:22:10,320 --> 00:22:12,320
descomponerla para el

635
00:22:12,320 --> 00:22:14,320
paso de tiempo anterior y actual, lo siento,

636
00:22:14,320 --> 00:22:16,080
bajo una política en particular, de modo que

637
00:22:16,080 --> 00:22:18,080
esto  es por eso que tenemos el condicionamiento y

638
00:22:18,080 --> 00:22:20,480
luego lo descomponemos de una

639
00:22:20,480 --> 00:22:23,520
manera específica um en la ecuación 15

640
00:22:23,520 --> 00:22:25,039
y luego escribimos la

641
00:22:25,039 --> 00:22:27,360
formulación matricial en la ecuación

642
00:22:27,360 --> 00:22:28,960
16. Entonces podemos volver a esto si hay

643
00:22:28,960 --> 00:22:31,039
alguna pregunta,

644
00:22:31,039 --> 00:22:32,880
pero la clave para tomar  lejos de la

645
00:22:32,880 --> 00:22:35,039
diapositiva es que ahora estamos incluyendo una

646
00:22:35,039 --> 00:22:37,120
dependencia funcional en el tiempo

647
00:22:37,120 --> 00:22:40,799
um para la variación de la energía um

648
00:22:40,799 --> 00:22:42,799
y esto nos permite pasar ahora a

649
00:22:42,799 --> 00:22:45,120
la formulación de energía libre esperada

650
00:22:45,120 --> 00:22:47,280
um pero la clave a tener en cuenta aquí es

651
00:22:47,280 --> 00:22:49,360
que estamos  solo considerando los puntos de tiempo,

652
00:22:49,360 --> 00:22:52,320
um, el punto de tiempo anterior y el

653
00:22:52,320 --> 00:22:52,799
presente,

654
00:22:52,799 --> 00:22:55,840
no el futuro en absoluto,

655
00:22:58,000 --> 00:23:00,320
así que usando la ecuación de energía libre antes de

656
00:23:00,320 --> 00:23:01,440
que podamos derivar,

657
00:23:01,440 --> 00:23:04,559
uh, la energía libre esperada, um,

658
00:23:04,559 --> 00:23:06,720
y cuál es la energía libre esperada,

659
00:23:06,720 --> 00:23:08,320
entonces, la energía libre esperada

660
00:23:08,320 --> 00:23:11,200
es um, la energía libre.  función de las trayectorias futuras

661
00:23:11,200 --> 00:23:12,159


662
00:23:12,159 --> 00:23:14,559
g y valora efectivamente la evidencia

663
00:23:14,559 --> 00:23:16,400
de políticas plausibles

664
00:23:16,400 --> 00:23:18,240
basadas en resultados que aún no se

665
00:23:18,240 --> 00:23:19,760
han observado, por lo que esa es la clave, por lo

666
00:23:19,760 --> 00:23:20,640
que está haciendo

667
00:23:20,640 --> 00:23:24,240
inferencias sobre las encuestas, el conjunto de

668
00:23:24,240 --> 00:23:26,000
trayectorias futuras que no ha

669
00:23:26,000 --> 00:23:28,000
observado um

670
00:23:28,000 --> 00:23:30,640
y hay dos heurísticas que se

671
00:23:30,640 --> 00:23:31,679
introducen

672
00:23:31,679 --> 00:23:34,159
aquí para llegar a la formulación g

673
00:23:34,159 --> 00:23:35,120
que vemos

674
00:23:35,120 --> 00:23:38,960
en la ecuación 17. la

675
00:23:38,960 --> 00:23:41,440
primera es incluir creencias sobre resultados futuros

676
00:23:41,440 --> 00:23:42,320


677
00:23:42,320 --> 00:23:44,159
en la expectativa que es nosotros'  estamos

678
00:23:44,159 --> 00:23:46,000
complementando la expectativa

679
00:23:46,000 --> 00:23:48,159
bajo el posterior aproximado con la

680
00:23:48,159 --> 00:23:50,000
probabilidad aquí, lo

681
00:23:50,000 --> 00:23:51,440
que da como resultado una

682
00:23:51,440 --> 00:23:53,279
distribución predictiva dada por estos dos primeros

683
00:23:53,279 --> 00:23:54,880
términos aquí

684
00:23:54,880 --> 00:23:57,600
y el segundo es que estamos

685
00:23:57,600 --> 00:23:58,559
implícitamente

686
00:23:58,559 --> 00:24:00,799
o supongo que explícitamente condicionando las

687
00:24:00,799 --> 00:24:02,799
probabilidades conjuntas de los estados

688
00:24:02,799 --> 00:24:06,080
y las observaciones en el género  modelo um

689
00:24:06,080 --> 00:24:08,960
perdón en el modelo de género um dependiente

690
00:24:08,960 --> 00:24:10,159


691
00:24:10,159 --> 00:24:13,200
del estado de cosas deseado en oposición

692
00:24:13,200 --> 00:24:14,880
a una política particular ahora, por lo que esto

693
00:24:14,880 --> 00:24:16,799
restringe el tipo de preferencias que

694
00:24:16,799 --> 00:24:17,440


695
00:24:17,440 --> 00:24:20,640
tendría el agente y lo que es útil con estos

696
00:24:20,640 --> 00:24:21,200
dos

697
00:24:21,200 --> 00:24:22,720
movimientos que estamos haciendo es que podemos

698
00:24:22,720 --> 00:24:24,480
ahora evalúe esta cantidad

699
00:24:24,480 --> 00:24:27,200
antes de tener las observaciones

700
00:24:27,200 --> 00:24:28,240
y la segunda

701
00:24:28,240 --> 00:24:30,400
es que la minimización de g

702
00:24:30,400 --> 00:24:32,080
realmente ayudaría  políticas de

703
00:24:32,080 --> 00:24:35,440
política de rabia para ser consistentes con el

704
00:24:35,440 --> 00:24:37,520
estado de cosas deseado en el que el agente

705
00:24:37,520 --> 00:24:39,760
espera estar

706
00:24:39,760 --> 00:24:41,600
um solo voy a mencionar brevemente

707
00:24:41,600 --> 00:24:44,240
que esta no es la única forma de obtener

708
00:24:44,240 --> 00:24:45,679
la energía libre esperada y ha habido

709
00:24:45,679 --> 00:24:47,760
algo de trabajo um que se ha buscado  en otras

710
00:24:47,760 --> 00:24:48,960
formulaciones

711
00:24:48,960 --> 00:24:52,240
eh incluido el trabajo de carl um donde

712
00:24:52,240 --> 00:24:53,600
la formulación de la energía libre esperada

713
00:24:53,600 --> 00:24:55,919
se puede descomponer en diferentes

714
00:24:55,919 --> 00:24:58,799
estructuras um así que si alguien está interesado

715
00:24:58,799 --> 00:25:02,480
en eso, podemos revisarlo más tarde

716
00:25:02,640 --> 00:25:04,799
um pero esta energía libre esperada energía libre

717
00:25:04,799 --> 00:25:05,679


718
00:25:05,679 --> 00:25:07,520
que acabo de presentar puede ser  descompuesto de

719
00:25:07,520 --> 00:25:10,320
ciertas maneras, por lo que las ecuaciones 20 y 21

720
00:25:10,320 --> 00:25:10,799
dan

721
00:25:10,799 --> 00:25:13,200
dos descomposiciones diferentes,

722
00:25:13,200 --> 00:25:14,960
siendo la primera la epistémica y el

723
00:25:14,960 --> 00:25:17,039
comerciante de valor extrínseco y la

724
00:25:17,039 --> 00:25:18,960
segunda siendo el término esperado y el

725
00:25:18,960 --> 00:25:22,320
costo y la ambigüedad, por lo que si solo

726
00:25:22,320 --> 00:25:23,360
consideramos

727
00:25:23,360 --> 00:25:26,640
la primera ecuación  podemos decir que si

728
00:25:26,640 --> 00:25:28,080
estamos minimizando esta ecuación,

729
00:25:28,080 --> 00:25:30,559
entonces estamos capturando este imperativo para

730
00:25:30,559 --> 00:25:31,360
maximizar

731
00:25:31,360 --> 00:25:33,760
la ganancia de información que tendría

732
00:25:33,760 --> 00:25:35,679
al observar  el entorno

733
00:25:35,679 --> 00:25:38,880
um sobre estados ocultos particulares

734
00:25:38,880 --> 00:25:41,840
mientras maximiza el valor esperado

735
00:25:41,840 --> 00:25:43,360
que se puntúa por las

736
00:25:43,360 --> 00:25:45,520
preferencias logarítmicas o el valor extrínseco

737
00:25:45,520 --> 00:25:46,559
aquí, por lo que

738
00:25:46,559 --> 00:25:48,159
esta formulación particular en realidad

739
00:25:48,159 --> 00:25:50,320
nos brinda una compensación muy clara entre

740
00:25:50,320 --> 00:25:50,880
el

741
00:25:50,880 --> 00:25:53,200
um el primer componente que es el

742
00:25:53,200 --> 00:25:54,960
valor epistémico que promueve

743
00:25:54,960 --> 00:25:57,120
comportamiento curioso, así que eso es lo que desea

744
00:25:57,120 --> 00:25:59,760
fomentar la exploración a medida que el agente

745
00:25:59,760 --> 00:26:00,799
busca

746
00:26:00,799 --> 00:26:02,400
estos nuevos estados que minimizan la

747
00:26:02,400 --> 00:26:04,960
incertidumbre sobre el entorno

748
00:26:04,960 --> 00:26:07,520
y la última parte es más pragmática

749
00:26:07,520 --> 00:26:08,400
y fomenta el

750
00:26:08,400 --> 00:26:11,360
comportamiento de explotación a través de esta

751
00:26:11,360 --> 00:26:13,360
comprensión del tipo de

752
00:26:13,360 --> 00:26:15,679
políticas que el agente haría.

753
00:26:15,679 --> 00:26:16,720
prefiero llegar a

754
00:26:16,720 --> 00:26:18,960
um en otras palabras, como esta

755
00:26:18,960 --> 00:26:20,480
formulación de energía libre esperada

756
00:26:20,480 --> 00:26:23,200
y que estamos viendo en la ecuación 20 es

757
00:26:23,200 --> 00:26:24,559
esencialmente tratar la

758
00:26:24,559 --> 00:26:26,640
exploración y la explotación como dos

759
00:26:26,640 --> 00:26:28,240
formas diferentes de abordar el mismo

760
00:26:28,240 --> 00:26:28,799
problema,

761
00:26:28,799 --> 00:26:31,440
por lo que se minimiza la incertidumbre que se mencionó

762
00:26:31,440 --> 00:26:34,720
al comienzo de la presentación,

763
00:26:34,720 --> 00:26:37,120
está bien, um  también podemos pensar en la

764
00:26:37,120 --> 00:26:38,159
segunda ecuación

765
00:26:38,159 --> 00:26:41,279
aquí wh  ich simplemente nos ofrece una

766
00:26:41,279 --> 00:26:43,200
perspectiva alternativa sobre la

767
00:26:43,200 --> 00:26:45,360
frecuencia esperada libre que es un

768
00:26:45,360 --> 00:26:47,919
agente desea minimizar la ambigüedad

769
00:26:47,919 --> 00:26:50,000
y el grado en que los resultados bajo una

770
00:26:50,000 --> 00:26:52,000
política particular se desvían de

771
00:26:52,000 --> 00:26:55,039
las preferencias previas, por lo tanto, la ambigüedad aquí es

772
00:26:55,039 --> 00:26:56,880
la expectativa de la

773
00:26:56,880 --> 00:26:58,880
entropía condicional o la incertidumbre sobre

774
00:26:58,880 --> 00:27:01,600
los resultados bajo la política actual en

775
00:27:01,600 --> 00:27:02,799
este entorno particular, la

776
00:27:02,799 --> 00:27:04,559
baja entropía sugeriría que los resultados

777
00:27:04,559 --> 00:27:06,240
son bastante destacados y únicamente

778
00:27:06,240 --> 00:27:08,320
informativos sobre los estados ocultos,

779
00:27:08,320 --> 00:27:10,320
por ejemplo, las señales visuales que

780
00:27:10,320 --> 00:27:11,760
podría ver si la habitación es realmente

781
00:27:11,760 --> 00:27:13,039
completamente dura

782
00:27:13,039 --> 00:27:14,720
en comparación con si está bastante

783
00:27:14,720 --> 00:27:16,080
oscuro  No voy a sacar nada

784
00:27:16,080 --> 00:27:19,440
importante de eso, además, al

785
00:27:19,440 --> 00:27:20,880
agente le gustará buscar resultados

786
00:27:20,880 --> 00:27:21,520
dependientes de la política

787
00:27:21,520 --> 00:27:23,760
que se asemejen a sus resultados preferidos,

788
00:27:23,760 --> 00:27:25,039


789
00:27:25,039 --> 00:27:27,120
por lo que es donado por c aquí y esto

790
00:27:27,120 --> 00:27:29,279
se logra cuando la atención diverge

791
00:27:29,279 --> 00:27:31,039
entre los resultados previstos

792
00:27:31,039 --> 00:27:34,320
y preferidos.  es minimizado

793
00:27:34,320 --> 00:27:37,679
por una política en particular

794
00:27:37,679 --> 00:27:40,880
bien um y estas creencias previas sobre  pero

795
00:27:40,880 --> 00:27:41,520
los

796
00:27:41,520 --> 00:27:42,960
resultados futuros equipan al agente con

797
00:27:42,960 --> 00:27:44,960
un comportamiento dirigido a un objetivo,

798
00:27:44,960 --> 00:27:46,960
que es uno de los

799
00:27:46,960 --> 00:27:48,320
casos que supongo que

800
00:27:48,320 --> 00:27:51,840
es realmente importante en la influencia activa.

801
00:27:51,840 --> 00:27:54,640


802
00:27:54,640 --> 00:27:56,240


803
00:27:56,240 --> 00:28:00,159


804
00:28:00,159 --> 00:28:02,240
se realiza derivando la

805
00:28:02,240 --> 00:28:04,080
probabilidad de cualquier política

806
00:28:04,080 --> 00:28:06,399
mediante la aplicación de una función soft max sobre la

807
00:28:06,399 --> 00:28:08,480
energía libre esperada

808
00:28:08,480 --> 00:28:09,919
y esto ilustra el

809
00:28:09,919 --> 00:28:11,760
comportamiento evidente de la inferencia activa

810
00:28:11,760 --> 00:28:12,559


811
00:28:12,559 --> 00:28:14,559
porque cualquier tipo de política o

812
00:28:14,559 --> 00:28:16,480
secuencia de acción que resulte en una energía libre esperada más baja

813
00:28:16,480 --> 00:28:18,240
son más  probable

814
00:28:18,240 --> 00:28:21,520
e intuitivamente, esto tendría

815
00:28:21,520 --> 00:28:24,159
sentido porque la energía libre esperada

816
00:28:24,159 --> 00:28:25,039


817
00:28:25,039 --> 00:28:26,960
encapsula todos los tipos de cosas

818
00:28:26,960 --> 00:28:28,640
que desea incluir

819
00:28:28,640 --> 00:28:30,399
o considerar cuando interactúa con

820
00:28:30,399 --> 00:28:31,600
el mundo, por lo que desea explorar,

821
00:28:31,600 --> 00:28:32,960
desea explotar pero desea  tenga

822
00:28:32,960 --> 00:28:36,000
un saldo de ese um y luego,

823
00:28:36,000 --> 00:28:38,159
cuando esté seleccionando su póliza,

824
00:28:38,159 --> 00:28:39,600
um, es solo una cuestión de determinar el

825
00:28:39,600 --> 00:28:40,159
conjunto de

826
00:28:40,159 --> 00:28:42,960
acciones que lo acercan más a esto

827
00:28:42,960 --> 00:28:44,000
núcleo particular

828
00:28:44,000 --> 00:28:45,760
um y esto puede definirse por un

829
00:28:45,760 --> 00:28:48,960
estado atractivo que está definido por su

830
00:28:48,960 --> 00:28:52,480
matriz c que

831
00:28:52,480 --> 00:28:55,200
describimos antes um y si no

832
00:28:55,200 --> 00:28:56,159
tiene eso,

833
00:28:56,159 --> 00:28:57,919
entonces es solo una exploración aleatoria que

834
00:28:57,919 --> 00:28:59,600
obtendría

835
00:28:59,600 --> 00:29:01,600
um a veces también puede incluir un

836
00:29:01,600 --> 00:29:03,760
parámetro de temperatura beta  aquí

837
00:29:03,760 --> 00:29:06,080
um y al tener un hiperprimo en

838
00:29:06,080 --> 00:29:07,520
esto, introduce un

839
00:29:07,520 --> 00:29:09,440
costo de complejidad adicional en la

840
00:29:09,440 --> 00:29:10,799


841
00:29:10,799 --> 00:29:14,720
formulación esperada de energía libre uh que le permite dar

842
00:29:14,720 --> 00:29:17,840
cuenta de um qué tan plano

843
00:29:17,840 --> 00:29:20,880
o cuán

844
00:29:20,880 --> 00:29:23,279
seguro o preciso desea que sean sus

845
00:29:23,279 --> 00:29:24,159


846
00:29:24,159 --> 00:29:27,919
preferencias sobre el espacio de políticas

847
00:29:27,919 --> 00:29:31,279
um la clave  Lo que hay que tener en cuenta es que,

848
00:29:31,279 --> 00:29:33,039
en aras de la simplicidad, no

849
00:29:33,039 --> 00:29:34,880
voy a analizar muchos de

850
00:29:34,880 --> 00:29:37,200
los detalles sobre cómo se

851
00:29:37,200 --> 00:29:37,919
optimizan,

852
00:29:37,919 --> 00:29:40,159
pero puede hacerlo de varias

853
00:29:40,159 --> 00:29:41,919
maneras diferentes, por ejemplo, en la inferencia activa

854
00:29:41,919 --> 00:29:42,720


855
00:29:42,720 --> 00:29:45,039
podemos optimizar la expectativa sobre el

856
00:29:45,039 --> 00:29:47,039
estados ocultos de interés las políticas

857
00:29:47,039 --> 00:29:48,640
las precisiones que se infieren

858
00:29:48,640 --> 00:29:50,320
y luego también podemos optimizar los parámetros del modelo a

859
00:29:50,320 --> 00:29:51,679
través de

860
00:29:51,679 --> 00:29:53,919
um a través de los procedimientos de aprendizaje

861
00:29:53,919 --> 00:29:54,960
involucrados

862
00:29:54,960 --> 00:29:58,080
um, pero ese tipo de diferencia depende de

863
00:29:58,080 --> 00:30:00,000
la configuración que esté viendo, por ejemplo,

864
00:30:00,000 --> 00:30:01,600
si está usando una base de variación,

865
00:30:01,600 --> 00:30:02,559
simplemente iteraría

866
00:30:02,559 --> 00:30:04,799
estas uh estas funciones o

867
00:30:04,799 --> 00:30:06,559
funciones objetivas hasta la convergencia

868
00:30:06,559 --> 00:30:08,960
o una inferencia activa um, hace un

869
00:30:08,960 --> 00:30:10,080
gradiente ascendente

870
00:30:10,080 --> 00:30:12,000
um para encontrar  las suficientes estadísticas de

871
00:30:12,000 --> 00:30:13,919
interés nuevamente, esto depende

872
00:30:13,919 --> 00:30:15,840
exactamente de la formulación y la configuración

873
00:30:15,840 --> 00:30:18,080
que esté viendo,

874
00:30:18,080 --> 00:30:20,240
pero la clave a tener en cuenta aquí es

875
00:30:20,240 --> 00:30:21,679
que hay

876
00:30:21,679 --> 00:30:23,760
tres aspectos particulares del

877
00:30:23,760 --> 00:30:25,360
algoritmo de inferencia activa que son útiles y

878
00:30:25,360 --> 00:30:26,159


879
00:30:26,159 --> 00:30:28,960
pueden tomarse de este particular  uh

880
00:30:28,960 --> 00:30:29,679
marco

881
00:30:29,679 --> 00:30:32,799
y aplicado a uh otras configuraciones

882
00:30:32,799 --> 00:30:34,559
um así que solo voy a reiterar y

883
00:30:34,559 --> 00:30:36,080
resumir brevemente

884
00:30:36,080 --> 00:30:38,000
para que primero tengamos el modelo de género que es

885
00:30:38,000 --> 00:30:39,679
crucial para que un agente

886
00:30:39,679 --> 00:30:41,440
interactúe y minimice su sorpresa

887
00:30:41,440 --> 00:30:43,520
necesita un modelo suave del mundo

888
00:30:43,520 --> 00:30:46,880
y  eso se describe simplemente como que

889
00:30:46,880 --> 00:30:48,640
no estoy incluyendo ninguno de los parámetros del modelo

890
00:30:48,640 --> 00:30:50,240
aquí, pero puede tener

891
00:30:50,240 --> 00:30:52,159
sus resultados, sus estados y sus

892
00:30:52,159 --> 00:30:53,679
políticas y esto,

893
00:30:53,679 --> 00:30:56,960
um, estos  se descomponen en um,

894
00:30:56,960 --> 00:30:59,120
sí, lo siento, hay una flecha con los

895
00:30:59,120 --> 00:31:00,000
corchetes aquí, pero

896
00:31:00,000 --> 00:31:03,120
estos se descomponen en su anterior,

897
00:31:03,120 --> 00:31:04,720
su probabilidad y su

898
00:31:04,720 --> 00:31:05,760
función de transición

899
00:31:05,760 --> 00:31:07,840
y luego establece una vez que tiene este

900
00:31:07,840 --> 00:31:10,000
modelo genital, el objetivo del agente

901
00:31:10,000 --> 00:31:11,519
es ajustar el modelo a las

902
00:31:11,519 --> 00:31:13,360
observaciones de muestra para reducir  el precio

903
00:31:13,360 --> 00:31:14,960
y eso es a través de

904
00:31:14,960 --> 00:31:17,279
la optimización de energía libre variacional, por lo que esta

905
00:31:17,279 --> 00:31:19,120
compensación particular que tenemos entre

906
00:31:19,120 --> 00:31:21,840
la complejidad y el costo de precisión y

907
00:31:21,840 --> 00:31:22,240
luego

908
00:31:22,240 --> 00:31:24,000
el segundo, oh, lo siento, la última parte de

909
00:31:24,000 --> 00:31:25,760
este algoritmo es planificar

910
00:31:25,760 --> 00:31:27,760
para seleccionar acciones que minimicen la

911
00:31:27,760 --> 00:31:29,200
incertidumbre

912
00:31:29,200 --> 00:31:32,000
que um que  es la energía libre esperada

913
00:31:32,000 --> 00:31:33,760
y la forma de hacerlo es teniendo un

914
00:31:33,760 --> 00:31:34,480
máximo suave

915
00:31:34,480 --> 00:31:37,600
sobre este uh g negativo de g la

916
00:31:37,600 --> 00:31:39,120
cantidad que tenemos aquí

917
00:31:39,120 --> 00:31:41,360
y luego muestreando de eso para

918
00:31:41,360 --> 00:31:42,720
seleccionar la siguiente

919
00:31:42,720 --> 00:31:46,080
mejor acción de especificaciones, está bien,

920
00:31:46,080 --> 00:31:49,600
eso es rápido y  profundizar en

921
00:31:49,600 --> 00:31:51,760
una gran cantidad de

922
00:31:51,760 --> 00:31:52,799
literatura de inferencia activa, pero solo quería

923
00:31:52,799 --> 00:31:53,600
resaltar que

924
00:31:53,600 --> 00:31:55,120
esos son los tres ingredientes principales

925
00:31:55,120 --> 00:31:57,120
que si está interesado  en la

926
00:31:57,120 --> 00:31:59,279
implementación de estos algoritmos usted mismo,

927
00:31:59,279 --> 00:32:01,440
está bien, así que ahora solo voy a cambiar

928
00:32:01,440 --> 00:32:02,720
un poco de marcha y hacer

929
00:32:02,720 --> 00:32:04,080
comparaciones con el

930
00:32:04,080 --> 00:32:07,679
aprendizaje por refuerzo, por lo que en nuestro trabajo

931
00:32:07,679 --> 00:32:09,360
consideramos una versión modificada del

932
00:32:09,360 --> 00:32:10,320


933
00:32:10,320 --> 00:32:12,880
entorno del lago congelado de los gimnasios abiertos de ai, por lo que el

934
00:32:12,880 --> 00:32:14,720
lago congelado tiene una cuadrícula.  como estructura

935
00:32:14,720 --> 00:32:17,120
con cuatro parches distintos por lo que tiene un

936
00:32:17,120 --> 00:32:18,399
punto de partida que es

937
00:32:18,399 --> 00:32:22,000
s um para que podamos verlo aquí um disculpas

938
00:32:22,000 --> 00:32:22,799
pero es muy

939
00:32:22,799 --> 00:32:25,679
pequeño así que s está aquí y tienes la

940
00:32:25,679 --> 00:32:26,640


941
00:32:26,640 --> 00:32:29,600
superficie congelada que es f así que de nuevo no

942
00:32:29,600 --> 00:32:31,120
creo  Puedo diferenciar porque

943
00:32:31,120 --> 00:32:32,399
solo muevo la boca,

944
00:32:32,399 --> 00:32:36,080
estoy haciendo zoom, pueden verlo

945
00:32:36,080 --> 00:32:38,080
perfectamente, así que tienes la f congelada y

946
00:32:38,080 --> 00:32:39,760
luego tienes la totalidad

947
00:32:39,760 --> 00:32:42,240
y, por último, tienes la meta, así que g

948
00:32:42,240 --> 00:32:43,360
aquí

949
00:32:43,360 --> 00:32:45,760
donde está la  la primera b está ubicada y todos los

950
00:32:45,760 --> 00:32:48,000
parches en esta configuración en particular

951
00:32:48,000 --> 00:32:50,320
son seguros, excepto el pasillo, donde si el

952
00:32:50,320 --> 00:32:51,279
agente va

953
00:32:51,279 --> 00:32:54,640
a h obtiene una recompensa negativa um,

954
00:32:54,640 --> 00:32:57,039
el agente comienza cada episodio en la

955
00:32:57,039 --> 00:32:57,679
primera

956
00:32:57,679 --> 00:32:59,519
posición um, que es la

957
00:32:59,519 --> 00:33:01,519
posición de inicio y desde allí necesita

958
00:33:01,519 --> 00:33:03,200
llegar al fr  la ubicación de isbee

959
00:33:03,200 --> 00:33:07,279
en um en la menor cantidad de pasos

960
00:33:07,279 --> 00:33:08,000
posibles

961
00:33:08,000 --> 00:33:09,840
y la forma en que puede hacerlo es

962
00:33:09,840 --> 00:33:11,679
realizando uh cuatro tipos diferentes de

963
00:33:11,679 --> 00:33:12,799
acciones, de modo que vaya a la

964
00:33:12,799 --> 00:33:15,840
izquierda, a la derecha, hacia abajo o hacia arriba y

965
00:33:15,840 --> 00:33:17,840
el agente puede continuar moviéndose a

966
00:33:17,840 --> 00:33:18,960
través del lago congelado

967
00:33:18,960 --> 00:33:21,279
uh con  múltiples revisitas para que pueda

968
00:33:21,279 --> 00:33:22,720
volver a la posición inicial teniendo

969
00:33:22,720 --> 00:33:24,480
como en otros lugares,

970
00:33:24,480 --> 00:33:26,960
pero cada episodio terminará cuando

971
00:33:26,960 --> 00:33:27,919
llegue a la

972
00:33:27,919 --> 00:33:31,600
sala o a la ubicación de destino y

973
00:33:31,600 --> 00:33:34,000
estas ubicaciones difieren según la

974
00:33:34,000 --> 00:33:35,120
configuración que

975
00:33:35,120 --> 00:33:38,320
tengamos en nuestras simulaciones, así que

976
00:33:38,320 --> 00:33:40,720
en  uno estableció um, la posición del

977
00:33:40,720 --> 00:33:41,440
conjunto es

978
00:33:41,440 --> 00:33:43,840
ocho y la meta es seis y otro

979
00:33:43,840 --> 00:33:45,679
estableció la posición del conjunto

980
00:33:45,679 --> 00:33:49,279
es seis y um,

981
00:33:49,279 --> 00:33:51,919
la meta es ocho y el objetivo es,

982
00:33:51,919 --> 00:33:53,679
como dije, alcanzar la meta

983
00:33:53,679 --> 00:33:56,000
idealmente en tan pocos pasos como  posible

984
00:33:56,000 --> 00:33:57,440
mientras evita el todo porque

985
00:33:57,440 --> 00:33:59,120
eso terminaría el episodio,

986
00:33:59,120 --> 00:34:00,880
eh, si alcanza la meta, obtiene una

987
00:34:00,880 --> 00:34:02,559
recompensa positiva de 100

988
00:34:02,559 --> 00:34:05,600
y negativa, de lo contrario,

989
00:34:05,600 --> 00:34:07,840
la clave a tener en cuenta aquí es que esta

990
00:34:07,840 --> 00:34:09,760
métrica de consulta en realidad nos permite una wa  y para

991
00:34:09,760 --> 00:34:11,280
comparar los algoritmos de inferencia activa con los

992
00:34:11,280 --> 00:34:12,960
algoritmos de aprendizaje por refuerzo

993
00:34:12,960 --> 00:34:14,879
, pero no es realmente

994
00:34:14,879 --> 00:34:16,639
importante que el

995
00:34:16,639 --> 00:34:18,839
algoritmo de

996
00:34:18,839 --> 00:34:22,000
inferencia activa tenga la función de recompensa como un comienzo

997
00:34:22,000 --> 00:34:24,000
porque aún puede moverse

998
00:34:24,000 --> 00:34:26,639
y usar solo la pestaña del juego de información

999
00:34:26,639 --> 00:34:28,879
para no tener la extrínseca

1000
00:34:28,879 --> 00:34:30,239
componente de valor

1001
00:34:30,239 --> 00:34:33,199
um y eso es bastante interesante

1002
00:34:33,199 --> 00:34:35,040
porque veremos las ramificaciones de

1003
00:34:35,040 --> 00:34:36,960
eso en nuestras asimilaciones

1004
00:34:36,960 --> 00:34:39,599
y para esta configuración particular limitamos

1005
00:34:39,599 --> 00:34:41,040
la cantidad máxima de pasos de tiempo para

1006
00:34:41,040 --> 00:34:42,320
cada episodio

1007
00:34:42,320 --> 00:34:45,679
a 15. Bien,

1008
00:34:45,679 --> 00:34:47,918
entonces lo que voy a hacer es primero  hable

1009
00:34:47,918 --> 00:34:49,199
sobre el modelo genital que usamos

1010
00:34:49,199 --> 00:34:51,359
para la formulación de inferencia activa,

1011
00:34:51,359 --> 00:34:53,440
um, así que aquí lo que está viendo en la

1012
00:34:53,440 --> 00:34:55,359
diapositiva es una representación gráfica del

1013
00:34:55,359 --> 00:34:57,520
modelo activo y, por ejemplo,

1014
00:34:57,520 --> 00:34:59,280
por lo que este modelo contiene cuatro

1015
00:34:59,280 --> 00:35:00,800
estados de acción, de derecha a

1016
00:35:00,800 --> 00:35:03,920
abajo, arriba y a la izquierda y

1017
00:35:03,920 --> 00:35:06,400
estos  controlar la capacidad de transición

1018
00:35:06,400 --> 00:35:08,240
entre estados ocultos,

1019
00:35:08,240 --> 00:35:11,359
eh hecho de ubicación, por ejemplo, si

1020
00:35:11,359 --> 00:35:12,240
está en la posición

1021
00:35:12,240 --> 00:35:14,960
uno y realiza la acción en ese momento,

1022
00:35:14,960 --> 00:35:15,760
entonces  Termino

1023
00:35:15,760 --> 00:35:20,000
en la posición dos o si estás en la ubicación

1024
00:35:20,000 --> 00:35:22,480
cinco y realizas la acción hacia arriba,

1025
00:35:22,480 --> 00:35:24,240
también terminarás en la posición dos.

1026
00:35:24,240 --> 00:35:27,920


1027
00:35:27,920 --> 00:35:30,560
En esta configuración particular, las

1028
00:35:30,560 --> 00:35:31,200
posiciones

1029
00:35:31,200 --> 00:35:33,040
seis y ocho son estados absorbentes

1030
00:35:33,040 --> 00:35:34,320
porque si recuerdas

1031
00:35:34,320 --> 00:35:36,480
una vez que el agente va a ese

1032
00:35:36,480 --> 00:35:37,680
lugar, no puede

1033
00:35:37,680 --> 00:35:40,640
moverse, así que es cuando termina el episodio, um,

1034
00:35:40,640 --> 00:35:42,720
y si un agente hace un movimiento improbable

1035
00:35:42,720 --> 00:35:44,480
en este laberinto en particular, por ejemplo, si

1036
00:35:44,480 --> 00:35:45,040


1037
00:35:45,040 --> 00:35:47,200
intenta ir de la posición uno a la izquierda,

1038
00:35:47,200 --> 00:35:50,000
simplemente se quedará.  en esa ubicación no se moverá

1039
00:35:50,000 --> 00:35:52,320
en este modelo de género en particular, así

1040
00:35:52,320 --> 00:35:53,920
que solo estoy mirando los estados ocultos

1041
00:35:53,920 --> 00:35:54,640
ahora

1042
00:35:54,640 --> 00:35:56,960
tenemos un producto de transferencia crónico

1043
00:35:56,960 --> 00:35:58,640
entre los dos factores, así que tenemos

1044
00:35:58,640 --> 00:36:00,720
ubicación y contexto aquí,

1045
00:36:00,720 --> 00:36:03,359
el contexto no puede ser cambiado por  el

1046
00:36:03,359 --> 00:36:04,000
agente

1047
00:36:04,000 --> 00:36:05,680
que tenemos porque esto es

1048
00:36:05,680 --> 00:36:07,040
algo que está determinado por el

1049
00:36:07,040 --> 00:36:08,160
entorno y

1050
00:36:08,160 --> 00:36:10,079
esto determina dónde está el objetivo de

1051
00:36:10,079 --> 00:36:12,800
todas las ubicaciones, mientras que la ubicación

1052
00:36:12,800 --> 00:36:14,480
es algo sobre lo que el agente tiene

1053
00:36:14,480 --> 00:36:16,160
control y ahí es donde tenemos la acción

1054
00:36:16,160 --> 00:36:16,800
estados

1055
00:36:16,800 --> 00:36:20,720
sobre este um así que con el contexto tenemos

1056
00:36:20,720 --> 00:36:21,839
dos contextos

1057
00:36:21,839 --> 00:36:25,599
el primero es donde el objetivo está en

1058
00:36:25,599 --> 00:36:28,560
su ubicación ocho y los pasillos en la

1059
00:36:28,560 --> 00:36:29,839
ubicación seis

1060
00:36:29,839 --> 00:36:32,079
y el segundo contexto es donde el objetivo

1061
00:36:32,079 --> 00:36:33,280
está en la ubicación

1062
00:36:33,280 --> 00:36:37,119
seis y el pasillo está en la ubicación ocho

1063
00:36:37,119 --> 00:36:41,119
um  en cada punto de tiempo, el agente

1064
00:36:41,119 --> 00:36:44,560
observará uh dos resultados, uno sería su

1065
00:36:44,560 --> 00:36:46,800
propia posición en este laberinto en particular

1066
00:36:46,800 --> 00:36:48,960
y el segundo sería la puntuación que

1067
00:36:48,960 --> 00:36:50,400
el agente obtendría

1068
00:36:50,400 --> 00:36:52,560
um, la probabilidad de la posición en la cuadrícula

1069
00:36:52,560 --> 00:36:54,720
está completamente determinada por la ubicación

1070
00:36:54,720 --> 00:36:56,079
del agente

1071
00:36:56,079 --> 00:37:01,440
y  la puntuación um está determinada

1072
00:37:01,599 --> 00:37:03,920
tanto por la ubicación como por el contexto en el

1073
00:37:03,920 --> 00:37:04,960
lugar, por lo que

1074
00:37:04,960 --> 00:37:08,960
si el agente está en la ubicación seis

1075
00:37:08,960 --> 00:37:11,680
y está en el contexto dos,

1076
00:37:11,680 --> 00:37:13,119
recibirá una recompensa positiva; de lo

1077
00:37:13,119 --> 00:37:15,119
contrario, recibirá una

1078
00:37:15,119 --> 00:37:16,320
recompensa negativa o mutua

1079
00:37:16,320 --> 00:37:20,320
dependiendo de dónde se encuentre um

1080
00:37:20,320 --> 00:37:22,640
basado en, um, tratar de hacer esa

1081
00:37:22,640 --> 00:37:24,079
comparación con el aprendizaje por refuerzo,

1082
00:37:24,079 --> 00:37:25,839
lo que estamos haciendo aquí es que

1083
00:37:25,839 --> 00:37:28,560
estamos introduciendo uh preferencias principales

1084
00:37:28,560 --> 00:37:29,119
donde

1085
00:37:29,119 --> 00:37:32,640
el agente tiene más cuatro um para

1086
00:37:32,640 --> 00:37:36,320
um recompensa positiva negativa  cuatro cuatro cuatro

1087
00:37:36,320 --> 00:37:38,720
perdón menos cuatro por recompensa negativa y de lo

1088
00:37:38,720 --> 00:37:39,760
contrario, y

1089
00:37:39,760 --> 00:37:42,240
en la primera etapa espera

1090
00:37:42,240 --> 00:37:43,839
estar en la

1091
00:37:43,839 --> 00:37:46,880
primera ubicación,

1092
00:37:47,440 --> 00:37:50,560
así que comparamos este

1093
00:37:50,560 --> 00:37:52,480
modelo de género en particular y el

1094
00:37:52,480 --> 00:37:53,760
agente de inferencia activo con

1095
00:37:53,760 --> 00:37:55,760
dos algoritmos de aprendizaje por refuerzo, por lo que

1096
00:37:55,760 --> 00:37:58,320
el primero fue el  q aprender usando la

1097
00:37:58,320 --> 00:38:00,400
exploración ávida de épsilon

1098
00:38:00,400 --> 00:38:02,079
y el segundo fue el algoritmo de

1099
00:38:02,079 --> 00:38:03,839
aprendizaje por refuerzo basado en el modelo bayesiano que

1100
00:38:03,839 --> 00:38:05,599
usa el muestreo de Thompson estándar

1101
00:38:05,599 --> 00:38:06,720


1102
00:38:06,720 --> 00:38:08,560
y el muestreo de Thompson es un

1103
00:38:08,560 --> 00:38:10,320
procedimiento apropiado aquí porque implica la

1104
00:38:10,320 --> 00:38:12,560
optimización de objetivos duales,

1105
00:38:12,560 --> 00:38:15,920
maximización de la recompensa y ganancia de información,

1106
00:38:15,920 --> 00:38:17,440
y esto se logra al tener esta

1107
00:38:17,440 --> 00:38:19,440
distribución  sobre una función particular

1108
00:38:19,440 --> 00:38:20,960
que primarizamos por

1109
00:38:20,960 --> 00:38:22,720
un previo simplemente teniendo una

1110
00:38:22,720 --> 00:38:26,879
distribución previa sobre ella que muestreamos de

1111
00:38:27,119 --> 00:38:32,320
acuerdo,

1112
00:38:32,320 --> 00:38:35,760
entonces para los dos algoritmos de revestimiento q

1113
00:38:35,760 --> 00:38:38,960
tenemos dos parámetros epsilon codiciosos,

1114
00:38:38,960 --> 00:38:41,680
uno en el que la exploración fija se establece

1115
00:38:41,680 --> 00:38:42,880
en 0.1

1116
00:38:42,880 --> 00:38:44,240
y luego  otro donde tenemos una

1117
00:38:44,240 --> 00:38:46,400
exploración en descomposición que comienza desde

1118
00:38:46,400 --> 00:38:49,920
uno  y se reduce a cero,

1119
00:38:50,880 --> 00:38:54,000
así que primero evaluamos um esto um cómo los

1120
00:38:54,000 --> 00:38:55,440
agentes interactuaban en un

1121
00:38:55,440 --> 00:38:56,320
entorno estacionario donde la

1122
00:38:56,320 --> 00:38:58,960
recompensa no cambiaba uh de tal manera que la

1123
00:38:58,960 --> 00:39:00,800
ubicación del objetivo siempre era seis y

1124
00:39:00,800 --> 00:39:01,760
toda la ubicación

1125
00:39:01,760 --> 00:39:04,240
siempre tenía esa edad y luego evaluamos el

1126
00:39:04,240 --> 00:39:06,000
desempeño de los agentes um, la

1127
00:39:06,000 --> 00:39:07,599
clave para sacar de aquí es que

1128
00:39:07,599 --> 00:39:08,240
tanto el

1129
00:39:08,240 --> 00:39:09,920
bayesiano rl como los

1130
00:39:09,920 --> 00:39:11,680
agentes de inferencia activos pueden

1131
00:39:11,680 --> 00:39:14,400
aprender rápidamente um dónde está la

1132
00:39:14,400 --> 00:39:16,720
ubicación de la recompensa y simplemente maximizarla

1133
00:39:16,720 --> 00:39:19,119
y esto es um este es el rendimiento es

1134
00:39:19,119 --> 00:39:21,119
consistente denotado  por los

1135
00:39:21,119 --> 00:39:23,440
límites de confianza realmente estrechos que vemos

1136
00:39:23,440 --> 00:39:26,960
en comparación, los agentes de aprendizaje q son

1137
00:39:26,960 --> 00:39:30,480
um así que para el que tenemos uh

1138
00:39:30,480 --> 00:39:31,680


1139
00:39:31,680 --> 00:39:34,640
exploración fija es bastante bueno y es capaz

1140
00:39:34,640 --> 00:39:36,240
de aterrizar donde

1141
00:39:36,240 --> 00:39:38,480
se encuentra la recompensa pero hay um alguna

1142
00:39:38,480 --> 00:39:40,320
desviación denotada por  que el

1143
00:39:40,320 --> 00:39:43,359
diez por ciento de seleccionar una acción aleatoria

1144
00:39:43,359 --> 00:39:46,320
um mientras que el q aprendizaje donde tenemos

1145
00:39:46,320 --> 00:39:48,640
epsilon es igual a uno para ganar a cero

1146
00:39:48,640 --> 00:39:51,440
um el rendimiento no es el mejor

1147
00:39:51,440 --> 00:39:53,119
y para el modelo nulo  la

1148
00:39:53,119 --> 00:39:55,520
inferencia activa donde no hay recompensa aquí,

1149
00:39:55,520 --> 00:39:57,680
el agente va aleatoriamente a la sala y

1150
00:39:57,680 --> 00:39:59,040
al azar va a la meta

1151
00:39:59,040 --> 00:40:02,560
50 veces, pero la clave a tener en cuenta

1152
00:40:02,560 --> 00:40:04,160
es que, aparte del no modelo, a

1153
00:40:04,160 --> 00:40:07,040
todos los modelos les está yendo bastante bien y

1154
00:40:07,040 --> 00:40:07,760
parecen  estar

1155
00:40:07,760 --> 00:40:09,760
funcionando bien dentro de la

1156
00:40:09,760 --> 00:40:10,960
configuración estacionaria,

1157
00:40:10,960 --> 00:40:12,720
así que lo siguiente no podría simplemente hacer un

1158
00:40:12,720 --> 00:40:14,319
punto sobre estos experimentos también

1159
00:40:14,319 --> 00:40:14,960
solo

1160
00:40:14,960 --> 00:40:16,640
porque también se desvían ligeramente

1161
00:40:16,640 --> 00:40:19,280
del rl tradicional,

1162
00:40:19,280 --> 00:40:22,480
que es sí genial, así que básicamente

1163
00:40:22,480 --> 00:40:24,000
como generalmente en ro lo que sucede es

1164
00:40:24,000 --> 00:40:25,760
tiene esta ambigüedad entre el

1165
00:40:25,760 --> 00:40:27,040
tiempo de entrenamiento y el rendimiento del tiempo de prueba

1166
00:40:27,040 --> 00:40:28,400
y, por lo general, especialmente

1167
00:40:28,400 --> 00:40:29,440
en algo como q aprendizaje

1168
00:40:29,440 --> 00:40:31,280
, simplemente alcanza el máximo sobre la

1169
00:40:31,280 --> 00:40:32,640
función q o tiene una política

1170
00:40:32,640 --> 00:40:34,800
que intenta hacer como alguna búsqueda sobre

1171
00:40:34,800 --> 00:40:35,680
la función q

1172
00:40:35,680 --> 00:40:39,440
y  toma ese máximo como si se le diera un estado,

1173
00:40:39,440 --> 00:40:40,720
pero hay una especie de

1174
00:40:40,720 --> 00:40:42,480
distinción artificial que es

1175
00:40:42,480 --> 00:40:44,960
como si, obviamente, a medida que adquiere

1176
00:40:44,960 --> 00:40:45,920
datos, comete errores en el entorno en el

1177
00:40:45,920 --> 00:40:46,960
que se encuentra.  Trabajando con el

1178
00:40:46,960 --> 00:40:48,640
entorno real, así que para

1179
00:40:48,640 --> 00:40:49,280
hacer

1180
00:40:49,280 --> 00:40:51,280
una comparación justa entre aquí y la

1181
00:40:51,280 --> 00:40:52,960
inferencia activa donde realmente no tienes

1182
00:40:52,960 --> 00:40:54,319
esta distinción entre el

1183
00:40:54,319 --> 00:40:55,760
tiempo de entrenamiento y el tiempo de prueba, todo es

1184
00:40:55,760 --> 00:40:58,240
solo interacción, esa es la razón

1185
00:40:58,240 --> 00:40:59,599
por la cual el agente de aprendizaje q

1186
00:40:59,599 --> 00:41:01,520
especialmente  cuando epsilon se fija para decir que

1187
00:41:01,520 --> 00:41:03,760
0.1 nunca logra la política óptima

1188
00:41:03,760 --> 00:41:04,720
simplemente porque

1189
00:41:04,720 --> 00:41:07,040
también tenemos una probabilidad de 0.1 de realizar una

1190
00:41:07,040 --> 00:41:08,000
acción aleatoria,

1191
00:41:08,000 --> 00:41:09,280
por lo que no estamos haciendo esta distinción

1192
00:41:09,280 --> 00:41:11,359
como lo haría a veces con un rl normal

1193
00:41:11,359 --> 00:41:11,839
entre el

1194
00:41:11,839 --> 00:41:13,520
tren y el tiempo de prueba, todos estamos diciendo

1195
00:41:13,520 --> 00:41:14,960
entrenar y probar algunos son básicamente

1196
00:41:14,960 --> 00:41:15,520
lo mismo,

1197
00:41:15,520 --> 00:41:17,520
así que sin importar cómo elijas interactuar

1198
00:41:17,520 --> 00:41:19,280
con el mundo es cómo debes ser

1199
00:41:19,280 --> 00:41:20,000
evaluado,

1200
00:41:20,000 --> 00:41:22,319
por eso es que al mirar inicialmente

1201
00:41:22,319 --> 00:41:23,359
a estas chicas podrías pensar

1202
00:41:23,359 --> 00:41:24,880
en por qué no está aprendiendo a resolver esto.

1203
00:41:24,880 --> 00:41:25,359
pero um,

1204
00:41:25,359 --> 00:41:27,599
sí, eso es solo para aclarar algo

1205
00:41:27,599 --> 00:41:28,800
si está más familiarizado, digamos, con algo

1206
00:41:28,800 --> 00:41:30,400
del tipo dprl

1207
00:41:30,400 --> 00:41:34,640
uh procedimiento experimental

1208
00:41:34,640 --> 00:41:38,640
perfecto, gracias, está bien,

1209
00:41:38,640 --> 00:41:41,200
um y luego um solo para  A raíz de

1210
00:41:41,200 --> 00:41:41,920
eso,

1211
00:41:41,920 --> 00:41:43,520
cambiamos un poco el entorno para

1212
00:41:43,520 --> 00:41:45,200
que sea un poco más difícil

1213
00:41:45,200 --> 00:41:46,480
ver si,

1214
00:41:46,480 --> 00:41:48,480
um, el bayesiano y los

1215
00:41:48,480 --> 00:41:49,839
agentes de inferencia activa podrían tener problemas

1216
00:41:49,839 --> 00:41:52,240
cuando empecemos a tener el

1217
00:41:52,240 --> 00:41:53,680
cambio de ubicación después de algunos

1218
00:41:53,680 --> 00:41:56,400
episodios, así que específicamente cambiamos el

1219
00:41:56,400 --> 00:41:58,079
objetivo.  y toda la ubicación

1220
00:41:58,079 --> 00:42:01,119
en um en el punto de tiempo 21

1221
00:42:01,119 --> 00:42:04,319
en el tiempo 121 141

1222
00:42:04,319 --> 00:42:08,319
251 y 451 para que pueda ver que

1223
00:42:08,319 --> 00:42:10,319
en estas figuras donde la línea

1224
00:42:10,319 --> 00:42:12,000
se muestran las líneas grises, por lo que estos

1225
00:42:12,000 --> 00:42:15,200
puntos están todos ubicados invertidos, como la

1226
00:42:15,200 --> 00:42:16,720
configuración estacionaria para el primero

1227
00:42:16,720 --> 00:42:19,440
20 ensayos, todos los asiáticos parecen estar

1228
00:42:19,440 --> 00:42:20,000
haciendo

1229
00:42:20,000 --> 00:42:22,480
lo que cabría esperar, por lo que tanto el

1230
00:42:22,480 --> 00:42:24,240
rl bayesiano como los agentes de inferencia activos lo están

1231
00:42:24,240 --> 00:42:24,640
haciendo

1232
00:42:24,640 --> 00:42:27,760
bastante bien, eh, el aterrizaje q con

1233
00:42:27,760 --> 00:42:30,800
um, el sector de exploración fijo 0.1 está funcionando

1234
00:42:30,800 --> 00:42:32,000
bastante bien,

1235
00:42:32,000 --> 00:42:34,960
como vimos antes y el aprendizaje agudo.

1236
00:42:34,960 --> 00:42:35,680
con la

1237
00:42:35,680 --> 00:42:38,400
exploración en descomposición está funcionando como lo

1238
00:42:38,400 --> 00:42:39,520
estaba haciendo antes,

1239
00:42:39,520 --> 00:42:42,640
pero cuando le das la vuelta, um, de modo que

1240
00:42:42,640 --> 00:42:44,240
las ubicaciones de los objetivos

1241
00:42:44,240 --> 00:42:47,520
cambian, um, lo que notas es que con

1242
00:42:47,520 --> 00:42:48,640
el

1243
00:42:48,640 --> 00:42:52,079
bayesiano rl a  gent la cantidad de

1244
00:42:52,079 --> 00:42:53,760
recompensa o el puntaje que obtiene

1245
00:42:53,760 --> 00:42:56,800
es bastante bajo y luego vemos una fase

1246
00:42:56,800 --> 00:42:57,119
en la

1247
00:42:57,119 --> 00:43:00,000
que luego hace la transición y se convierte nuevamente en

1248
00:43:00,000 --> 00:43:02,160
la política óptima

1249
00:43:02,160 --> 00:43:04,560
en comparación con el agente de inferencia activo

1250
00:43:04,560 --> 00:43:05,599


1251
00:43:05,599 --> 00:43:08,720
donde instantáneamente después del primer intento de

1252
00:43:08,720 --> 00:43:10,560
hacer el  incorrecto es capaz de

1253
00:43:10,560 --> 00:43:12,960
cambiar al activo y lo siento, la

1254
00:43:12,960 --> 00:43:14,640
política adecuada

1255
00:43:14,640 --> 00:43:17,680
y la razón de esto es por estas

1256
00:43:17,680 --> 00:43:19,440
configuraciones de uh rl donde lo estamos considerando

1257
00:43:19,440 --> 00:43:20,880
como un problema de aprendizaje

1258
00:43:20,880 --> 00:43:22,480
, primero debe hacer reversa y

1259
00:43:22,480 --> 00:43:24,480
aprender dónde está la ubicación de la recompensa

1260
00:43:24,480 --> 00:43:26,720
y  luego aprendemos la nueva

1261
00:43:26,720 --> 00:43:28,720
ubicación de la recompensa, por lo que está viendo que para

1262
00:43:28,720 --> 00:43:31,280
q aprende uh tanto para las diferentes

1263
00:43:31,280 --> 00:43:33,200
parametrizaciones codiciosas de épsilon

1264
00:43:33,200 --> 00:43:34,720
como para el rl bayesiano y lo

1265
00:43:34,720 --> 00:43:36,160
vemos de manera consistente

1266
00:43:36,160 --> 00:43:38,160
mientras que para el agente de inferencia activo

1267
00:43:38,160 --> 00:43:39,599
porque lo estamos tratando como  una planificación

1268
00:43:39,599 --> 00:43:41,599
como un problema de inferencia en el que los

1269
00:43:41,599 --> 00:43:42,960
posteriores del estado anterior se

1270
00:43:42,960 --> 00:43:45,359
trasladan al premio y se trasladan

1271
00:43:45,359 --> 00:43:46,160
como anteriores

1272
00:43:46,160 --> 00:43:49,200
y el agente puede darse cuenta instantáneamente de

1273
00:43:49,200 --> 00:43:52,160
que el  la política actual que

1274
00:43:52,160 --> 00:43:52,960
estaba

1275
00:43:52,960 --> 00:43:55,200
siguiendo este paso de tiempo es una adecuación

1276
00:43:55,200 --> 00:43:57,040
cuál es su política para el próximo

1277
00:43:57,040 --> 00:44:00,480
um al otro nuevamente

1278
00:44:00,480 --> 00:44:02,480
um el modelo nulo como se esperaba

1279
00:44:02,480 --> 00:44:04,000
realmente no hace mucho porque solo está

1280
00:44:04,000 --> 00:44:05,599
explorando realmente no le importa

1281
00:44:05,599 --> 00:44:08,160
dónde está la recompensa  o toda la ubicación

1282
00:44:08,160 --> 00:44:09,839
está

1283
00:44:09,839 --> 00:44:11,839
bien, um phil, ¿quieres agregar

1284
00:44:11,839 --> 00:44:14,799
algo a esto o

1285
00:44:15,440 --> 00:44:17,440
no?

1286
00:44:17,440 --> 00:44:18,480


1287
00:44:18,480 --> 00:44:22,160


1288
00:44:22,160 --> 00:44:23,760


1289
00:44:23,760 --> 00:44:25,760


1290
00:44:25,760 --> 00:44:29,440


1291
00:44:29,440 --> 00:44:32,319
tipos de agentes, incluidos el

1292
00:44:32,319 --> 00:44:34,160
bayesiano rl y el q-learning, serían

1293
00:44:34,160 --> 00:44:36,400
marcos razonables para usar, mientras que

1294
00:44:36,400 --> 00:44:39,280
con el uh con una

1295
00:44:39,280 --> 00:44:40,480
configuración estocástica no estacionaria que

1296
00:44:40,480 --> 00:44:42,240
tiene un agente de inferencia activo podría

1297
00:44:42,240 --> 00:44:44,079
ser una forma apropiada de manejar

1298
00:44:44,079 --> 00:44:45,200
dinámicas

1299
00:44:45,200 --> 00:44:47,359
cambiantes, pero la advertencia clave con eso  es

1300
00:44:47,359 --> 00:44:48,480
que puede

1301
00:44:48,480 --> 00:44:50,480
introducir mucha más

1302
00:44:50,480 --> 00:44:53,119
complejidad adicional en el rl bayesiano o el

1303
00:44:53,119 --> 00:44:55,599
aprendizaje q o el marco rl en general

1304
00:44:55,599 --> 00:44:56,560
para permitir

1305
00:44:56,560 --> 00:44:58,800
una forma de manejar la incertidumbre,

1306
00:44:58,800 --> 00:44:59,760
pero no  No sería

1307
00:44:59,760 --> 00:45:02,240
una forma natural de agregarlo,

1308
00:45:02,240 --> 00:45:04,960
tendrías que aumentar la

1309
00:45:04,960 --> 00:45:06,880
función o el algoritmo de una

1310
00:45:06,880 --> 00:45:09,359
manera particular para justificarlo,

1311
00:45:09,359 --> 00:45:12,319
está bien, así que una vez que hayamos hecho esta

1312
00:45:12,319 --> 00:45:15,040
comparación, lo que nos interesa es,

1313
00:45:15,040 --> 00:45:16,960
¿por qué querrías?  para usar la

1314
00:45:16,960 --> 00:45:19,200
inferencia activa um

1315
00:45:19,200 --> 00:45:22,079
cuando um no tiene una comprensión

1316
00:45:22,079 --> 00:45:22,880
sobre el mundo

1317
00:45:22,880 --> 00:45:25,440
o no tiene preferencia

1318
00:45:25,440 --> 00:45:27,040
sobre el tipo de cosas que se pueden

1319
00:45:27,040 --> 00:45:28,640
hacer porque como vimos el

1320
00:45:28,640 --> 00:45:30,240
modelo de inferencia activa el modelo nulo solo está

1321
00:45:30,240 --> 00:45:32,319
explorando  en realidad no está haciendo mucho

1322
00:45:32,319 --> 00:45:34,720
um y eso nos lleva a uno de los

1323
00:45:34,720 --> 00:45:36,319
puntos iniciales que introduje

1324
00:45:36,319 --> 00:45:38,160
al comienzo de la presentación, que

1325
00:45:38,160 --> 00:45:39,599
es que dentro de un

1326
00:45:39,599 --> 00:45:41,200
marco de influencia activa realmente no nos importa

1327
00:45:41,200 --> 00:45:42,560
tener una función de recompensa

1328
00:45:42,560 --> 00:45:45,920
que podemos aprender podemos aprender  eso basado en

1329
00:45:45,920 --> 00:45:48,640
alguna interacción con el entorno

1330
00:45:48,640 --> 00:45:52,240
um así que para eso um

1331
00:45:52,240 --> 00:45:54,480
lo que hicimos fue llevar a cabo algunas

1332
00:45:54,480 --> 00:45:56,160
simulaciones diferentes para ver cómo el

1333
00:45:56,160 --> 00:45:57,920
agente de inferencia activa puede seleccionar

1334
00:45:57,920 --> 00:45:59,520
diferentes tipos de políticas en

1335
00:45:59,520 --> 00:46:02,079
ausencia de una p previa  referencias

1336
00:46:02,079 --> 00:46:04,560
y para esto hicimos tres experimentos diferentes

1337
00:46:04,560 --> 00:46:05,280


1338
00:46:05,280 --> 00:46:08,160
um donde permitimos que la

1339
00:46:08,160 --> 00:46:09,040
probabilidad

1340
00:46:09,040 --> 00:46:11,359
y todas las preferencias de resultados se

1341
00:46:11,359 --> 00:46:12,240
aprendieran

1342
00:46:12,240 --> 00:46:15,440
con el tiempo y vimos cómo interactuaba el agente

1343
00:46:15,440 --> 00:46:16,560
cuando

1344
00:46:16,560 --> 00:46:19,680
este aprendizaje sobre las diferentes um

1345
00:46:19,680 --> 00:46:22,640
preferencias uh tenía lugar y la

1346
00:46:22,640 --> 00:46:24,160
forma en que  ¿Fue

1347
00:46:24,160 --> 00:46:26,960
usando los modelos de conjugación donde, debido a que

1348
00:46:26,960 --> 00:46:28,800
estos son modelos de estado discretos, tenemos

1349
00:46:28,800 --> 00:46:30,800
distribuciones categóricas sobre los parámetros de nuestro modelo

1350
00:46:30,800 --> 00:46:31,839


1351
00:46:31,839 --> 00:46:33,760
y estamos introduciendo

1352
00:46:33,760 --> 00:46:34,880
distribuciones direccionales en la parte superior

1353
00:46:34,880 --> 00:46:36,960
como un precio híbrido y aprendiendo esos

1354
00:46:36,960 --> 00:46:38,319
hiperprecios?

1355
00:46:38,319 --> 00:46:41,119
Podemos entrar en los detalles de  esto, si

1356
00:46:41,119 --> 00:46:42,800
alguien tiene alguna pregunta, pero

1357
00:46:42,800 --> 00:46:46,400
solo por simplicidad, suponga que

1358
00:46:46,400 --> 00:46:48,800
todas las distribuciones de Dirichlet se

1359
00:46:48,800 --> 00:46:50,880
configuraron en planas completamente planas y

1360
00:46:50,880 --> 00:46:54,079
todos los agentes tuvieron la oportunidad

1361
00:46:54,079 --> 00:46:54,800
de aterrizar en

1362
00:46:54,800 --> 00:46:57,359
función de las diferentes interacciones que tuvieron,

1363
00:46:57,359 --> 00:46:58,400
por lo que el primer conjunto de

1364
00:46:58,400 --> 00:47:00,160
experimentos como las simulaciones  que

1365
00:47:00,160 --> 00:47:02,319
ejecutamos fueron para comprender cómo

1366
00:47:02,319 --> 00:47:04,640
el agente reduciría la incertidumbre

1367
00:47:04,640 --> 00:47:05,520
sobre el

1368
00:47:05,520 --> 00:47:07,280
entorno en el que se encuentra.  interactuando con entonces,

1369
00:47:07,280 --> 00:47:09,040
si no

1370
00:47:09,040 --> 00:47:12,319
sabe el lago congelado,

1371
00:47:12,319 --> 00:47:14,720
¿cómo interactuaría o exploraría ese

1372
00:47:14,720 --> 00:47:15,599
lago?

1373
00:47:15,599 --> 00:47:18,880


1374
00:47:18,880 --> 00:47:21,920


1375
00:47:21,920 --> 00:47:23,680


1376
00:47:23,680 --> 00:47:25,920


1377
00:47:25,920 --> 00:47:28,559
el lugar donde estaba el objetivo o toda la ubicación

1378
00:47:28,559 --> 00:47:29,040


1379
00:47:29,040 --> 00:47:32,079
um y esto se destaca en

1380
00:47:32,079 --> 00:47:35,119
una serie de diferentes

1381
00:47:35,119 --> 00:47:36,800
trayectorias de exploración um que vemos, así que la

1382
00:47:36,800 --> 00:47:38,960
primera fue donde el agente cae en el

1383
00:47:38,960 --> 00:47:39,359
pasillo

1384
00:47:39,359 --> 00:47:42,400
um y luego otro donde da la

1385
00:47:42,400 --> 00:47:43,359
vuelta

1386
00:47:43,359 --> 00:47:46,079
y termina en  el segundo en el segundo

1387
00:47:46,079 --> 00:47:48,160
episodio en la ubicación objetivo

1388
00:47:48,160 --> 00:47:50,079
y en algún lugar simplemente termina el

1389
00:47:50,079 --> 00:47:51,200
episodio

1390
00:47:51,200 --> 00:47:52,960
simplemente yendo y viniendo, así que esto es

1391
00:47:52,960 --> 00:47:54,800
solo exploración del piso,

1392
00:47:54,800 --> 00:47:58,000
nada más que escuchar, um,

1393
00:47:58,000 --> 00:48:01,440
el siguiente conjunto de análisis o

1394
00:48:01,440 --> 00:48:03,520
simulaciones que ejecutamos fue para ver qué

1395
00:48:03,520 --> 00:48:05,440
sucedería si el agente supiera sobre el

1396
00:48:05,440 --> 00:48:07,599
mundo pero no tuviera preferencias sobre el tipo

1397
00:48:07,599 --> 00:48:08,480
de resultados en los que

1398
00:48:08,480 --> 00:48:12,160
esperaba estar y para esto,

1399
00:48:12,160 --> 00:48:15,920
ejecutamos múltiples simulaciones diferentes

1400
00:48:15,920 --> 00:48:19,200
y vimos que en ausencia de

1401
00:48:19,200 --> 00:48:22,240
um cualquier  tipo de preferencias,

1402
00:48:22,240 --> 00:48:24,079
um, todo podría volverse

1403
00:48:24,079 --> 00:48:25,599
realmente atractivo

1404
00:48:25,599 --> 00:48:28,480
si se encuentra primero, así que vemos que

1405
00:48:28,480 --> 00:48:29,440
en la

1406
00:48:29,440 --> 00:48:31,680
primera figura donde el agente aprende a

1407
00:48:31,680 --> 00:48:34,000
preferir esconderse en los pasillos

1408
00:48:34,000 --> 00:48:36,160
y en el segundo tipo de prueba que

1409
00:48:36,160 --> 00:48:38,000
vimos fue donde el agente

1410
00:48:38,000 --> 00:48:41,520
exhibió el  preferencia de ir realmente

1411
00:48:41,520 --> 00:48:43,359
a la ubicación objetivo, por lo que

1412
00:48:43,359 --> 00:48:45,200
depende completamente de la

1413
00:48:45,200 --> 00:48:47,680
instanciación o el tipo de estímulo al

1414
00:48:47,680 --> 00:48:49,119
que está expuesto el agente

1415
00:48:49,119 --> 00:48:51,119
inicialmente que determina el tipo de

1416
00:48:51,119 --> 00:48:52,319
preferencias que

1417
00:48:52,319 --> 00:48:55,200
aprendería a tener

1418
00:48:56,240 --> 00:48:59,520
bien y luego el último conjunto de

1419
00:48:59,520 --> 00:49:01,520
simulaciones que ejecutamos  fue solo para

1420
00:49:01,520 --> 00:49:03,119
verificar qué sucedería cuando

1421
00:49:03,119 --> 00:49:04,640
interactuáramos con los

1422
00:49:04,640 --> 00:49:06,640
imperativos epistémicos en realidad resolver la

1423
00:49:06,640 --> 00:49:08,000
incertidumbre sobre el

1424
00:49:08,000 --> 00:49:09,440
entorno con el que el agente estaba

1425
00:49:09,440 --> 00:49:11,200
interactuando, específicamente el

1426
00:49:11,200 --> 00:49:13,200
mapeo de probabilidad entre los resultados

1427
00:49:13,200 --> 00:49:14,480
dados los estados y

1428
00:49:14,480 --> 00:49:16,880
la incertidumbre sobre el estado

1429
00:49:16,880 --> 00:49:18,559
de cosas deseado que el agente esperaba

1430
00:49:18,559 --> 00:49:19,839
ella misma  estar adentro

1431
00:49:19,839 --> 00:49:22,960
y lo que vimos fue que um si

1432
00:49:22,960 --> 00:49:25,520
permitimos um un número suficiente de intentos

1433
00:49:25,520 --> 00:49:27,040
para pasar  s el agente

1434
00:49:27,040 --> 00:49:28,640
en este entorno particular aprende a

1435
00:49:28,640 --> 00:49:31,119
preferir montar agujeros de escondite

1436
00:49:31,119 --> 00:49:34,800
después de x número de episodios pero tenía una

1437
00:49:34,800 --> 00:49:36,000


1438
00:49:36,000 --> 00:49:38,960
preferencia muy distinta sobre el tipo de resultados en los

1439
00:49:38,960 --> 00:49:39,280
que

1440
00:49:39,280 --> 00:49:41,119
esperaba estar dependiendo

1441
00:49:41,119 --> 00:49:42,480
del

1442
00:49:42,480 --> 00:49:46,400
punto de tiempo en particular

1443
00:49:46,400 --> 00:49:50,079
eh  eso me lleva a la última

1444
00:49:50,079 --> 00:49:53,839
simulación, así que voy a terminar,

1445
00:49:53,839 --> 00:49:57,200
que está en inferencia activa,

1446
00:49:57,200 --> 00:49:59,040
comencé la presentación diciendo

1447
00:49:59,040 --> 00:50:00,640
que es un algoritmo particular que

1448
00:50:00,640 --> 00:50:01,200
nos

1449
00:50:01,200 --> 00:50:03,599
da cosas muy buenas para considerar cuando

1450
00:50:03,599 --> 00:50:04,559
estamos

1451
00:50:04,559 --> 00:50:07,760
operando en un bayesiano  o entorno basado en creencias

1452
00:50:07,760 --> 00:50:08,960
que es, en

1453
00:50:08,960 --> 00:50:11,680
primer lugar, la explicación principal de

1454
00:50:11,680 --> 00:50:13,359
la exploración epistémica y la

1455
00:50:13,359 --> 00:50:15,119
motivación intrínseca que obtenemos de la

1456
00:50:15,119 --> 00:50:17,040


1457
00:50:17,040 --> 00:50:19,440
descomposición de energía libre esperada particular por la que pasamos. La

1458
00:50:19,440 --> 00:50:20,400
segunda cosa

1459
00:50:20,400 --> 00:50:23,359
que quería resaltar era que,

1460
00:50:23,359 --> 00:50:25,040
bajo un entorno de inferencia activa

1461
00:50:25,040 --> 00:50:26,000
, no  tiene que

1462
00:50:26,000 --> 00:50:28,240
especificar explícitamente una función de recompensa que vimos

1463
00:50:28,240 --> 00:50:29,119
en el último

1464
00:50:29,119 --> 00:50:32,079
uh segundo conjunto de simulaciones donde

1465
00:50:32,079 --> 00:50:34,000
el agente también puede aprender su propia recompensa

1466
00:50:34,000 --> 00:50:35,680
a  y prefieren convertirse en algo que es

1467
00:50:35,680 --> 00:50:37,359
bastante contrario a la intuición

1468
00:50:37,359 --> 00:50:39,839
de un entorno rl donde la señal

1469
00:50:39,839 --> 00:50:41,440
del entorno dice que algo es

1470
00:50:41,440 --> 00:50:42,319
malo, pero las

1471
00:50:42,319 --> 00:50:44,160
motivaciones internas de

1472
00:50:44,160 --> 00:50:46,240
preferencia del agente le permiten

1473
00:50:46,240 --> 00:50:48,559
hacer algo que está bastante en desacuerdo

1474
00:50:48,559 --> 00:50:50,319
con lo que espera el entorno.

1475
00:50:50,319 --> 00:50:51,359
que hacer

1476
00:50:51,359 --> 00:50:53,599
y, por último, debido a esta creencia

1477
00:50:53,599 --> 00:50:55,280
, la incertidumbre del entorno de invasión es una

1478
00:50:55,280 --> 00:50:57,119
parte natural de la

1479
00:50:57,119 --> 00:51:00,559
actualización de creencias, por lo que dentro de los

1480
00:51:00,559 --> 00:51:02,319
entornos estacionarios, los

1481
00:51:02,319 --> 00:51:04,000
agentes de inferencia activos se desempeñan tan bien como

1482
00:51:04,000 --> 00:51:05,200
los agentes de aprendizaje por refuerzo,

1483
00:51:05,200 --> 00:51:07,359
sin embargo, en entornos no estacionarios se desempeñaron

1484
00:51:07,359 --> 00:51:08,960
mejor debido a su capacidad para

1485
00:51:08,960 --> 00:51:10,480
llevar a cabo esta planificación como

1486
00:51:10,480 --> 00:51:13,280
inferencia y eso es lo que creo que se sostiene,

1487
00:51:13,280 --> 00:51:15,280
esto no es una declaración concluyente,

1488
00:51:15,280 --> 00:51:15,760


1489
00:51:15,760 --> 00:51:18,240
pero es una buena manera de comenzar a pensar en

1490
00:51:18,240 --> 00:51:19,040
cómo,

1491
00:51:19,040 --> 00:51:21,280
si está ampliando los

1492
00:51:21,280 --> 00:51:23,440
agentes de inferencia activos para interactuar en el mismo tipo de

1493
00:51:23,440 --> 00:51:24,160
entornos

1494
00:51:24,160 --> 00:51:26,960
que el aprendizaje por refuerzo.  agentes que

1495
00:51:26,960 --> 00:51:27,760
pueden ser

1496
00:51:27,760 --> 00:51:30,079
un poco difíciles de resolver porque

1497
00:51:30,079 --> 00:51:30,839
hay algo de

1498
00:51:30,839 --> 00:51:32,640
no estacionariedad o algo extraño

1499
00:51:32,640 --> 00:51:33,920
las fluctuaciones que ocurren en el

1500
00:51:33,920 --> 00:51:34,640
entorno

1501
00:51:34,640 --> 00:51:36,240
y los agentes de inferencia activos

1502
00:51:36,240 --> 00:51:38,240
podrían funcionar bastante bien si

1503
00:51:38,240 --> 00:51:42,000
tenemos esta planificación como inferencia configurada

1504
00:51:42,079 --> 00:51:44,800
y eso me lleva al final de la

1505
00:51:44,800 --> 00:51:46,480
presentación, así que solo quiero agradecer a

1506
00:51:46,480 --> 00:51:48,240
todos los que participaron en este trabajo

1507
00:51:48,240 --> 00:51:48,559


1508
00:51:48,559 --> 00:51:51,520
llamado tom  y phil y todos los que

1509
00:51:51,520 --> 00:51:52,640
me ayudaron a pensar en estas

1510
00:51:52,640 --> 00:51:55,920
ideas interesantes que presenté,

1511
00:51:55,920 --> 00:51:58,079
así como a todos por escuchar,

1512
00:51:58,079 --> 00:51:59,920
gracias,

1513
00:51:59,920 --> 00:52:04,400
gracias, excelente charla

1514
00:52:04,400 --> 00:52:07,119
para que tal vez puedan dejar de compartir y podamos

1515
00:52:07,119 --> 00:52:08,000
hacer algunas preguntas

1516
00:52:08,000 --> 00:52:09,599
y si alguien que está viendo en vivo

1517
00:52:09,599 --> 00:52:11,359
quiere hacer preguntas.  son

1518
00:52:11,359 --> 00:52:14,400
más que bienvenidos, así que tal vez comenzaré

1519
00:52:14,400 --> 00:52:16,160
con solo

1520
00:52:16,160 --> 00:52:18,000
un punto general de que fue increíble

1521
00:52:18,000 --> 00:52:20,480
ver cuán claramente diferenció

1522
00:52:20,480 --> 00:52:21,280
entre

1523
00:52:21,280 --> 00:52:22,880
el aprendizaje por refuerzo y el

1524
00:52:22,880 --> 00:52:24,640
paradigma de inferencia activa

1525
00:52:24,640 --> 00:52:27,040
y solo una pregunta que tuve fue que

1526
00:52:27,040 --> 00:52:27,760
mencionó eso

1527
00:52:27,760 --> 00:52:30,319
en el caso  de un paso de tiempo de uno un

1528
00:52:30,319 --> 00:52:31,599
horizonte de tiempo de uno

1529
00:52:31,599 --> 00:52:33,359
había como una equivalencia entre

1530
00:52:33,359 --> 00:52:34,960
el enfoque de aprendizaje por refuerzo y

1531
00:52:34,960 --> 00:52:36,640
el infe activo  enfoque

1532
00:52:36,640 --> 00:52:38,640
actual ahora las personas están utilizando el

1533
00:52:38,640 --> 00:52:40,400
aprendizaje por refuerzo

1534
00:52:40,400 --> 00:52:42,400
para planificar el futuro en medio de la

1535
00:52:42,400 --> 00:52:43,599
incertidumbre,

1536
00:52:43,599 --> 00:52:46,240
entonces, ¿cómo están logrando ese

1537
00:52:46,240 --> 00:52:47,839
tipo de planificación

1538
00:52:47,839 --> 00:52:50,400
y dónde hay algunas situaciones en las que

1539
00:52:50,400 --> 00:52:51,440
la inferencia activa

1540
00:52:51,440 --> 00:52:53,920
podría entrar en esos

1541
00:52:53,920 --> 00:52:54,800
entornos de um y

1542
00:52:54,800 --> 00:52:57,760
potencialmente hacerlo mejor? Creo que eso

1543
00:52:57,760 --> 00:52:58,480
ya es  una pregunta,

1544
00:52:58,480 --> 00:53:00,400
una buena pregunta y siéntete libre de que participes

1545
00:53:00,400 --> 00:53:02,720
si me pierdo algo, pero

1546
00:53:02,720 --> 00:53:04,720
dentro de la configuración de URL por lo que

1547
00:53:04,720 --> 00:53:07,040
sé, si están considerando

1548
00:53:07,040 --> 00:53:09,280
horizontes temporales mayores que uno,

1549
00:53:09,280 --> 00:53:10,960
entonces tienen rl jerárquica

1550
00:53:10,960 --> 00:53:13,280
o tienen opciones donde están

1551
00:53:13,280 --> 00:53:14,800
considerando trayectorias que son

1552
00:53:14,800 --> 00:53:16,240
mayores que

1553
00:53:16,240 --> 00:53:18,800
una que les permite considerar una

1554
00:53:18,800 --> 00:53:20,720
secuencia completa de juego

1555
00:53:20,720 --> 00:53:24,319
um antes de que les guste um que se implementa

1556
00:53:24,319 --> 00:53:26,000
si seleccionan una política particular en

1557
00:53:26,000 --> 00:53:27,680
lugar de esa, en realidad

1558
00:53:27,680 --> 00:53:32,800
una sola acción para establecer el mapeo um

1559
00:53:32,800 --> 00:53:34,880
pero yo no soy yo  Realmente no he trabajado

1560
00:53:34,880 --> 00:53:36,559
tanto con opciones, tal vez

1561
00:53:36,559 --> 00:53:39,119
Phil, ¿sabes? Sí, las opciones son

1562
00:53:39,119 --> 00:53:41,280
como una forma de

1563
00:53:41,280 --> 00:53:45,119
lograr este tipo de pasos múltiples.  una especie de

1564
00:53:45,119 --> 00:53:47,680
um como supongo que de gran tamaño casi como

1565
00:53:47,680 --> 00:53:49,359
este sesgo inductivo en el sentido de que

1566
00:53:49,359 --> 00:53:52,000
le gustará que sus elecciones sean una especie

1567
00:53:52,000 --> 00:53:54,079
de bloques contiguos de

1568
00:53:54,079 --> 00:53:55,520
más de un paso, mientras que una

1569
00:53:55,520 --> 00:53:57,280
especie de política es como si fuera

1570
00:53:57,280 --> 00:53:58,720
a dar un paso y  entonces tendré este

1571
00:53:58,720 --> 00:53:59,359
tipo de

1572
00:53:59,359 --> 00:54:02,720
próximo estado en el que existo, um, pero creo que

1573
00:54:02,720 --> 00:54:04,240
lo que es importante eliminar la ambigüedad es que en

1574
00:54:04,240 --> 00:54:06,000
realidad puedes planificar las

1575
00:54:06,000 --> 00:54:07,440
cosas del estilo del modelo porque

1576
00:54:07,440 --> 00:54:09,119
puedes ver la inferencia activa como una especie

1577
00:54:09,119 --> 00:54:10,880
de modelo basado

1578
00:54:10,880 --> 00:54:13,680
en el tipo de  conoce el paradigma rl y, de

1579
00:54:13,680 --> 00:54:15,280
hecho, le gusta dentro del modelo basado en

1580
00:54:15,280 --> 00:54:18,400
que puede tener

1581
00:54:18,400 --> 00:54:20,800
métodos de planificación similares a los que

1582
00:54:20,800 --> 00:54:21,440
se hacen

1583
00:54:21,440 --> 00:54:23,040
en la inferencia activa, sin embargo, supongo que

1584
00:54:23,040 --> 00:54:24,480
la distinción clave aquí es

1585
00:54:24,480 --> 00:54:27,440
cuando planifica en el modelo basado en rl

1586
00:54:27,440 --> 00:54:27,760


1587
00:54:27,760 --> 00:54:29,920
simplemente está tratando de  toma, tiendes a

1588
00:54:29,920 --> 00:54:30,800
tomar algún tipo de

1589
00:54:30,800 --> 00:54:32,880
traje como una especie de mini

1590
00:54:32,880 --> 00:54:34,079
método evolutivo, generalmente algo así como

1591
00:54:34,079 --> 00:54:35,920
el método de entropía cruzada y lo que estás

1592
00:54:35,920 --> 00:54:37,440
haciendo es este tipo de búsqueda sobre todas las

1593
00:54:37,440 --> 00:54:38,640
acciones que tomas

1594
00:54:38,640 --> 00:54:40,480
en un cierto punto i  n tiempo que

1595
00:54:40,480 --> 00:54:42,319
maximiza su tipo de recompensa

1596
00:54:42,319 --> 00:54:45,200
en un horizonte de, digamos, 20 pasos um

1597
00:54:45,200 --> 00:54:46,559
y puede hacer algunas cosas inteligentes, puede

1598
00:54:46,559 --> 00:54:48,880
terminar con una función de valor q

1599
00:54:48,880 --> 00:54:51,280
um pero fundamentalmente como si

1600
00:54:51,280 --> 00:54:52,799
no lo alejara de este tipo de

1601
00:54:52,799 --> 00:54:55,359
diferencia de definición que es

1602
00:54:55,359 --> 00:54:57,760
en la inferencia activa, está haciendo este

1603
00:54:57,760 --> 00:54:59,119
tipo de planificación sobre las acciones a

1604
00:54:59,119 --> 00:55:00,480
pesar de que pueden parecer superficialmente

1605
00:55:00,480 --> 00:55:02,000
similares,

1606
00:55:02,000 --> 00:55:03,680
básicamente como dibujar muchas acciones

1607
00:55:03,680 --> 00:55:05,280
y ver cuáles maximizan algún tipo

1608
00:55:05,280 --> 00:55:06,000
de utilidad

1609
00:55:06,000 --> 00:55:09,119
en el caso de la inferencia activa, todo el

1610
00:55:09,119 --> 00:55:10,880
tipo de información útil  y la

1611
00:55:10,880 --> 00:55:12,559
tasa deseada sobre um

1612
00:55:12,559 --> 00:55:15,760
como exploración y su y um

1613
00:55:15,760 --> 00:55:17,839
explotación están envueltas en esa

1614
00:55:17,839 --> 00:55:20,000
maximización mientras que en rl

1615
00:55:20,000 --> 00:55:23,440
es un poco más difícil de entender um como

1616
00:55:23,440 --> 00:55:24,799
en el sentido clásico solo estamos tratando

1617
00:55:24,799 --> 00:55:26,960
de maximizar la recompensa pero um

1618
00:55:26,960 --> 00:55:28,720
puedes tener  heurística donde dices oh,

1619
00:55:28,720 --> 00:55:30,240
pero tal vez también quiero maximizar alguna

1620
00:55:30,240 --> 00:55:31,200
noción de

1621
00:55:31,200 --> 00:55:33,359
incertidumbre del modelo y sabes que

1622
00:55:33,359 --> 00:55:34,640
se vuelve un poco

1623
00:55:34,640 --> 00:55:37,280
más difícil de integrar de forma natural  califique

1624
00:55:37,280 --> 00:55:39,680
todos estos enfoques en lo mismo,

1625
00:55:39,680 --> 00:55:41,680
um porque, como en el ejemplo de la influencia activa,

1626
00:55:41,680 --> 00:55:42,400
porque todo es una

1627
00:55:42,400 --> 00:55:43,280


1628
00:55:43,280 --> 00:55:44,720
distribución, simplemente coloque un hiperanterior si lo

1629
00:55:44,720 --> 00:55:46,480
desea, simplemente intégrelo

1630
00:55:46,480 --> 00:55:47,760
si no desea lidiar con una

1631
00:55:47,760 --> 00:55:49,920
especie de ajuste.  así que sí, ahí es

1632
00:55:49,920 --> 00:55:50,880
donde siento que

1633
00:55:50,880 --> 00:55:53,920
hay este tipo de diferencia

1634
00:55:54,000 --> 00:55:57,280
genial y qué tipo de configuraciones

1635
00:55:57,280 --> 00:56:00,640
crees que esos tipos de

1636
00:56:00,640 --> 00:56:03,200
acción como inferencia y planificación como

1637
00:56:03,200 --> 00:56:04,240
inferencia podrían

1638
00:56:04,240 --> 00:56:06,960
utilizarse como qué tipo de conjuntos de datos o

1639
00:56:06,960 --> 00:56:08,880
preguntas o  los contextos

1640
00:56:08,880 --> 00:56:10,799
son personas que actualmente usan otro tipo

1641
00:56:10,799 --> 00:56:12,720
de método, pero luego te emociona ver que

1642
00:56:12,720 --> 00:56:17,839
la inferencia activa juega un papel.

1643
00:56:18,720 --> 00:56:21,280


1644
00:56:21,280 --> 00:56:23,520


1645
00:56:23,520 --> 00:56:26,640


1646
00:56:26,640 --> 00:56:30,079


1647
00:56:30,079 --> 00:56:31,839
interactuar con el entorno es a través de

1648
00:56:31,839 --> 00:56:33,839
una función de recompensa,

1649
00:56:33,839 --> 00:56:36,079
um, así que cualquier cosa en la que realmente no

1650
00:56:36,079 --> 00:56:37,280
tengas eso o tengas,

1651
00:56:37,280 --> 00:56:40,640
um, un entorno que está cambiando,

1652
00:56:40,640 --> 00:56:43,440
um, pero sé que hay un gran problema.

1653
00:56:43,440 --> 00:56:44,160
La mayoría de

1654
00:56:44,160 --> 00:56:46,720
las personas dentro de la comunidad rl

1655
00:56:46,720 --> 00:56:47,599
trabajan en

1656
00:56:47,599 --> 00:56:50,160
motivación intrínseca o motivación interna,

1657
00:56:50,160 --> 00:56:51,520


1658
00:56:51,520 --> 00:56:54,160
por lo que ese tipo de cosas se superponen con

1659
00:56:54,160 --> 00:56:54,559
la

1660
00:56:54,559 --> 00:56:58,960
formulación de influencia activa,

1661
00:56:58,960 --> 00:57:02,400
pero creo que para paradigmas particulares, para mí,

1662
00:57:02,400 --> 00:57:04,640
el aspecto más interesante de la

1663
00:57:04,640 --> 00:57:06,079
influencia activa proviene del hecho

1664
00:57:06,079 --> 00:57:09,440
cuando comienzas.  pensando en um

1665
00:57:09,440 --> 00:57:12,240
agentes biológicos y si está modelando a

1666
00:57:12,240 --> 00:57:13,280
un paciente

1667
00:57:13,280 --> 00:57:16,240
o alguien con esquizofrenia, puede

1668
00:57:16,240 --> 00:57:18,079
con este marco bayesiano puede

1669
00:57:18,079 --> 00:57:21,760
cambiar um los antecedentes para tratar de ver

1670
00:57:21,760 --> 00:57:24,960
cómo interactúa la persona

1671
00:57:24,960 --> 00:57:27,920
donde supongo que podría ser que

1672
00:57:27,920 --> 00:57:28,799
su política

1673
00:57:28,799 --> 00:57:30,079
como la  la forma en que están valorando sus

1674
00:57:30,079 --> 00:57:32,319
políticas están rotas o simplemente podrían ser

1675
00:57:32,319 --> 00:57:35,599
otras partes que son diferentes,

1676
00:57:35,599 --> 00:57:38,720
pero creo que dentro de la configuración de rl si

1677
00:57:38,720 --> 00:57:43,440
estás saliendo

1678
00:57:43,440 --> 00:57:46,480
del estándar, el modo de juego es correcto, como

1679
00:57:46,480 --> 00:57:47,359
magia o

1680
00:57:47,359 --> 00:57:49,040
entornos de gimnasio y comienza a entrar

1681
00:57:49,040 --> 00:57:50,799
en uno.  que son más abiertos y

1682
00:57:50,799 --> 00:57:52,480
no tienes ninguna recompensa, entonces

1683
00:57:52,480 --> 00:57:54,160
la influencia activa podría ser potencialmente

1684
00:57:54,160 --> 00:57:56,240
útil,

1685
00:57:56,240 --> 00:57:58,640
pero dudo un poco  diga

1686
00:57:58,640 --> 00:57:59,599
si será

1687
00:57:59,599 --> 00:58:02,640
mejor porque si comienza a

1688
00:58:02,640 --> 00:58:05,040
aumentar el rl bayesiano con todo tipo de

1689
00:58:05,040 --> 00:58:06,839
componentes interesantes,

1690
00:58:06,839 --> 00:58:10,160
entonces, hasta cierto punto, estará

1691
00:58:10,160 --> 00:58:13,680
activo al frente ampliado,

1692
00:58:13,680 --> 00:58:15,760
lo que es potencialmente un

1693
00:58:15,760 --> 00:58:16,880
punto

1694
00:58:16,880 --> 00:58:20,319
polémico, pero creo que depende

1695
00:58:20,319 --> 00:58:22,079
exactamente de qué  usted estaba incorporando y

1696
00:58:22,079 --> 00:58:23,920
es por eso que para esta

1697
00:58:23,920 --> 00:58:26,000
presentación en particular y nuestro trabajo tuvimos mucho

1698
00:58:26,000 --> 00:58:27,599
cuidado al definir qué

1699
00:58:27,599 --> 00:58:29,599
significaba el aprendizaje por refuerzo, que es

1700
00:58:29,599 --> 00:58:31,760
que debe tener esta

1701
00:58:31,760 --> 00:58:33,280
función de recompensa en juego y desea

1702
00:58:33,280 --> 00:58:35,040
maximizar esta función de recompensa

1703
00:58:35,040 --> 00:58:37,280
y cualquier cosa que incluya en el rl

1704
00:58:37,280 --> 00:58:39,040
El marco tiene que tener

1705
00:58:39,040 --> 00:58:42,000
eso, eh, que le guste el objeto, ese

1706
00:58:42,000 --> 00:58:44,000
objetivo tiene que estar allí, pero si usted,

1707
00:58:44,000 --> 00:58:45,839
si los pasa por alto, dice que está bien,

1708
00:58:45,839 --> 00:58:47,040
solo voy a agregar todo tipo de

1709
00:58:47,040 --> 00:58:48,480
componentes interesantes

1710
00:58:48,480 --> 00:58:51,200
para hacer el algoritmo o la forma, um, el

1711
00:58:51,200 --> 00:58:52,400
agente.

1712
00:58:52,400 --> 00:58:55,599
interactúa con el entorno, um,

1713
00:58:55,599 --> 00:58:57,760
supongo que es similar a un

1714
00:58:57,760 --> 00:58:59,119
marco de inferencia activo

1715
00:58:59,119 --> 00:59:02,000
o incluso quizás incluso mejor, entonces

1716
00:59:02,000 --> 00:59:03,040
no haces

1717
00:59:03,040 --> 00:59:06,319
esa distinción fina entre  re rl

1718
00:59:06,319 --> 00:59:08,400
es mejor o donde la influencia activa

1719
00:59:08,400 --> 00:59:10,960
es mejor, en realidad no está ahí

1720
00:59:10,960 --> 00:59:12,559
para mí,

1721
00:59:12,559 --> 00:59:15,680
porque creo que ambas comunidades, desde

1722
00:59:15,680 --> 00:59:16,079
mi

1723
00:59:16,079 --> 00:59:17,599
perspectiva, están trabajando a partir de

1724
00:59:17,599 --> 00:59:19,440
cosas similares, que es la toma de decisiones secuencial,

1725
00:59:19,440 --> 00:59:20,400


1726
00:59:20,400 --> 00:59:23,760
y con nuestro trabajo es principalmente

1727
00:59:23,760 --> 00:59:26,000
con la toma de decisiones secuencial en

1728
00:59:26,000 --> 00:59:27,520
el rostro de la incertidumbre,

1729
00:59:27,520 --> 00:59:29,920
mientras que parte de nuestro trabajo, mi trabajo de rl,

1730
00:59:29,920 --> 00:59:30,720
podría no estar

1731
00:59:30,720 --> 00:59:33,440
centrado en eso, así que creo que cuando

1732
00:59:33,440 --> 00:59:33,920
comienzas a

1733
00:59:33,920 --> 00:59:35,359
trazar los límites, se vuelve un

1734
00:59:35,359 --> 00:59:37,119
poco confuso donde las

1735
00:59:37,119 --> 00:59:40,079
cosas están separadas o no, y creo que

1736
00:59:40,079 --> 00:59:41,920
subí un poco  tangente

1737
00:59:41,920 --> 00:59:45,760
um pero phil, ¿quieres agregar

1738
00:59:45,760 --> 00:59:47,119
algo a eso en términos de

1739
00:59:47,119 --> 00:59:49,280
entornos y paradigmas que

1740
00:59:49,280 --> 00:59:49,839
podrían ser

1741
00:59:49,839 --> 00:59:53,520
útiles? Sí, creo que si

1742
00:59:53,520 --> 00:59:55,359
uno de tus objetivos es decir que te dan un

1743
00:59:55,359 --> 00:59:57,440
entorno en el que simplemente no tienes

1744
00:59:57,440 --> 00:59:58,640
conocimiento previo sobre

1745
00:59:58,640 --> 01:00:01,599
cómo  debería comportarse, podría argumentar

1746
01:00:01,599 --> 01:00:03,359
que podría implementar un agente rl que

1747
01:00:03,359 --> 01:00:05,200
use algún tipo de

1748
01:00:05,200 --> 01:00:08,720
mecanismo de reducción de incertidumbre epistémica o curiosidad,

1749
01:00:08,720 --> 01:00:10,559
pero quiero decir que sé que hay un

1750
01:00:10,559 --> 01:00:12,319
poco de  trabajo sobre el aprendizaje previo

1751
01:00:12,319 --> 01:00:14,160
sobre las funciones de recompensa y el aprendizaje de esas,

1752
01:00:14,160 --> 01:00:14,640
pero

1753
01:00:14,640 --> 01:00:17,920
no estoy muy consciente, pero

1754
01:00:17,920 --> 01:00:19,280
creo que lo que es importante entender

1755
01:00:19,280 --> 01:00:21,839
está en el límite de explorar

1756
01:00:21,839 --> 01:00:22,880
todo el entorno,

1757
01:00:22,880 --> 01:00:24,319
su incertidumbre epistémica se reducirá

1758
01:00:24,319 --> 01:00:26,319
a cero como usted, um,

1759
01:00:26,319 --> 01:00:27,760
usted  habrá observado todo y

1760
01:00:27,760 --> 01:00:29,359
luego no está claro qué hará su agente rl

1761
01:00:29,359 --> 01:00:30,480
en ese momento, especialmente usted

1762
01:00:30,480 --> 01:00:31,280
dice que tiene una

1763
01:00:31,280 --> 01:00:33,680
red neuronal profunda que

1764
01:00:33,680 --> 01:00:35,839
parametriza cómo toma acciones

1765
01:00:35,839 --> 01:00:37,440
mientras que en el caso y creo que

1766
01:00:37,440 --> 01:00:39,280
este documento fue realmente interesante

1767
01:00:39,280 --> 01:00:41,119
para mí  para ver, cuando realizamos estos

1768
01:00:41,119 --> 01:00:42,480
experimentos, en

1769
01:00:42,480 --> 01:00:44,319
realidad simplemente pusiste una prioridad sobre tus

1770
01:00:44,319 --> 01:00:46,079
preferencias anteriores y, finalmente, aprendes

1771
01:00:46,079 --> 01:00:46,480


1772
01:00:46,480 --> 01:00:48,960
un modo de comportamiento que puede no ser óptimo,

1773
01:00:48,960 --> 01:00:50,960
pero tu agente finalmente

1774
01:00:50,960 --> 01:00:53,040
aprende a adoptar un comportamiento que

1775
01:00:53,040 --> 01:00:54,480
se cumple a sí mismo porque reduce la

1776
01:00:54,480 --> 01:00:56,000
epidemia.  incertidumbre

1777
01:00:56,000 --> 01:00:58,400
y luego todo lo que queda es decir bueno,

1778
01:00:58,400 --> 01:00:59,520
creo que estos son

1779
01:00:59,520 --> 01:01:00,960
comportamientos útiles o al menos estos son

1780
01:01:00,960 --> 01:01:03,200
comportamientos que tengo que hacer en el mundo  y

1781
01:01:03,200 --> 01:01:04,000
eventualmente

1782
01:01:04,000 --> 01:01:05,680
obtienes un comportamiento bastante repetitivo

1783
01:01:05,680 --> 01:01:07,119
y podría ser un tipo de simulación más precisa

1784
01:01:07,119 --> 01:01:08,160


1785
01:01:08,160 --> 01:01:10,400
en ausencia de cualquier información, como

1786
01:01:10,400 --> 01:01:12,079
cómo algo inteligente podría comportarse realmente

1787
01:01:12,079 --> 01:01:12,559


1788
01:01:12,559 --> 01:01:15,040
en un mundo, mientras que en el paradigma rl

1789
01:01:15,040 --> 01:01:16,480
es menos claro que sabes una vez que reduce

1790
01:01:16,480 --> 01:01:17,599
todo eso  epistémico y ciertamente acerca de

1791
01:01:17,599 --> 01:01:18,480
dónde está el valor

1792
01:01:18,480 --> 01:01:20,160
y no ha encontrado nada parecido a lo

1793
01:01:20,160 --> 01:01:22,319
que realmente está haciendo su agente en ese momento,

1794
01:01:22,319 --> 01:01:24,240
pero creo que mi problema en este momento

1795
01:01:24,240 --> 01:01:26,079
con la acción desde la

1796
01:01:26,079 --> 01:01:28,960
formulación es que esto funciona como una

1797
01:01:28,960 --> 01:01:31,040
formulación de estado discreto, ¿verdad?  vimos

1798
01:01:31,040 --> 01:01:34,640
muy buenos resultados en esa configuración,

1799
01:01:34,640 --> 01:01:36,880
pero creo que si lo

1800
01:01:36,880 --> 01:01:38,240
amplía, los agentes de inferencia activos tendrán los

1801
01:01:38,240 --> 01:01:38,640
mismos

1802
01:01:38,640 --> 01:01:40,559
problemas si está usando una

1803
01:01:40,559 --> 01:01:42,640
inferencia amortizada o algo así para

1804
01:01:42,640 --> 01:01:44,640
aproximar realmente sus

1805
01:01:44,640 --> 01:01:47,359
funciones de transición de probabilidad, lo que significa que

1806
01:01:47,359 --> 01:01:49,280
es posible que no  obtenga estas buenas propiedades

1807
01:01:49,280 --> 01:01:51,119
que estamos viendo en

1808
01:01:51,119 --> 01:01:53,920
esta pequeña escala, por lo que ampliarlo es

1809
01:01:53,920 --> 01:01:55,200
como

1810
01:01:55,200 --> 01:01:57,119
un problema interesante y abierto

1811
01:01:57,119 --> 01:01:58,720
en este  El momento se amplía de la

1812
01:01:58,720 --> 01:01:59,760
manera correcta que puede

1813
01:01:59,760 --> 01:02:01,440
incluir como estos modelos de conjugación

1814
01:02:01,440 --> 01:02:02,880
que tenemos

1815
01:02:02,880 --> 01:02:05,280
en um la formulación de estado discreto y

1816
01:02:05,280 --> 01:02:06,000
así es

1817
01:02:06,000 --> 01:02:08,240
como hacemos el último conjunto de simulaciones

1818
01:02:08,240 --> 01:02:10,079
donde introducimos los conjugados previos

1819
01:02:10,079 --> 01:02:10,880
para hacer el

1820
01:02:10,880 --> 01:02:13,839
aprendizaje sobre el  las preferencias previas,

1821
01:02:13,839 --> 01:02:15,440
pero también la probabilidad,

1822
01:02:15,440 --> 01:02:17,839
por lo que se vuelve un poco confuso

1823
01:02:17,839 --> 01:02:18,720
cómo aprender

1824
01:02:18,720 --> 01:02:21,680
como si tuviera un hiperanterior en una

1825
01:02:21,680 --> 01:02:23,280
red neuronal completa si lo está ampliando de

1826
01:02:23,280 --> 01:02:24,400
esa manera,

1827
01:02:24,400 --> 01:02:27,760
por lo que les debe gustar un poco de trabajo.

1828
01:02:27,760 --> 01:02:29,039
esa área

1829
01:02:29,039 --> 01:02:31,520
y para demostrar realmente que es

1830
01:02:31,520 --> 01:02:32,960
apropiado,

1831
01:02:32,960 --> 01:02:35,680
um, que puede tener estos

1832
01:02:35,680 --> 01:02:37,520
componentes interesantes, entonces debe

1833
01:02:37,520 --> 01:02:40,079
comenzar a pensar en cómo

1834
01:02:40,079 --> 01:02:42,319
incluiría estos hiperprevios,

1835
01:02:42,319 --> 01:02:44,240
eh, porque no es razonable decir

1836
01:02:44,240 --> 01:02:45,920
que tendrá un anterior híbrido

1837
01:02:45,920 --> 01:02:49,119
sobre el espacio de parámetros,

1838
01:02:49,119 --> 01:02:51,760
como el beta hiper, de modo que hiper

1839
01:02:51,760 --> 01:02:53,359
antes sobre gamma que sería

1840
01:02:53,359 --> 01:02:55,359
suficiente en esos ajustes

1841
01:02:55,359 --> 01:02:58,319
um hiper antes de la forma en que el agente está

1842
01:02:58,319 --> 01:03:00,319
seleccionando sus acciones

1843
01:03:00,319 --> 01:03:02,640
tiene que ser sobre los parámetros del modelo

1844
01:03:02,640 --> 01:03:03,440
y si la

1845
01:03:03,440 --> 01:03:06,559
ampliación significa que estás perdiendo esa

1846
01:03:06,559 --> 01:03:08,559
buena manera de desenredar los parámetros particulares del

1847
01:03:08,559 --> 01:03:10,400
modelo, entonces se vuelve muy

1848
01:03:10,400 --> 01:03:13,440


1849
01:03:13,440 --> 01:03:15,039
incierto, no sé, es un

1850
01:03:15,039 --> 01:03:16,559
problema abierto para mí,

1851
01:03:16,559 --> 01:03:18,640
genial, sí, creo.  Los problemas de alta dimensión

1852
01:03:18,640 --> 01:03:19,760


1853
01:03:19,760 --> 01:03:21,599
todavía representan algo que, um, es

1854
01:03:21,599 --> 01:03:23,119
relativamente difícil,

1855
01:03:23,119 --> 01:03:25,760
especialmente debido al tipo de cosas que sabes que

1856
01:03:25,760 --> 01:03:27,359
cuanto más nos alejamos de las bahías

1857
01:03:27,359 --> 01:03:29,680
como, um, menos principio se vuelve

1858
01:03:29,680 --> 01:03:30,640
y, um, sabes

1859
01:03:30,640 --> 01:03:34,960
que es como si fuera una línea muy fina.

1860
01:03:34,960 --> 01:03:37,520
interesante acerca de cómo, ya sea

1861
01:03:37,520 --> 01:03:38,720
dentro del

1862
01:03:38,720 --> 01:03:40,960
paradigma rl o de inferencia activa, hay una

1863
01:03:40,960 --> 01:03:41,839
especie de

1864
01:03:41,839 --> 01:03:44,559
esqueleto disperso, los huesos básicos en el

1865
01:03:44,559 --> 01:03:45,520
núcleo

1866
01:03:45,520 --> 01:03:48,799
y luego, a veces, se necesitan estas otras capas

1867
01:03:48,799 --> 01:03:51,839
o ajustes

1868
01:03:51,839 --> 01:03:55,359
y es bastante interesante aprender sobre ellos

1869
01:03:55,359 --> 01:03:57,200
y también lo que dijiste antes

1870
01:03:57,200 --> 01:04:00,079
sobre cómo el desafío  está planificando

1871
01:04:00,079 --> 01:04:01,839
una acción secuencial

1872
01:04:01,839 --> 01:04:03,920
cuando recibe comentarios, ya sea simplemente

1873
01:04:03,920 --> 01:04:05,440
moviéndose por un entorno para que su

1874
01:04:05,440 --> 01:04:06,720
entorno local cambie o esté

1875
01:04:06,720 --> 01:04:07,839
jugando un juego.  yo

1876
01:04:07,839 --> 01:04:09,440
con un juego de mesa va a cambiar o

1877
01:04:09,440 --> 01:04:10,880
usted está operando en una

1878
01:04:10,880 --> 01:04:13,359
acción secuencial de mercado, no puede simplemente planificar los

1879
01:04:13,359 --> 01:04:15,280
pasos del uno al 100

1880
01:04:15,280 --> 01:04:17,440
sin al menos pensar en

1881
01:04:17,440 --> 01:04:18,720
algunos supuestos

1882
01:04:18,720 --> 01:04:22,240
y luego solo tener acceso a

1883
01:04:22,240 --> 01:04:24,559
datos de observación limitados y planificación en medio

1884
01:04:24,559 --> 01:04:25,520


1885
01:04:25,520 --> 01:04:27,680
incertidumbre fundamental, así que creo que muchos de esos

1886
01:04:27,680 --> 01:04:30,160
puntos de contacto con las motivaciones

1887
01:04:30,160 --> 01:04:32,559
del aprendizaje por refuerzo y el aprendizaje automático

1888
01:04:32,559 --> 01:04:33,680


1889
01:04:33,680 --> 01:04:35,599
tal vez aporten algo más de luz a la

1890
01:04:35,599 --> 01:04:37,440
inferencia activa y

1891
01:04:37,440 --> 01:04:39,920
empujen algunas de esas fronteras que acabas de

1892
01:04:39,920 --> 01:04:40,960
mencionar,

1893
01:04:40,960 --> 01:04:42,720
así que tengo una pregunta y cualquier otra persona en

1894
01:04:42,720 --> 01:04:45,119
el  el chat también puede hacer una pregunta, um

1895
01:04:45,119 --> 01:04:47,680
, digamos que alguien quiere aprender

1896
01:04:47,680 --> 01:04:48,160
sobre

1897
01:04:48,160 --> 01:04:51,280
esto y en realidad tiene

1898
01:04:51,280 --> 01:04:52,960
la perspectiva de la mente de un principiante afortunado

1899
01:04:52,960 --> 01:04:56,079
porque es posible que no se hayan sentido atraídos

1900
01:04:56,079 --> 01:04:57,760
por el aprendizaje por refuerzo, pero

1901
01:04:57,760 --> 01:04:58,960
se han vuelto curiosos

1902
01:04:58,960 --> 01:05:02,000
en la inferencia activa y emocionados por  su

1903
01:05:02,000 --> 01:05:03,280
presentación

1904
01:05:03,280 --> 01:05:05,599
y, por tanto, qué tipo de

1905
01:05:05,599 --> 01:05:06,720
lenguajes informáticos o

1906
01:05:06,720 --> 01:05:08,880
habilidades podrían querer aprender o qué

1907
01:05:08,880 --> 01:05:11,119
tipo de enfoques o mentalidades  e

1908
01:05:11,119 --> 01:05:12,000
útil

1909
01:05:12,000 --> 01:05:13,680
si alguien, digamos, no viniera

1910
01:05:13,680 --> 01:05:15,039
desde una

1911
01:05:15,039 --> 01:05:16,559
perspectiva clásica de aprendizaje automático

1912
01:05:16,559 --> 01:05:18,400
y aprendiera inferencia activa, sino más bien una

1913
01:05:18,400 --> 01:05:21,520
especie de mejora en la inferencia activa,

1914
01:05:21,520 --> 01:05:23,039
¿qué le recomendaría a cualquiera de ustedes?

1915
01:05:23,039 --> 01:05:25,119


1916
01:05:25,119 --> 01:05:27,119


1917
01:05:27,119 --> 01:05:28,480
Creo

1918
01:05:28,480 --> 01:05:30,640
que es una forma interesante de ver a una

1919
01:05:30,640 --> 01:05:32,079
especie de persona potencial

1920
01:05:32,079 --> 01:05:33,280
porque sentí

1921
01:05:33,280 --> 01:05:35,920
que era un poco como esta persona hace

1922
01:05:35,920 --> 01:05:36,799
mucho tiempo cuando,

1923
01:05:36,799 --> 01:05:38,079
como Nora, estaba empezando a tener

1924
01:05:38,079 --> 01:05:39,520
estas conversaciones sobre la inferencia activa,

1925
01:05:39,520 --> 01:05:41,440
um,

1926
01:05:41,440 --> 01:05:44,640
y me gusta la  Este documento

1927
01:05:44,640 --> 01:05:45,680
comenzó básicamente como un

1928
01:05:45,680 --> 01:05:47,680
tutorial que estaba escribiendo después de haber pasado dos

1929
01:05:47,680 --> 01:05:48,960
tres meses

1930
01:05:48,960 --> 01:05:51,359
por las noches leyendo sobre tratar

1931
01:05:51,359 --> 01:05:53,280
de examinar

1932
01:05:53,280 --> 01:05:54,799
el acto de la literatura de inferencia y

1933
01:05:54,799 --> 01:05:57,440
creo, um, sabes, creo que es justo

1934
01:05:57,440 --> 01:05:59,280
decir que a veces es sin disculpas.

1935
01:05:59,280 --> 01:05:59,920
denso

1936
01:05:59,920 --> 01:06:02,480
y bastante difícil de leer

1937
01:06:02,480 --> 01:06:03,760
para que sepas sin tratar de

1938
01:06:03,760 --> 01:06:06,079
auto respaldarte, pero creo que leer este

1939
01:06:06,079 --> 01:06:07,520
manuscrito en particular es todo el

1940
01:06:07,520 --> 01:06:08,079
objetivo

1941
01:06:08,079 --> 01:06:10,000
filosofía cuando nos gusta cuando comenzamos a

1942
01:06:10,000 --> 01:06:11,440
escribir esto fue para

1943
01:06:11,440 --> 01:06:14,640
entender realmente qué es lo que está

1944
01:06:14,640 --> 01:06:16,160
sucediendo aquí, qué es este tipo de

1945
01:06:16,160 --> 01:06:17,760
cantidad de energía libre esperada, por

1946
01:06:17,760 --> 01:06:18,880
qué nos importa,

1947
01:06:18,880 --> 01:06:21,359
así que lo sé desde una especie de

1948
01:06:21,359 --> 01:06:23,359
perspectiva teórica, creo que esto es  una

1949
01:06:23,359 --> 01:06:26,160
presentación muy lúcida del concepto, por lo que al

1950
01:06:26,160 --> 01:06:26,640


1951
01:06:26,640 --> 01:06:28,240
menos puede tener algún tipo de intuición sobre

1952
01:06:28,240 --> 01:06:30,319
lo que está sucediendo, en cuanto al

1953
01:06:30,319 --> 01:06:32,160
tipo de codificación, realmente no puedo hablar

1954
01:06:32,160 --> 01:06:33,760
de eso, estoy seguro, estoy seguro

1955
01:06:33,760 --> 01:06:35,760
ni tiene tiene  funcionó, funciona bastante con él,

1956
01:06:35,760 --> 01:06:36,960


1957
01:06:36,960 --> 01:06:38,960
um, iba a decir que

1958
01:06:38,960 --> 01:06:40,000


1959
01:06:40,000 --> 01:06:41,920
depende del objetivo principal de la persona,

1960
01:06:41,920 --> 01:06:43,440
es para

1961
01:06:43,440 --> 01:06:46,559
obtener una comprensión de

1962
01:06:46,559 --> 01:06:49,760
las ideas conceptuales de alto nivel o

1963
01:06:49,760 --> 01:06:51,359
tratarlo como un algoritmo.  porque si

1964
01:06:51,359 --> 01:06:52,799
vienes de un principio de energía libre, la

1965
01:06:52,799 --> 01:06:54,799
inferencia demasiado activa es una

1966
01:06:54,799 --> 01:06:56,400
historia diferente o si estás

1967
01:06:56,400 --> 01:06:59,440
tomando inferencias activas de

1968
01:06:59,440 --> 01:07:01,839
un tipo específico de toma de decisiones secuencial en silos,

1969
01:07:01,839 --> 01:07:02,799


1970
01:07:02,799 --> 01:07:06,559
eh esquema correcto um, así que dependiendo de eso

1971
01:07:06,559 --> 01:07:06,880
es una

1972
01:07:06,880 --> 01:07:09,599
especie de  diferencia  rs uh, pero como decía phil,

1973
01:07:09,599 --> 01:07:11,440
creo que este documento definitivamente

1974
01:07:11,440 --> 01:07:15,359
es realmente bueno en el sentido de que

1975
01:07:15,359 --> 01:07:16,960
trata de definir todos los diferentes

1976
01:07:16,960 --> 01:07:18,720
conceptos y analiza

1977
01:07:18,720 --> 01:07:21,920
las diferentes formulaciones tal vez no

1978
01:07:21,920 --> 01:07:25,440
con tanto detalle um con las suposiciones

1979
01:07:25,440 --> 01:07:26,559
en juego te da

1980
01:07:26,559 --> 01:07:29,520
la  el diseño de cómo

1981
01:07:29,520 --> 01:07:30,880
podría derivarlo,

1982
01:07:30,880 --> 01:07:34,079
um, pero ciertas cosas como

1983
01:07:34,079 --> 01:07:37,200
qué implica realmente la densidad aproximada,

1984
01:07:37,200 --> 01:07:40,319
esas son preguntas muy difíciles

1985
01:07:40,319 --> 01:07:41,200


1986
01:07:41,200 --> 01:07:43,440
en las que tendría que sumergirse en la

1987
01:07:43,440 --> 01:07:44,880
literatura de inferencia variacional para

1988
01:07:44,880 --> 01:07:46,000
comprender, así

1989
01:07:46,000 --> 01:07:47,839
que supongo que alguien desde esa perspectiva

1990
01:07:47,839 --> 01:07:49,920
quien ingresa al campo

1991
01:07:49,920 --> 01:07:51,440
debería dedicar un tiempo a pensar en la

1992
01:07:51,440 --> 01:07:53,280
inferencia variacional y cómo eso

1993
01:07:53,280 --> 01:07:53,760
realmente se

1994
01:07:53,760 --> 01:07:55,839
relaciona con la formulación del acto de influencia

1995
01:07:55,839 --> 01:07:56,960


1996
01:07:56,960 --> 01:07:59,119
porque la parte de percepción de la

1997
01:07:59,119 --> 01:08:00,559
inferencia activa es

1998
01:08:00,559 --> 01:08:03,359
en la mayoría de los casos exactamente igual que la

1999
01:08:03,359 --> 01:08:04,160


2000
01:08:04,160 --> 01:08:07,119
literatura de inferencia variacional y la optimización de

2001
01:08:07,119 --> 01:08:08,640
la evidencia del modelo.

2002
01:08:08,640 --> 01:08:12,240
eh o la maximización de la ley de evidencia

2003
01:08:12,240 --> 01:08:13,200
abundan

2004
01:08:13,200 --> 01:08:15,599
um esta es la segunda cosa que

2005
01:08:15,599 --> 01:08:16,399
iba a hacer  agregar

2006
01:08:16,399 --> 01:08:19,679
es um que el artículo

2007
01:08:19,679 --> 01:08:22,319
con lancelot de costa es realmente bueno

2008
01:08:22,319 --> 01:08:24,158
para alguien que quiere profundizar

2009
01:08:24,158 --> 01:08:27,198
en derivar todo por sí mismo, a

2010
01:08:27,198 --> 01:08:30,719
veces es súper técnico, así que

2011
01:08:30,719 --> 01:08:31,198
este

2012
01:08:31,198 --> 01:08:33,279
arte es el artículo que pasé

2013
01:08:33,279 --> 01:08:35,359
hoy, el de phil, tom y

2014
01:08:35,359 --> 01:08:36,080
carl

2015
01:08:36,080 --> 01:08:38,238
es  es una buena introducción para alguien

2016
01:08:38,238 --> 01:08:40,080
que no está familiarizado con las matemáticas

2017
01:08:40,080 --> 01:08:41,198
y solo quiere

2018
01:08:41,198 --> 01:08:44,399
obtener un resumen sencillo, eh, mientras que este es

2019
01:08:44,399 --> 01:08:46,560
el artículo en el que el

2020
01:08:46,560 --> 01:08:48,080
primer autor de Lance da

2021
01:08:48,080 --> 01:08:50,960
derivaciones detalladas y muchas de las

2022
01:08:50,960 --> 01:08:51,759
suposiciones

2023
01:08:51,759 --> 01:08:53,679
en su lugar, por lo que es de comprensión similar.

2024
01:08:53,679 --> 01:08:55,439
la parte de la teoría,

2025
01:08:55,439 --> 01:08:57,759
desde la perspectiva de la codificación,

2026
01:08:57,759 --> 01:08:59,759
depende completamente de cuál sea el objetivo final,

2027
01:08:59,759 --> 01:09:00,640
por lo que si quieren que

2028
01:09:00,640 --> 01:09:03,198
alguien quiera trabajar con

2029
01:09:03,198 --> 01:09:04,479
formulaciones de estado discreto,

2030
01:09:04,479 --> 01:09:07,679
entonces el código matlab que carl ha escrito

2031
01:09:07,679 --> 01:09:10,719
y son años de trabajo con muchas

2032
01:09:10,719 --> 01:09:13,600
simulaciones y ejemplos agradables que

2033
01:09:13,600 --> 01:09:14,319
usted  puede usar

2034
01:09:14,319 --> 01:09:17,439
y también nuestro código está en línea y puede

2035
01:09:17,439 --> 01:09:18,479
acceder a él,

2036
01:09:18,479 --> 01:09:21,679
por lo que hay un enlace en el papel um en

2037
01:09:21,679 --> 01:09:24,880
la sección de software um que da  dónde está

2038
01:09:24,880 --> 01:09:25,679
exactamente

2039
01:09:25,679 --> 01:09:27,040
el código y puede revisarlo

2040
01:09:27,040 --> 01:09:29,040
y ver cómo se realizaron las simulaciones

2041
01:09:29,040 --> 01:09:29,920


2042
01:09:29,920 --> 01:09:33,120
si alguien está interesado en um

2043
01:09:33,120 --> 01:09:36,719
más supongo um

2044
01:09:36,719 --> 01:09:39,120
formulaciones de alta dimensión de inferencia activa que

2045
01:09:39,120 --> 01:09:42,799
um hay algún trabajo reciente con eh

2046
01:09:42,799 --> 01:09:46,399
zephyr zephyrus um tan zeff fontes

2047
01:09:46,399 --> 01:09:48,799
como  primer autor, eh, donde tenemos una

2048
01:09:48,799 --> 01:09:50,080
buena otra vez, también tenemos un

2049
01:09:50,080 --> 01:09:52,479
git reaper para ese trabajo que,

2050
01:09:52,479 --> 01:09:53,198
eh,

2051
01:09:53,198 --> 01:09:55,920
da un desglose de cómo

2052
01:09:55,920 --> 01:09:57,360
implementaría un

2053
01:09:57,360 --> 01:09:59,600
agente de inferencia activo simple usando

2054
01:09:59,600 --> 01:10:01,280
codificadores muy cortos y una

2055
01:10:01,280 --> 01:10:03,679
red de transición simple, así que hay muchos  de

2056
01:10:03,679 --> 01:10:04,719


2057
01:10:04,719 --> 01:10:06,400
áreas diferentes similares, pero para alguien que comienza,

2058
01:10:06,400 --> 01:10:07,679
tiene que entender si

2059
01:10:07,679 --> 01:10:08,800
quiere enfocarse en el

2060
01:10:08,800 --> 01:10:10,159
lado teórico o si quiere

2061
01:10:10,159 --> 01:10:11,840
enfocarse en el lado de implementación, el

2062
01:10:11,840 --> 01:10:13,199
lado teórico

2063
01:10:13,199 --> 01:10:14,640
sería profundizar en la

2064
01:10:14,640 --> 01:10:16,000
influencia variacional y las matemáticas

2065
01:10:16,000 --> 01:10:17,920
detrás de esto y  si quieren centrarse en

2066
01:10:17,920 --> 01:10:18,239
el

2067
01:10:18,239 --> 01:10:19,600
aspecto de la codificación, entonces quieren

2068
01:10:19,600 --> 01:10:21,760
averiguar si se trata de

2069
01:10:21,760 --> 01:10:23,199
formulaciones de estado continuo o

2070
01:10:23,199 --> 01:10:25,199
discreto.

2071
01:10:25,199 --> 01:10:27,679
Si es

2072
01:10:27,679 --> 01:10:29,040
continuo, la

2073
01:10:29,040 --> 01:10:32,880
mayor parte estaría escribiendo

2074
01:10:32,880 --> 01:10:34,800
las coordenadas de los movimientos en sí mismos

2075
01:10:34,800 --> 01:10:36,960
o tendrían que usar algún tipo

2076
01:10:36,960 --> 01:10:40,640
de red neuronal para

2077
01:10:40,640 --> 01:10:42,480
aproximar esa distribución continua

2078
01:10:42,480 --> 01:10:43,280
de interés.

2079
01:10:43,280 --> 01:10:45,520
o el estado discreto que carl

2080
01:10:45,520 --> 01:10:46,400
escribió

2081
01:10:46,400 --> 01:10:49,199
um y si tienen preguntas sobre

2082
01:10:49,199 --> 01:10:50,480
el estado discreto

2083
01:10:50,480 --> 01:10:53,679
um estoy feliz de recibir correos electrónicos también

2084
01:10:53,679 --> 01:10:54,080
así que

2085
01:10:54,080 --> 01:10:57,040
si alguien tiene o el estado continuo

2086
01:10:57,040 --> 01:10:57,840


2087
01:10:57,840 --> 01:11:01,679
estado um espacio de estado también gracias por el

2088
01:11:01,679 --> 01:11:04,960
distinción y um, es una diferencia tan grande

2089
01:11:04,960 --> 01:11:06,159
entre el

2090
01:11:06,159 --> 01:11:09,040
código de matlab por el que pasamos con

2091
01:11:09,040 --> 01:11:09,679


2092
01:11:09,679 --> 01:11:12,159
ryan smith y christopher white, que

2093
01:11:12,159 --> 01:11:13,120
está haciendo

2094
01:11:13,120 --> 01:11:16,000
la multiplicación de matrices y demás, y luego

2095
01:11:16,000 --> 01:11:17,920
aquí vienen las redes neuronales

2096
01:11:17,920 --> 01:11:20,880
y ofrece sonidos como nuevas

2097
01:11:20,880 --> 01:11:23,120
oportunidades con alta dimensionalidad

2098
01:11:23,120 --> 01:11:26,159
y continuo  variables, pero también muchos

2099
01:11:26,159 --> 01:11:26,880


2100
01:11:26,880 --> 01:11:30,320
desafíos nuevos, entonces, ¿cuál es

2101
01:11:30,320 --> 01:11:33,360
la esencia compartida

2102
01:11:33,360 --> 01:11:37,440
por la forma matricial y por este estilo más de

2103
01:11:37,440 --> 01:11:40,480
aprendizaje automático

2104
01:11:40,480 --> 01:11:43,040
porque  e para algunas personas, um, podría ser

2105
01:11:43,040 --> 01:11:43,920
dividir un cabello,

2106
01:11:43,920 --> 01:11:46,480
literalmente, la diferencia entre

2107
01:11:46,480 --> 01:11:47,840
dos lenguajes informáticos

2108
01:11:47,840 --> 01:11:49,120
cuando están pensando en la

2109
01:11:49,120 --> 01:11:51,760
inferencia activa desde una

2110
01:11:51,760 --> 01:11:54,800
psicología ecológica o en una perspectiva

2111
01:11:54,800 --> 01:11:57,600
filosófica activa o de rendimiento incorporado

2112
01:11:57,600 --> 01:11:58,320
,

2113
01:11:58,320 --> 01:12:01,600
todos los antecedentes que convergen en la

2114
01:12:01,600 --> 01:12:02,880
inferencia activa y así

2115
01:12:02,880 --> 01:12:05,440
para alguien fuera del mechón de

2116
01:12:05,440 --> 01:12:06,800
cabello,

2117
01:12:06,800 --> 01:12:09,199
¿qué es lo que realmente podemos destilar?

2118
01:12:09,199 --> 01:12:09,920
Esa es

2119
01:12:09,920 --> 01:12:11,679
la inferencia activa central y escribí

2120
01:12:11,679 --> 01:12:13,440
algunas cosas para

2121
01:12:13,440 --> 01:12:14,800
que dijeras cuáles son esas

2122
01:12:14,800 --> 01:12:16,480
piezas centrales que nos permiten

2123
01:12:16,480 --> 01:12:19,120
sumergirnos en el modo matriz con matlab o

2124
01:12:19,120 --> 01:12:20,840
en el modo de red neuronal con como

2125
01:12:20,840 --> 01:12:22,960
python

2126
01:12:22,960 --> 01:12:24,640
um, así que creo que todo se reduce a mi

2127
01:12:24,640 --> 01:12:26,640
resumen, deslice la forma en que lo pienso, por

2128
01:12:26,640 --> 01:12:28,159
lo que la inferencia activa, los

2129
01:12:28,159 --> 01:12:29,679
ingredientes centrales están formulando el

2130
01:12:29,679 --> 01:12:30,960
modelo de género

2131
01:12:30,960 --> 01:12:32,480
y aquí formular el modelo general

2132
01:12:32,480 --> 01:12:33,840
es la parametrización del modelo general

2133
01:12:33,840 --> 01:12:35,520
entonces puede usar las

2134
01:12:35,520 --> 01:12:37,760
distribuciones categóricas de etapa discreta

2135
01:12:37,760 --> 01:12:38,719
o puede usar

2136
01:12:38,719 --> 01:12:40,560
una formulación de estado más continua y

2137
01:12:40,560 --> 01:12:42,719
nuevamente

2138
01:12:42,719 --> 01:12:44,560
la formulación de la red neuronal es una

2139
01:12:44,560 --> 01:12:46,400
instancia específica de eso,

2140
01:12:46,400 --> 01:12:49,679
por lo que sería una forma de

2141
01:12:49,679 --> 01:12:50,480
diferenciar

2142
01:12:50,480 --> 01:12:53,679
que la segunda es la

2143
01:12:53,679 --> 01:12:56,239
optimización de las funciones objetivas

2144
01:12:56,239 --> 01:12:57,440
en juego,

2145
01:12:57,440 --> 01:13:01,840
por lo que en el código de matlab

2146
01:13:01,840 --> 01:13:04,239
estamos haciendo gradientes para enviar usando un

2147
01:13:04,239 --> 01:13:06,400
mensaje de campo medio  algoritmo de paso,

2148
01:13:06,400 --> 01:13:07,760
una formulación tan específica que se

2149
01:13:07,760 --> 01:13:09,679
introdujo en un par de artículos

2150
01:13:09,679 --> 01:13:11,440
y específicamente no pasé por

2151
01:13:11,440 --> 01:13:12,880
eso o,

2152
01:13:12,880 --> 01:13:16,000
eh, estás haciendo retropropagación para

2153
01:13:16,000 --> 01:13:17,280


2154
01:13:17,280 --> 01:13:20,320
calcular realmente las distribuciones o lambda

2155
01:13:20,320 --> 01:13:21,760
y luego solo estás

2156
01:13:21,760 --> 01:13:23,840
resolviendo esas distribuciones que tienes.

2157
01:13:23,840 --> 01:13:26,560
en cierto modo depende de

2158
01:13:26,560 --> 01:13:29,840
la formulación a la que

2159
01:13:29,840 --> 01:13:32,400
te parezcas, depende de cómo optimices

2160
01:13:32,400 --> 01:13:33,840
esos objetivos, ya sea que estés

2161
01:13:33,840 --> 01:13:35,600
tomando el modelo implícito hacia adelante

2162
01:13:35,600 --> 01:13:37,600
o estés tomando un

2163
01:13:37,600 --> 01:13:39,199
modelo de género explícito, um

2164
01:13:39,199 --> 01:13:41,360
y una cosa que olvidé mencionar es

2165
01:13:41,360 --> 01:13:42,480
que  um

2166
01:13:42,480 --> 01:13:44,480
alex shams y connor hines han estado

2167
01:13:44,480 --> 01:13:47,520
trabajando en una formulación de estado discreto

2168
01:13:47,520 --> 01:13:50,880
de um inferencia activa en python

2169
01:13:50,880 --> 01:13:53,360
que podría ser de interés para las personas

2170
01:13:53,360 --> 01:13:55,280
que  Me gustaría centrarme en un lenguaje específico

2171
01:13:55,280 --> 01:13:56,239


2172
01:13:56,239 --> 01:13:58,960
que pueda funcionar como cosas más sofisticadas o de

2173
01:13:58,960 --> 01:14:00,640
alta dimensión

2174
01:14:00,640 --> 01:14:02,719
y la formulación de estado discreto que

2175
01:14:02,719 --> 01:14:03,920
tiene carl, um,

2176
01:14:03,920 --> 01:14:05,760
sé que también están buscando personas

2177
01:14:05,760 --> 01:14:08,719
para trabajar en la base del código si alguien está

2178
01:14:08,719 --> 01:14:09,840
interesado, así se llama.

2179
01:14:09,840 --> 01:14:13,520
inferir actividad, creo, pero eso también está en

2180
01:14:13,520 --> 01:14:17,760
github, si alguien está interesado,

2181
01:14:17,840 --> 01:14:21,760
genial, cualquier tipo de último

2182
01:14:21,760 --> 01:14:25,280
pensamiento o comentario de cualquiera de ustedes,

2183
01:14:27,440 --> 01:14:30,640
um, no, creo que estoy bien,

2184
01:14:30,640 --> 01:14:33,440
entonces, ¿qué crees?

2185
01:14:34,880 --> 01:14:37,199


2186
01:14:38,560 --> 01:14:41,360
un poco,

2187
01:14:41,360 --> 01:14:42,239


2188
01:14:42,239 --> 01:14:45,679
um, un aumento en el interés en la inferencia activa,

2189
01:14:45,679 --> 01:14:46,640


2190
01:14:46,640 --> 01:14:48,560
solo para darle un ejemplo, como quizás

2191
01:14:48,560 --> 01:14:49,840
áreas donde

2192
01:14:49,840 --> 01:14:51,040
incluso la inferencia activa se habría

2193
01:14:51,040 --> 01:14:53,840
considerado antes, como la robótica,

2194
01:14:53,840 --> 01:14:55,360
um, ahora está comenzando a ver más y

2195
01:14:55,360 --> 01:14:57,360
más, así que creo que si  quiero

2196
01:14:57,360 --> 01:14:58,960
involucrarme en esto ahora es un

2197
01:14:58,960 --> 01:15:01,199
buen momento

2198
01:15:01,199 --> 01:15:04,400
excelente llama a philip y volveré

2199
01:15:04,400 --> 01:15:07,199
a recomendar el excelente artículo

2200
01:15:07,199 --> 01:15:08,880
que estamos discutiendo está en la

2201
01:15:08,880 --> 01:15:11,120
descripción de este video realmente

2202
01:15:11,120 --> 01:15:12,719
agradezco a ambos

2203
01:15:12,719 --> 01:15:15,040
por unirse  siempre es bienvenido a

2204
01:15:15,040 --> 01:15:16,480


2205
01:15:16,480 --> 01:15:18,239
hablar sobre un artículo que haya escrito

2206
01:15:18,239 --> 01:15:19,520
o no, pero

2207
01:15:19,520 --> 01:15:21,679
nuevamente muchas gracias a nora y

2208
01:15:21,679 --> 01:15:23,679
phillip y espero verlos nuevamente en una

2209
01:15:23,679 --> 01:15:24,239
futura

2210
01:15:24,239 --> 01:15:27,679
transmisión de inferencia activa gracias

2211
01:15:27,679 --> 01:15:30,159
gracias por invitarnos  adiós, nos

2212
01:15:30,159 --> 01:15:31,440
vemos, daniel,

2213
01:15:31,440 --> 01:15:34,000
paz, adiós,

2214
01:15:36,840 --> 01:15:39,280
detén la transmisión, gran

2215
01:15:39,280 --> 01:15:41,040
conversación, muchas gracias a philip y

2216
01:15:41,040 --> 01:15:43,840
noor, de verdad.


1
00:00:07,839 --> 00:00:08,400


2
00:00:08,400 --> 00:00:11,120
大家好，欢迎大家来到活动

3
00:00:11,120 --> 00:00:12,160
推理实验室，

4
00:00:12,160 --> 00:00:15,759
这是

5
00:00:15,759 --> 00:00:18,960
2021 年 4 月 16 日的模型流编号 2.1

6
00:00:18,960 --> 00:00:20,720
，今天将是一个很棒的

7
00:00:20,720 --> 00:00:22,400
模型流，我们将

8
00:00:22,400 --> 00:00:24,400
简要介绍一下

9
00:00:24,400 --> 00:00:25,920
自己，然后

10
00:00:25,920 --> 00:00:27,920
我会提到如何 会议

11
00:00:27,920 --> 00:00:30,000
将在今天举行，然后我们会将其传递给

12
00:00:30,000 --> 00:00:33,200
noor 进行演示，所以我是 daniel

13
00:00:33,200 --> 00:00:35,120
，我是加利福尼亚的博士后研究员，

14
00:00:35,120 --> 00:00:36,719
我将传递给

15
00:00:36,719 --> 00:00:39,040
philip

16
00:00:40,000 --> 00:00:42,800
嗨，是的，我目前是牛津大学的博士生

17
00:00:42,800 --> 00:00:43,280


18
00:00:43,280 --> 00:00:46,719
我的第二年，是的，我想这

19
00:00:46,719 --> 00:00:48,320
是我在开始

20
00:00:48,320 --> 00:00:49,440
攻读博士学位之前

21
00:00:49,440 --> 00:00:52,640


22
00:00:52,640 --> 00:00:53,600
所做

23
00:00:53,600 --> 00:00:55,600


24
00:00:55,600 --> 00:00:57,280


25
00:00:57,280 --> 00:00:58,960


26
00:00:58,960 --> 00:01:00,879


27
00:01:00,879 --> 00:01:03,520
的工作 对解决这类

28
00:01:03,520 --> 00:01:06,239
研究

29
00:01:06,240 --> 00:01:09,360


30
00:01:09,360 --> 00:01:12,000


31
00:01:12,000 --> 00:01:13,840
问题很有用

32
00:01:13,840 --> 00:01:15,200


33
00:01:15,200 --> 00:01:16,159


34
00:01:16,159 --> 00:01:18,799


35
00:01:18,799 --> 00:01:19,920


36
00:01:19,920 --> 00:01:22,720
嗯，所以我在汽车监督下的博士专注

37
00:01:22,720 --> 00:01:23,920
于

38
00:01:23,920 --> 00:01:26,000
与适应有关的这些想法，

39
00:01:26,000 --> 00:01:27,759
我今天将重点关注其中之一，即使用主动推理

40
00:01:27,759 --> 00:01:29,759
在非平稳环境中进行行为适应，

41
00:01:29,759 --> 00:01:32,000


42
00:01:32,000 --> 00:01:35,119
非常感谢您的

43
00:01:35,119 --> 00:01:38,159
加入和我们的演示文稿 我将

44
00:01:38,159 --> 00:01:38,720


45
00:01:38,720 --> 00:01:41,280
听到一个谁知道

46
00:01:41,280 --> 00:01:42,720
nor 的演示文稿

47
00:01:42,720 --> 00:01:44,399
，然后我

48
00:01:44,399 --> 00:01:45,759
将从聊天中编译问题，

49
00:01:45,759 --> 00:01:48,240
所以请在他们

50
00:01:48,240 --> 00:01:48,880
来找你时输入问题

51
00:01:48,880 --> 00:01:51,119
，然后我们将在最后解决它们，

52
00:01:51,119 --> 00:01:53,200
再次感谢和 nor 请

53
00:01:53,200 --> 00:01:56,479
把它拿走完美谢谢你，

54
00:01:56,479 --> 00:01:58,880
嗯，所以今天我将展示一些

55
00:01:58,880 --> 00:01:59,520


56
00:01:59,520 --> 00:02:01,600
我与菲利普合作

57
00:02:01,600 --> 00:02:03,119


58
00:02:03,119 --> 00:02:06,399


59
00:02:06,399 --> 00:02:08,800


60
00:02:08,800 --> 00:02:12,720


61
00:02:13,599 --> 00:02:15,440
完成的工作 好的，所以

62
00:02:15,440 --> 00:02:17,440
演示文稿的结构如下

63
00:02:17,440 --> 00:02:20,480
，你

64
00:02:20,480 --> 00:02:22,160


65
00:02:22,160 --> 00:02:23,840


66
00:02:23,840 --> 00:02:26,000
能听到屏幕吗？

67
00:02:26,000 --> 00:02:28,640


68
00:02:29,920 --> 00:02:34,480
我告诉

69
00:02:34,480 --> 00:02:36,800
你我们走了，我会剪掉它，所以去吧，

70
00:02:36,800 --> 00:02:38,720
谢谢

71
00:02:38,720 --> 00:02:40,879
完美谢谢你，所以演示文稿的

72
00:02:40,879 --> 00:02:42,239
结构以及

73
00:02:42,239 --> 00:02:44,080
首先我将简要介绍问题

74
00:02:44,080 --> 00:02:45,280
设置并提供今天正在考虑

75
00:02:45,280 --> 00:02:47,360
的特定主动推理

76
00:02:47,360 --> 00:02:49,920
实例化的详细信息

77
00:02:49,920 --> 00:02:51,519
，即 离散状态空间

78
00:02:51,519 --> 00:02:52,319
设置

79
00:02:52,319 --> 00:02:54,160
和演示文稿的后半部分

80
00:02:54,160 --> 00:02:55,599
将集中在一些

81
00:02:55,599 --> 00:02:57,440
特定示例上，

82
00:02:57,440 --> 00:02:59,360
将主动影响公式与

83
00:02:59,360 --> 00:03:00,720
强化学习进行比较，

84
00:03:00,720 --> 00:03:03,599
特别是 q 学习和

85
00:03:03,599 --> 00:03:04,959
基于贝叶斯模型的

86
00:03:04,959 --> 00:03:07,519
算法，然后我要做的

87
00:03:07,519 --> 00:03:08,959
是提供一些面子

88
00:03:08,959 --> 00:03:10,640
为什么你甚至想要使用主动推理的特定方面的有效性

89
00:03:10,640 --> 00:03:14,080


90
00:03:14,080 --> 00:03:18,080
好吧，嗯，什么是主动推理，

91
00:03:18,080 --> 00:03:20,480
嗯，这是

92
00:03:20,480 --> 00:03:21,599
关于生物

93
00:03:21,599 --> 00:03:23,920
或人工代理如何在

94
00:03:23,920 --> 00:03:26,239
动态非静止环境中运行的首要原则，

95
00:03:26,239 --> 00:03:28,560
它规定这些代理

96
00:03:28,560 --> 00:03:30,000
为了维持 体内平衡

97
00:03:30,000 --> 00:03:31,680
存在于吸引状态，

98
00:03:31,680 --> 00:03:34,560
这些状态最小化它们的熵或惊喜，

99
00:03:34,560 --> 00:03:36,239
所以如果你采取 这个特殊的例子

100
00:03:36,239 --> 00:03:38,000
，我们看到这个过去的

101
00:03:38,000 --> 00:03:41,280
小饥饿代理打开冰箱

102
00:03:41,280 --> 00:03:42,879
的工作方式就像你

103
00:03:42,879 --> 00:03:44,959
需要知道为什么要正确打开冰箱，

104
00:03:44,959 --> 00:03:46,480
所以你想

105
00:03:46,480 --> 00:03:47,280


106
00:03:47,280 --> 00:03:50,080
在家里或外面吃饭之间做出一个特别的选择，

107
00:03:50,080 --> 00:03:51,120
为了做到这一点，您

108
00:03:51,120 --> 00:03:53,200
必须决定什么是最佳

109
00:03:53,200 --> 00:03:55,040
行动，可以让您解决

110
00:03:55,040 --> 00:03:56,959
自己对当前

111
00:03:56,959 --> 00:03:58,560
事务阶段的不确定性

112
00:03:58,560 --> 00:04:00,799
，嗯，这将帮助您

113
00:04:00,799 --> 00:04:02,480
决定是要在家做饭

114
00:04:02,480 --> 00:04:04,400
还是要走路 到餐厅

115
00:04:04,400 --> 00:04:06,720
和这个特定的实例，这

116
00:04:06,720 --> 00:04:08,560
导致代理打开冰箱以

117
00:04:08,560 --> 00:04:11,760
检查家里是否有食物，

118
00:04:11,760 --> 00:04:13,519
而主动推理的好处

119
00:04:13,519 --> 00:04:15,519
在于它允许您

120
00:04:15,519 --> 00:04:16,959


121
00:04:16,959 --> 00:04:19,358
通过指定以下方式以更正式的方式思考这些问题设置

122
00:04:19,358 --> 00:04:20,639
最佳行为

123
00:04:20,639 --> 00:04:23,199
是通过评估证据

124
00:04:23,199 --> 00:04:24,639
来

125
00:04:24,639 --> 00:04:26,639


126
00:04:26,639 --> 00:04:28,080


127
00:04:28,080 --> 00:04:30,400


128
00:04:30,400 --> 00:04:31,520


129
00:04:31,520 --> 00:04:33,360
确定的 我们要做的只是专注于支持

130
00:04:33,360 --> 00:04:35,280
主动推理的过程理论，

131
00:04:35,280 --> 00:04:37,199
而不是

132
00:04:37,199 --> 00:04:38,240
通过生物学来

133
00:04:38,240 --> 00:04:41,199
讨论主动

134
00:04:41,199 --> 00:04:44,240
推理消息传递方案的

135
00:04:44,240 --> 00:04:47,280
神经合理性

136
00:04:47,280 --> 00:04:48,479


137
00:04:48,479 --> 00:04:50,560
与

138
00:04:50,560 --> 00:04:52,000
一般的强化学习

139
00:04:52,000 --> 00:04:52,800
算法相比，

140
00:04:52,800 --> 00:04:54,720
我们首先需要

141
00:04:54,720 --> 00:04:56,080
理解的是，通过主动

142
00:04:56,080 --> 00:04:57,759
推理，有对

143
00:04:57,759 --> 00:04:59,600
纯信念方案的承诺，

144
00:04:59,600 --> 00:05:01,600
这意味着奖励函数

145
00:05:01,600 --> 00:05:03,520
不一定总是

146
00:05:03,520 --> 00:05:06,000
不必要的，

147
00:05:06,000 --> 00:05:07,360
因为您将拥有的任何策略

148
00:05:07,360 --> 00:05:10,240
都有 认知价值即使在没有偏好的情况下

149
00:05:10,240 --> 00:05:11,680
，

150
00:05:11,680 --> 00:05:14,000
额外的主动推理代理

151
00:05:14,000 --> 00:05:16,400
也可以学习自己的奖励功能

152
00:05:16,400 --> 00:05:19,120
，这有助于代理描述

153
00:05:19,120 --> 00:05:20,400


154
00:05:20,400 --> 00:05:23,199
它期望自己看到的行为类型，

155
00:05:23,199 --> 00:05:25,120
而不是它会

156
00:05:25,120 --> 00:05:28,000
从环境中得到的东西，这些

157
00:05:28,000 --> 00:05:29,600


158
00:05:29,600 --> 00:05:30,800
与强化学习相比，有

159
00:05:30,800 --> 00:05:33,039
两点非常重要 rning，因为在

160
00:05:33,039 --> 00:05:35,680
标准 rl 设置下，奖励函数

161
00:05:35,680 --> 00:05:38,160
uh 将定义代理

162
00:05:38,160 --> 00:05:39,360


163
00:05:39,360 --> 00:05:41,520
在特定环境设置中如何交互或行为，

164
00:05:41,520 --> 00:05:42,560
但是

165
00:05:42,560 --> 00:05:44,080
首先定义奖励

166
00:05:44,080 --> 00:05:46,000
函数非常困难，因为

167
00:05:46,000 --> 00:05:48,160
它假设有一个特定的

168
00:05:48,160 --> 00:05:49,520
信号

169
00:05:49,520 --> 00:05:51,919
来自 环境对代理来说可能是

170
00:05:51,919 --> 00:05:53,600
一致的好或坏

171
00:05:53,600 --> 00:05:56,160
，

172
00:05:56,160 --> 00:05:56,800


173
00:05:56,800 --> 00:06:00,400
在真实环境中不一定会成立，这些

174
00:06:00,400 --> 00:06:02,319
环境信号可能会根据环境而变化

175
00:06:02,319 --> 00:06:03,919


176
00:06:03,919 --> 00:06:05,680


177
00:06:05,680 --> 00:06:07,280


178
00:06:07,280 --> 00:06:09,919
这可能会让你变得更糟，

179
00:06:09,919 --> 00:06:11,199
这就是为什么首先

180
00:06:11,199 --> 00:06:12,800
构建这些奖励

181
00:06:12,800 --> 00:06:15,199
函数非常困难

182
00:06:15,199 --> 00:06:16,880
，如果你没有以

183
00:06:16,880 --> 00:06:19,360
适当的方式构建它们，即使使用 rl 设置，

184
00:06:19,360 --> 00:06:21,680
它可能会导致

185
00:06:21,680 --> 00:06:24,080
你的代理的次

186
00:06:24,080 --> 00:06:26,000
优行为

187
00:06:26,000 --> 00:06:27,520
从某种意义上说，主动推理确实很好，因为我们实际上是在

188
00:06:27,520 --> 00:06:29,680
替换或绕过

189
00:06:29,680 --> 00:06:31,360
传统的奖励函数 你

190
00:06:31,360 --> 00:06:31,919
会

191
00:06:31,919 --> 00:06:34,400
在 rl 环境中对首选结果有先验信念

192
00:06:34,400 --> 00:06:34,960
，

193
00:06:34,960 --> 00:06:36,880
所以

194
00:06:36,880 --> 00:06:39,039
你希望看到自己所处的那种期望的事态

195
00:06:39,039 --> 00:06:40,560


196
00:06:40,560 --> 00:06:43,840
，这

197
00:06:43,840 --> 00:06:47,039
在没有奖励或

198
00:06:47,039 --> 00:06:48,400


199
00:06:48,400 --> 00:06:50,479
对奖励或

200
00:06:50,479 --> 00:06:52,639
偏好的理解真的不准确的环境中变得很重要 设置应该看起来像

201
00:06:52,639 --> 00:06:55,360
，在这种情况下，在标准的

202
00:06:55,360 --> 00:06:56,319
主动影响

203
00:06:56,319 --> 00:06:58,160
离散状态公式中，我们可以

204
00:06:58,160 --> 00:07:00,319
做的是学习这些首选结果的经验先验

205
00:07:00,319 --> 00:07:01,599
分布

206
00:07:01,599 --> 00:07:05,280
嗯和内在的呃

207
00:07:05,280 --> 00:07:07,199
对不起那个代理的内部奖励函数

208
00:07:07,199 --> 00:07:07,599


209
00:07:07,599 --> 00:07:10,400
，这种让我喜欢

210
00:07:10,400 --> 00:07:11,120


211
00:07:11,120 --> 00:07:14,479


212
00:07:14,479 --> 00:07:17,919
rl 设置和主动推理之间的第一个不同的 um 概念化，因为

213
00:07:17,919 --> 00:07:20,639
在主动推理中奖励

214
00:07:20,639 --> 00:07:22,560
没有什么不同，它们只是

215
00:07:22,560 --> 00:07:24,560
代理

216
00:07:24,560 --> 00:07:26,800
从环境中获得的标准观察，而在

217
00:07:26,800 --> 00:07:27,840
rl 中，它们非常

218
00:07:27,840 --> 00:07:32,160
有必要采取适当的行动

219
00:07:32,160 --> 00:07:35,759
um 我要

220
00:07:35,759 --> 00:07:38,960
学习第二点，我们想要

221
00:07:38,960 --> 00:07:41,280
道歉，我们想要 要做的是，积极

222
00:07:41,280 --> 00:07:43,039
影响提供

223
00:07:43,039 --> 00:07:44,080
了认知

224
00:07:44,080 --> 00:07:46,160
探索和内在动机的主要说明，以

225
00:07:46,160 --> 00:07:48,160
最小化不确定性，

226
00:07:48,160 --> 00:07:50,080
并且在 rl 设置中这也是

227
00:07:50,080 --> 00:07:51,440
非常关键的，因为

228
00:07:51,440 --> 00:07:53,759


229
00:07:53,759 --> 00:07:54,479


230
00:07:54,479 --> 00:07:57,759
我们在 nrl 中看到的许多新算法的整个前提是尝试和

231
00:07:57,759 --> 00:07:58,000


232
00:07:58,000 --> 00:07:59,680
为探索和探索之间的平衡找到正确的权衡，

233
00:07:59,680 --> 00:08:02,000
那么

234
00:08:02,000 --> 00:08:03,919


235
00:08:03,919 --> 00:08:05,599


236
00:08:05,599 --> 00:08:06,400


237
00:08:06,400 --> 00:08:08,240
如果我继续选择

238
00:08:08,240 --> 00:08:10,160
它从未接触过的所有不同的冰淇淋口味，代理应该在

239
00:08:10,160 --> 00:08:10,800


240
00:08:10,800 --> 00:08:13,520
给定的时间点采取哪些正确的行动 嗯，像芥末

241
00:08:13,520 --> 00:08:14,479
等，

242
00:08:14,479 --> 00:08:16,879
还是应该一直用这个哦，抱歉，

243
00:08:16,879 --> 00:08:17,840


244
00:08:17,840 --> 00:08:20,000
冰淇淋的味道总是

245
00:08:20,000 --> 00:08:21,120
和过去一样，它

246
00:08:21,120 --> 00:08:23,199
真的很喜欢，比如榛子

247
00:08:23,199 --> 00:08:25,199
或花生酱，

248
00:08:25,199 --> 00:08:28,080
所以这个是 rl 的一个突出

249
00:08:28,080 --> 00:08:28,560
问题

250
00:08:28,560 --> 00:08:30,800
在贝叶斯

251
00:08:30,800 --> 00:08:32,958
框架下，主动推理自然地处理它

252
00:08:32,958 --> 00:08:36,159


253
00:08:36,159 --> 00:08:37,760


254
00:08:37,760 --> 00:08:39,760


255
00:08:39,760 --> 00:08:42,799
m，你可以

256
00:08:42,799 --> 00:08:44,560
在主动推理框架中看到的最后一点是

257
00:08:44,560 --> 00:08:46,080
，它自然地将

258
00:08:46,080 --> 00:08:47,920
不确定性作为信念

259
00:08:47,920 --> 00:08:48,560
更新

260
00:08:48,560 --> 00:08:51,839
过程的一部分，好吧，

261
00:08:51,839 --> 00:08:54,959
所以现在我已经列出了关于主动推理的

262
00:08:54,959 --> 00:08:57,200
三件事

263
00:08:57,200 --> 00:08:58,640
影响力计划

264
00:08:58,640 --> 00:09:00,640
与 rl 相比，我将

265
00:09:00,640 --> 00:09:04,480
提供一些直觉，为什么

266
00:09:04,480 --> 00:09:08,000
你可以原谅一些动机，

267
00:09:08,000 --> 00:09:09,760
为什么我们甚至可以

268
00:09:09,760 --> 00:09:13,120
按照我们的方式制定积极影响力公式，

269
00:09:13,120 --> 00:09:17,040
所以，

270
00:09:17,200 --> 00:09:18,959
对不起，我刚刚意识到我喜欢跳过

271
00:09:18,959 --> 00:09:20,480
，嗯

272
00:09:20,480 --> 00:09:24,399
不 不 抱歉 很棒的介绍

273
00:09:24,399 --> 00:09:27,519
是的 非常感谢，让我

274
00:09:27,519 --> 00:09:32,000
向下滚动到 嗯，

275
00:09:32,000 --> 00:09:35,040
好吧，所以我之前说过，通过

276
00:09:35,040 --> 00:09:36,399
积极的推理，

277
00:09:36,399 --> 00:09:38,480
它规定代理

278
00:09:38,480 --> 00:09:40,320


279
00:09:40,320 --> 00:09:42,080
通过居住在最小化惊喜的吸引状态来维持它们的稳态，

280
00:09:42,080 --> 00:09:43,440


281
00:09:43,440 --> 00:09:44,880
所以你一定一直在想 什么是

282
00:09:44,880 --> 00:09:46,560
惊喜，嗯

283
00:09:46,560 --> 00:09:48,800
，我们如何将惊喜定义为结果的负对数

284
00:09:48,800 --> 00:09:50,080
概率

285
00:09:50,080 --> 00:09:52,080
，为此我们引入一个随机

286
00:09:52,080 --> 00:09:53,760
变量，它

287
00:09:53,760 --> 00:09:55,760
是对应的 ds 到代理

288
00:09:55,760 --> 00:09:57,440
收到的特定结果，

289
00:09:57,440 --> 00:10:00,480
并且这个 um 存在于所有可能结果的有限

290
00:10:00,480 --> 00:10:01,040
集合中

291
00:10:01,040 --> 00:10:03,600
，这就是 o，

292
00:10:03,600 --> 00:10:04,560


293
00:10:04,560 --> 00:10:06,720
所以我们刚刚的第一个等式

294
00:10:06,720 --> 00:10:08,160
正式说明了

295
00:10:08,160 --> 00:10:10,240
，这里 p 表示结果的概率

296
00:10:10,240 --> 00:10:11,680
分布，

297
00:10:11,680 --> 00:10:14,239


298
00:10:15,839 --> 00:10:19,920
嗯，好吧 因此，在主动推理

299
00:10:19,920 --> 00:10:21,200
中，代理实际上

300
00:10:21,200 --> 00:10:23,200
将我们刚刚经历的这个意外数量最小化的方式

301
00:10:23,200 --> 00:10:24,959
是通过维护

302
00:10:24,959 --> 00:10:27,120
世界的性别模型，

303
00:10:27,120 --> 00:10:29,360
嗯，这很重要，因为在任何

304
00:10:29,360 --> 00:10:31,040
给定的时间点，代理

305
00:10:31,040 --> 00:10:32,640
不一定可以访问

306
00:10:32,640 --> 00:10:34,000


307
00:10:34,000 --> 00:10:36,000
对世界当前状态的真实测量，因此在

308
00:10:36,000 --> 00:10:37,600
您在这里看到的这个特定图形中，

309
00:10:37,600 --> 00:10:39,600
您已经获得了环境和代理

310
00:10:39,600 --> 00:10:41,200
um 以特定方式与环境交互，

311
00:10:41,200 --> 00:10:42,320


312
00:10:42,320 --> 00:10:44,480
它被暴露在感官信号中，

313
00:10:44,480 --> 00:10:46,480
但它不知道是什么

314
00:10:46,480 --> 00:10:49,600
o 的结果是由 o 再生的，

315
00:10:49,600 --> 00:10:51,760
所以它只能通过 o 感知自己和

316
00:10:51,760 --> 00:10:52,959
周围的世界

317
00:10:52,959 --> 00:10:55,120
，它需要

318
00:10:55,120 --> 00:10:57,279
推断什么类型 状态

319
00:10:57,279 --> 00:11:00,959
或 um 真正的原因 um

320
00:11:00,959 --> 00:11:03,279


321
00:11:03,279 --> 00:11:06,000
对它所暴露的特定感觉输入负责

322
00:11:06,000 --> 00:11:08,959
，这就是为什么在主动推理中，

323
00:11:08,959 --> 00:11:10,240
当我们制定问题时，我们将

324
00:11:10,240 --> 00:11:11,839
其表述为部分可观察的

325
00:11:11,839 --> 00:11:13,519
马尔可夫决策过程，

326
00:11:13,519 --> 00:11:16,560
因为 um 以这种方式我们 能够

327
00:11:16,560 --> 00:11:18,959
制定一个性别模型，它定义

328
00:11:18,959 --> 00:11:19,279


329
00:11:19,279 --> 00:11:22,160


330
00:11:22,160 --> 00:11:23,920
了代理

331
00:11:23,920 --> 00:11:26,399
将用来推断结果的内部状态的内部分布，

332
00:11:26,399 --> 00:11:27,920
因此他无法访问真实

333
00:11:27,920 --> 00:11:28,399
状态，

334
00:11:28,399 --> 00:11:30,320
但它可以

335
00:11:30,320 --> 00:11:32,000
对以下状态做出假设或信念 可能会

336
00:11:32,000 --> 00:11:32,640


337
00:11:32,640 --> 00:11:35,519
产生一种特殊的感觉，呃结果

338
00:11:35,519 --> 00:11:38,320
空间正在暴露给

339
00:11:38,320 --> 00:11:40,800
嗯，并且使用它，代理将

340
00:11:40,800 --> 00:11:43,600


341
00:11:43,600 --> 00:11:45,839
使用反向映射过程，

342
00:11:45,839 --> 00:11:48,079
特别是贝叶斯模型

343
00:11:48,079 --> 00:11:49,680
反演来推断真实状态，并使这更

344
00:11:49,680 --> 00:11:52,000
具体一点 您可以做的是

345
00:11:52,000 --> 00:11:55,680
将隐藏状态视为位置或颜色

346
00:11:55,680 --> 00:11:58,240
，例如

347
00:11:58,240 --> 00:12:00,079
，代理将暴露于的观察空间将

348
00:12:00,079 --> 00:12:00,560
是

349
00:12:00,560 --> 00:12:03,680
嗯，例如运动的速度

350
00:12:03,680 --> 00:12:04,480


351
00:12:04,480 --> 00:12:07,440
或特定的奖励，嗯，或者像

352
00:12:07,440 --> 00:12:10,959
他们正在暴露的一张快乐的脸，

353
00:12:10,959 --> 00:12:14,720
嗯，好吧，如果我们要

354
00:12:14,720 --> 00:12:16,320
更正式地考虑这个问题，

355
00:12:16,320 --> 00:12:19,920
那么温和模型是什么？ 嗯，

356
00:12:19,920 --> 00:12:21,760
之前描述的一位绅士是

357
00:12:21,760 --> 00:12:23,040


358
00:12:23,040 --> 00:12:25,200
这个积极和朋友

359
00:12:25,200 --> 00:12:26,160
公式

360
00:12:26,160 --> 00:12:29,040
中的部分可观察 mdp，它基于

361
00:12:29,040 --> 00:12:30,639
我们在这里考虑的一个简化设置，其中我们

362
00:12:30,639 --> 00:12:32,240
只有两个随机

363
00:12:32,240 --> 00:12:35,120
变量，第一个是我们已经讨论过的 o

364
00:12:35,120 --> 00:12:36,320
，第二个

365
00:12:36,320 --> 00:12:39,680
是 s 其中 s 表示 um 一个

366
00:12:39,680 --> 00:12:41,839
代表隐藏或潜在

367
00:12:41,839 --> 00:12:44,079
状态的随机变量，它们存在于

368
00:12:44,079 --> 00:12:45,760
所有可能

369
00:12:45,760 --> 00:12:47,519
隐藏状态的有限集合中，这里用

370
00:12:47,519 --> 00:12:49,279
大写 s 表示

371
00:12:49,279 --> 00:12:51,440
，我们

372
00:12:51,440 --> 00:12:52,399
超过 o 和

373
00:12:52,399 --> 00:12:54,880
s 的联合概率可以分解为似然

374
00:12:54,880 --> 00:12:55,440
函数

375
00:12:55,440 --> 00:12:59,120
这是 o 给定 s 的 p，然后你

376
00:12:59,120 --> 00:13:01,360
有内部状态的先验

377
00:13:01,360 --> 00:13:02,720
，即 pss，

378
00:13:02,720 --> 00:13:04,839
所以这给了你一个非常好的

379
00:13:04,839 --> 00:13:06,240
公式

380
00:13:06,240 --> 00:13:09,519
，我们将在

381
00:13:09,519 --> 00:13:11,839
接下来的几张幻灯片中使用它 我只是

382
00:13:11,839 --> 00:13:12,720
想问一下

383
00:13:12,720 --> 00:13:14,480
，当我

384
00:13:14,480 --> 00:13:15,920
突出显示或当我

385
00:13:15,920 --> 00:13:20,079
或不在那里时，你能看到我的鼠标吗？

386
00:13:20,079 --> 00:13:22,719


387
00:13:22,800 --> 00:13:24,560


388
00:13:24,560 --> 00:13:25,600


389
00:13:25,600 --> 00:13:27,360


390
00:13:27,360 --> 00:13:29,040
可能导致给定

391
00:13:29,040 --> 00:13:29,760
结果的隐藏状态

392
00:13:29,760 --> 00:13:31,440
，这可以通过使用

393
00:13:31,440 --> 00:13:33,440
我刚刚提到的

394
00:13:33,440 --> 00:13:34,160
可能性

395
00:13:34,160 --> 00:13:37,360
和先验

396
00:13:37,360 --> 00:13:40,320
um 的分解来实现，但问题是这不是一个

397
00:13:40,320 --> 00:13:42,399
简单的任务，因为

398
00:13:42,399 --> 00:13:43,519
隐藏的维度 状态

399
00:13:43,519 --> 00:13:45,360
um 可能非常大，如果您正在

400
00:13:45,360 --> 00:13:47,120
考虑

401
00:13:47,120 --> 00:13:48,480
我们将

402
00:13:48,480 --> 00:13:50,240
在稍后引入的其他随机变量，这将变得

403
00:13:50,240 --> 00:13:51,600
更加成问题

404
00:13:51,600 --> 00:13:54,480


405
00:13:54,480 --> 00:13:56,240


406
00:13:56,240 --> 00:13:57,199


407
00:13:57,199 --> 00:13:59,920
更吸引人，并且

408
00:13:59,920 --> 00:14:01,920
允许我们估计感兴趣的数量，

409
00:14:01,920 --> 00:14:05,839


410
00:14:05,839 --> 00:14:08,720
所以这将是一个自然的步骤来

411
00:14:08,720 --> 00:14:10,160
讨论变分自由能

412
00:14:10,160 --> 00:14:11,760
，这是 qua 的变分近似

413
00:14:11,760 --> 00:14:12,800


414
00:14:12,800 --> 00:14:15,519
感兴趣的实体嗯所以什么是变分自由

415
00:14:15,519 --> 00:14:17,199
能所以变分自由能被

416
00:14:17,199 --> 00:14:19,440
定义为惊喜的上限，

417
00:14:19,440 --> 00:14:20,560
所以我们考虑的第一个定义

418
00:14:20,560 --> 00:14:21,920
抱歉第一个定义

419
00:14:21,920 --> 00:14:24,240
是使用詹森不等式推导出来的

420
00:14:24,240 --> 00:14:25,519
，

421
00:14:25,519 --> 00:14:27,600
并且通常被称为

422
00:14:27,600 --> 00:14:29,760
变分推理中的负面证据下限

423
00:14:29,760 --> 00:14:32,480
所以我们从

424
00:14:32,480 --> 00:14:34,240
方程 4 中得到这个，我们刚刚

425
00:14:34,240 --> 00:14:37,199
通过在两边引入负对数

426
00:14:37,199 --> 00:14:37,760


427
00:14:37,760 --> 00:14:41,040
然后将该项

428
00:14:41,040 --> 00:14:43,680
乘以 1，这本质上是 s 的 q 超过 s 的 q

429
00:14:43,680 --> 00:14:45,360
所以我们假设

430
00:14:45,360 --> 00:14:48,560
s 的 q 不能相等 到零，

431
00:14:48,560 --> 00:14:51,199
然后我们应用 jensen

432
00:14:51,199 --> 00:14:53,199
不等式，我们将 log 移动

433
00:14:53,199 --> 00:14:54,000
到函数中

434
00:14:54,000 --> 00:14:56,560
，我们最终得到我们

435
00:14:56,560 --> 00:14:58,000
对 t of s 的期望，

436
00:14:58,000 --> 00:15:00,079
对于关节的 log 在

437
00:15:00,079 --> 00:15:01,920


438
00:15:01,920 --> 00:15:05,040
这里和然后 我们

439
00:15:05,040 --> 00:15:07,120
把负数放在里面，我们可以

440
00:15:07,120 --> 00:15:07,839
把它翻转过来

441
00:15:07,839 --> 00:15:11,120
，我们在这里得到了我们第一个

442
00:15:11,120 --> 00:15:14,160
很好的兴趣量，我们

443
00:15:14,160 --> 00:15:14,560
得到

444
00:15:14,560 --> 00:15:17,199
了我们感兴趣的界限

445
00:15:17,199 --> 00:15:17,600


446
00:15:17,600 --> 00:15:20,720
和术语

447
00:15:20,720 --> 00:15:24,240


448
00:15:24,480 --> 00:15:25,920
为了使这一点更

449
00:15:25,920 --> 00:15:27,760
具体一点，我们现在可以做的是

450
00:15:27,760 --> 00:15:29,279
进一步操纵变分自由

451
00:15:29,279 --> 00:15:29,759
能将

452
00:15:29,759 --> 00:15:34,959
um 召唤到 uh

453
00:15:34,959 --> 00:15:38,720
近似和真实后验之间的杀戮

454
00:15:38,720 --> 00:15:41,360
减去对数

455
00:15:41,360 --> 00:15:43,120
证明我们的模型证据

456
00:15:43,120 --> 00:15:45,920
，我们可以重新安排最后一个方程，

457
00:15:45,920 --> 00:15:47,519
以真正磨练出

458
00:15:47,519 --> 00:15:48,959


459
00:15:48,959 --> 00:15:51,759
意外和变分自由能 um 之间的联系，

460
00:15:51,759 --> 00:15:52,320
所以

461
00:15:52,320 --> 00:15:55,040
如果你记得 kl 是一个散度

462
00:15:55,040 --> 00:15:56,720
，这意味着它不能小于

463
00:15:56,720 --> 00:15:58,079
零，所以

464
00:15:58,079 --> 00:16:01,519
嗯，它总是严格地大于

465
00:16:01,519 --> 00:16:03,440
或等于零，这意味着

466
00:16:03,440 --> 00:16:06,399
当我们的近似值等于

467
00:16:06,399 --> 00:16:07,519
真实后验时，

468
00:16:07,519 --> 00:16:09,600
我们最终得到的变化自由能

469
00:16:09,600 --> 00:16:10,720
等于模型

470
00:16:10,720 --> 00:16:13,839
证据，这意味着最小化

471
00:16:13,839 --> 00:16:15,839
自由能本质上等同于

472
00:16:15,839 --> 00:16:18,320
最大化性别模型 证据

473
00:16:18,320 --> 00:16:23,839
嗯 好的

474
00:16:25,279 --> 00:16:28,240
嗯 我们可以重写之前的

475
00:16:28,240 --> 00:16:29,600
方程

476
00:16:29,600 --> 00:16:31,680
嗯 方程 10 将

477
00:16:31,680 --> 00:16:33,040
变分自由能表示

478
00:16:33,040 --> 00:16:34,800
为 po 的函数

479
00:16:34,800 --> 00:16:36,079
多种不同形式的先验信念，

480
00:16:36,079 --> 00:16:37,680
所以我只关注公式

481
00:16:37,680 --> 00:16:40,000
12，这里是复杂性

482
00:16:40,000 --> 00:16:42,800
减去准确性，所以这是一个权衡，呃

483
00:16:42,800 --> 00:16:45,440
，通常

484
00:16:45,440 --> 00:16:48,839
在论文基本上说

485
00:16:48,839 --> 00:16:50,160
复杂性 tam

486
00:16:50,160 --> 00:16:53,600
或复杂性成本本质上是时使用 嗯，

487
00:16:53,600 --> 00:16:57,120
你的羽衣甘蓝在你

488
00:16:57,120 --> 00:17:01,120
给定 pi 的大概 um 与

489
00:17:01,120 --> 00:17:05,199
uh 你给定 um pi 的 p 之间，这里 pi

490
00:17:05,199 --> 00:17:07,280
只是你的政策，这些可能

491
00:17:07,280 --> 00:17:09,280
与

492
00:17:09,280 --> 00:17:12,319
呃代理人将如何行动的假设无关，

493
00:17:12,319 --> 00:17:14,079
但我会来 回到政策

494
00:17:14,079 --> 00:17:15,679
真正需要的东西，但现在

495
00:17:15,679 --> 00:17:17,599
只将它们视为一个术语，它允许

496
00:17:17,599 --> 00:17:19,919
我们将自由能限制

497
00:17:19,919 --> 00:17:22,319
在一系列感兴趣的轨迹上

498
00:17:22,319 --> 00:17:23,679


499
00:17:23,679 --> 00:17:26,319
，我们拥有的第二个术语

500
00:17:26,319 --> 00:17:27,599


501
00:17:27,599 --> 00:17:31,200
是 o 给定 s 的对数概率，所以可能性 uh

502
00:17:31,200 --> 00:17:34,559
与 a 关于 s 的键，

503
00:17:34,559 --> 00:17:37,200
它可以为您提供准确度，所以一个简单

504
00:17:37,200 --> 00:17:39,120
的思考方式是，

505
00:17:39,120 --> 00:17:42,240
嗯，这只是 um

506
00:17:42,240 --> 00:17:43,440
模型的准确度，这是一些

507
00:17:43,440 --> 00:17:45,919
正则化项，它是 s 的惩罚

508
00:17:45,919 --> 00:17:47,200
项 确保它不会

509
00:17:47,200 --> 00:17:49,280
与我们最初的先验相差太远，

510
00:17:49,280 --> 00:17:52,320


511
00:17:52,320 --> 00:17:55,679
好吧，所以这个特定数量的

512
00:17:55,679 --> 00:17:56,960
变分自由能

513
00:17:56,960 --> 00:17:58,480
在这一点上是否有任何

514
00:17:58,480 --> 00:18:01,039
关于能量变化

515
00:18:01,039 --> 00:18:03,760


516
00:18:03,760 --> 00:18:05,360
的问题

517
00:18:05,360 --> 00:18:07,360
呢？ 感知环境

518
00:18:07,360 --> 00:18:09,200
并解决主动

519
00:18:09,200 --> 00:18:11,039
影响公式的一部分，该公式正在

520
00:18:11,039 --> 00:18:11,760


521
00:18:11,760 --> 00:18:14,240
对代理在给定时间点与之交互的给定世界进行推断，

522
00:18:14,240 --> 00:18:16,080


523
00:18:16,080 --> 00:18:16,880


524
00:18:16,880 --> 00:18:19,440
但是我们实际上并没有

525
00:18:19,440 --> 00:18:20,320
考虑到

526
00:18:20,320 --> 00:18:22,960
主动部分，而就像

527
00:18:22,960 --> 00:18:23,600


528
00:18:23,600 --> 00:18:25,679
我们拥有的这个特定代理一样 在主动推理公式下，

529
00:18:25,679 --> 00:18:26,880
可以

530
00:18:26,880 --> 00:18:29,520
采取一系列行动或

531
00:18:29,520 --> 00:18:31,360
与环境交互，从而

532
00:18:31,360 --> 00:18:34,240
影响未来的环境

533
00:18:34,240 --> 00:18:34,960


534
00:18:34,960 --> 00:18:37,280
嗯，为了进一步激发这一点

535
00:18:37,280 --> 00:18:38,880
，我们可以考虑的是，

536
00:18:38,880 --> 00:18:40,480
我们不仅要最小化 我们的变化

537
00:18:40,480 --> 00:18:42,240
自由能我们还希望最小化一个

538
00:18:42,240 --> 00:18:44,320
称为预期自由能的量

539
00:18:44,320 --> 00:18:46,320
，它取决于预期

540
00:18:46,320 --> 00:18:48,240
对未来或未来的观察

541
00:18:48,240 --> 00:18:50,799
以及

542
00:18:50,799 --> 00:18:52,080
对这个

543
00:18:52,080 --> 00:18:54,480
特定术语的最小化允许代理

544
00:18:54,480 --> 00:18:56,400
通过在当前采取特定行动来影响未来，这些

545
00:18:56,400 --> 00:18:58,400
行动

546
00:18:58,400 --> 00:19:00,080
是从一组政策中选择的，

547
00:19:00,080 --> 00:19:02,720
所以我现在提到了几次政策，

548
00:19:02,720 --> 00:19:03,520


549
00:19:03,520 --> 00:19:06,960
所以什么是 他们嗯，所以策略可以

550
00:19:06,960 --> 00:19:09,280
定义为时间 tau 的一系列动作

551
00:19:09,280 --> 00:19:10,240


552
00:19:10,240 --> 00:19:12,240
，使代理能够

553
00:19:12,240 --> 00:19:13,760
在隐藏状态

554
00:19:13,760 --> 00:19:17,520
和 tau 之间转换，这里本质上是

555
00:19:17,520 --> 00:19:19,120
一系列轨迹，直到

556
00:19:19,120 --> 00:19:21,039
特定的地平线，

557
00:19:21,039 --> 00:19:24,320
呃上限，考虑到总

558
00:19:24,320 --> 00:19:25,679


559
00:19:25,679 --> 00:19:27,760
时间 您在特定设置中考虑的步骤

560
00:19:27,760 --> 00:19:29,280


561
00:19:29,280 --> 00:19:32,320
以及为了我们正确定义 uh

562
00:19:32,320 --> 00:19:33,919
策略，我们需要引入两个

563
00:19:33,919 --> 00:19:35,440
额外的随机变量，

564
00:19:35,440 --> 00:19:38,160
因此第一个是一个

565
00:19:38,160 --> 00:19:38,960


566
00:19:38,960 --> 00:19:43,039
以 tau 为条件的动作，此处由 utau 表示，

567
00:19:43,039 --> 00:19:46,160
并且存在于有限的集合中

568
00:19:46,160 --> 00:19:48,320
代理可以采取的所有可能的行动

569
00:19:48,320 --> 00:19:49,200


570
00:19:49,200 --> 00:19:50,960
以及我们引入的第二个随机变量

571
00:19:50,960 --> 00:19:52,640
是呃

572
00:19:52,640 --> 00:19:54,559
策略，它是我们讨论的

573
00:19:54,559 --> 00:19:55,919
pi  ssed

574
00:19:55,919 --> 00:19:58,000
并且这存在于

575
00:19:58,000 --> 00:19:58,960
所有可能的

576
00:19:58,960 --> 00:20:02,000
uh 策略 um 或动作序列的有限集合中

577
00:20:02,000 --> 00:20:04,640
，就像我们在这里感兴趣的顺序策略

578
00:20:04,640 --> 00:20:06,240
优化一样，

579
00:20:06,240 --> 00:20:08,240
所以为了让它更具体一点

580
00:20:08,240 --> 00:20:09,520


581
00:20:09,520 --> 00:20:11,200
，随机变量可以是

582
00:20:11,200 --> 00:20:13,440


583
00:20:13,440 --> 00:20:16,559
在特定

584
00:20:16,559 --> 00:20:19,760
时间范围内分解为一系列动作 tau

585
00:20:19,760 --> 00:20:23,280
因此 u1 u2 和 um 上升到 utah 将

586
00:20:23,280 --> 00:20:25,120
表示从

587
00:20:25,120 --> 00:20:26,960
uh 动作在时间点一的动作到

588
00:20:26,960 --> 00:20:28,799
时间点二的动作，依此类推

589
00:20:28,799 --> 00:20:31,360
，当您考虑时链接是明确的

590
00:20:31,360 --> 00:20:32,320


591
00:20:32,320 --> 00:20:35,679
如果您在特定时间点考虑一项政策，

592
00:20:35,679 --> 00:20:38,880
那么 tau 那么

593
00:20:38,880 --> 00:20:41,840
您得到的行动将是那个行动，

594
00:20:41,840 --> 00:20:45,280
好吧，很酷，嗯，所以

595
00:20:45,280 --> 00:20:47,360
我还想强调一下，这个

596
00:20:47,360 --> 00:20:49,120
政策的定义实际上与

597
00:20:49,120 --> 00:20:50,559


598
00:20:50,559 --> 00:20:53,520
rl 中的考虑方式完全不同或不同

599
00:20:53,520 --> 00:20:54,240


600
00:20:54,240 --> 00:20:56,640
嗯，当他们说政策意味着国家

601
00:20:56,640 --> 00:20:58,000
行动政策

602
00:20:58,000 --> 00:20:59,840
时，正如我刚刚提到的推理行为，

603
00:20:59,840 --> 00:21:02,240
政策只是

604
00:21:02,240 --> 00:21:04,159
随着时间的推移对行动的一系列选择，这

605
00:21:04,159 --> 00:21:06,159
是一个顺序政策，

606
00:21:06,159 --> 00:21:08,320
并且 这与强化学习中的状态动作策略不同，

607
00:21:08,320 --> 00:21:10,400
后者

608
00:21:10,400 --> 00:21:12,080
是将状态映射到

609
00:21:12,080 --> 00:21:16,080
动作，因此您的 rl 策略考虑

610
00:21:16,080 --> 00:21:16,720


611
00:21:16,720 --> 00:21:19,919
了动作和状态，是

612
00:21:19,919 --> 00:21:20,799
在

613
00:21:20,799 --> 00:21:24,159
给定状态的情况下您的动作的概率，并且根据

614
00:21:24,159 --> 00:21:27,919
我们的 mdp 公式

615
00:21:27,919 --> 00:21:30,159
，动作的定义哦 抱歉

616
00:21:30,159 --> 00:21:31,760
，当我们考虑 tau 等于 1 的设置时，rl 中的策略定义

617
00:21:31,760 --> 00:21:33,760
和主动推理变得

618
00:21:33,760 --> 00:21:35,679
完全相同，

619
00:21:35,679 --> 00:21:37,679
所以你只

620
00:21:37,679 --> 00:21:41,120
考虑领先一步

621
00:21:41,120 --> 00:21:45,120


622
00:21:45,120 --> 00:21:45,760


623
00:21:45,760 --> 00:21:49,039


624
00:21:49,039 --> 00:21:50,960
他们期望自由能的定量兴趣，这

625
00:21:50,960 --> 00:21:54,080
就是我们如何推导它，所以

626
00:21:54,080 --> 00:21:56,240
为了推导它，我们首先需要扩展

627
00:21:56,240 --> 00:21:57,840


628
00:21:57,840 --> 00:22:00,000
我们在

629
00:22:00,000 --> 00:22:02,240
几张幻灯片之前拥有的变分自由能定义，现在让它

630
00:22:02,240 --> 00:22:03,360
依赖于

631
00:22:03,360 --> 00:22:06,799
时间 so tau 和 政策，我们

632
00:22:06,799 --> 00:22:07,919
所做的本质上

633
00:22:07,919 --> 00:22:10,320
是采用相同的等式，并

634
00:22:10,320 --> 00:22:12,320


635
00:22:12,320 --> 00:22:14,320


636
00:22:14,320 --> 00:22:16,080
在特定政策下将其分解为先前遗憾的先前和当前时间步长，

637
00:22:16,080 --> 00:22:18,080
这样就可以 这就是为什么我们有条件，

638
00:22:18,080 --> 00:22:20,480
然后我们

639
00:22:20,480 --> 00:22:23,520
在等式 15 中以特定方式分解它

640
00:22:23,520 --> 00:22:25,039
，然后

641
00:22:25,039 --> 00:22:27,360
在等式 16 中写出矩阵公式。

642
00:22:27,360 --> 00:22:28,960
所以如果有任何问题，我们可以回到这个

643
00:22:28,960 --> 00:22:31,039
问题，

644
00:22:31,039 --> 00:22:32,880
但要采取的关键事项 远离

645
00:22:32,880 --> 00:22:35,039
幻灯片的是，现在我们包括

646
00:22:35,039 --> 00:22:37,120
对时间 um 的函数依赖性，

647
00:22:37,120 --> 00:22:40,799
用于能量 um 的变化

648
00:22:40,799 --> 00:22:42,799
，这使我们现在可以

649
00:22:42,799 --> 00:22:45,120
转向预期的自由能公式

650
00:22:45,120 --> 00:22:47,280
um，但这里要注意的关键是

651
00:22:47,280 --> 00:22:49,360
我们 只考虑时间点

652
00:22:49,360 --> 00:22:52,320
um 之前的时间点和

653
00:22:52,320 --> 00:22:52,799
现在，

654
00:22:52,799 --> 00:22:55,840
而不是未来，

655
00:22:58,000 --> 00:23:00,320
所以在

656
00:23:00,320 --> 00:23:01,440
我们可以推导出

657
00:23:01,440 --> 00:23:04,559
uh 预期的自由能 um 和预期的自由能之前使用自由能方程，

658
00:23:04,559 --> 00:23:06,720


659
00:23:06,720 --> 00:23:08,320
那么预期的自由能

660
00:23:08,320 --> 00:23:11,200
是自由能 未来

661
00:23:11,200 --> 00:23:12,159
轨迹

662
00:23:12,159 --> 00:23:14,559
g 的函数，它有效地

663
00:23:14,559 --> 00:23:16,400


664
00:23:16,400 --> 00:23:18,240
根据尚未观察到的结果评估合理政策的证据，

665
00:23:18,240 --> 00:23:19,760
因此这是关键，因此

666
00:23:19,760 --> 00:23:20,640
您正在

667
00:23:20,640 --> 00:23:24,240
对民意调查进行推断

668
00:23:24,240 --> 00:23:26,000
你还没有

669
00:23:26,000 --> 00:23:28,000
观察到的未来轨迹

670
00:23:28,000 --> 00:23:30,640
，这里介绍了两种启发式方法

671
00:23:30,640 --> 00:23:31,679


672
00:23:31,679 --> 00:23:34,159
，以便得到

673
00:23:34,159 --> 00:23:35,120
我们

674
00:23:35,120 --> 00:23:38,960
在等式 17 中看到的 g 公式。第

675
00:23:38,960 --> 00:23:41,440
一个是将关于未来结果的信念包含

676
00:23:41,440 --> 00:23:42,320


677
00:23:42,320 --> 00:23:44,159
在我们的期望中

678
00:23:44,159 --> 00:23:46,000


679
00:23:46,000 --> 00:23:48,159
用这里的可能性补充近似后验下的期望，

680
00:23:48,159 --> 00:23:50,000


681
00:23:50,000 --> 00:23:51,440
这导致了这里

682
00:23:51,440 --> 00:23:53,279
前两个项给出的预测分布

683
00:23:53,279 --> 00:23:54,880


684
00:23:54,880 --> 00:23:57,600
，第二个是我们

685
00:23:57,600 --> 00:23:58,559
隐含地或者

686
00:23:58,559 --> 00:24:00,799
我想明确地以状态的

687
00:24:00,799 --> 00:24:02,799
联合概率

688
00:24:02,799 --> 00:24:06,080
和性别观察为条件 模型 嗯，

689
00:24:06,080 --> 00:24:08,960
对不起，性别模型 嗯，

690
00:24:08,960 --> 00:24:10,159


691
00:24:10,159 --> 00:24:13,200
取决于期望的事态，而

692
00:24:13,200 --> 00:24:14,880
不是现在的特定政策，因此

693
00:24:14,880 --> 00:24:16,799
这限制了代理将拥有的偏好类型，

694
00:24:16,799 --> 00:24:17,440


695
00:24:17,440 --> 00:24:20,640


696
00:24:20,640 --> 00:24:21,200


697
00:24:21,200 --> 00:24:22,720
而我们正在采取的这两个举措的帮助在于我们可以

698
00:24:22,720 --> 00:24:24,480
现在

699
00:24:24,480 --> 00:24:27,200
在实际进行观察之前评估这个数量

700
00:24:27,200 --> 00:24:28,240
，第二个

701
00:24:28,240 --> 00:24:30,400
是 g 的最小化

702
00:24:30,400 --> 00:24:32,080
实际上会遇到 愤怒的政体

703
00:24:32,080 --> 00:24:35,440
政策要与呃代理人期望自己处于的理想状态保持一致，嗯，

704
00:24:35,440 --> 00:24:37,520


705
00:24:37,520 --> 00:24:39,760


706
00:24:39,760 --> 00:24:41,600
我只想简单提一下

707
00:24:41,600 --> 00:24:44,240
，这不是获得预期自由能的唯一方法，

708
00:24:44,240 --> 00:24:45,679
而且还有

709
00:24:45,679 --> 00:24:47,760
一些工作，嗯，看起来 在其他

710
00:24:47,760 --> 00:24:48,960
公式中，

711
00:24:48,960 --> 00:24:52,240
包括 carl um 的工作，其中

712
00:24:52,240 --> 00:24:53,600
预期自由能的公式

713
00:24:53,600 --> 00:24:55,919
可以分解成不同的

714
00:24:55,919 --> 00:24:58,799
um 结构，所以如果有人对此

715
00:24:58,799 --> 00:25:02,480
感兴趣，我们可以稍后再讨论，

716
00:25:02,640 --> 00:25:04,799
但是我刚刚介绍的这个自由能预期自由

717
00:25:04,799 --> 00:25:05,679
能

718
00:25:05,679 --> 00:25:07,520
可以是 以

719
00:25:07,520 --> 00:25:10,320
某种方式分解 嗯，所以方程 20 和 21

720
00:25:10,320 --> 00:25:10,799
给出了

721
00:25:10,799 --> 00:25:13,200
两种不同的分解，

722
00:25:13,200 --> 00:25:14,960
第一个是认知和

723
00:25:14,960 --> 00:25:17,039
外在价值交易者，第二

724
00:25:17,039 --> 00:25:18,960
个是预期的，

725
00:25:18,960 --> 00:25:22,320
呃成本和歧义项，所以如果我们只

726
00:25:22,320 --> 00:25:23,360
考虑

727
00:25:23,360 --> 00:25:26,640
um，第一个方程 我们可以说，如果

728
00:25:26,640 --> 00:25:28,080
我们最小化这个等式，

729
00:25:28,080 --> 00:25:30,559
那么我们就抓住了这个必要性，以

730
00:25:30,559 --> 00:25:31,360
最大化

731
00:25:31,360 --> 00:25:33,760
你从观察中获得的信息增益

732
00:25:33,760 --> 00:25:35,679


733
00:25:35,679 --> 00:25:38,880
关于特定隐藏状态的环境 um，

734
00:25:38,880 --> 00:25:41,840
同时最大化

735
00:25:41,840 --> 00:25:43,360
由

736
00:25:43,360 --> 00:25:45,520
日志偏好或外部值评分的期望值

737
00:25:45,520 --> 00:25:46,559
，因此

738
00:25:46,559 --> 00:25:48,159
这个特定的公式实际上

739
00:25:48,159 --> 00:25:50,320
给了我们一个非常明确的权衡，

740
00:25:50,320 --> 00:25:50,880
即

741
00:25:50,880 --> 00:25:53,200
um 第一个组件，它

742
00:25:53,200 --> 00:25:54,960
是促进的认知值

743
00:25:54,960 --> 00:25:57,120
好奇的行为，所以这就是你想要

744
00:25:57,120 --> 00:25:59,760
的，鼓励探索，因为代理会

745
00:25:59,760 --> 00:26:00,799
寻找

746
00:26:00,799 --> 00:26:02,400
这些新的状态，以最大限度地减少

747
00:26:02,400 --> 00:26:04,960
环境的不确定性

748
00:26:04,960 --> 00:26:07,520
，而后者更务实，

749
00:26:07,520 --> 00:26:08,400
它

750
00:26:08,400 --> 00:26:11,360
通过

751
00:26:11,360 --> 00:26:13,360


752
00:26:13,360 --> 00:26:15,679
对代理的政策类型的理解来鼓励剥削行为 更喜欢

753
00:26:15,679 --> 00:26:16,720
达到

754
00:26:16,720 --> 00:26:18,960
um 换句话说，就像这个预期的

755
00:26:18,960 --> 00:26:20,480
自由能公式

756
00:26:20,480 --> 00:26:23,200
，我们在等式 20 中看到的

757
00:26:23,200 --> 00:26:24,559
本质上是将

758
00:26:24,559 --> 00:26:26,640
探索和开发视为

759
00:26:26,640 --> 00:26:28,240
解决同一问题的两种不同方式，

760
00:26:28,240 --> 00:26:28,799


761
00:26:28,799 --> 00:26:31,440
从而最大限度地减少演示文稿开头提到的不确定性

762
00:26:31,440 --> 00:26:34,720


763
00:26:34,720 --> 00:26:37,120
我们也可以考虑这里的

764
00:26:37,120 --> 00:26:38,159
第二个方程

765
00:26:38,159 --> 00:26:41,279
wh  ich 只是为我们提供了

766
00:26:41,279 --> 00:26:43,200
关于

767
00:26:43,200 --> 00:26:45,360
自由预期频率的另一种观点，即

768
00:26:45,360 --> 00:26:47,919
代理人希望最小化模糊性

769
00:26:47,919 --> 00:26:50,000
以及特定政策下的结果

770
00:26:50,000 --> 00:26:52,000
偏离

771
00:26:52,000 --> 00:26:55,039
先前偏好的程度，因此这里的模糊性

772
00:26:55,039 --> 00:26:56,880
是条件熵的期望

773
00:26:56,880 --> 00:26:58,880
或不确定性

774
00:26:58,880 --> 00:27:01,600
在这种特定环境下，当前政策下的结果

775
00:27:01,600 --> 00:27:02,799


776
00:27:02,799 --> 00:27:04,559
低熵表明结果

777
00:27:04,559 --> 00:27:06,240
非常显着，并且

778
00:27:06,240 --> 00:27:08,320
对于隐藏状态具有独特的信息

779
00:27:08,320 --> 00:27:10,320
，例如，

780
00:27:10,320 --> 00:27:11,760
与房间

781
00:27:11,760 --> 00:27:13,039


782
00:27:13,039 --> 00:27:14,720
是否非常黑暗相比，

783
00:27:14,720 --> 00:27:16,080
您可能会看到的视觉提示 '不会从中做出任何

784
00:27:16,080 --> 00:27:19,440
重要的事情，此外，

785
00:27:19,440 --> 00:27:20,880
代理人希望追求

786
00:27:20,880 --> 00:27:21,520


787
00:27:21,520 --> 00:27:23,760
类似于其首选结果的政策依赖结果，

788
00:27:23,760 --> 00:27:25,039


789
00:27:25,039 --> 00:27:27,120
所以这是由 c 捐赠的，这

790
00:27:27,120 --> 00:27:29,279
是

791
00:27:29,279 --> 00:27:31,039
在预测结果和首选结果之间的护理差异时实现的

792
00:27:31,039 --> 00:27:34,320


793
00:27:34,320 --> 00:27:37,679
被一个特定的政策最小化，

794
00:27:37,679 --> 00:27:40,880
好吧，嗯，这些先前的信念

795
00:27:40,880 --> 00:27:41,520
但是，未来的

796
00:27:41,520 --> 00:27:42,960
结果使代理具有

797
00:27:42,960 --> 00:27:44,960
目标导向的行为

798
00:27:44,960 --> 00:27:46,960
，这是我认为

799
00:27:46,960 --> 00:27:48,320


800
00:27:48,320 --> 00:27:51,840
在积极影响中真正重要的实例之一，

801
00:27:51,840 --> 00:27:54,640
嗯，好吧，所以一旦我们有了预期的

802
00:27:54,640 --> 00:27:56,240
自由能量，我们就可以推导

803
00:27:56,240 --> 00:28:00,159
出政策，嗯，

804
00:28:00,159 --> 00:28:02,240
嗯，这样 是通过在预期自由能上应用软最大函数来推导任何策略的概率来实现的，

805
00:28:02,240 --> 00:28:04,080


806
00:28:04,080 --> 00:28:06,399


807
00:28:06,399 --> 00:28:08,480


808
00:28:08,480 --> 00:28:09,919
并且这种说明了主动推理的不

809
00:28:09,919 --> 00:28:11,760
言自明的行为，

810
00:28:11,760 --> 00:28:12,559


811
00:28:12,559 --> 00:28:14,559
因为导致较低预期自由能的任何类型的策略或动作

812
00:28:14,559 --> 00:28:16,480
序列

813
00:28:16,480 --> 00:28:18,240
更多 可能

814
00:28:18,240 --> 00:28:21,520
且直观地说，这是

815
00:28:21,520 --> 00:28:24,159
有道理的，因为预期的自由能

816
00:28:24,159 --> 00:28:25,039
在某种程度上

817
00:28:25,039 --> 00:28:26,960
封装了

818
00:28:26,960 --> 00:28:28,640


819
00:28:28,640 --> 00:28:30,399
您在与世界互动时想要包括或考虑的所有类型的事物，

820
00:28:30,399 --> 00:28:31,600
因此您想要探索

821
00:28:31,600 --> 00:28:32,960
想要利用但想要利用 有

822
00:28:32,960 --> 00:28:36,000
一个平衡，然后

823
00:28:36,000 --> 00:28:38,159
当你选择你的政策时，

824
00:28:38,159 --> 00:28:39,600
这只是确定

825
00:28:39,600 --> 00:28:40,159


826
00:28:40,159 --> 00:28:42,960
让你最接近这一点的一组行动的问题

827
00:28:42,960 --> 00:28:44,000
特定的核心

828
00:28:44,000 --> 00:28:45,760
um 这可以由一个

829
00:28:45,760 --> 00:28:48,960
有吸引力的状态定义，该状态由

830
00:28:48,960 --> 00:28:52,480
我们之前描述的 c 矩阵定义

831
00:28:52,480 --> 00:28:55,200


832
00:28:55,200 --> 00:28:56,159


833
00:28:56,159 --> 00:28:57,919


834
00:28:57,919 --> 00:28:59,600


835
00:28:59,600 --> 00:29:01,600


836
00:29:01,600 --> 00:29:03,760
在这里，

837
00:29:03,760 --> 00:29:06,080
嗯，通过在这上面设置一个超素数，

838
00:29:06,080 --> 00:29:07,520
你

839
00:29:07,520 --> 00:29:09,440
在

840
00:29:09,440 --> 00:29:10,799
预期的自由能

841
00:29:10,799 --> 00:29:14,720
uh 公式中引入了额外的复杂性成本，这让你可以

842
00:29:14,720 --> 00:29:17,840
解释

843
00:29:17,840 --> 00:29:20,880


844
00:29:20,880 --> 00:29:23,279
你希望你的

845
00:29:23,279 --> 00:29:24,159


846
00:29:24,159 --> 00:29:27,919
偏好在政策空间上有多平坦或有多自信或精确，

847
00:29:27,919 --> 00:29:31,279
嗯，关键 需要注意的是，嗯

848
00:29:31,279 --> 00:29:33,039
，为了简单起见，我不会

849
00:29:33,039 --> 00:29:34,880


850
00:29:34,880 --> 00:29:37,200
详细介绍如何优化这些细节，

851
00:29:37,200 --> 00:29:37,919


852
00:29:37,919 --> 00:29:40,159
但是您可以通过多种不同的方式来做到这一点，

853
00:29:40,159 --> 00:29:41,919
例如在主动

854
00:29:41,919 --> 00:29:42,720
推理中，

855
00:29:42,720 --> 00:29:45,039
我们可以优化对

856
00:29:45,039 --> 00:29:47,039
隐藏的感兴趣状态 策略

857
00:29:47,039 --> 00:29:48,640
推断的精度

858
00:29:48,640 --> 00:29:50,320
然后我们还可以

859
00:29:50,320 --> 00:29:51,679


860
00:29:51,679 --> 00:29:53,919
通过所涉及的学习过程通过 um 优化模型参数

861
00:29:53,919 --> 00:29:54,960


862
00:29:54,960 --> 00:29:58,080
嗯，但是这些取决于

863
00:29:58,080 --> 00:30:00,000
您正在查看的设置，例如，

864
00:30:00,000 --> 00:30:01,600
如果您使用变体基础，您

865
00:30:01,600 --> 00:30:02,559
只需迭代

866
00:30:02,559 --> 00:30:04,799
这些函数或目标

867
00:30:04,799 --> 00:30:06,559
函数，直到收敛

868
00:30:06,559 --> 00:30:08,960
或主动推理，嗯，您进行

869
00:30:08,960 --> 00:30:10,080
梯度上升，

870
00:30:10,080 --> 00:30:12,000
嗯，以找到 再次感兴趣的足够统计数据，

871
00:30:12,000 --> 00:30:13,919
这取决于

872
00:30:13,919 --> 00:30:15,840


873
00:30:15,840 --> 00:30:18,080
您正在查看的公式和设置，

874
00:30:18,080 --> 00:30:20,240
但这里要注意的关键是，

875
00:30:20,240 --> 00:30:21,679


876
00:30:21,679 --> 00:30:23,760
主动

877
00:30:23,760 --> 00:30:25,360
推理算法有三个特定方面是有用的，

878
00:30:25,360 --> 00:30:26,159
可以

879
00:30:26,159 --> 00:30:28,960
从这个特定的 呃

880
00:30:28,960 --> 00:30:29,679
框架

881
00:30:29,679 --> 00:30:32,799
并应用于呃其他设置，

882
00:30:32,799 --> 00:30:34,559
嗯，所以我只是要重申和

883
00:30:34,559 --> 00:30:36,080
简要总结它们，

884
00:30:36,080 --> 00:30:38,000
所以我们首先有至关重要的性别模型，

885
00:30:38,000 --> 00:30:39,679
所以为了让代理进行

886
00:30:39,679 --> 00:30:41,440
交互并最大限度地减少它的惊喜，它

887
00:30:41,440 --> 00:30:43,520
需要一个温和的世界模型

888
00:30:43,520 --> 00:30:46,880
和 这被描述为简单我不是

889
00:30:46,880 --> 00:30:48,640
我不包括任何模型

890
00:30:48,640 --> 00:30:50,240
参数但是你可以有

891
00:30:50,240 --> 00:30:52,159
你的结果你的状态和你的

892
00:30:52,159 --> 00:30:53,679
政策和这个

893
00:30:53,679 --> 00:30:56,960
嗯这些 被分解成嗯

894
00:30:56,960 --> 00:30:59,120
是的对不起这里有一个带括号的箭头

895
00:30:59,120 --> 00:31:00,000
但是

896
00:31:00,000 --> 00:31:03,120
这些被分解成你之前

897
00:31:03,120 --> 00:31:04,720
的可能性和你的转换

898
00:31:04,720 --> 00:31:05,760
函数

899
00:31:05,760 --> 00:31:07,840
然后你设置一旦你有了这个

900
00:31:07,840 --> 00:31:10,000
生殖器模型代理的目标

901
00:31:10,000 --> 00:31:11,519
是让模型适应样本

902
00:31:11,519 --> 00:31:13,360
观察以减少 价格

903
00:31:13,360 --> 00:31:14,960
，那是通过变分自由

904
00:31:14,960 --> 00:31:17,279
能优化，

905
00:31:17,279 --> 00:31:19,120
所以我们

906
00:31:19,120 --> 00:31:21,840
在复杂性和准确性成本之间进行了这种特殊的权衡，

907
00:31:21,840 --> 00:31:22,240
然后

908
00:31:22,240 --> 00:31:24,000
是第二个哦，对不起，这个算法的最后一部分

909
00:31:24,000 --> 00:31:25,760
是计划

910
00:31:25,760 --> 00:31:27,760
所以选择最小化

911
00:31:27,760 --> 00:31:29,200
不确定性的行动

912
00:31:29,200 --> 00:31:32,000
，嗯 是预期的自由能

913
00:31:32,000 --> 00:31:33,760
，你这样做的方式是通过一个

914
00:31:33,760 --> 00:31:34,480
软最大值

915
00:31:34,480 --> 00:31:37,600
超过这个 uh g 负 g

916
00:31:37,600 --> 00:31:39,120
我们在这里的数量

917
00:31:39,120 --> 00:31:41,360
，然后从中采样，以便

918
00:31:41,360 --> 00:31:42,720
选择下一个规格

919
00:31:42,720 --> 00:31:46,080
最好的动作，

920
00:31:46,080 --> 00:31:49,600
好吧，这是一个快速和 深入

921
00:31:49,600 --> 00:31:51,760
研究大量活跃的推理

922
00:31:51,760 --> 00:31:52,799
文献，但我只是想

923
00:31:52,799 --> 00:31:53,600
强调一下，如果您有兴趣，

924
00:31:53,600 --> 00:31:55,120
这些是三个核心要素

925
00:31:55,120 --> 00:31:57,120


926
00:31:57,120 --> 00:31:59,279
自己实现这些算法

927
00:31:59,279 --> 00:32:01,440
没问题，所以现在我要

928
00:32:01,440 --> 00:32:02,720
稍微切换一下，并通过

929
00:32:02,720 --> 00:32:04,080
与强化学习的比较进行比较，

930
00:32:04,080 --> 00:32:07,679
所以在我们的工作中，我们

931
00:32:07,679 --> 00:32:09,360
考虑了

932
00:32:09,360 --> 00:32:10,320
开放 AI

933
00:32:10,320 --> 00:32:12,880
健身房冰冻湖环境的修改版本，所以冰冻

934
00:32:12,880 --> 00:32:14,720
湖有一个网格- 就像

935
00:32:14,720 --> 00:32:17,120
具有四个不同补丁的结构，所以它的

936
00:32:17,120 --> 00:32:18,399
起点

937
00:32:18,399 --> 00:32:22,000
是 um，所以我们可以在这里

938
00:32:22,000 --> 00:32:22,799


939
00:32:22,799 --> 00:32:25,679


940
00:32:25,679 --> 00:32:26,640


941
00:32:26,640 --> 00:32:29,600


942
00:32:29,600 --> 00:32:31,120
看到 我可以区分，因为我

943
00:32:31,120 --> 00:32:32,399
只是在移动我的嘴，

944
00:32:32,399 --> 00:32:36,080
嗯，我在放大，他们可以看到它

945
00:32:36,080 --> 00:32:38,080
完美，所以你有冻结的 f，

946
00:32:38,080 --> 00:32:39,760
然后你有整体

947
00:32:39,760 --> 00:32:42,240
，最后你有目标，所以 g

948
00:32:42,240 --> 00:32:43,360
在

949
00:32:43,360 --> 00:32:45,760
这里 第一个 b 被定位，并且

950
00:32:45,760 --> 00:32:48,000
这个特定设置

951
00:32:48,000 --> 00:32:50,320
中的所有补丁都是安全的，除了大厅，如果

952
00:32:50,320 --> 00:32:51,279
代理

953
00:32:51,279 --> 00:32:54,640
去 h，它会得到负奖励

954
00:32:54,640 --> 00:32:57,039


955
00:32:57,039 --> 00:32:57,679


956
00:32:57,679 --> 00:32:59,519


957
00:32:59,519 --> 00:33:01,519


958
00:33:01,519 --> 00:33:03,200
到达fr  isbee

959
00:33:03,200 --> 00:33:07,279
以尽可能少的步骤在 um 中定位

960
00:33:07,279 --> 00:33:08,000


961
00:33:08,000 --> 00:33:09,840
，你可以通过

962
00:33:09,840 --> 00:33:11,679
执行 uh 四种不同类型的

963
00:33:11,679 --> 00:33:12,799
动作，所以要么

964
00:33:12,799 --> 00:33:15,840
从左向右向下或向上，

965
00:33:15,840 --> 00:33:17,840
并且允许代理继续

966
00:33:17,840 --> 00:33:18,960
在冰冻的湖中移动

967
00:33:18,960 --> 00:33:21,279
多次重访，这样你就可以

968
00:33:21,279 --> 00:33:22,720
回到开始位置，

969
00:33:22,720 --> 00:33:24,480
在其他地方也有类似的情况，

970
00:33:24,480 --> 00:33:26,960
但每一集都会在

971
00:33:26,960 --> 00:33:27,919
到达

972
00:33:27,919 --> 00:33:31,600
大厅或目标位置时结束，

973
00:33:31,600 --> 00:33:34,000
这些位置会根据我们在模拟中的设置而有所不同，

974
00:33:34,000 --> 00:33:35,120


975
00:33:35,120 --> 00:33:38,320
所以

976
00:33:38,320 --> 00:33:40,720
在 一个设置，嗯整体的位置是

977
00:33:40,720 --> 00:33:41,440


978
00:33:41,440 --> 00:33:43,840
八，目标是六，另一个

979
00:33:43,840 --> 00:33:45,679
设置整体的位置

980
00:33:45,679 --> 00:33:49,279
是六，

981
00:33:49,279 --> 00:33:51,919
嗯，目标是八，目标

982
00:33:51,919 --> 00:33:53,679
就像我说的

983
00:33:53,679 --> 00:33:56,000
那样理想地以尽可能少的步骤达到目标 可能

984
00:33:56,000 --> 00:33:57,440
同时避免整体，因为这

985
00:33:57,440 --> 00:33:59,120
将结束这一集，

986
00:33:59,120 --> 00:34:00,880
呃，如果它达到目标，它会获得

987
00:34:00,880 --> 00:34:02,559
100 的正奖励，

988
00:34:02,559 --> 00:34:05,600
否则

989
00:34:05,600 --> 00:34:07,840
是负的，这里要注意的关键是这个

990
00:34:07,840 --> 00:34:09,760
查询指标实际上允许我们 y

991
00:34:09,760 --> 00:34:11,280
将主动推理

992
00:34:11,280 --> 00:34:12,960
算法与强化学习

993
00:34:12,960 --> 00:34:14,879
算法进行比较，但

994
00:34:14,879 --> 00:34:16,639
对于主动推理

995
00:34:16,639 --> 00:34:18,839
算法来说，

996
00:34:18,839 --> 00:34:22,000
将奖励功能作为开始并不重要，

997
00:34:22,000 --> 00:34:24,000
因为它仍然可以四处移动

998
00:34:24,000 --> 00:34:26,639
并仅使用信息游戏选项卡，

999
00:34:26,639 --> 00:34:28,879
因此没有外在 价值

1000
00:34:28,879 --> 00:34:30,239
组件

1001
00:34:30,239 --> 00:34:33,199
嗯，这很有趣，

1002
00:34:33,199 --> 00:34:35,040
因为我们将在同化中看到它的后果

1003
00:34:35,040 --> 00:34:36,960


1004
00:34:36,960 --> 00:34:39,599
，对于这个特定的设置，我们

1005
00:34:39,599 --> 00:34:41,040
将每集的最大时间步数限制

1006
00:34:41,040 --> 00:34:42,320


1007
00:34:42,320 --> 00:34:45,679
为 15。

1008
00:34:45,679 --> 00:34:47,918
好吧，我要做的是首先 讨论

1009
00:34:47,918 --> 00:34:49,199
我们用于主动推理公式的生殖器模型

1010
00:34:49,199 --> 00:34:51,359


1011
00:34:51,359 --> 00:34:53,440
嗯，所以您在幻灯片上看到的

1012
00:34:53,440 --> 00:34:55,359


1013
00:34:55,359 --> 00:34:57,520
是主动模型和示例模型的图形表示，

1014
00:34:57,520 --> 00:34:59,280
因此该模型包含四个动作

1015
00:34:59,280 --> 00:35:00,800
状态，

1016
00:35:00,800 --> 00:35:03,920
上下左右以及

1017
00:35:03,920 --> 00:35:06,400
这些 控制在隐藏状态之间转换的能力

1018
00:35:06,400 --> 00:35:08,240


1019
00:35:08,240 --> 00:35:11,359
uh location fact 例如，如果

1020
00:35:11,359 --> 00:35:12,240
你在位置

1021
00:35:12,240 --> 00:35:14,960
一并且你采取了正确的行动，那么你

1022
00:35:14,960 --> 00:35:15,760
会 我最终

1023
00:35:15,760 --> 00:35:20,000
在位置二，或者如果你在位置

1024
00:35:20,000 --> 00:35:22,480
五并且你采取了向上的动作，

1025
00:35:22,480 --> 00:35:24,240
你最终也会在位置

1026
00:35:24,240 --> 00:35:27,920
二，

1027
00:35:27,920 --> 00:35:30,560
在这个特定的设置中，

1028
00:35:30,560 --> 00:35:31,200
位置

1029
00:35:31,200 --> 00:35:33,040
六和八都是吸收状态，

1030
00:35:33,040 --> 00:35:34,320
因为如果你记得的话

1031
00:35:34,320 --> 00:35:36,480
一旦特工到达那个位置，

1032
00:35:36,480 --> 00:35:37,680
他们就无法

1033
00:35:37,680 --> 00:35:40,640
离开，所以那是情节结束的时候

1034
00:35:40,640 --> 00:35:42,720
，如果特工

1035
00:35:42,720 --> 00:35:44,480
在这个特定的迷宫中做出了不太可能的移动，例如，如果

1036
00:35:44,480 --> 00:35:45,040
它

1037
00:35:45,040 --> 00:35:47,200
试图从位置一到左，它

1038
00:35:47,200 --> 00:35:50,000
就会停留 在那个位置，它不会

1039
00:35:50,000 --> 00:35:52,320
在这个特定的 um 性别模型中移动，所以

1040
00:35:52,320 --> 00:35:53,920
我只是在查看隐藏状态，

1041
00:35:53,920 --> 00:35:54,640
现在

1042
00:35:54,640 --> 00:35:56,960
我们在这两个因素之间有一个长期的 um 转移产物

1043
00:35:56,960 --> 00:35:58,640
，所以我们

1044
00:35:58,640 --> 00:36:00,720
在这里有位置和上下文，

1045
00:36:00,720 --> 00:36:03,359
嗯，上下文不能被改变

1046
00:36:03,359 --> 00:36:04,000


1047
00:36:04,000 --> 00:36:05,680
我们拥有的代理 嗯，因为

1048
00:36:05,680 --> 00:36:07,040
这是由环境决定的

1049
00:36:07,040 --> 00:36:08,160
，

1050
00:36:08,160 --> 00:36:10,079
这决定了整个位置的目标在哪里，

1051
00:36:10,079 --> 00:36:12,800
而位置

1052
00:36:12,800 --> 00:36:14,480
是代理可以控制的东西

1053
00:36:14,480 --> 00:36:16,160
，这就是我们可以采取行动的地方

1054
00:36:16,160 --> 00:36:16,800


1055
00:36:16,800 --> 00:36:20,720
关于这个 um 的状态，所以我们有

1056
00:36:20,720 --> 00:36:21,839
两个

1057
00:36:21,839 --> 00:36:25,599
上下文，第一个是目标在

1058
00:36:25,599 --> 00:36:28,560
您的位置 8 和走廊在

1059
00:36:28,560 --> 00:36:29,839
位置 6

1060
00:36:29,839 --> 00:36:32,079
的位置，第二个上下文是

1061
00:36:32,079 --> 00:36:33,280
目标在位置

1062
00:36:33,280 --> 00:36:37,119
6 的位置，大厅在位置 8

1063
00:36:37,119 --> 00:36:41,119
嗯 在每个时间点，智能体将

1064
00:36:41,119 --> 00:36:44,560
观察到两个结果，一个是它

1065
00:36:44,560 --> 00:36:46,800
自己在这个特定迷宫中的位置，第二个是智能体

1066
00:36:46,800 --> 00:36:48,960
将获得的分数，

1067
00:36:48,960 --> 00:36:50,400


1068
00:36:50,400 --> 00:36:52,560
网格位置的

1069
00:36:52,560 --> 00:36:54,720
可能性完全由智能体的位置决定

1070
00:36:54,720 --> 00:36:56,079


1071
00:36:56,079 --> 00:37:01,440
， 分数 um

1072
00:37:01,599 --> 00:37:03,920
由位置和上下文决定，

1073
00:37:03,920 --> 00:37:04,960
因此

1074
00:37:04,960 --> 00:37:08,960
如果代理在位置 6

1075
00:37:08,960 --> 00:37:11,680
并且它在上下文 2 中，那么它将

1076
00:37:11,680 --> 00:37:13,119
获得正奖励，

1077
00:37:13,119 --> 00:37:15,119
否则它将重新获得负

1078
00:37:15,119 --> 00:37:16,320
奖励或相互奖励，

1079
00:37:16,320 --> 00:37:20,320
具体取决于它在哪里 um

1080
00:37:20,320 --> 00:37:22,640
基于 um 试图

1081
00:37:22,640 --> 00:37:24,079
与强化学习进行比较，

1082
00:37:24,079 --> 00:37:25,839
我们在这里所做的是，我们

1083
00:37:25,839 --> 00:37:28,560
正在引入 uh 主要偏好

1084
00:37:28,560 --> 00:37:29,119
，

1085
00:37:29,119 --> 00:37:32,640
其中代理具有加上四个 um 为

1086
00:37:32,640 --> 00:37:36,320
um 正奖励 neg  ative 四四

1087
00:37:36,320 --> 00:37:38,720
抱歉 减去四 负奖励

1088
00:37:38,720 --> 00:37:39,760
否则它

1089
00:37:39,760 --> 00:37:42,240
在第一阶段它期望自己

1090
00:37:42,240 --> 00:37:43,839
处于

1091
00:37:43,839 --> 00:37:46,880
第一个位置

1092
00:37:47,440 --> 00:37:50,560
所以我们将这个特定的

1093
00:37:50,560 --> 00:37:52,480
性别模型和主动推理

1094
00:37:52,480 --> 00:37:53,760
代理与

1095
00:37:53,760 --> 00:37:55,760
两个强化学习算法进行比较

1096
00:37:55,760 --> 00:37:58,320
所以第一个是 q 使用

1097
00:37:58,320 --> 00:38:00,400
epsilon 贪婪探索进行学习

1098
00:38:00,400 --> 00:38:02,079
，第二个是

1099
00:38:02,079 --> 00:38:03,839
基于贝叶斯模型的强化学习

1100
00:38:03,839 --> 00:38:05,599
算法，使用标准汤普森

1101
00:38:05,599 --> 00:38:06,720
采样

1102
00:38:06,720 --> 00:38:08,560
和汤普森采样在这里是合适的

1103
00:38:08,560 --> 00:38:10,320
程序，因为它需要

1104
00:38:10,320 --> 00:38:12,560
优化双目标

1105
00:38:12,560 --> 00:38:15,920
奖励最大化和信息增益

1106
00:38:15,920 --> 00:38:17,440
，这是通过具有此分布来实现的

1107
00:38:17,440 --> 00:38:19,440
在一个特定的函数

1108
00:38:19,440 --> 00:38:20,960
上，我们

1109
00:38:20,960 --> 00:38:22,720
只通过先验对其进行先验

1110
00:38:22,720 --> 00:38:26,879
分布，我们从

1111
00:38:27,119 --> 00:38:32,320
好的 um 中采样，

1112
00:38:32,320 --> 00:38:35,760
所以对于两个 q 衬里算法，我们

1113
00:38:35,760 --> 00:38:38,960
有两个 epsilon 贪婪 uh 参数

1114
00:38:38,960 --> 00:38:41,680
，所以一个固定探索设置

1115
00:38:41,680 --> 00:38:42,880
为 0.1

1116
00:38:42,880 --> 00:38:44,240
，然后 另一个我们

1117
00:38:44,240 --> 00:38:46,400
从一个开始的衰败探索

1118
00:38:46,400 --> 00:38:49,920
并且衰减到零，

1119
00:38:50,880 --> 00:38:54,000
所以首先我们评估了嗯，这个，嗯，

1120
00:38:54,000 --> 00:38:55,440
代理如何在奖励不变的静止环境中交互，

1121
00:38:55,440 --> 00:38:56,320


1122
00:38:56,320 --> 00:38:58,960
嗯，

1123
00:38:58,960 --> 00:39:00,800
目标位置总是在 6，

1124
00:39:00,800 --> 00:39:01,760
整个位置

1125
00:39:01,760 --> 00:39:04,240
总是那个年龄，然后我们评估

1126
00:39:04,240 --> 00:39:06,000
代理的性能 嗯

1127
00:39:06,000 --> 00:39:07,599
，从这里要带走的关键是

1128
00:39:07,599 --> 00:39:08,240


1129
00:39:08,240 --> 00:39:09,920
贝叶斯 rl 和主动推理

1130
00:39:09,920 --> 00:39:11,680
代理都能够

1131
00:39:11,680 --> 00:39:14,400
快速了解奖励位置

1132
00:39:14,400 --> 00:39:16,720
的位置，并最大化它，这

1133
00:39:16,720 --> 00:39:19,119
就是嗯，这就是性能是一致

1134
00:39:19,119 --> 00:39:21,119
的 通过比较我们看到的非常严格的

1135
00:39:21,119 --> 00:39:23,440
置信界限，

1136
00:39:23,440 --> 00:39:26,960
q 个学习代理是

1137
00:39:26,960 --> 00:39:30,480
嗯，所以对于我们有固定

1138
00:39:30,480 --> 00:39:31,680
探索的

1139
00:39:31,680 --> 00:39:34,640
那个来说，它相当不错，并且

1140
00:39:34,640 --> 00:39:36,240
能够降落在奖励所在的

1141
00:39:36,240 --> 00:39:38,480
位置，但是有一些

1142
00:39:38,480 --> 00:39:40,320
偏差，表示为

1143
00:39:40,320 --> 00:39:43,359
选择一个随机动作的百分之十，

1144
00:39:43,359 --> 00:39:46,320
而我们有 epsilon 的 q 学习

1145
00:39:46,320 --> 00:39:48,640
等于 1 以获得零

1146
00:39:48,640 --> 00:39:51,440
嗯，性能不是最好的

1147
00:39:51,440 --> 00:39:53,119
，对于空模型

1148
00:39:53,119 --> 00:39:55,520
这里没有奖励的主动推理

1149
00:39:55,520 --> 00:39:57,680
代理随机进入大厅并

1150
00:39:57,680 --> 00:39:59,040
随机进入目标

1151
00:39:59,040 --> 00:40:02,560
50 但要注意的关键

1152
00:40:02,560 --> 00:40:04,160
是，除了非模型之外，

1153
00:40:04,160 --> 00:40:07,040
所有模型都做得相当好并且

1154
00:40:07,040 --> 00:40:07,760
似乎

1155
00:40:07,760 --> 00:40:09,760
在静止的环境中表现还不错，

1156
00:40:09,760 --> 00:40:10,960


1157
00:40:10,960 --> 00:40:12,720
所以接下来我不能

1158
00:40:12,720 --> 00:40:14,319
就这些实验说明

1159
00:40:14,319 --> 00:40:14,960
一点，因为

1160
00:40:14,960 --> 00:40:16,640
它们也确实与传统的 rl 有点不同，

1161
00:40:16,640 --> 00:40:19,280


1162
00:40:19,280 --> 00:40:22,480
这很好，所以基本上

1163
00:40:22,480 --> 00:40:24,000
就像通常在 ro 中发生的事情是 您

1164
00:40:24,000 --> 00:40:25,760
在

1165
00:40:25,760 --> 00:40:27,040
训练时间和测试时间

1166
00:40:27,040 --> 00:40:28,400
性能之间存在这种歧义，并且您通常

1167
00:40:28,400 --> 00:40:29,440
尤其是在诸如 q 学习之类的事情中，例如

1168
00:40:29,440 --> 00:40:31,280
您只是在 q 函数上达到了最大值，

1169
00:40:31,280 --> 00:40:32,640
或者您有一个

1170
00:40:32,640 --> 00:40:34,800
策略试图对 q 函数进行一些搜索，

1171
00:40:34,800 --> 00:40:35,680


1172
00:40:35,680 --> 00:40:39,440
并且 就像给定一个状态一样接受那个最大值，

1173
00:40:39,440 --> 00:40:40,720
但是有一种人为的

1174
00:40:40,720 --> 00:40:42,480
区别，

1175
00:40:42,480 --> 00:40:44,960
很明显，当你获取数据时，

1176
00:40:44,960 --> 00:40:45,920
你在你所处的

1177
00:40:45,920 --> 00:40:46,960
环境中犯了错误 与

1178
00:40:46,960 --> 00:40:48,640
真实环境相结合，因此为了

1179
00:40:48,640 --> 00:40:49,280


1180
00:40:49,280 --> 00:40:51,280
在此处和主动推理之间进行公平比较，在这种情况

1181
00:40:51,280 --> 00:40:52,960
下，您

1182
00:40:52,960 --> 00:40:54,319
在训练

1183
00:40:54,319 --> 00:40:55,760
时间和测试时间之间没有真正的区别，这一切都

1184
00:40:55,760 --> 00:40:58,240
只是交互，这就是

1185
00:40:58,240 --> 00:40:59,599
为什么 q 学习代理

1186
00:40:59,599 --> 00:41:01,520
特别 当 epsilon 被固定为说

1187
00:41:01,520 --> 00:41:03,760
0.1 永远不会达到最佳策略时，

1188
00:41:03,760 --> 00:41:04,720
仅仅是因为

1189
00:41:04,720 --> 00:41:07,040
我们也有 0.1 的概率采取

1190
00:41:07,040 --> 00:41:08,000
随机行动，

1191
00:41:08,000 --> 00:41:09,280
所以我们不会

1192
00:41:09,280 --> 00:41:11,359
像你有时在训练和测试时间之间使用正常的 rl 那样做出这种区分

1193
00:41:11,359 --> 00:41:11,839


1194
00:41:11,839 --> 00:41:13,520
，我们都在说

1195
00:41:13,520 --> 00:41:14,960
训练和测试一些基本上是

1196
00:41:14,960 --> 00:41:15,520
一回事，

1197
00:41:15,520 --> 00:41:17,520
所以不管你选择

1198
00:41:17,520 --> 00:41:19,280
与世界互动，你应该如何被

1199
00:41:19,280 --> 00:41:20,000


1200
00:41:20,000 --> 00:41:22,319
评估，这就是为什么最初

1201
00:41:22,319 --> 00:41:23,359
看着这些女孩你可能会坚持

1202
00:41:23,359 --> 00:41:24,880
下去，为什么 q learning 不能解决这个问题

1203
00:41:24,880 --> 00:41:25,359
但是，

1204
00:41:25,359 --> 00:41:27,599
嗯，是的，这只是为了澄清一些事情，

1205
00:41:27,599 --> 00:41:28,800
如果你更熟悉说

1206
00:41:28,800 --> 00:41:30,400
一些像 dprl

1207
00:41:30,400 --> 00:41:34,640
呃实验程序

1208
00:41:34,640 --> 00:41:38,640
完美谢谢你好吧

1209
00:41:38,640 --> 00:41:41,200
从那开始

1210
00:41:41,200 --> 00:41:41,920
，所以我们

1211
00:41:41,920 --> 00:41:43,520
稍微改变了环境，以便

1212
00:41:43,520 --> 00:41:45,200
更难

1213
00:41:45,200 --> 00:41:46,480
看出当我们在每几集之后开始进行重定位时

1214
00:41:46,480 --> 00:41:48,480
，贝叶斯和主动推理

1215
00:41:48,480 --> 00:41:49,839
代理是否可能会遇到困难，

1216
00:41:49,839 --> 00:41:52,240


1217
00:41:52,240 --> 00:41:53,680


1218
00:41:53,680 --> 00:41:56,400
所以特别是我们交换了

1219
00:41:56,400 --> 00:41:58,079
目标 以及

1220
00:41:58,079 --> 00:42:01,119
在时间点 21 的时间点

1221
00:42:01,119 --> 00:42:04,319
121 141

1222
00:42:04,319 --> 00:42:08,319
251 和 451 的整个位置，因此您可以看到，

1223
00:42:08,319 --> 00:42:10,319
在这些图中显示了灰

1224
00:42:10,319 --> 00:42:12,000
线的线，因此这些点的

1225
00:42:12,000 --> 00:42:15,200
位置都被翻转了，就像第一个的

1226
00:42:15,200 --> 00:42:16,720
固定设置一样

1227
00:42:16,720 --> 00:42:19,440
20 次试验所有亚洲人似乎都

1228
00:42:19,440 --> 00:42:20,000
在

1229
00:42:20,000 --> 00:42:22,480
按照您的预期进行，所以贝叶斯

1230
00:42:22,480 --> 00:42:24,240
rl 和主动推理代理都

1231
00:42:24,240 --> 00:42:24,640
做得

1232
00:42:24,640 --> 00:42:27,760
很好

1233
00:42:27,760 --> 00:42:30,800


1234
00:42:30,800 --> 00:42:32,000


1235
00:42:32,000 --> 00:42:34,960


1236
00:42:34,960 --> 00:42:35,680
随着

1237
00:42:35,680 --> 00:42:38,400
衰变的探索正在像

1238
00:42:38,400 --> 00:42:39,520
以前一样做，

1239
00:42:39,520 --> 00:42:42,640
但是当你把它翻转过来时

1240
00:42:42,640 --> 00:42:44,240
，目标位置会

1241
00:42:44,240 --> 00:42:47,520
改变，你注意到的是

1242
00:42:47,520 --> 00:42:48,640


1243
00:42:48,640 --> 00:42:52,079
贝叶斯rl a  gent 奖励的数量

1244
00:42:52,079 --> 00:42:53,760
或它获得的分数

1245
00:42:53,760 --> 00:42:56,800
非常低，然后我们看到一个阶段

1246
00:42:56,800 --> 00:42:57,119
，

1247
00:42:57,119 --> 00:43:00,000
它然后转换并

1248
00:43:00,000 --> 00:43:02,160
再次成为最佳策略

1249
00:43:02,160 --> 00:43:04,560
，与主动推理

1250
00:43:04,560 --> 00:43:05,599
代理相比，

1251
00:43:05,599 --> 00:43:08,720
它在第一次尝试后立即

1252
00:43:08,720 --> 00:43:10,560
执行 不正确能够

1253
00:43:10,560 --> 00:43:12,960
切换到积极和抱歉

1254
00:43:12,960 --> 00:43:14,640
适当的政策

1255
00:43:14,640 --> 00:43:17,680
，其原因是对于这些呃

1256
00:43:17,680 --> 00:43:19,440
rl 设置，我们将其

1257
00:43:19,440 --> 00:43:20,880
视为一个学习问题，

1258
00:43:20,880 --> 00:43:22,480
您需要首先进行反向并

1259
00:43:22,480 --> 00:43:24,480
了解奖励位置的位置

1260
00:43:24,480 --> 00:43:26,720
和 然后我们学习新的奖励

1261
00:43:26,720 --> 00:43:28,720
位置，所以你会看到

1262
00:43:28,720 --> 00:43:31,280
q 学习 uh 对于不同的

1263
00:43:31,280 --> 00:43:33,200
epsilon 贪心参数化

1264
00:43:33,200 --> 00:43:34,720
和贝叶斯 rl，我们

1265
00:43:34,720 --> 00:43:36,160
看到的是一致的，

1266
00:43:36,160 --> 00:43:38,160
而对于主动推理代理，

1267
00:43:38,160 --> 00:43:39,599
因为我们将其视为

1268
00:43:39,599 --> 00:43:41,599
作为推理问题的规划，其中

1269
00:43:41,599 --> 00:43:42,960
来自先前状态的后验被

1270
00:43:42,960 --> 00:43:45,359
移动到奖品并

1271
00:43:45,359 --> 00:43:46,160
作为先验移动

1272
00:43:46,160 --> 00:43:49,200
并且代理能够立即

1273
00:43:49,200 --> 00:43:52,160
嗯意识到

1274
00:43:52,160 --> 00:43:52,960


1275
00:43:52,960 --> 00:43:55,200
遵循这个时间步骤的当前策略是一种适当性

1276
00:43:55,200 --> 00:43:57,040
，它是它对下

1277
00:43:57,040 --> 00:44:00,480
一个 um 到另一个的策略

1278
00:44:00,480 --> 00:44:02,480


1279
00:44:02,480 --> 00:44:04,000


1280
00:44:04,000 --> 00:44:05,599


1281
00:44:05,599 --> 00:44:08,160
或者整个位置

1282
00:44:08,160 --> 00:44:09,839
都

1283
00:44:09,839 --> 00:44:11,839
还可以，嗯，菲尔，你想添加

1284
00:44:11,839 --> 00:44:14,799
一些东西还是

1285
00:44:15,440 --> 00:44:17,440
不，我认为同样的一点也适用于

1286
00:44:17,440 --> 00:44:18,480
这个

1287
00:44:18,480 --> 00:44:22,160
是肯定的，嗯，

1288
00:44:22,160 --> 00:44:23,760
所以只是总结这两个

1289
00:44:23,760 --> 00:44:25,760
比较，

1290
00:44:25,760 --> 00:44:29,440
我用非焊机固定设置

1291
00:44:29,440 --> 00:44:32,319
嗯所有 包括

1292
00:44:32,319 --> 00:44:34,160
贝叶斯 rl 和 q-learning 在内的代理类型 um 将是

1293
00:44:34,160 --> 00:44:36,400
合理的使用框架，而

1294
00:44:36,400 --> 00:44:39,280
具有非平稳随机设置的 uh

1295
00:44:39,280 --> 00:44:40,480


1296
00:44:40,480 --> 00:44:42,240
具有主动推理代理可能

1297
00:44:42,240 --> 00:44:44,079
是处理不断变化的

1298
00:44:44,079 --> 00:44:45,200
动态

1299
00:44:45,200 --> 00:44:47,359
um 的适当方式，但关键警告是 是

1300
00:44:47,359 --> 00:44:48,480
你可以

1301
00:44:48,480 --> 00:44:50,480


1302
00:44:50,480 --> 00:44:53,119
在贝叶斯 rl 或 q

1303
00:44:53,119 --> 00:44:55,599
学习或 rl 框架中引入更多额外的复杂性，

1304
00:44:55,599 --> 00:44:56,560
以允许

1305
00:44:56,560 --> 00:44:58,800
呃有一种处理不确定性的方法，但

1306
00:44:58,800 --> 00:44:59,760
它不会 这不是

1307
00:44:59,760 --> 00:45:02,240
一种自然的添加方式，你

1308
00:45:02,240 --> 00:45:04,960
必须以特定的方式增加

1309
00:45:04,960 --> 00:45:06,880
函数或算法

1310
00:45:06,880 --> 00:45:09,359
来证明它是正确的

1311
00:45:09,359 --> 00:45:12,319


1312
00:45:12,319 --> 00:45:15,040


1313
00:45:15,040 --> 00:45:16,960
使用主动

1314
00:45:16,960 --> 00:45:19,200
推理，嗯，

1315
00:45:19,200 --> 00:45:22,079
当你

1316
00:45:22,079 --> 00:45:22,880
不了解世界，

1317
00:45:22,880 --> 00:45:25,440
或者你对

1318
00:45:25,440 --> 00:45:27,040
可以做的事情类型没有偏好时，

1319
00:45:27,040 --> 00:45:28,640
因为正如我们看到的主动

1320
00:45:28,640 --> 00:45:30,240
推理模型，空模型只是在

1321
00:45:30,240 --> 00:45:32,319
探索 它并没有真正做

1322
00:45:32,319 --> 00:45:34,720
太多，这让我们

1323
00:45:34,720 --> 00:45:36,319
想到了我

1324
00:45:36,319 --> 00:45:38,160
在演示开始时介绍的初始点之一，那

1325
00:45:38,160 --> 00:45:39,599
就是在一个积极的影响

1326
00:45:39,599 --> 00:45:41,200
框架内，我们并不真正关心

1327
00:45:41,200 --> 00:45:42,560


1328
00:45:42,560 --> 00:45:45,920
我们可以学习的奖励功能 我们可以学习 基于

1329
00:45:45,920 --> 00:45:48,640
与环境的一些交互

1330
00:45:48,640 --> 00:45:52,240
，嗯

1331
00:45:52,240 --> 00:45:54,480
，我们所做的是我们进行了一些

1332
00:45:54,480 --> 00:45:56,160
不同的模拟，以查看

1333
00:45:56,160 --> 00:45:57,920
主动推理代理如何

1334
00:45:57,920 --> 00:45:59,520
在

1335
00:45:59,520 --> 00:46:02,079
没有先验 p 的情况下选择不同类型的策略 参考资料

1336
00:46:02,079 --> 00:46:04,560
，为此我们做了三个不同的

1337
00:46:04,560 --> 00:46:05,280
实验

1338
00:46:05,280 --> 00:46:08,160
，我们允许随着时间的推移学习其中一个或

1339
00:46:08,160 --> 00:46:09,040
可能性

1340
00:46:09,040 --> 00:46:11,359
和所有结果偏好，

1341
00:46:11,359 --> 00:46:12,240


1342
00:46:12,240 --> 00:46:15,440
并观察

1343
00:46:15,440 --> 00:46:16,560
当

1344
00:46:16,560 --> 00:46:19,680
这种学习对不同的

1345
00:46:19,680 --> 00:46:22,640
嗯偏好发生时代理如何交互，以及

1346
00:46:22,640 --> 00:46:24,160
我们的方式 是不是使用

1347
00:46:24,160 --> 00:46:26,960
了共轭模型，因为

1348
00:46:26,960 --> 00:46:28,800
这些是离散状态模型，我们

1349
00:46:28,800 --> 00:46:30,800
在模型参数上有分类分布，我们

1350
00:46:30,800 --> 00:46:31,839


1351
00:46:31,839 --> 00:46:33,760


1352
00:46:33,760 --> 00:46:34,880
在顶部引入方向分布

1353
00:46:34,880 --> 00:46:36,960
作为混合价格并学习那些

1354
00:46:36,960 --> 00:46:38,319
超价格，

1355
00:46:38,319 --> 00:46:41,119
我们可以深入了解 如果

1356
00:46:41,119 --> 00:46:42,800
有人有任何问题，但

1357
00:46:42,800 --> 00:46:46,400
只是为了简单起见，嗯，假设

1358
00:46:46,400 --> 00:46:48,800
所有的狄利克雷分布都

1359
00:46:48,800 --> 00:46:50,880
设置为完全平坦，

1360
00:46:50,880 --> 00:46:54,079
并且代理都有机会

1361
00:46:54,079 --> 00:46:54,800


1362
00:46:54,800 --> 00:46:57,359
根据他们进行的不同交互进行着陆，

1363
00:46:57,359 --> 00:46:58,400
所以第一组

1364
00:46:58,400 --> 00:47:00,160
实验就像模拟 我们

1365
00:47:00,160 --> 00:47:02,319
运行的目的是了解

1366
00:47:02,319 --> 00:47:04,640
嗯代理将如何减少

1367
00:47:04,640 --> 00:47:05,520
对所处

1368
00:47:05,520 --> 00:47:07,280
环境的不确定性 互动，所以

1369
00:47:07,280 --> 00:47:09,040
如果它不

1370
00:47:09,040 --> 00:47:12,319
知道结冰的湖

1371
00:47:12,319 --> 00:47:14,720
，它将如何交互或探索那个

1372
00:47:14,720 --> 00:47:15,599
湖，

1373
00:47:15,599 --> 00:47:18,880
嗯，我们看到的是，

1374
00:47:18,880 --> 00:47:21,920
在这个特定的例子中，代理

1375
00:47:21,920 --> 00:47:23,680
只是对探索感兴趣，

1376
00:47:23,680 --> 00:47:25,920
并不特别关心

1377
00:47:25,920 --> 00:47:28,559
目标或整个位置在哪里

1378
00:47:28,559 --> 00:47:29,040


1379
00:47:29,040 --> 00:47:32,079
，这在我们看到

1380
00:47:32,079 --> 00:47:35,119
的一系列不同的探索 um

1381
00:47:35,119 --> 00:47:36,800
轨迹中突出显示，所以第

1382
00:47:36,800 --> 00:47:38,960
一个是代理落在

1383
00:47:38,960 --> 00:47:39,359
大厅

1384
00:47:39,359 --> 00:47:42,400
um 的地方，然后是另一个它

1385
00:47:42,400 --> 00:47:43,359
四处走动

1386
00:47:43,359 --> 00:47:46,079
并最终到达的地方

1387
00:47:46,079 --> 00:47:48,160
第二集的第二集在目标位置

1388
00:47:48,160 --> 00:47:50,079
，在某个地方，它只是通过来回来回结束这一

1389
00:47:50,079 --> 00:47:51,200
集，

1390
00:47:51,200 --> 00:47:52,960
所以这

1391
00:47:52,960 --> 00:47:54,800
只是地板探索，

1392
00:47:54,800 --> 00:47:58,000
没有更多的东西可以听到，我们运行

1393
00:47:58,000 --> 00:48:01,440
的下一组分析或

1394
00:48:01,440 --> 00:48:03,520
模拟是看看什么

1395
00:48:03,520 --> 00:48:05,440
如果代理了解这个

1396
00:48:05,440 --> 00:48:07,599
世界，但对他们期望自己所处的结果类型没有偏好，那么就会发生这种情况

1397
00:48:07,599 --> 00:48:08,480


1398
00:48:08,480 --> 00:48:12,160
，为此，

1399
00:48:12,160 --> 00:48:15,920
我们运行了多个不同的模拟，

1400
00:48:15,920 --> 00:48:19,200
并看到在没有

1401
00:48:19,200 --> 00:48:22,240
任何 某种偏好，

1402
00:48:22,240 --> 00:48:24,079
嗯，如果首先遇到它，整体实际上会变得

1403
00:48:24,079 --> 00:48:25,599
非常有吸引力

1404
00:48:25,599 --> 00:48:28,480
，所以我们看到，

1405
00:48:28,480 --> 00:48:29,440
在

1406
00:48:29,440 --> 00:48:31,680
第一个图中，代理学会了

1407
00:48:31,680 --> 00:48:34,000
更喜欢躲在大厅里，

1408
00:48:34,000 --> 00:48:36,160
而在我们看到的第二种类型的试验

1409
00:48:36,160 --> 00:48:38,000
中，代理

1410
00:48:38,000 --> 00:48:41,520
呃展示了 实际

1411
00:48:41,520 --> 00:48:43,359
去目标位置的偏好，所以这

1412
00:48:43,359 --> 00:48:45,200
完全取决于代理最初暴露的

1413
00:48:45,200 --> 00:48:47,680
实例化或刺激类型，

1414
00:48:47,680 --> 00:48:49,119


1415
00:48:49,119 --> 00:48:51,119
这决定了

1416
00:48:51,119 --> 00:48:52,319
它将

1417
00:48:52,319 --> 00:48:55,200
学习的

1418
00:48:56,240 --> 00:48:59,520
偏好类型，然后是我们运行的最后一组

1419
00:48:59,520 --> 00:49:01,520
模拟 只是为了

1420
00:49:01,520 --> 00:49:03,119
检查当我们与认知命令交互时会发生什么

1421
00:49:03,119 --> 00:49:04,640
，

1422
00:49:04,640 --> 00:49:06,640
实际上解决

1423
00:49:06,640 --> 00:49:08,000


1424
00:49:08,000 --> 00:49:09,440
了代理正在

1425
00:49:09,440 --> 00:49:11,200
与之交互的环境的不确定性，特别是给定状态

1426
00:49:11,200 --> 00:49:13,200
的结果之间的可能性映射

1427
00:49:13,200 --> 00:49:14,480


1428
00:49:14,480 --> 00:49:16,880


1429
00:49:16,880 --> 00:49:18,559
与代理期望自己期望的事务状态的不确定性

1430
00:49:18,559 --> 00:49:19,839
进入

1431
00:49:19,839 --> 00:49:22,960
，我们看到的是，如果我们

1432
00:49:22,960 --> 00:49:25,520
允许足够数量的

1433
00:49:25,520 --> 00:49:27,040
试验通过 s

1434
00:49:27,040 --> 00:49:28,640
在这个特定设置中的代理学会了

1435
00:49:28,640 --> 00:49:31,119


1436
00:49:31,119 --> 00:49:34,800
在 x 集之后更喜欢骑抱歉的隐藏洞，但是它对它期望自己的结果类型有一个

1437
00:49:34,800 --> 00:49:36,000
非常明显的

1438
00:49:36,000 --> 00:49:38,960
偏好

1439
00:49:38,960 --> 00:49:39,280


1440
00:49:39,280 --> 00:49:41,119
，这

1441
00:49:41,119 --> 00:49:42,480
取决于特定的

1442
00:49:42,480 --> 00:49:46,400
呃一个抱歉的特定时间点

1443
00:49:46,400 --> 00:49:50,079
所以这把我带到了最后一个

1444
00:49:50,079 --> 00:49:53,839
模拟，所以我要总结一下

1445
00:49:53,839 --> 00:49:57,200
，嗯，在主动推理中，

1446
00:49:57,200 --> 00:49:59,040
我开始演示时

1447
00:49:59,040 --> 00:50:00,640
说它是一种特殊的算法，当我们在贝叶斯中操作时，它给了

1448
00:50:00,640 --> 00:50:01,200
我们

1449
00:50:01,200 --> 00:50:03,599
非常好的东西来考虑

1450
00:50:03,599 --> 00:50:04,559


1451
00:50:04,559 --> 00:50:07,760
或基于信念的

1452
00:50:07,760 --> 00:50:08,960
设置，这

1453
00:50:08,960 --> 00:50:11,680
首先

1454
00:50:11,680 --> 00:50:13,359


1455
00:50:13,359 --> 00:50:15,119
是我们从

1456
00:50:15,119 --> 00:50:17,040
特定的预期自由能

1457
00:50:17,040 --> 00:50:19,440
分解中获得的认知探索和内在动机的主要说明，我们经历了

1458
00:50:19,440 --> 00:50:20,400
第二件事

1459
00:50:20,400 --> 00:50:23,359
，我想强调的是，

1460
00:50:23,359 --> 00:50:25,040
在积极的推理设置下，我们

1461
00:50:25,040 --> 00:50:26,000
没有 必须明确

1462
00:50:26,000 --> 00:50:28,240
指定一个奖励函数，我们

1463
00:50:28,240 --> 00:50:29,119
在最后，

1464
00:50:29,119 --> 00:50:32,079
呃第二组模拟

1465
00:50:32,079 --> 00:50:34,000
中看到代理也可以学习自己的

1466
00:50:34,000 --> 00:50:35,680
奖励 并且更愿意从 rl 设置中变成

1467
00:50:35,680 --> 00:50:37,359
非常违反直觉的东西，

1468
00:50:37,359 --> 00:50:39,839
其中

1469
00:50:39,839 --> 00:50:41,440
来自环境的信号说某事

1470
00:50:41,440 --> 00:50:42,319
不好，但

1471
00:50:42,319 --> 00:50:44,160
代理的内部

1472
00:50:44,160 --> 00:50:46,240
偏好动机允许它然后

1473
00:50:46,240 --> 00:50:48,559
做一些

1474
00:50:48,559 --> 00:50:50,319
与环境期望完全不一致的事情

1475
00:50:50,319 --> 00:50:51,359


1476
00:50:51,359 --> 00:50:53,599
最后因为这种信念 um

1477
00:50:53,599 --> 00:50:55,280
入侵设置不确定性

1478
00:50:55,280 --> 00:50:57,119
是信念更新 um 的自然组成部分，

1479
00:50:57,119 --> 00:51:00,559
因此在

1480
00:51:00,559 --> 00:51:02,319
静态设置中，主动推理

1481
00:51:02,319 --> 00:51:04,000
代理的性能与强化学习代理一样好，

1482
00:51:04,000 --> 00:51:05,200


1483
00:51:05,200 --> 00:51:07,359
但是在非静态设置中，它们的

1484
00:51:07,359 --> 00:51:08,960
表现优于它们的能力

1485
00:51:08,960 --> 00:51:10,480
执行这个计划作为

1486
00:51:10,480 --> 00:51:13,280
推理，我

1487
00:51:13,280 --> 00:51:15,280
认为这不是一个结论性的陈述，

1488
00:51:15,280 --> 00:51:15,760
但

1489
00:51:15,760 --> 00:51:18,240
它是一个很好的方式来开始

1490
00:51:18,240 --> 00:51:19,040


1491
00:51:19,040 --> 00:51:21,280
思考如果你正在扩展主动推理

1492
00:51:21,280 --> 00:51:23,440
代理以在与强化学习相同类型的环境中进行交互

1493
00:51:23,440 --> 00:51:24,160


1494
00:51:24,160 --> 00:51:26,960


1495
00:51:26,960 --> 00:51:27,760
可能

1496
00:51:27,760 --> 00:51:30,079
有点难以解决的代理，因为

1497
00:51:30,079 --> 00:51:30,839
存在一些

1498
00:51:30,839 --> 00:51:32,640
非平稳性或一些奇怪的东西

1499
00:51:32,640 --> 00:51:33,920


1500
00:51:33,920 --> 00:51:34,640
环境中发生的波动

1501
00:51:34,640 --> 00:51:36,240
和主动推理代理

1502
00:51:36,240 --> 00:51:38,240
可能会表现得非常好，如果我们

1503
00:51:38,240 --> 00:51:42,000
有这个计划作为推理设置

1504
00:51:42,079 --> 00:51:44,800
嗯，这让我到了演示的结尾，

1505
00:51:44,800 --> 00:51:46,480
所以我只想感谢

1506
00:51:46,480 --> 00:51:48,240
参与这项工作的每个人

1507
00:51:48,240 --> 00:51:48,559
，

1508
00:51:48,559 --> 00:51:51,520
所谓的汤姆 菲尔和所有

1509
00:51:51,520 --> 00:51:52,640
帮助我思考

1510
00:51:52,640 --> 00:51:55,920
我提出的有趣想法的人，

1511
00:51:55,920 --> 00:51:58,079
以及每个人的聆听，谢谢

1512
00:51:58,079 --> 00:51:59,920
，

1513
00:51:59,920 --> 00:52:04,400
谢谢你精彩的演讲，

1514
00:52:04,400 --> 00:52:07,119
所以你可以取消分享，我们可以

1515
00:52:07,119 --> 00:52:08,000
问一些问题

1516
00:52:08,000 --> 00:52:09,599
，如果有人在看直播

1517
00:52:09,599 --> 00:52:11,359
想问问题 他们

1518
00:52:11,359 --> 00:52:14,400
非常受欢迎，所以也许我会从

1519
00:52:14,400 --> 00:52:16,160


1520
00:52:16,160 --> 00:52:18,000
一个一般性的观点开始，

1521
00:52:18,000 --> 00:52:20,480
看到你

1522
00:52:20,480 --> 00:52:21,280
在

1523
00:52:21,280 --> 00:52:22,880
强化学习和

1524
00:52:22,880 --> 00:52:24,640
主动推理范式

1525
00:52:24,640 --> 00:52:27,040
之间有多么清晰的区别真是太棒了，我只有一个问题是你

1526
00:52:27,040 --> 00:52:27,760


1527
00:52:27,760 --> 00:52:30,319
在案例中提到的 一个时间步长 一个时间

1528
00:52:30,319 --> 00:52:31,599
范围

1529
00:52:31,599 --> 00:52:33,359


1530
00:52:33,359 --> 00:52:34,960
强化学习方法和主动 infe 之间是等价的

1531
00:52:34,960 --> 00:52:36,640


1532
00:52:36,640 --> 00:52:38,640
现在人们正在使用强化

1533
00:52:38,640 --> 00:52:40,400
学习

1534
00:52:40,400 --> 00:52:42,400
在不确定的情况下对未来进行规划，

1535
00:52:42,400 --> 00:52:43,599


1536
00:52:43,599 --> 00:52:46,240
那么他们如何完成

1537
00:52:46,240 --> 00:52:47,839
这些规划

1538
00:52:47,839 --> 00:52:50,400
以及在哪些情况下

1539
00:52:50,400 --> 00:52:51,440
主动推理

1540
00:52:51,440 --> 00:52:53,920
可能能够进入这些 um

1541
00:52:53,920 --> 00:52:54,800
设置并

1542
00:52:54,800 --> 00:52:57,760
可能做得更好，我认为这

1543
00:52:57,760 --> 00:52:58,480
已经 一个问题，

1544
00:52:58,480 --> 00:53:00,400
一个好问题，

1545
00:53:00,400 --> 00:53:02,720
如果我错过了什么，你可以随意加入，但是

1546
00:53:02,720 --> 00:53:04,720
在 url 设置中，据我

1547
00:53:04,720 --> 00:53:07,040
所知，如果他们正在考虑

1548
00:53:07,040 --> 00:53:09,280
大于 1 的时间范围，那么

1549
00:53:09,280 --> 00:53:10,960
他们有分层 rl

1550
00:53:10,960 --> 00:53:13,280
或者他们有选择的地方

1551
00:53:13,280 --> 00:53:14,800
考虑

1552
00:53:14,800 --> 00:53:16,240
大于

1553
00:53:16,240 --> 00:53:18,800
一个的轨迹，允许他们

1554
00:53:18,800 --> 00:53:20,720


1555
00:53:20,720 --> 00:53:24,319
在他们喜欢之前考虑

1556
00:53:24,319 --> 00:53:26,000


1557
00:53:26,000 --> 00:53:27,680


1558
00:53:27,680 --> 00:53:32,800
整个游戏

1559
00:53:32,800 --> 00:53:34,880
序列 还没有真正

1560
00:53:34,880 --> 00:53:36,559
与选项一起工作过，也许

1561
00:53:36,559 --> 00:53:39,119
菲尔你知道吗，嗯，是的，所以选项

1562
00:53:39,119 --> 00:53:41,280
就像

1563
00:53:41,280 --> 00:53:45,119
实现这种多步骤的一种方法 ort of

1564
00:53:45,119 --> 00:53:47,680
um like I guess oversized 几乎就像

1565
00:53:47,680 --> 00:53:49,359
这种归纳偏见一样，

1566
00:53:49,359 --> 00:53:52,000
你会喜欢你的选择将是

1567
00:53:52,000 --> 00:53:54,079
一种连续的块，比如

1568
00:53:54,079 --> 00:53:55,520
不止一个步骤，而

1569
00:53:55,520 --> 00:53:57,280
一种政策就像我

1570
00:53:57,280 --> 00:53:58,720
要迈出一步一样 然后我将拥有

1571
00:53:58,720 --> 00:53:59,359


1572
00:53:59,359 --> 00:54:02,720
我存在的这种下一个状态，但是我

1573
00:54:02,720 --> 00:54:04,240
认为消除歧义的重要之处

1574
00:54:04,240 --> 00:54:06,000
实际上是您可以对模型样式进行某种规划，

1575
00:54:06,000 --> 00:54:07,440
因为

1576
00:54:07,440 --> 00:54:09,119
您可以将主动推理视为一种

1577
00:54:09,119 --> 00:54:10,880
基于模型的模型

1578
00:54:10,880 --> 00:54:13,680
您知道 rl 范式，并且

1579
00:54:13,680 --> 00:54:15,280
确实像在基于模型的情况下

1580
00:54:15,280 --> 00:54:18,400
一样，您可以拥有

1581
00:54:18,400 --> 00:54:20,800
类似于在主动推理中所做的规划方法，

1582
00:54:20,800 --> 00:54:21,440


1583
00:54:21,440 --> 00:54:23,040
但是我想

1584
00:54:23,040 --> 00:54:24,480
这里的关键区别是

1585
00:54:24,480 --> 00:54:27,440
当您在基于模型的 rl 中进行规划时，

1586
00:54:27,440 --> 00:54:27,760
您

1587
00:54:27,760 --> 00:54:29,920
只是在尝试 带你倾向于

1588
00:54:29,920 --> 00:54:30,800
采取某种

1589
00:54:30,800 --> 00:54:32,880
西装，比如一种迷你进化

1590
00:54:32,880 --> 00:54:34,079
方法，通常

1591
00:54:34,079 --> 00:54:35,920
是交叉熵方法，你正在

1592
00:54:35,920 --> 00:54:37,440
做的是

1593
00:54:37,440 --> 00:54:38,640
对你在某个时刻采取的所有行动的这种搜索

1594
00:54:38,640 --> 00:54:40,480


1595
00:54:40,480 --> 00:54:42,319


1596
00:54:42,319 --> 00:54:45,200
在假设 20 步的范围内最大化你的奖励的时间，嗯

1597
00:54:45,200 --> 00:54:46,559
，你可以做一些聪明的事情，你可以

1598
00:54:46,559 --> 00:54:48,880
用 q 值函数 um 终止，

1599
00:54:48,880 --> 00:54:51,280
但从根本上来说，它不会

1600
00:54:51,280 --> 00:54:52,799
让你远离这种

1601
00:54:52,799 --> 00:54:55,359
定义差异，即

1602
00:54:55,359 --> 00:54:57,760
在主动推理中，您正在

1603
00:54:57,760 --> 00:54:59,119
对动作进行这种计划，

1604
00:54:59,119 --> 00:55:00,480
即使它们看起来表面上

1605
00:55:00,480 --> 00:55:02,000
相似，

1606
00:55:02,000 --> 00:55:03,680
基本上就像绘制大量动作

1607
00:55:03,680 --> 00:55:05,280
并查看

1608
00:55:05,280 --> 00:55:06,000


1609
00:55:06,000 --> 00:55:09,119
在主动推理的情况下哪些动作最大化某种效用 所有

1610
00:55:09,119 --> 00:55:10,880
有用的信息 以及

1611
00:55:10,880 --> 00:55:12,559
关于 um 的期望值，

1612
00:55:12,559 --> 00:55:15,760
比如探索，它和

1613
00:55:15,760 --> 00:55:17,839
um 开发都包含在最大化中，

1614
00:55:17,839 --> 00:55:20,000
而在 rl 中，

1615
00:55:20,000 --> 00:55:23,440
有点难以理解，就像

1616
00:55:23,440 --> 00:55:24,799
在经典意义上，我们只是

1617
00:55:24,799 --> 00:55:26,960
试图最大化奖励，但是

1618
00:55:26,960 --> 00:55:28,720
你可以拥有 启发式你说哦，

1619
00:55:28,720 --> 00:55:30,240
但也许我也想最大化

1620
00:55:30,240 --> 00:55:31,200


1621
00:55:31,200 --> 00:55:33,359
模型不确定性的一些概念，你知道它

1622
00:55:33,359 --> 00:55:34,640


1623
00:55:34,640 --> 00:55:37,280
有点难以自然整合 将

1624
00:55:37,280 --> 00:55:39,680
所有这些方法评价为同一件事，

1625
00:55:39,680 --> 00:55:41,680
嗯，因为就像在积极影响的例子中一样，

1626
00:55:41,680 --> 00:55:42,400
因为一切都是一个

1627
00:55:42,400 --> 00:55:43,280
分布

1628
00:55:43,280 --> 00:55:44,720
，如果你愿意，你只需将它放在一个超优先级之上，

1629
00:55:44,720 --> 00:55:46,480
如果你不想处理它，你只需将

1630
00:55:46,480 --> 00:55:47,760


1631
00:55:47,760 --> 00:55:49,920
其整合出来 所以是的，这

1632
00:55:49,920 --> 00:55:50,880
就是我觉得

1633
00:55:50,880 --> 00:55:53,920
有这种差异

1634
00:55:54,000 --> 00:55:57,280
很酷的地方，你认为什么样的设置

1635
00:55:57,280 --> 00:56:00,640
可以像推理那样使用那些类型的呃

1636
00:56:00,640 --> 00:56:03,200
动作和计划作为

1637
00:56:03,200 --> 00:56:04,240
推理，

1638
00:56:04,240 --> 00:56:06,960
比如什么样的数据集或

1639
00:56:06,960 --> 00:56:08,880
问题或 上下文

1640
00:56:08,880 --> 00:56:10,799
是人们目前正在使用另一种类型

1641
00:56:10,799 --> 00:56:12,720
的方法，但是您很高兴看到

1642
00:56:12,720 --> 00:56:17,839
主动推理发挥作用

1643
00:56:18,720 --> 00:56:21,280


1644
00:56:21,280 --> 00:56:23,520


1645
00:56:23,520 --> 00:56:26,640


1646
00:56:26,640 --> 00:56:30,079


1647
00:56:30,079 --> 00:56:31,839
与环境互动是

1648
00:56:31,839 --> 00:56:33,839
通过奖励功能，

1649
00:56:33,839 --> 00:56:36,079
嗯，所以任何你没有真正

1650
00:56:36,079 --> 00:56:37,280
拥有的地方，或者你有

1651
00:56:37,280 --> 00:56:40,640
一个正在改变的环境，

1652
00:56:40,640 --> 00:56:43,440
但我知道这就像一个整体

1653
00:56:43,440 --> 00:56:44,160


1654
00:56:44,160 --> 00:56:46,720
rl 社区中的很多人都

1655
00:56:46,720 --> 00:56:47,599
在研究

1656
00:56:47,599 --> 00:56:50,160
内在动机或内部

1657
00:56:50,160 --> 00:56:51,520
动机，

1658
00:56:51,520 --> 00:56:54,160
所以这些事情确实与

1659
00:56:54,160 --> 00:56:54,559


1660
00:56:54,559 --> 00:56:58,960
积极影响公式重叠，

1661
00:56:58,960 --> 00:57:02,400
但是我认为对我来说

1662
00:57:02,400 --> 00:57:04,640
，积极影响更有趣的方面

1663
00:57:04,640 --> 00:57:06,079
来自于

1664
00:57:06,079 --> 00:57:09,440
你开始时的事实 考虑一下

1665
00:57:09,440 --> 00:57:12,240
生物制剂，如果你正在

1666
00:57:12,240 --> 00:57:13,280
为

1667
00:57:13,280 --> 00:57:16,240
患者或精神分裂症患者建模，你可以

1668
00:57:16,240 --> 00:57:18,079
使用这个贝叶斯框架，你可以

1669
00:57:18,079 --> 00:57:21,760
改变先验，

1670
00:57:21,760 --> 00:57:24,960
看看这个人是如何互动的

1671
00:57:24,960 --> 00:57:27,920
，我想这可能是

1672
00:57:27,920 --> 00:57:28,799
他们的政策

1673
00:57:28,799 --> 00:57:30,079
喜欢 他们评估

1674
00:57:30,079 --> 00:57:32,319
政策的方式被打破了，或者可能只是

1675
00:57:32,319 --> 00:57:35,599
其他部分不同，但

1676
00:57:35,599 --> 00:57:38,720
我认为在 rl 设置中，如果

1677
00:57:38,720 --> 00:57:43,440
你走出

1678
00:57:43,440 --> 00:57:46,480
um 标准游戏模式，就像

1679
00:57:46,480 --> 00:57:47,359
魔术或

1680
00:57:47,359 --> 00:57:49,040
健身房环境一样，开始进入

1681
00:57:49,040 --> 00:57:50,799
那些 哪个更开放，你

1682
00:57:50,799 --> 00:57:52,480
没有任何奖励，那么

1683
00:57:52,480 --> 00:57:54,160
积极的影响可能会

1684
00:57:54,160 --> 00:57:56,240
很有用，但

1685
00:57:56,240 --> 00:57:58,640
我有点犹豫 说它

1686
00:57:58,640 --> 00:57:59,599
是否会

1687
00:57:59,599 --> 00:58:02,640
更好，因为如果你开始

1688
00:58:02,640 --> 00:58:05,040
用各种有趣的组件来增强贝叶斯 rl，

1689
00:58:05,040 --> 00:58:06,839


1690
00:58:06,839 --> 00:58:10,160
那么在一定程度上它会

1691
00:58:10,160 --> 00:58:13,680
在放大的前面活跃

1692
00:58:13,680 --> 00:58:15,760
，这可能是一个有争议的

1693
00:58:15,760 --> 00:58:16,880
点，

1694
00:58:16,880 --> 00:58:20,319
但我认为这取决于

1695
00:58:20,319 --> 00:58:22,079
究竟是什么 你正在整合，

1696
00:58:22,079 --> 00:58:23,920
这就是为什么对于这个特定的

1697
00:58:23,920 --> 00:58:26,000
演示和我们的工作，我们非常

1698
00:58:26,000 --> 00:58:27,599
谨慎地定义

1699
00:58:27,599 --> 00:58:29,599
强化学习的含义，

1700
00:58:29,599 --> 00:58:31,760
即你必须有这个奖励

1701
00:58:31,760 --> 00:58:33,280
函数，你想

1702
00:58:33,280 --> 00:58:35,040
最大化这个奖励函数

1703
00:58:35,040 --> 00:58:37,280
以及你包含在 rl 中的任何东西

1704
00:58:37,280 --> 00:58:39,040


1705
00:58:39,040 --> 00:58:42,000
框架必须像目标必须存在的对象一样，

1706
00:58:42,000 --> 00:58:44,000
但是

1707
00:58:44,000 --> 00:58:45,839
如果您绕过它们说好的，

1708
00:58:45,839 --> 00:58:47,040
我将添加各种

1709
00:58:47,040 --> 00:58:48,480
有趣的组件

1710
00:58:48,480 --> 00:58:51,200
来制作算法或代理方式

1711
00:58:51,200 --> 00:58:52,400


1712
00:58:52,400 --> 00:58:55,599
与环境交互 嗯，

1713
00:58:55,599 --> 00:58:57,760
我想它类似于主动

1714
00:58:57,760 --> 00:58:59,119
推理框架，

1715
00:58:59,119 --> 00:59:02,000
或者甚至可能更好，然后你

1716
00:59:02,000 --> 00:59:03,040


1717
00:59:03,040 --> 00:59:06,319
没有区分 whe  re

1718
00:59:06,319 --> 00:59:08,400
rl 更好，或者积极

1719
00:59:08,400 --> 00:59:10,960
影响更好，这

1720
00:59:10,960 --> 00:59:12,559
对我来说并不是

1721
00:59:12,559 --> 00:59:15,680
真的，因为我认为从

1722
00:59:15,680 --> 00:59:16,079
我的

1723
00:59:16,079 --> 00:59:17,599
角度来看，两个社区都在从类似的事情上工作，

1724
00:59:17,599 --> 00:59:19,440
这是顺序

1725
00:59:19,440 --> 00:59:20,400
决策，

1726
00:59:20,400 --> 00:59:23,760
嗯，我们的工作主要

1727
00:59:23,760 --> 00:59:26,000
是呃顺序决策

1728
00:59:26,000 --> 00:59:27,520
面对不确定性，

1729
00:59:27,520 --> 00:59:29,920
而我们的一些工作我的 rl 工作

1730
00:59:29,920 --> 00:59:30,720
可能不

1731
00:59:30,720 --> 00:59:33,440
专注于此，所以我认为当你

1732
00:59:33,440 --> 00:59:33,920
开始

1733
00:59:33,920 --> 00:59:35,359
划定界限时，它会变得

1734
00:59:35,359 --> 00:59:37,119
有点朦胧，就像

1735
00:59:37,119 --> 00:59:40,079
事情是分开的，我想我

1736
00:59:40,079 --> 00:59:41,920
上升了一点 切线

1737
00:59:41,920 --> 00:59:45,760
嗯，但是 phil 你

1738
00:59:45,760 --> 00:59:47,119
想在

1739
00:59:47,119 --> 00:59:49,280
环境和范式方面添加任何

1740
00:59:49,280 --> 00:59:49,839
可能

1741
00:59:49,839 --> 00:59:53,520
有用的东西

1742
00:59:53,520 --> 00:59:55,359
吗是的

1743
00:59:55,359 --> 00:59:57,440


1744
00:59:57,440 --> 00:59:58,640


1745
00:59:58,640 --> 01:00:01,599
应该表现你可以争辩

1746
01:00:01,599 --> 01:00:03,359
说你可以部署一个

1747
01:00:03,359 --> 01:00:05,200
使用某种好奇心或认知

1748
01:00:05,200 --> 01:00:08,720
不确定性减少机制的 rl 代理，但

1749
01:00:08,720 --> 01:00:10,559
我的意思是我知道我知道有一

1750
01:00:10,559 --> 01:00:12,319
点点 关于学习先验

1751
01:00:12,319 --> 01:00:14,160
而不是奖励函数并学习这些的工作，

1752
01:00:14,160 --> 01:00:14,640
但

1753
01:00:14,640 --> 01:00:17,920
我不太了解，但

1754
01:00:17,920 --> 01:00:19,280
我认为重要的是要

1755
01:00:19,280 --> 01:00:21,839
理解就像探索

1756
01:00:21,839 --> 01:00:22,880
整个环境一样，

1757
01:00:22,880 --> 01:00:24,319
你的认知不确定性会

1758
01:00:24,319 --> 01:00:26,319
像你一样

1759
01:00:26,319 --> 01:00:27,760
归零 会观察到一切，

1760
01:00:27,760 --> 01:00:29,359
然后不清楚你的 rl 代理

1761
01:00:29,359 --> 01:00:30,480
在这一点上会做什么，特别是你

1762
01:00:30,480 --> 01:00:31,280
说你有一个

1763
01:00:31,280 --> 01:00:33,680
深度神经网络，可以参数化

1764
01:00:33,680 --> 01:00:35,839
你如何采取行动，

1765
01:00:35,839 --> 01:00:37,440
而在这种情况下，我认为

1766
01:00:37,440 --> 01:00:39,280
这篇论文对我来说真的很有趣

1767
01:00:39,280 --> 01:00:41,119
看看当我们进行这些

1768
01:00:41,119 --> 01:00:42,480
实验时，

1769
01:00:42,480 --> 01:00:44,319
实际上你只是将先验置于你

1770
01:00:44,319 --> 01:00:46,079
之前的偏好之上，最终你

1771
01:00:46,079 --> 01:00:46,480
学习

1772
01:00:46,480 --> 01:00:48,960
了一种行为模式，它可能不是最优的，

1773
01:00:48,960 --> 01:00:50,960
但你的代理最终

1774
01:00:50,960 --> 01:00:53,040
学会了采用一种自我实现的行为，

1775
01:00:53,040 --> 01:00:54,480
因为它减少了

1776
01:00:54,480 --> 01:00:56,000
流行病 不确定性

1777
01:00:56,000 --> 01:00:58,400
，然后剩下的就是说得好我

1778
01:00:58,400 --> 01:00:59,520
认为这些是

1779
01:00:59,520 --> 01:01:00,960
有用的行为，或者至少这些

1780
01:01:00,960 --> 01:01:03,200
是我在这个世界上要做的行为 最终

1781
01:01:03,200 --> 01:01:04,000


1782
01:01:04,000 --> 01:01:05,680
你会得到一个相当重复的行为，

1783
01:01:05,680 --> 01:01:07,119
它可能是一种更准确的

1784
01:01:07,119 --> 01:01:08,160
模拟，

1785
01:01:08,160 --> 01:01:10,400
在没有任何信息的情况下，

1786
01:01:10,400 --> 01:01:12,079
比如智能事物

1787
01:01:12,079 --> 01:01:12,559


1788
01:01:12,559 --> 01:01:15,040
在世界中的实际行为，而在 rl 范式中

1789
01:01:15,040 --> 01:01:16,480
，一旦你减少所有这些，你就不太清楚

1790
01:01:16,480 --> 01:01:17,599
了 认知的，当然关于

1791
01:01:17,599 --> 01:01:18,480
价值在哪里

1792
01:01:18,480 --> 01:01:20,160
，你还没有发现

1793
01:01:20,160 --> 01:01:22,319
你的代理人在那个时候真正在做什么，

1794
01:01:22,319 --> 01:01:24,240
但我认为我目前的问题是

1795
01:01:24,240 --> 01:01:26,079
，就像从公式化的行为一样

1796
01:01:26,079 --> 01:01:28,960
，这就像

1797
01:01:28,960 --> 01:01:31,040
离散状态公式一样起作用 我们

1798
01:01:31,040 --> 01:01:34,640
在该设置中看到了非常好的结果，

1799
01:01:34,640 --> 01:01:36,880
但我认为如果你扩大它，

1800
01:01:36,880 --> 01:01:38,240


1801
01:01:38,240 --> 01:01:38,640


1802
01:01:38,640 --> 01:01:40,559
如果你使用类似摊销

1803
01:01:40,559 --> 01:01:42,640
推理或其他东西来实际

1804
01:01:42,640 --> 01:01:44,640
近似你的似然

1805
01:01:44,640 --> 01:01:47,359
转换函数，那么主动推理代理将会遇到同样的问题，这意味着

1806
01:01:47,359 --> 01:01:49,280
你可能不会 获得

1807
01:01:49,280 --> 01:01:51,119
我们在

1808
01:01:51,119 --> 01:01:53,920
这个小规模上看到的这些不错的属性，所以扩大规模就像

1809
01:01:53,920 --> 01:01:55,200


1810
01:01:55,200 --> 01:01:57,119
是一个有趣且开放式的

1811
01:01:57,119 --> 01:01:58,720
问题 时刻以

1812
01:01:58,720 --> 01:01:59,760
正确的方式扩大规模，您可以

1813
01:01:59,760 --> 01:02:01,440
包括这些共轭模型

1814
01:02:01,440 --> 01:02:02,880
，我们

1815
01:02:02,880 --> 01:02:05,280
在 um 中拥有离散状态公式，

1816
01:02:05,280 --> 01:02:06,000
这

1817
01:02:06,000 --> 01:02:08,240
就是我们在最后一组模拟

1818
01:02:08,240 --> 01:02:10,079
中引入共轭先验

1819
01:02:10,079 --> 01:02:10,880
来进行

1820
01:02:10,880 --> 01:02:13,839
学习的方式 先验偏好

1821
01:02:13,839 --> 01:02:15,440
还有可能性

1822
01:02:15,440 --> 01:02:17,839
嗯，所以如果你以这种方式扩大它，那么

1823
01:02:17,839 --> 01:02:18,720
如何学习

1824
01:02:18,720 --> 01:02:21,680
就像我对整个神经网络有一个超先验一样变得有点模糊

1825
01:02:21,680 --> 01:02:23,280


1826
01:02:23,280 --> 01:02:24,400
，

1827
01:02:24,400 --> 01:02:27,760
嗯，所以他们必须喜欢需要做一些工作

1828
01:02:27,760 --> 01:02:29,039
那个区域

1829
01:02:29,039 --> 01:02:31,520
，为了真正

1830
01:02:31,520 --> 01:02:32,960


1831
01:02:32,960 --> 01:02:35,680
表明你可以拥有这些有趣的

1832
01:02:35,680 --> 01:02:37,520
组件是合适的，那么

1833
01:02:37,520 --> 01:02:40,079
你需要开始考虑如何

1834
01:02:40,079 --> 01:02:42,319
包含这些超先验，

1835
01:02:42,319 --> 01:02:44,240
嗯，因为

1836
01:02:44,240 --> 01:02:45,920
说你将拥有一个混合先验是不合理的

1837
01:02:45,920 --> 01:02:49,119
在参数空间上，

1838
01:02:49,119 --> 01:02:51,760
例如 beta 超，因此超

1839
01:02:51,760 --> 01:02:53,359
先于 gamma，这

1840
01:02:53,359 --> 01:02:55,359
在这些设置中就足够了，

1841
01:02:55,359 --> 01:02:58,319
超先于代理

1842
01:02:58,319 --> 01:03:00,319
选择他的动作的方式

1843
01:03:00,319 --> 01:03:02,640
它必须超过模型参数

1844
01:03:02,640 --> 01:03:03,440
，如果

1845
01:03:03,440 --> 01:03:06,559
扩大规模意味着你失去

1846
01:03:06,559 --> 01:03:08,559
了解开特定

1847
01:03:08,559 --> 01:03:10,400
模型参数的好方法，那么它变得非常

1848
01:03:10,400 --> 01:03:13,440
不确定

1849
01:03:13,440 --> 01:03:15,039
我不知道这对我来说是一个开放式

1850
01:03:15,039 --> 01:03:16,559
问题

1851
01:03:16,559 --> 01:03:18,640
很酷是的我想 高维

1852
01:03:18,640 --> 01:03:19,760
问题

1853
01:03:19,760 --> 01:03:21,599
仍然代表着一些

1854
01:03:21,599 --> 01:03:23,119
相对困难的东西，尤其

1855
01:03:23,119 --> 01:03:25,760
是因为你知道你

1856
01:03:25,760 --> 01:03:27,359
知道的那种我们离海湾

1857
01:03:27,359 --> 01:03:29,680
越远，它变得

1858
01:03:29,680 --> 01:03:30,640
越少，你知道

1859
01:03:30,640 --> 01:03:34,960
它就像它是一条非常细的线

1860
01:03:34,960 --> 01:03:37,520
有趣的是，无论是

1861
01:03:37,520 --> 01:03:38,720
在 rl

1862
01:03:38,720 --> 01:03:40,960
还是主动推理范式中，都有

1863
01:03:40,960 --> 01:03:41,839
一种

1864
01:03:41,839 --> 01:03:44,559
稀疏的骨架，核心是裸露的骨骼

1865
01:03:44,559 --> 01:03:45,520


1866
01:03:45,520 --> 01:03:48,799
，然后有时需要这些其他层

1867
01:03:48,799 --> 01:03:51,839
或调整，

1868
01:03:51,839 --> 01:03:55,359
而且学习起来非常有趣

1869
01:03:55,359 --> 01:03:57,200
，还有你之前所说的

1870
01:03:57,200 --> 01:04:00,079
关于挑战的内容 正在计划

1871
01:04:00,079 --> 01:04:01,839


1872
01:04:01,839 --> 01:04:03,920
在您获得反馈时采取顺序行动，无论是通过

1873
01:04:03,920 --> 01:04:05,440
在环境中移动以改变您的

1874
01:04:05,440 --> 01:04:06,720
本地环境还是您正在

1875
01:04:06,720 --> 01:04:07,839
玩游戏 我

1876
01:04:07,839 --> 01:04:09,440
的棋盘游戏将要发生变化，或者

1877
01:04:09,440 --> 01:04:10,880
您正在根据市场

1878
01:04:10,880 --> 01:04:13,359
顺序行动进行交易，您不能只计划

1879
01:04:13,359 --> 01:04:15,280
步骤 1 到 100

1880
01:04:15,280 --> 01:04:17,440
，至少不考虑一些

1881
01:04:17,440 --> 01:04:18,720
假设

1882
01:04:18,720 --> 01:04:22,240
，然后只能访问有限的

1883
01:04:22,240 --> 01:04:24,559
观察数据并在其中进行计划

1884
01:04:24,559 --> 01:04:25,520
基本的

1885
01:04:25,520 --> 01:04:27,680
不确定性，所以我认为很多与

1886
01:04:27,680 --> 01:04:30,160


1887
01:04:30,160 --> 01:04:32,559
强化学习和机器学习动机的联系点

1888
01:04:32,559 --> 01:04:33,680


1889
01:04:33,680 --> 01:04:35,599
可能会给主动推理带来更多

1890
01:04:35,599 --> 01:04:37,440
启示，并且

1891
01:04:37,440 --> 01:04:39,920
推动你刚才提到的一些前沿，

1892
01:04:39,920 --> 01:04:40,960


1893
01:04:40,960 --> 01:04:42,720
所以我有一个问题，还有其他人

1894
01:04:42,720 --> 01:04:45,119
在 聊天也可以问一个问题，嗯，

1895
01:04:45,119 --> 01:04:47,680
假设有人想了解

1896
01:04:47,680 --> 01:04:48,160


1897
01:04:48,160 --> 01:04:51,280
这个，他们实际上是幸运的

1898
01:04:51,280 --> 01:04:52,960
初学者，

1899
01:04:52,960 --> 01:04:56,079
因为他们可能没有

1900
01:04:56,079 --> 01:04:57,760
被学习强化学习所吸引，但

1901
01:04:57,760 --> 01:04:58,960
他们

1902
01:04:58,960 --> 01:05:02,000
对主动推理感到好奇并兴奋 你的

1903
01:05:02,000 --> 01:05:03,280
演讲

1904
01:05:03,280 --> 01:05:05,599
，他们想学习什么样的计算机

1905
01:05:05,599 --> 01:05:06,720
语言或

1906
01:05:06,720 --> 01:05:08,880
技能，或者他们想学习什么

1907
01:05:08,880 --> 01:05:11,119
样的方法或心态

1908
01:05:11,119 --> 01:05:12,000


1909
01:05:12,000 --> 01:05:13,680
如果有人说不是来自

1910
01:05:13,680 --> 01:05:15,039


1911
01:05:15,039 --> 01:05:16,559
经典机器学习的角度

1912
01:05:16,559 --> 01:05:18,400
和学习主动推理，

1913
01:05:18,400 --> 01:05:21,520
而是将技能提升到主动推理

1914
01:05:21,520 --> 01:05:23,039
，你会向他们推荐你们中的任何一个

1915
01:05:23,039 --> 01:05:25,119


1916
01:05:25,119 --> 01:05:27,119


1917
01:05:27,119 --> 01:05:28,480
吗？ 认为

1918
01:05:28,480 --> 01:05:30,640
这是一种有趣的方式来看待

1919
01:05:30,640 --> 01:05:32,079
这种潜在的人，因为

1920
01:05:32,079 --> 01:05:33,280
我觉得

1921
01:05:33,280 --> 01:05:35,920
我有点像这个人，就像

1922
01:05:35,920 --> 01:05:36,799
回到

1923
01:05:36,799 --> 01:05:38,079
过去的时候，就像诺拉一样，我开始有

1924
01:05:38,079 --> 01:05:39,520
这些关于主动

1925
01:05:39,520 --> 01:05:41,440
推理的对话，嗯

1926
01:05:41,440 --> 01:05:44,640
，我喜欢 这篇论文

1927
01:05:44,640 --> 01:05:45,680
基本上是作为一个

1928
01:05:45,680 --> 01:05:47,680
教程开始的

1929
01:05:47,680 --> 01:05:48,960


1930
01:05:48,960 --> 01:05:51,359


1931
01:05:51,359 --> 01:05:53,280


1932
01:05:53,280 --> 01:05:54,799


1933
01:05:54,799 --> 01:05:57,440


1934
01:05:57,440 --> 01:05:59,280


1935
01:05:59,280 --> 01:05:59,920
密集

1936
01:05:59,920 --> 01:06:02,480
且很难阅读，因此您

1937
01:06:02,480 --> 01:06:03,760
无需尝试就知道

1938
01:06:03,760 --> 01:06:06,079
自我认可，但我确实认为阅读这份

1939
01:06:06,079 --> 01:06:07,520
手稿特别像整个

1940
01:06:07,520 --> 01:06:08,079
目标

1941
01:06:08,079 --> 01:06:10,000
哲学当我们喜欢当我们开始

1942
01:06:10,000 --> 01:06:11,440
写作的时候，这是为了

1943
01:06:11,440 --> 01:06:14,640
真正理解

1944
01:06:14,640 --> 01:06:16,160
这里发生了什么，比如这种

1945
01:06:16,160 --> 01:06:17,760
预期的自由能是什么，比如

1946
01:06:17,760 --> 01:06:18,880
我们为什么关心它，

1947
01:06:18,880 --> 01:06:21,359
所以我从一种理论的角度知道，

1948
01:06:21,359 --> 01:06:23,359
我认为这是 一个非常

1949
01:06:23,359 --> 01:06:26,160
清晰的概念介绍，所以至少

1950
01:06:26,160 --> 01:06:26,640


1951
01:06:26,640 --> 01:06:28,240
你可以

1952
01:06:28,240 --> 01:06:30,319
对正在发生的事情有

1953
01:06:30,319 --> 01:06:32,160
某种直觉

1954
01:06:32,160 --> 01:06:33,760


1955
01:06:33,760 --> 01:06:35,760
工作了

1956
01:06:35,760 --> 01:06:36,960
很多，

1957
01:06:36,960 --> 01:06:38,960
嗯，我想说这有点

1958
01:06:38,960 --> 01:06:40,000
取决于

1959
01:06:40,000 --> 01:06:41,920
这个人的主要目标

1960
01:06:41,920 --> 01:06:43,440
是什么，是为了

1961
01:06:43,440 --> 01:06:46,559


1962
01:06:46,559 --> 01:06:49,760
理解高级概念思想还是

1963
01:06:49,760 --> 01:06:51,359
将其视为算法 因为如果

1964
01:06:51,359 --> 01:06:52,799
你来自自由能

1965
01:06:52,799 --> 01:06:54,799
原理，那么积极的推理是

1966
01:06:54,799 --> 01:06:56,400
另一回事，或者如果你正在

1967
01:06:56,400 --> 01:06:59,440
采取孤立的积极推理，那么

1968
01:06:59,440 --> 01:07:01,839
一种特定的顺序

1969
01:07:01,839 --> 01:07:02,799
决策，

1970
01:07:02,799 --> 01:07:06,559
嗯，计划对，嗯，所以取决于那

1971
01:07:06,559 --> 01:07:06,880


1972
01:07:06,880 --> 01:07:09,599
是 不同的 rs 呃，但正如 phil

1973
01:07:09,599 --> 01:07:11,440
所说，我认为这篇论文

1974
01:07:11,440 --> 01:07:15,359
在某种意义上确实非常好，就像它确实

1975
01:07:15,359 --> 01:07:16,960
尝试定义所有不同的

1976
01:07:16,960 --> 01:07:18,720
概念并

1977
01:07:18,720 --> 01:07:21,920
通过不同的表述可能没有

1978
01:07:21,920 --> 01:07:25,440
那么详细，嗯，

1979
01:07:25,440 --> 01:07:26,559
它给你

1980
01:07:26,559 --> 01:07:29,520
的假设 您可能

1981
01:07:29,520 --> 01:07:30,880
如何推导出它的

1982
01:07:30,880 --> 01:07:34,079
布局，但是某些事情，例如

1983
01:07:34,079 --> 01:07:37,200
近似密度甚至真正需要什么，

1984
01:07:37,200 --> 01:07:40,319
这些都是非常困难的

1985
01:07:40,319 --> 01:07:41,200
问题

1986
01:07:41,200 --> 01:07:43,440
，您必须深入研究

1987
01:07:43,440 --> 01:07:44,880
变分推理文献才能

1988
01:07:44,880 --> 01:07:46,000
理解，所以

1989
01:07:46,000 --> 01:07:47,839
我想从这个角度来看有人

1990
01:07:47,839 --> 01:07:49,920
谁进入该领域

1991
01:07:49,920 --> 01:07:51,440
应该花一些时间思考

1992
01:07:51,440 --> 01:07:53,280
变分推理以及它如何

1993
01:07:53,280 --> 01:07:53,760
真正

1994
01:07:53,760 --> 01:07:55,839
与影响制定行为联系起来，

1995
01:07:55,839 --> 01:07:56,960


1996
01:07:56,960 --> 01:07:59,119
因为主动推理的感知部分

1997
01:07:59,119 --> 01:08:00,559


1998
01:08:00,559 --> 01:08:03,359
在大多数情况下与

1999
01:08:03,359 --> 01:08:04,160
变分

2000
01:08:04,160 --> 01:08:07,119
呃推理文献和优化

2001
01:08:07,119 --> 01:08:08,640
呃模型证据完全相同

2002
01:08:08,640 --> 01:08:12,240
嗯，或者最大化证据法

2003
01:08:12,240 --> 01:08:13,200
比比皆是，

2004
01:08:13,200 --> 01:08:15,599
嗯，这是我要做的第二件事

2005
01:08:15,599 --> 01:08:16,399
补充

2006
01:08:16,399 --> 01:08:19,679


2007
01:08:19,679 --> 01:08:22,319
一下 lancelot de costa 的论文

2008
01:08:22,319 --> 01:08:24,158
对于想要深入研究

2009
01:08:24,158 --> 01:08:27,198
自己推导一切的人来说

2010
01:08:27,198 --> 01:08:30,719


2011
01:08:30,719 --> 01:08:31,198


2012
01:08:31,198 --> 01:08:33,279


2013
01:08:33,279 --> 01:08:35,359


2014
01:08:35,359 --> 01:08:36,080


2015
01:08:36,080 --> 01:08:38,238
非常有用 对于

2016
01:08:38,238 --> 01:08:40,080
不熟悉数学

2017
01:08:40,080 --> 01:08:41,198
并且只想

2018
01:08:41,198 --> 01:08:44,399
获得外行摘要的人来说，这是

2019
01:08:44,399 --> 01:08:46,560


2020
01:08:46,560 --> 01:08:48,080


2021
01:08:48,080 --> 01:08:50,960
一个很好的介绍

2022
01:08:50,960 --> 01:08:51,759


2023
01:08:51,759 --> 01:08:53,679


2024
01:08:53,679 --> 01:08:55,439
理论部分

2025
01:08:55,439 --> 01:08:57,759
嗯，从编码的角度来看，它

2026
01:08:57,759 --> 01:08:59,759
完全取决于最终目标是什么，

2027
01:08:59,759 --> 01:09:00,640
所以如果他们想要

2028
01:09:00,640 --> 01:09:03,198
有人想要使用离散

2029
01:09:03,198 --> 01:09:04,479
状态公式，

2030
01:09:04,479 --> 01:09:07,679
那么 matlab 代码 carl 已经

2031
01:09:07,679 --> 01:09:10,719
编写了多年的工作，有很多

2032
01:09:10,719 --> 01:09:13,600
很好的模拟和例子，

2033
01:09:13,600 --> 01:09:14,319
你 可以使用

2034
01:09:14,319 --> 01:09:17,439
，而且我们的代码是在线的，您可以

2035
01:09:17,439 --> 01:09:18,479
访问它，

2036
01:09:18,479 --> 01:09:21,679
所以在软件 um 部分的论文中有一个链接，

2037
01:09:21,679 --> 01:09:24,880
它给出了

2038
01:09:24,880 --> 01:09:25,679


2039
01:09:25,679 --> 01:09:27,040
代码到底在哪里，你可以看

2040
01:09:27,040 --> 01:09:29,040
一下，看看模拟是如何

2041
01:09:29,040 --> 01:09:29,920
完成

2042
01:09:29,920 --> 01:09:33,120


2043
01:09:33,120 --> 01:09:36,719


2044
01:09:36,719 --> 01:09:39,120


2045
01:09:39,120 --> 01:09:42,799


2046
01:09:42,799 --> 01:09:46,399


2047
01:09:46,399 --> 01:09:48,799
的 第一作者 呃，我们又得到了一个

2048
01:09:48,799 --> 01:09:50,080
不错的结果，我们也有一个

2049
01:09:50,080 --> 01:09:52,479
git reaper 来完成这项工作

2050
01:09:52,479 --> 01:09:53,198


2051
01:09:53,198 --> 01:09:55,920


2052
01:09:55,920 --> 01:09:57,360


2053
01:09:57,360 --> 01:09:59,600


2054
01:09:59,600 --> 01:10:01,280


2055
01:10:01,280 --> 01:10:03,679


2056
01:10:03,679 --> 01:10:04,719
喜欢不同的

2057
01:10:04,719 --> 01:10:06,400
领域，但对于刚开始的人来说，他们

2058
01:10:06,400 --> 01:10:07,679
必须了解他们是

2059
01:10:07,679 --> 01:10:08,800
要专注于

2060
01:10:08,800 --> 01:10:10,159
理论方面还是要

2061
01:10:10,159 --> 01:10:11,840
专注于实施方面

2062
01:10:11,840 --> 01:10:13,199
理论方面

2063
01:10:13,199 --> 01:10:14,640
将深入研究

2064
01:10:14,640 --> 01:10:16,000
变分影响及其背后的数学

2065
01:10:16,000 --> 01:10:17,920
， 如果他们想专注

2066
01:10:17,920 --> 01:10:18,239


2067
01:10:18,239 --> 01:10:19,600
于编码方面，那么他们想

2068
01:10:19,600 --> 01:10:21,760
弄清楚是对连续

2069
01:10:21,760 --> 01:10:23,199
状态还是离散状态公式

2070
01:10:23,199 --> 01:10:25,199
感兴趣 输入然后

2071
01:10:25,199 --> 01:10:27,679
分解成如果它是

2072
01:10:27,679 --> 01:10:29,040
连续的，那么

2073
01:10:29,040 --> 01:10:32,880
大部分要么是写

2074
01:10:32,880 --> 01:10:34,800
运动本身的坐标，要么

2075
01:10:34,800 --> 01:10:36,960
他们必须使用

2076
01:10:36,960 --> 01:10:40,640
某种类似的神经网络来

2077
01:10:40,640 --> 01:10:42,480
逼近兴趣的连续分布

2078
01:10:42,480 --> 01:10:43,280


2079
01:10:43,280 --> 01:10:45,520
或者是 carl 写出的离散状态

2080
01:10:45,520 --> 01:10:46,400


2081
01:10:46,400 --> 01:10:49,199
，如果他们对离散状态有疑问，

2082
01:10:49,199 --> 01:10:50,480


2083
01:10:50,480 --> 01:10:53,679
嗯，我很乐意接受电子邮件，

2084
01:10:53,679 --> 01:10:54,080
所以

2085
01:10:54,080 --> 01:10:57,040
如果有人有或连续

2086
01:10:57,040 --> 01:10:57,840
状态状态，

2087
01:10:57,840 --> 01:11:01,679
嗯，状态空间以及感谢

2088
01:11:01,679 --> 01:11:04,960
区别，嗯

2089
01:11:04,960 --> 01:11:06,159


2090
01:11:06,159 --> 01:11:09,040
，我们与

2091
01:11:09,040 --> 01:11:09,679


2092
01:11:09,679 --> 01:11:12,159
ryan smith 和 christopher

2093
01:11:12,159 --> 01:11:13,120
white 进行

2094
01:11:13,120 --> 01:11:16,000
矩阵乘法等的 matlab 代码之间存在如此大的差异，

2095
01:11:16,000 --> 01:11:17,920
然后神经网络出现了

2096
01:11:17,920 --> 01:11:20,880
，它提供了听起来像是

2097
01:11:20,880 --> 01:11:23,120
高维

2098
01:11:23,120 --> 01:11:26,159
和连续的新机会 变量，但也有

2099
01:11:26,159 --> 01:11:26,880
很多新的

2100
01:11:26,880 --> 01:11:30,320
挑战，所以

2101
01:11:30,320 --> 01:11:33,360


2102
01:11:33,360 --> 01:11:37,440
矩阵形式和这种更

2103
01:11:37,440 --> 01:11:40,480
机器学习的风格所

2104
01:11:40,480 --> 01:11:43,040
共有的本质是什么，因为 e 对于某些人来说，嗯，

2105
01:11:43,040 --> 01:11:43,920


2106
01:11:43,920 --> 01:11:46,480


2107
01:11:46,480 --> 01:11:47,840


2108
01:11:47,840 --> 01:11:49,120
当他们

2109
01:11:49,120 --> 01:11:51,760
从

2110
01:11:51,760 --> 01:11:54,800
生态心理学或从积极的

2111
01:11:54,800 --> 01:11:57,600
哲学或体现的性能

2112
01:11:57,600 --> 01:11:58,320
角度考虑主动推理时，这可能会毫不夸张地说两种计算机语言之间的差异，

2113
01:11:58,320 --> 01:12:01,600
所有背景都集中在主动

2114
01:12:01,600 --> 01:12:02,880
推理上，等等

2115
01:12:02,880 --> 01:12:05,440
对于那些头发之外的人

2116
01:12:05,440 --> 01:12:06,800


2117
01:12:06,800 --> 01:12:09,199
，我们真正可以提炼

2118
01:12:09,199 --> 01:12:09,920
出的

2119
01:12:09,920 --> 01:12:11,679
核心主动推理是什么，我写了

2120
01:12:11,679 --> 01:12:13,440
一些东西，

2121
01:12:13,440 --> 01:12:14,800
以便您说过

2122
01:12:14,800 --> 01:12:16,480
那些允许我们

2123
01:12:16,480 --> 01:12:19,120
使用 matlab 或深入矩阵模式的核心部分是什么

2124
01:12:19,120 --> 01:12:20,840
像 python um 一样进入神经网络模式，

2125
01:12:20,840 --> 01:12:22,960


2126
01:12:22,960 --> 01:12:24,640
所以我认为它归结为我的

2127
01:12:24,640 --> 01:12:26,640
总结幻灯片，我认为它的方式

2128
01:12:26,640 --> 01:12:28,159
是主动推理，核心

2129
01:12:28,159 --> 01:12:29,679
成分正在制定性别

2130
01:12:29,679 --> 01:12:30,960
模型

2131
01:12:30,960 --> 01:12:32,480
，这里制定一般模型

2132
01:12:32,480 --> 01:12:33,840
是一般模型的参数化

2133
01:12:33,840 --> 01:12:35,520
因此，您可以使用

2134
01:12:35,520 --> 01:12:37,760
离散阶段分类分布

2135
01:12:37,760 --> 01:12:38,719
，也可以

2136
01:12:38,719 --> 01:12:40,560
使用更连续的状态公式并

2137
01:12:40,560 --> 01:12:42,719
再次使用

2138
01:12:42,719 --> 01:12:44,560
神经网络公式是它的一个

2139
01:12:44,560 --> 01:12:46,400
特定实例，

2140
01:12:46,400 --> 01:12:49,679
因此这将是一种

2141
01:12:49,679 --> 01:12:50,480
区分方式

2142
01:12:50,480 --> 01:12:53,679
，第二种方式是优化正在运行

2143
01:12:53,679 --> 01:12:56,239
的目标函数

2144
01:12:56,239 --> 01:12:57,440
，

2145
01:12:57,440 --> 01:13:01,840
因此在 matlab 代码中，

2146
01:13:01,840 --> 01:13:04,239
我们正在使用平均场消息进行梯度发送

2147
01:13:04,239 --> 01:13:06,400
传递算法如此

2148
01:13:06,400 --> 01:13:07,760
具体的公式，已经

2149
01:13:07,760 --> 01:13:09,679
在几篇论文中介绍过

2150
01:13:09,679 --> 01:13:11,440
，我特别没有走过

2151
01:13:11,440 --> 01:13:12,880
那个，

2152
01:13:12,880 --> 01:13:16,000
或者你正在做反向传播来

2153
01:13:16,000 --> 01:13:17,280
实际

2154
01:13:17,280 --> 01:13:20,320
计算或 lambda 分布

2155
01:13:20,320 --> 01:13:21,760
，然后你只是

2156
01:13:21,760 --> 01:13:23,840
解决你所

2157
01:13:23,840 --> 01:13:26,560
拥有的那些分布 这有点取决于

2158
01:13:26,560 --> 01:13:29,840
您喜欢哪种公式，

2159
01:13:29,840 --> 01:13:32,400
这取决于您如何优化

2160
01:13:32,400 --> 01:13:33,840
这些目标，或者您喜欢

2161
01:13:33,840 --> 01:13:35,600
采用隐式前向模型，

2162
01:13:35,600 --> 01:13:37,600
还是采用显式性别

2163
01:13:37,600 --> 01:13:39,199
模型，嗯

2164
01:13:39,199 --> 01:13:41,360
，我忘记提及的一件事

2165
01:13:41,360 --> 01:13:42,480
是 嗯，

2166
01:13:42,480 --> 01:13:44,480
alex shams 和 connor hines，他们一直

2167
01:13:44,480 --> 01:13:47,520


2168
01:13:47,520 --> 01:13:50,880
在研究 Python 中 um 主动推理的离散状态公式，

2169
01:13:50,880 --> 01:13:53,360
这可能对

2170
01:13:53,360 --> 01:13:55,280
那些 想要专注于一种特定的

2171
01:13:55,280 --> 01:13:56,239
语言

2172
01:13:56,239 --> 01:13:58,960
，它可以做更高端或

2173
01:13:58,960 --> 01:14:00,640
高维的东西，

2174
01:14:00,640 --> 01:14:02,719
以及 carl 拥有的离散状态公式，

2175
01:14:02,719 --> 01:14:03,920


2176
01:14:03,920 --> 01:14:05,760
我知道如果有人感兴趣，他们也在寻找人

2177
01:14:05,760 --> 01:14:08,719
在代码库上工作，

2178
01:14:08,719 --> 01:14:09,840
所以它被称为

2179
01:14:09,840 --> 01:14:13,520
推断活动，我认为嗯，但那也在

2180
01:14:13,520 --> 01:14:17,760
github 上，如果有人感兴趣的话，

2181
01:14:17,840 --> 01:14:21,760
很酷，嗯

2182
01:14:21,760 --> 01:14:25,280
，你们中的任何一个最后的想法或评论，

2183
01:14:27,440 --> 01:14:30,640
嗯，不，我认为我还好，

2184
01:14:30,640 --> 01:14:33,440
所以你认为

2185
01:14:34,880 --> 01:14:37,199
嗯，

2186
01:14:38,560 --> 01:14:41,360
我认为我们有点注意到 有点

2187
01:14:41,360 --> 01:14:42,239


2188
01:14:42,239 --> 01:14:45,679
嗯，对主动推理的兴趣增加了，

2189
01:14:45,679 --> 01:14:46,640


2190
01:14:46,640 --> 01:14:48,560
就像给你一个例子，比如可能

2191
01:14:48,560 --> 01:14:49,840
以前

2192
01:14:49,840 --> 01:14:51,040
甚至会考虑主动推理的领域，

2193
01:14:51,040 --> 01:14:53,840
比如机器人，

2194
01:14:53,840 --> 01:14:55,360
你现在开始看到越来越

2195
01:14:55,360 --> 01:14:57,360
多的东西，所以我想如果你 现在确实

2196
01:14:57,360 --> 01:14:58,960
想参与其中是一个特别

2197
01:14:58,960 --> 01:15:01,199
好的时间

2198
01:15:01,199 --> 01:15:04,400
伟大的电话菲利普我会

2199
01:15:04,400 --> 01:15:07,199
重新

2200
01:15:07,199 --> 01:15:08,880
推荐我们正在讨论的优秀论文在

2201
01:15:08,880 --> 01:15:11,120
这个视频的描述中非常

2202
01:15:11,120 --> 01:15:12,719
感谢你们俩

2203
01:15:12,719 --> 01:15:15,040
的加入 总是欢迎你来

2204
01:15:15,040 --> 01:15:16,480


2205
01:15:16,480 --> 01:15:18,239
谈论你是否写过一篇论文，

2206
01:15:18,239 --> 01:15:19,520
但

2207
01:15:19,520 --> 01:15:21,679
再次感谢诺拉和

2208
01:15:21,679 --> 01:15:23,679
菲利普，我希望在

2209
01:15:23,679 --> 01:15:24,239
未来的

2210
01:15:24,239 --> 01:15:27,679
活跃推理流中再次见到你谢谢你

2211
01:15:27,679 --> 01:15:30,159
感谢你邀请我们 再见，

2212
01:15:30,159 --> 01:15:31,440
再见，丹尼尔·

2213
01:15:31,440 --> 01:15:34,000
和平，再见，

2214
01:15:36,840 --> 01:15:39,280
很棒，停止直播，很棒的

2215
01:15:39,280 --> 01:15:41,040
谈话，非常感谢菲利普和

2216
01:15:41,040 --> 01:15:43,840
诺尔，真的


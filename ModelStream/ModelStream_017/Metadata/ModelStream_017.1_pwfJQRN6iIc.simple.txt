SPEAKER_02:
hello welcome everyone it's march 21st 2025 and we are here with a bunch of the developers of rx infer and lazy dynamics we will be having a presentation followed by a discussion so looking forward to everyone's comments here on the panel and to anyone watching live looking forward to your questions in the live chat albert to you for the presentation and thanks everybody again for joining


SPEAKER_03:
hi hi everyone yeah thanks daniel for uh this introduction um yeah so uh for everyone my name is albert i'm a ceo of lazy dynamics which is a spin out from einhoven university of technology and lazy dynamics essentially builds uh infrastructure for developing adaptive agents such as inference agents and today i'm pleased to to have almost full lazy dynamics team here with me

so yeah let's uh let's get started um i must say that actually yeah this talk is uh going to be admittedly uh provocative perhaps even a click bait uh but uh yeah let's just have some fun here and actually i hope that many of active interference practitioners can relate to the pain point i'm about to discuss

So yeah, now I kind of expect that some people won't like the way we pose the question.

And yeah, I'm hoping, I'm actually not going to answer in the way that people might anticipate because yes, active inference is a theoretical framework, which is very elegant and very powerful.

But if you look at this from a practitioner perspective,

it leaves much to be desired.

So from a framework that claims to be superior to reinforcement learning, that promises to revolutionize AI, that's supposed to explain actually everything from bacterial behavior to human consciousness, what do we see in practice?

What do people who are getting excited actually see in practice?

Well, look at this beautiful theory.

uh groundbreaking uh or like uh equations and yet uh this is sort of a typical reaction from uh from a developer so they're literally like running away and i don't think we can actually blame them uh for that because uh well we've somehow managed to take one of the most promising frameworks for the intelligent systems and make it so complex

that even a phd student would think twice before implementing it so and i just want to draw sort of a parallel and look at what's happening in the rest of ai yeah so five lines of code that's all it takes to access the state-of-the-art transformer models so if you want to analyze satellite imagery done want to create a conversational agent done

I want to synthesize a video or audio.

Done.

So half a page of the unique code.

But meanwhile, implementing an active inference agent.

Well, let me just walk you through this.

So what it takes.

First, you read through hundreds of pages of dense mathematical theory.

Then you spend months on implementing some custom algorithms.

Then you debug some mysterious convergence issues.

And after all that work,

do we have well more often than not it's going to be a teammates example or like maybe a mountain car yeah i mean come on what is what is this you just like you just made me read dozens of pages of heavy math to console me with the moving pixels on the screen right so this is quite a leap between i'm better than reinforcement learning and i can do a team is better than reinforcement learning

So, I mean, come on.

So we're like in 2025 now.

So the rest of AI is solving like really interesting world problems and like having really interesting demos.

And we are still celebrating when our agents can turn left or right.

So, and truth to be told, I mean, I think my personal experience is that, yeah, like each time I'm getting to this active inference literature, it's, to put it mildly, not always very much reproducible or like half of it is not, there's not much to reproduce it.

so the point here is that this needs to be changed so uh and not just for convenience but because this complexity is i think actively holding back the entire field we are basically limiting the active inference to a tiny group of theoretical specialists when it should be in the hands of every ai developer

um and the saddest part actually of all these that people who used to be passionate about the theory they just give up on it and we just see it all around uh like in the community so how can we fix that how can we actually make this as accessible as transformers so for starters uh we must recognize what makes active inference agent unique yeah so well

according to the theory it's obviously not the inference mechanics right because the inference mechanics is same across all agents it's just free energy minimization so what's left what's left for us like these three three lines of code is basically the model and constraints so this is what um this is the only unique part this is the only like this unique three lines of code if you wish um

uh which is which defines the agent so it's it's should be only about model definition and constraints um and that's exactly what we are trying to do with ericsson fur uh we developed the framework that we are when we are continuously developing this framework that allows you to provide a model constraints and run the inference guided by the free energy minimization

And I'll talk about this in the next slide a bit more.

And what we actually can see is that once you have the framework right, or let's say almost right, because it's a development project, you can streamline the development of these agents by orders of magnitude.

And seriously, like when I started my PhD, I spent

few years trying to build an audio counterpart of a T-Maze demo that people can now build in just a few hours.

So that's kind of the order of magnitude you should think of.

And yeah, just to zoom in a little bit on some of the examples.

Now, basically, if you want to build a drone that does planning through inference,

would just take about 10 lines of code to define the drone model and then like run inference to get this nice path planning um so here you see like two 3d drone uh examples one is fine in the rainy day another one is in the sunny day and another one is like 2d drone which i'm more excited about because it actually fills this reactivity aspect of our framework so you could see that

we are changing the environment and the drone doesn't fault it continues to sort of reinfer its policies and it states in order to reach this goal no matter the environmental changes so yeah 10 lines of code so um if you want to yeah same same applies to locomotion control so there are two examples one for robotic arm another one for this uh

is the name for this uh robot i forgot but basically this swinging swinging uh monkey robot right so let's uh let's phrase it this way um so again define your model run inference um if you want to have multi-agent navigation in a dynamic environment with collision avoidance and speed constraints that would take you like 15 miles of code not uh thousands just uh 15. that's uh

Quite a boost, I would say.

So yeah, another just a few demos from our repository.

Again, if you want to integrate deep learning into RxInfer, it's also possible.

You can learn the dynamics and predict the future sequences all in one go.

So it's not two separate function calls.

You just define it.

You just call the inference within the same model.

So here's about eight lines of code.

and yeah surely how could we avoid mentioning a better version of teammates so that's that's something not publicly available though but that's something we've been working on this kind of an agent that needs to solve a mini grid so there is an agent that needs first to locate the key then take the key then figure out that he needs to open the door and then go to the to the exit

So that's something that works.

This is the only example which is not publicly available yet, but it will be soon.

And yeah, before I go into how we are doing that, I just want to say also that these examples themselves, they're hardly as impressive as reinforcement learning demos or MPC demos.

uh but they're better than i would say this stigmatized examples of uh active inference supremacy so um yeah um i think that um it's still ongoing uh but it's it's it's going pretty fast um and um yeah so how we do it what's our approach um so we do it

through essentially four key innovation so that the first one is most important is like free energy minimization it's automated uh you no manual derivations uh needed for a very large class of models and they're like they're composable but that's basically why we adhere to factor graphs because a factor graphs is a very convenient underlying model representation

which allows us to essentially seamlessly combine different models into bigger models, or the other way around, decompose bigger models into smaller submodels.

And another thing, of course, we adhere to asynchronous data such that we can handle and facilitate this real-time adaptation to changing environments.

And the really key thing here is reactive message passing technology.

um and in our opinion this is the actual key to successful implementation of functional agents or like functional active inference agents um and the argument here is that see the world uh the real world i mean it doesn't start and stop at the agent's convenience so the agent must react adapt act upon in one environmental data streams so if something unexpected

happens and like the world always brings some some uh unexpected uh things to the yeah to you and yeah you essentially you you have to adapt so we must compute our policies we must compute our posteriors no matter the failure so that was what you could see with that little drone demo um and so this is what our implementation of um

I think so.

This is something that like what the other implementation of active inference aren't getting quite right.

I think actually, to be fair, most of the reinforcement learning libraries adhere also to this imperative updates of their policies and so on.

Nevertheless, I don't want to give the impression that the current implementation of reactive message passing works as we wish it would.

It's super fast.

We are beating, in terms of inference, many other frameworks.

But we also recognize how it can be improved.

And the truth to be told, it can be improved.

We can actually make it faster and better.

That is something we are working on as well.

So what's the state of RxInfer today?

RxInfer is...

was and will remain the open source library that has more than now 10 years of cumulative development time we are about 10 active developers from laser dynamics and bias lab bias lab is the laboratory where we spun out from uh and of course active inference institute uh who's helping us with the development of the graphical interface with demos like there's a really lots of help coming from active inference that we appreciate

um so we have over 700 uh 30 stars on github across reactive base packages and more than 300 monthly downloads now i mean that that these are not the craziest number you could think of but i think here's why these numbers actually matter they show that when despite all the barriers despite all the complexity despite having to learn

julia despite having to learn probabilistic programming uh people are still pushing to use active inference uh they push to use ericsson fur in general and we are seeing this uh and it just just try to imagine like if we are seeing that much um interest uh when it is hard i just want you to imagine uh what would happen when we make it easy

And that's kind of exactly what we are building towards.

So before RxInfer, in the early days of my PhD, and I think even prior to that, I had to write quite some code and actually do quite some math.

Now with BiasLab and reactive base community, we push it to the one page of model definition and like everything else is automated.

But at Lazy Dynamics, we also recognize that

even that is too hard so even even that is uh really not going to streamline the development of active information because there is like a huge python community and like people want to integrate uh our models with with their models and support so that's what that's where lazy dynamics came in

So we are actually going to hide the complexity even further and wrap the models into a nice open IP available for Python, Rust, and so on.

So we do recognize that model specification is still hard for many users.

So in the beginning, we will provide models for users and developers.

We will start simple, and then we will make it more and more interesting and complex.

So with the vision of that people can actually will be able to submit their models, post their models, and we will just provide the API to trigger those.

So a bit of teaser.

It's not available for mass use.

It's just something we are testing internally.

So how that's going to look like?

Well, so one would basically have to load the model from our pool of RxInfer models, and then run inference slash planning, take actions, and learn.

So we will do it now in the separate API calls for clarity.

I mean, the mechanics under the hood are the same.

So it's just free energy minimization.

We just realized that people get a hang of it faster if you just pause it like this.

But yeah, it's a work in progress.

We plan to release this API in the early Q2.

So it won't be as flexible as RxInfer, but it will expose endpoints and we will make it better over time.

um yeah and this is uh the team that has this ambition like this is lazy dynamics team but i must say that there are more people behind this now so there are more people behind the rx infer development uh this is just solely the lazy dynamics team uh the yeah um but there are more developers as i said outside this lazy dynamics team so uh yeah just uh

Yeah, I think last few bites for me is that yeah, I think that active inference needs to move finally from like this beautiful beyond beautiful theory and into practical applications.

We are going that it's possible and the framework is indeed revolutionary.

And I think that now it's time to make it usable.

So yeah, thank you for listening.

And here I'd like to stop and actually invite people for questions.


SPEAKER_02:
Awesome.

Thank you.

If any of the others here just would like to go around, say hello if they'd like.

Any first thoughts?

And meanwhile, anyone in the live chat can write questions.


SPEAKER_06:
Which order do you want to go, Daniel?

I mean, I can start.

I think it's fantastic what Albert and the team is developing.

I still believe active inference is the future, also to reinforcement learning problems.

And it's the tool set, user-friendly tools, that's lacking.

So this is an initiative, I think, that hopefully will get us closer to recognition of active inference

as the underlying framework for future intelligence agents.

And everybody is invited to contribute, to critique, to give feedback.

So yeah, we're very open in terms of how we should develop towards the future because we don't have all the answers ourselves.

But yeah, this is our push, first push.

to make it as simple as working with transformers.


SPEAKER_03:
Dimitri, do you want to add a few words as a core architect of Rick's Infer?


SPEAKER_05:
Well, I can only add that you're a little bit wrong saying that it's not publicly available.

Actually, the API is

almost publicly available we have released the initial version of it we just didn't advertise it as much and it doesn't really have all the models that we have but we we are planning to release the api that albert has mentioned the server for free for now so everyone can experiment and everyone can break it as fast as possible because as soon as we fail we know how to fix it

So fail-fast approach, we will deliver new models, we will test it, we will fail, we will improve.

And for this, we essentially need more people to try it out.

Because we have our internal use cases, we try it out, it works on our internal cases.

But there are many other people who also want to try this approach.

so we will co-develop the model so we'll release them for free for now for the free api and we'll see how it goes


SPEAKER_01:
this is just the tip of the iceberg right so i think the team of lazy dynamics we have already had the pleasure to work with the api so a year ago we all didn't think it would be possible but especially with a lot of effort from the meet free we're now able to call rx for from python so this should integrate very easily with existing um reinforcement learning toolboxes out there and

I've already had the opportunity to play with it.

I'm playing with it also before this live stream and I'm really enjoying it so far.

So basically merging the infrastructure that's available in Julia with everything that's already available in Python.

And I think that's just the tip of the iceberg.

There's a lot more in store coming up real quick.

Yeah.

So everyone would want to try it out.

Also just contact us, reach out.

share contact details, whatever you want, because I think we should all adopt this approach.


SPEAKER_02:
Maybe just Bart, one follow-up question there, like in integrating through Python calls to RxInfer, where do you see that plugging in for somebody who wants to use it amidst the other machine learning methods they're already deploying?

Like what would that look like?

How would they conceptualize of their pipeline and where these kinds of probabilistic models would fit in?


SPEAKER_01:
I think these probabilistic models themselves are very important when you are trying to model a process that involves uncertainty or making these autonomous systems.

So in those regards, I think that's where current machine learning frameworks often lack, is that usually there's this black box neural network.

I think personally that

having a probabilistic take on specific aspects being just in the latent space of a neural network right i think that could benefit a lot of enterprises and a lot of products making them more reliable and robust and because most of these products have like some back end some machine learning models already in python i think that would be amazing to integrate rx refer from julia into their existing infrastructure


SPEAKER_03:
Just on top of that, there is an opportunity that, well, not the opportunity, there are a few demos people are trying out, is basically to connect their LLM-based agents with, to connect them to the backend of RxInfer.

That's also something feasible.

And apart from it, so there are many problems indeed that

can be tackled with probabilistic programming languages right like uh you can there are many at the table like imc and non-pyro and yeah people use them as components into their bigger like machine learning pipelines and here the

Well, now you basically can do it with RxInfer, just basically without leaving your Python environment, you can call RxInfer to provide you with some uncertainty estimates and confidence over the answers for other parts of your machine learning pipelines.


SPEAKER_05:
We can also add maybe here a little bit to expand how we can integrate with LLMs, because we can imagine that LLM is a source of noisy data.

For example, you can ask questions to LLM to give some numerical answers.

uh but you should not really trust it so you may you may think of it as the noisy observation of a real data so and you can create a model that takes into it into account and basically treat this as your observations and then the underlying probabilistic model will infer the real parameters of the process

so i think uh yeah ericsson fair can be integrated very naturally in the existing pipeline so it shouldn't really be or it's not necessary to have just ericsson in your pipeline it can be integrated with like other data collection libraries or the existing machine learning libraries so yeah i think uh but uh

It excels when you really need to create a plan given a noisy set of observation and given the model of the process.


SPEAKER_02:
Cool.

Ismail, if you'd like to add anything, and then otherwise maybe to Fraser, and then we'll go to some pre-written questions in the live chat.


SPEAKER_04:
Yeah, sure.

I'd like a couple of things to say.

First of all, the new RxInfo server, I think, will add extraordinary capabilities for both Python developers and for Julia developers that are struggling to find the right platform to develop agents that are not hallucinating.

Yeah, so for that big

Many thanks to Dimitri for realizing this.

And yeah, I would like to invite everyone who is keen on learning about machine learning and developing agents or developing models for whatever task they have in mind to try out RxInfer and

engage with us their problems and their experiences, whether it's positive or negative, that will be a very valuable information for us.

And that's mostly it.


SPEAKER_05:
If I can add, I see a little bit like a big focus on Python, but Albert didn't really focus on that much.

But the way we created Ericsson First Server is with the open API specification that allows

to use the same API from any language that supports OpenAPI, which includes basically all languages that exist, including Julia.

So it is possible to use Julia for extender server or Python or JavaScript or Rust or even C. And if I'm not mistaken, even microcontrollers, they do have some sort of

a layer between this open api specification so it really opens doors not just for python developers but uh also from the browser we can now use our extender because javascript is also supported so it really opens doors like for experimentation in many fields


SPEAKER_02:
Awesome.

Brazier, you prepared some excellent comments, so however you'd like to continue, go for it.


SPEAKER_00:
Cool.

Thank you.

I should just clarify, I'm not a developer on RxInfer.

I'm an open source contributor, and I'm a PhD student at Glasgow, but I'm not part of the core team.

So I have, I think, Daniel, what I'll do is I'll condense those questions I had down into two, hopefully just to save a bit of time.

The first question is about active inference more generally, because this talk was, well, thank you.

First of all, thank you for the talk.

It was very enlightening.

It's excellent stuff.

Thank you, Albert.

But about active inference, the problems with active inference, you know, and how Rx and For is kind of addressed to solve them.

It seems to my mind, basically, that

We have issues with real-time inference.

We have issues with robust inference and scalable inference.

And then even beyond that, scaling, say, like the generative model itself to be able to take care of more sophisticated inference queries.

Those kind of three things seem to me to be the current issues with active inference.

making it real-time, robust, and reliable.

I mean, RxInfer has effectively been kind of the best tool in the game for addressing those concerns because of its very nature.

But can anyone speak to, let's say, perhaps how continuing development of RxInfer are going to assist in those three kind of domains?

And I have a follow-up question about RxInfer server.

So I know it's very broad, but alas, I had to ask it.

Mitr, do you want to answer which one?


SPEAKER_05:
Yeah, yeah, I can answer.

It's a great question.

So right after Ericsson FUR server, what we plan, we want to change and refactor the core of Ericsson FUR.

to make the points that you mentioned even easier to do, even easier to scale and faster to run inference, but also to make it easier to debug and to contribute for other people.

Because also with the experience with you, we noticed that it's quite difficult to contribute because

uh the first of all it's written in julia so not all of people know julia but also because it's uh it has been it has started as a phd project and certain parts weren't really designed very well so we will take a step back and we'll given all the knowledge that we have now and given all the feedback and given all the goals that we and other people want to achieve

will redesign the the entire core of reactive mp uh and this this potentially will open the doors also to run the inference on gpus for example so that's that's how transformers basically scaled themselves right they they started to use gpus at all uh at large scale

So we have big plans to improve the scalability even further and make the inference even faster and easier to maintain and debug and contribute for other people.


SPEAKER_00:
Excellent.

That sounds like a small problem.

Did I cut someone else off there?

I sensed another voice.


SPEAKER_06:
Yeah, well, that was me.

I mean, you didn't really cut me off.

But yeah, perhaps I can add to that a bit.

I'm more of a big picture person.

And so maybe also for people that are just wondering about active inference and are still in other areas of machine learning, like reinforcement learning or deep learning, why would they be interested?

So I think most people agree that the Bayesian inference is a correct framework for dealing with uncertainty.

But the big thing is the computational cost and that limits us in scaling this all up, right?

So people have mentioned ideas like, well, maybe we should do planning as inference.

um this is possible with arcs and fur and we're working on stuff that we will probably reveal also in the next half year where i think it will be a nice surprise and how to do planning as inference and then there is a an even more broader idea that this idea that you shouldn't do bayesian inference because of the computational cost which limits the scaling i think it's almost the opposite around by um

doing Bayesian inference you have access to where the uncertainties are and therefore you also have access to when you just should stop computing because you shouldn't you shouldn't compute more accurately than than than is needed right when when your data is uh when you have a large variance about your data or your variables you don't need very accurate computing so um

i think once we're done with bayesian inference i mean not just lazy dynamics but the whole community is going to turn out that it is not that it's going to be because of doing it doing bayesian inference that we can do actually scale up intelligent agents to very large problems so asian inference will not be uh the the hindrance it will be a catalyst for scaling up um so that's the i think why people should get interested in uh

in active infants, even if they're still in just researching at this moment.

How is it?


SPEAKER_00:
Excellent.

Thank you, Bart.

I'm very, very, very

keen on the entire methodology here.

Just my last question, and then I'll open it up to everybody else.

So about RxInfer server, I myself, I played around creating a few active inference agents, bona fide agents with RxInfer, which is currently difficult because RxInfer, it specifies the inference procedure and automates the way that that happens.

The actual design of an agent is kind of another thing.

The inference procedure is one part.

But at the moment, I and anybody else making an agent has to do everything kind of ad hoc.

We have to specify, well, if an action happens here, that affects the environment in some specific way.

It's all very ad hoc and so on.

So my understanding is that the agent API

as part of rx and first server is going to perhaps ameliorate these issues or standardize the way in which we talk about how an action influences the environment and vice versa is is that's uh let's say a big focus of the api or is that just now a part of the api in the sense that we can now do much broader things like have endpoints in python and so on so it's kind of a larger uh concern than just this api so


SPEAKER_03:
Yeah, sure.

I think Dmitry would definitely add upon that.

Just from my side, there is actually ongoing discussion that I forgot to mention, like in our GitHub repository about the standardizing API.

I think one of the things which we are also striving with our

uh this ericsson first server and generally like with streamlining this development of agent is actually to standardize how the api should look like that there should be a certain function that should be triggered uh in uh well in the reactive way or in the loop if you prefer imperative style coding but there should be like really uh sort of we want to impose

one way or few ways to actually doing that so yeah another thing is just want to welcome everyone to to this discussion on um uh yeah ericsson for repository where uh you fraser and like and other people from active inference institute also participated but the goal of course is uh the end goal is sort of to to bring in the key components which are needed to yeah run agents uh in the way we run transformers

I mean, low code.


SPEAKER_00:
Okay, I see.

Thank you.

Well, Daniel, I'll hand it back to you.

Maybe we can look at some questions in the live chat.

I'd just like to say thank you very much.

This is an incredible project.

I'm very enthusiastic about the future.

Thank you.


SPEAKER_02:
Awesome.

I'll read some questions in the live chat, and then we'll see if we have time for some of the more

pre-discussed questions or any other questions popping up okay so two questions on computational resources Indian goat wrote can you scale to large data sets and Jeff Beck wrote I would have thought that memory costs would have been the critical bottleneck for scaling care to comment and so to that I'll just sort of also add a little bit of my own computational resourcing just

how do you find that storage memory and cpu cost scale is it possible to do static analysis how do you find that resourcing scales in comparison with alternative methods


SPEAKER_05:
yeah i can start then maybe you can enroll a bit on this vqva for example how can we scale further but the when we talk about scaling and we can look at it from two perspectives the first is the amount of variables we have in the model right so i have a million of variables but all of them would say univariate variables

So for this kind of scaling, Ericsson-Ferr already does a pretty good job.

So because we use factor graphs, and if you have a sparse connectivity, it's only limited indeed by memory in your computer.

And personally, I run inference for two million univariate variables, and it has been done under like five minutes or so to the state space model.

so it was it was pretty easy but another aspect of scalability is indeed the the dimension of each individual variable so for example all the the multivariate and this is indeed this is still a hard problem so if if you have a very large state vector

then indeed we need to propagate their covariances if they're gaussian and this might be tough for which uh now we prefer to use uh the qva

Basically, we compress the state into a smaller dimensional state.

Albert can talk a little bit more about that.

Or, for example, we use techniques similar to mean field, that when you approximate the covariance matrix with a diagonal matrix, it also reduces the memory requirements.

There are many, many, many, many techniques that we can use.

The thing is that Ericsson-Ferr doesn't really choose it for you.

It's more like a playground for a user for now, where they can choose different approximation methods or different techniques, but RxInfer doesn't restrict you here.

You can even try your own approximation techniques, but that would require you to write your own message passing rules, which again, we noticed that many users


SPEAKER_03:
they cannot do that because it's it's hard yeah it's hard and it requires a lot of knowledge on how to do that yeah just to add on on that about this uh what was the question from jeff was like about this memory cost um yeah so i'm going to answer that in in three parts sort of so the first one is to use

uh sorry to say that non-bayesian techniques right so or not entirely bayesian so and that's something like actually active inference community should have been doing like earlier i think so basically what you could do to scale that what mitri said you can use uh semi-bayesian approaches like rational encoders to compress

to basically cast your high-dimensional vector representation into a low-dimensional vector, right?

So that's the basic thing you can actually think of.

You can do it also on multiple levels.

It doesn't have to be only in the likelihoods, right?

You can do it in the states.

I mean, as you move in the hierarchy of your model, you can embed these variational autoencoders one after another.

uh that's one thing uh and another thing is the downside here is that you would require pre-training right so there isn't pre-training involved uh not cool uh sometimes sometimes it's okay so uh another way of tackling that is to actually use hierarchical model representation so

uh you can encode your state see in uh as a sort of hierarchical multi-scale uh model uh right so you don't have to um when you have a grid world for instance you don't have to uh yeah basically model the uh

all these uh uh components of your grid into one state right so like you will end up uh yeah so it doesn't scale well so and we know a few frameworks that adhere to that and well i think it will not progress much but if you actually manage to uh

have sort of multi-scale representation of of this grid world right so you can have sort of like a location in terms of uh you can break it down right so first you can have a big grid world which you can divide by four elements and then you can zoom in zoom in zoom in so basically you would have sort of this granular representation of your of your state so and this scales pretty well um and

Another thing is like, well, it's kind of what Bert touched upon, right, is this vision going forward is indeed like to use this reactive approach to, well, to solve this scaling once and forever, right?

So it's basically the idea here is not to spend much computational resources, just basically spend as much as you can.

So, but

Yeah, that's something work in progress.

Yeah, and I know if Bert, you want to add something on top of it about this computational cost, I think you have something.


SPEAKER_06:
Well, yeah, I mean, we just discussed that, right?

I mean, I think ultimately a Bayesian framework will help computational costs because you have access to how much precision you want for your computations and

you know that means you can stop early you can the the nice thing about the message passing process is reactive message passing is that it you can interrupt it at any time and take the result as it is and if you wait longer you get more accurate result but if you stop early

then you have a less accurate result, but that may be okay.

In Bayesian framework, you have access to how accurately you want to compute.

At the higher levels in these hierarchical models, you don't need a lot of accuracy.

If I want to park my car, I'm not going to worry about the centimeters of where I actually park.

I just want to find a parking spot.

I'm talking about tens, twenties of meters, right?

So it's partly in the architecture and it's partly of doing a smart inference in the end.


SPEAKER_03:
Yeah.

And two more things here.

I just realized that

well you it's so i've mentioned that yeah you can use non-basin approaches to train your uh say variation autoencoder but actually now there are like ways of training your uh neural networks in online manner also so it's it's uh not inherently like prohibitive like to actually uh do learning and inference and in the goal there uh yeah there are some developments in in in that part of uh of that world as well and i think

uh the last piece here is that hardware for bayesian inference is also emerging so it's uh it's something that people are also working on uh it's it's not at mass scale yet but there will be hardware breakthroughs i think that would be specifically targeting this applications for of bayesian inference


SPEAKER_00:
If there are no other comments or questions just now, I would like to follow up on this issue of hierarchical modeling.

Maybe we've exhausted it just now, but it appears to me, as it does to many people, that this issue of hierarchy is going to be very important for just active inference going forward.

This is very much how biological organisms are arranged.

They have characteristic units that deal with their own time scales, and they're connected to some other level above and level below.

I know that in RxInfo you can create bonafide sub-models within other at-model blocks and so on, but I'm interested in the sort of landscape of hierarchical modeling for RxInfo in particular.

Are there things that you guys would like to highlight or emphasize in that regard for users like me?


SPEAKER_03:
I think, Dmitry, maybe you can just tease a bit about this idea of multi-scale or hierarchical models are available right now.

But what's not available out of the box is basically, as you said, different updating schemes for different levels.

So that's something Mitri can elaborate.


SPEAKER_05:
That's what I mentioned about rewriting the core of Ericsson's theorem to also support this feature more naturally.

So I can imagine indeed that we have a really huge model with big hierarchies, but we don't really want to update deep hierarchy as often as we update, for example, the layer that is connected directly to our observations.

uh because yeah as bert mentioned for certain tasks we we don't really need a lot of accuracy so we can we can sacrifice it there and do less accurate predictions but in the current implementation it's a bit hard to do it's like it requires a lot of hacks so we we tried it once it it

In principle, it's possible to do, but it's user unfriendly.

So that's also one of the directions that we want to improve when you're writing Ericsson for tools to better support hierarchical models, to make it easier to initialize hierarchical model, to make it easier to support different update rates for different hierarchies.


SPEAKER_02:
Awesome.

I'll ask a few more questions in the chat.

So Michael Lennon asks, the difficulty of implementation is a great provocation point.

What have been use cases where you have seen some of the greatest benefits from your approach and what have been your most surprising findings?


SPEAKER_03:
Great question.

I think...

if what what what are the grades i look so i think technically if you just go to um what was most surprising i think it's just the fact that uh you use the same technique same inference uh approach same back same back end to tackle all sorts of different problems like here there's many problems on control of course but like it goes way be

beyond that right so there are like some more classical examples on uh outer latent autoregressive models uh gaussian processes uh sensor fusion hand and missing data so i think the benefit of our approach in particular like first of all is that it it works close to real time and certain applications is definitely real time not everywhere uh but uh yeah i think just the uh

like the the fact that it's so versatile in terms of different domains uh while uh kind of connecting all of them under the same uh umbrella principle right so that's that's i think uh the most powerful finding um of uh arix infer in particular uh yeah so it's also um


SPEAKER_06:
a reason why just generally this approach is not super popular, I think, in engineering communities, right?

Inference is not a big name in engineering communities because as engineers, and we are an engineering group,

we are used to start with a problem and then you go to the literature you find 10 possible solutions and you pick one and you change it a bit and now you have 11 solutions approaches this is entirely the other way around for every whatever the problem is we have the same approach we build a model we do inference it's one solution approach to

any model, any problem, right?

So it's completely the opposite way of thinking of problem solving for an engineer.

And it's very hard.

I noticed it with the students in my class.

It's very hard to make that switch from one problem has lots of possible solutions to

We have one solution method for all the problems.

That's very hard, but it works.

I think this is the right way, but it's going to be difficult to make that switch in engineering communities.


SPEAKER_02:
awesome just again to footnote and include some of these great questions and and welcome people into this continuing conversation and open source development so a few more okay urban avsec wrote does rx infer and if so to what extent support structure and parameter learning to train and improve models automatically


SPEAKER_03:
Okay, so yeah, that's a good question.

I wouldn't say that there is an automatic way of doing that.

There are, I think, examples on structured learning to some extent.

And look, so one thing is that people

uh so sometimes people mean different things by referring to structural learning if we're talking about uh finding this kind of so sort of this discrete program search to automatically discover what is the right graph for your particular problem uh it's i think there's a lot of ongoing research uh it's not set in stone how to do it uh and uh i think we have good uh

wouldn't say intuition, we have good understanding on how to prune the models based on this, well, essentially, Bayesian model evidence approaches.

So you can definitely use that within ARIX Infer, but how do you extend the model, how you grow sort of your graph?

I think this is still an open question and an interesting one.

And

as for another way of looking at actually structured learning right or you can think of non-parametric methods including gaussian processes for instance and those are also going to be integrated in rickson fur i think yeah hopefully this year but the idea is that yeah you can just basically discover uh this distribution over your uh

over the functions uh that drive you that explain the dynamics of the environment so uh there's we are certainly working on that from different angles and there is also bias lab that birth is leading that uh also tackles some of these issues uh as for parameter learning yeah i think this is sort of uh i wouldn't say solved right not not nothing is like 100 solved but i think that's something we are uh good at there is no problem with uh

parameter learning and, as I said, pruning that's available.

Yeah.

Awesome.


SPEAKER_02:
Yeah.

And batch and streaming inference for parameter learning.

Okay.

Andrew Pashaya wrote, thank you for the updates.

How are you currently handling multi-agent agent environment loops iteratively, e.g.

creating N agents at the top of the script and using loops or functions for creating and storing them as opposed to hard coding each individual agents?

So what kind of design patterns have you been working with or do you think are relevant for multi-agent situations?


SPEAKER_03:
Yeah, thanks for the question.

Multi-agent is, of course, the pinnacle for all these inference problems.

I think one way is to hack it through your loop.

Another way is...

actually like what you what you see here in this multi-agent path planning is that it's a sort of a super agent so there's this kind of this sort of a meta agent that encapsulates the models of all other agents and then they share this information now the problem is that it doesn't scale well i would say because well like four agents five agents ten agents fine but then like uh you you you you get right you get it so i think

uh one of the ways i mean dmitry probably can can also add upon that but uh what you could do like as for the design pattern is uh you run your agent uh for instance like speaking of this path planning so you run your agent into the you let it go into your environment and you only interact when you are in the proximity of other agents so there is not

need to share any information if you are far away from each other and like once you are getting closer you can start sharing the information you can sort of uh fuse the models figure out the conflict and then just like depart so that's that's one of the things uh I can suggest but uh Andrew's question there is no uh we we didn't think of


SPEAKER_02:
how we didn't let me put it this way we didn't standardize the api ball for multi-agents so not yet all right great and one more question from live chat jeff beck wrote my question is can the parameters of the flows and the vaes be learned in rx infer or are they imported if it is learned is that by traditional gradient descent


SPEAKER_03:
Well, I think this is, if Ismail is still here, I think he can elaborate on that.

But you can... Yeah, maybe I can talk a bit on that.


SPEAKER_04:
So it's possible to update rates or biases of the neural network that is parametrizing your VAE or your flow within RxInfer.

However, if you want to use deep and wide networks,

then you need the GPU architecture that Dimitri was referring to.

So for small

size neural networks really easy.

You can use RxInfer to tune the weights, but for larger scales in its current weight strain outside of the model and then use these parameters for prediction purposes.

But it is certainly possible to incorporate the training of neural networks within the model macros.


SPEAKER_02:
awesome okay in sort of our closing round here one of the pre-written questions was what are the upcoming milestones of lazy dynamics and what is your vision so just on that though or however else you want to give some last thoughts uh if each of the developers would just want to

Give any last thoughts.

What are they looking forward to this year?

Where might we be by the midpoint of the year, by whatever milestone or timeline you want to point to?

And that'll be a great closing round.

So how about Albert and then continuing on with the developers?


SPEAKER_03:
bring this approach in particular to enterprise use.

So we see a lot of opportunities.


SPEAKER_02:
I'm sorry, I made just an audio mistake.

So people might have lost audio for a few seconds.

Could you just start again?


SPEAKER_03:
Start again?

Yes, sorry.


SPEAKER_02:
From where?

All over.

Yeah, sorry about that.


SPEAKER_03:
yeah so uh two two things then uh like first very first thing is that i i definitely want to invite uh so lazy dynamics is going to support the uh open source development of rx infer so that's our priority on the development side uh we're actually going to we're planning to run a sort of not a hackathon but the uh development uh event um co-development event

uh probably this summer uh and uh yeah so i just want to invite more developers to help and testers to help with um agent development uh to help with demos uh even though you are not uh into iris and furry uh sorry active inference yet right so you're everyone is welcome and um one more thing for us i think for this year from the development perspective is of course to

uh bring uh to to put rx defer to the state where it becomes easier to contribute easier to contribute for uh to the sort of to the engine not just on the kind of this demo side but also to improve the engine to make it faster more scalable so that's something we are uh

yeah, looking for, and I want everyone to, uh, invite, uh, to that venture.

Um, and then I think from laser dynamics perspective, uh, specifically, yeah, we're, uh, inviting the partners and like forward thinking, uh, um,

people, individuals, right, who just want to connect and want to join us in this adventure of building this, well, very scalable Bayesian inference solutions.

I think there's a lot of opportunities and possibilities for education and enterprise and yeah, you name it.

So yeah, that's some closing words from me.


SPEAKER_06:
Okay, so from the research side, I think a big issue for active inference is the computation of expected free energy and computation of policies, but minimizing expected free energy.

And I think we're making some nice progress there.

And I think we will also release that this year somewhere at a conference and in the package on how to do this with RxInfer in an efficient way, of course.


SPEAKER_05:
From the development side, of course, as has been mentioned already, we are releasing a Rixenfer server for now for everyone to use, to play.

And as soon as it's finished, it's never finished, but of course, improve it.

But as soon as it has some stable form, we will rewrite the core of our Rixenfer.

And then we also have other pending topics like integrating Gaussian processes very easily, making it very easy to use mixture models, making it easy to introduce hierarchies in the model.

So yes, we have a lot of plans for the development.

We also have plans to improve the visualizations since Fraser helped a lot with that.

And we have plans to improve that even further.


SPEAKER_01:
I think all the improvements and milestones were already mentioned.

So I'm not going to add more to that.

I think we already have quite a big list.

Nonetheless, there are a lot of surprises coming up.

So definitely stay tuned.

I think we already gave a good impression of what direction we'll be working towards.

But along the way, there will always be more cool stuff coming.

So definitely follow us everywhere.

And I think that already gives a good impression of what we'll be up to.


SPEAKER_04:
Yeah, it's hard to add on top of what Albert Barth and all the team said, but as a personal interest, I'm very interested in stochastic differential equations and continuous time systems.

how we numerically approximate these systems identify their parameters and so on and this is a one of the research that we are doing also at rx and how we can integrate them so that's a big research plan for myself

and yeah it's too much on the plate and we could enjoy contribution from everyone to this big plate of ideas pressure where does that take you


SPEAKER_00:
I just want to say I'm extraordinarily excited about continuing to contribute with you guys and develop the package further, maybe even help with some forays into the refactor of the package.

I know that can be intimidating, but so we'll see.

So yeah, I look forward to working with you guys in the future.

I look forward to it all.

Thank you.


SPEAKER_06:
Thanks, Faisal.


SPEAKER_02:
Awesome.

Thank you again for joining and just another chapter in the journey and in the learning and in the application of Active Inference.

So, until next time.

Thank you all.


SPEAKER_06:
Thanks, Daniel.

Thanks all.

Bye-bye.


SPEAKER_03:
Okay, bye.

Bye-bye.
[
  {
    "start": 9.329,
    "end": 9.65,
    "text": " All right.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 9.89,
    "end": 10.171,
    "text": "Hello.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 10.772,
    "end": 13.578,
    "text": "This is Active Inference Model Stream 11.1.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 14.84,
    "end": 16.724,
    "text": "We're with Hadi Vafayi.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 17.285,
    "end": 21.533,
    "text": "We'll be discussing the recent paper, Poisson Variational Autoencoder.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 21.554,
    "end": 23.798,
    "text": "There'll be a presentation and a discussion.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 24.079,
    "end": 25.802,
    "text": "So thank you to you.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 27.082,
    "end": 50.642,
    "text": " all right thanks for having me um all right i'm excited to tell you about this new work that we did along with jake who's my postdoc mentor and deckel who's a phd student in our lab so the big picture motivation behind this work is that we think we're going to understand the brain through the study of brain-like artificial neural networks",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 51.854,
    "end": 53.937,
    "text": " But why do we think this way?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 54.418,
    "end": 63.832,
    "text": "So, all right, think about a dream experiment where we recorded every single neuron in the brain of many animals doing complicated tasks in their natural environments.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 64.032,
    "end": 71.864,
    "text": "We also have electron microscopy connectome of every single neuron, and then learning dynamics is available to us as well.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 72.244,
    "end": 74.908,
    "text": "We know everything about those brains.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 75.8,
    "end": 77.442,
    "text": " This experiment is very difficult.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 77.482,
    "end": 81.308,
    "text": "It's probably going to be impossible for the next, I don't know, 50 million years.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 81.508,
    "end": 86.916,
    "text": "But if we had that data, the challenge is we don't even know what to do with that kind of data set.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 87.257,
    "end": 102.198,
    "text": "So the idea is that we're going to use ANNs as computational models of the brain to generate knowledge and theories and computational models to be able to analyze that kind of data whenever they're available.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 102.599,
    "end": 104.001,
    "text": "So that's the motivation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 106.495,
    "end": 107.557,
    "text": " But there's a challenge here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 107.577,
    "end": 112.547,
    "text": "So if you want to use ANNs to study the brain, you better make them brain-like.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 112.767,
    "end": 118.999,
    "text": "Because as the cliche saying goes, all models are wrong, but some are useful.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 119.861,
    "end": 127.997,
    "text": "And the degree of usefulness of these ANNs corresponds to how brain-like they are.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 128.247,
    "end": 132.293,
    "text": " And in this particular work, I'm going to show you what I mean by brain-like.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 132.413,
    "end": 139.142,
    "text": "This word could mean many different things to different people, but I have a very specific meaning in mind, which I will tell you about.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 140.223,
    "end": 144.73,
    "text": "And in this work, Arvo focuses on models of visual perception.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 144.79,
    "end": 150.998,
    "text": "So we're going to build an ANN model that perceives visual stimuli, and we're going to make it brain-like.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 151.479,
    "end": 152.28,
    "text": "That's the whole idea.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 154.403,
    "end": 160.172,
    "text": " So if you wanna build brain-like models, you better draw inspiration from neuroscience.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 160.953,
    "end": 167.523,
    "text": "And the idea is that we want to narrow down our search space because the space of all ANNs is huge.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 169.005,
    "end": 173.232,
    "text": "And these are the specific, there are three inspirations that we're gonna rely on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 173.452,
    "end": 176.958,
    "text": "Perception as inference, rate coding, and predictive coding.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 176.978,
    "end": 179.442,
    "text": "I'm gonna describe each of these separately.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 181.425,
    "end": 183.027,
    "text": "First, let's start by this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 184.205,
    "end": 188.109,
    "text": " All right, so there's this idea that perception involves two components.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 188.189,
    "end": 197.039,
    "text": "There's this external component that is provided to us by the sensory data, for example, the photons that land on your retina.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 197.84,
    "end": 199.242,
    "text": "And there's this internal component.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 199.342,
    "end": 209.253,
    "text": "You have some subjective experience by living this world that gives rise to prior expectations that is combined with the sensory data to give rise to perceptions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 211.275,
    "end": 214.178,
    "text": "And this idea is not really new.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 214.293,
    "end": 218.298,
    "text": " You can trace it back to over a thousand years ago by Alhausen.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 218.919,
    "end": 227.089,
    "text": "He mentioned vision occurs in the brain rather than the eyes, and he was the first to discuss the subjective elements of perception.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 228.071,
    "end": 237.583,
    "text": "And also more famously, Helmholtz said perceptions are best guess as to what is in the world, given our current sensory evidence and our prior experience.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 237.985,
    "end": 261.023,
    "text": " the current sensory evidence is just this external component and prior experience is the internal component let me give you an example so this is known as the ames room illusion we see this image and we immediately think that oh this must be a giant man and this must be a really short man but in reality um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 261.661,
    "end": 263.383,
    "text": " there's a bit different explanation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 264.204,
    "end": 269.011,
    "text": "So this room is designed in a very specific way to give rise to this illusion.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 269.031,
    "end": 274.678,
    "text": "So we usually think rooms are rectangular, like this dashed line over here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 275.419,
    "end": 276.32,
    "text": "But this room is not.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 276.881,
    "end": 282.268,
    "text": "And the sensory data that is coming from person A on this corner",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 282.248,
    "end": 302.265,
    "text": " is consistent with two different possible explanations one explanation that the room is rectangular and the person is short and the other explanation that the actual explanation is that the room is not rectangular the person standing far behind person a",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 302.245,
    "end": 304.949,
    "text": " And that's why we see it as short.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 305.47,
    "end": 322.034,
    "text": "But because we have this prior expectation built in in our brain by living in this world where every room or most rooms we encounter are rectangular, we immediately just default to this explanation that, oh, yeah, this person must be short.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 322.375,
    "end": 323.176,
    "text": "So that's the idea.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 323.196,
    "end": 329.265,
    "text": "Our prior expectations can actually trick us into perceiving things that are not true.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 332.097,
    "end": 339.441,
    "text": " And you can actually formalize this using mathematics of Bayesian probability.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 340.214,
    "end": 344.381,
    "text": " Here I'm going to introduce you very briefly to the idea of a generative model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 345.182,
    "end": 364.213,
    "text": "Imagine you have some latent variable z that you can sample from, from the prior p of z, and then condition some likelihood function based on whatever you sampled, and then sample some observation x. So x could be an image, z could be some abstract latent variable.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 365.662,
    "end": 377.069,
    "text": " Now, within this context, it's interesting to ask, if I saw some observation, some image x, what are the likely causes underlying generation of x?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 377.47,
    "end": 378.733,
    "text": "And that's the reverse direction.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 378.813,
    "end": 383.805,
    "text": "So we have inference, where we want to",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 383.903,
    "end": 392.092,
    "text": " compute this Bayesian posterior P of Z given X, which according to Bayes' rule is proportional to these terms.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 392.833,
    "end": 394.034,
    "text": "And I'll explain what they are.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 394.775,
    "end": 401.322,
    "text": "So inference goes in the reverse direction from the image to the underlying cause of that image.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 402.884,
    "end": 407.529,
    "text": "And now let's actually relate this back to that quote earlier from Helmholtz.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 407.549,
    "end": 409.331,
    "text": "So he says, perceptions are best guess.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 409.951,
    "end": 413.455,
    "text": "That's the posterior over here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 413.603,
    "end": 427.073,
    "text": " as to what is in the world given our current sensory evidence that's the likelihood function and our prior experience and that's the blue over here so um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 428.251,
    "end": 435.042,
    "text": " We think, or the theory of perception as inference, states that that's really what's going on in the brain.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 435.362,
    "end": 443.014,
    "text": "When you show an image to the brain, the brain thinks, what latent configuration likely caused my current observation?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 443.675,
    "end": 450.807,
    "text": "And in order to answer that, we need to compute a posterior distribution over some latent causes given the observation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 450.827,
    "end": 453.651,
    "text": "This is the essence of perception as inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 456.044,
    "end": 480.904,
    "text": " next thing is rate coding and this idea is okay how do biological neurons communicate and store information we know that neurons produce all or non-action potentials or we call them spike and the first evidence that neurons use so rate coding is the idea that neurons encode information in the rate of these spikes",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 482.2,
    "end": 491.256,
    "text": " And this goes back to all the way to 1920s, where Adrian and Zetterman build amplifiers to be able to measure spikes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 491.556,
    "end": 503.938,
    "text": "And they show that if you apply some stimulus to some neurons, their firing rate increases and in a proportional way to the magnitude of that stimulus.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 504.458,
    "end": 510.529,
    "text": " And this type of figure, you can see it in many other different neuroscience papers.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 512.092,
    "end": 515.859,
    "text": "Here they have some weight that is hanging from the muscle.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 516.02,
    "end": 527.361,
    "text": "But you can actually, this situation happens when you show a stimulus and record from visual neurons or play a sound and record from auditory neurons and so on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 527.425,
    "end": 532.853,
    "text": " So the idea is that there's a baseline firing rate or number of spikes per second.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 533.534,
    "end": 543.909,
    "text": "And then whenever some stimulus is shown to the brain, the firing rate increases, which encodes or represents the presence of that stimulus.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 544.43,
    "end": 545.872,
    "text": "This is the idea of rate coding.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 545.892,
    "end": 547.174,
    "text": "We're going to put this in our model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 550.619,
    "end": 554.665,
    "text": "Last, the idea of predictive coding.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 555.674,
    "end": 560.82,
    "text": " The summary is that brains can be thought of as prediction machines.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 561.44,
    "end": 569.71,
    "text": "This is the first sentence that this beautiful review by Andy Clark starts, that the abstract starts by saying this, basically.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 571.091,
    "end": 575.616,
    "text": "And the summary of the idea is that the brain contains an internal model of the environment.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 576.758,
    "end": 582.424,
    "text": "The brain uses this internal model to predict or anticipate sensory inputs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 582.775,
    "end": 590.307,
    "text": " And then when the sensory inputs actually are received by the brain, they're compared to the predictions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 590.467,
    "end": 597.919,
    "text": "And then there are some errors, of course, because even though we live in a predictable world, it's not fully predictable.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 597.939,
    "end": 601.925,
    "text": "There's always some things that the brain is going to miss, and those are the errors.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 602.646,
    "end": 609.036,
    "text": "And these errors are propagated up the cortical hierarchy to update and maintain this internal model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 610.4,
    "end": 619.892,
    "text": " And this predictive coding idea and perception as inference, they're overlapping concepts, but ultimately they're distinct.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 620.595,
    "end": 622.722,
    "text": "And we're going to put all of these in our models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 623.123,
    "end": 625.406,
    "text": " Here are some references if you are interested.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 625.446,
    "end": 634.096,
    "text": "If you combine these three ideas, it narrows down your search space to this model, which we call Poisson Variational Autoencoder.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 634.857,
    "end": 651.477,
    "text": "The Poisson part comes from rate coding because in computational neuroscience, there's a long history of modeling the observed spike counts from real biological neurons using the Poisson distribution conditioned on some rate variable.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 651.457,
    "end": 668.777,
    "text": " The variational autoencoder part comes from perception as inference, because if you actually formulate what Helmholtz said in modern Bayesian terms, it turns out that these architectures, variational autoencoders, exactly optimize for that loss.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 668.958,
    "end": 673.363,
    "text": "So that's why they're very, very similar.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 674.003,
    "end": 680.451,
    "text": "And then down the line, I'll show you how predictive coding is incorporated in this model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 682.0,
    "end": 690.932,
    "text": " Okay, but before telling you about Poisson variational autoencoder, I should tell you what is a DAE, a variational autoencoder.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 691.753,
    "end": 696.56,
    "text": "So the architecture has two components in the simplest DAEs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 697.501,
    "end": 702.608,
    "text": "There's some input, it could be an image, and it gets encoded.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 702.672,
    "end": 708.102,
    "text": " by this encoder network, or you can refer to it as inference recognition.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 709.144,
    "end": 726.115,
    "text": "You can use these interchangeably to infer some latents or infer some posterior distribution conditioned on x. And then you can sample from the posterior and decode, map this latent back to the observation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 727.023,
    "end": 735.915,
    "text": " And the idea is that when you are encoding this image into some set of Latents Z, this encoding process should be good enough.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 736.055,
    "end": 742.604,
    "text": "The Latents should contain enough information about the input image such that you can actually reconstruct it back.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 743.345,
    "end": 743.986,
    "text": "That's the idea.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 744.527,
    "end": 750.394,
    "text": "And that's what the word autoencoder means, because you're mapping some inputs to itself.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 753.278,
    "end": 756.743,
    "text": "And, you know, VAs can have different forms.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 757.364,
    "end": 765.674,
    "text": " But the last function is you want to actually just do posterior inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 765.694,
    "end": 770.159,
    "text": "And this is what the posterior is, p of z given x proportional to this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 771.841,
    "end": 785.717,
    "text": "And what we want to do is approximate this true but intractable posterior with some q, which we are going to refer to this as approximate posterior.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 785.967,
    "end": 794.039,
    "text": " And we're going to parameterize it with some theta and learn those parameters such that this term here is minimized.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 794.079,
    "end": 802.992,
    "text": "This is the KL divergence of Q and P. You can think of it, if Q is identical to P, this term is going to be zero.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 803.052,
    "end": 807.418,
    "text": "But if Q is different from P, it's going to be non-zero, a positive value.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 809.643,
    "end": 815.591,
    "text": " And actually, if you just do some math and arrange some terms, you end up with this loss.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 816.032,
    "end": 818.936,
    "text": "I'm not going to derive this because this is pretty standard.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 818.956,
    "end": 824.864,
    "text": "You can look up any tutorial or blog post on VAEs, and they're going to explain this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 825.745,
    "end": 834.537,
    "text": "But what we end up with is this term called evidence lower bound or ELBO, which has two components.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 834.77,
    "end": 846.126,
    "text": " So the first component is this expectation over Q log of P of X given Z. And if you think about this, it's like it maps some latents to observations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 847.287,
    "end": 849.29,
    "text": "And we want to maximize this term.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 849.31,
    "end": 859.444,
    "text": "So we are going to refer to this first term as the reconstruction term because it will have a high value if your reconstruction is good.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 860.251,
    "end": 861.494,
    "text": " And there's this other term.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 862.115,
    "end": 872.015,
    "text": "Now, another KL term emerges, but this time between the approximate posterior, E of Z given X, and the prior, which is different than this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 872.096,
    "end": 874.26,
    "text": "And we're going to just refer to this as KL term.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 876.097,
    "end": 881.126,
    "text": " And it's called evidence lower bound because this relationship holds.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 881.146,
    "end": 885.313,
    "text": "Elbow is always less than or equal log probability.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 885.814,
    "end": 887.176,
    "text": "This is called model evidence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 887.356,
    "end": 890.221,
    "text": "That's why evidence lower bound.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 890.241,
    "end": 892.485,
    "text": "It's a lower bound on model evidence.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 892.887,
    "end": 912.933,
    "text": " And if you actually multiply ELBO by minus one, it turns out that this is exactly the variation of free energy, which is larger than surprisal, which is negative log P. And this variation of free energy is exactly the same thing as it appears in Fristonian free energy principle.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 914.896,
    "end": 921.004,
    "text": "So, okay, via ELAS, we're going to maximize ELBO or equivalently minimize variation of free energy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 923.65,
    "end": 929.717,
    "text": " Now let's talk about how do you actually, let's get a little more specific.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 929.737,
    "end": 932.14,
    "text": "So we're going to build a Gaussian VAE.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 933.342,
    "end": 941.051,
    "text": "I should have mentioned that, you know, that the first of probability distributions, all the prior posterior likelihood, that's up to the practitioner.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 941.111,
    "end": 946.097,
    "text": "You have, you know, you have freedom to choose any distribution you want.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 946.499,
    "end": 951.306,
    "text": " But most of the literature in VAEs is actually just using Gaussians for those distributions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 952.067,
    "end": 954.931,
    "text": "Our contribution is we're going to replace Gaussians with Poisson.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 955.011,
    "end": 957.975,
    "text": "But let's actually just understand what Gaussian VAEs are about.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 959.617,
    "end": 963.683,
    "text": "OK, so both prior and posteriors are Gaussians.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 965.225,
    "end": 972.034,
    "text": "The prior is chosen to be a fixed standard Gaussian with zero mean and unit variance.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 972.385,
    "end": 974.269,
    "text": " And this is, again, another choice.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 974.309,
    "end": 980.202,
    "text": "You can actually learn the prior, but in standard VAEs, there's no learnable parameter.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 980.322,
    "end": 981.625,
    "text": "It's just a fixed distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 982.286,
    "end": 990.323,
    "text": "And the posterior is parameterized with some mean and variance that is produced by the encoder network.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 990.343,
    "end": 991.305,
    "text": "And if you just",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 991.943,
    "end": 994.748,
    "text": " compute the KL term, this is what you get.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 995.369,
    "end": 998.273,
    "text": "Again, I'm not gonna derive this because it's pretty standard at this point.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 999.215,
    "end": 1010.273,
    "text": "And from this closed form solution to the KL term, you can see that it will be zero, then mu is zero and sigma is one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1010.253,
    "end": 1024.498,
    "text": " that means the posterior collapses the prior this is known as posterior collapse so if they're identical of course their divergence is going to be minimized which is equal to zero and um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1025.204,
    "end": 1052.308,
    "text": " operationally this is what happens when you have a gaussian ga so you give it and present an image to the encoder and it spits out some mean and variance vectors and it's red that means it's parameterized by the encoder network and so on and then you construct this posterior distribution this is the same as q of z given x and then you sample some latent from it",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1052.795,
    "end": 1058.232,
    "text": " distribution and then you map this sample blatant back to its reconstruction",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1059.68,
    "end": 1061.782,
    "text": " And this is what happens during the forward pass.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1062.123,
    "end": 1064.485,
    "text": "But you want to be able to compute the loss.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1064.545,
    "end": 1072.314,
    "text": "You want to take this x hat and compare it to x and compute gradients, update your parameters.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1072.815,
    "end": 1078.701,
    "text": "But it's impossible to do it if you actually just sample from this posterior.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1079.622,
    "end": 1084.928,
    "text": "And to circumvent this problem, the original VAE paper",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1085.448,
    "end": 1087.192,
    "text": " introduced the reparameterization trick.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1087.713,
    "end": 1101.162,
    "text": "So instead of sampling from this distribution, which is your posterior, you can sample noise from some unit Gaussian, and then compute z as mu plus epsilon times sigma.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1101.142,
    "end": 1116.096,
    "text": " It turns out that sampling from this distribution and this operation, they're exactly identical, but this is better because now you can actually compute gradients and use back propagation to learn your network's parameters.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1117.038,
    "end": 1122.984,
    "text": " So what we're going to do is we're going to replace these Gaussians with Poisson because of the rate coding idea.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1123.065,
    "end": 1124.046,
    "text": "That's the inspiration.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1125.067,
    "end": 1129.432,
    "text": "And in order to make this VAE work, we need to do two things.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1129.852,
    "end": 1136.299,
    "text": "First of all, we need to introduce an equivalent reparameterization trick for the Poisson case.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1136.88,
    "end": 1138.902,
    "text": "We also need to compute the KL term.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1138.922,
    "end": 1141.865,
    "text": "We're going to do both of those in the next few slides.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1144.107,
    "end": 1145.509,
    "text": "All right, so as I mentioned,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1146.012,
    "end": 1148.655,
    "text": " Now we just take the prior and posterior.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1148.675,
    "end": 1150.978,
    "text": "We're going to replace Gaussians with Poisson.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1151.699,
    "end": 1155.543,
    "text": "And so Gaussians need both mean and variance.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1155.583,
    "end": 1159.929,
    "text": "You have to define both a mean and a variance to have a Gaussian distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1160.609,
    "end": 1162.392,
    "text": "But for Poisson, you only need one number.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1162.752,
    "end": 1164.274,
    "text": "That's just the rate parameter.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1165.075,
    "end": 1168.659,
    "text": "And this is the PDF of a Poisson distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1168.979,
    "end": 1172.003,
    "text": "Give me a rate, and I have this distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1171.983,
    "end": 1186.771,
    "text": " So we have this prior rate and the posterior rate, which depends on X. And this could be any general, you can learn the prior rates, you can output some generic posterior rate.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1187.512,
    "end": 1190.818,
    "text": "But here is where we add the predictive coding assumption.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1191.76,
    "end": 1207.604,
    "text": " we say that, okay, look, let's call the prior rates just R and interpret those as some representation units that are just maintaining a prediction or anticipation of what is coming, what kind of sensory information is coming their way.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1208.265,
    "end": 1216.738,
    "text": "And then let's assume that the posterior rates are actually just the modulation of those existing rates.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1217.461,
    "end": 1222.65,
    "text": " So we just say, take R, and the encoder only computes some quantity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1222.67,
    "end": 1227.718,
    "text": "We call it delta R. That is a function of the specific stimulus shown to the encoder.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1228.299,
    "end": 1234.93,
    "text": "And then you obtain the posterior rates by multiplying these two together.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1237.643,
    "end": 1239.825,
    "text": " And this is just an illustration of the idea.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1239.865,
    "end": 1241.928,
    "text": "So you have this input.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1241.948,
    "end": 1252.059,
    "text": "It is processed by some encoder to produce delta r. And it is understood that this delta r is going to interact multiplicatively with existing prior rates.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1252.98,
    "end": 1259.047,
    "text": "And then once you have this multiplication, now you have your posterior distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1259.087,
    "end": 1261.209,
    "text": "This is your approximate posterior.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1261.99,
    "end": 1265.254,
    "text": "And then once you sample from this,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1265.943,
    "end": 1269.523,
    "text": " you're going to get discrete pipe counts.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1269.605,
    "end": 1270.987,
    "text": " because now you have Poisson.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1271.467,
    "end": 1277.835,
    "text": "In the Gaussian case, you would get continuous values, but now we're going to get discrete integers, which is illustrated over here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1277.855,
    "end": 1282.14,
    "text": "Like for example, let's say we have five neurons in the latent space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1282.581,
    "end": 1284.143,
    "text": "We sample from this posterior.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1284.863,
    "end": 1288.988,
    "text": "We have a vector of 1, 3, 0, 0, 1.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1289.389,
    "end": 1296.878,
    "text": "So we take that vector, we pass it through the encoder, and the encoder learns to map it back to whatever image that was shown.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1297.904,
    "end": 1300.988,
    "text": " And these are all learnable parameters.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1301.008,
    "end": 1306.675,
    "text": "So the decoder network, the encoder network, and the prior rates, they're all learnable parameters.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1308.377,
    "end": 1313.664,
    "text": "So I want to emphasize there's like a bunch of difference between Poisson and Gaussian VAEs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1313.924,
    "end": 1316.668,
    "text": "The first difference is, okay, it's Poisson, it's not Gaussian.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1317.368,
    "end": 1319.892,
    "text": "The other difference is prior is learned.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1321.073,
    "end": 1324.958,
    "text": "And then the last difference is that the...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1325.798,
    "end": 1332.586,
    "text": " These prior rates are parameters that are shared between both the prior and the approximate posterior.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1335.63,
    "end": 1337.953,
    "text": "Now let's talk about the Poisson reparameterization trick.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1338.133,
    "end": 1348.847,
    "text": "We want to be able to, when we sample event counts or spike counts from this Poisson distribution, we want to be able to differentiate through this operation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1348.967,
    "end": 1350.709,
    "text": "So how can we do that?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1352.478,
    "end": 1364.377,
    "text": " In order to show you the intuition behind our reprimetization trick, let's actually focus on the process that generates event counts.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1364.397,
    "end": 1371.668,
    "text": "So if z is some event count that is distributed according to Poisson, we know that",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1372.643,
    "end": 1378.909,
    "text": " there are some inter-event intervals that are distributed according to this exponential distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1379.59,
    "end": 1388.898,
    "text": "What delta t is, is that suppose one event has happened, how long do you have to wait until you observe another event?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1389.559,
    "end": 1396.786,
    "text": "Those wait times, inter-event intervals, they're distributed according to this exponential distribution with parameter lambda.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1397.667,
    "end": 1401.25,
    "text": "And this is just a well-known property of Poisson distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1401.652,
    "end": 1407.361,
    "text": " We're going to use this, we're going to exploit this fact to introduce our trick.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1409.324,
    "end": 1426.09,
    "text": "All right, so now suppose we want to actually just run this process, sample many times from this exponential distribution, and then in a finite window of observation time of one second, we want to see how many events would we get.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1426.88,
    "end": 1432.554,
    "text": " So when we sample from this exponential, maybe we get first wait time is delta t1.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1432.614,
    "end": 1436.644,
    "text": "So we wait this much until we observe the first event.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1437.707,
    "end": 1438.409,
    "text": "We sample again.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1438.449,
    "end": 1441.396,
    "text": "It takes this much time to observe the second event.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1442.258,
    "end": 1444.163,
    "text": "Again, third event.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1444.818,
    "end": 1450.625,
    "text": " But the fourth event doesn't really occur between 0 and 1, which is our previously defined observation time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1451.065,
    "end": 1452.847,
    "text": "Therefore, this doesn't count.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1453.488,
    "end": 1460.195,
    "text": "So we have a total of three events because the sum of these three delta t's is less than 1.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1460.235,
    "end": 1473.39,
    "text": "So sampling these from exponential and then counting them like that, like we did, is equivalent to just sampling from Poisson and getting number 3 as our spike count.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1474.889,
    "end": 1479.813,
    "text": " So we can actually just sample these numbers in parallel from the exponential distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1480.516,
    "end": 1482.486,
    "text": "We compute their cumulative sum.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1483.394,
    "end": 1485.656,
    "text": " which is visualized over here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1485.716,
    "end": 1496.727,
    "text": "And we can just apply the thresholds and say, all right, whatever number of cumulative sums that there's less than one, let's take that as our spike count.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1497.207,
    "end": 1502.052,
    "text": "And here, which means we're going to have three events, and this is not going to count.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1502.392,
    "end": 1507.117,
    "text": "And any other delta Ts after this is not going to count because it's just going to add over here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1507.157,
    "end": 1510.38,
    "text": "It extends beyond our wait time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1510.85,
    "end": 1512.757,
    "text": " So again, we have total event counts of three.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1513.62,
    "end": 1517.013,
    "text": "It turns out that you can just take this and turn it into an algorithm.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1519.001,
    "end": 1522.092,
    "text": "So the input to the algorithm is just some rates.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1522.223,
    "end": 1545.379,
    "text": " are produced by the encoder network then we have we have to specify how many exponential samples that we want to generate there's a temperature that that controls the sharpness of the thresholding so if you want to threshold put a hard threshold of like anything less than one that's not going to work you have to approximate that with some sigmoid that's what we have over here",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1546.523,
    "end": 1549.246,
    "text": " Okay, so that's our reparameterization trick.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1549.527,
    "end": 1556.255,
    "text": "And we hope that this trick, it can be used beyond the realm of VAEs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1556.275,
    "end": 1561.162,
    "text": "For example, in spiking neural networks, people deal with this issue all the time.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1561.522,
    "end": 1565.507,
    "text": "It's really difficult to work with discrete stochastic variables.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1565.527,
    "end": 1572.115,
    "text": "So we're hoping that this algorithm will find applications elsewhere as well.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1572.135,
    "end": 1575.72,
    "text": "But now let's talk about the loss function derivation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1576.257,
    "end": 1581.744,
    "text": " As a reminder, we have our prior given like this and the posterior given like this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1582.465,
    "end": 1584.968,
    "text": "And our goal is to compute the KL term.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1586.73,
    "end": 1589.914,
    "text": "So let's first start by just the definition of the KL term.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1590.855,
    "end": 1603.21,
    "text": "For any distribution Q and P, the KL between Q and P is this expectation over Q of log of Q divided by P.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1603.713,
    "end": 1610.404,
    "text": " So far, this is just general definition, but now we're going to plug in the actual Poisson distributions that we have.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1612.227,
    "end": 1613.99,
    "text": "So just put that over here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1614.271,
    "end": 1618.798,
    "text": "We define lambda to be r times delta r. It's just our posterior rate.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1619.66,
    "end": 1622.244,
    "text": "And this is what Poisson distribution looks like.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1622.264,
    "end": 1624.067,
    "text": "So just replace q and p.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1625.077,
    "end": 1631.526,
    "text": " And immediately we see that z factorials cancel, and we can actually just write this term as follows.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1632.227,
    "end": 1642.541,
    "text": "It's just log of lambda over r to the power z plus log of e to the minus lambda plus r. And then z comes down.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1642.981,
    "end": 1650.531,
    "text": "Lambda over r is delta r. So we're going to just end up with z log delta r. And log of e to the something is very simple.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1650.571,
    "end": 1653.295,
    "text": "It's just, you know, you bring the exponent down.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1654.0,
    "end": 1661.467,
    "text": " And in this entire expression, the only thing that has an expectation is Z. Everything else is a constant, so it comes out.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1662.327,
    "end": 1666.751,
    "text": "And the expectation of Z over Q is just the mean.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1667.051,
    "end": 1670.634,
    "text": "And the mean of Poisson distribution is just the rate.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1671.195,
    "end": 1678.802,
    "text": "So Z is going to be replaced by lambda, which is the rate of Q. And that's what we get here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1678.862,
    "end": 1682.725,
    "text": "So R minus lambda plus lambda log delta R.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1683.734,
    "end": 1708.392,
    "text": " And again, you just take r, take it out, and it's like 1 minus delta r plus delta r log delta r. So we see that nicely by having, by virtue of choosing this kind of predictive coding-like assumptions, our KL term nicely factorizes to a term that depends on prior and another term that depends on the modulation that comes from the encoder.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1708.372,
    "end": 1720.896,
    "text": " and then we define this whole thing in parentheses to be f so that's f is one minus this plus whatever and if you plot it it looks like this so delta r is by definition always positive",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1720.994,
    "end": 1722.076,
    "text": " It's greater than zero.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1722.096,
    "end": 1727.827,
    "text": "And if delta R is one, it says like, okay, do not modulate anything.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1728.108,
    "end": 1730.653,
    "text": "Let's just keep the prior rate.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1731.334,
    "end": 1733.959,
    "text": "Then this F becomes zero.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1733.979,
    "end": 1740.893,
    "text": "And if delta R is really large, you pay the cost of, you know, it really increases the magnitude of F of Y.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1742.594,
    "end": 1745.82,
    "text": " So, OK, so this KL term was just for one neuron.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1746.741,
    "end": 1760.627,
    "text": "If you have K statistically independent neurons, K being the dimensionality over a latent space, you can just easily extend this derivation to see that the KL term is just the sum of these terms.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1761.909,
    "end": 1763.191,
    "text": "And now this is the interesting part.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1763.211,
    "end": 1766.998,
    "text": "We're going to interpret this term as a metabolic cost term.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1768.497,
    "end": 1775.272,
    "text": " The reason is, the prior firing rates, they are positive values by definition.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1776.375,
    "end": 1781.045,
    "text": "And we ended up with some term, F, that also is positive by definition.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1781.085,
    "end": 1785.375,
    "text": "So you have a positive term multiplied by another positive term.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1785.878,
    "end": 1790.242,
    "text": " So this whole thing is just a positive value, and we want to minimize it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1791.484,
    "end": 1797.67,
    "text": "And we can minimize this, for example, by just putting zeros in all of our prior rates.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1797.75,
    "end": 1799.191,
    "text": "All of these neurons are silent.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1799.231,
    "end": 1800.132,
    "text": "They're not doing anything.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1801.013,
    "end": 1813.185,
    "text": "And that's why we are interpreting this as a metabolic cost, because producing action potentials is a very metabolically costly operation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1813.165,
    "end": 1821.7,
    "text": " The neurons in the brain, they don't want to fire action potentials unless it's absolutely necessary to encode some information or communicate something.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1822.701,
    "end": 1827.85,
    "text": "That's why this is very interesting that it just comes out of the math.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1830.428,
    "end": 1835.739,
    "text": " And if you just put everything together, this is just the loss of Poisson VAE.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1836.32,
    "end": 1838.425,
    "text": "The first term is the reconstruction term.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1839.948,
    "end": 1844.758,
    "text": "It's just the L2 norm of x input.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1845.734,
    "end": 1847.596,
    "text": " minus reconstruction.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1847.856,
    "end": 1854.021,
    "text": "This is like decoder network applied to one sample, which is drawn from the posterior.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1854.822,
    "end": 1861.428,
    "text": "So this is the reconstruction plus this term, which comes from the KL term.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1861.448,
    "end": 1873.298,
    "text": "And this is very curious because now we have some term that says reconstruct the image and another term that says do it while optimizing or minimizing neural resource use.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1873.318,
    "end": 1875.64,
    "text": "And this is reminiscent of sparse coding.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1876.733,
    "end": 1878.075,
    "text": " which I will introduce here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1878.855,
    "end": 1883.34,
    "text": "So in 96, Olshausen and Field introduced sparse coding.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1883.721,
    "end": 1894.853,
    "text": "They said, OK, let's imagine there are some images x and some latent variable z, or let's say neural activation z in their case.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1895.254,
    "end": 1904.344,
    "text": "And let's assume that we represent these images as a linear sum of some basis elements from a dictionary phi.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1905.488,
    "end": 1917.979,
    "text": " While we're representing these images, let's also try to minimize the magnitude of neural activity, which is this L1 term over here, and just minimize this whole loss.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1918.54,
    "end": 1920.762,
    "text": "That'll give you sparse coding.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1920.782,
    "end": 1928.789,
    "text": "Sparse because if you apply this term, it's going to push a lot of these neural activities to zero.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1928.809,
    "end": 1931.692,
    "text": "It's going to give you a sparse activity vector.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1932.973,
    "end": 1935.175,
    "text": "And this really resembles",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1935.543,
    "end": 1937.585,
    "text": " what we get from PVAE.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1939.908,
    "end": 1947.716,
    "text": "And in order to actually make this connection more clear, we just take this general decoder term that appears in the loss.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1947.956,
    "end": 1951.52,
    "text": "We also assume we have a linear decoder, just like sparse coding.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1952.32,
    "end": 1955.864,
    "text": "And it turns out, in that case, you have a closed form solution for the loss.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1956.805,
    "end": 1958.327,
    "text": "I'm not going to derive this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1958.467,
    "end": 1959.888,
    "text": "It's in the paper, if you're curious.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1959.968,
    "end": 1962.411,
    "text": "But there is a closed form solution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1962.391,
    "end": 1969.903,
    "text": " And in the actual, you know, full loss, there is this expectation value which you can compute.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1970.124,
    "end": 1972.333,
    "text": "It gives you this final loss.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1973.647,
    "end": 1977.132,
    "text": " And again, we see this correspondence nicely.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1977.152,
    "end": 1984.542,
    "text": "So there is one term that says faithfully represent images with some linear decoder.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1985.844,
    "end": 1991.032,
    "text": "And then there are two terms here, actually, as opposed to just one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1991.572,
    "end": 1996.92,
    "text": "There are two terms that push your firing rates to low values.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 1996.9,
    "end": 2003.03,
    "text": " So this diagonal of phi transpose phi is positive by definition and lambda is positive.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2003.09,
    "end": 2005.273,
    "text": "So this thing needs to minimize.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2005.293,
    "end": 2007.437,
    "text": "Therefore, it forces lambda to minimize.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2008.218,
    "end": 2015.93,
    "text": "And again, just a reminder that this is coming from the KL term and be interpreted as a metabolic cost.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2016.471,
    "end": 2019.195,
    "text": "OK, so let's just pause a little bit here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2019.918,
    "end": 2023.951,
    "text": " We started from all those three neuroscience inspirations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2024.412,
    "end": 2028.806,
    "text": "We said, we want to build a model that does, you know, perceives visual inputs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2028.966,
    "end": 2030.451,
    "text": "We want to build in",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2031.055,
    "end": 2036.222,
    "text": " perception as inference, rate coding, and predictive coding, all of those in the model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2036.782,
    "end": 2041.368,
    "text": "It gave us the Poisson VAE idea.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2042.029,
    "end": 2050.32,
    "text": "We derived the loss, and we saw that it resembles sparse coding, so it's just gifted to us by the theory.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2050.34,
    "end": 2052.002,
    "text": "Now we want to test this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2052.322,
    "end": 2059.712,
    "text": "We want to say, okay, the loss resembles sparse coding, but let's actually just train some models and see if we get sparse coding-like results.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2059.692,
    "end": 2061.821,
    "text": " That's what we're going to do next, the experiments.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2064.532,
    "end": 2069.192,
    "text": "Oh yeah, so this is the architecture of linear PVAE.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2069.212,
    "end": 2069.955,
    "text": "So you have",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2070.83,
    "end": 2079.587,
    "text": " Again, some image is encoded into delta R multiplied by R to give you this posterior.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2080.148,
    "end": 2083.074,
    "text": "And importantly, now the decoder is just a linear layer.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2083.274,
    "end": 2084.476,
    "text": "That's what this is about.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2084.997,
    "end": 2093.614,
    "text": "And another important assumption that is central to sparse coding is that you have to have an over-complete latent space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2093.594,
    "end": 2109.983,
    "text": " many more latent dimensions or many more neurons than you have pixels so m is the number of pixels of the input image k is your latent dimensionality now we're going to train this model and compare it to sparse coding",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2111.785,
    "end": 2124.599,
    "text": " But also, in order to understand what components of the PVAE might reproduce sparse loading, we're going to compare it to alternative VAE models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2125.299,
    "end": 2127.221,
    "text": "So we have discrete VAEs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2127.882,
    "end": 2130.505,
    "text": "We have PVAE over here, which is what we introduced.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2130.945,
    "end": 2133.228,
    "text": "And we're going to use categorical VAE.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2133.828,
    "end": 2138.113,
    "text": "And there are continuous VAEs, Gaussian VAE, and Lock-Bus VAE.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2138.482,
    "end": 2153.205,
    "text": " And the reason we chose Laplace to include in this table is that people have shown before us that if you have a Laplace distributed prior, you can also get sparse coding-like results.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2153.225,
    "end": 2159.916,
    "text": "So we want to test which one of these models is actually more sparse coding-like, if you will.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2161.448,
    "end": 2167.255,
    "text": " And then we're going to compare to actual, you know, very well-established standard sparse coding algorithms.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2167.936,
    "end": 2171.781,
    "text": "One is the locally competitive algorithm from 2008.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2171.861,
    "end": 2178.77,
    "text": "It's a biologically inspired algorithm that optimizes the sparse coding loss.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2179.11,
    "end": 2181.874,
    "text": "And another algorithm is ISTA.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2181.894,
    "end": 2187.561,
    "text": "These are both very standard and well-established as well as the Gaussian.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2187.601,
    "end": 2190.865,
    "text": "So you're going to compare to a bunch of models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2192.38,
    "end": 2201.576,
    "text": " So if you train this PVAE on linear PVAE on natural image patches, this is what we get.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2201.777,
    "end": 2202.999,
    "text": "These are the decoder weights.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2204.241,
    "end": 2210.372,
    "text": "And if you're in neuroscience, in visual neuroscience, you already know that these are",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2210.588,
    "end": 2222.525,
    "text": " We refer to these as like Gabor patches, and these resemble the selectivity in the primary visual cortex of monkeys and mice and humans.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2223.647,
    "end": 2226.331,
    "text": "So this is very biology-like.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2226.791,
    "end": 2235.724,
    "text": "And for the first time, these types of Gabor patches, they emerge from the sparse coding models, which we were able to reproduce with PVAE.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2236.846,
    "end": 2239.65,
    "text": "And if I show you the other models,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2240.677,
    "end": 2243.922,
    "text": " It's not actually trivial, but let's go from left.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2243.942,
    "end": 2245.624,
    "text": "So we have Gaussian VAE over here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2245.644,
    "end": 2263.791,
    "text": "I'm not sure if you can see, but there's a lot of noise and there's only like last three rows look, you know, they look like PCA and that's expected because a linear Gaussian VAE is theoretically equivalent to probabilistic PCA.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2264.933,
    "end": 2268.017,
    "text": "Now we have also Laplace VAE down here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2268.098,
    "end": 2279.953,
    "text": " There are some Gabors that emerge from the Laplace VAE, but again, there's a lot of dead neurons, which I will define what dead neuron means, and I'm going to show how we quantify them.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2280.113,
    "end": 2284.919,
    "text": "But for now, let's just visually inspect these learned decoder rates.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2286.18,
    "end": 2289.865,
    "text": "This is the Poisson VAE, and this is the categorical VAE.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2289.885,
    "end": 2296.553,
    "text": "Categorical VAE is discrete, so it also learns some Gabors, but it's very limited.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2297.36,
    "end": 2306.476,
    "text": " that there's a lot of noisy elements and certain spatial frequencies are missing and certain orientations are missing as well from the categorical VAE.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2307.357,
    "end": 2312.847,
    "text": "And finally, in the last column, we have these results from the standard sparse coding algorithms.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2312.867,
    "end": 2320.6,
    "text": "These are the best, you know, game in town if you want to learn sparse coding like dictionary elements.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2321.288,
    "end": 2332.14,
    "text": " And we see that among the VAE models, Poisson VAE actually is the most similar one to these standard sparse coding algorithms.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2333.161,
    "end": 2348.578,
    "text": "So in conclusion, because both the loss function of a PVAE just emerged to resemble sparse coding, and also empirically we tested it and also learned sparse coding-like results,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2348.795,
    "end": 2352.961,
    "text": " we conclude that it actually contains sparse coding as a special case.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2353.902,
    "end": 2354.163,
    "text": "Why?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2354.323,
    "end": 2357.407,
    "text": "Because we made a linear decoder assumption.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2357.567,
    "end": 2360.952,
    "text": "So it's like PVAE can be general.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2361.092,
    "end": 2361.914,
    "text": "It could be anything.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2361.934,
    "end": 2364.978,
    "text": "It could be nonlinear encoder, nonlinear decoder.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2365.439,
    "end": 2372.068,
    "text": "But if you make the decoder linear and make the latent space overcomplete, you're going to recover sparse coding-like results.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2374.242,
    "end": 2385.889,
    "text": " okay so here's um how i quantify number of dead neurons this is just uh let me just first define what the posterior collapse is this is a big kind of prevalent issue in daes",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2386.527,
    "end": 2411.657,
    "text": " when a latent dimension stops encoding information it's going to be always the posterior in that latent dimension is going to be identical to the prior so there's no deviation the KL term is very small or zero and that's how we measure dead neurons and here this dashed line shows the gap between the post the KL of",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2411.637,
    "end": 2437.56,
    "text": " dead neurons and active neurons that's how we quantified and we found that for the pvae across three different data sets it consistently had much larger number of active neurons compared to lactose and gaussian vaes so it kind of automatically avoids this posterior collapse issue because of discreteness that's the main cause of this observation",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2440.358,
    "end": 2461.894,
    "text": " okay so we want to actually do some other experiments um so far we just saw that if you start from those neuroscience inspirations you get a model that also you know recovers or reproduces some other neuroscience-like uh models results like sparse coding but now let's just um",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2462.515,
    "end": 2463.577,
    "text": " be a little more general.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2464.118,
    "end": 2468.887,
    "text": "We want to evaluate PVAE in a general representation learning setting.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2469.628,
    "end": 2470.85,
    "text": "That's what this is about.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2470.87,
    "end": 2479.065,
    "text": "All right, so we want to test these unsupervised representations on a downstream classification task, and we use MNIST for that purpose.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2479.646,
    "end": 2482.431,
    "text": "We're going to compare all those four VAE models for this task.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2483.221,
    "end": 2489.29,
    "text": " The approach is that after the training is done, we extract encoder representations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2489.791,
    "end": 2494.178,
    "text": "This means we push MNIST digits from the validation set through the encoder.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2494.819,
    "end": 2498.905,
    "text": "We extract the representations, and we're going to do things with those.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2498.925,
    "end": 2499.706,
    "text": "All right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2499.726,
    "end": 2504.233,
    "text": "And then once you have these representations, sample a limited number of supervised labels.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2504.754,
    "end": 2506.637,
    "text": "So MNIST, they have labels.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2506.677,
    "end": 2509.682,
    "text": "We know each image of a digit has a label.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2510.002,
    "end": 2511.945,
    "text": "So it's a supervised data set.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2511.925,
    "end": 2520.077,
    "text": " But we're going to look at sub-samples, like very small samples of supervised subsets.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2521.139,
    "end": 2525.786,
    "text": "And the reason is that, you know, having labels is expensive.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2525.806,
    "end": 2532.657,
    "text": "So if a model can achieve good performance with a small subset of supervised labels is a good model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2532.677,
    "end": 2533.478,
    "text": "That's the idea.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2534.538,
    "end": 2541.606,
    "text": " Then we're gonna apply k-nearest neighbor classifier to classify these digits.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2542.967,
    "end": 2546.691,
    "text": "And the idea of using KNN is that it's a non-parametric model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2546.731,
    "end": 2549.915,
    "text": "So you don't need to learn anything on top of these representations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2550.475,
    "end": 2555.461,
    "text": "And the output of a KNN classifier directly depends on the geometry of representation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2555.481,
    "end": 2564.19,
    "text": "So it's gonna tell us something about if there's something fundamentally different about the geometry of representations in one of these VAEs like PDAE versus the others.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2565.267,
    "end": 2570.554,
    "text": " And then as a measure of success, we're going to report classification accuracy.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2572.037,
    "end": 2572.778,
    "text": "This is the results.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2574.6,
    "end": 2576.683,
    "text": "We have different latent dimensionalities.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2576.723,
    "end": 2577.645,
    "text": "We have different models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2578.125,
    "end": 2582.471,
    "text": "There's a lot of details that I'm going to skip, but let me highlight just one thing.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2582.792,
    "end": 2584.474,
    "text": "So let's look at this number.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2584.534,
    "end": 2594.228,
    "text": "So for 10 latent dimensions with only 200 samples, the PVAE achieves accuracy of around 82%.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2595.389,
    "end": 2605.058,
    "text": " But Gaussian VAE requires five times that number, a thousand label samples to achieve a similar performance.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2605.959,
    "end": 2612.785,
    "text": "In this sense, we claim that PVAE is five times more sample efficient than the standard Gaussian VAE.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2613.506,
    "end": 2622.915,
    "text": "And across the board, it just outperforms all these other models, which shows that it learns useful representations that can be applied in downstream tasks.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2624.701,
    "end": 2627.126,
    "text": " Finally, let's quantify sparsity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2627.507,
    "end": 2630.673,
    "text": "We're going to use this measure from Vinci and Gallant.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2631.014,
    "end": 2632.737,
    "text": "This is called lifetime sparsity.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2633.96,
    "end": 2644.06,
    "text": "Zi is the response to i-th stimulus, number of spikes in the Poisson VAE, and n is the total number of stimuli.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2644.732,
    "end": 2657.665,
    "text": " Just the intuition about this formula is that if a neuron only responds to a single stimulus and doesn't respond to any other stimuli, the sparsity score is going to be one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2658.346,
    "end": 2666.314,
    "text": "And if the neuron or the latent dimension responds to all stimuli equally, then the sparsity score is going to be zero.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2666.934,
    "end": 2668.736,
    "text": "So zero means not sparse at all.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2669.477,
    "end": 2671.819,
    "text": "One means very sparse.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2673.672,
    "end": 2674.313,
    "text": " And we use that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2674.333,
    "end": 2680.906,
    "text": "This is the y-axis, the lifetime sparsity of these different VAE models.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2681.207,
    "end": 2685.636,
    "text": "And on the x-axis, we have the reconstruction error and mean squared error.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2687.86,
    "end": 2692.93,
    "text": "We see that Poisson VAE is very sparse.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2693.467,
    "end": 2700.795,
    "text": " But because of that sparsity, it has larger reconstruction error compared to, let's say, Gaussian, which is down here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2701.676,
    "end": 2709.645,
    "text": "And it turns out if you apply some ReLU to the Gaussian, it's going to increase the sparsity, but not as much as Poisson.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2711.087,
    "end": 2718.956,
    "text": "And then this curve over here came from fitting a sigmoid to a bunch of Poisson DAEs that were trained with different betas.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2718.976,
    "end": 2722.36,
    "text": "So if you remember, beta is a parameter that...",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2722.762,
    "end": 2728.588,
    "text": " kind of changes the trade-off between the reconstruction term and the KL term.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2729.429,
    "end": 2738.238,
    "text": "So if you increase beta, you're going to get much sparser results, but at a cost of larger reconstruction errors.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2739.038,
    "end": 2744.504,
    "text": "And if you decrease the beta, you're going to get less sparse results, but better reconstruction.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2744.524,
    "end": 2748.448,
    "text": "So that kind of traces rate distortion trade-off.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2750.437,
    "end": 2760.227,
    "text": " Finally, one thing that we notice is that there's still a large amortization gap for the PoSAM DAE, and I'll explain what that means.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2760.359,
    "end": 2781.366,
    "text": " So if you compare Poisson VAE with those standard sparse coding algorithms, including LCA, which is this dark gray, and ISTA, which is this light gray, we see that they achieve the same level of sparsity roughly, but Poisson VAE still has a larger reconstruction error.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2781.987,
    "end": 2783.869,
    "text": "But what happened here?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2784.11,
    "end": 2788.375,
    "text": "Why is LCA much better than Poisson VAE?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2789.148,
    "end": 2799.406,
    "text": " So to test this, we thought that maybe the dictionary elements learned by Poisson VAE are pretty good, but the actual inference is not very good.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2799.527,
    "end": 2803.955,
    "text": "The encoder architecture is not doing a good job in inferring the activations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2804.496,
    "end": 2809.705,
    "text": "So what we did is we took the Poisson VAE and we took, sorry,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2809.685,
    "end": 2814.872,
    "text": " The dictionary elements learned by the Poisson, we even applied LCA inference on those.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2815.473,
    "end": 2820.981,
    "text": "And that gave us these purple dots, which we also fit a sigmoid to.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2821.602,
    "end": 2832.638,
    "text": "And it turns out that you can actually just close the amortization gap by applying a better inference, which in this case is the LCA inference, to just bring this over here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2833.664,
    "end": 2858.085,
    "text": " So the conclusion from this slide is that the Poisson VAE learns reasonable dictionary elements, but its encoder or the encoder architectures we tried in this paper, they weren't adequate and there was this large amortization gap that in the future work, we hope to kind of close this gap by building better encoders, maybe iterative encoders and so on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2859.145,
    "end": 2860.448,
    "text": " OK, let's just conclude.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2860.508,
    "end": 2865.801,
    "text": "So we started from these three inspirations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2866.262,
    "end": 2869.189,
    "text": "Perception as inference, rate coding, predictive coding.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2869.71,
    "end": 2871.855,
    "text": "It gave us this architecture.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2873.168,
    "end": 2881.481,
    "text": " And it's interesting for many reasons because, you know, Poisson VAE encodes its inputs in discrete spike counts.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2881.982,
    "end": 2884.967,
    "text": "Therefore, it's more biologically realistic.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2885.808,
    "end": 2889.574,
    "text": "And this metabolic cost term emerged in the model objective for free.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2889.594,
    "end": 2891.177,
    "text": "You didn't have to",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2891.157,
    "end": 2911.073,
    "text": " put it by hand or anything and it actually revealed this connection to sparse coding which we verified empirically and then from a theoretical perspective it kind of unites all these different ideas like predictive coding rate coding sparse coding under the umbrella of bayesian inference which is nice",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2912.47,
    "end": 2924.185,
    "text": " And then the takeaway message is that, if you draw inspiration from centuries of neuroscience research, combine it with modern machine learning, you might get a model that's more brain-like than alternatives.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2927.108,
    "end": 2931.093,
    "text": "And it outperforms alternatives in key aspects like sample efficiency.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2932.715,
    "end": 2934.017,
    "text": "All right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2934.037,
    "end": 2936.079,
    "text": "This is all I had for the slides.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2936.3,
    "end": 2941.566,
    "text": "Now, if there are questions, you could go over there, or maybe I can show the code.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2942.896,
    "end": 2943.297,
    "text": " Thank you.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2943.357,
    "end": 2944.318,
    "text": "That was awesome.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2944.338,
    "end": 2944.679,
    "text": "Yeah.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2944.699,
    "end": 2945.56,
    "text": "How about the code?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2945.961,
    "end": 2949.526,
    "text": "And meanwhile, for people who are watching, write any questions, and then I'll ask them.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2950.268,
    "end": 2950.608,
    "text": "Thank you.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 2953.653,
    "end": 2954.114,
    "text": "All right.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2955.055,
    "end": 2962.727,
    "text": "Yeah, so I actually haven't prepared anything specific to say about the code, but I'm just gonna riff through, you know, what exists.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2963.652,
    "end": 2966.576,
    "text": " Hopefully some people might find this inspiring.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2967.017,
    "end": 2970.121,
    "text": "Okay, so this code is actually not online right now.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2970.201,
    "end": 2978.333,
    "text": "It's not available, but because the paper is under review, but once the paper gets out, I'm gonna make this code publicly available.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2979.474,
    "end": 2989.288,
    "text": "So then maybe the more important part of the code is that you go in the code in the VAE, you have VAE.py,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2989.403,
    "end": 2995.173,
    "text": " which has all the implementations of different VAEs that we discussed in this talk.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 2995.193,
    "end": 3000.421,
    "text": "So there's a base VAE that has all these different functions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3001.203,
    "end": 3004.668,
    "text": "It encodes inputs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3004.688,
    "end": 3008.234,
    "text": "You can have a convolutional encoder.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3008.254,
    "end": 3010.558,
    "text": "You can have a linear encoder, MLP encoder.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3010.578,
    "end": 3011.8,
    "text": "It's all implemented.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3013.096,
    "end": 3026.667,
    "text": " know same with decoder you have convolutional linear mlp it computes the loss which is just the nsc and you can also compute the exact loss when you have a linear decoder",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3027.608,
    "end": 3036.797,
    "text": " So the KL loss is not implemented because we're gonna have specific instances of VAEs like Poisson VAE, Gaussian VAE, et cetera.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3037.478,
    "end": 3039.48,
    "text": "And they each have their own KL loss.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3039.5,
    "end": 3043.984,
    "text": "So that's not implemented over here in the base VAE and so on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3044.004,
    "end": 3046.406,
    "text": "There's a bunch of different functions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3046.606,
    "end": 3050.07,
    "text": "So let me just show you the actual implementations.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3053.073,
    "end": 3055.455,
    "text": "So Poisson VAE is over here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3057.325,
    "end": 3087.012,
    "text": " the forward direction you have this this infer function that accepts x which is an image temperature and it computes so the self.log rate this is the prior rates these are parameters that are learned along with every other parameter and you have this log r then you have log dr dr is just a delta r that's the output of the encoder where you encode the image as an output you get log dr",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3087.481,
    "end": 3090.526,
    "text": " And then, let's skip this part.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3091.427,
    "end": 3094.251,
    "text": "Then you compute, you make this distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3094.752,
    "end": 3100.26,
    "text": "And just to remind yourself that dist is just plus on, which I will show you later.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3100.3,
    "end": 3101.762,
    "text": "It's defined somewhere else.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3102.764,
    "end": 3104.547,
    "text": "And then you make this distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3104.787,
    "end": 3106.069,
    "text": "You provide the log rates.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3106.329,
    "end": 3107.471,
    "text": "This is the Nx.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3107.491,
    "end": 3110.856,
    "text": "If you remember from the reparameterization,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3111.798,
    "end": 3133.401,
    "text": " slide you have to provide how many exponential samples you're going to draw which you know it's it's adaptively computed somewhere else in the code i'll tell you where later you provide the temperature and then you have a distribution which you know this is going to be the infra function provides you with the distribution that's your approximate posterior",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3134.123,
    "end": 3137.648,
    "text": " And then you sample from it and get spikes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3138.529,
    "end": 3141.073,
    "text": "You decode those spikes, that's your reconstruction.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3141.634,
    "end": 3141.995,
    "text": "That's it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3143.877,
    "end": 3149.085,
    "text": "There's other functions that I'm not gonna go, but maybe let's have a look at the KL term.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3149.185,
    "end": 3154.373,
    "text": "So you can easily understand this.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3155.194,
    "end": 3160.722,
    "text": "If you understood the slide deriving the KL term in theory, then you would understand this term.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3161.528,
    "end": 3166.293,
    "text": " Maybe I'll show you quickly the distributions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3167.314,
    "end": 3183.612,
    "text": "So we have also this base distributions, which contains our implementation of the Poisson distribution and the reparameterization for it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3187.056,
    "end": 3188.397,
    "text": "There we go, it's here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3188.528,
    "end": 3192.294,
    "text": " Yeah, so you provide it with log rates in the initialization.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3192.334,
    "end": 3199.406,
    "text": "You provide the temperature, which is the thresholding temperature, number of exponential samples.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3200.488,
    "end": 3211.366,
    "text": "And the clamp is just basically says, if the rate is larger than e to the, you know, it applies a soft clamp, which is e to the disvalue.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3212.207,
    "end": 3214.671,
    "text": "So it applies an upper bound on the rates.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3215.208,
    "end": 3217.872,
    "text": " This is for the stability of training.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3218.133,
    "end": 3219.415,
    "text": "It's not very important.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3220.857,
    "end": 3223.201,
    "text": "But the important part is here, this function.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3223.502,
    "end": 3226.667,
    "text": "That's our reparameterization algorithm.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3227.008,
    "end": 3234.28,
    "text": "Okay, so first of all, you initialize this exponential distribution in this init function.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3235.576,
    "end": 3248.314,
    "text": " So you get log rates, apply the soft clamp, and then you define rates as this just exponentiate log rates plus this really tiny value so that it's non-zero.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3250.978,
    "end": 3254.002,
    "text": "Because if it's zero, then this exponential is going to raise an error.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3254.202,
    "end": 3257.787,
    "text": "It has to be slightly above zero.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3258.053,
    "end": 3263.921,
    "text": " So you define this exponential distribution, which takes the rate as input.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3264.862,
    "end": 3266.965,
    "text": "And here I'm just borrowing from PyTorch.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3266.985,
    "end": 3273.073,
    "text": "So this, if I go up, I'm importing, it's not over here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3273.174,
    "end": 3278.24,
    "text": "I imported somewhere else, but basically, where is it?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3281.565,
    "end": 3284.689,
    "text": "This is the torch.distributions.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3285.29,
    "end": 3287.453,
    "text": "I'm just importing it as dist.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3288.023,
    "end": 3293.21,
    "text": " So this exponential is exactly PyTorch's implementation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3293.23,
    "end": 3299.699,
    "text": "All right, once we initialize this self.x object, then we can actually just perform pre-parameterized sampling.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3300.5,
    "end": 3310.234,
    "text": "So x is just the first, you know, all those delta t's that I showed in the slide, these are your x's, and you have to sample n times.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3311.075,
    "end": 3316.022,
    "text": "And then you perform this cumulative sum to get the times, and then",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3317.251,
    "end": 3323.098,
    "text": " If temperature is greater than zero, you have this soft indicator, which is just sigmoid.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3324.179,
    "end": 3329.967,
    "text": "If temperature was zero, then you would have this hard indicator, which is just times less than one.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3331.088,
    "end": 3337.155,
    "text": "If you take this operation, if you make it continuous, you will get this, basically.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3337.195,
    "end": 3337.856,
    "text": "That's the idea.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3337.876,
    "end": 3346.667,
    "text": "Once you have this indicator function, then you can sum them in the first dimension to get your spike counts.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3349.6,
    "end": 3353.248,
    "text": " All right, so that's for the Poisson, but I want to also mention",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3354.038,
    "end": 3364.371,
    "text": " this continuous VAE because this is a more general implementation that you can use for any VAE that uses a continuous distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3364.391,
    "end": 3380.35,
    "text": "In our case, we used Gaussian and Laplace, but you can actually take this implementation and easily build your own continuous VAE like Cauchy or any other distribution you like, as long as you can reparameterize it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3380.33,
    "end": 3387.42,
    "text": " which a lot of PyTorch implementations, they have this function called RSample, which is a reference to a sample.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3388.582,
    "end": 3391.527,
    "text": " But again, this is a very similar idea.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3391.888,
    "end": 3406.433,
    "text": "You encode some features, take some X, encode it into this feature H. And then because for these continuous VAEs, you need both the mean, which is location, and the variance, which is log scale.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3407.114,
    "end": 3408.997,
    "text": "So you just divide H into two parts.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3409.277,
    "end": 3413.104,
    "text": "You get these parameters, and then you",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3413.725,
    "end": 3419.23,
    "text": " you know, construct this posterior distribution and then",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3419.716,
    "end": 3420.577,
    "text": " return it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3420.797,
    "end": 3421.739,
    "text": "That's what infer does.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3422.019,
    "end": 3424.823,
    "text": "It always gives you the posterior distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3425.484,
    "end": 3432.494,
    "text": "Once you have the posterior distribution, you can rsample or reparameterized sample to get your latency.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3432.934,
    "end": 3433.655,
    "text": "Ignore this part.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3434.096,
    "end": 3436.279,
    "text": "You can actually just apply an activation function.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3436.299,
    "end": 3442.307,
    "text": "This is the Gaussian plus ReLU that I kind of skipped that didn't really talk about it, but that's there.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3443.068,
    "end": 3446.533,
    "text": "And then you can decode and y as your reconstruction.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3447.965,
    "end": 3453.333,
    "text": " So this continuous VAE implementation is fairly complete.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3453.514,
    "end": 3457.54,
    "text": "You can, in order to build your own, that's all you have to do.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3458.241,
    "end": 3461.086,
    "text": "So just say, okay, I want to make a Gaussian VAE.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3461.807,
    "end": 3464.551,
    "text": "I'm gonna inherit continuous VAE object.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3464.811,
    "end": 3467.095,
    "text": "I'm gonna say, okay, it's soft that this should be normal.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3467.496,
    "end": 3468.437,
    "text": "That's it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3468.755,
    "end": 3475.064,
    "text": " And for Laplace, again, you inherit the continuous VAE and just say the distribution should be Laplace.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3475.545,
    "end": 3479.11,
    "text": "And you can do this for any other continuous distribution.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3479.791,
    "end": 3484.097,
    "text": "Maybe that, you know, these are PyTorch's implementation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3484.117,
    "end": 3494.492,
    "text": "So any continuous distribution implemented by PyTorch that has re-permit trace sampling, you can use this to build your own VAE.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3495.13,
    "end": 3499.919,
    "text": " And then we have this categorical, which I'm going to skip.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3500.099,
    "end": 3503.726,
    "text": "But we use the, again, PyTorch's implementation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3506.011,
    "end": 3510.519,
    "text": "And OK, so this module defines VAEs.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3511.261,
    "end": 3515.028,
    "text": "And there's another module that trains them.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3515.048,
    "end": 3517.172,
    "text": "So let me just briefly go over that.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3518.384,
    "end": 3522.328,
    "text": " Again, there's a bunch of code that just defines base trainers.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3522.628,
    "end": 3539.204,
    "text": "There's a lot of details here that maybe not very interesting, but you have this trainer that accepts all these different VAE types, Poisson, Gaussian, categorical, Laplace, and it goes through, this is just one iteration.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3543.168,
    "end": 3547.412,
    "text": "There's a validation function, all of that, and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3547.882,
    "end": 3577.585,
    "text": " it's fairly complete so there's a bunch of arguments that you have to provide to be able to run this code but because i spend a lot of time to make it intuitive it's going to be just a single line if you want to train your own pvae you can just run that with a single line like you know train vae you specify the architecture in one string you specify the data set and",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3577.767,
    "end": 3580.03,
    "text": " and the GPU that you want to train on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3580.611,
    "end": 3584.517,
    "text": "And then you can actually do that if you want.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3584.957,
    "end": 3587.241,
    "text": "But yeah, this code is not available right now.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3587.862,
    "end": 3599.218,
    "text": "When I put it online, I'm going to make some documentation and put some scripts that make it easy and intuitive to just train your own model if you're interested.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3600.717,
    "end": 3604.402,
    "text": " There's a lot more, but I think at this point I'm going to stop here.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3604.442,
    "end": 3608.689,
    "text": "There's a bunch of other stuff that is secondary.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3608.889,
    "end": 3611.413,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3611.433,
    "end": 3613.195,
    "text": "Awesome.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3613.215,
    "end": 3613.536,
    "text": "Okay.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3614.197,
    "end": 3621.307,
    "text": "While I crop back the screen, maybe just let's start with how did you get to this problem?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3621.788,
    "end": 3624.512,
    "text": "Were you pursuing Poisson distributions and this came up?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3624.552,
    "end": 3629.239,
    "text": "Or how did you kind of in your research path come to this approach?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3630.89,
    "end": 3638.672,
    "text": " Oh, so the real initial motivation is just we did another paper.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3639.243,
    "end": 3643.109,
    "text": " last year on a Gaussian VAE that was hierarchical.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3643.209,
    "end": 3652.824,
    "text": "And we found that we trained that Gaussian VAE, that Gaussian hierarchical VAE, on a motion data set.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3653.445,
    "end": 3663.139,
    "text": "And we found that it learned latent representations that were very aligned to real biological neurons from the monkey brain.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3663.58,
    "end": 3667.005,
    "text": "So we have another data set from",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3668.183,
    "end": 3677.434,
    "text": " the dataset was they showed some moving dust monkeys and they recorded from their MT, which is a brain region that's, you know,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3677.954,
    "end": 3680.218,
    "text": " that is responsible for motion perception.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3680.639,
    "end": 3686.369,
    "text": "Anyway, we found that these VAEs actually have a really high chance of being brain-like.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3687.19,
    "end": 3691.998,
    "text": "And that gave us motivation to think about, OK, how we can make them even more brain-like.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3692.018,
    "end": 3702.156,
    "text": "So Gaussian VAEs just encode inputs in continuous values that are not constrained by any means.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3702.136,
    "end": 3704.684,
    "text": " But we know that that's not how brain works.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3704.764,
    "end": 3707.392,
    "text": "We know that neurons produce spikes.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3707.814,
    "end": 3708.797,
    "text": "That's how they communicate.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3709.278,
    "end": 3713.27,
    "text": "So we said like, okay, VAEs have potential, but how can we make them better?",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3713.371,
    "end": 3715.437,
    "text": "That was really the initial motivation.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3716.716,
    "end": 3717.177,
    "text": " Awesome.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3717.617,
    "end": 3732.376,
    "text": "Well, just to kind of recap a little bit of how I saw this, often parameters in biological or cognitive modeling are modeled as continuously dynamically varying because there are certain simplifying assumptions.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3732.957,
    "end": 3744.411,
    "text": "However, the empirical data, be it neural spiking or events or like ants foraging, how do you bridge the discrete events",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3744.526,
    "end": 3769.015,
    "text": " to the continuous and the um mathematical equivalence which is well known between the event arrival distribution with the prasad and the waiting time i saw that as a way that you implemented with the statistical methods to and also in doing so generalized like what was relevant for the base model for continuous or discrete",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3769.653,
    "end": 3784.651,
    "text": " And then how can that be articulated with the statistical packages so that there's good course handles to explore these learning settings?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3784.671,
    "end": 3789.036,
    "text": "I think that's actually a very interesting and deep point.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3789.617,
    "end": 3794.563,
    "text": "Even though neurons communicate with spites or discrete action potentials,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3795.1,
    "end": 3803.516,
    "text": " when we are modeling them, you can actually think as, you know, the actual underlying dynamics could happen in a continuous space.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3803.536,
    "end": 3812.934,
    "text": "Like, for example, we have these log rates, you know, those log rates are continuous values between minus infinity and plus infinity, really.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3812.974,
    "end": 3815.539,
    "text": "I mean, even though in reality, it doesn't go that far, but",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3815.519,
    "end": 3839.907,
    "text": " they're defined in if you have n neurons log rates are defined in rn right and then you can assume that the model is just doing some trajectory like the brain is doing going through some trajectory in that log rate space and in each given time you can actually take this that log rate and then map it to some posterior by computing the exponential of that to get the rates and then draw samples",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3839.887,
    "end": 3850.698,
    "text": " And in that sense, you can actually see that a continuous underlying dynamical system can be mapped into a discrete observation through Poisson.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3851.618,
    "end": 3860.087,
    "text": "And also that every time point in the dynamics of the brain also corresponds to some posterior inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3860.107,
    "end": 3865.472,
    "text": "So these concepts, I think, can be nicely reconciled with this view.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3866.195,
    "end": 3878.567,
    "text": " yeah that was great and to kind of connect it to active inference and a little bit of the precursors there with spm statistical parametric mapping the whole setting is there's underlying neural activity",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3878.75,
    "end": 3896.292,
    "text": " hidden state, external state, and then there's noise plus signal representing a static mapping between the hidden and the observable, what you had as X and Z. So that's like the partial observability situation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3896.312,
    "end": 3899.897,
    "text": "And then that's the experimental situation.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3900.434,
    "end": 3902.417,
    "text": " but you don't get to observe neural rates.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3902.457,
    "end": 3916.157,
    "text": "So like in the SPM textbook and the kind of modeling, you model it with the predator-prey type models or with dynamical attractors, but you stay in the purely dynamical, and then that doesn't let you get to the rate coding.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3916.178,
    "end": 3929.918,
    "text": "But also people have integrated those models, however, in more of a two-step process rather than the one-step unified objective of the autoencoder model.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3931.771,
    "end": 3953.695,
    "text": " yeah so that's pretty interesting so because rate is in a sense the rate itself is a latent variable right we never observe it we it's a it's a made-up concept really by the theorists we say okay then if there is an underlying rate and that is producing these observed spikes",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3954.316,
    "end": 3959.651,
    "text": " you know, in the brain, then you can actually try to infer the rates by observing many spikes over many trials and so on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3960.293,
    "end": 3969.338,
    "text": "So in that sense, yeah, rate itself is a latent variable, but that maybe shouldn't go there because that like adds another layer",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 3969.318,
    "end": 3994.38,
    "text": " complexity well one question i had about rate was if you're going to model it you kind of can't get away from it as a underlying dynamical variable because there's going to be variability in delta t variance amongst arrival time so maybe it's a metronome but in the non-metronome case there's going to be kind of like faster and slower intervals so that's kind of a common filter",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 3994.732,
    "end": 4002.804,
    "text": " question, which is, or ARIMA or any time series, ultimately, how smoothly should you interpolate that?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4003.265,
    "end": 4020.291,
    "text": "Like with the splines, with what method will the data kind of have a relationship with a curve that has the smoother differential properties that ultimately allow the closed form solution and the simple algorithms?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4022.995,
    "end": 4023.456,
    "text": "Makes sense.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4025.562,
    "end": 4032.542,
    "text": " Okay, I'll see if anyone asks a question, but just one question now, and then I'll ask maybe one or two more.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4033.103,
    "end": 4038.458,
    "text": "Like, where do you see this being useful slash applied now?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4040.446,
    "end": 4052.907,
    "text": " So we are actually currently building based on this observation that there was a large amortization gap between Poisson VAE and this kind of relatively old sparse coding algorithm from 2008.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4054.069,
    "end": 4059.378,
    "text": "The one conclusion we drew from that is that, you know, we need iterative inference.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4059.899,
    "end": 4063.024,
    "text": "So right now the inference is just one pass.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4063.324,
    "end": 4064.867,
    "text": "You take one image,",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4064.847,
    "end": 4069.214,
    "text": " You infer something, compute delta r, and then you just get the posterior.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4069.294,
    "end": 4086.921,
    "text": "But now suppose you have a sequence, you have a video rather than images, and you want to just keep iteratively making your inference better and better such that your posterior at time t informs your prior at time t plus 1.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4086.961,
    "end": 4089.224,
    "text": "That's very essential to Bayesian thinking.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4089.805,
    "end": 4092.93,
    "text": "So we're kind of right now developing that model.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4092.91,
    "end": 4096.643,
    "text": " And our hope, very long-term hope is that",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4097.77,
    "end": 4112.47,
    "text": " when we add that component and maybe we add hierarchical structure to the latent space, then we get something that is truly brain-like and it reproduces what we've observed, what we've learned about visual processing in the last 50 years.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4113.091,
    "end": 4121.662,
    "text": "So if we get that model, then we can perform in silico experiments in it, learn something about brain-like visual processing and so on.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4121.682,
    "end": 4126.849,
    "text": "So that's kind of roughly the trajectory we imagine.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4127.858,
    "end": 4128.239,
    "text": " Awesome.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4129.682,
    "end": 4130.964,
    "text": "Is there anything else you want to add?",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4131.065,
    "end": 4138.099,
    "text": "Otherwise, I think this was amazing on the theory and really seems useful with the sample efficiency, all these other features.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4140.003,
    "end": 4141.386,
    "text": "Yeah, I guess that's it.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4141.406,
    "end": 4141.566,
    "text": "Yeah.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4141.746,
    "end": 4142.488,
    "text": "Thanks for having me.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4142.708,
    "end": 4143.229,
    "text": "It was really fun.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4143.951,
    "end": 4144.292,
    "text": "Awesome.",
    "speaker": "SPEAKER_00"
  },
  {
    "start": 4145.654,
    "end": 4145.935,
    "text": "Okay.",
    "speaker": "SPEAKER_01"
  },
  {
    "start": 4147.366,
    "end": 4174.352,
    "text": " just read one comment from someone wrote great work thanks to active and agha hadi for sharing looking forward to find out what a different inference scheme could do the results and the effect it could have on the metabolic cost okay yeah thanks for the comment yeah cool all right thank you see you all right thank you bye",
    "speaker": "SPEAKER_01"
  }
]
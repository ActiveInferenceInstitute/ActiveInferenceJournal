SPEAKER_00:
all right hello and welcome to active inference model stream number 13.1 it's August 23rd 2024. we're here with two of the authors of the paper synthesizing the born rule with reinforcement learning so thank you both for joining for this presentation and discussion to you Josh for the presentation


SPEAKER_01:
Great.

Thanks for inviting me, Daniel.

And yeah, it's really fun to learn about the Active Inference community and to speak to you guys.

So I think there'll be as many questions from us to you as there will be from you guys to us.

um so i'm not going to take up too much time i hope with my slides i wanted to just give a kind of high level overview of this article which hopefully many of you have already had time to look over but i'll give you a high level overview and that shouldn't take more than about 30 minutes i guess and after that we'll have plenty of time for discussion um there will be

many points where you might feel that you want more information, more technical details.

But since I couldn't really predict which points might strike you as the most interesting, I'll leave it to you to ask at the end.

And as necessary, I might be able to pull up the article and point to the relevant parts.

oh yeah uh let's dive into it oh and one last thing you might hear some background noises of either drilling or uh baby screaming that's just uh because i'm at home at the moment and uh it's part of the domestic atmosphere okay let's go so

This is a very busy slide, sorry for that, but we do have a lot of people involved in this from many different institutions.

I'll put this slide also at the end of the talk in case you're curious, but it's important to mention we've got five people based at the Institute of Physics in Lab Q Rio, Rio de Janeiro, and that's where I'm currently based.

Although before this, during the time that this paper was mostly written, I was at the University of Massachusetts, Boston, together with two other co-authors on this paper are from there as well.

And we're all part of the Cubism group.

run by Chris Fuchs.

So I'll speak a little bit about cubism, which is an interpretation of quantum theory based on subjective Bayesian decision theory.

A bit of a mouthful, but if you're curious about that, we can talk about it.

at the end.

And John has since gone on to the University of New Mexico.

And the lead author of this work is Rodrigo Piero, who is a PhD student at the time of this work.

And he's now at the Technology Innovation Institute in Abu Dhabi.

So this is another paper that was published

Some years ago now, just during the pandemic or towards the end of the pandemic,

And this was by us at the Cubism group.

So you can see John and I are there.

Chris Fuchs is there.

This was a purely theoretical paper, but it contains the seeds for the article that I'm going to talk about.

So I'm going to just take a little diversion to explain what we did in this paper as a motivation for the article that I'll talk about.

Here's a little excerpt from

from the abstract.

The title of the paper is Born's Rule as a Quantum Extension of Bayesian Coherence.

And explaining that a little bit from the abstract, the subjective Bayesian interpretation of quantum mechanics that we're calling cubism asserts that the Born rule is a normative rule in analogy to Dutch book coherence.

I'll explain what Born rule and Dutch book coherence are, don't worry.

but with the addition of one or more empirical assumptions characterizing the particularities of the physical world.

So the idea is that we're taking this rule called the Born Rule from quantum physics and we're saying that you can understand that rule as coming from decision theory.

Decision theory plus certain empirical assumptions.

Okay, so let me explain what that means.

bit of terminology.

What is this interpretation, the subject of Bayesian interpretation of quantum theory?

Well, there's a lot I could say about it, but for the purposes of this paper, decision theory, in other words, the theory of agents who are

trying their best to make good decisions in the world, and they use probability theory in order to do that.

And quantum theory takes into account certain empirical facts, things that we've learned from experiments, and gives us an upgrade to our decision theory.

And that's what we call quantum mechanics, according to Cubism.

And when I talk about Born's theory,

You might have come across it in what I'm calling the legacy form.

It's the rule that tells you when you have some quantum state and you do a measurement on it, which traditionally is an observable that has some set of eigenstates labeled with, I've just labeled them abstractly with an index j. I'm assuming a discrete spectrum here.

Then to find out the probability for getting some outcome j,

You just have to take the overlap

of the quantum state with the relevant eigenprojector.

And you take the modulus squared of that to give you a positive real number.

And that's the probability.

But nowadays, we've generalized the rules.

So from the literature on open systems and information theory, we have accommodated more general measurements.

So you don't just have to measure observables, which kind of project you into an eigenbasis

where each projector is orthogonal to the other ones.

You have these things called positive operator value measures, which is a horrible name.

We just call them POVMs and everyone even forgets what they stand for.

But you can basically think about them as a generalization of observables.

They're the most general thing, the most general kind of measurement you could possibly do in quantum theory.

So that's what that D subscript J stands for.

It's an element corresponding to outcome J of the most general quantum measurement.

And then in that case, the quantum state is represented by a density matrix, which is that little Greek letter rho under the trace.

And if you take the trace of the state with the POVM element dj, then that will give you the probability for the outcome j in that quantum state rho.

I hope that's familiar to at least some people watching this, but if not, feel free to quiz us on it.

It's not super important to the rest of the talk, though.

The important thing is Born's rule is you can think of it like a piece of the machinery of quantum mechanics that takes your quantum state and measurement as input, and it outputs the probability that you should assign for each outcome.

So it's a very important piece of the quantum formalism.

And what we found in this paper, which is the motivation for all of this, is the following.

Imagine that you have a perfectly rational decision-making agent.

Okay, perfectly rational agents don't exist in reality, but they're useful to contemplate in the abstract.

And by perfectly rational, I just mean that

it respects the basic laws of probability theory.

So it assigns probabilities to events which add up to one complete mutually exclusive set of events that the probabilities are positive between zero one and so forth.

And in particular, it assigns probabilities in a consistent way.

And I'll explain a bit more what that means.

So suppose this agent is placing bets on the outcomes of measurements on some physical system.

And we're going to assume that they hold certain beliefs about how the system responds to those measurements.

The first set of beliefs

Again, this is a high level.

If you want to know the actual assumptions, you'll have to look at the paper or ask me about it afterwards.

But there's a set of assumptions that are generic.

So they are things we would expect to hold in any physical theory.

And they include things like the possibility of state tomography.

In other words, you should be able to reconstruct the physical state of the system by doing a sufficiently complete measurement on it and looking at the statistics of the outcomes.

And maximality is just a principle we use in any situation where there is ambiguity about how to choose things.

We'll just use a maximality principle, which kind of assumes the least structure necessary in order to draw conclusions.

You could also call it a minimality assumption.

I mean, these things can always be flipped around.

It's a kind of all else being equal.

assumption so generic stuff but then there's an empirically inspired I call it a proto-quantum assumption why proto-quantum because this assumption actually itself is not sufficient to imply quantum theory but it's also something which you would not expect to hold in a classical theory

So this is the key assumption which takes us outside of the domain of classical physics and takes the first step in the direction of quantum theory without actually presuming the full quantum formalism.

And this assumption is defined purely at the operational level.

So using information theory, you can consider things like how much data can I reliably store and retrieve in my system?

And that gives you a concept that I'll call the informational dimension of your system.

So if you have a quantum bit, for example, even though a quantum bit is in a certain sense more informationally rich than a classical bit, ultimately you can only store a single classical bit of information in a way that can be reliably, perfectly encoded and retrieved.

So the informational dimension of a classical bit and a qubit is two.

But then there's another concept of dimension, which this is not conventional usage of this term, but I find it convenient.

I call it the tomographic dimension.

which basically says, remember, I'm assuming state tomography.

So there's some measurement you could do on the system.

This is an assumption such that if you knew the statistics for the measurement outcomes, you could perfectly reconstruct what the state of the system was.

And then you can ask the question, well, what's the minimum number of outcomes which that measurement needs to have?

So that's what I'm calling the tomographic dimension.

And essentially our proto-quantum assumption is that the tomographic dimension is equal to the square of the informational dimension.

I know that's a lot to take in.

um if you want we can come back to it it's not super important for what follows but i thought it's worth mentioning that this assumption is is inspired by experiment there's no reason why out of the blue somebody would believe that this should be true in fact in classical physics it's not true classically the tomographic dimension is exactly equal to the informational dimension

So the only reasons an agent would believe in this particular assumption would be because of trying to explain statistics which we have from measuring quantum systems, among other reasons.

And our main finding in this old paper is that if the agent

assigns their probabilities according to these two sets of assumptions, then on pain of inconsistency, in other words, in order to be perfectly rational and consistent, they would have to assign their probabilities according to the Born rule.

So even though we haven't assumed the mathematical formalism of quantum theory,

We have found that just a minimal non-classical assumption is already enough to get you the Born rule, exactly the quantum Born rule.

So that's quite interesting.

I mentioned here a Dutch book.

If you're wondering what that is, it's part of what we mean by a rational agent.

In the Bayesian literature, a Dutch book is a series of bits

which the agent would agree to anyone taking individually.

But if you consider them together, they would be irrational to accept.

So it's kind of a litmus test for whether an agent's beliefs are internally consistent.

The point is just that the agent would be irrational unless they use the born rule in this case.

So what does this mean?

What's the upshot of this result?

Well, putting it in context, it's important to emphasize that this is the first time that I know of that the Born Rule is derived from essentially pure decision theory plus some pretty minimal empirical assumptions without presupposing

uh the quantum mechanical formalism by which i mean things like hilbert spaces unitary maps and so on you know complex numbers all of that previously in the literature the born rule has been assumed as an axiom um just posited uh that was how it was first introduced by max born himself

And there have been attempts to derive it from decision theory, but they usually need to invoke some of the formal structure of quantum mechanics, such as unitary dynamics.

So we've really made significant progress compared to that literature.

But I think the deeper point is that this shows that Born's rule is far more universal than quantum theory.

it actually applies in a range of settings where your theory might not be quantum theory, it might be some more general theory that's just not classical.

And it really just stems from some fairly basic assumptions about what the agent believes about the system.

And you could say one conclusion to draw from this is that a perfectly rational agent who has the right kind of perceptual apparatus in order for them to see these empirical quantum effects in the right kind of environment would essentially be forced to use Born's rule to make their decisions.

Well, that's nice.

And it got us thinking, just how universal is it?

So we've been talking about rational agents who explicitly make decisions using probability theory.

That's pretty advanced for an agent.

That means that they can think, they can reason explicitly.

They have minds and consciousness and not just consciousness, but mathematics, formal mathematics.

They can write down probabilities and so on.

And we were wondering, well, but the elements here are so simple that it seems like you might not even need that much.

Firstly, what if the agent wasn't perfectly rational?

There's a lot of literature studying deviations from perfect rationality in

in conscious reasoning agents, which is super interesting.

But we went to an even more basic level, and we were kind of inspired by the act of inference, the ant colony paper, which is why we cited it in that work.

because that work kind of inspired us to wonder whether simple organisms like ants, which definitely don't have any explicit representation of mathematics, are nevertheless able to behave in a way which conforms to some fairly sophisticated abstract rules.

So we set out to discover whether a really simple simulation of an agent

could end up behaving in a way that would conform to the quantum born rule without having any explicit knowledge of quantum theory or even being able to explicitly use probabilities.

So in order to actually carry out this research,

There's a lot of things that one has to decide.

So there's so many ways that you might think of modeling the simple agent.

And to say nothing of the question of how to model the environment.

So I'll talk about both of those in turn.

First of all, we want our agent to be ignorant of quantum mechanics, as I said.

But it's survival or there should be some success parameter which depends on how well it's able to predict the outcome of a measurement on some quantum system.

The idea being that it's through its measurements on quantum systems that it will ensure its own survival and that's why it will ultimately, we hope, learn to behave in a way that respects the

the born rule.

And so our agent will learn from its experience.

And to do that, we will use the kind of action response feedback loop.

So at each moment, the agent will place bets in which it kind of tries to guess the probability for some outcome to occur.

And then depending on what happens and what the actual outcome is, it will get some reward slash penalty

And that will then cause it to adjust its behavior for future vetting.

And we made some additional simplifying assumptions.

We really wanted to start with the simplest thing we could possibly think of just as a preliminary test.

And in many ways, we

perhaps oversimplified, but we also did quite a lot to, you know, even though our algorithm you'll see is not very sophisticated, we also helped it quite a lot in other ways.

So I'll explain that.

So we assume firstly that it's Markovian.

In other words, the decision it makes at any given moment should only depend on the average expected reward for each of the possible actions it could take.

So it doesn't remember exactly the sequence of rewards that it obtained for a given action.

It just remembers the relative, the average expected reward for each one.

So it kind of has a memory just in terms of its state at a given time, but that state only keeps the information of the relative weightings.

And I call it a proto-Bayesian agent.

It can't be a Bayesian agent because it's not using probability theory.

It's just reacting in a kind of a dumb, simplistic way.

But it's proto-Bayesian in the sense that if you give it enough data, so if you take the limit as the data goes to infinity,

you want to find that it will behave exactly the same way that an intelligent conscious rational Bayesian agent would.

So that would kind of be the limit of, you know, the limit of large data corresponds to the limit of perfect rationality.

And when you look to lower amounts of data, you can think of that as a weakening of perfect rationality due to the finite resources of the agent.

And that's what we wanted to investigate.

What happens when you go to smaller amounts of data, which is what you would expect for a real organism.

Can't sit around collecting data without paying some kind of cost.

It doesn't have infinite time or energy.

And a very natural way to implement this kind of algorithm is a reinforcement learning algorithm.

And this is a sketch of how it runs in our implementation.

So at each step s, and for each possible outcome j of whatever measurement we consider, I'll get to that, the agent has to choose a bet B of j from a discrete set of n options.

which take values in the interval between zero and one.

So the idea here is that the agents, when in choosing a bet, they are guessing essentially what the probability is for the outcome J. Of course, they don't use probabilities explicitly for the agent.

This is just an action.

The reason for us putting it in this interval is because we know as outsiders that at the level of analysis, these things will, in the limit of large numbers of steps S, these bets should correspond exactly to probabilities.

But it's from the agent's point of view, if you like, this is just a discrete set of N possible actions that it could take.

And that's really all it means to the agent.

And there is an interesting issue here because you would say, well, if these things are supposed to correspond to bets on probabilities, we're kind of fundamentally limiting the agents because we're not allowing them to pick any real number between zero and one.

And that's an interesting point.

We had to do that because you can imagine that if the agent were allowed to pick any real number,

And they would start with, say, uniform prior on the unit interval.

then they would never actually choose the same action more than once.

The probability would be vanishingly small that they would ever repeat an action.

And an agent that behaves like that just has no hope of surviving.

You can't really work with that.

So it's important that the agent is able to repeatedly choose the same action so that they can learn what kind of responses they get.

There might be more sophisticated ways to deal with this, but we found that actually beyond a certain threshold of the number of choices n, the value of n doesn't matter.

So as long as you have kind of a fine enough splitting up of the interval that gets you approximately close to whatever probabilities you want to give, it works well enough.

So the agent chooses the bet.

And then the measurement happens.

So again, I'll discuss the implementation of the measurement next.

But the measurement results in some outcome, which I'm calling J prime.

And then we check whether the current outcome that we're making bets on is equal to J prime.

And we define this parameter E. So E is one, if we're looking at J prime and zero, if we're not.

And then the reward is basically just the difference between the agent's bet and E, which is zero or one.

and you take that to the square and subtract it.

So why do you do that?

This is called the quadratic loss function.

And the usual motivation is that this is the reward function, which guarantees that if you were actually a Bayesian rational agent assigning probabilities explicitly, this reward function guarantees that the bets that you place

will match your subjective probability assignments.

So again, this comes from our demand that the agent be a proto-abation agent.

Basically, choosing this reward function guarantees that in the limit of many steps, the relative

the weightings that the agent assigns to the bets that it chooses will correspond exactly to the actual probabilities for the outcomes.

So that's why we picked it also because it is mathematically the simplest possible reward function.

Yeah, I mean, I call it a reward function, but it's negative reward.

So it's actually a loss function technically.

and the agent is just going to optimize that to try to minimize its losses.

How does the agent do that?

What strategy does it use?

Now it's made a bet, some outcome happens and it receives some feedback.

How does it then choose what to do on the next step?

And we use a pretty standard algorithm for that called an epsilon greedy algorithm.

Basically, there's some parameter epsilon, which is supposed to be small, meaning significantly smaller than one, but not infinitesimal.

You can kind of play around to make it what you want.

And with probability epsilon, the agent just chooses randomly.

So it kind of just

chooses to see what will happen without any particular reason.

But then most of the time, probability one minus epsilon, it will go with the safe bet.

So it'll pick the action which in the past has given it the highest return.

And this is a pretty intuitive algorithm when you're thinking about actual organisms, because if you just imagine in everyday life, you're picking what restaurant to go to is a classic example.

Some of the time you want to just go with a safe bet, the restaurant which you've been to before and you know that it's good.

But if you were to just do that, you would never actually find potentially better restaurants.

So sometimes it's worth taking a risk and just trying a random new restaurant just to see if there might be something better out there.

So this is a classic exploration versus exploitation balance and the parameter epsilon tunes how much the agent favors one or the other.

And again, it's a very natural thing to pick in the context of models of actual organisms.

So finally, let's talk about the environment, because this is in a way where the action happens.

If our agent is going to have any hope of learning to behave in a way that's sensitive to quantum mechanics, at least that conforms to the Born Rule, then there has to be something special about the environment.

If we just dumped it in a thermal bath or in some environment that could be perfectly explained by some classical physical model, there would be no reason to expect the agent to behave in a way that follows the Born rule, because the Born rule is inherently a non-classical

It applies outside the domain of classical experimental physics.

So what do we need for our environment to have that sufficiently rich structure?

Basically, we need to make sure that the measurement statistics, whatever measurement we pick on a quantum system has to be a rich enough measurement that it exhibits quantum structure.

Well, it has to exhibit enough structure that it can't be explained classically by some classical model of the environment.

And that led us to use something called an informationally complete quantum measurement, POVM.

And you also need to consider states of the system that are being measured.

You need a variety of different environmental states, and that also should be informationally complete.

So together, this is often called a tomographically complete set.

because the idea is that if you know the statistics for the measurements for all of these states, you can reconstruct the mathematical form of the quantum state.

So it's kind of a safe bet.

If you have an informationally complete measurement and set of states, you can be sure that the structure of quantum theory is represented within the statistics.

Now, this may be a stronger assumption than we needed to make.

But as I said, it's definitely sufficient.

And this is one of the ways that we helped out our agent, made their life easier, because realistically, we don't make any claim that a hypothetical agent that was evolving in a quantum environment would actually be doing this kind of measurement.

And indeed, it's not even clear what that would even mean, what kind of measurements they would be doing.

So we're just avoiding that whole thicket of questions and just saying, okay, let's just do whatever would make it easiest for the agent.

And we go even a little bit further just to simplify our own lives as well.

We assume that this measurement has a certain mathematical symmetry property.

It's what they call symmetric information complete POVM.

We like to call it a SIC, S-I-C.

Please don't call it a SIC.

They're really interesting mathematical objects.

John knows all about them, and I'm sure we'll be happy to tell you anything you want to know.

But for our purposes, what's great about them is that they really simplify the form of our equations.

And it turns out that at least in the lowest number of dimensions where this is interesting, you can actually implement them experimentally, which I'll talk about soon.

So in this article, we actually didn't do an experiment.

We just proposed one.

We did a simulation.

And that's another way in which we made life easier for the agent.

Because in a simulation, you can assume that your quantum states are exactly

what you want them to be, pure states, no noise, measurements you can assume are perfect.

And again, realistically, if this were an organism, the kinds of measurements that we'd be doing would be completely messy.

The kind of states describing the variant would not be pure states.

They would be thermal, they would be mixed.

Again, we're just sidestepping that and saying, let's just see what makes it easiest for the agent, even in that best case scenario, how well would the agent perform?

in a simulation and we can propose an experiment to kind of see how that would play out.

I wanted to just show you this slide to give you an idea of what it would look like to do a symmetric information complete measurement in the lab.

So this is something that they can do here in Rio.

um actually have done um it's a really neat way of implementing uh this kind of um actually not just a metric but any uh kind of four outcome povm so this is the an apparatus called the displaced sagnac interferometer in quantum optics um the idea is that you um

So that's a laser beam going through there that, well, it's actually not laser, it's the path of a single photon that's coming from spontaneous parametric down conversion, which produces pairs of entangled photons.

And you send one photon into this apparatus following that red line.

And the other one goes to a detector, which will indicate

whether its partner photon is present.

That's important because it's a spontaneous process.

You don't actually know when there's a photon in there unless you grab the partner photon.

So the presence of the partner photon tells you, yep, now there's a photon in the apparatus.

OK, so the other photon goes into this complicated looking thing.

I won't walk you through it, but you can see there that there are four different output ports.

which label the four possible outcomes.

So here we're talking about a system that has informational dimension two, but it has tomographic dimension four.

And since the seek is information complete, it needs to have four outcomes.

The reason that it's informational dimension is two is because here we're using essentially as a qubit, we're using the polarization degrees of a single photon.

So that photon will go in there and it will pop out at one of these four detectors.

And depending which one it pops out at, that gives you the outcome of your measurement.

And the quantum system, which is kind of representing the states of our environment, is the single photon itself.

which can be in different quantum superpositions of polarization.

So it's a very contrived implementation, but again, it's like the simplest thing you might possibly consider.

And you can imagine that your agent somehow depends on its livelihood, depends on predicting where these photons will, where the photon will pop out in each step.

So every time you send a photon through, the agent tries to

bet on the likelihood that it will pop out at each of these ports.

And every time the photon pops out somewhere, then depending on what bets the agent place, they will get rewarded accordingly.

So this is how one could implement this.

Now, we ran our simulation and we simulated exactly that setup

as a kind of prelude to see what we should expect.

And we had a working hypothesis going in, which is that the agent should loan the bond rule fairly easily.

And our reasoning was that we've gone to a lot of trouble to make this experiment, this proposal as ideal as possible.

We're assuming

pure states, perfect measurements and so on.

So how hard can it be?

This is like the most quantum thing that you could possibly do in dimension two.

You cannot get more quantum than this.

If you like, it's like you're saying this is an environment where we've dialed up the quantumness to maximum.

So come on, it can't be that hard.

But it turns out it is pretty hard for the agent to learn to behave according to the Born rule.

In other words, if you do an analysis of its betting strategy,

you can measure how close its bedding strategy is to the quantum rule, the Born rule.

There's a bit of math involved in how you do that comparison, but basically you can formulate it as a comparison between two matrices, and you can compute something called the Hilbert-Schmidt distance of the two matrices, and you want to see that distance basically

getting down to zero ideally as the agent gets closer and closer to making bets according to the quantum the optimal quantum strategy now indeed if you go up to more than 10 to the power of five steps the agent starts to approximate um the born rule which is but that's what you would expect at that number of steps you're in the regime of large data

At that point, the agent should be by construction, it should be behaving like a perfectly rational agent.

So there's no surprises there.

What did surprise us is that it couldn't get such good performance without such a large amount of data.

So it seemed that no matter how we fiddled with the parameters, we just couldn't get it to perform better than that.

And this is an open question.

I mean, whether we could think of more clever algorithms, more sophisticated algorithms that could get it there.

But there's a subtlety because you can't just pick any algorithm that would give you optimal performance.

because your algorithm's doing double duty.

On the one hand, it has to be good at detecting the quantum effects in the statistics, but it also has to be a plausible representation of an information-gathering organism that's concerned with its own survival.

And it's that second point that makes this all interesting.

because it's not super well defined what class of agents meet that description.

So I'll just mention that somewhat frustratingly, we found that at relatively low numbers of steps, like 500 steps, the value of n, like the number of possible bets the agent can make, doesn't seem to matter much.

So I mentioned that earlier on.

And I guess that's kind of intuitive because realistically, when we make bets, we don't make bets of values below one cent.

Our decimal places all terminate at two when it comes to placing bets, unless you're using Bitcoin.

But let's not talk about that.

So the idea is it's natural to have a kind of coarse graining of your bets, aka probabilities for the rational agent.

And that shouldn't really matter as long as they're finely grained enough.

And indeed they don't.

Something that was more of a surprise is that epsilon doesn't matter at relatively low step counts.

You can see why at very large

data sets, epsilon shouldn't matter.

Remember, epsilon is the probability for exploration, trying new things.

The reason is that as long as you have some non-zero epsilon, when you run enough steps, that will be sufficient for the agent to explore the whole space of possible actions many times.

So you would expect that for sufficiently large data, epsilon should stop being important.

But we didn't quite expect that it would stop being important so soon.

So that's kind of interesting.

But what the upshot of this means is just that the main limiting factor on the agent's performance turned out to be just the amount of data itself, just the sheer number of steps.

So the lesson that we drew from that is that unless the agent could afford the time and energy needed to acquire this much data, and think 10 to the five steps, depending what field of science you're coming from, that may or may not seem like a large number.

But if you think of things that an organism has to do in order to take an action and get a response from the world, eat a piece of fruit and see if it kills you or not, that kind of thing, 10 to the 5 is pretty demanding.

And so if we grant the 10 to the 5 is an unfeasibly large number for a typical, a reasonable model of an agent, then we would have to conclude that it's just not going to learn to behave in a manner that reflects

the finer quantum structure of its environment, even when that environment is explicitly quite strongly quantum.

And this was surprising to us.

We really didn't expect that.

Now, it's difficult to draw a definitive conclusion from this, but we are tempted to say something like the following, inspired by some work by cognitive scientist Donald Hoffman.

Some of you might know of it.

who he argues that in general, there's a selection pressure against organisms evolving to perceive the world as it is truly, that in fact organisms which are too accurate in their perceptions of the world will be selected out.

It's just not an adaptive advantage.

And he argues essentially that true information is not necessarily useful information, that what we're evolved to perceive should definitely not be expected to be what's true, but rather what's relevant to our survival.

And that made us wonder.

We posed the following question based on these preliminary results.

Could quantum sensitive behavior ever be beneficial to an organism's survival?

Or is it just too hard to learn?

If we could make more formal and well-defined and rigorous definitions of these terms, this would be a really interesting question to answer.

The problem is that to really decide the issue, it seems that we need a generally accepted criteria for what we mean by life and what we mean by viability of an organism that would then constrain the space of allowed agent models.

As I said, there's a subtlety in making sure that you're not just picking an optimal model from a performance point of view, but choosing a model that plausibly

represents an actual living agent.

And as I said, our model, in some ways, it's too constrained because our algorithm is so simplistic.

You might think that there might be algorithms which are still realistic models of agents, but which might perform better because they just have more possible decision making machinery available to them.

But in other ways, our model is not really constrained enough because, as I said, we're assuming ideal measurements and so on.

So a more general approach is needed to be decisive.

Our preliminary results are suggestive, but I wouldn't say they're conclusive.

So this is the conjecture, just to put it up on the screen.

Even if quantum effects are strong in the environment, quantum sensitive life is generally selected against by survival pressures, i.e.

is not viable.

In other words, maybe, conjecture, maybe the only route to quantum sensitive behavior is via higher order cognition like us.

We humans, we're building quantum computers.

We're definitely behaving in a way that's sensitive to quantum mechanics, but that's through explicit reasoning.

The conjecture is that there's no shortcut, that there's no way a mindless agent could evolve adaptively

to become sensitive to quantum effects at the behavioral level just through evolution.

Somehow, knowing quantum mechanics has to be like an accident of cognition, a bit like appreciating music.

I mean, it's a pipe dream.

I don't know if we'll ever be able to resolve this conjecture, but I put it there to stimulate discussion.

So thanks again for listening, and I will open for questions.


SPEAKER_00:
Thank you.

Perhaps, John, if you'd like to give a first comment, and then Ander, welcome.


SPEAKER_02:
Sure.

Thanks, Jack.

I think you pretty much covered all the bases.

And somehow I feel like I learned a little bit about our paper just hearing you sketch it just now, even though I'm one of the authors.

But yeah, maybe I'll just reiterate a few of the things you said.

because maybe the points that I think are most important to really take away, because there's a lot of quantum detailing here that doesn't matter for the kind of conclusion that we want to talk about maybe.

And so I guess the main thing is that if you really had an ideal Bayesian agent, well, they would do what they do.

Also, you teach them quantum mechanics.

And then they behave in a certain way.

The key then ultimately is that behavior.

What we want to figure out is whether there could be a shortcut, right?

So we came to cubism from the idea that decision theory can explain the born rule, like you said in our previous paper.

But real agents are part of the world.

They're not ideal.

They are limited.

And so

You know, the question is whether there's a shortcut to the same behavior so that from the outside, it's sort of approximately indistinguishable from an ideal Bayesian agent.

And again, we thought it was really cool that, you know, you can model things like an ant colony as a Bayesian agent, even though of course it's not writing down probabilities and utilities and maximizing expected utility and all of those things.

It's just that it takes behaviors that are sort of indistinguishable from that whole general process.

Um, so if there were a shortcut, then, um, then you don't really need full blown Bayesian agent concept, right?

That would be the idea.

So, and then, okay, so that that's, that's the motivation as Jacques already covered.

I'm not really saying anything new.

Um, but we came at this in just the very first order kind of way, I think, because what we did was we took the Bayesian agent concept and weakened it in pretty much the weakest way that you could weaken it.

We have agents here that are making bets.

They don't know probability theory, but it's very thinly veiled.

We basically arranged so that in the infinite data and continuous limit, then their bets are probabilities, full stop.

And in order to be coherent, they need to obey probability theory and they need to obey the Born rule and that infinite data would just inevitably lead them there.

And indeed, we do get there when we get enough data.

Um, but if it had been the case that sort of a low amount of exposure to their environment allowed them to get pretty close to the born rule, that would have made us think, okay, well.

uh maybe the the whole idea of these higher order thinking beings that kind of can implement something like bayesian decision theory and tilbert spaces and so forth that's overkill and what you really get you can really get all of this behaviorally and what actually matters just from a simple agent-based model

And we found the opposite.

Basically, we found that it took a lot of data and a lot of jerry rigging and helping and idealizations and, you know, simulating things without noise or anything like that before we can actually get the agent to behave in a way that is close enough to the born role to be meaningful.

So that is some evidence, as Jacques said, in the direction of, well, maybe we really do need these higher order agents that

have to use Bayesian decision theory and actually think and abstract and so forth.

But it's also, I think we shouldn't forget that it's definitely possible, in fact, maybe even likely that we didn't weaken the Bayesian agent framework idea enough in our simulation.

And I say that because the bets here are sort of different probability values.

What if we had something far more heuristic?

And it's hard for me to think about how we would actually compare it to the real born rule in that case.

But we've certainly not ruled out the possibility that a different kind of simple agent model could effectively learn to behave in a way that

aligns with quantum theory with lower resource cost.

In the way we did generalize it, it's sort of the goalpost is still you get everything that you get in the Bayesian agent setting.

And so maybe by definition, it requires a lot of resources to win that game.

It's not clear.

But anyway, those are pretty much my comments.


SPEAKER_00:
Awesome.

Thank you.

Thank you, Andrew, for joining.

Would you like to give an initial comment?

And then I'll read any questions.

So then we'll continue the discussion.


SPEAKER_03:
Yeah, I'm not really in a position to comment much because I didn't get to read the paper.

But it was a very interesting talk.

The one thing I want to ask is, could you show the slide on the Dutch book again?

That's around the time I showed up, I think.

And then

Yeah.


SPEAKER_02:
Was it a couple of slides later, or was it this one?


SPEAKER_03:
Maybe a bit later.

Yeah, yeah, yeah.

Here?

Can we recall what was going on here?

Because it reminded me of something else that we've seen with Chris Fields, Daniel.

But I think I missed the explanation.

So it reminds me of contextuality in Chris's work.

But I might be messing it up.

I didn't hear the entire explanation.


SPEAKER_01:
Do you want to know how do we use the Dutch book here?

Like, what role is it playing?

Is that what you're asking?


SPEAKER_03:
Yeah, no, I sort of got it right.

I think any one decision here on its own, you could approach optimality, right?

But when you start taking into account several possibilities, that's when you have the Dutch book phenomenon.

Is that right?


SPEAKER_01:
Yeah, I think so, if I understood you right.

Basically, anytime you have an agent who has a set of beliefs expressed as probability assignments to some set of events that they're concerned with, then there's always going to be a question of whether those different

constraints so their beliefs will generally be expressed as some constraint on their probability assignments right and they might have a bunch of constraints and you want to make sure that those are all consistent with each other so the dutch book is is just mathematically a really nice way of testing that basically if you can find a dutch book

Using the agent's own probability assignments, that means that their set of beliefs, like the set of constraints that they're trying to adhere to, are mutually inconsistent.

Did I explain it well, John?


SPEAKER_02:
Yeah, that's true.

I think one other thing to point out here, maybe, that Ander might be asking about is

So the two things you've emphasized here, essentially, well, actually, you only really emphasized one of them.

But in general, the Dutch book idea is like a logical consistency between the different beliefs that you have.

And like, for instance, if I said there's a 60% chance it's going to rain today and a 50% chance it won't rain today, well, I can't hold both of those beliefs.

And the idea of a Dutch book is that I could ask you to pay a certain amount for one ticket that would pay the dollar if it rains and a certain amount for a ticket that would pay 50 cents if it doesn't rain, and then you just lose money for sure.

The Dutch book idea is you're vulnerable to a sure loss.

So that's a purely logical constraint on your beliefs.

But the idea that was motivating this previous work was that what if that's, you have that, of course, being consistent is important, but you also perhaps need to be consistent with some particular kind of characteristic of the physical world.

So there's a sort of coherence that's imposed logically and a coherence that's imposed physically.

And in the context of informationally complete measurements,

Just consider a single measurement and just your probability for that.

Well, logically, the only constraint there is that you have a probability simplex and you don't assign something like probability 110% to some event or below 0.

So it has to be between 0 and 1 inclusive.

But for an informationally complete measurement, physically, we just notice empirically

quantum mechanics implies a restriction of the probabilities that are valid for full stop for the assignment for a minimal informationally complete measurement.

And so quantum state space, the set of valid quantum states you can have corresponds to a proper subset of a big informationally complete probability simplex.

So that's another kind of,

consistency you need to keep in mind if you want to actually be consistent with logic and with sort of physics in the world.


SPEAKER_03:
i see thank you yeah so i i think i think i sort of get it and and it the point is well taken right because um the whole i don't know if you guys are aware of the course that happened last year but one of the things that chris mentioned with uh the quantum framework of active inference is that you have this this idea of contextuality which happens whenever you have

non-commuting QRFs, the quantum reference frames, which is what an active inference agent deploys to perform measurements and actions.

And whenever you have non-commuting QRFs deployed, I think basically the idea is that you cannot have a well-defined joint probability on the boundary.

So it's morally very related, but I will have to check the details.

Thank you, guys.


SPEAKER_01:
Actually, now I get what you're aiming at.

Yeah, it's not just morally related.

I think it is the same thing or closely related, like formally, because actually the way that our co-author on the paper, Blake Stacey, put it is that it's an assumption of no hidden variables.

which amounts to contextuality basically saying that there can't be a joint probability distribution.

I mean let me try to say something that is both precise and probably correct.

If you believe that you're dealing with a contextual phenomenon

that you have measurements which are non-commuting in the relevant sense, then I think you would need to have something like our assumption two.

There is, although I'm not sure that it would give you, in quantum theory, then two would definitely hold.

So this tomographic dimension is information dimension squared.

Because in quantum theory, the minimal number of outcomes for an information and complete measurement is d squared, the Hilbert space dimension squared.

But if you look in a more general probabilistic framework, if you have some definition of contextuality, it's not clear to me if that would give you the squared.

It would give you something like this.

So I would expect you to have a non equality here.

So it would be non classical in the sense that your tomographic dimension should not be equal to the information dimension when you have contextuality.

proof pending.

I bet that that's true.


SPEAKER_02:
I believe it is the case that the assumption of the possibility of a non-contextual hidden variable model is possible to formulate in terms of these proper subsets of quantum state space that I was talking about.

I mean, of the informationally complete simplex that I was talking about.

It just so happens that quantum state space itself pokes outside of that.

And so that means that in general, it's not compatible with the idea of a non-contextual hidden variable model.

Or to put it more positively, if you want to talk about hidden variables, which we don't personally, but if you do, then you have to have a contextual model.


SPEAKER_00:
Well, thank you.

that's very good awesome okay I'll read some of the comments in the live chat um and then I'll I'll have a few things I wrote down so um the text a little small but rev says is the agent always trying to guess the same quantum state


SPEAKER_01:
Well, so first the point of clarification, the agent isn't really trying to guess a quantum state.

They're doing something much more basic.

They're just trying to guess what the outcome of the measurement will be.

and well they're trying to guess the probability for for each outcome is it a fixed coin flip with a static or changing yeah so i mean those those probabilities will be different for different quantum states uh you know of that for instance in our implementation you your environment is is just a single photon with some polarization state and that polarization state depending what it is will give you different probabilities to get the outcomes

And the answer is yes, we do need to consider different quantum states.

The reason is because if you just hold the quantum state fixed, your statistics will not be rich enough to be distinctively quantum.

So in that case, you would be able to explain those statistics with a classical model and you would not expect your agent to be able to learn anything quantum in that situation.

So you need to have


SPEAKER_02:
not just several quantum states, but quantum states which are sufficiently varied across state space that it... Yeah, but when an agent is trying to learn a certain probability, though, we assume that they're in the same context where they have the same state and the same measurement, and they repeat that a bunch of times.

So that might have been the question.

I just want to make sure.

Oh, yeah, good point.


SPEAKER_01:
Yeah, I didn't clarify that.

Yeah, so the agent is allowed to know when it's in a different environmental context corresponding to a different state.

So it can put the relevant labels on its bedding behavior.


SPEAKER_00:
Awesome.

Very connected with context switching and attention.

I'll read a question or a comment in the chat.

Andrew Peshea wrote,

actinth agents priors over observations their preferences which is what generalizes and includes the reward maximization in the pragmatic value and then expected free energy adds in the epistemic value actinth agents priors over observation their preferences as survival related homeostats expected free energy drops the need for defining an epsilon for random slash exploratory behavior hmm


SPEAKER_01:
I have difficulty in responding to that because I'm not sure how free energy connects with this.

But this is something I wanted to ask.


SPEAKER_02:
Maybe this might be exactly the kind of reason why we need to talk with you guys because I didn't even understand the question.

But clearly, you're coming from a background that knows more about the agent modeling side of things than we do.


SPEAKER_00:
Let me pull back in just a few of the things that I wrote, because there's kind of a triple intersection that's funny and led to this through citation letters, etc.

So kind of.

from how I understood the journey and the generalization slash weakening, but experimentally designed and the constraints of the real experimental setup and the more generic equations and also weakening those or figuring out how they could change.

So you set out from the quantum born across the real experimental interfaces and got to some of these multi-scale, possibly adaptive dynamics like reward, surprise, fitness,

that are tautological about situations, then in active inference, there's kind of a dual origin, which is like one of the figures in chapter one of the 2022 textbook with this idea of active inference in the middle or coming from or to the low road and the high road.

So the low road is like the generalized sensory motor loop.

So kind of like cybernetics or with the embodied connection like robotics or embodiment.

And then also the high road is the Bayesian mechanics, free energy principle, principle of least action for Bayesian, all of those kinds of things.

So those are the first two threads with the quantum born empirical setting and then the active inference kind of, well, there is this sequentiality to behavioral updating

on the inbound sensory and on the outbound action which is like selecting a bet is like an action and chris field takes that into the quantum fvp which is like more of a recent development and then the third element is like the ants in for ants which was the paper from 2021 so a lot to say about ants but the colony for analyzing the colony as a phenomena it

it highlights and it starts from the lateral interactions amongst nest mates like the collective behavior and the interactions with the niche including like different kinds of pheromone decay or something like that like a sunny context might have a higher rate of decay and then also there's this kind of pragmatism of measuring anything

which you have in the laboratory in the quantum setting and then also there's that experimental constraint in the laboratory for ants and also in the field for ants so it's kind of like the pragmatism and the real finite constraint and kind of our experimental reality and then it's just interesting that you came at this from the best

experimental and updating process about systems that that many might say are non-cognitive or subcognitive or something like that or just differently and then also but from the more generalized cognitive side that has led into quantum bays whether or not there's like a mechanistically quantum effect in the brain that doesn't mean that the decision theory isn't already transferable

What do you think?

A lot of things in there.

I'm busy absorbing that.


SPEAKER_01:
Yeah.

You know, you mentioned, I mean, one of the things about this work on active inference is the multi-scale structure.

And this was something that in our paper, we kind of hand wave towards the end about our work really focusing on, I guess, what you were calling the low road, which in the sense that we thought of our little proto agent as more like an ant.

But then we kind of posed the question of whether if you got a lot of these guys together and have them interacting or somehow working together, you could kind of look at the whole system and get an emergent behavior to which you could attribute, you could model from a kind of Bayesian rational agent point of view.

Um, but that, you know, we didn't really get as far as addressing the, the kind of multi-scale stuff, I think, because we're still trying to work out the fundamentals, you know, pure action response dynamics.


SPEAKER_02:
I mean, maybe we need the multi-scale thing in order to meaningfully generalize or to weaken.

Like that might be the right thing to do.

Um, and it may maybe make sense that.

It's so hard to make a simplified sort of non-cognitive approximation of the Bayesian agent model in the way that we did.

But we did it this way because we didn't know any better, essentially.


SPEAKER_00:
It's awesome.

Two of the things from Chris Fields' course, the physics as information processing, was also relaxing sometimes constraints about space and time, like having agents make their own spatial temporal reference frames.

So rather than just plugging in agents into kind of a grid work that's defined by an observer, whether or not they're modeling themselves.

And then also just the environments and the partition, the interface, the blanket, all those different ways to describe the interface between internal and external states or whatever notation or variables are used.

So that's kind of like the communication setting, like Alice and Bob.

But then maybe Bob is the ground and it's pheromone distributions as actions, but then it's not about like whether or not that's like an action of the ground.

It's like, well, that's being modeled by a person making another finite model.

So it's a map territory for how they're drawing it.

So there's a lot of interesting pieces and great, great topics to go into.

Yeah.

one other connection is you said something about the general born extending into the unobservables and this is like one of the key moves with the separation of observations and latent or hidden or external states so there's like a the ambiguity matrix which is just a bi-directional mapping what would the measurement distribution be from a hidden state what does this observation

then the observations can be attached with epistemic value and a pragmatic value related like the cost of sampling and just the preferences against expected future observations those are all kind of like cognitive motifs like we could explore what experiments do or do not show that a nestmate plans can you falsify or put a probability distribution over the idea that it's doing something beyond its immediate

sensory blanket?

How many time steps of memory?

How many time steps of anticipation?

That's a model selection question with a certain lab or field data set.

It's like, well, there's there could be a difference, but it would require getting 10 to the five foraging runs by the same nest.

Yeah, but it may never take that many.

And so that also kind of talks back into the evolutionary priors and the distributional starting position.

which is like always a problem in applying Bayesian statistics.

And then there's like a kind of neat fitness oriented way to give the prior.


SPEAKER_02:
Yeah.

I mean, and also, I mean, use the nest mate language, which is a fun thing to talk about.

I mean, in our sense, like one single ant alone is pretty useless because it can never live long enough to get 10 to the five things.

But if it's got a bunch of nest mates and they can communicate with their pheromones and so forth,

then maybe 10 to the 5 isn't a lot when you have a bunch of agents interacting and so forth.


SPEAKER_00:
It's a great, interesting debate with the ant colony world.

And what's interesting about ants, I mean, among things is they're obligately colony living.

Whereas other hymenopteran, like even the bees and the wasps, they have a lot more variability with different life histories, but then the ants have the obligate eusociality.

So that kind of like the termites are different than those other ones, like in both those or any obligately social form, then the unit of fitness can be kind of targeted as, and so it sets a norm for,

not the only possible norm but it's like optimal foraging theory it's like is about one nestmate getting the most food per unit time it's like maybe if that were a special special case of water loss and time and colony hunger and the seasonality then it could be considered optimal but if the optimal strategy is not then the sheer amount of foraging activity or type of it

It can't be considered like it's only optimal within an evolutionary context.

Otherwise, you're just saying, who is foraging more?

Right.


SPEAKER_01:
Right.

This is a bit off topic, but it's a wild idea that just kind of popped into my head now that we're talking about actual organisms like ants.

It occurs to me that most of the

living creatures that we could think of off the top of our heads so far as we know are not specifically sensitive to quantum effects i know there is a literature on quantum effects in biology but it's usually really down at the chemical level like in photosynthesis um or uh but you know i wonder like there's work that you know the human eye

is it they know that the the cells in the retina are sensitive down to the single photon level like in principle just that a handful of photons can trigger a retinal cell to fire and there are experiments kind of working on i know because my wife who's also one of the co-workers on this paper is working on this stuff here in brazil um but

uh it makes me wonder if like maybe there are because there are creatures out there with some pretty amazing capabilities like there's the mantis shrimp which has these crazy eyes that have trinocular vision and i don't know what else you know maybe there's some living creatures that we could actually get into a controlled lab environment and have them actually interacting with

uh quantum stuff you know if they're photon sensitive you could kind of see if you can fire photons at them and get them to behave in in a responsive way i don't know that just seems like it might be really cool i'm just throwing that out what would it like mean or show if there was a way to do it


SPEAKER_00:
would just be cool man that made me think of having a photon instead of killing a hypothetical cat it releases one like pheromone for a moth or one molecule of glucose for a bacteria so and that's kind of what the synapse does with the discretization into the packets and then to the neurotransmitter discretization level with release

And that also relates to something that came up a lot in the Chris Fields was the interplay of kind of classical and non-classical state spaces and the interface as being like the more classical barrier and then different aspects of that when it's interacting with the quantum Alice and quantum Bob.

Andrew, want to add any more?

Where else do you think it would like connect?


SPEAKER_03:
Yeah, that was an interesting thought by Jax.

My question to you is, don't you think that the issue of agents being able to observe quantum effects

you know, I see it related to the FEP in the following way.

Like one of the latest papers, he talks about the FEP driving compartmentalization.

And you can imagine that as an organism, you know, becomes larger and more sophisticated, we'll have more degrees of freedom

available to be able to pick up very subtle quantum effects and an even more outdoor idea you know it almost feels like no difference to how we need high energies in accelerators to observe real small stuff you know yeah one other kind of topic i'd love to hear more about was the fourfold nature of the experiment


SPEAKER_00:
because that made me think about a collision of the particles and how that's often thought of as like two interacting components.

However, there's also like, let's just say that there are two real interacting

real question mark, then you have an interface and then like a S and an O, you have observation and then at least a measurement frame that's been learned to calibrate like the A matrix projecting onto like a P value or base factor, some other kind of other further downstream analysis.

So it's like, are there four things there?

And then what are those four

And so what is it about the fourness of the empirical setup and how is it exactly two for the exponents or how does, is that variable or is that always an integer?


SPEAKER_02:
That too is purely from Hilbert space geometry.

And so insofar as we use quantum theory and well, insofar as the model is right,

and associated to any system, you can always write down a Hilbert space of some dimension, then it's just a matter of mathematical fact that the state space only spans a d squared.

Well, actually, d squared minus 1 because there's a normalization.

But anyway, it's a d squared dimensional operator space, which is where quantum states live.

Yeah, it can't be d squared plus one or d squared minus one unless you're talking about something other than quantum theory.

And so far, we don't seem to have anything other than quantum theory.

So yeah, that empirical part, I mean, we call it empirical, but it's empirical at the level of, is this theory we've been using for 100 years actually right?

Not empirical at the, well, we measured this mass, and it looks like it's within this and this, and it's plus or minus.

So it's like...


SPEAKER_01:
It's a bit of a lie to call it, it's not purely empirical.


SPEAKER_00:
It's meta-empirical and it's pragmatic in the sense of pragmatism, which isn't some kind of pure utilitarianism, but pragmatism just deals with the pragmatic aspects of operations.

Mm-hmm, OK.


SPEAKER_01:
Yeah, like, mathematically, if you want to do state tomography, you're going to need at least d squared outcomes, and they better be good ones.


SPEAKER_00:
Yes.

Yeah.

and then an interesting extension into the behavioral space where neither kind of i mean mass-based classical models but even kind of classical inspired models don't describe all behavioral data and then like if the observation has dimensionality n

like n sensors for sensor fusion then a has n squared because it's square or second powered to to map all the matrix mapping all by all observations to hidden states and then b which is what the control policy is exerted as a slice from which is like a behavioral collapse from makes it another power because there's how the the s vector

changes.

So a kind of balloons it out, but then projects it back down to some other possible space.

And then B, it squares us because it's a transition selection.

And so then there's a lot of interesting traces to follow there.


SPEAKER_01:
I confess that mostly went over my head.


SPEAKER_02:
But yeah, it's not Yeah.


SPEAKER_00:
What about the plus or minus 1?

Which plus or minus 1?

From the d squared plus or minus 1.

Why could it be plus or minus?


SPEAKER_02:
Oh, no, I was saying it couldn't be.

Oh, okay.


SPEAKER_01:
Yeah, yeah, sorry.


SPEAKER_02:
Yeah, if it were different, it would be a different theory.

That's what I'm saying.

Yeah, so it looks like it has to be d squared.


SPEAKER_01:
Yeah, I mean...

it's something that would be nice to explore one day.

Like what what are these theories that have different numbers that are not just the squared?

But yeah, too many questions too little time.


SPEAKER_00:
We're only on a 4d live stream.

You can't.

Well, what happens next?


UNKNOWN:
Or


SPEAKER_00:
Where should others, or what are you going to explore?


SPEAKER_02:
I think, you know, based on this conversation, the thing that sounds most promising to me, if we were going to take this further is the multi-scale part.

Like I think really we want to model agents or an agent or the, the, the, the, the target of Bayesian agent has to be sort of the emergent property of many little guys.

I think that's, that's probably.

the most pertinent thing to take from your kind of community and what we totally ignored in this work.

And so we barely mentioned that in the paper, but I think, personally, that feels like the most exciting possible way to push this direction further.

I don't know about Jacques.


SPEAKER_01:
I think, John, I think you're probably right.

Just pragmatically speaking, that seems like something that's concrete and well-defined enough that one could actually make progress in that direction.

And it would be interesting.

but you know just speaking from like what what is like the deep question that i would really like to know the answer to but i'm not even sure how to formulate it in a way that could be attacked with research is you know there's this idea that quantum theory is a bit weird because of this you know like the non-contextuality thing

And the world that we live in and that so far as we know all living creatures live in is not a quantum world.

We're macroscopic, thermal, completely decoherent creatures.

Decoherent in the sense that quantum theory just isn't important to our everyday

interactions with our world.

And I guess I'm wondering whether that's just an accident of the fact that life evolved in a very hot, wet regime where quantum coherence doesn't survive.

or whether there might be a deeper reason behind it, that somehow if you try to create life under really controlled conditions where you enabled quantum coherence to survive and quantum coherence to be really strong in the environment,

know imagine like if if Planck's constant were just some really large value in a hypothetical universe could life evolve in that universe or is there something about the weirdness of quantum theory that that forces life to only exist in in effectively in decoherent regimes you know that's kind of a deep question I have I'm sure that our very preliminary work hasn't shed any light on it but I think it did

do the valuable job of raising it as a question.


SPEAKER_02:
Yeah.

I mean, cause what could have happened was we could have just immediately found that even in the first attempt to weaken the Bayesian agent thing, it just gets the born role immediately.

And then you can say, well, probably, well, but it also would have, I mean, you know, it would have said, oh, well, we're sort of barking up the wrong tree with this high level Bayesian decision theory approach.

It's not necessary.

Um, so far as we know, it is necessary still.

Uh, but, um, anyway, we didn't rule it out.


SPEAKER_00:
Yeah.

An interesting thread there.

It's the paper discussed in live stream number 49 with Ramstad and Saktive Devel with showing the kind of development and generalization of classical thermodynamic, infodynamic, and then quantum, and then getting into the cognitive like

a quantum observer or interface that being the bayesian mechanics which is the where the path of least action free energy principle all that is existing and it's like another generalization from the it's another degree of freedom which is the analytic aspect and the pragmatically constrained reality of it for the quantum investigator

So you're doing it, so that's cool.

We're on the path.


SPEAKER_02:
We're doing something.


SPEAKER_00:
Well, any last comments for anyone?


SPEAKER_02:
Read the paper.


SPEAKER_00:
Read the paper.

It's a classic.


SPEAKER_01:
It will be when people read it, otherwise.


SPEAKER_00:
Yeah, maybe.

It will be a classic, yeah.

Looking forward to it.

We'd be happy to stay in touch.

And Ander and others and Chris Fields, we could explore a lot more.


SPEAKER_02:
Yeah.


SPEAKER_00:
And the measurements and the quantum.

And we can explore with the AMP multi-agent frameworks.

Yeah.

So there's a lot of great paths.


SPEAKER_02:
I think Jacques and I need to read up on that and then we'll come back to you.

That seems promising to me.

Okay.

Thank you.

Thanks for having us.

Bye.


SPEAKER_03:
Thanks for having us.